URL: http://www.cis.ohio-state.edu/lair/TechReports/91-pa-hraams.ps
Refering-URL: http://www.cis.ohio-state.edu/lair/Papers/Directories/h-dir.html
Root-URL: 
Email: pja@cis.ohio-state.edu pollack@cis.ohio-state.edu  
Title: Hierarchical RAAMs: A Uniform Modular Architecture  
Author: Peter J. Angeline and Jordan B. Pollack 
Address: Columbus, Ohio 43210  
Affiliation: Laboratory for Artificial Intelligence Research Department of Computer and Information Science The Ohio State University  
Date: June 4, 1993 1  
Note: The Ohio State University  
Abstract: Scaling-up is a well-known problem for connectionist and neural network architectures. One solution is the modular use of smaller networks. However, pre-wiring modules with ad-hoc engineering is not a satisfying solution. Recursive Auto-Associative Memory (Pollack, 1988,1990), an architecture which develops representations for symbolic trees, is not immune from scale-up problems. As the set of trees become larger and/or deeper, RAAMs require larger representation widths and longer convergence times. We solved this problem through a new kind of modulariza-tion, where multiple thin RAAMS are used hierarchically instead of one wide RAAM. The Hierarchical RAAM for a particular task is more compact, taking advantage of intermediate levels within the large trees. Further, the overall cost of training is significantly lower due to the need for fewer weights. Finally, this architecture is a step towards the automatic discovery of appropriate boundaries between cognitive modules. 
Abstract-found: 1
Intro-found: 0
Reference: <author> Angeline, P. J., & Pollack, J. B., </author> <title> Avoiding Fusion in Floating Symbol Systems, </title> <institution> Laboratory for Artificial Intelligence Research Technical Report #PA-91-AVFUSION, The Ohio State University, Columbus, Ohio. The Ohio State University June 4, </institution> <note> 1993 6 Ballard, </note> <author> D. H. </author> <year> (1987), </year> <title> Modular Learning in Neural Networks, </title> <booktitle> Proceedings of the Sixth National Conference on Artificial Intelligence, </booktitle> <pages> 279-284. </pages>
Reference: <author> Barnden, J. A. </author> <year> (1991), </year> <title> Encoding Complex Symbolic Data Structures with Some Unusual Connec tionist Techniques, </title> <editor> In Barnden, J., and Pollack, J., editors, </editor> <title> Advances in Connectionist and Neural Computation Theory: High-Level Connectionist Models, </title> <publisher> Ablex Publishing Corp., </publisher> <address> Norwood, NJ. </address>
Reference: <author> Dyer, M. G. </author> <year> (1991), </year> <title> Symbolic NeuroEngineering for Natural Language Processing: A Multilevel Research Approach, </title> <editor> In Barnden, J., and Pollack, J., editors, </editor> <title> Advances in Connectionist and Neural Computation Theory: High-Level Connectionist Models, </title> <publisher> Ablex Publishing Corp., </publisher> <address> Norwood, NJ. </address>
Reference: <author> Gibson, J. J., </author> <year> (1966), </year> <title> The Senses Considered as Perceptual Systems, </title> <publisher> Houghton Mifin Company, </publisher> <address> Houston. </address>
Reference-contexts: We will argue in the conclusion that this type of uniform modular architecture will be capable eventually of discovering the boundaries between cognitive modules solely from the organization of the environment <ref> (Gibson, 1966) </ref>. 2.0 Experiments As the domain of demonstration, we have extended an example from (Pollack, 1988) in which encodings for several words constructed solely from the letters in the set -B, R, A, I, N- were created by a RAAM. <p> The presence of modularity in the information associated with cognitive activities, such as written words composed of letters or spoken words composed of phonemes, is not merely a useful fiction but is an important quality of the data which indicates something of the evolution of the mind-brain <ref> (Gibson, 1966) </ref>. While it was beyond the scope of this study to allow the architecture to discover an appropriate modularization of the data, our future systems will be able to organize their own stable levels according to discovered regularities in the environment.
Reference: <author> Hinton, G., E. </author> <year> (1986), </year> <title> Learning Distributed Representations of Concepts, </title> <booktitle> Proceedings of the Eigth Annual Conference of the Cognitive Science Society, </booktitle> <pages> 1-12, </pages> <address> Amherst, Mass. </address>
Reference: <author> Jacobs, R. A., Jordan, M. I. and Barto, A. G. </author> <year> (1990), </year> <title> Task Decomposition Through Competition in a Modular Connectionist Architecture: The What and Where Vision Tasks, </title> <type> COINS Technical Report 90-27, </type> <institution> Massachusetts Institute of Technology, </institution> <address> Cambridge Mass. </address>
Reference: <author> Miikkulainen, R., & Dyer, M. G. </author> <year> (1988), </year> <title> Forming Global Representations with Extended Back Propagation, </title> <booktitle> Proceedings of the IEEE Second Annual Conference on Neural Networks, </booktitle> <address> San Diego. </address>
Reference: <author> Minsky, M. L. & Papert, S. </author> <year> (1988), </year> <title> Perceptrons, </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass. </address>
Reference-contexts: 1.0 Introduction It is a widely held view that neural networks do not scale well <ref> (Minsky & Papert, 1988) </ref>. This is not merely a problem for neural networks, but has aficted symbolic AI as well (Pollack, 1989).
Reference: <author> Nowlan, S. J., and Hinton, G. E.(1991), </author> <title> From Competitive Learning to Adaptive Mixtures of Experts, </title> <editor> In Touretzky, D. S., editor, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <publisher> Morgan Kaufmann Publishers Inc. </publisher>
Reference: <author> Pollack, J. B. </author> <year> (1990), </year> <title> Recursive Distributed Representations, </title> <journal> Artificial Intelligence, </journal> <volume> 46 (1):77-105 Pollack, J. B. (1989), Connectionism: Past, Present, and Future, AI Review, 3: </volume> <pages> 3-20. </pages>
Reference-contexts: Furthermore, if interesting and powerful configurations of uniform modules were found, these organizations would be more acceptable (Ballard, 1987). It is along this latter line that our work is offered. In this paper, we reaffirm Simons parable through experiments on a large-scale application of Recursive Auto-Associative Memory <ref> (Pollack, 1990) </ref>. RAAM is a connectionist architecture that develops distributed representations of variable sized, compositional data structures. It has been used to model the encoding of hierarchical structures such as those found in language syntax and The Ohio State University June 4, 1993 3 logical expressions (Pollack, 1990). <p> of Recursive Auto-Associative Memory <ref> (Pollack, 1990) </ref>. RAAM is a connectionist architecture that develops distributed representations of variable sized, compositional data structures. It has been used to model the encoding of hierarchical structures such as those found in language syntax and The Ohio State University June 4, 1993 3 logical expressions (Pollack, 1990). Conceptually, a RAAM consists of two machines, a compressor and a reconstructor. The compressor is trained to recursively encode sets of fixed-width patterns into single patterns of the same size.
Reference: <author> Pollack, J. B., </author> <year> (1988), </year> <title> Recursive Auto-Associative Memory: Devising Compositional Distributed Representations, </title> <booktitle> Proceedings of the Tenth Annual Conference of the Cognitive Science Society, </booktitle> <address> Montreal. </address>
Reference-contexts: We will argue in the conclusion that this type of uniform modular architecture will be capable eventually of discovering the boundaries between cognitive modules solely from the organization of the environment (Gibson, 1966). 2.0 Experiments As the domain of demonstration, we have extended an example from <ref> (Pollack, 1988) </ref> in which encodings for several words constructed solely from the letters in the set -B, R, A, I, N- were created by a RAAM. The structures we used to represent these words was meant to capture many of the perceptual levels involved in visual word recognition.
Reference: <author> Rumelhart, D. E., Hinton, G. E. & Williams, R. J., </author> <title> Learning Representations by Back-Propagat ing Errors, </title> <journal> Nature, </journal> <volume> 323: </volume> <pages> 533-536. </pages>

References-found: 12

