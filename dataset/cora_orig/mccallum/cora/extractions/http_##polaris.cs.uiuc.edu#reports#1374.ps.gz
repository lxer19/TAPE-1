URL: http://polaris.cs.uiuc.edu/reports/1374.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: COMPILER OPTIMIZATIONS FOR PARALLEL LOOPS WITH FINE-GRAINED SYNCHRONIZATION  
Author: BY DING-KAI CHEN 
Degree: 1989 THESIS Submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy in Computer Science in the Graduate College of the  
Address: 1985  1994 Urbana, Illinois  
Affiliation: B.S., National Taiwan University,  M.S., University of Illinois at Urbana-Champaign,  University of Illinois at Urbana-Champaign,  
Abstract-found: 0
Intro-found: 1
Reference: [AK87] <author> R. Allen and K. Kennedy. </author> <title> Automatic transformation of Fortran program to vector form. </title> <journal> ACM Trans. on Programming Languages and Systems, </journal> <volume> 9(4) </volume> <pages> 491-542, </pages> <month> Oct. </month> <year> 1987. </year>
Reference-contexts: In fact, the transformations and optimizations introduced in this thesis do not distinguish whether there are dependence cycles in the loops, as long as there are loop-carried dependences. The alternative for parallelizing loops with loop-carried dependences but without dependence cycles is to distribute the loops <ref> [AK87] </ref> to create many smaller DOALL loops. One side effect of this kind of transformation is that the barriers inserted between DOALL loops may offset the speedup of the small loops. 3.2 Vectorization, DOALL Parallelization, and Software Pipelining Other loop parallelization methods abound. Vectorization is used on vector machines.
Reference: [All86] <institution> Alliant Computer System Corp. Alliant FX/Series Architecture Manual, </institution> <year> 1986. </year>
Reference-contexts: This usually implies that there are dependence cycles. Statements involved in the same dependence cycle will be put in a serial loop. Another similar parallelization technique, DOALL parallelization, is used primarily on machines without vector capability or to exploit parallelism in addition to vectorization <ref> [All86] </ref>. The procedure is similar to the vectorization, except that, instead of generating vector instructions, statements are included in a DOALL loop as long as there is no loop-carried dependence. <p> Three conditions for identifying dominant locks are provided, but they cover only very limited cases. Midkiff and Padua [MP86, MP87] described schemes to generate synchronization instructions by a compiler using test and testset instructions, which are very similar to await and advance instructions used in the Alliant FX/80 <ref> [All86] </ref>. They also introduced the Control Path Graph (CPG) to show the ordering imposed by the synchronization and the control dependences on an MIMD machine. To determine if a synchronization is uniformly redundant in a single loop, they calculate the transitive closure of the remaining synchronizations to cover it.
Reference: [Amd67] <author> G. M. </author> <title> Amdahl. Validity of the single processor approach to achieving large scale computing capability. </title> <booktitle> AFIPS Spring Joint Computer Conf., </booktitle> <volume> 30 </volume> <pages> 483-487, </pages> <month> August </month> <year> 1967. </year>
Reference-contexts: The four optimization strategies discussed in this thesis demonstrate clearly the possibility of obtaining good speedup with smaller communication overhead for loops with uniform, non-uniform, or even unknown dependences. These parallelization/optimization schemes for the serial loops are very important because according to the infamous Amdahl's law <ref> [Amd67] </ref>, a small speedup in the serial part can bring significant improvement in overall speedup. 132 8.2 Future Research Directions Our discussion so far has been based on the assumption that we have serial execution within each iteration.
Reference: [AMS88] <author> R. Anderson, A. Munshi, and B. Simons. </author> <title> A scheduling problem arising from loop paralleliza-tion on MIMD machines. </title> <booktitle> In 3rd Aegean Workshop on Computing, AWOC 88,Corfu,Greece, </booktitle> <pages> pages 124-133, </pages> <month> June/July </month> <year> 1988. </year>
Reference-contexts: The heuristics algorithms previously proposed basically have two phases <ref> [Cyt84, AMS88] </ref>. In the first phase, the graph is partitioned and the partitions are then ordered. In the second phase, statements within each partition are ordered. <p> In fact, anomalies could occur where the optimized order actually has a longer delay. 23 A similar method was also proposed by Anderson et al. <ref> [AMS88, SM90] </ref>. In their first phase, statements are level-sorted into partitions considering only the loop-independent dependences. The source and the sink statements of a loop-independent dependence are placed in different partitions. <p> of all the loop-carried dependences, which limits the strategies that can be used in the second phase. 4.3.2 Hierarchical Scheduling ANew Approach Because it is not adequate to preserve all forward dependences [Cyt84], and it is difficult to determine which loop-carried dependences should become forward dependences in the first phase <ref> [AMS88] </ref>, our 24 strategy is to postpone making the decision for each loop-carried dependence until it is necessary.
Reference: [Ban88] <author> U. Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer Academic Publishers, Norwell, </publisher> <address> MA, </address> <year> 1988. </year>
Reference-contexts: Vectorization is used on vector machines. It can handle loops with forward dependences, but not with backward dependences. The loops are examined with data dependence analysis <ref> [Ban88] </ref>. The statements in the loops are then arranged in a way that as many dependences go forward as possible. Then vector instructions are generated for each statement unless it is the sink of a backward dependence. This usually implies that there are dependence cycles. <p> Other optimizations, such as loop blocking for better locality or unrolling for better instruction-level parallelism, are orthogonal to our discussion and hence are not included. First, each loop is examined for loop-carried dependences by dependence analysis <ref> [Ban88] </ref>. If there is no such dependence, the loop is a DOALL loop and the parallelization is straightforward. If loop-carried dependences exist, we have to determine if the dependence distances (vectors) are constant (vectors) , or equivalently, if the dependences are uniform. <p> The equations to be solved are: 71 ( i 2 + 4i 3 = j 1 + j 2 : (6.3) We can use the algorithm presented in <ref> [Ban88] </ref> to determine their integer solutions. If there is no integer solution, no synchronization is needed for this dependence. If there is exactly one solution, only the two iterations involved, I p and I q , need to synchronize their execution. <p> Proof: After substituting Equation (6.4) into Equation (6.2), the constraints in Equation (6.2) are closed half-spaces in R n and therefore are convex. The intersection V of these convex sets is also convex. Equation (6.4), which is the general solution to Equation (6.1) solved using the algorithm in <ref> [Ban88] </ref>, 72 has the following matrix form (i 1 ; : : : ; i l p ; j 1 ; : : : ; j l q ) = tG (6.6) where G is an (m + n) fi (m + n) unimodular matrix and t = (c 1 ;
Reference: [Ban90] <author> U. Banerjee. </author> <title> Unimodular transformation of double loops. </title> <booktitle> In 3rd Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1990. </year>
Reference-contexts: More recently, a unified framework for loop transformation, called Unimodular Transformation, becomes increasingly popular <ref> [Ban90, WL91] </ref>. Given more accurate characterization on the range of each entry in the dependence distance vectors (Equation (6.7)), we can determine when loop reversal, interchange, or skewing is legal.
Reference: [BBN87] <institution> BBN Advanced Computers. Butterfly products overview, </institution> <year> 1987. </year>
Reference-contexts: Second, the strips are assigned to the processors statically so the senders and the receivers of the synchronization signals are known at compile time. This is especially useful in distributed shared memory machines such as IBM RP3 [PBG + 85], BBN Butterfly <ref> [BBN87] </ref>, and Stanford DASH [LLG + 90], where a processor can access the local storage of other processors so each processor can busy-wait on the synchronization variables allocated locally but modified by remote processors. Such a technique reduces the busy-waiting traffic to the global communication network [SY91].
Reference: [BC + 89] <author> M. Berry, D.-K. Chen, et al. </author> <title> The PERFECT club benchmarks: Effective performance evaluation of supercomputers. </title> <booktitle> Int'l. J. of Supercomputer Applications, </booktitle> <pages> pages 5-40, </pages> <month> Fall </month> <year> 1989. </year>
Reference-contexts: Column 4 shows the range of values chosen. We use three loops selected from two widely used benchmark programs to illustrate corresponding parameter values. They are loop 100 of subroutine PREDIC (L1) and loop 900 of subroutine CORREC (L2) of BDNA from Perfect Benchmark Suite <ref> [BC + 89] </ref>, and loop 2 of subroutine COOCSR (L3) from the Sparskit [Saa90]. Note that loops from the benchmarks tend to have smaller problem sizes, and hence the smaller number of iterations -, than real application codes.
Reference: [Che89] <author> D.-K. Chen. </author> <title> MaxPar: An execution driven simulator for studying parallel systems. </title> <type> Master's thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> October </month> <year> 1989. </year> <note> Also available as CSRD Report No. 917. </note>
Reference-contexts: These graphs are all from the inner loops of several subroutines of the Eispack (a set of mathematics library routines developed at Argonne National Laboratory to solve eigenvalues and eigenvectors [SBD + 76]). We randomly chose one test driver routine chtest.f and used a source-level performance analysis tool, MaxPar <ref> [Che89] </ref>, to obtain loop-carried flow dependence information. 2 A dependence graph from each inner loop was constructed from its intermediate form by an experimental optimizing compiler developed under EPG-sim environment [PY93] and was augmented with the loop 2 We assume the anti- and output-dependences can be eliminated by the optimization using
Reference: [CKS90] <author> D. Callahan, K. Kennedy, and J. Subhlok. </author> <title> Analysis of event synchronization in a parallel programming tool. </title> <booktitle> In 2nd ACM SIGPLAN Symp. on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 21-30, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: A synchronization is transitive if any other successors of the synchronization source have at least one path to the sink. This algorithm has time complexity of O (n 3 ) where n is the number of nodes in the task graph. 65 The work done by Callahan <ref> [CKS90] </ref> used data flow analysis and algebraic formulations to determine whether a dependence is properly covered by synchronizations. The synchronization primitives used are post&wait, and they can handle more general program constructs not limited to parallel loops; and hence the time complexity is potentially higher.
Reference: [CS92] <author> Z. Chen and W. Shang. </author> <title> On uniformization of affine dependence algorithms. </title> <booktitle> In 4th IEEE Conference on Parallel and Distributed Processing, </booktitle> <month> December </month> <year> 1992. </year>
Reference-contexts: Because an optimal schedule is expensive to compute and we want to compare the BDVSs independently of the number of processors, we use instead the dependence cone size as the measure of choosing a good BDVS. Definition 8 (Dependence cone and dependence cone size <ref> [CS92] </ref>) For a dependence vector set V = f ~v 1 ; ~v 2 ; : : : ; ~v m g, the dependence cone denoted C (V ) is defined as the set C (V ) = f~v 2 R n : ~v = 1 ~v 1 + 2 ~v <p> In general, a larger dependence cone implies a smaller solution space for ~ P as shown in Figure 6.4a. The following theorem further shows the relation between the dependence cone size and the total execution time when a dependence cone is contained in another dependence cone. Theorem 11 <ref> [CS92] </ref> Let ~v 1 ; ~v 2 ; : : : ; ~v m be the vectors in BDVS 1 and ~u 1 ; ~u 2 ; : : : ; ~u k be the vectors in BDVS 2 . <p> Also, t (BDVS 1 ) 77 t (BDVS 2 ) where t (BDVS) is the execution time of the loop using the corresponding optimal linear schedule of BDVS. Proof: See <ref> [CS92] </ref> for details. 2 That is, if the dependence cone for BDVS 1 is contained in BDVS 2 , then the solution space for ~ P 2 of BDVS 2 must be contained in the solution space for ~ P 1 of BDVS 1 . <p> That is, if we have (x; y) executed before (x; y + 1) because of (0; 1), then we cannot have (0; 1) because it will mean that we also can have (x; y + 1) executed before (x; y). Example 2 (Basic Method I in <ref> [CS92] </ref>) BDVS=f (1; 0); (0; 1); (22; 323)g: This BDVS is very similar to the previous one except for the last vector (22; 323). It is derived from the lower bounds of each component of all the non-uniform dependence vectors. <p> Such lower bounds can be obtained with a linear programming technique to optimize the problem, min T 2DCH f (d i (T )g for i = 1; 2, where DCH is the dependence convex hull discussed in Section 6.2.2 <ref> [CS92] </ref>. Let x min and y min be the minimum of d 1 ; d 2 , respectively. Then any valid dependence vector ~ d = (x; y) in the doubly nested 78 (a) (c) scheme (Example 5). <p> As a result, the dependences represented by Equation (6.9) include some of the inverted real dependences. This problem can be eliminated by enumerating all possible direction vectors and combining them with the DCH to obtain the ultimate minimal values. Example 3 (Basic Method II in <ref> [CS92] </ref>) BDVS=f (1; 2); (1; 3); (3; 1); (0; 10)g : The validity argument of this BDVS follows from Equation (6.9). <p> In addition, the index synchronization requires explicit synchronization for every iteration, which will increase proportionally to the total number of loop iterations. Chen and Shang <ref> [CS92] </ref> discussed several optimization metrics for affine dependence uniformization such as schedule lengths, BDVS cardinalities, and dependence cone sizes. They also described two basic methods for generating initial BDVSs (see Examples 2 and 3) and then showed how to optimize these initial BDVSs.
Reference: [Cyt84] <author> R. Cytron. </author> <title> Compile-time Scheduling and Optimization for Asychronous Machines. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1984. </year>
Reference-contexts: Also the term shows that these loops are different from the DOALL loops which have no loop-carried dependences whose parallelization 12 is described in the next section. Traditionally, the same term was used only to describe loops with dependence cycles in their dependence graph <ref> [Cyt84] </ref>. We feel that the new definition is more meaningful because even a loop with no dependence cycle but with loop-carried dependences can take advantage of the DOACROSS parallelization. <p> However, the extent of the overlap between iterations, or equivalently, the parallelism, is determined by the statement order. It is very important in DOACROSS loops to order the statements for the minimal delay (or the maximal overlap) between iterations. As has been proven by Cytron <ref> [Cyt84] </ref>, the optimization problem of statement re-ordering is NP-complete. Hence, in practice, algorithms using heuristics have to be used. This chapter describes a compiler optimization strategy that performs statement re-ordering for DOACROSS loops to maximize the parallelism. <p> present our algorithm, we describe earlier works done by other people and where they fail to produce better results. 4.3.1 Previous Works It has been shown that even for a simpler dependence graph with only loop-carried dependences (hence, no restriction on its statement ordering), the delay minimization problem is NP-complete <ref> [Cyt84] </ref>. The heuristics algorithms previously proposed basically have two phases [Cyt84, AMS88]. In the first phase, the graph is partitioned and the partitions are then ordered. In the second phase, statements within each partition are ordered. <p> The heuristics algorithms previously proposed basically have two phases <ref> [Cyt84, AMS88] </ref>. In the first phase, the graph is partitioned and the partitions are then ordered. In the second phase, statements within each partition are ordered. <p> In the second phase, statements within each partition are ordered. The idea is to determine the partition order in the first phase such that, during the local optimization in the second phase, no loop-independent dependences will be violated. In the partitioning phase of the method proposed by Cytron <ref> [Cyt84] </ref>, it forms antichain-rows by level-sorting the dependence graph considering only forward dependences. The source and the sink of a forward dependence are placed in different partitions. In the second phase, a weight function for each statement s i in a partition (or an antichain-row) is calculated. <p> In summary, the two-phase algorithms try to preserve loop-independent dependences in the first phase without a proper consideration of all the loop-carried dependences, which limits the strategies that can be used in the second phase. 4.3.2 Hierarchical Scheduling ANew Approach Because it is not adequate to preserve all forward dependences <ref> [Cyt84] </ref>, and it is difficult to determine which loop-carried dependences should become forward dependences in the first phase [AMS88], our 24 strategy is to postpone making the decision for each loop-carried dependence until it is necessary. <p> In such cases, the dependence distance vectors will have more than one component (instead of the scalar d used in Equation (4.1) in page 20). It is shown that to determine the optimal delays for each loop level requires using the simplex algorithm to solve a linear programming problem <ref> [Cyt84] </ref>. Our problem is slightly different in that we try to determine an order to minimize the delays. Therefore, the goal is to minimize the statement distance for critical backward loop-carried dependences. They are usually indicated by small dependence distance vectors.
Reference: [EPY89] <author> P. Emrath, D. Padua, and P.-C. Yew. </author> <title> Cedar architecture and its software. </title> <booktitle> In Hawaii Int'l Conf. on System Science, </booktitle> <month> January </month> <year> 1989. </year> <month> 134 </month>
Reference-contexts: Other vectors in the BDVS have to be enforced only at the boundaries of the adjacent strips with explicit synchronization using synchronization primitives such as post&wait [Par88], FULL/EMPTY bit [Smi78], Cedar synchronization <ref> [EPY89] </ref>; or by a process-based synchronization scheme described in [SY89]; that is, many explicit synchronizations can be eliminated if the source and the sink of a dependence are in the same strip and are executed by the same processor.
Reference: [Fly72] <author> M. Flynn. </author> <title> Some computer organizations and their effectiveness. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 21 </volume> <pages> 948-960, </pages> <year> 1972. </year>
Reference-contexts: Each processor has its own program counter and can execute instruction different from other processors. The progress of processors is not synchronized unless at the point of synchronization using one of the primitives in the last section. This general architecture includes all the Multiple-Instruction-Multiple-Data (MIMD) machines <ref> [Fly72] </ref>. Our other architecture assumption is the shared address space; that is, each processor sees the memory as a homogeneous collection of storages, and every storage location can be addressed identically by all processors.
Reference: [GGH91] <author> K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> Performance evaluation of memory consistency models for shared memory multiprocessors. </title> <booktitle> In Int'l. Conf. on Architecture Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 245-257, </pages> <month> Apr. </month> <year> 1991. </year>
Reference-contexts: In this thesis, it is important to note that a shared-memory architecture is assumed. Unless specified, we make no distinction between the UMA and the NUMA machines. Explicit assumption will be made if the transformations are targeted for the NUMA machines. In addition, we assume a weak-consistency model <ref> [GGH91] </ref> where the memory transactions are guaranteed to be globally performed at every 8 synchronization point.
Reference: [GGK + 83] <author> A. Gottlieb, R. Grishman, C. Kruskal, K. McAuliffe, L. Rudolph, and M. Snir. </author> <title> The NYU Ultracomputer designing an MIMD shared memory parallel computer. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-32(2), </volume> <month> February </month> <year> 1983. </year>
Reference-contexts: If the test fails, then the operation aborts. Otherwise, it performs operations on target.key and target.data and then unlocks target. The whole operation is specified in 1 instruction. It is completed atomically in global memory similarly to the fetch-and- operations of the NYU Ultracomputer <ref> [GGK + 83] </ref>. Hence, only one trip for each synchronization access is required, and no remote busy-waiting across the network is necessary. In the case of Temp1 and Temp2 in Figure 7.6, we use the key fields of the elements of the arrays and ignore the data fields.
Reference: [GJ79] <author> M. Garey and D. Johnson. </author> <title> Computers and Intractability: A Guide to the Theory of NP-Completeness. W.H. </title> <publisher> Freeman, </publisher> <year> 1979. </year>
Reference-contexts: A smaller number of inversions implies higher covering potential. Our optimization problem can thus be reduced to finding an order such that the number of inversions in the ordered source vector is minimized. This is equivalent to solving the following special case of the QUADRATIC ASSIGNMENT PROBLEM <ref> [GJ79] </ref> with the cost ij being the number of inversions in (sv i ; sv j ) which is constructed from the source vectors of SCC i and j: Problem: Assume we have non-negative integer costs cost ij , 1 i; j n and distances d kl , 1 k; l <p> Proof: It is easy to see that the problem is in NP because a nondeterministic algorithm needs only guess an order of f1; 2; : : :; ng and check in polynomial time whether the total number of inversions is K. We transform the following FEEDBACK ARC SET problem <ref> [GJ79] </ref> to our quadratic assignment problem: INSTANCE: Directed graph G = (V; A), positive integer K j A j.
Reference: [Hal85] <author> R. H. Halstead Jr. </author> <title> Multilisp: A language for concurrent symbolic computation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(4) </volume> <pages> 501-538, </pages> <month> Oct. </month> <year> 1985. </year>
Reference-contexts: Our other architecture assumption is the shared address space; that is, each processor sees the memory as a homogeneous collection of storages, and every storage location can be addressed identically by all processors. The programming paradigm that utilizes this shared-memory architecture <ref> [Hal85] </ref> has been shown to be the easiest to use when programming for parallel application on a multiprocessor.
Reference: [Jor78] <author> H. F. Jordan. </author> <title> A special purpose architecture for finite element analysis. </title> <booktitle> In Int'l. Conf. on Parallel Processing, </booktitle> <pages> pages 263-266, </pages> <year> 1978. </year>
Reference-contexts: This guarantees that the consumer will not try to use the data before it is generated by the producer. 7 * Barrier. A more sophisticated synchronization primitive is a barrier <ref> [Jor78] </ref>. It is used to synchronize the progress of all processors. No processor can go beyond a barrier until all processors have arrived at the barrier. Because a processor that arrives at the barrier early has to wait for the late processors, processor idle time is introduced by the barriers.
Reference: [K + 93] <author> D. Kuck et al. </author> <title> The Cedar system and an initial performance study. </title> <booktitle> In 20th Int'l Symp. on Computer Architecture, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: Furthermore, compared to an older scheme with the same generality [ZY84], it speeds up execution by significantly reducing the amount of communication required and by increasing the overlap among dependent iterations. The effectiveness of this algorithm is evaluated via measurements in the 32-processor Cedar shared-memory multiprocessor <ref> [K + 93] </ref>. The results show speedups that reach up to 13 with the full overhead of the analysis overhead and up to 26 if part of the analysis work is reused over loop invocations. <p> In this section, we describe the machine, the synchronization support, and the workloads used. 7.4.1 The Cedar Multiprocessor Our experiments are timing runs on the Cedar <ref> [K + 93] </ref>, a 32-processor scalable shared-memory multiprocessor designed at the Center for Supercomputing Research and Development. The machine has 4 clusters of 8 processors each (Figure 7.8). Each cluster is a bus-based Alliant FX/8 with 32 Mbytes of memory shared by the processors in the cluster.
Reference: [KS88] <author> V. Krothapalli and P. Sadayappan. </author> <title> An approach to synchronization of parallel computing. </title> <booktitle> In ACM Int'l. Conf. on Supercomputing, </booktitle> <pages> pages 573-581, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: For the loops where dependences are unknown at compile time, we can still perform the dependence analysis at runtime and maybe execute the loop in parallel. This approach is called runtime parallelization. Much previous research has been done to design effective runtime parallelization algorithms <ref> [KS88, LZ93, MP87, SMC91, ZY84, ZY87] </ref>. The main differences between the schemes proposed are the types of dependence patterns that are handled and the required system or architecture support. <p> To do so, the scheme requires data synchronization with more than one key field. However, it still requires high communication as Zhu Yew's scheme. Other, less general schemes have been proposed <ref> [KS88, Pol87] </ref>. They restrict the problem in three ways. First, no two index elements of a given index array (I1 and I2 in Figure 7.1 (b)) have the same 106 values. This makes scheduling simpler. Second, they require a serial pre-processing loop, which can reduce speedup.
Reference: [KS91] <author> V. Krothapalli and P. Sadayappan. </author> <title> Removal of redundant dependences in doacross loops with constant dependences. </title> <booktitle> In 3rd ACM SIGPLAN Symp. on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 51-60, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Figure 5.1 (b) shows the corresponding CPG in the shaded area. There are three columns because D max = 2. Midkiff [Mid86] showed that to identify redundant data dependences in a single loop, we only need to examine D max + 1 iterations. 43 Krothapalli <ref> [KS91] </ref> used a Sequential Execution Order Depth First Search (SEODFS) algorithm on an Iteration Space Dependence Graph (ISDG), where the entire loop body of an iteration is represented by a single node in the ISDG. <p> Krothapalli <ref> [KS91] </ref> showed that in a double loop, if the inner loop has at least p max + an min + 1 iterations, where p max is the most positive distance and an min is the absolute value of the most negative distance in the inner loop, then the redundancy checking is <p> In this case d is no longer uniformly redundant because for iteration (1; 3), there is no alternative path from the dependence source to sink. However, Krothapalli's criteria <ref> [KS91] </ref> would conclude that d is still redundant because the inner loop has 3 p max + an min + 1 iterations. This is correct if the synchronization is done at the loop iteration level. <p> The synchronization primitives used are post&wait, and they can handle more general program constructs not limited to parallel loops; and hence the time complexity is potentially higher. A very efficient algorithm for finding redundant synchronization in DOACROSS loops with uniform dependences is proposed by Krothapalli and Sadayappan <ref> [KS91, KS92] </ref>. This algorithm works on either single statement loops or synchronization at the iteration level instead of the statement level. Because the dependences are uniform in each iteration, only the dependences from the first iteration need to be verified.
Reference: [KS92] <author> V. Krothapalli and P. Sadayappan. </author> <title> Removal of redundant dependences in doacross loops with constant dependences. </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 281-290, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: The synchronization primitives used are post&wait, and they can handle more general program constructs not limited to parallel loops; and hence the time complexity is potentially higher. A very efficient algorithm for finding redundant synchronization in DOACROSS loops with uniform dependences is proposed by Krothapalli and Sadayappan <ref> [KS91, KS92] </ref>. This algorithm works on either single statement loops or synchronization at the iteration level instead of the statement level. Because the dependences are uniform in each iteration, only the dependences from the first iteration need to be verified.
Reference: [Kuc78] <author> D. J. Kuck. </author> <title> The Structure of Computers and Computations, volume I, chapter 2, page 139. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: Next, we find all of the strongly connected components (SCCs) in the dependence graph. They are also called -blocks in <ref> [Kuc78, Pad79] </ref>. After they are ordered to maximize the dependence covering, statements within each SCC are then ordered. Using the same heuristics, we try to determine, in each SCC, the strongly connected sub-structures (PSCCs), which consist of closely related statements in terms of their dependences.
Reference: [Lam74] <author> L. Lamport. </author> <title> The parallel execution of DO loops. </title> <journal> Comm. ACM, </journal> <volume> 17(2) </volume> <pages> 83-93, </pages> <month> Feb. </month> <year> 1974. </year>
Reference-contexts: The significance of the dependence cone size can be explained by using the wavefront method <ref> [Mur71, Lam74] </ref> as shown in Figure 6.2b where all of the iterations on each wavefront can be executed in parallel.
Reference: [Lam88] <author> M. S. Lam. </author> <title> Software pipelining: An effective scheduling technique for vliw machines. </title> <booktitle> In ACM Conf. on Programming Language Design and Implementation, </booktitle> <pages> pages 318-329, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: The loops to be parallelized are assumed to be perfectly nested. First we show that DOACROSS parallelization is a general parallelization technique for a shared-memory MIMD machine. We then compare it with other parallelizing techniques such as vectorization, DOALL parallelization [PW86], and software pipelining <ref> [Lam88] </ref>, which all target for loop parallelism. Finally, we cover the important issues of optimization and transformation for DOACROSS parallelization. 3.1 DOACROSS Parallelization DOACROSS parallelization was first proposed by Padua [Pad79] to take advantage of the asynchrony of the MIMD machines.
Reference: [LAS85] <author> Z. Li and W. Abu-Sufah. </author> <title> A technique for reducing synchronization overhead in large scale multiprocessors. </title> <booktitle> In Int'l. Symp. on Computer Architecture, </booktitle> <pages> pages 406-413, </pages> <month> June </month> <year> 1985. </year>
Reference-contexts: The last iteration in the CPG will be the first one in which the exit-loop branch is taken. 5.4.3 Previous Works The redundant synchronization elimination problem was first described by Li and Abu-sufah <ref> [LAS85, LAS87] </ref> using dominant locks. A lock specifies a dependence relation, and it dominates another lock if enforcing the first lock ensures that the second lock is preserved. Three conditions for identifying dominant locks are provided, but they cover only very limited cases.
Reference: [LAS87] <author> Z. Li and W. Abu-Sufah. </author> <title> On reducing data synchronization in multiprocessed loops. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-36(1):105-109, </volume> <month> January </month> <year> 1987. </year>
Reference-contexts: Another related optimization problem, which has been overlooked in the past, is to reduce the amount of synchronization through statement re-ordering. By making more dependences covered by others, more synchronization becomes redundant and therefore can be eliminated without affecting the correctness of the execution <ref> [LAS87] </ref> This problem should be considered along with the delay minimiza 18 tion problem mentioned above. Although there is a trade-off between parallelism and synchronization overhead, we shall make parallelism our main optimization goal. The reduced synchronization will be achieved without compromising the parallelism available. <p> The last iteration in the CPG will be the first one in which the exit-loop branch is taken. 5.4.3 Previous Works The redundant synchronization elimination problem was first described by Li and Abu-sufah <ref> [LAS85, LAS87] </ref> using dominant locks. A lock specifies a dependence relation, and it dominates another lock if enforcing the first lock ensures that the second lock is preserved. Three conditions for identifying dominant locks are provided, but they cover only very limited cases.
Reference: [LLG + 90] <author> D. Lenoski, K. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The directory-based cache coherence protocol for the DASH multiprocessor. </title> <booktitle> In Int'l. Symp. on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <month> May </month> <year> 1990. </year> <month> 135 </month>
Reference-contexts: Second, the strips are assigned to the processors statically so the senders and the receivers of the synchronization signals are known at compile time. This is especially useful in distributed shared memory machines such as IBM RP3 [PBG + 85], BBN Butterfly [BBN87], and Stanford DASH <ref> [LLG + 90] </ref>, where a processor can access the local storage of other processors so each processor can busy-wait on the synchronization variables allocated locally but modified by remote processors. Such a technique reduces the busy-waiting traffic to the global communication network [SY91].
Reference: [LZ93] <author> S.-T. Leung and J. Zahorjan. </author> <title> Improving the performance of runtime parallelization. </title> <booktitle> In 4th ACM SIGPLAN Symp. on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 83-91, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: For the loops where dependences are unknown at compile time, we can still perform the dependence analysis at runtime and maybe execute the loop in parallel. This approach is called runtime parallelization. Much previous research has been done to design effective runtime parallelization algorithms <ref> [KS88, LZ93, MP87, SMC91, ZY84, ZY87] </ref>. The main differences between the schemes proposed are the types of dependence patterns that are handled and the required system or architecture support. <p> Unfortunately, this can be slow and complicated in large-scale multiprocessors. How to do it is not described. Lately, schemes for loops without output dependences have been proposed by Saltz et al. [SM91, SMC91] and Leung and Zahorjan <ref> [LZ93] </ref>. This assumption simplifies the problem. The emphasis of Saltz et al.'s work is on the scheduling issues. The results show that the best scheduling method is to statically partition the iterations among the processors and to rearrange the iteration order within each partition.
Reference: [Mid86] <author> S. Midkiff. </author> <title> Automatic generation of synchronization instructions for parallel processors. </title> <type> Master's thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> May </month> <year> 1986. </year> <note> Also available as CSRD Report No. 588. </note>
Reference-contexts: The loop in Figure 5.1 (a) has loop-carried data dependences of distances 1 and 2. Figure 5.1 (b) shows the corresponding CPG in the shaded area. There are three columns because D max = 2. Midkiff <ref> [Mid86] </ref> showed that to identify redundant data dependences in a single loop, we only need to examine D max + 1 iterations. 43 Krothapalli [KS91] used a Sequential Execution Order Depth First Search (SEODFS) algorithm on an Iteration Space Dependence Graph (ISDG), where the entire loop body of an iteration is <p> Such statements abort loop execution and are discussed in Section 5.4. With branches, there can be more than one control flow path for each iteration. We have to examine every possible control flow path to make sure that a synchronization is 46 really redundant in all paths. Midkiff <ref> [Mid86] </ref> showed that examining CPGs with all possible control flow combinations in the windows of D max + 1 iterations is necessary and sufficient for determining all redundant synchronization in a single loop.
Reference: [MP86] <author> S. Midkiff and D. Padua. </author> <title> Compiler generated synchronization for DO loops. </title> <booktitle> In Int'l. Conf. on Parallel Processing, </booktitle> <pages> pages 544-551, </pages> <month> Aug. </month> <year> 1986. </year>
Reference-contexts: A lock specifies a dependence relation, and it dominates another lock if enforcing the first lock ensures that the second lock is preserved. Three conditions for identifying dominant locks are provided, but they cover only very limited cases. Midkiff and Padua <ref> [MP86, MP87] </ref> described schemes to generate synchronization instructions by a compiler using test and testset instructions, which are very similar to await and advance instructions used in the Alliant FX/80 [All86].
Reference: [MP87] <author> S. Midkiff and D. Padua. </author> <title> Compiler algorithms for synchronization. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-36(12), </volume> <month> December </month> <year> 1987. </year>
Reference-contexts: A lock specifies a dependence relation, and it dominates another lock if enforcing the first lock ensures that the second lock is preserved. Three conditions for identifying dominant locks are provided, but they cover only very limited cases. Midkiff and Padua <ref> [MP86, MP87] </ref> described schemes to generate synchronization instructions by a compiler using test and testset instructions, which are very similar to await and advance instructions used in the Alliant FX/80 [All86]. <p> Next, we show the important dependence information that is generated from the data dependence analysis. It will be used later to construct the Basic Dependence Vector Set. 6.2.1 DOACROSS Loop Example with Non-uniform Data Dependences Synchronization for a uniform loop-carried dependence is relatively easy to generate <ref> [MP87, SY89] </ref> because the distance between the source and the sink statement is fixed. However, for a non-uniform dependence, it is very difficult because each iteration has varying dependence distances. In most cases, loops with non-uniform dependences must be executed serially. Consider the loop in Figure 6.1. <p> For the loops where dependences are unknown at compile time, we can still perform the dependence analysis at runtime and maybe execute the loop in parallel. This approach is called runtime parallelization. Much previous research has been done to design effective runtime parallelization algorithms <ref> [KS88, LZ93, MP87, SMC91, ZY84, ZY87] </ref>. The main differences between the schemes proposed are the types of dependence patterns that are handled and the required system or architecture support. <p> Done (i)=.TRUE. END IF END DOALL END REPEAT unexecuted iteration i requires at least 3r memory accesses, where r is the number of references to array A per iteration. A second scheme, proposed by Midkiff and Padua <ref> [MP87] </ref>, improved Zhu-Yew's scheme by allowing concurrent reads to the same array element by several iterations. To do so, the scheme requires data synchronization with more than one key field. However, it still requires high communication as Zhu Yew's scheme. Other, less general schemes have been proposed [KS88, Pol87].
Reference: [MP91] <author> S. Midkiff and D. Padua. </author> <title> A comparison of four synchronization optimization techniques. </title> <booktitle> In Int'l. Conf. on Parallel Processing, </booktitle> <volume> volume II, </volume> <pages> pages 9-16, </pages> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: However, as pointed out by Midkiff <ref> [MP91] </ref>, this is true only when the loop body has only a single statement or the synchronization is done at the iteration level instead of at the statement level. <p> Therefore, what appears as an alternative path in the ISDG may not be connected as a path in the corresponding CPG. Midkiff proposed a restricted algorithm for the loops with multiple statements <ref> [MP91] </ref>. It can find a subset of redundant synchronizations that are guaranteed to be uniformly redundant. Unfortunately, that algorithm could also miss many uniformly redundant synchronizations. For example, consider the loop and the corresponding iteration space in Figure 5.4. <p> The difficulty arises from the fact that the lexical order of the statements in an iteration has to be considered to form paths in CPG, whereas the order is immaterial in ISDG. On the other hand, given the four dependences, the scheme in <ref> [MP91] </ref> would fail to identify d as uniformly redundant because it 49 DOACROSS I DOACROSS J s 1 : A (I+3,J)=B (I-1,J-1)+: : : s 3 : : : : =B (I-2,J+1)+C (I-1,J-1) END DOACROSS J END DOACROSS I (a) only examines the rectangle region specified by the source and sink <p> That approach is very efficient in finding redundant synchronizations between iterations. However, synchronization at the iteration level is often not very satisfactory because the parallelism provided by overlapping loop iterations in a DOACROSS loop execution is lost. Recently, a scheme proposed by Midkiff and Padua in <ref> [MP91] </ref> is equivalent to examining the intersection of all subregions for an alternative path of the dependence d. In Figure 5.5, the intersection is the rectangle with corner (0; 0); (0; b); (a; b), and (a; 0).
Reference: [Mur71] <author> Y. Muroaka. </author> <title> Parallelism Exposure and Exploitation in Programs. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Univ. of Illinois at Urbana-Champaign, </institution> <month> Feb. </month> <year> 1971. </year>
Reference-contexts: The significance of the dependence cone size can be explained by using the wavefront method <ref> [Mur71, Lam74] </ref> as shown in Figure 6.2b where all of the iterations on each wavefront can be executed in parallel.
Reference: [Pad79] <author> D. A. Padua. </author> <title> Multiprocessors: Discussion of Some Theoretical and Practical Problems. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Univ. of Illinois at Urbana-Champaign, </institution> <month> October </month> <year> 1979. </year>
Reference-contexts: We then compare it with other parallelizing techniques such as vectorization, DOALL parallelization [PW86], and software pipelining [Lam88], which all target for loop parallelism. Finally, we cover the important issues of optimization and transformation for DOACROSS parallelization. 3.1 DOACROSS Parallelization DOACROSS parallelization was first proposed by Padua <ref> [Pad79] </ref> to take advantage of the asynchrony of the MIMD machines. In this scheme, each iteration is assigned to a processor for execution. <p> Next, we find all of the strongly connected components (SCCs) in the dependence graph. They are also called -blocks in <ref> [Kuc78, Pad79] </ref>. After they are ordered to maximize the dependence covering, statements within each SCC are then ordered. Using the same heuristics, we try to determine, in each SCC, the strongly connected sub-structures (PSCCs), which consist of closely related statements in terms of their dependences. <p> One of them is loop alignment <ref> [Pad79] </ref>, which changes the distance of the dependences. Figure 4.8 gives a loop alignment example. The original dependences among statement instances are shown in no statement re-ordering can improve the loop-level parallelism.
Reference: [Par88] <author> The Parallel Computing Forum. </author> <title> PCF Fortran: Language definition, </title> <address> 1 edition, </address> <month> August </month> <year> 1988. </year>
Reference-contexts: In this thesis, we will use the following synchronization primitives commonly available on multiprocessors. * post&wait. This primitive can be used to resolve the producer-consumer type of dependences <ref> [Par88] </ref>. Initially, a shared synchronization variable is reset to 0. When the producer makes the data ready to be used by the consumer, it sets the variable to 1 by doing a post. <p> The dependence vector (1; 0) can be automatically preserved by the forward execution of the outer loop in each processor. Other vectors in the BDVS have to be enforced only at the boundaries of the adjacent strips with explicit synchronization using synchronization primitives such as post&wait <ref> [Par88] </ref>, FULL/EMPTY bit [Smi78], Cedar synchronization [EPY89]; or by a process-based synchronization scheme described in [SY89]; that is, many explicit synchronizations can be eliminated if the source and the sink of a dependence are in the same strip and are executed by the same processor.
Reference: [PBG + 85] <author> G. Pfister, W. Brantley, D. George, S. Harvey, W. Kleinfelder, K. McAuliffe, E. Melton, V. Norton, and J. Weiss. </author> <title> The IBM research parallel processor prototype (RP3): Introduction and architecture. </title> <booktitle> In 1985 Int'l. Conf. on Parallel Processing, </booktitle> <pages> pages 764-771, </pages> <month> August </month> <year> 1985. </year>
Reference-contexts: Second, the strips are assigned to the processors statically so the senders and the receivers of the synchronization signals are known at compile time. This is especially useful in distributed shared memory machines such as IBM RP3 <ref> [PBG + 85] </ref>, BBN Butterfly [BBN87], and Stanford DASH [LLG + 90], where a processor can access the local storage of other processors so each processor can busy-wait on the synchronization variables allocated locally but modified by remote processors.
Reference: [Pol87] <author> C. D. Polychronopoulos. </author> <title> Advanced loop optimizations for parallel computers. </title> <editor> In E. N. Houstis, T. S. Papatheodorou, and C. D. Polychronopoulos, editors, </editor> <booktitle> Lecture Notes in Computer Science No. 297: Proc. First Int'l. Conf. on Supercomputing, </booktitle> <address> Athens, Greece, </address> <pages> pages 255-277, </pages> <address> New York, June 1987. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: To do so, the scheme requires data synchronization with more than one key field. However, it still requires high communication as Zhu Yew's scheme. Other, less general schemes have been proposed <ref> [KS88, Pol87] </ref>. They restrict the problem in three ways. First, no two index elements of a given index array (I1 and I2 in Figure 7.1 (b)) have the same 106 values. This makes scheduling simpler. Second, they require a serial pre-processing loop, which can reduce speedup.
Reference: [PS85] <author> F. P. Preparata and M. I. Shamos. </author> <title> Computational Geometry: An Introduction. </title> <publisher> Springer-Verlag, </publisher> <address> New York, NY, </address> <year> 1985. </year>
Reference-contexts: 8 &gt; &gt; &gt; &gt; &gt; : 1 t 1 2t 2 + 5 t 1 1 t 2 + 3t 3 1000 9 &gt; &gt; &gt; &gt; &gt; ; The inequalities of Equation (6.2) form a set of half spaces, and the intersection of them is a convex polytope <ref> [PS85] </ref> as shown in Theorem 10. Theorem 10 The intersection V of the half-spaces, which are formed by substituting Equation (6.4) into Equation (6.2), is a convex polytope in R n .
Reference: [PSC93] <author> R. Ponnusamy, J. Saltz, and A. Choudhary. </author> <title> Runtime compilation techniques for data partitioning and communication schedule reuse. </title> <booktitle> In Supercomputing '91, </booktitle> <pages> pages 361-370, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: This assumption simplifies the problem. The emphasis of Saltz et al.'s work is on the scheduling issues. The results show that the best scheduling method is to statically partition the iterations among the processors and to rearrange the iteration order within each partition. Their recent work <ref> [PSC93] </ref> focuses on runtime compilation techniques for FORALL loops with only output dependences for reduction operations on distributed memory computers. There are similar issues, but the context is different. A common limitation of most of the previous works is the lack of performance results.
Reference: [PW86] <author> D. A. Padua and M. J. Wolfe. </author> <title> Advanced compiler optimization for supercomputers. </title> <journal> Comm. ACM, </journal> <volume> 29(12), </volume> <month> December </month> <year> 1986. </year>
Reference-contexts: The loops to be parallelized are assumed to be perfectly nested. First we show that DOACROSS parallelization is a general parallelization technique for a shared-memory MIMD machine. We then compare it with other parallelizing techniques such as vectorization, DOALL parallelization <ref> [PW86] </ref>, and software pipelining [Lam88], which all target for loop parallelism. Finally, we cover the important issues of optimization and transformation for DOACROSS parallelization. 3.1 DOACROSS Parallelization DOACROSS parallelization was first proposed by Padua [Pad79] to take advantage of the asynchrony of the MIMD machines.
Reference: [PY93] <author> D. K. Poulsen and P.-C. Yew. </author> <title> Execution-driven tools for parallel simulation of parallel architectures and applications. </title> <booktitle> In Supercomputing '91, </booktitle> <pages> pages 860-869, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: We randomly chose one test driver routine chtest.f and used a source-level performance analysis tool, MaxPar [Che89], to obtain loop-carried flow dependence information. 2 A dependence graph from each inner loop was constructed from its intermediate form by an experimental optimizing compiler developed under EPG-sim environment <ref> [PY93] </ref> and was augmented with the loop 2 We assume the anti- and output-dependences can be eliminated by the optimization using scalar and array expansion or privatization. 36 carried dependence information mentioned above.
Reference: [RND77] <author> E. Reingold, J. Nievergelt, and N. Deo. </author> <title> Combinatorial Algorithms: Theory and Practice. </title> <publisher> Prentice-Hall Inc., </publisher> <year> 1977. </year> <month> 136 </month>
Reference-contexts: A strongly connected component (SCC) in a directed graph is a maximal subgraph such that each node in this subgraph has a directed path to any other node in the same subgraph <ref> [RND77] </ref>. Conceptually, our proposed Hierarchical Scheduling (HS) strategy finds Pseudo-Strongly Connected Components (PSCC) first in the dependence graph and orders the components to minimize the potential statement distances of the backward dependences among the components. For each PSCC, this process is recursively applied until all statements are scheduled.
Reference: [Saa90] <author> Y. Saad. SPARSKIT: </author> <title> A Basic Tool Kit for Sparse Matrix Computation. </title> <type> Technical report, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomputing Res. & Dev., </institution> <month> August </month> <year> 1990. </year> <note> CSRD Report No. 1029. </note>
Reference-contexts: They are loop 100 of subroutine PREDIC (L1) and loop 900 of subroutine CORREC (L2) of BDNA from Perfect Benchmark Suite [BC + 89], and loop 2 of subroutine COOCSR (L3) from the Sparskit <ref> [Saa90] </ref>. Note that loops from the benchmarks tend to have smaller problem sizes, and hence the smaller number of iterations -, than real application codes. In Table 7.1, iteration count N is assumed to be the same as the size of the array A.
Reference: [SBD + 76] <author> B. T. Smith, J. M. Boyle, J. J. Dongarra, B. S. Garbow, Y. Ikebe, V. C. Klema, and C. B. Moler. </author> <title> Matrix eigensystem routines - eispack guide. </title> <address> Heidelberg, </address> <year> 1976. </year>
Reference-contexts: These graphs are all from the inner loops of several subroutines of the Eispack (a set of mathematics library routines developed at Argonne National Laboratory to solve eigenvalues and eigenvectors <ref> [SBD + 76] </ref>).
Reference: [Sha89] <author> P. Shaffer. </author> <title> Minimization of interprocessor synchronization in multiprocessors with shared and private memory. </title> <booktitle> In Int'l. Conf. on Parallel Processing, volume III, </booktitle> <pages> pages 138-141, </pages> <month> Aug. </month> <year> 1989. </year>
Reference-contexts: So the complexity can also be shown as O (n 5 ). For a single loop, they showed that the columns of CPG needed is at most D max + 1, where D max is the largest dependence distance. Shaffer <ref> [Sha89] </ref> showed an algorithm that can find all transitive synchronization arcs between tasks. A synchronization is transitive if any other successors of the synchronization source have at least one path to the sink.
Reference: [SLY90] <author> Z. Shen, Z. Li, and P.-C. Yew. </author> <title> An empirical study of fortran programs for parallelizing compilers. </title> <journal> IEEE Trans. on Parallel and Distributed System, </journal> <volume> 1(3) </volume> <pages> 356-364, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: A recent empirical study showed that, before compiler optimizations such as forward substitution, induction variable elimination, and constant folding are applied, 66% of the array references have linear or partially linear subscript expressions <ref> [SLY90] </ref>. It also showed that approximately 45% of the reference pairs with linear or partially linear subscript expressions have coupled subscripts; that is, the same loop index variable appears in more than one dimension. They are one of the major causes of non-uniform data dependences.
Reference: [SM90] <author> B. Simons and A. Munshi. </author> <title> Scheduling loops on processors: Algorithms and complexity. </title> <journal> SIAM J. of Computing, </journal> <volume> 19(4) </volume> <pages> 728-741, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: In fact, anomalies could occur where the optimized order actually has a longer delay. 23 A similar method was also proposed by Anderson et al. <ref> [AMS88, SM90] </ref>. In their first phase, statements are level-sorted into partitions considering only the loop-independent dependences. The source and the sink statements of a loop-independent dependence are placed in different partitions.
Reference: [SM91] <author> J. Saltz and R. Mirchandaney. </author> <title> The preprocessed doacross loop. </title> <booktitle> In Int'l. Conf. on Parallel Processing, </booktitle> <volume> volume II, </volume> <pages> pages 174-179, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Finally, if the index arrays are not one-to-one mappings, they require shared-memory allocation. Unfortunately, this can be slow and complicated in large-scale multiprocessors. How to do it is not described. Lately, schemes for loops without output dependences have been proposed by Saltz et al. <ref> [SM91, SMC91] </ref> and Leung and Zahorjan [LZ93]. This assumption simplifies the problem. The emphasis of Saltz et al.'s work is on the scheduling issues. The results show that the best scheduling method is to statically partition the iterations among the processors and to rearrange the iteration order within each partition.
Reference: [SMC91] <author> J. Saltz, R. Mirchandaney, and K. Crowley. </author> <title> Run-time parallelization and scheduling of loops. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 40(5) </volume> <pages> 603-611, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: For the loops where dependences are unknown at compile time, we can still perform the dependence analysis at runtime and maybe execute the loop in parallel. This approach is called runtime parallelization. Much previous research has been done to design effective runtime parallelization algorithms <ref> [KS88, LZ93, MP87, SMC91, ZY84, ZY87] </ref>. The main differences between the schemes proposed are the types of dependence patterns that are handled and the required system or architecture support. <p> The values of arrays I1 and I2 are assumed to remain constant during the execution of one invocation of the loop, although they are allowed to change outside the loop. 104 Run-time parallelization schemes usually have two stages, namely inspector and executor <ref> [SMC91, ZY87] </ref>, both performed at runtime. The inspector determines the dependence relations among the data accesses. The executor uses this information to execute the iterations in an order that preserves the dependences. <p> Finally, if the index arrays are not one-to-one mappings, they require shared-memory allocation. Unfortunately, this can be slow and complicated in large-scale multiprocessors. How to do it is not described. Lately, schemes for loops without output dependences have been proposed by Saltz et al. <ref> [SM91, SMC91] </ref> and Leung and Zahorjan [LZ93]. This assumption simplifies the problem. The emphasis of Saltz et al.'s work is on the scheduling issues. The results show that the best scheduling method is to statically partition the iterations among the processors and to rearrange the iteration order within each partition.
Reference: [Smi78] <author> B. J. Smith. </author> <title> A pipelined, shared resource mimd computer. </title> <booktitle> In Int'l. Conf. on Parallel Processing, </booktitle> <pages> pages 6-8, </pages> <month> Aug. </month> <year> 1978. </year>
Reference-contexts: The dependence vector (1; 0) can be automatically preserved by the forward execution of the outer loop in each processor. Other vectors in the BDVS have to be enforced only at the boundaries of the adjacent strips with explicit synchronization using synchronization primitives such as post&wait [Par88], FULL/EMPTY bit <ref> [Smi78] </ref>, Cedar synchronization [EPY89]; or by a process-based synchronization scheme described in [SY89]; that is, many explicit synchronizations can be eliminated if the source and the sink of a dependence are in the same strip and are executed by the same processor.
Reference: [SY89] <author> H.-M. Su and P.-C. Yew. </author> <title> On data synchronization for multiprocessors. </title> <booktitle> In Int'l. Symp. on Computer Architecture, </booktitle> <pages> pages 416-423, </pages> <month> may </month> <year> 1989. </year>
Reference-contexts: Next, we show the important dependence information that is generated from the data dependence analysis. It will be used later to construct the Basic Dependence Vector Set. 6.2.1 DOACROSS Loop Example with Non-uniform Data Dependences Synchronization for a uniform loop-carried dependence is relatively easy to generate <ref> [MP87, SY89] </ref> because the distance between the source and the sink statement is fixed. However, for a non-uniform dependence, it is very difficult because each iteration has varying dependence distances. In most cases, loops with non-uniform dependences must be executed serially. Consider the loop in Figure 6.1. <p> Other vectors in the BDVS have to be enforced only at the boundaries of the adjacent strips with explicit synchronization using synchronization primitives such as post&wait [Par88], FULL/EMPTY bit [Smi78], Cedar synchronization [EPY89]; or by a process-based synchronization scheme described in <ref> [SY89] </ref>; that is, many explicit synchronizations can be eliminated if the source and the sink of a dependence are in the same strip and are executed by the same processor. For the example given in Figure 6.6, suppose that ck = 10 and the vector (5; 1) 2 BDVS .
Reference: [SY91] <author> H.-M. Su and P.-C. Yew. </author> <title> Efficient doacross execution for distributed shared memory multiprocessors. </title> <booktitle> In Supercomputing '91, </booktitle> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: Such a technique reduces the busy-waiting traffic to the global communication network <ref> [SY91] </ref>. Also, synchronization will keep the execution of different processors 96 more or less in lockstep, which mitigates the load-balancing problem.
Reference: [Tar72] <author> R.E. Tarjan. </author> <title> Depth first search and linear graph algorithms. </title> <journal> SIAM J. Comupting, </journal> <volume> 1(2), </volume> <year> 1972. </year>
Reference-contexts: In the main algorithm, we use Tarjan's <ref> [Tar72] </ref> well-known algorithm to determine SCCs. However, given a strongly connected graph G, we cannot use the same algorithm to find PSCC because we will obtain the same graph as G. If G has n nodes, the first PSCC 25 Input: a dependence graph from a DOACROSS loop.
Reference: [TN91] <author> T. H. Tzen and L. Ni. </author> <title> Dependence uniformization: A loop parallelization technique. </title> <type> Technical Report MSU-CPS-ACS-41, </type> <institution> Michigan State University, </institution> <month> June </month> <year> 1991. </year> <note> To appear in IEEE Trans. on Parallel and Distributed Systems. </note>
Reference-contexts: Case 3: If both (0; 1) and (0; 1) are in the BDVS at the beginning of the step 5 of the algorithm in Figure 6.8 and the compatibility problem occurs, then the index synchronization <ref> [TN91] </ref> as explained in the previous section has to be used.
Reference: [TN93] <author> T. H. Tzen and L. Ni. </author> <title> Dependence uniformization: A loop parallelization technique. </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> 4(5), </volume> <month> May </month> <year> 1993. </year>
Reference-contexts: However, our scheme, described in Section 6.3, considers both choices of source and sink statement and therefore is not affected by this problem. 6.3 Parallelizing Non-uniform Dependence Loops For loops with non-uniform dependences, we can uniformize the dependences <ref> [TN93] </ref> to simplify required synchronization. The uniformization process is illustrated as in Figure 6.2. <p> Example 4 (S-test in <ref> [TN93] </ref>) BDVS=f (1; 1); (0; 1)g (see Figure 6.5b). This BDVS consists of only two uniform dependence vectors (1; dsl max e) and (0; 1) where sl max is an upper bound of n d 1 (T ) ; 8T 2 DCH . <p> It occurs when ub 2 1 &gt; 1 and lb 2 1 &lt; 1. In this case, it degenerates to that of Example 1 and we have to solve the compatibility problem. We can use the BDVS in <ref> [TN93] </ref>, which is shown in Example 4. An Index Synchronization scheme was also described in [TN93] to parallelize the loop nests given a BDVS similar to that of Example 4. Figure 6.7 illustrates the index synchronization. <p> In this case, it degenerates to that of Example 1 and we have to solve the compatibility problem. We can use the BDVS in <ref> [TN93] </ref>, which is shown in Example 4. An Index Synchronization scheme was also described in [TN93] to parallelize the loop nests given a BDVS similar to that of Example 4. Figure 6.7 illustrates the index synchronization. The performance analysis for static strip scheduling and index synchronization is shown in a later section. <p> Case 3 uses BDVS of Example 4, which can be generated in step 5 of the algorithm in Figure 6.8, and the index synchronization <ref> [TN93] </ref>. Note that all BDVSs of all three cases can be used to uniformize the dependences, but they have quite different performance characteristics. <p> BDVS of case 1 can be obtained. The static strip scheduling scheme can then be applied. 99 6.7 Previous Works Tzen and Ni <ref> [TN93] </ref> proposed the first Dependence Uniformization method to parallelize doubly-nested loops with non-uniform affine dependences. They defined the notion of Dependence Slope, which is the ratio of d 1 and d 2 for a dependence distance vector (d 1 ; d 2 ) in a doubly-nested loops.
Reference: [WL91] <author> M. E. Wolf and M. S. Lam. </author> <title> A loop transformation theory and an algorithm to maximize parallelism. </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 452-471, </pages> <year> 1991. </year>
Reference-contexts: More recently, a unified framework for loop transformation, called Unimodular Transformation, becomes increasingly popular <ref> [Ban90, WL91] </ref>. Given more accurate characterization on the range of each entry in the dependence distance vectors (Equation (6.7)), we can determine when loop reversal, interchange, or skewing is legal.
Reference: [Wol82] <author> M. J. Wolfe. </author> <title> Optimizing Compilers for Supercomputers. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1982. </year>
Reference-contexts: It is called the Dependence Distance Vector Optimization and deserves further study. If some of the entries in the dependence distance vectors Equation (6.7) are zeros, they correspond to DOALL loops and can be interchanged freely with other loop nests without violating dependence relations <ref> [Wol82] </ref>. In parallelization, it is desirable to have DOALL loops at the outer levels so that the granularity of the tasks can be increased to amortize the overhead. Therefore, we should try to move all DOALL loops to the outermost levels. <p> In order to determine if there is any loop-carried dependence <ref> [Wol82] </ref> between statements s p and s q across iterations and to compute its dependence distance, we have to solve the following integer equation f (i) = g (i 0 ) DO i=1,N s p : A (f (i))= : : : s q : : : : = A (g
Reference: [ZY84] <author> C.-Q. Zhu and P.-C. Yew. </author> <title> A synchronization scheme and its application for large multiprocessor systems. </title> <booktitle> In 4h Int. Conf. on Distributed Computing Systems, </booktitle> <pages> pages 486-493, </pages> <month> May </month> <year> 1984. </year>
Reference-contexts: For the loops where dependences are unknown at compile time, we can still perform the dependence analysis at runtime and maybe execute the loop in parallel. This approach is called runtime parallelization. Much previous research has been done to design effective runtime parallelization algorithms <ref> [KS88, LZ93, MP87, SMC91, ZY84, ZY87] </ref>. The main differences between the schemes proposed are the types of dependence patterns that are handled and the required system or architecture support. <p> In this chapter, we describe and evaluate a new scheme for the runtime parallelization of loops. Our scheme handles any type of dependence pattern without requiring any special architectural support. Furthermore, compared to an older scheme with the same generality <ref> [ZY84] </ref>, it speeds up execution by significantly reducing the amount of communication required and by increasing the overlap among dependent iterations. The effectiveness of this algorithm is evaluated via measurements in the 32-processor Cedar shared-memory multiprocessor [K + 93]. <p> The first one was proposed by Zhu and Yew <ref> [ZY84] </ref>. Their scheme is general enough to handle any dependence pattern. Given a loop like that in Figure 7.1 (b), the compiler transforms it into the code shown in Figure 7.2. <p> We used loops with varying parameters, such as number of iterations 127 and references. The results show that our algorithm gives good speedups that reach 13 if the inspector is not reused and 26 if it is. Furthermore, our algorithm outperforms Zhu-Yew's scheme <ref> [ZY84] </ref> in nearly all cases, reaching a 37-fold speedup when the loop has many dependences. There are a few issues in runtime parallelization that require further study. <p> Our performance evaluation is conducted on the 32-processor Cedar multiprocessor. The result indicates a speedup of over 13 with both inspection and execution phases. When only execution is considered, the speedup can even go beyond 26. In comparison with an earlier scheme <ref> [ZY84] </ref> with similar applicability, our scheme shows less sensitivity to the dependence and reference patterns; that is, it offers better consistent performance.

References-found: 60

