URL: http://ptolemy.eecs.berkeley.edu/papers/96/resync/resync_asap.ps.Z
Refering-URL: http://ptolemy.eecs.berkeley.edu/papers/96/resync/
Root-URL: 
Title: LATENCY-CONSTRAINED RESYNCHRONIZATION FOR MULTIPROCESSOR DSP IMPLEMENTATION  applied extensively for digital signal processing systems.  
Author: Shuvra S. Bhattacharyya, Sundararajan Sriram, and Edward A. Lee L max E. A. Lee 
Keyword: timed execution of iterative dataow programs,  
Address: East Tasman Drive, San Jose, CA 95134, USA,  Instruments.  Berkeley.  
Affiliation: Semiconductor Research Laboratory, Hitachi America, Ltd., 201  Department of Electrical Engineering and Computer Sciences, University of California at Berkeley.  Centre, Texas  Department of Electrical Engineering and Computer Sciences, University of California at  
Note: which is an implementation model that has been  S. S. Bhattacharyya is with the  S. Sriram was with the  He is now with the DSP R&D  is with the  Submitted to Intl.Conf. on Application-specific Systems, Architectures Processors  
Email: shuvra@halsrl.com,  
Phone: fax: (408)954-8907.  
Date: February 1, 1996  
Abstract: Resynchronization is a post-optimization for static multiprocessor schedules in which extraneous synchronization operations are introduced in such a way that the number of original synchronizations that consequently become redundant significant exceeds the number of additional synchronizations. Redundant synchronizations are synchronization operations whose corresponding sequencing requirements are enforced completely by other synchronizations in the system. The amount of run-time overhead required for synchronization can be reduced significantly by eliminating redundant synchronizations [3, 19]. Thus, effective resynchronization reduces the net synchronization overhead in the implementation of a multiprocessor schedule, and thus improves the overall throughput. However, since additional serialization is imposed by the new synchronizations, resyn-chronization can produce significant increase in latency. This paper addresses the problem of computing an optimal resynchronization (one that results in the lowest average rate at which synchronization operations have to be performed) among all resynchronizations that do not increase the latency beyond a prespecified upper bound . Our study is based in the context of self This research was partially funded as part of the Ptolemy project, which is supported by the Advanced Research Projects Agency and the U.S. Air Force (under the RASSP program, contract F33615-93-C-1317), the Semiconductor Research Corporation (project 94-DC-008), the National Science Foundation (MIP-9201605), the State of California MICRO program, and the following companies: Bellcore, Bell Northern Research, Dolby Laboratories, Hitachi, LG Electronics, Mentor Graphics, Mitsubishi, Motorola, NEC, Pacific Bell, Philips, and Rockwell. 
Abstract-found: 1
Intro-found: 1
Reference: <institution> 1 </institution>
Reference-contexts: We refer to a homogeneous SDF graph as a DFG. The techniques developed in this paper can be used as a post-processing step to improve the performance of any static multiprocessor scheduling technique for iterative DFGs, such as those described in <ref> [1, 6, 9, 17, 20] </ref>. Our implementation model involves a self-timed scheduling strategy [14]. Each processor executes the tasks assigned to it in a fixed order that is specified at compile time. Before firing an actor, a processor waits for the data needed by that actor to become available.
Reference: [1] <author> S. Banerjee, D. Pick er, D. Fellman, and P. M. Chau, </author> <title> Improved Scheduling of Signal Flo w Graphs onto Multiprocessor Systems Through an Accurate Netw ork Modeling Technique, VLSI Signal Processing VII, </title> <publisher> IEEE Press, </publisher> <year> 1994. </year>
Reference-contexts: We refer to a homogeneous SDF graph as a DFG. The techniques developed in this paper can be used as a post-processing step to improve the performance of any static multiprocessor scheduling technique for iterative DFGs, such as those described in <ref> [1, 6, 9, 17, 20] </ref>. Our implementation model involves a self-timed scheduling strategy [14]. Each processor executes the tasks assigned to it in a fixed order that is specified at compile time. Before firing an actor, a processor waits for the data needed by that actor to become available.
Reference: [2] <author> S. S. Bhattacharyya, S. Sriram, and E. A. Lee, </author> <title> Optimizing Synchronization in Multiprocessor Implementations of Iterative Dataflow Programs, </title> <institution> Electronics Research Laboratory, University of California at Berkeley, </institution> <month> January, </month> <year> 1995. </year>
Reference-contexts: Sender-receiver synchronization is also assumed to take place by setting and checking ags in shared memory (see <ref> [2] </ref> for details on the assumed synchronization protocols). Thus, resynchronization achieves its benefit by reducing the rate of accesses to shared memory for the purpose of synchronization. 2. <p> The following theorem, which is developed in <ref> [2] </ref>, underlies the validity of resynchronization. <p> A synchronization edge is redundant in a synchronization graph if its removal yields a graph that preserves . For example, in Figure 1 (c), the synchronization edge is redundant due to the path . In <ref> [2] </ref>, it is shown that if all redundant edges in a synchronization graph are removed, then the resulting graph preserves the original synchronization graph. Given a synchronization graph , a synchronization edge in , and an ordered pair of actors in , we say that subsumes in if .
Reference: [3] <author> S. S. Bhattacharyya, S. Sriram, and E. A. Lee, </author> <title> Minimizing Synchronization Ov erhead in Statically Scheduled Multiprocessor Systems, </title> <booktitle> Proc. Intl. Conf. on Application Specific Array Processors, </booktitle> <month> July, </month> <year> 1995. </year>
Reference-contexts: It has been demonstrated that the amount of run-time overhead required for synchronization can be reduced significantly by detecting and eliminating redundant synchronizations <ref> [3, 19] </ref>. The objective of resynchronization is to introduce new synchronizations in such a way that the number of original synchronizations that consequently become redundant is significantly greater that the number of new synchronizations. <p> In contrast this paper addresses the problem of resynchronization under fixed latency constraints. In [19], Shaffer presents an algorithm that removes redundant synchronizations in the self-timed execution of a non-iterative DFG. This technique was subsequently extended to handle iterative execution and DFG edges that have delay <ref> [3] </ref>. These approaches differ from the techniques of this paper in that they only consider the redundancy induced by the original synchronizations; they do not consider the addition of new synchronizations. In [3], an efficient algorithm, called Convert-to-SC-graph, is described for introducing new synchronization edges so that the synchronization graph becomes <p> This technique was subsequently extended to handle iterative execution and DFG edges that have delay <ref> [3] </ref>. These approaches differ from the techniques of this paper in that they only consider the redundancy induced by the original synchronizations; they do not consider the addition of new synchronizations. In [3], an efficient algorithm, called Convert-to-SC-graph, is described for introducing new synchronization edges so that the synchronization graph becomes strongly connected, which allows all synchronization edges to be implemented with a more efficient synchronization protocol.
Reference: [4] <author> S. S. Bhattacharyya, S. Sriram, and E. A. Lee, </author> <title> Resynchronization for Embedded Multiprocessors, </title> <institution> Electronics Research Laboratory, University of California at Berkeley, </institution> <month> September, </month> <year> 1995. </year>
Reference-contexts: of synchronization graphs in which the first invocation of the latency output is inuenced by the first invocation of the latency input; equivalently, it is the class of graphs that have at least one delay-less path in the corresponding application DFG directed from the latency input to the latency output <ref> [4] </ref>. Since the first invocation of any execution source starts execution at time 0, the latency of a transparent synchronization graph is given by (the completion time of the first invocation of ), where is the associated latency output. <p> This acyclic graph, denoted , is constructed by removing all edges from that have nonzero-delay, adding a vertex , setting , and adding delayless edges from to each execution source of . In <ref> [4] </ref>, it is shown that if is a transparent synchronization graph with latency output , then , where if there is no path from to in , and if there is a path from to , then is defined to be the maximum cumulative execution time (sum of the execution times <p> However, for a significant class of synchronization graphs, the latency is not affected by Convert-to-SC-graph, and thus, for such systems resynchronization and Convert-to-SC-graph are fully complementary <ref> [4] </ref>. Resynchronization has been studied earlier in the context of hardware synthesis [8]. <p> However in this work, the scheduling model and implementation model are significantly different from the structure of self-timed multiprocessor implementations, and as a consequence, the analysis techniques and algorithmic solutions do not apply to our context, and vice-versa <ref> [4] </ref>. 5. Intractability We have established that latency-constrained resynchronization is NP-hard even for the very restricted subclass of transparent synchronization graphs in which each SCC corresponds to a single actor, and all synchronization edges have zero delay. <p> In this section, we outline the intuition behind this result; a detailed proof can be found in <ref> [4] </ref>. The intractability of latency-constrained resynchronization can be established by a reduction from the set covering problem, which is a well-known NP-hard problem [7]. <p> The time complexity of this transformation is . Now let denote an arbitrary instance of set covering, and let be the synchroniza tion graph that results when the construction of Figure 3 is applied to . In <ref> [4] </ref> it is shown that from any optimal LCR for , an optimal LCR for can be derived in polynomial time such that for each resynchronization edge in , , and , and (5) in out G Instantiate actors , with execution times , , , , and , respectively, and <p> In this section, we illustrate how delayless 2LCR can be solved in time quadratic in the number of vertices in the synchronization graph. We have extended this approach to solve the general (not necessarily delayless) 2LCR problem in cubic time; we refer the reader to <ref> [4] </ref> for details on this extension, and for formal proofs of the optimality of our techniques for delayless 2LCR and general 2LCR. <p> Thus, while general set covering involves covering a set from a collection of subsets, interval covering amounts to covering an interval from a collection of sub-intervals. The interval covering problem can be solved in time using a straightforward approach <ref> [4] </ref>. Our algorithm for the 2LCR problem is based on the following lemma, which is established in [4]. Lemma 1: If is a resynchronization of , then , where for , and for . <p> The interval covering problem can be solved in time using a straightforward approach <ref> [4] </ref>. Our algorithm for the 2LCR problem is based on the following lemma, which is established in [4]. Lemma 1: If is a resynchronization of , then , where for , and for . The set in the interval covering instance that we derive from is the set of synchronization edges in . <p> Here, , and from (8), we have , The set of interval subsets of to be covered is then computed as . (10) In <ref> [4] </ref> we show that the family of subsets defined by (10) together with the ordering speci fied by (7) always forms an instance of interval covering, and that given a solution (minimal cover) to this instance of interval covering, is an optimal latency-constrained resynchronization of .
Reference: [5] <author> S. S. Bhattacharyya, S. Sriram, and E. A. Lee, </author> <title> Self-Timed Resynchronization: a Post-optimization for Static Multiprocessor Schedules, </title> <note> to appear in Proc. Intl. Parallel Processing Symposium, </note> <month> April, </month> <year> 1996. </year>
Reference-contexts: Related work In <ref> [5] </ref>, the problem of finding a resynchronization that has minimal cardinality (the resyn-chronization problem) is shown to be NP-hard, and an efficient family of heuristics is presented. Also, a class of synchronization graphs is identified for which optimal resynchronizations can be computed using an efficient polynomial-time algorithm. The developments of [5] <p> <ref> [5] </ref>, the problem of finding a resynchronization that has minimal cardinality (the resyn-chronization problem) is shown to be NP-hard, and an efficient family of heuristics is presented. Also, a class of synchronization graphs is identified for which optimal resynchronizations can be computed using an efficient polynomial-time algorithm. The developments of [5] assume that arbitrary increases in latency can be tolerated (unbounded-latency resynchronization); such a scenario may arise, for example, in simulation applications. In contrast this paper addresses the problem of resynchronization under fixed latency constraints.
Reference: [6] <author> L. F. Chao and E. Sha, </author> <title> Unfolding and Retiming Data-Flo w DSP Programs for RISC Multiprocessor Scheduling, </title> <booktitle> Proc. Intl. Conf. on Acoustics, Speech, and Signal Processing, </booktitle> <month> April </month> <year> 1992. </year>
Reference-contexts: We refer to a homogeneous SDF graph as a DFG. The techniques developed in this paper can be used as a post-processing step to improve the performance of any static multiprocessor scheduling technique for iterative DFGs, such as those described in <ref> [1, 6, 9, 17, 20] </ref>. Our implementation model involves a self-timed scheduling strategy [14]. Each processor executes the tasks assigned to it in a fixed order that is specified at compile time. Before firing an actor, a processor waits for the data needed by that actor to become available.
Reference: [7] <author> T. H. Cormen, C. E. Leiserson, and R. L. Ri vest, </author> <title> Introduction to Algorithms, </title> <publisher> McGraw-Hill, </publisher> <year> 1990. </year> <note> 1. References 2, 3, and 4 are available by anonymous ftp from ptolemy.eecs.berkeley.edu in the directory pub/ptolemy/papers/synchOpt. 25 </note>
Reference-contexts: the synchronization graph that results from inserting a new synchronization edge (a resynchronization edge) into can be computed from . (4) The values for all pairs can be computed in time, where is the number of actors in , by using a simple adaptation of the Floyd-Warshall algorithm specified in <ref> [7] </ref>. Such an efficient means for computing latency permits the development of systematic resynchro-nization techniques to trade off synchronization overhead and latency. <p> In this section, we outline the intuition behind this result; a detailed proof can be found in [4]. The intractability of latency-constrained resynchronization can be established by a reduction from the set covering problem, which is a well-known NP-hard problem <ref> [7] </ref>. In the set covering problem, one is given a finite set and a family of subsets of , and asked to find a minimal (fewest number of members) subfamily such that .
Reference: [8] <author> D. Filo, D. C. Ku, and G. De Micheli, </author> <title> Optimizing the Control-unit through the Resynchroni-zation of Operations, INTEGRATION, </title> <journal> the VLSI Journal, </journal> <volume> Vol. 13, </volume> <year> 1992. </year>
Reference-contexts: However, for a significant class of synchronization graphs, the latency is not affected by Convert-to-SC-graph, and thus, for such systems resynchronization and Convert-to-SC-graph are fully complementary [4]. Resynchronization has been studied earlier in the context of hardware synthesis <ref> [8] </ref>. However in this work, the scheduling model and implementation model are significantly different from the structure of self-timed multiprocessor implementations, and as a consequence, the analysis techniques and algorithmic solutions do not apply to our context, and vice-versa [4]. 5.
Reference: [9] <author> R. Govindarajan, G. R. Gao, and P. Desai, </author> <title> Minimizing Memory Requirements in Rate-Optimal Schedules, </title> <booktitle> Proc. Intl. Conf. on Application Specific Array Processors, </booktitle> <month> August, </month> <year> 1994. </year>
Reference-contexts: We refer to a homogeneous SDF graph as a DFG. The techniques developed in this paper can be used as a post-processing step to improve the performance of any static multiprocessor scheduling technique for iterative DFGs, such as those described in <ref> [1, 6, 9, 17, 20] </ref>. Our implementation model involves a self-timed scheduling strategy [14]. Each processor executes the tasks assigned to it in a fixed order that is specified at compile time. Before firing an actor, a processor waits for the data needed by that actor to become available.
Reference: [10] <author> T. C. Hu, </author> <title> Parallel Sequencing and Assembly Line Problems, </title> <journal> Operations Research, </journal> <volume> Vol. 9, </volume> <year> 1961. </year>
Reference-contexts: pattern of complexity that is analogous to the classic nonpreemptive processor scheduling problem with deterministic execution times, in which the problem is also intractable for general systems, but an efficient greedy algorithm suffices to yield optimal solutions for two-processor systems in which the execution times of all tasks are identical <ref> [10] </ref>. However, for latency-constrained resynchronization, the tractability for two-processor systems does not depend on any constraints on the task (actor) execution times. 1.
Reference: [11] <author> D. S. Johnson, </author> <title> Approximation Algorithms for Combinatorial Problems, </title> <journal> Journal of Computer and System Sciences, </journal> <volume> Vol. 9, </volume> <year> 1974. </year>
Reference-contexts: Our heuristic is based on an approximation algorithm for set covering that repeatedly selects a subset that covers the largest number of remaining elements, where a remaining element is an element that is not contained in any of the subsets that have already been selected <ref> [11, 15] </ref>.
Reference: [12] <author> R. Lauwereins, M. Engels, J.A. Peperstraete, E. Stee gmans, and J. Van Ginderdeuren, </author> <title> GRAPE: A CASE Tool for Digital Signal P arallel Processing, </title> <journal> IEEE ASSP Magazine, </journal> <volume> Vol. 7, No. 2, </volume> <month> April, </month> <year> 1990. </year>
Reference-contexts: Examples of commercial tools that use this model are the Signal Processing Worksystem (SPW) [18], and COSSAP [21], and examples of tools developed at various universities are Ptolemy [17], the Warp compiler [20], DESCARTES [21], and GRAPE <ref> [12] </ref>. In SDF, a program is represented as a directed graph in which vertices (actors) represent L max 3 computational tasks, edges specify data dependences, and the number of data values (tokens) produced and consumed by each actor is fixed.
Reference: [13] <author> E. A. Lee and D. G. Messerschmitt, </author> <title> Synchronous Dataow, </title> <booktitle> Proceedings of the IEEE, </booktitle> <address> Sep-tember, </address> <year> 1987. </year>
Reference-contexts: This paper addresses the problem of computing an optimal resynchronization among all resynchronizations that do not increase the latency beyond a prespecified upper bound . We address this problem in the context of self-timed execution of iterative synchronous dataow (SDF) <ref> [13] </ref> programs. An iterative dataow program consists of a dataow representation of the body of a loop that is to be iterated infinitely. Iterative SDF programming is used extensively for the implementation of digital signal processing systems. <p> We assume that the input SDF graph is homogeneous, which means that the numbers of tokens produced and consumed are identically unity. However, since efficient techniques have been developed to convert general SDF graphs into homogeneous graphs <ref> [13] </ref>, our techniques can easily be adapted to general SDF graphs. We refer to a homogeneous SDF graph as a DFG.
Reference: [14] <author> E. A. Lee and S. Ha, </author> <title> Scheduling Strategies for Multiprocessor Real-Time DSP, </title> <booktitle> Globecom, </booktitle> <month> November </month> <year> 1989. </year>
Reference-contexts: The techniques developed in this paper can be used as a post-processing step to improve the performance of any static multiprocessor scheduling technique for iterative DFGs, such as those described in [1, 6, 9, 17, 20]. Our implementation model involves a self-timed scheduling strategy <ref> [14] </ref>. Each processor executes the tasks assigned to it in a fixed order that is specified at compile time. Before firing an actor, a processor waits for the data needed by that actor to become available. Thus, processors are required to perform run-time synchronization when they communicate data.
Reference: [15] <author> L. Lovasz, </author> <title> On the Ratio of Optimal Integral and Fractional Covers, </title> <journal> Discrete Mathematics, </journal> <volume> Vol. 13, </volume> <year> 1975. </year>
Reference-contexts: Our heuristic is based on an approximation algorithm for set covering that repeatedly selects a subset that covers the largest number of remaining elements, where a remaining element is an element that is not contained in any of the subsets that have already been selected <ref> [11, 15] </ref>.
Reference: [16] <author> K. Parhi and D. G. Messerschmitt, </author> <title> Static Rate-optimal Scheduling of Iterati ve Data-flow Programs via Optimum Unfolding, </title> <journal> IEEE Transactions on Computers, </journal> <month> February </month> <year> 1991. </year>
Reference: [17] <author> J. Pino, S. Ha, E. A. Lee, and J. T. Buck, </author> <title> Software Synthesis for DSP Using Ptolemy, </title> <journal> Journal of VLSI Signal Processing, </journal> <month> January, </month> <year> 1995. </year> <month> 26 </month>
Reference-contexts: Iterative SDF programming is used extensively for the implementation of digital signal processing systems. Examples of commercial tools that use this model are the Signal Processing Worksystem (SPW) [18], and COSSAP [21], and examples of tools developed at various universities are Ptolemy <ref> [17] </ref>, the Warp compiler [20], DESCARTES [21], and GRAPE [12]. In SDF, a program is represented as a directed graph in which vertices (actors) represent L max 3 computational tasks, edges specify data dependences, and the number of data values (tokens) produced and consumed by each actor is fixed. <p> We refer to a homogeneous SDF graph as a DFG. The techniques developed in this paper can be used as a post-processing step to improve the performance of any static multiprocessor scheduling technique for iterative DFGs, such as those described in <ref> [1, 6, 9, 17, 20] </ref>. Our implementation model involves a self-timed scheduling strategy [14]. Each processor executes the tasks assigned to it in a fixed order that is specified at compile time. Before firing an actor, a processor waits for the data needed by that actor to become available.
Reference: [18] <author> D. B. Powell, E. A. Lee, and W. C. Newman, </author> <title> Direct Synthesis of Optimized DSP Assembly Code from Signal Flow Block Diagrams, </title> <booktitle> Proc. Intl. Conf. on Acoustics, Speech, and Signal Processing, </booktitle> <month> March, </month> <year> 1992. </year>
Reference-contexts: An iterative dataow program consists of a dataow representation of the body of a loop that is to be iterated infinitely. Iterative SDF programming is used extensively for the implementation of digital signal processing systems. Examples of commercial tools that use this model are the Signal Processing Worksystem (SPW) <ref> [18] </ref>, and COSSAP [21], and examples of tools developed at various universities are Ptolemy [17], the Warp compiler [20], DESCARTES [21], and GRAPE [12].
Reference: [19] <author> P. L. Shaffer, </author> <title> Minimization of Interprocessor Synchronization in Multiprocessors with Shared and Private Memory, </title> <booktitle> Proc. Intl. Conf. on Parallel Processing, </booktitle> <year> 1989. </year>
Reference-contexts: It has been demonstrated that the amount of run-time overhead required for synchronization can be reduced significantly by detecting and eliminating redundant synchronizations <ref> [3, 19] </ref>. The objective of resynchronization is to introduce new synchronizations in such a way that the number of original synchronizations that consequently become redundant is significantly greater that the number of new synchronizations. <p> The developments of [5] assume that arbitrary increases in latency can be tolerated (unbounded-latency resynchronization); such a scenario may arise, for example, in simulation applications. In contrast this paper addresses the problem of resynchronization under fixed latency constraints. In <ref> [19] </ref>, Shaffer presents an algorithm that removes redundant synchronizations in the self-timed execution of a non-iterative DFG. This technique was subsequently extended to handle iterative execution and DFG edges that have delay [3].
Reference: [20] <author> H. Printz, </author> <title> Compilation of Narro wband Spectral Detection Systems for Linear MIMD Machines, </title> <booktitle> Proc. Intl. Conf. on Application Specific Array Processors, </booktitle> <month> August, </month> <year> 1992. </year>
Reference-contexts: Iterative SDF programming is used extensively for the implementation of digital signal processing systems. Examples of commercial tools that use this model are the Signal Processing Worksystem (SPW) [18], and COSSAP [21], and examples of tools developed at various universities are Ptolemy [17], the Warp compiler <ref> [20] </ref>, DESCARTES [21], and GRAPE [12]. In SDF, a program is represented as a directed graph in which vertices (actors) represent L max 3 computational tasks, edges specify data dependences, and the number of data values (tokens) produced and consumed by each actor is fixed. <p> We refer to a homogeneous SDF graph as a DFG. The techniques developed in this paper can be used as a post-processing step to improve the performance of any static multiprocessor scheduling technique for iterative DFGs, such as those described in <ref> [1, 6, 9, 17, 20] </ref>. Our implementation model involves a self-timed scheduling strategy [14]. Each processor executes the tasks assigned to it in a fixed order that is specified at compile time. Before firing an actor, a processor waits for the data needed by that actor to become available.
Reference: [21] <author> S. Ritz, M. Pankert, and H. Meyr, </author> <title> High Level Software Synthesis for Signal Processing Systems, </title> <booktitle> Proc. Intl. Conf. on Application Specific Array Processors, </booktitle> <month> August, </month> <year> 1992. </year>
Reference-contexts: Iterative SDF programming is used extensively for the implementation of digital signal processing systems. Examples of commercial tools that use this model are the Signal Processing Worksystem (SPW) [18], and COSSAP <ref> [21] </ref>, and examples of tools developed at various universities are Ptolemy [17], the Warp compiler [20], DESCARTES [21], and GRAPE [12]. <p> Iterative SDF programming is used extensively for the implementation of digital signal processing systems. Examples of commercial tools that use this model are the Signal Processing Worksystem (SPW) [18], and COSSAP <ref> [21] </ref>, and examples of tools developed at various universities are Ptolemy [17], the Warp compiler [20], DESCARTES [21], and GRAPE [12]. In SDF, a program is represented as a directed graph in which vertices (actors) represent L max 3 computational tasks, edges specify data dependences, and the number of data values (tokens) produced and consumed by each actor is fixed.
Reference: [22] <author> R. Reiter, </author> <title> Scheduling Parallel Computations, </title> <journal> Journal of the ACM, </journal> <month> October </month> <year> 1968. </year>
Reference-contexts: i k,( ) end v j k v j v i ,( )( )delay,( ) start v k,( ) end v k,( ) k v G ipc G ipc 6 the time required for IPC is ignored (assumed to be zero), then as a consequence of Reiters anal ysis in <ref> [22] </ref>, the throughput (number of DFG iterations per unit time) of a synchronization graph is given by , (2) where is the sum of the delays of all edges that are traversed by the cycle .
References-found: 23

