URL: ftp://ftp.idsia.ch/pub/rafal/ICONIP_soccer.ps.gz
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00408.html
Root-URL: 
Email: e-mail: frafal, marco, juergeng@idsia.ch  
Title: Evolving Soccer Strategies  
Author: Rafa l Sa lustowicz, Marco Wiering, Jurgen Schmidhuber 
Address: Corso Elvezia 36, 6900 Lugano, Switzerland  
Affiliation: IDSIA,  
Abstract: In N. Kasabov, R. Kozma, K. Ko, R. O'Shea, G. Coghill, and T. Gedeon, editors, Progress in Connectionist-based Information Systems: Proceedings of the Fourth International Conference on Neural Information Processing ICONIP'97, volume 1, pages 502-505. Springer-Verlag Singapore, 1997. Abstract We study multiagent learning in a simulated soccer scenario. Players from the same team share a common policy for mapping inputs to actions. They get rewarded or punished collectively in case of goals. For varying team sizes we compare the following learning algorithms: TD-Q learning with linear neural networks (TD-Q-LIN), with a neural gas network (TD-Q-NG), Probabilistic Incremental Program Evolution (PIPE), and a PIPE variant based on coevolution (CO-PIPE). TD-Q-LIN and TD-Q-NG try to learn evaluation functions (EFs) mapping input/action pairs to expected reward. PIPE and CO-PIPE search policy space directly. They use adaptive probability distributions to synthesize programs that calculate action probabilities from current inputs. We find that learning appropriate EFs is hard for both EF-based approaches. Direct search in policy space discovers more reliable policies and is faster. 
Abstract-found: 1
Intro-found: 1
Reference: [Bertsekas and Tsitsiklis, 1996] <author> Bertsekas, D. P. and Tsitsiklis, J. N. </author> <year> (1996). </year> <title> Neuro-Dynamic Programming. </title> <publisher> Athena Scientific, </publisher> <address> Belmont, MA. </address>
Reference: [Cramer, 1985] <author> Cramer, N. L. </author> <year> (1985). </year> <title> A representation for the adaptive generation of simple sequential programs. </title> <editor> In Grefenstette, J., editor, </editor> <booktitle> Proceedings of an International Conference on Genetic Algorithms and Their Applications, </booktitle> <address> Hillsdale NJ. </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference-contexts: EFs are then exploited to select actions. Methods from the second class do not require EFs. Their policy space consists of complete algorithms defining agent behaviors, and they search policy space directly. Members of this class are Levin search [Levin, 1973], Genetic Programming, e.g., <ref> [Cramer, 1985] </ref>, and Probabilistic Incremental Program Evolution (PIPE) [Sa lustowicz and Schmidhuber, 1997]. Previous Results. Recently we compared two learning algorithms [Sa lustowicz et al., 1997b], each representative of its class: TD-Q learning [Lin, 1993] with linear neural nets (TD-Q-LIN) and PIPE.
Reference: [Fritzke, 1995] <author> Fritzke, B. </author> <year> (1995). </year> <title> A growing neural gas network learns topologies. </title> <editor> In Tesauro, G., Touretzky, D. S., and Leen, T. K., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <pages> pages 625-632. </pages> <publisher> MIT Press, </publisher> <address> Cambridge MA. </address>
Reference-contexts: Here we use a neural gas network instead <ref> [Fritzke, 1995] </ref>. The goal is to map a player-specific input ~ i (p; t) to action evaluations Q ( ~ i (p; t); a d ), where a d 2 ASET . We use the same neural gas network for all policy-sharing players.
Reference: [Levin, 1973] <author> Levin, L. A. </author> <year> (1973). </year> <title> Universal sequential search problems. </title> <journal> Problems of Information Transmission, </journal> <volume> 9(3) </volume> <pages> 265-266. </pages>
Reference-contexts: EFs are then exploited to select actions. Methods from the second class do not require EFs. Their policy space consists of complete algorithms defining agent behaviors, and they search policy space directly. Members of this class are Levin search <ref> [Levin, 1973] </ref>, Genetic Programming, e.g., [Cramer, 1985], and Probabilistic Incremental Program Evolution (PIPE) [Sa lustowicz and Schmidhuber, 1997]. Previous Results. Recently we compared two learning algorithms [Sa lustowicz et al., 1997b], each representative of its class: TD-Q learning [Lin, 1993] with linear neural nets (TD-Q-LIN) and PIPE.
Reference: [Lin, 1993] <author> Lin, L. J. </author> <year> (1993). </year> <title> Reinforcement Learning for Robots Using Neural Networks. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, Pittsburgh. </institution>
Reference-contexts: Members of this class are Levin search [Levin, 1973], Genetic Programming, e.g., [Cramer, 1985], and Probabilistic Incremental Program Evolution (PIPE) [Sa lustowicz and Schmidhuber, 1997]. Previous Results. Recently we compared two learning algorithms [Sa lustowicz et al., 1997b], each representative of its class: TD-Q learning <ref> [Lin, 1993] </ref> with linear neural nets (TD-Q-LIN) and PIPE. We let both approaches compete against a biased random opponent (BRO). PIPE quickly learned to beat BRO. TD-Q-LIN had difficulties in learning appropriate shared EFs, especially in case of multiple agents per team. Comparisons. <p> CO-PIPE adapts PPT's probabilities so that the probability of creating the winning program increases. 4 TD-Q Learning In a previous paper [Sa lustowicz et al., 1997b] we found that learning correct soccer EFs was hard for an o*ine TD () Q-variant <ref> [Lin, 1993] </ref> with linear neural nets. Here we use a neural gas network instead [Fritzke, 1995]. The goal is to map a player-specific input ~ i (p; t) to action evaluations Q ( ~ i (p; t); a d ), where a d 2 ASET .
Reference: [Sa lustowicz and Schmidhuber, 1997] <author> Sa lustowicz, R. P. and Schmidhuber, J. </author> <year> (1997). </year> <title> Probabilistic incremental program evolution. </title> <booktitle> Evolutionary Computation, </booktitle> <pages> 5(2). </pages>
Reference-contexts: Methods from the second class do not require EFs. Their policy space consists of complete algorithms defining agent behaviors, and they search policy space directly. Members of this class are Levin search [Levin, 1973], Genetic Programming, e.g., [Cramer, 1985], and Probabilistic Incremental Program Evolution (PIPE) <ref> [Sa lustowicz and Schmidhuber, 1997] </ref>. Previous Results. Recently we compared two learning algorithms [Sa lustowicz et al., 1997b], each representative of its class: TD-Q learning [Lin, 1993] with linear neural nets (TD-Q-LIN) and PIPE. We let both approaches compete against a biased random opponent (BRO). <p> ASET contains: go forward, turn to ball, turn to goal and shoot. Shots are noisy and noise makes long shots less precise than close passes. For a detailed description of the simulator see [Sa lustowicz et al., 1997a]. 3 Probabilistic Incremental Pro gram Evolution (PIPE) PIPE <ref> [Sa lustowicz and Schmidhuber, 1997] </ref> synthesizes programs which select actions from ASET, given player p's input vector ~ i (p; t). Action Selection. Action selection depends on 5 variables: g 2 IR, A i 2 IR, 8i 2 ASET . <p> The offset 100 is sufficient to ensure a positive score difference. PIPE then adapts PPT's probabilities so that the probability of creating the best program of the current population increases. Finally PPT's probabilities are mutated to better explore the search space. All details can be found in <ref> [Sa lustowicz and Schmidhuber, 1997] </ref>. Coevolution (CO-PIPE). CO-PIPE works exactly like PIPE, except that: (a) the population contains only two programs and (b) we let both programs play against each other rather than against a prewired opponent.
Reference: [Sa lustowicz et al., 1997a] <author> Sa lustowicz, R. P., Wiering, M. A., and Schmidhuber, J. </author> <year> (1997a). </year> <title> Learning team strategies with multiple policy-sharing agents: A soccer case study. </title> <type> Technical Report IDSIA-29-97, </type> <institution> IDSIA. </institution>
Reference-contexts: Actions. Players may execute actions from action set ASET. ASET contains: go forward, turn to ball, turn to goal and shoot. Shots are noisy and noise makes long shots less precise than close passes. For a detailed description of the simulator see <ref> [Sa lustowicz et al., 1997a] </ref>. 3 Probabilistic Incremental Pro gram Evolution (PIPE) PIPE [Sa lustowicz and Schmidhuber, 1997] synthesizes programs which select actions from ASET, given player p's input vector ~ i (p; t). Action Selection.
Reference: [Sa lustowicz et al., 1997b] <author> Sa lustowicz, R. S., Wiering, M. A., and Schmidhuber, J. </author> <year> (1997b). </year> <title> On learning soccer strategies. </title> <booktitle> In Proceedings of the 7th International Conference on Artificial Neural Networks (ICANN'97), Lecture Notes in Computer Science. </booktitle> <address> Springer-Verlag Berlin Heidelberg. </address> <note> To appear. </note>
Reference-contexts: Their policy space consists of complete algorithms defining agent behaviors, and they search policy space directly. Members of this class are Levin search [Levin, 1973], Genetic Programming, e.g., [Cramer, 1985], and Probabilistic Incremental Program Evolution (PIPE) [Sa lustowicz and Schmidhuber, 1997]. Previous Results. Recently we compared two learning algorithms <ref> [Sa lustowicz et al., 1997b] </ref>, each representative of its class: TD-Q learning [Lin, 1993] with linear neural nets (TD-Q-LIN) and PIPE. We let both approaches compete against a biased random opponent (BRO). PIPE quickly learned to beat BRO. <p> CO-PIPE adapts PPT's probabilities so that the probability of creating the winning program increases. 4 TD-Q Learning In a previous paper <ref> [Sa lustowicz et al., 1997b] </ref> we found that learning correct soccer EFs was hard for an o*ine TD () Q-variant [Lin, 1993] with linear neural nets. Here we use a neural gas network instead [Fritzke, 1995]. <p> This is partially due to PIPE's and CO-PIPE's ability to efficiently select relevant input features for each action. TD-Q-LIN's score differences first in crease until TD-Q-LIN runs into an "outlier problem", which lets its linear nets unlearn previously discovered good policies (see <ref> [Sa lustowicz et al., 1997b] </ref> for details). TD-Q-NG initially learns faster than TD-Q-LIN, but does not continue improving.
References-found: 8

