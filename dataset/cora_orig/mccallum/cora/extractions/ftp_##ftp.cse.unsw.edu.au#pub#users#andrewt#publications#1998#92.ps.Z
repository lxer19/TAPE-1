URL: ftp://ftp.cse.unsw.edu.au/pub/users/andrewt/publications/1998/92.ps.Z
Refering-URL: http://www.cse.unsw.edu.au/school/publications/1998/SCSE_publications.html
Root-URL: http://www.cse.unsw.edu.au
Title: Measuring the Effectiveness of Retrieval Systems that Create Rankings  
Author: B. J. Briedis and T. D. Gedeon 
Keyword: Retrieval systems, ranked output, effectiveness measures.  
Address: Sydney NSW 2052 Australia  Sydney NSW 2052 Australia  
Affiliation: School of Computer Science Engineering The University of New South Wales  School of Computer Science Engineering The University of New South Wales  
Abstract: The method used to measure the success of a retrieval system is of primary importance. The nature of retrieval engines has been evolving, with very large retrieval engines becoming common, and with the use of ranking becoming widespread. In particular many search engines on the World Wide Web return huge numbers of documents and it is left solely to the user to decide when to stop searching through a list of retrieved items. This paper presents a new measure suitable for testing search algorithms that provide documents in this fashion. The measure is a single figure that is easy to use when comparing different systems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. C. Blair and M. E. Maron, </author> <title> An Evaluation of Retrieval Effectiveness for a Full-Text Document-Retrieval System, </title> <journal> Communications of the ACM, </journal> <volume> Vol. 28, No. 3, </volume> <pages> pp. 289-299, </pages> <year> 1985. </year>
Reference-contexts: In other systems, such as various legal databases, a high level of recall may be sought <ref> [1, 13] </ref>. The reading of the graph must be varied according to the nature of the document collection. In order to overcome some of the shortfalls of the recall-precision curve a number of statistics have been used, some of them seeking to summarize the curve itself.
Reference: [2] <author> B. J. Briedis and T. D. Gedeon, </author> <title> Using the Grow-and-Prune Network to Solve Problems of Large Dimensionality, </title> <booktitle> Proceedings of the 1998 Aus-tralian Conference on Neural Networks, </booktitle> <address> Brisbane, </address> <year> 1998. </year>
Reference-contexts: It is, for example, necessary with the LSI technique to decide on the number of dimensions the document representations are to have [4]. If a neural network is used, it may be necessary to decide when to stop training, when to add nodes or when to prune the network <ref> [2] </ref>. In the case of a genetic algorithm a single value of success is referred to continuously throughout evolution (e.g. [8]). The traditional indicators of retrieval effectiveness are precision and recall.
Reference: [3] <author> C. W. Cleverdon, </author> <title> User Evaluation of Information Retrieval Systems, </title> <journal> Journal of Documentation, </journal> <volume> Vol. 30, No. 2, </volume> <year> 1974, </year> <journal> pp. </journal> <volume> 170-180. 0.1 0.3 0.5 0.7 0.9 0 0 0 0 0 0 0 0 0 0 0 1 0 2 1 0 4 1 0 6 1 0 8 1 0 0 x' Retrieval Effectiveness M adi time med cran cisi npl 1 </volume>
Reference-contexts: A number of other evaluation strategies are summarized in van Rijsbergen [12]. It has occasionally been suggested that, rather than using precision and recall, an attempt should be made to model the situation the user is in when using a retrieval system <ref> [3, 5] </ref>. One simple move in this direction is to plot the number of relevant documents retrieved against the number of documents looked at by the user (e.g. see [13]). A problem with this approach is that different users view different numbers of documents. <p> The relevancy judgments may be binary (relevant or not relevant) or fuzzy (e.g. 70% relevant). Another possible method for describing the relevance of documents is to have the assessor list those documents that are relevant in the order of their relevance. This apparently results in more consistent assessments <ref> [5, 3] </ref>. In order for our measure to be used with relevance judgments expressed as a ranking it is necessary first to convert the ranking to fuzzy relevance judgments. <p> One likely effect of this would be for the documents to be rated according to utility rather than relevance, which has been suggested as being preferable <ref> [3] </ref>. It also allows an assessor to make judgments about the rankings themselves, not just the documents that they contain. An assessor might, for instance, decide that it is important to have a mix of different types of document in the top part of the ranking.
Reference: [4] <author> S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer and R. Harshman, </author> <title> Indexing by Latent Semantic Analysis, </title> <journal> Journal of the American Society for Information Science, </journal> <volume> Vol. 41, No. 6, </volume> <pages> pp. 391-407, </pages> <year> 1990. </year>
Reference-contexts: This is particularly important when it is necessary to set parameters for a retrieval system or when the retrieval system incorporates a training algorithm. It is, for example, necessary with the LSI technique to decide on the number of dimensions the document representations are to have <ref> [4] </ref>. If a neural network is used, it may be necessary to decide when to stop training, when to add nodes or when to prune the network [2]. In the case of a genetic algorithm a single value of success is referred to continuously throughout evolution (e.g. [8]).
Reference: [5] <author> H. P. Frei and P. Schuble, </author> <title> Determining the Effectiveness of Retrieval Algorithms, </title> <booktitle> Information Processing & Management, </booktitle> <volume> Vol. 27, No. 2/3, </volume> <pages> pp. 153-164, </pages> <year> 1991. </year>
Reference-contexts: This is a very demanding requirement, and in some cases it may be better to use an alternative evaluation method, such as that proposed by Frei and Schuble <ref> [5] </ref>. Nonetheless, having a complete set of relevancy judgments is very convenient: it allows for the consistent benchmarking of techniques and the benchmarking may be done quickly and repeatedly, without the need for an assessor to make subsequent relevance judgments, as is necessary with the Frei and Schuble method. <p> A number of other evaluation strategies are summarized in van Rijsbergen [12]. It has occasionally been suggested that, rather than using precision and recall, an attempt should be made to model the situation the user is in when using a retrieval system <ref> [3, 5] </ref>. One simple move in this direction is to plot the number of relevant documents retrieved against the number of documents looked at by the user (e.g. see [13]). A problem with this approach is that different users view different numbers of documents. <p> The relevancy judgments may be binary (relevant or not relevant) or fuzzy (e.g. 70% relevant). Another possible method for describing the relevance of documents is to have the assessor list those documents that are relevant in the order of their relevance. This apparently results in more consistent assessments <ref> [5, 3] </ref>. In order for our measure to be used with relevance judgments expressed as a ranking it is necessary first to convert the ranking to fuzzy relevance judgments.
Reference: [6] <author> M. E. Lesk and G. Salton, </author> <title> Relevance Assessments and Retrieval System Evaluation, </title> <booktitle> Information Storage and Retrieval, </booktitle> <volume> Vol. 4, </volume> <pages> pp. 343-359, </pages> <year> 1969. </year>
Reference: [7] <author> M. F. Porter, </author> <title> An Algorithm for Suffix Stripping, </title> <booktitle> Program, </booktitle> <volume> Vol. 14, No. 3, </volume> <pages> pp. 130-137, </pages> <year> 1980. </year>
Reference-contexts: Test collection results The effectiveness of a simple retrieval algorithm has been calculated for a number of standard test collections and the results are given in Figure 2. The vector retrieval algorithm suggested by Salton [10] was used, with stemming being done using the Porter algorithm <ref> [7] </ref> and stop words being removed. These results may be useful to other researchers as a guide as to what constitutes reasonable scores on these test collections.
Reference: [8] <author> A. M. Robertson and P. Willett, </author> <title> An Upperbound to the Performance of Ranked-Output Searching: Optimal Weighting of Query Terms using a Genetic Algorithm, </title> <journal> Journal of Documentation, </journal> <volume> Vol. 52, No. 4, </volume> <pages> pp. 405-420, </pages> <year> 1996. </year>
Reference-contexts: If a neural network is used, it may be necessary to decide when to stop training, when to add nodes or when to prune the network [2]. In the case of a genetic algorithm a single value of success is referred to continuously throughout evolution (e.g. <ref> [8] </ref>). The traditional indicators of retrieval effectiveness are precision and recall. In order to calculate precision and recall it is necessary to decide which documents are relevant and which are not.
Reference: [9] <author> G. Salton and M. E. Lesk, </author> <title> Computer Evaluation of Indexing and Text Processing, </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> Vol. 15, No. 1, </volume> <pages> pp. 8-36, </pages> <year> 1968. </year>
Reference-contexts: A problem with both these measures is that all recall values are treated as being of equal importance an invalid assumption. Other related measures that have been used are the harmonic mean of precision and recall [12], normalised recall, normalised precision, rank recall and log precision <ref> [9, 10] </ref>. These measures are all dependent on precision and/or recall. This strong reliance on precision and recall is surprising as they are themselves composite measures. The abundance of (quite different) measures based upon them suggests that they are not well suited to summarizing the effectiveness of retrieval systems. <p> The abundance of (quite different) measures based upon them suggests that they are not well suited to summarizing the effectiveness of retrieval systems. Sometimes several of these measures are used in the same paper (e.g. <ref> [9] </ref>), apparently because none of the measures in isolation is capable of adequately encapsulating all of the information required. A number of other evaluation strategies are summarized in van Rijsbergen [12].
Reference: [10] <author> G. Salton, </author> <title> Introduction to Modern Information Retrieval, </title> <address> New York: </address> <publisher> McGraw-Hill, </publisher> <year> 1983. </year>
Reference-contexts: A problem with both these measures is that all recall values are treated as being of equal importance an invalid assumption. Other related measures that have been used are the harmonic mean of precision and recall [12], normalised recall, normalised precision, rank recall and log precision <ref> [9, 10] </ref>. These measures are all dependent on precision and/or recall. This strong reliance on precision and recall is surprising as they are themselves composite measures. The abundance of (quite different) measures based upon them suggests that they are not well suited to summarizing the effectiveness of retrieval systems. <p> Test collection results The effectiveness of a simple retrieval algorithm has been calculated for a number of standard test collections and the results are given in Figure 2. The vector retrieval algorithm suggested by Salton <ref> [10] </ref> was used, with stemming being done using the Porter algorithm [7] and stop words being removed. These results may be useful to other researchers as a guide as to what constitutes reasonable scores on these test collections.
Reference: [11] <author> J. Savoy, </author> <title> Statistical Inference in Retrieval Effectiveness Evaluation, </title> <booktitle> Information Processing & Management, </booktitle> <volume> Vol. 33, No. 4, </volume> <pages> pp. 495-512, </pages> <year> 1997. </year>
Reference-contexts: One commonly used measure is the mean of the precision taken at some number of evenly spread recall values (e.g. see [12]). It has also been suggested that the median of these values might give better results <ref> [11] </ref>. A problem with both these measures is that all recall values are treated as being of equal importance an invalid assumption. Other related measures that have been used are the harmonic mean of precision and recall [12], normalised recall, normalised precision, rank recall and log precision [9, 10].
Reference: [12] <author> C. J. van Rijsbergen, </author> <title> Information Retrieval, </title> <publisher> Lon-don: Butterworths, </publisher> <year> 1979. </year>
Reference-contexts: In order to overcome some of the shortfalls of the recall-precision curve a number of statistics have been used, some of them seeking to summarize the curve itself. One commonly used measure is the mean of the precision taken at some number of evenly spread recall values (e.g. see <ref> [12] </ref>). It has also been suggested that the median of these values might give better results [11]. A problem with both these measures is that all recall values are treated as being of equal importance an invalid assumption. <p> A problem with both these measures is that all recall values are treated as being of equal importance an invalid assumption. Other related measures that have been used are the harmonic mean of precision and recall <ref> [12] </ref>, normalised recall, normalised precision, rank recall and log precision [9, 10]. These measures are all dependent on precision and/or recall. This strong reliance on precision and recall is surprising as they are themselves composite measures. <p> Sometimes several of these measures are used in the same paper (e.g. [9]), apparently because none of the measures in isolation is capable of adequately encapsulating all of the information required. A number of other evaluation strategies are summarized in van Rijsbergen <ref> [12] </ref>. It has occasionally been suggested that, rather than using precision and recall, an attempt should be made to model the situation the user is in when using a retrieval system [3, 5].
Reference: [13] <author> P. Wallis and J. A. Thom, </author> <title> Relevance Judgments for Accessing Recall, </title> <booktitle> Information Processing & Management, </booktitle> <volume> Vol. 32, No. 3, </volume> <pages> pp. 273-286, </pages> <year> 1996. </year>
Reference-contexts: In other systems, such as various legal databases, a high level of recall may be sought <ref> [1, 13] </ref>. The reading of the graph must be varied according to the nature of the document collection. In order to overcome some of the shortfalls of the recall-precision curve a number of statistics have been used, some of them seeking to summarize the curve itself. <p> One simple move in this direction is to plot the number of relevant documents retrieved against the number of documents looked at by the user (e.g. see <ref> [13] </ref>). A problem with this approach is that different users view different numbers of documents. It is thus not possible to select any one point on the curve as being a good summary of retrieval performance. 3.
References-found: 13

