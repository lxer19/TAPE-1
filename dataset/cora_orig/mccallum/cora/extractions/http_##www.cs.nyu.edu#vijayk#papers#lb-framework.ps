URL: http://www.cs.nyu.edu/vijayk/papers/lb-framework.ps
Refering-URL: http://www.cs.nyu.edu/vijayk/papers.html
Root-URL: http://www.cs.nyu.edu
Email: vijayk@cs.nyu.edu  achien@cs.uiuc.edu  
Title: A Hierarchical Load-Balancing Framework for Dynamic Multithreaded Computations  
Author: Vijay Karamcheti Andrew A. Chien 
Address: New York University  
Affiliation: Department of Computer Science  Department of Computer Science University of Illinois at Urbana-Champaign  
Abstract: High-level parallel programming models supporting dynamic fine-grained threads in a global object space, are becoming increasingly popular for expressing irregular applications based on sophisticated adaptive algorithms and pointer-based data structures. However, implementing these multithreaded computations on scalable parallel machines poses significant challenges, particularly with respect to load-balancing. Load-balancing techniques must simultaneously incur low overhead to support fine-grained threads as well as be sophisticated enough to preserve data locality and thread execution priority. This paper presents a hierarchical framework which addresses these conflicting goals by viewing the computation as being made up of different thread subsets, each of which are load-balanced independently. In contrast to previous processor-centric approaches that have advocated the use of a uniform policy for load-balancing all threads in a computation, our framework allows each thread subset to be load-balanced using a policy most suited to its characteristics (e.g., locality or priority sensitivity). The framework consists of two parts: (i) language support which permits a programmer to tag different thread subsets with appropriate policies, and (ii) run-time support which synthesizes overall application load-balance by composing these individual policies. This framework has been implemented in the Illinois Concert runtime system, an execution platform for fine-grained concurrent object-oriented languages. Results for four large irregular applications on the Cray T3D and the SGI Origin 2000 demonstrate advantages of the hierarchical framework: performance improves by up to an order of magnitude as compared to using a uniform load-balancing policy.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Barnes and P. Hut. </author> <title> A hierarchical O(N log N) force calculation algorithm. </title> <type> Technical report, </type> <institution> The Institute for Advanced Study, Princeton, </institution> <address> New Jersey, </address> <year> 1986. </year>
Reference-contexts: 1 Introduction Irregular parallel computations are increasingly being encountered in applications such as molecular dynamics, particle simulations, and visualization, spanning a number of engineering and scientific domains. State-of-the-art techniques for these computations rely on sophisticated adaptive [2] and tree-based algorithms <ref> [1, 15] </ref>, and pointer-based data structures such as linked lists, trees, and sparse matrices. High-level programming models supporting dynamic thread creation and multithreading [17, 4, 26, 7, 20, 12] enable convenient expression of such computations by relieving the programmer from having to manage details of thread and data structuring.
Reference: [2] <author> Marsha J. Berger and Joseph Oliger. </author> <title> Adaptive mesh refinement for hyperbolic partial differential equations. </title> <journal> Journal of Computational Physics, </journal> <volume> 53:484512, </volume> <year> 1984. </year>
Reference-contexts: 1 Introduction Irregular parallel computations are increasingly being encountered in applications such as molecular dynamics, particle simulations, and visualization, spanning a number of engineering and scientific domains. State-of-the-art techniques for these computations rely on sophisticated adaptive <ref> [2] </ref> and tree-based algorithms [1, 15], and pointer-based data structures such as linked lists, trees, and sparse matrices.
Reference: [3] <author> B.N. Bershad, E.D. Lazowska, and H.M. Levy. </author> <title> Presto: A system for object-oriented parallel programming. </title> <journal> SoftwarePractice and Experience, </journal> <volume> 18(8):713732, </volume> <month> August </month> <year> 1988. </year>
Reference-contexts: The primary difference of our approach with respect to real-time and multimedia operating systems is that the integration between different schedulers is considerably more efficient, relying on non-preemptive scheduling of cooperating application threads. On the other hand, only a handful of programming systems, such as Presto <ref> [3] </ref> and Converse [21], possess similar capabilities. While support for attaching custom schedulers in both Presto and Converse share similar objectives to our approach, there are important differences. First, the thread scheduler interface in both systems is more suitable for medium-grained threads, requiring multiple indirect function calls per scheduling operation.
Reference: [4] <author> Robert D. Blumofe, Christopher F. Joerg, Bradley C. Kuszmaul, Charles E. Leiserson, Keith H. Ran-dall, Andrew Shaw, and Yuli Zhou. Cilk: </author> <title> An efficient multithreaded runtime system. </title> <booktitle> In Proceedings of Principles and Practice of Parallel Programming, </booktitle> <pages> pages 207216, </pages> <year> 1995. </year>
Reference-contexts: State-of-the-art techniques for these computations rely on sophisticated adaptive [2] and tree-based algorithms [1, 15], and pointer-based data structures such as linked lists, trees, and sparse matrices. High-level programming models supporting dynamic thread creation and multithreading <ref> [17, 4, 26, 7, 20, 12] </ref> enable convenient expression of such computations by relieving the programmer from having to manage details of thread and data structuring. <p> Unfortunately, load-balancing solutions in literature do not do a good job of meeting these requirements: most are either too heavyweight for use with fine-grained threads [12, 17, 23, 21, 7], completely ignore locality <ref> [4, 24] </ref>, or work well only with regular computations [7, 26]. In this paper, we describe a general load-balancing framework for systems with all of the above characteristics. Our framework views the computation as a collection of different thread subsets, each of which can be load-balanced independently. <p> This approach represents a major departure from conventional approaches to load-balancing <ref> [18, 12, 17, 23, 21, 7, 26, 4, 24] </ref> which employ a a single unified solution for application wide load balancing.
Reference: [5] <author> B. </author> <title> Buchberger. Multidimensional Systems Theory, chapter Grobner basis: an algorithmic method in polynomial ideal theory, </title> <booktitle> pages 184232. </booktitle> <address> D. </address> <publisher> Reidel Publishing Company, </publisher> <address> Dordrecht, </address> <year> 1985. </year>
Reference-contexts: Consequently, as compared to the Origin, idle time eliminated by the custom policies accounts for a larger fraction of the overall execution time. 4.3 Gr obner The Grobner application computes the Grobner basis <ref> [5] </ref> of a set of polynomials. The algorithm starts off with an initial basis set of polynomials equal to the input set, and evaluates each pair of polynomials in a rank order determined by a heuristic metric.
Reference: [6] <author> S. Chakrabarti and K. Yelick. </author> <title> Implementing an irregular application on a distributed memory multiprocessor. </title> <booktitle> In Proceedings of the Fourth ACM/SIGPLAN Symposium on Principles and Practices of Parallel Programming, </booktitle> <pages> pages 169179, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: APPLICATION DESCRIPTION INPUT IC-Cedar protein dynamics [35] simulates protein macromolecular dynamics. myoglobin Radiosity computer graphics [33] computes illumination using the hierarchical radiosity method. room Grobner computational algebra <ref> [6] </ref> computes Grobner basis. ae-4 Phylogeny evolutionary history [19] determines the evolutionary history of a set of species. prim.40 Table 1: The irregular applications suite. <p> Moreover, each application achieves performance comparable to that obtained using low-level programming approaches <ref> [35, 6, 19, 29] </ref>, attesting to the framework's sufficiency.
Reference: [7] <author> Rohit Chandra, Anoop Gupta, and John L. Hennessy. </author> <title> Data locality and load balancing in COOL. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 249259, </pages> <year> 1993. </year>
Reference-contexts: State-of-the-art techniques for these computations rely on sophisticated adaptive [2] and tree-based algorithms [1, 15], and pointer-based data structures such as linked lists, trees, and sparse matrices. High-level programming models supporting dynamic thread creation and multithreading <ref> [17, 4, 26, 7, 20, 12] </ref> enable convenient expression of such computations by relieving the programmer from having to manage details of thread and data structuring. <p> Unfortunately, load-balancing solutions in literature do not do a good job of meeting these requirements: most are either too heavyweight for use with fine-grained threads <ref> [12, 17, 23, 21, 7] </ref>, completely ignore locality [4, 24], or work well only with regular computations [7, 26]. In this paper, we describe a general load-balancing framework for systems with all of the above characteristics. <p> Unfortunately, load-balancing solutions in literature do not do a good job of meeting these requirements: most are either too heavyweight for use with fine-grained threads [12, 17, 23, 21, 7], completely ignore locality [4, 24], or work well only with regular computations <ref> [7, 26] </ref>. In this paper, we describe a general load-balancing framework for systems with all of the above characteristics. Our framework views the computation as a collection of different thread subsets, each of which can be load-balanced independently. <p> This approach represents a major departure from conventional approaches to load-balancing <ref> [18, 12, 17, 23, 21, 7, 26, 4, 24] </ref> which employ a a single unified solution for application wide load balancing.
Reference: [8] <author> Andrew Chien, Julian Dolby, Bishwaroop Ganguly, Vijay Karamcheti, and Xingbin Zhang. </author> <title> Supporting high level programming with high performance: The Illinois Concert system. </title> <booktitle> In Proceedings of the Second International Workshop on High-level Parallel Programming Models and Supportive Environments, </booktitle> <pages> pages 1524, </pages> <month> April </month> <year> 1997. </year>
Reference-contexts: Run-time support defines a custom-scheduler interface that enables the construction of application-specific load-balancing policies, and integrates them with the rest of the language implementation. This framework has been implemented in the Illinois Concert System <ref> [9, 8] </ref>, an execution platform for concurrent object-oriented languages on scalable parallel platforms. Studies using four large irregular applicationsIC-Cedar, Hierarchical Radiosity, Gr obner, and Phylogenyshow the advantages of using the framework. <p> ICC++ extends the C++ array syntax to support collections, providing predefined methods which allow collections to implement an aggregate behavior and interface. ICC++ programs are executed using the Illinois Concert System <ref> [8, 9] </ref>, a high-performance compiler and run-time system for implementing concurrent object-oriented languages on scalable parallel platforms. The Concert compiler employs global interprocedural analyses and transformations such as method and object inlining, and procedure cloning to successfully eliminate overheads arising from the high level object-oriented nature of the programming model.
Reference: [9] <author> Andrew Chien, Julian Dolby, Bishwaroop Ganguly, Vijay Karamcheti, and Xingbin Zhang. </author> <title> Evaluating high level parallel programming support for irregular applications in ICC++. </title> <journal> SoftwarePractice and Engineering, </journal> <year> 1998. </year>
Reference-contexts: Run-time support defines a custom-scheduler interface that enables the construction of application-specific load-balancing policies, and integrates them with the rest of the language implementation. This framework has been implemented in the Illinois Concert System <ref> [9, 8] </ref>, an execution platform for concurrent object-oriented languages on scalable parallel platforms. Studies using four large irregular applicationsIC-Cedar, Hierarchical Radiosity, Gr obner, and Phylogenyshow the advantages of using the framework. <p> ICC++ extends the C++ array syntax to support collections, providing predefined methods which allow collections to implement an aggregate behavior and interface. ICC++ programs are executed using the Illinois Concert System <ref> [8, 9] </ref>, a high-performance compiler and run-time system for implementing concurrent object-oriented languages on scalable parallel platforms. The Concert compiler employs global interprocedural analyses and transformations such as method and object inlining, and procedure cloning to successfully eliminate overheads arising from the high level object-oriented nature of the programming model.
Reference: [10] <author> Andrew Chien and Uday Reddy. </author> <title> ICC++ language definition. Concurrent Systems Architecture Group Memo, </title> <note> Also available from http://www-csag.cs.uiuc.edu, February 1995. </note>
Reference-contexts: High-level programming models supporting dynamic thread creation and multithreading [17, 4, 26, 7, 20, 12] enable convenient expression of such computations by relieving the programmer from having to manage details of thread and data structuring. These models, exemplified by several concurrent object-oriented <ref> [16, 34, 10] </ref> and message-driven [20] languages, permit application concurrency to be expressed in terms of arbitrary user-defined computation units and simplify implementation of pointer-based data structures by providing a global name space.
Reference: [11] <author> Andrew A. Chien, Uday S. Reddy, John Plevyak, and Julian Dolby. </author> <title> ICC++ a C++ dialect for high-performance parallel computation. </title> <booktitle> In Proceedings of the 2nd International Symposium on Object Technologies for Advanced Software, </booktitle> <pages> pages 7695, </pages> <month> March </month> <year> 1996. </year>
Reference-contexts: Objects reside in a globally shared name space and communicate via method invocations. We consider a specific language, Illinois Concert C++ (ICC++) <ref> [11] </ref> which extends C++ with concurrent statements, concurrent objects, and multiaccess object collections.
Reference: [12] <author> I. Foster, C. Kesselman, and S. Tuecke. </author> <title> The Nexus approach to integrating multithreading and communication. </title> <journal> J. Parallel and Distributed Computing, </journal> <volume> 37(1):7082, </volume> <year> 1996. </year>
Reference-contexts: State-of-the-art techniques for these computations rely on sophisticated adaptive [2] and tree-based algorithms [1, 15], and pointer-based data structures such as linked lists, trees, and sparse matrices. High-level programming models supporting dynamic thread creation and multithreading <ref> [17, 4, 26, 7, 20, 12] </ref> enable convenient expression of such computations by relieving the programmer from having to manage details of thread and data structuring. <p> Unfortunately, load-balancing solutions in literature do not do a good job of meeting these requirements: most are either too heavyweight for use with fine-grained threads <ref> [12, 17, 23, 21, 7] </ref>, completely ignore locality [4, 24], or work well only with regular computations [7, 26]. In this paper, we describe a general load-balancing framework for systems with all of the above characteristics. <p> This approach represents a major departure from conventional approaches to load-balancing <ref> [18, 12, 17, 23, 21, 7, 26, 4, 24] </ref> which employ a a single unified solution for application wide load balancing.
Reference: [13] <author> Pawan Goyal, Xingang Guo, and Harrick M. Vin. </author> <title> A hierarchical CPU scheduler for multimedia operating systems. </title> <booktitle> In Proceedings of the 2nd Symposium on Operating Systems Design and Implementation (OSDI'96), </booktitle> <pages> pages 107121, </pages> <year> 1996. </year>
Reference-contexts: Most similar to our approach are systems that allow different subsets of threads to be subject to different load balancing and scheduling policies during execution. Custom scheduling is a feature of several real-time and multimedia operating systems <ref> [13, 31] </ref>. The primary difference of our approach with respect to real-time and multimedia operating systems is that the integration between different schedulers is considerably more efficient, relying on non-preemptive scheduling of cooperating application threads.
Reference: [14] <author> Ananth Y. Grama, Vipin Kumar, and Ahmed Sameh. </author> <title> Scalable parallel formulations of the Barnes-Hut method for N-body simulations. </title> <booktitle> In Proceedings of Supercomputing Conference, </booktitle> <pages> pages 439448, </pages> <month> November </month> <year> 1994. </year> <month> 17 </month>
Reference-contexts: We specify a distribution of the collection of atom objects which collocates objects that are spatially close (GroupDistribution). The specific distribution we use is based on a Peano-Hilbert encoding [27, 28] traversal of spatial cells, a technique borrowed from astrophysics applications <ref> [14, 32] </ref>. This object-placement policy achieves load balance while reducing data access costs. Performance impact of the framework.
Reference: [15] <author> L. Greengard and V Rokhlin. </author> <title> A fast algorithm for particle simulations. </title> <journal> Journal of Computational Physics, </journal> <volume> 73:32548, </volume> <year> 1987. </year>
Reference-contexts: 1 Introduction Irregular parallel computations are increasingly being encountered in applications such as molecular dynamics, particle simulations, and visualization, spanning a number of engineering and scientific domains. State-of-the-art techniques for these computations rely on sophisticated adaptive [2] and tree-based algorithms <ref> [1, 15] </ref>, and pointer-based data structures such as linked lists, trees, and sparse matrices. High-level programming models supporting dynamic thread creation and multithreading [17, 4, 26, 7, 20, 12] enable convenient expression of such computations by relieving the programmer from having to manage details of thread and data structuring.
Reference: [16] <author> A. Grimshaw. </author> <title> Easy-to-use object-oriented parallel processing with Mentat. </title> <journal> IEEE Computer, </journal> <volume> 5(26):39 51, </volume> <month> May </month> <year> 1993. </year>
Reference-contexts: High-level programming models supporting dynamic thread creation and multithreading [17, 4, 26, 7, 20, 12] enable convenient expression of such computations by relieving the programmer from having to manage details of thread and data structuring. These models, exemplified by several concurrent object-oriented <ref> [16, 34, 10] </ref> and message-driven [20] languages, permit application concurrency to be expressed in terms of arbitrary user-defined computation units and simplify implementation of pointer-based data structures by providing a global name space. <p> The following code fragment shows examples of three object placement and collection distribution policies: 1 Foo *obj1 = new ( LOCAL ) Foo (4); // object placement Foo *obj2 = new ( location of (obj1) ) Foo (5); // object collocation Accum *coll1 = new ( BLOCK ) Accum <ref> [16] </ref>; // collection distribution Thread placement policies directly control the balanced distribution of threads belonging to a particular subset. In ICC++, such policies are specified by annotating thread creation sites using the !scheduler keyword which takes as an argument the specific load-balancing policy.
Reference: [17] <author> Matthew Haines, David Cronk, and Piyush Mehrotra. </author> <title> On the design of Chant: A talking threads package. </title> <booktitle> In Proceedings of Supercomputing'94, </booktitle> <pages> pages 350359, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: State-of-the-art techniques for these computations rely on sophisticated adaptive [2] and tree-based algorithms [1, 15], and pointer-based data structures such as linked lists, trees, and sparse matrices. High-level programming models supporting dynamic thread creation and multithreading <ref> [17, 4, 26, 7, 20, 12] </ref> enable convenient expression of such computations by relieving the programmer from having to manage details of thread and data structuring. <p> Unfortunately, load-balancing solutions in literature do not do a good job of meeting these requirements: most are either too heavyweight for use with fine-grained threads <ref> [12, 17, 23, 21, 7] </ref>, completely ignore locality [4, 24], or work well only with regular computations [7, 26]. In this paper, we describe a general load-balancing framework for systems with all of the above characteristics. <p> This approach represents a major departure from conventional approaches to load-balancing <ref> [18, 12, 17, 23, 21, 7, 26, 4, 24] </ref> which employ a a single unified solution for application wide load balancing.
Reference: [18] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Compiling FORTRAN D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8):6680, </volume> <month> August </month> <year> 1992. </year>
Reference-contexts: This approach represents a major departure from conventional approaches to load-balancing <ref> [18, 12, 17, 23, 21, 7, 26, 4, 24] </ref> which employ a a single unified solution for application wide load balancing.
Reference: [19] <author> Jeff A. Jones. </author> <title> Parallelizing the phylogeny problem. </title> <type> Master's thesis, </type> <institution> University of California, Berkeley, </institution> <address> CA, </address> <month> December </month> <year> 1994. </year>
Reference-contexts: APPLICATION DESCRIPTION INPUT IC-Cedar protein dynamics [35] simulates protein macromolecular dynamics. myoglobin Radiosity computer graphics [33] computes illumination using the hierarchical radiosity method. room Grobner computational algebra [6] computes Grobner basis. ae-4 Phylogeny evolutionary history <ref> [19] </ref> determines the evolutionary history of a set of species. prim.40 Table 1: The irregular applications suite. <p> Moreover, each application achieves performance comparable to that obtained using low-level programming approaches <ref> [35, 6, 19, 29] </ref>, attesting to the framework's sufficiency.
Reference: [20] <author> L. V. Kale and Sanjeev Krishnan. CHARM++: </author> <title> A portable concurrent object oriented system based on C++. </title> <booktitle> In Proceedings of OOPSLA'93, </booktitle> <pages> pages 91108, </pages> <year> 1993. </year>
Reference-contexts: State-of-the-art techniques for these computations rely on sophisticated adaptive [2] and tree-based algorithms [1, 15], and pointer-based data structures such as linked lists, trees, and sparse matrices. High-level programming models supporting dynamic thread creation and multithreading <ref> [17, 4, 26, 7, 20, 12] </ref> enable convenient expression of such computations by relieving the programmer from having to manage details of thread and data structuring. <p> High-level programming models supporting dynamic thread creation and multithreading [17, 4, 26, 7, 20, 12] enable convenient expression of such computations by relieving the programmer from having to manage details of thread and data structuring. These models, exemplified by several concurrent object-oriented [16, 34, 10] and message-driven <ref> [20] </ref> languages, permit application concurrency to be expressed in terms of arbitrary user-defined computation units and simplify implementation of pointer-based data structures by providing a global name space.
Reference: [21] <author> Laxmikant V. Kale, Milind Bhandarkar, Narain Jagathesan, Sanjeev Krishnan, and Joshua M. Yelon. </author> <title> Converse: an interoperable framework for parallel programming. </title> <booktitle> In Proceedings of the International Parallel Processing Symposium, </booktitle> <pages> pages 212217, </pages> <year> 1996. </year> <note> Available from http://charm.cs.uiuc.edu/ papers/converse-ipps96.ps. </note>
Reference-contexts: Unfortunately, load-balancing solutions in literature do not do a good job of meeting these requirements: most are either too heavyweight for use with fine-grained threads <ref> [12, 17, 23, 21, 7] </ref>, completely ignore locality [4, 24], or work well only with regular computations [7, 26]. In this paper, we describe a general load-balancing framework for systems with all of the above characteristics. <p> This approach represents a major departure from conventional approaches to load-balancing <ref> [18, 12, 17, 23, 21, 7, 26, 4, 24] </ref> which employ a a single unified solution for application wide load balancing. <p> The primary difference of our approach with respect to real-time and multimedia operating systems is that the integration between different schedulers is considerably more efficient, relying on non-preemptive scheduling of cooperating application threads. On the other hand, only a handful of programming systems, such as Presto [3] and Converse <ref> [21] </ref>, possess similar capabilities. While support for attaching custom schedulers in both Presto and Converse share similar objectives to our approach, there are important differences. First, the thread scheduler interface in both systems is more suitable for medium-grained threads, requiring multiple indirect function calls per scheduling operation.
Reference: [22] <author> Vijay Karamcheti. </author> <title> Run-Time Techniques for Dynamic Multithreaded Computations. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, Urbana, Illinois, </institution> <year> 1998. </year>
Reference-contexts: Third, despite minor differences, the policies employed by each of the application programs are generic enough to be supplied by the run-time system as part of a library of load-balancing policies. In related work reported elsewhere <ref> [22] </ref>, we show how once a library of policies is identified, a synthetic task-generation microkernel can be used to provide qualitative guidance to the programmer about which policy is suitable for a particular region of the application space.
Reference: [23] <author> R. Keller and F. C. H. Lin. </author> <title> Simulated performance of a reduction based multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 17(7), </volume> <month> July </month> <year> 1984. </year>
Reference-contexts: Unfortunately, load-balancing solutions in literature do not do a good job of meeting these requirements: most are either too heavyweight for use with fine-grained threads <ref> [12, 17, 23, 21, 7] </ref>, completely ignore locality [4, 24], or work well only with regular computations [7, 26]. In this paper, we describe a general load-balancing framework for systems with all of the above characteristics. <p> This approach represents a major departure from conventional approaches to load-balancing <ref> [18, 12, 17, 23, 21, 7, 26, 4, 24] </ref> which employ a a single unified solution for application wide load balancing.
Reference: [24] <author> E. Mohr, D. Kranz, and R. Halstead Jr. </author> <title> Lazy task creation: A technique for increasing the granularity of parallel programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3):264280, </volume> <month> July </month> <year> 1991. </year>
Reference-contexts: Unfortunately, load-balancing solutions in literature do not do a good job of meeting these requirements: most are either too heavyweight for use with fine-grained threads [12, 17, 23, 21, 7], completely ignore locality <ref> [4, 24] </ref>, or work well only with regular computations [7, 26]. In this paper, we describe a general load-balancing framework for systems with all of the above characteristics. Our framework views the computation as a collection of different thread subsets, each of which can be load-balanced independently. <p> This approach represents a major departure from conventional approaches to load-balancing <ref> [18, 12, 17, 23, 21, 7, 26, 4, 24] </ref> which employ a a single unified solution for application wide load balancing.
Reference: [25] <author> Scott Pakin, Vijay Karamcheti, and Andrew A. Chien. </author> <title> Fast Messages: Efficient, portable communication for workstation clusters and mpps. </title> <journal> IEEE Concurrency, </journal> <note> 5(2):6073, April-June 1997. Available from http://www-csag.cs.uiuc.edu/ papers/ fm-pdt.ps. </note>
Reference-contexts: Scheduler counterparts communicate using an active messages paradigm [30], which combines a message send with the invocation of a user-handler at the destination. The Concert run-time system is built on top of the Illinois Fast Messages communication library <ref> [25] </ref>: communication operations in Figure 3 are identified by the FM * prefix. The steal request handler receives steal requests, and either responds to them with the contents of a task, or forwards the request to the next processor. The steal response handler enqueues the task into the local queue.
Reference: [26] <author> A. Rogers, M. Carlisle, J. Reppy, and L. Hendren. </author> <title> Supporting dynamic data structures on distributed memory machines. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 17(2):233263, </volume> <month> March </month> <year> 1995. </year>
Reference-contexts: State-of-the-art techniques for these computations rely on sophisticated adaptive [2] and tree-based algorithms [1, 15], and pointer-based data structures such as linked lists, trees, and sparse matrices. High-level programming models supporting dynamic thread creation and multithreading <ref> [17, 4, 26, 7, 20, 12] </ref> enable convenient expression of such computations by relieving the programmer from having to manage details of thread and data structuring. <p> Unfortunately, load-balancing solutions in literature do not do a good job of meeting these requirements: most are either too heavyweight for use with fine-grained threads [12, 17, 23, 21, 7], completely ignore locality [4, 24], or work well only with regular computations <ref> [7, 26] </ref>. In this paper, we describe a general load-balancing framework for systems with all of the above characteristics. Our framework views the computation as a collection of different thread subsets, each of which can be load-balanced independently. <p> This approach represents a major departure from conventional approaches to load-balancing <ref> [18, 12, 17, 23, 21, 7, 26, 4, 24] </ref> which employ a a single unified solution for application wide load balancing.
Reference: [27] <author> Hanan Samet. </author> <title> The Design and Analysis of Spatial Data Structures. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1990. </year>
Reference-contexts: We specify a distribution of the collection of atom objects which collocates objects that are spatially close (GroupDistribution). The specific distribution we use is based on a Peano-Hilbert encoding <ref> [27, 28] </ref> traversal of spatial cells, a technique borrowed from astrophysics applications [14, 32]. This object-placement policy achieves load balance while reducing data access costs. Performance impact of the framework.
Reference: [28] <author> Jaswinder Pal Singh. </author> <title> Parallel Hierarchical N-Body Methods and Their Implications For Multiprocessors. </title> <type> PhD thesis, </type> <institution> Stanford University, Stanford, </institution> <address> CA, </address> <month> February </month> <year> 1993. </year>
Reference-contexts: We specify a distribution of the collection of atom objects which collocates objects that are spatially close (GroupDistribution). The specific distribution we use is based on a Peano-Hilbert encoding <ref> [27, 28] </ref> traversal of spatial cells, a technique borrowed from astrophysics applications [14, 32]. This object-placement policy achieves load balance while reducing data access costs. Performance impact of the framework.
Reference: [29] <author> Jaswinder Pal Singh, Anoop Gupta, and Marc Levoy. </author> <title> Parallel visualization algorithms: Performance and architectural implications. </title> <journal> IEEE Computer, </journal> <volume> 27(7):4556, </volume> <month> July </month> <year> 1994. </year>
Reference-contexts: Moreover, each application achieves performance comparable to that obtained using low-level programming approaches <ref> [35, 6, 19, 29] </ref>, attesting to the framework's sufficiency.
Reference: [30] <author> T. von Eicken, D. Culler, S. Goldstein, and K. Schauser. </author> <title> Active Messages: a mechanism for integrated communication and computation. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <pages> pages 256266, </pages> <year> 1992. </year> <month> 18 </month>
Reference-contexts: The helper functions initialize the scheduler structure and serve as handlers for messages requesting additional work, and for messages containing the response (task contents). Scheduler counterparts communicate using an active messages paradigm <ref> [30] </ref>, which combines a message send with the invocation of a user-handler at the destination. The Concert run-time system is built on top of the Illinois Fast Messages communication library [25]: communication operations in Figure 3 are identified by the FM * prefix.
Reference: [31] <author> C. Waldspurger and W. Weihl. </author> <title> Lottery scheduling: Flexible, proportional share resource management. </title> <booktitle> In Proceedings of the Symposium on Operating Systems Design and Implementation, </booktitle> <year> 1994. </year>
Reference-contexts: Most similar to our approach are systems that allow different subsets of threads to be subject to different load balancing and scheduling policies during execution. Custom scheduling is a feature of several real-time and multimedia operating systems <ref> [13, 31] </ref>. The primary difference of our approach with respect to real-time and multimedia operating systems is that the integration between different schedulers is considerably more efficient, relying on non-preemptive scheduling of cooperating application threads.
Reference: [32] <author> M. Warren and J. Salmon. </author> <title> A parallel hashed oct-tree N-body algorithm. </title> <booktitle> In Proceedings of Supercomputing Conference, </booktitle> <pages> pages 1221, </pages> <year> 1993. </year>
Reference-contexts: We specify a distribution of the collection of atom objects which collocates objects that are spatially close (GroupDistribution). The specific distribution we use is based on a Peano-Hilbert encoding [27, 28] traversal of spatial cells, a technique borrowed from astrophysics applications <ref> [14, 32] </ref>. This object-placement policy achieves load balance while reducing data access costs. Performance impact of the framework.
Reference: [33] <author> Steven Cameron Woo, Moriyoshi Ohara, Evan Torrie, Jaswinder Pal Singh, and Anoop Gupta. </author> <title> The SPLASH-2 programs: Characterization and methodological considerations. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <pages> pages 2436, </pages> <year> 1995. </year>
Reference-contexts: APPLICATION DESCRIPTION INPUT IC-Cedar protein dynamics [35] simulates protein macromolecular dynamics. myoglobin Radiosity computer graphics <ref> [33] </ref> computes illumination using the hierarchical radiosity method. room Grobner computational algebra [6] computes Grobner basis. ae-4 Phylogeny evolutionary history [19] determines the evolutionary history of a set of species. prim.40 Table 1: The irregular applications suite.
Reference: [34] <editor> Akinori Yonezawa, editor. </editor> <title> ABCL: An Object-Oriented Concurrent System. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year> <note> ISBN 0-262-24029-7. </note>
Reference-contexts: High-level programming models supporting dynamic thread creation and multithreading [17, 4, 26, 7, 20, 12] enable convenient expression of such computations by relieving the programmer from having to manage details of thread and data structuring. These models, exemplified by several concurrent object-oriented <ref> [16, 34, 10] </ref> and message-driven [20] languages, permit application concurrency to be expressed in terms of arbitrary user-defined computation units and simplify implementation of pointer-based data structures by providing a global name space.
Reference: [35] <author> Xingbin Zhang, Vijay Karamcheti, Tony Ng, and Andrew Chien. </author> <title> Optimizing COOP languages: Study of a protein dynamics program. </title> <booktitle> In IPPS'96, </booktitle> <pages> pages 235240, </pages> <year> 1996. </year> <month> 19 </month>
Reference-contexts: APPLICATION DESCRIPTION INPUT IC-Cedar protein dynamics <ref> [35] </ref> simulates protein macromolecular dynamics. myoglobin Radiosity computer graphics [33] computes illumination using the hierarchical radiosity method. room Grobner computational algebra [6] computes Grobner basis. ae-4 Phylogeny evolutionary history [19] determines the evolutionary history of a set of species. prim.40 Table 1: The irregular applications suite. <p> Moreover, each application achieves performance comparable to that obtained using low-level programming approaches <ref> [35, 6, 19, 29] </ref>, attesting to the framework's sufficiency.
References-found: 35

