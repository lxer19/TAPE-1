URL: http://www.wi.leidenuniv.nl/home/joost/pact97.ps.gz
Refering-URL: http://www.wi.leidenuniv.nl/home/joost/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: joost@cs.leidenuniv.nl  elena@cwi.nl  max@hilbert.math.unipd.it  crossi@moo.dsi.unive.it  
Phone: 2  3  4  
Title: Evolutionary Training of CLP-Constrained Neural Networks  
Author: Joost N. Kok Elena Marchiori ; Massimo Marchiori Claudio Rossi 
Address: P.O. Box 9512, 2300 RA Leiden, The Netherlands  P.O. Box 94079, 1090 GB Amsterdam, The Netherlands  via Belzoni 7, 35131 Padova, Italy  via Torino 155, 30173 Mestre-Venezia, Italy  
Affiliation: 1 Dept. of Computer Science, University of Leiden  CWI  Dept. of Pure and Applied Mathematics, University of Padova  Dip. di Matematica Applicata e Informatica, Universita di Ca' Foscari  
Abstract: The paper is concerned with the integration of constraint logic programming systems (CLP) with systems based on genetic algorithms (GA). The resulting framework is tailored for applications that require a first phase in which a number of constraints need to be generated, and a second phase in which an optimal solution satisfying these constraints is produced. The first phase is carried by the CLP and the second one by the GA. We present a specific framework where ECL i PS e (ECRC Common Logic Programming System) and GENOCOP (GEnetic algorithm for Numerical Optimization for COnstrained Problems) are integrated in a framework called CoCo (COmputational intelligence plus COnstraint logic programming). The CoCo system is applied to the training problem for neural networks. We consider constrained networks, e.g. neural networks with shared weights, constraints on the weights for example domain constraints for hardware implementation etc. Then ECL i PS e is used to generate the chromosome representation together with other constraints which ensure, in most cases, that each network is specified by exactly one chromosome. Thus the problem becomes a constrained optimization problem, where the optimization criterion is to optimize the error of the network, and GENOCOP is used to find an optimal solution. Note: The work of the second author was partially supported by SION, a department of the NWO, the National Foundation for Scientific Research. This work has been carried out while the third author was visiting CWI, Amsterdam, and the fourth author was visiting Leiden University. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Branke. </author> <title> Evolutionary algorithms for neural network design and training. </title> <booktitle> In Proc. of the 1st Nordic Workshop on Genetic Algorithms and its Applications. </booktitle> <address> Vaasa, Finland, </address> <year> 1995. </year>
Reference-contexts: As mentioned in the introduction, this problem consists in the fact that many structurally different networks can represent the same functional mapping (see [17, 4]). This lack of a unique representation, as well known, leads to serious drawbacks with evolutionary training algorithms (see e.g. <ref> [1] </ref>). <p> Abstractly, we can see the competing conventions problem as the existence of some transformations that take a network and give a structurally different but functionally equivalent one (ccp-transformations for short). There are two main classes of ccp-transformations (see <ref> [17, 4, 1, 10] </ref>). The first class is the permutation of k nodes belonging to the same hidden layer. This affects the search space with a maximal total complexity of n! (n is the number of hidden neurons).
Reference: [2] <author> A. Chamard, A. Fischler, D. Guinaudeau, and A. Guillaud. </author> <title> Chic lessons on CLP methodology. </title> <type> Technical report, </type> <institution> ECRC, </institution> <year> 1995. </year> <note> Available via ftp at ftp.ecrc.de/pub/ECRC tech reports/reports/. </note>
Reference-contexts: We discuss in the follow some related work that integrates constraint logic programming and computational intelligence. We are aware of three related papers. A study on the possibility of integrating novel search techniques like genetic algorithms in CLP was done in the CHIC Esprit project <ref> [2] </ref>. In [11], the author has considered the effect of the combination of local search, genetic algorithms and finite domain solver. In particular, constrained genetic algorithms are proposed, where the creation of a new chromosome is supervised by a constraint solver.
Reference: [3] <author> R.A. Fisher. </author> <title> The use of multiple measurements in taxonomic problems. </title> <booktitle> Annual Eugenics, </booktitle> <address> 7(II):179-188, </address> <year> 1936. </year>
Reference-contexts: We did two series of experiments: one series of experiments on a standard dataset about Iris flowers <ref> [3] </ref>, and a second series of experiments with a real-life data set that is used to assess how the air pollution affects on children's Peak Expiratory Flow. We did runs with and without the "competing conventions" constraints, put constraints on the weights, and used several non-differentiable error criteria.
Reference: [4] <author> P.J.B. Hancock. </author> <title> Coding Strategies for Genetic Algorithms and Neural Nets. </title> <type> PhD thesis, </type> <institution> Dept. of Computing Science and Mathematics, University of Stirling, </institution> <year> 1992. </year>
Reference-contexts: Thus they do not use gradient information, and do not require restrictions on the network topology. However, the drawback of genetic algorithms is that they seem to have difficulties in the fine tuning of the parameters [9]. Moreover, there is the "competing conventions problem" <ref> [4, 17] </ref>: when one chooses a representation (in this case for a neural network), then it can be the case that the same individual has more than one representation. This enlarges (in an artificial way) the search space and affects the convergence speed of the algorithms. <p> So, for instance, the string <ref> [5; 4; 7] </ref> represents a network with 5 nodes in its first layer, 4 in the second and 7 in the third. Note that this compact representation of the network graph is possible since we employ layered feedforward neural networks. <p> As mentioned in the introduction, this problem consists in the fact that many structurally different networks can represent the same functional mapping (see <ref> [17, 4] </ref>). This lack of a unique representation, as well known, leads to serious drawbacks with evolutionary training algorithms (see e.g. [1]). <p> Abstractly, we can see the competing conventions problem as the existence of some transformations that take a network and give a structurally different but functionally equivalent one (ccp-transformations for short). There are two main classes of ccp-transformations (see <ref> [17, 4, 1, 10] </ref>). The first class is the permutation of k nodes belonging to the same hidden layer. This affects the search space with a maximal total complexity of n! (n is the number of hidden neurons).
Reference: [5] <author> S. Haykin. </author> <title> Neural Networks, A Comprehensive Foundation. </title> <publisher> Macmillan, </publisher> <year> 1994. </year>
Reference-contexts: We have applied the CoCo system to solve the training problem for constrained neural networks. Neural networks have been used in many applications, for example in planning, control, content-addressable memory, optimization, constraint satisfaction, and classification (see e.g. <ref> [5] </ref>). They are being promoted for their robustness, massive parallelism, and ability to learn. <p> In this section we apply CoCo to a specific problem, namely the training of a constrained neural network. We consider feedforward neural networks (FFNN) (cf. <ref> [5] </ref>), where there are only connections from nodes of one layer to nodes of the successive layer. For the generalization of the results to recurrent neural networks, the reader is referred to [10]. The program is composed by several modules. <p> So, for instance, the string <ref> [5; 4; 7] </ref> represents a network with 5 nodes in its first layer, 4 in the second and 7 in the third. Note that this compact representation of the network graph is possible since we employ layered feedforward neural networks.
Reference: [6] <author> J. Hertz, A. Krogh, and R.G. Palmer. </author> <title> Introduction to the Theory of Neural Computation. </title> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference-contexts: For this kind of neural networks the standard back-propagation learning rule is not in general applicable. Nevertheless, this type of networks can be very useful in applications (for example in applications based on time sequences <ref> [6] </ref>). Genetic algorithms usually avoid local minima by searching several regions simultaneously. They act on a population of trial solutions, and use information on some performance value describing the `quality' of a set of weights.
Reference: [7] <author> A. Ruiz-Andino Illera and J.J. Ruz Ortiz. </author> <title> Labelling in CLP(FD) with evolutionary programming. </title> <editor> In M. Alpuente and M.I. Sessa, editors, </editor> <booktitle> Proc. of the GULP-PRODE'95 Joint Conference on Declarative Programming, </booktitle> <pages> pages 569-580. </pages> <publisher> Poligraph Press, </publisher> <year> 1995. </year>
Reference-contexts: So, for instance, the string <ref> [5; 4; 7] </ref> represents a network with 5 nodes in its first layer, 4 in the second and 7 in the third. Note that this compact representation of the network graph is possible since we employ layered feedforward neural networks. <p> This approach is orthogonal to our, because there CLP is used inside the genetic algorithm, while we use GENOCOP to deal also with constraints, and use a CLP system to produce these constraints. Another paper on the integration of CLP and GA is <ref> [7] </ref>. Their approach is a kind of dual of the one in [11], since they use a genetic algorithm for labeling in CLP over finite integer domain.
Reference: [8] <author> J. Jaffar and M.J. Maher. </author> <title> Constraint logic programming: A survey. </title> <journal> J. Logic Programming, </journal> <volume> 19,20:503-581, </volume> <year> 1994. </year>
Reference-contexts: Examples of such applications include design of VLSDC, finance, medical engineering and many others (cf. e.g. <ref> [8] </ref>). These applications have been tackled using different methods, for instance approximation methods, like genetic algorithms and neural networks, or exact methods like those of operational research or constraint logic programming. Recent works have shown the advantages of an integrated use of exact and approximation methods.
Reference: [9] <author> H. Kitano. </author> <title> Empirical studies on the speed of convergence of neural network training using genetic algorithms. </title> <booktitle> In Proc. AAAI, </booktitle> <pages> pages 789-795, </pages> <year> 1990. </year>
Reference-contexts: Thus they do not use gradient information, and do not require restrictions on the network topology. However, the drawback of genetic algorithms is that they seem to have difficulties in the fine tuning of the parameters <ref> [9] </ref>. Moreover, there is the "competing conventions problem" [4, 17]: when one chooses a representation (in this case for a neural network), then it can be the case that the same individual has more than one representation.
Reference: [10] <author> J.N. Kok, E. Marchiori, M. Marchiori, and C. Rossi. </author> <title> Constraining of weights using regularities. </title> <editor> In M. Verleysen, editor, </editor> <booktitle> Proc. of the 4th European Symposium on Artificial Neural Networks. </booktitle> <address> D facto, </address> <year> 1996. </year> <note> To appear. </note>
Reference-contexts: We consider feedforward neural networks (FFNN) (cf. [5]), where there are only connections from nodes of one layer to nodes of the successive layer. For the generalization of the results to recurrent neural networks, the reader is referred to <ref> [10] </ref>. The program is composed by several modules. At top level, there are three main modules (see Figure 2): 1. The first Start Module asks the user the representation of the FFNN, and builds up the corresponding data that will be used in the sequel. 2. <p> Abstractly, we can see the competing conventions problem as the existence of some transformations that take a network and give a structurally different but functionally equivalent one (ccp-transformations for short). There are two main classes of ccp-transformations (see <ref> [17, 4, 1, 10] </ref>). The first class is the permutation of k nodes belonging to the same hidden layer. This affects the search space with a maximal total complexity of n! (n is the number of hidden neurons).
Reference: [11] <author> V. Kuchenhoff. </author> <title> Novel search and constraints an integration. </title> <type> Technical Report IR-LP-92-16i, </type> <institution> ECRC, </institution> <year> 1992. </year>
Reference-contexts: We discuss in the follow some related work that integrates constraint logic programming and computational intelligence. We are aware of three related papers. A study on the possibility of integrating novel search techniques like genetic algorithms in CLP was done in the CHIC Esprit project [2]. In <ref> [11] </ref>, the author has considered the effect of the combination of local search, genetic algorithms and finite domain solver. In particular, constrained genetic algorithms are proposed, where the creation of a new chromosome is supervised by a constraint solver. <p> Another paper on the integration of CLP and GA is [7]. Their approach is a kind of dual of the one in <ref> [11] </ref>, since they use a genetic algorithm for labeling in CLP over finite integer domain. They design a genetic algorithm where variables are labeled with an integer domain, hence a chromosome encodes an area of the search space, which can contain none or many solutions.
Reference: [12] <author> J.H.M. Lee and V.W.L. Tam. </author> <title> Towards the integration of artificial neural networks and constraint logic programming. </title> <type> Technical Report CS-TR-94-14, </type> <institution> The Chinese University of Hong Kong, </institution> <year> 1994. </year>
Reference-contexts: Recent works have shown the advantages of an integrated use of exact and approximation methods. For instance, constraint logic programming and neural networks have been integrated in a system called PROCLANN <ref> [12] </ref>, which exploits the elegant formalism of CLP and the efficiency of constraint solvers based on stochastic systems, like GENET [18]. <p> Moreover, they report the lack of a self adaptive parameter tuning feature, which is essential to achieve a self contained optimization predicate for constraint logic programming over finite integer domains. Finally, in <ref> [12] </ref> a programming language called PROCLANN that integrates CLP and neural networks is proposed. Their language is a committed-choice logic programming language with a stochastic constraint solver.
Reference: [13] <author> E. Marchiori, M. Marchiori, and J.N. Kok. </author> <title> From failure to success with forward-tracking. </title> <note> Submitted, </note> <year> 1996. </year>
Reference-contexts: The reader is referred to <ref> [13] </ref> for a formal definition and operational semantics of this mechanism. Finally, let us spend some words on the way each of the two available constraining sub-modules is implemented.
Reference: [14] <author> Z. Michalewicz. </author> <title> Genetic Algorithms + Data Structures = Evolution Programs. </title> <publisher> Springer, </publisher> <year> 1994. </year>
Reference-contexts: The first phase is carried by a suitable CLP and the second one by a suitable GA. In order to test the adequacy of such an integration, we have combined ECL i PS e (ECRC Common Logic Programming System) and GENOCOP <ref> [14] </ref> (GEnetic algorithm for Numerical Optimization for COnstrained Problems) in a framework called CoCo (COmputational intelligence plus COnstraint logic programming). We have applied the CoCo system to solve the training problem for constrained neural networks. <p> We describe briefly the two systems. The reader is referred to the rich documentation on ECL i PS e available from ECRC at the ftp address ftp.ecrc.de/pub/ECRC tech reports/reports, and to the book by Michalewicz <ref> [14] </ref> for GENOCOP. ECL i PS e (ECRC Common Logic Programming System) is a Prolog based system built to facilitate the integration of various programming extensions.
Reference: [15] <author> D.E. Rumelhart, G.E. Hinton, and R.J. Williams. </author> <title> Learning representations by back-propagating errors. </title> <journal> Nature, </journal> <volume> 323 </volume> <pages> 533-536, </pages> <year> 1986. </year>
Reference-contexts: The standard method for that problem is a local gradient search method known as the back-propagation algorithm <ref> [15] </ref>. Since the problem's error surface is highly dimensional and usually it contains many local minima, this method can get stuck in local minima. Moreover, it needs gradient information.
Reference: [16] <author> H.A. Simon. </author> <title> Search and reasoning in artificial intelligence. </title> <journal> Artificial Intelligence, </journal> <volume> 21 </volume> <pages> 7-29, </pages> <year> 1983. </year>
Reference-contexts: 1 Introduction There are two major paradigms for problem solving that play a distinguished role in Computing Science and Artificial Intelligence: Reasoning and Search <ref> [16] </ref>. Although every automated reasoning process involves search aspects (e.g. how to traverse the derivation tree of a logic program), the two paradigms and the corresponding research communities are rather disjoint.
Reference: [17] <author> D. Thierens, J. Suykens, J. Vanderwalle, and B. De Moor. </author> <title> Genetic weight optimization of a feedforward neural network controller. </title> <booktitle> In Proc. of the Conference on Artificial Neural Nets and Genetic Algorithms, </booktitle> <pages> pages 658-663. </pages> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: Thus they do not use gradient information, and do not require restrictions on the network topology. However, the drawback of genetic algorithms is that they seem to have difficulties in the fine tuning of the parameters [9]. Moreover, there is the "competing conventions problem" <ref> [4, 17] </ref>: when one chooses a representation (in this case for a neural network), then it can be the case that the same individual has more than one representation. This enlarges (in an artificial way) the search space and affects the convergence speed of the algorithms. <p> As mentioned in the introduction, this problem consists in the fact that many structurally different networks can represent the same functional mapping (see <ref> [17, 4] </ref>). This lack of a unique representation, as well known, leads to serious drawbacks with evolutionary training algorithms (see e.g. [1]). <p> Abstractly, we can see the competing conventions problem as the existence of some transformations that take a network and give a structurally different but functionally equivalent one (ccp-transformations for short). There are two main classes of ccp-transformations (see <ref> [17, 4, 1, 10] </ref>). The first class is the permutation of k nodes belonging to the same hidden layer. This affects the search space with a maximal total complexity of n! (n is the number of hidden neurons). <p> This is in general a creative task: in the field of evolutionary training, there are some studies that introduced some suitable effective representation for this application (e.g. <ref> [17, 19] </ref>), but of course it is still questionable what representation is the best one.
Reference: [18] <author> E.P.K. Tsang. </author> <title> Foundations of Constraint Satisfaction. </title> <publisher> Academic Press, </publisher> <year> 1993. </year>
Reference-contexts: For instance, constraint logic programming and neural networks have been integrated in a system called PROCLANN [12], which exploits the elegant formalism of CLP and the efficiency of constraint solvers based on stochastic systems, like GENET <ref> [18] </ref>. The specific integration we consider for our application, called CoCo (COmputational Intelligence plus COnstraint Logic Programming), employs the CLP system ECL i PS e for generating constraints, and the GA system GENOCOP for the optimization phase. We describe briefly the two systems. <p> Finally, in [12] a programming language called PROCLANN that integrates CLP and neural networks is proposed. Their language is a committed-choice logic programming language with a stochastic constraint solver. The GENET system <ref> [18] </ref> is used as constraint solver, which translates dynamically a binary constraint problem over finite domain into a suitable neural network, and simulates the network convergence procedure. 6 Conclusion This paper advocates the integration of constraint logic programming systems (CLP) and evolutionary systems (ES), for applications that require a first phase
Reference: [19] <author> B. Yoon, D.J. Holmes, G. Langholz, and A. Kandel. </author> <title> Efficient genetic algorithms for training layered feedforward neural networks. </title> <journal> Information Sciences, </journal> <volume> 76 </volume> <pages> 67-85, </pages> <year> 1994. </year>
Reference-contexts: This is in general a creative task: in the field of evolutionary training, there are some studies that introduced some suitable effective representation for this application (e.g. <ref> [17, 19] </ref>), but of course it is still questionable what representation is the best one.
References-found: 19

