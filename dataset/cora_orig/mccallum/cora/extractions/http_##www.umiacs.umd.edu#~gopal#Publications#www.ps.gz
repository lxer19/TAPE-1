URL: http://www.umiacs.umd.edu/~gopal/Publications/www.ps.gz
Refering-URL: http://www.umiacs.umd.edu/users/gopal/professional.html
Root-URL: 
Title: Multiresolution Gauss Markov Random Field Models for Texture Segmentation  
Author: Santhana Krishnamachari and Rama Chellappa 
Note: Supported in part by Grant #ASC 9318183 from National Science Foundation.  
Address: Clarksburg, MD 20871  College Park, MD 20742  
Affiliation: Image Processing Department Communication Technology Division COMSAT Laboratories  Department of Electrical Engineering and Center for Automation Research University of Maryland  
Abstract: This paper presents multiresolution models for Gauss Markov random fields (GMRF) with applications to texture segmentation. Coarser resolution sample fields are obtained by subsampling the sample field at the fine resolution. Although the Markov property is lost under such resolution transformation, coarse resolution non-Markov random fields can be effectively approximated by Markov fields. We present two techniques to estimate the GMRF parameters at coarser resolutions from the fine resolution parameters, one by minimizing the Kullback-Leibler distance and another based on local conditional distribution invariance. We also allude to the fact that different GMRF parameters at the fine resolution can result in the same probability measure after subsampling and present the results for first and second order cases. We apply this multiresolution model to texture segmentation. Different texture regions in an image are modeled by GMRFs and the associated parameters are assumed to be known. Parameters at lower resolutions are estimated from the fine resolution parameters. The coarsest resolution data is first segmented and the segmentation results are propagated upwards to the finer resolution. We use the iterated conditional mode (ICM) minimization at all resolutions. A confidence measure is attached to the segmentation result at each pixel and passed on to the higher resolution. At each resolution, ICM is restricted only to pixels 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Besag, </author> <title> "On the Statistical Analysis of Dirty Pictures," </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> Vol. 48, </volume> <pages> pp. 259-302, </pages> <year> 1986. </year>
Reference-contexts: We also show that the computations for these two estimators turn out to be similar to the traditional maximum likelihood [25], [19], [30] and the pseudo likelihood estimators <ref> [1] </ref>, except that the sample covariances are replaced by covariances calculated with respect to the exact non-Markov measure that is being approximated. We present results on the existence of different sets of GMRF parameters at fine resolution that result in statistically identical coarser resolution random fields. <p> We also exemplify the connection between these two estimators and the estimators that are commonly used to estimate the GMRF parameters from a data sample, the maximum likelihood [25], [19], [30] and the pseudo likelihood estimators <ref> [1] </ref>. 3.1 Kullback-Leibler (KL) Distance Minimization In this section, we show that given any pdf p, it is possible to obtain a GMRF approximation of p by minimizing the KL distance D (p k q) [18], where q belongs to the family of GMRF pdfs. <p> However, the joint densities p (x and q (x ) on the whole lattice are not the same, p (x ) is a non-Markov density and q (x ) is a Markov density. 2. It is worth observing that Eq. (13) is similar to the pseudo likelihood estimate [4], <ref> [1] </ref> where the GMRF parameters are obtained by minimizing the products of local conditional densities over the entire lattice. <p> So, we restrict ourselves to the iterated conditional mode method. The ICM solution is obtained by performing the following optimization at each lattice site <ref> [1] </ref>: max P (X s jL s ; X s+r ; r 2 )P (L s jL s+r ; r 2 ): This is equivalent to, min 1 log ( 2 (v)) + 2 2 (v) X r (v)x s+r ] 2 fiU (L s = v) the minimization is performed
Reference: [2] <author> C. Bouman and B. Liu, </author> <title> "Multiple Resolution Segmentation of Textured Images," </title> <journal> IEEE Trans. Patt. Anal. Mach. Intell., </journal> <volume> Vol. 13, </volume> <pages> pp. 99-113, </pages> <month> Feb. </month> <year> 1991. </year>
Reference-contexts: Multigrid methods proposed by Terzopoulos [31], substantially reduce the computation when applied to some computer vision problems. Chen and Pavlidis [5] use a hierarchical approach to texture segmentation, but do not directly use MRFs. Bouman and Liu <ref> [2] </ref> use a causal autoregressive 2 Gaussian model and a quad-tree structure to perform multiresolution segmentation. Gidas [14] shows some connection between the renormalization group studies in statistical mechanics [10] and the multiresolution MRF framework to process images. <p> Let X (k) represent a random field, obtained by ordering the random variables on (k) . The parameters of a GMRF defined on a lattice (k) are denoted by f (k) ; <ref> [ 2 ] </ref> (k) g and the associated neighborhood is denoted by (k) . The covariance matrix and the power spectrum associated with X (k) are denoted by (k) and S (k) x (!) respectively. <p> But the results can be easily extended to block-to-point type transformation, where the coarse resolution data is obtained by averaging the fine resolution data over a local window. Let X (0) be a GMRF defined on (0) with parameters f (0) ; <ref> [ 2 ] </ref> (0) g and a neighborhood (0) . The power spectrum of X (0) can be written as in Eq. (5) : S (0) [ 2 ] (0) P (0) M r 1 ! 1 + 2 (6) where ! = f (! 1 ; ! 2 ) : <p> Let X (0) be a GMRF defined on (0) with parameters f (0) ; <ref> [ 2 ] </ref> (0) g and a neighborhood (0) . The power spectrum of X (0) can be written as in Eq. (5) : S (0) [ 2 ] (0) P (0) M r 1 ! 1 + 2 (6) where ! = f (! 1 ; ! 2 ) : 0 ! 1 M 1; 0 ! 2 N 1g: The subsampling resolution transfor mation is defined as: X (k) (k1) defined for all s 2 <p> C (r) = s2 and the determinant of the B matrix can be written as [17]: det B = s2 s ): Using the above equations, q (x ) can be written as: q (x ) = s (1 T (2 2 ) 2 expf 1 Let f fl ; <ref> [ 2 ] </ref> fl g be the parameters corresponding to q fl (x ). Now, rewriting Eq. (9) and performing the minimization in terms of the parameters: ( fl ; [ 2 ] fl ) = arg min E p log q (X ) = arg max E p [log q <p> be written as: q (x ) = s (1 T (2 2 ) 2 expf 1 Let f fl ; <ref> [ 2 ] </ref> fl g be the parameters corresponding to q fl (x ). Now, rewriting Eq. (9) and performing the minimization in terms of the parameters: ( fl ; [ 2 ] fl ) = arg min E p log q (X ) = arg max E p [log q (X )] ( ; 2 ) " 2 s s ) 2 (C (0) T C ) # ( ; 2 ) 2 s s ) 2 2 2 (E <p> ) belongs to the family of GMRF densities, q (x s jx s+r ; r 2 ) will be of the form given in Eq. (4). q (x s jx s+r ; r 2 ) = p expf [x s r2 r x s+r ] 2 Let ( fl ; <ref> [ 2 ] </ref> fl ) be the parameters corresponding to q fl (x ). To simplify the notation, let Y the vector containing the neighborhood random variables in a proper order. <p> Y r ] 2 ): It can be seen that the fl parameters corresponding to q fl (x ) are obtained by minimizing the second term in the Eq. (13) fl = arg min X r Y r ] 2 (14) and using the fl obtained, we can estimate the <ref> [ 2 ] </ref> fl that minimizes Eq. (13), [ 2 ] fl = E p [X s r2 r Y r ] 2 : (15) then, fl = arg min fl = [E p (Y Y T )] 1 E p (X s Y ) (16) [ 2 ] fl = <p> seen that the fl parameters corresponding to q fl (x ) are obtained by minimizing the second term in the Eq. (13) fl = arg min X r Y r ] 2 (14) and using the fl obtained, we can estimate the <ref> [ 2 ] </ref> fl that minimizes Eq. (13), [ 2 ] fl = E p [X s r2 r Y r ] 2 : (15) then, fl = arg min fl = [E p (Y Y T )] 1 E p (X s Y ) (16) [ 2 ] fl = E p (X 2 = E p (X <p> we can estimate the <ref> [ 2 ] </ref> fl that minimizes Eq. (13), [ 2 ] fl = E p [X s r2 r Y r ] 2 : (15) then, fl = arg min fl = [E p (Y Y T )] 1 E p (X s Y ) (16) [ 2 ] fl = E p (X 2 = E p (X 2 In addition, the estimated fl parameters should satisfy the positivity conditions in Eq. (2): 1 [ fl ] T Now, returning back to multiresolution discussion, let X (0) be a GMRF defined by ( and X (k) <p> Since p (x ) is Gaussian, p (x s jx s+r ; r 2 ) is also Gaussian with conditional mean P r x s+r (which is the best linear estimate of X s in terms of X s+r ; r 2 ) and conditional variance <ref> [ 2 ] </ref> fl (which is the corresponding minimum mean square error of the estimator) [26]. q fl (x ) being a GMRF with parameters ( fl ; [ 2 ] fl ), from the discussion at the beginning of this section, has the conditional distribution q fl (x s jx <p> s+r (which is the best linear estimate of X s in terms of X s+r ; r 2 ) and conditional variance <ref> [ 2 ] </ref> fl (which is the corresponding minimum mean square error of the estimator) [26]. q fl (x ) being a GMRF with parameters ( fl ; [ 2 ] fl ), from the discussion at the beginning of this section, has the conditional distribution q fl (x s jx s+r ; r 2 ) with the conditional mean P r x s+r and conditional variance [ 2 ] fl . <p> fl (x ) being a GMRF with parameters ( fl ; <ref> [ 2 ] </ref> fl ), from the discussion at the beginning of this section, has the conditional distribution q fl (x s jx s+r ; r 2 ) with the conditional mean P r x s+r and conditional variance [ 2 ] fl . However, the joint densities p (x and q (x ) on the whole lattice are not the same, p (x ) is a non-Markov density and q (x ) is a Markov density. 2. <p> Let X (0) be a second order GMRF with = 0.15g and <ref> [ 2 ] </ref> (0) = 6:0. Let S x (!) be the exact power spectral density function at (k) and let M S x (!; m) be the power spectrum for the m-th order Markov approximation. <p> Therefore, we look at the power spectrum of the subsampled random fields which are simpler functions of the parameters. We show that there exists different sets of GMRF parameters, which on subsampling result in the same pdf at the lower resolution. Since the parameter <ref> [ 2 ] </ref> (0) is a multiplicative factor in the power spectral function, we assume it be equal to one and investigate the existence of different sets of parameters that result in the identical coarser resolution random fields.
Reference: [3] <author> S. Chatterjee and R. Chellappa, </author> <title> "Maximum Likelihood Texture Segmentation Using Gaussian Markov Random Field Models," </title> <booktitle> IEEE International Conf. on Acoustics, Speech, and Signal Processing, </booktitle> <month> June </month> <year> 1985. </year> <month> 24 </month>
Reference-contexts: More complex statistical models using MRFs [33] emerged later and have been used in various image processing problems. Cross and Jain [7] and Chellappa [4] have shown the applicability of GMRF models to synthesize textures. Chellappa and Chatterjee <ref> [3] </ref> have used GMRF models for maximum likelihood segmentation of textures. Geman and Graffigne [13] used a general MRF model for segmentation. They showed that this non-Gaussian MRF model was not very well suited for synthesis, but still performed well for segmentation.
Reference: [4] <author> R. Chellappa, </author> <title> "Two-dimensional Discrete Gaussian Markov Random Field Models for Image Processing," in Progress in Pattern Recognition (L. </title> <editor> N. Kanal and A. Rosenfeld, </editor> <booktitle> eds.), </booktitle> <pages> pp. 79-112, </pages> <address> Elsavier, </address> <year> 1985. </year>
Reference-contexts: Before presenting the details, we will provide a known result regarding the linear estimation of a GMRF. Let Z be a GMRF defined by ( ; 2 ) with a neighborhood . Then the best estimate of Z s based on the elements of is given by <ref> [4] </ref>: ^z s = r2 and the mean square error E (Z s ^ Z s ) 2 = 2 : The conditional density p (z s jz r ; r 2 ) is Gaussian with conditional mean P conditional variance 2 : Let X be a random field with a <p> However, the joint densities p (x and q (x ) on the whole lattice are not the same, p (x ) is a non-Markov density and q (x ) is a Markov density. 2. It is worth observing that Eq. (13) is similar to the pseudo likelihood estimate <ref> [4] </ref>, [1] where the GMRF parameters are obtained by minimizing the products of local conditional densities over the entire lattice. <p> Haralick [15] suggested various local statistical measures based on the gray level dependence, later generalized by Davis, et.al. [8]. More complex statistical models using MRFs [33] emerged later and have been used in various image processing problems. Cross and Jain [7] and Chellappa <ref> [4] </ref> have shown the applicability of GMRF models to synthesize textures. Chellappa and Chatterjee [3] have used GMRF models for maximum likelihood segmentation of textures. Geman and Graffigne [13] used a general MRF model for segmentation. <p> Multiresolution results presented in this section are obtained by performing the algorithm over three resolutions. In all cases, percentages of correct classification are reported. 6.1 Synthetic Image We generated texture images using the technique given in <ref> [4] </ref>.
Reference: [5] <author> P. C. Chen and T. Pavlidis, </author> <title> "Image Segmentation as an Estimation Problem," Computer Vision, </title> <journal> Graphics, and Image Processing, </journal> <volume> Vol. 12, </volume> <pages> pp. 153-173, </pages> <year> 1980. </year>
Reference-contexts: The second approach is to use multiresolution techniques. Two important aspects of mul-tiresolution approaches are: (1) divide and conquer and (2) action at a distance [29]. Multigrid methods proposed by Terzopoulos [31], substantially reduce the computation when applied to some computer vision problems. Chen and Pavlidis <ref> [5] </ref> use a hierarchical approach to texture segmentation, but do not directly use MRFs. Bouman and Liu [2] use a causal autoregressive 2 Gaussian model and a quad-tree structure to perform multiresolution segmentation.
Reference: [6] <author> F. S. Cohen and D. B. Cooper, </author> <title> "Simple Parallel Hierarchical and Relaxation Algorithms for Segmenting Noncausal Markovian Random Fields," </title> <journal> IEEE Trans. Patt. Anal. Mach. Intell., </journal> <volume> Vol. 9, </volume> <pages> pp. 195-219, </pages> <month> March </month> <year> 1987. </year>
Reference-contexts: Perez and Heitz [27] present a multiscale MRF model, where optimization is performed sequentially over specific subsets of original configuration space. The result of optimization at each scale is used to initialize the optimization at the subsequent finer scale. Cohen and Cooper <ref> [6] </ref> present a hierarchical scheme, where the data at lower resolution is divided into blocks and the conditional probabilities of these blocks given the neighboring blocks, are obtained. They use a recursive scheme to calculate the within-block and between-block interaction values.
Reference: [7] <author> G. R. Cross and A. K. Jain, </author> <title> "Markov Random Field Texture Models," </title> <journal> IEEE Trans. Patt. Anal. Mach. Intell., </journal> <volume> Vol. 5, </volume> <pages> pp. 25-39, </pages> <month> Jan </month> <year> 1983. </year>
Reference-contexts: Haralick [15] suggested various local statistical measures based on the gray level dependence, later generalized by Davis, et.al. [8]. More complex statistical models using MRFs [33] emerged later and have been used in various image processing problems. Cross and Jain <ref> [7] </ref> and Chellappa [4] have shown the applicability of GMRF models to synthesize textures. Chellappa and Chatterjee [3] have used GMRF models for maximum likelihood segmentation of textures. Geman and Graffigne [13] used a general MRF model for segmentation.
Reference: [8] <author> L. S. Davis, S. Johns, and J. K. Aggrawal, </author> <title> "Texture analysis using generalized co-occurance matrices," </title> <journal> IEEE Trans. Patt. Anal. Mach. Intell., </journal> <volume> Vol. 1, </volume> <pages> pp. 251-259, </pages> <month> July </month> <year> 1979. </year>
Reference-contexts: Statistical methods are better suited for such micro textures. Most of the early statistical methods use first and second order properties to discriminate between textures. Haralick [15] suggested various local statistical measures based on the gray level dependence, later generalized by Davis, et.al. <ref> [8] </ref>. More complex statistical models using MRFs [33] emerged later and have been used in various image processing problems. Cross and Jain [7] and Chellappa [4] have shown the applicability of GMRF models to synthesize textures. Chellappa and Chatterjee [3] have used GMRF models for maximum likelihood segmentation of textures.
Reference: [9] <author> H. Derin and H. Elliot, </author> <title> "Modeling and Segmentation of Noisy and Textured Images Using Gibbs Random Field," </title> <journal> IEEE Trans. Patt. Anal. Mach. Intell., </journal> <volume> Vol. 9, </volume> <pages> pp. 39-55, </pages> <month> Jan. </month> <year> 1987. </year>
Reference-contexts: Chellappa and Chatterjee [3] have used GMRF models for maximum likelihood segmentation of textures. Geman and Graffigne [13] used a general MRF model for segmentation. They showed that this non-Gaussian MRF model was not very well suited for synthesis, but still performed well for segmentation. Derin and Elliot <ref> [9] </ref> used Gibbs distributions and presented a non-optimal method using dynamic programming. Manjunath et.al. [23] proposed a stochastic learning technique to improve the ICM results and presented a neural network implementation for texture segmentation.
Reference: [10] <author> C. Domb and M. Green, </author> <title> "Phase Transition and Critical Phenomena." </title> <publisher> Academic Press, </publisher> <year> 1979. </year>
Reference-contexts: Chen and Pavlidis [5] use a hierarchical approach to texture segmentation, but do not directly use MRFs. Bouman and Liu [2] use a causal autoregressive 2 Gaussian model and a quad-tree structure to perform multiresolution segmentation. Gidas [14] shows some connection between the renormalization group studies in statistical mechanics <ref> [10] </ref> and the multiresolution MRF framework to process images. A hierarchical image analysis scheme based on renormalization group ideas is presented also by Matsuba in [24]. Perez and Heitz [27] present a multiscale MRF model, where optimization is performed sequentially over specific subsets of original configuration space.
Reference: [11] <author> D. Geiger and F. Girosi, </author> <title> "Parallel and Deterministic Algorithms for MRFs: Surface Reconstruction," </title> <journal> IEEE Trans. Patt. Anal. Mach. Intell., </journal> <volume> Vol. 13, </volume> <pages> pp. 401-413, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: There are two different approaches that have been used to reduce the computational burden. The first is to use non-optimal, deterministic methods that converge to a local optimal point, but still provide reasonably good results. Geiger and Girosi <ref> [11] </ref> and Zhang [35] use mean field approximations that lead to deterministic relaxation algorithms. Wu and Doerschuk [34] use a tree approximation that replaces the lattice on which an MRF is defined by an acyclic tree which allows replacing the iterative MRF computations by recursive computations.
Reference: [12] <author> S. Geman and D. Geman, </author> <title> "Stochastic Relaxation, Gibbs Distribution and the Bayesian Restoration of Images," </title> <journal> IEEE Trans. Patt. Anal. Mach. Intell., </journal> <volume> Vol. 6, </volume> <pages> pp. 721-741, </pages> <month> Nov </month> <year> 1984. </year>
Reference-contexts: Even though the individual iterations involve only simple local computations, the iterative nature of these algorithms contributes to the computational burden. Energy functions associated with most non-trivial MRF problems are extremely non-convex, so conventional gradient based methods are to no avail. Best results are obtained by using simulated annealing <ref> [12] </ref>, which is a stochastic relaxation technique, but it is computationally very taxing. There are two different approaches that have been used to reduce the computational burden. The first is to use non-optimal, deterministic methods that converge to a local optimal point, but still provide reasonably good results.
Reference: [13] <author> S. Geman and C. Graffigne, </author> <title> "Markov Random Field Image Models and Their Application to Computer Vision," </title> <booktitle> Proc. International Congress of Mathematicians, </booktitle> <pages> pp. 1496-1517, </pages> <year> 1986. </year>
Reference-contexts: Cross and Jain [7] and Chellappa [4] have shown the applicability of GMRF models to synthesize textures. Chellappa and Chatterjee [3] have used GMRF models for maximum likelihood segmentation of textures. Geman and Graffigne <ref> [13] </ref> used a general MRF model for segmentation. They showed that this non-Gaussian MRF model was not very well suited for synthesis, but still performed well for segmentation. Derin and Elliot [9] used Gibbs distributions and presented a non-optimal method using dynamic programming.
Reference: [14] <author> B. Gidas, </author> <title> "A Renormalization Group Approach to Image Processing," </title> <journal> IEEE Trans. Patt. Anal. Mach. Intell., </journal> <volume> Vol. 11, No. 2, </volume> <pages> pp. 164-180, </pages> <year> 1989. </year>
Reference-contexts: Chen and Pavlidis [5] use a hierarchical approach to texture segmentation, but do not directly use MRFs. Bouman and Liu [2] use a causal autoregressive 2 Gaussian model and a quad-tree structure to perform multiresolution segmentation. Gidas <ref> [14] </ref> shows some connection between the renormalization group studies in statistical mechanics [10] and the multiresolution MRF framework to process images. A hierarchical image analysis scheme based on renormalization group ideas is presented also by Matsuba in [24].
Reference: [15] <author> R. Haralick, </author> <title> "Statistical and Structural Approaches to Texture," </title> <journal> Proc. IEEE, </journal> <volume> Vol. 67, </volume> <pages> pp. 610-621, </pages> <month> May </month> <year> 1979. </year> <month> 25 </month>
Reference-contexts: Statistical methods are better suited for such micro textures. Most of the early statistical methods use first and second order properties to discriminate between textures. Haralick <ref> [15] </ref> suggested various local statistical measures based on the gray level dependence, later generalized by Davis, et.al. [8]. More complex statistical models using MRFs [33] emerged later and have been used in various image processing problems.
Reference: [16] <author> F. C. Jeng, </author> <title> "Subsampling of Markov Random Fields," </title> <booktitle> Jour. of Visual Communication and Image Representation, </booktitle> <volume> Vol. 3, </volume> <pages> pp. 225-229, </pages> <month> Sep. </month> <year> 1992. </year>
Reference-contexts: Cohen and Cooper [6] present a hierarchical scheme, where the data at lower resolution is divided into blocks and the conditional probabilities of these blocks given the neighboring blocks, are obtained. They use a recursive scheme to calculate the within-block and between-block interaction values. Jeng, in <ref> [16] </ref>, discusses the effect of subsampling resolution transformation on MRFs and presents two results: first, the Markov property is not preserved for a general subsampling scheme and, second, it is preserved under some specific subsampling schemes depending on the size and shape of the neighborhood.
Reference: [17] <author> R. L. Kashyap, </author> <title> "Analysis and Synthesis of Image Patterns by Spatial Interaction Models," in Progress in Pattern Recognition (L. </title> <editor> N. Kanal and A. Rosenfeld, eds.), </editor> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1981. </year>
Reference-contexts: For the first order case, = f (1; 0); (0; 1); (1; 0); (0; 1)g, and s = fs + r : r 2 g. If X is modeled by a GMRF with a symmetric neighborhood , then X can be written as <ref> [17] </ref>: X s = r2 B X = e : For a finite lattice, the neighborhood for the lattice sites at the boundaries of the lattice is not complete. This problem is circumvented by assuming that the lattice is folded into a toroid. <p> The covariance matrix of e is 2 B and it can be shown that the covariance matrix of X = 2 B 1 <ref> [17] </ref>. The joint probability density function of X can be written as follows: p (X = x ) = det B MN expf 1 Also X exhibits the Markov property [17], p (x s jx t ; 8t 6= s; t 2 ) = p (x s jx s+r ; r <p> of e is 2 B and it can be shown that the covariance matrix of X = 2 B 1 <ref> [17] </ref>. The joint probability density function of X can be written as follows: p (X = x ) = det B MN expf 1 Also X exhibits the Markov property [17], p (x s jx t ; 8t 6= s; t 2 ) = p (x s jx s+r ; r 2 ) 1 2 2 expf [x s r2 r x s+r ] 2 (4) The power spectrum S x (!) of X can be shown to be [17]: S <p> property <ref> [17] </ref>, p (x s jx t ; 8t 6= s; t 2 ) = p (x s jx s+r ; r 2 ) 1 2 2 expf [x s r2 r x s+r ] 2 (4) The power spectrum S x (!) of X can be shown to be [17]: S x (!) = 1 r2 r cos [ 2 N r 2 ! 2 ] where ! = f! 1 ; ! 2 g; and 0 ! 1 M 1; 0 ! 2 N 1. 2.2 GMRFs and Resolution Transformation Let (0) = = f (s 1 ; s <p> B x = C (0) T C where C is a vector whose length is equal to the number of elements in the neighborhood and 8 is as follows: C (0) = s2 s ; C (r) = s2 and the determinant of the B matrix can be written as <ref> [17] </ref>: det B = s2 s ): Using the above equations, q (x ) can be written as: q (x ) = s (1 T (2 2 ) 2 expf 1 Let f fl ; [ 2 ] fl g be the parameters corresponding to q fl (x ). <p> X (k) (0) E p (k) [X (k) (k) (0) (0) For any two lattice sites u and v in (0) the correlation is given by <ref> [17] </ref>: E p (0) [X (0) v ] = M N s2 (0) M s 2 u 2 M s 2 v 2 1 [ (0) ] T (18) where i n = exp ( 1 2i Under the assumption that the covariance matrix with respect to p measure is positive
Reference: [18] <author> S. Kullback and R. A. Leibler, </author> <title> "On Information and Sufficiency," </title> <journal> Ann. Math. Stat., </journal> <volume> Vol. 22, </volume> <pages> pp. 79-86, </pages> <year> 1951. </year>
Reference-contexts: from a data sample, the maximum likelihood [25], [19], [30] and the pseudo likelihood estimators [1]. 3.1 Kullback-Leibler (KL) Distance Minimization In this section, we show that given any pdf p, it is possible to obtain a GMRF approximation of p by minimizing the KL distance D (p k q) <ref> [18] </ref>, where q belongs to the family of GMRF pdfs. The KL distance measure is widely used to obtain approximate probability measures with desired properties. The estimator presented in this subsection is same as the estimator originally proposed by Lakshmanan and Derin [20].
Reference: [19] <author> H. Kunsch, </author> <title> "Thermodynamics and Statistical Analysis of Gaussian Random Fields," </title> <journal> Z. Wahr. Ver. </journal> <volume> Geb., </volume> <pages> pp. 407-421, </pages> <month> Nov. </month> <year> 1981. </year>
Reference-contexts: We also show that the computations for these two estimators turn out to be similar to the traditional maximum likelihood [25], <ref> [19] </ref>, [30] and the pseudo likelihood estimators [1], except that the sample covariances are replaced by covariances calculated with respect to the exact non-Markov measure that is being approximated. <p> We also exemplify the connection between these two estimators and the estimators that are commonly used to estimate the GMRF parameters from a data sample, the maximum likelihood [25], <ref> [19] </ref>, [30] and the pseudo likelihood estimators [1]. 3.1 Kullback-Leibler (KL) Distance Minimization In this section, we show that given any pdf p, it is possible to obtain a GMRF approximation of p by minimizing the KL distance D (p k q) [18], where q belongs to the family of GMRF <p> s s ) 2 2 2 (E p [C (0)] T E p [C ]) (10) where, E p [C (r)] = E p [ s2 = (M N )E p [X s X s+r ]: (11) Remark: Observe that Eq. (10) is similar to the maximum likelihood expression [25], <ref> [19] </ref>, [30], except that the C (r) values obtained from the data are replaced by expectation values with respect to the p measure. Hence, in terms of computation, the maximization is exactly similar to the maximum likelihood computation.
Reference: [20] <author> S. Lakshmanan and H. Derin, </author> <title> "Gaussian Markov Random Fields at Multiple Resolutions," in Markov Random Fields: Theory and Applications (R. Chellappa, </title> <publisher> ed.), </publisher> <pages> pp. 131-157, </pages> <publisher> Academic Press, </publisher> <year> 1993. </year>
Reference-contexts: However, these subsampling schemes under which the Markov property is preserved are not very interesting. In <ref> [20] </ref> Lakshmanan and Derin present an excellent discussion on multiresolution GMRF models. It is shown that the GMRFs lose their Markov property under subsampling and expressions for the power spectral density functions at coarser resolution are obtained. <p> Many interesting properties of this estimator such as maximizing the entropy and minimizing the Kullback-Leibler (KL) distance can be found in <ref> [20] </ref>. Luettgen, et al. [22] propose a multiscale model using a tree construction for bilateral (called reciprocal processes by the authors) Markov processes in 1-D by using a midpoint deflection construction. <p> Equivalently, X (k) = D k where the matrix D k 0 , has to be properly defined <ref> [20] </ref>. The resulting subsampled field X (k) is Gaussian, with covariance (k) = [D k 0 ] (0) [D k power spectrum of X (k) can be shown to be [20]: S (k) 1 X S (0) 0 where r 0 2 k r 1 ; N It can be observed <p> Equivalently, X (k) = D k where the matrix D k 0 , has to be properly defined <ref> [20] </ref>. The resulting subsampled field X (k) is Gaussian, with covariance (k) = [D k 0 ] (0) [D k power spectrum of X (k) can be shown to be [20]: S (k) 1 X S (0) 0 where r 0 2 k r 1 ; N It can be observed that S (k) x (!) cannot be written in the form of Eq. (5) with a finite neighborhood. <p> Therefore, the subsampled fields X (k) are non-Markov, except for the special case of second order separable correlation GMRFs <ref> [20] </ref>. 3 Markov Approximations As mentioned in the last section, GMRFs become non-Markov under subsampling and the prob ability density functions (pdf) of the subsampled random fields can be obtained. <p> The KL distance measure is widely used to obtain approximate probability measures with desired properties. The estimator presented in this subsection is same as the estimator originally proposed by Lakshmanan and Derin <ref> [20] </ref>. The estimator presented in [20] is motivated by "covariance matching" and is also shown to minimize the KL distance. Our presentation is based on directly minimizing the KL distance, but the two estimators are the same, because of the uniqueness of this estimator. <p> The KL distance measure is widely used to obtain approximate probability measures with desired properties. The estimator presented in this subsection is same as the estimator originally proposed by Lakshmanan and Derin <ref> [20] </ref>. The estimator presented in [20] is motivated by "covariance matching" and is also shown to minimize the KL distance. Our presentation is based on directly minimizing the KL distance, but the two estimators are the same, because of the uniqueness of this estimator. <p> Hence, in terms of computation, the maximization is exactly similar to the maximum likelihood computation. Given the p measure, we only need a few moment values E p [C (r)], followed by the maximization of Eq. (10) to obtain the Markov approximation. Lakshmanan and Derin <ref> [20] </ref> also remark that their covariance invariance estimator, which minimizes the KL distance, also maximizes the likelihood of a sample observation with sample covariance equal to the covariance of the p measure. 9 3.2 Local Conditional Distribution Invariance Approximation We propose another method to estimate the best GMRF parameters of a
Reference: [21] <author> S. Lu and S. Fu, </author> <title> "Stochastic Tree Grammar Inference for Texture Synthesis and Discrimination," Computer Vision, </title> <journal> Graphics, and Image Processing, </journal> <volume> Vol. 9, </volume> <pages> pp. 234-245, </pages> <year> 1979. </year>
Reference-contexts: Structural approaches are aimed at regular textures that exhibit a strong structural behavior. These approaches define a basic primitive and placement rules. Such techniques were used by Rosenfeld [28], Lu and Fu <ref> [21] </ref> and Tomita [32]. Structural approaches are well suited for macro textures, but are not useful for micro textures where it is hard to define a basic primitive 16 and placement rules. Statistical methods are better suited for such micro textures.
Reference: [22] <author> M. R. Luettgen, W. C. Karl, A. S. Willsky, and R. R. Tenney, </author> <title> "Multiscale Representations of Markov Random Fields," </title> <journal> IEEE Trans. on Signal Processing, </journal> <volume> Vol. 41, </volume> <pages> pp. 3377-3397, </pages> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: Many interesting properties of this estimator such as maximizing the entropy and minimizing the Kullback-Leibler (KL) distance can be found in [20]. Luettgen, et al. <ref> [22] </ref> propose a multiscale model using a tree construction for bilateral (called reciprocal processes by the authors) Markov processes in 1-D by using a midpoint deflection construction.
Reference: [23] <author> B. S. Manjunath, T. Simchony, and R. Chellappa, </author> <title> "Stochastic and Deterministic Networks for Texture Segmentation," </title> <journal> IEEE Trans. on Acoustics, Speech, and Signal Processing, </journal> <volume> Vol. 38, </volume> <pages> pp. 1039-1049, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Geman and Graffigne [13] used a general MRF model for segmentation. They showed that this non-Gaussian MRF model was not very well suited for synthesis, but still performed well for segmentation. Derin and Elliot [9] used Gibbs distributions and presented a non-optimal method using dynamic programming. Manjunath et.al. <ref> [23] </ref> proposed a stochastic learning technique to improve the ICM results and presented a neural network implementation for texture segmentation. Texture segmentation problem is the labeling of pixels in a lattice to one of V texture classes, based on a texture model and the observed intensity field.
Reference: [24] <author> I. Matsuba, </author> <title> "Renormalization Group Approach to Hierarchical Image Analysis," </title> <booktitle> IEEE International Conf. on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pp. 1044-1047, </pages> <year> 1988. </year>
Reference-contexts: Gidas [14] shows some connection between the renormalization group studies in statistical mechanics [10] and the multiresolution MRF framework to process images. A hierarchical image analysis scheme based on renormalization group ideas is presented also by Matsuba in <ref> [24] </ref>. Perez and Heitz [27] present a multiscale MRF model, where optimization is performed sequentially over specific subsets of original configuration space. The result of optimization at each scale is used to initialize the optimization at the subsequent finer scale.
Reference: [25] <author> P. A. P. Moran and J. Besag, </author> <title> "On the Estimation and Testing of Spatial Interaction in Gaussian Lattice," </title> <journal> Biometrika, </journal> <volume> Vol. 62, </volume> <pages> pp. 552-562, </pages> <year> 1975. </year>
Reference-contexts: We also show that the computations for these two estimators turn out to be similar to the traditional maximum likelihood <ref> [25] </ref>, [19], [30] and the pseudo likelihood estimators [1], except that the sample covariances are replaced by covariances calculated with respect to the exact non-Markov measure that is being approximated. <p> Two dif-ferent estimators to obtain the parameters of GMRFs at lower resolutions from the parameters at the fine resolution are presented. We also exemplify the connection between these two estimators and the estimators that are commonly used to estimate the GMRF parameters from a data sample, the maximum likelihood <ref> [25] </ref>, [19], [30] and the pseudo likelihood estimators [1]. 3.1 Kullback-Leibler (KL) Distance Minimization In this section, we show that given any pdf p, it is possible to obtain a GMRF approximation of p by minimizing the KL distance D (p k q) [18], where q belongs to the family of <p> 2 s s ) 2 2 2 (E p [C (0)] T E p [C ]) (10) where, E p [C (r)] = E p [ s2 = (M N )E p [X s X s+r ]: (11) Remark: Observe that Eq. (10) is similar to the maximum likelihood expression <ref> [25] </ref>, [19], [30], except that the C (r) values obtained from the data are replaced by expectation values with respect to the p measure. Hence, in terms of computation, the maximization is exactly similar to the maximum likelihood computation.
Reference: [26] <author> A. Papoulis, </author> <title> Probability, Random Variables, and Stochastic Processes, </title> <publisher> McGraw-Hill Book Company, </publisher> <year> 1965. </year> <month> 26 </month>
Reference-contexts: ; r 2 ) is also Gaussian with conditional mean P r x s+r (which is the best linear estimate of X s in terms of X s+r ; r 2 ) and conditional variance [ 2 ] fl (which is the corresponding minimum mean square error of the estimator) <ref> [26] </ref>. q fl (x ) being a GMRF with parameters ( fl ; [ 2 ] fl ), from the discussion at the beginning of this section, has the conditional distribution q fl (x s jx s+r ; r 2 ) with the conditional mean P r x s+r and conditional
Reference: [27] <author> P. Perez and F. Heitz, </author> <title> "Multiscale Markov Random Fields and Constrained Relaxation in Low Level Image Analysis," </title> <booktitle> IEEE International Conf. on Acoustics, Speech, and Signal Processing, </booktitle> <volume> Vol. 3, </volume> <pages> pp. 61-64, </pages> <year> 1992. </year>
Reference-contexts: Gidas [14] shows some connection between the renormalization group studies in statistical mechanics [10] and the multiresolution MRF framework to process images. A hierarchical image analysis scheme based on renormalization group ideas is presented also by Matsuba in [24]. Perez and Heitz <ref> [27] </ref> present a multiscale MRF model, where optimization is performed sequentially over specific subsets of original configuration space. The result of optimization at each scale is used to initialize the optimization at the subsequent finer scale.
Reference: [28] <author> A. Rosenfeld, </author> <title> "Visual Texture Analysis," </title> <type> Tech. Rep. 70-116, </type> <institution> University of Maryland, </institution> <month> June </month> <year> 1970. </year>
Reference-contexts: Structural approaches are aimed at regular textures that exhibit a strong structural behavior. These approaches define a basic primitive and placement rules. Such techniques were used by Rosenfeld <ref> [28] </ref>, Lu and Fu [21] and Tomita [32]. Structural approaches are well suited for macro textures, but are not useful for micro textures where it is hard to define a basic primitive 16 and placement rules. Statistical methods are better suited for such micro textures.
Reference: [29] <author> A. Rosenfeld, </author> <title> "Some Useful Properties of Pyramids," in Multiresolution Image Processing and Analysis (A. </title> <editor> Rosenfeld, ed.), </editor> <publisher> Springer-Verlag, </publisher> <year> 1984. </year>
Reference-contexts: The second approach is to use multiresolution techniques. Two important aspects of mul-tiresolution approaches are: (1) divide and conquer and (2) action at a distance <ref> [29] </ref>. Multigrid methods proposed by Terzopoulos [31], substantially reduce the computation when applied to some computer vision problems. Chen and Pavlidis [5] use a hierarchical approach to texture segmentation, but do not directly use MRFs.
Reference: [30] <author> G. Sharma and R. Chellappa, </author> <title> "Two-Dimensional Maximum Entropy Power Spectra," </title> <journal> IEEE Transaction on Information Theory, </journal> <volume> Vol. IT-31, </volume> <pages> pp. 90-99, </pages> <month> Jan </month> <year> 1985. </year>
Reference-contexts: We also show that the computations for these two estimators turn out to be similar to the traditional maximum likelihood [25], [19], <ref> [30] </ref> and the pseudo likelihood estimators [1], except that the sample covariances are replaced by covariances calculated with respect to the exact non-Markov measure that is being approximated. <p> We also exemplify the connection between these two estimators and the estimators that are commonly used to estimate the GMRF parameters from a data sample, the maximum likelihood [25], [19], <ref> [30] </ref> and the pseudo likelihood estimators [1]. 3.1 Kullback-Leibler (KL) Distance Minimization In this section, we show that given any pdf p, it is possible to obtain a GMRF approximation of p by minimizing the KL distance D (p k q) [18], where q belongs to the family of GMRF pdfs. <p> As seen before, GMRFs can be completely characterized by ( ; 2 ). From Eq. (3), q (x ) can be written as: q (x ) = det B MN expf 1 The quadratic form x ) can be simplified as <ref> [30] </ref>: x T B x = C (0) T C where C is a vector whose length is equal to the number of elements in the neighborhood and 8 is as follows: C (0) = s2 s ; C (r) = s2 and the determinant of the B matrix can be <p> s ) 2 2 2 (E p [C (0)] T E p [C ]) (10) where, E p [C (r)] = E p [ s2 = (M N )E p [X s X s+r ]: (11) Remark: Observe that Eq. (10) is similar to the maximum likelihood expression [25], [19], <ref> [30] </ref>, except that the C (r) values obtained from the data are replaced by expectation values with respect to the p measure. Hence, in terms of computation, the maximization is exactly similar to the maximum likelihood computation.
Reference: [31] <author> D. Terzopoulos, </author> <title> "Image Analysis using Multigrid Relaxation Methods," </title> <journal> IEEE Trans. Patt. Anal. Mach. Intell., </journal> <volume> Vol. 8, </volume> <pages> pp. 129-139, </pages> <month> March </month> <year> 1986. </year>
Reference-contexts: The second approach is to use multiresolution techniques. Two important aspects of mul-tiresolution approaches are: (1) divide and conquer and (2) action at a distance [29]. Multigrid methods proposed by Terzopoulos <ref> [31] </ref>, substantially reduce the computation when applied to some computer vision problems. Chen and Pavlidis [5] use a hierarchical approach to texture segmentation, but do not directly use MRFs. Bouman and Liu [2] use a causal autoregressive 2 Gaussian model and a quad-tree structure to perform multiresolution segmentation.
Reference: [32] <author> F. Tomita, </author> <title> "Description of texture by a structural analysis," </title> <booktitle> Proc. IJCAI, </booktitle> <year> 1979. </year>
Reference-contexts: Structural approaches are aimed at regular textures that exhibit a strong structural behavior. These approaches define a basic primitive and placement rules. Such techniques were used by Rosenfeld [28], Lu and Fu [21] and Tomita <ref> [32] </ref>. Structural approaches are well suited for macro textures, but are not useful for micro textures where it is hard to define a basic primitive 16 and placement rules. Statistical methods are better suited for such micro textures.
Reference: [33] <author> J. W. Woods, </author> <title> "Two-dimensional Discrete Markovian Fields," </title> <journal> IEEE Transaction on Information Theory, </journal> <volume> Vol. 18, </volume> <pages> pp. 232-240, </pages> <year> 1972. </year>
Reference-contexts: Most of the early statistical methods use first and second order properties to discriminate between textures. Haralick [15] suggested various local statistical measures based on the gray level dependence, later generalized by Davis, et.al. [8]. More complex statistical models using MRFs <ref> [33] </ref> emerged later and have been used in various image processing problems. Cross and Jain [7] and Chellappa [4] have shown the applicability of GMRF models to synthesize textures. Chellappa and Chatterjee [3] have used GMRF models for maximum likelihood segmentation of textures.
Reference: [34] <author> C. H. Wu and P. C. Doerschuk, </author> <title> "Tree Approximations to Markov Random Fields," </title> <journal> IEEE Trans. Patt. Anal. Mach. Intell., </journal> <volume> Vol. 17, </volume> <pages> pp. 391-343, </pages> <month> Apr. </month> <year> 1995. </year>
Reference-contexts: The first is to use non-optimal, deterministic methods that converge to a local optimal point, but still provide reasonably good results. Geiger and Girosi [11] and Zhang [35] use mean field approximations that lead to deterministic relaxation algorithms. Wu and Doerschuk <ref> [34] </ref> use a tree approximation that replaces the lattice on which an MRF is defined by an acyclic tree which allows replacing the iterative MRF computations by recursive computations. The second approach is to use multiresolution techniques.

References-found: 34

