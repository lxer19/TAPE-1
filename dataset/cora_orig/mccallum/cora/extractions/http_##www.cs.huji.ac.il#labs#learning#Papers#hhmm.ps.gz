URL: http://www.cs.huji.ac.il/labs/learning/Papers/hhmm.ps.gz
Refering-URL: http://www.cs.huji.ac.il/labs/learning/Papers/MLT_list.html
Root-URL: http://www.cs.huji.ac.il
Email: Email: ffshai,singer,tishbyg@cs.huji.ac.il  
Title: The Hierarchical Hidden Markov Model: Analysis and Applications  
Author: Shai Fine Yoram Singer Naftali Tishby 
Address: Jerusalem 91904, Israel  
Affiliation: Institute of Computer Science and Center for Neural Computation The Hebrew University  
Abstract: We introduce, analyze and demonstrate a recursive hierarchical generalization of the widely used hidden Markov models, which we name Hierarchical Hidden Markov Models (HHMM). Our model is motivated by the complex multiscale structure which appears in many natural sequences, particularly in language, handwriting and speech. We seek for a systematic, unsupervised approach to the modeling of such structures. By extending the standard Baum-Welch procedure, we derive an efficient estimation procedure of the model parameters from unlabeled data. Given the trained model, we apply it to automatic hierarchical parsing of an observation sequence as a dendrogram. Moreover, we suggest an efficient approximation to the full estimation scheme which can further be used to construct models that adapt both their topology and parameters. Two applications of our model and its estimation procedure are described. In the first, we show how to construct hierarchical models of natural english text. In these models different levels of the hierarchy correspond to structures on different length scales in the text. In the second application we demonstrate how HHMMs can be used to automatically spot and identify combinations of letters in cursive handwriting. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> N. Abe and M. Warmuth, </author> <title> On the Computational Complexity of Approximating Distributions by Probabilistic Automata, </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 205-260, </pages> <year> 1992. </year>
Reference: [2] <author> L.E Baum and T. Petrie, </author> <title> Statistical Inference for Probabilistic Functions of Finite State Markov Chains, </title> <journal> Ann. Math. Stat. </journal> <volume> Vol. 37, </volume> <year> 1966. </year>
Reference-contexts: Hidden Markov models are also used for natural language modeling (see e.g. [6]). In most of these applications the HMM topology is fixed in advance and the model parameters are estimated by an EM procedure, known as the Baum-Welch algorithm in this context <ref> [2] </ref>. Only recently there have been some works suggesting the inference of the model structure as well [16]. In most of the above applications, there are, however, difficulties due to the multiplicity of length scales and recursive nature of the sequences.
Reference: [3] <author> T. Cover and J. Thomas, </author> <title> Elements of Information Theory, </title> <publisher> Wiley, </publisher> <year> 1991. </year>
Reference: [4] <author> A.P. Dempster, N.M Laird, D.B. Rubin. </author> <title> Maximum-Likelihood from Incomplete Data via the EM Algorithm, </title> <journal> J. Royal Stat. Soc., </journal> <volume> B39:1-38, </volume> <year> 1977. </year>
Reference: [5] <author> D. Gillman and M. Sipser, </author> <title> Inference and Minimization of Hidden Markov Chains, </title> <booktitle> Proceedings of the Seventh Annual Workshop on Computational Learning Theory, </booktitle> <pages> pp. 147-158, </pages> <year> 1994. </year>
Reference: [6] <author> F. Jelinek, </author> <title> Robust Part-Of-Speech Tagging Using A Hidden Markov Model, </title> <type> IBM Tech. Report, </type> <year> 1985. </year>
Reference-contexts: 1 Introduction Hidden Markov Models (HMMs) have become the method of choice for modeling stochastic processes and sequences such as speech [12], handwriting [11] and DNA [9]. Hidden Markov models are also used for natural language modeling (see e.g. <ref> [6] </ref>). In most of these applications the HMM topology is fixed in advance and the model parameters are estimated by an EM procedure, known as the Baum-Welch algorithm in this context [2]. Only recently there have been some works suggesting the inference of the model structure as well [16].
Reference: [7] <author> F. Jelinek, </author> <title> Markov Source Modeling of Text Generation, </title> <type> Technical report, </type> <institution> IBM T.J. Watson Research Center, </institution> <year> 1983. </year>
Reference: [8] <author> F. Jelinek, </author> <title> Self-Organized Language Modeling for Speech Recognition, </title> <institution> IBM T.J. Watson Research Center, </institution> <year> 1985. </year>
Reference: [9] <author> A. Krogh, S.I. Mian, D. Haussler, </author> <title> A Hidden Markov Model that finds genes in E.coli DNA, </title> <address> UCSC-CRL-93-16, </address> <year> 1993. </year>
Reference-contexts: 1 Introduction Hidden Markov Models (HMMs) have become the method of choice for modeling stochastic processes and sequences such as speech [12], handwriting [11] and DNA <ref> [9] </ref>. Hidden Markov models are also used for natural language modeling (see e.g. [6]). In most of these applications the HMM topology is fixed in advance and the model parameters are estimated by an EM procedure, known as the Baum-Welch algorithm in this context [2].
Reference: [10] <author> K. Lari and S.J. Young, </author> <title> The Estimation of Stochastic Context Free Grammars using the Inside-Outside algorithm, </title> <journal> Comp. Speech & Lang. </journal> <volume> 4, </volume> <year> 1990. </year>
Reference-contexts: Some of those difficulties can be overcome by using stochastic context free grammars. Yet, such models are much more difficult to estimate and the common algorithm, called the inside-outside algorithm <ref> [10] </ref>, has time complexity cubic in the length of the observed sequences. In this paper we present an hierarchical generalization of the hidden Markov model.
Reference: [11] <author> R. Nag, K.H. Wong, F. Fallside, </author> <title> Script Recognition Using Hidden Markov Models, </title> <booktitle> Proc. of ICASSP, </booktitle> <pages> pp. </pages> <year> 2071-2074,1985. </year>
Reference-contexts: 1 Introduction Hidden Markov Models (HMMs) have become the method of choice for modeling stochastic processes and sequences such as speech [12], handwriting <ref> [11] </ref> and DNA [9]. Hidden Markov models are also used for natural language modeling (see e.g. [6]). In most of these applications the HMM topology is fixed in advance and the model parameters are estimated by an EM procedure, known as the Baum-Welch algorithm in this context [2].
Reference: [12] <author> L.R. Rabiner and B.H. Juang, </author> <title> An Introduction to Hidden Markov Models, </title> <journal> IEEE ASSP Magazine, </journal> <volume> No. 3, </volume> <year> 1986. </year>
Reference-contexts: 1 Introduction Hidden Markov Models (HMMs) have become the method of choice for modeling stochastic processes and sequences such as speech <ref> [12] </ref>, handwriting [11] and DNA [9]. Hidden Markov models are also used for natural language modeling (see e.g. [6]). In most of these applications the HMM topology is fixed in advance and the model parameters are estimated by an EM procedure, known as the Baum-Welch algorithm in this context [2].
Reference: [13] <author> L.R. Rabiner, </author> <title> A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition, </title> <booktitle> Proceedings of the IEEE, </booktitle> <year> 1989. </year>
Reference: [14] <author> J. Rissanen, </author> <title> Complexity of Strings in the Class of Markov sources, </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> 32(4) </volume> <pages> 526-532, </pages> <year> 1986. </year>
Reference: [15] <author> Y. Singer and N. Tishby, </author> <title> Dynamical Encoding of Cursive Handwriting, </title> <journal> Biological Cybernetics, </journal> <volume> 71(3), </volume> <pages> pp. 227-237, </pages> <year> 1994. </year>
Reference-contexts: Those tuned probabilities reflects the formation of temporal experts. 4.2 Unsupervised Learning and Spotting of Cursive Handwriting In <ref> [15] </ref>, the second and the third authors proposed a dynamic encoding scheme for cursive handwriting based on an oscillatory model of handwriting. The process described in [15] performs inverse mapping from continuous pen trajectories to strings over a discrete set of symbols which efficiently encode cursive handwriting. <p> Those tuned probabilities reflects the formation of temporal experts. 4.2 Unsupervised Learning and Spotting of Cursive Handwriting In <ref> [15] </ref>, the second and the third authors proposed a dynamic encoding scheme for cursive handwriting based on an oscillatory model of handwriting. The process described in [15] performs inverse mapping from continuous pen trajectories to strings over a discrete set of symbols which efficiently encode cursive handwriting. These symbols are named motor control commands.
Reference: [16] <author> A. Stolcke and S.M. Omohundro, </author> <title> Best-first Model Merging for Hidden Markov Model Induction, </title> <booktitle> ICSI TR-94-003, </booktitle> <year> 1994. </year>
Reference-contexts: In most of these applications the HMM topology is fixed in advance and the model parameters are estimated by an EM procedure, known as the Baum-Welch algorithm in this context [2]. Only recently there have been some works suggesting the inference of the model structure as well <ref> [16] </ref>. In most of the above applications, there are, however, difficulties due to the multiplicity of length scales and recursive nature of the sequences. Some of those difficulties can be overcome by using stochastic context free grammars.
Reference: [17] <author> A.J. </author> <title> Viterbi, Error Bounds for Convulutional Codes and an Asymptotically Optimal Decoding Algorithm, </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> 13 </volume> <pages> 260-269, </pages> <year> 1967. </year> <title> 13 The deeper model developed richer varieties of strings and a multi-scale temporal behavior can be detected. Different time scale `expertise' were developed in this model. </title> <type> 12 </type>
References-found: 17

