URL: http://www-eksl.cs.umass.edu/papers/jensenkdd97.ps
Refering-URL: http://eksl-www.cs.umass.edu/publications.html
Root-URL: 
Title: A  Adjusting for Multiple Comparisons in Decision Tree Pruning  
Author: f j g David Jensen and Matt Schmill 
Note: evaluating  
Abstract:  
Abstract-found: 1
Intro-found: 1
Reference: <author> The authors would like to thank Paul Cohen, Tim Oates, Paul Utgoff, </author> <note> and two anonymous reviewers for their comments on earlier versions of this paper. M. </note>
Reference: <institution> Zwitter and M. Soklic of the University Medical Cen-tre, Institute of Oncology, Ljubljana, Yugoslavia provided the and datasets, and Dr. </institution>
Reference: <author> William H. </author> <title> Wolberg of the University of Wisconsin Hospitals provided the dataset. This research is supported by DARPA/Rome Laboratory under contract No. F30602-93-C-0076. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright notation hereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements either expressed or implied, </title> <institution> of the Defense Advanced Research Projects Agency, Rome Laboratory or the U.S. Government. </institution>
Reference: <author> Breiman, L.; Friedman, J.; Olshen, R.; and Stone, C. </author> <year> 1984. </year> . <note> Wadsworth and Brooks. </note>
Reference: <author> Cohen, P., and Jensen, D. </author> <year> 1997. </year> <title> Overfitting explained. </title> <booktitle> In </booktitle> . 
Reference-contexts: Nearly all induction algorithms implicitly or explicitly search a space of candidate models. Decision tree algorithms examine many possible subtrees and select the best one. Pruning should adjust for the number of subtrees examined, because multiple comparisons affect the distribution of the maximum score on training data <ref> (Cohen & Jensen 1997) </ref>. This paper examines one pruning method that adjusts for multiple comparisons | . Bonferroni pruning adjusts the results of a statistical hypothesis test for the number of subtrees examined at a particular node of a decision tree. Pruning techniques have been based on several different criteria.
Reference: <author> Feelders, A., and Verkooijen, W. </author> <year> 1995. </year> <title> Which method learns the most from data? methodological issues in the analysis of comparative studies. </title> <booktitle> In </booktitle> . 
Reference: <author> Gascuel, O., and Caraux, G. </author> <year> 1992. </year> <title> Statistical significance in inductive learning. </title> <note> In , 435-439. </note>
Reference: <author> Jensen, D. </author> <year> 1992. </year> . <type> Ph.D. Dissertation, </type> <institution> Washington University, St. Louis Missouri. </institution>
Reference-contexts: Portions of these models are wrong, and can mislead users. Overfitted models are less efficient to store and use than their correctly-sized counterparts. Finally, overfitting can reduce the accuracy of induced models on new data <ref> (Jensen 1992) </ref>. Decision tree algorithms (Breiman 1984; Quinlan 1993) often use to correct overfit-ting. Pruning examines individual subtrees and removes those subtrees deemed to be unnecessary. Pruning techniques primarily differ in the criterion used to judge subtrees.
Reference: <author> Kass, G. </author> <year> 1980. </year> <title> An exploratory technique for investigating large quantities of categorical data. </title> <booktitle> 29 </booktitle> <pages> 199-127. </pages>
Reference-contexts: The statistic is used because it has a known reference distribution and because it is additive, allowing portions of contingency tables to be evaluated individually. During tree construction, selects attributes using an approach suggested by Kass <ref> (Kass 1980) </ref> and Kerber (Kerber 1992). For each attribute, a contingency table is constructed with a row for each class value and a column for each of attribute values | every possible value for discrete attributes or every unique interval for discretized continuous attributes.
Reference: <author> Kerber, R. </author> <year> 1992. </year> <title> Chimerge: Discretization of numeric attributes. </title> <note> In , 123-128. </note>
Reference-contexts: The statistic is used because it has a known reference distribution and because it is additive, allowing portions of contingency tables to be evaluated individually. During tree construction, selects attributes using an approach suggested by Kass (Kass 1980) and Kerber <ref> (Kerber 1992) </ref>. For each attribute, a contingency table is constructed with a row for each class value and a column for each of attribute values | every possible value for discrete attributes or every unique interval for discretized continuous attributes.
Reference: <author> Kotz, S., and Johnson, N., eds. </author> <title> 1982. </title> . <publisher> Wiley. </publisher>
Reference-contexts: As a result, adjusting for multiple comparisons becomes essential for an accurate pruning. Multiple comparisons affect all threshold-based pruning techniques, regardless of whether they explicitly employ statistical tests. Equation 1 is one of a class of Bonferroni equations <ref> (Kotz & Johnson 1982) </ref>, that can be used to adjust for multiple comparisons. Specifically, we can set and solve Equation 1 for , so that, for a given and number of comparisons , an appropriate critical value can be selected based on a reference distribution for a single comparison. <p> The class label of the leaf node is determined by the majority class of the training instances present at that node. Trees are pruned bottom-up by each of four techniques: 1) | Fisher's exact test <ref> (Kotz & Johnson 1982) </ref> with = 0 10, which does not adjust for multiple comparisons; 2) | The technique used in (Quinlan 1993); 3) | Minimum description length, using Utgoff's formulation (Utgoff 1995); and 4) | Fisher's exact test with adjusted for the number of comparisons using the Bonferroni adjustment.
Reference: <author> Oates, T., and Jensen, D. </author> <year> 1997. </year> <title> The effects of training set size on decision tree complexity. </title> <booktitle> In </booktitle> . 
Reference-contexts: Recall that all pruning methods begin with the same trees. Because produces pruned trees with nearly optimal accuracy, it is clear that the true structure is present in the original trees. For , , and , complexity is strongly associated with training set size. A related paper <ref> (Oates & Jensen 1997) </ref> explores this behavior for a wide variety of realistic datasets and several pruning approaches. 0 60 40 20 error-based fishers mdl bonferroni unpruned Number of instances 0 50 100 150 200 250 Complexity unpruned fisher s 0.90 0.70 0.50 bonferroni mdl error-based Accuracy t t within among
Reference: <author> Quinlan, J., and Rivest, R. </author> <year> 1989. </year> <title> Inferring decision trees using the minimum description length principle. </title> <booktitle> 80 </booktitle> <pages> 227-248. </pages>
Reference-contexts: Pruning examines individual subtrees and removes those subtrees deemed to be unnecessary. Pruning techniques primarily differ in the criterion used to judge subtrees. Common criteria include statistical significance tests (Quinlan 1986), corrected error estimates (Quinlan 1993), and minimum description length calculations <ref> (Quinlan & Rivest 1989) </ref>. Most common pruning techniques do not adjust for multiple comparisons. Multiple comparisons occur when an induction algorithm examines several candidate models and selects the one with the maximum score. Nearly all induction algorithms implicitly or explicitly search a space of candidate models. <p> Statistical significance (Quinlan 1986) has generally been rejected based on its empirical performance. Error-based criteria, such as the approach used in (Quinlan 1993), estimate true error rates by deflating the accuracy of subtrees on the training data. Minimum Description Length (MDL) criteria <ref> (Quinlan & Rivest 1989) </ref> characterize both datasets and models by the number of bits needed to encode them. The best tree is the one with the smallest total "description length" for the data, that is, the smallest sum of model description and description of the exceptions to the model's predictions.
Reference: <author> Quinlan, J. </author> <year> 1986. </year> <title> Induction of decision trees. </title> <booktitle> 1 </booktitle> <pages> 81-106. </pages>
Reference-contexts: Decision tree algorithms (Breiman 1984; Quinlan 1993) often use to correct overfit-ting. Pruning examines individual subtrees and removes those subtrees deemed to be unnecessary. Pruning techniques primarily differ in the criterion used to judge subtrees. Common criteria include statistical significance tests <ref> (Quinlan 1986) </ref>, corrected error estimates (Quinlan 1993), and minimum description length calculations (Quinlan & Rivest 1989). Most common pruning techniques do not adjust for multiple comparisons. Multiple comparisons occur when an induction algorithm examines several candidate models and selects the one with the maximum score. <p> This paper examines one pruning method that adjusts for multiple comparisons | . Bonferroni pruning adjusts the results of a statistical hypothesis test for the number of subtrees examined at a particular node of a decision tree. Pruning techniques have been based on several different criteria. Statistical significance <ref> (Quinlan 1986) </ref> has generally been rejected based on its empirical performance. Error-based criteria, such as the approach used in (Quinlan 1993), estimate true error rates by deflating the accuracy of subtrees on the training data. <p> b p ff ff : TBA: Tree-building with Bonferroni Adjustment tba Tba tba Tba Tba tba c4.5 tba tba These experiments evaluate the performance of a new algorithm | Tree-building with Bonferroni Adjustment ( ) | on 13 UCI datasets. differs from other algorithms for top-down induction of decision trees <ref> (Quinlan 1986) </ref> in three important ways: its evaluation function, how it selects partitions during tree construction, and how it selects subtrees during tree pruning. uses the statistic to evaluate contingency tables during tree construction and pruning.
Reference: <author> Quinlan, J. </author> <title> 1993. </title> . <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: Decision tree algorithms (Breiman 1984; Quinlan 1993) often use to correct overfit-ting. Pruning examines individual subtrees and removes those subtrees deemed to be unnecessary. Pruning techniques primarily differ in the criterion used to judge subtrees. Common criteria include statistical significance tests (Quinlan 1986), corrected error estimates <ref> (Quinlan 1993) </ref>, and minimum description length calculations (Quinlan & Rivest 1989). Most common pruning techniques do not adjust for multiple comparisons. Multiple comparisons occur when an induction algorithm examines several candidate models and selects the one with the maximum score. <p> Pruning techniques have been based on several different criteria. Statistical significance (Quinlan 1986) has generally been rejected based on its empirical performance. Error-based criteria, such as the approach used in <ref> (Quinlan 1993) </ref>, estimate true error rates by deflating the accuracy of subtrees on the training data. Minimum Description Length (MDL) criteria (Quinlan & Rivest 1989) characterize both datasets and models by the number of bits needed to encode them. <p> The values of the classification are systematically corrupted by complementing each value with a probability of 0.1, producing a theoretical upper bound of 90% on classification accuracy. The theoretically correct tree has three nodes | a decision node and two leaf nodes. The tree-building algorithm uses information gain <ref> (Quinlan 1993) </ref> to choose the best attribute for a particular decision node. A leaf node is created when no partition can improve accuracy on the training set. The class label of the leaf node is determined by the majority class of the training instances present at that node. <p> Trees are pruned bottom-up by each of four techniques: 1) | Fisher's exact test (Kotz & Johnson 1982) with = 0 10, which does not adjust for multiple comparisons; 2) | The technique used in <ref> (Quinlan 1993) </ref>; 3) | Minimum description length, using Utgoff's formulation (Utgoff 1995); and 4) | Fisher's exact test with adjusted for the number of comparisons using the Bonferroni adjustment. Training set size is varied from 5 to 250 by increments of 5 instances.
Reference: <author> Utgoff, P. </author> <year> 1995. </year> <title> Decision tree induction based on efficient tree restructuring. </title> <type> Technical Report 95-18, </type> <institution> Department of Computer Science, University of Mas-sachusetts, Amherst. </institution>
Reference-contexts: Trees are pruned bottom-up by each of four techniques: 1) | Fisher's exact test (Kotz & Johnson 1982) with = 0 10, which does not adjust for multiple comparisons; 2) | The technique used in (Quinlan 1993); 3) | Minimum description length, using Utgoff's formulation <ref> (Utgoff 1995) </ref>; and 4) | Fisher's exact test with adjusted for the number of comparisons using the Bonferroni adjustment. Training set size is varied from 5 to 250 by increments of 5 instances. For each value of , 100 trials are conducted.
References-found: 16

