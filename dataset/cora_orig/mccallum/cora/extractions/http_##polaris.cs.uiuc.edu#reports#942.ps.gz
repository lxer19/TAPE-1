URL: http://polaris.cs.uiuc.edu/reports/942.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: The Impact of Synchronization and Granularity on Parallel Systems  
Author: Ding-Kai Chen, Hong-Men Su, and Pen-Chung Yew 
Address: Urbana, Illinois, 61820  
Affiliation: Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign  
Abstract: In this paper, we study the impact of synchronization and granularity on the performance of parallel systems using an execution-driven simulation technique. We find that even though there can be a lot of parallelism at the fine grain level, synchronization and scheduling strategies determine the ultimate performance of the system. Loop-iteration level parallelism seems to be a more appropriate level when those factors are considered. We also study barrier synchronization and data synchronization at the loop-iteration level and found both schemes are needed for a better performance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Allen, D. Gallahan, and K. Kennedy. </author> <title> Automatic decomposition of scientific programs for parallel execution. </title> <booktitle> ACM Symp. on Principles of Programming Languages, </booktitle> <pages> 63-76, </pages> <month> Jan. </month> <year> 1987. </year>
Reference-contexts: No further overhead is needed within the task for the control dependences. 2.3 Barrier Synchronization Another way to enforce data dependences is to use barrier synchronization. Barrier synchronization is quite effective in loop-iteration level parallelism. It has been attempted in statement level parallelism as well [7]. Some optimizing compilers <ref> [1, 2] </ref> can generate barrier synchronization for parallel programs. A carefully placed barrier can eliminate a lot of inter-task data synchronization and its overhead.
Reference: [2] <institution> Alliant. FX/Series Architecture Manual. Alliant Computer Systems Corp., </institution> <month> Jan. </month> <year> 1986. </year> <month> 9 </month>
Reference-contexts: No further overhead is needed within the task for the control dependences. 2.3 Barrier Synchronization Another way to enforce data dependences is to use barrier synchronization. Barrier synchronization is quite effective in loop-iteration level parallelism. It has been attempted in statement level parallelism as well [7]. Some optimizing compilers <ref> [1, 2] </ref> can generate barrier synchronization for parallel programs. A carefully placed barrier can eliminate a lot of inter-task data synchronization and its overhead.
Reference: [3] <author> G. </author> <title> Amdahl. Validity of the single-processor ap-proach to achieving large-scale computer capabilities. </title> <booktitle> AFIPS Conf., </booktitle> <pages> 483-485, </pages> <year> 1967. </year>
Reference-contexts: 1 Introduction As hardware components become more powerful and less expensive, parallel processing has become an indispensable means for achieving higher performance. From the well-known Amdahl's law <ref> [3] </ref>, software issues are very critical in such parallel systems. Due to the added complexity in multiple system components, the interaction between software and hardware becomes more complicated.
Reference: [4] <author> Arvind, D. Culler, and G. Maa. </author> <title> Assessing the benefits of fine-grain parallelism in dataflow programs. </title> <booktitle> Supercomputing, </booktitle> <pages> 60-69, </pages> <month> Nov. </month> <year> 1988. </year>
Reference-contexts: People have been exploiting all possible levels of granularity, from fine grain to coarse grain. VLIW machines [9] and superscalar machines [12, 22] exploit operation level parallelism across basic blocks using either software or hardware techniques to schedule low level operations. Dataflow machines <ref> [4] </ref> exploit lowest operation level parallelism with drastically different processor architecture and language support. In machines like Cedar [13], RP3 [19], Ul-tracomputer [11], and Alliant FX/80, loop-iteration level parallelism is exploited. These systems all have different architecture and system requirements. <p> In machines like Cedar [13], RP3 [19], Ul-tracomputer [11], and Alliant FX/80, loop-iteration level parallelism is exploited. These systems all have different architecture and system requirements. There have been some studies of parallelism for different granularities and constraints. Some of them need optimizing compilers for specific target machines <ref> [4, 14] </ref>, and others consider only unlimited resources or small kernels [16, 17]. In this paper we discuss some of those issues using real application programs. We concentrate primarily on numerical applications. A simulator called MaxPar is used to collect the necessary statistics for those programs. <p> I P E I L N A K C F O 2 T A K D benchmarks less than 4 except LINPACK). As the granularity becomes smaller, more parallelism can be exploited in a program <ref> [4] </ref>. These results confirm that we should move away from coarse grain parallelism to a finer grain parallelism. For example, Cray has moved away from macrotasking to support microtasking for better performance. However, we did not consider synchronization overhead here. <p> This technique allows us to ignore the limitation imposed by a paral-lelizing compiler and to study directly the "inherent" characteristics of a program. We study four granularity levels: operation level, statement level, loop-iteration level, and subprogram level. From the simulations, we confirm the findings of other studies <ref> [4, 16, 17] </ref> that as the granularity becomes smaller, more parallelism can be found in a program. However, our studied also shows that the performance cannot be determined by the amount of parallelism alone. It also depends on the synchronization overhead and the scheduling strategies required to support such parallelism.
Reference: [5] <author> D.-K. Chen. </author> <title> MaxPar: An Execution Driven Simulator for Studying Parallel Systems. </title> <institution> CSRD TR-917, Center for Supercomputing Research and Development, Univ. of Illinois at Urbana-Champaign, </institution> <month> Sep. </month> <year> 1989. </year>
Reference-contexts: Many of those parallelizing techniques are still at the research stage and are not available for use. To avoid these problems, we use a technique similar to [16] and [10] in MaxPar <ref> [5] </ref>. Instead of writing a simulator for a target system, simulation instructions are actually instrumented into application programs. The instrumented application programs are then executed with the simulation instructions to simulate and collect statistics for the target parallel system. <p> But if the true branch is taken, MaxPar will allow statement 4 to be executed before statement 2 because there is no data dependence from statement 3 to statement 4 (for more details, refer to [16] and <ref> [5] </ref>). Without loss of generality, MaxPar assumes 1 time unit for each operation. All other timing parameters (e.g., synchronization overhead) use that time as the base time unit. <p> The maximum inherent parallelism can thus be measured for a program with different grain sizes. We can also add delays into the timing computation to account for possible scheduling overhead <ref> [5] </ref>. When an unlimited number of processors are assumed, the obtained parallelism is the performance upper bound for that particular program assuming its maximum inherent parallelism can be fully exploited without considering scheduling strategies.
Reference: [6] <author> R. Cytron. </author> <title> Doacross: beyond vectorization for multiprocessors. </title> <booktitle> 1986 Int. Conf. on Parallel Processing, </booktitle> <pages> 836-845, </pages> <month> Aug. </month> <year> 1986. </year>
Reference-contexts: With efficient low-level data synchronization supports such as that used in Cedar [25], HEP [21], and Horizon [15], data dependences can be enforced explicitly, which allows a loop with cross iteration data dependences (a so-called Doacross loop <ref> [6] </ref>), to execute its iterations in parallel with some degree of overlap. It also allows iterations from different loops to be executed in parallel to achieve even higher performance. Such a technique is called high-level spreading [24], or task pipelining.
Reference: [7] <author> H. Dietz, T. Schwederski, M. O'keefe, and A. Za-afrani. </author> <title> Static synchronization beyond VLIW. </title> <booktitle> Supercomputing, </booktitle> <pages> 416-425, </pages> <month> Nov. </month> <year> 1989. </year>
Reference-contexts: No further overhead is needed within the task for the control dependences. 2.3 Barrier Synchronization Another way to enforce data dependences is to use barrier synchronization. Barrier synchronization is quite effective in loop-iteration level parallelism. It has been attempted in statement level parallelism as well <ref> [7] </ref>. Some optimizing compilers [1, 2] can generate barrier synchronization for parallel programs. A carefully placed barrier can eliminate a lot of inter-task data synchronization and its overhead.
Reference: [8] <author> Z. Fang, P. Yew, P. Tang, and C. Zhu. </author> <title> Dynamic processor self-scheduling for general parallel nested loops. </title> <booktitle> 1987 Int. Conf. Parallel Processing, </booktitle> <pages> 1-10, </pages> <month> Aug. </month> <year> 1987. </year>
Reference-contexts: The basic idea is to keep all processors as fully utilized as possible. The less frequently the processors are left idle, the better the scheduling strategy is. This strategy is particularly effective in the dynamic processor self-scheduling on parallel loops <ref> [8, 20] </ref>.
Reference: [9] <author> J. Fisher. </author> <title> Very long word instruction architecture and the ELI-512. </title> <booktitle> Int. Sym. Computer Architecture, </booktitle> <pages> 140-150, </pages> <month> June </month> <year> 1983. </year>
Reference-contexts: The kind of architectural support needed in exploiting a particular level of granularity and its synchronization and scheduling requirements can be determined only through studying those issues. People have been exploiting all possible levels of granularity, from fine grain to coarse grain. VLIW machines <ref> [9] </ref> and superscalar machines [12, 22] exploit operation level parallelism across basic blocks using either software or hardware techniques to schedule low level operations. Dataflow machines [4] exploit lowest operation level parallelism with drastically different processor architecture and language support. <p> A op B 3 1 A=5 False True B=C branching statement techniques such as trace scheduling <ref> [9] </ref>. For example, in Figure 2 statement 3 can be executed after the condition in statement 2 is resolved.
Reference: [10] <author> M. Flynn, C. Mitchell, and J. </author> <title> Mulder. And now a case for more complex instruction sets. </title> <booktitle> IEEE Computer, </booktitle> <pages> 71-83, </pages> <month> Sep. </month> <year> 1987. </year>
Reference-contexts: Many of those parallelizing techniques are still at the research stage and are not available for use. To avoid these problems, we use a technique similar to [16] and <ref> [10] </ref> in MaxPar [5]. Instead of writing a simulator for a target system, simulation instructions are actually instrumented into application programs. The instrumented application programs are then executed with the simulation instructions to simulate and collect statistics for the target parallel system.
Reference: [11] <author> A. Gottlieb, R. Grishman, C. Kruskal, K. McAuliffe, L. Rudolph, and M. Snir. </author> <title> The NYU Ultracomputer designing an MIMD shared memory parallel computer. </title> <journal> IEEE Trans. Comput., </journal> <pages> 175-189, </pages> <month> Feb. </month> <year> 1983. </year>
Reference-contexts: Dataflow machines [4] exploit lowest operation level parallelism with drastically different processor architecture and language support. In machines like Cedar [13], RP3 [19], Ul-tracomputer <ref> [11] </ref>, and Alliant FX/80, loop-iteration level parallelism is exploited. These systems all have different architecture and system requirements. There have been some studies of parallelism for different granularities and constraints.
Reference: [12] <author> N. Jouppi and D. Wall. </author> <title> Available instruction-level parallelism for superscalar and superpipelined machines. </title> <booktitle> Int. Conf. Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> 272-282, </pages> <month> Apr. </month> <year> 1989. </year>
Reference-contexts: The kind of architectural support needed in exploiting a particular level of granularity and its synchronization and scheduling requirements can be determined only through studying those issues. People have been exploiting all possible levels of granularity, from fine grain to coarse grain. VLIW machines [9] and superscalar machines <ref> [12, 22] </ref> exploit operation level parallelism across basic blocks using either software or hardware techniques to schedule low level operations. Dataflow machines [4] exploit lowest operation level parallelism with drastically different processor architecture and language support.
Reference: [13] <author> D. Kuck, E. Davidson, D. Lawrie, and A. Sameh. </author> <title> Parallel supercomputing today and the Cedar approach. </title> <journal> Science, </journal> <volume> 231 </volume> <pages> 967-974, </pages> <month> Feb. </month> <year> 1986. </year>
Reference-contexts: VLIW machines [9] and superscalar machines [12, 22] exploit operation level parallelism across basic blocks using either software or hardware techniques to schedule low level operations. Dataflow machines [4] exploit lowest operation level parallelism with drastically different processor architecture and language support. In machines like Cedar <ref> [13] </ref>, RP3 [19], Ul-tracomputer [11], and Alliant FX/80, loop-iteration level parallelism is exploited. These systems all have different architecture and system requirements. There have been some studies of parallelism for different granularities and constraints.
Reference: [14] <author> D. Kuck, A. Sameh, R. Cytron, A. Veidenbaum, C. Polychronopoulos, G. Lee, T. McDaniel, B. Leasure, C. Beckman, J. Davies, and C. Kruskal. </author> <title> The effects of program restructuring, algorithm change, and architecture choice on program performance. </title> <booktitle> 1984 Int. Conf. on Parallel Processing, </booktitle> <month> Aug. </month> <year> 1984. </year>
Reference-contexts: In machines like Cedar [13], RP3 [19], Ul-tracomputer [11], and Alliant FX/80, loop-iteration level parallelism is exploited. These systems all have different architecture and system requirements. There have been some studies of parallelism for different granularities and constraints. Some of them need optimizing compilers for specific target machines <ref> [4, 14] </ref>, and others consider only unlimited resources or small kernels [16, 17]. In this paper we discuss some of those issues using real application programs. We concentrate primarily on numerical applications. A simulator called MaxPar is used to collect the necessary statistics for those programs.
Reference: [15] <author> J. Kuehn and B. Smith. </author> <title> The Horizon supercomputing system: </title> <booktitle> architecture and software. Supercomputing, </booktitle> <pages> 28-34, </pages> <month> Nov. </month> <year> 1988. </year>
Reference-contexts: Barriers are quite effective in enforcing data dependences between different loops if they are used properly. However, they are not for enforcing data dependences between iterations within the same loop. With efficient low-level data synchronization supports such as that used in Cedar [25], HEP [21], and Horizon <ref> [15] </ref>, data dependences can be enforced explicitly, which allows a loop with cross iteration data dependences (a so-called Doacross loop [6]), to execute its iterations in parallel with some degree of overlap. It also allows iterations from different loops to be executed in parallel to achieve even higher performance.
Reference: [16] <author> M. Kumar. </author> <title> Effect of storage allocation/reclamation methods on parallelism and storage requirements. </title> <booktitle> Int. Symp. on Computer Architecture, </booktitle> <pages> 197-205, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: These systems all have different architecture and system requirements. There have been some studies of parallelism for different granularities and constraints. Some of them need optimizing compilers for specific target machines [4, 14], and others consider only unlimited resources or small kernels <ref> [16, 17] </ref>. In this paper we discuss some of those issues using real application programs. We concentrate primarily on numerical applications. A simulator called MaxPar is used to collect the necessary statistics for those programs. The techniques used in MaxPar are described in section 2. <p> Many of those parallelizing techniques are still at the research stage and are not available for use. To avoid these problems, we use a technique similar to <ref> [16] </ref> and [10] in MaxPar [5]. Instead of writing a simulator for a target system, simulation instructions are actually instrumented into application programs. The instrumented application programs are then executed with the simulation instructions to simulate and collect statistics for the target parallel system. <p> It can also produce real results from the application program to verify the correctness of a simulation. The basic idea is to associate each data element with several timing stamps and some accounting information, called "shadows" in <ref> [16] </ref>. One of the timing stamps is to record when a data element was last fetched; another is to record when the data element was last updated (see Figure 1). <p> But if the true branch is taken, MaxPar will allow statement 4 to be executed before statement 2 because there is no data dependence from statement 3 to statement 4 (for more details, refer to <ref> [16] </ref> and [5]). Without loss of generality, MaxPar assumes 1 time unit for each operation. All other timing parameters (e.g., synchronization overhead) use that time as the base time unit. <p> This technique allows us to ignore the limitation imposed by a paral-lelizing compiler and to study directly the "inherent" characteristics of a program. We study four granularity levels: operation level, statement level, loop-iteration level, and subprogram level. From the simulations, we confirm the findings of other studies <ref> [4, 16, 17] </ref> that as the granularity becomes smaller, more parallelism can be found in a program. However, our studied also shows that the performance cannot be determined by the amount of parallelism alone. It also depends on the synchronization overhead and the scheduling strategies required to support such parallelism.
Reference: [17] <author> A. Nicolau and J. Fisher. </author> <title> Using an oracle to measure potential parallelism in single instruction stream programs. </title> <booktitle> Annual Microprogramming Workshop, </booktitle> <pages> 171-182, </pages> <year> 1981. </year>
Reference-contexts: These systems all have different architecture and system requirements. There have been some studies of parallelism for different granularities and constraints. Some of them need optimizing compilers for specific target machines [4, 14], and others consider only unlimited resources or small kernels <ref> [16, 17] </ref>. In this paper we discuss some of those issues using real application programs. We concentrate primarily on numerical applications. A simulator called MaxPar is used to collect the necessary statistics for those programs. The techniques used in MaxPar are described in section 2. <p> This technique allows us to ignore the limitation imposed by a paral-lelizing compiler and to study directly the "inherent" characteristics of a program. We study four granularity levels: operation level, statement level, loop-iteration level, and subprogram level. From the simulations, we confirm the findings of other studies <ref> [4, 16, 17] </ref> that as the granularity becomes smaller, more parallelism can be found in a program. However, our studied also shows that the performance cannot be determined by the amount of parallelism alone. It also depends on the synchronization overhead and the scheduling strategies required to support such parallelism.
Reference: [18] <editor> The Perfect Club, et al. </editor> <title> The Perfect Club benchmarks: effective performance evaluation of supercomputers Int. </title> <editor> J. </editor> <booktitle> of Supercomputer Applications, </booktitle> <pages> 5-40, </pages> <month> Fall </month> <year> 1989. </year>
Reference-contexts: Four of them (QCD, FLO52Q, TRACK, and MDG) are from PERFECT CLUB benchmarks <ref> [18] </ref> which are used to measure the performance of supercomputers. The problem sizes of these programs are carefully chosen so that we can obtain the simulation results in a shorter time without losing important program characteristics. Those programs are listed in Table 1.
Reference: [19] <author> G. Pfister, W. Brantley, D. George, S. Harvey, W. Kleinfelder, K. McAuliffe, E. Melton, V. Norton, and J. Weiss. </author> <title> The IBM research parallel processor prototype (RP3): introduction and architecture. </title> <booktitle> 1985 Int. Conf. on Parallel Processing, </booktitle> <pages> 764-771, </pages> <month> Aug. </month> <year> 1985. </year>
Reference-contexts: VLIW machines [9] and superscalar machines [12, 22] exploit operation level parallelism across basic blocks using either software or hardware techniques to schedule low level operations. Dataflow machines [4] exploit lowest operation level parallelism with drastically different processor architecture and language support. In machines like Cedar [13], RP3 <ref> [19] </ref>, Ul-tracomputer [11], and Alliant FX/80, loop-iteration level parallelism is exploited. These systems all have different architecture and system requirements. There have been some studies of parallelism for different granularities and constraints.
Reference: [20] <author> C. Polychronopoulos and D. Kuck. </author> <title> Guided self-scheduling: a practical scheduling scheme for parallel supercomputers. </title> <journal> IEEE Trans. Computer, </journal> <pages> 1425-1439, </pages> <month> Dec. </month> <year> 1987. </year>
Reference-contexts: The basic idea is to keep all processors as fully utilized as possible. The less frequently the processors are left idle, the better the scheduling strategy is. This strategy is particularly effective in the dynamic processor self-scheduling on parallel loops <ref> [8, 20] </ref>.
Reference: [21] <author> B. J. Smith. </author> <title> A pipelined, shared resource mimd computer. </title> <booktitle> 1978 Int. Conf. on Parallel Processing, </booktitle> <pages> 6-8, </pages> <month> Aug. </month> <year> 1978. </year>
Reference-contexts: Barriers are quite effective in enforcing data dependences between different loops if they are used properly. However, they are not for enforcing data dependences between iterations within the same loop. With efficient low-level data synchronization supports such as that used in Cedar [25], HEP <ref> [21] </ref>, and Horizon [15], data dependences can be enforced explicitly, which allows a loop with cross iteration data dependences (a so-called Doacross loop [6]), to execute its iterations in parallel with some degree of overlap.
Reference: [22] <author> M. Smith, M. Johnson, and M. Horowitz. </author> <title> Limits on Multiple Instruction Issue. </title> <booktitle> Int. Conf. Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> 290-302, </pages> <month> Apr. </month> <year> 1989. </year>
Reference-contexts: The kind of architectural support needed in exploiting a particular level of granularity and its synchronization and scheduling requirements can be determined only through studying those issues. People have been exploiting all possible levels of granularity, from fine grain to coarse grain. VLIW machines [9] and superscalar machines <ref> [12, 22] </ref> exploit operation level parallelism across basic blocks using either software or hardware techniques to schedule low level operations. Dataflow machines [4] exploit lowest operation level parallelism with drastically different processor architecture and language support.
Reference: [23] <author> H. Su and P. Yew. </author> <title> On data synchronization for multiprocessors. </title> <booktitle> Int. Sym. Computer Architecture, </booktitle> <pages> 416-423, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: Because we assume tasks are our scheduling units and they can be assigned to different processors, if a data element is updated in one task and fetched in another, some kind of synchronization scheme is needed to enforce such an order. It is called data synchronization in <ref> [23] </ref>. If the granularity is at the operation level, every single data dependence needs to be explicitly enforced, possibly with a lot of synchronization overhead. But in the loop-iteration level parallelism, data dependences within the same iteration can be satisfied automatically because they are executed in the same processor sequentially.
Reference: [24] <author> A. Veidenbaum. </author> <title> Compiler Optimizations and Architecture Design Issues for Multiprocessors. </title> <type> Ph.D. Thesis, </type> <institution> Dept. of Computer Science, Univ. of Illinois at Urbana-Champaign, Champaign, </institution> <month> May </month> <year> 1985. </year>
Reference-contexts: It also allows iterations from different loops to be executed in parallel to achieve even higher performance. Such a technique is called high-level spreading <ref> [24] </ref>, or task pipelining. An example is shown in Figure 8, where high-level spreading allows Loop-i and Loop-j to be partially overlapped.
Reference: [25] <author> C. Zhu and P. Yew. </author> <title> A scheme to enforce data dependence on large multiprocessor systems. </title> <journal> IEEE Trans. Software Eng., </journal> <pages> 726-739, </pages> <month> June </month> <year> 1987. </year> <month> 10 </month>
Reference-contexts: Barriers are quite effective in enforcing data dependences between different loops if they are used properly. However, they are not for enforcing data dependences between iterations within the same loop. With efficient low-level data synchronization supports such as that used in Cedar <ref> [25] </ref>, HEP [21], and Horizon [15], data dependences can be enforced explicitly, which allows a loop with cross iteration data dependences (a so-called Doacross loop [6]), to execute its iterations in parallel with some degree of overlap.
References-found: 25

