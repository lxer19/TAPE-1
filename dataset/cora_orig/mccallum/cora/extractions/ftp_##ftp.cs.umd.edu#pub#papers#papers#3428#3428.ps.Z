URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3428/3428.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Email: fbkmoon, uysal, saltzg@cs.umd.edu  
Title: Index Translation Schemes for Adaptive Computations on Distributed Memory Multicomputers  
Author: Bongki Moon Mustafa Uysal Joel Saltz 
Address: College Park, MD 20742  
Affiliation: Institute for Advanced Computer Studies and Department of Computer Science University of Maryland  
Abstract: Current research in parallel programming is focused on closing the gap between globally indexed algorithms and the separate address spaces of processors on distributed memory multicomputers. A set of index translation schemes have been implemented as a part of CHAOS runtime support library, so that the library functions can be used for implementing a global index space across a collection of separate local index spaces. These schemes include two software-cached translation schemes aimed at adaptive irregular problems as well as a distributed translation table technique for statically irregular problems. To evaluate and demonstrate the efficiency of the software-cached translation schemes, experiments have been performed with an adaptively irregular loop kernel and a full-fledged 3D DSMC code from NASA Langley on the Intel Paragon and Cray T3D. This paper also discusses and analyzes the operational conditions under which each scheme can produce optimal performance.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Ray Barriuso and Allan Knies. </author> <title> Shmem user's guide. </title> <type> Report, </type> <institution> Cray Research, Inc., </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: The current implementation has used vendor-supplied message passing libraries. It should be noted, though, that the translation schemes have been further optimized using the low latency shared memory functions on the Cray T3D <ref> [1] </ref>. The shared memory functions copy blocks of data directly from one processor's memory to another. These shared memory functions remove a substantial amount of overhead for synchronization. The last two columns in Table 1 demonstrate the optimized performance obtained from Cray T3D over that from Intel Paragon.
Reference: [2] <author> T. J. Bartel and S. J. Plimpton. </author> <title> DSMC simulation of rarefied gas dynamics on a large hypercube supercomputer, </title> <booktitle> AIAA-92-2860. In Proceedings of the 27th AIAA Thermophysics Conference, </booktitle> <address> Nashville, TN, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: (34) 8.3 (25) 6.3 (19) 0.125 7.5 (22) 10.0 (28) 6.4 (20) 2.7 (09) 0.175 6.5 (20) 8.5 (25) 5.9 (18) 2.4 (08) applications such as upper-atmosphere flows for hypersonic cruise vehicles and rocket plumes, and in vacuum-related technologies for the semiconductor industry modelling plasma etching or chemical vapor deposition <ref> [2] </ref>. The DSMC method includes movement and collision handling of simulated particles on a spatial flow field domain overlaid by a Cartesian mesh. The spatial location of each particle is associated with a Cartesian mesh cell.
Reference: [3] <author> Graeme A. Bird. </author> <title> Molecular Gas Dynamics and the Direct Simulation of Gas Flows. </title> <publisher> Clarendon Press, Oxford, </publisher> <year> 1994. </year>
Reference-contexts: Then, since the index translation information stored in local memory can not be reused, the globally indexed data items should be dereferenced whenever the access patterns change. In adaptive applications such as DSMC <ref> [3] </ref> and CHARMM [5], data access patterns change frequently and irregular data distribution is preferred for better performance over regular data distribution. Thus, minimization of the dereferencing cost is crucial for efficient processing of such applications on distributed memory multicomputers. <p> DSMC is a well-established technique for modelling rarefied gas dynamics via direct particle simulation on a grid <ref> [3] </ref>. It has been widely used in aerospace 9 Table 1: Effects of replication factor in DSMC computation (the numbers in the parentheses are the percentage of the translation overhead) Times in Translation Schemes seconds Distributed Paged Hashed Hashed/T3D Rep.
Reference: [4] <author> F. Bodin, P. Beckman, D. Gannon, S. Yang, S. Kesavan, A. Malony, and B. Mohr. </author> <title> Implementing a parallel C++ runtime system for scalable parallel system. </title> <booktitle> In Proceedings Supercomputing '93, </booktitle> <pages> pages 588-597. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1993. </year>
Reference-contexts: Thus, current research in parallel programming is focused on closing the gap between globally indexed algorithms independent of the underlying distribution of data and the separate address spaces of processors. Compilers for various languages such as Fortran [8] and C++ <ref> [4] </ref> have been developed to give the illusion of a shared address space on distributed memory multicomputers. For structured problems, such compilers as Fortran D [8] use distribution directives to partition computation across processors.
Reference: [5] <author> B. R. Brooks and M. Hodoscek. </author> <title> Parallelization of CHARMM for MIMD machines. </title> <journal> Chemical Design Automation News, </journal> <volume> 7, </volume> <year> 1992. </year>
Reference-contexts: Then, since the index translation information stored in local memory can not be reused, the globally indexed data items should be dereferenced whenever the access patterns change. In adaptive applications such as DSMC [3] and CHARMM <ref> [5] </ref>, data access patterns change frequently and irregular data distribution is preferred for better performance over regular data distribution. Thus, minimization of the dereferencing cost is crucial for efficient processing of such applications on distributed memory multicomputers.
Reference: [6] <author> J. Lawrence Carter and Mark N. Wegman. </author> <title> Universal classes of hash functions. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 18 </volume> <pages> 143-154, </pages> <year> 1979. </year>
Reference-contexts: For this purpose, the current implementation of the hashed translation scheme allows the option of choosing a hash function from a universal 2 class of hash functions H 1 defined in <ref> [6] </ref>. It is 12 experimentally shown that for a given set of keys, by choosing functions at random from the class H 1 , the theoretically predicted performance of the hash functions can be achieved in practice, independent of the key distribution [11].
Reference: [7] <author> R. Das, D. J. Mavriplis, J. Saltz, S. Gupta, and R. Ponnusamy. </author> <title> The design and implementation of a parallel unstructured Euler solver using software primitives, </title> <booktitle> AIAA-92-0562. In Proceedings of the 30th Aerospace Sciences Meeting, </booktitle> <month> January </month> <year> 1992. </year>
Reference-contexts: Briefly outlined is how the distributed translation table can be used in the preprocessing stage of the inspector/executor model of parallelization <ref> [15, 7] </ref>. On distributed memory machines, large data arrays may not fit in a single-processor's memory, hence they are divided among processors. Also computational work is divided among individual processors to 2 achieve parallelism.
Reference: [8] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C. Tseng, and M. Wu. </author> <title> Fortran D language specification. </title> <institution> Department of Computer Science Technical Report TR90-141, Rice University, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: Thus, current research in parallel programming is focused on closing the gap between globally indexed algorithms independent of the underlying distribution of data and the separate address spaces of processors. Compilers for various languages such as Fortran <ref> [8] </ref> and C++ [4] have been developed to give the illusion of a shared address space on distributed memory multicomputers. For structured problems, such compilers as Fortran D [8] use distribution directives to partition computation across processors. <p> Compilers for various languages such as Fortran <ref> [8] </ref> and C++ [4] have been developed to give the illusion of a shared address space on distributed memory multicomputers. For structured problems, such compilers as Fortran D [8] use distribution directives to partition computation across processors. Using the directives, the compilers can statically determine the processor that owns a data item and the processor that requires the value of the data item.
Reference: [9] <author> Milan Milenkovic. </author> <title> Operating Systems Concepts and Design. </title> <publisher> McGraw-Hill, Inc., </publisher> <year> 1987. </year>
Reference-contexts: Since implementation of the well-known page replacement algorithm LRU (Least-Recently-Used) imposes too much overhead to be handled by software alone, implemented here is the NRU (Not-Recently-Used) page replacement algorithm, one of the approximations of LRU, using the reference counters in the page table <ref> [9] </ref>. 3.2 Hashed Translation Table The structure of a hashed translation table differs from that of a paged translation table in that a hashed translation table consists of a hash table and a set of hash nodes instead of a page table and a set of page frames.
Reference: [10] <author> Bongki Moon and Joel Saltz. </author> <title> Adaptive runtime support for direct simulation Monte Carlo methods on distributed memory architectures. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference (SHPCC-94), </booktitle> <pages> pages 176-183, </pages> <address> Knoxville, TN, May 1994. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: Furthermore, since the computations associated with performing probabilistic chemistry and collisions can be distributed across processors cell by cell, the DSMC method in principle is a good match for parallel processing on distributed memory multicomputers <ref> [16, 10] </ref>. Changes in position coordinates may cause the particles to move across cell boundaries. In the 10 particular corner flow DSMC code presented here, about 30 percent of the particles change their cell locations every time step.
Reference: [11] <author> M. V. Ramakrishna. </author> <title> Hashing in practice, analysis of hashing and universal hashing. </title> <booktitle> In Proceedings of the 1988 ACM SIGMOD, </booktitle> <pages> pages 191-199, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: It is 12 experimentally shown that for a given set of keys, by choosing functions at random from the class H 1 , the theoretically predicted performance of the hash functions can be achieved in practice, independent of the key distribution <ref> [11] </ref>. These translation schemes have been implemented as a part of the CHAOS runtime support library on various distributed memory multicomputers such as Intel Paragon, IBM SP-1/2, Thinking Machine CM-5 and Cray T3D. The current implementation has used vendor-supplied message passing libraries.
Reference: [12] <author> Steven K. Reinhardt, James R. Larus, and David A. Wood. Tempest and Typhoon: </author> <title> User-level shared memory. </title> <booktitle> In Proceedings of the 21th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 325-336. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> April </month> <year> 1994. </year>
Reference-contexts: edges y (ia (i)) = 0.85 * x (ia (i)) + 0.42 * x (ib (i)) enddo L3: do i = 1, n grids x (i) = y (i) enddo enddo approach called Distributed Shared Memory (DSM) enables an application's user-level code to support shared memory and message passing efficiently <ref> [12] </ref>. Distributed shared memory is typically supported by the processor's address translation hardware. This paper describes and evaluates a set of index translation schemes for implementing a global index space across a collection of distributed memories. <p> These schemes can also be incorporated into distributed shared memory systems such as the one used in the Wisconsin Wind Tunnel project to support user-level shared memory <ref> [12] </ref>. To illustrate the need of runtime support for address translation, consider the Jacobi iterative method for solving a partial differential equation on an irregular numerical grid, which arises in molecular dynamics codes and sparse linear solvers.
Reference: [13] <editor> J. Saltz et al. </editor> <title> A manual for the CHAOS runtime library. </title> <type> Technical report, </type> <institution> University of Maryland, Department of Computer Science and UMIACS, </institution> <year> 1993. </year>
Reference-contexts: A dereferencing operation using the distributed translation table requires communication between processors to exchange the information stored in each processor's portion of the distributed translation table. library <ref> [13] </ref>.
Reference: [14] <author> Joel Saltz, Harry Berryman, and Janet Wu. </author> <title> Multiprocessors and runtime compilation. </title> <type> Technical Report 90-59, </type> <institution> ICASE, NASA Langley Research Center, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: These schemes can be incorporated into a runtime support library so that calls to the library functions can be invoked by manually parallelized programs or can be generated by compilers <ref> [14] </ref>. These schemes can also be incorporated into distributed shared memory systems such as the one used in the Wisconsin Wind Tunnel project to support user-level shared memory [12].
Reference: [15] <author> Joel Saltz, Kathleen Crowley, Ravi Mirchandaney, and Harry Berryman. </author> <title> Run-time scheduling and execution of loops on message passing machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8(4) </volume> <pages> 303-312, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: Briefly outlined is how the distributed translation table can be used in the preprocessing stage of the inspector/executor model of parallelization <ref> [15, 7] </ref>. On distributed memory machines, large data arrays may not fit in a single-processor's memory, hence they are divided among processors. Also computational work is divided among individual processors to 2 achieve parallelism.
Reference: [16] <author> Richard G. Wilmoth. </author> <title> Direct simulation Monte Carlo analysis of rarefied flows on parallel processors. </title> <journal> AIAA Journal of Thermophysics and Heat Transfer, </journal> <volume> 5(3) </volume> <pages> 292-300, </pages> <month> July-Sept. </month> <year> 1991. </year> <month> 14 </month>
Reference-contexts: Furthermore, since the computations associated with performing probabilistic chemistry and collisions can be distributed across processors cell by cell, the DSMC method in principle is a good match for parallel processing on distributed memory multicomputers <ref> [16, 10] </ref>. Changes in position coordinates may cause the particles to move across cell boundaries. In the 10 particular corner flow DSMC code presented here, about 30 percent of the particles change their cell locations every time step.
References-found: 16

