URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/reports/91/tr1051a.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/reports/91/
Root-URL: http://www.cs.wisc.edu
Title: A Unified Formalization of Four Shared-Memory Models  
Author: Sarita V. Adve Mark D. Hill 
Keyword: Index Terms: data-race-free-0, data-race-free-1, memory model, release consistency, sequential consistency, shared-memory multiprocessor, weak ordering.  
Note: This work was supported in part by a National Science Foundation Presidential Young Investigator Award (MIPS-8957278) with matching funds from A.T. T. Bell Laboratories, Cray Research Foundation, and Digital Equipment Corporation. Sarita Adve is also supported by an IBM graduate fellowship. 1  
Address: Madison, Wisconsin 53706  
Date: #1051, September 1991, Revised September 1992  
Affiliation: Computer Sciences  Department of Computer Sciences University of Wisconsin  
Pubnum: Technical Report  
Abstract: This paper presents a shared-memory model, data-race-free-1, that unifies four earlier models: weak ordering, release consistency (with sequentially consistent special operations), the VAX memory model, and data-race-free-0. The most intuitive and commonly assumed shared-memory model, sequential consistency, limits performance. The models of weak ordering, release consistency, the VAX, and data-race-free-0 are based on the common intuition that if programs synchronize explicitly and correctly, then sequential consistency can be guaranteed with high performance. However, each model formalizes this intuition differently and has different advantages and disadvantages with respect to the other models. Data-race-free-1 unifies the models of weak ordering, release consistency, the VAX, and data-race-free-0 by formalizing the above intuition in a manner that retains the advantages of each of the four models. A multiprocessor is data-race-free-1 if it guarantees sequential consistency to data-race-free programs. Data-race-free-1 unifies the four models by providing a programmer's interface similar to the four models, and by allowing all implementations allowed by the four models. Additionally, data-race-free-1 expresses the programmer's interface more explicitly and formally than weak ordering and the VAX, and allows an implementation not allowed by weak ordering, release consistency, or data-race-free-0. The new implementation proposal for data-race-free-1 differs from earlier implementations mainly by permitting the execution of all synchronization operations of a processor even while previous data operations of the processor are in progress. To ensure sequential consistency, two synchronizing processors exchange information to delay later operations of the second processor that conflict with an incomplete data operation of the first processor. 1992 IEEE. Used with permission. This paper is scheduled for publication in an upcoming issue of IEEE Transactions on Parallel and Distributed Systems. Reuse of this material is strictly prohibited without the express written consent of the authors and The Institute of Electrical and Electronics Engineers, Inc. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> S. V. ADVE and M. D. HILL, </author> <title> Weak Ordering ANew Definition, </title> <booktitle> Proc. 17th Annual Intl. Symp. on Computer Architecture, </booktitle> <month> May </month> <year> 1990, </year> <pages> 2-14. </pages>
Reference-contexts: Gharachorloo et al. have described hhhhhhhhhhhhhhhhhh 1. An earlier version of this work appears in the Proceedings of the 17th Annual International Symposium on Computer Architecture, June 1990 <ref> [1] </ref>. The data-race-free-1 memory model developed in this paper extends the data-race-free-0 model of [1] by distinguishing unpaired synchronization operations from paired release and acquire synchronization operations. <p> Gharachorloo et al. have described hhhhhhhhhhhhhhhhhh 1. An earlier version of this work appears in the Proceedings of the 17th Annual International Symposium on Computer Architecture, June 1990 <ref> [1] </ref>. The data-race-free-1 memory model developed in this paper extends the data-race-free-0 model of [1] by distinguishing unpaired synchronization operations from paired release and acquire synchronization operations. The definition of data-race-free-1 in Section 2 uses the notions of how different operations are distinguished, when the distinction is correct, the synchronization-order-1 and happens-before-1 relations, and data races. <p> This dichotomy hhhhhhhhhhhhhhhhhh 2. Figure 1 is a modified version of Figure 1 in <ref> [1] </ref> and is presented with the permission of the IEEE. - 3 - - -- between memory operations is the motivation behind the four models of weak ordering [9], release consistency with sequentially consistent special operations (henceforth called release consistency) [11], the VAX [8], and data-race-free-0 (originally called weak ordering with <p> with the permission of the IEEE. - 3 - - -- between memory operations is the motivation behind the four models of weak ordering [9], release consistency with sequentially consistent special operations (henceforth called release consistency) [11], the VAX [8], and data-race-free-0 (originally called weak ordering with respect to data-race-free-0) <ref> [1] </ref>. Although the four memory models are very similar, small differences in their formalization lead to differences in the way they satisfy the above two properties. Weak ordering [9] and release consistency [11] restrict hardware to actually execute specific memory operations in program order. <p> Before accessing shared writable data, the programmer must acquire control of the data structure. Seven instructions are provided to permit interlocked access to a control variable.'' Data-race-free-0 <ref> [1] </ref> states that sequential consistency will be provided to data-race-free programs. A data-race-free program (discussed formally in Sections 2 and 3) distinguishes between synchronization operations and data operations and ensures that conflicting data operations do not race (i.e., cannot execute concurrently). <p> Section 6 concludes the paper. 2. The Data-Race-Free-1 Memory Model Section 2.1 first clarifies common terminology that will be used throughout the paper and then informally motivates the data-race-free-1 memory model. Section 2.2 gives the formal definition of data-race-free-1. Data-race-free-1 is an extension of our earlier model data-race-free-0 <ref> [1] </ref>. - 5 - - -- 2.1. Terminology and Motivation for Data-Race-Free-1 The rest of the paper assumes the following terminology. The terms system, program, and operations (as in definition 1.1 of sequential consistency) can be used at several levels. <p> Two memory operations conflict if at least one of them is a write and they access the same location [27]. The motivation for data-race-free-1, which is similar to that for weak ordering, release consistency, the VAX model and data-race-free-0, is based on the following observations made in <ref> [1] </ref>. 3 Assuming processors maintain uniprocessor data and control dependencies, sequential consistency can be violated only when two or more processors interact through memory operations on common locations. These interactions can be classified as data memory operations and synchronization memory operations. <p> The write due to an unset is paired with the test that returns the unset value; the unset write is a release operation hhhhhhhhhhhhhhhhhh 3. The observations are paraphrased from <ref> [1] </ref> with the permission of the IEEE. - 6 - - -- and the test read is an acquire operation because the unset value returned by the test is used to conclude the completion of the memory operations of the previous invocation of the critical section. <p> However, programmers using high-level parallel programming languages can use data-race-free-1 by extending the definition of data-race-free to high-level programs (as discussed for data-race-free-0 in <ref> [1] </ref>). The extension is straightforward, but requires high-level parallel languages to provide special constructs for synchronization, e.g., semaphores, monitors, fork-joins, and task rendezvous. Data-race-free-1 does not place any restrictions on the high-level synchronization mechanisms. <p> Data-race-free-0 and release consistency provide a formal interface for programmers. Data-race-free-1 provides a similar interface with a few minor differences. The programs for which data-race-free-0 ensures sequential consistency are also called data-race-free programs <ref> [1] </ref>. The difference is that data-race-free-0 does not distinguish between different synchronization operations; it effectively pairs all conflicting synchronization operations depending on the order in which they execute. This distinction does not significantly affect programmers, but can be exploited by hardware designers. <p> Previous work discusses - 11 - - -- how the requirement of data-race-free programs for all the above models is not very restrictive for programmers <ref> [1, 11] </ref>, and how data races [2] or violations of sequential consistency due to data races [14] may be dynamically detected with these models. 4. Data-Race-Free-1 vs. <p> to all data-race-free programs; g all implementations of data-race-free-0 obey data-race-free-1 because, again as discussed in Section 3, all implementations of data-race-free-0 ensure sequential consistency to all data-race-free programs; g all implementations of weak ordering obey data-race-free-1 because our earlier work shows that all implementations of weak ordering obey data-race-free-0 <ref> [1] </ref>, and from the above argument, all implementations of data-race-free-0 obey data-race-free-1; g data-race-free-1 formalizes the VAX model; therefore, all implementations of the VAX model obey data race-free-1. 4.2.
Reference: 2. <author> S. V. ADVE, M. D. HILL, B. P. MILLER and R. H. B. NETZER, </author> <title> Detecting Data Races on Weak Memory Systems, </title> <booktitle> Proc. 18th Annual Intl. Symp. on Computer Architecture, </booktitle> <month> May </month> <year> 1991, </year> <pages> 234-243. </pages>
Reference-contexts: These notions are extensions of similar concepts developed for data-race-free-0. Also, in parallel with our work on this paper, we published a technique for detecting data races on a data-race-free-1 system <ref> [2] </ref>. Consequently, [2] reviews data races and the data-race-free-1 memory model, and contains the definitions (in slightly different form) of Section 2. <p> These notions are extensions of similar concepts developed for data-race-free-0. Also, in parallel with our work on this paper, we published a technique for detecting data races on a data-race-free-1 system <ref> [2] </ref>. Consequently, [2] reviews data races and the data-race-free-1 memory model, and contains the definitions (in slightly different form) of Section 2. <p> Previous work discusses - 11 - - -- how the requirement of data-race-free programs for all the above models is not very restrictive for programmers [1, 11], and how data races <ref> [2] </ref> or violations of sequential consistency due to data races [14] may be dynamically detected with these models. 4. Data-Race-Free-1 vs.
Reference: 3. <author> S. V. ADVE and M. D. HILL, </author> <title> Sufficient Conditions for Implementing the Data-Race-Free-1 Memory Model, </title> <type> Computer Sciences Technical Report #1107, </type> <institution> University of Wisconsin, Madison, </institution> <month> September </month> <year> 1992. </year>
Reference-contexts: In the absence of control flow operations (such as branches), the above is automatically ensured. In the presence of control flow operations, however, an extra requirement, called the control requirement, is needed to ensure the above <ref> [3] </ref>. Weak ordering, release consistency, and all proposed implementations of data-race-free-0 satisfy the synchronization requirement explicitly and the control requirement implicitly (by requiring ``uniprocessor control dependencies'' to be maintained). <p> Since the key difference between implementations of the earlier models and the new implementation of data-race-free-1 is in the data requirement, the following describes an implementation proposal only for the data requirement conditions. In <ref> [3] </ref>, we formalize the above three requirements and give explicit conditions for the synchronization and control requirements. <p> A conservative way to satisfy the control requirement is for a processor to also block on a read that controls program flow until the read completes. Note that further optimizations on the data requirement conditions and on the implementation of the following section are possible <ref> [3] </ref>. For example, for the release-acquire condition, the acquire can complete even while operations transferred to the releasing processor are incomplete, as long as the releasing processor transfers the identity of those incomplete operations to the acquiring processor.
Reference: 4. <author> A. AGARWAL, R. SIMONI, M. HOROWITZ and J. HENNESSY, </author> <title> An Evaluation of Directory Schemes for Cache Coherence, </title> <booktitle> Proc. 15th Annual Intl. Symp. on Computer Architecture, </booktitle> <address> Honolulu, Hawaii, </address> <month> June </month> <year> 1988, </year> <pages> 280-289. </pages>
Reference-contexts: The proposal also assumes a directory-based, writeback, invalidation, ownership, hardware cache-coherence protocol, similar in most respects to those discussed by Agarwal et al. <ref> [4] </ref>.
Reference: 5. <author> M. AHAMAD, P. W. HUTTO and R. JOHN, </author> <title> Implementing and Programming Causal Distributed Shared Memory, </title> <institution> College of Computing Technical Report GIT-CC-90-49, Georgia Institute of Technology, </institution> <month> November </month> <year> 1990. </year>
Reference-contexts: After a value written by (say) processor P i is read, the values of earlier conflicting writes by P i cannot be returned. The causal memory model <ref> [5, 18] </ref> ensures that any write that causally precedes a read is observed by the read. Causal precedence is a transitive relation established by program order or due to a read that returns the value of a write.
Reference: 6. <author> J. ARCHIBALD and J. BAER, </author> <title> Cache Coherence Protocols: Evaluation Using a Multiprocessor Simulation Model, </title> <journal> ACM Trans. on Computer Systems 4, </journal> <month> 4 (November </month> <year> 1986), </year> <pages> 273-298. </pages>
Reference-contexts: Systems with general interconnection networks without caches The execution is possible even if processors issue memory operations in program order, if the operations reach memory modules in a different order [21]. Shared-bus systems with caches Even with a cache coherence protocol <ref> [6] </ref>, the execution is possible if processors issue memory operations out-of-order or allow reads to pass writes in write buffers.
Reference: 7. <author> R. DELEONE and O. L. MANGASARIAN, </author> <title> Asynchronous Parallel Successive Overrelaxation for the Symmetric Linear Complementarity Problem, </title> <booktitle> Mathematical Programming 42, </booktitle> <year> 1988, </year> <pages> 347-361. </pages>
Reference-contexts: Also note that in programs based on asynchronous algorithms <ref> [7] </ref>, some operations access data, but are not ordered by synchronization. <p> A potential disadvantage of data-race-free-1 relative to weak ordering and release consistency is for programmers of asynchronous algorithms that do not rely on sequential consistency for correctness <ref> [7] </ref>. Weak ordering and release consistency provide such programmers the option of reasoning with their explicit hardware conditions and writing programs that are not data-race-free, but work correctly and possibly faster. Data-race-free-1 is based on the assumption that programmers prefer to reason with sequential consistency.
Reference: 8. <author> DEC., </author> <title> VAX Architecture Handbook, </title> <year> 1981. </year>
Reference-contexts: a modified version of Figure 1 in [1] and is presented with the permission of the IEEE. - 3 - - -- between memory operations is the motivation behind the four models of weak ordering [9], release consistency with sequentially consistent special operations (henceforth called release consistency) [11], the VAX <ref> [8] </ref>, and data-race-free-0 (originally called weak ordering with respect to data-race-free-0) [1]. Although the four memory models are very similar, small differences in their formalization lead to differences in the way they satisfy the above two properties. <p> For example, it distinguishes synchronization operations from ordinary data operations. The VAX and data-race-free-0 models differ from weak ordering and release consistency by avoiding explicit restrictions on the actual order of execution of specific memory operations. In the VAX architecture handbook <ref> [8] </ref>, the data sharing and synchronization section states the following. ``Accesses to explicitly shared data that may be written must be synchronized. Before accessing shared writable data, the programmer must acquire control of the data structure.
Reference: 9. <author> M. DUBOIS, C. SCHEURICH and F. A. BRIGGS, </author> <title> Memory Access Buffering in Multiprocessors, </title> <booktitle> Proc. 13th Annual Intl. Symp. on Computer Architecture 14, </booktitle> <month> 2 (June </month> <year> 1986), </year> <pages> 434-442. </pages>
Reference-contexts: This dichotomy hhhhhhhhhhhhhhhhhh 2. Figure 1 is a modified version of Figure 1 in [1] and is presented with the permission of the IEEE. - 3 - - -- between memory operations is the motivation behind the four models of weak ordering <ref> [9] </ref>, release consistency with sequentially consistent special operations (henceforth called release consistency) [11], the VAX [8], and data-race-free-0 (originally called weak ordering with respect to data-race-free-0) [1]. <p> Although the four memory models are very similar, small differences in their formalization lead to differences in the way they satisfy the above two properties. Weak ordering <ref> [9] </ref> and release consistency [11] restrict hardware to actually execute specific memory operations in program order. <p> A processor sees a read when the read returns its value. These notions are similar to those of ``performed with respect to a processor'' and ``performed'' <ref> [9] </ref>.) - 14 - - -- The following gives three requirements (data, synchronization, and control) that are together sufficient for hardware to satisfy the data-race-free-1 conditions, and therefore to obey data-race-free-1. <p> For conflicting operations from the same processor, it is sufficient to maintain intra-processor data dependencies. The conditions below assume these are maintained. In the rest of this section, preceding and following refer to the ordering by program order. An operation, either synchronization or data, completes (or performs <ref> [9] </ref>) when it is seen (as defined above) by all processors. Data Requirement Conditions: Let Rel and Acq be release and acquire operations issued by processors P rel and P acq respectively. Let Rel and Acq be paired with each other.
Reference: 10. <author> M. DUBOIS and C. SCHEURICH, </author> <title> Memory Access Dependencies in Shared-Memory Multiprocessor, </title> <journal> IEEE Trans. on Software Engineering SE-16, </journal> <month> 6 (June </month> <year> 1990), </year> <pages> 660-673. </pages>
Reference-contexts: For programmers, the authors of weak ordering later stated that mutual exclusion should be ensured for each access to a shared variable by using constructs such as critical sections, which are implemented with hardware-recognizable synchronization operations <ref> [10, 26] </ref>. The authors of release consistency formalize a group of programs called properly labeled programs, for which release consistency ensures sequential consistency. A properly labeled program distinguishes its memory operations depending on their use. For example, it distinguishes synchronization operations from ordinary data operations.
Reference: 11. <author> K. GHARACHORLOO, D. LENOSKI, J. LAUDON, P. GIBBONS, A. GUPTA and J. HENNESSY, </author> <title> Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors, </title> <booktitle> Proc. 17th Annual Intl. Symp. on Computer Architecture, </booktitle> <month> May </month> <year> 1990, </year> <pages> 15-26. </pages>
Reference-contexts: Figure 1 is a modified version of Figure 1 in [1] and is presented with the permission of the IEEE. - 3 - - -- between memory operations is the motivation behind the four models of weak ordering [9], release consistency with sequentially consistent special operations (henceforth called release consistency) <ref> [11] </ref>, the VAX [8], and data-race-free-0 (originally called weak ordering with respect to data-race-free-0) [1]. Although the four memory models are very similar, small differences in their formalization lead to differences in the way they satisfy the above two properties. Weak ordering [9] and release consistency [11] restrict hardware to actually <p> (henceforth called release consistency) <ref> [11] </ref>, the VAX [8], and data-race-free-0 (originally called weak ordering with respect to data-race-free-0) [1]. Although the four memory models are very similar, small differences in their formalization lead to differences in the way they satisfy the above two properties. Weak ordering [9] and release consistency [11] restrict hardware to actually execute specific memory operations in program order. For programmers, the authors of weak ordering later stated that mutual exclusion should be ensured for each access to a shared variable by using constructs such as critical sections, which are implemented with hardware-recognizable synchronization operations [10, 26]. <p> Additionally, synchronization operations can be characterized as paired acquire and release synchronization operations or as unpaired synchronization operations as follows. (The characterization is similar to that used for properly labeled programs for release consistency <ref> [11] </ref>; Section 3 discusses the differences.) In an execution, consider a write and a read synchronization operation to the same location, where the read returns the value of the write, and the value is used by the reading processor to conclude the completion of all memory operations of the writing processor <p> This distinction does not significantly affect programmers, but can be exploited by hardware designers. The programs for which release consistency ensures sequential consistency are called properly labeled programs <ref> [11] </ref>. All data-race-free programs are properly labeled, but there are some properly labeled programs that are not data-race-free (as defined by Definition 2.4) [15]. The difference is minor and arises because properly labeled programs have a less explicit notion of pairing. <p> Previous work discusses - 11 - - -- how the requirement of data-race-free programs for all the above models is not very restrictive for programmers <ref> [1, 11] </ref>, and how data races [2] or violations of sequential consistency due to data races [14] may be dynamically detected with these models. 4. Data-Race-Free-1 vs. <p> Before executing a serialization operation, a processor completes all operations that are before the serialization operation according to program order. Before executing any nonserialization operation, a processor completes all serialization operations that are before that nonserializa-tion operation according to program order. The processor consistency <ref> [11, 16] </ref>, PRAM [22] and total store ordering [25] models ensure that writes of a given processor appear to execute in the same order to all other processors. The models mainly differ in whether a write appears to become visible to all other processors simultaneously or at different times. <p> The partial store ordering model [25] is similar to total store ordering except that it orders writes by a processor only if they are separated by a store barrier operation. The model known as release consistency with processor-consistent special operations <ref> [11] </ref> is similar to release consistency with sequentially consistent special operations except that it requires special operations (syncs and nsyncs) to be processor-consistent.
Reference: 12. <author> K. GHARACHORLOO, A. GUPTA and J. HENNESSY, </author> <title> Two Techniques to Enhance the Performance of Memory Consistency Models, </title> <booktitle> Proc. Intl. Conf. on Parallel Processing, </booktitle> <year> 1991, </year> <month> I355-I364. </month>
Reference-contexts: This material is used in Section 2 with the permission of the ACM. - 2 - - -- mechanisms that allow these optimizations to be used with the sequential consistency model, but the mechanisms require hardware support for prefetching and rollback <ref> [12] </ref>. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh Initially X = Y = 0 P 1 P 2 r1 = Y r2 = X Result: r1 = r2 = 0 2 X and Y are shared variables and r1 and r2 are local registers. <p> Recently, an implementation of release consistency has been proposed that uses a rollback mechanism to let a processor conditionally execute its reads following its acquire (such as P 1 's Test&Set) before the acquire completes <ref> [12] </ref>; our optimization will benefit such implementations also because it allows the writes following the acquire to be issued and completed earlier, and lets the reads following the acquire to be committed earlier.
Reference: 13. <author> K. GHARACHORLOO, A. GUPTA and J. HENNESSY, </author> <title> Performance Evaluation of Memory Consistency Models for Shared-Memory Multiprocessors, </title> <booktitle> Proc. 4th Intl. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <year> 1991, </year> <pages> 245-257. </pages>
Reference-contexts: These optimizations significantly improve performance and will become increasingly important in the future as processor cycle times decrease and memory latencies increase <ref> [13] </ref>. Gharachorloo et al. have described hhhhhhhhhhhhhhhhhh 1. An earlier version of this work appears in the Proceedings of the 17th Annual International Symposium on Computer Architecture, June 1990 [1].
Reference: 14. <author> K. GHARACHORLOO and P. B. GIBBONS, </author> <title> Detecting Violations of Sequential Consistency, </title> <booktitle> Proc. Symp. on Parallel Algorithms and Architectures, </booktitle> <month> July </month> <year> 1991, </year> <pages> 316-326. </pages>
Reference-contexts: Previous work discusses - 11 - - -- how the requirement of data-race-free programs for all the above models is not very restrictive for programmers [1, 11], and how data races [2] or violations of sequential consistency due to data races <ref> [14] </ref> may be dynamically detected with these models. 4. Data-Race-Free-1 vs. Weak Ordering, Release Consistency, the VAX Model, and Data-Race-Free-0 for Hardware Designers This section compares data-race-free-1 to weak ordering, release consistency, the VAX model, and data-race-free-0 from a hardware designer's viewpoint.
Reference: 15. <author> P. B. GIBBONS, M. MERRITT and K. GHARACHORLOO, </author> <title> Proving Sequential Consistency of High-Performance Shared Memories, </title> <booktitle> Proc. Symp. on Parallel Algorithms and Architectures, </booktitle> <month> July </month> <year> 1991, </year> <pages> 292-303. </pages>
Reference-contexts: The programs for which release consistency ensures sequential consistency are called properly labeled programs [11]. All data-race-free programs are properly labeled, but there are some properly labeled programs that are not data-race-free (as defined by Definition 2.4) <ref> [15] </ref>. The difference is minor and arises because properly labeled programs have a less explicit notion of pairing. They allow conflicting data operations to be ordered by operations (nsyncs) that correspond to the nonpairable synchronization operations of data-race-free-1.
Reference: 16. <author> J. R. GOODMAN, </author> <title> Cache Consistency and Sequential Consistency, </title> <type> Computer Sciences Technical Report #1006, </type> <institution> Univ. of Wisconsin, Madison, </institution> <month> February </month> <year> 1991. </year> <month> - 23 </month> - - -- 
Reference-contexts: Before executing a serialization operation, a processor completes all operations that are before the serialization operation according to program order. Before executing any nonserialization operation, a processor completes all serialization operations that are before that nonserializa-tion operation according to program order. The processor consistency <ref> [11, 16] </ref>, PRAM [22] and total store ordering [25] models ensure that writes of a given processor appear to execute in the same order to all other processors. The models mainly differ in whether a write appears to become visible to all other processors simultaneously or at different times.
Reference: 17. <author> A. GOTTLIEB, R. GRISHMAN, C. P. KRUSKAL, K. P. MCAULIFFE, L. RUDOLPH and M. SNIR, </author> <title> The NYU Ultracomputer - Designing an MIMD Shared Memory Parallel Computer, </title> <journal> IEEE Trans. on Computers, </journal> <month> February </month> <year> 1983, </year> <pages> 175-189. </pages>
Reference-contexts: The executions occur on sequentially consistent hardware and their operations execute in the order shown. op,x denotes an operation op on location x. DataRead and DataWrite denote data operations. The Test&Set and Fetch&Inc <ref> [17] </ref> instructions are defined to be atomic instructions. Their read and write operations are represented together as Test&Set,x or Fetch&Inc,x. Paired operations are connected with arrows. location to the value 0. A write due to an Unset and a read due to a Test&Set are pairable.
Reference: 18. <author> P. W. HUTTO and M. AHAMAD, </author> <title> Slow Memory: Weakening Consistency to Enhance Concurrency in Distributed Shared Memories, </title> <booktitle> Proc. 10th Intl. Conf. on Distributed Computing Systems, </booktitle> <year> 1990, </year> <pages> 302-311. </pages>
Reference-contexts: The concurrent-consistency model [26] ensures sequential consistency to all programs except those ``which explicitly test for sequential consistency or take access timings into consideration.'' The slow memory model <ref> [18] </ref> requires that a read return the value of some previous conflicting write. After a value written by (say) processor P i is read, the values of earlier conflicting writes by P i cannot be returned. <p> After a value written by (say) processor P i is read, the values of earlier conflicting writes by P i cannot be returned. The causal memory model <ref> [5, 18] </ref> ensures that any write that causally precedes a read is observed by the read. Causal precedence is a transitive relation established by program order or due to a read that returns the value of a write.
Reference: 19. <author> IBM, </author> <title> IBM System/370 Principles of Operation, Publication Number GA22-7000-9, File Number S370-01, </title> <month> May </month> <year> 1983. </year>
Reference-contexts: This section first summarizes other memory models proposed in the literature, and then examines how data-race-free-1 relates to them. - 20 - - -- The IBM 370 memory model <ref> [19] </ref> guarantees that except for a write followed by a read to a different location, operations of a single processor will appear to execute in program order, and writes will appear to execute atomically. The 370 also provides serialization operations.
Reference: 20. <author> D. KROFT, </author> <title> Lockup-Free Instruction Fetch/Prefetch Cache Organization, </title> <booktitle> Proc. Eighth Symp. on Computer Architecture, </booktitle> <month> May </month> <year> 1981, </year> <pages> 81-87. </pages>
Reference-contexts: Figure 1 shows that in multiprocessor systems, both with and without caches, common uniprocessor hardware optimizations, such as write buffers, overlapped memory operations, out-of-order memory operations, and lockup-free caches <ref> [20] </ref>, can violate sequential consistency. These optimizations significantly improve performance and will become increasingly important in the future as processor cycle times decrease and memory latencies increase [13]. Gharachorloo et al. have described hhhhhhhhhhhhhhhhhh 1.
Reference: 21. <author> L. LAMPORT, </author> <title> How to Make a Multiprocessor Computer That Correctly Executes Multiprocess Programs, </title> <journal> IEEE Trans. on Computers C-28, </journal> <month> 9 (September </month> <year> 1979), </year> <pages> 690-691. </pages>
Reference-contexts: This model of uniprocessor memory, therefore, has the advantage of simplicity and yet allows for high performance optimizations. The most commonly (and often implicitly) assumed memory model for shared-memory multiprocessor systems is sequential consistency, formalized by Lamport <ref> [21] </ref> as follows. <p> Systems with general interconnection networks without caches The execution is possible even if processors issue memory operations in program order, if the operations reach memory modules in a different order <ref> [21] </ref>. Shared-bus systems with caches Even with a cache coherence protocol [6], the execution is possible if processors issue memory operations out-of-order or allow reads to pass writes in write buffers.
Reference: 22. <author> R. J. LIPTON and J. S. SANDBERG, </author> <title> PRAM: A Scalable Shared Memory, </title> <type> Technical Report CS-Tech. </type> <institution> Rep.-180-88, Princeton University, </institution> <month> September </month> <year> 1988. </year>
Reference-contexts: Before executing a serialization operation, a processor completes all operations that are before the serialization operation according to program order. Before executing any nonserialization operation, a processor completes all serialization operations that are before that nonserializa-tion operation according to program order. The processor consistency [11, 16], PRAM <ref> [22] </ref> and total store ordering [25] models ensure that writes of a given processor appear to execute in the same order to all other processors. The models mainly differ in whether a write appears to become visible to all other processors simultaneously or at different times.
Reference: 23. <author> J. M. MELLOR-CRUMMEY and M. L. SCOTT, </author> <title> Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors, </title> <journal> ACM Transactions on Computer Systems, </journal> <month> February </month> <year> 1991, </year> <pages> 21-65. </pages>
Reference-contexts: A write due to a Fetch&Inc is pairable with a read due to another Fetch&Inc and a write due to a SyncWrite is pairable with a read due to a SyncRead. Also shown is code where N processors synchronize on a barrier <ref> [23] </ref>, and its execution for N = 2. The variable local_flag is implemented in a local register of the processor and operations on it are not shown in the execution. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh - 8 - - -- function exactly.
Reference: 24. <author> J. PATEL, </author> <title> Mulitprocessor Cache Memories, </title> <institution> Seminar at Texas Instruments Research Labs (Dallas, </institution> <month> Dec. </month> <year> 1983), </year> <institution> Intel (Aloha, Oregon, </institution> <month> April </month> <year> 1984), </year> <institution> Digital Equipment (Hudson, </institution> <address> Mass., </address> <month> June </month> <year> 1984), </year> <institution> IBM (Yorktown, </institution> <month> Oct. </month> <year> 1984), </year> <institution> IBM (Poughkeepsie, </institution> <month> Aug. </month> <year> 1986). </year>
Reference-contexts: In other words, a sequentially consistent multiprocessor appears like a multiprogrammed uniprocessor <ref> [24] </ref>. Although sequential consistency retains the simplicity of the uniprocessor memory model, it limits performance by preventing the use of several optimizations.
Reference: 25. <author> SUN, </author> <title> The SPARC Architecture Manual, </title> <institution> Sun Microsystems Inc., </institution> <note> No. 800-199-12, Version 8, </note> <month> January </month> <year> 1991. </year>
Reference-contexts: Before executing any nonserialization operation, a processor completes all serialization operations that are before that nonserializa-tion operation according to program order. The processor consistency [11, 16], PRAM [22] and total store ordering <ref> [25] </ref> models ensure that writes of a given processor appear to execute in the same order to all other processors. The models mainly differ in whether a write appears to become visible to all other processors simultaneously or at different times. The partial store ordering model [25] is similar to total <p> and total store ordering <ref> [25] </ref> models ensure that writes of a given processor appear to execute in the same order to all other processors. The models mainly differ in whether a write appears to become visible to all other processors simultaneously or at different times. The partial store ordering model [25] is similar to total store ordering except that it orders writes by a processor only if they are separated by a store barrier operation.
Reference: 26. <author> C. E. SCHEURICH, </author> <title> Access Ordering and Coherence in Shared Memory Multiprocessors, </title> <type> Ph.D. Thesis, </type> <institution> Department of Computer Engineering, </institution> <type> Technical Report CENG 89-19, </type> <institution> University of Southern California, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: For programmers, the authors of weak ordering later stated that mutual exclusion should be ensured for each access to a shared variable by using constructs such as critical sections, which are implemented with hardware-recognizable synchronization operations <ref> [10, 26] </ref>. The authors of release consistency formalize a group of programs called properly labeled programs, for which release consistency ensures sequential consistency. A properly labeled program distinguishes its memory operations depending on their use. For example, it distinguishes synchronization operations from ordinary data operations. <p> The model known as release consistency with processor-consistent special operations [11] is similar to release consistency with sequentially consistent special operations except that it requires special operations (syncs and nsyncs) to be processor-consistent. The concurrent-consistency model <ref> [26] </ref> ensures sequential consistency to all programs except those ``which explicitly test for sequential consistency or take access timings into consideration.'' The slow memory model [18] requires that a read return the value of some previous conflicting write.
Reference: 27. <author> D. SHASHA and M. SNIR, </author> <title> Efficient and Correct Execution of Parallel Programs that Share Memory, </title> <journal> ACM Trans. on Programming Languages and Systems 10, </journal> <month> 2 (April </month> <year> 1988), </year> <pages> 282-312. </pages>
Reference-contexts: The program order for an execution is a partial order on the memory operations of the execution imposed by the program text <ref> [27] </ref>. The result of an execution refers to the values returned by the read operations in the execution. A sequentially consistent execution is an execution that could occur on sequentially consistent hardware. <p> A sequentially consistent execution is an execution that could occur on sequentially consistent hardware. Two memory operations conflict if at least one of them is a write and they access the same location <ref> [27] </ref>.
Reference: 28. <author> R. N. ZUCKER, </author> <title> A Study of Weak Consistency Models, Dissertation Proposal, </title> <institution> University of Washington, </institution> <year> 1991. </year> <month> - 24 </month> - 
Reference-contexts: However, this delay is not necessary to maintain sequential consistency (as also observed by Zucker <ref> [28] </ref>), and it is not imposed by the implementation proposal for data-race-free-1 described next. Instead, the implementation maintains sequential consistency by requiring that P 0 's data write on x completes before P 1 executes its data read on x.
References-found: 28

