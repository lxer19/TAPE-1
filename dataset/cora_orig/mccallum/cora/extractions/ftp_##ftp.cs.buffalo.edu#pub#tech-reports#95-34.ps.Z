URL: ftp://ftp.cs.buffalo.edu/pub/tech-reports/95-34.ps.Z
Refering-URL: ftp://ftp.cs.buffalo.edu/pub/tech-reports/README.html
Root-URL: 
Email: wu@cs.buffalo.edu  
Title: On Runtime Parallel Scheduling  
Author: Min-You Wu 
Address: Buffalo, NY 14260  
Affiliation: Department of Computer Science State University of New York at Buffalo  
Abstract: Parallel scheduling is a new approach for load balancing. In parallel scheduling, all processors cooperate together to schedule work. Parallel scheduling is able to accurately balance the load by using global load information at compile-time or runtime. It provides a high-quality load balancing. This paper presents an overview of the parallel scheduling technique. Particular scheduling algorithms for tree, hypercube, and mesh networks are presented. These algorithms can fully balance the load and maximize locality at runtime. Communication costs are significantly reduced compared to other existing algorithms. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> I. Ahmad and Y.K. Kwok. </author> <title> A parallel approach to multiprocessor scheduling. </title> <booktitle> In Int'l Parallel Processing Symposium, </booktitle> <year> 1995. </year>
Reference-contexts: This scheduling strategy should be able to generate well-balanced load without incurring large overhead. With advanced parallel scheduling techniques, this ideal scheduling becomes feasible. In a parallel scheduling, all processors are cooperated together to schedule work. Some parallel scheduling algorithms have been introduced in <ref> [26, 13, 7, 12, 33, 1] </ref>. Parallel scheduling utilizes global load information stored in all processors and is able to accurately balance the load. As an alternative strategy to the other static and dynamic scheduling, it provides a high-quality, scalable scheduling. <p> A low-overhead algorithm can be developed in the horizontal scheme. However, this kind of algorithm is not scalable to massively parallel systems, and usually provides a sub-optimal scheduling. Kwok and Ahmad have developed an algorithm in the horizontal scheme <ref> [1] </ref>. The scheduling length obtained by the parallel scheduling algorithm is about 10% longer than its sequential counterpart. 2 When parallel scheduling is applied at runtime, it becomes an incremental collective scheduling. It is applied whenever the load becomes unbalanced. All processors collectively schedule the workload.
Reference: [2] <author> W. C. Athas. </author> <title> Fine Grain Concurrent Computations. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, California Institute of Technology, </institution> <month> May </month> <year> 1987. </year>
Reference-contexts: Eager et al. compared the sender-initiated algorithm and receiver-initiated algorithm [15]. Work with a similar assumption as ours includes the Gradient Model developed by Lin and Keller [24]. The randomized allocation algorithms developed by different authors are quite simple and effective <ref> [2, 3, 14, 21, 10] </ref>. The receiver-initiated diffusion (RID) is another effective algorithm [33]. 31 Runtime parallel scheduling is similar to the dynamic scheduling in a certain degree. Both methods schedule tasks at runtime instead of compile-time.
Reference: [3] <author> W. C. Athas and C. L. Seitz. </author> <title> Multicomputers: Message-passing concurrent computers. </title> <journal> IEEE Computer, </journal> <volume> 21(8) </volume> <pages> 9-24, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Eager et al. compared the sender-initiated algorithm and receiver-initiated algorithm [15]. Work with a similar assumption as ours includes the Gradient Model developed by Lin and Keller [24]. The randomized allocation algorithms developed by different authors are quite simple and effective <ref> [2, 3, 14, 21, 10] </ref>. The receiver-initiated diffusion (RID) is another effective algorithm [33]. 31 Runtime parallel scheduling is similar to the dynamic scheduling in a certain degree. Both methods schedule tasks at runtime instead of compile-time.
Reference: [4] <author> S. B. Baden. </author> <title> Dynamic load balancing of a vortex calculation running on multiprocessors. </title> <type> Technical Report Vol. 22584, </type> <institution> Lawrence Berkeley Lab., </institution> <year> 1986. </year>
Reference-contexts: A category of scheduling sometimes referred to as prescheduling is closely related to the idea presented in this paper. Fox et al. first adapted prescheduling to application problems with geographical structures [19, 26]. Some other works also deal with geographically structured problems <ref> [13, 6, 4] </ref>. The project PARTI automates prescheduling for nonuniform problems [27]. The dimension exchange method (DEM) is applied to application problems without geographical structure [12]. It was conceptually designed for a hypercube system but may be applied to other topologies, such as k-ary n-cubes [35].
Reference: [5] <author> A. Barak and A. Shiloh. </author> <title> A distributed load-balancing policy for a multicomputer. </title> <journal> Software-Practice and Experience, </journal> <volume> 15(9) </volume> <pages> 901-913, </pages> <month> September </month> <year> 1985. </year>
Reference-contexts: Third, it eliminates the requirement of large memory space to store task graphs, as scheduling is conducted in an incremental fashion. It then leads to a better scalability for massively parallel machines and large size applications. Large research efforts have been directed towards the process allocation in distributed systems <ref> [28, 14, 15, 31, 8, 9, 20, 30, 32, 5, 25] </ref>. A recent comparison study of dynamic load balancing strategies on highly parallel computers is given by Willebeek-LeMair and Reeves [33]. Eager et al. compared the sender-initiated algorithm and receiver-initiated algorithm [15].
Reference: [6] <author> M.J. Berger and S. Bokhari. </author> <title> A partitioning strategy for non-uniform problems on multiprocessors. </title> <journal> IEEE Trans. Computers, </journal> <volume> C-26:570-580, </volume> <year> 1987. </year>
Reference-contexts: A category of scheduling sometimes referred to as prescheduling is closely related to the idea presented in this paper. Fox et al. first adapted prescheduling to application problems with geographical structures [19, 26]. Some other works also deal with geographically structured problems <ref> [13, 6, 4] </ref>. The project PARTI automates prescheduling for nonuniform problems [27]. The dimension exchange method (DEM) is applied to application problems without geographical structure [12]. It was conceptually designed for a hypercube system but may be applied to other topologies, such as k-ary n-cubes [35].
Reference: [7] <author> H. Berryman, J. Saltz, and J. Scroggs. </author> <title> Execution time support for adaptive scientific algorithms on distributed memory machines. </title> <journal> Concurrency: Practice and Experience, </journal> <note> accepted for publication, </note> <year> 1991. </year>
Reference-contexts: This scheduling strategy should be able to generate well-balanced load without incurring large overhead. With advanced parallel scheduling techniques, this ideal scheduling becomes feasible. In a parallel scheduling, all processors are cooperated together to schedule work. Some parallel scheduling algorithms have been introduced in <ref> [26, 13, 7, 12, 33, 1] </ref>. Parallel scheduling utilizes global load information stored in all processors and is able to accurately balance the load. As an alternative strategy to the other static and dynamic scheduling, it provides a high-quality, scalable scheduling.
Reference: [8] <author> T. L. Casavant and J. G. Kuhl. </author> <title> A formal model of distributed decision-making and its application to distributed load balancing. </title> <booktitle> In Int'l Conf. on Distributed Computing System, </booktitle> <pages> pages 232-239, </pages> <month> May </month> <year> 1986. </year>
Reference-contexts: Third, it eliminates the requirement of large memory space to store task graphs, as scheduling is conducted in an incremental fashion. It then leads to a better scalability for massively parallel machines and large size applications. Large research efforts have been directed towards the process allocation in distributed systems <ref> [28, 14, 15, 31, 8, 9, 20, 30, 32, 5, 25] </ref>. A recent comparison study of dynamic load balancing strategies on highly parallel computers is given by Willebeek-LeMair and Reeves [33]. Eager et al. compared the sender-initiated algorithm and receiver-initiated algorithm [15].
Reference: [9] <author> T. L. Casavant and J. G. Kuhl. </author> <title> Analysis of three dynamic distributed load-balancing strategies with varying global information requirements. </title> <booktitle> In Int'l Conf. on Distributed Computing System, </booktitle> <pages> pages 185-192, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: Third, it eliminates the requirement of large memory space to store task graphs, as scheduling is conducted in an incremental fashion. It then leads to a better scalability for massively parallel machines and large size applications. Large research efforts have been directed towards the process allocation in distributed systems <ref> [28, 14, 15, 31, 8, 9, 20, 30, 32, 5, 25] </ref>. A recent comparison study of dynamic load balancing strategies on highly parallel computers is given by Willebeek-LeMair and Reeves [33]. Eager et al. compared the sender-initiated algorithm and receiver-initiated algorithm [15].
Reference: [10] <author> S. Chakrabarti, A. Ranade, and K. Yelick. </author> <title> Randomized load balancing for tree structured computation. </title> <booktitle> In IEEE Scalable High Performance Computing Conference, </booktitle> <pages> pages 666-673, </pages> <year> 1994. </year>
Reference-contexts: Eager et al. compared the sender-initiated algorithm and receiver-initiated algorithm [15]. Work with a similar assumption as ours includes the Gradient Model developed by Lin and Keller [24]. The randomized allocation algorithms developed by different authors are quite simple and effective <ref> [2, 3, 14, 21, 10] </ref>. The receiver-initiated diffusion (RID) is another effective algorithm [33]. 31 Runtime parallel scheduling is similar to the dynamic scheduling in a certain degree. Both methods schedule tasks at runtime instead of compile-time.
Reference: [11] <author> Y.C. Chung and S. Ranka. </author> <title> Applications and performance analysis of a compile-time optimization approach for list scheduling algorithms on distributed memory multiprocessors. </title> <booktitle> In Supercomputer '92, </booktitle> <month> November </month> <year> 1992. </year>
Reference-contexts: Static scheduling utilizes the knowledge of problem characteristics to reach a global optimal, or 1 nearly-optimal, solution with well-balanced load. It has recently attracted considerable attention among the research community <ref> [17, 34, 16, 36, 11, 22] </ref>. The quality of scheduling heavily relies on accuracy of weight estimation. The requirement of large memory space to store the task graph restricts the scalability of static scheduling. In addition, static scheduling is not able to balance the load for dynamic problems. <p> With either CWA or MWA, the longest path has two edges. Therefore, there is no negative cycle. 2 7. Previous Works Parallel scheduling and the static scheduling share some common ideas <ref> [17, 34, 16, 36, 11] </ref>. Both of them utilize the systemwise information and perform scheduling globally to achieve high-quality of load balancing. They also clearly separate the time to conduct scheduling and the time to perform computation. But, parallel scheduling is different from the static scheduling in three aspects.
Reference: [12] <author> G. Cybenko. </author> <title> Dynamic load balancing for distributed memory multiprocessors. </title> <journal> J. of Parallel Distrib. Comput., </journal> <volume> 7 </volume> <pages> 279-301, </pages> <year> 1989. </year>
Reference-contexts: This scheduling strategy should be able to generate well-balanced load without incurring large overhead. With advanced parallel scheduling techniques, this ideal scheduling becomes feasible. In a parallel scheduling, all processors are cooperated together to schedule work. Some parallel scheduling algorithms have been introduced in <ref> [26, 13, 7, 12, 33, 1] </ref>. Parallel scheduling utilizes global load information stored in all processors and is able to accurately balance the load. As an alternative strategy to the other static and dynamic scheduling, it provides a high-quality, scalable scheduling. <p> A node will not send its tasks to other nodes unless the number of tasks in the node exceeds the average. Therefore, only necessary tasks are migrated. Other algorithms, such as DEM <ref> [12] </ref>, do not utilize this global information, leading to unnecessary communications. 3. Scheduling Algorithm for the Tree Topology When the network topology is a tree, the complexity of optimal scheduling can be reduced. Here we present a parallel scheduling algorithm for tree structured interconnection network. <p> Scheduling Algorithm for the Hypercube Topology In this section, we study two algorithms designed for the hypercube topology. First, we study the DEM algorithm. Then, a new algorithm, Cube Walking Algorithm, is presented. The DEM algorithm, proposed by Cybenko, is a synchronous approach <ref> [12] </ref>. In DEM, small domains are balanced first and these then combine to form larger domains until ultimately the entire system is balanced. The algorithm is described in Figure 7. <p> Fox et al. first adapted prescheduling to application problems with geographical structures [19, 26]. Some other works also deal with geographically structured problems [13, 6, 4]. The project PARTI automates prescheduling for nonuniform problems [27]. The dimension exchange method (DEM) is applied to application problems without geographical structure <ref> [12] </ref>. It was conceptually designed for a hypercube system but may be applied to other topologies, such as k-ary n-cubes [35]. It balances load for independent tasks with an equal grain size.
Reference: [13] <author> K. M. Dragon and J. L. Gustafson. </author> <title> A low-cost hypercube load balance algorithm. </title> <booktitle> In Proc. of the 4th Conf. on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 583-590, </pages> <year> 1989. </year> <month> 33 </month>
Reference-contexts: This scheduling strategy should be able to generate well-balanced load without incurring large overhead. With advanced parallel scheduling techniques, this ideal scheduling becomes feasible. In a parallel scheduling, all processors are cooperated together to schedule work. Some parallel scheduling algorithms have been introduced in <ref> [26, 13, 7, 12, 33, 1] </ref>. Parallel scheduling utilizes global load information stored in all processors and is able to accurately balance the load. As an alternative strategy to the other static and dynamic scheduling, it provides a high-quality, scalable scheduling. <p> A category of scheduling sometimes referred to as prescheduling is closely related to the idea presented in this paper. Fox et al. first adapted prescheduling to application problems with geographical structures [19, 26]. Some other works also deal with geographically structured problems <ref> [13, 6, 4] </ref>. The project PARTI automates prescheduling for nonuniform problems [27]. The dimension exchange method (DEM) is applied to application problems without geographical structure [12]. It was conceptually designed for a hypercube system but may be applied to other topologies, such as k-ary n-cubes [35].
Reference: [14] <author> D. L. Eager, E. D. Lazowska, and J. Zahorjan. </author> <title> Adaptive load sharing in homogeneous distributed systems. </title> <journal> IEEE Trans. Software Eng., </journal> <volume> SE-12(5):662-674, </volume> <month> May </month> <year> 1986. </year>
Reference-contexts: In addition, static scheduling is not able to balance the load for dynamic problems. Dynamic scheduling has certain advantages. It is a general approach suitable for a wide range of applications. It can adjust load distribution based on runtime system load information <ref> [14, 15, 28] </ref>. However, most runtime scheduling algorithms utilize neither the characteristics information of application problems, nor global load information for load balancing decision. Efforts to collect load information for a scheduling decision certainly compete the resource with the underlying computation during runtime. <p> Third, it eliminates the requirement of large memory space to store task graphs, as scheduling is conducted in an incremental fashion. It then leads to a better scalability for massively parallel machines and large size applications. Large research efforts have been directed towards the process allocation in distributed systems <ref> [28, 14, 15, 31, 8, 9, 20, 30, 32, 5, 25] </ref>. A recent comparison study of dynamic load balancing strategies on highly parallel computers is given by Willebeek-LeMair and Reeves [33]. Eager et al. compared the sender-initiated algorithm and receiver-initiated algorithm [15]. <p> Eager et al. compared the sender-initiated algorithm and receiver-initiated algorithm [15]. Work with a similar assumption as ours includes the Gradient Model developed by Lin and Keller [24]. The randomized allocation algorithms developed by different authors are quite simple and effective <ref> [2, 3, 14, 21, 10] </ref>. The receiver-initiated diffusion (RID) is another effective algorithm [33]. 31 Runtime parallel scheduling is similar to the dynamic scheduling in a certain degree. Both methods schedule tasks at runtime instead of compile-time.
Reference: [15] <author> D. L. Eager, E. D. Lazowska, and J. Zahorjan. </author> <title> A comparison of receiver-initiated and sender-initiated adaptive load sharing. </title> <booktitle> Performance Eval., </booktitle> <volume> 6(1) </volume> <pages> 53-68, </pages> <month> March </month> <year> 1986. </year>
Reference-contexts: In addition, static scheduling is not able to balance the load for dynamic problems. Dynamic scheduling has certain advantages. It is a general approach suitable for a wide range of applications. It can adjust load distribution based on runtime system load information <ref> [14, 15, 28] </ref>. However, most runtime scheduling algorithms utilize neither the characteristics information of application problems, nor global load information for load balancing decision. Efforts to collect load information for a scheduling decision certainly compete the resource with the underlying computation during runtime. <p> Third, it eliminates the requirement of large memory space to store task graphs, as scheduling is conducted in an incremental fashion. It then leads to a better scalability for massively parallel machines and large size applications. Large research efforts have been directed towards the process allocation in distributed systems <ref> [28, 14, 15, 31, 8, 9, 20, 30, 32, 5, 25] </ref>. A recent comparison study of dynamic load balancing strategies on highly parallel computers is given by Willebeek-LeMair and Reeves [33]. Eager et al. compared the sender-initiated algorithm and receiver-initiated algorithm [15]. <p> A recent comparison study of dynamic load balancing strategies on highly parallel computers is given by Willebeek-LeMair and Reeves [33]. Eager et al. compared the sender-initiated algorithm and receiver-initiated algorithm <ref> [15] </ref>. Work with a similar assumption as ours includes the Gradient Model developed by Lin and Keller [24]. The randomized allocation algorithms developed by different authors are quite simple and effective [2, 3, 14, 21, 10].
Reference: [16] <author> H. El-Rewini and H. H. Ali. </author> <title> Scheduling conditional branching using representative task graphs. </title> <journal> The Journal of Combinatorial Mathematics and Combinatorial Computing, </journal> <year> 1991. </year>
Reference-contexts: Static scheduling utilizes the knowledge of problem characteristics to reach a global optimal, or 1 nearly-optimal, solution with well-balanced load. It has recently attracted considerable attention among the research community <ref> [17, 34, 16, 36, 11, 22] </ref>. The quality of scheduling heavily relies on accuracy of weight estimation. The requirement of large memory space to store the task graph restricts the scalability of static scheduling. In addition, static scheduling is not able to balance the load for dynamic problems. <p> With either CWA or MWA, the longest path has two edges. Therefore, there is no negative cycle. 2 7. Previous Works Parallel scheduling and the static scheduling share some common ideas <ref> [17, 34, 16, 36, 11] </ref>. Both of them utilize the systemwise information and perform scheduling globally to achieve high-quality of load balancing. They also clearly separate the time to conduct scheduling and the time to perform computation. But, parallel scheduling is different from the static scheduling in three aspects.
Reference: [17] <author> H. El-Rewini and T. G. Lewis. </author> <title> Scheduling parallel program tasks onto arbitrary target machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <month> June </month> <year> 1990. </year>
Reference-contexts: Static scheduling utilizes the knowledge of problem characteristics to reach a global optimal, or 1 nearly-optimal, solution with well-balanced load. It has recently attracted considerable attention among the research community <ref> [17, 34, 16, 36, 11, 22] </ref>. The quality of scheduling heavily relies on accuracy of weight estimation. The requirement of large memory space to store the task graph restricts the scalability of static scheduling. In addition, static scheduling is not able to balance the load for dynamic problems. <p> With either CWA or MWA, the longest path has two edges. Therefore, there is no negative cycle. 2 7. Previous Works Parallel scheduling and the static scheduling share some common ideas <ref> [17, 34, 16, 36, 11] </ref>. Both of them utilize the systemwise information and perform scheduling globally to achieve high-quality of load balancing. They also clearly separate the time to conduct scheduling and the time to perform computation. But, parallel scheduling is different from the static scheduling in three aspects.
Reference: [18] <author> H. El-Rewini, T. G. Lewis, and H. H. Ali. </author> <title> Task Scheduling in Parallel and Distributed Systems. </title> <publisher> Prentice Hall, </publisher> <year> 1994. </year>
Reference-contexts: 1. Introduction One of the challenges in programming parallel machines is to schedule work to processors <ref> [18] </ref>. There are two types of application problem structures: problems with a predictable structure, also called static problems, and problems with an unpredictable structure, called dynamic problems.
Reference: [19] <author> G. C. Fox, M. A. Johnson, G. A. Lyzenga, S. W. Otto, J. K. Salmon, and D. W. Walker. </author> <title> Solving Problems on Concurrent Processors, volume I. </title> <publisher> Prentice-Hall, </publisher> <year> 1988. </year>
Reference-contexts: Whereas in parallel scheduling, the scheduling activity is always an aggregate operation, based on global system information. A category of scheduling sometimes referred to as prescheduling is closely related to the idea presented in this paper. Fox et al. first adapted prescheduling to application problems with geographical structures <ref> [19, 26] </ref>. Some other works also deal with geographically structured problems [13, 6, 4]. The project PARTI automates prescheduling for nonuniform problems [27]. The dimension exchange method (DEM) is applied to application problems without geographical structure [12].
Reference: [20] <author> A. Hac and X. Jin. </author> <title> Dynamic load balancing in a distributed system using a decentralized algorithm. </title> <booktitle> In Int'l Conf. on Distributed Computing System, </booktitle> <pages> pages 170-177, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: Third, it eliminates the requirement of large memory space to store task graphs, as scheduling is conducted in an incremental fashion. It then leads to a better scalability for massively parallel machines and large size applications. Large research efforts have been directed towards the process allocation in distributed systems <ref> [28, 14, 15, 31, 8, 9, 20, 30, 32, 5, 25] </ref>. A recent comparison study of dynamic load balancing strategies on highly parallel computers is given by Willebeek-LeMair and Reeves [33]. Eager et al. compared the sender-initiated algorithm and receiver-initiated algorithm [15].
Reference: [21] <author> R. M. Karp and Y. Zhang. </author> <title> A randomized parallel branch-and-bound procedure. </title> <journal> Journal of ACM, </journal> <volume> 40 </volume> <pages> 765-789, </pages> <year> 1993. </year>
Reference-contexts: Eager et al. compared the sender-initiated algorithm and receiver-initiated algorithm [15]. Work with a similar assumption as ours includes the Gradient Model developed by Lin and Keller [24]. The randomized allocation algorithms developed by different authors are quite simple and effective <ref> [2, 3, 14, 21, 10] </ref>. The receiver-initiated diffusion (RID) is another effective algorithm [33]. 31 Runtime parallel scheduling is similar to the dynamic scheduling in a certain degree. Both methods schedule tasks at runtime instead of compile-time.
Reference: [22] <author> Y.K. Kwok and I.Ahmad. </author> <title> A static scheduling algorithm using dynamic critical path for assigning parallel algorithms onto multiprocessors. </title> <booktitle> In Proc. of Int'l Conference on Parallel Processing, </booktitle> <address> pages II-155 | II-159, </address> <year> 1994. </year>
Reference-contexts: Static scheduling utilizes the knowledge of problem characteristics to reach a global optimal, or 1 nearly-optimal, solution with well-balanced load. It has recently attracted considerable attention among the research community <ref> [17, 34, 16, 36, 11, 22] </ref>. The quality of scheduling heavily relies on accuracy of weight estimation. The requirement of large memory space to store the task graph restricts the scalability of static scheduling. In addition, static scheduling is not able to balance the load for dynamic problems.
Reference: [23] <author> E. L. Lawler. </author> <title> Combinatorial Optimization: Networks and Matroids. </title> <publisher> Holt, Rinehart and Winston, </publisher> <year> 1976. </year>
Reference-contexts: An optimal scheduling minimizes task communications. The objective function is to minimize the number of task-hops: X e k ; where e k is the number of tasks transmitted through the edge k. In general, this problem can be converted to the minimum-cost maximum-flow problem <ref> [23] </ref> as follows. Each edge is given a tuple (capacity; cost), where capacity is the capacity of the edge and cost is the cost of the edge. Set capacity = 1, cost = 1, for all edges. <p> A minimum cost integral flow yields a solution to the problem. Figure 2 shows a load distribution 4 in an eight-node hypercube network. The graph constructed for Figure 2 is given in Figure 3, where w avg = 8. The minimum cost algorithm <ref> [23] </ref> generates a solution as shown in Figure 4. 19 11 0 9 s 2 0 4 (11,0) (3,0)(1,0) (1,0)(2,0) All other edges have (1; 1) (6,0) (8,0) (4,0) The complexity of the minimum cost algorithm is O (N 2 v), where N is the number of nodes and v is <p> shown in Figure 4. 19 11 0 9 s 2 0 4 (11,0) (3,0)(1,0) (1,0)(2,0) All other edges have (1; 1) (6,0) (8,0) (4,0) The complexity of the minimum cost algorithm is O (N 2 v), where N is the number of nodes and v is the desired flow value <ref> [23] </ref>. The complexity of its corresponding parallel algorithm on N nodes is at least O (N v). This high complexity is not realistic for runtime scheduling. For certain topology, such as trees, the complexity can be reduced to O (log N ) on N processors. <p> Lemma 3: The CWA and MWA algorithms minimize the communication cost in a system with two or four processors. Proof: The communication cost in a system is minimized if there is no negative cycle <ref> [23] </ref>. In a system of two processors, there is no cycle. In a system of four processors, only a path consisting of at least three edges can form a negative cycle. With either CWA or MWA, the longest path has two edges. Therefore, there is no negative cycle. 2 7.
Reference: [24] <author> F. C. H. Lin and R. M. Keller. </author> <title> The gradient model load balancing method. </title> <journal> IEEE Trans. Software Engineering, </journal> <volume> 13 </volume> <pages> 32-38, </pages> <month> January </month> <year> 1987. </year>
Reference-contexts: A recent comparison study of dynamic load balancing strategies on highly parallel computers is given by Willebeek-LeMair and Reeves [33]. Eager et al. compared the sender-initiated algorithm and receiver-initiated algorithm [15]. Work with a similar assumption as ours includes the Gradient Model developed by Lin and Keller <ref> [24] </ref>. The randomized allocation algorithms developed by different authors are quite simple and effective [2, 3, 14, 21, 10]. The receiver-initiated diffusion (RID) is another effective algorithm [33]. 31 Runtime parallel scheduling is similar to the dynamic scheduling in a certain degree.
Reference: [25] <author> Z. Lin. </author> <title> A distributed fair polling scheme applied to parallel logic programming. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 20, </volume> <month> August </month> <year> 1991. </year>
Reference-contexts: Third, it eliminates the requirement of large memory space to store task graphs, as scheduling is conducted in an incremental fashion. It then leads to a better scalability for massively parallel machines and large size applications. Large research efforts have been directed towards the process allocation in distributed systems <ref> [28, 14, 15, 31, 8, 9, 20, 30, 32, 5, 25] </ref>. A recent comparison study of dynamic load balancing strategies on highly parallel computers is given by Willebeek-LeMair and Reeves [33]. Eager et al. compared the sender-initiated algorithm and receiver-initiated algorithm [15].
Reference: [26] <author> J.K. Salmon. </author> <title> Parallel hierarchical N-body methods. </title> <type> Technical report, Tech. Report, </type> <institution> CRPC-90-14, Center for Research in Parallel Computing, Caltech, </institution> <year> 1990., 1990. </year>
Reference-contexts: This scheduling strategy should be able to generate well-balanced load without incurring large overhead. With advanced parallel scheduling techniques, this ideal scheduling becomes feasible. In a parallel scheduling, all processors are cooperated together to schedule work. Some parallel scheduling algorithms have been introduced in <ref> [26, 13, 7, 12, 33, 1] </ref>. Parallel scheduling utilizes global load information stored in all processors and is able to accurately balance the load. As an alternative strategy to the other static and dynamic scheduling, it provides a high-quality, scalable scheduling. <p> Whereas in parallel scheduling, the scheduling activity is always an aggregate operation, based on global system information. A category of scheduling sometimes referred to as prescheduling is closely related to the idea presented in this paper. Fox et al. first adapted prescheduling to application problems with geographical structures <ref> [19, 26] </ref>. Some other works also deal with geographically structured problems [13, 6, 4]. The project PARTI automates prescheduling for nonuniform problems [27]. The dimension exchange method (DEM) is applied to application problems without geographical structure [12].
Reference: [27] <author> J. Saltz, R. Mirchandaney, R. Smith, D. Nicol, and K. Crowley. </author> <title> The PARTY parallel run-time system. </title> <booktitle> In Proceedings of the SIAM Conference on Parallel Processing for Scientific Computing. </booktitle> <publisher> SIAM, </publisher> <year> 1987. </year>
Reference-contexts: Fox et al. first adapted prescheduling to application problems with geographical structures [19, 26]. Some other works also deal with geographically structured problems [13, 6, 4]. The project PARTI automates prescheduling for nonuniform problems <ref> [27] </ref>. The dimension exchange method (DEM) is applied to application problems without geographical structure [12]. It was conceptually designed for a hypercube system but may be applied to other topologies, such as k-ary n-cubes [35]. It balances load for independent tasks with an equal grain size.
Reference: [28] <author> Niranjan G. Shivaratri, Phillip Krieger, and Mukesh Singhal. </author> <title> Load distributing for locally distributed systems. </title> <journal> IEEE Computer, </journal> <volume> 25(12) </volume> <pages> 33-44, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: In addition, static scheduling is not able to balance the load for dynamic problems. Dynamic scheduling has certain advantages. It is a general approach suitable for a wide range of applications. It can adjust load distribution based on runtime system load information <ref> [14, 15, 28] </ref>. However, most runtime scheduling algorithms utilize neither the characteristics information of application problems, nor global load information for load balancing decision. Efforts to collect load information for a scheduling decision certainly compete the resource with the underlying computation during runtime. <p> Third, it eliminates the requirement of large memory space to store task graphs, as scheduling is conducted in an incremental fashion. It then leads to a better scalability for massively parallel machines and large size applications. Large research efforts have been directed towards the process allocation in distributed systems <ref> [28, 14, 15, 31, 8, 9, 20, 30, 32, 5, 25] </ref>. A recent comparison study of dynamic load balancing strategies on highly parallel computers is given by Willebeek-LeMair and Reeves [33]. Eager et al. compared the sender-initiated algorithm and receiver-initiated algorithm [15].
Reference: [29] <author> W. Shu and M. Y. Wu. </author> <title> Runtime Incremental Parallel Scheduling (RIPS) for large-scale parallel computers. </title> <booktitle> In Proceedings of the 5th Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 456-463, </pages> <year> 1995. </year>
Reference-contexts: It is applied whenever the load becomes unbalanced. All processors collectively schedule the workload. Such a system has been described in <ref> [29] </ref>. This parallel scheduling system is shown in Figure 1. It starts with a system phase which schedules initial tasks. It is followed by a user computation phase to execute the scheduled tasks, and possibly to generate new tasks. <p> Runtime parallel scheduling can provide a low-overhead task scheduling because tasks are packed to be sent to other processors, which reduces significantly the number of messages. It is scalable to at least 512 processors <ref> [29] </ref>. Furthermore, in a dynamic scheduling system, when it intends to quickly and accurately balance the load, the system could become unstable. In parallel scheduling, a synchronous approach removes the stability problem and is able to balance the load 32 quickly and accurately.
Reference: [30] <author> V. Singh and M. R. Genesereth. </author> <title> A variable supply model for distributing deductions. </title> <booktitle> In 9th Intel. Joint Conf. Artificial Intelligence, </booktitle> <pages> pages 39-45, </pages> <month> August </month> <year> 1985. </year>
Reference-contexts: Third, it eliminates the requirement of large memory space to store task graphs, as scheduling is conducted in an incremental fashion. It then leads to a better scalability for massively parallel machines and large size applications. Large research efforts have been directed towards the process allocation in distributed systems <ref> [28, 14, 15, 31, 8, 9, 20, 30, 32, 5, 25] </ref>. A recent comparison study of dynamic load balancing strategies on highly parallel computers is given by Willebeek-LeMair and Reeves [33]. Eager et al. compared the sender-initiated algorithm and receiver-initiated algorithm [15].
Reference: [31] <author> J. A. Stankovic. </author> <title> Simulations of three adaptive, decentralized controlled, job scheduling algorithms. </title> <journal> Computer Networks, </journal> <volume> 8(3) </volume> <pages> 199-217, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: Third, it eliminates the requirement of large memory space to store task graphs, as scheduling is conducted in an incremental fashion. It then leads to a better scalability for massively parallel machines and large size applications. Large research efforts have been directed towards the process allocation in distributed systems <ref> [28, 14, 15, 31, 8, 9, 20, 30, 32, 5, 25] </ref>. A recent comparison study of dynamic load balancing strategies on highly parallel computers is given by Willebeek-LeMair and Reeves [33]. Eager et al. compared the sender-initiated algorithm and receiver-initiated algorithm [15].
Reference: [32] <author> Y.-T. Wang and R. J. T. Morris. </author> <title> Load sharing in distributed systems. </title> <journal> IEEE Trans. Comput., </journal> <volume> C-34(3):204-217, </volume> <month> March </month> <year> 1985. </year> <month> 34 </month>
Reference-contexts: Third, it eliminates the requirement of large memory space to store task graphs, as scheduling is conducted in an incremental fashion. It then leads to a better scalability for massively parallel machines and large size applications. Large research efforts have been directed towards the process allocation in distributed systems <ref> [28, 14, 15, 31, 8, 9, 20, 30, 32, 5, 25] </ref>. A recent comparison study of dynamic load balancing strategies on highly parallel computers is given by Willebeek-LeMair and Reeves [33]. Eager et al. compared the sender-initiated algorithm and receiver-initiated algorithm [15].
Reference: [33] <author> Marc Willebeek-LeMair and Anthony P. Reeves. </author> <title> Strategies for dynamic load balancing on highly parallel computers. </title> <journal> IEEE Trans. Parallel and Distributed System, </journal> <volume> 9(4) </volume> <pages> 979-993, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: This scheduling strategy should be able to generate well-balanced load without incurring large overhead. With advanced parallel scheduling techniques, this ideal scheduling becomes feasible. In a parallel scheduling, all processors are cooperated together to schedule work. Some parallel scheduling algorithms have been introduced in <ref> [26, 13, 7, 12, 33, 1] </ref>. Parallel scheduling utilizes global load information stored in all processors and is able to accurately balance the load. As an alternative strategy to the other static and dynamic scheduling, it provides a high-quality, scalable scheduling. <p> Next, all nodes pairs in the second dimension balance the load between themselves, and so forth, until each node has balanced its load with each of its neighbors. The number of communication steps of the DEM algorithm is 3d, where d is the number of dimensions <ref> [33] </ref>. <p> Large research efforts have been directed towards the process allocation in distributed systems [28, 14, 15, 31, 8, 9, 20, 30, 32, 5, 25]. A recent comparison study of dynamic load balancing strategies on highly parallel computers is given by Willebeek-LeMair and Reeves <ref> [33] </ref>. Eager et al. compared the sender-initiated algorithm and receiver-initiated algorithm [15]. Work with a similar assumption as ours includes the Gradient Model developed by Lin and Keller [24]. The randomized allocation algorithms developed by different authors are quite simple and effective [2, 3, 14, 21, 10]. <p> Work with a similar assumption as ours includes the Gradient Model developed by Lin and Keller [24]. The randomized allocation algorithms developed by different authors are quite simple and effective [2, 3, 14, 21, 10]. The receiver-initiated diffusion (RID) is another effective algorithm <ref> [33] </ref>. 31 Runtime parallel scheduling is similar to the dynamic scheduling in a certain degree. Both methods schedule tasks at runtime instead of compile-time. Their scheduling decisions, in principle, depend on and adapt to the runtime system information. However, there exist substantial differences, making them appear as two separate categories. <p> It was conceptually designed for a hypercube system but may be applied to other topologies, such as k-ary n-cubes [35]. It balances load for independent tasks with an equal grain size. The method has been extended by Willebeek-LeMair and Reeves <ref> [33] </ref> so that the algorithm can run incrementally to correct the unbalanced load due to varied grain sizes. 8. Concluding Remarks Parallel scheduling combines the advantages of static scheduling and dynamic scheduling. It balances the load in parallel.
Reference: [34] <author> M. Y. Wu and D. D. Gajski. Hypertool: </author> <title> A programming aid for message-passing systems. </title> <journal> IEEE Trans. Parallel and Distributed Systems, </journal> <volume> 1(3) </volume> <pages> 330-343, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: Static scheduling utilizes the knowledge of problem characteristics to reach a global optimal, or 1 nearly-optimal, solution with well-balanced load. It has recently attracted considerable attention among the research community <ref> [17, 34, 16, 36, 11, 22] </ref>. The quality of scheduling heavily relies on accuracy of weight estimation. The requirement of large memory space to store the task graph restricts the scalability of static scheduling. In addition, static scheduling is not able to balance the load for dynamic problems. <p> With either CWA or MWA, the longest path has two edges. Therefore, there is no negative cycle. 2 7. Previous Works Parallel scheduling and the static scheduling share some common ideas <ref> [17, 34, 16, 36, 11] </ref>. Both of them utilize the systemwise information and perform scheduling globally to achieve high-quality of load balancing. They also clearly separate the time to conduct scheduling and the time to perform computation. But, parallel scheduling is different from the static scheduling in three aspects.
Reference: [35] <author> C. Z. Xu and F. C. M. Lau. </author> <title> The generalized dimension exchange method for load balancing in k-ary n-cubes and variants. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 24(1) </volume> <pages> 72-85, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: The project PARTI automates prescheduling for nonuniform problems [27]. The dimension exchange method (DEM) is applied to application problems without geographical structure [12]. It was conceptually designed for a hypercube system but may be applied to other topologies, such as k-ary n-cubes <ref> [35] </ref>. It balances load for independent tasks with an equal grain size. The method has been extended by Willebeek-LeMair and Reeves [33] so that the algorithm can run incrementally to correct the unbalanced load due to varied grain sizes. 8.
Reference: [36] <author> T. Yang and A. Gerasoulis. </author> <title> PYRROS: Static task scheduling and code generation for message-passing multiprocessors. </title> <booktitle> The 6th ACM Int'l Conf. on Supercomputing, </booktitle> <month> July </month> <year> 1992. </year> <month> 35 </month>
Reference-contexts: Static scheduling utilizes the knowledge of problem characteristics to reach a global optimal, or 1 nearly-optimal, solution with well-balanced load. It has recently attracted considerable attention among the research community <ref> [17, 34, 16, 36, 11, 22] </ref>. The quality of scheduling heavily relies on accuracy of weight estimation. The requirement of large memory space to store the task graph restricts the scalability of static scheduling. In addition, static scheduling is not able to balance the load for dynamic problems. <p> With either CWA or MWA, the longest path has two edges. Therefore, there is no negative cycle. 2 7. Previous Works Parallel scheduling and the static scheduling share some common ideas <ref> [17, 34, 16, 36, 11] </ref>. Both of them utilize the systemwise information and perform scheduling globally to achieve high-quality of load balancing. They also clearly separate the time to conduct scheduling and the time to perform computation. But, parallel scheduling is different from the static scheduling in three aspects.
References-found: 36

