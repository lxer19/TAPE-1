URL: http://www.cs.bu.edu/techreports/94-001-maxclique.ps.Z
Refering-URL: http://cs-www.bu.edu/techreports/Home.html
Root-URL: 
Email: homer@cs.bu.edu mpe@cs.bu.edu  
Title: On the Performance of Polynomial-time CLIQUE Approximation Algorithms on Very Large Graphs  
Author: Steven Homer Marcus Peinado 
Address: Boston University  
Affiliation: Department of Computer Science and Center for Computational Science  
Abstract: The performance of a randomized version of the subgraph-exclusion algorithm (called Ramsey) for CLIQUE by Boppana and Halldorsson is studied on very large graphs. We compare the performance of this algorithm with the performance of two common heuristic algorithms, the greedy heuristic and a version of simulated annealing. These algorithms are tested on graphs with up to 10,000 vertices on a workstation and graphs as large as 70,000 vertices on a Connection Machine. Our implementations establish the ability to run clique approximation algorithms on very large graphs. We test our implementations on a variety of different graphs. Our conclusions indicate that on randomly generated graphs minor changes to the distribution can cause dramatic changes in the performance of the heuristic algorithms. The Ramsey algorithm, while not as good as the others for the most common distributions, seems more robust and provides a more even overall performance. In general, and especially on deterministically generated graphs, a combination of simulated annealing with either the Ramsey algorithm or the greedy heuristic seems to perform best. This combined algorithm works particularly well on large Keller and Hamming graphs and has a competitive overall performance on the DIMACS benchmark graphs. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E.H.L. Aarts and J.H.M Korst. </author> <title> Simulated Annealing and Boltzmann Machines. </title> <publisher> John Wiley & Sons, </publisher> <address> Chichester, U.K., </address> <year> 1989. </year>
Reference-contexts: Peinado [11] investigates the construction of graphs which are hard for this randomized subgraph exclusion algorithm. Simulated Annealing There is a large body of literature on the general principle of simulated annealing and its application to particular optimization problems (e.g. <ref> [1] </ref>). The application of simulated annealing to the CLIQUE problem is described in [1, p. 81]. We use 4 the penalty function approach described there: The current state of the algorithm is given by any subset V (not necessarily a clique) of the vertex set of the graph. <p> Simulated Annealing There is a large body of literature on the general principle of simulated annealing and its application to particular optimization problems (e.g. [1]). The application of simulated annealing to the CLIQUE problem is described in <ref> [1, p. 81] </ref>. We use 4 the penalty function approach described there: The current state of the algorithm is given by any subset V (not necessarily a clique) of the vertex set of the graph.
Reference: [2] <author> R. Boppana and M. Halldorsson. </author> <title> Approximating maximum independent sets by excluding subgraphs. </title> <booktitle> In SWAT, </booktitle> <pages> pages 11-25. </pages> <publisher> Springer Verlag, </publisher> <year> 1990. </year>
Reference-contexts: Unlike many other NP-hard problems, the CLIQUE problem has for a long time resisted attempts by researchers to construct polynomial-time approximation algorithms with a non-trivial performance guarantee. This was changed recently by the subgraph exclusion algorithm of Boppana and Halldorsson <ref> [2] </ref>. Its performance guarantee of O (n= log 2 n) is tight. However, it is not clear if it remains tight for a randomized version of the algorithm which we study. <p> It works well on random graphs and some graphs with large cliques but it is trivial to construct graphs where this algorithm performs arbitrarily badly. The Ramsey Algorithm The algorithm is due to Boppana and Halldorsson. Our presentation of it follows the one given in [3], <ref> [2] </ref>. The algorithm consists of a subgraph exclusion procedure and a recursive sub-procedure (Ramsey) which is motivated by Ramsey theory and which, given an input graph, returns a clique and an independent set. <p> By running the algorithm repeatedly using different sequences of pivot nodes we hope to achieve better performance. In fact, we have found this to be the case for certain classes of graphs. It is not clear that the performance bounds given in <ref> [2] </ref> remain tight for this version of the algorithm. Peinado [11] investigates the construction of graphs which are hard for this randomized subgraph exclusion algorithm. Simulated Annealing There is a large body of literature on the general principle of simulated annealing and its application to particular optimization problems (e.g. [1]).
Reference: [3] <author> R. Boppana and M. Halldorsson. </author> <title> Approximating maximum independent sets by excluding subgraphs. </title> <journal> BIT, </journal> <volume> 32 </volume> <pages> 180-196, </pages> <year> 1992. </year>
Reference-contexts: It works well on random graphs and some graphs with large cliques but it is trivial to construct graphs where this algorithm performs arbitrarily badly. The Ramsey Algorithm The algorithm is due to Boppana and Halldorsson. Our presentation of it follows the one given in <ref> [3] </ref>, [2]. The algorithm consists of a subgraph exclusion procedure and a recursive sub-procedure (Ramsey) which is motivated by Ramsey theory and which, given an input graph, returns a clique and an independent set. <p> vertex v 2 V (C 1 ; I 1 ) := Ramsey (N (v)) (C 2 ; I 2 ) := Ramsey ( N (v)) return (larger of (C 1 [ fvg; C 2 ), larger of (I 1 ; I 2 [ fvg)) Using Ramsey theory, Boppana and Halldorsson <ref> [3] </ref> show for the clique C and independent set I returned by Ramsey (G) that jCj jIj log 2 n=4. This bound in itself does not guarantee a minimum size of C since jIj can be large. <p> This implies a lower bound on jCj log 2 =(4k). If the largest clique is small, the performance of the algorithm on the graph if trivially good. The result of this analysis is a performance guarantee of O (n= log 2 n) (cf. <ref> [3] </ref> for details). It is easy to see that the performance of the algorithm depends on the sequence of pivot nodes chosen during the procedure. In the Boppana/Halldorsson paper [3] the pivot nodes are chosen deterministically in an arbitrary manner and the worst case behavior is analyzed. <p> The result of this analysis is a performance guarantee of O (n= log 2 n) (cf. <ref> [3] </ref> for details). It is easy to see that the performance of the algorithm depends on the sequence of pivot nodes chosen during the procedure. In the Boppana/Halldorsson paper [3] the pivot nodes are chosen deterministically in an arbitrary manner and the worst case behavior is analyzed. The analysis establishes a performance guarantee and shows nonconstructively that for certain graphs and particular sequences of pivot nodes the algorithm performs no better than this guarantee.
Reference: [4] <author> A.E. Brouwer, J.B. Shearer, N.J.A. Sloane and W.D. Smith. </author> <title> A New Table of Constant Weight Codes. </title> <journal> J. IEEE Trans. Information Theory, </journal> <volume> 36: </volume> <pages> 1334-1380, </pages> <year> 1990. </year>
Reference-contexts: (1) We exhibit the first test data on the Boppana/Halldorsson subgraph exclusion algorithm, a combinatorially interesting algorithm with a non-trivial performance guarantee. (2) We present experiments on the largest graphs yet implemented and tested for approximating maximum size cliques, including Hamming and Keller graphs of practical and theoretical interest [9], <ref> [4] </ref>. As with many NP-hard optimization problems, the clique problem, has been attacked from two sides. There are exact algorithms which solve the problem to optimality in superpolynomial time. Then there are algorithms which run in polynomial time but return only an approximate solution. <p> The motivation for considering this type of graph is explained below. 4. Graphs that are related to Keller's conjecture [9]. 5. Hamming graphs <ref> [4] </ref>. We have concentrated on the case in which the required Hamming distance is 4. The first three examples were chosen to demonstrate the behavior of the algorithm on graphs with certain well known properties. The last two examples are motivated by practical problems.
Reference: [5] <author> T. A. Feo, M.G.C. Resende and S.H. Smith. </author> <title> A greedy randomized adaptive search procedure for maximum independent set. </title> <type> Manuscript, </type> <year> 1992. </year> <title> 15 0 20 40 60 size running time in seconds Time/clique size tradeoff for SA on Keller 6 </title>
Reference-contexts: As expected from theoretical considerations, as the graph size increases, the clique sizes found by all three algorithms approaches log n. To do significantly better would require a more complicated heuristic (like GRASP <ref> [5] </ref>) and a much longer running time which would be 6 8 12 16 20 n Random Graphs G n;0:5 log n Ramsey 3 3 3 3 3 SimAnn 3 3 3 3 SimAnn + + + + + Greedy 2 2 2 2 2 2 20 60 100 140 2000
Reference: [6] <author> Mark Jerrum. </author> <title> Large cliques elude the metropolis process. Random Structures and Algorithms, </title> <booktitle> 3(4) </booktitle> <pages> 347-360, </pages> <year> 1992. </year>
Reference-contexts: This indicates that the number of cliques declines drastically as the maximum clique size is approached. Versions of this idea have been used in other contexts to prove negative results about simulated annealing <ref> [6] </ref>, [12], [13]. While we cannot prove our observation in any exact sense, it seems to indicate that Hamming and Keller graphs become exceedingly hard as their size increases.
Reference: [7] <author> D. S. Johnson. </author> <title> Approximation algorithms for combinatorial problems. </title> <journal> JCSS, </journal> <volume> no. 9, </volume> <pages> pages 256-278, </pages> <year> 1974. </year>
Reference-contexts: This process is repeated until all of the vertices have been deleted. The algorithm dates back to Johnson <ref> [7] </ref> and has been used in several applications, for example in the work of Lecky, Murphy and Absher [10] on the PLA folding problem. It works well on random graphs and some graphs with large cliques but it is trivial to construct graphs where this algorithm performs arbitrarily badly.
Reference: [8] <author> D. S. Johnson, C.R. Aragon, L.A. McGeoch and C. Schevon. </author> <title> Optimization by simulated annealing: An experimental evaluation; Part II, graph coloring and number partitioning Operations Research, </title> <journal> Vol. </journal> <volume> 39, no. 1, </volume> <year> 1991. </year>
Reference-contexts: By way of comparison we have implemented a version of simulated annealing which uses a penalty function to measure the quality of solutions and can be found in <ref> [8] </ref> . And we have implemented the simple greedy heuristic algorithm of always picking nodes of highest degree to attempt to add to our clique as it is being built. This algorithm can be found, for example, in [10] . <p> The allowed state changes are adding to V or deleting from V a randomly chosen vertex. The penalty function approach to the GRAPH COLORING has been studied in Johnson, et al <ref> [8] </ref>. The cooling schedule is simple: During the first 25% of the annealing time, the temperature parameter T is reduced linearly from 1 to 0:5. During the remaining 75% of the annealing steps T is reduced linearly from 0:5 to 0.
Reference: [9] <author> J.C. Lagarias and P.W. Shor. </author> <title> Keller's cube-packing conjecture is false in high dimensions. </title> <journal> Bull. AMS (New Series), </journal> <volume> Vol. 27, no. 2, </volume> <pages> pages 279-283, </pages> <year> 1992. </year>
Reference-contexts: two-fold: (1) We exhibit the first test data on the Boppana/Halldorsson subgraph exclusion algorithm, a combinatorially interesting algorithm with a non-trivial performance guarantee. (2) We present experiments on the largest graphs yet implemented and tested for approximating maximum size cliques, including Hamming and Keller graphs of practical and theoretical interest <ref> [9] </ref>, [4]. As with many NP-hard optimization problems, the clique problem, has been attacked from two sides. There are exact algorithms which solve the problem to optimality in superpolynomial time. Then there are algorithms which run in polynomial time but return only an approximate solution. <p> The motivation for considering this type of graph is explained below. 4. Graphs that are related to Keller's conjecture <ref> [9] </ref>. 5. Hamming graphs [4]. We have concentrated on the case in which the required Hamming distance is 4. The first three examples were chosen to demonstrate the behavior of the algorithm on graphs with certain well known properties. The last two examples are motivated by practical problems.
Reference: [10] <author> J.E. Lecky, O.J. Murphy and R.G. Absher. </author> <title> Graph theoretic algorithms for the PLA folding problem. </title> <journal> IEEE Trans. on Comp.-Aided Design, </journal> <volume> Vol. 8, no. 9, </volume> <pages> pages 1014-1021, </pages> <year> 1989. </year>
Reference-contexts: And we have implemented the simple greedy heuristic algorithm of always picking nodes of highest degree to attempt to add to our clique as it is being built. This algorithm can be found, for example, in <ref> [10] </ref> . All three algorithms are similar in terms of their running time (they can be made to run very fast) and the fact that they give no, or only very weak, guarantees for their worst case behaviors. <p> This process is repeated until all of the vertices have been deleted. The algorithm dates back to Johnson [7] and has been used in several applications, for example in the work of Lecky, Murphy and Absher <ref> [10] </ref> on the PLA folding problem. It works well on random graphs and some graphs with large cliques but it is trivial to construct graphs where this algorithm performs arbitrarily badly. The Ramsey Algorithm The algorithm is due to Boppana and Halldorsson.
Reference: [11] <author> M. Peinado. </author> <title> Hard graphs for randomized subgraph exclusion algorithms. </title> <type> Manuscript, </type> <note> in preparation. </note>
Reference-contexts: In fact, we have found this to be the case for certain classes of graphs. It is not clear that the performance bounds given in [2] remain tight for this version of the algorithm. Peinado <ref> [11] </ref> investigates the construction of graphs which are hard for this randomized subgraph exclusion algorithm. Simulated Annealing There is a large body of literature on the general principle of simulated annealing and its application to particular optimization problems (e.g. [1]).
Reference: [12] <author> G. H. Sasaki and B. Hajek. </author> <title> The time complexity of maximum matching by simulated annealing. </title> <journal> J. ACM, </journal> <volume> Vol. 35, </volume> <pages> pages 387-403, </pages> <year> 1988. </year>
Reference-contexts: This indicates that the number of cliques declines drastically as the maximum clique size is approached. Versions of this idea have been used in other contexts to prove negative results about simulated annealing [6], <ref> [12] </ref>, [13]. While we cannot prove our observation in any exact sense, it seems to indicate that Hamming and Keller graphs become exceedingly hard as their size increases.
Reference: [13] <author> G. H. Sasaki. </author> <title> The effect of the density of states on the Metropolis algorithm. </title> <journal> Inf. Process. Lett., </journal> <volume> 37, </volume> <pages> pages 159-163, </pages> <year> 1991. </year> <month> 16 </month>
Reference-contexts: This indicates that the number of cliques declines drastically as the maximum clique size is approached. Versions of this idea have been used in other contexts to prove negative results about simulated annealing [6], [12], <ref> [13] </ref>. While we cannot prove our observation in any exact sense, it seems to indicate that Hamming and Keller graphs become exceedingly hard as their size increases. The Connection Machine has also enabled us to run our algorithms on much larger random graphs and random graphs with large embedded cliques.
References-found: 13

