URL: http://www.pdos.lcs.mit.edu/~lew/papers/dimacs94.ps
Refering-URL: http://www.pdos.lcs.mit.edu/~lew/
Root-URL: 
Title: A Case Study of Shared-Memory and Message-Passing Implementations of Parallel Breadth-First Search: The Triangle Puzzle
Author: Kevin Lew, Kirk Johnson, and Frans Kaashoek 
Date: September 15, 1994  
Address: Cambridge, MA 02139  
Affiliation: MIT Laboratory for Computer Science  
Abstract: In this case study, we compare shared-memory and message-passing implementations of parallel breadth-first search that solves the triangle puzzle search problem. The goal of this search problem is to count the number of solutions to a simple puzzle in which a set of pegs is reduced to one peg by jumping over and removing a peg with another, as in checkers. We evaluate these implementations by measuring their total execution times on the MIT Alewife machine, a multiprocessor that supports both shared-memory and message- passing programming models in hardware. The shared- memory implementations explore lock distribution, different lock implementations, an explicit load distribution policy, prefetching strategies, and the distribution of a table of explored positions across different shared-memory banks. The message-passing implementation uses messages only to communicate information about the progress of the search among processors. We have three main conclusions. First, parallel breadth-first search performs better than parallel depth-first search for this search problem. Second, how locks are implemented has a greater impact on performance than how locks are distributed. Lastly, the performance of the message-passing implementation is more scalable 1 than the shared-memory implementations as the number of processors increases. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Applegate, G. Jacobson, and D. </author> <type> Sleator. </type> <institution> Computer Analysis of Sprouts. Carnegie Mellon University, School of Computer Science, </institution> <type> TR 91-144, </type> <month> May </month> <year> 1991. </year>
Reference-contexts: Thus, when an extension of a position is found in the transposition table, the subtree generated from this extension does not need to be explored again, and we can instead join the extension and the position in the transposition table (this is also known as folding or fusing positions) <ref> [1, 24] </ref>. For problem sizes 5, 6, and 7, 66%, 83%, and 90% of all positions explored are joins, respectively 4 . 4. These percentages correspond to exploiting symmetry opti <br>- mally. For a discussion about exploiting symmetry, see Section 3.3. of symmetry. <p> These include compression <ref> [1] </ref>, evicting positions from the transposition table when a threshold is reached [5, 16], pre-loading the transposition table [1], pruning heuristics [10, 22], and move ordering [16]. <p> These include compression <ref> [1] </ref>, evicting positions from the transposition table when a threshold is reached [5, 16], pre-loading the transposition table [1], pruning heuristics [10, 22], and move ordering [16].
Reference: [2] <author> A. Agarwal, et al. Sparcle: </author> <title> An Evolutionary Processor Design for Large-Scale Multiprocessors. </title> <booktitle> IEEE Micro, </booktitle> <month> June </month> <year> 1993, </year> <pages> pp. 48-61. </pages>
Reference-contexts: Each node consists of a 25-MHz Sparcle processor <ref> [2] </ref>, a 64KB cache (direct-mapped, 16-byte cache lines), part of a globally shared distributed memory (4MB per node), a communications and memory management unit (CMMU), a oating-point coprocessor, and a network switch [3].
Reference: [3] <author> A. Agarwal, et al. </author> <title> The MIT Alewife Machine: A LargeScale Distributed-Memory Multiprocessor. </title> <institution> MIT Laboratory for Computer Science, </institution> <note> Technical Memo 454, </note> <month> June </month> <year> 1991. </year>
Reference-contexts: By fine-grained, we mean that the time spent computing between either shared-memory accesses or communication events is short. These implementations run on the MIT Alewife multiprocessor <ref> [3] </ref>. The message-passing implementation was ported from a message-passing implementation that runs on Think- ing Machines CM-5 family of processors [9]. The original CM-5 implementation won first place in an Internet newsgroup contest [13] because it solved the triangle puzzle search problem the fastest. <p> Each node consists of a 25-MHz Sparcle processor [2], a 64KB cache (direct-mapped, 16-byte cache lines), part of a globally shared distributed memory (4MB per node), a communications and memory management unit (CMMU), a oating-point coprocessor, and a network switch <ref> [3] </ref>. The end-to-end latency of an active message [25], in which delivery interrupts the receiving processor, is just over 100 cycles.
Reference: [4] <author> R. Barua. </author> <title> Global Partitioning of Parallel Loops and Data for Caches and Distributed Memory in Multiprocessors. </title> <type> Masters Thesis, </type> <institution> MIT Laboratory for Computer Science, </institution> <month> May </month> <year> 1994. </year> <month> MIT-LCS-TR-630. </month>
Reference-contexts: In Figure 9, we see that the DTTAB implementation generally performs a little worse than the SM implementation. The reason is that the DTTAB implementation uses distributed arrays that are supported by the compiler <ref> [4] </ref>. The indices of slots of the distributed transposition table are calculated at runtime. The benefit of decentralizing the transposition table is probably outweighed by this runtime overhead [18].
Reference: [5] <author> M. Bischoff. </author> <title> Approaches for Solving the Tri-Puzzle. </title> <month> November </month> <year> 1993. </year> <note> Available via anonymous ftp from lucy.ifi.unibas.ch as tri-puzzle/michael/doku.ps. </note>
Reference-contexts: For problem sizes 5 and 6, if each node represents six positions, the number of positions explored is reduced by a factor of two. However, doing the same for problem size 7 cuts the number of positions explored by a factor of six <ref> [5] </ref>. It does not make sense to perform between one and five reflections, causing each node of the search tree to represent between two and six positions. The reason is that a position can be found in two different nodes that have two different canonical positions. <p> These include compression [1], evicting positions from the transposition table when a threshold is reached <ref> [5, 16] </ref>, pre-loading the transposition table [1], pruning heuristics [10, 22], and move ordering [16].
Reference: [6] <author> D. Chaiken and A. Agarwal. </author> <title> Software-Extended Coherent Shared Memory: Performance and Cost. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <year> 1994. </year> <pages> pp. 314-324. </pages>
Reference-contexts: To support this conclusion, we simulated these implementations for problem size 5 on NWOP <ref> [6] </ref>, an Ale- wife simulator that runs on the CM-5 family of multiprocessors. Simulating a 64-node Alewife machine on a 64-node 6.
Reference: [7] <author> S. Chandra, J. Larus, and A. Rogers. </author> <title> Where Is Time Spent in Message-Passing and Shared-Memory implementations? To appear in Proceedings of Architectural Support for Programming Languages and Operating Systems VI, </title> <month> October </month> <year> 1994. </year>
Reference-contexts: In fact, Alewife is the only machine of its class in existence, which makes it a unique platform on which to compare shared-memory and message-passing implementations. Previous research that compares shared-memory and message-passing implementations of the same application have either resorted to only simulation <ref> [7] </ref> or using different hardware platforms [19]. The Stanford FLASH multiprocessor [15] also supports these two programming models in hardware, but FLASH has yet to be built.
Reference: [8] <author> P.-C. Chen. </author> <title> Heuristic Sampling on Backtrack Trees. </title> <type> Ph.D. thesis. </type> <institution> Stanford University. </institution> <month> May </month> <year> 1989. </year> <note> (Also available as Stanford Technical Report CS 89-1258.) </note>
Reference-contexts: For problem size 5 on 64 processors, performance degrades by a factor of three. The triangle puzzle is similar to other tree-search problems, such as the N-queens problem <ref> [8, 11, 24, 26, 27] </ref>, the Hi-Q puzzle [24], and chess [16].
Reference: [9] <institution> Connection Machine CM-5 Technical Summary. Thinking Machines Corporation, </institution> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: By fine-grained, we mean that the time spent computing between either shared-memory accesses or communication events is short. These implementations run on the MIT Alewife multiprocessor [3]. The message-passing implementation was ported from a message-passing implementation that runs on Think- ing Machines CM-5 family of processors <ref> [9] </ref>. The original CM-5 implementation won first place in an Internet newsgroup contest [13] because it solved the triangle puzzle search problem the fastest. Alewife supports both message-passing and shared- memory programming models in hardware.
Reference: [10] <author> L. Crowl, et al. </author> <title> Beyond Data Parallelism: The Advantages of Multiple Parallelizations in Combinatorial Search. </title> <institution> University of Rochester, Dept. of Computer Science, </institution> <type> TR 451, </type> <month> April </month> <year> 1993. </year>
Reference-contexts: These include compression [1], evicting positions from the transposition table when a threshold is reached [5, 16], pre-loading the transposition table [1], pruning heuristics <ref> [10, 22] </ref>, and move ordering [16].
Reference: [11] <author> R. Finkel and U. Manber. </author> <title> DIB--A Distributed Implementation of Backtracking. </title> <journal> ACM Transactions on Programming Languages and Systems, April 1987, </journal> <volume> Vol. 9, No. 2. </volume> <pages> pp. 235256. </pages>
Reference-contexts: For problem size 5 on 64 processors, performance degrades by a factor of three. The triangle puzzle is similar to other tree-search problems, such as the N-queens problem <ref> [8, 11, 24, 26, 27] </ref>, the Hi-Q puzzle [24], and chess [16].
Reference: [12] <author> J. Gittinger; T. Chikayama and K. Kumon. </author> <title> Proofs that there are no solutions for the triangle puzzle for size 3N+1. </title> <note> Available via anonymous ftp from lucy.ifi.unibas.ch in direc-tories tri-puzzle/gitting and tri-puzzle/jm. </note> <month> November </month> <year> 1993. </year>
Reference-contexts: We call the position represented by node B an extension of the position represented by node A. 3. Formal proofs that there are zero solutions for problem sizes 3N+1 (N 2) are given in <ref> [12] </ref>. = peg = hole puzzle. For an initial placement of P pegs, a sequence of P-1 moves is required to arrive at a solution to the triangle puzzle, since each move removes exactly one peg.
Reference: [13] <author> S. Gutzwiller and G. Haechler. </author> <title> Contest: How to Win a Swiss Toblerone Chocolate! August 1993, </title> <publisher> Usenet newsgroup comp.parallel. </publisher>
Reference-contexts: These implementations run on the MIT Alewife multiprocessor [3]. The message-passing implementation was ported from a message-passing implementation that runs on Think- ing Machines CM-5 family of processors [9]. The original CM-5 implementation won first place in an Internet newsgroup contest <ref> [13] </ref> because it solved the triangle puzzle search problem the fastest. Alewife supports both message-passing and shared- memory programming models in hardware. In fact, Alewife is the only machine of its class in existence, which makes it a unique platform on which to compare shared-memory and message-passing implementations. <p> Problem size 5 has 1,550 solutions. Problem size 6 has 29,235,690,234 solutions. Problem size 7 has zero solutions 3 . Because problem size 8 is expected to require several gigabytes of memory <ref> [13] </ref>, no one to the best of our knowledge has solved it yet. 3 Solving the Search Problem This section describes using tree search to solve the triangle puzzle search problem, using a transposition table, exploiting symmetry to optimize the search process, and using two classes of algorithms for tree search.
Reference: [14] <author> P. Hatcher and M. Quinn. </author> <title> Data-Parallel Programming on MIMD Computers. </title> <publisher> The MIT Press: </publisher> <year> 1991. </year>
Reference-contexts: A solution to the puzzle is a sequence of moves that leaves only one peg on the board. Every solution to the triangle puzzle 2 has exactly thirteen moves. Counting the number of distinct solutions to the triangle puzzle is the goal of the triangle puzzle search problem <ref> [14] </ref>. This search problem can be extended for puzzles with boards that have more than five holes on each side. We refer to the number of holes per side as the problem size. We solved problem sizes 5 and 6 on a 16-node Alewife and a 128-node CM-5. <p> The triangle puzzle search problem has been solved by others. Hatcher and Quinn <ref> [14] </ref> solve this problem on different machines that support either shared memory or message passing in hardware.
Reference: [15] <author> J. Kuskin, et al. </author> <title> The Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <year> 1994. </year> <pages> pp. 302-313. </pages>
Reference-contexts: Previous research that compares shared-memory and message-passing implementations of the same application have either resorted to only simulation [7] or using different hardware platforms [19]. The Stanford FLASH multiprocessor <ref> [15] </ref> also supports these two programming models in hardware, but FLASH has yet to be built. We chose to write parallel implementations that solve the triangle search problem because this problem is simple enough to understand and solve, yet exhibits many of the characteristics of complex tree-search problems.
Reference: [16] <author> B. Kuszmaul. </author> <title> Synchronized MIMD Computing. </title> <type> Ph.D. </type> <institution> the-sis. MIT Laboratory for Computer Science. </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: These include compression [1], evicting positions from the transposition table when a threshold is reached <ref> [5, 16] </ref>, pre-loading the transposition table [1], pruning heuristics [10, 22], and move ordering [16]. <p> These include compression [1], evicting positions from the transposition table when a threshold is reached [5, 16], pre-loading the transposition table [1], pruning heuristics [10, 22], and move ordering <ref> [16] </ref>. We did not explore the first two optimizations; the remaining optimizations are not applicable to the triangle puzzle search problem. 3.5 Two Algorithms for Parallel Tree Search Parallel breadth-first search (PBFS) and parallel depth- first search (PDFS) are two classes of algorithms that perform parallel tree search. <p> For problem size 5 on 64 processors, performance degrades by a factor of three. The triangle puzzle is similar to other tree-search problems, such as the N-queens problem [8, 11, 24, 26, 27], the Hi-Q puzzle [24], and chess <ref> [16] </ref>. Many of the search techniques, such as exploiting symmetry to reduce the search space and using a transposition table, arise in solving these problems. 6 Conclusions and Future Work We presented shared-memory and message-passing implementations of parallel breadth-first search that solves the triangle puzzle search problem.
Reference: [17] <author> B.-H. Lim and A. Agarwal. </author> <title> Reactive Synchronization Algo 10 rithms for Multiprocessors. </title> <booktitle> To appear in Proceedings of Architectural Support for Programming Languages and Operating Systems VI, </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: The last two conclusions require further discussion. The 9 second conclusion leads to exploring different lock implementations and multiple contexts to hide or reduce synchronization overhead. One implementation that has not been fully explored is reactive locks <ref> [17] </ref>. Since spin-lock protocols perform well when contention is low, whereas the protocol used in MCS queue locks performs well when contention is high, reactive locks take advantage of this fact by dynamically changing the locking protocol as the level of contention changes.
Reference: [18] <author> B.-H. Lim. </author> <type> Personal communication, </type> <month> September </month> <year> 1994. </year>
Reference-contexts: The reason is that the DTTAB implementation uses distributed arrays that are supported by the compiler [4]. The indices of slots of the distributed transposition table are calculated at runtime. The benefit of decentralizing the transposition table is probably outweighed by this runtime overhead <ref> [18] </ref>. To improve the performance of the DTTAB implementation, one goal for future work is to implement our own distributed arrays and eliminate this runtime overhead. 4.3 Message-Passing Implementation In the message-passing implementation of PBFS, each processor has its own private copy of the four main data structures.
Reference: [19] <author> M. Martonosi and A. Gupta. </author> <title> Trade-offs in Message Passing and Shared Memory Implementations of a Standard Cell Router. </title> <booktitle> In Proceedings of the 1989 International Conference on Parallel Processing. </booktitle> <pages> pp. </pages> <note> III-88 to III-96. </note>
Reference-contexts: Previous research that compares shared-memory and message-passing implementations of the same application have either resorted to only simulation [7] or using different hardware platforms <ref> [19] </ref>. The Stanford FLASH multiprocessor [15] also supports these two programming models in hardware, but FLASH has yet to be built.
Reference: [20] <author> J.M. Mellor-Crummey and M.L. Scott. </author> <title> Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors. </title> <institution> Computer Science Dept., University of Rochester, </institution> <type> TR 342, </type> <month> April </month> <year> 1990. </year>
Reference-contexts: To improve performance, we need to address the problem that a spin lock is a centralized bottleneck. 4.2.2 Different Lock Implementations To overcome the performance problem of spin locks, we employ Mellor-Crummey-Scott (MCS) queue locks <ref> [20] </ref>. In this locking scheme, processors that want to acquire a lock must first wait in a queue. Processors are serviced in a first- in-first-out fashion. When waiting to acquire a lock, each processor spins on a local memory location.
Reference: [21] <author> T. Mowry, M. Lam, and A. Gupta. </author> <title> Design and Evaluation of a Compiler Algorithm for Prefetching. </title> <booktitle> In Proceedings of Architectural Support for Programming Languages and Operating Systems V, </booktitle> <month> October </month> <year> 1992. </year> <pages> pp. 62-73. </pages>
Reference-contexts: Lastly, processors prefetch a pointer to the end of L i+1 , add a position to it, then modify this pointer. Typically, prefetching improves performance when shared data is examined in a predictable pattern, such as in a loop <ref> [21] </ref>. Of the three places prefetching is performed, only one performs prefetching in a loop: when PBFS scans a slot of the transposition table to determine if an extension of a position is already in the table.
Reference: [22] <author> J. Pearl. </author> <title> Heuristics: Intelligent Search Strategies for Computer Problem Solving. </title> <publisher> Addison-Wesley, </publisher> <year> 1984. </year>
Reference-contexts: These include compression [1], evicting positions from the transposition table when a threshold is reached [5, 16], pre-loading the transposition table [1], pruning heuristics <ref> [10, 22] </ref>, and move ordering [16].
Reference: [23] <author> C. Polychronopoulos and D. Kuck. </author> <title> Guided Self-Scheduling: </title>
Reference-contexts: We have not explored this load distribution policy yet, or others which attempt to address this problem, such as more intelligent self-scheduling <ref> [23] </ref>. 4.2.4 Prefetching The P implementation is the same as the SM implementation, but prefetches data in three places. First, processors prefetch one position from L i and its counter of reaching paths after acquiring the lock on L i , but before releasing the lock and extending the position.
References-found: 23

