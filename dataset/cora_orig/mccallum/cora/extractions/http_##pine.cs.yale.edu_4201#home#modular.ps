URL: http://pine.cs.yale.edu:4201/home/modular.ps
Refering-URL: http://pine.cs.yale.edu:4201/home/modular-abstract.html
Root-URL: http://www.cs.yale.edu
Email: E-mail: aspnes-james@cs.yale.edu  E-Mail: waarts@cs.berkeley.edu  
Title: A Modular Measure of Competitive Performance for Distributed Algorithms  
Author: James Aspnes Orli Waarts 
Note: 8285. Supported by NSF grants CCR-9410228 and CCR-9415410.  Supported in part by an NSF postdoctoral fellowship.  
Address: 51 Prospect Street/P.O. Box 208285, New Haven CT 06520  
Affiliation: Yale University, Department of Computer Science,  Computer Science Division, U. C. Berkeley.  
Date: July 18, 1995  
Abstract: We define a novel measure of competitive performance for distributed algorithms based on throughput, the number of tasks that an algorithm can carry out in a fixed amount of work. This new measure complements the latency measure of Ajtai, Aspnes, Dwork, and Waarts [4], which measures how quickly an algorithm can finish tasks that start at specified times. An advantage of the throughput measure is that it is modular: we define a notion of relative competitiveness with the property that a k-relatively competitive implementation of an object T using a subroutine U , combined with an l-competitive implementation of U , gives a kl-competitive algorithm for T . We prove the throughput-competitiveness of an algorithm for a fundamental building block of many well-known distributed algorithms. This permits a straightforward construction of competitive versions of these algorithms; to our knowledge these are the first examples of algorithms obtained through a general method for modular construction of competitive distributed algorithms. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. Abrahamson. </author> <title> On achieving consensus using a shared memory. </title> <booktitle> In Proc. 7th ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pp. 291-302, </pages> <month> August </month> <year> 1988. </year> <month> 16 </month>
Reference: [2] <author> B. Awerbuch and Y. Azar. </author> <title> Local optimization of global objectives: Competitive distributed deadlock resolution and resource allocation. </title> <booktitle> In Proc. 35th IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pp. 240-249, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: In this and subsequent work <ref> [15, 5, 21, 2] </ref>, the cost of a distributed algorithm is compared to the cost of an optimal global-control algorithm. (This is also done implicitly in the earlier work of Awerbuch and Peleg [18].) The idea of having a distributed algorithm compete against other distributed algorithms faced with the same timing
Reference: [3] <author> Y. Afek, H. Attiya, D. Dolev, E. Gafni, M. Merritt, and N. Shavit. </author> <title> Atomic snapshots of shared memory. </title> <booktitle> Proc. 9th ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pp. 1-13, </pages> <year> 1990. </year>
Reference-contexts: Examples include snapshot algorithms <ref> [3, 6, 9, 11, 13, 23] </ref>, the bounded round numbers abstraction [29], concurrent timestamping systems [25, 28, 31, 33, 34, 39], and time-lapse snapshot [28]. Here we elaborate on some simple examples. Atomic snapshots. <p> Snapshot objects are very useful tools for constructing more complicated shared-memory algorithms, and they have been extensively studied <ref> [3, 6, 9, 11, 23] </ref> culminating in the protocol of Attiya and Rachman [13] which uses only O (log n) alternating writes and collects to complete a scan-update operation. We will apply Theorem 4 to get a competitive snapshot. Let T be a snapshot object and U a write-collect object.
Reference: [4] <author> M. Ajtai, J. Aspnes, C. Dwork, and O. Waarts. </author> <title> A theory of competitive analysis for distributed algorithms. </title> <booktitle> In Proc. 33rd IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pp. 401-411, </pages> <month> November </month> <year> 1994. </year> <note> Full version available. </note>
Reference-contexts: 1 Introduction Competitive analysis and distributed algorithms. The tool of competitive analysis [48] has long been used to deal with nondeterminism in the form of unpredictable request sequences in on-line settings, and has recently been extended <ref> [4] </ref> to include the failures and asynchrony found in distributed settings. <p> More recently, the competitive latency model of Ajtai, Aspnes, Dwork, and Waarts <ref> [4] </ref> has taken the approach of applying the same input and schedule to both the on-line and the off-line algorithms. This model has an advantage over its predecessors in cases where changes in the schedule have a bigger impact on performance than changes in the input. <p> A second complication is that the notion of a "clairvoyant" distributed algorithm is inherently problematic. Distributed systems are characterized by having a large number of processes, which do not in general have up-to-date knowledge of other processes' states. However, as observed in <ref> [4] </ref>, allowing clairvoyance in the sense of letting all processes know the entire input and schedule converts distributed algorithms into centralized-control algorithms, since each process can simulate the behavior of the other processes and thereby deduce the entire state of the system at any time. <p> the simplest implementation of a collect by n consecutive reads.) The frequent appearance of collects in shared-memory algorithms motivated the abstraction by Saks, Shavit, and Woll [47] of the collect operation as an object of separate study and its use as a test case for the competitive latency measure of <ref> [4] </ref>. The competitive latency measure counts the amount of work needed to complete tasks starting at specified times in a schedule in which processes may halt (and thus perform no work) in between tasks. In [4] it is shown that collects could be carried out cooperatively with a latency competitiveness of <p> separate study and its use as a test case for the competitive latency measure of <ref> [4] </ref>. The competitive latency measure counts the amount of work needed to complete tasks starting at specified times in a schedule in which processes may halt (and thus perform no work) in between tasks. In [4] it is shown that collects could be carried out cooperatively with a latency competitiveness of O ( p n log 2 n). <p> we prove that any collect algorithm with certain natural properties can be extended to a throughput-competitive implementation of a slightly stronger primitive, a write-collect. (The techniques used to prove throughput-competitiveness are likely to be of more general use.) This method can be applied, for example, to the latency-competitive collect of <ref> [4] </ref> to obtain a throughput-competitive write-collect. In Section 8 we use this write-collect primitive, together with our composition theorem (Theorem 4), to derive competitive versions of many well-known shared-memory algorithms. The competitive ratio of the above collect algorithm is quite high. <p> is compared to the cost of an optimal global-control algorithm. (This is also done implicitly in the earlier work of Awerbuch and Peleg [18].) The idea of having a distributed algorithm compete against other distributed algorithms faced with the same timing patterns was introduced by Ajtai, Aspnes, Dwork, and Waarts <ref> [4] </ref>, motivated by the observation that it is impossible to compete against a global-control algorithm on tasks whose difficulty arises solely from problems of coordination and communication. (Such tasks include nearly all problems studied in the wait-free shared-memory literature). <p> Our competitive throughput model is closely related to the competitive latency model of <ref> [4] </ref>. <p> By combining each write with the following collect we get an object whose operations are all of comparable difficulty, and thus likely to fit well within the throughput-competitive framework. 6 A Throughput-Competitive Implementation of Write-Collect To implement a write-collect we start with the cooperative collect algorithm of <ref> [4] </ref>. This algorithm has several desirable properties: 1. All communication is through a set of single-writer registers, one for each process, and the first operation of the cooperative collect is a write operation. 2. No collect operation ever requires more than 2n steps to complete. 3. <p> For any schedule, and any set of collects that are in progress at some time t, there is a bound on the total number of steps required to complete these collects. (Showing this bound is nontrivial; a proof can be found in <ref> [4] </ref>). These properties are what we need from a cooperative collect implementation to prove that it gives a throughput-competitive write-collect. <p> In effect, our throughput-competitive write-collect algorithm is simply the latency-competitive collect of <ref> [4] </ref>, augmented by merging the write in a write-collect with the first write done as part of the collect implementation. <p> In Section 7, we show a bound on the throughput-competitiveness of any algorithm with the above properties. For the algorithm of <ref> [4] </ref> the proof gives a competitive ratio of O (n 3=4 log 2 n). This is the best algorithm currently known for doing collects. It is likely that better algorithms are possible, although the analysis of competitive collect algorithms can be very difficult. <p> If this quantity is bounded for all , p, and t, denote the bound by UPL (A). Similarly, given an algorithm A, the collective latency at time t is defined <ref> [4] </ref> as the sum over all processes p of the private latency for p at time t, and is denoted by CL (A; t). If this quantity is bounded for all A and , denote the bound by UCL (A). <p> If this quantity is bounded for all A and , denote the bound by UCL (A). Note that UCL (A) may be much smaller than n UPL (A) as concurrent processes may cooperate to finish their collects quickly; for example, in the cooperative collect protocol of <ref> [4] </ref>, UPL = 2n but UCL = O (n 3=2 log 2 n). We denote by FCTh p (A; t) the fractional collective throughput in algorithm A of a process p at point t in time, and define it inductively as follows. <p> The theorem can be applied to give an immediate upper bound on the throughput-competitiveness of any cooperative collect algorithm A for which the private and collective latencies are always bounded. For example, plugging the bounds on the private and collective latencies of the two algo rithms of <ref> [4] </ref> into the formula in Theorem 9 it immediately follows that (i) the non-constructive cooperative collect algorithm of [4] is O (n 3=4 log n)-throughput-competitive; and (ii) the constructive 14 cooperative collect algorithm of [4] is O (n 7=8 log n)-throughput-competitive. <p> For example, plugging the bounds on the private and collective latencies of the two algo rithms of <ref> [4] </ref> into the formula in Theorem 9 it immediately follows that (i) the non-constructive cooperative collect algorithm of [4] is O (n 3=4 log n)-throughput-competitive; and (ii) the constructive 14 cooperative collect algorithm of [4] is O (n 7=8 log n)-throughput-competitive. <p> example, plugging the bounds on the private and collective latencies of the two algo rithms of <ref> [4] </ref> into the formula in Theorem 9 it immediately follows that (i) the non-constructive cooperative collect algorithm of [4] is O (n 3=4 log n)-throughput-competitive; and (ii) the constructive 14 cooperative collect algorithm of [4] is O (n 7=8 log n)-throughput-competitive.
Reference: [5] <author> N. Alon, G. Kalai, M. Ricklin, and L. Stockmeyer. </author> <title> Lower bounds on the competitive ratio for mobile user tracking and distributed job scheduling. </title> <booktitle> In Proc. 33rd IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 334-343, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: In this and subsequent work <ref> [15, 5, 21, 2] </ref>, the cost of a distributed algorithm is compared to the cost of an optimal global-control algorithm. (This is also done implicitly in the earlier work of Awerbuch and Peleg [18].) The idea of having a distributed algorithm compete against other distributed algorithms faced with the same timing
Reference: [6] <author> J. Anderson. </author> <title> Composite registers. </title> <booktitle> Proc. 9th ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pp. 15-30, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: Examples include snapshot algorithms <ref> [3, 6, 9, 11, 13, 23] </ref>, the bounded round numbers abstraction [29], concurrent timestamping systems [25, 28, 31, 33, 34, 39], and time-lapse snapshot [28]. Here we elaborate on some simple examples. Atomic snapshots. <p> Snapshot objects are very useful tools for constructing more complicated shared-memory algorithms, and they have been extensively studied <ref> [3, 6, 9, 11, 23] </ref> culminating in the protocol of Attiya and Rachman [13] which uses only O (log n) alternating writes and collects to complete a scan-update operation. We will apply Theorem 4 to get a competitive snapshot. Let T be a snapshot object and U a write-collect object.
Reference: [7] <author> J. Aspnes. </author> <title> Time- and space-efficient randomized consensus. </title> <journal> Journal of Algorithms 14(3) </journal> <pages> 414-431, </pages> <month> May </month> <year> 1993. </year> <title> An earlier version appeared in Proc. </title> <booktitle> 9th ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pp. 325-331, </pages> <month> August </month> <year> 1990. </year>
Reference: [8] <author> J. Aspnes and M. Herlihy. </author> <title> Fast randomized consensus using shared memory. </title> <note> In Journal of Algorithms 11(3), pp.441-461, </note> <month> September </month> <year> 1990. </year>
Reference: [9] <author> J. Aspnes and M. P. Herlihy. </author> <title> Wait-free data structures in the Asynchronous PRAM Model. </title> <booktitle> In Proceedings of the 2nd Annual Symposium on Parallel Algorithms and Architectures, </booktitle> <month> July </month> <year> 1990, </year> <pages> pp. 340-349, </pages> <address> Crete, Greece. </address>
Reference-contexts: Examples include snapshot algorithms <ref> [3, 6, 9, 11, 13, 23] </ref>, the bounded round numbers abstraction [29], concurrent timestamping systems [25, 28, 31, 33, 34, 39], and time-lapse snapshot [28]. Here we elaborate on some simple examples. Atomic snapshots. <p> Snapshot objects are very useful tools for constructing more complicated shared-memory algorithms, and they have been extensively studied <ref> [3, 6, 9, 11, 23] </ref> culminating in the protocol of Attiya and Rachman [13] which uses only O (log n) alternating writes and collects to complete a scan-update operation. We will apply Theorem 4 to get a competitive snapshot. Let T be a snapshot object and U a write-collect object.
Reference: [10] <author> J. Aspnes and O. Waarts. </author> <title> Randomized consensus in expected O(n log 2 n) operations per processor. </title> <booktitle> In Proc. 33rd IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pp. 137-146, </pages> <month> October </month> <year> 1992. </year>
Reference: [11] <author> H. Attiya, M. Herlihy, and O. Rachman. </author> <title> Efficient atomic snapshots using lattice agreement. </title> <type> Technical report, </type> <institution> Technion, Haifa, Israel, </institution> <year> 1992. </year> <note> A preliminary version appeared in proceedings of the 6th International Workshop on Distributed Algorithms, </note> <institution> Haifa, Israel, </institution> <month> November </month> <year> 1992, </year> <editor> (A. Segall and S. Zaks, eds.), </editor> <booktitle> Lecture Notes in Computer Science #647, </booktitle> <publisher> Springer-Verlag, </publisher> <pages> pp. 35-53. </pages>
Reference-contexts: Examples include snapshot algorithms <ref> [3, 6, 9, 11, 13, 23] </ref>, the bounded round numbers abstraction [29], concurrent timestamping systems [25, 28, 31, 33, 34, 39], and time-lapse snapshot [28]. Here we elaborate on some simple examples. Atomic snapshots. <p> Snapshot objects are very useful tools for constructing more complicated shared-memory algorithms, and they have been extensively studied <ref> [3, 6, 9, 11, 23] </ref> culminating in the protocol of Attiya and Rachman [13] which uses only O (log n) alternating writes and collects to complete a scan-update operation. We will apply Theorem 4 to get a competitive snapshot. Let T be a snapshot object and U a write-collect object.
Reference: [12] <author> H. Attiya, A. Herzberg, and S. Rajsbaum. </author> <title> Optimal clock synchronization under different delay assumptions. </title> <booktitle> In Proc. 12th ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pp. 109-120, </pages> <month> Aug. </month> <year> 1993. </year>
Reference-contexts: In addition, there is a long history of interest in optimality of a distributed algorithm given certain conditions, such as a particular pattern of failures [26, 30, 36, 42, 43, 44], or a particular pattern of message delivery <ref> [12, 32, 46] </ref>.
Reference: [13] <author> H. Attiya and O. Rachman. </author> <title> Atomic snapshots in O(n log n) operations. </title> <booktitle> In Proc. 12th ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pp. 29-40, </pages> <month> Aug. </month> <year> 1993. </year>
Reference-contexts: Examples include snapshot algorithms <ref> [3, 6, 9, 11, 13, 23] </ref>, the bounded round numbers abstraction [29], concurrent timestamping systems [25, 28, 31, 33, 34, 39], and time-lapse snapshot [28]. Here we elaborate on some simple examples. Atomic snapshots. <p> Snapshot objects are very useful tools for constructing more complicated shared-memory algorithms, and they have been extensively studied [3, 6, 9, 11, 23] culminating in the protocol of Attiya and Rachman <ref> [13] </ref> which uses only O (log n) alternating writes and collects to complete a scan-update operation. We will apply Theorem 4 to get a competitive snapshot. Let T be a snapshot object and U a write-collect object.
Reference: [14] <author> B. Awerbuch, Y. Azar, and S. Plotkin. </author> <title> Throughput-competitive on-line routing. </title> <booktitle> In Proc. 34th IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pp. 32-40, </pages> <month> November </month> <year> 1993. </year> <month> 17 </month>
Reference-contexts: Note that there is no direct connection between the use of the term throughput-competitive in the present work, where "throughput" is used in a somewhat metaphorical sense to describe the number of tasks completed by a distributed algorithm; and its use in the on-line routing literature (e.g., <ref> [14] </ref>), where "throughput" is just the traditional measure of network performance. 4 1.3 Possible Extensions Our work defines modular competitiveness and relative competitiveness, by distinguishing between two sources of nondeterminism, one of which is shared between the on-line and off-line algorithms, i.e. the input, and the other is not, i.e. the
Reference: [15] <author> B. Awerbuch, Y. Bartal, and A. Fiat. </author> <title> Competitive distributed file allocation. </title> <booktitle> In Proc. 25th ACM Symposium on Theory of Computing, </booktitle> <pages> pp. 164-173, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: In this and subsequent work <ref> [15, 5, 21, 2] </ref>, the cost of a distributed algorithm is compared to the cost of an optimal global-control algorithm. (This is also done implicitly in the earlier work of Awerbuch and Peleg [18].) The idea of having a distributed algorithm compete against other distributed algorithms faced with the same timing
Reference: [16] <author> B. Awerbuch, Y. Bartal, and A. Fiat. </author> <title> Heat and Dump: competitive distributed paging. </title> <booktitle> In Proc. 34th IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pp. 22-31, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Some examples of specialized constructions of competitive algorithms from other competitive algorithms in a distributed setting are the natural potential function construction of Bartal, Fiat, and Rabani [19] and the distributed paging work of Awerbuch, Bartal, and Fiat <ref> [16] </ref>.
Reference: [17] <author> B. Awerbuch, S. Kutten, and D. Peleg. </author> <title> Competitive distributed job scheduling. </title> <booktitle> In Proc. 24th ACM Symposium on Theory of Computing, </booktitle> <pages> pp. 571-580, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: n competitiveness, and is (n 1=4 = log n) times better than any other known algorithm. 3 1.2 Related Work Competitive analysis has been used to analyze distributed algorithms in the context of data management and job scheduling by Bartal, Fiat, and Rabani [19] and by Awerbuch, Kutten, and Peleg <ref> [17] </ref>.
Reference: [18] <author> B. Awerbuch and D. Peleg. </author> <title> Sparse partitions. </title> <booktitle> In Proc. 31st IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pp. 503-513, </pages> <month> November </month> <year> 1990. </year>
Reference-contexts: In this and subsequent work [15, 5, 21, 2], the cost of a distributed algorithm is compared to the cost of an optimal global-control algorithm. (This is also done implicitly in the earlier work of Awerbuch and Peleg <ref> [18] </ref>.) The idea of having a distributed algorithm compete against other distributed algorithms faced with the same timing patterns was introduced by Ajtai, Aspnes, Dwork, and Waarts [4], motivated by the observation that it is impossible to compete against a global-control algorithm on tasks whose difficulty arises solely from problems of
Reference: [19] <author> Y. Bartal, A. Fiat, and Y. Rabani. </author> <title> Competitive algorithms for distributed data management. </title> <booktitle> In Proc. 24th ACM Symposium on Theory of Computing, </booktitle> <pages> pp. 39-50, </pages> <year> 1992. </year>
Reference-contexts: first to cross the trivial bound of n competitiveness, and is (n 1=4 = log n) times better than any other known algorithm. 3 1.2 Related Work Competitive analysis has been used to analyze distributed algorithms in the context of data management and job scheduling by Bartal, Fiat, and Rabani <ref> [19] </ref> and by Awerbuch, Kutten, and Peleg [17]. <p> Some examples of specialized constructions of competitive algorithms from other competitive algorithms in a distributed setting are the natural potential function construction of Bartal, Fiat, and Rabani <ref> [19] </ref> and the distributed paging work of Awerbuch, Bartal, and Fiat [16].
Reference: [20] <author> E. Borowsky and E. Gafni. </author> <title> Immediate atomic snapshots and fast renaming. </title> <booktitle> In Proc. 12th ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pp. 41-51, </pages> <month> August </month> <year> 1993. </year>
Reference: [21] <author> Y. Bartal, and A. Rosen. </author> <title> The distributed k-server problem A competitive distributed translator for k-server algorithms. </title> <booktitle> In Proc. 33rd IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pp. 344-353, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: In this and subsequent work <ref> [15, 5, 21, 2] </ref>, the cost of a distributed algorithm is compared to the cost of an optimal global-control algorithm. (This is also done implicitly in the earlier work of Awerbuch and Peleg [18].) The idea of having a distributed algorithm compete against other distributed algorithms faced with the same timing
Reference: [22] <author> G. Bracha and O. Rachman. </author> <title> Randomized consensus in expected O(n 2 log n) operations. </title> <booktitle> Proceedings of the Fifth International Workshop on Distributed Algorithms. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference: [23] <author> T. Chandra and C. Dwork. </author> <title> Using consensus to solve atomic snapshots. </title> <note> Submitted for Publication </note>
Reference-contexts: Examples include snapshot algorithms <ref> [3, 6, 9, 11, 13, 23] </ref>, the bounded round numbers abstraction [29], concurrent timestamping systems [25, 28, 31, 33, 34, 39], and time-lapse snapshot [28]. Here we elaborate on some simple examples. Atomic snapshots. <p> Snapshot objects are very useful tools for constructing more complicated shared-memory algorithms, and they have been extensively studied <ref> [3, 6, 9, 11, 23] </ref> culminating in the protocol of Attiya and Rachman [13] which uses only O (log n) alternating writes and collects to complete a scan-update operation. We will apply Theorem 4 to get a competitive snapshot. Let T be a snapshot object and U a write-collect object.
Reference: [24] <author> B. Chor, A. Israeli, and M. Li. </author> <title> On processor coordination using asynchronous hardware. </title> <booktitle> In Proc. 6th ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pp. 86-97, </pages> <year> 1987. </year>
Reference: [25] <author> D. Dolev and N. Shavit. </author> <title> Bounded concurrent time-stamp systems are constructible! In Proc. </title> <booktitle> 21st ACM Symposium on Theory of Computing, </booktitle> <pages> pp. 454-465, </pages> <year> 1989. </year> <note> An extended version appears in IBM Research Report RJ 6785, </note> <month> March </month> <year> 1990. </year>
Reference-contexts: Examples include snapshot algorithms [3, 6, 9, 11, 13, 23], the bounded round numbers abstraction [29], concurrent timestamping systems <ref> [25, 28, 31, 33, 34, 39] </ref>, and time-lapse snapshot [28]. Here we elaborate on some simple examples. Atomic snapshots.
Reference: [26] <author> D. Dolev, R. Reischuk, and H.R. </author> <title> Strong. Early stopping in Byzantine agreement. </title> <journal> JACM 34:7, </journal> <month> Oct. </month> <year> 1990, </year> <pages> pp. 720-741. </pages> <note> First appeared in: Eventual is Earlier than Immediate, </note> <institution> IBM RJ 3915, </institution> <year> 1983. </year>
Reference-contexts: A generalization of this approach has recently been described by Koutsoupias and Papadimitriou [40]. In addition, there is a long history of interest in optimality of a distributed algorithm given certain conditions, such as a particular pattern of failures <ref> [26, 30, 36, 42, 43, 44] </ref>, or a particular pattern of message delivery [12, 32, 46].
Reference: [27] <author> C. Dwork, J. Halpern, and O. Waarts. </author> <title> Accomplishing work in the presence of failures. </title> <booktitle> In Proc. 11th ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pp. 91-102, </pages> <year> 1992. </year>
Reference: [28] <author> C. Dwork, M. Herlihy, S. Plotkin, and O. Waarts. </author> <title> Time-lapse snapshots. </title> <booktitle> Proceedings of Israel Symposium on the Theory of Computing and Systems, </booktitle> <year> 1992. </year>
Reference-contexts: Examples include snapshot algorithms [3, 6, 9, 11, 13, 23], the bounded round numbers abstraction [29], concurrent timestamping systems <ref> [25, 28, 31, 33, 34, 39] </ref>, and time-lapse snapshot [28]. Here we elaborate on some simple examples. Atomic snapshots. <p> Examples include snapshot algorithms [3, 6, 9, 11, 13, 23], the bounded round numbers abstraction [29], concurrent timestamping systems [25, 28, 31, 33, 34, 39], and time-lapse snapshot <ref> [28] </ref>. Here we elaborate on some simple examples. Atomic snapshots. A snapshot object simulates an array of n single-writer registers that support a scan-update operation which writes a value to one of the registers (an "update") and returns a vector of values for all of the registers (a "scan").
Reference: [29] <author> C. Dwork, M. Herlihy, and O. Waarts. </author> <title> Bounded round numbers. </title> <booktitle> In Proc. 12th ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pp. 53-64, </pages> <year> 1993. </year>
Reference-contexts: Examples include snapshot algorithms [3, 6, 9, 11, 13, 23], the bounded round numbers abstraction <ref> [29] </ref>, concurrent timestamping systems [25, 28, 31, 33, 34, 39], and time-lapse snapshot [28]. Here we elaborate on some simple examples. Atomic snapshots. <p> Moreover, the process's actions are not affected by any process whose round number lags behind 15 its own by more than a finite limit. The round numbers increase unboundedly over the lifetime of the system. Dwork, Herlihy and Waarts <ref> [29] </ref> introduced the bounded round numbers abstraction, which can be plugged into any algorithm that uses round numbers in this fashion, transforming it into a bounded algorithm. The bounded round numbers implementation in [29] provides four operations of varying difficulty; however, the use of these operations is restricted. <p> The round numbers increase unboundedly over the lifetime of the system. Dwork, Herlihy and Waarts <ref> [29] </ref> introduced the bounded round numbers abstraction, which can be plugged into any algorithm that uses round numbers in this fashion, transforming it into a bounded algorithm. The bounded round numbers implementation in [29] provides four operations of varying difficulty; however, the use of these operations is restricted. As a result, we can coalesce these operations into a single operation, an advance-collect, which advances the current process's round number to the next round and collects the round numbers of the other processes.
Reference: [30] <author> C. Dwork and Y. Moses. </author> <title> Knowledge and common knowledge in a Byzantine environment: crash failures. </title> <booktitle> In Information and Computation 88(2) (1990), originally in Proc. TARK 1986. </booktitle> <pages> 18 </pages>
Reference-contexts: A generalization of this approach has recently been described by Koutsoupias and Papadimitriou [40]. In addition, there is a long history of interest in optimality of a distributed algorithm given certain conditions, such as a particular pattern of failures <ref> [26, 30, 36, 42, 43, 44] </ref>, or a particular pattern of message delivery [12, 32, 46].
Reference: [31] <author> C. Dwork and O. Waarts. </author> <title> Simple and efficient bounded concurrent timestamping or bounded concurrent timestamp systems are comprehensible!, </title> <booktitle> In Proc. 24th ACM Symposium on Theory of Computing, </booktitle> <pages> pp. 655-666, </pages> <year> 1992. </year>
Reference-contexts: Examples include snapshot algorithms [3, 6, 9, 11, 13, 23], the bounded round numbers abstraction [29], concurrent timestamping systems <ref> [25, 28, 31, 33, 34, 39] </ref>, and time-lapse snapshot [28]. Here we elaborate on some simple examples. Atomic snapshots.
Reference: [32] <author> M. Fischer and A. Michael, </author> <title> Sacrificing serializability to attain high availability of data in an unreliable network. </title> <type> Research Report 221, </type> <institution> Yale U., </institution> <month> Feb. </month> <year> 1982. </year>
Reference-contexts: In addition, there is a long history of interest in optimality of a distributed algorithm given certain conditions, such as a particular pattern of failures [26, 30, 36, 42, 43, 44], or a particular pattern of message delivery <ref> [12, 32, 46] </ref>.
Reference: [33] <author> R. Gawlick, N. Lynch, and N. Shavit. </author> <title> Concurrent timestamping made simple. </title> <booktitle> Proceedings of Israel Symposium on Theory of Computing and Systems, </booktitle> <year> 1992. </year>
Reference-contexts: Examples include snapshot algorithms [3, 6, 9, 11, 13, 23], the bounded round numbers abstraction [29], concurrent timestamping systems <ref> [25, 28, 31, 33, 34, 39] </ref>, and time-lapse snapshot [28]. Here we elaborate on some simple examples. Atomic snapshots.
Reference: [34] <author> S. Haldar. </author> <title> Efficient bounded timestamping using traceable use abstraction Is writer's guessing better than reader's telling? Technical Report RUU-CS-93-28, </title> <institution> Department of Computer Science, Utrecht, </institution> <month> September </month> <year> 1993. </year>
Reference-contexts: Examples include snapshot algorithms [3, 6, 9, 11, 13, 23], the bounded round numbers abstraction [29], concurrent timestamping systems <ref> [25, 28, 31, 33, 34, 39] </ref>, and time-lapse snapshot [28]. Here we elaborate on some simple examples. Atomic snapshots.
Reference: [35] <author> J. Y. Halpern and Y. Moses. </author> <title> Knowledge and common knowledge in a distributed environment, </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> Vol 37, No 3, </volume> <month> January </month> <year> 1990, </year> <pages> pp. 549-587. </pages> <note> A preliminary version appeared in Proc. 3rd ACM Symposium on Principles of Distributed Computing, </note> <year> 1984. </year>
Reference: [36] <author> J.Y. Halpern, Y. Moses, and O. Waarts. </author> <title> A characterization of eventual Byzantine agreement. </title> <booktitle> In Proc. 9th ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pp. 333-346, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: A generalization of this approach has recently been described by Koutsoupias and Papadimitriou [40]. In addition, there is a long history of interest in optimality of a distributed algorithm given certain conditions, such as a particular pattern of failures <ref> [26, 30, 36, 42, 43, 44] </ref>, or a particular pattern of message delivery [12, 32, 46].
Reference: [37] <author> M.P. Herlihy. </author> <title> Randomized wait-free concurrent objects. </title> <booktitle> In Proc. 10th ACM Symposium on Principles of Distributed Computing, </booktitle> <month> August </month> <year> 1991. </year>
Reference: [38] <author> A. Israeli and M. Li. </author> <title> Bounded time stamps. </title> <booktitle> In Proc. 28th IEEE Symposium on Foundations of Computer Science, </booktitle> <year> 1987. </year>
Reference: [39] <author> A. Israeli and M. Pinhasov. </author> <title> A concurrent time-stamp scheme which is linear in time and space. </title> <type> Manuscript, </type> <year> 1991. </year>
Reference-contexts: Examples include snapshot algorithms [3, 6, 9, 11, 13, 23], the bounded round numbers abstraction [29], concurrent timestamping systems <ref> [25, 28, 31, 33, 34, 39] </ref>, and time-lapse snapshot [28]. Here we elaborate on some simple examples. Atomic snapshots.
Reference: [40] <author> E. Koutsoupias and C. Papadimitriou. </author> <title> Beyond competitive analysis. </title> <booktitle> In Proc. 33rd IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pp. 394-400, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: This was introduced by Papadimitriou and Yannakakis [45] in the context of linear programming; their model corresponds to a distributed system with no communication. A generalization of this approach has recently been described by Koutsoupias and Papadimitriou <ref> [40] </ref>. In addition, there is a long history of interest in optimality of a distributed algorithm given certain conditions, such as a particular pattern of failures [26, 30, 36, 42, 43, 44], or a particular pattern of message delivery [12, 32, 46].
Reference: [41] <author> L. M. Kirousis, P. Spirakis and P. Tsigas. </author> <title> Reading many variables in one atomic operation: solutions with linear or sublinear complexity. </title> <booktitle> In Proceedings of the 5th International Workshop on Distributed Algorithms, </booktitle> <year> 1991. </year>
Reference: [42] <author> Y. Moses and M.R. Tuttle. </author> <title> Programming simultaneous actions using common knowledge. </title> <journal> Algorithmica 3(1), </journal> <pages> pp. 121-169, </pages> <booktitle> 1988 (Also appeared in Proc. 28th IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pp. 208-221, </pages> <year> 1986.) </year>
Reference-contexts: A generalization of this approach has recently been described by Koutsoupias and Papadimitriou [40]. In addition, there is a long history of interest in optimality of a distributed algorithm given certain conditions, such as a particular pattern of failures <ref> [26, 30, 36, 42, 43, 44] </ref>, or a particular pattern of message delivery [12, 32, 46].
Reference: [43] <author> G. Neiger. </author> <title> Using knowledge to achieve consistent coordination in distributed systems. </title> <type> Manuscript, </type> <year> 1990. </year>
Reference-contexts: A generalization of this approach has recently been described by Koutsoupias and Papadimitriou [40]. In addition, there is a long history of interest in optimality of a distributed algorithm given certain conditions, such as a particular pattern of failures <ref> [26, 30, 36, 42, 43, 44] </ref>, or a particular pattern of message delivery [12, 32, 46].
Reference: [44] <author> G. Neiger and M. R. Tuttle. </author> <title> Common knowledge and consistent simultaneous coordination. </title> <booktitle> In Proceedings of the 4th International Workshop on Distributed Algorithms, </booktitle> <year> 1990. </year>
Reference-contexts: A generalization of this approach has recently been described by Koutsoupias and Papadimitriou [40]. In addition, there is a long history of interest in optimality of a distributed algorithm given certain conditions, such as a particular pattern of failures <ref> [26, 30, 36, 42, 43, 44] </ref>, or a particular pattern of message delivery [12, 32, 46].
Reference: [45] <author> C.H. Papadimitriou and M. Yannakakis. </author> <title> Linear programming without the matrix. </title> <booktitle> In Proc. 25th ACM Symposium on Theory of Computing, </booktitle> <pages> pp. 121-129, </pages> <month> May </month> <year> 1993. </year> <month> 19 </month>
Reference-contexts: A notion related to allowing only correct distributed algorithms as champions is the idea of comparing algorithms with partial information only against other algorithms with partial information. This was introduced by Papadimitriou and Yannakakis <ref> [45] </ref> in the context of linear programming; their model corresponds to a distributed system with no communication. A generalization of this approach has recently been described by Koutsoupias and Papadimitriou [40].
Reference: [46] <author> B. Patt-Shamir and S. Rajsbaum. </author> <title> A theory of clock synchronization. </title> <booktitle> In Proc. 26th ACM Symposium on Theory of Computing, </booktitle> <pages> pp. 810-819, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: In addition, there is a long history of interest in optimality of a distributed algorithm given certain conditions, such as a particular pattern of failures [26, 30, 36, 42, 43, 44], or a particular pattern of message delivery <ref> [12, 32, 46] </ref>.
Reference: [47] <author> M. Saks, N. Shavit, and H. Woll. </author> <title> Optimal time randomized consensus | making resilient algorithms fast in practice. </title> <booktitle> In Proceedings of the 2nd ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pp. 351-362, </pages> <year> 1991. </year>
Reference-contexts: in the register at the start of the collect or was written to the register during the collect. (This condition is trivially satisfied by the simplest implementation of a collect by n consecutive reads.) The frequent appearance of collects in shared-memory algorithms motivated the abstraction by Saks, Shavit, and Woll <ref> [47] </ref> of the collect operation as an object of separate study and its use as a test case for the competitive latency measure of [4].
Reference: [48] <author> D. D. Sleator and R. E. Tarjan. </author> <title> Amortized efficiency of list update and paging rules. </title> <journal> Comm. of the ACM 28(2), </journal> <pages> pp. 202-208, </pages> <year> 1985. </year>
Reference-contexts: 1 Introduction Competitive analysis and distributed algorithms. The tool of competitive analysis <ref> [48] </ref> has long been used to deal with nondeterminism in the form of unpredictable request sequences in on-line settings, and has recently been extended [4] to include the failures and asynchrony found in distributed settings.
Reference: [49] <author> P. M. B. Vitanyi and B. Awerbuch. </author> <title> Atomic shared register access by asynchronous hardware. </title> <booktitle> In Proc. 27th IEEE Symposium on Foundations of Computer Science, </booktitle> <year> 1986. </year> <month> 20 </month>
References-found: 49

