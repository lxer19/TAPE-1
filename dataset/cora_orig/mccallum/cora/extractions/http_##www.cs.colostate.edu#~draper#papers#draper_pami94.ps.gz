URL: http://www.cs.colostate.edu/~draper/papers/draper_pami94.ps.gz
Refering-URL: http://www.cs.colostate.edu/~draper/
Root-URL: 
Email: bdraper@cs.umass.edu  
Title: Goal-Directed Classification using Linear Machine Decision Trees  
Author: Bruce A. Draper Carla E. Brodley Paul E. Utgoff 
Keyword: Decision Trees, Non-Parametric Classification, Pattern Recognition, Object Recognition, Computer Vision, Machine Learning.  
Date: September 17, 1993  
Address: Amherst, MA., USA. 01003  
Affiliation: Department of Computer Science University of Massachusetts  
Abstract: Recent work in feature-based classification has focused on non-parametric techniques that can classify instances even when the underlying feature distributions are unknown. The inference algorithms for training these techniques, however, are designed to maximize the accuracy of the classifier, with all errors weighted equally. In many applications, certain errors are far more costly than others, and the need arises for non-parametric classification techniques that can be trained to optimize task-specific cost functions. This paper reviews the Linear Machine Decision Tree (LMDT) algorithm for inducing multivariate decision trees, and shows how LMDT can be altered to induce decision trees that minimize arbitrary misclassification cost functions (MCFs). Demonstrations of pixel classification in outdoor scenes show how MCFs can optimize the performance of embedded classifiers within the context of larger image understand ing systems.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Breiman, L., Freidman, J.H., Olshen, R.A., and Stone, C.J. </author> <title> Classification and Regression Trees, </title> <publisher> Wadsworth Inc., </publisher> <address> Belmont, CA., </address> <year> 1984. </year>
Reference-contexts: Because these tests are based on a single input variable, univariate trees can only divide feature space orthogonally to a feature's axis. This introduces a bias that may be inappropriate for problems with linearly related features <ref> [1, 11] </ref>. Utgoff and Brodley [11] overcome this problem by using the perceptron learning rule to induce decision trees in which the tests are linear combinations of features. <p> matcher will verify a false hypothesis can be ignored, and the cost of the system is dominated by the matching algorithm. 3 2 Linear Machine Decision Trees As the name implies, linear machine decision trees are a marriage of two well-known classification techniques, linear machines [8, 5] and decision trees <ref> [1, 9] </ref>. The LMDT algorithm builds a multivariate decision tree from the top down. LMDT trains a linear machine to classify the initial set of training instances by partitioning the feature space into R regions, one for each of the R observed classes. <p> Such a classifier will perform poorly for previously unseen instances. To avoid overfitting, the LMDT classifier is pruned back to reduce the estimated classification error, as computed for an independent set of instances <ref> [1, 10] </ref>. <p> There are many different tree pruning methods, but all use some estimate of the true error to prune back the tree <ref> [1] </ref>. LMDT uses reduced-error pruning, which computes the estimate of the true error using a set of instances that is independent from the training set [10].
Reference: [2] <author> Brodley, C.E. and Utgoff, P.E. </author> <title> "Multivariate Decision Trees", </title> <journal> Machine Learning, </journal> <note> to appear. </note>
Reference-contexts: Linear machine decision trees generalize the two-category multivariate splits permitted by the perceptron training rule to n-category multivariate splits by the use of linear machines <ref> [2] </ref>. Because the linear machine decision tree algorithm (LMDT) is unknown to most vision researchers, we begin by reviewing this powerful yet simple classification technique. The focus of this paper, however, is on the development of goal-directed classifiers. <p> A linear machine infers instance Y to belong to class i if and only if (8j; i 6= j) g i (Y ) &gt; g j (Y ). The LMDT algorithm uses the thermal training procedure <ref> [6, 2] </ref> to find the coefficients of linear machines. Unlike the absolute error correction rule, thermal training ensures convergence to a set of coefficients regardless of whether or not the data is linearly seperable. <p> During training, decreasing attention is paid to large errors by using the correction c = fi fi+k , 2 Throughout this discussion we will assume that all features are in standard normal form, i.e. zero men and unit standard deviation. See Brodley and Utgoff <ref> [2] </ref> for a discussion of normalization and missing feature values. 4 Table 1: Training a Thermal Linear Machine 1. Initialize fi to 2. 2. If linear machine is correct for all instances or fi &lt; 0:001, then return. 3.
Reference: [3] <author> Draper, B., Buluswar, S., Hanson, A. and Riseman, E. </author> <title> "Information Acquisition and Fusion in the Mobile Perception Laboratory," </title> <booktitle> Proc. of Sensor Fusion VI, </booktitle> <address> Boston, MA., </address> <month> Aug. </month> <year> 1993, </year> <pages> pp. 175-187. </pages>
Reference-contexts: Positives Ratio False False Accuracy FN : FP Negatives Positives Test Set 1 : 1 44.6 26.8 83.6 5 : 1 17.8 53.0 83.3 20 : 1 9.2 190.8 79.3 classifier is part of a potential road-following algorithm for the UMass Mobile Perception Laboratory (MPL), an unmanned, outdoor autonomous vehicle <ref> [3] </ref>. The classifier is applied to the RGB pixel values from one-half of a stereo pair of color images, and the resulting labeling is used as a mask for a stereo-disparity algorithm which scans for obstacles.
Reference: [4] <author> Duda, R.O., and Fossum, H. </author> <title> "Pattern Classification by Iteratively Determined Linear and Piecewise Linear Discriminant Functions," </title> <journal> IEEE Trans. on Electronic Computers, </journal> <volume> 15(2) </volume> <pages> 220-232. </pages> <year> 1966. </year>
Reference-contexts: This criterion for reducing fi was selected because the magnitude of a linear machine initially increases rapidly during training, and then stabilizes when the decision boundary is near its final location [5]. 3 k is the well-known absolute error correction coefficient <ref> [4] </ref> 5 2.2 Eliminating features In order to produce accurate and understandable trees that do not evaluate unnecessary features, one wants to eliminate features that do not contribute to classification accuracy at a node. Noisy or irrelevant features may impair classification, and LMDT finds and eliminates such features.
Reference: [5] <author> Duda, R.O., and Hart, P.E. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley & Sons, </publisher> <address> New York. </address> <year> 1973. </year>
Reference-contexts: In goal-directed classification, the penalties for confusing two instance labels is determined by a misclassification cost function (MCF), and the goal of the training algorithm is to minimize the total misclassification cost. Although Bayesian techniques have long been able to minimize arbitrary MCFs <ref> [5] </ref>, non-parametric techniques have not previously been able to do so. <p> result, the possibility that the matcher will verify a false hypothesis can be ignored, and the cost of the system is dominated by the matching algorithm. 3 2 Linear Machine Decision Trees As the name implies, linear machine decision trees are a marriage of two well-known classification techniques, linear machines <ref> [8, 5] </ref> and decision trees [1, 9]. The LMDT algorithm builds a multivariate decision tree from the top down. LMDT trains a linear machine to classify the initial set of training instances by partitioning the feature space into R regions, one for each of the R observed classes. <p> This criterion for reducing fi was selected because the magnitude of a linear machine initially increases rapidly during training, and then stabilizes when the decision boundary is near its final location <ref> [5] </ref>. 3 k is the well-known absolute error correction coefficient [4] 5 2.2 Eliminating features In order to produce accurate and understandable trees that do not evaluate unnecessary features, one wants to eliminate features that do not contribute to classification accuracy at a node. <p> each pair of classes; for each feature, F, LMDT computes 4 If there are fewer instances than the capacity of a hyperplane (twice the dimensionality of the feature vector), then there is insufficient data to pick the best hyperplane orientation, and the linear machine is said to underfit the data. <ref> [5] </ref> 6 dispersion = P nclasses i;j=1 (weight F;i weight F;j ) 2 . The feature with the smallest dispersion is then eliminated.
Reference: [6] <author> Frean, M. </author> <title> Small nets and short paths: Optimising Neural Computation. </title> <type> Ph.D. thesis, </type> <institution> Center for Cognitive Science, University of Edinburgh. </institution> <year> 1990. </year>
Reference-contexts: A linear machine infers instance Y to belong to class i if and only if (8j; i 6= j) g i (Y ) &gt; g j (Y ). The LMDT algorithm uses the thermal training procedure <ref> [6, 2] </ref> to find the coefficients of linear machines. Unlike the absolute error correction rule, thermal training ensures convergence to a set of coefficients regardless of whether or not the data is linearly seperable.
Reference: [7] <author> Mingers, J. </author> <title> "An Empirical Comparison of Pruning Methods for Decision Tree Induction," </title> <journal> Machine Learning, </journal> <volume> 4 </volume> <pages> 227-243. </pages> <year> 1989. </year>
Reference: [8] <author> Nilsson, N.J. </author> <title> Learning Machines. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1965. </year>
Reference-contexts: result, the possibility that the matcher will verify a false hypothesis can be ignored, and the cost of the system is dominated by the matching algorithm. 3 2 Linear Machine Decision Trees As the name implies, linear machine decision trees are a marriage of two well-known classification techniques, linear machines <ref> [8, 5] </ref> and decision trees [1, 9]. The LMDT algorithm builds a multivariate decision tree from the top down. LMDT trains a linear machine to classify the initial set of training instances by partitioning the feature space into R regions, one for each of the R observed classes. <p> To classify an instance, one follows the branch indicated by the linear machine, starting at the root of the tree and working toward the leaves. When a leaf node is reached, the instance is assigned the corresponding label. 2.1 Training a Linear Machine As per Nilsson <ref> [8] </ref>, a linear machine is a set of R linear discriminant functions that are used collectively to assign an instance to one of R classes.
Reference: [9] <author> Quinlan, J.R. </author> <title> "Induction of Decision Trees," </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: Decision trees classify instances by recursively subdividing feature space with hyper-planes until each subdivided region contains instances of a single type. The best known algorithm for inducing decision trees from training instances is Quinlan's ID3 <ref> [9] </ref>, which uses an information-theoretic measure to choose the best hyperplane at each recursive step. Typically, the decision tree is expanded until every training instance is correctly classified, and then the tree is pruned to avoid overfitting to the training data. <p> matcher will verify a false hypothesis can be ignored, and the cost of the system is dominated by the matching algorithm. 3 2 Linear Machine Decision Trees As the name implies, linear machine decision trees are a marriage of two well-known classification techniques, linear machines [8, 5] and decision trees <ref> [1, 9] </ref>. The LMDT algorithm builds a multivariate decision tree from the top down. LMDT trains a linear machine to classify the initial set of training instances by partitioning the feature space into R regions, one for each of the R observed classes.
Reference: [10] <author> Quinlan, J.R. </author> <title> "Simplifying Decision Trees", </title> <journal> International Journal of Man-machine Studies, </journal> <volume> 27 </volume> <pages> 221-234. </pages> <year> 1987. </year>
Reference-contexts: Such a classifier will perform poorly for previously unseen instances. To avoid overfitting, the LMDT classifier is pruned back to reduce the estimated classification error, as computed for an independent set of instances <ref> [1, 10] </ref>. <p> There are many different tree pruning methods, but all use some estimate of the true error to prune back the tree [1]. LMDT uses reduced-error pruning, which computes the estimate of the true error using a set of instances that is independent from the training set <ref> [10] </ref>. When deciding whether to prune back a subtree, the cost of keeping the subtree is compared to the cost of replacing the subtree with a leaf node.

References-found: 10

