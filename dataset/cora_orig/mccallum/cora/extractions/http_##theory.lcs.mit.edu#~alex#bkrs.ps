URL: http://theory.lcs.mit.edu/~alex/bkrs.ps
Refering-URL: http://theory.lcs.mit.edu/~alex/bkrs.html
Root-URL: 
Title: Parallel Algorithms with Processor Failures and Delays  
Author: Jonathan F. Buss Paris C. Kanellakis Prabhakar L. Ragde Alex A. Shvartsman 
Address: Waterloo, Waterloo, Ontario N2L 3G1, Canada.  PO Box 1910, Providence, RI 02912, USA.  Waterloo, Waterloo, Ontario N2L 3G1, Canada.  545 technology Sq., NE43-340, Cam-bridge, MA 02139, USA.  
Affiliation: Department of Computer Science, University of  Computer Science Department, Brown University,  Department of Computer Science, University of  -Massachusetts Institute of Technology, Laboratory for Computer Science,  Brown University.  
Note: To appear in Journal of Algorithms in 1995.  Electronic mail: jfbuss@maytag.waterloo.edu. The research of this author was supported by NSERC Operating Grant OGP0009171.  Electronic mail: pck@cs.brown.edu. The research of this author was supported by NSF grant IRI-8617344 and ONR grant N00014-91-J-1613.  Electronic mail: plragde@maytag.waterloo.edu. The research of this author was supported by NSERC Operating Grant OGP0041913.  Electronic mail: alex@theory.lcs.mit.edu. The work of this author was performed in part at  
Abstract: We study efficient deterministic parallel algorithms on two models: restartable fail-stop CRCW PRAMs and asynchronous PRAMs. In the first model, synchronous processors are subject to arbitrary stop failures and restarts determined by an on-line adversary and involving loss of private but not shared memory; the complexity measures are completed work (where processors are charged for completed fixed-size update cycles) and overhead ratio (completed work amortized over necessary work and failures). In the second model, the result of the computation is a serializaton of the actions of the processors determined by an on-line adversary; the complexity measure is total work (number of steps taken by all processors). Despite their differences the two models share key algorithmic techniques. We present new algorithms for the Write-All problem (in which P processors write ones into an array of size N ) for the two models. These algorithms can be used to implement a simulation strategy for any N processor PRAM on a restartable fail-stop P processor CRCW PRAM such that it guarantees a terminating execution of each simulated N processor step, with O(log 2 N ) overhead ratio, and O(minfN + P log 2 N + M log N , N P 0:59 g) (sub-quadratic) completed work (where M is the number of failures during this step's simulation). This strategy has a range of optimality. We also show that the Write-All requires N + (P log P ) completed/total work on these models for P N . 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. B. Adams III, D. P. Agrawal, H. J. Seigel, </author> <title> "A Survey and Comparison of Fault-tolerant Multistage Interconnection Networks", </title> <journal> IEEE Computer, </journal> <volume> 20, 6, </volume> <pages> pp. 14-29, </pages> <year> 1987. </year>
Reference-contexts: A combining interconnection network that is well suited for implementing synchronous concurrent reads and writes is studied in [27] (the combining properties are used in their simplest form only to implement concurrent access to memory). The network can be made more reliable by employing redundancy <ref> [1] </ref>. With this architecture, our algorithmic techniques become applicable; i.e., the algorithms and simulations we develop will work correctly, and within the claimed complexity bounds (under the uniform cost memory access assumption) when the underlying components are subject to the failures within their respective design parameters.
Reference: [2] <author> Y. Afek, B. Awerbuch, S. Plotkin, M. Saks, </author> <title> "Local Management of a Global Resource in a Communication Network", </title> <booktitle> Proc. of the 28th IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pp. 347-357, </pages> <year> 1987. </year>
Reference: [3] <author> R. Anderson, </author> <title> "Parallel Algorithms for Generating Random Permutations on a Shared Memory Machine", </title> <booktitle> Proc. of the 2nd ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 95-102, </pages> <year> 1990. </year>
Reference-contexts: The goal in this situation, when the underlying machine is synchronous, is to find a method whose parallel time complexity is at most the sequential time complexity divided by the number of processors plus a small additive overhead; see <ref> [3] </ref> for an example of such an algorithm. Note that constant factors are important and cannot be hidden in O-notation. When considering algorithms on fail-stop or asynchronous models, the goal is to have the parallel work complexity be equal to the sequential complexity plus small overhead.
Reference: [4] <author> R. Anderson and H. Woll, </author> <title> "Wait-Free Parallel Algorithms for the Union-Find Problem", </title> <booktitle> Proc. of the 23rd ACM Symposium on Theory of Computing, </booktitle> <pages> pp. 370-380, </pages> <year> 1991. </year>
Reference-contexts: asynchronous models? Upper bounds: Is O (N log O (1) N ) completed/total work for solving Write-All with N processors and input of size N achievable in the restartable fail-stop/asynchronous model? Recently, an existence proof for an algorithm achieving O (N 1+* ) work was given by Anderson and Woll <ref> [4] </ref>. In the fail-stop no restart model, Lopez-Ortiz recently exhibited the known worst fail-stop work for algorithm X of fi (N log 2 N= log log N ) [29].
Reference: [5] <author> J. Aspnes and M. Herlihy, </author> <title> "Wait-Free Data Structures in the Asynchronous PRAM Model", </title> <booktitle> Proc. of the 2nd ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 340-349, </pages> <year> 1990. </year>
Reference: [6] <author> S. Assaf and E. Upfal, </author> <title> "Fault Tolerant Sorting Network," </title> <booktitle> in Proc. of the 31st IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pp. 275-284, </pages> <year> 1990. </year>
Reference: [7] <author> J. Buss and P. Ragde, </author> <title> "Certified Write-All on a Strongly Asynchronous PRAM", </title> <type> manuscript, </type> <year> 1990. </year>
Reference-contexts: These stalking adversaries are described in Section 6, where we also conclude with some open problems. Preliminary versions of this work were reported in <ref> [7, 22] </ref>. 1.3 Motivation and relation to physical systems The models we present and study are intended to capture certain features of actual systems. Processor delay and failure: Processor delay is a feature of any multi-user environment, in which processing priorities are not specified by a single user. <p> Each processor uses some constant amount of private memory to perform simple arithmetic computations. An important private constant is PID, containing the initial processor identifier. 4 ALGORITHMS FOR THE WRITE-ALL PROBLEM 17 0 1 2 3 4 5 6 <ref> [7] </ref> fl fl fl fl fl C CO C CW fl C CW C A Q A AK s s Thus, the overall memory used is O (N + P ) and the data-structures are simple. Control-flow: The algorithm consists of a single initialization and of the parallel loop.
Reference: [8] <author> R. Cole and O. Zajicek, </author> <title> "The APRAM: Incorporating Asynchrony into the PRAM Model," </title> <booktitle> in Proc. of the 1989 ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 170-178, </pages> <year> 1989. </year>
Reference: [9] <author> R. Cole and O. Zajicek, </author> <title> "The Expected Advantage of Asynchrony," </title> <booktitle> in Proc. 2nd ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 85-94, </pages> <year> 1990. </year>
Reference-contexts: Some of the models in these last two papers are similar to our restartable fail-stop model, but failures are probabilistic and restarts do not destroy private memory. Because of our worst-case assumptions, these analyses are inappropriate. Furthermore, notions of time used in <ref> [9] </ref> do not work here, because our scheduling adversary may introduce arbitrarily long delays. The notion of wait-free asynchronous computation, in which any one processor terminates in a finite number of steps regardless of the speeds of the other processors, is introduced in [16].
Reference: [10] <author> B. Chor, A. Israeli and M. Li, </author> <title> "On processor coordination using asynchronous hardware", </title> <booktitle> in Proc. of the 6th ACM Symp. on Principles of Distributed Computing, </booktitle> <pages> pp. 86-97, </pages> <year> 1987. </year>
Reference-contexts: When randomization is used, it is possible to construct efficient simulations for off-line adversaries as recently shown by Kedem et al. [24]. When asynchronous processors also have initial private data, the computational capability of the model is further moderated by the asynchronous consensus impossibility results <ref> [10, 16, 30] </ref>. 2.3 Comparison of the models On the surface, the two models of restartable fail-stop processors and of asynchronous processors are designed for quite different situations.
Reference: [11] <author> F. Cristian, </author> <title> "Understanding Fault-Tolerant Distributed Systems", </title> <journal> in Communications of the ACM, </journal> <volume> vol. 3, no. 2, </volume> <pages> pp. 56-78, </pages> <year> 1991. </year>
Reference-contexts: This architecture is more abstract than, for example, a realization in terms of hypercubes, but it is simpler to program in. Moreover, basic fault-tolerant technologies (as described in surveys <ref> [11, 18, 19] </ref>) contribute towards concrete realizations of its components. 1. There are P fail-stop processors (see [40]), each with a unique address and some local memory. 2. There are Q shared memory cells, the input of size N Q is stored in shared memory.
Reference: [12] <author> C. Dwork, D. Peleg, N. Pippenger, E. Upfal, </author> <title> "Fault Tolerance in Networks of Bounded Degree", </title> <booktitle> in Proc. of the 18th ACM Symposium on Theory of Computing, </booktitle> <pages> pp. 370-379, </pages> <year> 1986. </year>
Reference: [13] <author> D. Eppstein and Z. Galil, </author> <title> "Parallel Techniques for Combinatorial Computation", </title> <booktitle> Annual Computer Science Review, 3 (1988), </booktitle> <pages> pp. 233-83. </pages>
Reference: [14] <author> S. Fortune and J. Wyllie, </author> <title> "Parallelism in Random Access Machines", </title> <booktitle> Proc. the 10th ACM Symposium on Theory of Computing, </booktitle> <pages> pp. 114-118, </pages> <year> 1978. </year>
Reference-contexts: For the processors, we allow any dynamic pattern of fail-stop failures and restarts. 2 Models of computation 2.1 The restartable fail-stop CRCW PRAM We use as a basis the PRAM model of Fortune and Wyllie <ref> [14] </ref>, where all concurrently writing processors write the same value (common CRCW). Processors are subject to stop failures and restarts as in [40]. Our algorithms are described using the forall/parbegin/parend parallel construct. 2 MODELS OF COMPUTATION 5 1. There are P synchronous processors. <p> The parameters of the update cycle, i.e., the number of read and write instructions, are fixed, but depend on the instruction set of the PRAM; see <ref> [14] </ref> for a typical PRAM instruction set. The values quoted (4 and 2) are sufficient for our exposition. It is an interesting question whether smaller values would suffice to implement efficient algorithms. We use the fail-stop with restart failure model, where time instances are the PRAM synchronous clock-ticks: 1.
Reference: [15] <author> P. Gibbons, </author> <title> "A More Practical PRAM Model," </title> <booktitle> in Proc. of the 1989 ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 158-168, </pages> <year> 1989. </year>
Reference: [16] <author> M. P. Herlihy, </author> <title> "Impossibility and universality results for wait-free synchronization", </title> <booktitle> in Proc. of the 7th ACM Symp. on Principles of Distributed Computing, </booktitle> <pages> pp. 276-290, </pages> <year> 1988. </year> <note> REFERENCES 27 </note>
Reference-contexts: Furthermore, notions of time used in [9] do not work here, because our scheduling adversary may introduce arbitrarily long delays. The notion of wait-free asynchronous computation, in which any one processor terminates in a finite number of steps regardless of the speeds of the other processors, is introduced in <ref> [16] </ref>. In the asynchronous PRAM, by definition any algorithm with bounded work must be wait-free. The same paper shows that atomic reads and writes are insufficient to solve two-processor consensus, and demonstrates a hierarchy of stronger primitives for accessing memory (such as test-and-set or compare-and-swap). <p> When randomization is used, it is possible to construct efficient simulations for off-line adversaries as recently shown by Kedem et al. [24]. When asynchronous processors also have initial private data, the computational capability of the model is further moderated by the asynchronous consensus impossibility results <ref> [10, 16, 30] </ref>. 2.3 Comparison of the models On the surface, the two models of restartable fail-stop processors and of asynchronous processors are designed for quite different situations.
Reference: [17] <author> S. W. Hornick and F. P. Preparata, </author> <title> "Deterministic P-RAM: Simulation with Constant Redundancy," </title> <booktitle> in Proc. of the 1989 ACM Symposium on Parallel Algorithms and Architectures., </booktitle> <pages> pp. 103-109, </pages> <year> 1989. </year>
Reference: [18] <author> IEEE Computer, </author> <title> "Fault-Tolerant Computing," </title> <journal> special issue, </journal> <volume> vol. 17, no. 8, </volume> <year> 1984. </year>
Reference-contexts: This architecture is more abstract than, for example, a realization in terms of hypercubes, but it is simpler to program in. Moreover, basic fault-tolerant technologies (as described in surveys <ref> [11, 18, 19] </ref>) contribute towards concrete realizations of its components. 1. There are P fail-stop processors (see [40]), each with a unique address and some local memory. 2. There are Q shared memory cells, the input of size N Q is stored in shared memory.
Reference: [19] <author> IEEE Computer, </author> <title> "Fault-Tolerant Systems", </title> <journal> (special issue), </journal> <volume> vol. 23, no. 7, </volume> <year> 1990. </year>
Reference-contexts: This architecture is more abstract than, for example, a realization in terms of hypercubes, but it is simpler to program in. Moreover, basic fault-tolerant technologies (as described in surveys <ref> [11, 18, 19] </ref>) contribute towards concrete realizations of its components. 1. There are P fail-stop processors (see [40]), each with a unique address and some local memory. 2. There are Q shared memory cells, the input of size N Q is stored in shared memory.
Reference: [20] <author> C. Kaklamanis, A. Karlin, F. Leighton, V. Milenkovic, P. Raghavan, S. Rao, C. Thomborson, A. Tsantilas, </author> <title> "Asymptotically Tight Bounds for Computing with Arrays of Processors," </title> <booktitle> in Proc. of the 31st IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pp. 285-296, </pages> <year> 1990. </year>
Reference: [21] <author> P. C. Kanellakis and A. A. Shvartsman, </author> <title> "Efficient Parallel Algorithms Can Be Made Robust", </title> <journal> Distributed Computing, </journal> <volume> vol. 5, </volume> <pages> pp. 201-217, </pages> <year> 1992; </year> <month> prel. </month> <title> version in Proc. </title> <booktitle> of the 8th ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pp. 211-222, </pages> <year> 1989. </year>
Reference-contexts: In Section 4 we present three efficient algorithms for the Write-All problem. The first (algorithm V ) is a modification of the algorithm of Kanellakis and Shvartsman <ref> [21] </ref> for the fail-stop no-restart model, and runs on the restartable fail-stop model with completed work O (N +P log 2 where M is the number of failures. This algorithm is based on an analysis of the lower bounds in Section 3. <p> Algorithms using this assumption can be easily converted to use only single bit atomic writes as in <ref> [21] </ref>. 2 MODELS OF COMPUTATION 6 We investigate two natural complexity measures, completed work and overhead ratio. The completed work measure generalizes the standard Parallel-time fi Processors product and the Available Processor Steps (S) of [21]. The overhead ratio is an amortized measure. <p> this assumption can be easily converted to use only single bit atomic writes as in <ref> [21] </ref>. 2 MODELS OF COMPUTATION 6 We investigate two natural complexity measures, completed work and overhead ratio. The completed work measure generalizes the standard Parallel-time fi Processors product and the Available Processor Steps (S) of [21]. The overhead ratio is an amortized measure. Definition 2.2 Consider an algorithm with P initial processors that terminates in time t after completing its task on some input data I and in the presence of a failure pattern F . <p> When M = O (P ) as in the case of the stop 2 MODELS OF COMPUTATION 7 failures without restarts in <ref> [21] </ref>, S properly describes the algorithm efficiency, and = O ( S N;M;P However, when F can be large relative to N and P (as is the case when restarts are allowed) better reflects the efficiency of a fault-tolerant algorithm. <p> However, update cycles are necessary for the following two reasons. Update cycles and termination: Our failure model requires that at any time, at least one processor is executing an update cycle that completes. (This condition subsumes the condition of <ref> [21] </ref> that one processor does not fail during the computation). This requirement is formulated in terms of update cycles and assures that some progress is made. Since the processors lose their context after a failure, they have to read something to regain it. <p> - ~ P=j = V 0 + O @ P j=1 j + 1 A = N + O (P log P ) . 2 Remark: Under the memory snapshot assumption, it can be shown that the (N log N= log log N ) lower bound of Kanellakis and Shvartsman <ref> [21] </ref> is the best possible bound for failures without restarts. This is done by adapting the analysis of algorithm W by Martel [32]. <p> Nonpowers of 2 can be handled using conventional padding techniques. All logarithms are base 2. 4.1 Algorithm V : a modification of a no-restart algorithm Algorithm W of <ref> [21] </ref> is an efficient fail-stop (no restart) Write-All solution. The algorithm uses two full binary trees as its basic data structures (the processor counting and the progress measurement trees). <p> Even if the extended algorithm were to terminate, its completed work is not bounded by a function of N and P . In addition, the proof framework of <ref> [21] </ref> does not easily extend to include processor restarts: the processor enumeration and allocation phases become inefficient and possibly incorrect, since no accurate estimates of active processors can be obtained when the adversary can revive any of the failed processors at any time. <p> Therefore we present a modified version of algorithm W , that we call V . To avoid a complete restatement of the details of algorithm V , the reader is urged to refer to <ref> [21] </ref>. V uses the data structures of the optimized algorithm W of [21] (i.e., full binary trees with N log N leaves) for progress estimation and processor allocation. There are log N array elements associated with each leaf. <p> Therefore we present a modified version of algorithm W , that we call V . To avoid a complete restatement of the details of algorithm V , the reader is urged to refer to <ref> [21] </ref>. V uses the data structures of the optimized algorithm W of [21] (i.e., full binary trees with N log N leaves) for progress estimation and processor allocation. There are log N array elements associated with each leaf. <p> As in the proof of Theorem 3.3, when the first N log N P leaves are visited, there is no more than one processor allocated to each leaf by the balanced allocation phase (ballanced allocation is assured as in algorithm W <ref> [21] </ref>). When the remaining P or less leaves are visited, the work is O (P log P ) by Theorem 3.3 (not counting processor allocation). <p> For the details on this technique, the reader is referred to <ref> [21, 25, 42] </ref>. Application of these techniques in conjunction with the algorithms V and X 0 yield efficient and terminating executions of any non-fault-tolerant PRAM programs in the presence of arbitrary failure and restart patterns. Theorem 5.1 follows from Theorem 5.2 and the results of [25] or [42].
Reference: [22] <author> P. C. Kanellakis and A. A. Shvartsman, </author> <title> "Efficient Parallel Algorithms On Restartable Fail-Stop Processors", </title> <booktitle> in Proc. of the 10th ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pp. 23-36, </pages> <year> 1991. </year>
Reference-contexts: These stalking adversaries are described in Section 6, where we also conclude with some open problems. Preliminary versions of this work were reported in <ref> [7, 22] </ref>. 1.3 Motivation and relation to physical systems The models we present and study are intended to capture certain features of actual systems. Processor delay and failure: Processor delay is a feature of any multi-user environment, in which processing priorities are not specified by a single user.
Reference: [23] <author> R. M. Karp and V. Ramachandran, </author> <title> "A Survey of Parallel Algorithms for Shared-Memory Machines", </title> <note> in Handbook of Theoretical Computer Science (ed. </note> <editor> J. van Leeuwen), </editor> <publisher> North-Holland, </publisher> <year> 1990. </year>
Reference: [24] <author> Z. M. Kedem, K. V. Palem, M. O. Rabin and A. Raghunathan, </author> <title> "Efficient Program Transformations for Resilient Parallel Computation via Randomization," </title> <booktitle> in Proc. 24th ACM Symposium on Theory of Computing, </booktitle> <pages> pp. 306-318, </pages> <year> 1992. </year>
Reference-contexts: It does not appear to be the case that efficient deterministic simulations are possible in the asynchronous model. When randomization is used, it is possible to construct efficient simulations for off-line adversaries as recently shown by Kedem et al. <ref> [24] </ref>.
Reference: [25] <author> Z. M. Kedem, K. V. Palem, and P. Spirakis, </author> <title> "Efficient Robust Parallel Computations," </title> <booktitle> in Proc. 22nd ACM Symposium on Theory of Computing, </booktitle> <pages> pp. 138-148, </pages> <year> 1990. </year>
Reference-contexts: An approach for executing arbitrary PRAM programs on fail-stop CRCW PRAMs (without restart) was presented independently in <ref> [25] </ref> and [42]. The execution is based on simulating individual PRAM computation steps using the Write-All paradigm. <p> Here we describe how algorithms V and X 0 are combined with the framework of <ref> [25] </ref> or [42] to yield efficient executions of PRAM programs on PRAMs that are subject to stop-failures and restarts as stated in Theorem 5.1. 5 GENERAL SIMULATIONS ON RESTARTABLE FAIL-STOP PROCESSORS 24 Theorem 5.2 There exists a Write-All solution using P N processors on instances of size N such that for <p> For the details on this technique, the reader is referred to <ref> [21, 25, 42] </ref>. Application of these techniques in conjunction with the algorithms V and X 0 yield efficient and terminating executions of any non-fault-tolerant PRAM programs in the presence of arbitrary failure and restart patterns. Theorem 5.1 follows from Theorem 5.2 and the results of [25] or [42]. <p> Application of these techniques in conjunction with the algorithms V and X 0 yield efficient and terminating executions of any non-fault-tolerant PRAM programs in the presence of arbitrary failure and restart patterns. Theorem 5.1 follows from Theorem 5.2 and the results of <ref> [25] </ref> or [42]. The following corollaries are also interesting: Corollary 5.3 Under the hypothesis of Theorem 5.1, and if jF j P N , then: S = O (N + P log N ), and = O (log N ).
Reference: [26] <author> Z. M. Kedem, K. V. Palem, A. Raghunathan, and P. Spirakis, </author> <title> "Combining Tentative and Definite Executions for Dependable Parallel Computing," </title> <booktitle> in Proc 23d ACM STOC, </booktitle> <pages> pp. 381-390, </pages> <year> 1991. </year>
Reference-contexts: Note that even given the lower bound of Kedem et al. <ref> [26] </ref>, our lower bound results are still of interest because: (a) they demonstrate that any improvement to the lower bound must take account of the fact that processors can read only a constant number of cells in constant time, (b) they present a simple processor allocation strategy that we use to <p> A stronger result was given by Kedem et al. <ref> [26] </ref> who showed similar lower bounds but for a more constrained (fail-stop no-restart) model. The bound in [26] can also be extended to test-and-set operations. The results in this section are of interest for various reasons. <p> A stronger result was given by Kedem et al. <ref> [26] </ref> who showed similar lower bounds but for a more constrained (fail-stop no-restart) model. The bound in [26] can also be extended to test-and-set operations. The results in this section are of interest for various reasons. The analysis of algorithm V in Section 4 uses the bounds shown in Theorems 3.1 and 3.3. <p> Recently, Martel and Subramonian [33] have extended the Kedem et al. deterministic lower bound <ref> [26] </ref> to randomized algorithms against off-line adversaries. The lower bounds of Section 3 apply to both the worst case performance of deterministic algorithms and the expected performance of randomized algorithms subject to on-line adversaries. A randomized asynchronous coupon clipping (ACC) algorithm for Write-All was analyzed by Martel et al. [34].
Reference: [27] <author> C. P. Kruskal, L. Rudolph, M. Snir, </author> <title> "Efficient Synchronization on Multiprocessors with Shared Memory," </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> vol. 10, no. 4, </volume> <pages> pp. </pages> <month> 579-601 </month> <year> 1988. </year>
Reference-contexts: Processors and memory are interconnected via a synchronous network (e.g., as in the Ultra-computer [41]). A combining interconnection network that is well suited for implementing synchronous concurrent reads and writes is studied in <ref> [27] </ref> (the combining properties are used in their simplest form only to implement concurrent access to memory). The network can be made more reliable by employing redundancy [1].
Reference: [28] <author> L. Lamport, </author> <title> "On Interprocess Communication", </title> <booktitle> Distributed Computing, 1 (1986), </booktitle> <pages> pp. 77-101. </pages>
Reference-contexts: This change in cost measure allows sub-quadratic solutions. 2.2 The Asynchronous PRAM The asynchronous PRAM model departs from the standard PRAM models in that the processors are completely asynchronous. The only synchronizing assumption is that reads and writes to memory are atomic and serializable, in the sense of Lamport <ref> [28] </ref>.
Reference: [29] <author> A. Lopez-Ortiz, </author> <title> "Algorithm X takes work (n log 2 n= log log n) in a synchronous fail-stop (no restart) PRAM", </title> <type> unpublished manuscript, </type> <year> 1992. </year>
Reference-contexts: In the fail-stop no restart model, Lopez-Ortiz recently exhibited the known worst fail-stop work for algorithm X of fi (N log 2 N= log log N ) <ref> [29] </ref>. As the corollary of this result, the upper bound for algorithm X is no better than the upper bound for algorithm W for the fail-stop no-restart model.
Reference: [30] <author> M. Loui and H. Abu-Amara, </author> <title> "Memory requirements for agreement among unreliable asynchronous processes," </title> <booktitle> in Advances in Computing Research, </booktitle> <volume> 4, </volume> <pages> pp. 163-183, </pages> <year> 1987. </year>
Reference-contexts: When randomization is used, it is possible to construct efficient simulations for off-line adversaries as recently shown by Kedem et al. [24]. When asynchronous processors also have initial private data, the computational capability of the model is further moderated by the asynchronous consensus impossibility results <ref> [10, 16, 30] </ref>. 2.3 Comparison of the models On the surface, the two models of restartable fail-stop processors and of asynchronous processors are designed for quite different situations.
Reference: [31] <author> N.A. Lynch, N.D. Griffeth, M.J. Fischer, L.J. Guibas, </author> <title> "Probabilistic Analysis of a Network Resource Allocation Algorithm", </title> <journal> Information and Control, </journal> <volume> vol. 68, </volume> <pages> pp. 47-85, </pages> <year> 1986. </year>
Reference: [32] <author> C. </author> <title> Martel, </title> <type> personal communication, </type> <month> March, </month> <year> 1991. </year>
Reference-contexts: This is done by adapting the analysis of algorithm W by Martel <ref> [32] </ref>. According to the analysis, the number of "block-steps" of W for P = N is O (N log N= log log N ) and each block-step can be realized at unit cost using memory snaphsots. A similar situation holds in the asynchronous model.
Reference: [33] <author> C. Martel and R. Subramonian, </author> <title> "On the Complexity of Certified Write-All Algorithms", to appear in Journal of Algorithms (a prel. version is in the Proc. </title> <booktitle> of the 12th Conference on Foundations of Software Technology and Theoretical Computer Science, </booktitle> <address> New Delhi, India, </address> <month> December </month> <year> 1992). </year> <note> REFERENCES 28 </note>
Reference-contexts: Recently, Martel and Subramonian <ref> [33] </ref> have extended the Kedem et al. deterministic lower bound [26] to randomized algorithms against off-line adversaries. The lower bounds of Section 3 apply to both the worst case performance of deterministic algorithms and the expected performance of randomized algorithms subject to on-line adversaries.
Reference: [34] <author> C. Martel, R. Subramonian, and A. Park, </author> <title> "Asynchronous PRAMs are (Almost) as Good as Synchronous PRAMs," </title> <booktitle> in Proc. 32d IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pp. 590-599, </pages> <year> 1990. </year> <note> Also see Tech. Rep. </note> <institution> CSE-89-6, Univ. of Calif.-Davis, </institution> <year> 1989. </year>
Reference-contexts: Randomization does not seem to help, given on-line (non-prespecified) patterns of failures. For example, it is easy to construct on-line failure and restart (resp. no-restart) patterns that lead to exponential (resp. quadratic) in N expected performance for the algorithms presented in <ref> [34] </ref>. These stalking adversaries are described in Section 6, where we also conclude with some open problems. Preliminary versions of this work were reported in [7, 22]. 1.3 Motivation and relation to physical systems The models we present and study are intended to capture certain features of actual systems. <p> The lower bounds of Section 3 apply to both the worst case performance of deterministic algorithms and the expected performance of randomized algorithms subject to on-line adversaries. A randomized asynchronous coupon clipping (ACC) algorithm for Write-All was analyzed by Martel et al. <ref> [34] </ref>. Assuming off-line adversaries, it was shown in [34] that ACC algorithm performs expected O (N ) work using P = N=(log N log fl N ) processors on inputs of size N . <p> A randomized asynchronous coupon clipping (ACC) algorithm for Write-All was analyzed by Martel et al. <ref> [34] </ref>. Assuming off-line adversaries, it was shown in [34] that ACC algorithm performs expected O (N ) work using P = N=(log N log fl N ) processors on inputs of size N .
Reference: [35] <author> N. Nishimura, </author> <title> "Asynchronous Shared Memory Parallel Computation," </title> <booktitle> in Proc. 3rd ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 76-84, </pages> <year> 1990. </year>
Reference: [36] <author> N. Pippenger, </author> <title> "On Networks of Noisy Gates", </title> <booktitle> Proc. of 26th IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pp. 30-38, </pages> <year> 1985. </year>
Reference: [37] <author> A. Ranade, </author> <title> "How to Emulate Shared Memory", </title> <booktitle> Proc. of 28th IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pp. 185-194, </pages> <year> 1987. </year>
Reference: [38] <author> L. Rudolph, </author> <title> "A Robust Sorting Network", </title> <journal> IEEE Trans. on Comp., </journal> <volume> vol. 34, no. 4, </volume> <pages> pp. 326-335, </pages> <year> 1985. </year>
Reference: [39] <author> D. B. Sarrazin and M. Malek, </author> <title> "Fault-Tolerant Semiconductor Memories", </title> <journal> IEEE Computer, </journal> <volume> vol. 17, no. 8, </volume> <pages> pp. 49-56, </pages> <year> 1984. </year>
Reference-contexts: There are Q shared memory cells, the input of size N Q is stored in shared memory. These semiconductor memories can be manufactured with built-in fault tolerance using replication and coding techniques without appreciably degrading performance <ref> [39] </ref>. 3. Processors and memory are interconnected via a synchronous network (e.g., as in the Ultra-computer [41]).
Reference: [40] <author> R. D. Schlichting and F. B. Schneider, </author> <title> "Fail-Stop Processors: an Approach to Designing Fault-tolerant Computing Systems", </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> vol. 1, no. 3, </volume> <pages> pp. 222-238, </pages> <year> 1983. </year>
Reference-contexts: This architecture is more abstract than, for example, a realization in terms of hypercubes, but it is simpler to program in. Moreover, basic fault-tolerant technologies (as described in surveys [11, 18, 19]) contribute towards concrete realizations of its components. 1. There are P fail-stop processors (see <ref> [40] </ref>), each with a unique address and some local memory. 2. There are Q shared memory cells, the input of size N Q is stored in shared memory. These semiconductor memories can be manufactured with built-in fault tolerance using replication and coding techniques without appreciably degrading performance [39]. 3. <p> Processors are subject to stop failures and restarts as in <ref> [40] </ref>. Our algorithms are described using the forall/parbegin/parend parallel construct. 2 MODELS OF COMPUTATION 5 1. There are P synchronous processors. Each processor has a unique permanent identifier (pid) in the range 0; : : :; P 1, and each processor has access to P and its own pid. 2. <p> Since the processors lose their context after a failure, they have to read something to regain it. Without at least one active update cycle completing, the adversary can force the PRAM to thrash by allowing only these reads to be performed. Similar concerns are discussed in <ref> [40] </ref>. Update cycles as a unit of accounting: In our definition of completed work we only count completed update cycles.
Reference: [41] <author> J. T. Schwartz, </author> <title> "Ultracomputers", </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> vol. 2, no. 4, </volume> <pages> pp. 484-521, </pages> <year> 1980. </year>
Reference-contexts: These semiconductor memories can be manufactured with built-in fault tolerance using replication and coding techniques without appreciably degrading performance [39]. 3. Processors and memory are interconnected via a synchronous network (e.g., as in the Ultra-computer <ref> [41] </ref>). A combining interconnection network that is well suited for implementing synchronous concurrent reads and writes is studied in [27] (the combining properties are used in their simplest form only to implement concurrent access to memory). The network can be made more reliable by employing redundancy [1].
Reference: [42] <author> A. A. Shvartsman, </author> <title> "Achieving Optimal CRCW PRAM Fault-Tolerance", </title> <journal> Information Processing Letters, </journal> <volume> vol. 39, no. 2, </volume> <pages> pp. 59-66, </pages> <year> 1991. </year>
Reference-contexts: same type. 2 Remark: Priority CRCW PRAMs cannot be directly simulated using the same framework, for one of the algorithms used (namely algorithm X in Section 4) does not possess the processor allocation monotonicity property that assures that higher numbered processors simulate the steps of the higher numbered original processors <ref> [42] </ref>. An approach for executing arbitrary PRAM programs on fail-stop CRCW PRAMs (without restart) was presented independently in [25] and [42]. The execution is based on simulating individual PRAM computation steps using the Write-All paradigm. <p> used (namely algorithm X in Section 4) does not possess the processor allocation monotonicity property that assures that higher numbered processors simulate the steps of the higher numbered original processors <ref> [42] </ref>. An approach for executing arbitrary PRAM programs on fail-stop CRCW PRAMs (without restart) was presented independently in [25] and [42]. The execution is based on simulating individual PRAM computation steps using the Write-All paradigm. <p> Here we describe how algorithms V and X 0 are combined with the framework of [25] or <ref> [42] </ref> to yield efficient executions of PRAM programs on PRAMs that are subject to stop-failures and restarts as stated in Theorem 5.1. 5 GENERAL SIMULATIONS ON RESTARTABLE FAIL-STOP PROCESSORS 24 Theorem 5.2 There exists a Write-All solution using P N processors on instances of size N such that for any pattern <p> For the details on this technique, the reader is referred to <ref> [21, 25, 42] </ref>. Application of these techniques in conjunction with the algorithms V and X 0 yield efficient and terminating executions of any non-fault-tolerant PRAM programs in the presence of arbitrary failure and restart patterns. Theorem 5.1 follows from Theorem 5.2 and the results of [25] or [42]. <p> Application of these techniques in conjunction with the algorithms V and X 0 yield efficient and terminating executions of any non-fault-tolerant PRAM programs in the presence of arbitrary failure and restart patterns. Theorem 5.1 follows from Theorem 5.2 and the results of [25] or <ref> [42] </ref>. The following corollaries are also interesting: Corollary 5.3 Under the hypothesis of Theorem 5.1, and if jF j P N , then: S = O (N + P log N ), and = O (log N ).
Reference: [43] <author> A. A. Shvartsman, </author> <title> "Efficient Write-All Algorithm for Fail-Stop PRAM Without Initialized Memory", </title> <journal> Information Processing Letters, </journal> <volume> vol. 44, no. 6, </volume> <pages> pp. 223-231, </pages> <year> 1992. </year>
Reference: [44] <author> E. Upfal, </author> <title> "An O(log N) Deterministic Packet Routing Scheme," </title> <booktitle> in Proc. 21st ACM Symposium on Theory of Computing, </booktitle> <pages> pp. 241-250, </pages> <year> 1989. </year>
Reference: [45] <author> L. Valiant, </author> <title> "General Purpose Parallel Architectures," </title> <note> in Handbook of Theoretical Computer Science (ed. </note> <editor> J. van Leeuwen), </editor> <publisher> North-Holland, </publisher> <year> 1990. </year>
Reference: [46] <author> L. Valiant, </author> <title> "A Bridging Model for Parallel Computation," </title> <journal> Communications of the ACM, </journal> <volume> vol. 33, no. 8, </volume> <pages> pp. 103-111, </pages> <year> 1990. </year>
References-found: 46

