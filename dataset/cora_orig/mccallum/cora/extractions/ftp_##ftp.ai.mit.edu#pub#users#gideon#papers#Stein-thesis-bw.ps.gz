URL: ftp://ftp.ai.mit.edu/pub/users/gideon/papers/Stein-thesis-bw.ps.gz
Refering-URL: http://www.ai.mit.edu/people/gideon/Thesis/thesis-instructions.html
Root-URL: 
Title: Geometric and Photometric Constraints: Motion and Structure from Three Views  
Author: by Gideon P. Stein W. Eric L. Grimson Arthur C. Smith 
Degree: (1993) Submitted to the Department of Electrical Engineering and Computer Science in partial fulfillment of the requirements for the degree of Doctor of Philosophy at the  Signature of Author  Certified by  Professor of Computer Science and Engineering Thesis Supervisor Certified by Amnon Shashua Senior Lecturer of Computer Science,  Thesis Supervisor Accepted by  Chairman, Departmental Committee on Graduate Students  
Note: c Massachusetts Institute of Technology  
Date: June 1998  1998  February 27,1998  
Address: (1990)  Jerusalem, Israel  
Affiliation: B.Sc., Technion, Israel Institute of Technology  M.S., Massachusetts Institute of Technology  Massachusetts Institute of Technology  Department of Electrical Engineering and Computer Science  Hebrew University of  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> M. Abdel-Mottaleb, R. Chellappa, and A. </author> <title> Rosenfield. Binocular motion stereo using map estimation. </title> <booktitle> In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 321-327, </pages> <address> New York, NY, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: One also computes a confidence measure for each depth estimate. Then one registers the stereo pair with the current 3D model to compute the motion and finally one updates the model (using a Kalman filter or a batch method). Abdel-Mottaleb et al <ref> [1] </ref> use the motion to get a rough depth map in one image which is then used to simplify the correspondence problem between the images in a stereo pair. Waxman & Duncan [82] first use the monocular optical flow to determine depth discontinuities which cause problems in stereo matching.
Reference: [2] <author> S. Avidan and A. Shashua. </author> <title> Tensorial transfer: On the representation of n &gt; 3 views of a 3D scene. </title> <booktitle> In Proceedings of the ARPA Image Understanding Workshop, </booktitle> <address> Palm Springs, CA, </address> <month> February </month> <year> 1996. </year>
Reference-contexts: The link between the two and the generalization to projective space was identified later by Hartley [24, 25]. Additional work in this area can be found in <ref> [62, 17, 80, 30, 59, 2] </ref>, and applications in [3, 74]. 2.4.1 Tensor Slices and Contractions From Figure 2-1 it is clear that the tensor can be sliced in three principle ways. This can be generalized into performing a contraction operation on the tensor with a vector.
Reference: [3] <author> S. Avidan and A. Shashua. </author> <title> View synthesis in tensor space. </title> <booktitle> In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <address> Puerto Rico, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: The link between the two and the generalization to projective space was identified later by Hartley [24, 25]. Additional work in this area can be found in [62, 17, 80, 30, 59, 2], and applications in <ref> [3, 74] </ref>. 2.4.1 Tensor Slices and Contractions From Figure 2-1 it is clear that the tensor can be sliced in three principle ways. This can be generalized into performing a contraction operation on the tensor with a vector.
Reference: [4] <author> A. Azarbayejani and A. P. Pentland. </author> <title> Recursive estimation of motion, structure and focal length. </title> <journal> pami, </journal> <volume> 17(6) </volume> <pages> 562-575, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: After computing the correspondences the camera motion between two or three views and the 3D structure of the scene are recovered using geometric constraints. Here we include both `optical flow' based methods [42, 43, 28, 55, 77] and feature based methods <ref> [79, 16, 65, 67, 4] </ref>. The main problem with these approaches is that computing the correspondence is in itself a difficult and error prone process. Dense correspondence methods such as `optical flow' suffer from the aperture problem and it is necessary to assume smoothness in the flow [33, 43, 49]. <p> Then the correspondences are used to compute the camera motion and scene structure. The advantage of our method over both optical flow methods [42, 43, 28, 55, 77] and feature based methods <ref> [79, 16, 65, 67, 4] </ref> is that no prior computation of correspondences is needed, a computation which in itself is error prone. Since we obtain a linear system of equations that combines the information from all the pixels in the image, we avoid the aperture problem.
Reference: [5] <author> J. R. Bergen, P. Anandan, K. J. Hanna, and R. Hingorani. </author> <title> Hierarchical model-based motion estimation. </title> <booktitle> In Proceedings of the European Conference on Computer Vision, </booktitle> <address> Santa Margherita Ligure, Italy, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: This stabilizes regions where there is no image gradient. 5.4.3 Coarse to Fine Processing and Iterative Refinement In order to deal with image motions larger than 1 pixel we use a Gaussian pyramid for coarse to fine processing <ref> [5, 10] </ref>. For a 640 fi 480 image we used a 5 level pyramid. The linear solution can be thought of as a single iteration of Newton's method applied to the problem. At each level of the pyramid we iterate as follows: 1. Calculate motion (using equation 5.8). 2.
Reference: [6] <author> J. Bergen, P. Anandan, K. Hanna, and R. Hingorani. </author> <title> Hierarchical model-based motion estimation. </title> <booktitle> In Proceedings of the European Conference on Computer Vision, </booktitle> <address> Santa Margherita Ligure, Italy, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Figure 3-2 shows a detail of the optical flow field in the upper left of the scene where these two problems occur. The flow was computed using a widely available, `industrial strength', optical flow program <ref> [7, 6] </ref>. (For more examples see Section (4.2.3).) Finally, the texture on the plaster bust varies smoothly and is low contrast thereby giving rise only to a relatively small number of reliable features to track. <p> The program is based on code by Bergen & Hingorani of the Sarnoff Corp <ref> [7, 6] </ref>. We use two sets of images which have been chosen to be particularly difficult for optical flow programs. Experimental Procedure and a black metal bar in front of a background of vertical stripes. The two motions are pure translation, vertical and horizontal, parallel to the image plane.
Reference: [7] <author> J. Bergen and R. Hingorani. </author> <title> Hierarchical motion-based frame rate conversion. </title> <type> Technical report, </type> <institution> David Sarnoff Research Center, </institution> <year> 1990. </year>
Reference-contexts: Figure 3-2 shows a detail of the optical flow field in the upper left of the scene where these two problems occur. The flow was computed using a widely available, `industrial strength', optical flow program <ref> [7, 6] </ref>. (For more examples see Section (4.2.3).) Finally, the texture on the plaster bust varies smoothly and is low contrast thereby giving rise only to a relatively small number of reliable features to track. <p> The program is based on code by Bergen & Hingorani of the Sarnoff Corp <ref> [7, 6] </ref>. We use two sets of images which have been chosen to be particularly difficult for optical flow programs. Experimental Procedure and a black metal bar in front of a background of vertical stripes. The two motions are pure translation, vertical and horizontal, parallel to the image plane.
Reference: [8] <author> D. C. Brown. </author> <title> Close-range camera calibration. </title> <journal> Photogrammetric Engineering, </journal> <volume> 37 </volume> <pages> 855-866, </pages> <year> 1971. </year>
Reference-contexts: Projective constraints: Under perspective projection, straight lines in space project to straight lines in the image. With real lenses the lines appear instead to be slightly to moderately curved. By searching for lens distortion parameters which straighten the lines the Plumb Line method and its derivatives <ref> [8] </ref> [20] [71] [12] find the lens distortion without needing to find the external parameters or the other internal camera parameters. One or more images can be used.
Reference: [9] <author> T. Buchanan. </author> <title> On the critical set for photogrammetric reconstruction using line tokens in p3(c). </title> <journal> Geometriae Dedicata, </journal> <volume> 44 </volume> <pages> 223-232, </pages> <year> 1992. </year>
Reference-contexts: In general this system of equations has a unique linear solution but for particular 3D line configurations in space this system of equations is degenerate. There are some critical line configurations from which a unique solution is not possible ( see <ref> [44, 9, 50] </ref>). There are other configurations where a linear solution is degenerate but the additional non-linear constraints lead to a unique solution. <p> Note also that the vertical line representing the lamp-post also meets the edge of the building (at infinity) thereby included in the 1 This in contrast to critical line configurations from which a unique solution is not possible, see <ref> [44, 9, 50] </ref> LLC configuration. This leaves very few lines (the sidewalk and the oblique line of the lamp-post) not part of the LLC. The common line in Figure 6-1b is the edge of the book-case leaning on the wall.
Reference: [10] <author> P. J. Burt and E. H. Adelson. </author> <title> The laplacian pyramid as a compact image code. </title> <journal> IEEE Transactions on Communications, </journal> <volume> 31 </volume> <pages> 532-540, </pages> <year> 1983. </year>
Reference-contexts: This stabilizes regions where there is no image gradient. 5.4.3 Coarse to Fine Processing and Iterative Refinement In order to deal with image motions larger than 1 pixel we use a Gaussian pyramid for coarse to fine processing <ref> [5, 10] </ref>. For a 640 fi 480 image we used a 5 level pyramid. The linear solution can be thought of as a single iteration of Newton's method applied to the problem. At each level of the pyramid we iterate as follows: 1. Calculate motion (using equation 5.8). 2.
Reference: [11] <author> R. Deriche and G. Giraudon. </author> <title> Accurate corner detection: An analytical study. </title> <booktitle> In Proceedings of the International Conference on Computer Vision, </booktitle> <pages> pages 66-70, </pages> <address> Osaka, Japan, </address> <month> December </month> <year> 1990. </year>
Reference-contexts: Sparse methods try to find matching features such as lines or corner points. It is still an open subject of research as to what makes a good feature <ref> [11, 83, 16] </ref>. Some points and lines in the image do not correspond to any real feature in 3D (e.g. the occluding contour of a cylinder or of the head in Figure 1-1). This issue is discussed in more depth in the introduction to Chapter 3. <p> These might be either dense correspondence in the form of optical flow [33, 43, 49] or feature correspondence <ref> [11, 83, 16] </ref>. Then the correspondences are used to compute the camera motion and scene structure.
Reference: [12] <author> F. Devernay and O. D. Faugeras. </author> <title> Automatic calibration and removal of distortion from scenes of structured environments. </title> <booktitle> In Proceedings of SPIE Conference, </booktitle> <address> San Diego, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Projective constraints: Under perspective projection, straight lines in space project to straight lines in the image. With real lenses the lines appear instead to be slightly to moderately curved. By searching for lens distortion parameters which straighten the lines the Plumb Line method and its derivatives [8] [20] [71] <ref> [12] </ref> find the lens distortion without needing to find the external parameters or the other internal camera parameters. One or more images can be used.
Reference: [13] <author> F. Du and M. Brady. </author> <title> Self calibration of the intrinsic parameters of cameras for active vision systems. </title> <booktitle> In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 477-482, </pages> <address> New York, NY, </address> <month> June </month> <year> 1993. </year> <month> 165 </month>
Reference-contexts: One or more images can be used. Unknown world coordinates: Stein [71], and Du and Brady <ref> [13] </ref> use corresponding points or edges in images where the camera has undergone pure rotation to find the internal camera parameters including lens distortion. The 3D location of the points is not required. 148 7.3 Mathematical Background We will review here the basic relevant material from Chapter 2.
Reference: [14] <author> R. Dutta and M. Snyder. </author> <title> Robustness of correspondence based structure from motion. </title> <booktitle> In Proceedings of the International Conference on Computer Vision, </booktitle> <pages> pages 106-110, </pages> <address> Osaka, Japan, </address> <month> December </month> <year> 1990. </year>
Reference-contexts: Figure 4-27 and Figure 4-28 show results from a more complex scene. Errors in depth reconstruction when we neglect the rotation are to be expected according to <ref> [14] </ref>. They show that for small total rotation angles (jwj) the error in 105 Table 4.1: Rotation estimates (in degrees) from the images in Figure 4-23. <p> In our case by assuming pure translation we get a rotation error of 1:45 o or 0:025rad (Table 4.1). The average image motions were on the order of 15pixels and the focal length f 180. So we expect: ffiZ Z <ref> [14] </ref> refers to depth from image motion. The errors we observe are not so large but are definitely noticeable.
Reference: [15] <author> O. Faugeras and T. Papadopoulo. </author> <title> A nonlinear method for estimating the projective geometry of three views. </title> <note> To be published ICCV 98, </note> <month> January </month> <year> 1998. </year>
Reference-contexts: Likewise, the contraction ffi j T jk i is a homography matrix from Image 1 onto Image 3, denoted W. For proof and applications see [62]. 2.4.2 Tensor Admissibility Constraints The 27 coefficients T jk i are not independent. It has been shown in <ref> [15] </ref> and [58] that the tensor is determined by only 18 parameters. We will first present the explanation from [58]. We will then show how from the contraction properties discussed above, one can come up with the constraints among the 27 coefficients, referred to as admissibility constraints. <p> In practice, in the presence of errors in image measurements one often starts with the linear solution (that might not satisfy the admissibility constraints) and improves it further by employing a numerical Gauss-Newton style iterative procedure until a solution that satisfies the admissibility constraints is obtained (for example, <ref> [15] </ref>). 2.5 Photometric Constraints the Optical Flow Constraint Equation We now move on to the photometric constraints between two images. We will derive the optical flow constraint equation following [31]. <p> In general, 7 corresponding points or 13 corresponding lines, in 3 views, give enough equations in form of equation (2.40) or equation (2.42) to solve for the tensor coefficients using linear methods. One can then refine the tensor coefficients to enforce the algebraic constraints <ref> [15] </ref>. As we show in Chapter 6 there are certain line configurations where the linear solution is degenerate but the admissibility constraints enable us to arrive at a unique solution. <p> In order to do so we have treated the 15 intermediate parameters as linearly independent parameters while they are in fact bilinear combinations of 12 independent parameters. It is often possible to improve the motion estimates by using the linear solution as a starting point for non-linear optimization techniques <ref> [15] </ref>. In our experiments we did not find that a nonlinear optimization stage improved the results. This is possibly because the iterative framework (Section 3.7.2) is in itself a form of nonlinear optimization. The nonlinear optimization procedure is brought here for completeness.
Reference: [16] <author> O. D. Faugeras. </author> <title> Three Dimensional Computer Vision: a Geometric Viewpoint. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1993. </year>
Reference-contexts: After computing the correspondences the camera motion between two or three views and the 3D structure of the scene are recovered using geometric constraints. Here we include both `optical flow' based methods [42, 43, 28, 55, 77] and feature based methods <ref> [79, 16, 65, 67, 4] </ref>. The main problem with these approaches is that computing the correspondence is in itself a difficult and error prone process. Dense correspondence methods such as `optical flow' suffer from the aperture problem and it is necessary to assume smoothness in the flow [33, 43, 49]. <p> Sparse methods try to find matching features such as lines or corner points. It is still an open subject of research as to what makes a good feature <ref> [11, 83, 16] </ref>. Some points and lines in the image do not correspond to any real feature in 3D (e.g. the occluding contour of a cylinder or of the head in Figure 1-1). This issue is discussed in more depth in the introduction to Chapter 3. <p> To build a complete model of a scene we would like to combine together multiple 3D views. One solution is to use a moving stereo rig (two cameras mounted onto the same rigid frame). The standard approach to reconstruction from a moving stereo rig <ref> [39, 91, 85, 16] </ref> has been to first compute a depth map from each pair (or compute the depth of a 19 (a) (b) to show detail. set of features points), together with a confidence measure for each depth estimate. <p> We then develop the constraint equations among point and line correspondences in two and three views. These are well known results based mainly on the work of Faugeras <ref> [16] </ref> for two views, and on Shashua's work on the trilinear tensor constraint for 3 views [57]. Chapters 3 through 5 should be read in sequence. <p> This thesis uses results from projective geometry. A very readable introduction to the subject of projective geometry is given in the book by Young [89]. A modern book dealing more specifically with the application of projective geometry to computer vision is <ref> [16] </ref>. The appendix of [48] provides a concise summary of the basic material. 2.1 Notation We will use uppercase bold to denote matrices (e.g. A). Vectors representing 3D points which will be upper case and not bold. Other vectors and scalars will be in lower case. <p> Translation from camera position of Image 1 to the camera position of Image 2 will be denoted t 0 , translation from camera position of Image 1 to the camera position of Image 3 will be denoted t 00 , and so forth. 2.1.1 Points and Lines Following <ref> [16] </ref> we use projective coordinates. A point in three-dimensional space is represented by a point in the projective space P 3 , P ~ = (X; Y; Z; W ) &gt; . ( ~ = indicates an equality up to a scale factor). <p> H 1 p 0 and 2 : Equation (2.33) can then be written in the form: p 0&gt; ~ H 1 EH 2 p 0 1 Fp 0 where ~ H 1 = H (1) 1 , and F = ~ H 1 EH 2 is called the fundamental matrix <ref> [16] </ref>. 40 views obtained from cameras centered at o, o 0 and o 00 respectively. <p> These might be either dense correspondence in the form of optical flow [33, 43, 49] or feature correspondence <ref> [11, 83, 16] </ref>. Then the correspondences are used to compute the camera motion and scene structure. <p> Then the correspondences are used to compute the camera motion and scene structure. The advantage of our method over both optical flow methods [42, 43, 28, 55, 77] and feature based methods <ref> [79, 16, 65, 67, 4] </ref> is that no prior computation of correspondences is needed, a computation which in itself is error prone. Since we obtain a linear system of equations that combines the information from all the pixels in the image, we avoid the aperture problem. <p> The motion estimates between stereo pairs enable us to combine depth maps from all the pairs in the sequence to form an extended scene reconstruction. The typical approach to reconstruction from a moving stereo rig <ref> [39, 91, 85, 16, 88, 22] </ref> has been to first compute a depth map from each pair (or 3D location of features points). One also computes a confidence measure for each depth estimate.
Reference: [17] <author> O. D. Faugeras and B. Mourrain. </author> <title> On the geometry and algebra of the point and line correspondences between N images. </title> <booktitle> In Proceedings of the International Conference on Computer Vision, </booktitle> <pages> pages 951-956, </pages> <address> Cambridge, MA, June 1995. </address> <publisher> IEEE Computer Society Press, IEEE Computer Society Press. </publisher>
Reference-contexts: The link between the two and the generalization to projective space was identified later by Hartley [24, 25]. Additional work in this area can be found in <ref> [62, 17, 80, 30, 59, 2] </ref>, and applications in [3, 74]. 2.4.1 Tensor Slices and Contractions From Figure 2-1 it is clear that the tensor can be sliced in three principle ways. This can be generalized into performing a contraction operation on the tensor with a vector. <p> Navab et al [51] have shown that two line correspondences is enough. We can trace this inconsistency to the fact that we do not use Image I 00 i+1 . It was not used because Faugeras & Mourrain <ref> [17] </ref> show that there are no constraints among four (or more) images that are not simply two and three image constraints. Thus it was assumed that other image triplets would provide redundant equations (which also be used to give better noise properties).
Reference: [18] <author> C. Fermuller and Y. Aloimonos. </author> <title> Global rigidity constraints in image displacement fields. </title> <booktitle> In Proceedings of the International Conference on Computer Vision, </booktitle> <address> Cambridge, MA, June 1995. </address> <publisher> IEEE Computer Society Press, IEEE Computer Society Press. </publisher>
Reference-contexts: The epipole of the residual motion is found using iterative techniques starting with an initial guess. Others who have explored planar alignment include [36, 56]. The theoretical framework for planar alignment can also be found in [60]. More recently, Fermuller & Aloimonos <ref> [18] </ref> describe global geometric properties of the flow-field that give rise to direct relationships between the measurement of normal flow and the ego-motion parameters solved by means of search techniques. The tensor brightness constraint was first presented in [64] and a practical implementation with results was first described in [74].
Reference: [19] <author> C. Fermuller and Y. Aloimonos. </author> <title> On the geometry of visual correspondence. </title> <journal> International Journal of Computer Vision, </journal> <volume> 21(3) </volume> <pages> 223-47, </pages> <month> February </month> <year> 1997. </year>
Reference-contexts: The errors we observe are not so large but are definitely noticeable. We also note that even if we neglect the rotation none of the depth estimates were negative so a depth is positive constraint, as suggested by <ref> [19] </ref>, would not help in this example. 4.4.2 Experiment 8: Outdoor Scenes Church Wall In this experiment we tested the algorithm on outdoor scenes. These experiments are important because they show that the photometric constraints can be used in uncontrolled lighting conditions.
Reference: [20] <author> J. G. Fryer and S. O. Mason. </author> <title> Rapid lens calibration of a video camera. </title> <journal> Photogrammetric Engineering, </journal> <volume> 55(4) </volume> <pages> 437-442, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: Projective constraints: Under perspective projection, straight lines in space project to straight lines in the image. With real lenses the lines appear instead to be slightly to moderately curved. By searching for lens distortion parameters which straighten the lines the Plumb Line method and its derivatives [8] <ref> [20] </ref> [71] [12] find the lens distortion without needing to find the external parameters or the other internal camera parameters. One or more images can be used.
Reference: [21] <author> P. Fua. </author> <title> Reconstructing complex surfaces from multiple stereo views. </title> <booktitle> In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 1078-1085. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> June </month> <year> 1995. </year>
Reference-contexts: By concatenating these motion estimates one can bring all the depth maps into one coordinate frame. Combining incremental motion in this way will accumulate errors. To get a more accurate reconstruction one can use the results as a starting point for global reconstruction schemes such as <ref> [21, 23] </ref>. 5.1.1 Overview of the Chapter Based on the geometric constraints on three views (Section 2.4), we derive, in Section 5.2, the geometric constraints for a moving stereo rig based. <p> We have also not dealt with the issue of how to represent the final 3D reconstruction. At this point we draw multiple 2 1 2 D surfaces. Better methods are described in <ref> [21] </ref>. The idea of using multi-view geometric constraints to recover the motion of a stereo rig can also be applied to feature correspondences, either point or line features. For example, let us denote the i'th image pair Images I 0 i and I 00 i .
Reference: [22] <author> E. Grosso and M. Tistarelli. </author> <title> Active/dynamic stereo vision. </title> <journal> Pattern Analysis and Machine Intelligence, </journal> <volume> 17(11) </volume> <pages> 1117-1128, </pages> <month> November </month> <year> 1995. </year>
Reference-contexts: The motion estimates between stereo pairs enable us to combine depth maps from all the pairs in the sequence to form an extended scene reconstruction. The typical approach to reconstruction from a moving stereo rig <ref> [39, 91, 85, 16, 88, 22] </ref> has been to first compute a depth map from each pair (or 3D location of features points). One also computes a confidence measure for each depth estimate.
Reference: [23] <author> K. J. Hanna and N. E. Okamoto. </author> <title> Combining stereo and motion analysis for direct estimation of scene structure. </title> <booktitle> In Proceedings of the International Conference on Computer Vision, </booktitle> <address> Berlin, Germany, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: The estimated rotation was 1:62 o . There are of course errors around the boundary of the image where there is no overlap between the images. are described in Chapter 4. 1.2 Combining Motion and Stereo Stereo and motion together form a powerful combination <ref> [23, 91] </ref> with a variety of applications from camera motion estimation to extended scene reconstruction. A stereo system (with two or more cameras) can give accurate depth estimates but only from one view point. <p> The method is stable in the case of collinear motion, in the case of pure rotation and also for planar objects. There is no scale ambiguity since all motions are relative to the known baseline length <ref> [23] </ref>. We present an example of a 3D reconstruction from a sequence of 50 stereo image pairs. 1.3 Degeneracy of the Linear Solution to the Tri linear Tensor in the Case of a Linear Line Com plex The tensor brightness constraint combines both photometric and geometric constraints. <p> Note in overhead view (d) true Euclidean structure is recovered with correct 90 o angles. 116 Chapter 5 Direct Estimation of Motion and Extended Scene Structure from a Moving Stereo Rig 5.1 Introduction Stereo and motion together form a powerful combination <ref> [23, 91] </ref> with a variety of applications from ego motion estimation to extended scene reconstruction. In this chapter we describe a new method for directly estimating the motion of a stereo rig thus greatly simplifying the process of combining the information from multiple image pairs. <p> There are further advantages. The method is stable in the case of collinear motion, in the case of pure rotation and also for planar objects. There is no scale ambiguity since all motions are relative to the known baseline length <ref> [23] </ref>. In order to avoid the need to find feature correspondences we can use the model-based brightness constraints (Chapter 3), which combines geometric and photometric constraints of three views. <p> By concatenating these motion estimates one can bring all the depth maps into one coordinate frame. Combining incremental motion in this way will accumulate errors. To get a more accurate reconstruction one can use the results as a starting point for global reconstruction schemes such as <ref> [21, 23] </ref>. 5.1.1 Overview of the Chapter Based on the geometric constraints on three views (Section 2.4), we derive, in Section 5.2, the geometric constraints for a moving stereo rig based.
Reference: [24] <author> R. </author> <title> Hartley. Lines and points in three views | a unified approach. </title> <booktitle> In Proceedings of the ARPA Image Understanding Workshop, </booktitle> <address> Monterey, CA, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: The link between the two and the generalization to projective space was identified later by Hartley <ref> [24, 25] </ref>. Additional work in this area can be found in [62, 17, 80, 30, 59, 2], and applications in [3, 74]. 2.4.1 Tensor Slices and Contractions From Figure 2-1 it is clear that the tensor can be sliced in three principle ways.
Reference: [25] <author> R. </author> <title> Hartley. A linear method for reconstruction from lines and points. </title> <booktitle> In Proceedings of the International Conference on Computer Vision, </booktitle> <pages> pages 882-887, </pages> <address> Cambridge, MA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: The link between the two and the generalization to projective space was identified later by Hartley <ref> [24, 25] </ref>. Additional work in this area can be found in [62, 17, 80, 30, 59, 2], and applications in [3, 74]. 2.4.1 Tensor Slices and Contractions From Figure 2-1 it is clear that the tensor can be sliced in three principle ways. <p> Using robust estimation techniques determine the line correspondences which belong to the LLC and compute the matrix B (see Theorem 3). Here one might use a robust version of the 8 point algorithm <ref> [25] </ref>. 2. From the matrix B create the 3 'ghost' tensors: v 1 = (B; 0; 0) v 3 = (0; 0; B) 3. Using the point-line-line correspondences from the 3 views compute W, the N fi 27; N 27 estimation matrix for the linear estimation of the tensor. 4. <p> We chose N = 28 pairs of points which defined lines all belonging to the LLC. These are overlaid as white lines in the figures. For each image pair (1,2), (2,3) and (1,3) we used the 8 point algorithm <ref> [25] </ref> applied to the line correspondences to compute the matrices B 12 , B 23 and B 13 that minimize: E 12 = N i=1 i ) 2 (6.5) 1 N X (s 0 i ) 2 1 N X (s i B 13 s 00 140 (a) (b) line complex. <p> Extra Line E 23 E 12 E 13 None 0.000055 0.00064 0.000079 Close 0.0019 0.0016 0.00054 Middle 0.0065 0.0038 0.0017 Far 0.0119 0.0919 0.0062 respectively. The coordinates of the lines s; s 0 ; s 00 have been scaled as described in <ref> [25] </ref>. From Theorem 3, the left and right null spaces of B 23 (for example) are the projections of the line L in Image 2 and Image 3. The dashed black line in Figures 6-3b, and 6-3c show the lines corresponding to the null spaces overlaid on the input images.
Reference: [26] <author> R. </author> <title> Hartley. In defence of the 8-point algorithm. </title> <booktitle> In Proceedings of the International Conference on Computer Vision, </booktitle> <pages> pages 1064-1070, </pages> <address> Cambridge, MA, June 1995. </address> <publisher> IEEE Computer Society Press, IEEE Computer Society Press. </publisher>
Reference-contexts: Given at least 8 point correspondences we can find the epipoles and epipolar lines in a linear manner <ref> [26] </ref>. With more than 8 points we can find the least squares solution. The epipolar constraint holds for ideal pinhole cameras. Due to noise in the 147 feature detection and due to lens distortion we get some error. <p> The equation of that line is given by: Given 8 or more point correspondences, the fundamental matrix (F), can be determined up to a scale factor using the eight point algorithm which is described in <ref> [26] </ref> together with many important implementation details. 7.3.2 The Trilinear Tensor Constraint In Section 2.4 we derive the trilinear equations of [57]. Given a set of 3D points there exists a set of trilinear equations between the projections of those points into any three perspective views.
Reference: [27] <author> R. </author> <title> Hartley. A linear method for reconstruction from lines and points. </title> <booktitle> In Proceedings of the International Conference on Computer Vision, </booktitle> <address> Cambridge, MA, June 1995. </address> <publisher> IEEE Computer Society Press, IEEE Computer Society Press. </publisher> <pages> 166 </pages>
Reference-contexts: We must therefore use three view geometry, where a point in one view, and lines through the corresponding points in two other views, provide a constraint, which can be written in the form of the trilinear tensor <ref> [57, 27, 69] </ref>. The 27 coefficients of the `trilinear tensor' encapsulate the camera motions and the internal parameters of the camera such as the focal length.
Reference: [28] <author> D. J. Heeger and A. Jepson. </author> <title> Simple method for computing 3d motion and depth. </title> <booktitle> In Proceedings of the International Conference on Computer Vision, </booktitle> <pages> pages 96-100, </pages> <address> Osaka, Japan, </address> <month> December </month> <year> 1990. </year>
Reference-contexts: After computing the correspondences the camera motion between two or three views and the 3D structure of the scene are recovered using geometric constraints. Here we include both `optical flow' based methods <ref> [42, 43, 28, 55, 77] </ref> and feature based methods [79, 16, 65, 67, 4]. The main problem with these approaches is that computing the correspondence is in itself a difficult and error prone process. <p> These might be either dense correspondence in the form of optical flow [33, 43, 49] or feature correspondence [11, 83, 16]. Then the correspondences are used to compute the camera motion and scene structure. The advantage of our method over both optical flow methods <ref> [42, 43, 28, 55, 77] </ref> and feature based methods [79, 16, 65, 67, 4] is that no prior computation of correspondences is needed, a computation which in itself is error prone.
Reference: [29] <author> J. Heel. </author> <title> Direct estimation of structure and motion from multiple frames. </title> <type> AI Memo 1190, </type> <institution> Massachusetts Institute of Technolgy Artificial Intelligence Labratory, </institution> <month> March </month> <year> 1990. </year>
Reference-contexts: Since this uses only a subset of the image points, one ends up discarding most of the image data. Heel <ref> [29] </ref> constructs a Kalman filter to build up a structure model from more than one image pair but the core computation is fundamentally the same single image pair computation.
Reference: [30] <author> A. </author> <title> Heyden. Reconstruction from image sequences by means of relative depths. </title> <booktitle> In Proceedings of the International Conference on Computer Vision, </booktitle> <pages> pages 1058-1063, </pages> <address> Cambridge, MA, June 1995. </address> <publisher> IEEE Computer Society Press, IEEE Computer Society Press. </publisher>
Reference-contexts: The link between the two and the generalization to projective space was identified later by Hartley [24, 25]. Additional work in this area can be found in <ref> [62, 17, 80, 30, 59, 2] </ref>, and applications in [3, 74]. 2.4.1 Tensor Slices and Contractions From Figure 2-1 it is clear that the tensor can be sliced in three principle ways. This can be generalized into performing a contraction operation on the tensor with a vector.
Reference: [31] <author> B. K. P. Horn. </author> <title> Robot Vision. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1986. </year>
Reference-contexts: tensor (matrix) c j i whose i; j entries are a i b j | note that in matrix form C = ba &gt; . 2.2 Imaging Geometry and Motion Models 2.2.1 Perspective and Projective Cameras We use the classic pinhole camera model, details of which can be found in <ref> [31] </ref>. A real lens has non-linear geometric distortion. We will defer the issue of lens distortion to Chapter 7 and for now assume the lens distortion, if significant, can be calibrated and compensated for. <p> We will derive the optical flow constraint equation following <ref> [31] </ref>. The basic assumption of the optical flow equation is that the irradiance (brightness) of an image point (x; y) at time t (i.e.
Reference: [32] <author> B. K. P. Horn. </author> <title> Relative orientation revisited. </title> <journal> Journal of the Optical Society of America, </journal> <volume> 8 </volume> <pages> 1630-1638, </pages> <year> 1991. </year>
Reference-contexts: With the traditional method for combining motion and stereo, the stereo pair provides the 3D coordinates of the three points before and after the motion. Given the 3D coordinates of three points in two images the relative orientation can be determined <ref> [32] </ref>. 5.3 The Brightness Constraint of a Moving Stereo Rig The Tensor Brightness Constraint is developed in Section 3.2.
Reference: [33] <author> B. K. P. Horn and B. G. Schunk. </author> <title> Determining optical flow. </title> <journal> Artificial Intelligence, </journal> <volume> 17 </volume> <pages> 185-203, </pages> <year> 1981. </year>
Reference-contexts: The main problem with these approaches is that computing the correspondence is in itself a difficult and error prone process. Dense correspondence methods such as `optical flow' suffer from the aperture problem and it is necessary to assume smoothness in the flow <ref> [33, 43, 49] </ref>. Sparse methods try to find matching features such as lines or corner points. It is still an open subject of research as to what makes a good feature [11, 83, 16]. <p> The wide area of support for the computation means that the motion estimates are less affected by regions where the brightness constraints are violated, such as on occluding contours. We now provide the key ideas behind the new method. The optical flow constraint equation <ref> [33] </ref> provides a matching constraint between each point in one image, and a line passing through the corresponding point in the second image. <p> Chapters 3 through 5 should be read in sequence. Chapter 3 combines photogram-metric constraints in the form of the optical flow constraint equation <ref> [33] </ref> with the geometric constraint of the trilinear tensor to develop the tensor brightness constraint. We then develop simpler `model-based brightness constraints' and describe a method for recovering structure and motion directly from image gradients. Section 3.2 derives the projective form of the constraint. <p> Cancelling the terms I (x; y; t) from both sides, dividing by ffit and taking the limit as ffit ! 0 results in the optical flow constraint equation of Horn & Schunk <ref> [33] </ref>: u 0 I x + v 0 I y + I 0 where: u 0 (x; y) = lim ffit!0 ffix ffit = dt v 0 (x; y) = lim ffit!0 ffiy ffit = dt are the optical flow values at (x; y) (i.e. the motion of the image point <p> These might be either dense correspondence in the form of optical flow <ref> [33, 43, 49] </ref> or feature correspondence [11, 83, 16]. Then the correspondences are used to compute the camera motion and scene structure.
Reference: [34] <author> B. K. P. Horn and E. J. Weldon, Jr. </author> <title> Direct methods for recovering motion. </title> <journal> International Journal of Computer Vision, </journal> <volume> 2 </volume> <pages> 51-76, </pages> <year> 1988. </year>
Reference-contexts: The 27 coefficients of the `trilinear tensor' encapsulate the camera motions and the internal parameters of the camera such as the focal length. The new method can therefore be viewed as a necessary extension of the `direct methods' of Horn & Weldon <ref> [34] </ref> from two views (one motion) to three views (two motions). <p> The result is a new method for computing the camera motion (ego-motion) between three views in an image sequence. The motion estimates are then used to compute dense structure. This method can be viewed as an extension of the `direct methods' of Horn & Weldon <ref> [34] </ref> from two views (one motion) to three views (two motions). These methods are dubbed `direct methods' because they do not require prior computation of optical flow. <p> In (b) the texture map was removed to show the detail and the flaws. Direct Methods The `direct methods' were pioneered by Horn & Weldon in <ref> [34] </ref>. Using only a single image pair they ended up with N equations in N +5 unknowns, where N is the number of points in the image. <p> + yI y ) 1 A (3.18) Note that v is a vector that can be computed from image brightness, and should not be confused with v 0 which is the y component of the image flow between Image 1 and Image 2. (Equation (3.16) which was first derived in <ref> [52, 34] </ref> can also be derived 60 directly from equation (3.11) by making the LH&P assumptions: t 0 z w 0 y x and f t 1. ) Similarly for the second motion: ks &gt; t 00 + v &gt; w 00 + I 00 By eliminating k from equations (3.16) <p> One solution to this problem might be to first find a rotation that best aligns the images (perhaps using a pyramid version of Horn & Weldon <ref> [34] </ref>) and only then try and find the translation and residual rotation. While this is similar to the idea of prealligning a plane it does not remove all rotation but on the other hand leaves us in the Euclidean realm. The program converges well for non-collinear purely translating motion. <p> The method we presented, the model-based brightness constraints, combines photometric constraints in the form of the optical flow constraint equation, together with geometric constraints between corresponding points and lines in three views. Thus, following the `direct methods' approach of Horn & Weldon <ref> [34] </ref>, we avoid the need to use feature correspondences .
Reference: [35] <author> T. Huang and C. Lee. </author> <title> Motion and structure from orthographic projections. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-11:536-540, </volume> <year> 1989. </year>
Reference-contexts: While not the main focus of this thesis, we show some good shape reconstruction results (for example Figure 1-2). Good results have also been achieved using orthographic or weak perspective camera models and an object centered coordinate system <ref> [79, 35, 81, 75, 54] </ref>.
Reference: [36] <author> M. Irani, B. Rousso, and S. Peleg. </author> <title> Recovery of ego-motion using image stabilization'. </title> <booktitle> In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 454-460, </pages> <address> Seattle, Washington, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: Kumar et al [40] first align a dominant plane in the images. The residual image motion is epipolar motion (i.e. pure translation). The epipole of the residual motion is found using iterative techniques starting with an initial guess. Others who have explored planar alignment include <ref> [36, 56] </ref>. The theoretical framework for planar alignment can also be found in [60]. More recently, Fermuller & Aloimonos [18] describe global geometric properties of the flow-field that give rise to direct relationships between the measurement of normal flow and the ego-motion parameters solved by means of search techniques. <p> This becomes a six parameter unconstrained optimization problem. 3.6.4 Reduce to Pure Translation by Planar Alignment As an alternative to solving the set of bilinear equations we can follow the ideas suggested by <ref> [36, 56, 40] </ref> (see also [60]) and first align a dominant plane in the image. After planar alignment the resulting image motion is due to pure translation, and we can use pure translation version of the tensor brightness constraint. The resulting 3D geometry will be relative to the aligned plane. <p> The resulting reconstruction will be a projective transformation of the true Euclidean shape and the motion field information will be correct. For some purposes such as finding correspondences or depth discontinuities this is sufficient. If needed the true (Euclidean) translation and rotation can be found as a second step <ref> [36] </ref>. 65 3.7 Implementation Details In this section we will provide the details needed to build a program that actually works. 3.7.1 Computing the Depth, Smoothing and Interpolation After recovering the camera motion (Section 3.6.2) we use equations (3.16) and (3.19) to compute depth at every point. <p> Calibration of the stereo pair is performed in two stages. First we take an image of a distant scene (the plane at infinity) and find the homography between Image 2 and Image 1 using the method described in <ref> [36] </ref>. Since the rotation angle is small we can assume an affine model rather than a full planar projective transformation. This stage takes into account both the rotation between the two cameras and also the variation in internal camera parameters.
Reference: [37] <author> A. N. Jourjine. </author> <title> Surface and albedo from a sequence of images. </title> <institution> Siemens Corp, </institution> <month> April </month> <year> 1997. </year>
Reference-contexts: This is a good approximation to the images of non-specular surfaces captured by a moving camera. It does not hold in general, in the case of a moving rigid object in front of stationary camera. Jourjine <ref> [37] </ref> provides a theoretical extension to this framework to the case of rigid object motion assuming a Lambertian surface model and parallel lighting. In this case both surface and albedo can be recovered.
Reference: [38] <author> T. Kanade, Okutomi, and Nakahara. </author> <title> A multiple-baseline stereo method. </title> <booktitle> In Proceedings of the ARPA Image Understanding Workshop, </booktitle> <pages> pages 409-426. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <month> January </month> <year> 1992. </year>
Reference-contexts: This can now be viewed as a multi-baseline stereo system with all the advantages of such as system: it reduces aliasing, helps deal with occlusions and it extends the dynamic range of the system as detailed by <ref> [38] </ref> and others. Applying an edge detector to the depth map (Figure 5-1e) highlights possible areas of occlusion. In a traffic scene, for example, these are locations where a pedestrian might suddenly appear.
Reference: [39] <author> R. Koch. </author> <title> 3-d surface reconstruction from stereoscopic image sequences. </title> <booktitle> In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 109-114. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> June </month> <year> 1995. </year>
Reference-contexts: To build a complete model of a scene we would like to combine together multiple 3D views. One solution is to use a moving stereo rig (two cameras mounted onto the same rigid frame). The standard approach to reconstruction from a moving stereo rig <ref> [39, 91, 85, 16] </ref> has been to first compute a depth map from each pair (or compute the depth of a 19 (a) (b) to show detail. set of features points), together with a confidence measure for each depth estimate. <p> The motion estimates between stereo pairs enable us to combine depth maps from all the pairs in the sequence to form an extended scene reconstruction. The typical approach to reconstruction from a moving stereo rig <ref> [39, 91, 85, 16, 88, 22] </ref> has been to first compute a depth map from each pair (or 3D location of features points). One also computes a confidence measure for each depth estimate.
Reference: [40] <author> R. Kumar and P. Anandan. </author> <title> Direct recovery of shape from multiple views: A parallax based approach. </title> <booktitle> In Proceedings of the International Conference on Pattern Recognition, </booktitle> <address> Jerusalem, Israel, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: Since estimation of motion given structure is a key stage in the process he suggests that a wide field of view is required for the whole process of estimation of structure and motion. Kumar et al <ref> [40] </ref> first align a dominant plane in the images. The residual image motion is epipolar motion (i.e. pure translation). The epipole of the residual motion is found using iterative techniques starting with an initial guess. Others who have explored planar alignment include [36, 56]. <p> This becomes a six parameter unconstrained optimization problem. 3.6.4 Reduce to Pure Translation by Planar Alignment As an alternative to solving the set of bilinear equations we can follow the ideas suggested by <ref> [36, 56, 40] </ref> (see also [60]) and first align a dominant plane in the image. After planar alignment the resulting image motion is due to pure translation, and we can use pure translation version of the tensor brightness constraint. The resulting 3D geometry will be relative to the aligned plane.
Reference: [41] <author> H. Longuet-Higgins. </author> <title> A computer algorithm for reconstructing a scene from two projections. </title> <journal> Nature, </journal> <volume> 293 </volume> <pages> 133-135, </pages> <year> 1981. </year>
Reference-contexts: x = 6 0 t z t y t y t x 0 7 Substituting ~ O 1 P = P 1 , ~ O 2 P = RP 2 + T in equation (2.31) we get: P &gt; where E = [t] x R is called the essential matrix <ref> [41] </ref>. The normalized image coordinates p 1 and p 2 are proportional to the 3D Cartesian point coordinates P 1 and P 2 thus: p &gt; The above equation was developed for the Euclidean case.
Reference: [42] <author> H. Longuett-Higgins and K. Prazdny. </author> <title> The interpretation of a moving retinal image. </title> <journal> In Proceedings of the Royal Society of London B, </journal> <volume> volume 208, </volume> <pages> pages 385-397. </pages> <publisher> Royal Society of London, </publisher> <year> 1980. </year> <month> 167 </month>
Reference-contexts: After computing the correspondences the camera motion between two or three views and the 3D structure of the scene are recovered using geometric constraints. Here we include both `optical flow' based methods <ref> [42, 43, 28, 55, 77] </ref> and feature based methods [79, 16, 65, 67, 4]. The main problem with these approaches is that computing the correspondence is in itself a difficult and error prone process. <p> We then proceed through a hierarchy of simpler motion models first by assuming calibrated cameras and small rotations, then by assuming the Longuet-Higgins & Prazdny small motion model <ref> [42] </ref>, where we assume also that the camera translation in depth is small relative to the depth of the objects in the scene. Thus we have a hierarchy of model-based brightness constraints. <p> 1 Z w 2 Z (2.20) dt = d Z = Z Y _ Z = t 2 X Z t 3 Y X Noting that x = X Z and y = Y Z , and rearranging the terms we get the infinitesimal motion equations of Longuet-Higgins & Prazdny <ref> [42] </ref>: u = z 1 xy (2.22) 1 (t 2 yt 3 ) + w 3 x w 1 (1 + y 2 ) + w 0 where we use u = dx dt and v = dy dt . <p> We then proceed through a hierarchy of reduced motion models first by assuming calibrated cameras and small rotations, and then by assuming the Longuet-Higgins & Prazdny small motion model <ref> [42] </ref>, resulting in reduced model-based brightness constraints for those motion models. Since these methods are based on the image gradients and the `constant brightness constraint', they are only exact for infinitesimal image motion. <p> These might be either dense correspondence in the form of optical flow [33, 43, 49] or feature correspondence [11, 83, 16]. Then the correspondences are used to compute the camera motion and scene structure. The advantage of our method over both optical flow methods <ref> [42, 43, 28, 55, 77] </ref> and feature based methods [79, 16, 65, 67, 4] is that no prior computation of correspondences is needed, a computation which in itself is error prone. <p> This results in a 20-parameter model but again the linear solution is degenerate. Therefore we develop a simpler model in Section 3.4 using the small motion approximations of Longuet-Higgins & Prazdny <ref> [42] </ref>. This results in a 15-parameter model-based brightness constraint. Before showing how to solve for the ego-motion parameters for the general case of rotation and translation, we show how to solve for the motion parameters in the simpler example of pure translation in Section 3.5. <p> the high degeneracy of the linear equations neither the 24-parameter model nor the 20-parameter model are convenient to work with. 3.4 The 15-Parameter, Small Motion Model A unique solution is obtained when we reduce the motion model further to include infinitesimal motion using the model introduced by Longuet-Higgins & Prazdny <ref> [42] </ref> (see Section 2.2.4).
Reference: [43] <author> B. Lucas and T. Kanade. </author> <title> An iterative image registration technique with an application to stereo vision. </title> <booktitle> In Proceedings IJCAI, </booktitle> <pages> pages 674-679, </pages> <address> Vancouver, </address> <year> 1981. </year>
Reference-contexts: After computing the correspondences the camera motion between two or three views and the 3D structure of the scene are recovered using geometric constraints. Here we include both `optical flow' based methods <ref> [42, 43, 28, 55, 77] </ref> and feature based methods [79, 16, 65, 67, 4]. The main problem with these approaches is that computing the correspondence is in itself a difficult and error prone process. <p> The main problem with these approaches is that computing the correspondence is in itself a difficult and error prone process. Dense correspondence methods such as `optical flow' suffer from the aperture problem and it is necessary to assume smoothness in the flow <ref> [33, 43, 49] </ref>. Sparse methods try to find matching features such as lines or corner points. It is still an open subject of research as to what makes a good feature [11, 83, 16]. <p> These might be either dense correspondence in the form of optical flow <ref> [33, 43, 49] </ref> or feature correspondence [11, 83, 16]. Then the correspondences are used to compute the camera motion and scene structure. <p> These might be either dense correspondence in the form of optical flow [33, 43, 49] or feature correspondence [11, 83, 16]. Then the correspondences are used to compute the camera motion and scene structure. The advantage of our method over both optical flow methods <ref> [42, 43, 28, 55, 77] </ref> and feature based methods [79, 16, 65, 67, 4] is that no prior computation of correspondences is needed, a computation which in itself is error prone.
Reference: [44] <author> S. Maybank. </author> <title> The critical line congruence for reconstruction from three images. </title> <journal> Applicable Algebra in Engineering Communication and Computing (AAECC), </journal> <volume> 6 </volume> <pages> 89-113, </pages> <year> 1995. </year>
Reference-contexts: In general this system of equations has a unique linear solution but for particular 3D line configurations in space this system of equations is degenerate. There are some critical line configurations from which a unique solution is not possible ( see <ref> [44, 9, 50] </ref>). There are other configurations where a linear solution is degenerate but the additional non-linear constraints lead to a unique solution. <p> Note also that the vertical line representing the lamp-post also meets the edge of the building (at infinity) thereby included in the 1 This in contrast to critical line configurations from which a unique solution is not possible, see <ref> [44, 9, 50] </ref> LLC configuration. This leaves very few lines (the sidewalk and the oblique line of the lamp-post) not part of the LLC. The common line in Figure 6-1b is the edge of the book-case leaning on the wall.
Reference: [45] <author> I. S. McQuirk. </author> <title> An analog vlsi chip for estimating the focus of expansion. AITR 1577, </title> <publisher> MIT, </publisher> <year> 1996. </year>
Reference-contexts: The problem is therefore ill posed and additional constraints are needed. Negahdaripour & Horn [52] present a closed form solution assuming a planar or quadratic surface. Szeliski & Kang [76] describe an iterative solution using splines to enforce a smoothness constraint on the depth. McQuirk <ref> [45] </ref> shows that in a pure translation model the subset of the image points with a nonzero spatial derivative but a zero time derivative gives the direction of motion, thus the focus of expansion (FOE) is on a line perpendicular to the gradient at these points.
Reference: [46] <author> D. Michaels. </author> <title> Exploiting Continuity-in-Time in Motion Vision. </title> <type> PhD thesis, </type> <institution> Department of Electrical engineering and Computer Science, Massachusetts Institute of Technology, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: A coarse-to-fine framework (Section 3.7.4) enables us to use much larger image motions, thus increasing the dynamic range by an order of magnitude. Michaels <ref> [46] </ref> uses three frames (two motions). Each motion gives N equations totaling 2N equations. The unknowns are the N unknown depths and the 2 fi 6 translation and rotation parameters. Again, one parameter drops out because of the translation and depth scale ambiguity. <p> The Solution As we have seen, in order to get reasonable rotation and translation estimates a wide field of view is required. The theoretical results of <ref> [46] </ref> and simulation results indicate 70 narrow fields of view. The arrows for the medium and wide have been offset in the Y direction for clarity.
Reference: [47] <author> J. J. More, B. S. Garbow, and K. E. Hillstrom. </author> <title> User guide for minpack-1. </title> <type> Technical Report ANL-80-74, </type> <institution> Argonne National Laboratory, Argonne, Illinois, </institution> <month> August </month> <year> 1980. </year>
Reference-contexts: The unknowns are the N unknown depths and the 2 fi 6 translation and rotation parameters. Again, one parameter drops out because of the translation and depth scale ambiguity. He solves the 2N equations with N + 11 unknowns as a large nonlinear optimization problem using the Levenberg-Marquart algorithm <ref> [47] </ref>. Since it took a very long time to converge results are shown for very low resolution images only. He also shows theoretically that a large field of view ( 120 o ) is required for accurate direct estimation of motion given structure. <p> Image capture and processing was performed on an SGI Indy workstation. The camera parameters were found using a nonlinear optimization program based on the subroutine LMDIF from the software package MINPACK-1 <ref> [47] </ref>.
Reference: [48] <author> J. Mundy and A. Zisserman. </author> <title> Appendix | projective geometry for machine vision. </title> <editor> In J. Mundy and A. Zisserman, editors, </editor> <booktitle> Geometric invariances in computer vision. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1992. </year>
Reference-contexts: This thesis uses results from projective geometry. A very readable introduction to the subject of projective geometry is given in the book by Young [89]. A modern book dealing more specifically with the application of projective geometry to computer vision is [16]. The appendix of <ref> [48] </ref> provides a concise summary of the basic material. 2.1 Notation We will use uppercase bold to denote matrices (e.g. A). Vectors representing 3D points which will be upper case and not bold. Other vectors and scalars will be in lower case.
Reference: [49] <author> H. H. Nagel. </author> <title> Direct estimation of optical flow and of its derivates. </title> <editor> In G. A. Orban and H. H. Nagel, editors, </editor> <booktitle> Artificial and Biological Vision Systems, Basic Research Series, </booktitle> <pages> pages 193-224. </pages> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: The main problem with these approaches is that computing the correspondence is in itself a difficult and error prone process. Dense correspondence methods such as `optical flow' suffer from the aperture problem and it is necessary to assume smoothness in the flow <ref> [33, 43, 49] </ref>. Sparse methods try to find matching features such as lines or corner points. It is still an open subject of research as to what makes a good feature [11, 83, 16]. <p> Various attempts have been made to overcome the aperture problem by adding constraints on the brightness derivatives: d (rI) = 0 (2.54) r dI ! A good survey of these approaches appears in <ref> [49] </ref>. These approaches lead to additional constraint equations on the optical flow which involve second partial derivatives of the image brightness. <p> These might be either dense correspondence in the form of optical flow <ref> [33, 43, 49] </ref> or feature correspondence [11, 83, 16]. Then the correspondences are used to compute the camera motion and scene structure.
Reference: [50] <author> N. Navab, O. D. Faugeras, and T. Vieville. </author> <title> The critical sets of lines for camera displacement estimation: A mixed euclidean-projective and constructive approach. </title> <booktitle> In Proc. 4th International Conference on Computer Vision, </booktitle> <address> Berlin, Germany, </address> <month> May </month> <year> 1993. </year> <note> IEEE. </note>
Reference-contexts: In general this system of equations has a unique linear solution but for particular 3D line configurations in space this system of equations is degenerate. There are some critical line configurations from which a unique solution is not possible ( see <ref> [44, 9, 50] </ref>). There are other configurations where a linear solution is degenerate but the additional non-linear constraints lead to a unique solution. <p> Note also that the vertical line representing the lamp-post also meets the edge of the building (at infinity) thereby included in the 1 This in contrast to critical line configurations from which a unique solution is not possible, see <ref> [44, 9, 50] </ref> LLC configuration. This leaves very few lines (the sidewalk and the oblique line of the lamp-post) not part of the LLC. The common line in Figure 6-1b is the edge of the book-case leaning on the wall.
Reference: [51] <author> N. Navab, R. Deriche, and O. D. Faugeras. </author> <title> Recovering 3d motion and structure from stereo and 2d token tracking cooperation. </title> <booktitle> In Proceedings of the International Conference on Computer Vision, </booktitle> <pages> pages 513-516, </pages> <address> Osaka, Japan, </address> <month> December </month> <year> 1990. </year>
Reference-contexts: Next, one registers the stereo pair with the current 3D model to compute the motion, and finally, one updates the model (using a Kalman filter or a batch method). In contrast, Navab et al <ref> [51] </ref> combine motion and stereo for the reconstruction of motion and structure from line correspondences. Given two distinct non-parallel lines in two stereo image pairs they obtain a set of 8 linear equations in the 6 motion parameters. The method requires accurate tracking of straight lines in an image. <p> Tirumalai et al [78] use stereo depth values and initial motion estimates to help matching features over the images sequence. Navab et al <ref> [51] </ref>, in contrast to other approaches, combine motion and stereo. This is applied to the reconstruction of motion and structure from line correspondences. Given two distinct non-parallel lines in two stereo image pairs they obtain a set of 8 linear equations in the 6 motion parameters. <p> Then if we use line features, then every line correspondence from Images I 0 i ,I 00 i+1 will provide us with two constraint equations. Thus, three line correspondences will give us enough equations to solve for the six motion parameters. Navab et al <ref> [51] </ref> have shown that two line correspondences is enough. We can trace this inconsistency to the fact that we do not use Image I 00 i+1 .
Reference: [52] <author> S. Negahdaripour and B. K. P. Horn. </author> <title> Direct passive navigation. </title> <journal> Pattern Analysis and Machine Intelligence, </journal> <volume> 9(1) </volume> <pages> 168-176, </pages> <year> 1987. </year>
Reference-contexts: The unknowns are the N unknown depths and the 3 translation and 3 rotation parameters with one unknown dropping out because translation and depth can only be found up to a common scale factor. The problem is therefore ill posed and additional constraints are needed. Negahdaripour & Horn <ref> [52] </ref> present a closed form solution assuming a planar or quadratic surface. Szeliski & Kang [76] describe an iterative solution using splines to enforce a smoothness constraint on the depth. <p> + yI y ) 1 A (3.18) Note that v is a vector that can be computed from image brightness, and should not be confused with v 0 which is the y component of the image flow between Image 1 and Image 2. (Equation (3.16) which was first derived in <ref> [52, 34] </ref> can also be derived 60 directly from equation (3.11) by making the LH&P assumptions: t 0 z w 0 y x and f t 1. ) Similarly for the second motion: ks &gt; t 00 + v &gt; w 00 + I 00 By eliminating k from equations (3.16)
Reference: [53] <author> W. H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P. Flannery. </author> <title> Numerical Recipes in C: </title> <booktitle> The Art of Scientific Computing. </booktitle> <publisher> Cambridge University Press, </publisher> <address> second edition edition, </address> <year> 1992. </year>
Reference-contexts: During the iteration process we typically used a region R of 7 fi 7 to 11 fi 11. For `prettier' results the last iteration, we typically reduced the region R to 1 fi 1 but added a very weak global smoothness term and performed multi-grid membrane interpolation <ref> [53] </ref>. The smoothness term stabilizes regions where there is no image gradient so very small regions of support can be used. 3.7.2 Iterative Refinement The constant brightness constraint is a linearized form of the Sum Square Difference (SSD) criteria.
Reference: [54] <author> L. Quan and T. Kanade. </author> <title> Affine structure from line correspondences with uncalibrated affine cameras. </title> <journal> Pattern Analysis and Machine Intelligence, </journal> <volume> 19(8) </volume> <pages> 834-845, </pages> <month> August </month> <year> 1997. </year>
Reference-contexts: While not the main focus of this thesis, we show some good shape reconstruction results (for example Figure 1-2). Good results have also been achieved using orthographic or weak perspective camera models and an object centered coordinate system <ref> [79, 35, 81, 75, 54] </ref>. <p> Using this method it is simple to bring 3D reconstructions from various view points into the same coordinate system. 8.1 Future Work Affine Camera Models It would be interesting to try and extend this framework to orthographic, weak perspective and affine camera models. Recently, Quan & Kanade <ref> [54] </ref> have shown that for affine cameras there exists a constraint between the directions of corresponding lines in three views which takes the form of a 2 fi 2 fi 2 trilinear tensor.
Reference: [55] <author> J. Reiger and D. T. Lawton. </author> <title> Processing differential image motion. </title> <journal> Journal of the Optical Society of America, </journal> <volume> 2 </volume> <pages> 354-359, </pages> <year> 1985. </year>
Reference-contexts: After computing the correspondences the camera motion between two or three views and the 3D structure of the scene are recovered using geometric constraints. Here we include both `optical flow' based methods <ref> [42, 43, 28, 55, 77] </ref> and feature based methods [79, 16, 65, 67, 4]. The main problem with these approaches is that computing the correspondence is in itself a difficult and error prone process. <p> These might be either dense correspondence in the form of optical flow [33, 43, 49] or feature correspondence [11, 83, 16]. Then the correspondences are used to compute the camera motion and scene structure. The advantage of our method over both optical flow methods <ref> [42, 43, 28, 55, 77] </ref> and feature based methods [79, 16, 65, 67, 4] is that no prior computation of correspondences is needed, a computation which in itself is error prone.
Reference: [56] <author> H. Sawhney. </author> <title> 3D geometry from planar parallax. </title> <booktitle> In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 929-934, </pages> <address> Seattle, Washington, </address> <month> June </month> <year> 1994. </year> <month> 168 </month>
Reference-contexts: Kumar et al [40] first align a dominant plane in the images. The residual image motion is epipolar motion (i.e. pure translation). The epipole of the residual motion is found using iterative techniques starting with an initial guess. Others who have explored planar alignment include <ref> [36, 56] </ref>. The theoretical framework for planar alignment can also be found in [60]. More recently, Fermuller & Aloimonos [18] describe global geometric properties of the flow-field that give rise to direct relationships between the measurement of normal flow and the ego-motion parameters solved by means of search techniques. <p> This becomes a six parameter unconstrained optimization problem. 3.6.4 Reduce to Pure Translation by Planar Alignment As an alternative to solving the set of bilinear equations we can follow the ideas suggested by <ref> [36, 56, 40] </ref> (see also [60]) and first align a dominant plane in the image. After planar alignment the resulting image motion is due to pure translation, and we can use pure translation version of the tensor brightness constraint. The resulting 3D geometry will be relative to the aligned plane.
Reference: [57] <author> A. Shashua. </author> <title> Algebraic functions for recognition. </title> <journal> IEEE Transactions on Pattern Anal--ysis and Machine Intelligence, </journal> <volume> 17(8) </volume> <pages> 779-789, </pages> <year> 1995. </year>
Reference-contexts: We must therefore use three view geometry, where a point in one view, and lines through the corresponding points in two other views, provide a constraint, which can be written in the form of the trilinear tensor <ref> [57, 27, 69] </ref>. The 27 coefficients of the `trilinear tensor' encapsulate the camera motions and the internal parameters of the camera such as the focal length. <p> We then develop the constraint equations among point and line correspondences in two and three views. These are well known results based mainly on the work of Faugeras [16] for two views, and on Shashua's work on the trilinear tensor constraint for 3 views <ref> [57] </ref>. Chapters 3 through 5 should be read in sequence. Chapter 3 combines photogram-metric constraints in the form of the optical flow constraint equation [33] with the geometric constraint of the trilinear tensor to develop the tensor brightness constraint. <p> In a calibrated setting, (i.e. if the intrinsic parameters are known), the matrices A and B are rotations and the vectors t 0 and t 00 are the translations. Finally, in the calibrated setting is replaced by k = 1 z . It has been shown <ref> [57] </ref> that the image coordinates of a point in three views are related through 4 trilinear equations. The basic idea can be neatly summarized in the form of the trilinear tensor in the following proposition. <p> s 0 (and s 00 ) represent either the vertical or horizontal scan lines, i.e., s 0 = B 1 x 0 C 0 @ 1 1 A s 00 = B 1 x 00 C 0 @ 1 1 A Then the four trilinear forms, referred to as trilinearities <ref> [57] </ref>, have the following explicit form: x 00 T 13 i p i + x 0 T 31 i p i = 0; (2.42) i p i y 00 x 0 T 33 i p i T 12 x 00 T 23 i p i + y 0 T 31 i <p> p i y 00 x 0 T 33 i p i T 12 x 00 T 23 i p i + y 0 T 31 i p i = 0; i p i y 00 y 0 T 33 i p i T 22 These constraints were first derived in <ref> [57] </ref>; the tensorial derivation leading to equations (2.41) and (2.40) was first derived in [63]. <p> Thus, we readily see there can be at most 21 parameters defining the tensor. We can drop out three more parameters by noticing that the matrices a j and b k i belong to a family of homography matrices that leaves the tensor unchanged (uniqueness proof in <ref> [57] </ref>), as detailed below: T i = t 0j b k j 45 = t 0j (b k j = T i + ff i t 0j t 00k ff i t 0j t 00k (2.49) jk hence, we have three free parameters ff i (in geometric terms there is a <p> As we have noted, in order to get a distinct minimum in the error function, the angle between views must be large. 7.1.2 The Three Image Method Corresponding points in three images are related by 4 independent trilinear equations. <ref> [57] </ref>. These equations have 27 parameters which can be found in a linear manner given at least 7 point correspondences. These parameters allow us to reproject corresponding points given in two of the images into the third image. Due to noise and lens distortion, the reprojection is not perfect. <p> given by: Given 8 or more point correspondences, the fundamental matrix (F), can be determined up to a scale factor using the eight point algorithm which is described in [26] together with many important implementation details. 7.3.2 The Trilinear Tensor Constraint In Section 2.4 we derive the trilinear equations of <ref> [57] </ref>. Given a set of 3D points there exists a set of trilinear equations between the projections of those points into any three perspective views. In total there exist 9 such equations for each point with at most 4 being independent.
Reference: [58] <author> A. Shashua. </author> <title> Trilinear tensor: The fundamental construct of multiple-view geometry and its applications. </title> <note> Submitted for journal publication, </note> <month> June </month> <year> 1997. </year>
Reference-contexts: We will mostly focus on the contraction: ffi i T i (2.43) where ffi i is an arbitrary contravariant vector and the results of the contraction is a 3 fi 3 matrix, which we denote by T. The general meaning of the matrix T can be found in <ref> [58] </ref> where we have the proposition: Proposition 2 (Correlation Contractions) The contraction ffi i T jk i for some arbitrary ffi i is a rank 2 correlation matrix from Image 2 onto Image 3, that maps the dual image plane (the space of lines in Image 2) onto a set of <p> Similarly, the transpose of T is a correlation from Image 3 onto Image 2 with the null space being the epipolar line in Image 3 corresponding to the point ffi i in Image 1. For a full proof see <ref> [58] </ref>. We will provide here a simple proof of the rank 2 constraint: Proposition 3 Rank (ffi i T jk i )=2 for all choices of ffi i . <p> Likewise, the contraction ffi j T jk i is a homography matrix from Image 1 onto Image 3, denoted W. For proof and applications see [62]. 2.4.2 Tensor Admissibility Constraints The 27 coefficients T jk i are not independent. It has been shown in [15] and <ref> [58] </ref> that the tensor is determined by only 18 parameters. We will first present the explanation from [58]. We will then show how from the contraction properties discussed above, one can come up with the constraints among the 27 coefficients, referred to as admissibility constraints. <p> For proof and applications see [62]. 2.4.2 Tensor Admissibility Constraints The 27 coefficients T jk i are not independent. It has been shown in [15] and <ref> [58] </ref> that the tensor is determined by only 18 parameters. We will first present the explanation from [58]. We will then show how from the contraction properties discussed above, one can come up with the constraints among the 27 coefficients, referred to as admissibility constraints. These can are grouped into three classes. (For further details see [58]). 18 Parameters The tensor T i = t 0j b k <p> We will first present the explanation from <ref> [58] </ref>. We will then show how from the contraction properties discussed above, one can come up with the constraints among the 27 coefficients, referred to as admissibility constraints. These can are grouped into three classes. (For further details see [58]). 18 Parameters The tensor T i = t 0j b k j is determined by 24 parameters given by the two camera matrices, each has 12 parameters.
Reference: [59] <author> A. Shashua and S. Avidan. </author> <title> The rank4 constraint in multiple view geometry. </title> <booktitle> In Proceedings of the European Conference on Computer Vision, </booktitle> <address> Cambridge, UK, </address> <month> April </month> <year> 1996. </year>
Reference-contexts: The link between the two and the generalization to projective space was identified later by Hartley [24, 25]. Additional work in this area can be found in <ref> [62, 17, 80, 30, 59, 2] </ref>, and applications in [3, 74]. 2.4.1 Tensor Slices and Contractions From Figure 2-1 it is clear that the tensor can be sliced in three principle ways. This can be generalized into performing a contraction operation on the tensor with a vector.
Reference: [60] <author> A. Shashua and N. Navab. </author> <title> Relative affine structure: Theory and application to 3D reconstruction from perspective views. </title> <booktitle> In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 483-489, </pages> <address> Seattle, Washington, </address> <year> 1994. </year>
Reference-contexts: The residual image motion is epipolar motion (i.e. pure translation). The epipole of the residual motion is found using iterative techniques starting with an initial guess. Others who have explored planar alignment include [36, 56]. The theoretical framework for planar alignment can also be found in <ref> [60] </ref>. More recently, Fermuller & Aloimonos [18] describe global geometric properties of the flow-field that give rise to direct relationships between the measurement of normal flow and the ego-motion parameters solved by means of search techniques. <p> This becomes a six parameter unconstrained optimization problem. 3.6.4 Reduce to Pure Translation by Planar Alignment As an alternative to solving the set of bilinear equations we can follow the ideas suggested by [36, 56, 40] (see also <ref> [60] </ref>) and first align a dominant plane in the image. After planar alignment the resulting image motion is due to pure translation, and we can use pure translation version of the tensor brightness constraint. The resulting 3D geometry will be relative to the aligned plane.
Reference: [61] <author> A. Shashua and N. Navab. </author> <title> Relative affine structure: Canonical model for 3D from 2D geometry and applications. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 18(9) </volume> <pages> 873-883, </pages> <year> 1996. </year>
Reference-contexts: A) and a 3 fi 1 vector (e.g. t) which we will write as: ~ A = [A; t]: 2.2.2 Motion Model for the Projective Case We will present here some of the important results from <ref> [61] </ref>. Let p be the projection a 3D point P in Image 1. <p> Figure 7-4d shows the RMS reprojection error using the images in Figure 7-3a and Figure 7-3b. The minimum error is obtained near the value of K 1 found using the distortion calibration. 7.6.4 Experiment 3: Euclidean Reconstruction Projective reconstruction from the three images was performed according to <ref> [61] </ref>. Transformation to Euclidean 3D coordinates then required 5 control points. Three points were chosen from the planar surface and two other points were chosen such that the five were in general position, no 4 points coplanar.
Reference: [62] <author> A. Shashua and M. Werman. </author> <title> Trilinearity of three perspective views and its associated tensor. </title> <booktitle> In Proceedings of the International Conference on Computer Vision, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: The link between the two and the generalization to projective space was identified later by Hartley [24, 25]. Additional work in this area can be found in <ref> [62, 17, 80, 30, 59, 2] </ref>, and applications in [3, 74]. 2.4.1 Tensor Slices and Contractions From Figure 2-1 it is clear that the tensor can be sliced in three principle ways. This can be generalized into performing a contraction operation on the tensor with a vector. <p> The matrix E has a general meaning introduced in <ref> [62] </ref>: Proposition 4 (Homography Contractions) The contraction ffi k T jk i for some ar bitrary ffi k is a homography matrix from Image 1 onto Image 2 determined by the plane containing the third camera center C 00 and the line ffi k in the third image plane. <p> Gen erally, the rank of E is 3. Likewise, the contraction ffi j T jk i is a homography matrix from Image 1 onto Image 3, denoted W. For proof and applications see <ref> [62] </ref>. 2.4.2 Tensor Admissibility Constraints The 27 coefficients T jk i are not independent. It has been shown in [15] and [58] that the tensor is determined by only 18 parameters. We will first present the explanation from [58].
Reference: [63] <author> A. Shashua and P. Anandan. </author> <title> Trilinear constraints revisited: Generalized trilinear constraints and the tensor brightness constraint. </title> <booktitle> In Proceedings of the ARPA Image Understanding Workshop, </booktitle> <address> Palm Springs, CA, </address> <month> February </month> <year> 1996. </year>
Reference-contexts: 00 T 23 i p i + y 0 T 31 i p i = 0; i p i y 00 y 0 T 33 i p i T 22 These constraints were first derived in [57]; the tensorial derivation leading to equations (2.41) and (2.40) was first derived in <ref> [63] </ref>. The trilinear tensor has been well 43 known in disguise in the context of Euclidean line correspondences and was not iden-tified at the time as a tensor but as a collection of three matrices (a particular contraction of the tensor, correlation contractions, as explained next) [68, 70, 84].
Reference: [64] <author> A. Shashua and K. J. Hanna. </author> <title> The tensor brightness constraints: Direct estimation of motion revisited. </title> <type> Technical report, </type> <institution> Technion, Haifa, Israel, </institution> <month> November </month> <year> 1995. </year>
Reference-contexts: More recently, Fermuller & Aloimonos [18] describe global geometric properties of the flow-field that give rise to direct relationships between the measurement of normal flow and the ego-motion parameters solved by means of search techniques. The tensor brightness constraint was first presented in <ref> [64] </ref> and a practical implementation with results was first described in [74]. During the course of implementation various subtleties and limitations were discovered. 3.1.2 Overview of this Chapter In Section 3.2 we derive the the tensor brightness constraint, the model-based brightness constraint for the projective camera model. <p> There is a unique non-trivial solution if the esti mation matrix is of rank = 19. But as in the projective case, the system of homogeneous equations is degenerate and in this case the rank of the estimation matrix is 16 not 19. In <ref> [64, 74] </ref> a model-based brightness constraint for the calibrated, small rotation model was derived in a different way.
Reference: [65] <author> A. Shashua and M. Werman. </author> <title> Trilinearity of three perspective views and its associated tensor. </title> <booktitle> In Proceedings of the International Conference on Computer Vision, </booktitle> <pages> pages 920-925, </pages> <address> Cambridge, MA, June 1995. </address> <publisher> IEEE Computer Society Press, IEEE Computer Society Press. </publisher>
Reference-contexts: After computing the correspondences the camera motion between two or three views and the 3D structure of the scene are recovered using geometric constraints. Here we include both `optical flow' based methods [42, 43, 28, 55, 77] and feature based methods <ref> [79, 16, 65, 67, 4] </ref>. The main problem with these approaches is that computing the correspondence is in itself a difficult and error prone process. Dense correspondence methods such as `optical flow' suffer from the aperture problem and it is necessary to assume smoothness in the flow [33, 43, 49]. <p> Then the correspondences are used to compute the camera motion and scene structure. The advantage of our method over both optical flow methods [42, 43, 28, 55, 77] and feature based methods <ref> [79, 16, 65, 67, 4] </ref> is that no prior computation of correspondences is needed, a computation which in itself is error prone. Since we obtain a linear system of equations that combines the information from all the pixels in the image, we avoid the aperture problem.
Reference: [66] <author> C. Slama, </author> <title> editor. Manual of Photogrammetry. </title> <journal> American Society of Photogrammetry, </journal> <note> 4 edition, </note> <year> 1980. </year>
Reference-contexts: Due to noise and lens distortion, the reprojection is not perfect. We define our cost function as the RMS reprojection error and find lens distortion parameters that minimize it. 7.2 Related Work Known world coordinates: The classic method for lens distortion calibration is the bundle adjustment method <ref> [66] </ref>. It uses one or more views of a calibration object with known 3D coordinates (control points). Using iterative methods it then finds both external parameters (position and orientation) and internal camera parameters. <p> This is a good model for long focal lengths, but medium to wide angle lenses have noticeable lens distortion. The standard model for lens distortion <ref> [66] </ref> is a mapping from the distorted image coordinates, (x d ; y d ), that are observable, to the undistorted image plane coordinates, (x u ; y u ), which are not physically measurable using the equation: x u = x d + x 0 0 2 0 4 y <p> d = (x 2 d c yr ) 2 (7.5) It has been shown in [71] that allowing the center of radial distortion, (c xr ; c yr ) to be different from the principal point is a good approximation to adding the terms for decentering distortion as given in <ref> [66] </ref>. 7.4 The Algorithm The step by step algorithm is as follows: 1. Find point correspondences between 3 views. 2. Make an initial guess of the distortion parameters: an appropriate guess for (C x ; C y ) is the center of the image. <p> Especially, since most of the computational machinery required for tasks such as finding the trilinear tensor or fundamental matrix probably already exists in the overall system. The Manual of Photogrammetry <ref> [66] </ref> warns us that "the strong coupling that exists between interior elements of orientation [principal point, focal length] and exterior elements can be expected to result in unacceptably large variances for these particular projective parameters when recovered on a frame-by-frame basis".
Reference: [67] <author> S. Soatto and P. Perona. </author> <title> Motion from fixation. </title> <booktitle> In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 817-824, </pages> <address> San Fransico, CA, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: After computing the correspondences the camera motion between two or three views and the 3D structure of the scene are recovered using geometric constraints. Here we include both `optical flow' based methods [42, 43, 28, 55, 77] and feature based methods <ref> [79, 16, 65, 67, 4] </ref>. The main problem with these approaches is that computing the correspondence is in itself a difficult and error prone process. Dense correspondence methods such as `optical flow' suffer from the aperture problem and it is necessary to assume smoothness in the flow [33, 43, 49]. <p> Then the correspondences are used to compute the camera motion and scene structure. The advantage of our method over both optical flow methods [42, 43, 28, 55, 77] and feature based methods <ref> [79, 16, 65, 67, 4] </ref> is that no prior computation of correspondences is needed, a computation which in itself is error prone. Since we obtain a linear system of equations that combines the information from all the pixels in the image, we avoid the aperture problem.
Reference: [68] <author> M. Spetsakis and J. Aloimonos. </author> <title> Structure from motion using line correspondences. </title> <journal> International Journal of Computer Vision, </journal> <volume> 4(3) </volume> <pages> 171-183, </pages> <year> 1990. </year>
Reference-contexts: The trilinear tensor has been well 43 known in disguise in the context of Euclidean line correspondences and was not iden-tified at the time as a tensor but as a collection of three matrices (a particular contraction of the tensor, correlation contractions, as explained next) <ref> [68, 70, 84] </ref>. The link between the two and the generalization to projective space was identified later by Hartley [24, 25]. <p> The three standard correlations date back to the work on structure from motion of lines across three views <ref> [68, 84] </ref> where these matrices were first introduced.
Reference: [69] <author> M. Spetsakis and J. Aloimonos. </author> <title> A unified theory of structure from motion. </title> <booktitle> In Proceedings of the ARPA Image Understanding Workshop, </booktitle> <year> 1990. </year>
Reference-contexts: We must therefore use three view geometry, where a point in one view, and lines through the corresponding points in two other views, provide a constraint, which can be written in the form of the trilinear tensor <ref> [57, 27, 69] </ref>. The 27 coefficients of the `trilinear tensor' encapsulate the camera motions and the internal parameters of the camera such as the focal length. <p> In particular, we may use as the lines the tangents to the iso-brightness contours at p 0 and p 00 , respectively, as suggested by <ref> [69] </ref>. However, there still remains the problem of finding those matching tangents in the first place. In order to avoid the need to first find correspondences we use the results from Section 2.5.2.
Reference: [70] <author> M. Spetsakis and J. Aloimonos. </author> <title> A unified theory of structure from motion. </title> <booktitle> In Proceedings of the ARPA Image Understanding Workshop, </booktitle> <year> 1990. </year>
Reference-contexts: The trilinear tensor has been well 43 known in disguise in the context of Euclidean line correspondences and was not iden-tified at the time as a tensor but as a collection of three matrices (a particular contraction of the tensor, correlation contractions, as explained next) <ref> [68, 70, 84] </ref>. The link between the two and the generalization to projective space was identified later by Hartley [24, 25].
Reference: [71] <author> G. P. Stein. </author> <title> Internal camera calibration using rotation and geometric shapes. </title> <type> Technical Report AITR-1426, </type> <institution> Artificial Intelligence Laboratory, Massachusetts Institute of Technology, </institution> <address> Cambridge, MA, </address> <year> 1993. </year> <month> 169 </month>
Reference-contexts: For medium and wide angle lenses, non-linear lens distortion and in particular radial lens distortion, can be a significant factor with errors of 10100pixels near the edges of the image <ref> [71] </ref>. These are typically the lenses used when performing image based 3D reconstruction of large objects or in a confined space, and which are used in the experiments throughout this thesis. <p> Further engineering would be involved in making a practical system. 146 Chapter 7 Lens Distortion Calibration Using Point Correspondences 7.1 Introduction Radial lens distortion can be a significant factor in medium to wide angle lenses. The errors can be 10-100 pixels at the edges of the image <ref> [71] </ref>. These are typically the lenses used when performing image based 3D reconstruction of large objects or in a confined space. These are also the type of lenses which have been used in the experiments throughout this thesis. <p> Projective constraints: Under perspective projection, straight lines in space project to straight lines in the image. With real lenses the lines appear instead to be slightly to moderately curved. By searching for lens distortion parameters which straighten the lines the Plumb Line method and its derivatives [8] [20] <ref> [71] </ref> [12] find the lens distortion without needing to find the external parameters or the other internal camera parameters. One or more images can be used. Unknown world coordinates: Stein [71], and Du and Brady [13] use corresponding points or edges in images where the camera has undergone pure rotation to <p> By searching for lens distortion parameters which straighten the lines the Plumb Line method and its derivatives [8] [20] <ref> [71] </ref> [12] find the lens distortion without needing to find the external parameters or the other internal camera parameters. One or more images can be used. Unknown world coordinates: Stein [71], and Du and Brady [13] use corresponding points or edges in images where the camera has undergone pure rotation to find the internal camera parameters including lens distortion. <p> y u = y d + y 0 0 2 0 4 where K 1 and K 2 are the first and second parameters of radial distortion and: r d = x d + y d = (x 2 d c yr ) 2 (7.5) It has been shown in <ref> [71] </ref> that allowing the center of radial distortion, (c xr ; c yr ) to be different from the principal point is a good approximation to adding the terms for decentering distortion as given in [66]. 7.4 The Algorithm The step by step algorithm is as follows: 1.
Reference: [72] <author> G. P. Stein. </author> <title> Accurate internal camera calibration using rotation, with analysis of sources of error. </title> <booktitle> In Proceedings of the International Conference on Computer Vision, </booktitle> <pages> pages 230-236. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> June </month> <year> 1995. </year>
Reference-contexts: In particular, the issues of calibration are unique to this application. 5.4.1 Calibration We assume the internal parameters of the first camera are known. These can be found using methods described in <ref> [72] </ref>. For true Euclidean reconstruction an accurate estimate of the focal length is required but the whole process degrades gracefully when only approximate values are provided. Calibration of the stereo pair is performed in two stages.
Reference: [73] <author> G. P. Stein. </author> <title> Lens distortion calibration using point correspondences. </title> <booktitle> In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <address> Puerto Rico, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: This does not affect the stability of the method but the accuracy of the motion estimates is reduced and the 3D reconstruction suffers non-projective distortion. Flat surfaces and straight lines appear slightly curved. A variety of methods to compute lens distortion appear in the literature (see <ref> [73] </ref>) and a new method is presented in Chapter 7). 123 5.4.2 Computing Depth To compute the depth at every point we use equations (3.16). <p> The image resolution was 640 fi 480 pixels . To achieve the results presented here we had to take into account radial lens distortion. Only the first term of radial distortion was used. The radial distortion parameter, K 1 = 6e 7 was found using the method described in <ref> [73] </ref>. We note that that parameter value also minimized the error terms in equation (6.5). 6.3.2 Determining the LLC The three input images (Figure 6-3a, Figure 6-3b, and Figure 6-3c) will be denoted Image 1, Image 2 and Image 3 respectively.
Reference: [74] <author> G. P. Stein and A. Shashua. </author> <title> Model based brightness constraints: On direct estimation of structure and motion. </title> <booktitle> In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <address> Puerto Rico, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: By combining the optical flow constraint equation with the geometric model of the `trilinear tensor' we obtain the tensor brightness constraint <ref> [74] </ref> that describes the relationship between the spatio-temporal brightness derivatives at each pixel in the image, with the camera parameters modeled by the 27 coefficients of the trilinear tensor. <p> The link between the two and the generalization to projective space was identified later by Hartley [24, 25]. Additional work in this area can be found in [62, 17, 80, 30, 59, 2], and applications in <ref> [3, 74] </ref>. 2.4.1 Tensor Slices and Contractions From Figure 2-1 it is clear that the tensor can be sliced in three principle ways. This can be generalized into performing a contraction operation on the tensor with a vector. <p> The tensor brightness constraint was first presented in [64] and a practical implementation with results was first described in <ref> [74] </ref>. During the course of implementation various subtleties and limitations were discovered. 3.1.2 Overview of this Chapter In Section 3.2 we derive the the tensor brightness constraint, the model-based brightness constraint for the projective camera model. <p> There is a unique non-trivial solution if the esti mation matrix is of rank = 19. But as in the projective case, the system of homogeneous equations is degenerate and in this case the rank of the estimation matrix is 16 not 19. In <ref> [64, 74] </ref> a model-based brightness constraint for the calibrated, small rotation model was derived in a different way. <p> These are hard to achieve with standard lenses. With narrower fields of view the iterative scheme described in Section 3.7.2 fails to converge. In previously published work <ref> [74] </ref> we managed to stabilize the results for small rotation angles by unknowingly biasing the results towards small rotation. This was done by setting the focal length parameter to 50 instead of around 600. <p> As a result the rotation estimates were considerably smaller that the true values. This also resulted in errors in the translation and depth estimates. It is important to note that while the magnitude of the angle estimates in <ref> [74] </ref> were too small, the estimated axis of rotation was correct. In fact the results show a linear relationship between true angle and estimated angle. That the rotation values were always smaller than the true values is consistent with the simulation results shown in (Section 3.7.2): 1. <p> Experiment 3 tests a standard optical flow program on some of the same input images and compares the results with the `direct methods'. We then move on to case where the motion involves both translation and rotation. Experiment 4 revisits the image sequence used in Stein & Shashua <ref> [74] </ref>. We show that the improvements described in Section 3.7.5 greatly improve the rotation estimates. In Experiment 5 we repeat the heading estimation experiments but this time the motion includes some rotation. <p> the complexity of such a system. 4.3 Translation and Rotation 4.3.1 Experiment 4: Translation and Rotation (Reanalysis of Data from Stein & Shashua, CVPR97) In this experiment we test the modified algorithm for recovering both translation and rotation (Section 3.7.5) on the same data set used by Stein & Shashua <ref> [74] </ref>. Experimental Procedure: The images used by [74] were taken with an 8:5mm lens in the following way: The camera was translated first vertically (10mm) and then horizontally (5mm) to the right. <p> Translation and Rotation 4.3.1 Experiment 4: Translation and Rotation (Reanalysis of Data from Stein & Shashua, CVPR97) In this experiment we test the modified algorithm for recovering both translation and rotation (Section 3.7.5) on the same data set used by Stein & Shashua <ref> [74] </ref>. Experimental Procedure: The images used by [74] were taken with an 8:5mm lens in the following way: The camera was translated first vertically (10mm) and then horizontally (5mm) to the right. At this third position the camera was rotated to various angles ranging from 4:0 o to 1:0 o . <p> This preprocessing stage is often called rectification. The second stage is to find the translation between the two cameras. We move the whole stereo rig in pure translation. We then use equation (3.20) which gives accurate results under the assumption of pure translation <ref> [74] </ref> to compute both the translation of the rig and the displacement between the two cameras. Lens Distortion Since we us are using a lens with a wide field of view there is noticeable lens distortion.
Reference: [75] <author> R. Szeliski and S. Kang. </author> <title> Recovering 3d shape and motion from image streams using non-linear least squares. </title> <type> Technical Report CRL 93/3, </type> <institution> DEC CRL, </institution> <month> March </month> <year> 1992. </year>
Reference-contexts: While not the main focus of this thesis, we show some good shape reconstruction results (for example Figure 1-2). Good results have also been achieved using orthographic or weak perspective camera models and an object centered coordinate system <ref> [79, 35, 81, 75, 54] </ref>.
Reference: [76] <author> R. Szeliski and S. B. Kang. </author> <title> Direct methods for visual scene reconstruction. </title> <booktitle> In Proceedings IEEE Workshop on Representation of Visual Scenes, </booktitle> <address> Cambridge, MA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: The problem is therefore ill posed and additional constraints are needed. Negahdaripour & Horn [52] present a closed form solution assuming a planar or quadratic surface. Szeliski & Kang <ref> [76] </ref> describe an iterative solution using splines to enforce a smoothness constraint on the depth.
Reference: [77] <author> T. Y. Tian, C. Tomasi, and D. J. Heeger. </author> <title> Comparison of approaches to egomotion computation. </title> <booktitle> In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 315-320, </pages> <address> San Francisco, California, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: After computing the correspondences the camera motion between two or three views and the 3D structure of the scene are recovered using geometric constraints. Here we include both `optical flow' based methods <ref> [42, 43, 28, 55, 77] </ref> and feature based methods [79, 16, 65, 67, 4]. The main problem with these approaches is that computing the correspondence is in itself a difficult and error prone process. <p> These might be either dense correspondence in the form of optical flow [33, 43, 49] or feature correspondence [11, 83, 16]. Then the correspondences are used to compute the camera motion and scene structure. The advantage of our method over both optical flow methods <ref> [42, 43, 28, 55, 77] </ref> and feature based methods [79, 16, 65, 67, 4] is that no prior computation of correspondences is needed, a computation which in itself is error prone.
Reference: [78] <author> A. P. Tirumalai, B. G. Schunk, and R. C. Jain. </author> <title> Dynamic stereo with self calibration. </title> <booktitle> In Proceedings of the International Conference on Computer Vision, </booktitle> <pages> pages 466-470, </pages> <address> Osaka, Japan, </address> <month> December </month> <year> 1990. </year>
Reference-contexts: Then they use the relationship between stereo disparity and the change in stereo disparity to help resolve the correspondence problem for stereo and also compute the motion of the stereo rig. Tirumalai et al <ref> [78] </ref> use stereo depth values and initial motion estimates to help matching features over the images sequence. Navab et al [51], in contrast to other approaches, combine motion and stereo. This is applied to the reconstruction of motion and structure from line correspondences.
Reference: [79] <author> C. Tomasi and T. Kanade. </author> <title> Shape and motion from image streams a factorization method. </title> <journal> International Journal of Computer Vision, </journal> <volume> 9(2) </volume> <pages> 137-154, </pages> <year> 1992. </year>
Reference-contexts: While not the main focus of this thesis, we show some good shape reconstruction results (for example Figure 1-2). Good results have also been achieved using orthographic or weak perspective camera models and an object centered coordinate system <ref> [79, 35, 81, 75, 54] </ref>. <p> After computing the correspondences the camera motion between two or three views and the 3D structure of the scene are recovered using geometric constraints. Here we include both `optical flow' based methods [42, 43, 28, 55, 77] and feature based methods <ref> [79, 16, 65, 67, 4] </ref>. The main problem with these approaches is that computing the correspondence is in itself a difficult and error prone process. Dense correspondence methods such as `optical flow' suffer from the aperture problem and it is necessary to assume smoothness in the flow [33, 43, 49]. <p> Then the correspondences are used to compute the camera motion and scene structure. The advantage of our method over both optical flow methods [42, 43, 28, 55, 77] and feature based methods <ref> [79, 16, 65, 67, 4] </ref> is that no prior computation of correspondences is needed, a computation which in itself is error prone. Since we obtain a linear system of equations that combines the information from all the pixels in the image, we avoid the aperture problem.
Reference: [80] <author> B. Triggs. </author> <title> Matching constraints and the joint image. </title> <booktitle> In Proceedings of the International Conference on Computer Vision, </booktitle> <pages> pages 338-343, </pages> <address> Cambridge, MA, June 1995. </address> <publisher> IEEE Computer Society Press, IEEE Computer Society Press. </publisher>
Reference-contexts: The link between the two and the generalization to projective space was identified later by Hartley [24, 25]. Additional work in this area can be found in <ref> [62, 17, 80, 30, 59, 2] </ref>, and applications in [3, 74]. 2.4.1 Tensor Slices and Contractions From Figure 2-1 it is clear that the tensor can be sliced in three principle ways. This can be generalized into performing a contraction operation on the tensor with a vector.
Reference: [81] <author> S. Ullman and R. Basri. </author> <title> Recognition by linear combination of models. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <note> PAMI-13:992|1006, 1991. Also in M.I.T AI Memo 1152, </note> <year> 1989. </year>
Reference-contexts: While not the main focus of this thesis, we show some good shape reconstruction results (for example Figure 1-2). Good results have also been achieved using orthographic or weak perspective camera models and an object centered coordinate system <ref> [79, 35, 81, 75, 54] </ref>.
Reference: [82] <author> A. M. Waxman and J. H. Duncan. </author> <title> Binocular image flows: steps towards stereo-motion fusion. </title> <journal> Pattern Analysis and Machine Intelligence, </journal> <volume> 8(6) </volume> <pages> 715-729, </pages> <month> November </month> <year> 1986. </year>
Reference-contexts: Abdel-Mottaleb et al [1] use the motion to get a rough depth map in one image which is then used to simplify the correspondence problem between the images in a stereo pair. Waxman & Duncan <ref> [82] </ref> first use the monocular optical flow to determine depth discontinuities which cause problems in stereo matching. Then they use the relationship between stereo disparity and the change in stereo disparity to help resolve the correspondence problem for stereo and also compute the motion of the stereo rig.
Reference: [83] <author> J. Weng, N. Ahuja, and T. Huang. </author> <title> Two view matching. </title> <booktitle> In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 64-73, </pages> <address> Tampa, Florida, </address> <month> December </month> <year> 1989. </year>
Reference-contexts: Sparse methods try to find matching features such as lines or corner points. It is still an open subject of research as to what makes a good feature <ref> [11, 83, 16] </ref>. Some points and lines in the image do not correspond to any real feature in 3D (e.g. the occluding contour of a cylinder or of the head in Figure 1-1). This issue is discussed in more depth in the introduction to Chapter 3. <p> These might be either dense correspondence in the form of optical flow [33, 43, 49] or feature correspondence <ref> [11, 83, 16] </ref>. Then the correspondences are used to compute the camera motion and scene structure.
Reference: [84] <author> J. Weng, T. Huang, and N. Ahuja. </author> <title> Motion and structure from line correspondences: Closed form solution, uniqueness and optimization. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 14(3), </volume> <year> 1992. </year>
Reference-contexts: The trilinear tensor has been well 43 known in disguise in the context of Euclidean line correspondences and was not iden-tified at the time as a tensor but as a collection of three matrices (a particular contraction of the tensor, correlation contractions, as explained next) <ref> [68, 70, 84] </ref>. The link between the two and the generalization to projective space was identified later by Hartley [24, 25]. <p> The three standard correlations date back to the work on structure from motion of lines across three views <ref> [68, 84] </ref> where these matrices were first introduced. <p> An LLC includes in particular the case of lines on parallel planes whose degeneracy was observed in <ref> [84] </ref>. To appreciate the practical importance of investigating LLC configurations, consider a few typical outdoor and indoor scene examples depicted in Figure 6-1. In lines on the two faces of the building meet the edge in the image plane, and the vertical lines meet the edge at infinity.
Reference: [85] <author> J. Weng, P. Cohen, and N. Rebibo. </author> <title> Motion and structure estimation from stereo image sequnces. </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> 8(3) </volume> <pages> 362-382, </pages> <month> June </month> <year> 1992. </year> <month> 170 </month>
Reference-contexts: To build a complete model of a scene we would like to combine together multiple 3D views. One solution is to use a moving stereo rig (two cameras mounted onto the same rigid frame). The standard approach to reconstruction from a moving stereo rig <ref> [39, 91, 85, 16] </ref> has been to first compute a depth map from each pair (or compute the depth of a 19 (a) (b) to show detail. set of features points), together with a confidence measure for each depth estimate. <p> The motion estimates between stereo pairs enable us to combine depth maps from all the pairs in the sequence to form an extended scene reconstruction. The typical approach to reconstruction from a moving stereo rig <ref> [39, 91, 85, 16, 88, 22] </ref> has been to first compute a depth map from each pair (or 3D location of features points). One also computes a confidence measure for each depth estimate.
Reference: [86] <author> J. Weng et al. </author> <title> Camera calibration with distortion models and accuracy evaluation. </title> <journal> Pattern Analysis and Machine Intelligence, </journal> <volume> 14 </volume> <pages> 965-980, </pages> <year> 1992. </year>
Reference-contexts: Using iterative methods it then finds both external parameters (position and orientation) and internal camera parameters. The internal camera parameters include the parameters of the pinhole camera model (principal point, principal distance) and the parameters of lens distortion. See also Weng et al. <ref> [86] </ref>. Projective constraints: Under perspective projection, straight lines in space project to straight lines in the image. With real lenses the lines appear instead to be slightly to moderately curved.
Reference: [87] <author> G. Wolberg. </author> <title> Digital Image Warping. </title> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> California, </address> <year> 1990. </year>
Reference-contexts: Image warping in general is described in <ref> [87] </ref>. Let I 0 old be the original Image 2 and let I 0 new be the warped image we are trying to create.
Reference: [88] <author> S. Xuening and M. E. Spetsakis. </author> <title> A comparison of weighted ls methods with ls methods in 3d motion estimation from stereo image sequences. </title> <booktitle> In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 200-205, </pages> <address> New York, NY, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: The motion estimates between stereo pairs enable us to combine depth maps from all the pairs in the sequence to form an extended scene reconstruction. The typical approach to reconstruction from a moving stereo rig <ref> [39, 91, 85, 16, 88, 22] </ref> has been to first compute a depth map from each pair (or 3D location of features points). One also computes a confidence measure for each depth estimate.
Reference: [89] <author> J. W. Young. </author> <title> Projective Geometry. </title> <journal> The Mathematical Society of America, </journal> <year> 1930. </year>
Reference-contexts: Then we review the photometric constraints, which take the form of the optical flow constraint equation. This thesis uses results from projective geometry. A very readable introduction to the subject of projective geometry is given in the book by Young <ref> [89] </ref>. A modern book dealing more specifically with the application of projective geometry to computer vision is [16]. The appendix of [48] provides a concise summary of the basic material. 2.1 Notation We will use uppercase bold to denote matrices (e.g. A).
Reference: [90] <author> Z. Zhang. </author> <title> On the epipolar geometry between two images with lens distortion. </title> <booktitle> In Proceedings of the International Conference on Pattern Recognition, </booktitle> <volume> volume 1, </volume> <pages> pages 407-411, </pages> <address> Vienna, Austria, </address> <month> August a996. </month>
Reference-contexts: We then find lens distortion parameters that correct the image coordinates of the feature points to minimize this error. The error function is in general well behaved, and the distortion parameters can easily be found by nonlinear search techniques. A variant of this method was proposed by Zhang <ref> [90] </ref> using two cameras from a stereo pair. He found that it was difficult to find the lens distortion parameters unless the distortion was severe or the points were found very accurately.
Reference: [91] <author> Z. Zhang and O. D. Faugeras. </author> <title> Three dimensional motion computation and segemen-tation in a long sequnce of stereo frames. </title> <journal> International Journal of Computer Vision, </journal> <volume> 7(3) </volume> <pages> 211-241, </pages> <year> 1992. </year>
Reference-contexts: The estimated rotation was 1:62 o . There are of course errors around the boundary of the image where there is no overlap between the images. are described in Chapter 4. 1.2 Combining Motion and Stereo Stereo and motion together form a powerful combination <ref> [23, 91] </ref> with a variety of applications from camera motion estimation to extended scene reconstruction. A stereo system (with two or more cameras) can give accurate depth estimates but only from one view point. <p> To build a complete model of a scene we would like to combine together multiple 3D views. One solution is to use a moving stereo rig (two cameras mounted onto the same rigid frame). The standard approach to reconstruction from a moving stereo rig <ref> [39, 91, 85, 16] </ref> has been to first compute a depth map from each pair (or compute the depth of a 19 (a) (b) to show detail. set of features points), together with a confidence measure for each depth estimate. <p> Note in overhead view (d) true Euclidean structure is recovered with correct 90 o angles. 116 Chapter 5 Direct Estimation of Motion and Extended Scene Structure from a Moving Stereo Rig 5.1 Introduction Stereo and motion together form a powerful combination <ref> [23, 91] </ref> with a variety of applications from ego motion estimation to extended scene reconstruction. In this chapter we describe a new method for directly estimating the motion of a stereo rig thus greatly simplifying the process of combining the information from multiple image pairs. <p> The motion estimates between stereo pairs enable us to combine depth maps from all the pairs in the sequence to form an extended scene reconstruction. The typical approach to reconstruction from a moving stereo rig <ref> [39, 91, 85, 16, 88, 22] </ref> has been to first compute a depth map from each pair (or 3D location of features points). One also computes a confidence measure for each depth estimate.
References-found: 91

