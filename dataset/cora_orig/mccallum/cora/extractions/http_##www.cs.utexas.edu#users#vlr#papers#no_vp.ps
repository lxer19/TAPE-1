URL: http://www.cs.utexas.edu/users/vlr/papers/no_vp.ps
Refering-URL: http://www.cs.utexas.edu/users/vlr/pub.html
Root-URL: 
Title: Implementation of Parallel Graph Algorithms on the MasPar (Preprint)  
Author: TSAN-SHENG HSU, VIJAYA RAMACHANDRAN, AND NATHANIEL DEAN 
Date: July 22, 1993  
Note: DIMACS Series in Discrete Mathematics and Theoretical Computer Science Volume 00, 0000  
Abstract: Graphs play an important role in modeling the underlying structure of many real world problems. Over the past couple of decades, efficient sequential algorithms have been developed for several graph problems and have been implemented on sequential machines. The NETPAD system at Bellcore is a general tool for graph manipulations and algorithm design that facilitates such implementations. More recently, several research results on efficient parallel algorithms have been developed, but not much implementation has been done. We have implemented some of the parallel algorithms for basic graph problems on the massively parallel machine MasPar, and we have interfaced these algorithms with NETPAD. In this paper, we give a description of our implementation together with some performance data. We also describe the interface that we have built between our library of parallel graph algorithms and the NETPAD system. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> B. Awerbuch and Y. Shiloach, </author> <title> New connectivity and MSF algorithms for shu*e-exchange network and PRAM, </title> <booktitle> IEEE Tran. on Computers (1987), </booktitle> <pages> 1258-1263. </pages>
Reference-contexts: Instead, an alternative search technique called ear decomposition has proved to be a very useful tool for designing parallel graph algorithms [7, 6, 10, 17, 20, 19]. Combined with an efficient parallel routine for finding connected components <ref> [1] </ref> and the Euler tour technique [24], we have efficient parallel algorithms for several important graph problems which include various connectivity problems [6, 17, 19], st-numbering [10], planarity testing and embedding [20], finding a strong orientation and finding a minimum cost spanning forest z . <p> We implemented the CRCW PRAM algorithm described in <ref> [1] </ref>. The PRAM algorithm runs in O (log n) time using O (n + m) processors on a graph with n vertices and m edges. The concurrent read operation was implemented by using the global router. The concurrent write operation needed in the algorithm was implemented by the sendwith operation. <p> After the execution of the algorithm, the ith PE gets a number indicating the connected component containing the ith vertex. The component number is the vertex number of the vertex with the least vertex number in the connected component. (ii) Spanning forest. We modified the algorithm in <ref> [1] </ref> for finding connected components to find a spanning forest of the input graph. The original algorithm partitions the set of vertices into a set of disjoint sets such that vertices in each set are in the same connected component. Initially, the algorithm puts a vertex in each set. <p> Our program selects an edge connecting a vertex in one set to a vertex in the other set while merging these two disjoint vertex sets. (iii) Minimum cost spanning forest. Given the input graph with an integer weight assigned on each edge, we modify the algorithm in <ref> [1] </ref> for finding connected components to find a minimum cost spanning forest for the input graph. This algorithm also partitions the graph into disjoint sets of vertices.
Reference: 2. <author> G. E. Blelloch, </author> <title> Scan primitives and parallel vector models, </title> <type> Ph.D. thesis, </type> <institution> M.I.T., </institution> <month> October </month> <year> 1989. </year>
Reference-contexts: These global memory accesses can be implemented only as requests to the global router, since elements in a list are not structurally allocated such that we can use the XNET configuration. An operation on an array of elements called prefix sums [9] (or scan <ref> [2, 11] </ref>) is to compute the prefix sum of all the elements before each array element; the prefix sum operator can be any associative operator. This computation is similar to list ranking, except that the input is in an array rather than a linked list.
Reference: 3. <author> N. Dean, M. Mevenkamp, and C. L. Monma, NETPAD: </author> <title> An interface graphics system for network modeling and optimization, </title> <booktitle> Proc. Computer Science & Operations Research: New Developments in their Interfaces, </booktitle> <publisher> Pergamon Press, </publisher> <year> 1992, </year> <pages> pp. 231-243. </pages>
Reference-contexts: In addition, we also built an interface between the graph algorithm design package developed at Bellcore called NETPAD <ref> [3, 14, 15, 16] </ref> and our parallel programs. Our purpose was to experiment and set up programming environments for implementing parallel algorithms on massively parallel computers. The organization of the paper is as follows. Section 2 gives an introduction to efficient parallel graph algorithms.
Reference: 4. <author> T.-s. Hsu, V. Ramachandran, and N. Dean, </author> <title> Implementation of parallel graph algorithms on a massively parallel SIMD computer with virtual processing, </title> <booktitle> Proc. 9th International Parallel Processing Symp., </booktitle> <year> 1995, </year> <pages> pp. 106-112. </pages>
Reference-contexts: We have also written sequential programs for solving the same graph problems and studied the relative speed-up of the parallel programs over the sequential ones. The performance presented in this paper is further analyzed in <ref> [4] </ref> where least-squares-fit curves are obtained for each set of data. We note a few observations. * PRAM based graph algorithms can be implemented efficiently and easily. The PRAM model has proven to be a very good theoretical model for designing parallel algorithms. <p> Because the MPL programming language requires explicit allocation of PE's, we need to modify our code to handle this. We also need to implement optimal PRAM algorithms to obtain the best speed-up results when using 24 HSU, RAMACHANDRAN, AND DEAN virtual processors. (These issues are discussed in <ref> [4] </ref>.) * Further extension of the parallel graph algorithm library.
Reference: 5. <institution> IEEE Computer, New products column, </institution> <month> January </month> <year> 1991, </year> <pages> pp. 113-115. </pages>
Reference-contexts: In some tests, it is more than 15 times slower. Our tests also show that the SPARC I workstation is about 1.75 times slower than the SPARC II workstation. (This figure is confirmed by data in <ref> [5] </ref>.) Thus the SPARC II workstation is at least 200 times faster than each MasPar PE and the SPARC I workstation is at least 114 times faster than each MasPar PE.
Reference: 6. <author> A. Kanevsky and V. Ramachandran, </author> <title> Improved algorithms for graph four-connectivity, </title> <journal> J. Comp. System Sci. </journal> <volume> 42 (1991), </volume> <pages> 288-306. </pages>
Reference-contexts: Hence we are unable to obtain efficient parallel algorithms by parallelizing sequential algorithms based on depth-first search or breadth-first search. Instead, an alternative search technique called ear decomposition has proved to be a very useful tool for designing parallel graph algorithms <ref> [7, 6, 10, 17, 20, 19] </ref>. <p> Combined with an efficient parallel routine for finding connected components [1] and the Euler tour technique [24], we have efficient parallel algorithms for several important graph problems which include various connectivity problems <ref> [6, 17, 19] </ref>, st-numbering [10], planarity testing and embedding [20], finding a strong orientation and finding a minimum cost spanning forest z . Figure 1 illustrates the building blocks for designing parallel graph algorithms using ear decomposition, the Euler tour technique and the routine for finding connected components.
Reference: 7. <author> R. M. Karp and V. Ramachandran, </author> <title> Parallel algorithms for shared-memory machines, </title> <booktitle> Handbook of Theoretical Computer Science (J. </booktitle> <editor> van Leeuwen, ed.), </editor> <publisher> North Holland, </publisher> <year> 1990, </year> <pages> pp. 869-941. </pages>
Reference-contexts: Parallel Graph Algorithms In designing sequential graph algorithms, depth-first search and breadth-first search have been used as basic search strategies for solving various graph problems [22]. Unfortunately, at present no efficient parallel algorithms are known for these two search techniques <ref> [7] </ref>. Hence we are unable to obtain efficient parallel algorithms by parallelizing sequential algorithms based on depth-first search or breadth-first search. Instead, an alternative search technique called ear decomposition has proved to be a very useful tool for designing parallel graph algorithms [7, 6, 10, 17, 20, 19]. <p> Hence we are unable to obtain efficient parallel algorithms by parallelizing sequential algorithms based on depth-first search or breadth-first search. Instead, an alternative search technique called ear decomposition has proved to be a very useful tool for designing parallel graph algorithms <ref> [7, 6, 10, 17, 20, 19] </ref>. <p> Implementation Strategies for PRAM Algorithms. In this section, we describe the PRAM model and the strategies we used to map algorithms designed on a PRAM onto the MasPar architecture. PRAM Models A PRAM machine <ref> [7] </ref> consists of a pool of random access machines (RAM) and a global memory. Each random access machine has a processor with a reasonable set of instructions and a local memory. Each RAM has an unique ID numbered from 1 to the number of processors in the PRAM machine. <p> In the PRIORITY CRCW model, if several processors try to write different data in the same global memory location at any given time, the data sent by the processor with the least ID is written into the memory location. There are various other CRCW models. For details, see <ref> [7] </ref>. Mapping of the PRAM Model onto the MasPar Architecture We map part of the local memory in each PE and the local memory of the ACU onto the PRAM global memory. <p> The PRAM algorithms often link all the edges or all the nodes in a special ordered linked list and perform list processing computations such as list ranking <ref> [7] </ref> (a list ranking on a linked list requires each element in the list to compute the suffix sum of all the elements in front of it; the sum could be any associative operator). <p> To understand the behavior of the concurrent read operation executed by the global router, we implemented a routine to transform concurrent read requests to exclusive read requests using the standard simulation algorithm (see, e.g., <ref> [7] </ref>). We found that the performance of our parallel algorithms when using the global router and using our simulated concurrent read routines is very similar. For concurrent write, the global router does not allow more than one PE to write into the global memory bank of any particular PE.
Reference: 8. <author> B. W. Kernighan and D. M. Ritchie, </author> <title> The C programming language, </title> <publisher> Prentice Hall, </publisher> <address> Engle-wood Cliffs, NJ, </address> <year> 1978. </year>
Reference-contexts: More details are described in Section 3.3. We used the MasPar Parallel Language (MPL) [11, 12] to implement our algorithms. MPL is an extension of the C language described in <ref> [8] </ref>. In addition to all of the standard C language features, it allows the user program a set of PE's to execute the same instruction on their own local data. MPL leaves the responsibility of processor allocation to the programmer. <p> It uses the X-window system [18], and one can edit graphs and display them easily. The NETPAD system also provides a rich set of basic graph manipulation operations. By calling these operations in one's own program (preferably written in C <ref> [8] </ref>), users can implement their own graph algorithms easily. NETPAD was used in the following ways to support the design of our parallel graph algorithm packages. It provided routines for generating test graphs. We also used it as a standard interface to input graphs generated by other packages.
Reference: 9. <author> R. E. Ladner and M. J. Fischer, </author> <title> Parallel prefix computation, </title> <editor> J. </editor> <booktitle> ACM 27 (1980), </booktitle> <pages> 831-838. </pages>
Reference-contexts: These global memory accesses can be implemented only as requests to the global router, since elements in a list are not structurally allocated such that we can use the XNET configuration. An operation on an array of elements called prefix sums <ref> [9] </ref> (or scan [2, 11]) is to compute the prefix sum of all the elements before each array element; the prefix sum operator can be any associative operator. This computation is similar to list ranking, except that the input is in an array rather than a linked list.
Reference: 10. <author> Y. Maon, B. Schieber, and U. Vishkin, </author> <title> Parallel ear decomposition search (EDS) and st-numbering in graphs, </title> <type> Theoret. </type> <institution> Comput. Sci. </institution> <year> (1986), </year> <pages> 277-298. </pages>
Reference-contexts: Hence we are unable to obtain efficient parallel algorithms by parallelizing sequential algorithms based on depth-first search or breadth-first search. Instead, an alternative search technique called ear decomposition has proved to be a very useful tool for designing parallel graph algorithms <ref> [7, 6, 10, 17, 20, 19] </ref>. <p> Combined with an efficient parallel routine for finding connected components [1] and the Euler tour technique [24], we have efficient parallel algorithms for several important graph problems which include various connectivity problems [6, 17, 19], st-numbering <ref> [10] </ref>, planarity testing and embedding [20], finding a strong orientation and finding a minimum cost spanning forest z . Figure 1 illustrates the building blocks for designing parallel graph algorithms using ear decomposition, the Euler tour technique and the routine for finding connected components. Our parallel implementations followed this approach.
Reference: 11. <institution> MasPar Computer Co., </institution> <note> MasPar parallel application language (MPL) reference manual, version 2.0 ed., </note> <month> March </month> <year> 1991. </year>
Reference-contexts: In order to perform the latter transfer, the set IMPLEMENTATION OF PARALLEL GRAPH ALGORITHMS 5 6 HSU, RAMACHANDRAN, AND DEAN of PE's to receive data from the front end must form a rectangular block. More details are described in Section 3.3. We used the MasPar Parallel Language (MPL) <ref> [11, 12] </ref> to implement our algorithms. MPL is an extension of the C language described in [8]. In addition to all of the standard C language features, it allows the user program a set of PE's to execute the same instruction on their own local data. <p> These global memory accesses can be implemented only as requests to the global router, since elements in a list are not structurally allocated such that we can use the XNET configuration. An operation on an array of elements called prefix sums [9] (or scan <ref> [2, 11] </ref>) is to compute the prefix sum of all the elements before each array element; the prefix sum operator can be any associative operator. This computation is similar to list ranking, except that the input is in an array rather than a linked list. <p> There is already such a routine called scan which is implemented in MasPar system library <ref> [11] </ref>. The scan operation is very fast compared to the list ranking algorithm we implemented. Note that a linked list can be converted into an array by first performing a list ranking computation and then rearranging the list elements into an array using a global memory write. <p> The prefix sums operation is more than 10 times faster than the EREW list ranking. IMPLEMENTATION OF PARALLEL GRAPH ALGORITHMS 11 system library routine sendwith <ref> [11] </ref> can be used to implement concurrent write. The sendwith routine allows each PE to send data to the global memory bank of any PE.
Reference: 12. <author> MasPar Computer Co., </author> <title> MasPar parallel application language (MPL) user guide, </title> <note> version 2.0 ed., </note> <month> March </month> <year> 1991. </year>
Reference-contexts: The XNET configuration is faster, but requires all PE's getting their needed data from the same direction. For transmitting a 32-bit data through distance 1, the XNET communication is slightly less than 100 times faster than the time needed to go through the global router <ref> [12] </ref>. During the computation, it might be necessary for the PE's to access the local memory of the ACU (about 1 M bytes). Such I/O requests are sequentialized and carried out one at a time. This process is very time-consuming. <p> In order to perform the latter transfer, the set IMPLEMENTATION OF PARALLEL GRAPH ALGORITHMS 5 6 HSU, RAMACHANDRAN, AND DEAN of PE's to receive data from the front end must form a rectangular block. More details are described in Section 3.3. We used the MasPar Parallel Language (MPL) <ref> [11, 12] </ref> to implement our algorithms. MPL is an extension of the C language described in [8]. In addition to all of the standard C language features, it allows the user program a set of PE's to execute the same instruction on their own local data. <p> IMPLEMENTATION OF PARALLEL GRAPH ALGORITHMS 23 accomplished within a period of 11 weeks.) * Global routing bottleneck. The current global router on the MasPar is very slow compared to the XNET configuration. (It is 100 times slower than the XNET for transferring a 32-bit data to each PE <ref> [12] </ref>.) Although we use the XNET configuration when we can in our implementations, our parallel graph algorithms often need to use the global router for performing list computations.
Reference: 13. <institution> MasPar Computer Co., MasPar system overview, </institution> <note> version 2.0 ed., </note> <month> March </month> <year> 1991. </year>
Reference-contexts: DIMACS Series in Discrete Mathematics and Theoretical Computer Science, American Mathematical Society, volume 15, 1994, pp. 165-198. c fl0000 American Mathematical Society 0000-0000/00 $1.00 + $.25 per page 1 2 HSU, RAMACHANDRAN, AND DEAN mented efficient parallel algorithms for solving several undirected graph problems using the massively parallel computer MasPar <ref> [13] </ref> at Bellcore. In addition, we also built an interface between the graph algorithm design package developed at Bellcore called NETPAD [3, 14, 15, 16] and our parallel programs. Our purpose was to experiment and set up programming environments for implementing parallel algorithms on massively parallel computers. <p> Then we implemented efficient parallel graph algorithms developed on the PRAM model by calling routines in the kernel. 3. Implementation Environment In this section we describe the environment in which we implemented several efficient PRAM graph algorithms. In Section 3.1, we first describe the architecture of the MasPar machine <ref> [13] </ref>. Section 3.2 discusses the general is z In this paper, a spanning forest of a graph G is a maximal subgraph of G (w.r.t. the edges in G) that is a forest. <p> Our parallel implementations are integrated with a graph algorithm design package called NETPAD [14, 15, 16]. Section 3.3 describes the NETPAD software and our interface between it and our parallel programs. 3.1. The Parallel Machine MasPar. The MasPar computer <ref> [13] </ref> is a fine-grained massively parallel single-instruction-multiple-data (SIMD) computer. All of its parallel processors synchronously execute the same instruction at the same time. A simplified version of its architecture is shown in Figure 2. <p> Programs are stored in one special local memory bank of ACU and broadcast to each PE simultaneously. The architecture of MasPar allows very efficient broadcasting operation from the ACU to all PE's. Since the ACU is about 10 times faster than each individual PE <ref> [13] </ref>, the other purpose of the ACU is to perform simple local computations and broadcast the results to all PE's. Since the ACU is a special processor that is designed for load and save operations, it might not be very efficient to do complex arithmetic operations on it. <p> To compute the relative speed-up between our parallel programs running on the MasPar and the sequential programs running on SPARC workstations, we need to have the ratio of the computation speed of a MasPar PE to that of a SPARC workstation. In <ref> [13] </ref> it is stated that each PE is about 10 times slower than the MasPar ACU. We tested programs running on the MasPar ACU and the MasPar front end.
Reference: 14. <author> M. Mevenkamp, </author> <title> NETPAD programmer's guide, </title> <institution> Bellcore, </institution> <month> August </month> <year> 1991. </year> <title> 15. , NETPAD reference guide, </title> <institution> Bellcore, </institution> <month> August </month> <year> 1991. </year>
Reference-contexts: In addition, we also built an interface between the graph algorithm design package developed at Bellcore called NETPAD <ref> [3, 14, 15, 16] </ref> and our parallel programs. Our purpose was to experiment and set up programming environments for implementing parallel algorithms on massively parallel computers. The organization of the paper is as follows. Section 2 gives an introduction to efficient parallel graph algorithms. <p> IMPLEMENTATION OF PARALLEL GRAPH ALGORITHMS 3 connected components, the Euler tour technique and ear decom position. 4 HSU, RAMACHANDRAN, AND DEAN sues involved in implementing PRAM algorithms on the MasPar. Our parallel implementations are integrated with a graph algorithm design package called NETPAD <ref> [14, 15, 16] </ref>. Section 3.3 describes the NETPAD software and our interface between it and our parallel programs. 3.1. The Parallel Machine MasPar. The MasPar computer [13] is a fine-grained massively parallel single-instruction-multiple-data (SIMD) computer. All of its parallel processors synchronously execute the same instruction at the same time. <p> In our implementation of PRIORITY CRCW algorithms, we used the global router to satisfy concurrent read requests and the sendwith operations to implement concurrent write requests. 3.3. NETPAD Interface. The NETPAD software <ref> [14, 15, 16] </ref> is a general tool for graph manipulations and graph algorithm design. It uses the X-window system [18], and one can edit graphs and display them easily. The NETPAD system also provides a rich set of basic graph manipulation operations.
Reference: 16. <author> M. Mevenkamp, N. Dean, and C. L. Monma, </author> <title> NETPAD user's guide, </title> <institution> Bellcore, </institution> <month> August </month> <year> 1991. </year>
Reference-contexts: In addition, we also built an interface between the graph algorithm design package developed at Bellcore called NETPAD <ref> [3, 14, 15, 16] </ref> and our parallel programs. Our purpose was to experiment and set up programming environments for implementing parallel algorithms on massively parallel computers. The organization of the paper is as follows. Section 2 gives an introduction to efficient parallel graph algorithms. <p> IMPLEMENTATION OF PARALLEL GRAPH ALGORITHMS 3 connected components, the Euler tour technique and ear decom position. 4 HSU, RAMACHANDRAN, AND DEAN sues involved in implementing PRAM algorithms on the MasPar. Our parallel implementations are integrated with a graph algorithm design package called NETPAD <ref> [14, 15, 16] </ref>. Section 3.3 describes the NETPAD software and our interface between it and our parallel programs. 3.1. The Parallel Machine MasPar. The MasPar computer [13] is a fine-grained massively parallel single-instruction-multiple-data (SIMD) computer. All of its parallel processors synchronously execute the same instruction at the same time. <p> In our implementation of PRIORITY CRCW algorithms, we used the global router to satisfy concurrent read requests and the sendwith operations to implement concurrent write requests. 3.3. NETPAD Interface. The NETPAD software <ref> [14, 15, 16] </ref> is a general tool for graph manipulations and graph algorithm design. It uses the X-window system [18], and one can edit graphs and display them easily. The NETPAD system also provides a rich set of basic graph manipulation operations.
Reference: 17. <author> G. L. Miller and V. Ramachandran, </author> <title> A new triconnectivity algorithm and its applications, </title> <booktitle> Combinatorica 12 (1992), </booktitle> <pages> 53-76. </pages>
Reference-contexts: Hence we are unable to obtain efficient parallel algorithms by parallelizing sequential algorithms based on depth-first search or breadth-first search. Instead, an alternative search technique called ear decomposition has proved to be a very useful tool for designing parallel graph algorithms <ref> [7, 6, 10, 17, 20, 19] </ref>. <p> Combined with an efficient parallel routine for finding connected components [1] and the Euler tour technique [24], we have efficient parallel algorithms for several important graph problems which include various connectivity problems <ref> [6, 17, 19] </ref>, st-numbering [10], planarity testing and embedding [20], finding a strong orientation and finding a minimum cost spanning forest z . Figure 1 illustrates the building blocks for designing parallel graph algorithms using ear decomposition, the Euler tour technique and the routine for finding connected components.
Reference: 18. <author> A. Nye, </author> <title> X window system user's guide, </title> <publisher> O'Reilly & Associates, Inc., </publisher> <year> 1988. </year>
Reference-contexts: NETPAD Interface. The NETPAD software [14, 15, 16] is a general tool for graph manipulations and graph algorithm design. It uses the X-window system <ref> [18] </ref>, and one can edit graphs and display them easily. The NETPAD system also provides a rich set of basic graph manipulation operations. By calling these operations in one's own program (preferably written in C [8]), users can implement their own graph algorithms easily.
Reference: 19. <author> V. Ramachandran, </author> <title> Parallel open ear decomposition with applications to graph biconnec-tivity and triconnectivity, Synthesis of Parallel Algorithms (J. </title> <editor> H. Reif, ed.), </editor> <publisher> Morgan-Kaufmann, </publisher> <year> 1993, </year> <pages> pp. 275-340. </pages>
Reference-contexts: Hence we are unable to obtain efficient parallel algorithms by parallelizing sequential algorithms based on depth-first search or breadth-first search. Instead, an alternative search technique called ear decomposition has proved to be a very useful tool for designing parallel graph algorithms <ref> [7, 6, 10, 17, 20, 19] </ref>. <p> Combined with an efficient parallel routine for finding connected components [1] and the Euler tour technique [24], we have efficient parallel algorithms for several important graph problems which include various connectivity problems <ref> [6, 17, 19] </ref>, st-numbering [10], planarity testing and embedding [20], finding a strong orientation and finding a minimum cost spanning forest z . Figure 1 illustrates the building blocks for designing parallel graph algorithms using ear decomposition, the Euler tour technique and the routine for finding connected components. <p> The external program exits and sends a signal to the NETPAD system. The NETPAD system gets the output file and displays the graph on the drawing window. In Figure 6, we illustrate this interface by using an example of calling a MasPar routine to perform ear decomposition <ref> [19] </ref> from NETPAD. 4. Our Implementation of Parallel Graph Algorithms In this section, we describe the parallel graph algorithms we have implemented. In Section 4.1, we describe the data structures used in implementing the algorithms. <p> Once the merger is completed, the edge that caused the merging is marked as one of the edges in the minimum cost spanning forest. (iv) Ear decomposition of a two-edge connected undirected graph. We implemented the PRAM parallel algorithm in <ref> [19] </ref> for finding an ear decomposition by calling the MasPar system sorting routine, routines in the kernel and the routine for finding a spanning forest. (v) Strong orientation of a two-edge connected undirected graph. We first find an ear decomposition for the input graph. <p> for finding all cut edges in the graph based on the recursive version of depth-first search [22]. (iii) A routine for finding a strong orientation based on the recursive version of depth-first search [22]. (iv) A routine for finding an ear decomposition based on the recursive version of depth-first search <ref> [19] </ref>. (v) Kruskal's algorithm [23] for finding a minimum cost spanning forest. All but the last of the above five routines are based on depth-first search with some special book keeping. The routine for finding an ear decomposition also needs a linear time bucket sort routine. <p> Since we have implemented most of the commonly used routines for implementing PRAM undirected graph algorithms, we expect that it will be fairly easy to implement other graph algorithms; for example, the routines for finding an open ear decomposition, biconnectivity, 3-edge connectivity, triconnectivity and planarity <ref> [19, 20] </ref>, since we have already implemented most of the basic subroutines for these problems. Acknowledgment. We would like to thank Monika Mevenkamp for her help in developing the interface between the parallel programs on the MasPar and NETPAD.
Reference: 20. <author> V. Ramachandran and J. Reif, </author> <title> Planarity testing in parallel, </title> <journal> Jour. Comput. and Sys. Sci. </journal> <volume> 49 (1994), no. 3, </volume> <pages> 517-561, </pages> <note> Special Issue for FOCS '89. </note>
Reference-contexts: Hence we are unable to obtain efficient parallel algorithms by parallelizing sequential algorithms based on depth-first search or breadth-first search. Instead, an alternative search technique called ear decomposition has proved to be a very useful tool for designing parallel graph algorithms <ref> [7, 6, 10, 17, 20, 19] </ref>. <p> Combined with an efficient parallel routine for finding connected components [1] and the Euler tour technique [24], we have efficient parallel algorithms for several important graph problems which include various connectivity problems [6, 17, 19], st-numbering [10], planarity testing and embedding <ref> [20] </ref>, finding a strong orientation and finding a minimum cost spanning forest z . Figure 1 illustrates the building blocks for designing parallel graph algorithms using ear decomposition, the Euler tour technique and the routine for finding connected components. Our parallel implementations followed this approach. <p> Since we have implemented most of the commonly used routines for implementing PRAM undirected graph algorithms, we expect that it will be fairly easy to implement other graph algorithms; for example, the routines for finding an open ear decomposition, biconnectivity, 3-edge connectivity, triconnectivity and planarity <ref> [19, 20] </ref>, since we have already implemented most of the basic subroutines for these problems. Acknowledgment. We would like to thank Monika Mevenkamp for her help in developing the interface between the parallel programs on the MasPar and NETPAD.
Reference: 21. <author> D. M. Ritchie and K. Thompson, </author> <title> The Unix timesharing system, </title> <booktitle> Communications of the ACM 17 (1974), </booktitle> <pages> 365-375. </pages>
Reference-contexts: The MasPar computer [13] is a fine-grained massively parallel single-instruction-multiple-data (SIMD) computer. All of its parallel processors synchronously execute the same instruction at the same time. A simplified version of its architecture is shown in Figure 2. The MasPar has a front end processor running the Unix operating system <ref> [21] </ref> and a Data Parallel Unit (DPU) for execution of parallel programs. The front end machine is a micro-VAX workstation. The DPU consists of an Array Control Unit (ACU) and 16384 Processor Elements (PE's). The ACU is a special purpose processor for controlling the execution of all of the PE's.
Reference: 22. <author> R. E. Tarjan, </author> <title> Depth-first search and linear graph algorithms, </title> <journal> SIAM J. Comput. </journal> <volume> 1 (1972), </volume> <pages> 146-160. </pages> <month> 23. </month> , <title> Data structures and network algorithms, </title> <publisher> SIAM Press, </publisher> <address> Philadelphia, PA, </address> <year> 1983. </year>
Reference-contexts: Finally, Section 6 gives conclusions and directions for further research. 2. Parallel Graph Algorithms In designing sequential graph algorithms, depth-first search and breadth-first search have been used as basic search strategies for solving various graph problems <ref> [22] </ref>. Unfortunately, at present no efficient parallel algorithms are known for these two search techniques [7]. Hence we are unable to obtain efficient parallel algorithms by parallelizing sequential algorithms based on depth-first search or breadth-first search. <p> After we implemented the parallel graph algorithm library, we also implemented the following sequential graph algorithms using NETPAD. (i) A recursive version of depth-first search for finding connected compo nents. (ii) A routine for finding all cut edges in the graph based on the recursive version of depth-first search <ref> [22] </ref>. (iii) A routine for finding a strong orientation based on the recursive version of depth-first search [22]. (iv) A routine for finding an ear decomposition based on the recursive version of depth-first search [19]. (v) Kruskal's algorithm [23] for finding a minimum cost spanning forest. <p> using NETPAD. (i) A recursive version of depth-first search for finding connected compo nents. (ii) A routine for finding all cut edges in the graph based on the recursive version of depth-first search <ref> [22] </ref>. (iii) A routine for finding a strong orientation based on the recursive version of depth-first search [22]. (iv) A routine for finding an ear decomposition based on the recursive version of depth-first search [19]. (v) Kruskal's algorithm [23] for finding a minimum cost spanning forest. All but the last of the above five routines are based on depth-first search with some special book keeping.
Reference: 24. <author> R. E. Tarjan and U. Vishkin, </author> <title> An efficient parallel biconnectivity algorithm, </title> <journal> SIAM J. Com-put. </journal> <volume> 14 (1985), </volume> <pages> 862-874. </pages> <institution> Department of Computer Sciences, Univ. of Texas at Austin, Austin, TX 78712 E-mail address: tshsu@cs.utexas.edu Department of Computer Sciences, Univ. of Texas at Austin, Austin, TX 78712 E-mail address: vlr@cs.utexas.edu Room 2M-389, 445 South Street, Bellcore, Morristown, </institution> <address> NJ 07960 E-mail address: nate@bellcore.com </address>
Reference-contexts: Instead, an alternative search technique called ear decomposition has proved to be a very useful tool for designing parallel graph algorithms [7, 6, 10, 17, 20, 19]. Combined with an efficient parallel routine for finding connected components [1] and the Euler tour technique <ref> [24] </ref>, we have efficient parallel algorithms for several important graph problems which include various connectivity problems [6, 17, 19], st-numbering [10], planarity testing and embedding [20], finding a strong orientation and finding a minimum cost spanning forest z . <p> For this, we implemented the O (log n)-time O (n)-processor PRAM algorithm in <ref> [24] </ref> for solving the range minimum problem on n elements. In this algorithm, the n elements are stored in an array A. <p> To process each query, a PE with ID i reads W [s i ; k i ] and W [e i 2 k i + 1; k i ] through the global router and returns the minimum of the two values. (v) Euler tour construction <ref> [24] </ref>. Given a tree represented by an adjacency list, we store each edge in the adjacency list in a different PE. Edges that are adjacent to a vertex are stored in consecutive PE's.
References-found: 22

