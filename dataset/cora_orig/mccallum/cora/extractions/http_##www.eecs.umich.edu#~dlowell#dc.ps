URL: http://www.eecs.umich.edu/~dlowell/dc.ps
Refering-URL: http://www.eecs.umich.edu/~dlowell/research.html
Root-URL: http://www.eecs.umich.edu
Abstract: 1 Abstract: Checkpointing is a general technique for recovering applications. Unfortunately, current checkpointing systems add many seconds of overhead per checkpoint. Their high overhead prevents them from making failures transparent to users and other external entities, so failures lose visible state. This paper presents a checkpointing system called Discount Checking that is built on reliable main memory and high-speed transactions. Discount Checking can be used to make general-purpose applications recoverable easily and with low overhead. The checkpoints taken by Discount Checking are extremely fast, ranging for our target applications from 50 ms to a few milliseconds. Discount Checkings low overhead makes it possible to provide ideal failure transparency by checkpointing each externally visible event. Yet even with this high rate of checkpointing, Discount Checking slows real applications down by less than 0.6%. 
Abstract-found: 1
Intro-found: 1
Reference: [Appel91] <author> Andrew W. Appel and Kai Li. </author> <title> Virtual Memory Primitives for User Programs. </title> <booktitle> In Proceedings of the 1991 International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-IV), </booktitle> <pages> pages 96107, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: In contrast, other checkpointing libraries execute the process in volatile memory and copy the process state at each checkpoint. To rollback after a process crash, Discount Checking must undo the memory modifications made during the current interval. Vista logs this undo data in Rio using copy-on-write <ref> [Appel91] </ref> and restores the memory image during recovery. A processs address space is easy to checkpoint because it can be mapped into Vistas transactional memory. However, a processs state also includes register contents, which can not be mapped into memory. <p> Discount Checking also saves the signal mask at a checkpoint and restores it during recovery. Timer: Discount Checking saves the current timer interval and restores the interval during recovery. Page protections: Some applications manipulate page protections to implement functions such as copy-on-write, distributed shared memory, and garbage collection <ref> [Appel91] </ref>. Vista also uses page protections to copy the before-image of modified pages to its undo log. Vista supports applications that manipulate page protections by intercepting mprotect, saving the applications page protections, and installing the logical-and of Vistas protection and the applications protection.
Reference: [Bartlett81] <author> Joel F. Bartlett. </author> <title> A NonStop Kernel. </title> <booktitle> In Proceedings of the 1981 Symposium on Operating System Principles, </booktitle> <pages> pages 2229, </pages> <month> December </month> <year> 1981. </year>
Reference-contexts: Related Work Recovering a failed process means reconstructing the state of the process, then restarting it. The process may be restarted on the same machine (perhaps after a reboot) or on a different machine (as is done with process pairs) <ref> [Bartlett81, Gray86] </ref>. There are two main techniques for reconstructing the state of a failed process: checkpointing and log-and-replay [Elnozahy96]. 2.1. Checkpointing Checkpointing has been used for many years [Chandy72, Koo87] and in many systems [Li90, Plank95, Tannenbaum95, Wang95].
Reference: [Birman96] <author> Kenneth P. Birman. </author> <title> Building Secure and Reliable Network Applications. </title> <publisher> Manning Publications, </publisher> <year> 1996. </year>
Reference-contexts: For example, [Slye96] required a custom thread library and object-code instrumentation to successfully track thread scheduling events and signals. The complexity of dealing with these and other sources of non-determinism has prevented the widespread use of log-and-replay in recovering general applications <ref> [Birman96, Huang95] </ref>. 2.3. Comparison of Recovery Techniques Checkpointing is a more general recovery technique than log-and-replay, because checkpointing saves the crashed state and so obviates the need for reconstructing state using repeatable re-execution. In other words, check-pointing works for non-deterministic processes, whereas log-and-replay must turn non-deterministic processes into deterministic ones.
Reference: [Borg89] <author> Anita Borg, Wolfgang Blau, Wolfgang Gra-etsch, Ferdinand Herrman, and Wolfgang Oberle. </author> <title> Fault Tolerance Under UNIX. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(1):1 24, </volume> <month> February </month> <year> 1989. </year>
Reference-contexts: Memory exclusion can reduce overhead dramatically for applications that touch a large amount of data that is not needed in recovery or is soon deallocated. However, memory exclusion adds a significant burden to the programmer using the checkpoint library. 2.2. Log-and-Replay Log-and-replay is another general-purpose recovery technique <ref> [Borg89] </ref>. Whereas checkpointing saves the state of the failed process, log-and-replay recomputes the state of the failed process. Log-and-replay starts from a prior state and rolls the process forward by re-executing the instructions. <p> In particular, all events that may cause non-deterministic execution must be logged and replayed carefully to ensure repeatability. The following are examples of these events: Message-logging systems focus on logging and replaying messages in the original order <ref> [Strom85, Koo87, Johnson87, Borg89, Lomet98] </ref>. Input from the user can be considered a form of messages. Signals and other asynchronous events are difficult to log and replay, because the effect of these events may depend on the exact processor cycle the process received the signal [Slye96]. <p> Signals and other asynchronous events are difficult to log and replay, because the effect of these events may depend on the exact processor cycle the process received the signal [Slye96]. For this reason, Targon/32 chose to checkpoint before each signal rather than log <ref> [Borg89] </ref>. In general, timing dependencies are a difficult input to log and replay. Thread scheduling events must be logged and replayed in the same order to ensure repeatability during the recovery of multi-threaded applications. Multi-threaded applications may also need to log shared-memory accesses between cooperating threads.
Reference: [Chandra98] <author> Subhachandra Chandra and Peter M. Chen. </author> <booktitle> How Fail-Stop are Faulty Programs? In Proceedings of the 1998 Symposium on Fault-Tolerant Computing (FTCS), </booktitle> <pages> pages 240249, </pages> <month> June </month> <year> 1998. </year>
Reference-contexts: For example, taking a checkpoint before sending each message guarantees globally consistent recovery without sending extra messages [Koo87]. With fast checkpoints, these schemes provide low-overhead recovery for general-purpose, distributed applications. We are also considering ways to recover from bugs that violate the fail-stop model <ref> [Schneider84, Chandra98, Chandra99] </ref>. Programs with such a bug may run for a long time after the bug is activated. We know of no current recovery system that can recover from such a bug, because the corruption caused by the bug may be preserved in the recovery data (checkpoint or log).
Reference: [Chandra99] <author> Subhachandra Chandra and Peter M. Chen. </author> <title> Recovery in the Presence of Fail-Stop Violations. </title> <note> Submitted to the 1999 Symposium on Fault-Tolerant Computing (FTCS). </note> <month> June </month> <year> 1999. </year>
Reference-contexts: For example, taking a checkpoint before sending each message guarantees globally consistent recovery without sending extra messages [Koo87]. With fast checkpoints, these schemes provide low-overhead recovery for general-purpose, distributed applications. We are also considering ways to recover from bugs that violate the fail-stop model <ref> [Schneider84, Chandra98, Chandra99] </ref>. Programs with such a bug may run for a long time after the bug is activated. We know of no current recovery system that can recover from such a bug, because the corruption caused by the bug may be preserved in the recovery data (checkpoint or log).
Reference: [Chandy72] <author> K. M. Chandy and C. V. Ramamoorthy. </author> <title> Rollback and Recovery Strategies for Computer Programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-21(6):546556, </volume> <month> June </month> <year> 1972. </year>
Reference-contexts: There are two main techniques for reconstructing the state of a failed process: checkpointing and log-and-replay [Elnozahy96]. 2.1. Checkpointing Checkpointing has been used for many years <ref> [Chandy72, Koo87] </ref> and in many systems [Li90, Plank95, Tannenbaum95, Wang95]. The primary limitation of current checkpointing systems is the overhead they impose per checkpoint. For example, [Plank95] measures the overhead of a basic checkpointing system to be 20-159 seconds per checkpoint on a variety of scientific applications.
Reference: [Chen96] <author> Peter M. Chen, Wee Teck Ng, Subhachan-dra Chandra, Christopher M. Aycock, Gu-rushankar Rajamani, and David Lowell. </author> <title> The Rio File Cache: Surviving Operating System Crashes. </title> <booktitle> In Proceedings of the 1996 International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: Reliable main memory is a form of fast, stable storage that can be mapped directly into a pro-cesss address space. In our project, we use the reliable main memory provided by the Rio file cache <ref> [Chen96] </ref> and the fast transactions provided by the Vista transaction library [Lowell97]. Like most file caches, Rio caches recently used file data in main memory to speed up future accesses. Rio seeks to protect this area of memory from its two common modes of failure: power loss and system crashes.
Reference: [Elnozahy92] <author> E. N. Elnozahy, David B. Johnson, and Wil-ly Zwaenepoel. </author> <title> The Performance of Consistent Checkpointing. </title> <booktitle> In Proceedings of the 1992 Symposium on Reliable Distributed Systems, </booktitle> <pages> pages 3947, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: In contrast, we would like to make checkpointing a general tool for recovering general-purpose applications. In particular, interactive applications require frequent checkpoints (at each user-visible event) to mask failures from users. Researchers have developed many optimizations to lower the overhead of checkpointing. Incremental check-pointing <ref> [Elnozahy92] </ref> only saves data that was modified in the last checkpoint interval, using the page protection hardware to identify the modified data. Incremental checkpoint-ing often, but not always, improves performance. For example, [Plank95] measures the overhead of incremental checkpointing to be 4-53 seconds per checkpoint.
Reference: [Elnozahy93] <author> E. N. Elnozahy. Manetho: </author> <title> Fault Tolerance in Distributed Systems Using Rollback-Recovery and Process Replication. </title> <type> Technical Report TR93-212, </type> <institution> Rice University, </institution> <month> October </month> <year> 1993. </year> <type> Ph.D. thesis. </type>
Reference-contexts: Peter Chen was also supported by an NSF CAREER Award (MIP-9624869). 2 which the system outputs a message to the user will never be rolled back. This checkpoint is a form of out put commit <ref> [Strom85, Elnozahy93] </ref>. Programmer transparency: Since checkpointing is a general recovery technique, it offers the potential for easily making many applications recoverable. A programmer should be able to use the checkpointing system to make a program recoverable with minimal programming effort. <p> Thread scheduling events must be logged and replayed in the same order to ensure repeatability during the recovery of multi-threaded applications. Multi-threaded applications may also need to log shared-memory accesses between cooperating threads. The results of many system calls must also be logged and replayed <ref> [Elnozahy93, Russinovich93] </ref>. For example, the application could execute code based on the time of day returned by a system call. During recovery, the system call must return the same time of day to enable to process to execute the same code. <p> During recovery, the system call must return the same time of day to enable to process to execute the same code. With sufficient effort, many of these inputs can be logged and replayed <ref> [Elnozahy93] </ref>. As evident from the above list, however, it is no simple matter to track down, log, and replay repeatably all inputs that affect the roll-forward phase. For example, [Slye96] required a custom thread library and object-code instrumentation to successfully track thread scheduling events and signals.
Reference: [Elnozahy94] <author> E. N. Elnozahy and W. Zwaenepoel. </author> <title> On the Use and Implementation of Message Logging. </title> <booktitle> In Proceedings of the 1994 International Symposium on Fault-Tolerant Computing (FTCS), </booktitle> <pages> pages 298307, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: In other words, check-pointing works for non-deterministic processes, whereas log-and-replay must turn non-deterministic processes into deterministic ones. The main motive for using log-and-replay instead of checkpointing is its speed for output commit. Logging inputs is faster than current checkpointing systems, unless the checkpoint interval is very long <ref> [Elnozahy94] </ref>. In summary, prior work has provided two general-purpose recovery techniques: checkpointing and log-and-replay. Prior checkpointing systems add many seconds of overhead per checkpoint, which prevents them from providing failure transparency for general applications.
Reference: [Elnozahy96] <author> E. N. Elnozahy, D. B. Johnson, and Y. M. Wang. </author> <title> A Survey of Rollback-Recovery Protocols in Message-Passing Systems. </title> <type> Technical Report CMU TR 96-181, </type> <institution> Carnegie Mellon University, </institution> <year> 1996. </year>
Reference-contexts: The process may be restarted on the same machine (perhaps after a reboot) or on a different machine (as is done with process pairs) [Bartlett81, Gray86]. There are two main techniques for reconstructing the state of a failed process: checkpointing and log-and-replay <ref> [Elnozahy96] </ref>. 2.1. Checkpointing Checkpointing has been used for many years [Chandy72, Koo87] and in many systems [Li90, Plank95, Tannenbaum95, Wang95]. The primary limitation of current checkpointing systems is the overhead they impose per checkpoint.
Reference: [Fujimoto90] <author> R. M. Fujimoto. </author> <title> Parallel Discrete Event Simulation. </title> <journal> Communications of the ACM, </journal> <volume> 33(10):3053, </volume> <month> October </month> <year> 1990. </year>
Reference-contexts: Distributed simulation systems may allow a process to speculatively execute a path rather than wait to synchronize with other processes. If the speculation is incorrect, the system can use checkpointing to recover the process to the point in time before the incorrect speculation <ref> [Fujimoto90] </ref>. Checkpointing systems strive to be transparent in terms of overhead during failure-free operation, handling of failures, and modifications of the program needed to support recovery.
Reference: [Gray78] <author> J. N. Gray. </author> <title> Operating Systems: An Advanced Course. </title> <publisher> Springer-Verlag, </publisher> <year> 1978. </year> <booktitle> Notes on Database Operating Systems. </booktitle>
Reference-contexts: Future Work We are exploring more fully how fast checkpoints affect distributed recovery. For example, fast checkpoints remove the main bottleneck (writing to stable storage) in algorithms used in distributed transactions and coordinated checkpointing, such as two-phase commit <ref> [Gray78] </ref>. As mentioned in section 6, fast checkpoints also make practical new algorithms in distributed recovery. For example, taking a checkpoint before sending each message guarantees globally consistent recovery without sending extra messages [Koo87]. With fast checkpoints, these schemes provide low-overhead recovery for general-purpose, distributed applications.
Reference: [Gray86] <author> Jim Gray. </author> <title> Why do computers stop and what can be done about it? In Proceedings of the 1986 Symposium on Reliability in Distributed Software and Database Systems, </title> <booktitle> pages 312, </booktitle> <month> January </month> <year> 1986. </year>
Reference-contexts: Related Work Recovering a failed process means reconstructing the state of the process, then restarting it. The process may be restarted on the same machine (perhaps after a reboot) or on a different machine (as is done with process pairs) <ref> [Bartlett81, Gray86] </ref>. There are two main techniques for reconstructing the state of a failed process: checkpointing and log-and-replay [Elnozahy96]. 2.1. Checkpointing Checkpointing has been used for many years [Chandy72, Koo87] and in many systems [Li90, Plank95, Tannenbaum95, Wang95]. <p> One way to recover from these bugs is to keep a number of past checkpoints and roll back more than one checkpoint until before the bug was activated [Wang93]. Most bugs in production systems are triggered by non-deterministic events (so-called Heisenbugs) <ref> [Gray86] </ref> and may not occur again after recovery. 8. Conclusions This paper has presented a checkpointing system that is built on reliable main memory and high-speed transactions. Discount Checking can be used to make general-purpose applications recoverable easily and with low overhead.
Reference: [Gray93] <author> Jim Gray and Andreas Reuter. </author> <title> Transaction Processing: Concepts and Techniques. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1993. </year>
Reference-contexts: This duplicate event is often harmless. For example, applications must already cope with duplicate messages when using todays unreliable networks. The duplicate event can be eliminated if the event is testable or can be made atomic with the checkpoint <ref> [Gray93] </ref>. Otherwise the probability of the event being duplicated may be minimized by taking another checkpoint right after the event.
Reference: [Haerder83] <author> Theo Haerder and Andreas Reuter. </author> <title> Principles of Transaction-Oriented Database Recovery. </title> <journal> ACM Computing Surveys, </journal> <volume> 15(4):287317, </volume> <month> December </month> <year> 1983. </year>
Reference-contexts: Vista uses several optimizations to lower transaction overhead. First, all data is stored or logged in Rios reliable memory, thus eliminating all disk I/O for working sets that fit in main memory. Second, Vista uses a force policy <ref> [Haerder83] </ref> to update the transactional memory eagerly, thus eliminating the redo log and its associated complexity. Third, Vista maps the transactional memory directly into the address space. This style of mapping eliminates all systems calls and all-but-one memory-to-memory copy, while not hurting reliability [Ng97].
Reference: [Huang95] <author> Yennun Huang and Yi-Min Wang. </author> <title> Why Optimistic Message Logging Has Not Been Used in Telecommunications Systems. </title> <booktitle> In Proceedings of the 1995 International Symposium on Fault-Tolerant Computing (FTCS), </booktitle> <pages> pages 459463, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: For example, [Slye96] required a custom thread library and object-code instrumentation to successfully track thread scheduling events and signals. The complexity of dealing with these and other sources of non-determinism has prevented the widespread use of log-and-replay in recovering general applications <ref> [Birman96, Huang95] </ref>. 2.3. Comparison of Recovery Techniques Checkpointing is a more general recovery technique than log-and-replay, because checkpointing saves the crashed state and so obviates the need for reconstructing state using repeatable re-execution. In other words, check-pointing works for non-deterministic processes, whereas log-and-replay must turn non-deterministic processes into deterministic ones.
Reference: [Johnson87] <author> David B. Johnson and Willy Zwaenepoel. </author> <title> Sender-Based Message Logging. </title> <booktitle> In Proceedings of the 1987 International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 1419, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: In particular, all events that may cause non-deterministic execution must be logged and replayed carefully to ensure repeatability. The following are examples of these events: Message-logging systems focus on logging and replaying messages in the original order <ref> [Strom85, Koo87, Johnson87, Borg89, Lomet98] </ref>. Input from the user can be considered a form of messages. Signals and other asynchronous events are difficult to log and replay, because the effect of these events may depend on the exact processor cycle the process received the signal [Slye96].
Reference: [Koo87] <author> R. Koo and S. Toueg. </author> <title> Checkpointing and Rollback-Recovery for Distributed Systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-13(1):2331, </volume> <month> January </month> <year> 1987. </year>
Reference-contexts: There are two main techniques for reconstructing the state of a failed process: checkpointing and log-and-replay [Elnozahy96]. 2.1. Checkpointing Checkpointing has been used for many years <ref> [Chandy72, Koo87] </ref> and in many systems [Li90, Plank95, Tannenbaum95, Wang95]. The primary limitation of current checkpointing systems is the overhead they impose per checkpoint. For example, [Plank95] measures the overhead of a basic checkpointing system to be 20-159 seconds per checkpoint on a variety of scientific applications. <p> In particular, all events that may cause non-deterministic execution must be logged and replayed carefully to ensure repeatability. The following are examples of these events: Message-logging systems focus on logging and replaying messages in the original order <ref> [Strom85, Koo87, Johnson87, Borg89, Lomet98] </ref>. Input from the user can be considered a form of messages. Signals and other asynchronous events are difficult to log and replay, because the effect of these events may depend on the exact processor cycle the process received the signal [Slye96]. <p> Sending a message to a non-abortable entity (such as a display server) must be considered a non-abortable event and hence must induce a checkpoint. Taking a checkpoint for each message send guarantees consistent, distributed recovery <ref> [Lamport78, Koo87] </ref>. If the receivers state can be rolled back, we can consider message sends abortable events. This requires an atomic commitment protocol (such as two-phase commit) between the sender and receiver. 3.5. Minimizing Memory Copies As discussed above, Vista logs the before-image of memory pages into Rios reliable memory. <p> As mentioned in section 6, fast checkpoints also make practical new algorithms in distributed recovery. For example, taking a checkpoint before sending each message guarantees globally consistent recovery without sending extra messages <ref> [Koo87] </ref>. With fast checkpoints, these schemes provide low-overhead recovery for general-purpose, distributed applications. We are also considering ways to recover from bugs that violate the fail-stop model [Schneider84, Chandra98, Chandra99]. Programs with such a bug may run for a long time after the bug is activated.
Reference: [Lamport78] <author> Leslie Lamport. </author> <title> Time, Clocks, and the Ordering of Events in a Distributed System. </title> <journal> Communications of the ACM, </journal> <volume> 21(7):558 12 565, </volume> <month> July </month> <year> 1978. </year>
Reference-contexts: Sending a message to a non-abortable entity (such as a display server) must be considered a non-abortable event and hence must induce a checkpoint. Taking a checkpoint for each message send guarantees consistent, distributed recovery <ref> [Lamport78, Koo87] </ref>. If the receivers state can be rolled back, we can consider message sends abortable events. This requires an atomic commitment protocol (such as two-phase commit) between the sender and receiver. 3.5. Minimizing Memory Copies As discussed above, Vista logs the before-image of memory pages into Rios reliable memory.
Reference: [Li90] <author> C-C. J. Li and W. K. Fuchs. </author> <title> CATCHCom-piler-Assisted Techniques for Checkpoint-ing. </title> <booktitle> In Proceedings of the 1990 International Symposium on Fault-Tolerant Computing (FTCS), </booktitle> <pages> pages 7481, </pages> <year> 1990. </year>
Reference-contexts: There are two main techniques for reconstructing the state of a failed process: checkpointing and log-and-replay [Elnozahy96]. 2.1. Checkpointing Checkpointing has been used for many years [Chandy72, Koo87] and in many systems <ref> [Li90, Plank95, Tannenbaum95, Wang95] </ref>. The primary limitation of current checkpointing systems is the overhead they impose per checkpoint. For example, [Plank95] measures the overhead of a basic checkpointing system to be 20-159 seconds per checkpoint on a variety of scientific applications.
Reference: [Li94] <author> Kai Li, J. F. Naughton, and James S. Plank. </author> <title> Low-Latency, Concurrent Checkpointing for Parallel Programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 5(8):874879, </volume> <month> August </month> <year> 1994. </year>
Reference-contexts: Incremental checkpoint-ing often, but not always, improves performance. For example, [Plank95] measures the overhead of incremental checkpointing to be 4-53 seconds per checkpoint. Asynchronous checkpointing (sometimes called forked checkpointing) writes the checkpoint to stable storage while simultaneously continuing to execute the program <ref> [Li94] </ref>. In contrast, synchronous checkpointing (sometimes called sequential checkpointing) waits until the write to stable storage is complete before continuing executing the process. Asynchronous checkpointing can lower total overhead by allowing the process to execute in parallel with the act of taking the checkpoint.
Reference: [Litzkow92] <author> Michael Litzkow and Marvin Solomon. </author> <title> Supporting Checkpointing and Process Migration outside the Unix Kernel. </title> <booktitle> In Proceedings of the Winter 1992 USENIX Conference, </booktitle> <month> January </month> <year> 1992. </year>
Reference-contexts: Checkpointing with rollback recovery is used most often as a fault-recovery technique, but it can be used in other areas as well. Process migration can use checkpointing to move a running process to a new computer <ref> [Litzkow92] </ref>. Debuggers can use checkpoints to examine the state of a process before a crash. Distributed simulation systems may allow a process to speculatively execute a path rather than wait to synchronize with other processes.
Reference: [Lomet98] <author> David Lomet and Gerhard Weikum. </author> <title> Efficient Transparent Application Recovery in Client-Server Information Systems. </title> <booktitle> In Proceedings of the 1998 ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 460471, </pages> <month> June </month> <year> 1998. </year>
Reference-contexts: In particular, all events that may cause non-deterministic execution must be logged and replayed carefully to ensure repeatability. The following are examples of these events: Message-logging systems focus on logging and replaying messages in the original order <ref> [Strom85, Koo87, Johnson87, Borg89, Lomet98] </ref>. Input from the user can be considered a form of messages. Signals and other asynchronous events are difficult to log and replay, because the effect of these events may depend on the exact processor cycle the process received the signal [Slye96].
Reference: [Lowell97] <author> David E. Lowell and Peter M. Chen. </author> <title> Free Transactions with Rio Vista. </title> <booktitle> In Proceedings of the 1997 Symposium on Operating Systems Principles, </booktitle> <month> October </month> <year> 1997. </year>
Reference-contexts: Reliable main memory is a form of fast, stable storage that can be mapped directly into a pro-cesss address space. In our project, we use the reliable main memory provided by the Rio file cache [Chen96] and the fast transactions provided by the Vista transaction library <ref> [Lowell97] </ref>. Like most file caches, Rio caches recently used file data in main memory to speed up future accesses. Rio seeks to protect this area of memory from its two common modes of failure: power loss and system crashes. <p> The version of Rio used in this paper is a modification of FreeBSD 2.2.7. FreeBSD-Rio runs on standard PCs without modifications to the hardware, firmware, or processor configuration. Vista builds on the persistent memory provided by Rio to provide fast transactions <ref> [Lowell97] </ref>. Applications use Vista to allocate areas of persistent memory and perform atomic, durable transactions on that memory. Vista uses several optimizations to lower transaction overhead. First, all data is stored or logged in Rios reliable memory, thus eliminating all disk I/O for working sets that fit in main memory. <p> This occurs because of how Vista handles memory allocation within a transaction <ref> [Lowell97] </ref>. Vista defers free operations until the end of the transaction unless the corresponding malloc was performed in the current transaction. Hence, longer intervals allow Vista to re-use memory regions for new allocations. 6. Contributions This paper makes a number of contributions to recovery research.
Reference: [Ng97] <author> Wee Teck Ng and Peter M. Chen. </author> <title> Integrating Reliable Memory in Databases. </title> <booktitle> In Proceedings of the 1997 International Conference on Very Large Data Bases (VLDB), </booktitle> <pages> pages 7685, </pages> <month> August </month> <year> 1997. </year>
Reference-contexts: Second, Vista uses a force policy [Haerder83] to update the transactional memory eagerly, thus eliminating the redo log and its associated complexity. Third, Vista maps the transactional memory directly into the address space. This style of mapping eliminates all systems calls and all-but-one memory-to-memory copy, while not hurting reliability <ref> [Ng97] </ref>. Fourth, Vistas simplicity and small code size (700 4 lines of C) lead to very short code paths. As a result of these optimizations, Vistas transactions are extremely fast: small transactions can complete in under 2 ms. 3.2.
Reference: [Plank95] <author> James S. Plank, Micah Beck, and Gerry Kingsley. Libckpt: </author> <title> Transparent Check-pointing under Unix. </title> <booktitle> In Proceedings of the Winter 1995 USENIX Conference, </booktitle> <month> January </month> <year> 1995. </year>
Reference-contexts: There are two main techniques for reconstructing the state of a failed process: checkpointing and log-and-replay [Elnozahy96]. 2.1. Checkpointing Checkpointing has been used for many years [Chandy72, Koo87] and in many systems <ref> [Li90, Plank95, Tannenbaum95, Wang95] </ref>. The primary limitation of current checkpointing systems is the overhead they impose per checkpoint. For example, [Plank95] measures the overhead of a basic checkpointing system to be 20-159 seconds per checkpoint on a variety of scientific applications. <p> Checkpointing Checkpointing has been used for many years [Chandy72, Koo87] and in many systems [Li90, Plank95, Tannenbaum95, Wang95]. The primary limitation of current checkpointing systems is the overhead they impose per checkpoint. For example, <ref> [Plank95] </ref> measures the overhead of a basic checkpointing system to be 20-159 seconds per checkpoint on a variety of scientific applications. To amortize this high overhead, todays systems take checkpoints infrequently. For example, the default interval between checkpoints for libckp is 30 minutes [Wang95]. <p> Researchers have developed many optimizations to lower the overhead of checkpointing. Incremental check-pointing [Elnozahy92] only saves data that was modified in the last checkpoint interval, using the page protection hardware to identify the modified data. Incremental checkpoint-ing often, but not always, improves performance. For example, <ref> [Plank95] </ref> measures the overhead of incremental checkpointing to be 4-53 seconds per checkpoint. Asynchronous checkpointing (sometimes called forked checkpointing) writes the checkpoint to stable storage while simultaneously continuing to execute the program [Li94]. <p> This may be acceptable for programs that do not communicate frequently with external entities, but it hinders the use of checkpointing for general applications. Memory exclusion is another technique used to lower the overhead of checkpointing <ref> [Plank95] </ref>. In this technique, the programmer explicitly specifies ranges of data that do not need to be saved. Memory exclusion can reduce overhead dramatically for applications that touch a large amount of data that is not needed in recovery or is soon deallocated.
Reference: [Russinovich93] <author> Mark Russinovich, Zary Segall, and Daniel P. Siewiorek. </author> <title> Application Transparent Fault Management in Fault Tolerant Mach. </title> <booktitle> In Proceedings of the 1993 International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 1019, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Thread scheduling events must be logged and replayed in the same order to ensure repeatability during the recovery of multi-threaded applications. Multi-threaded applications may also need to log shared-memory accesses between cooperating threads. The results of many system calls must also be logged and replayed <ref> [Elnozahy93, Russinovich93] </ref>. For example, the application could execute code based on the time of day returned by a system call. During recovery, the system call must return the same time of day to enable to process to execute the same code.
Reference: [Schneider84] <author> Fred B. Schneider. </author> <title> Byzantine Generals in Action: Implementing Fail-Stop Processors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(2):145154, </volume> <month> May </month> <year> 1984. </year>
Reference-contexts: For example, taking a checkpoint before sending each message guarantees globally consistent recovery without sending extra messages [Koo87]. With fast checkpoints, these schemes provide low-overhead recovery for general-purpose, distributed applications. We are also considering ways to recover from bugs that violate the fail-stop model <ref> [Schneider84, Chandra98, Chandra99] </ref>. Programs with such a bug may run for a long time after the bug is activated. We know of no current recovery system that can recover from such a bug, because the corruption caused by the bug may be preserved in the recovery data (checkpoint or log).
Reference: [Slye96] <author> J. H. Slye and E. N. Elnozahy. </author> <title> Supporting Nondeterministic Execution in Fault-Tolerant Systems. </title> <booktitle> In Proceedings of the 1990 International Symposium on Fault-Tolerant Computing (FTCS), </booktitle> <pages> pages 250259, </pages> <month> June </month> <year> 1996. </year>
Reference-contexts: Input from the user can be considered a form of messages. Signals and other asynchronous events are difficult to log and replay, because the effect of these events may depend on the exact processor cycle the process received the signal <ref> [Slye96] </ref>. For this reason, Targon/32 chose to checkpoint before each signal rather than log [Borg89]. In general, timing dependencies are a difficult input to log and replay. Thread scheduling events must be logged and replayed in the same order to ensure repeatability during the recovery of multi-threaded applications. <p> With sufficient effort, many of these inputs can be logged and replayed [Elnozahy93]. As evident from the above list, however, it is no simple matter to track down, log, and replay repeatably all inputs that affect the roll-forward phase. For example, <ref> [Slye96] </ref> required a custom thread library and object-code instrumentation to successfully track thread scheduling events and signals. The complexity of dealing with these and other sources of non-determinism has prevented the widespread use of log-and-replay in recovering general applications [Birman96, Huang95]. 2.3.
Reference: [Strom85] <author> Robert E. Strom and Shaula Yemini. </author> <title> Optimistic Recovery in Distributed Systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 3(3):204226, </volume> <month> August </month> <year> 1985. </year>
Reference-contexts: Peter Chen was also supported by an NSF CAREER Award (MIP-9624869). 2 which the system outputs a message to the user will never be rolled back. This checkpoint is a form of out put commit <ref> [Strom85, Elnozahy93] </ref>. Programmer transparency: Since checkpointing is a general recovery technique, it offers the potential for easily making many applications recoverable. A programmer should be able to use the checkpointing system to make a program recoverable with minimal programming effort. <p> In particular, all events that may cause non-deterministic execution must be logged and replayed carefully to ensure repeatability. The following are examples of these events: Message-logging systems focus on logging and replaying messages in the original order <ref> [Strom85, Koo87, Johnson87, Borg89, Lomet98] </ref>. Input from the user can be considered a form of messages. Signals and other asynchronous events are difficult to log and replay, because the effect of these events may depend on the exact processor cycle the process received the signal [Slye96].
Reference: [Tannenbaum95] <author> T. Tannenbaum and M. Litzkow. </author> <title> The Condor Distributed Processing System. </title> <journal> Dr. Dobbs Journal, </journal> <pages> pages 4048, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: There are two main techniques for reconstructing the state of a failed process: checkpointing and log-and-replay [Elnozahy96]. 2.1. Checkpointing Checkpointing has been used for many years [Chandy72, Koo87] and in many systems <ref> [Li90, Plank95, Tannenbaum95, Wang95] </ref>. The primary limitation of current checkpointing systems is the overhead they impose per checkpoint. For example, [Plank95] measures the overhead of a basic checkpointing system to be 20-159 seconds per checkpoint on a variety of scientific applications.
Reference: [Wang93] <author> Yi-Min Wang, Yennun Huang, and W. Kent Fuchs. </author> <title> Progressive Retry for Software Error Recovery in Distributed Systems. </title> <booktitle> In Proceedings of the 1993 International Symposium on Fault-Tolerant Computing (FTCS), </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: One way to recover from these bugs is to keep a number of past checkpoints and roll back more than one checkpoint until before the bug was activated <ref> [Wang93] </ref>. Most bugs in production systems are triggered by non-deterministic events (so-called Heisenbugs) [Gray86] and may not occur again after recovery. 8. Conclusions This paper has presented a checkpointing system that is built on reliable main memory and high-speed transactions.
Reference: [Wang95] <author> Yi-Min Wang, Yennun Huang, Kiem-Phong Vo, Pi-Yu Chung, and Chandra Kin-tala. </author> <title> Checkpointing and Its Applications. </title> <booktitle> In Proceedings of the 1995 International Symposium on Fault-Tolerant Computing (FTCS), </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: There are two main techniques for reconstructing the state of a failed process: checkpointing and log-and-replay [Elnozahy96]. 2.1. Checkpointing Checkpointing has been used for many years [Chandy72, Koo87] and in many systems <ref> [Li90, Plank95, Tannenbaum95, Wang95] </ref>. The primary limitation of current checkpointing systems is the overhead they impose per checkpoint. For example, [Plank95] measures the overhead of a basic checkpointing system to be 20-159 seconds per checkpoint on a variety of scientific applications. <p> For example, [Plank95] measures the overhead of a basic checkpointing system to be 20-159 seconds per checkpoint on a variety of scientific applications. To amortize this high overhead, todays systems take checkpoints infrequently. For example, the default interval between checkpoints for libckp is 30 minutes <ref> [Wang95] </ref>. The high overhead per checkpoint and long interval between checkpoints limit the use of checkpointing to long-running programs with a minimal need for failure transparency, such as scientific computations. In contrast, we would like to make checkpointing a general tool for recovering general-purpose applications.
References-found: 35

