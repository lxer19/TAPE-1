URL: http://www.cs.wustl.edu/~sg/ubq.ps.Z
Refering-URL: http://www.cs.wustl.edu/~sg/
Root-URL: 
Email: avrim@theory.cs.cmu.edu  chal@lanl.gov  sg@cs.wustl.edu  slonim@theory.lcs.mit.edu  
Title: Learning With Unreliable Boundary Queries  
Author: Avrim Blum Prasad Chalasani Sally A. Goldman Donna K. Slonim 
Address: Pittsburgh, PA 15213  Los Alamos, NM 87544  St. Louis, MO 63130  545 Technology Square Cambridge, MA 02139  
Affiliation: School of Computer Science Carnegie Mellon University  Los Alamos National Lab.  Dept. of Computer Science Washington University  MIT Lab. for CS  
Abstract: We introduce a new model for learning with membership queries in which queries near the boundary of a target concept may receive incorrect or don't care responses. In partial compensation, we assume the distribution of examples has zero probability mass on the boundary region. The motivation behind this model is that the reason for the incorrect (or don't care) response is that these examples are extremely rare in practice. Thus, it does not matter how the learner classifies them. We present several positive results in this new model. We show how to learn the intersection of two halfspaces when membership queries near the boundary may be answered incorrectly. Our algorithm is an extension of an algorithm of Baum [6, 5] which learns intersections of two homogeneous halfspaces in the PAC-with-membership-queries model. We also describe algorithms for learning several subclasses of monotone DNF formulas.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Angluin. </author> <title> Exact learning of -DNF formulas with malicious membership queries. </title> <type> Technical Report YALEU/DCS/TR-1020, </type> <institution> Yale University Department of Computer Science, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: Angluin and Krikis [3] introduced a similar model of malicious membership queries in which the adversary may respond with incorrect answers instead of don't know. Their paper proved that the class of monotone DNF formulas is learnable in this model. An-gluin <ref> [1] </ref> has shown that read-once DNF formulas are also learnable with malicious membership queries.
Reference: [2] <author> D. Angluin. </author> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 319-342, </pages> <month> April </month> <year> 1988. </year>
Reference-contexts: queries) several subclasses of monotone DNF formulas when there are one-sided false positive errors in the boundary queries for a small boundary size. 2 Definitions We assume the reader is familiar with Valiant's probably approximately correct (PAC) learning model [22] and An-gluin's model of learning with membership and equivalence queries <ref> [2] </ref>. We use PAC-memb to refer to the variation of the PAC model in which the learner can make membership queries. Likewise we say that a concept class is exactly learnable if it is learnable with membership and equivalence queries. <p> Finally, we extend these definitions to the exact learning model by requiring that counterexamples to equivalence queries not be chosen from the boundary region. 3 Related Work There has been much theoretical work on PAC or mistake-bound learning in cases where the training examples may be mislabelled <ref> [2, 15, 20, 13] </ref> and additional work in models which allow attribute noise [19, 10, 17].
Reference: [3] <author> D. Angluin and M. Krikis. </author> <title> Learning with malicious membership queries and exceptions. </title> <booktitle> In Proc. 7th Annu. ACM Workshop on Comput. Learning Theory, </booktitle> <pages> pages 57-66. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: Sloan and Turan presented algorithms in this model for learning the class of monotone k-term DNF formulas with membership queries alone and the class of monotone DNF formulas with membership and equivalence queries. Angluin and Krikis <ref> [3] </ref> introduced a similar model of malicious membership queries in which the adversary may respond with incorrect answers instead of don't know. Their paper proved that the class of monotone DNF formulas is learnable in this model. <p> Our model is more difficult than those above in that the membership query errors or omissions are chosen by an adversary (unlike [4]) and algorithms must run in time that is polynomial in the usual parameters regardless of the number of queries that might receive incorrect answers (unlike <ref> [21, 3] </ref>). For example, in the case of a 1-term monotone DNF formula with the boundary radius r = 1, there may be exponentially many (in n) in-stances in the boundary region. (Example: let x 4 x 7 x 9 be the target term.
Reference: [4] <author> D. Angluin and D. K. </author> <title> Slonim. Randomly fallible teachers: Learning monotone DNF with an incomplete membership oracle. </title> <journal> Machine Learning, </journal> <volume> 14(1) </volume> <pages> 7-26, </pages> <month> January </month> <year> 1994. </year> <note> A preliminary version of this paper appeared in COLT '91. </note>
Reference-contexts: Their work uses membership queries to simulate a particular distribution. Frazier and Pitt [9] showed that CLASSIC sentences are learnable in this noise model, using the fact that many distinct membership queries can be formulated that redundantly give the same information. Angluin and Slonim <ref> [4] </ref> introduced a model of incomplete membership queries, in which a membership query on a given instance may persistently generate a don't know response. The don't know instances are chosen uniformly at random from the entire domain and may account for up to a constant fraction of the instances. <p> Our model is more difficult than those above in that the membership query errors or omissions are chosen by an adversary (unlike <ref> [4] </ref>) and algorithms must run in time that is polynomial in the usual parameters regardless of the number of queries that might receive incorrect answers (unlike [21, 3]).
Reference: [5] <author> E. Baum. </author> <title> Neural net algorithms that learn in polynomial time from examples and queries. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 2 </volume> <pages> 5-19, </pages> <year> 1991. </year>
Reference-contexts: Our algorithm is an extension of an algorithm of Baum <ref> [6, 5] </ref> for learning the simpler class of intersections of two homogeneous halfspaces in the standard PAC-with-queries model 2 .
Reference: [6] <author> E. B. Baum. </author> <title> Polynomial time algorithms for learning neural nets. </title> <booktitle> In Proc. 3rd Annu. Workshop on Com-put. Learning Theory, </booktitle> <pages> pages 258-272, </pages> <address> San Mateo, CA, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Our algorithm is an extension of an algorithm of Baum <ref> [6, 5] </ref> for learning the simpler class of intersections of two homogeneous halfspaces in the standard PAC-with-queries model 2 .
Reference: [7] <author> A. Blum and R. L. Rivest. </author> <title> Training a 3-node neural net is NP-Complete. </title> <booktitle> In Advances in Neural Information Processing Systems I, </booktitle> <pages> pages 494-501. </pages> <publisher> Morgan Kauf-mann, </publisher> <year> 1989. </year>
Reference-contexts: The idea of Baum's algorithm is to reduce the problem of learning an intersection of two homogeneous halfspaces to the problem of learning an XOR of halfspaces, for which a PAC algorithm exists <ref> [7] </ref>. (That algorithm produces a hypothesis that is the threshold of a degree-2 polynomial.) The idea of the reduction is to notice that negative examples in the quadrant opposite from the positive quadrantthe troublesome examples keeping the data set from being consistent with an XOR of halfspacesare exactly those examples ~x <p> Then find a linear function P such that P (~x) &lt; 0 for all the marked (negative) examples and P (~x) 0 for all the positives. Finally, run the XOR-of-halfspaces learning algorithm of <ref> [7] </ref> to find a hypothesis H 0 that correctly classifies f~x 2 S : P (~x) 0g. The final hypothesis is: If P (~x) &lt; 0 then predict negative, else predict H 0 (~x).
Reference: [8] <author> M. Frazier, S. Goldman, N. Mishra, and L. Pitt. </author> <title> Learning from a consistently ignorant teacher. </title> <booktitle> In Proc. 7th Annu. ACM Workshop on Comput. Learning Theory, </booktitle> <pages> pages 328-339. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1994. </year> <note> To appear, J. </note> <institution> of Comput. Syst. Sci. </institution>
Reference-contexts: In other related work, Frazier, Goldman, Mishra and Pitt <ref> [8] </ref> introduced a learning model in which there is incomplete information about the target function due to an ill-defined boundary. While the omissions in their model may be ad-versarially placed, all examples labeled with ? (indicating unknown classification) must be consistent with knowledge about the concept class.
Reference: [9] <author> M. Frazier and L. Pitt. </author> <title> CLASSIC learning. </title> <booktitle> In Proc. 7th Annu. ACM Workshop on Comput. Learning Theory, </booktitle> <pages> pages 23-34. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: Goldman, Kearns and Schapire [11] gave a positive result for learning certain classes of read-once formulas under this noise model. Their work uses membership queries to simulate a particular distribution. Frazier and Pitt <ref> [9] </ref> showed that CLASSIC sentences are learnable in this noise model, using the fact that many distinct membership queries can be formulated that redundantly give the same information.
Reference: [10] <author> S. Goldman and R. Sloan. </author> <title> Can PAC learning algorithms tolerate random noise? Technical Report WUCS-92-25, </title> <institution> Washington University Department of Computer Science, </institution> <month> July </month> <year> 1992. </year> <note> To appear, Algorithmica. </note>
Reference-contexts: by requiring that counterexamples to equivalence queries not be chosen from the boundary region. 3 Related Work There has been much theoretical work on PAC or mistake-bound learning in cases where the training examples may be mislabelled [2, 15, 20, 13] and additional work in models which allow attribute noise <ref> [19, 10, 17] </ref>. The p-concepts model of Kearns and Schapire [14] falls somewhat into this category and is related to our work since their model allows a graded boundary between the positive and negative portions of the instance space.
Reference: [11] <author> S. A. Goldman, M. J. Kearns, and R. E. Schapire. </author> <title> Exact identification of circuits using fixed points of amplification functions. </title> <journal> SIAM J. Comput., </journal> <volume> 22(4) </volume> <pages> 705-726, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: A more realistic model is that of persistent membership query noise in which repeated queries to the same example receive the same answer as in the first call. Goldman, Kearns and Schapire <ref> [11] </ref> gave a positive result for learning certain classes of read-once formulas under this noise model. Their work uses membership queries to simulate a particular distribution.
Reference: [12] <author> S. A. Goldman and H. D. Mathias. </author> <title> Learning k-term DNF formulas with an incomplete membership oracle. </title> <booktitle> In Proc. 5th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 77-84. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1992. </year>
Reference-contexts: The don't know instances are chosen uniformly at random from the entire domain and may account for up to a constant fraction of the instances. Additional positive results in this model were obtained by Goldman and Mathias <ref> [12] </ref>. This model allows for a large number of don't know instances, but positive results in this model are typically highly dependent on the precisely uniform nature of the noise. Sloan and Turan [21] introduced the limited membership query model.
Reference: [13] <author> M. Kearns and M. Li. </author> <title> Learning in the presence of malicious errors. </title> <journal> SIAM J. Comput., </journal> <volume> 22 </volume> <pages> 807-837, </pages> <year> 1993. </year>
Reference-contexts: Finally, we extend these definitions to the exact learning model by requiring that counterexamples to equivalence queries not be chosen from the boundary region. 3 Related Work There has been much theoretical work on PAC or mistake-bound learning in cases where the training examples may be mislabelled <ref> [2, 15, 20, 13] </ref> and additional work in models which allow attribute noise [19, 10, 17].
Reference: [14] <author> M. J. Kearns and R. E. Schapire. </author> <title> Efficient distribution-free learning of probabilistic concepts. </title> <booktitle> In Proc. of the 31st Symposium on the Foundations of Comp. Sci., </booktitle> <pages> pages 382-391. </pages> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1990. </year>
Reference-contexts: The p-concepts model of Kearns and Schapire <ref> [14] </ref> falls somewhat into this category and is related to our work since their model allows a graded boundary between the positive and negative portions of the instance space. There have also been a number of results on learning with randomly generated noisy responses to membership queries.
Reference: [15] <author> Philip D. Laird. </author> <title> Learning from Good and Bad Data. </title> <booktitle> Kluwer international series in engineering and computer science. </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1988. </year>
Reference-contexts: Finally, we extend these definitions to the exact learning model by requiring that counterexamples to equivalence queries not be chosen from the boundary region. 3 Related Work There has been much theoretical work on PAC or mistake-bound learning in cases where the training examples may be mislabelled <ref> [2, 15, 20, 13] </ref> and additional work in models which allow attribute noise [19, 10, 17].
Reference: [16] <author> K.J. Lang and E.B. Baum. </author> <title> Query learning can work poorly when a human oracle is used. </title> <booktitle> In Proceedings of International Joint Conference on Neural Networks, IEEE, </booktitle> <year> 1992. </year>
Reference-contexts: A problem with this type of approach 1 , as noticed by Lang and Baum <ref> [16] </ref> is that questions of this sort that are near the concept boundary may result in unreliable answers.
Reference: [17] <author> N. Littlestone. </author> <title> Redundant noisy attributes, attribute errors, and linear threshold learning using Winnow. </title> <booktitle> In Proc. 4th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 147-156, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: by requiring that counterexamples to equivalence queries not be chosen from the boundary region. 3 Related Work There has been much theoretical work on PAC or mistake-bound learning in cases where the training examples may be mislabelled [2, 15, 20, 13] and additional work in models which allow attribute noise <ref> [19, 10, 17] </ref>. The p-concepts model of Kearns and Schapire [14] falls somewhat into this category and is related to our work since their model allows a graded boundary between the positive and negative portions of the instance space.
Reference: [18] <author> Y. Sakakibara. </author> <title> On learning from queries and counterexamples in the presence of noise. </title> <journal> Inform. Proc. Lett., </journal> <volume> 37(5) </volume> <pages> 279-284, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: There have also been a number of results on learning with randomly generated noisy responses to membership queries. Sakakibara <ref> [18] </ref> considered the case where each membership query is incorrectly answered with a fixed probability, so that one could increase reliability by asking the same membership query a sufficient number of times and taking a majority vote.
Reference: [19] <author> G. Shackelford and D. Volper. </author> <title> Learning k-DNF with noise in the attributes. </title> <booktitle> In Proc. 1st Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 97-103, </pages> <address> San Mateo, CA, 1988. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: by requiring that counterexamples to equivalence queries not be chosen from the boundary region. 3 Related Work There has been much theoretical work on PAC or mistake-bound learning in cases where the training examples may be mislabelled [2, 15, 20, 13] and additional work in models which allow attribute noise <ref> [19, 10, 17] </ref>. The p-concepts model of Kearns and Schapire [14] falls somewhat into this category and is related to our work since their model allows a graded boundary between the positive and negative portions of the instance space.
Reference: [20] <author> R. Sloan. </author> <title> Types of noise in data for concept learning. </title> <booktitle> In Proc. 1st Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 91-96, </pages> <address> San Mateo, CA, 1988. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Finally, we extend these definitions to the exact learning model by requiring that counterexamples to equivalence queries not be chosen from the boundary region. 3 Related Work There has been much theoretical work on PAC or mistake-bound learning in cases where the training examples may be mislabelled <ref> [2, 15, 20, 13] </ref> and additional work in models which allow attribute noise [19, 10, 17].
Reference: [21] <author> R. H. Sloan and G. </author> <title> Tur an. Learning with queries but incomplete information. </title> <booktitle> In Proc. 7th Annu. ACM Workshop on Comput. Learning Theory, </booktitle> <pages> pages 237-245. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: Additional positive results in this model were obtained by Goldman and Mathias [12]. This model allows for a large number of don't know instances, but positive results in this model are typically highly dependent on the precisely uniform nature of the noise. Sloan and Turan <ref> [21] </ref> introduced the limited membership query model. In this model, an adversary may arbitrarily select some number ` of examples on which it refuses to answer membership queries (or answers don't know), but the number of queries the learner asks may be polynomial in `. <p> Our model is more difficult than those above in that the membership query errors or omissions are chosen by an adversary (unlike [4]) and algorithms must run in time that is polynomial in the usual parameters regardless of the number of queries that might receive incorrect answers (unlike <ref> [21, 3] </ref>). For example, in the case of a 1-term monotone DNF formula with the boundary radius r = 1, there may be exponentially many (in n) in-stances in the boundary region. (Example: let x 4 x 7 x 9 be the target term.
Reference: [22] <author> L. G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Commun. ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <month> November </month> <year> 1984. </year>
Reference-contexts: are noisy, and (3) efficient algorithms to exactly learn (with membership queries) several subclasses of monotone DNF formulas when there are one-sided false positive errors in the boundary queries for a small boundary size. 2 Definitions We assume the reader is familiar with Valiant's probably approximately correct (PAC) learning model <ref> [22] </ref> and An-gluin's model of learning with membership and equivalence queries [2]. We use PAC-memb to refer to the variation of the PAC model in which the learner can make membership queries.
References-found: 22

