URL: ftp://cl-ftp.dfki.uni-sb.de/pub/papers/local/revgram.ps.Z
Refering-URL: http://cl-www.dfki.uni-sb.de/cl/papers/cl-abstracts.html
Root-URL: 
Title: REVERSIBILITY AND SELF-MONITORING IN NATURAL LANGUAGE GENERATION  
Author: Gunter Neumann*, Gertjan van Noord** 
Affiliation: Deutsches Forschungszentrum fur Kunstliche Intelligenz Saarbrucken Vakgroep Alfa-informatica Rijksuniversiteit Groningen  
Abstract: This paper shows how the use of reversible grammars may lead to efficient and flexible natural language parsing and generation systems. In particular a mechanism is described which ensures that only non-ambiguous utterances are produced. This mechanism uses the parsing component to monitor the generation component. The relevant communication between the two components is performed using derivation trees. For this reason the proposed mechanism only makes sense for systems in which a single grammar is used for both parsing and generation. Furthermore we define a variant of the monitoring strategy which can be used to paraphrase a given input sentence (for interactive disambiguation). In this case, the generation component is used to guide the parsing system. Again the proposed technique is possible only in the case of a single, reversible grammar. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Douglas E. Appelt. </author> <title> Planning English Sentences. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1985. </year>
Reference-contexts: With few exceptions (e.g., <ref> [1] </ref>) the following two components are assumed: `what to say' part (strategic component) `how to say it' part (tactical component) Currently, in systems where the separation is advocated it is assumed that the strategic component is able to provide all information needed by the tactical component to make decisions about lexical
Reference: [2] <author> Douglas E. Appelt. </author> <title> Bidirectional grammars and the design of natural language generation systems. </title> <editor> In Y. Wilks, editor, </editor> <booktitle> Theoretical Issues in Natural Language Processing, </booktitle> <pages> pages 206-212. </pages> <address> Hillsdale, N.J.: </address> <publisher> Erlbaum, </publisher> <year> 1989. </year>
Reference-contexts: In such approaches, tactical components are only front-ends while the strategic component needs detailed information about the language to use. In <ref> [2] </ref> it is shown that the use of a reversible grammar affects the modular status of a NLG in such a way that only the tactical component should be concerned with the specific details of a grammar and the strategic component should only perform general reasoning and planning tasks.
Reference: [3] <author> Stefan Busemann. </author> <title> Generierung naturlicher Sprache mit Generalisierten Phrasenstruktur-Grammatiken. </title> <type> PhD thesis, </type> <institution> University of Saarland (Saarbrucken), </institution> <year> 1990. </year>
Reference-contexts: components are assumed: `what to say' part (strategic component) `how to say it' part (tactical component) Currently, in systems where the separation is advocated it is assumed that the strategic component is able to provide all information needed by the tactical component to make decisions about lexical and syntactic choices <ref> [15, 17, 3, 10] </ref>. As a consequence, this implies that the input for tactical components is tailored to determine uniquely a good sentence, making the use of powerful Reversibility and Self-Monitoring 11 grammatical processes redundant.
Reference: [4] <author> Robert Dale. </author> <title> Generating receipes: An overview of epicure. </title> <editor> In Robert Dale, Chris Mellish, and Michael Zock, editors, </editor> <booktitle> Current Research in Natural Language Generation, </booktitle> <pages> pages 229-255. </pages> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1990. </year>
Reference-contexts: 1 INTRODUCTION Recently, there is a strong tendency to use the same grammar for parsing and generation. The general motivations for reversible grammars are reviewed in section 2. Most of the generation systems use grammars that are specifically designed for generation purposes (cf. <ref> [4] </ref>, [10], [16], [20]). The purpose of this paper 1 2 Chapter 1 is to show that the use of a reversible architecture for grammatical processing has important influences on the generation task.
Reference: [5] <author> K. De Smedt and G. Kempen. </author> <title> Incremental sentence production, self-correction and coordination. </title> <editor> In G. Kempen, editor, </editor> <booktitle> Natural Language Generation, </booktitle> <pages> pages 365-376. </pages> <publisher> Martinus Nijhoff, </publisher> <address> Dordrecht, </address> <year> 1987. </year>
Reference-contexts: Monitoring In order to maintain a modular design additional mechanisms are necessary to perform some monitoring of the generator's output. The need for such mechanisms during natural language generation has already been noted by several Reversibility and Self-Monitoring 15 authors <ref> [5, 12, 14] </ref>. For example, [14] points out that "speakers monitor what they are saying and how they are saying it".
Reference: [6] <author> Wolfgang Finkler and Gunter Neumann. Popel-How: </author> <title> A distributed parallel model for incremental natural language production with feedback. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1518-1523, </pages> <address> Detroit, </address> <year> 1989. </year>
Reference-contexts: Our strategy is an example of revision. Optimisations are useful when changes have to be done during the initial generation process. For example, in <ref> [6, 20] </ref> an incremental and parallel grammatical component is described that is able to handle under-specified input such that it detects and requests missing but necessary grammatical information. Comparison and Implementations In [18] strategies for paraphrasing are described.
Reference: [7] <author> Lyn Frazier. </author> <title> Shared components of production and perception. </title> <editor> In M. A. Arbib et al., editor, </editor> <booktitle> Neural Models of Language Processes, </booktitle> <pages> pages 225-236. </pages> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1982. </year>
Reference-contexts: Clearly, this would explain why humans speak the same language they understand and vice versa. Some empirical evidence for shared processors or facilities is discussed in [8], <ref> [7] </ref> and [11]. In practice speakers often understand sentences they would never produce. This observation may have several reasons. Many differences are due to the fact that people often are able to understand otherwise mysterious utterances, because of the context and situation | using intelligence rather than grammatical knowledge.
Reference: [8] <author> Merrill F. Garrett. </author> <title> Remarks on the relation between language production and language comprehension systems. </title> <editor> In M. A. Arbib et al., editor, </editor> <booktitle> Neural Models of Language Processes, </booktitle> <pages> pages 209-224. </pages> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1982. </year>
Reference-contexts: Psychological motivation An interesting question might be whether humans base their language production and language understanding on a single body of grammatical knowledge. Clearly, this would explain why humans speak the same language they understand and vice versa. Some empirical evidence for shared processors or facilities is discussed in <ref> [8] </ref>, [7] and [11]. In practice speakers often understand sentences they would never produce. This observation may have several reasons. Many differences are due to the fact that people often are able to understand otherwise mysterious utterances, because of the context and situation | using intelligence rather than grammatical knowledge.
Reference: [9] <author> John E. Hopcroft and Jeffrey D. Ullman. </author> <title> Introduction to Automata Theory, Languages and Computation. </title> <publisher> Addison Wesley, </publisher> <year> 1979. </year>
Reference-contexts: Therefore, we define what it means for a relation to be effectively reversible. A relation is effectively reversible if it can be effectively computed in both directions, i.e. there exists a program computing the relation in both directions, and furthermore the program always halts. In the terminology of <ref> [9] </ref>, we require that there exists an algorithm computing the relation. 10 Chapter 1 Definition 3 (Effectively reversible) A program P is effectively r-reversible iff Pis r-reversible; and Pis guaranteed to terminate (for every input). A relation r is effectively reversible iff there exists an effectively r-reversible program.
Reference: [10] <author> Helmut Horacek. </author> <title> The architecture of a generation component in a complete natural language dialogue system. </title> <editor> In Robert Dale, Chris Mellish, and Michael Zock, editors, </editor> <booktitle> Current Research in Natural Language Generation, </booktitle> <pages> pages 193 - 227. </pages> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1990. </year>
Reference-contexts: 1 INTRODUCTION Recently, there is a strong tendency to use the same grammar for parsing and generation. The general motivations for reversible grammars are reviewed in section 2. Most of the generation systems use grammars that are specifically designed for generation purposes (cf. [4], <ref> [10] </ref>, [16], [20]). The purpose of this paper 1 2 Chapter 1 is to show that the use of a reversible architecture for grammatical processing has important influences on the generation task. <p> components are assumed: `what to say' part (strategic component) `how to say it' part (tactical component) Currently, in systems where the separation is advocated it is assumed that the strategic component is able to provide all information needed by the tactical component to make decisions about lexical and syntactic choices <ref> [15, 17, 3, 10] </ref>. As a consequence, this implies that the input for tactical components is tailored to determine uniquely a good sentence, making the use of powerful Reversibility and Self-Monitoring 11 grammatical processes redundant.
Reference: [11] <author> Ray Jackendoff. </author> <title> Consciousness and the Computational Mind. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1987. </year> <note> Reversibility and Self-Monitoring 37 </note>
Reference-contexts: Clearly, this would explain why humans speak the same language they understand and vice versa. Some empirical evidence for shared processors or facilities is discussed in [8], [7] and <ref> [11] </ref>. In practice speakers often understand sentences they would never produce. This observation may have several reasons. Many differences are due to the fact that people often are able to understand otherwise mysterious utterances, because of the context and situation | using intelligence rather than grammatical knowledge.
Reference: [12] <editor> Aravind K. Joshi. </editor> <booktitle> Generation anew frontier of natural language processing? In Theoretical Issues in Natural Language Processing 3, </booktitle> <address> New Mexico State University, </address> <year> 1987. </year>
Reference-contexts: Monitoring In order to maintain a modular design additional mechanisms are necessary to perform some monitoring of the generator's output. The need for such mechanisms during natural language generation has already been noted by several Reversibility and Self-Monitoring 15 authors <ref> [5, 12, 14] </ref>. For example, [14] points out that "speakers monitor what they are saying and how they are saying it".
Reference: [13] <author> Martin Kay. </author> <title> Syntactic processing and functional sentence perspective. </title> <booktitle> In Theoretical Issues in Natural Language Processing | supplement to the Proceedings, </booktitle> <pages> pages 12-15, </pages> <address> Cambridge Massachusetts, </address> <year> 1975. </year>
Reference-contexts: Because constraint-based grammars do not enforce a specific processing regime, it is possible to conceive of constraint-based grammars, which can be used both for parsing and generation. Such grammars may be called reversible <ref> [13] </ref>. Section 2.2 defines the notion reversibility somewhat more precisely, and discusses some of its properties. We will define a reversible grammar as a grammar for which parsing and generation are both guaranteed to terminate.
Reference: [14] <author> Willem J. M. Levelt. </author> <title> Speaking: From Intention to Articulation. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1989. </year>
Reference-contexts: Monitoring In order to maintain a modular design additional mechanisms are necessary to perform some monitoring of the generator's output. The need for such mechanisms during natural language generation has already been noted by several Reversibility and Self-Monitoring 15 authors <ref> [5, 12, 14] </ref>. For example, [14] points out that "speakers monitor what they are saying and how they are saying it". <p> Monitoring In order to maintain a modular design additional mechanisms are necessary to perform some monitoring of the generator's output. The need for such mechanisms during natural language generation has already been noted by several Reversibility and Self-Monitoring 15 authors [5, 12, 14]. For example, <ref> [14] </ref> points out that "speakers monitor what they are saying and how they are saying it". In particular he shows that a speaker is also able to note that what she is saying involves a potential ambiguity for the hearer and can handle this problem by means of self-monitoring. <p> Hence our approach is much more independent of the underlying grammar. Reversibility and Self-Monitoring 31 7 DISCUSSION Limitations It should be clear that monitoring and revision involves more than the avoidance of ambiguities. <ref> [14] </ref> discusses also monitoring on the conceptual level and monitoring with respect to social standards, lexical errors, loudness, precision and others. Obviously, our approach is restricted in the sense that no changes to the input LF are made. <p> For example, during generation they use a procedural grammar where it is assumed that all relevant linguistic information is specified in the input of the tactical component. Our work is much more general because it is independent of the grammar and the underlying parser and generator. In <ref> [14] </ref> and [19] the need for monitoring or revision is discussed in detail although they describe no implementations. As far as we know our approach is the first implementation that solves the problem of revising a produced utterance in order to find an unambiguous alternative.
Reference: [15] <author> David D. McDonald. </author> <title> Natural language generation as a computational problem: An introduction. </title> <editor> In M. Brady and C. Berwick, editors, </editor> <booktitle> Computational Models of Discourse. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1983. </year>
Reference-contexts: components are assumed: `what to say' part (strategic component) `how to say it' part (tactical component) Currently, in systems where the separation is advocated it is assumed that the strategic component is able to provide all information needed by the tactical component to make decisions about lexical and syntactic choices <ref> [15, 17, 3, 10] </ref>. As a consequence, this implies that the input for tactical components is tailored to determine uniquely a good sentence, making the use of powerful Reversibility and Self-Monitoring 11 grammatical processes redundant.
Reference: [16] <author> Kathleen R. McKeown, Michael Elhadad, Yumiko Fukomoto, Jong Lim, Christine Lombardi, Jacques Robin, and Frank Smadja. </author> <title> Natural language generation in comet. </title> <editor> In Robert Dale, Chris Mellish, and Michael Zock, editors, </editor> <booktitle> Current Research in Natural Language Generation, </booktitle> <pages> pages 103 - 139. </pages> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1990. </year>
Reference-contexts: 1 INTRODUCTION Recently, there is a strong tendency to use the same grammar for parsing and generation. The general motivations for reversible grammars are reviewed in section 2. Most of the generation systems use grammars that are specifically designed for generation purposes (cf. [4], [10], <ref> [16] </ref>, [20]). The purpose of this paper 1 2 Chapter 1 is to show that the use of a reversible architecture for grammatical processing has important influences on the generation task.
Reference: [17] <author> Kathleen R. McKeown. </author> <title> Text Generation: Using Discourse Strategies and Focus Constraints to Generate Natural Language Text. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1985. </year>
Reference-contexts: components are assumed: `what to say' part (strategic component) `how to say it' part (tactical component) Currently, in systems where the separation is advocated it is assumed that the strategic component is able to provide all information needed by the tactical component to make decisions about lexical and syntactic choices <ref> [15, 17, 3, 10] </ref>. As a consequence, this implies that the input for tactical components is tailored to determine uniquely a good sentence, making the use of powerful Reversibility and Self-Monitoring 11 grammatical processes redundant.
Reference: [18] <author> Marie M. Meteer and Varda Shaked. </author> <title> Strategies for effective paraphrasing. </title> <booktitle> In Proceedings of the 12th International Conference on Computational Linguistics (COLING), </booktitle> <address> Budapest, </address> <year> 1988. </year>
Reference-contexts: The advantage of our approach is that only one paraphrase for each interpretation is produced and that the source of the ambiguity is used directly. Therefore, the generation of irrelevant paraphrases is avoided. Furthermore, we do not need special predefined `ambiguity specialists', as proposed by <ref> [18] </ref>, but rather use the parser to detect possible ambiguities. Hence our approach is much more independent of the underlying grammar. <p> Optimisations are useful when changes have to be done during the initial generation process. For example, in [6, 20] an incremental and parallel grammatical component is described that is able to handle under-specified input such that it detects and requests missing but necessary grammatical information. Comparison and Implementations In <ref> [18] </ref> strategies for paraphrasing are described. They propose an approach where during the repeated parse of an ambiguous utterance potential sources of ambiguity can be detected.
Reference: [19] <author> Marie M. Meteer. </author> <title> The Generation Gap the problem of expressibility in text planning. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, </institution> <year> 1990. </year>
Reference-contexts: Of course, it is possible to provide the user only with the produced paraphrases. This is reasonable in the case that she can find a good candidate. But if she says e.g., `none of these' then the paraphrasing algorithm is of no help in this particular situation. In <ref> [19] </ref> a strict distinction is made between processes that can change decisions that operate on intermediate levels of representation (optimisations) and others that operate on produced text (revisions). Our strategy is an example of revision. Optimisations are useful when changes have to be done during the initial generation process. <p> For example, during generation they use a procedural grammar where it is assumed that all relevant linguistic information is specified in the input of the tactical component. Our work is much more general because it is independent of the grammar and the underlying parser and generator. In [14] and <ref> [19] </ref> the need for monitoring or revision is discussed in detail although they describe no implementations. As far as we know our approach is the first implementation that solves the problem of revising a produced utterance in order to find an unambiguous alternative. Our strategies are implemented in Prolog.
Reference: [20] <author> Gunter Neumann and Wolfgang Finkler. </author> <title> A head-driven approach to incremental and parallel generation of syntactic structures. </title> <booktitle> In Proceedings of the 13th International Conference on Computational Linguistics (COLING), </booktitle> <pages> pages 288-293, </pages> <address> Helsinki, </address> <year> 1990. </year>
Reference-contexts: 1 INTRODUCTION Recently, there is a strong tendency to use the same grammar for parsing and generation. The general motivations for reversible grammars are reviewed in section 2. Most of the generation systems use grammars that are specifically designed for generation purposes (cf. [4], [10], [16], <ref> [20] </ref>). The purpose of this paper 1 2 Chapter 1 is to show that the use of a reversible architecture for grammatical processing has important influences on the generation task. <p> Our strategy is an example of revision. Optimisations are useful when changes have to be done during the initial generation process. For example, in <ref> [6, 20] </ref> an incremental and parallel grammatical component is described that is able to handle under-specified input such that it detects and requests missing but necessary grammatical information. Comparison and Implementations In [18] strategies for paraphrasing are described.
Reference: [21] <author> Carl Pollard and Ivan Sag. </author> <title> Head-driven Phrase Structure Grammar. Center for the Study of Language and Information Stanford, </title> <note> 1993. in press. </note>
Reference-contexts: 6 head : phon : hremovei synsem : V P [f in] comp : h phon : hthe folderi synsem : N P [acc] adjunct : hwith the system toolsi 3 7 7 3 7 7 7 7 5 4 We are using an HPSG-like notation close to that of <ref> [21] </ref>. 12 Chapter 1 &gt;From the generator point of view this utterance is grammatical reflects exactly what the generator wants to express.
Reference: [22] <author> Stuart M. Shieber, Gertjan van Noord, Robert C. Moore, and Fer-nando C.N. Pereira. </author> <title> Semantic-head-driven generation. </title> <journal> Computational Linguistics, </journal> <volume> 16(1), </volume> <year> 1990. </year> <note> 38 Chapter 1 </note>
Reference-contexts: As far as we know our approach is the first implementation that solves the problem of revising a produced utterance in order to find an unambiguous alternative. Our strategies are implemented in Prolog. The underlying parser and generator are described in <ref> [22] </ref> and [23]. We are using lexicalized unification-based grammars for German and Dutch. 8 FUTURE WORK It is important to investigate the implications contextual information has for disambiguation of single utterances. <p> Such an approach can be integrated in head-driven generators of the type described in <ref> [22] </ref>. For example, assume that for each recursive call to the generator the revised monitor is called, with an extra argument Head which represents the context for the parse wrt context predicate.
Reference: [23] <author> Gertjan van Noord. </author> <title> Head corner parsing for discontinuous constituency. </title> <booktitle> In 29th Annual Meeting of the Association for Computational Linguistics, </booktitle> <address> Berkeley, </address> <year> 1991. </year>
Reference-contexts: As far as we know our approach is the first implementation that solves the problem of revising a produced utterance in order to find an unambiguous alternative. Our strategies are implemented in Prolog. The underlying parser and generator are described in [22] and <ref> [23] </ref>. We are using lexicalized unification-based grammars for German and Dutch. 8 FUTURE WORK It is important to investigate the implications contextual information has for disambiguation of single utterances.
Reference: [24] <author> Gertjan van Noord. </author> <title> Reversibility in Natural Language Processing. </title> <type> PhD thesis, </type> <institution> University of Utrecht, </institution> <year> 1993. </year>
Reference-contexts: We will define a reversible grammar as a grammar for which parsing and generation are both guaranteed to terminate. The notion of a grammar that can be used both for parsing and generation is intuitively appealing; but we now provide explicit motivation for the use of reversible grammars (cf. <ref> [24] </ref>). 2.1 Motivations Motivations for reversible grammars can be divided into linguistic, language technological, and psychological motivation. The different kinds of motivation are now discussed in turn. Linguistic motivation If we assume a reversible grammar, then we make two claims.
Reference: [25] <author> R. Wilensky, Y. Arens, and D. Chin. </author> <title> Talking to unix in english: An overview of uc. </title> <journal> Communications of the ACM, </journal> <pages> pages 574 - 593, </pages> <year> 1984. </year>
Reference-contexts: The same can happen during the generation of paraphrases. If for example an intelligent help-system that supports a user by using an operation system (e.g. Unix, <ref> [25] </ref>), receives as input the utterance `Remove the folder with the system tools' then the system is not able to perform the corresponding action directly because it is ambiguous.
References-found: 25

