URL: http://seclab.cs.ucdavis.edu/~samorodi/papers/USENIX/shirriff.ps
Refering-URL: http://seclab.cs.ucdavis.edu/~samorodi/papers/USENIX/
Root-URL: http://www.cs.ucdavis.edu
Title: Abstract  
Abstract: This paper describes the implementation of Sawmill, a network file system using the RAID-II storage system. Sawmill takes advantage of the direct data path in RAID-II between the disks and the network, which bypasses the file server CPU. The key ideas in the implementation of Sawmill are combining logging (LFS) with RAID to obtain fast small writes, using new log layout techniques to improve bandwidth, and pipelining through the controller memory to reduce latency. The file system can currently read data at 21 MB/s and write data at 15 MB/s, close to the raw disk array bandwidth, while running on a relatively slow Sun-4. Performance measurements show that LFS improved performance of a stream of small writes by over a order of magnitude compared to writing directly to the RAID, and this improvement would be even larger with a faster CPU. Sawmill demonstrates that by using a storage system with a direct data path, a file system can provide data at bandwidths much higher than the file server itself could handle. However, processor speed is still an important factor, especially when handling many small requests in parallel. 
Abstract-found: 1
Intro-found: 1
Reference: [CK91] <author> Ann L. Chervenak and Randy H. Katz. </author> <title> Performance of a disk array prototype. </title> <booktitle> In Proceedings of the 1991 ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems , pages 188197, </booktitle> <year> 1991. </year>
Reference-contexts: For example, the RAID group at Berkeley built a prototype system called RAID-I, using a Sun-4/280 workstation connected to an array of 28 disks <ref> [CK91] </ref>. Unfortunately, the bandwidth available through the system was very low, only 2.3 MB/s, mainly because the memory system of the Sun 4 file server was a bottleneck. To avoid the file server bottleneck, the Berkeley RAID group designed a storage system called RAID-II [DSH + 94]. <p> Thus, small writes are relatively expensive with a RAID, and writes of a parity stripe are most efficient. 2.2 The RAID-II storage architecture The Berkeley RAID project found that a high-bandwidth disk array could easily saturate the memory bandwidth of a typical workstation file server <ref> [CK91] </ref>. The RAID-I prototype used a Sun-4/280 workstation connected to an array of 28 disks. The bandwidth available through the system was very low , only 2.3 MB/s, mainly due to the low bandwidth of the Sun 4 file servers memory system and backplane.
Reference: [CL91] <author> L. Cabrera and D. </author> <title> Long. Exploiting multiple I/O streams to provide high data-rates. </title> <booktitle> In Proceedings of the 1991 USENIX Summer Conference, </booktitle> <month> June </month> <year> 1991. </year>
Reference-contexts: An example is the Swift system <ref> [CL91] </ref>. In this system, data is striped across multiple file servers and networks to provide more bandwidth than a single server could provide. A second system that stripes data across multiple servers is Zebra [HO93]. In Zebra, each client writes its data to a sequential log.
Reference: [CMS90] <author> B. Collins, C. Mercier, and T. Stup. </author> <title> Mass-storage system advances at Los Alamos. </title> <booktitle> In Digest of Papers, Proc. Tenth IEEE Symposium on Mass Storage Systems, </booktitle> <pages> pages 7781, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: One example is the LINCS storage system at Lawrence Livermore National Laboratory [HCF + 90]; this storage system has 200 GB of disk connected to an Amdahl 5868. Another example is a system at the Los Alamos National Laboratory <ref> [CMS90] </ref>, which has an IBM 3090 running the Los Alamos Common File System. These systems solve the problem of the file server being a performance bottleneck by using a mainframe as the file server.
Reference: [dJKH93] <author> Wiebran de Jonge, M. Frans Kaashoek, and Wilson C. Hsieh. </author> <title> The Logical Disk: A new approach to improving file systems. </title> <booktitle> In Proceedings of the Fourteenth SOSP, Operating Systems Review, </booktitle> <volume> volume 27, </volume> <pages> pages 1528, </pages> <note> December 1993. </note> <author> [DSH + 94] Ann L. Drapeau, Ken Shirrif f, John H. Hartman, Ethan L. Miller, Srinivasan Seshan, Randy H. Katz, Ken Lutz, David A. Patterson, Edward K. Lee, Peter M. Chen, and Garth A. Gibson. </author> <title> RAID-II: A high-bandwidth network file server. </title> <booktitle> In Proceedings of the 21st International Symposium on Computer Architecture, </booktitle> <address> Chicago, IL, </address> <month> April </month> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: Floating Parity [MRK93] is a second technique for minimizing parity cost. In this technique, multiple parity blocks are reserved on disk. Updates can use the block closest to the current rotational position of the disk. The Logical Disk <ref> [dJKH93] </ref> implements a logstruc-tured file system at the disk level rather than the f ile system level by writing all blocks sequentially to a log and maintaining the mapping between logical disk blocks and physical disk blocks.
Reference: [GPS93] <author> G. A. Gibson, R. H. Patterson, and M. Satya-narayanan. </author> <title> A status report on research in transparent informed prefetching. </title> <journal> ACM Operating System Review, </journal> <volume> 27(2):2134, </volume> <month> April </month> <year> 1993. </year> <note> [HCF + 90] C. </note> <author> Hogan, L. Cassell, J. Foglesong, J. Kor-das, M. Nemanic, and G. Richmond. </author> <title> The Livermore Distributed Storage System: Requirements and overview. </title> <booktitle> In Digest of Papers, Proc. Tenth IEEE Symposium on Mass Storage Systems , pages 617, </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: Prefetching can be used to improve the performance of individual requests; if the data is fetched before it is required, the disk latency will not af fect latency to handle the request. One technique for this is Gibsons Transparent Informed Prefetching <ref> [GPS93] </ref>; by obtaining hints from the application, the f ile system can fetch data before it is required. Large reads and writes already achieve close to the raw system performance with Sawmill. Thus, large request performance will only increase with storage systems that have more or faster disks.
Reference: [HO93] <author> John H. Hartman and John K. Ousterhout. </author> <title> The Zebra striped network file system. </title> <booktitle> In Proceedings of the 14th Symposium on Operating System Principles , pages 2943, </booktitle> <address> Asheville, NC, </address> <month> December </month> <year> 1993. </year> <note> ACM. </note>
Reference-contexts: An example is the Swift system [CL91]. In this system, data is striped across multiple file servers and networks to provide more bandwidth than a single server could provide. A second system that stripes data across multiple servers is Zebra <ref> [HO93] </ref>. In Zebra, each client writes its data to a sequential log. This log is then striped across multiple servers, each with a disk. Zebra and Sawmill both combine LFS and RAID.
Reference: [IBM91] <author> IBM. </author> <title> 0661 Functional Specification Model 371, </title> <note> Version 0.7, February 18 1991. </note>
Reference-contexts: This indicates the performance available to a single application. Figure 4 shows read requests and Figure 5 shows write requests. 4.1.1 Raw disk array performance To understand the performance, the first item to examine is the disk array. The array contains IBM 0661 disks <ref> [IBM91] </ref>. These disks have average rotational latency of 7 ms and average seek time of 12 ms. Each disk can read and write at about 1.6 MB/s. The disk array was configured with 16 disks on four controllers.
Reference: [MJLF84] <author> M. McKusick, W. Joy, S. Leffler, and R. Fabry. </author> <title> A fast file system for UNIX. </title> <journal> ACM Transactions on Computer Systems , 2(3):181197, </journal> <month> August </month> <year> 1984. </year>
Reference-contexts: Small write performance would scale almost linearly, since small writes are totally CPU bound. This would result in small write performance of 2 MB/s, about 200 times the performance of writing directly to the RAID. <ref> [MJLF84] </ref> running on RAID-II and compare it with Sawmill.
Reference: [MK91] <author> L. McVoy and S. Kielman. </author> <title> Extent-like performance from a UNIX file system. </title> <booktitle> In Proceedings of the 1991 USENIX Winter Conference, </booktitle> <year> 1991. </year>
Reference-contexts: A Unix f ile system with clustering <ref> [MK91] </ref> would shift performance along the RAID line, since the writes would be in lar ger units. concurrency of the system. Only 3 or 4 Sawmill oper-ations can be run in parallel before the CPU becomes a bottleneck.
Reference: [MRK93] <author> J. Menon, J. Roche, and J. Kasson. </author> <title> Floating parity and data disk arrays. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 17:129139, </volume> <year> 1993. </year>
Reference-contexts: One technique is Parity Logging [SGH93]. In this technique, parity updates are written to a log. At regular intervals, the log is scanned and the parity modif ications are applied to the standard RAID parity blocks. Floating Parity <ref> [MRK93] </ref> is a second technique for minimizing parity cost. In this technique, multiple parity blocks are reserved on disk. Updates can use the block closest to the current rotational position of the disk.
Reference: [Nel90] <author> B. Nelson. </author> <title> An overview of functional multiprocessing for network servers. </title> <type> Technical report, </type> <institution> Aus-pex Systems Inc., </institution> <month> July </month> <year> 1990. </year>
Reference-contexts: Such a system avoids the problem of the server being a performance bottleneck by using multiple processors, with the associated gain in server CPU power and memory bandwidth. One example of this is the Auspex NFS server <ref> [Nel90] </ref>. The Auspex system uses asymmetric functional multiprocessing, in which separate processors deal with the Ethernet, f iles, disk, and management. The necessary disk bandwidth is provided by parallel SCSI disks.
Reference: [PGK88] <author> David Patterson, Garth Gibson, and Randy Katz. </author> <title> A case for redundant arrays of inexpensive disks (RAID). </title> <booktitle> In ACM SIGMOD Conference, </booktitle> <pages> pages 109116, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: 1. Introduction An I/O gap has arisen between the data demands of processors and the data rates individual disks can supply <ref> [PGK88] </ref>. This gap is worsening as processor speeds continue to increase and as new applications such as multimedia and scientif ic visualization demand ever higher data rates. One common solution to the I/O bottleneck is the disk array, where several disks provide data in parallel. <p> This section gives background information on these ideas. 2.1 RAID A RAID (Redundant Array of Inexpensive Disks) combines two ideas: parallelism and redundancy <ref> [PGK88] </ref>. A RAID uses multiple disks in parallel to provide much higher bandwidth than a single disk. A RAID can also perform multiple operations in parallel. Redundancy in the form of parity is used to maintain reliability. By storing parity, data can be fully recovered after a single disk failure.
Reference: [Ros92] <author> Mendel Rosenblum. </author> <title> The Design and Implementation of a Logstructured File System. </title> <type> PhD thesis, </type> <institution> U.C. Berkeley, </institution> <month> June </month> <year> 1992. </year> <note> Report UCB/CSD 92/696. </note>
Reference-contexts: Another problem with a RAID disk array is that small random writes are very expensive due to parity computation, which is used for reliability. One solution is a logstructured file system (LFS) <ref> [Ros92] </ref>, which writes everything to a sequential log so there are only large sequential writes. Thus, a log-structured file system can greatly improve performance of small writes. This paper describes the Sawmill f ile system, which has been designed to provide high bandwidths by taking advantage of the RAID-II architecture. <p> Thus, four disk operations are required. Data xor Parity 2.3 Log-structured file systems The third idea used by the Sawmill f ile system is the logstructured file system (LFS). A logstructured file system <ref> [Ros92] </ref> writes data only to a sequential log. (A file block does not have a f ixed position on disk, but instead its position changes every time the block is rewritten.) The log is written to disk in lar ge units, called segments, on the order of 1 MB in length. <p> Cleaning is the process of garbage-collecting the log to free up space. Cleaning is not yet operational in Sawmill, so performance measurements are not available. Previous work <ref> [Ros92] </ref> indicated that overall cleaning costs would be low. However, [SBMS93] found cleaning costs to be high for some workloads, such as transaction processing. Costs were high particularly in environments with largely full disks, and cleaning could potentially cause service interruption.
Reference: [SBMS93] <author> M. Seltzer, K. Bostic, M. K. McKusick, and C. Staelin. </author> <title> An implementation of a log-structured file system for UNIX. </title> <booktitle> In 1993 Winter Usenix, </booktitle> <pages> pages 307326, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: Cleaning is the process of garbage-collecting the log to free up space. Cleaning is not yet operational in Sawmill, so performance measurements are not available. Previous work [Ros92] indicated that overall cleaning costs would be low. However, <ref> [SBMS93] </ref> found cleaning costs to be high for some workloads, such as transaction processing. Costs were high particularly in environments with largely full disks, and cleaning could potentially cause service interruption.
Reference: [SGH93] <author> D. Stodolsky, G. Gibson, and M. Holland. </author> <title> Parity logging overcoming the small write problem in redundant disk arrays. Computer Architecture News, </title> <address> 2(2):6475, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: There is a cost tradeoff: Sawmill requires a special controller, while Swift and Zebra require multiple fast servers. 5.4 RAID parity updates There are several techniques to reduce the cost of updating parity after a partial stripe write. One technique is Parity Logging <ref> [SGH93] </ref>. In this technique, parity updates are written to a log. At regular intervals, the log is scanned and the parity modif ications are applied to the standard RAID parity blocks. Floating Parity [MRK93] is a second technique for minimizing parity cost.
Reference: [Wil90] <author> D. Wilson. </author> <title> The Auspex NS5000 fileserver. Unix Review, </title> <address> 8(8):91102, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: However, the performance of the Auspex is limited by its use of NFS, Ethernet, and a single 55 MB/s VME bus; measurements show it can supply about 400 KB/s to a single client and can satu rate an Ethernet network with 1MB/s per Ethernet connection <ref> [Wil90] </ref>. The DataMesh project proposed a dif ferent approach to multiprocessing [Wil91]. The proposed system would consist of a lar ge array of disk nodes, where each node had a fast CPU (20 MIPS) and 8 to 32 MB of memory.

References-found: 16

