URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/awd/www/sirs96_dubrawski.ps.gz
Refering-URL: http://www.cs.cmu.edu/afs/cs/user/awd/www/papers.html
Root-URL: 
Title: Memory-based Stochastic Optimization for Automated Tuning of Neural Network's High Level Parameters  
Author: Artur Dubrawski 
Address: 5000 Forbes Avenue, Pittsburgh, PA 15213, USA  
Affiliation: The Robotics Institute, Carnegie Mellon University,  
Date: July 22-26, 1996  
Note: Proceedings of the 4th International Symposium on Intelligent Robotic Systems SIRS'96, Lisbon, Portugal,  
Abstract: In this paper we describe a new method for automated tuning of high level parameters of supervised learning systems. It uses memory-based learning principles and follows certain ideas of experimental design. The described method allows not only for an efficient search through a decision space, but also for a concurrent validation of the learning algorithm performance on a given data. Potential usefulness of the proposed approach is illustrated with the Fuzzy-ARTMAP neural network application to learning a qualitative positioning of an indoor mobile robot equipped with sonar range sensors. Automatically selected neural network setpoints reach a comparable performance to those achieved by human experts in relatively simple 2D cases. Migration of the proposed method to higher order optimization domains bears a big promise, but requires further research. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Balakrishnan K., Honavar V. </author> <title> Evolutionary Design of Neural Architectures A Preliminary Taxonomy and Guide to Literature. </title> <type> Technical Reprort CS TR 95-01, </type> <institution> Iowa State University, </institution> <year> 1995. </year>
Reference-contexts: A vast majority of attempts to automate neural networks' high level parameter tuning is based on the evolutionary computation. Due to the space limits we do not include a review of such methods here (for up to date reviews refer to <ref> [1, 14] </ref>), instead we only express a general opinion that they are usually very expensive in terms of a number of network configurations to try before coming up with a satisfactory one.
Reference: 2. <author> Box G.E.P., Draper N.R. </author> <title> Empirical Model-Building and Response Surfaces, </title> <publisher> Wiley 1987. </publisher>
Reference-contexts: In contrast, new optimization techniques emerging from experimental design <ref> [2] </ref> and memory-based learning [11], offer a very attractive way of using both the computational time and the empirical knowledge about the neural network's performance so far. <p> With the above described stochastic approach to validation we have managed to turn a deterministic machine learning algorithm into a random process relevant to the memory based optimization. Memory based stochastic optimization [10] is a new technique combining memory-based learning [11] with experimental design methods <ref> [2] </ref>. It differs form a conventional numerical optimization because it accepts noisy samples, operates on non-linear or just locally linear approximations of the objective function surface, makes use of uncertainty of the maintained objective function model, and attempts to perform a relatively small number of deliberatively selected optimization steps. <p> Results of the execution of the experiments are then used for the model update. For the purpose of this paper we consider three of the experimental design heuristics, briefly described below. Response surface methods (RSM) are statistical techniques traditionally used in experimental design <ref> [2] </ref>. At the beginning of a search for an optimum, a region of interest is determined, and experiments are made at points indicated by a low-order polynomial regression as locally the best for identyfying the underlying objective function's properties.
Reference: 3. <author> Carpenter G.A., Grossberg S., Rosen D. </author> <title> Fuzzy ART: Fast Stable Learning of Analog Patterns by an Adaptive Resonance System. </title> <booktitle> Neural Networks, </booktitle> <pages> 759-771, Vol.4, </pages> <year> 1991. </year>
Reference-contexts: The proposed method of validation and tuning may be actually applied to any kind of a supervised learning algorithm adjusted with numeric gains. 2 Fuzzy-ARTMAP's parameters Fuzzy-ARTMAP [4] is a modular architecture composed of two Fuzzy-ART <ref> [3] </ref> networks interconnected with an associative memory, and capable of a stable supervised learning of multidimensional mappings in response to arbitrary sequences of real valued training vectors. The component Fuzzy-ART modules are self-organizing incremental categorization engines.
Reference: 4. <author> Carpenter G.A., Grossberg S., Markuzon N., Reynolds J.H., </author> <title> Rosen D.B. Fuzzy ARTMAP: A Neural Network Architecture for Incremental Supervised Learning of Analog Multidimensional Maps. </title> <journal> IEEE Trans. on Neural Networks, </journal> <volume> 689-713, 3(5), </volume> <year> 1992. </year>
Reference-contexts: The proposed method of validation and tuning may be actually applied to any kind of a supervised learning algorithm adjusted with numeric gains. 2 Fuzzy-ARTMAP's parameters Fuzzy-ARTMAP <ref> [4] </ref> is a modular architecture composed of two Fuzzy-ART [3] networks interconnected with an associative memory, and capable of a stable supervised learning of multidimensional mappings in response to arbitrary sequences of real valued training vectors. The component Fuzzy-ART modules are self-organizing incremental categorization engines.
Reference: 5. <author> Dubrawski A., Crowley J.L. </author> <title> Self-Supervised System for Reactive Navigation. </title> <booktitle> 1994 IEEE International Conference on Robotics and Automation, </booktitle> <address> San Diego, USA, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: Unfortunately, despite a growing popularity of the ART neural networks and an increasing number of their successful uses (also in robotics: <ref> [5, 6, 7, 12] </ref>), there are no complete design principles developed so far. Fuzzy-ART recognition cycle follows a winner-takes-all competitive activation and learning rule.
Reference: 6. <author> Dubrawski A., Reignier P. </author> <title> Learning to Categorize Perceptual Space of a Mobile Robot Using Fuzzy-ART Neural Network. </title> <booktitle> IEEE/RSJ Int. Conf. on Intelligent Robots and Systems IROS'94, </booktitle> <address> Munich, Germany, </address> <month> September </month> <year> 1994. </year>
Reference-contexts: Unfortunately, despite a growing popularity of the ART neural networks and an increasing number of their successful uses (also in robotics: <ref> [5, 6, 7, 12] </ref>), there are no complete design principles developed so far. Fuzzy-ART recognition cycle follows a winner-takes-all competitive activation and learning rule.
Reference: 7. <author> Dubrawski A. </author> <title> Neurocomputing for mobile robot navigation successful and promising attempts. </title> <booktitle> (unpublished lecture notes) 3rd Int. Symp. on Intelligent Robotic Systems SIRS'95, </booktitle> <address> Pisa, Italy, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: ie. the necessity of assigning proper values to neural network's high level parameters (such as a number of hidden layers and hidden units, a learning speed, momentum factors, categorization resolution, and so on) is apparently a serious practical drawback of neurocomputing attempts to tackle various, including robotic, real world problems <ref> [7] </ref>. Typically, it takes an expert a significant time to achieve a satisfactory performance of a neural system, however the design effort strongly depends on the particular task setup and on the characteristics of a selected network type. <p> Unfortunately, despite a growing popularity of the ART neural networks and an increasing number of their successful uses (also in robotics: <ref> [5, 6, 7, 12] </ref>), there are no complete design principles developed so far. Fuzzy-ART recognition cycle follows a winner-takes-all competitive activation and learning rule.
Reference: 8. <author> Dubrawski A., Moore A.W., Schneider J. </author> <title> Memory-Based Stochastic Validation and Tuning of Supervised Learning Algorithms. </title> <type> Technical Report CMU-RI-TR-96-25, </type> <institution> The Robotics Institute, Carnegie Mellon University, </institution> <year> 1996. </year>
Reference-contexts: IEMAX tries to spot a maximum of the upper confidence limit surface, which then becomes the next experiment suggestion. The third of the selected experimental design methods, called OPTEX <ref> [8] </ref>, makes its suggestions at the location of the largest expected improvement over the current model's maximum y fl [8]: x sugg = max 8x y min where x is the input query vector, y; y min ; y max are the output and its limits, and p (y j x) <p> IEMAX tries to spot a maximum of the upper confidence limit surface, which then becomes the next experiment suggestion. The third of the selected experimental design methods, called OPTEX <ref> [8] </ref>, makes its suggestions at the location of the largest expected improvement over the current model's maximum y fl [8]: x sugg = max 8x y min where x is the input query vector, y; y min ; y max are the output and its limits, and p (y j x) is a probablility density function of y at a given query point x. 4 Learning task setup The particular
Reference: 9. <author> Kaelbling L.P. </author> <title> Learning in Embedded Systems. </title> <type> PhD Thesis. Technical Report TR-90-04, </type> <institution> Department of Computer Science, Stanford University, </institution> <year> 1990. </year>
Reference-contexts: It is called AutoRSM [10]. Response surface optimization is designed to make rather cautious changes to operating conditions of the analyzed process. Quite an opposite idea led to the IEMAX algorithm [10], which is an extension of the interval estimation method <ref> [9] </ref> to continous domains. IEMAX suggests experiment locations with the principle of optimism.
Reference: 10. <author> Moore A.W., Schneider J. </author> <title> Memory-based Stochastic Optimization. </title> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <publisher> MIT Press, </publisher> <year> 1995 </year> <month> (to appear). </month>
Reference-contexts: In this paper we describe a concept and preliminary results obtained with memory-based stochastic optimization <ref> [10] </ref> used to spot useful and statistically valid settings for high level parameters of the Fuzzy-ARTMAP neural network trained to predict a position of a mobile robot equipped with ultrasonic range sensors. <p> With the above described stochastic approach to validation we have managed to turn a deterministic machine learning algorithm into a random process relevant to the memory based optimization. Memory based stochastic optimization <ref> [10] </ref> is a new technique combining memory-based learning [11] with experimental design methods [2]. <p> If the observations are noisy, the local regression fit provides a filtered estimate of the expected output value. A query at a selected point of the model reveals not only the output expectation, but also Bayesian estimates of its confidence <ref> [10] </ref>. Experimental design methods use that information to suggest input space locations to try, in order to maximize the expected output and/or efficiently enrich the model. Results of the execution of the experiments are then used for the model update. <p> The model, containing the information on the objective function and on its confidence, significantly improves efficiency of the RSM search decisions and makes an algorithmization of the technique possible. It is called AutoRSM <ref> [10] </ref>. Response surface optimization is designed to make rather cautious changes to operating conditions of the analyzed process. Quite an opposite idea led to the IEMAX algorithm [10], which is an extension of the interval estimation method [9] to continous domains. IEMAX suggests experiment locations with the principle of optimism. <p> It is called AutoRSM <ref> [10] </ref>. Response surface optimization is designed to make rather cautious changes to operating conditions of the analyzed process. Quite an opposite idea led to the IEMAX algorithm [10], which is an extension of the interval estimation method [9] to continous domains. IEMAX suggests experiment locations with the principle of optimism.
Reference: 11. <author> Moore A.W., Atkeson C.G., Schaal S. </author> <title> Memory-based Learning for Control. </title> <type> Technical Report CMU-RI-TR-95-18, </type> <institution> The Robotics Institute, Carnegie Mellon University, </institution> <year> 1995. </year>
Reference-contexts: In contrast, new optimization techniques emerging from experimental design [2] and memory-based learning <ref> [11] </ref>, offer a very attractive way of using both the computational time and the empirical knowledge about the neural network's performance so far. <p> With the above described stochastic approach to validation we have managed to turn a deterministic machine learning algorithm into a random process relevant to the memory based optimization. Memory based stochastic optimization [10] is a new technique combining memory-based learning <ref> [11] </ref> with experimental design methods [2]. <p> Unfortunately a conventional RSM requires human intervention. Moreover it makes no use of old data, ie. as soon as the region of interest is changed, the method makes no reference to the previously explored areas. These are fundamental motivations for the application of a memory-based learning <ref> [11] </ref> to retain the data gathered during exploration, and to build a global nonlinear plant model with them. The model, containing the information on the objective function and on its confidence, significantly improves efficiency of the RSM search decisions and makes an algorithmization of the technique possible.
Reference: 12. <author> Racz J., Dubrawski A. </author> <title> Artificial neural network for mobile robot topological localization. </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <volume> 16(1), </volume> <pages> 73-80, </pages> <year> 1995. </year>
Reference-contexts: Unfortunately, despite a growing popularity of the ART neural networks and an increasing number of their successful uses (also in robotics: <ref> [5, 6, 7, 12] </ref>), there are no complete design principles developed so far. Fuzzy-ART recognition cycle follows a winner-takes-all competitive activation and learning rule. <p> max are the output and its limits, and p (y j x) is a probablility density function of y at a given query point x. 4 Learning task setup The particular robotic task we consider here is a mobile robot qualitative localization in a setup described in more detail in <ref> [12] </ref>. A robotic vehicle, equipped with ultrasonic range sensors, is placed somewhere in proximity of a selected object of the scene (a doorway in particular). <p> Otherwise it depends mainly on the particular choice of the high level parameters' settings. In the research described in this paper we additionally constrain the size of memory occupied by a neural network representation. It has an obvious practical justification and extends the work <ref> [12] </ref>. <p> It is interesting to compare the performance of automated tuning of the high level parameters against the outcomes provided by human experts. Work <ref> [12] </ref> presents the results of manual tuning of the Fuzzy-ARTMAP system in the same as ours robotic application. The best predictive accuracy reported there was 16% at % a = 0:89 and fi a = 0:3, calculated with the hold-out method.
Reference: 13. <author> Weiss S.M., </author> <title> Kulikowski C.A. Computer Systems That Learn, </title> <publisher> Morgan Kaufman Publishers, </publisher> <year> 1991. </year>
Reference-contexts: Unfortunately, as in the case of any other supervised learning algorithm, validation based on a single fold of the available data (so called train-and-test or hold-out method <ref> [13] </ref>) gives a cheap, but usually biased estimate of the system performance. A much more relevant approximation can be obtained with cross-validation or bootstraping, but these techniques require several repetitive train-and-test cycles, and thus are relatively expensive.
Reference: 14. <author> Yao X. </author> <title> A Review of Evolutionary Artificial Neural Networks. </title> <journal> Int. J. of Intelligent Systems, </journal> <volume> 8(4), </volume> <pages> 539-567, </pages> <year> 1993. </year> <title> This article was processed using the T E X macro package with SIRS96 style </title>
Reference-contexts: A vast majority of attempts to automate neural networks' high level parameter tuning is based on the evolutionary computation. Due to the space limits we do not include a review of such methods here (for up to date reviews refer to <ref> [1, 14] </ref>), instead we only express a general opinion that they are usually very expensive in terms of a number of network configurations to try before coming up with a satisfactory one.
References-found: 14

