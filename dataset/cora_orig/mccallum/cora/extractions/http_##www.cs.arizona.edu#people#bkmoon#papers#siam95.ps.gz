URL: http://www.cs.arizona.edu/people/bkmoon/papers/siam95.ps.gz
Refering-URL: http://www.cs.arizona.edu/people/bkmoon/papers.html
Root-URL: http://www.cs.arizona.edu
Title: Chapter 1 Runtime Support and Dynamic Load Balancing Strategies for Structured Adaptive Applications  
Author: Bongki Moon Gopal Patnaik Robert Bennett David Fyfe Alan Sussman Craig Douglas Joel Saltz K. Kailasanath 
Date: Feb. 1995.  
Address: San Francisco, CA,  
Affiliation: Computing,  
Note: Proceedings of the 7th SIAM Conference on Parallel Processing for Scientific  Paragon is about half that of a single processor Cray C90.  
Abstract: One class of scientific and engineering applications involves structured meshes. One example of a code in this class is a flame modelling code developed at the Naval Research Laboratory (NRL). The numerical model used in the NRL flame code is predominantly based on structured finite volume methods. The chemistry process of the reactive flow is modeled by a system of ordinary differential equations which is solved independently at each grid point. Thus, though the model uses a mesh structure, the workload at each grid point can vary considerably. It is this feature that requires the use of both structured and unstructured methods in the same code. We have applied the Multiblock PARTI and CHAOS runtime support libraries to parallelize the NRL flame code with minimal changes to the sequential code. We have also developed parallel algorithms to carry out dynamic load balancing. It has been observed that the overall performance scales reasonably up to 256 Paragon processors and that the total runtime on a 256-node 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Agrawal, A. Sussman, and J. Saltz, </author> <title> Compiler and runtime support for structured and block structured applications, </title> <booktitle> in Proceedings Supercomputing '93, </booktitle> <publisher> IEEE Computer Society Press, </publisher> <month> Nov. </month> <year> 1993, </year> <pages> pp. 578-587. </pages> <note> An extended version available as University of Maryland Technical Report CS-TR-3052 and UMIACS-TR-93-29. </note>
Reference-contexts: The two types of processes require the use of both structured and unstructured methods in the same code. The Multiblock PARTI (structured) and CHAOS (unstructured) runtime support libraries have been applied to parallelize the NRL flame code with minimal changes to the existing sequential code <ref> [1, 2] </ref>. 2 Computational Model The various physical processes in the flame can be placed into one of two groups depending on their solution techniques: 1. Structured processes, where the computation is based on structured meshes.
Reference: [2] <author> R. Das, M. Uysal, J. Saltz, and Y.-S. Hwang, </author> <title> Communication optimizations for irregular scientific computations on distributed memory architectures, </title> <type> Tech. Rep. </type> <institution> CS-TR-3163 and UMIACS-TR-93-109, University of Maryland, Department of Computer Science and UMIACS, </institution> <month> Oct. </month> <year> 1993. </year> <note> To appear in Journal of Parallel and Distributed Computing. </note>
Reference-contexts: The two types of processes require the use of both structured and unstructured methods in the same code. The Multiblock PARTI (structured) and CHAOS (unstructured) runtime support libraries have been applied to parallelize the NRL flame code with minimal changes to the existing sequential code <ref> [1, 2] </ref>. 2 Computational Model The various physical processes in the flame can be placed into one of two groups depending on their solution techniques: 1. Structured processes, where the computation is based on structured meshes.
Reference: [3] <author> C. C. Douglas, </author> <title> Implementing abstract multigrid or multilevel methods, </title> <booktitle> in Sixth Copper Mountain Conference on Multigrid Methods, </booktitle> <editor> N. D. Melson, T. A. Manteuffel, and S. F. McCormick, eds., </editor> <volume> vol. </volume> <pages> CP 3224, </pages> <address> Hampton, VA, 1993, </address> <publisher> NASA, </publisher> <pages> pp. 127-141. </pages>
Reference-contexts: The bounds of the parallelized loops are adjusted to 2 i; j N 2 + 1 for each processor. In the NRL flame code, a multigrid method, based on MADPACK <ref> [3] </ref>, is used as the solution technique for the elliptic equation that arises in the fluid convection model. Multigrid methods employ a number of nested meshes at different levels of resolution. The restriction and prolongation operations for shifting between different multigrid levels require moving regular array sections with non-unit strides.
Reference: [4] <author> M. Gerndt, </author> <title> Updating distributed variables in local computations, </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 2 (1990), </volume> <pages> pp. 171-193. </pages>
Reference-contexts: In order to parallelize this nested loop structure, several steps must be taken. First, it must be ensured that a processor has allocated enough memory for its portion of the global array and any additional off-processor data (i.e., a layer of overlap cells <ref> [4] </ref> in each dimension). Second, the original loop bounds must be adjusted from global indices to local indices since the arrays no longer reside on one processor. Last, any references to data from adjacent processors (i.e., vx0 (i-1,j)) must be resolved before entering the loop.
Reference: [5] <author> G. Patnaik, K. J. Laskey, K. Kailasanath, E. S. Oran, and T. A. Brun, </author> <title> FLIC | a detailed, two-dimensional flame model, </title> <type> Memorandum Report 6555, </type> <institution> Naval Research Laboratory, </institution> <month> Sept. </month> <year> 1989. </year>
Reference-contexts: To date, sufficiently detailed calculations for hydrocarbon flames have only been carried out for steady-state flames. Some preliminary calculations of transient methane flames with moderately detailed chemistry have been carried out at the Naval Research Laboratory (NRL) <ref> [5] </ref>. For heavier hydrocarbons, which are of more interest for Navy applications, the computational requirements are currently beyond the capabilities of current vector supercomputers. Thus, it is imperative that the current detailed flame code be ported to massively parallel computers.
References-found: 5

