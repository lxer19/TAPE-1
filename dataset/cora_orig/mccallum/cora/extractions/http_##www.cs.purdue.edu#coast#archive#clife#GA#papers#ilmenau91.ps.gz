URL: http://www.cs.purdue.edu/coast/archive/clife/GA/papers/ilmenau91.ps.gz
Refering-URL: http://www.cs.purdue.edu/coast/archive/clife/GA/papers/
Root-URL: http://www.cs.purdue.edu
Email: baeck@lumpi.informatik.uni-dortmund.de  
Title: Optimization by Means of Genetic Algorithms  
Author: Thomas Back 
Address: P.O. Box 50 05 00 D-4600 Dortmund 50 Germany  
Affiliation: University of Dortmund Department of Computer Science Chair XI  
Abstract: Genetic Algorithms (GAs) are powerful heuristic search strategies based upon a simple model of organic evolution. The basic working scheme of GAs as developed by Holland [Hol75] is described within this paper in a formal way, and extensions based upon the second-level learning principle for strategy parameters as introduced in Evolution Strategies (ESs) are proposed. First experimental results concerning this extension of GAs are also reported.
Abstract-found: 1
Intro-found: 1
Reference: [AK89] <author> Emile H. L. Aarts and Jan Korst. </author> <title> Simulated Annealing and Boltzmann Machines. </title> <publisher> Wiley, </publisher> <address> Chichester, </address> <year> 1989. </year>
Reference-contexts: However, these results are | like similar ones for Simulated Annealing <ref> [AK89] </ref> and ESs [Bor78] | of no practical worth. Due to this fact, a lot of additional heuristics have been introduced and tested for GA-applications.
Reference: [Bak85] <author> James Edward Baker. </author> <title> Adaptive selection methods for genetic algorithms. </title> <editor> In J. J. Grefenstette, editor, </editor> <booktitle> Proceedings of the First International Conference on Genetic Algorithms and Their Applications, </booktitle> <pages> pages 101-111, </pages> <address> Hillsdale, New Jersey, 1985. </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference-contexts: In the following generations this superindividual may completely dominate the population, thus leading to premature convergence towards a suboptimal solution. Two attempts to solve these problems are described here in short: (1) Use static distributions for selection probabilities instead of dynamically changing, fitness-dependent ones. The first approach by Baker <ref> [Bak85] </ref> is called linear ranking and assumes the population to be sorted by fitness values, the most fit individual being a 1 . <p> A lot of scaling techniques has been suggested [For85, GB89, Gol89], but only very few of them have been used practically. Experimental results give strong indications for the advantage of ranking in case of multimodal problems <ref> [Bak85, Whi89] </ref>, although these techniques have been neglected by the GA-community for a long time. Furthermore, the mutation probability is a critical parameter of the algorithm, and any efforts to find an optimal probability by theoretical or experimental investigations did not lead to universally valid results [SCED89, HM91]. <p> For ranking the usual setting of max = 1:1 was chosen according to <ref> [Bak85] </ref>. A GA with normal ranking was compared to (10,50)-linear ranking with 1 adaptive mutation rate for f 1 and 20 adaptive mutation rates for f 7 as well as the standard GA using proportional selection and the normal mutation rate p m = 0:001.
Reference: [BH91a] <author> Thomas Back and Frank Hoffmeister. </author> <title> Extended selection mechanisms in genetic algorithms. </title> <editor> In Richard K. Belew, editor, </editor> <booktitle> Proceedings of the Fourth International Conference on Genetic Algorithms and their Applications, </booktitle> <address> San Diego, California, USA, 1991. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: Here extinctive means, that only &lt; of the individuals are allowed to be selected | the rest of the population is discarded. Borrowed from ESs, the notion (,)-linear ranking or (,)-proportional selection is used to describe such a selection scheme <ref> [BH91a] </ref>. 4 Experimental Results To illustrate the self-learning capability of GAs concerning mutation rates a simple uni modal and a multimodal test function as described in table 1 are used. Name Description Dim.
Reference: [BH91b] <author> Thomas Back and Frank Hoffmeister. </author> <title> Global optimization by means of evolutionary algorithms. </title> <editor> In Alexander N. Antamoshkin, editor, </editor> <title> Random Search as a Method for Adaptation and Optimization of Complex Systems, </title> <address> pages 17-21, Divnogorsk, UdSSR, </address> <month> March </month> <year> 1991. </year> <institution> Kras-nojarsk Space Technology University. </institution> <month> 6 </month>
Reference-contexts: Even the reproduction process often favours structures of higher quality. Thus, during the evolution process the average quality of the population increases, hopefully leading to an optimum solution. Algorithms following this general approach have been summarized under the term Evolutionary Algorithms elsewhere <ref> [BH91b] </ref>. Some important representatives are the Genetic Algorithm (GA) [Hol75] and Classifier System (CS) [Hol86] by Holland, the Evolution Strategy (ES) [Rec73, Sch81] by Rechenberg and Schwefel, and the Genetic Programming Paradigm [Koz89].
Reference: [Bor78] <author> Joachim Born. </author> <title> Evolutionsstrategien zur numerischen Losung von Adaptationsaufgaben. Dis--sertation A, Humboldt-Universitat, </title> <address> Berlin, GDR, </address> <year> 1978. </year>
Reference-contexts: However, these results are | like similar ones for Simulated Annealing [AK89] and ESs <ref> [Bor78] </ref> | of no practical worth. Due to this fact, a lot of additional heuristics have been introduced and tested for GA-applications.
Reference: [For85] <editor> Stephanie Forrest. </editor> <title> Documentation for Prisoner's Dilemma and Norms Programs that use the Genetic Algorithm. </title> <institution> The University of New Mexico, </institution> <address> Albuquerque, NM, </address> <year> 1985. </year>
Reference-contexts: The aim of these techniques is to stretch the range of fitness values if the population has almost converged, and to penalize superindividuals. A lot of scaling techniques has been suggested <ref> [For85, GB89, Gol89] </ref>, but only very few of them have been used practically. Experimental results give strong indications for the advantage of ranking in case of multimodal problems [Bak85, Whi89], although these techniques have been neglected by the GA-community for a long time.
Reference: [GB89] <author> John J. Grefenstette and James E. Baker. </author> <title> How genetic algorithms work: A critical look at implicit parallelism. </title> <booktitle> In Schaffer [Sch89], </booktitle> <pages> pages 20-27. </pages>
Reference-contexts: The aim of these techniques is to stretch the range of fitness values if the population has almost converged, and to penalize superindividuals. A lot of scaling techniques has been suggested <ref> [For85, GB89, Gol89] </ref>, but only very few of them have been used practically. Experimental results give strong indications for the advantage of ranking in case of multimodal problems [Bak85, Whi89], although these techniques have been neglected by the GA-community for a long time.
Reference: [Gol89] <author> David E. Goldberg. </author> <title> Genetic algorithms in search, optimization and machine learning. </title> <publisher> Addison Wesley, </publisher> <year> 1989. </year>
Reference-contexts: The choice of using binary alleles is theoretically founded by the argument of maximizing the number of schemata (hyperplanes of varying dimension in the l-dimensional binary space) available with respect to a given code <ref> [Hol75, Gol89] </ref>. P 0 is the randomly generated initial population, and the parameters and l describe the number of individuals representing one generation and the length of the `genetic' representation of each individual, respectively. <p> The aim of these techniques is to stretch the range of fitness values if the population has almost converged, and to penalize superindividuals. A lot of scaling techniques has been suggested <ref> [For85, GB89, Gol89] </ref>, but only very few of them have been used practically. Experimental results give strong indications for the advantage of ranking in case of multimodal problems [Bak85, Whi89], although these techniques have been neglected by the GA-community for a long time. <p> x max ] to which the bitstrings are mapped, in order to achieve a maximum resolution x = (x max x min )=(2 32 1). 5 5 Conclusions Although global convergence is not guaranteed within finite time, GAs have proven to be robust heuristic search techniques in many real-world applications <ref> [Gol89] </ref>. However, a lot of heuristic extensions were proposed and tested in the past, without leading to a clear statement how the GA in general should be parameterized and configurated. Self-adaptation with respect to the actual topology of the fitness function could be a promising way to solve these problems.
Reference: [Gre87] <author> John J. Grefenstette. </author> <title> A User's Guide to GENESIS. </title> <booktitle> Navy Center for Applied Research in Artificial Intelligence, </booktitle> <address> Washington, D. C., </address> <year> 1987. </year>
Reference-contexts: As an abstraction from the work of Hol-land [Hol75] and the nowadays often used quasi-standard given by a computer-implementation of John Grefenstette <ref> [Gre87] </ref>, the following 8-tuple formalization is used here: GA = (P 0 ; ; l; s; ; ; f; t) (1) where P 0 = (a 0 ) 2 I I = f0; 1g l initial population 2 N population size l 2 N length of individuals' representation s : I <p> f 7 (~x) = nA + i=1 x 2 i A cos (!x i ) f 1 with sine A = 10 ; ! = 2 ; 5:12 x i 5:12 wave superposition Table 1: The set of test functions To obtain the results, a modified version of Grefenstette's GENESIS-GA <ref> [Gre87] </ref> was used. The GA is defined by the following parameter and configuration settings: Crossover rate p c = 0:6; population size = 50; length of an individual l = 32n, where n denotes the dimension of the objective function 3 ; two-point crossover; Gray code.
Reference: [Har90] <author> Richard F. Hartl. </author> <title> A global convergence proof for a class of genetic algorithms. </title> <institution> University of Technology, Vienna, </institution> <year> 1990. </year>
Reference-contexts: Recently it has been proven that under certain assumptions the algorithm described so far finds a global optimum point a fl 2 I (f fl := f (a fl ) = maxff (a) j a 2 Ig) with probability 1, i.e. <ref> [Har90] </ref>: lim P [a fl 2 P t ] = 1 where P [] denotes a probability. <p> Due to this fact, a lot of additional heuristics have been introduced and tested for GA-applications. Some of the main extensions are presented in the next section. 2 The detailed proof as well as the exact definitions are omitted here, but can be found in <ref> [Har90] </ref>. 3 3 Extensions of the traditional Genetic Algorithm Critical components of the GA are the selection operator, the recombination operator, and the mutation and recombination probabilities.
Reference: [HB91] <author> Frank Hoffmeister and Thomas Back. </author> <title> Genetic algorithms and evolution strategies: Similarities and differences. </title> <booktitle> In Schwefel and Manner [SM91], </booktitle> <pages> pages 455-470. </pages>
Reference-contexts: Name Description Dim. Characteristics Ref. f 1 sphere model n = 30 unimodal, <ref> [HB91] </ref> f 1 (~x) = i=1 x 2 i high-dimensional [Jon75] 5:12 x i 5:12 [Sch81] f 7 generalized Rastrigin's n = 20 multimodal, [HB91] function high-dimensional, [TZ89] f 7 (~x) = nA + i=1 x 2 i A cos (!x i ) f 1 with sine A = 10 ; <p> Name Description Dim. Characteristics Ref. f 1 sphere model n = 30 unimodal, <ref> [HB91] </ref> f 1 (~x) = i=1 x 2 i high-dimensional [Jon75] 5:12 x i 5:12 [Sch81] f 7 generalized Rastrigin's n = 20 multimodal, [HB91] function high-dimensional, [TZ89] f 7 (~x) = nA + i=1 x 2 i A cos (!x i ) f 1 with sine A = 10 ; ! = 2 ; 5:12 x i 5:12 wave superposition Table 1: The set of test functions To obtain the results, a modified version
Reference: [HM91] <author> J. Hesser and R. </author> <title> Manner. Towards an optimal mutation probability in genetic algorithms. </title> <booktitle> In Schwefel and Manner [SM91], </booktitle> <pages> pages 23-32. </pages>
Reference-contexts: Furthermore, the mutation probability is a critical parameter of the algorithm, and any efforts to find an optimal probability by theoretical or experimental investigations did not lead to universally valid results <ref> [SCED89, HM91] </ref>. The problem is to balance between destroying good information and effectively using mutation as an additional search operator. Concerning mutation rates a promising approach is that of self-learning of strategy parameters as introduced in ESs.
Reference: [Hol75] <author> John H. Holland. </author> <title> Adaptation in natural and artificial systems. </title> <publisher> The University of Michigan Press, </publisher> <address> Ann Arbor, </address> <year> 1975. </year>
Reference-contexts: Thus, during the evolution process the average quality of the population increases, hopefully leading to an optimum solution. Algorithms following this general approach have been summarized under the term Evolutionary Algorithms elsewhere [BH91b]. Some important representatives are the Genetic Algorithm (GA) <ref> [Hol75] </ref> and Classifier System (CS) [Hol86] by Holland, the Evolution Strategy (ES) [Rec73, Sch81] by Rechenberg and Schwefel, and the Genetic Programming Paradigm [Koz89]. The approaches are mainly differing with respect to the structure of the individuals, which directly influences the recombination and mutation operators. <p> As an abstraction from the work of Hol-land <ref> [Hol75] </ref> and the nowadays often used quasi-standard given by a computer-implementation of John Grefenstette [Gre87], the following 8-tuple formalization is used here: GA = (P 0 ; ; l; s; ; ; f; t) (1) where P 0 = (a 0 ) 2 I I = f0; 1g l initial population <p> The choice of using binary alleles is theoretically founded by the argument of maximizing the number of schemata (hyperplanes of varying dimension in the l-dimensional binary space) available with respect to a given code <ref> [Hol75, Gol89] </ref>. P 0 is the randomly generated initial population, and the parameters and l describe the number of individuals representing one generation and the length of the `genetic' representation of each individual, respectively. <p> This selection scheme is called proportional selection <ref> [Hol75] </ref>. It leads to the expectation of individual a t i to occur t i = p s (a t i ) times in generation t + 1 (generally t i is called the expected value of a t i ). <p> determines an operator ! t i 2 for each individual a 0t i 2 P 0t which will be applied to this individual: (a 0t i ) = ! t The genetic operator set f! : I fi I ! P ! Ig includes genetic operators like crossover and mutation <ref> [Hol75] </ref>. The stochastic elements of these operators (application probabilities, e.g. p m 0:001 for mutation and p c 0:6 for crossover, choice of loci) are included in a somehow abstract manner in the probability distribution p 2 P.
Reference: [Hol86] <author> John H. Holland. </author> <title> Escaping brittleness: The possibilities of general-purpose learning algorithms applied to parallel rule-based systems. </title> <editor> In Ryszard S. Michalski, Jaime G. Carbonell, and Tom M. Mitchell, editors, </editor> <booktitle> Machine Learning Vol.II, chapter 20, </booktitle> <pages> pages 593-623. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1986. </year>
Reference-contexts: Thus, during the evolution process the average quality of the population increases, hopefully leading to an optimum solution. Algorithms following this general approach have been summarized under the term Evolutionary Algorithms elsewhere [BH91b]. Some important representatives are the Genetic Algorithm (GA) [Hol75] and Classifier System (CS) <ref> [Hol86] </ref> by Holland, the Evolution Strategy (ES) [Rec73, Sch81] by Rechenberg and Schwefel, and the Genetic Programming Paradigm [Koz89]. The approaches are mainly differing with respect to the structure of the individuals, which directly influences the recombination and mutation operators.
Reference: [Jon75] <author> Kenneth De Jong. </author> <title> An analysis of the behaviour of a class of genetic adaptive systems. </title> <type> PhD thesis, </type> <institution> University of Michigan, </institution> <year> 1975. </year> <note> Diss. Abstr. Int. 36(10), 5140B, University Microfilms No. 76-9381. </note>
Reference-contexts: Name Description Dim. Characteristics Ref. f 1 sphere model n = 30 unimodal, [HB91] f 1 (~x) = i=1 x 2 i high-dimensional <ref> [Jon75] </ref> 5:12 x i 5:12 [Sch81] f 7 generalized Rastrigin's n = 20 multimodal, [HB91] function high-dimensional, [TZ89] f 7 (~x) = nA + i=1 x 2 i A cos (!x i ) f 1 with sine A = 10 ; ! = 2 ; 5:12 x i 5:12 wave superposition
Reference: [Koz89] <author> John R. Koza. </author> <title> Hierarchical genetic algorithms operating on populations of computer programs. </title> <editor> In N. S. Sridharan, editor, </editor> <booktitle> Eleventh international joint conference on artificial intelligence, </booktitle> <pages> pages 768-774. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <month> August </month> <year> 1989. </year>
Reference-contexts: Algorithms following this general approach have been summarized under the term Evolutionary Algorithms elsewhere [BH91b]. Some important representatives are the Genetic Algorithm (GA) [Hol75] and Classifier System (CS) [Hol86] by Holland, the Evolution Strategy (ES) [Rec73, Sch81] by Rechenberg and Schwefel, and the Genetic Programming Paradigm <ref> [Koz89] </ref>. The approaches are mainly differing with respect to the structure of the individuals, which directly influences the recombination and mutation operators.
Reference: [Rec73] <author> Ingo Rechenberg. </author> <title> Evolutionsstrategie: Optimierung technischer Systeme nach Prinzipien der biologischen Evolution. </title> <publisher> Frommann-Holzboog Verlag, Stuttgart, </publisher> <year> 1973. </year>
Reference-contexts: Algorithms following this general approach have been summarized under the term Evolutionary Algorithms elsewhere [BH91b]. Some important representatives are the Genetic Algorithm (GA) [Hol75] and Classifier System (CS) [Hol86] by Holland, the Evolution Strategy (ES) <ref> [Rec73, Sch81] </ref> by Rechenberg and Schwefel, and the Genetic Programming Paradigm [Koz89]. The approaches are mainly differing with respect to the structure of the individuals, which directly influences the recombination and mutation operators.
Reference: [SCED89] <author> J. David Schaffer, Richard A. Caruna, Larry J. Eshelman, and Rajarshi Das. </author> <title> A study of control parameters affecting online performance of genetic algorithms for function optimization. </title> <booktitle> In Schaffer [Sch89], </booktitle> <pages> pages 51-60. </pages>
Reference-contexts: Furthermore, the mutation probability is a critical parameter of the algorithm, and any efforts to find an optimal probability by theoretical or experimental investigations did not lead to universally valid results <ref> [SCED89, HM91] </ref>. The problem is to balance between destroying good information and effectively using mutation as an additional search operator. Concerning mutation rates a promising approach is that of self-learning of strategy parameters as introduced in ESs.
Reference: [Sch81] <editor> Hans-Paul Schwefel. </editor> <title> Numerical Optimization of Computer Models. </title> <publisher> Wiley, </publisher> <address> Chichester, </address> <year> 1981. </year>
Reference-contexts: Algorithms following this general approach have been summarized under the term Evolutionary Algorithms elsewhere [BH91b]. Some important representatives are the Genetic Algorithm (GA) [Hol75] and Classifier System (CS) [Hol86] by Holland, the Evolution Strategy (ES) <ref> [Rec73, Sch81] </ref> by Rechenberg and Schwefel, and the Genetic Programming Paradigm [Koz89]. The approaches are mainly differing with respect to the structure of the individuals, which directly influences the recombination and mutation operators. <p> The approaches are mainly differing with respect to the structure of the individuals, which directly influences the recombination and mutation operators. ESs benefit from the additional capability of learning on the level of strategy parameters by self-adapting them during the search (second-level learning) <ref> [Sch81] </ref> | a feature that has not yet been introduced into the general concept of other Evolutionary Algorithms. 1 2 The traditional Genetic Algorithm A general definition of the GA does not exist. <p> Name Description Dim. Characteristics Ref. f 1 sphere model n = 30 unimodal, [HB91] f 1 (~x) = i=1 x 2 i high-dimensional [Jon75] 5:12 x i 5:12 <ref> [Sch81] </ref> f 7 generalized Rastrigin's n = 20 multimodal, [HB91] function high-dimensional, [TZ89] f 7 (~x) = nA + i=1 x 2 i A cos (!x i ) f 1 with sine A = 10 ; ! = 2 ; 5:12 x i 5:12 wave superposition Table 1: The set of
Reference: [Sch88] <editor> Hans-Paul Schwefel. </editor> <booktitle> Evolutionary learning optimum-seeking on parallel computer architectures. </booktitle> <editor> In A. Sydow, S.G. Tzafestas, and R. Vichnevetsky, editors, </editor> <booktitle> Proceedings of the International Symposium on Systems Analysis and Simulation 1988, I: Theory and Foundations, </booktitle> <pages> pages 217-225, </pages> <address> Berlin, </address> <month> Sept. </month> <year> 1988. </year> <institution> Akademie der Wissenschaften der DDR, Akademie-Verlag. </institution>
Reference-contexts: Self-adaptation with respect to the actual topology of the fitness function could be a promising way to solve these problems. Research from ESs confirms that under certain conditions (possibility to forget good information, recombination, extinctive selection) self-adaptation works well and enlarges effectivity and efficiency of an Evolutionary Algorithm <ref> [Sch88] </ref>.
Reference: [Sch89] <editor> J. David Schaffer, editor. </editor> <booktitle> Proceedings of the Third International Conference on Genetic Algorithms and Their Applications, </booktitle> <address> San Mateo, California, June 1989. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference: [SM91] <editor> Hans-Paul Schwefel and Reinhard Manner, editors. </editor> <title> Parallel Problem Solving from Nature. </title> <booktitle> Lecture Notes in Computer Science 496. </booktitle> <publisher> Springer, </publisher> <address> Berlin, </address> <year> 1991. </year>
Reference: [TZ89] <author> A. Torn and A. Zilinskas. </author> <title> Global Optimization, </title> <booktitle> volume 350 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer, </publisher> <address> Berlin, FRG, </address> <year> 1989. </year> <month> 7 </month>
Reference-contexts: Name Description Dim. Characteristics Ref. f 1 sphere model n = 30 unimodal, [HB91] f 1 (~x) = i=1 x 2 i high-dimensional [Jon75] 5:12 x i 5:12 [Sch81] f 7 generalized Rastrigin's n = 20 multimodal, [HB91] function high-dimensional, <ref> [TZ89] </ref> f 7 (~x) = nA + i=1 x 2 i A cos (!x i ) f 1 with sine A = 10 ; ! = 2 ; 5:12 x i 5:12 wave superposition Table 1: The set of test functions To obtain the results, a modified version of Grefenstette's GENESIS-GA
Reference: [Whi89] <author> Darrell Whitley. </author> <title> The GENITOR algorithm and selection pressure: Why rank-based allocation of reproductive trials is best. </title> <booktitle> In Schaffer [Sch89], </booktitle> <pages> pages 116-121. 8 </pages>
Reference-contexts: A lot of scaling techniques has been suggested [For85, GB89, Gol89], but only very few of them have been used practically. Experimental results give strong indications for the advantage of ranking in case of multimodal problems <ref> [Bak85, Whi89] </ref>, although these techniques have been neglected by the GA-community for a long time. Furthermore, the mutation probability is a critical parameter of the algorithm, and any efforts to find an optimal probability by theoretical or experimental investigations did not lead to universally valid results [SCED89, HM91].
References-found: 24

