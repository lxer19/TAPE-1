URL: ftp://ftp.cag.lcs.mit.edu/pub/papers/sensitivity-bw-lat-97.ps.Z
Refering-URL: http://www.cag.lcs.mit.edu/alewife/papers/sensitivity-bw-lat-97.html
Root-URL: 
Title: The Sensitivity of Communication Mechanisms to Bandwidth and Latency  
Author: Frederic T. Chong Rajeev Barua Fredrik Dahlgren John D. Kubiatowicz and Anant Agarwal 
Address: Berkeley  
Affiliation: Massachusetts Institute of Technology University of California at Davis Chalmers University of Technology University of California at  
Date: Feb 1-4, 1998.  
Note: Appears in Proceedings of the 4th Int'l Symposium on High Performance Computer Architecture, Las Vegas, NV,  
Abstract: The goal of this paper is to gain insight into the relative performance of communication mechanisms as bisection bandwidth and network latency vary. We compare shared memory with and without prefetch-ing, message passing with interrupts and with polling, and bulk transfer via DMA. We present two sets of experiments involving four irregular applications on the MIT Alewife multiprocessor. First, we introduce I/O cross-traffic to vary bisection bandwidth. Second, we change processor clock speeds to vary relative network latency. We establish a framework from which to understand a range of results. On Alewife, shared memory provides good performance, even on producer-consumer applications with little data-reuse. On machines with lower bisection bandwidth and higher network latency, however, message-passing mechanisms become important. In particular, the high communication volume of shared memory threatens to become difficult to support on future machines without expensive, high-dimensional networks. Furthermore, the round-trip nature of shared memory may not be able to tolerate the latencies of future networks. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anant Agarwal et al. </author> <title> The MIT Alewife machine: Architecture and performance. </title> <booktitle> In ISCA `22, </booktitle> <year> 1995. </year>
Reference-contexts: For example, machines such as the BBN Butterfly have long supported shared memory and bulk transfer, the Cray T3E [27] supports both shared memory and messaging styles of communication, the Stanford Dash [18] supports shared memory and prefetching, MIT Alewife <ref> [1] </ref>, Fugu [20], and the Wisconsin Typhoon [25] support several variants of shared memory and messaging styles. The availability of machines with multiple mechanisms has led to an increasing amount of insight on the effectiveness of the various mechanisms for different applications [6] [11] [28] [30] [15] [8]. <p> Because message passing and prefetching can have some number of outstanding requests, the slope of their performance degradation is shallower than that for shared memory without prefetching. 3 Experimental Platform In this study, we made use of the MIT Alewife machine <ref> [1] </ref>. Alewife provides a unique opportunity to explore the behavior of a number of different communication mechanisms in a single hardware environment.
Reference: [2] <author> Arvind, David E. Culler, and Gino K. Maa. </author> <title> Assessing the benefits of fine-grained parallelism in dataflow programs. </title> <booktitle> In Supercomputing `88. IEEE, </booktitle> <year> 1988. </year>
Reference-contexts: We started from existing parallel ICCG algorithms [14] [26]. Im plementation details are available in [7]. ICCG with Message Passing The ICCG computation graph is essentially a dataflow computation <ref> [2] </ref> and is easily implemented via active messages. For each node in its local memory, a processor keeps track of when all incoming edges have been satisfied, whereafter the outgoing edges can be processed. Bulk transfer is also straightforward.
Reference: [3] <author> M. J. Berger and S. H. Bokhari. </author> <title> A partitioning strategy for PDEs across multiprocessors. </title> <booktitle> In ICPP, </booktitle> <year> 1985. </year>
Reference-contexts: A molecule's position is determined by its own velocity and the force employed by other molecules within a certain cut-off radius. The molecules are distributed between the processing nodes using the RCB algorithm <ref> [3] </ref>, minimizing the communication. The force of a molecule is affected by all its interactions with other molecules, and can therefore be updated by both local and remote processors, while the coordinates are written by one processor but potentially read by many.
Reference: [4] <author> Eric A. Brewer et al. </author> <title> Remote queues: Exposing message queues for optimization and atomicity. </title> <booktitle> In SPAA `95, </booktitle> <year> 1995. </year>
Reference-contexts: Typical shared memory miss penalties Remote Queues abstraction <ref> [4] </ref>, which supports polling with selective interrupts for system messages. Bulk transfer: Bulk transfer is accomplished in Alewife by adding (address, length) pairs that describe blocks of data to the end of an active message. <p> Polling cuts this overhead by about 35 percent. More importantly, interrupts cause dramatically more synchronization time than polling, because of the large load imbalance caused by asynchronous interrupts [5]. Polling provides greater control of message reception, which allows for a more balanced computation <ref> [4] </ref>. The other mechanisms also avoid load imbalance. Shared memory mechanisms do not use interrupts and are similar to polling.
Reference: [5] <author> Eric A. Brewer and Bradley C. Kuszmaul. </author> <title> How to get good performance from the CM-5 data network. </title> <booktitle> In IPPS, </booktitle> <year> 1994. </year>
Reference-contexts: The low computation-to-communication ratio of ICCG results in a large number of messages, which makes interrupt overhead significant. Polling cuts this overhead by about 35 percent. More importantly, interrupts cause dramatically more synchronization time than polling, because of the large load imbalance caused by asynchronous interrupts <ref> [5] </ref>. Polling provides greater control of message reception, which allows for a more balanced computation [4]. The other mechanisms also avoid load imbalance. Shared memory mechanisms do not use interrupts and are similar to polling.
Reference: [6] <author> Satish Chandra, James R. Larus, and Anne Rogers. </author> <title> Where is time spent in message-passing and shared-memory programs. </title> <booktitle> In ASPLOS VI, </booktitle> <pages> pages 6173, </pages> <year> 1994. </year>
Reference-contexts: The availability of machines with multiple mechanisms has led to an increasing amount of insight on the effectiveness of the various mechanisms for different applications <ref> [6] </ref> [11] [28] [30] [15] [8]. <p> Because the relative effectiveness of communication mechanisms is also tied to basic machine parameters such as available bandwidth and latency, not surprisingly, various studies offer differing conclusions on the relative effectiveness of the mechanisms on the same application. For example, the simulation study of Chandra, Rogers, and Larus <ref> [6] </ref> using a basic machine model similar to the CM5 found that message passing EM3D performed roughly a factor of two better than the shared memory version. The two mechanisms were more or less indistinguishable on Alewife for the same application. <p> EM3D is iterative, and is barrier-synchronized between two phases. Communication is taking place because of data being updated within one phase that will be used by other nodes in the subsequent phase. Our versions are based on the code from the University of Wis-consin <ref> [6] </ref>. Program parameters are 10000 nodes, degree 10, 20 percent non-local edges, span of 3, and 50 iterations. <p> Our comparison of communication mechanisms is similar to Chandra, Larus and Rogers <ref> [6] </ref>, but we have available a larger set of mechanisms and we generalize to a range of system parameters. This generalization is similar to the study of latency, occupancy, and bandwidth by Holt et. al [15], which focuses exclusively upon shared-memory mechanisms.
Reference: [7] <author> Frederic T. Chong and Anant Agarwal. </author> <title> Shared memory versus message passing for iterative solution of sparse, irregular problems. </title> <type> Tech rpt, </type> <institution> mit-lcs-tr-697, MIT Lab for Comp Sci, </institution> <address> Cambridge, MA, </address> <year> 1996. </year>
Reference-contexts: We measure the performance of the ICCG sparse triangular solve kernel running on a large structural finite-element matrix, the BC SSTK32 2-million element automobile chassis, obtained from the Harwell-Boeing benchmark suite [9]. We started from existing parallel ICCG algorithms [14] [26]. Im plementation details are available in <ref> [7] </ref>. ICCG with Message Passing The ICCG computation graph is essentially a dataflow computation [2] and is easily implemented via active messages. For each node in its local memory, a processor keeps track of when all incoming edges have been satisfied, whereafter the outgoing edges can be processed.
Reference: [8] <author> Frederic T. Chong et al. </author> <title> Application performance on the mit alewife multiprocessor. </title> <booktitle> IEEE Computer, </booktitle> <month> Dec </month> <year> 1996. </year>
Reference-contexts: The availability of machines with multiple mechanisms has led to an increasing amount of insight on the effectiveness of the various mechanisms for different applications [6] [11] [28] [30] [15] <ref> [8] </ref>.
Reference: [9] <author> Ian S. Duff, Roger G. Grimes, and John G. Lewis. </author> <title> User's guide for the Harwell-Boeing sparse matrix collection. </title> <type> Tech Rpt TR/PA/92/86, CERFACS, 42 Ave G. Coriolis, </type> <address> 31057 Toulouse Cedex, France, </address> <year> 1992. </year>
Reference-contexts: We measure the performance of the ICCG sparse triangular solve kernel running on a large structural finite-element matrix, the BC SSTK32 2-million element automobile chassis, obtained from the Harwell-Boeing benchmark suite <ref> [9] </ref>. We started from existing parallel ICCG algorithms [14] [26]. Im plementation details are available in [7]. ICCG with Message Passing The ICCG computation graph is essentially a dataflow computation [2] and is easily implemented via active messages.
Reference: [10] <author> Thorsten von Eicken et al. </author> <title> Active messages: a mechanism for integrated communication and computation. </title> <booktitle> In ISCA `19, </booktitle> <year> 1992. </year>
Reference-contexts: We will briefly describe how each of these operates on the Alewife machine. Message passing with interrupts: For message passing, Alewife supports active messages <ref> [10] </ref> of the form: send am (proc, handler, args...) which causes a message to be sent to processor proc, interrupt the processor, and invoke handler with args. An active message with a null handler, no body and no arguments, only takes 102 cycles plus .8 cycles per hop.
Reference: [11] <author> Babak Falsafi et al. </author> <title> Application-specific protocols for user-level shared memory. </title> <booktitle> In Supercomputing 94, </booktitle> <year> 1994. </year>
Reference-contexts: The availability of machines with multiple mechanisms has led to an increasing amount of insight on the effectiveness of the various mechanisms for different applications [6] <ref> [11] </ref> [28] [30] [15] [8]. <p> They also found that node-to-network bandwidth was not critical in modern multiprocessors. Our study shows, however, that bandwidth across the bisection of the machine may become a critical cost in supporting shared memory on modern machines. Such costs will make message passing and specialized user-level protocols <ref> [11] </ref> increasingly important as processor speeds increase. Woo et al. [30] compared bulk transfer with shared memory on simulations of the FLASH multiprocessor [13] running the SPLASH [12] suite.
Reference: [12] <author> Maya Gokhale et al. </author> <title> Building and using a highly parallel programmable logic array. </title> <journal> Computer, </journal> <volume> 24(1), </volume> <month> January </month> <year> 1991. </year>
Reference-contexts: Such costs will make message passing and specialized user-level protocols [11] increasingly important as processor speeds increase. Woo et al. [30] compared bulk transfer with shared memory on simulations of the FLASH multiprocessor [13] running the SPLASH <ref> [12] </ref> suite. They found bulk transfer performance to be disappointing due to the high cost of initiating transfer and the difficulty in finding computation to overlap with the transfer. Although, Alewife's DMA mechanism is cheaper to initiate than theirs, we also found bulk transfer to have performance problems.
Reference: [13] <author> Mark Heinrich et al. </author> <title> The performance impact of flexibility in the Stan-ford FLASH multiprocessor. </title> <booktitle> In ASPLOS VI, </booktitle> <pages> pages 274285, </pages> <year> 1994. </year>
Reference-contexts: Note that prefetching is not precisely modeled, since the success of a prefetch is dependent upon Alewife's original network latency rather than the emulated latency. The message-passing and bulk transfer curves are plot 1 Note that FLASH has been redesigned to use the Origin network since <ref> [13] </ref>. ted for reference only. Their network latencies are not varied and are based upon Alewife network latencies. However, since our message-passing applications use asynchronous, unacknowledged communication, we expect that message-passing performance will remain relatively constant. <p> Such costs will make message passing and specialized user-level protocols [11] increasingly important as processor speeds increase. Woo et al. [30] compared bulk transfer with shared memory on simulations of the FLASH multiprocessor <ref> [13] </ref> running the SPLASH [12] suite. They found bulk transfer performance to be disappointing due to the high cost of initiating transfer and the difficulty in finding computation to overlap with the transfer.
Reference: [14] <author> Bruce Hendrickson and Robert Leland. </author> <title> The Chaco user's guide. </title> <institution> Tech Rpt SAND94-2692, Sandia National Labs, </institution> <year> 1995. </year>
Reference-contexts: We measure the performance of the ICCG sparse triangular solve kernel running on a large structural finite-element matrix, the BC SSTK32 2-million element automobile chassis, obtained from the Harwell-Boeing benchmark suite [9]. We started from existing parallel ICCG algorithms <ref> [14] </ref> [26]. Im plementation details are available in [7]. ICCG with Message Passing The ICCG computation graph is essentially a dataflow computation [2] and is easily implemented via active messages.
Reference: [15] <author> C. Holt et al. </author> <title> The effects of latency, occupancy and bandwidth on the performance of cache-coherent multprocessors. </title> <type> Tech rpt, </type> <institution> Stanford Univ, Stanford, </institution> <address> CA, </address> <month> Jan </month> <year> 1995. </year>
Reference-contexts: The availability of machines with multiple mechanisms has led to an increasing amount of insight on the effectiveness of the various mechanisms for different applications [6] [11] [28] [30] <ref> [15] </ref> [8]. <p> Our comparison of communication mechanisms is similar to Chandra, Larus and Rogers [6], but we have available a larger set of mechanisms and we generalize to a range of system parameters. This generalization is similar to the study of latency, occupancy, and bandwidth by Holt et. al <ref> [15] </ref>, which focuses exclusively upon shared-memory mechanisms. Although the Alewife machine provides an excellent starting point for the comparison of a large number of communication mechanisms, our results are greatly enhanced by our use of emulation, an approach inspired by the work at Wisconsin [25].
Reference: [16] <author> A. Klaiber and H. Levy. </author> <title> A comparison of message passing and shared memory for data-parallel programs. </title> <booktitle> In ISCA `21, </booktitle> <year> 1994. </year>
Reference-contexts: This class includes papers by Lin and Snyder [19], Martonosi and Gupta [23], and LeBlanc and Markatos [17]. These studies however do not compare machine implementations, as all programs ultimately used shared memory only. Klaiber and Levy <ref> [16] </ref> study the performance of programs which accesses shared memory or message passing runtime libraries. These libraries generated traces for shared memory and message passing simulators, to generate statistics on message traffic. However, their programs were not fined tuned for any particular architecture, and hence not fair to either.
Reference: [17] <author> T. LeBlanc and E. Markatos. </author> <title> Shared memory vs. message passing in shared-memory multiprocesors. </title> <booktitle> In 4th SPDP, </booktitle> <year> 1992. </year>
Reference-contexts: Another group of studies simulated message passing on a shared memory machine, and compared the performance of message passing programs using this simulation, against programs using shared memory directly. This class includes papers by Lin and Snyder [19], Martonosi and Gupta [23], and LeBlanc and Markatos <ref> [17] </ref>. These studies however do not compare machine implementations, as all programs ultimately used shared memory only. Klaiber and Levy [16] study the performance of programs which accesses shared memory or message passing runtime libraries.
Reference: [18] <author> Daniel Lenoski and others. </author> <title> The Stanford Dash multiprocessor. </title> <booktitle> Computer, </booktitle> <address> 25(3):6380, </address> <year> 1992. </year>
Reference-contexts: Many research and commercial machines also sport various combinations of mechanisms. For example, machines such as the BBN Butterfly have long supported shared memory and bulk transfer, the Cray T3E [27] supports both shared memory and messaging styles of communication, the Stanford Dash <ref> [18] </ref> supports shared memory and prefetching, MIT Alewife [1], Fugu [20], and the Wisconsin Typhoon [25] support several variants of shared memory and messaging styles.
Reference: [19] <author> C. Lin and L. Snyder. </author> <title> A comparison of programming models for shared-memory multiprocessors. </title> <booktitle> In ICPP, </booktitle> <year> 1990. </year>
Reference-contexts: Another group of studies simulated message passing on a shared memory machine, and compared the performance of message passing programs using this simulation, against programs using shared memory directly. This class includes papers by Lin and Snyder <ref> [19] </ref>, Martonosi and Gupta [23], and LeBlanc and Markatos [17]. These studies however do not compare machine implementations, as all programs ultimately used shared memory only. Klaiber and Levy [16] study the performance of programs which accesses shared memory or message passing runtime libraries.
Reference: [20] <author> K. Mackenzie et al. </author> <title> Exploiting two-case delivery for fast protected messaging. </title> <booktitle> In HPCA-4, </booktitle> <year> 1998. </year>
Reference-contexts: For example, machines such as the BBN Butterfly have long supported shared memory and bulk transfer, the Cray T3E [27] supports both shared memory and messaging styles of communication, the Stanford Dash [18] supports shared memory and prefetching, MIT Alewife [1], Fugu <ref> [20] </ref>, and the Wisconsin Typhoon [25] support several variants of shared memory and messaging styles. The availability of machines with multiple mechanisms has led to an increasing amount of insight on the effectiveness of the various mechanisms for different applications [6] [11] [28] [30] [15] [8].
Reference: [21] <author> N. K. Madsen. </author> <title> Divergence preserving discrete surface integral methods for Maxwell's curl equations using non-orthogonal unstructured grids. </title> <institution> Tech Rpt 92.04, RIACS, </institution> <year> 1992. </year>
Reference-contexts: We also observe this effect, to a lesser degree, on EM3D and UNSTRUC. The remainder of this section provides brief application descriptions and more detail on performance results. 4.1 EM3D EM3D from Berkeley models the propagation of electromagnetic waves through three-dimensional objects <ref> [21] </ref>. It uses an implicit red-black computation on an irregular bipartite graph. EM3D is iterative, and is barrier-synchronized between two phases. Communication is taking place because of data being updated within one phase that will be used by other nodes in the subsequent phase.
Reference: [22] <author> Richard P. Martin et al. </author> <title> Effects of communication latency, overhead, and bandwidth in a cluster architecture. </title> <booktitle> In ISCA `24, </booktitle> <pages> pages 8597, </pages> <year> 1997. </year>
Reference-contexts: Their network latencies are not varied and are based upon Alewife network latencies. However, since our message-passing applications use asynchronous, unacknowledged communication, we expect that message-passing performance will remain relatively constant. Other studies <ref> [22] </ref> have found that asynchronous implementations of applications such as EM3D are relatively insensitive to microsecond-latencies on networks of workstations. Referring back to Table 1, we see that network latency is a serious issue for shared memory that will worsen as processor speeds increase. <p> Although, Alewife's DMA mechanism is cheaper to initiate than theirs, we also found bulk transfer to have performance problems. Our problems arose from the ir 9 regularity of our application suite, which caused high scatter/gather copying costs and limited data transfer size. Concurrent work at Berkeley <ref> [22] </ref> explores the effect of message-passing latency, overhead and bandwidth on networks of workstations. They measured performance of several programs written in Split-C and compared their results with predictions from the LogP model. The effects of overhead and gap on applications were predicted well by LogP.
Reference: [23] <author> M. Martonosi and A. Gupta. </author> <title> Tradeoffs in message passing and shared memory implementations of a standard cell router. </title> <booktitle> In ICPP, </booktitle> <year> 1989. </year>
Reference-contexts: Another group of studies simulated message passing on a shared memory machine, and compared the performance of message passing programs using this simulation, against programs using shared memory directly. This class includes papers by Lin and Snyder [19], Martonosi and Gupta <ref> [23] </ref>, and LeBlanc and Markatos [17]. These studies however do not compare machine implementations, as all programs ultimately used shared memory only. Klaiber and Levy [16] study the performance of programs which accesses shared memory or message passing runtime libraries.
Reference: [24] <author> Shubhendu S. Mukherjee et al. </author> <title> Efficient support for irregular applications on distributed-memory machines. </title> <booktitle> In PPoPP'95, </booktitle> <pages> pages 6879, </pages> <year> 1995. </year>
Reference-contexts: EM3D is our only application which benefits significantly from prefetching. As explained earlier in this section, this stems from its low ratio of computation to communication, making gains in communication time more significant. 4.2 UNSTRUC UNSTRUC simulates fluid flows over three-dimensional physical objects, represented by an unstructured mesh <ref> [24] </ref>. The code operates upon nodes, edges between nodes, and faces that connect three or four nodes. We used MESH2K as an input dataset, a 2000 node irregular mesh provided with the code. The shared-memory versions obtained were optimized for data distribution and privatization. <p> Shared memory mechanisms do not use interrupts and are similar to polling. Bulk transfer uses fewer messages than finer-grained message-passing and thus few interrupts. 4.4 MOLDYN MOLDYN is a molecular dynamics application developed by the University of Maryland and the University of Wisconsin <ref> [24] </ref>. A molecule's position is determined by its own velocity and the force employed by other molecules within a certain cut-off radius. The molecules are distributed between the processing nodes using the RCB algorithm [3], minimizing the communication.
Reference: [25] <author> S. K. Reinhart, J. R. Larus, and D. A. Wood. Tempest and Typhoon: </author> <title> User-level shared memory. </title> <booktitle> In ISCA `21, </booktitle> <year> 1994. </year>
Reference-contexts: For example, machines such as the BBN Butterfly have long supported shared memory and bulk transfer, the Cray T3E [27] supports both shared memory and messaging styles of communication, the Stanford Dash [18] supports shared memory and prefetching, MIT Alewife [1], Fugu [20], and the Wisconsin Typhoon <ref> [25] </ref> support several variants of shared memory and messaging styles. The availability of machines with multiple mechanisms has led to an increasing amount of insight on the effectiveness of the various mechanisms for different applications [6] [11] [28] [30] [15] [8]. <p> Although the Alewife machine provides an excellent starting point for the comparison of a large number of communication mechanisms, our results are greatly enhanced by our use of emulation, an approach inspired by the work at Wisconsin <ref> [25] </ref>. Chandra, Larus and Rogers compare four applications on a simulation of a message-passing machine similar to a CM-5 multiprocessor against a simulation of a hypothetical machine also similar to a CM-5, but extended by shared-memory hardware.
Reference: [26] <author> R. Schreiber and W. Tang. </author> <title> Vectorizing the conjugate gradient method. </title> <booktitle> In Proceedings Symposium CYBER 205 Applications, </booktitle> <year> 1982. </year>
Reference-contexts: We measure the performance of the ICCG sparse triangular solve kernel running on a large structural finite-element matrix, the BC SSTK32 2-million element automobile chassis, obtained from the Harwell-Boeing benchmark suite [9]. We started from existing parallel ICCG algorithms [14] <ref> [26] </ref>. Im plementation details are available in [7]. ICCG with Message Passing The ICCG computation graph is essentially a dataflow computation [2] and is easily implemented via active messages.
Reference: [27] <author> Steven L. Scott. </author> <title> Synchronization and communication in the T3E multiprocessor. </title> <booktitle> In ASPLOS VII, </booktitle> <year> 1996. </year>
Reference-contexts: We now understand to a much greater extent than before the implementation tradeoffs. Many research and commercial machines also sport various combinations of mechanisms. For example, machines such as the BBN Butterfly have long supported shared memory and bulk transfer, the Cray T3E <ref> [27] </ref> supports both shared memory and messaging styles of communication, the Stanford Dash [18] supports shared memory and prefetching, MIT Alewife [1], Fugu [20], and the Wisconsin Typhoon [25] support several variants of shared memory and messaging styles.
Reference: [28] <author> Jaswinder Pal Singh, Chris Holt, and John Hennessy. </author> <title> Load balancing and data locality in adaptive hierarchical N-body methods: Barnes-hut, fast multipole, and radiosity. </title> <journal> JPDC, </journal> <volume> 27(2), </volume> <year> 1995. </year>
Reference-contexts: The availability of machines with multiple mechanisms has led to an increasing amount of insight on the effectiveness of the various mechanisms for different applications [6] [11] <ref> [28] </ref> [30] [15] [8].
Reference: [29] <institution> Thinking Machines Corporation, Cambridge, MA. </institution> <type> CM-5 Technical Summary, </type> <month> November </month> <year> 1993. </year>
Reference-contexts: This interrupt-driven approach is the most intuitive notion of active messages, but processor interrupts can be very expensive. Message passing with polling: Active messages come in two flavors, those received via interrupt and those received via polling. In fact, on systems such as the Thinking Machines CM5 <ref> [29] </ref>, the expense of interrupts led to the predominant use of polling.
Reference: [30] <author> Steven Cameron Woo, Jaswinder Pal Singh, and John L. Hennessy. </author> <title> The performance advantages of integrating block data transfer in cache-coherent multiprocessors. </title> <booktitle> In Asplos VI, </booktitle> <pages> pages 219229, </pages> <year> 1994. </year> <month> 10 </month>
Reference-contexts: The availability of machines with multiple mechanisms has led to an increasing amount of insight on the effectiveness of the various mechanisms for different applications [6] [11] [28] <ref> [30] </ref> [15] [8]. <p> Our study shows, however, that bandwidth across the bisection of the machine may become a critical cost in supporting shared memory on modern machines. Such costs will make message passing and specialized user-level protocols [11] increasingly important as processor speeds increase. Woo et al. <ref> [30] </ref> compared bulk transfer with shared memory on simulations of the FLASH multiprocessor [13] running the SPLASH [12] suite. They found bulk transfer performance to be disappointing due to the high cost of initiating transfer and the difficulty in finding computation to overlap with the transfer.
References-found: 30

