URL: http://www-dbv.informatik.uni-bonn.de/postscript/puzicha.TR97.ps.gz
Refering-URL: 
Root-URL: 
Email: email: fjan,jbg@cs.uni-bonn.de  
Title: Multiscale Annealing for Real-Time Unsupervised Texture Segmentation IAI-TR-97-4  
Author: Rheinische Friedrich-Wilhelms-Universitat Jan Puzicha and Joachim M. Buhmann 
Date: April, 1997  
Web: WWW: http://www-dbv.cs.uni-bonn.de  
Address: Romerstrae 164, D-53117 Bonn, Germany  
Affiliation: Institut fur Informatik III,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> A. Bovik, M. Clark, and W. Geisler, </author> <title> "Multichannel texture analysis using localized spatial filters," </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> vol. 12, </volume> <year> 1990. </year>
Reference-contexts: Numerous approaches to texture segmentation have been proposed over the past decades, most of which obey a two-stage scheme: 1. A modeling stage: characteristic features are extracted from the textured input image, which range from spatial frequencies <ref> [1, 2, 3] </ref>, MRF-models [4, 5, 6, 7, 8], co-occurrence matrices [9] to wavelet coefficients [10], wave-packets [11] and fractal indices [12]. 2. In the optimization stage features are grouped into homogeneous segments by minimizing an appropriate quality measure. <p> Gabor filters essentially perform a local Fourier analysis. They are optimally localized in the sense of the fundamental uncertainty relation [24] and have empirically proven to possess excellent discrimination properties for a wide range of textures <ref> [1, 25] </ref>. The vector of Gabor coefficients at a position i is non-linearly transformed by using the absolute value of the hyperbolic tangents of the real part. Then a Gaussian smoothing filter is applied and the resulting feature vectors are rescaled to zero mean and unit variance.
Reference: [2] <author> A. Jain and F. Farrokhnia, </author> <title> "Unsupervised texture segmentation using Gabor filters," </title> <journal> Pattern Recognition, </journal> <volume> vol. 24, no. 12, </volume> <pages> pp. 1167-1186, </pages> <year> 1991. </year>
Reference-contexts: Numerous approaches to texture segmentation have been proposed over the past decades, most of which obey a two-stage scheme: 1. A modeling stage: characteristic features are extracted from the textured input image, which range from spatial frequencies <ref> [1, 2, 3] </ref>, MRF-models [4, 5, 6, 7, 8], co-occurrence matrices [9] to wavelet coefficients [10], wave-packets [11] and fractal indices [12]. 2. In the optimization stage features are grouped into homogeneous segments by minimizing an appropriate quality measure. <p> In the optimization stage features are grouped into homogeneous segments by minimizing an appropriate quality measure. This is most widely achieved by a few types of clustering cost functions <ref> [2, 5, 6, 7, 8, 12, 11, 3] </ref>. Occasionally additional a-priori information is used in spatial interaction models [9, 13]. <p> In this contribution we focus solely on the optimization stage while adopting different feature extraction methods from the literature <ref> [2, 6, 3] </ref>. More specifically, we present a novel real-time approach to the global optimization of the quality measures commonly used by most texture segmentation models. It relies on the concept of multiscale optimization [14]. <p> There exist basically two types of feature representations: (a) representing the local characteristic of each image site by a feature vector <ref> [2, 5, 6, 7, 8, 12, 11] </ref> and (b) representing the pairwise relation between sites by similarity values [9, 15, 16]. There are two corresponding types of cost functions: (a) central or K-means clustering and (b) pairwise clustering. <p> All methods extract multi-resolution features to cover a broad range of different texture scales. 2.1 Gabor Feature Clustering As an example for a frequency-based feature extraction we follow the approach of Jain & Farrokhnia <ref> [2] </ref>, which we refer to as Gabor Feature Clustering (GFC). The input image is J. Puzicha, J.M.
Reference: [3] <author> T. Hofmann, J. Puzicha, and J. Buhmann, </author> <title> "Deterministic annealing for unsupervised texture segmentation," </title> <booktitle> in Proceedings of the International Workshop on Energy Minimization Methods in Computer Vision and Pattern Recognition, </booktitle> <month> May </month> <year> 1997. </year>
Reference-contexts: Numerous approaches to texture segmentation have been proposed over the past decades, most of which obey a two-stage scheme: 1. A modeling stage: characteristic features are extracted from the textured input image, which range from spatial frequencies <ref> [1, 2, 3] </ref>, MRF-models [4, 5, 6, 7, 8], co-occurrence matrices [9] to wavelet coefficients [10], wave-packets [11] and fractal indices [12]. 2. In the optimization stage features are grouped into homogeneous segments by minimizing an appropriate quality measure. <p> In the optimization stage features are grouped into homogeneous segments by minimizing an appropriate quality measure. This is most widely achieved by a few types of clustering cost functions <ref> [2, 5, 6, 7, 8, 12, 11, 3] </ref>. Occasionally additional a-priori information is used in spatial interaction models [9, 13]. <p> In this contribution we focus solely on the optimization stage while adopting different feature extraction methods from the literature <ref> [2, 6, 3] </ref>. More specifically, we present a novel real-time approach to the global optimization of the quality measures commonly used by most texture segmentation models. It relies on the concept of multiscale optimization [14]. <p> Indeed it is often more successful to assume an underlying probability for each texture and to apply non-parametric statistical tests to measure local texture similarity [9, 15]. Here we follow the recent approach of Hofmann et al. <ref> [26, 3] </ref> which is based on the modulus applied to a Gabor filter representation. For each site the empirical distribution of coefficients in a surrounding (Gabor-channel specific) window is determined. <p> In addition, DA has been developed for central [36, 37] and pairwise clustering <ref> [16, 3] </ref>. In Deterministic Annealing (DA) approaches the stochasticity of SA is incorporated in a probabilistic formulation of the optimization problem and a deterministic optimization is performed over a probabilistic state space Q (M). The minimization is carried out over the J. Puzicha, J.M. <p> This is due to two reasons. First, in designing a proper annealing schedule there is a trade-off between run-time and segmentation quality. As we are concerned with real-time optimization, we used fast annealing schedules for DA. Note that in the original work on PDC-based texture segmentation <ref> [26, 3] </ref> better results in terms of segmentation quality are reported for a similar bench-mark. The second reason stems from the implicit a-priori assumption of large regions with homogeneous texture made in multiscale optimization, which is not necessarily reflected by the cost-function.
Reference: [4] <author> H. Derin and W. Cole, </author> <title> "Segmentation of textured images using Gibbs random fields," Computer, Vision, </title> <journal> Graphics, and Image Processing, </journal> <volume> vol. 35, no. 1, </volume> <pages> pp. 72-98, </pages> <year> 1986. </year>
Reference-contexts: Numerous approaches to texture segmentation have been proposed over the past decades, most of which obey a two-stage scheme: 1. A modeling stage: characteristic features are extracted from the textured input image, which range from spatial frequencies [1, 2, 3], MRF-models <ref> [4, 5, 6, 7, 8] </ref>, co-occurrence matrices [9] to wavelet coefficients [10], wave-packets [11] and fractal indices [12]. 2. In the optimization stage features are grouped into homogeneous segments by minimizing an appropriate quality measure. <p> the K-means clustering criterion with an Euclidean norm H cc (M; y) = -=1 i=1 where the y - 2 IR d are the cluster prototypes to be determined by minimization. 2.2 Multi-resolution Autoregressive Models As an example for the popular Markov Random Field (MRF) modeling approach to texture segmentation <ref> [4, 5, 7, 8] </ref> we follow the work of Mao and Jain [6] using multi-resolution simultaneous autoregressive (MSAR) models.
Reference: [5] <author> B. Manjunath and R. Chellappa, </author> <title> "Unsupervised texture segmentation using Markov random field models," </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> vol. 13, </volume> <pages> pp. 478-482, </pages> <year> 1991. </year> <editor> J. Puzicha, J.M. </editor> <title> Buhmann: Real-Time Texture Segmentation 29 </title>
Reference-contexts: Numerous approaches to texture segmentation have been proposed over the past decades, most of which obey a two-stage scheme: 1. A modeling stage: characteristic features are extracted from the textured input image, which range from spatial frequencies [1, 2, 3], MRF-models <ref> [4, 5, 6, 7, 8] </ref>, co-occurrence matrices [9] to wavelet coefficients [10], wave-packets [11] and fractal indices [12]. 2. In the optimization stage features are grouped into homogeneous segments by minimizing an appropriate quality measure. <p> In the optimization stage features are grouped into homogeneous segments by minimizing an appropriate quality measure. This is most widely achieved by a few types of clustering cost functions <ref> [2, 5, 6, 7, 8, 12, 11, 3] </ref>. Occasionally additional a-priori information is used in spatial interaction models [9, 13]. <p> There exist basically two types of feature representations: (a) representing the local characteristic of each image site by a feature vector <ref> [2, 5, 6, 7, 8, 12, 11] </ref> and (b) representing the pairwise relation between sites by similarity values [9, 15, 16]. There are two corresponding types of cost functions: (a) central or K-means clustering and (b) pairwise clustering. <p> the K-means clustering criterion with an Euclidean norm H cc (M; y) = -=1 i=1 where the y - 2 IR d are the cluster prototypes to be determined by minimization. 2.2 Multi-resolution Autoregressive Models As an example for the popular Markov Random Field (MRF) modeling approach to texture segmentation <ref> [4, 5, 7, 8] </ref> we follow the work of Mao and Jain [6] using multi-resolution simultaneous autoregressive (MSAR) models.
Reference: [6] <author> J. Mao and A. Jain, </author> <title> "Texture classification and segmentation using multiresolution simultaneous autoregressive models," </title> <journal> Pattern Recognition, </journal> <volume> vol. 25, </volume> <pages> pp. 173-188, </pages> <year> 1992. </year>
Reference-contexts: Numerous approaches to texture segmentation have been proposed over the past decades, most of which obey a two-stage scheme: 1. A modeling stage: characteristic features are extracted from the textured input image, which range from spatial frequencies [1, 2, 3], MRF-models <ref> [4, 5, 6, 7, 8] </ref>, co-occurrence matrices [9] to wavelet coefficients [10], wave-packets [11] and fractal indices [12]. 2. In the optimization stage features are grouped into homogeneous segments by minimizing an appropriate quality measure. <p> In the optimization stage features are grouped into homogeneous segments by minimizing an appropriate quality measure. This is most widely achieved by a few types of clustering cost functions <ref> [2, 5, 6, 7, 8, 12, 11, 3] </ref>. Occasionally additional a-priori information is used in spatial interaction models [9, 13]. <p> In this contribution we focus solely on the optimization stage while adopting different feature extraction methods from the literature <ref> [2, 6, 3] </ref>. More specifically, we present a novel real-time approach to the global optimization of the quality measures commonly used by most texture segmentation models. It relies on the concept of multiscale optimization [14]. <p> There exist basically two types of feature representations: (a) representing the local characteristic of each image site by a feature vector <ref> [2, 5, 6, 7, 8, 12, 11] </ref> and (b) representing the pairwise relation between sites by similarity values [9, 15, 16]. There are two corresponding types of cost functions: (a) central or K-means clustering and (b) pairwise clustering. <p> = -=1 i=1 where the y - 2 IR d are the cluster prototypes to be determined by minimization. 2.2 Multi-resolution Autoregressive Models As an example for the popular Markov Random Field (MRF) modeling approach to texture segmentation [4, 5, 7, 8] we follow the work of Mao and Jain <ref> [6] </ref> using multi-resolution simultaneous autoregressive (MSAR) models. <p> According to <ref> [6] </ref> symmetric J. Puzicha, J.M. Buhmann: Real-Time Texture Segmentation 6 models are independently estimated at 3 different neighborhoods of size 8 each.
Reference: [7] <author> C. S. Won and H. Derin, </author> <title> "Unsupervised segmentation of noisy and textured images using Markov random fields," CVGIP: Graphical Models and Image Processing, </title> <journal> vol. </journal> <volume> 54, </volume> <pages> pp. 308-328, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Numerous approaches to texture segmentation have been proposed over the past decades, most of which obey a two-stage scheme: 1. A modeling stage: characteristic features are extracted from the textured input image, which range from spatial frequencies [1, 2, 3], MRF-models <ref> [4, 5, 6, 7, 8] </ref>, co-occurrence matrices [9] to wavelet coefficients [10], wave-packets [11] and fractal indices [12]. 2. In the optimization stage features are grouped into homogeneous segments by minimizing an appropriate quality measure. <p> In the optimization stage features are grouped into homogeneous segments by minimizing an appropriate quality measure. This is most widely achieved by a few types of clustering cost functions <ref> [2, 5, 6, 7, 8, 12, 11, 3] </ref>. Occasionally additional a-priori information is used in spatial interaction models [9, 13]. <p> There exist basically two types of feature representations: (a) representing the local characteristic of each image site by a feature vector <ref> [2, 5, 6, 7, 8, 12, 11] </ref> and (b) representing the pairwise relation between sites by similarity values [9, 15, 16]. There are two corresponding types of cost functions: (a) central or K-means clustering and (b) pairwise clustering. <p> the K-means clustering criterion with an Euclidean norm H cc (M; y) = -=1 i=1 where the y - 2 IR d are the cluster prototypes to be determined by minimization. 2.2 Multi-resolution Autoregressive Models As an example for the popular Markov Random Field (MRF) modeling approach to texture segmentation <ref> [4, 5, 7, 8] </ref> we follow the work of Mao and Jain [6] using multi-resolution simultaneous autoregressive (MSAR) models.
Reference: [8] <author> D. Panjwani and G. Healey, </author> <title> "Markov random field models for unsupervised segmentation of textured color images," </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> vol. 17, no. 10, </volume> <pages> pp. 939-954, </pages> <year> 1995. </year>
Reference-contexts: Numerous approaches to texture segmentation have been proposed over the past decades, most of which obey a two-stage scheme: 1. A modeling stage: characteristic features are extracted from the textured input image, which range from spatial frequencies [1, 2, 3], MRF-models <ref> [4, 5, 6, 7, 8] </ref>, co-occurrence matrices [9] to wavelet coefficients [10], wave-packets [11] and fractal indices [12]. 2. In the optimization stage features are grouped into homogeneous segments by minimizing an appropriate quality measure. <p> In the optimization stage features are grouped into homogeneous segments by minimizing an appropriate quality measure. This is most widely achieved by a few types of clustering cost functions <ref> [2, 5, 6, 7, 8, 12, 11, 3] </ref>. Occasionally additional a-priori information is used in spatial interaction models [9, 13]. <p> There exist basically two types of feature representations: (a) representing the local characteristic of each image site by a feature vector <ref> [2, 5, 6, 7, 8, 12, 11] </ref> and (b) representing the pairwise relation between sites by similarity values [9, 15, 16]. There are two corresponding types of cost functions: (a) central or K-means clustering and (b) pairwise clustering. <p> the K-means clustering criterion with an Euclidean norm H cc (M; y) = -=1 i=1 where the y - 2 IR d are the cluster prototypes to be determined by minimization. 2.2 Multi-resolution Autoregressive Models As an example for the popular Markov Random Field (MRF) modeling approach to texture segmentation <ref> [4, 5, 7, 8] </ref> we follow the work of Mao and Jain [6] using multi-resolution simultaneous autoregressive (MSAR) models.
Reference: [9] <author> D. Geman, S. Geman, C. Graffigne, and P. Dong, </author> <title> "Boundary detection by constrained optimization," </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> vol. 12, no. 7, </volume> <pages> pp. 609-628, </pages> <year> 1990. </year>
Reference-contexts: Numerous approaches to texture segmentation have been proposed over the past decades, most of which obey a two-stage scheme: 1. A modeling stage: characteristic features are extracted from the textured input image, which range from spatial frequencies [1, 2, 3], MRF-models [4, 5, 6, 7, 8], co-occurrence matrices <ref> [9] </ref> to wavelet coefficients [10], wave-packets [11] and fractal indices [12]. 2. In the optimization stage features are grouped into homogeneous segments by minimizing an appropriate quality measure. This is most widely achieved by a few types of clustering cost functions [2, 5, 6, 7, 8, 12, 11, 3]. <p> In the optimization stage features are grouped into homogeneous segments by minimizing an appropriate quality measure. This is most widely achieved by a few types of clustering cost functions [2, 5, 6, 7, 8, 12, 11, 3]. Occasionally additional a-priori information is used in spatial interaction models <ref> [9, 13] </ref>. The strict separation of the modeling and the optimization stage is important in that it enables us to evaluate the quality and speed of optimization procedures independently from the feature extraction stage. <p> There exist basically two types of feature representations: (a) representing the local characteristic of each image site by a feature vector [2, 5, 6, 7, 8, 12, 11] and (b) representing the pairwise relation between sites by similarity values <ref> [9, 15, 16] </ref>. There are two corresponding types of cost functions: (a) central or K-means clustering and (b) pairwise clustering. <p> Indeed it is often more successful to assume an underlying probability for each texture and to apply non-parametric statistical tests to measure local texture similarity <ref> [9, 15] </ref>. Here we follow the recent approach of Hofmann et al. [26, 3] which is based on the modulus applied to a Gabor filter representation. For each site the empirical distribution of coefficients in a surrounding (Gabor-channel specific) window is determined.
Reference: [10] <author> E. Salari and Z. Ling, </author> <title> "Texture segmentation using hierarchical wavelet decomposition," </title> <journal> Pattern Recognition, </journal> <volume> vol. 28, no. 12, </volume> <pages> pp. 1819-1824, </pages> <year> 1995. </year>
Reference-contexts: A modeling stage: characteristic features are extracted from the textured input image, which range from spatial frequencies [1, 2, 3], MRF-models [4, 5, 6, 7, 8], co-occurrence matrices [9] to wavelet coefficients <ref> [10] </ref>, wave-packets [11] and fractal indices [12]. 2. In the optimization stage features are grouped into homogeneous segments by minimizing an appropriate quality measure. This is most widely achieved by a few types of clustering cost functions [2, 5, 6, 7, 8, 12, 11, 3].
Reference: [11] <author> A. Laine and J. Fan, </author> <title> "Frame representations for texture segmentation," </title> <journal> IEEE Transactions on Image Processing, </journal> <volume> vol. 5, no. 5, </volume> <pages> pp. 771-779, </pages> <year> 1996. </year>
Reference-contexts: A modeling stage: characteristic features are extracted from the textured input image, which range from spatial frequencies [1, 2, 3], MRF-models [4, 5, 6, 7, 8], co-occurrence matrices [9] to wavelet coefficients [10], wave-packets <ref> [11] </ref> and fractal indices [12]. 2. In the optimization stage features are grouped into homogeneous segments by minimizing an appropriate quality measure. This is most widely achieved by a few types of clustering cost functions [2, 5, 6, 7, 8, 12, 11, 3]. <p> In the optimization stage features are grouped into homogeneous segments by minimizing an appropriate quality measure. This is most widely achieved by a few types of clustering cost functions <ref> [2, 5, 6, 7, 8, 12, 11, 3] </ref>. Occasionally additional a-priori information is used in spatial interaction models [9, 13]. <p> There exist basically two types of feature representations: (a) representing the local characteristic of each image site by a feature vector <ref> [2, 5, 6, 7, 8, 12, 11] </ref> and (b) representing the pairwise relation between sites by similarity values [9, 15, 16]. There are two corresponding types of cost functions: (a) central or K-means clustering and (b) pairwise clustering.
Reference: [12] <author> B. Chaudhuri and N. Sarkar, </author> <title> "Texture segmentation using fractal dimension," </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> vol. 17, no. 1, </volume> <pages> pp. 72-77, </pages> <year> 1995. </year>
Reference-contexts: A modeling stage: characteristic features are extracted from the textured input image, which range from spatial frequencies [1, 2, 3], MRF-models [4, 5, 6, 7, 8], co-occurrence matrices [9] to wavelet coefficients [10], wave-packets [11] and fractal indices <ref> [12] </ref>. 2. In the optimization stage features are grouped into homogeneous segments by minimizing an appropriate quality measure. This is most widely achieved by a few types of clustering cost functions [2, 5, 6, 7, 8, 12, 11, 3]. <p> In the optimization stage features are grouped into homogeneous segments by minimizing an appropriate quality measure. This is most widely achieved by a few types of clustering cost functions <ref> [2, 5, 6, 7, 8, 12, 11, 3] </ref>. Occasionally additional a-priori information is used in spatial interaction models [9, 13]. <p> There exist basically two types of feature representations: (a) representing the local characteristic of each image site by a feature vector <ref> [2, 5, 6, 7, 8, 12, 11] </ref> and (b) representing the pairwise relation between sites by similarity values [9, 15, 16]. There are two corresponding types of cost functions: (a) central or K-means clustering and (b) pairwise clustering.
Reference: [13] <author> T.-S. Lee, D. Mumford, and A. Yuille, </author> <title> "Texture segmentation by minimizing vector-valued energy-functionals: The coupled membrane model," </title> <booktitle> in Proceedings of the Second European Conference on Computer Vision (ECCV) (G. Sandini, </booktitle> <publisher> ed.), </publisher> <pages> pp. 165-183, </pages> <year> 1992. </year>
Reference-contexts: In the optimization stage features are grouped into homogeneous segments by minimizing an appropriate quality measure. This is most widely achieved by a few types of clustering cost functions [2, 5, 6, 7, 8, 12, 11, 3]. Occasionally additional a-priori information is used in spatial interaction models <ref> [9, 13] </ref>. The strict separation of the modeling and the optimization stage is important in that it enables us to evaluate the quality and speed of optimization procedures independently from the feature extraction stage.
Reference: [14] <author> F. Heitz, P. Perez, and P. Bouthemy, </author> <title> "Multiscale minimization of global energy functions in some visual recovery problems," CVGIP: </title> <booktitle> Image Understanding, </booktitle> <volume> vol. 59, no. 1, </volume> <pages> pp. 125-134, </pages> <year> 1994. </year>
Reference-contexts: More specifically, we present a novel real-time approach to the global optimization of the quality measures commonly used by most texture segmentation models. It relies on the concept of multiscale optimization <ref> [14] </ref>. The available topological information is used to greatly accelerate computation by exploiting the fact, that nearby locations in the image have high probability to belong to the same segment. The optimization problem is redefined on different scales such that the original J. Puzicha, J.M. <p> We like to stress that for both clustering variants in contrast to <ref> [14] </ref> cost functions of uniform functional form are obtained at all resolutions, enabling highly efficient optimization. Several optimization approaches relying on coarse versions of a cost function have been proposed in the past. In the remaining introduction we compare the most influential ones with multiscale optimization. <p> Note that there is no guarantee that coarse grid solutions are good initializations, as different cost functions are optimized and interactions between variables are often severely resolution dependent. Mul-tiscale optimization significantly outperformed semantic multi-resolution methods in visual reconstruction and motion estimation problems both in accuracy and speed <ref> [14, 23] </ref>. J. Puzicha, J.M. Buhmann: Real-Time Texture Segmentation 4 2 Optimization for Texture Segmentation In this section we summarize three optimization approaches to unsupervised texture segmentation presented in the literature. In the sequel we then demonstrate how to significantly accelerate these algorithms by applying multiscale annealing techniques. <p> DA can be understood as a continuation method [38], as the (unique) minimum at high temperature is tracked while gradually lowering (`annealing') T . For T = 0 again the local ICM-algorithm is obtained, which has already been used by Heitz et al. <ref> [14] </ref> for multiscale optimization. For the central clustering cost function (3) we obtain g i= kx i y -k 2 , which is independent of Q. Thus the hM i- i are computed without iteration in one step. <p> A segmentation resolution of 64x64 pixels is therefore sufficient for most purposes. The second important question is concerned with the benefits of multiscale techniques J. Puzicha, J.M. Buhmann: Real-Time Texture Segmentation 22 w.r.t. quality. Heitz et al. <ref> [14] </ref> reported a significant smoothing of the landscape by multiscale optimization and thus used multiscale ICM in their application. As seen from Tab. 2 M-ICM performs slightly worse for central clustering.
Reference: [15] <author> T. Ojala and M. Pietikainen, </author> <title> "Unsupervised texture segmentation using feature distributions," </title> <type> Tech. Rep. </type> <institution> CAR-TR-837, Center for Automation Research, University of Maryland, </institution> <year> 1996. </year> <editor> J. Puzicha, J.M. </editor> <title> Buhmann: Real-Time Texture Segmentation 30 </title>
Reference-contexts: There exist basically two types of feature representations: (a) representing the local characteristic of each image site by a feature vector [2, 5, 6, 7, 8, 12, 11] and (b) representing the pairwise relation between sites by similarity values <ref> [9, 15, 16] </ref>. There are two corresponding types of cost functions: (a) central or K-means clustering and (b) pairwise clustering. <p> Indeed it is often more successful to assume an underlying probability for each texture and to apply non-parametric statistical tests to measure local texture similarity <ref> [9, 15] </ref>. Here we follow the recent approach of Hofmann et al. [26, 3] which is based on the modulus applied to a Gabor filter representation. For each site the empirical distribution of coefficients in a surrounding (Gabor-channel specific) window is determined.
Reference: [16] <author> T. Hofmann and J. Buhmann, </author> <title> "Pairwise data clustering by deterministic annealing," </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> vol. 19, no. 1, </volume> <year> 1997. </year>
Reference-contexts: There exist basically two types of feature representations: (a) representing the local characteristic of each image site by a feature vector [2, 5, 6, 7, 8, 12, 11] and (b) representing the pairwise relation between sites by similarity values <ref> [9, 15, 16] </ref>. There are two corresponding types of cost functions: (a) central or K-means clustering and (b) pairwise clustering. <p> In addition, DA has been developed for central [36, 37] and pairwise clustering <ref> [16, 3] </ref>. In Deterministic Annealing (DA) approaches the stochasticity of SA is incorporated in a probabilistic formulation of the optimization problem and a deterministic optimization is performed over a probabilistic state space Q (M). The minimization is carried out over the J. Puzicha, J.M.
Reference: [17] <author> W. Briggs and S. McCormick, </author> <title> "Introduction," in Multigrid Methods (S. </title> <editor> McCormick, ed.), </editor> <booktitle> Frontiers in Applied Mathematics, ch. </booktitle> <volume> 1, </volume> <pages> pp. 1-31, </pages> <institution> Society for Industrial and Applied Mathematics, </institution> <year> 1987. </year>
Reference-contexts: Several optimization approaches relying on coarse versions of a cost function have been proposed in the past. In the remaining introduction we compare the most influential ones with multiscale optimization. Technically similar are multi-grid methods, which have first been developed for the solution of partial differential equations <ref> [17] </ref> and have since then been adopted to a broad range of optimization problems with locally interacting variables including image processing tasks [18, 19, 20]. Multi-grid methods rely on incremental coarse-grid corrections and therefore on continuous optimization variables. Similar in spirit but technically different are renormalization group approaches [21].
Reference: [18] <author> D. Terzopoulos, </author> <title> "Image analysis using multigrid relaxation methods," </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> vol. 8, no. 2, </volume> <pages> pp. 129-139, </pages> <year> 1986. </year>
Reference-contexts: Technically similar are multi-grid methods, which have first been developed for the solution of partial differential equations [17] and have since then been adopted to a broad range of optimization problems with locally interacting variables including image processing tasks <ref> [18, 19, 20] </ref>. Multi-grid methods rely on incremental coarse-grid corrections and therefore on continuous optimization variables. Similar in spirit but technically different are renormalization group approaches [21]. For discrete problems multiscale optimization techniques are better suited.
Reference: [19] <author> E. Mjolsness, C. Garrett, and W. Miranker, </author> <title> "Multiscale optimization in neural nets," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 2, no. 2, </volume> <pages> pp. 263-273, </pages> <year> 1991. </year>
Reference-contexts: Technically similar are multi-grid methods, which have first been developed for the solution of partial differential equations [17] and have since then been adopted to a broad range of optimization problems with locally interacting variables including image processing tasks <ref> [18, 19, 20] </ref>. Multi-grid methods rely on incremental coarse-grid corrections and therefore on continuous optimization variables. Similar in spirit but technically different are renormalization group approaches [21]. For discrete problems multiscale optimization techniques are better suited.
Reference: [20] <author> K. Zhou and C. Rushforth, </author> <title> "Image restoration using multigrid methods," </title> <journal> Applied Optics, </journal> <volume> vol. 30, no. 20, </volume> <pages> pp. 2906-2912, </pages> <year> 1991. </year>
Reference-contexts: Technically similar are multi-grid methods, which have first been developed for the solution of partial differential equations [17] and have since then been adopted to a broad range of optimization problems with locally interacting variables including image processing tasks <ref> [18, 19, 20] </ref>. Multi-grid methods rely on incremental coarse-grid corrections and therefore on continuous optimization variables. Similar in spirit but technically different are renormalization group approaches [21]. For discrete problems multiscale optimization techniques are better suited.
Reference: [21] <author> B. Gidas, </author> <title> "A renormalisation approach to image processing problems," </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> vol. 11, </volume> <pages> pp. 164-180, </pages> <year> 1989. </year>
Reference-contexts: Multi-grid methods rely on incremental coarse-grid corrections and therefore on continuous optimization variables. Similar in spirit but technically different are renormalization group approaches <ref> [21] </ref>. For discrete problems multiscale optimization techniques are better suited. As texture segmentation is essentially a discrete assignment problem, we advocate the use of multiscale optimization methods.
Reference: [22] <author> C. Bouman and B. Liu, </author> <title> "Multiple resolution segmentation of textured images," </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> vol. 13, </volume> <pages> pp. 99-113, </pages> <year> 1991. </year>
Reference-contexts: Most multi-resolution techniques developed for image processing tasks are semantic multi-resolution techniques in that they adopt the same cost function at all scales and apply a coarse-to-fine optimization. An example in the texture segmentation context is the approach of Bouman <ref> [22] </ref>, who used Gaussian autoregressive features extracted on multiple image scales to design a coarse-to-fine optimization strategy. Note that there is no guarantee that coarse grid solutions are good initializations, as different cost functions are optimized and interactions between variables are often severely resolution dependent.
Reference: [23] <author> C. Stiller, </author> <title> "Object-based estimation of dense motion fields," </title> <journal> IEEE Transactions on Image Processing, </journal> <volume> vol. 6, no. 2, </volume> <pages> pp. 234-250, </pages> <year> 1997. </year>
Reference-contexts: Note that there is no guarantee that coarse grid solutions are good initializations, as different cost functions are optimized and interactions between variables are often severely resolution dependent. Mul-tiscale optimization significantly outperformed semantic multi-resolution methods in visual reconstruction and motion estimation problems both in accuracy and speed <ref> [14, 23] </ref>. J. Puzicha, J.M. Buhmann: Real-Time Texture Segmentation 4 2 Optimization for Texture Segmentation In this section we summarize three optimization approaches to unsupervised texture segmentation presented in the literature. In the sequel we then demonstrate how to significantly accelerate these algorithms by applying multiscale annealing techniques.
Reference: [24] <author> J. Daugman, </author> <title> "Uncertainty relation for resolution in space, spatial frequency, and orientation optimized by two-dimensional visual cortical filters," </title> <journal> Journal of the Optical Society Am. A, </journal> <volume> vol. 2, no. 7, </volume> <pages> pp. 1160-1169, </pages> <year> 1985. </year>
Reference-contexts: Gabor filters essentially perform a local Fourier analysis. They are optimally localized in the sense of the fundamental uncertainty relation <ref> [24] </ref> and have empirically proven to possess excellent discrimination properties for a wide range of textures [1, 25]. The vector of Gabor coefficients at a position i is non-linearly transformed by using the absolute value of the hyperbolic tangents of the real part.
Reference: [25] <author> I. Fogel and D. Sagi, </author> <title> "Gabor filters as texture discriminators," </title> <journal> Biological Cybernetics, </journal> <volume> vol. 61, </volume> <pages> pp. 103-113, </pages> <year> 1989. </year>
Reference-contexts: Gabor filters essentially perform a local Fourier analysis. They are optimally localized in the sense of the fundamental uncertainty relation [24] and have empirically proven to possess excellent discrimination properties for a wide range of textures <ref> [1, 25] </ref>. The vector of Gabor coefficients at a position i is non-linearly transformed by using the absolute value of the hyperbolic tangents of the real part. Then a Gaussian smoothing filter is applied and the resulting feature vectors are rescaled to zero mean and unit variance.
Reference: [26] <author> T. Hofmann, J. Puzicha, and J. Buhmann, </author> <title> "A deterministic annealing framework for textured image segmentation," </title> <type> Tech. Rep. </type> <institution> IAI-TR-96-2, Institut fur Informatik III, </institution> <year> 1996. </year>
Reference-contexts: Indeed it is often more successful to assume an underlying probability for each texture and to apply non-parametric statistical tests to measure local texture similarity [9, 15]. Here we follow the recent approach of Hofmann et al. <ref> [26, 3] </ref> which is based on the modulus applied to a Gabor filter representation. For each site the empirical distribution of coefficients in a surrounding (Gabor-channel specific) window is determined. <p> Thus F T can be understood as a coarse version of H while T takes the role of a scale parameter. Each minimum of H has a corresponding minimum of F 0 . By the fundamental relationship <ref> [26] </ref> hM i- i Q = P ; (27) which is valid for distributions Q minimizing (26), a transcendental system of equations is obtained, which is solved by an asynchronous, iterative and convergent fixed-point relaxation. <p> This is due to two reasons. First, in designing a proper annealing schedule there is a trade-off between run-time and segmentation quality. As we are concerned with real-time optimization, we used fast annealing schedules for DA. Note that in the original work on PDC-based texture segmentation <ref> [26, 3] </ref> better results in terms of segmentation quality are reported for a similar bench-mark. The second reason stems from the implicit a-priori assumption of large regions with homogeneous texture made in multiscale optimization, which is not necessarily reflected by the cost-function.
Reference: [27] <author> S. Kirkpatrick, C. Gelatt, and M. Vecchi, </author> <title> "Optimization by simulated annealing," </title> <journal> Science, </journal> <volume> vol. 220, no. 4598, </volume> <pages> pp. 671-680, </pages> <year> 1983. </year> <editor> J. Puzicha, J.M. </editor> <title> Buhmann: Real-Time Texture Segmentation 31 </title>
Reference-contexts: The derivation for the denominator is along the same line. fl J. Puzicha, J.M. Buhmann: Real-Time Texture Segmentation 12 3.4 Deterministic Annealing for Clustering Simulated Annealing (SA) techniques <ref> [27, 28] </ref> have been developed as a general random optimization strategy primarily for combinatorial optimization problems.
Reference: [28] <author> S. Geman and D. Geman, </author> <title> "Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images," </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> vol. 6, no. 6, </volume> <pages> pp. 721-741, </pages> <year> 1984. </year>
Reference-contexts: The derivation for the denominator is along the same line. fl J. Puzicha, J.M. Buhmann: Real-Time Texture Segmentation 12 3.4 Deterministic Annealing for Clustering Simulated Annealing (SA) techniques <ref> [27, 28] </ref> have been developed as a general random optimization strategy primarily for combinatorial optimization problems. <p> Denote by M i- the matrix obtained by replacing the i-th row of M with the unit vector e - and let g i= H (M i). An efficient Monte Carlo algorithm is defined by the Gibbs-Sampler <ref> [28] </ref>, which samples from the conditional probability distribution spanned by site x i for fixed assignments of sites x j ; j 6= i: P M i- exp (g i-=T ) ff exp (g iff =T ) In the limit T ! 0 the Iterative Conditional Mode (ICM) algorithm is obtained, <p> We now turn to the development of Gibbs-Sampling equations for the general multi resolution cost function (19). A straightforward implementation of the Gibbs sampler would partition the cost function into a sum of clique potentials and at each step recalculate all potentials of cliques to which the site belongs <ref> [28] </ref>. For (19) this procedure is inefficient, since the assignments enter in the denominator.
Reference: [29] <author> C. Peterson and B. Soderberg, </author> <title> "A new method for mapping optimization problems onto neural networks," </title> <journal> International Journal of Neural Systems, </journal> <volume> vol. 1, no. 1, </volume> <pages> pp. 3-22, </pages> <year> 1989. </year>
Reference-contexts: Deterministic Annealing (DA) is a more efficient global optimization procedure suitable for real-time optimization, which has been successfully applied to a variety of combinatorial optimization and computer vision problems, e.g., the traveling salesman problem <ref> [29] </ref>, graph partitioning [30], quadratic assignment and graph matching [31, 32], surface reconstruction [33], image enhancement [34] and edge detection [35]. In addition, DA has been developed for central [36, 37] and pairwise clustering [16, 3].
Reference: [30] <author> D. van den Bout and T. Miller, </author> <title> "Graph partitioning using annealed neural networks," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 1, no. 2, </volume> <pages> pp. 192-203, </pages> <year> 1990. </year>
Reference-contexts: Deterministic Annealing (DA) is a more efficient global optimization procedure suitable for real-time optimization, which has been successfully applied to a variety of combinatorial optimization and computer vision problems, e.g., the traveling salesman problem [29], graph partitioning <ref> [30] </ref>, quadratic assignment and graph matching [31, 32], surface reconstruction [33], image enhancement [34] and edge detection [35]. In addition, DA has been developed for central [36, 37] and pairwise clustering [16, 3].
Reference: [31] <author> A. Yuille, </author> <title> "Generalized deformable models, statistical physics, and matching problems," </title> <journal> Neural Computation, </journal> <volume> vol. 2, </volume> <pages> pp. 1-24, </pages> <year> 1990. </year>
Reference-contexts: Deterministic Annealing (DA) is a more efficient global optimization procedure suitable for real-time optimization, which has been successfully applied to a variety of combinatorial optimization and computer vision problems, e.g., the traveling salesman problem [29], graph partitioning [30], quadratic assignment and graph matching <ref> [31, 32] </ref>, surface reconstruction [33], image enhancement [34] and edge detection [35]. In addition, DA has been developed for central [36, 37] and pairwise clustering [16, 3].
Reference: [32] <author> S. Gold and A. Rangarajan, </author> <title> "A graduated assignment algorithm for graph matching," </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> vol. 18, no. 4, </volume> <pages> pp. 377-388, </pages> <year> 1996. </year>
Reference-contexts: Deterministic Annealing (DA) is a more efficient global optimization procedure suitable for real-time optimization, which has been successfully applied to a variety of combinatorial optimization and computer vision problems, e.g., the traveling salesman problem [29], graph partitioning [30], quadratic assignment and graph matching <ref> [31, 32] </ref>, surface reconstruction [33], image enhancement [34] and edge detection [35]. In addition, DA has been developed for central [36, 37] and pairwise clustering [16, 3].
Reference: [33] <author> D. Geiger and F. Girosi, </author> <title> "Parallel and deterministic algorithms from MRF's: Surface reconstruction," </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <pages> pp. 401-412, </pages> <year> 1991. </year>
Reference-contexts: Deterministic Annealing (DA) is a more efficient global optimization procedure suitable for real-time optimization, which has been successfully applied to a variety of combinatorial optimization and computer vision problems, e.g., the traveling salesman problem [29], graph partitioning [30], quadratic assignment and graph matching [31, 32], surface reconstruction <ref> [33] </ref>, image enhancement [34] and edge detection [35]. In addition, DA has been developed for central [36, 37] and pairwise clustering [16, 3].
Reference: [34] <author> J. Zhang, </author> <title> "The mean field theory in EM procedures for blind Markov random fields," </title> <journal> IEEE Transactions on Image Processing, </journal> <volume> vol. 2, no. 1, </volume> <pages> pp. 27-40, </pages> <year> 1993. </year>
Reference-contexts: Deterministic Annealing (DA) is a more efficient global optimization procedure suitable for real-time optimization, which has been successfully applied to a variety of combinatorial optimization and computer vision problems, e.g., the traveling salesman problem [29], graph partitioning [30], quadratic assignment and graph matching [31, 32], surface reconstruction [33], image enhancement <ref> [34] </ref> and edge detection [35]. In addition, DA has been developed for central [36, 37] and pairwise clustering [16, 3].
Reference: [35] <author> J. Zerubia and R. Chellappa, </author> <title> "Mean field annealing using compound Gauss-Markov random fields for edge detection and image estimation," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 4, no. 4, </volume> <pages> pp. 703-709, </pages> <year> 1993. </year>
Reference-contexts: a more efficient global optimization procedure suitable for real-time optimization, which has been successfully applied to a variety of combinatorial optimization and computer vision problems, e.g., the traveling salesman problem [29], graph partitioning [30], quadratic assignment and graph matching [31, 32], surface reconstruction [33], image enhancement [34] and edge detection <ref> [35] </ref>. In addition, DA has been developed for central [36, 37] and pairwise clustering [16, 3]. In Deterministic Annealing (DA) approaches the stochasticity of SA is incorporated in a probabilistic formulation of the optimization problem and a deterministic optimization is performed over a probabilistic state space Q (M).
Reference: [36] <author> K. Rose, E. Gurewitz, and G. Fox, </author> <title> "A deterministic annealing approach to clustering," </title> <journal> Pattern Recognition Letters, </journal> <volume> vol. 11, </volume> <pages> pp. 589-594, </pages> <year> 1990. </year>
Reference-contexts: In addition, DA has been developed for central <ref> [36, 37] </ref> and pairwise clustering [16, 3]. In Deterministic Annealing (DA) approaches the stochasticity of SA is incorporated in a probabilistic formulation of the optimization problem and a deterministic optimization is performed over a probabilistic state space Q (M). The minimization is carried out over the J. Puzicha, J.M. <p> One of the key advantages of the DA approach is the inherent splitting strategy, as clusters degenerate at high temperature and successively split in bifurcations or phase transitions when T is lowered <ref> [36, 37] </ref>. Therefore at a specific temperature scale an easily measurable J. Puzicha, J.M. Buhmann: Real-Time Texture Segmentation 16 effective number K T of clusters is visible.
Reference: [37] <author> K. Rose, E. Gurewitz, and G. Fox, </author> <title> "Vector quantization by deterministic annealing," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> vol. 38, no. 4, </volume> <pages> pp. 1249-1257, </pages> <year> 1992. </year>
Reference-contexts: In addition, DA has been developed for central <ref> [36, 37] </ref> and pairwise clustering [16, 3]. In Deterministic Annealing (DA) approaches the stochasticity of SA is incorporated in a probabilistic formulation of the optimization problem and a deterministic optimization is performed over a probabilistic state space Q (M). The minimization is carried out over the J. Puzicha, J.M. <p> One of the key advantages of the DA approach is the inherent splitting strategy, as clusters degenerate at high temperature and successively split in bifurcations or phase transitions when T is lowered <ref> [36, 37] </ref>. Therefore at a specific temperature scale an easily measurable J. Puzicha, J.M. Buhmann: Real-Time Texture Segmentation 16 effective number K T of clusters is visible.
Reference: [38] <author> G. Bilbro, W. Snyder, S. Garnier, and J. Gault, </author> <title> "Mean field annealing: A formalism for constructing GNC-like algorithms," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 3, no. 1, </volume> <year> 1992. </year> <editor> J. Puzicha, J.M. </editor> <title> Buhmann: Real-Time Texture Segmentation 32 </title>
Reference-contexts: Expressions for hg iff i Q are obtained exploiting the independence of random variables at different sites. DA can be understood as a continuation method <ref> [38] </ref>, as the (unique) minimum at high temperature is tracked while gradually lowering (`annealing') T . For T = 0 again the local ICM-algorithm is obtained, which has already been used by Heitz et al. [14] for multiscale optimization.
Reference: [39] <author> Y. Linde, A. Buzo, and R. Gray, </author> <title> "An algorithm for vector quantizer design," </title> <journal> IEEE Transactions on Communications, </journal> <volume> vol. 28, no. 1, </volume> <pages> pp. 84-95, </pages> <year> 1980. </year>
Reference-contexts: The same bookkeeping (30)-(31) is used. Note that the complexity rises to O as a bookkeeping update is no longer restricted to two clusters. 3.5 Multiscale Annealing Algorithms like K-means or the Linde-Buzo-Gray (LBG) algorithm <ref> [39] </ref> efficiently minimize H cc by splitting techniques to obtain successive solutions for a growing number of clusters. As the number of data points available drastically reduces at coarser resolution levels, splitting strategy and coarse-to-fine optimization should be interleaved.
Reference: [40] <author> T. Linder, G. Lugosi, and K. Zeger, </author> <title> "Rates of convergence in the source coding theorem, in empirical quantizer design, and in universal lossy source coding," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> vol. 40, no. 6, </volume> <pages> pp. 1728-1740, </pages> <year> 1994. </year>
Reference-contexts: P (kxk 2 &lt; Z) = 1 and a nearest-neighbor rule for M given y. Then bounds for the deviation of the empirical costs H cc ((x i ); y) from the expected costs of a segmentation can be derived independently of the underlying distribution <ref> [40] </ref>: P (kH cc ((x i ); y) E [H cc (x; y)]k &gt; *) 4 (2N ) K (d+1) e N* 2 =512Z 2 Thus for a given accuracy * a lower bound for the maximal number of clusters K max distin guishable with probability 1 ffi given N data
Reference: [41] <author> P. Brodatz, </author> <title> Textures: A Photographic Album for Artists and Designers. </title> <address> New York: </address> <publisher> Dover Publications, </publisher> <year> 1966. </year>
Reference-contexts: The essential structure of the segmentation is detected on the coarser scales using global optimization techniques and is then propagated using fast low or zero temperature algorithms. 4 Results 4.1 Implementation Details We selected a representative set of 86 micro-patterns from the Brodatz texture album <ref> [41] </ref> to empirically test the segmentation algorithms on a wide range of textures. 1 A database of random mixtures (512 fi 512 pixels each) containing 100 entities of five textures each (as depicted Fig. 3) was constructed from this collection.
Reference: [42] <author> J. Buhmann, W. Burgard, A. Cremers, D. Fox, T. Hofmann, F. Schneider, I. Strikos, and S. Thrun, </author> <title> "The mobile robot RHINO," </title> <journal> AI Magazin, </journal> <volume> vol. 16, no. 1, </volume> <year> 1995. </year>
Reference-contexts: Unsupervised segmentation is of major importance in vision-guided autonomous robotics and the presented algorithms have been developed for the autonomous platform RHINO <ref> [42] </ref>. An example image of a typical office environment recorded by RHINO is presented in Fig. 7. The segmentations achieved by multiscale-annealing are both visually and semantically satisfactory. Note, that the GFC feature extraction performs inferior, as wall and floor are not distinguished.
References-found: 42

