URL: ftp://ftp.research.microsoft.com/pub/tr/TR-97-08.ps
Refering-URL: http://www.research.microsoft.com/~meek/meek.htm
Root-URL: http://www.research.microsoft.com
Email: heckerma@microsoft.com  meek@microsoft.com  
Title: Models and Selection Criteria for Regression and Classification  
Author: David Heckerman Christopher Meek 
Address: One Microsoft Way Redmond, WA 98052  
Affiliation: Microsoft Research Advanced Technology Division Microsoft Corporation  
Date: May 1997  
Abstract: Technical Report MSR-TR-97-08 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bernardo, J. </author> <year> (1979). </year> <title> Expected information as expected utility. </title> <journal> Annals of Statistics, </journal> <volume> 7 </volume> <pages> 686-690. </pages>
Reference: <author> Bishop, C. </author> <year> (1995). </year> <title> Neural networks for pattern recognition. </title> <publisher> Clarendon Press, Oxford. </publisher>
Reference-contexts: For this example, it is not difficult to derive the corresponding conditional model <ref> (see, for example, Bishop, 1995, Chapter 6) </ref>. Namely, we have kx log p (y 1 jx; m ; m) (y k ) + i=1 (x i jy k ) (2) that has the same conditional distribution for Y . for k = 2; : : : ; r. <p> We call this model defined by Equations 6 and 7 a Bayesian regression/classification (BRC) model. Simple examples of BRC models include ordinary linear regression (e.g., Gelman et al., 1995, Chapter8), and generalized linear models <ref> (e.g., Bishop, 1995, Chapter 10) </ref>. Note that our Bayesian analogue to the conditional model is a special case of a BJ model. One could imagine using a Bayesian model that encodes only the conditional likelihood p (yjx; m ; m) and a joint distribution for m and m. <p> We say that ( 0 m ; m 0 ) is a Bayesian embedded regression/classification (BERC) model obtained from ( m ; m). Several researchers have suggested using BERC models, at least implicitly <ref> (see Bishop, 1995, Chapter 10, and references therein) </ref>. An example of a BERC model obtained from a naive Bayes model is shown in Figure 2.
Reference: <author> Buntine, W. </author> <year> (1993). </year> <title> Learning classification trees. </title> <booktitle> In Artificial Intelligence Frontiers in Statistics: AI and statistics III. </booktitle> <publisher> Chapman and Hall, </publisher> <address> New York. </address>
Reference: <author> Chickering, D., Heckerman, D., and Meek, C. </author> <year> (1997). </year> <title> A Bayesian approach to learning Bayesian networks with local structure. </title> <booktitle> In Proceedings of Thirteenth Conference on Uncertainty in Artificial Intelligence, </booktitle> <address> Providence, RI. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Dawid, P. </author> <year> (1984). </year> <title> Present position and potential developments: some personal views. Statistical theory. The prequential approach (with Discussion). </title> <journal> Journal of the Royal Statistical Society A, </journal> <volume> 147 </volume> <pages> 178-292. </pages>
Reference: <author> Friedman, N. and Goldszmidt, M. </author> <year> (1996). </year> <title> Building classifiers using Bayesian networks. </title> <booktitle> In Proceedings AAAI-96 Thirteenth National Conference on Artificial Intelligence, </booktitle> <address> Portland, OR, </address> <pages> pages 1277-1284. </pages> <publisher> AAAI Press, </publisher> <address> Menlo Park, CA. </address>
Reference: <author> Geiger, D., Heckerman, D., and Meek, C. </author> <year> (1996). </year> <title> Asymptotic model selection for directed networks with hidden variables. </title> <booktitle> In Proceedings of Twelth Conference on Uncertainty in Artificial Intelligence, </booktitle> <address> Portland, OR. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: 3 j m ; m), and p (x i jy; m ; m), respectively.It is not difficult to show that the rank of the Jacobian matrix @ x =@ m is full (i.e., equal to the number of non-redundant parameters in m ) for almost all values of fi m <ref> (see, e.g., Geiger et al., 1996) </ref>.
Reference: <author> Gelman, A., Carlin, J., Stern, H., and Rubin, D. </author> <year> (1995). </year> <title> Bayesian Data Analysis. </title> <publisher> Chapman and Hall. </publisher>
Reference-contexts: In particular, if we care only about the conditional distribution, we can analyze it on its own. We call this model defined by Equations 6 and 7 a Bayesian regression/classification (BRC) model. Simple examples of BRC models include ordinary linear regression <ref> (e.g., Gelman et al., 1995, Chapter8) </ref>, and generalized linear models (e.g., Bishop, 1995, Chapter 10). Note that our Bayesian analogue to the conditional model is a special case of a BJ model.
Reference: <author> Goodman, L. </author> <year> (1974). </year> <title> Exploratory latent structure analysis using both identifiable and unidentifiable models. </title> <journal> Biometrika, </journal> <volume> 61 </volume> <pages> 215-231. </pages>
Reference-contexts: More generally, conditional models|often referred to as regression/classification models|should not be used without consideration of variational dependencies that may arise from the joint model. 3 The parameters m are said to be locally identifiable given observations of X <ref> (e.g., Goodman, 1974) </ref>. 6 4 Learning Regression/Classification Models: Averaging Ver- sus Selection Now that we have examined several classes of models for the regression/classification task, let us concentrate on Bayesian methods for learning such models. First, consider model averaging.
Reference: <author> Heckerman, D. </author> <year> (1995). </year> <title> A tutorial on learning Bayesian networks. </title> <type> Technical Report MSR-TR-95-06, </type> <institution> Microsoft Research, Redmond, WA. </institution> <note> Revised January, </note> <year> 1996. </year>
Reference-contexts: Under these same assumptions, the exact computation of the class sequential criterion is exponential in the sample size N . Monte-Carlo or asymptotic techniques can be used to perform the computation for large N <ref> (see, e.g., Heckerman, 1995) </ref>. We have applied both criteria to small Bayesian networks and small data sets chosen arbitrarily. In all cases, we have found that the two criteria differ. Nonetheless, there are conditions under which the two criteria are the same.
Reference: <author> Meek, C. and Heckerman, D. </author> <year> (1997). </year> <title> Structure and parameter learning for causal independence and causal interaction models. </title> <booktitle> In Proceedings of Thirteenth Conference on Uncertainty in Artificial Intelligence, </booktitle> <address> Providence, RI. </address> <publisher> Morgan Kaufmann. </publisher> <address> 10 Spiegelhalter, D., </address> <note> Dawid, </note> <author> A., Lauritzen, S., and Cowell, R. </author> <year> (1993). </year> <title> Bayesian analysis in expert systems. </title> <journal> Statistical Science, </journal> <volume> 8 </volume> <pages> 219-282. 11 </pages>
References-found: 11

