URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3207/3207.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Title: The Block Distributed Memory Model  
Author: Joseph F. JaJa ; Kwan Woo Ryu ;; 
Address: College Park, MD 20742  
Affiliation: Dept. of Electrical Engineering Dept. of Computer Engineering Inst. for Advanced Comp. Studies Kyungpook Natl Univ., KOREA Inst. for Systems Research Inst. for Advanced Comp. Studies University of Maryland  
Abstract: We introduce a computation model for developing and analyzing parallel algorithms on distributed memory machines. The model allows the design of algorithms using a single address space and does not assume any particular interconnection topology. We capture performance by incorporating a cost measure for interprocessor communication induced by remote memory accesses. The cost measure includes parameters reflecting memory latency, communication bandwidth, and spatial locality. Our model allows the initial placement of the input data and pipelined prefetching. We use our model to develop parallel algorithms for various data rearrangement problems, load balancing, sorting, FFT, and matrix multiplication. We show that most of these algorithms achieve optimal or near optimal communication complexity while simultaneously guaranteeing an optimal speed-up in computational complexity. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Aggarwal, A. Chandra, and M. Snir, </author> <title> On Communication Latency in PRAM Computations, </title> <booktitle> Proc. 1st ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 11-21, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: However there are significant differences between our model and each of these models. For example, both the Asynchronous PRAM [9] and the Block PRAM <ref> [1] </ref> assume the presence of a shared memory where intermediate results can be held; in particular, they both assume that the data is initially stored in this shared memory. This makes data movement operations considerably simpler than in our model. <p> Let k be an integer to be determined later. The algorithm can be viewed as a k-ary tree rooted at location A [0]; there are dlog k pe rounds. During the first round, A [0] is broadcast to locations A <ref> [1] </ref>; A [2]; : : : ; A [k 1], using the algorithm described in Lemma 3.1, followed by a synchronization barrier. Then during the second round, each element in locations A [0]; A [1]; : : : ; A [k 1] is broadcast to a distinct set of k 1 <p> During the first round, A [0] is broadcast to locations A <ref> [1] </ref>; A [2]; : : : ; A [k 1], using the algorithm described in Lemma 3.1, followed by a synchronization barrier. Then during the second round, each element in locations A [0]; A [1]; : : : ; A [k 1] is broadcast to a distinct set of k 1 locations, and so on. The communication cost incurred during each round is given by t + (k 1)m (Lemma 3.1). <p> We show that product C = AB can be computed in O ( n 3 p ) computation time and 6 (2t d log p 3 log ( t p 2=3 ) communication time. The overall strategy is similar to the one used in <ref> [1, 10] </ref>, where some related experimental results supporting the efficiency of the algorithm appear in [10]. Before presenting the algorithm, we establish the following lemma.
Reference: [2] <author> A. Aggarwal, A.K. Chandra, and M. Snir, </author> <title> Hierarchical Memory with Block Transfer, </title> <booktitle> Proc. 28th Annual Symp. on Foundations of Computer Science, </booktitle> <pages> pp. 204-216, </pages> <month> Oct. </month> <year> 1987. </year>
Reference-contexts: This can be done in a similar fashion as in the memory hierarchy model studied in <ref> [2] </ref> for sequential processors. However in this paper we have opted for simplicity and decided not to include the processor locality into consideration. Several models that have been discussed in the literature, other than the LogP and the postal models referred to earlier, are related to our BDM model. <p> Let k be an integer to be determined later. The algorithm can be viewed as a k-ary tree rooted at location A [0]; there are dlog k pe rounds. During the first round, A [0] is broadcast to locations A [1]; A <ref> [2] </ref>; : : : ; A [k 1], using the algorithm described in Lemma 3.1, followed by a synchronization barrier. <p> 2 n )e, or (3) (3t + (p 1)d 5 ln n p ) communication time with high probability, if p 2 &lt; n 6 ln n . 2 Two other sorting algorithms are worth considering: (1) radix sort (see e.g. [6] and related references) and (2) approximate median sort <ref> [2, 22] </ref>.
Reference: [3] <author> Anant Agarwal, B.-H. Lim, D. Kranzs, and J. Kubiatowicz, </author> <month> APRIL: </month> <title> A Processor Architecture for Multiprocessing, </title> <booktitle> Proc. of the 17th Annual International Symp. on Computer Architecture, </booktitle> <pages> pp. 104-114, </pages> <month> May </month> <year> 1990. </year>
Reference: [4] <author> A. Bar-Noy and S. Kipnis, </author> <title> Designing Broadcasting Algorithms in the Postal Model for Message-Passing Systems, </title> <booktitle> Proc. 4th Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 13-22, </pages> <month> July </month> <year> 1992. </year>
Reference: [5] <author> A. Bar-Noy and S. Kipnis, </author> <title> Multiple Message Broadcasting in the Postal Model, </title> <booktitle> Proc. 7th International Parallel Processing Symp., </booktitle> <pages> pp. 463-470, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: In our model, we allow pipelined prefetching for hiding memory latency. In particular, k prefetch read operations issued by a processor can be completed in t + km time. The underlying communication model for BDM is consistent with the LogP and the postal models <ref> [8, 13, 5] </ref> but with the addition of the parameter m that incorporates spatial locality. However, our model does not allow low-level handling of message passing primitives except implicitly through data accesses. <p> Since T comm is treated separately from T comp , we can normalize this measure by dividing it by . The underlying communication model for BDM can now be viewed as the postal model <ref> [5] </ref> but with the added parameter m reflecting spatial locality. Hence an access operation to a remote location takes t + m time, and k prefetch read operations can be executed in t + km time.
Reference: [6] <author> G.E. Blelloch et.al., </author> <title> A Comparison of Sorting Algorithms for the Connection Machine CM-2, </title> <booktitle> Proc. 3th Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 3-16, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Three strategies seem to perform best on our model; these are (1) column sort [15], (2) sample sort (see e.g. <ref> [6] </ref> and related references), and (3) rotate sort [18]. It turns out that the column sort algorithm is best when n 2p (p 1) 2 , and that the sample sort and the rotate sort are better when p 2 n &lt; 2p (p 1) 2 . <p> The complexity bounds are guaranteed with high probability if we use the randomized routing algorithm described in Section 3. 12 The overall idea of the algorithm has been used in various sample sort algorithms. Our algorithm described below follows more closely the scheme described in <ref> [6] </ref> for sorting on the connection machine CM-2; however the first three steps are different. Algorithm Sample Sort Input: n elements distributed evenly over a p-processor BDM such that p 2 &lt; n 6 ln n . <p> the elements in the p sets, S 0;i ; S 1;i ; : : : ; S p1;i , by using Algorithm Randomized Routing. [Step 6 ] Each processor P i sorts the elements in its local memory. end The following lemma can be immediately deduced from the results of <ref> [6] </ref>. Lemma 5.1 For any ff &gt; 0, the probability that any processor contains more than ff n p elements after Step 5 is at most ne (1 1 2 : 2 Next, we show the following theorem. <p> where k = 6 + 2dlog (1 + 6p 2 n )e, or (3) (3t + (p 1)d 5 ln n p ) communication time with high probability, if p 2 &lt; n 6 ln n . 2 Two other sorting algorithms are worth considering: (1) radix sort (see e.g. <ref> [6] </ref> and related references) and (2) approximate median sort [2, 22].
Reference: [7] <author> H. Burkhardt III, S. Frank, B. Knobe, and J. Rothnie, </author> <title> Overview of the KSRI Computer System, </title> <type> TR KSR TR 9202001, </type> <institution> Kendall Square Rescard, </institution> <address> Boston, </address> <month> Feb. </month> <year> 1992. </year>
Reference: [8] <author> D. Culler et. al., </author> <title> LogP: Toward a Realistic Model of Parallel Computation, </title> <booktitle> Proc. 4th ACM PPOPP, </booktitle> <pages> pp. 1-12, </pages> <month> May </month> <year> 1993. </year> <month> 18 </month>
Reference-contexts: Since a computation model should predict performance on real machines, we start with a discussion on the basis of our measure of communication costs incurred by accessing remote data. As indicated in <ref> [8] </ref>, the hardware organizations of massively parallel processors (MPPs) seem to be converging towards a collection of powerful processors connected by a communication network that can be modeled as a complete graph on which communication is subject to the restrictions imposed by the latency and the bandwidth properties of the network. <p> In our model, we allow pipelined prefetching for hiding memory latency. In particular, k prefetch read operations issued by a processor can be completed in t + km time. The underlying communication model for BDM is consistent with the LogP and the postal models <ref> [8, 13, 5] </ref> but with the addition of the parameter m that incorporates spatial locality. However, our model does not allow low-level handling of message passing primitives except implicitly through data accesses. <p> Clearly, if q = p, this corresponds to the usual notion of matrix transpose. An efficient algorithm to perform matrix transposition on the BDM model is similar to the algorithm reported in <ref> [8] </ref>. There are p 1 rounds that can be fully pipelined by using prefetch read operations. During the first round, the appropriate block of q=p elements in the ith column of A is read by processor P (i+1)modp into the appropriate locations, for 0 i p 1. <p> When mp 2 divides n, the communication time reduces to t + ( n p n Remark: The algorithm described in <ref> [8] </ref> can also be implemented on our model within the same complexity bounds. However our algorithm is somewhat simpler to implement. 2 5.3 Matrix Multiplication We finally consider the problem of multiplying two nfin matrices A and B. We assume that p n 3 log n .
Reference: [9] <author> P.B. Gibbons, </author> <title> Asynchronous PRAM Algorithms, a chapter in Synthesis of Par--allel Algorithms, </title> <editor> J.H. Reif, editor, Morgan-Kaufman, </editor> <year> 1990. </year>
Reference-contexts: Several models that have been discussed in the literature, other than the LogP and the postal models referred to earlier, are related to our BDM model. However there are significant differences between our model and each of these models. For example, both the Asynchronous PRAM <ref> [9] </ref> and the Block PRAM [1] assume the presence of a shared memory where intermediate results can be held; in particular, they both assume that the data is initially stored in this shared memory. This makes data movement operations considerably simpler than in our model.
Reference: [10] <author> A. Gupta and V. Kumar, </author> <title> Scalability of Parallel Algorithms for the Matrix Multiplication, </title> <booktitle> 1993 International Conference on Parallel Processing, </booktitle> <volume> Vol. III, </volume> <pages> pp. 115-123. </pages>
Reference-contexts: We show that product C = AB can be computed in O ( n 3 p ) computation time and 6 (2t d log p 3 log ( t p 2=3 ) communication time. The overall strategy is similar to the one used in <ref> [1, 10] </ref>, where some related experimental results supporting the efficiency of the algorithm appear in [10]. Before presenting the algorithm, we establish the following lemma. <p> The overall strategy is similar to the one used in [1, 10], where some related experimental results supporting the efficiency of the algorithm appear in <ref> [10] </ref>. Before presenting the algorithm, we establish the following lemma. Lemma 5.2 Given p matrices each of size n fi n distributed one matrix per proces sor, their sum can be computed in O (n 2 ) computation time and 2 (2t d log p log ( t communication time.
Reference: [11] <author> E. Hagersten, S. Haridi, and D. Warren, </author> <title> The Cache-Coherence Protocol of the Data Diffusion Machine, </title> <editor> M. Dubois and S. Thakkar, editors, </editor> <title> Cache and Interconnect Architectures in Multiprocessors, </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1990. </year>
Reference: [12] <author> J. JaJa and K.W. Ryu, </author> <title> Load Balancing and Routing on the Hypercube and Related Networks, </title> <journal> Journal of Parallel and Distributed Computing 14, </journal> <pages> pp. 431-435, </pages> <year> 1992. </year>
Reference-contexts: The load balancing problem is also important in developing fast solutions for basic combinatorial problems such as sorting, selection, list ranking, and graph problems <ref> [12, 21] </ref>. This problem can be defined as follows.
Reference: [13] <author> R.M. Karp, A. Sahay, E.E. Santos, K.E. Schauser, </author> <title> Optimal Broadcast and Summation in the LogP Model, </title> <booktitle> Proc. 5th Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 142-153, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: In our model, we allow pipelined prefetching for hiding memory latency. In particular, k prefetch read operations issued by a processor can be completed in t + km time. The underlying communication model for BDM is consistent with the LogP and the postal models <ref> [8, 13, 5] </ref> but with the addition of the parameter m that incorporates spatial locality. However, our model does not allow low-level handling of message passing primitives except implicitly through data accesses.
Reference: [14] <author> C.P. Kruskal, L. Rudolph, and M. Snir, </author> <title> A Complexity Theory of Efficient Parallel Algorithms, </title> <booktitle> Theoretical Computer Science 71, </booktitle> <pages> pp. 95-132, </pages> <year> 1990. </year>
Reference-contexts: This makes data movement operations considerably simpler than in our model. Another example is the Direct Connection Machine (DCM) with latency <ref> [14] </ref> that uses message passing primitives; in particular, this model does not allow pipelined prefetching as we do in the BDM model. 3 Basic Algorithms for Data Movements The design of communication efficient parallel algorithms depends on the existence of efficient schemes for handling frequently occurring transformations on data layouts.
Reference: [15] <author> T. Leighton, </author> <title> Tight bounds on the Complexity of Parallel Sorting, </title> <journal> IEEE Trans. Comp. </journal> <volume> C-34(4), </volume> <pages> pp. 344-354, </pages> <month> April </month> <year> 1985. </year>
Reference-contexts: The basic strategies used are well-known but the implementations on our model require a careful attention to several technical details. 11 5.1 Sorting We first consider the sorting problem on the BDM model. Three strategies seem to perform best on our model; these are (1) column sort <ref> [15] </ref>, (2) sample sort (see e.g. [6] and related references), and (3) rotate sort [18].
Reference: [16] <author> D. Lenoski et. al., </author> <title> The Stanford Dash Multiprocessor, </title> <booktitle> IEEE Computer 25(3), </booktitle> <pages> pp. 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference: [17] <author> C.V. Loan, </author> <title> Computational Frameworks for the Fast Fourier Transform, </title> <publisher> SIAM, </publisher> <year> 1992. </year>
Reference-contexts: Our implementation on the BDM model is based on the following well-known fact (e.g. see <ref> [17] </ref>). Let the n-dimensional vector x be stored in the n p fi p matrix X in row-major order form, where p is an arbitrary integer that divides n.
Reference: [18] <author> J.M. Marberg and E. Gafni, </author> <title> Sorting in Constant Number of Row and Column Phases on a Mesh, </title> <journal> Algorithmica 3, </journal> <pages> pp. 561-572, </pages> <year> 1988. </year>
Reference-contexts: Three strategies seem to perform best on our model; these are (1) column sort [15], (2) sample sort (see e.g. [6] and related references), and (3) rotate sort <ref> [18] </ref>. It turns out that the column sort algorithm is best when n 2p (p 1) 2 , and that the sample sort and the rotate sort are better when p 2 n &lt; 2p (p 1) 2 . <p> The computation time for all the steps is clearly O ( n log n p ) with high probability if p 2 &lt; n 6 ln n , and the theorem follows. 2 13 Rotate Sort The rotate sort algorithm <ref> [18] </ref> sorts elements on a mesh by alternately applying transformations on the rows and columns. The algorithm runs in a constant number of row-transformation and column-transformation phases (16 phases). We assume here that n 6p 2 . <p> A block is a subarray of size p p p, consisting of all positions (i; j) such that l p p and r p j (r + 1) p 1 for some l 0 and r 0. We now describe the algorithm briefly; all the details appear in <ref> [18] </ref>. We begin by specifying three procedures, which serve as building blocks for the main algorithm. Each procedure consists of a sequence of phases that accomplish a specific transformation on the array. <p> UNBLOCK the array. 4. BALANCE each slice as if it were a p fi p p array lying on its side. 5. UNBLOCK the array. 6. "Transpose" the array. 14 7. SHEAR the array. 8. Sort all the columns downwards. For the complete correctness proof of the algorithm, see <ref> [18] </ref>. We can easily prove that this algorithm can be performed in at most 14 local sorting steps within each processor.
Reference: [19] <author> K. Mehrotra, S. Ranka, and J.-C. Wang, </author> <title> A Probabilistic Analysis of a Locality Maintaining Load Balancing Algorithm, </title> <booktitle> Proc. 7th International Parallel Processing Symp., </booktitle> <pages> pp. 369-373, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: , the effect of the parameter m is dominated by the bound c n p (as n p &gt; 6p ln n mp, assuming m 6 ln n). 2 9 4 Load Balancing Balancing load among processors is very important since poor balance of load generally causes poor processor utilization <ref> [19] </ref>. The load balancing problem is also important in developing fast solutions for basic combinatorial problems such as sorting, selection, list ranking, and graph problems [12, 21]. This problem can be defined as follows.
Reference: [20] <author> S. Rajasekaran and T. Tsantilas, </author> <title> Optimal Routing Algorithms for Mesh-connected Processor Arrays, </title> <journal> Algorithmica (8), </journal> <pages> pp. 21-38, </pages> <year> 1992. </year>
Reference-contexts: The overall idea of the algorithm has been used in various randomized routing algorithms on the mesh. Here we follow more closely the scheme described in <ref> [20] </ref> for randomized routing on the mesh with bounded queue size. Before describing our algorithm, we introduce some terminology.
Reference: [21] <author> K.W. Ryu and J. JaJa, </author> <title> Efficient Algorithms for List Ranking and for Solving Graph Problems on the Hypercube, </title> <journal> IEEE Trans. Parallel and Distributed Systems 1(1), </journal> <pages> pp. 83-90, </pages> <month> Jan. </month> <year> 1990. </year>
Reference-contexts: The load balancing problem is also important in developing fast solutions for basic combinatorial problems such as sorting, selection, list ranking, and graph problems <ref> [12, 21] </ref>. This problem can be defined as follows.
Reference: [22] <author> H. Shi and J. Schaeffer, </author> <title> Parallel Sorting by Regular Sampling, </title> <editor> J. </editor> <booktitle> of Parallel and Distributed Computing 14, </booktitle> <pages> pp. 361-372, </pages> <year> 1992. </year> <month> 19 </month>
Reference-contexts: 2 n )e, or (3) (3t + (p 1)d 5 ln n p ) communication time with high probability, if p 2 &lt; n 6 ln n . 2 Two other sorting algorithms are worth considering: (1) radix sort (see e.g. [6] and related references) and (2) approximate median sort <ref> [2, 22] </ref>.
Reference: [23] <editor> H.J. Siegel et. al., </editor> <booktitle> Report of the Purdue Workshop in Grand Challenges in Com--puter Architecture for the Support of High Performance Computing, J. of Parallel and Distributed Computing 16(3), </booktitle> <pages> pp. 198-211, </pages> <year> 1992. </year>
Reference-contexts: It is widely recognized <ref> [23] </ref> that an important ingredient for the success of this technology is the emergence of computational models that can be used for algorithms development and for accurately predicting the performance of these algorithms on real machines.
Reference: [24] <author> L. G. Valiant, </author> <title> A Bridging Model for Parallel Computation, </title> <journal> CACM 33(8), </journal> <pages> pp. 103-111, </pages> <month> Aug. </month> <year> 1990. </year> <month> 20 </month>
Reference-contexts: It is widely recognized [23] that an important ingredient for the success of this technology is the emergence of computational models that can be used for algorithms development and for accurately predicting the performance of these algorithms on real machines. We take a similar view as in <ref> [24] </ref> in that the computation model should be a "bridging model" that links the two layers of hardware and software. Existing computation models tend to be biased towards one or the other layer, except for very few exceptions. The Bulk Synchronous Parallel (BSP) model advocated by Valiant [24] is one of <p> view as in <ref> [24] </ref> in that the computation model should be a "bridging model" that links the two layers of hardware and software. Existing computation models tend to be biased towards one or the other layer, except for very few exceptions. The Bulk Synchronous Parallel (BSP) model advocated by Valiant [24] is one of the few exceptions. In this paper, we introduce a computation model that specifically attempts to be a bridging model between the shared memory (single address) programming model and the distributed-memory message passing architectures.
References-found: 24

