URL: ftp://ftp.engr.orst.edu/pub/dambrosi/local.ps.Z
Refering-URL: http://www.cs.orst.edu/~dambrosi/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Note: Bruce D'Ambrosio  
Abstract:  
Abstract-found: 1
Intro-found: 1
Reference: <institution> References </institution>
Reference: 1. <author> J. M. Agosta. </author> <title> Conditional inter-causally interdependent node distributions ... In , pages 9-16. </title> <publisher> Morgan Kaufmann, Publishers, </publisher> <month> July </month> <year> 1991. </year>
Reference-contexts: LOCAL EXPRESSION LANGUAGES FOR PROBABILISTIC KNOWLEDGE complexity than the full conditional. The noisy-or [16], [17], [9], [12], <ref> [1] </ref> for example, can be used to model independent causes of an event, and inference complexity is linear in both space and time in the number of antecedents for many inferences.
Reference: 2. <author> S. Andreassen and K. Olesen. </author> <title> Specification of models in large expert systems based on causal probabilistic networks. </title> , <year> 1993. </year>
Reference-contexts: This restriction does not exist in our representation. However, their representation offers the strong advantage that consistency can be easily established. As we noted earlier, checking the consistency of local expressions in our language is exponential in the number of parent variables. Andreassen <ref> [2] </ref> presents a convenient method for describing interaction models such as noisy or. Dagum and Galper [4] have proposed an additive decomposition of conditional dependence that is easily captured using local expressions.
Reference: 3. <author> G. Cooper. </author> <title> A method for using belief networks as influence diagrams. </title> <booktitle> In , pages 55-63. Association for Uncertainty in AI, </booktitle> <month> August </month> <year> 1988. </year>
Reference-contexts: Second, as Dagum and Galper point out, the decomposition can be into conditioning subsets rather than individual variables. Finally, decision models can be represented as Bayesian nets, as noted by Cooper <ref> [3] </ref>. Value structures are often factorable. Consider, for example, the classic drilling example [18]. <p> The full expression language has been implemented and is in use on a variety of monitoring, diagnosis, assessment and control projects. Acknowledgements Thanks to Bob Fung of Prevision, Inc. and Peter Raulefs and Bob Culley of Intel for many useful discussions. Thanks to NSF <ref> (IRI88-21660, IRI91-00530) </ref>, AFOSR, and Intel for providing the support which made this work possible.
Reference: 4. <author> P. Dagum and A. Galper. </author> <title> Additive belief-network models. </title> <editor> In D. Heckerman and A. Mamdani, editors, </editor> , <address> pages 91-98. </address> <publisher> Morgan Kaufmann, </publisher> <month> July </month> <year> 1993. </year>
Reference-contexts: Nonetheless, this mass must be accounted for so that this expression can be properly combined with others to recover the full joint. The Dagum and Galper <ref> [4] </ref> recently noted the utility of additive decomposition of conditional distributions. Their model is easily expressed in our local expression language: ( ) = + Two notes about this example. <p> As we noted earlier, checking the consistency of local expressions in our language is exponential in the number of parent variables. Andreassen [2] presents a convenient method for describing interaction models such as noisy or. Dagum and Galper <ref> [4] </ref> have proposed an additive decomposition of conditional dependence that is easily captured using local expressions. Srinivas [22] has developed a generalized noisy-or model which is compatible in spirit, but might require extension of the set of operators available in our local expression language.
Reference: 5. <author> B. D'Ambrosio. </author> <title> Symbolic probabilistic inference. </title> <type> Technical report, </type> <institution> CS Dept., Oregon State University, </institution> <year> 1989. </year>
Reference-contexts: When the graph is sparse, this will involve a much smaller set of numbers than the full joint. Equally important, the graphical structure can be used to guide processing to find efficient ways to evaluate queries against the model. For more details, see [16], [19], <ref> [5] </ref>, [20], [14], [13], [15]. All is not as rosy at it might seem, though. The graphical level is not capable of representing all interesting structural information which might simplify representation or inference. <p> Dvalue &lt; var &gt; : D E exp E c c c c t t t;f t t;f f t;f f t;f (distribution Dvalue/test ((value t f)) ((test y n)) (( y) .03 -.03) We ignore evidence for purposes of this introduction. It introduces only minor complications, see <ref> [5] </ref>, [20] for details. There seems to be something very strange in the above, namely a negative value in a distribution. The explanation is simple: the three / distributions jointly define the full conditional. <p> This will be needed later when we show how the method can be extended to perform inference over nets defined using our local expression language. For further details, see <ref> [5] </ref> or [20]. Computation of probabilities in a Bayesian net can be done quite straightforwardly, albeit somewhat inefficiently . I illustrate this process with a simple network, shown in figure 1.
Reference: 6. <author> B. D'Ambrosio. </author> <title> Incremental evaluation and construction of defeasible probabilistic models. </title> , <month> July </month> <year> 1990. </year>
Reference-contexts: Currently there is no way to make this structure explicit using our representation. We began our exploration of probabilistic inference in the context of truth maintenance systems, and at that time used symbolic representation at the level of individual probability mass elements <ref> [6] </ref>. Later, motivated by efficiency concerns, we changed to a symbolic representation at the distribution level [20]. We now seem to have come full circle: the implementation described here again performs symbolic reasoning on elements as small as individual probabilities.
Reference: 7. <author> B. D'Ambrosio. </author> <title> Spi in large bn2o networks. </title> <editor> In Poole and Lopez de Mantaras, editors, </editor> . <publisher> Morgan Kaufmann, </publisher> <month> July </month> <year> 1994. </year>
Reference-contexts: As shown in the charts which follow, we then recorded the number of findings, the number of multiplications needed (shown in thousands), and the posterior probability. We chose a limit of 2,000,000 multiplies because this 8 12 For an indepth study of this particular problem see <ref> [7] </ref>. Sci Am using quickscore Sci Am using extended set-factoring corresponded to about five minutes of compute time on a Macintosh Quadra 700 in Common Lisp.
Reference: 8. <author> D. Geiger and D. Heckerman. </author> <booktitle> Advances in probabilistic reasoning. </booktitle> <pages> In , pages 118-126. </pages> <publisher> Morgan Kaufmann, Publishers, </publisher> <month> July </month> <year> 1991. </year>
Reference-contexts: We next consider the use of the local expression language to represent several other commonly occurring intra-distribution structures. It has been pointed out that probabilistic relationships are often asymmetric <ref> [8] </ref>. <p> One must define two contingent versions of and , giving them separate names (eg, 1 and 2). It is then up to the user to remember to query all instances of and sum the results. Geiger and Heckerman <ref> [8] </ref> have proposed multi-nets as a general representation for asymmetries. Their representation, however, has a significant limitation: it is not usable as a component in an arbitrary Bayesian net. In particular, they assume that the hypothesis variable of a multi-net is a root variable.
Reference: 9. <author> D. Heckerman. </author> <title> A tractable inference algorithm for diagnosing multiple diseases. </title> <booktitle> In , pages 174-181, </booktitle> <month> August </month> <year> 1989. </year>
Reference-contexts: LOCAL EXPRESSION LANGUAGES FOR PROBABILISTIC KNOWLEDGE complexity than the full conditional. The noisy-or [16], [17], <ref> [9] </ref>, [12], [1] for example, can be used to model independent causes of an event, and inference complexity is linear in both space and time in the number of antecedents for many inferences. <p> D exp term term term set term term set term term set term exp distribution: term set term term term set: distribution name dimensions conditioned conditioned conditioning conditioned variable name variable name conditioned conditioning variable name variable name conditioning domain value value set value set value value; value set Heckerman <ref> [9] </ref> has developed an algorithm, called Quickscore, which provides this efficiency for two level bipartite graphs. However, I am unaware of any implemented system other than the one reported here which can efficiently incorporate a noisy-or within an arbitrary Bayesian net. <p> The disease-disease interaction is modeled using a noisy-or, which reduces inference complexity to linear in the number of negative findings, linear in the number of diseases, and exponential in the number of positive findings (Quickscore, <ref> [9] </ref>). However, a typical case can have up to 50 positive findings, and findings have an average of 10 antecedents (some have hundreds, and tend to be common findings). As a result, Heckerman's original trials of quickscore resulted in inference times of one minute for 9 positive findings [9]. <p> findings (Quickscore, <ref> [9] </ref>). However, a typical case can have up to 50 positive findings, and findings have an average of 10 antecedents (some have hundreds, and tend to be common findings). As a result, Heckerman's original trials of quickscore resulted in inference times of one minute for 9 positive findings [9]. Remember that the local expression language representation we use for the noisy or is as follows.
Reference: 10. <author> D. Heckerman, J. Breese, and E. Horvitz. </author> <title> The compilation of decision models. </title> <booktitle> In , pages 162-173, </booktitle> <month> August </month> <year> 1989. </year>
Reference-contexts: Now we can reexpress the above as: ( = ) = 1 ( ) ( ) This notation is intuitively appealing. It is compact (linear in the number of antecedents), captures the structure of the interaction, and, as Heckerman has shown <ref> [10] </ref>, can be manually manipulated to perform efficient inference. However, it is not sufficiently formal to permit automated inference.
Reference: 11. <author> D. E. </author> <title> Heckerman. </title> . <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: Similarly, contingencies [21], as well as the value-dependent independence exploited in similarity nets <ref> [11] </ref> are inefficiently represented using a full conditional. In this paper we present an extension to the standard Bayesian belief-net representation which is capable of explicitly capturing much of this lower level structural detail, and which permits use of these structures within arbitrary belief nets.
Reference: 12. <author> M. Henrion. </author> <title> Towards efficient probabilistic diagnosis with a very large knowledge-base. </title> <booktitle> In , 1990. </booktitle>
Reference-contexts: LOCAL EXPRESSION LANGUAGES FOR PROBABILISTIC KNOWLEDGE complexity than the full conditional. The noisy-or [16], [17], [9], <ref> [12] </ref>, [1] for example, can be used to model independent causes of an event, and inference complexity is linear in both space and time in the number of antecedents for many inferences.
Reference: 13. <author> Z. Li. </author> . <type> PhD thesis, </type> <institution> Computer Science Dept., Oregon State University, </institution> <month> June </month> <year> 1993. </year> <booktitle> 16 Proceedings of the Annual Canadian Artificial Intelligence Conference Intl Journal of Approximate Reasoning Probabilistic Reasoning in Intelligent Systems IEEE Trans. on Systems, Man, and Cybernetics: special issue on diagnosis Decision Analysis. Operations Research Proceedings Eighth National Conference on AI Ninth Annual Conference on Uncertainty on AI </booktitle>
Reference-contexts: When the graph is sparse, this will involve a much smaller set of numbers than the full joint. Equally important, the graphical structure can be used to guide processing to find efficient ways to evaluate queries against the model. For more details, see [16], [19], [5], [20], [14], <ref> [13] </ref>, [15]. All is not as rosy at it might seem, though. The graphical level is not capable of representing all interesting structural information which might simplify representation or inference.
Reference: 14. <author> Z. Li and B. D'Ambrosio. </author> <title> An efficient approach to probabilistic inference in belief nets. </title> <booktitle> In . Canadian Association for Artificial Intelligence, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: When the graph is sparse, this will involve a much smaller set of numbers than the full joint. Equally important, the graphical structure can be used to guide processing to find efficient ways to evaluate queries against the model. For more details, see [16], [19], [5], [20], <ref> [14] </ref>, [13], [15]. All is not as rosy at it might seem, though. The graphical level is not capable of representing all interesting structural information which might simplify representation or inference.
Reference: 15. <author> Z. Li and B. D'Ambrosio. </author> <title> Efficient inference in bayes nets as a combinatorial optimization problem. </title> , <type> 10(5), </type> <year> 1994. </year>
Reference-contexts: When the graph is sparse, this will involve a much smaller set of numbers than the full joint. Equally important, the graphical structure can be used to guide processing to find efficient ways to evaluate queries against the model. For more details, see [16], [19], [5], [20], [14], [13], <ref> [15] </ref>. All is not as rosy at it might seem, though. The graphical level is not capable of representing all interesting structural information which might simplify representation or inference. <p> In this paper we present an extension to the standard Bayesian belief-net representation which is capable of explicitly capturing much of this lower level structural detail, and which permits use of these structures within arbitrary belief nets. We further present extensions to a factoring-based <ref> [15] </ref> inference algorithm in the SPI (Symbolic Probabilistic Inference) family which capture both the space and time advantages of these structures. <p> The factoring is developed incrementally, and factoring is intermixed with expression evaluation. In the next section we briefly review our current factoring heuristic. A full discussion of the current heuristics for constructing factorings, their theoretical basis, and experimental evaluation of their efficacy appears in a companion article recently published <ref> [15] </ref>. We have developed an efficient heuristic algorithm, called set-factoring, for finding good factorings for probability computation. We review it here because we will later extend it for networks in which variable dependence is described using arbitrary local expressions. <p> Similarly, making the factoring algorithm recursive does not change complexity since the number of factors is still reduced by one each time through the basic loop. For further analysis see <ref> [15] </ref>. 11 fl fl 0 0 0 0 0 0 0 0 D D A D B D D A D B A B B exp D c c exp D c c t t t;f t t;f t t t;f t t;f 5.2. Inference in bipartite graphs: QMR-DT 5.2.0.1.
Reference: 16. <author> J. </author> <title> Pearl. </title> . <publisher> Morgan Kaufmann, </publisher> <address> Palo Alto, </address> <year> 1988. </year>
Reference-contexts: We discuss the expressivity of the local expression language, and present early experimental results showing the efficacy of the approach. A Bayesian belief net <ref> [16] </ref> is a compact, localized representation of a probabilistic model. <p> When the graph is sparse, this will involve a much smaller set of numbers than the full joint. Equally important, the graphical structure can be used to guide processing to find efficient ways to evaluate queries against the model. For more details, see <ref> [16] </ref>, [19], [5], [20], [14], [13], [15]. All is not as rosy at it might seem, though. The graphical level is not capable of representing all interesting structural information which might simplify representation or inference. <p> However, a number of restricted interaction models have been identified which have lower space and time ED 2 " ! " ! " ! " ! " ! @ @ @I @ @ @I 2. LOCAL EXPRESSION LANGUAGES FOR PROBABILISTIC KNOWLEDGE complexity than the full conditional. The noisy-or <ref> [16] </ref>, [17], [9], [12], [1] for example, can be used to model independent causes of an event, and inference complexity is linear in both space and time in the number of antecedents for many inferences. <p> If the interaction between the effects of and on in the net shown in figure 1 can be modeled as a noisy-or interaction, then we might write the following expression for the dependence of on and , following Pearl <ref> [16] </ref>: ( = ) = 1 (1 ( ))(1 ( )) Where ( ) is the probability that is true given that is true and is false . We use rather than to emphasize that these are not standard conditional probabilities. We will use a slightly more compact notation. <p> The full expression language has been implemented and is in use on a variety of monitoring, diagnosis, assessment and control projects. Acknowledgements Thanks to Bob Fung of Prevision, Inc. and Peter Raulefs and Bob Culley of Intel for many useful discussions. Thanks to NSF <ref> (IRI88-21660, IRI91-00530) </ref>, AFOSR, and Intel for providing the support which made this work possible.
Reference: 17. <author> Y. Peng and J. Reggia. </author> <title> A probabilistic causal model for diagnostic problem solving part 1: Integrating symbolic causal inference with numeric probabilistic inference. </title> , <address> SMC-17(2):146-162, </address> <year> 1987. </year>
Reference-contexts: However, a number of restricted interaction models have been identified which have lower space and time ED 2 " ! " ! " ! " ! " ! @ @ @I @ @ @I 2. LOCAL EXPRESSION LANGUAGES FOR PROBABILISTIC KNOWLEDGE complexity than the full conditional. The noisy-or [16], <ref> [17] </ref>, [9], [12], [1] for example, can be used to model independent causes of an event, and inference complexity is linear in both space and time in the number of antecedents for many inferences.
Reference: 18. <author> H. </author> <title> Raiffa. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1968. </year>
Reference-contexts: Second, as Dagum and Galper point out, the decomposition can be into conditioning subsets rather than individual variables. Finally, decision models can be represented as Bayesian nets, as noted by Cooper [3]. Value structures are often factorable. Consider, for example, the classic drilling example <ref> [18] </ref>. The overall utility is the value of the oil recovered, minus the cost of drilling and the cost of testing: ( ) = In this case it is interesting to look at sample numeric values in the various component distribu tions: 5 D A fl fl modifiers 6 3.
Reference: 19. <author> R. Shachter. </author> <title> Evaluating influence diagrams. </title> , <address> 34(6):871 - 882, </address> <month> November-December </month> <year> 1986. </year>
Reference-contexts: When the graph is sparse, this will involve a much smaller set of numbers than the full joint. Equally important, the graphical structure can be used to guide processing to find efficient ways to evaluate queries against the model. For more details, see [16], <ref> [19] </ref>, [5], [20], [14], [13], [15]. All is not as rosy at it might seem, though. The graphical level is not capable of representing all interesting structural information which might simplify representation or inference.
Reference: 20. <author> R. Shachter, B. D'Ambrosio, and B. DelFavero. </author> <title> Symbolic probabilistic inference in belief networks. In , pages 126-131. </title> <publisher> AAAI, </publisher> <month> August </month> <year> 1990. </year>
Reference-contexts: When the graph is sparse, this will involve a much smaller set of numbers than the full joint. Equally important, the graphical structure can be used to guide processing to find efficient ways to evaluate queries against the model. For more details, see [16], [19], [5], <ref> [20] </ref>, [14], [13], [15]. All is not as rosy at it might seem, though. The graphical level is not capable of representing all interesting structural information which might simplify representation or inference. <p> Dvalue &lt; var &gt; : D E exp E c c c c t t t;f t t;f f t;f f t;f (distribution Dvalue/test ((value t f)) ((test y n)) (( y) .03 -.03) We ignore evidence for purposes of this introduction. It introduces only minor complications, see [5], <ref> [20] </ref> for details. There seems to be something very strange in the above, namely a negative value in a distribution. The explanation is simple: the three / distributions jointly define the full conditional. The first is the base definition, and so each row adds to 1 0 as we expect. <p> This will be needed later when we show how the method can be extended to perform inference over nets defined using our local expression language. For further details, see [5] or <ref> [20] </ref>. Computation of probabilities in a Bayesian net can be done quite straightforwardly, albeit somewhat inefficiently . I illustrate this process with a simple network, shown in figure 1. <p> We will refer to each such subset as a , and use the following algorithm to combine these factors. Note also that intermediate results will not, in general, be true probability distributions, but will rather be generalized distributions, as defined in <ref> [20] </ref>. Partition the set of factors into independent (ie, no shared variables) subsets if possible. Evaluate each subset separately using the following method, then combine the results: Construct a A which contains all factors to be chosen for the next combination (initially all the relevant net distributions). <p> We began our exploration of probabilistic inference in the context of truth maintenance systems, and at that time used symbolic representation at the level of individual probability mass elements [6]. Later, motivated by efficiency concerns, we changed to a symbolic representation at the distribution level <ref> [20] </ref>. We now seem to have come full circle: the implementation described here again performs symbolic reasoning on elements as small as individual probabilities. The difference is that we now have a choice of representation grain-size, and can select the grain-size appropriate for the dependence model being described.
Reference: 21. <author> R. Shachter and R. Fung. </author> <title> Contingent influence diagrams. </title> <type> Tech report, </type> <institution> Advanced Decision Systems, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: The noisy-or [16], [17], [9], [12], [1] for example, can be used to model independent causes of an event, and inference complexity is linear in both space and time in the number of antecedents for many inferences. Similarly, contingencies <ref> [21] </ref>, as well as the value-dependent independence exploited in similarity nets [11] are inefficiently represented using a full conditional. <p> The various costs and values are scaled to keep utility in the range [0, 1.0] as described by Cooper. Another form of asymmetry occurs when a variable has a contingent existence . For example, might only be defined when = , and might depend on the values of and <ref> [21] </ref>: ( ) = + The second term in the above expression may be unexpected. Remember, however, that in a normal Bayesian network we must be able to recover the joint across all variables by forming the product of all distributions. <p> The view within SPI of inference as essentially a symbolic algebra factoring problem, however, is readily extensible, as we have shown. Fung and Shachter <ref> [21] </ref>, have proposed a general representation for contingencies and asymmetries. Their representation is more general than the one described here in one important way: it permits representation of asymmetries which induce cycles. For example, may depend on when = , but may depend on when = .
Reference: 22. <author> S. Srinivas. </author> <title> A generalization of the noisy-or model. </title> <booktitle> In , pages 208-218, </booktitle> <month> July </month> <year> 1993. </year>
Reference-contexts: However, there are generalizations to the multi-valued case which require ( 1) or ( 1) parameters for each antecedent, where is the number of values a variable can take. For example, the local expression language presented here can represent the Noisy Max generalization of Srinivas <ref> [22] </ref>. Prior work with Intel provides one example of the efficiency of the methods presented. In this work we explored the application of Bayesian networks to diagnosis of problems in semiconductor fabrication. <p> Andreassen [2] presents a convenient method for describing interaction models such as noisy or. Dagum and Galper [4] have proposed an additive decomposition of conditional dependence that is easily captured using local expressions. Srinivas <ref> [22] </ref> has developed a generalized noisy-or model which is compatible in spirit, but might require extension of the set of operators available in our local expression language. This, together with extension to capture influence diagrams, is work in progress. 15 8.
References-found: 23

