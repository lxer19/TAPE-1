URL: http://www.cogs.susx.ac.uk/users/christ/papers/statistical-biases.ps
Refering-URL: http://www.cogs.susx.ac.uk/users/christ/index-noframes.html
Root-URL: 
Email: Email: Chris.Thornton@cogs.susx.ac.uk  
Phone: Tel: (44)1273 678856  
Title: Statistical Biases in Backpropagation Learning  Keywords: Cognitive Science, Pattern recognition  
Author: Chris Thornton 
Date: June 12, 1996  
Web: WWW: http://www.cogs.susx.ac.uk  
Address: Brighton BN1 9QN  
Affiliation: Cognitive and Computing Sciences University of Sussex  
Abstract: The paper investigates the statistical effects which may need to be exploited in supervised learning. It notes that these effects can be classified according to their conditionality and their order and proposes that learning algorithms will typically have some form of bias towards particular classes of effect. It presents the results of an empirical study of the statistical bias of backpropagation. The study involved applying the algorithm to a wide range of learning problems using a variety of different internal architectures. The results of the study revealed that backpropagation has a very specific bias in the general direction of statistical rather than relational effects. The paper shows how the existence of this bias effectively constitutes a weakness in the algorithm's ability to discount noise. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Duda, R. and Hart, P. </author> <year> (1973). </year> <title> Pattern Classification and Scene Analysis. </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference-contexts: To have any chance of success the learner requires some source of feedback regarding the mapping to be acquired. In the much studied supervised learning scenario, this feedback takes the form of a set 1 of examples taken from the target mapping. <ref> [1] </ref> The learner's aim is to arrive at the point at which it is able to map any input taken from the mapping onto its associated output.
Reference: [2] <author> Thornton, C. </author> <year> (1993). </year> <title> Supervised learning of conditional approach: a case study. </title> <type> CSRP 308, </type> <institution> Cognitive and Computing Sciences, University of Sussex, UK. </institution>
Reference-contexts: They are justified indirectly if they cannot be observed directly but can be systematically derived from those examples; or | which amounts to the same thing | if they can be observed in data which are derived from the original data. <ref> [2] </ref>. In this paper I will be concerned only with the direct (i.e., statistical) form of justification, and with the ways it which it is exploited by the backpropagation learning algorithm (but see [3]). The nature of this form of justification can be illustrated with an example.
Reference: [3] <author> Clark, A. and Thornton, C. </author> <year> (1993). </year> <title> Trading spaces: computation, representation and the limits of learning. </title> <booktitle> Cognitive Science Research Paper 291, </booktitle> <address> Brighton BN1 9QH: </address> <institution> University of Sussex (Price:1.50). </institution>
Reference-contexts: In this paper I will be concerned only with the direct (i.e., statistical) form of justification, and with the ways it which it is exploited by the backpropagation learning algorithm (but see <ref> [3] </ref>). The nature of this form of justification can be illustrated with an example. Consider the following training set. This is based on two input variables (x 1 and x 2 ) and one output variable (y 1 ). There are six training examples in all.
Reference: [4] <author> Quinlan, J. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <booktitle> Machine Learning, </booktitle> <pages> 1 (pp. 81-106). </pages>
Reference-contexts: It is to expected therefore that general-purpose learning algorithms such as ID3 <ref> [4] </ref> and Backpropagation [5] will be able to exploit them most effectively.
Reference: [5] <author> Rumelhart, D., Hinton, G. and Williams, R. </author> <year> (1986). </year> <title> Learning representations by back-propagating errors. </title> <booktitle> Nature, </booktitle> <pages> 323 (pp. 533-6). </pages>
Reference-contexts: It is to expected therefore that general-purpose learning algorithms such as ID3 [4] and Backpropagation <ref> [5] </ref> will be able to exploit them most effectively.
Reference: [6] <author> McClelland, J. and Rumelhart, D. </author> <year> (1988). </year> <title> Explorations in Parallel Distributed Processing: A handbook of Models, Programs, and Exercises. </title> <address> Cam-bridge, Mass.: </address> <publisher> MIT Press. </publisher>
Reference-contexts: The study showed that although backpropagation exploits all statistical effects quite effectively it often deals with higher-order effects better than with low order effects. 3 The study The study involved training a standard backpropagation implementation (the PDP package of <ref> [6] </ref> was used) on a variety of artificial learning problems. The 4 solution of each artificial learning problem was based on a statistical effect of a particular order. The results showed clearly that the generalization performance of backpropagation varies monotonically with the order of the underlying statistical effect.
Reference: [7] <author> Hinton, G. and Sejnowski, T. </author> <year> (1986). </year> <title> Learning and relearning in boltzmann machines. </title> <editor> In D. Rumelhart, J. McClelland and the PDP Research Group (Eds.), </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructures of Cognition. Vols I and II (pp. </booktitle> <pages> 282-317). </pages> <address> Cambridge, Mass.: </address> <publisher> MIT Press. </publisher> <pages> 9 </pages>
Reference-contexts: In other words, we would hope to see zero weights on all the connections from all input units except input unit number 1. However, when we come to examine the Hinton diagram <ref> [7] </ref> for non-input nodes in the the network (see Figure 3), we certainly do not see this effect. In fact what we see is that the learning has produced quite pronounced 7 positive and negative weightings for connections from higher-numbered input units.
References-found: 7

