URL: ftp://cns.brown.edu/nin/papers/connsci.ps.Z
Refering-URL: http://www.math.tau.ac.il/~nin/research.html
Root-URL: 
Email: nin@math.tau.ac.il  edelman@wisdom.weizmann.ac.il  
Phone: Phone 401-863-3920  
Title: Making a low-dimensional representation suitable for diverse tasks  
Author: Nathan Intrator Shimon Edelman 
Keyword: Runing head: Biasing Neural Networks Keywords: Imposing Bias on NN, Multiple Task Training, Transfer in NN  
Note: To appear: Connection Science, Special issue on Transfer in Neural Networks. Current address:  
Date: May, 1996  
Address: Tel Aviv 69978, Israel  Rehovot 76100, Israel  Box 1843, Brown University, Providence, RI 02912,  
Affiliation: School of Mathematical Sciences Sackler Faculty of Exact Sciences Tel Aviv University  Dept. of Applied Mathematics and Computer Science The Weizmann Institute of Science  Institute for Brain and Neural Systems,  
Abstract: We introduce a new approach to the training of classifiers for performance on multiple tasks. The proposed hybrid training method leads to improved generalization via a better low-dimensional representation of the problem space. The quality of the representation is assessed by embedding it in a 2D space using multidimensional scaling, allowing a direct visualization of the results. The performance of the approach is demonstrated on a highly nonlinear image classification task. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Baxt, W. G. and White, H. </author> <year> (1995). </year> <title> Bootstrapping confidence intervals for clinical input variable effects in network trained to identify the presence of acute myocardial infraction. </title> <journal> Neural Computation, </journal> <volume> 7(3) </volume> <pages> 624-638. </pages>
Reference-contexts: In such cases, innovative use of training data becomes essential. Methods for data reuse such as cross-validation (Stone, 1974) and bootstrap (Efron and Tibshirani, 1993) can help in obtaining confidence intervals <ref> (Baxt and White, 1995) </ref> and improved performance (Breiman, 1992; Breiman, 1994; LeBlanc and Tibshirani, 1994; Raviv and Intrator, 1995). Unlike data, class labels are not often reused (see however, (Grossman and Lapedes, 1993)), in particular, multiple-class labels. <p> Along these lines, Baxter has proposed recently to train different data sets with the same class association, on the same network, for constructing a rich internal representation <ref> (Baxter, 1995) </ref>.
Reference: <author> Baxter, J. </author> <year> (1995). </year> <title> Learning internal representations. </title> <booktitle> In Proc. </booktitle> <address> COLT'95. </address>
Reference-contexts: Along these lines, Baxter has proposed recently to train different data sets with the same class association, on the same network, for constructing a rich internal representation <ref> (Baxter, 1995) </ref>.
Reference: <author> Bellman, R. E. </author> <year> (1961). </year> <title> Adaptive Control Processes. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ. </address>
Reference-contexts: Unfortunately, the recovery of LDR by training a classifier with the class labels alone is highly nontrivial, because, in general, the class labels do not possess enough structure to direct the classifier to the correct solution. This is another manifestation of the curse of dimensionality <ref> (Bellman, 1961) </ref>, which explains why in general there is not enough data to recover the true model (the underlying representation) directly from the classification task. In fact, searching for LDR using a combination of exploratory and class-label approaches together yields 5 in some cases better results (Intrator, 1993).
Reference: <author> Borg, I. and Lingoes, J. </author> <year> (1987). </year> <title> Multidimensional Similarity Structure Analysis. </title> <publisher> Springer, </publisher> <address> Berlin. </address>
Reference-contexts: This observation illustrates the point made by Borg and Lingoes, who caution against using the MDS stress as a sole indicator of the goodness of the configuration <ref> (Borg and Lingoes, 1987) </ref>.
Reference: <author> Breiman, L. </author> <year> (1992). </year> <title> Stacked regression. </title> <type> Technical Report TR-367, </type> <institution> Department of Statistics, University of California, Berkeley. </institution> <note> 14 Breiman, </note> <author> L. </author> <year> (1994). </year> <title> Bagging predictors. </title> <type> Technical Report TR-421, </type> <institution> Department of Statistics, University of California, Berkeley. </institution>
Reference: <author> Brigham, J. C. </author> <year> (1986). </year> <title> The influence of race on face recognition. </title> <editor> In Ellis, H. D., Jeeves, M. A., and Newcombe, F., editors, </editor> <booktitle> Aspects of face processing, </booktitle> <pages> pages 170-177. </pages> <publisher> Martinus Nijhoff, Dordrecht. </publisher>
Reference-contexts: A favorite example is the own-race effect in face recognition: people perform much better in various face perception tasks when the faces that serve as stimuli belong to the same race as the subjects <ref> (Brigham, 1986) </ref>. This kind of transfer has been reported recently also with random-dot patterns, generated according to a set of complex statistical criteria (McLaren et al., 1994). A recent review (Reder and Klatzky, 1994) lists a number of issues regarding transfer on which experimental evidence can be brought to bear.
Reference: <author> Caruana, R. </author> <year> (1993). </year> <title> Multitask connectionist learning. </title> <booktitle> In Proceedings of the 1993 Connectionist Models Summer School, </booktitle> <pages> pages 372-379, </pages> <address> San Mateo, CA. </address>
Reference-contexts: Thus, our LDR-based approach to the transfer of 2 learning may be called Manhattan Transfer. It has been observed in the past that training a classifier on multiple tasks (using the same data) may be an efficient way to introduce desirable bias into the solution <ref> (Caruana, 1993) </ref>. Our motivation for multiple-task training is, however, fundamentally different from the subsequent development of that idea by Caruana (1995), who implicitly assumes that the different tasks are on the same level of categorization.
Reference: <author> Caruana, R. </author> <year> (1995). </year> <title> Learning many related tasks at the same time with backpropagation. </title> <editor> In Tesauro, G., Touretzky, D., and Leen, T., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 7, </volume> <pages> pages 657-664. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Cutzu, F. and Edelman, S. </author> <year> (1995). </year> <title> Explorations of shape space. </title> <type> CS-TR 95-01, </type> <institution> Weizmann Institute of Science. </institution>
Reference: <author> Edelman, S. </author> <year> (1995a). </year> <title> Representation of similarity in 3D object discrimination. </title> <journal> Neural Computation, </journal> <volume> 7 </volume> <pages> 407-422. </pages>
Reference: <author> Edelman, S. </author> <year> (1995b). </year> <title> Representation, Similarity, and the Chorus of Prototypes. </title> <journal> Minds and Machines, </journal> <volume> 5 </volume> <pages> 45-68. </pages>
Reference-contexts: Thus, the LDR of the stimulus space should be well defined for all patterns in the vicinity of the familiar ones, 3 although classification performance may be expected to be better for the familiar patterns proper; cf. the distinction between persistent and ephemeral entities in the representational scheme proposed in <ref> (Edelman, 1995b) </ref>. 1.1.2 Low-dimensional representation as a substrate for transfer in visual perception The hypothesis, stated in the preceding Section, that transfer across classification tasks is supported by learning a common LDR for the set of patterns, has been entertained for decades in the context of perceptual generalization (e.g., in the
Reference: <author> Efron, B. and Tibshirani, R. </author> <year> (1993). </year> <title> An introduction to the bootstrap. </title> <publisher> Chapman and Hall, London. </publisher>
Reference-contexts: In practice however, very often the amount of data is far smaller than what is assumed for the required generalization task. In such cases, innovative use of training data becomes essential. Methods for data reuse such as cross-validation (Stone, 1974) and bootstrap <ref> (Efron and Tibshirani, 1993) </ref> can help in obtaining confidence intervals (Baxt and White, 1995) and improved performance (Breiman, 1992; Breiman, 1994; LeBlanc and Tibshirani, 1994; Raviv and Intrator, 1995). Unlike data, class labels are not often reused (see however, (Grossman and Lapedes, 1993)), in particular, multiple-class labels.
Reference: <author> Gasser, M. </author> <year> (1995). </year> <title> Transfer in a connectionist model of the acquisition of morphology. </title> <type> CogSci TR 147, </type> <institution> Indiana University, Bloomington, </institution> <note> IN. an expanded version of a paper presented at the Morphology Workshop, </note> <institution> Nijmegen, </institution> <month> June 13, </month> <year> 1995. </year>
Reference-contexts: We do not consider this approach in the present paper. 2 I.e., high-resolution patterns or images of objects, as opposed to uniform-color patches or repeating textures. An example of a nonvisual task of a parallel level of complexity is morphological inflection <ref> (Gasser, 1995) </ref>.
Reference: <author> Grossman, T. and Lapedes, A. </author> <year> (1993). </year> <title> Use of bad training data for better prediction. </title> <editor> In J. D. Cowan, G. T. and Alspector, J., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 6, </volume> <pages> pages 342-350. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Unlike data, class labels are not often reused (see however, <ref> (Grossman and Lapedes, 1993) </ref>), in particular, multiple-class labels. Humans make natural and extensive use of the fact that objects may have several class associations (say, at different category levels).
Reference: <author> Hintzman, D. L. </author> <year> (1994). </year> <title> Twenty-five years of learning and memory: was the cognitive revolution a mistake? In Umilta, </title> <editor> C. and Moscovitch, M., editors, </editor> <title> Attention and Performance, </title> <booktitle> volume XV, chapter 16, </booktitle> <pages> pages 360-391. </pages> <publisher> MIT Press. </publisher>
Reference-contexts: This ubiquity of transfer makes a claim that "all demonstrations of learning and memory involve transfer" <ref> (Hintzman, 1994) </ref> easily understood.
Reference: <author> Hofmann, T. and Buhmann, J. </author> <year> (1994). </year> <title> Multidimensional scaling and data clustering. </title> <editor> In J. D. Cowan, G. T. and Alspector, J., editors, </editor> <booktitle> Neural Information Processing Systems, </booktitle> <volume> volume 7, </volume> <pages> pages 459-466. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Recent improvements of the MDS procedure include an implementation using deterministic annealing <ref> (Hofmann and Buhmann, 1994) </ref>, which may prove to be better in avoiding local minima in the search for an optimal configuration.
Reference: <author> Huber, P. J. </author> <year> (1985). </year> <title> Projection pursuit (with discussion). </title> <journal> The Annals of Statistics, </journal> <volume> 13 </volume> <pages> 435-475. </pages>
Reference: <author> Intrator, N. </author> <year> (1993). </year> <title> Combining exploratory projection pursuit and projection pursuit regression with application to neural networks. </title> <journal> Neural Computation, </journal> <volume> 5(3) </volume> <pages> 443-455. </pages>
Reference-contexts: In fact, searching for LDR using a combination of exploratory and class-label approaches together yields 5 in some cases better results <ref> (Intrator, 1993) </ref>.
Reference: <author> Intrator, N. and Cooper, L. N. </author> <year> (1992). </year> <title> Objective function formulation of the BCM theory of visual cortical plasticity: Statistical connections, stability conditions. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 3-17. </pages>
Reference: <author> Kramer, A. F., Strayer, D. L., and Buckley, J. </author> <year> (1990). </year> <title> Development and transfer of automatic processing. </title> <journal> Journal of Experimental Psychology: Human Perception and Performance, </journal> <volume> 16 </volume> <pages> 505-522. </pages>
Reference-contexts: Logan, 1988; Maddox and Ashby, 1993). Although transfer is facilitated by having identical elements in the two tasks, as noted in (Reder and Klatzky, 1994), there is evidence that exemplar substitution affects transfer to a much smaller degree than context (rule) substitution <ref> (Kramer et al., 1990) </ref>.
Reference: <author> Kruskal, J. B. </author> <year> (1964). </year> <title> Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis. </title> <journal> Psychometrika, </journal> <volume> 29(1) </volume> <pages> 1-27. </pages>
Reference-contexts: The power of MDS as a tool for the study of internal representations was revealed when Shepard discovered in 1962 that fixing the relative distances of a set of points effectively determines their coordinates (Shepard, 1966). This discovery led to the development of the nonmetric MDS algorithm <ref> (Kruskal, 1964) </ref>, which employs gradient descent to seek a monotonic transformation between measured distances and distances computed from the hypothesized point configuration, which would minimize stress (defined as the discrepancy between the ranks of the measured and the computed distances).
Reference: <author> Lando, M. and Edelman, S. </author> <year> (1995). </year> <title> Receptive field spaces and class-based generalization from a single view in face recognition. </title> <journal> Network, </journal> <volume> 6 </volume> <pages> 551-576. </pages>
Reference-contexts: particular kind of circumstances under which transfer is known to occur, namely, on tasks that involve perceptual classification of complex visual stimuli. 2 1 A complementary approach here is to learn, instead of a variety of labeling schemes for a given data set, the transformations which leave its members invariant <ref> (Lando and Edelman, 1995) </ref>, or the invariances of the individual data items (Simard et al., 1992; Thrun and Mitchell, 1995). We do not consider this approach in the present paper. 2 I.e., high-resolution patterns or images of objects, as opposed to uniform-color patches or repeating textures.
Reference: <author> LeBlanc, M. and Tibshirani, R. </author> <year> (1994). </year> <title> Combining estimates in regression and classification. </title> <type> Preprint. </type>
Reference: <author> Logan, G. </author> <year> (1988). </year> <title> Towards an instance theory of automatization. </title> <journal> Psychological Review, </journal> <volume> 95 </volume> <pages> 492-527. </pages>
Reference: <author> Maddox, W. T. and Ashby, F. G. </author> <year> (1993). </year> <title> Comparing decision bound and exemplar models of categorization. </title> <journal> Perception and Psychophysics, </journal> <volume> 53 </volume> <pages> 49-70. </pages>
Reference: <author> Martin, G. </author> <year> (1988). </year> <title> The effects of old learning on new in hopfield and backpropagation nets. </title> <type> Technical Report ACA-HI-019, </type> <institution> Microelectronics and Computer Technology Corporation (MCC). </institution> <note> 15 McLaren, </note> <author> I. P. L., Leevers, H. J., and Mackintosh, N. J. </author> <year> (1994). </year> <title> Recognition, categorization, and perceptual learning (or, how learning to classify things together helps one to tell them apart). </title> <editor> In Umilta, C. and Moscovitch, M., editors, </editor> <title> Attention and Performance, </title> <booktitle> volume XV, chapter 35, </booktitle> <pages> pages 889-909. </pages> <publisher> MIT Press. </publisher>
Reference: <author> Murre, J. M. J. </author> <year> (1995). </year> <title> Transfer of learning in backpropagation networks and in related neural network models. </title> <editor> In Levy, Bairaktaris, Bullinaria, and Cairns, editors, </editor> <title> Connectionist Models of Memory and Language. </title> <publisher> UCL Press, </publisher> <address> London. </address> <note> To appear. </note>
Reference: <author> Pickover, C. </author> <year> (1990). </year> <title> Computers, Pattern, Chaos, and Beauty. </title> <address> St. </address> <publisher> Martin's Press. </publisher>
Reference-contexts: The *, + and o in the picture represent additional class labels that were used as an additional information (bias) during training (see Figure 10.) 2.1 Data generation The fractal patterns were generated using publicly available software (Xfractint 2.03) <ref> (Pickover, 1990, chapter 10) </ref>, and were imported into Matlab for processing and classification. We chose the quaternion Julia set (entry quatjul in the Xfractint pattern menu), which is parameterized by six variables and is therefore well-suited for generating complicated patterns that depend on up to six parameters. <p> iteration formula is q (0) = (xpixel; ypixel; z j ; z k ) q (n + 1) = q (n) fl q (n) + c; where both q and c = (c 1 ; c i ; c j ; c k ) are quaternions (for further details, see <ref> (Pickover, 1990) </ref>, chapter 10). The three dimensions shown in Figure 1 correspond to the variation of parameters c 1 ; c j , and c k , respectively.
Reference: <author> Pratt, L. Y. </author> <year> (1993). </year> <title> Transferring previously learned back-propagation neural networks to new learning tasks. </title> <type> Technical report ml-tr-37, </type> <institution> Rutgers University, CS Dept. </institution>
Reference-contexts: This approach naturally facilitates generalization across tasks, also known as transfer of skill | a hallmark of human cognitive prowess (see Section 1.1). Connectionist approaches to the problem of transfer, or cross-task generalization <ref> (Pratt, 1993) </ref> tend to offer as a solution some kind of information sharing between networks trained on different tasks. Along these lines, Baxter has proposed recently to train different data sets with the same class association, on the same network, for constructing a rich internal representation (Baxter, 1995).
Reference: <author> Price, D., Knerr, S., Personnaz, L., and Dreyfus, G. </author> <year> (1995). </year> <title> Pairwise neural network classifiers with probabilistic outputs. </title> <editor> In G. Tesauro, D. S. T. and Leen, T. K., editors, </editor> <booktitle> Advances in Neural Information Processing 7, </booktitle> <pages> pages 1109-1116. </pages> <publisher> MIT Press. </publisher>
Reference-contexts: We note that a natural extrapolation of this strategy would be to teach the network many possible dichotomies, in the hope that the structure of the underlying LDR can be recovered from the multiple two-way classifications <ref> (Price et al., 1995) </ref>. The advantage of operating at the level of 18 classes (or of three classes, with six subclasses each) is in the much shorter training procedure.
Reference: <author> Raviv, Y. and Intrator, N. </author> <year> (1995). </year> <title> Bootstrapping with noise: An effective regularization technique. </title> <note> Submitted to Connection Science, Special issue on Combining Estimators. </note>
Reference: <author> Reder, L. and Klatzky, R. L. </author> <year> (1994). </year> <title> Transfer: training for performance. </title> <editor> In Druckman, D. and Bjork, R. A., editors, </editor> <title> Learning, remembering, believing: enhancing human performance, </title> <booktitle> chapter 3, </booktitle> <pages> pages 25-56. </pages> <publisher> National Academy Press, </publisher> <address> Washington, DC. </address> <note> Also available as TR CMU-CS-94-187; The effect of context on training: is learning situated? Sammon, </note> <author> J. W. </author> <year> (1969). </year> <title> A nonlinear mapping for data structure analysis. </title> <journal> IEEE Trans. Comput., </journal> <volume> 18 </volume> <pages> 401-409. </pages> <note> Sas (1989). SAS/STAT User's Guide, Version 6. </note> <institution> SAS Institute Inc., Cary, NC. </institution>
Reference-contexts: In the former example, the degree of generalization between stimuli is governed by their perceptual similarity (Shepard, 1987), while the latter transfer is usually hypothesized to be mediated by more complex cognitive structures or schemata <ref> (Reder and Klatzky, 1994) </ref>. This ubiquity of transfer makes a claim that "all demonstrations of learning and memory involve transfer" (Hintzman, 1994) easily understood. <p> This kind of transfer has been reported recently also with random-dot patterns, generated according to a set of complex statistical criteria (McLaren et al., 1994). A recent review <ref> (Reder and Klatzky, 1994) </ref> lists a number of issues regarding transfer on which experimental evidence can be brought to bear. <p> Logan, 1988; Maddox and Ashby, 1993). Although transfer is facilitated by having identical elements in the two tasks, as noted in <ref> (Reder and Klatzky, 1994) </ref>, there is evidence that exemplar substitution affects transfer to a much smaller degree than context (rule) substitution (Kramer et al., 1990).
Reference: <author> Shepard, R. N. </author> <year> (1966). </year> <title> Metric structures in ordinal data. </title> <journal> J. Math. Psychology, </journal> <volume> 3 </volume> <pages> 287-315. </pages>
Reference-contexts: The power of MDS as a tool for the study of internal representations was revealed when Shepard discovered in 1962 that fixing the relative distances of a set of points effectively determines their coordinates <ref> (Shepard, 1966) </ref>.
Reference: <author> Shepard, R. N. </author> <year> (1980). </year> <title> Multidimensional scaling, tree-fitting, and clustering. </title> <journal> Science, </journal> <volume> 210 </volume> <pages> 390-397. </pages>
Reference-contexts: imposed by multitask training bias the internal representation of the MLP to a better solution and thus serve to reduce the variance due to the limit on the capacity of the learning machine. 3.3 MDS as a tool for representation visualization Nonmetric MDS, which is relatively widely used in exploratory <ref> (Shepard, 1980) </ref> as well as confirmatory (Edelman, 1995a; Cutzu and Edelman, 1995) data analysis in experimental psychology, has been only rarely applied in the study of representations produced by neural networks.
Reference: <author> Shepard, R. N. </author> <year> (1987). </year> <title> Toward a universal law of generalization for psychological science. </title> <journal> Science, </journal> <volume> 237 </volume> <pages> 1317-1323. </pages>
Reference-contexts: In the former example, the degree of generalization between stimuli is governed by their perceptual similarity <ref> (Shepard, 1987) </ref>, while the latter transfer is usually hypothesized to be mediated by more complex cognitive structures or schemata (Reder and Klatzky, 1994). This ubiquity of transfer makes a claim that "all demonstrations of learning and memory involve transfer" (Hintzman, 1994) easily understood. <p> In particular, it has been observed that the human visual system performs as if it represents the stimuli in a low-dimensional metric psychological space (see <ref> (Shepard, 1987) </ref>, for a review). Recently Edelman and colleagues have investigated the ability of human subjects to form low-dimensional representations in the context of complex 3D shape classification (Edelman, 1995a; Cutzu and Edelman, 1995).
Reference: <author> Simard, P., Victorri, B., LeCun, Y., and Denker, J. </author> <year> (1992). </year> <title> Tangent prop a formalism for specifying selected invariances in an adaptive network. </title> <editor> In Moody, J., Lippman, R., and Hanson, S. J., editors, </editor> <booktitle> Neural Information Processing Systems, </booktitle> <volume> volume 4, </volume> <pages> pages 895-903. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Stone, M. </author> <year> (1974). </year> <title> Cross-validatory choice and assessment of statistical predictions (with discussion). </title> <journal> J. Royal Statistics Society B, </journal> <volume> 36 </volume> <pages> 111-147. </pages>
Reference-contexts: In practice however, very often the amount of data is far smaller than what is assumed for the required generalization task. In such cases, innovative use of training data becomes essential. Methods for data reuse such as cross-validation <ref> (Stone, 1974) </ref> and bootstrap (Efron and Tibshirani, 1993) can help in obtaining confidence intervals (Baxt and White, 1995) and improved performance (Breiman, 1992; Breiman, 1994; LeBlanc and Tibshirani, 1994; Raviv and Intrator, 1995).
Reference: <author> Thrun, S. and Mitchell, T. </author> <year> (1995). </year> <title> Learning one more thing. </title> <editor> In Mellish, C., editor, </editor> <booktitle> Proc. 14th IJCAI, </booktitle> <volume> volume 2, </volume> <pages> pages 1217-1223, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>

References-found: 38

