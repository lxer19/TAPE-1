URL: http://www.cs.berkeley.edu/~nir/Papers/FGL1.ps
Refering-URL: http://www.cs.berkeley.edu/~nir/Abstracts/FGL1.html
Root-URL: http://www.cs.berkeley.edu
Email: nir@cs.berkeley.ed  moises@erg.sri.com  tomlee@erg.sri.com  
Title: Bayesian Network Classification with Continuous Attributes: Getting the Best of Both Discretization and Parametric Fitting  
Author: Nir Friedman Moises Goldszmidt Thomas J. Lee 
Address: Berkeley, CA 94920  333 Ravenswood Avenue Menlo Park, CA 94025  333 Ravenswood Avenue Menlo Park, CA 94025  
Affiliation: Computer Science Division University of California  SRI International  SRI International  
Abstract: In a recent paper, Friedman, Geiger, and Goldszmidt [8] introduced a classifier based on Bayesian networks, called Tree Augmented Naive Bayes (TAN), that outperforms naive Bayes and performs competitively with C4.5 and other state-of-the-art methods. This classifier has several advantages including robustness and polynomial computational complexity. One limitation of the TAN classifier is that it applies only to discrete attributes, and thus, continuous attributes must be prediscretized. In this paper, we extend TAN to deal with continuous attributes directly via parametric (e.g., Gaussians) and semiparametric (e.g., mixture of Gaussians) conditional probabilities. The result is a classifier that can represent and combine both discrete and continuous attributes. In addition, we propose a new method that takes advantage of the modeling language of Bayesian networks in order to represent attributes both in discrete and continuous form simultaneously, and use both versions in the classification. This automates the process of deciding which form of the attribute is most relevant to the classification task. It also avoids the commitment to either a discretized or a (semi)parametric form, since different attributes may correlate better with one version or the other. Our empirical results show that this latter method usually achieves classification performance that is as good as or better than either the purely discrete or the purely continuous TAN models.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. M. Bishop. </author> <title> Neural Networks for Pattern Recognition, </title> <year> 1995. </year>
Reference-contexts: In this paper, we attempt to model the continuous attributes directly within the TAN network. To do so, we need to learn CPDs for continuous attributes. In this section, we discuss Gaussian distributions for such CPDs. The theory of training such representations is standard (see, for example, <ref> [1, 5] </ref>). We only review the indispensable concepts. A more interesting issue pertains to the structure of the network. As we shall see, when we mix discrete and continuous attributes, the algorithms must induce directed trees. <p> Standard arguments (e.g., see <ref> [1] </ref>) show that we can rewrite S (A i j Pa (A i )) as a function of E [A i j pa (A i )] and E [A 2 i j pa (A i )]the expectations of A i and A 2 i in these instances of the data where <p> If we want also to model arcs from continuous to discrete, then we need to introduce additional types of parametric models, such as logistic regression <ref> [1] </ref>. As we will show, an alternative solution is provided by the dual representation approach introduced in this paper. 3.4 SMOOTHING One of the main risks in parameter estimation is overfit-ting. <p> Thus, we need to perform many passes over the training data to learn the parameters. Because of space restrictions we do not review the EM procedure here, and refer the reader to <ref> [1, pp. 65-73] </ref>. With regard to selecting the number of components in the mixture, it is easy to see that a mixture with k+1 components can easily attain the same or better likelihood as any mixture with k components.
Reference: [2] <author> C. K. Chow and C. N. Liu. </author> <title> Approximating discrete probability distributions with dependence trees. </title> <journal> IEEE Trans. on Info. Theory, </journal> <volume> 14 </volume> <pages> 462-467, </pages> <year> 1968. </year>
Reference-contexts: The procedure Construct-TAN builds a TAN network B that maximizes LL (B : D) and has time complexity O (n 2 N ). The TAN classifier is related to the classifier introduced by Chow and Liu <ref> [2] </ref>. That method learns a different tree for each class value. FGG's results show that the TAN and Chow and Liu's classifier perform roughly the same. In domains where there is substantial differences in the interactions between attributes for different class values, Chow and Liu's method performs better.
Reference: [3] <author> T. M. Cover and J. A. Thomas. </author> <title> Elements of Information Theory, </title> <year> 1991. </year>
Reference-contexts: The I () term is the conditional mutual information between A i and A j given C <ref> [3] </ref>. Roughly speaking, it measures how much information A j provides about A i if already know the value of C.
Reference: [4] <author> J. Dougherty, R. Kohavi, and M. Sahami. </author> <title> Supervised and unsupervised dis-cretization of continuous features. </title> <booktitle> In ICML '95. </booktitle> <year> 1995. </year>
Reference-contexts: Since we do not currently deal with missing data, we removed instances with missing values from the data sets. To construct discretizations, we used a variant of the method of Fayyad and Irani [7], using only the training data, in the manner described in <ref> [4] </ref>. These preprocessing stages were carried out by the MLC++ system. We note that experiments with the various learning procedures were carried out on exactly the same training sets and evaluated on exactly the same test sets.
Reference: [5] <author> R. O. Duda and P. E. Hart. </author> <title> Pattern Classification and Scene Analysis, </title> <year> 1973. </year>
Reference-contexts: This is the definition of the naive Bayesian classifier commonly found in the literature <ref> [5] </ref>. The naive Bayesian classifier has been used extensively for classification. It has the attractive properties of being robust and easy to learnwe only need to estimate the CPDs Pr (C) and Pr (A i j C) for all attributes. <p> In this paper, we attempt to model the continuous attributes directly within the TAN network. To do so, we need to learn CPDs for continuous attributes. In this section, we discuss Gaussian distributions for such CPDs. The theory of training such representations is standard (see, for example, <ref> [1, 5] </ref>). We only review the indispensable concepts. A more interesting issue pertains to the structure of the network. As we shall see, when we mix discrete and continuous attributes, the algorithms must induce directed trees.
Reference: [6] <author> S. </author> <title> Even. Graph Algorithms, </title> <year> 1979. </year>
Reference-contexts: A branching is a set of edges that have at most one member pointing into each vertex and does not contain cycles. Finding a maximally weighed branching is a standard graph-theoretic problem that can be solved in low-order polynomial time <ref> [6, 17] </ref>. 4. Construct the TAN model that contains arc from C to each A i , and arc from A j to A i if j ! i is in A.
Reference: [7] <author> U. M. Fayyad and K. B. Irani. </author> <title> Multi-interval discretization of continuous-valued attributes for classification learning. </title> <booktitle> In IJCAI '93. </booktitle> <year> 1993. </year>
Reference-contexts: In experiments run on data sets with continuous attributes, FGG use the prediscretizion described by Fayyad and Irani <ref> [7] </ref> before learning a classifier. In this paper, we attempt to model the continuous attributes directly within the TAN network. To do so, we need to learn CPDs for continuous attributes. In this section, we discuss Gaussian distributions for such CPDs. <p> We denote by A fl k the corresponding discretized attributes (i.e., A fl 1 is the discretized version of A 1 ), based on a predetermined discretization policy (e.g., using a standard method, such as Fayyad and Irani's <ref> [7] </ref>). Given this semantics for the discretized variables, we know that that each A fl i is a deterministic function of A i . <p> Accuracy was evaluated using 5-fold cross validation (using the methods described in [13]). Since we do not currently deal with missing data, we removed instances with missing values from the data sets. To construct discretizations, we used a variant of the method of Fayyad and Irani <ref> [7] </ref>, using only the training data, in the manner described in [4]. These preprocessing stages were carried out by the MLC++ system. We note that experiments with the various learning procedures were carried out on exactly the same training sets and evaluated on exactly the same test sets.
Reference: [8] <author> N. Friedman, D. Geiger, and M. Goldszmidt. </author> <title> Bayesian network classifiers. </title> <journal> Machine Learning, </journal> <volume> 29 </volume> <pages> 131-163, </pages> <year> 1997. </year>
Reference-contexts: Note that the resulting models can (and usually do) involve both parametric and discretized models of interactions among attributes. In this paper we focus our attention on classification tasks. We extend a Bayesian network classifier, introduced by Friedman, Geiger, and Goldszmidt (FGG) <ref> [8] </ref> called Tree Augmented Naive Bayes (TAN). FGG show that TAN outperforms naive Bayes, yet at the same time maintains the computational simplicity (no search involved) and robustness that characterize naive Bayes. <p> Nonetheless, the naive Bayesian classifier embodies the strong independence dashed lines represent edges required by the naive Bayesian clas sifier. The solid lines are the tree augmenting edges representing correlations between attributes. assumption that, given the value of the class, attributes are independent of each other. FGG <ref> [8] </ref> suggest the removal of these independence assumptions by considering a richer class of networks. They define the TAN Bayesian classifier that learns a network in which each attribute has the class and at most one other attribute as parents. <p> Roughly speaking, networks with higher likelihood match the data better. FGG describe a procedure Construct-TAN for learning TAN models and show the following theorem. Theorem 2.1: <ref> [8] </ref> Let D be a collection of N instances of C; A 1 ; : : : ; A n . The procedure Construct-TAN builds a TAN network B that maximizes LL (B : D) and has time complexity O (n 2 N ).
Reference: [9] <author> N. Friedman and M. Goldszmidt. </author> <title> Discretization of continuous attributes while learning Bayesian networks. </title> <booktitle> In ICML '96. </booktitle> <year> 1996. </year>
Reference-contexts: The extension of the dual representation to arbitrary Bayesian networks, and the extension of the discretization approach introduced by Friedman and Goldszmidt <ref> [9] </ref> to take the dual representation into account, are the subjects of current research. 2 REVIEW OF TAN In this discussion we use capital letters such as X; Y; Z for variable names, and lower-case letters such as x; y; z to denote specific values taken by those variables. <p> For example, to deal with several discretizations of the same attributes in order to select the granularity of discretization that is most useful for predicting other attributes. Another direction involves adapting the discretization to the particular edges that are present in the model. As argued Friedman and Goldszmidt <ref> [9] </ref>, it is possible to discretize attributes to gain the most information about the neighboring attributes. Thus, we might follow the approach in [9] and iteratively readjust the structure and discretization to improve the score. <p> Another direction involves adapting the discretization to the particular edges that are present in the model. As argued Friedman and Goldszmidt <ref> [9] </ref>, it is possible to discretize attributes to gain the most information about the neighboring attributes. Thus, we might follow the approach in [9] and iteratively readjust the structure and discretization to improve the score. Finally, it is clear that this hybrid method is applicable not only to classification, but also to density estimation and related tasks using general Bayesian networks. We are currently pursuing these directions.
Reference: [10] <author> A. Gelman, J. B. Carlin, H. S. Stern, and D. B. Rubin. </author> <title> Bayesian Data Analysis. </title> <year> 1995. </year>
Reference-contexts: This smoothing operation is similar to (and motivated by) well-known methods in statistics such as hierarchical Bayesian and shrinkage methods <ref> [10] </ref>. We can think of this smoothing operation as pretending that there are s additional instances in which A j is distributed according to its marginal distribution.
Reference: [11] <author> D. Heckerman and D. Geiger. </author> <title> Learning Bayesian networks: a unification for discrete and Gaussian domains. </title> <booktitle> In UAI '95. </booktitle> <year> 1995. </year>
Reference-contexts: A continuous variable is a Gaussian with mean and variance 2 if the pdf of X has the form '(x : ; 2 ) = 1 p (x) 2 2 2 : If all the parents of a continuous A i are discrete, then we learn a conditional Gaussian CPD <ref> [11, 15] </ref> by assigning to A i different mean a i jpa (A i ) and variance 2 a i jpa (A i ) for each joint value of its parents.
Reference: [12] <author> D. Heckerman, D. Geiger, and D. M. Chickering. </author> <title> Learning Bayesian networks: The combination of knowledge and statistical data. </title> <journal> Machine Learning, </journal> <volume> 20 </volume> <pages> 197-243, </pages> <year> 1995. </year>
Reference-contexts: We describe the procedure for inducing directed trees next. 3.1 THE BASIC PROCEDURE We now extend the TAN algorithm for directed trees. This extension is fairly straight forward and similar ideas have been suggested for learning tree-like Bayesian networks <ref> [12] </ref>. For completeness, and to facilitate later extensions, we rederive the procedure from basic principles. Assume that we are given a data set D that consists of N identically and independently distributed (i.i.d.) instances that assign values to A 1 ; : : : ; A n and C.
Reference: [13] <author> R. Kohavi. </author> <title> A study of cross-validation and bootstrap for accuracy estimation and model selection. </title> <booktitle> In IJCAI '95. </booktitle> <year> 1995. </year>
Reference-contexts: We estimate the prediction accuracy of each classifier as well as the variance of this accuracy by using the MLC++ system [14]. Accuracy was evaluated using 5-fold cross validation (using the methods described in <ref> [13] </ref>). Since we do not currently deal with missing data, we removed instances with missing values from the data sets. To construct discretizations, we used a variant of the method of Fayyad and Irani [7], using only the training data, in the manner described in [4].
Reference: [14] <author> R. Kohavi, G. John, R. Long, D. Manley, and K. Pfleger. MLC++: </author> <title> A machine learning library in C++. </title> <booktitle> In Proc. 6'th Inter. Conf. on Tools with AI, </booktitle> <year> 1994. </year>
Reference-contexts: In (b), the dashed lines connect points that correspond to the same data set. dictions on the test sets of each data set. We estimate the prediction accuracy of each classifier as well as the variance of this accuracy by using the MLC++ system <ref> [14] </ref>. Accuracy was evaluated using 5-fold cross validation (using the methods described in [13]). Since we do not currently deal with missing data, we removed instances with missing values from the data sets.
Reference: [15] <author> S. L. Lauritzen and N. Wermuth. </author> <title> Graphical models for associations between variables, some of which are qualitative and some quantitative. </title> <journal> Annals of Statistics, </journal> <volume> 17 </volume> <pages> 31-57, </pages> <year> 1989. </year>
Reference-contexts: A continuous variable is a Gaussian with mean and variance 2 if the pdf of X has the form '(x : ; 2 ) = 1 p (x) 2 2 2 : If all the parents of a continuous A i are discrete, then we learn a conditional Gaussian CPD <ref> [11, 15] </ref> by assigning to A i different mean a i jpa (A i ) and variance 2 a i jpa (A i ) for each joint value of its parents.
Reference: [16] <author> P. M. Murphy and D. W. Aha. </author> <title> UCI repository of machine learning databases. </title> <note> http://www.ics.uci.edu/mlearn/MLRepository. html, </note> <year> 1995. </year>
Reference-contexts: FGG show that TAN outperforms naive Bayes, yet at the same time maintains the computational simplicity (no search involved) and robustness that characterize naive Bayes. They tested TAN on problems from the UCI repository <ref> [16] </ref>, and compared it to C4.5, naive Bayes, and wrapper methods for feature selection with good results. The original version of TAN is restricted to multinomial distributions and discrete attributes. <p> All of these data sets are from the UCI repository <ref> [16] </ref>, and are accessible at the MLC++ ftp site. The accuracy of each classifier is based on the percentage of successful pre (a) (b) (c) and (c) of H/Mix (x axis) vs. H/Mix-FS (y axis).
Reference: [17] <author> R. Tarjan. </author> <title> Finding optimal branching. </title> <journal> Networks, </journal> <volume> 7 </volume> <pages> 25-35, </pages> <year> 1977. </year>
Reference-contexts: A branching is a set of edges that have at most one member pointing into each vertex and does not contain cycles. Finding a maximally weighed branching is a standard graph-theoretic problem that can be solved in low-order polynomial time <ref> [6, 17] </ref>. 4. Construct the TAN model that contains arc from C to each A i , and arc from A j to A i if j ! i is in A.
References-found: 17

