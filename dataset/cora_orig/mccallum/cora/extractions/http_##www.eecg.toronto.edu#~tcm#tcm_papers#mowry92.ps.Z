URL: http://www.eecg.toronto.edu/~tcm/tcm_papers/mowry92.ps.Z
Refering-URL: http://wwwipd.ira.uka.de/~hopp/seminar97.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Design and Evaluation of a Compiler Algorithm for Prefetching  
Author: Todd C. Mowry, Monica S. Lam and Anoop Gupta 
Address: CA 94305  
Affiliation: Computer Systems Laboratory Stanford University,  
Abstract: Software-controlled data prefetching is a promising technique for improving the performance of the memory subsystem to match today's high-performance processors. While prefetching is useful in hiding the latency, issuing prefetches incurs an instruction overhead and can increase the load on the memory subsystem. As a result, care must be taken to ensure that such overheads do not exceed the benefits. This paper proposes a compiler algorithm to insert prefetch instructions into code that operates on dense matrices. Our algorithm identifies those references that are likely to be cache misses, and issues prefetches only for them. We have implemented our algorithm in the SUIF (Stanford University Intermediate Form) optimizing compiler. By generating fully functional code, we have been able to measure not only the improvements in cache miss rates, but also the overall performance of a simulated system. We show that our algorithm significantly improves the execution speed of our benchmark programs|some of the programs improve by as much as a factor of two. When compared to an algorithm that indiscriminately prefetches all array accesses, our algorithm can eliminate many of the unnecessary prefetches without any significant decrease in the coverage of the cache misses. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> W. Abu-Sufah, D. J. Kuck, and D. H. Lawrie. </author> <title> Automatic program transformations for virtual memory computers. </title> <booktitle> Proc. of the 1979 National Computer Conference, </booktitle> <pages> pages 969-974, </pages> <month> June </month> <year> 1979. </year>
Reference-contexts: One important example of such a transform is blocking <ref> [1, 9, 10, 12, 21, 23, 29] </ref>. Instead of operating on entire rows or columns of an array, blocked algorithms operate on submatrices or blocks, so that data loaded into the faster levels of the memory hierarchy are reused.
Reference: [2] <author> J-L. Baer and T-F. Chen. </author> <title> An effective on-chip preloading scheme to reduce data access penalty. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <year> 1991. </year>
Reference-contexts: Lee [20] proposed an elaborate lookahead scheme for prefetching in a multiprocessor where all shared data is uncacheable. He found that the effectiveness of the scheme was limited by branch prediction and by synchronization. Baer and Chen <ref> [2] </ref> proposed a scheme that uses a history buffer to detect strides. In their scheme, a "look ahead PC" speculatively walks through the program ahead of the normal PC using branch prediction. When the look ahead PC finds a matching stride entry in the table, it issues a prefetch.
Reference: [3] <author> D. Bailey, J. Barton, T. Lasinski, and H. Simon. </author> <title> The NAS Parallel Benchmarks. </title> <type> Technical Report RNR-91-002, </type> <institution> NASA Ames Research Center, </institution> <month> August </month> <year> 1991. </year>
Reference-contexts: This collection includes NASA7 and TOMCATV from the SPEC benchmarks [27], OCEAN a uniprocessor version of a SPLASH benchmark [25], and CG (conjugate gradient), EP (embarassingly parallel), IS (integer sort), MG (multi-grid) from the NAS Parallel Benchmarks <ref> [3] </ref>. Since the NASA7 benchmark really consists of 7 independent kernels, we study each kernel separately (MXM, CFFT2D, CHOLSKY, BTRIX, GMTRY, EMIT and VPENTA).
Reference: [4] <author> D. Callahan, K. Kennedy, and A. Porterfield. </author> <title> Software prefetching. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 40-52, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Ideally, isolating the cache miss instances will not increase the instruction overhead. One of the advantages of having implemented the prefetching schemes in the compiler is that we can quantify this instruction overhead. Previous studies have only been able to estimate instruction overhead <ref> [4] </ref>. <p> Some of these approaches use software support to issue prefetches, while others are strictly hardware-based. In this section, we discuss previous work in both categories. 5.1 Software Prefetching Porterfield <ref> [4, 23] </ref> presented a compiler algorithm for inserting pre-fetches. He implemented it as a preprocessing pass that inserted prefetching into the source code. His initial algorithm prefetched all array references in inner loops one iteration ahead.
Reference: [5] <author> W. Y. Chen, S. A. Mahlke, P. P. Chang, and W. W. Hwu. </author> <title> Data access microarchitectures for superscalar processors with compiler-assisted data prefetching. </title> <booktitle> In Proceedings of Micro-computing 24, </booktitle> <year> 1991. </year>
Reference-contexts: A promising technique to mitigate the impact of long cache miss penalties is software-controlled prefetching <ref> [5, 13, 16, 22, 23] </ref>. Software-controlled prefetching requires support from both hardware and software. The processor must provide a special "prefetch" instruction. <p> This work is targeted for a block prefetch instruction, rather than the single-line prefetches considered in this paper. Chen et al. <ref> [5] </ref> considered prefetching of non-scientific code. They attempt to move address generation back as far as possible before loads to hide a small cache miss latency (10 cycles). The paper also suggested that prefetch buffers would be helpful for reducing the effects of cache pollution.
Reference: [6] <author> R. P. Colwell, R. P. Nix, J. J. O'Donnell, D. B. Papworth, and P. K. Rodman. </author> <title> A vliw architecture for a trace scheduling compiler. </title> <booktitle> In Proc. Second Intl. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 180-192, </pages> <month> Oct. </month> <year> 1987. </year>
Reference-contexts: While the effectiveness of caches has been well established for general-purpose code, their effectiveness for scientific applications has not. One manifestation of this is that several of the scalar machines designed for scientific computation do not use caches <ref> [6, 7] </ref>. 1.1 Cache Performance on Scientific Code To illustrate the need for improving the cache performance of microprocessor-based systems, we present results below for a set of scientific programs. For the sake of concreteness, we pattern our memory subsystem after the MIPS R4000.
Reference: [7] <author> J. C. Dehnert, P. Y.-T. Hsu, and J. P. Bratt. </author> <title> Overlapped loop support in the cydra 5. </title> <booktitle> In Third International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS III), </booktitle> <pages> pages 26-38, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: While the effectiveness of caches has been well established for general-purpose code, their effectiveness for scientific applications has not. One manifestation of this is that several of the scalar machines designed for scientific computation do not use caches <ref> [6, 7] </ref>. 1.1 Cache Performance on Scientific Code To illustrate the need for improving the cache performance of microprocessor-based systems, we present results below for a set of scientific programs. For the sake of concreteness, we pattern our memory subsystem after the MIPS R4000.
Reference: [8] <author> J. Ferrante, V. Sarkar, and W. Thrash. </author> <title> On estimating and enhancing cache effectiveness. </title> <booktitle> In Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> Aug </month> <year> 1991. </year>
Reference-contexts: We estimate the amount of data used for each level of loop nesting, using the reuse vector information. Our algorithm is a simplified version of those proposed previously <ref> [8, 11, 23] </ref>. We assume loop iteration counts that cannot be determined at compile time to be small|this tends to minimize the number of prefetches. (Later, in Section 4.2, we present results where unknown loop iteration counts are assumed to be large).
Reference: [9] <author> K. Gallivan, W. Jalby, U. Meier, and A. Sameh. </author> <title> The impact of hierarchical memory systems on linear algebra algorithm design. </title> <type> Technical Report UIUCSRD 625, </type> <institution> University of Illinios, </institution> <year> 1987. </year>
Reference-contexts: One important example of such a transform is blocking <ref> [1, 9, 10, 12, 21, 23, 29] </ref>. Instead of operating on entire rows or columns of an array, blocked algorithms operate on submatrices or blocks, so that data loaded into the faster levels of the memory hierarchy are reused.
Reference: [10] <author> D. Gannon and W. Jalby. </author> <title> The influence of memory hierarchy on algorithm organization: Programming FFTs on a vector multiprocessor. In The Characteristics of Parallel Algorithms. </title> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: One important example of such a transform is blocking <ref> [1, 9, 10, 12, 21, 23, 29] </ref>. Instead of operating on entire rows or columns of an array, blocked algorithms operate on submatrices or blocks, so that data loaded into the faster levels of the memory hierarchy are reused.
Reference: [11] <author> D. Gannon, W. Jalby, and K. Gallivan. </author> <title> Strategies for cache and local memory management by global program transformation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5 </volume> <pages> 587-616, </pages> <year> 1988. </year>
Reference-contexts: Similar analysis can be used to find spatial reuse. For reuse among different array references, Gannon et al. observe that data reuse is exploitable only if the references are uniformly generated; that is, references whose array index expressions differ in at most the constant term <ref> [11] </ref>. For example, references B [j][0] and B [j+1][0] are uniformly generated; references C [i] and C [j] are not. Pairs of uniformly generated references can be analyzed in a similar fashion [29]. <p> We estimate the amount of data used for each level of loop nesting, using the reuse vector information. Our algorithm is a simplified version of those proposed previously <ref> [8, 11, 23] </ref>. We assume loop iteration counts that cannot be determined at compile time to be small|this tends to minimize the number of prefetches. (Later, in Section 4.2, we present results where unknown loop iteration counts are assumed to be large).
Reference: [12] <author> G. H. Golub and C. F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> Johns Hopkins University Press, </publisher> <year> 1989. </year>
Reference-contexts: One important example of such a transform is blocking <ref> [1, 9, 10, 12, 21, 23, 29] </ref>. Instead of operating on entire rows or columns of an array, blocked algorithms operate on submatrices or blocks, so that data loaded into the faster levels of the memory hierarchy are reused.
Reference: [13] <author> E. Gornish, E. Granston, and A. Veidenbaum. </author> <title> Compiler-Directed Data Prefetching in Multiprocessors with Memory Hierarchies. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <year> 1990. </year>
Reference-contexts: A promising technique to mitigate the impact of long cache miss penalties is software-controlled prefetching <ref> [5, 13, 16, 22, 23] </ref>. Software-controlled prefetching requires support from both hardware and software. The processor must provide a special "prefetch" instruction. <p> In particular, we focus on those numerical algorithms that operate on dense matrices. Various algorithms have previously been proposed for this problem <ref> [13, 16, 23] </ref>. In this work, we improve upon previous algorithms and evaluate our algorithm in the context of a full optimizing compiler. We also study the interaction of prefetching with other data locality optimizations such as cache blocking. There are a few important concepts useful for developing prefetch algorithms. <p> However, our results have demonstrated that prefetching directly into the cache can provide impressive speedups, and without the disadvantage of sacrificing cache size to accommodate a fetchbuffer. Gornish, Granston and Veidenbaum <ref> [13, 14] </ref> presented an algorithm for determining the earliest time when it is safe to prefetch shared data in a multiprocessor with software-controlled cache coherency. This work is targeted for a block prefetch instruction, rather than the single-line prefetches considered in this paper.
Reference: [14] <author> E. H. Gornish. </author> <title> Compile time analysis for data prefetching. </title> <type> Master's thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> December </month> <year> 1989. </year>
Reference-contexts: However, our results have demonstrated that prefetching directly into the cache can provide impressive speedups, and without the disadvantage of sacrificing cache size to accommodate a fetchbuffer. Gornish, Granston and Veidenbaum <ref> [13, 14] </ref> presented an algorithm for determining the earliest time when it is safe to prefetch shared data in a multiprocessor with software-controlled cache coherency. This work is targeted for a block prefetch instruction, rather than the single-line prefetches considered in this paper.
Reference: [15] <author> A. Gupta, J. Hennessy, K. Gharachorloo, T. Mowry, and W-D. Weber. </author> <title> Comparative evaluation of latency reducing and tolerating techniques. </title> <booktitle> In Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 254-263, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: In fact, 8 out of the 13 programs spend more than half of their time stalled for memory accesses. 1.2 Memory Hierarchy Optimizations Various hardware and software approaches to improve the memory performance have been proposed recently <ref> [15] </ref>. A promising technique to mitigate the impact of long cache miss penalties is software-controlled prefetching [5, 13, 16, 22, 23]. Software-controlled prefetching requires support from both hardware and software. The processor must provide a special "prefetch" instruction.
Reference: [16] <author> A. C. Klaiber and H. M. Levy. </author> <title> Architecture for software-controlled data prefetching. </title> <booktitle> In Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 43-63, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: A promising technique to mitigate the impact of long cache miss penalties is software-controlled prefetching <ref> [5, 13, 16, 22, 23] </ref>. Software-controlled prefetching requires support from both hardware and software. The processor must provide a special "prefetch" instruction. <p> In particular, we focus on those numerical algorithms that operate on dense matrices. Various algorithms have previously been proposed for this problem <ref> [13, 16, 23] </ref>. In this work, we improve upon previous algorithms and evaluate our algorithm in the context of a full optimizing compiler. We also study the interaction of prefetching with other data locality optimizations such as cache blocking. There are a few important concepts useful for developing prefetch algorithms. <p> Overall performance numbers were not presented. Also, the more sophisticated scheme was not automated (the overflow iterations were calculated by hand) and did not take cache line reuse into account. Klaiber and Levy <ref> [16] </ref> extended the work by Porterfield by recognizing the need to prefetch more than a single iteration ahead. They included several memory system parameters in their equation for how many iterations ahead to prefetch, and inserted prefetches by hand at the assembly-code level.
Reference: [17] <author> D. Kroft. </author> <title> Lockup-free instruction fetch/prefetch cache organization. </title> <booktitle> In Proceedings of the 8th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 81-85, </pages> <year> 1981. </year>
Reference-contexts: The processor must provide a special "prefetch" instruction. The software uses this instruction to inform the hardware of its intent to use a particular data item; if the data is not currently in the cache, the data is fetched in from memory. The cache must be lockup-free <ref> [17] </ref>; that is, the cache must allow multiple outstanding misses. While the memory services the data miss, the program can continue to execute as long as it does not need the requested data. <p> We augment the instruction set to include a prefetch instruction that uses a base-plus-offset addressing format and is defined to not take any memory exceptions. Both levels of the cache are lockup-free <ref> [17] </ref> in the sense that multiple prefetches can be outstanding. The primary cache is checked in the cycle the prefetch instruction is executed. If the line is already in the cache, the prefetch is discarded.
Reference: [18] <author> M. S. Lam. </author> <title> Software pipelining: An effective scheduling technique for vliw machines. </title> <booktitle> In Proc. ACM SIGPLAN 88 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 318-328, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: This software pipelining transformation is necessary to issue the prefetches enough iterations ahead of their use <ref> [18, 24] </ref>. This example illustrates the three major steps in the prefetch algorithm: 1. For each reference, determine the accesses that are likely to be cache misses and therefore need to be prefetched. 2. Isolate the predicted cache miss instances through loop splitting.
Reference: [19] <author> M. S. Lam, E. E. Rothberg, and M. E. Wolf. </author> <title> The cache performance and optimizations of blocked algorithms. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 63-74, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: For scientific programs, one important source of cache conflicts is due to accessing data in the same matrix with a constant stride. Such conflicts can be predicted, and can even be avoided by embedding the matrix in a larger matrix with dimensions that are less problematic <ref> [19] </ref>. We have not implemented this optimization in our compiler. <p> Therefore, the lowest prefetch latency (100 cycles) offers the best the performance, as we see in Figure 6 (c). However, in such cases the best approach may be to eliminate the cache conflicts cause this behavior <ref> [19] </ref>. In summary, the performance of our selective algorithm was affected noticeably in only one of the 13 benchmarks for each parameter we varied. Overall, the algorithm appears to be quite robust. 4.3 Dropping Prefetches vs.
Reference: [20] <author> R. L. Lee. </author> <title> The Effectiveness of Caches and Data Prefetch Buffers in Large-Scale Shared Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> May </month> <year> 1987. </year>
Reference-contexts: Porterfield [23] evaluated several cacheline-based hardware prefetching schemes. In some cases they were quite effective at reducing miss rates, but at the same time they often increased memory traffic substantially. Lee <ref> [20] </ref> proposed an elaborate lookahead scheme for prefetching in a multiprocessor where all shared data is uncacheable. He found that the effectiveness of the scheme was limited by branch prediction and by synchronization. Baer and Chen [2] proposed a scheme that uses a history buffer to detect strides.
Reference: [21] <author> A. C. McKeller and E. G. Coffman. </author> <title> The organization of matrices and matrix operations in a paged multiprogramming environment. </title> <journal> CACM, </journal> <volume> 12(3) </volume> <pages> 153-165, </pages> <year> 1969. </year>
Reference-contexts: One important example of such a transform is blocking <ref> [1, 9, 10, 12, 21, 23, 29] </ref>. Instead of operating on entire rows or columns of an array, blocked algorithms operate on submatrices or blocks, so that data loaded into the faster levels of the memory hierarchy are reused.
Reference: [22] <author> T. Mowry and A. Gupta. </author> <title> Tolerating latency through software-controlled prefetching in shared-memory multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 12(2) </volume> <pages> 87-106, </pages> <year> 1991. </year>
Reference-contexts: A promising technique to mitigate the impact of long cache miss penalties is software-controlled prefetching <ref> [5, 13, 16, 22, 23] </ref>. Software-controlled prefetching requires support from both hardware and software. The processor must provide a special "prefetch" instruction. <p> For a commercially available microprocessor that is targeted for many different memory systems, this lack of flexibility can be a serious limitation|not only in terms of tuning for different memory latencies, but also a prefetching scheme that is appropriate for a uniprocessor may be entirely inappropriate for a multiprocessor <ref> [22] </ref>. Finally, while hardware-based schemes have no software cost, they may have a significant hardware cost, both in terms of chip area and possibly gate delays. 6 Future Work The scope of this compiler algorithm was limited to affine array accesses within scientific applications.
Reference: [23] <author> A. K. Porterfield. </author> <title> Software Methods for Improvement of Cache Performance on Supercomputer Applications. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Rice University, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: A promising technique to mitigate the impact of long cache miss penalties is software-controlled prefetching <ref> [5, 13, 16, 22, 23] </ref>. Software-controlled prefetching requires support from both hardware and software. The processor must provide a special "prefetch" instruction. <p> One important example of such a transform is blocking <ref> [1, 9, 10, 12, 21, 23, 29] </ref>. Instead of operating on entire rows or columns of an array, blocked algorithms operate on submatrices or blocks, so that data loaded into the faster levels of the memory hierarchy are reused. <p> In particular, we focus on those numerical algorithms that operate on dense matrices. Various algorithms have previously been proposed for this problem <ref> [13, 16, 23] </ref>. In this work, we improve upon previous algorithms and evaluate our algorithm in the context of a full optimizing compiler. We also study the interaction of prefetching with other data locality optimizations such as cache blocking. There are a few important concepts useful for developing prefetch algorithms. <p> This table suggests that if prefetches were issued for all the affine array accesses, then over 60% of the prefetches would be unnecessary for most of the programs. It is important to minimize unnecessary prefetches <ref> [23] </ref>. Unnecessary prefetches incur a computation overhead due to the prefetches themselves and instructions needed to calculate the addresses. Prefetching can also increase the demand for memory bandwidth, which can result in delays for normal memory accesses as well as prefetches. <p> We estimate the amount of data used for each level of loop nesting, using the reuse vector information. Our algorithm is a simplified version of those proposed previously <ref> [8, 11, 23] </ref>. We assume loop iteration counts that cannot be determined at compile time to be small|this tends to minimize the number of prefetches. (Later, in Section 4.2, we present results where unknown loop iteration counts are assumed to be large). <p> Some of these approaches use software support to issue prefetches, while others are strictly hardware-based. In this section, we discuss previous work in both categories. 5.1 Software Prefetching Porterfield <ref> [4, 23] </ref> presented a compiler algorithm for inserting pre-fetches. He implemented it as a preprocessing pass that inserted prefetching into the source code. His initial algorithm prefetched all array references in inner loops one iteration ahead. <p> At the same time, these applications tend to make better use of the cache in a uniprocessor. 5.2 Hardware Prefetching While software-controlled prefetching schemes require support from both hardware and software, several schemes have been proposed that are strictly hardware-based. Porterfield <ref> [23] </ref> evaluated several cacheline-based hardware prefetching schemes. In some cases they were quite effective at reducing miss rates, but at the same time they often increased memory traffic substantially. Lee [20] proposed an elaborate lookahead scheme for prefetching in a multiprocessor where all shared data is uncacheable.
Reference: [24] <author> B. R. Rau and C. D. Glaeser. </author> <title> Some Scheduling Techniques and an Easily Schedulable Horizontal Architecture for High Performance Scientific Computing. </title> <booktitle> In Proceedings of the 14th Annual Workshop on Microprogramming, </booktitle> <pages> pages 183-198, </pages> <month> Oc-tober </month> <year> 1981. </year>
Reference-contexts: This software pipelining transformation is necessary to issue the prefetches enough iterations ahead of their use <ref> [18, 24] </ref>. This example illustrates the three major steps in the prefetch algorithm: 1. For each reference, determine the accesses that are likely to be cache misses and therefore need to be prefetched. 2. Isolate the predicted cache miss instances through loop splitting.
Reference: [25] <author> J. P. Singh, W-D. Weber, and A. Gupta. </author> <title> Splash: Stanford parallel applications for shared memory. </title> <type> Technical Report CSL-TR-91-469, </type> <institution> Stanford University, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: We present results for a collection of scientific programs drawn from several benchmark suites. This collection includes NASA7 and TOMCATV from the SPEC benchmarks [27], OCEAN a uniprocessor version of a SPLASH benchmark <ref> [25] </ref>, and CG (conjugate gradient), EP (embarassingly parallel), IS (integer sort), MG (multi-grid) from the NAS Parallel Benchmarks [3]. Since the NASA7 benchmark really consists of 7 independent kernels, we study each kernel separately (MXM, CFFT2D, CHOLSKY, BTRIX, GMTRY, EMIT and VPENTA).
Reference: [26] <author> M. D. Smith. </author> <title> Tracing with pixie. </title> <type> Technical Report CSL-TR-91-497, </type> <institution> Stanford University, </institution> <month> November </month> <year> 1991. </year>
Reference-contexts: Since the NASA7 benchmark really consists of 7 independent kernels, we study each kernel separately (MXM, CFFT2D, CHOLSKY, BTRIX, GMTRY, EMIT and VPENTA). The performance of the benchmarks was simulated by instrumenting the MIPS object code using pixie <ref> [26] </ref> and piping the resulting trace into our cache simulator. instruction execution and stalls due to memory accesses. We observe that many of the programs spend a significant amount of time on memory accesses.
Reference: [27] <author> SPEC. </author> <title> The SPEC Benchmark Report. </title> <publisher> Waterside Associates, </publisher> <address> Fremont, CA, </address> <month> January </month> <year> 1990. </year>
Reference-contexts: We present results for a collection of scientific programs drawn from several benchmark suites. This collection includes NASA7 and TOMCATV from the SPEC benchmarks <ref> [27] </ref>, OCEAN a uniprocessor version of a SPLASH benchmark [25], and CG (conjugate gradient), EP (embarassingly parallel), IS (integer sort), MG (multi-grid) from the NAS Parallel Benchmarks [3].
Reference: [28] <author> S. W. K. Tjiang and J. L. Hennessy. Sharlit: </author> <title> A tool for building optimizers. </title> <booktitle> In SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <year> 1992. </year>
Reference-contexts: We have implemented our prefetch algorithm in the SUIF (Stanford University Intermediate Form) compiler. The SUIF compiler includes many of the standard optimizations and generates code competitive with the MIPS compiler <ref> [28] </ref>.
Reference: [29] <author> M. E. Wolf and M. S. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 30-44, </pages> <month> June </month> <year> 1991. </year> <pages> Page 12 </pages>
Reference-contexts: One important example of such a transform is blocking <ref> [1, 9, 10, 12, 21, 23, 29] </ref>. Instead of operating on entire rows or columns of an array, blocked algorithms operate on submatrices or blocks, so that data loaded into the faster levels of the memory hierarchy are reused. <p> Instead of operating on entire rows or columns of an array, blocked algorithms operate on submatrices or blocks, so that data loaded into the faster levels of the memory hierarchy are reused. Other useful transformations include unimodular loop transforms such as interchange, skewing and reversal <ref> [29] </ref>. Since these optimizations improve the code's data locality, they not only reduce the effective memory access time but also reduce the memory bandwidth requirement. <p> It is important to focus on the overall performance, because simple characterizations such as the miss rates alone are often misleading. We have also evaluated the interactions between prefetching and locality optimizations such as blocking <ref> [29] </ref>. The organization of the rest of the paper is as follows. Section 2 describes our compiler algorithm, Section 3 describes our experimental framework, and Section 4 presents our results. <p> We represent an n-dimensional loop nest as a polytope in an n-dimensional iteration space, with the outermost loop represented by the first dimension in the space. We represent the shape of the set of iterations that use the same data by a reuse vector space <ref> [29] </ref>. <p> For example, references B [j][0] and B [j+1][0] are uniformly generated; references C [i] and C [j] are not. Pairs of uniformly generated references can be analyzed in a similar fashion <ref> [29] </ref>. <p> Instead of trying to represent exactly which reuses would result in a cache hit, we capture only the dimensionality of the iteration space that has data locality <ref> [29] </ref>. We define the localized iteration space to be the set of loops that can exploit reuse. <p> Our compiler can do both things automatically by first applying locality optimizations and then inserting prefetches. We compiled each of the benchmarks with the locality optimizer enabled <ref> [29] </ref>. In two of the cases (GMTRY and VPENTA) there was a significant improvement in locality. Both of those cases are presented in Figure 8.
References-found: 29

