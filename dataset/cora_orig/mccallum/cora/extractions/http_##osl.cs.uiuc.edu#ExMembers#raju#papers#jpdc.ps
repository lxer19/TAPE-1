URL: http://osl.cs.uiuc.edu/ExMembers/raju/papers/jpdc.ps
Refering-URL: http://osl.cs.uiuc.edu/ExMembers/raju/
Root-URL: http://www.cs.uiuc.edu
Email: Email: fpanwar j agha g@cs.uiuc.edu  
Title: A Methodology for Programming Scalable Architectures  referees for providing several constructive suggestions.  
Author: Rajendra Panwar and Gul Agha Svend Frtlund, Daniel Stur-man, Wooyoung Kim, Shangping Ren 
Note: The authors thank Daniel Sturman for his development of Broadway and  and other members of the Open Systems Laboratory have provided helpful suggestions and discussions. The authors would also like to thank the  
Address: 1304 W. Springfield Avenue  Urbana, IL 61801, USA  
Affiliation: Open Systems Laboratory Department of Computer Science  University of Illinois at Urbana-Champaign  
Abstract: fl The research described has been made possible by support from the Office of Naval Research (ONR contract numbers N00014-90-J-1899 and N00014-93-1-0273), by an Incentives for Excellence Award from the Digital Equipment Corporation Faculty Program, and by joint support from the Defense Advanced Research Projects Agency and the National Science Foundation (NSF CCR 90-07195). 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Agha. </author> <title> Actors: A Model of Concurrent Computation in Distributed Systems. </title> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: Actors are message driven objects which unify data and the operations modifying the data. Actors are self-contained, interactive components of 4 a computing system that communicate by asynchronous message passing <ref> [1] </ref>. Each actor has a mail address and a behavior. Mail addresses may be communicated, thereby providing a dynamic communication topology. Note that the actors whose addresses an actor knows are called its acquaintances.
Reference: [2] <author> G. Agha. </author> <title> Concurrent object-oriented programming. </title> <journal> Communications of the ACM, </journal> <volume> 33(9) </volume> <pages> 125-141, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: Data parallelism can be naturally expressed by messages broadcast to a group of actors. Functional parallelism may be expressed by sending messages concurrently to a number of actors and using a join continuation actor to synchronize their responses <ref> [2] </ref>. Moreover, because actors are message-driven, they naturally overlap communication and computation, thus masking latency whenever feasible. This provides a form of pipeline concurrency [4].
Reference: [3] <author> G. Agha, S. Frtlund, R. Panwar, and D. Sturman. </author> <title> A linguistic framework for dynamic composition of dependability protocols. In Dependable Computing for Critical Applications III, IFIP Transactions. </title> <publisher> Elsevier Science Publisher, </publisher> <year> 1993. </year>
Reference-contexts: In our methodology such triggering is done by actors called managers. Managers may interact with system level actors, for example, to determine the load, and may examine messages sent to actors they are managing. To install a PDS, they may modify a meta-actor's behavior (see <ref> [3] </ref> for a discussion of customizing meta-actors). 4 A Detailed Example: Sparse Cholesky Factorization In this section we illustrate how our methodology may be used to implement PDS's for a sparse matrix representation. The ideal algorithm discussed is the parallel sparse Cholesky 8 Factorization algorithm [23, 38].
Reference: [4] <author> G. Agha, C. Houck, and R. Panwar. </author> <title> Distributed execution of actor systems. </title> <editor> In D. Gelernter, T. Gross, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 1-17. </pages> <publisher> Springer-Verlag, </publisher> <year> 1992. </year> <note> Lecture Notes in Computer Science 589. </note>
Reference-contexts: Functional parallelism may be expressed by sending messages concurrently to a number of actors and using a join continuation actor to synchronize their responses [2]. Moreover, because actors are message-driven, they naturally overlap communication and computation, thus masking latency whenever feasible. This provides a form of pipeline concurrency <ref> [4] </ref>. Because of limitations on computation and communication resources in practical architectures, implementations of a parallel algorithm may not use all the parallelism available in the ideal version of the algorithm. A scalable model of parallel computing is realized in multicomputers.
Reference: [5] <author> G. Agha and R. Panwar. </author> <title> An actor-based framework for heterogeneous systems. </title> <booktitle> In Proceedings of the Workshop on Heterogeneous Processing, </booktitle> <pages> pages 35-42. </pages> <publisher> IEEE, </publisher> <year> 1991. </year>
Reference-contexts: For example, in dense Cholesky factorization, a load balanced PDS is more efficient when the load is sufficiently high. If the architecture is scaled up significantly, the communication overhead becomes more significant. A PDS providing greater locality is more efficient in that case <ref> [5] </ref>. Second, the best PDS may be a function of the input size because the input size may influence the choice of the target architecture. Consider again the dense Cholesky decomposition of a matrix.
Reference: [6] <author> G. Agha and K. Wooyoung. </author> <title> Compilation of a highly parallel actor-based language. </title> <editor> In U. Banerjee, D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Proceedings of the Workshop on Languages and Compilers for Parallel Computing. </booktitle> <publisher> Yale University, Springer-Verlag, </publisher> <year> 1992. </year> <note> LNCS, to be published. 13 </note>
Reference-contexts: Creation of a large number of actors results in significant overhead. The overhead may be reduced by using a number of compiler optimization techniques we are studying (e.g., see <ref> [6] </ref>) however, it is too early to tell how much of the overhead will remain after such optimizations. The benchmarks we report are Gaussian elimination and matrix multiplication since results are available in literature for executing these benchmarks on other systems. <p> Finally, it should be noted that the efficiency with which a flexible system such as ours can be implemented remains open to question. We are investigating compiler optimizations for generating efficient code for the high level language constructs we use (see, for example, <ref> [6] </ref>).
Reference: [7] <author> G. A. Agha, S. Frtlund, W. Kim, R. Panwar, A. Patterson, and D. Sturman. </author> <title> Ab--straction and modularity mechanisms for concurrent computing. </title> <journal> IEEE Parallel and Distributed Technology: Systems and Applications, </journal> <volume> 1(2) </volume> <pages> 3-14, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The two 2 specifications, namely the ideal algorithm and the placement (or migration) policy, may be combined to obtain an efficient implementation of the algorithm for a given problem size and architecture (see figure 1 <ref> [7] </ref>). We call the policies used to place and migrate actors representing a data structure a partitioning and distribution strategy (PDS). Our methodology allows expression of PDS's for static as well as dynamic data structures.
Reference: [8] <author> W. Athas and C. Seitz. </author> <title> Multicomputers: Message-passing concurrent computers. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 9-23, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: A number of actor based languages have been developed and used for parallel and distributed programming (for example, Rosette [36], Acore [31], Cantor <ref> [8] </ref>, HAL [24], ABCL [35], Concurrent Aggregates [16], ACT++ [26] and Charm [27]). Other concurrent object-oriented programming languages using providing mechanisms for defining groups of objects is PC++ [30], and Mentat [22] which provides an efficient runtime system.
Reference: [9] <author> J. Bruno and P. R. Cappello. </author> <title> Implementing the beam and warming method on the hypercube. </title> <booktitle> In Proceedings of the Third Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 1073-1087, </pages> <address> Pasadena, CA, </address> <month> January </month> <year> 1988. </year>
Reference-contexts: In such cases, objects need to be migrated during execution in order to improve performance. For example, using 32 processors to solve Navier-Stokes equations using the Beam-Warming algorithm, an implicit time-marching scheme, the execution efficiency is 28% with fixed mapping <ref> [9] </ref> and 80% using migration [33]. As we will describe later, migration is required by a PDS which is dynamic. This discussion illustrates the importance of using an appropriate PDS to obtain efficient utilization of resources for many algorithms. Two other points may be emphasized.
Reference: [10] <author> P. A. Buhr, G. Ditchfield, R. A. Stroobosscher, B. M. Younger, and C. R. Zaranke. </author> <title> C++: Concurrency in the object-oriented language C++. </title> <journal> Software Practice and Experience, </journal> <volume> 22(2) </volume> <pages> 137-172, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: CC++ allows multiple threads per object and objects do not necessarily represent computational agents that compute concurrently. Synchronization 12 between concurrently executing components uses single assignment objects called sync ob-jects. Another effort whose goal is to introduce concurrency in C++ is C++ <ref> [10] </ref>. C++, adds features such as coroutines, monitors and threads to C++ and uses a single-memory model: communication and synchronization between concurrently executing threads is through shared variables and monitors.
Reference: [11] <author> P. Mehrotra C. Koelbel. </author> <title> Compiling global name-space parallel loops for distibuted execution. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 440-451, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Support for partitioning and distributing arrays is provided in languages such as Kali <ref> [11] </ref>, Vienna Fortran [14] and Fortran D [28]. The languages Kali and Vienna Fortran allow the programmer to declare a processor array and specify distribution of each dimension of a data array onto the processor array.
Reference: [12] <author> C. J. Callsen and G. A. Agha. </author> <title> Open heterogeneous computing in actorspace. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> 1994. (to appear). </note>
Reference-contexts: The procedure to be invoked is specified in the message. The group of actors have a common group name which may be used in conjunction with a more specific parameter to access individual actors (or a subgroup of actors) within the group (cf. <ref> [15, 12] </ref>). Given the name and a pattern, we get a particular member of the group represented by an actor address. In our case, actor groups are references to a collection of addresses and are represented by a variable reference.
Reference: [13] <author> K. M. Chandy and C. Kesselman. </author> <title> Compositional C++: Compositional parallel programming. </title> <editor> In G. Agha, P. Wegner, and A. Yonezawa, editors, </editor> <booktitle> Research Directions in Object-Oriented Programming. </booktitle> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: A number of approaches extend the sequential object-oriented language C++ with constructs for parallel computing. In particular, Chandy and Kesselman have developed Compositional C++ (CC++) which extends C++ with parallel programming constructs such as par , parfor and spawn <ref> [13] </ref>. CC++ allows multiple threads per object and objects do not necessarily represent computational agents that compute concurrently. Synchronization 12 between concurrently executing components uses single assignment objects called sync ob-jects. Another effort whose goal is to introduce concurrency in C++ is C++ [10].
Reference: [14] <author> B. M. Chapman, P. Mehrotra, and H. P. Zima. </author> <title> Vienna fortran a fortran language extension for distributed memory multiprocessors. </title> <editor> In J. Saltz and P. Mehrotra, editors, </editor> <title> Compilers and Runtime Software for Scalable Multiprocessors. </title> <publisher> Elsevier, </publisher> <year> 1991. </year>
Reference-contexts: Support for partitioning and distributing arrays is provided in languages such as Kali [11], Vienna Fortran <ref> [14] </ref> and Fortran D [28]. The languages Kali and Vienna Fortran allow the programmer to declare a processor array and specify distribution of each dimension of a data array onto the processor array.
Reference: [15] <author> A. Chien. </author> <title> Concurrent Aggregates: An Object-Oriented Language for Fine-Grained Message-Passing Machines. </title> <type> PhD thesis, </type> <institution> MIT, </institution> <year> 1990. </year>
Reference-contexts: The procedure to be invoked is specified in the message. The group of actors have a common group name which may be used in conjunction with a more specific parameter to access individual actors (or a subgroup of actors) within the group (cf. <ref> [15, 12] </ref>). Given the name and a pattern, we get a particular member of the group represented by an actor address. In our case, actor groups are references to a collection of addresses and are represented by a variable reference.
Reference: [16] <author> A. Chien. </author> <title> Supporting modularity in highly-parallel programs. </title> <editor> In G. Agha, P. Wegner, and A. Yonezawa, editors, </editor> <booktitle> Research Directions in Object-Oriented Programming. </booktitle> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: A number of actor based languages have been developed and used for parallel and distributed programming (for example, Rosette [36], Acore [31], Cantor [8], HAL [24], ABCL [35], Concurrent Aggregates <ref> [16] </ref>, ACT++ [26] and Charm [27]). Other concurrent object-oriented programming languages using providing mechanisms for defining groups of objects is PC++ [30], and Mentat [22] which provides an efficient runtime system. However, none of these concurrent object-oriented programming languages or systems provides support for the modular specification of PDS's.
Reference: [17] <author> W. Clinger. </author> <title> Foundations of actor semantics. </title> <type> AI-TR- 633, </type> <institution> MIT Artificial Intelligence Laboratory, </institution> <month> May </month> <year> 1981. </year>
Reference-contexts: For our purposes, a PDS is fixed or dynamic with respect to some set of events in computation. Events in a distributed system are partially ordered but may be mapped to a linear global time which represents the events as they may be observed by a hypothetical observer <ref> [17] </ref>. The global time is not unique.
Reference: [18] <author> R. Das, R. Ponnusamy, J. Saltz, and D. Mavriplis. </author> <title> Distributed memory compiler methods for irregular problems data copy reuse and runtime partitioning. </title> <editor> In J. Saltz and P. Mehrotra, editors, </editor> <title> Languages, Compilers and Run-Time Environments for Distributed Memory Machines. </title> <publisher> Elsevier Science Publishers, </publisher> <year> 1992. </year>
Reference-contexts: The ALIGN statement is used to map arrays onto decompositions. The decompositions are mapped to the physical machine by using the DISTRIBUTE statement. Both Vienna Fortran and Fortran D allow certain intrinsic distribution functions such as BLOCK, CYCLIC, and BLOCK-CYCLIC. Kali and PARTI <ref> [18] </ref> support sparse and unstructured computations using distributed arrays accessed using indirection. They transform a sequential loop into two constructs namely, inspector and the executor.
Reference: [19] <author> S. Frtlund and G. Agha. </author> <title> A language framework for multi-object coordination. </title> <booktitle> In Proceedings of ECOOP 1993. </booktitle> <publisher> Springer Verlag, </publisher> <month> July </month> <year> 1993. </year> <note> LNCS 627. </note>
Reference-contexts: We allow external pattern based specification of events which trigger a PDS mechanisms to specify abstract (coordination patterns are described in <ref> [19] </ref>). For example, a pattern may specify a load imbalance in the system, or it may represent invocation of a method that starts the computation of a new phase of an algorithm, which triggers migration of actors in a group.
Reference: [20] <author> G. A. Geist and E. Ng. </author> <title> Task scheduling for parallel sparse cholesky factorization. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 18(4) </volume> <pages> 291-314, </pages> <year> 1989. </year> <month> 14 </month>
Reference-contexts: For a given symmetric positive definite matrix A of size n fi n the Cholesky Factorization algorithm computes a lower triangular matrix L, of size nfin such that A = LL T [21]. PDS's for parallel sparse Cholesky Factorization are discussed in <ref> [23, 20] </ref>. The given sparse matrix A is represented using the edge list representation of a graph. Each column of the original matrix is represented as an actor. The details of the sparse Cholesky Factorization algorithm can be obtained from [23]. <p> Each column of the original matrix is represented as an actor. The details of the sparse Cholesky Factorization algorithm can be obtained from [23]. Before computing the elements of matrix L, the algorithm constructs an elimination tree <ref> [20] </ref> which contains dependence information between the columns of the sparse matrix A. If node i is a descendant of node j in the elimination tree then column i of L must be computed before column j.
Reference: [21] <author> G. Golub and C. Van Loan. </author> <title> Matrix Computations. </title> <publisher> The Johns Hopkins University Press, </publisher> <year> 1983. </year>
Reference-contexts: The ideal algorithm discussed is the parallel sparse Cholesky 8 Factorization algorithm [23, 38]. For a given symmetric positive definite matrix A of size n fi n the Cholesky Factorization algorithm computes a lower triangular matrix L, of size nfin such that A = LL T <ref> [21] </ref>. PDS's for parallel sparse Cholesky Factorization are discussed in [23, 20]. The given sparse matrix A is represented using the edge list representation of a graph. Each column of the original matrix is represented as an actor.
Reference: [22] <author> A. Grimshaw, W. T. Strayer, and P. Narayan. </author> <title> Dynamic object-oriented parallel processing. </title> <journal> IEEE Parallel and Distributed Technology: Systems and Applications, </journal> <volume> 1(2) </volume> <pages> 33-47, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: In fact, on standard benchmarks, the absolute performance of the current implementation is not competitive with implementations of other more restrictive programming models. For example, the performance figures for Mentat <ref> [22] </ref> are several fold better. However, it should be noted that the network we used in our experiments was not a dedicated network for running Broadway but was shared with other users. <p> Other concurrent object-oriented programming languages using providing mechanisms for defining groups of objects is PC++ [30], and Mentat <ref> [22] </ref> which provides an efficient runtime system. However, none of these concurrent object-oriented programming languages or systems provides support for the modular specification of PDS's. In this paper, we presented a methodology for programming concurrent computers which allows separate specification of an ideal algorithm and the PDS's.
Reference: [23] <author> Michael T. Heath, Esmond Ng, and Barry W. Peyton. </author> <title> Parallel algorithms for sparse linear systems. </title> <journal> SIAM Review, </journal> <volume> 33(3) </volume> <pages> 420-460, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: The ideal algorithm discussed is the parallel sparse Cholesky 8 Factorization algorithm <ref> [23, 38] </ref>. For a given symmetric positive definite matrix A of size n fi n the Cholesky Factorization algorithm computes a lower triangular matrix L, of size nfin such that A = LL T [21]. PDS's for parallel sparse Cholesky Factorization are discussed in [23, 20]. <p> For a given symmetric positive definite matrix A of size n fi n the Cholesky Factorization algorithm computes a lower triangular matrix L, of size nfin such that A = LL T [21]. PDS's for parallel sparse Cholesky Factorization are discussed in <ref> [23, 20] </ref>. The given sparse matrix A is represented using the edge list representation of a graph. Each column of the original matrix is represented as an actor. The details of the sparse Cholesky Factorization algorithm can be obtained from [23]. <p> PDS's for parallel sparse Cholesky Factorization are discussed in [23, 20]. The given sparse matrix A is represented using the edge list representation of a graph. Each column of the original matrix is represented as an actor. The details of the sparse Cholesky Factorization algorithm can be obtained from <ref> [23] </ref>. Before computing the elements of matrix L, the algorithm constructs an elimination tree [20] which contains dependence information between the columns of the sparse matrix A.
Reference: [24] <author> C. Houck and G. Agha. HAL: </author> <title> A high-level actor language and its distributed implementation. </title> <booktitle> In Proceedings of th 21st International Conference on Parallel Processing (ICPP '92), </booktitle> <volume> volume II, </volume> <pages> pages 158-165, </pages> <address> St. Charles, IL, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: A number of actor based languages have been developed and used for parallel and distributed programming (for example, Rosette [36], Acore [31], Cantor [8], HAL <ref> [24] </ref>, ABCL [35], Concurrent Aggregates [16], ACT++ [26] and Charm [27]). Other concurrent object-oriented programming languages using providing mechanisms for defining groups of objects is PC++ [30], and Mentat [22] which provides an efficient runtime system.
Reference: [25] <author> L. H. Jamieson. </author> <title> Characterizing parallel algorithms. </title> <editor> In R. J. Douglass L.H. Jamieson, D.B. Gannon, editor, </editor> <booktitle> The Characteristics of Parallel Algorithms, </booktitle> <pages> pages 65-100. </pages> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: 1 Introduction In parallel computing, the set of operations and the partial order in which they may be carried out define an ideal algorithm <ref> [25] </ref>. We use Actors, a form of concurrent objects, to specify ideal algorithms. Actors naturally express an ideal algorithm without introducing unnecessary sequentiality in the code. Data parallelism can be naturally expressed by messages broadcast to a group of actors.
Reference: [26] <author> D. Kafura. </author> <title> Concurrent object-oriented real-time systems research. </title> <journal> SIGPLAN Notices, </journal> <volume> 24(4) </volume> <pages> 203-205, </pages> <month> April </month> <year> 1989. </year> <booktitle> (Proceedings of the Workshop on Object-Based Concurrent Programming, </booktitle> <year> 1988). </year>
Reference-contexts: A number of actor based languages have been developed and used for parallel and distributed programming (for example, Rosette [36], Acore [31], Cantor [8], HAL [24], ABCL [35], Concurrent Aggregates [16], ACT++ <ref> [26] </ref> and Charm [27]). Other concurrent object-oriented programming languages using providing mechanisms for defining groups of objects is PC++ [30], and Mentat [22] which provides an efficient runtime system. However, none of these concurrent object-oriented programming languages or systems provides support for the modular specification of PDS's.
Reference: [27] <author> L. Kale. </author> <title> The CHARM(3.0) Programming Language Manual. </title> <institution> University of Illinois, </institution> <month> October </month> <year> 1991. </year>
Reference-contexts: A number of actor based languages have been developed and used for parallel and distributed programming (for example, Rosette [36], Acore [31], Cantor [8], HAL [24], ABCL [35], Concurrent Aggregates [16], ACT++ [26] and Charm <ref> [27] </ref>). Other concurrent object-oriented programming languages using providing mechanisms for defining groups of objects is PC++ [30], and Mentat [22] which provides an efficient runtime system. However, none of these concurrent object-oriented programming languages or systems provides support for the modular specification of PDS's.
Reference: [28] <author> Ken Kennedy and Ulrich Kremer. </author> <title> Automatic data alignment and distribution for loosely synchronous problems in an interactive programming environment. </title> <institution> Technical Report Rice COMP TR91-155, Rice University, </institution> <year> 1991. </year>
Reference-contexts: Support for partitioning and distributing arrays is provided in languages such as Kali [11], Vienna Fortran [14] and Fortran D <ref> [28] </ref>. The languages Kali and Vienna Fortran allow the programmer to declare a processor array and specify distribution of each dimension of a data array onto the processor array. In Fortran D, data distribution is specified using abstract structures called decompositions which provide a frame of reference for inter-array alignment.
Reference: [29] <author> V. Kumar, A. Grama, A. Gupta, and G. Karypis. </author> <title> Introduction to Parallel Computing: Design and Analysis of Algorithms. </title> <publisher> Benjamin/Cummings Publishing Company, Inc., </publisher> <year> 1994. </year>
Reference-contexts: The PDS used in calculating this estimate is cyclic-checkerboard and results are based on an analytical model as described in <ref> [29] </ref>. The analysis suggests that in order to reduce the total time taken to 1000 seconds for a 20; 000 fi 20; 000 matrix, we may use a mesh multicomputer with a fast network but not a network of workstations. <p> A dense matrix may be partitioned on a parallel computer using a number of PDS's as described in <ref> [29] </ref>. Below we describe how to implement two such PDS's. The i th 6 row of the matrix is represented as an actor group A [i] which is a collection of matrix elements.
Reference: [30] <author> Jenq Kuen Lee and Dennis Gannon. </author> <title> Object-oriented parallel programming experiments and results. </title> <booktitle> In Proceedings Supercomputing 91, </booktitle> <pages> pages 273-282, </pages> <year> 1991. </year>
Reference-contexts: Other concurrent object-oriented programming languages using providing mechanisms for defining groups of objects is PC++ <ref> [30] </ref>, and Mentat [22] which provides an efficient runtime system. However, none of these concurrent object-oriented programming languages or systems provides support for the modular specification of PDS's. In this paper, we presented a methodology for programming concurrent computers which allows separate specification of an ideal algorithm and the PDS's.
Reference: [31] <author> Carl Manning. Acore: </author> <title> The design of a core actor language and its compiler. </title> <type> Master's thesis, </type> <institution> MIT, Artificial Intelligence Laboratory, </institution> <month> August </month> <year> 1987. </year>
Reference-contexts: A number of actor based languages have been developed and used for parallel and distributed programming (for example, Rosette [36], Acore <ref> [31] </ref>, Cantor [8], HAL [24], ABCL [35], Concurrent Aggregates [16], ACT++ [26] and Charm [27]). Other concurrent object-oriented programming languages using providing mechanisms for defining groups of objects is PC++ [30], and Mentat [22] which provides an efficient runtime system.
Reference: [32] <editor> P. Mehrotra, J. Saltz, and R. Voigt, editors. </editor> <title> Unstructured Scientific Computation on Scalable Multiprocessors. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachussets, </address> <year> 1992. </year>
Reference-contexts: Similarly, in a binary search tree, the structure and topology of the nodes may depend on the order in which the elements are added to the tree. For a good review of parallel algorithms which would naturally use dynamic data structures, see <ref> [32] </ref>. Dynamic PDS's may have to interact with the ongoing computation to decide the placement of newly created actors or to migrate existing ones.
Reference: [33] <author> P. Porta. </author> <title> Implicit finite-difference simulation of an internal flow on hypercube. </title> <institution> Research Report YALEU/DCS/RR-594, Yale University, </institution> <month> January </month> <year> 1988. </year>
Reference-contexts: In such cases, objects need to be migrated during execution in order to improve performance. For example, using 32 processors to solve Navier-Stokes equations using the Beam-Warming algorithm, an implicit time-marching scheme, the execution efficiency is 28% with fixed mapping [9] and 80% using migration <ref> [33] </ref>. As we will describe later, migration is required by a PDS which is dynamic. This discussion illustrates the importance of using an appropriate PDS to obtain efficient utilization of resources for many algorithms. Two other points may be emphasized.
Reference: [34] <author> D. C. Sturman. </author> <title> Fault-adaptation for systems in unpredictable environments. </title> <type> Master's thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1993. </year> <month> 15 </month>
Reference-contexts: Broadway performs its own scheduling. Broadway currently runs on a network of DEC and Sun workstations. The platform module uses sockets for internode communication. For more details on Broadway see <ref> [34] </ref>. We present performance results of an implementation of two commonly used benchmarks written in Screed and executed on a network of Sun Sparc stations.
Reference: [35] <author> K. Taura, S. Matsuoka, and A. Yonezawa. </author> <title> An efficient implementation scheme of concurrent object-oriented languages on stock multicomputers. </title> <booktitle> In Fourth ACM SIG-PLAN Symposium on Principles and Practice of Parallel Programming PPOPP, </booktitle> <pages> pages 218-228, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: A number of actor based languages have been developed and used for parallel and distributed programming (for example, Rosette [36], Acore [31], Cantor [8], HAL [24], ABCL <ref> [35] </ref>, Concurrent Aggregates [16], ACT++ [26] and Charm [27]). Other concurrent object-oriented programming languages using providing mechanisms for defining groups of objects is PC++ [30], and Mentat [22] which provides an efficient runtime system.
Reference: [36] <author> C. Tomlinson, P. Cannata, G. Meredith, and D. Woelk. </author> <title> The extensible services switch in carnot. </title> <journal> IEEE Parallel and Distributed Technology: Systems and Applications, </journal> <volume> 1(2), </volume> <month> May </month> <year> 1993. </year>
Reference-contexts: C++, adds features such as coroutines, monitors and threads to C++ and uses a single-memory model: communication and synchronization between concurrently executing threads is through shared variables and monitors. A number of actor based languages have been developed and used for parallel and distributed programming (for example, Rosette <ref> [36] </ref>, Acore [31], Cantor [8], HAL [24], ABCL [35], Concurrent Aggregates [16], ACT++ [26] and Charm [27]). Other concurrent object-oriented programming languages using providing mechanisms for defining groups of objects is PC++ [30], and Mentat [22] which provides an efficient runtime system.
Reference: [37] <author> E. S. Wu, R. Wesley, and D. Calahan. </author> <title> Performance analysis and projections for a massively-parallel Navier-Stokes implementation. </title> <booktitle> In Proceedings of the Fourth Conference on Hypercube Concurrent Computers and Applications, </booktitle> <address> Monterey, CA, </address> <month> March </month> <year> 1989. </year>
Reference-contexts: For example, an im-plementation of the 3D Navier-Stokes equation based on the explicit MacCormack scheme yields 90% efficiency on a 1024 processor NCUBE using a Gray code mapping for ensuring nearest neighbor communication. Without the use of Gray code mapping, the efficiency falls to 68% <ref> [37] </ref>. The choice of a PDS is affected by scalability in two ways. First, the best PDS may be a function of the architecture and problem sizes. For some parallel algorithms, when the architecture size is scaled up, different PDS's may become more efficient.

References-found: 37

