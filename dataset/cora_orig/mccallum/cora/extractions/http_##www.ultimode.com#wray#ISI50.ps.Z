URL: http://www.ultimode.com/wray/ISI50.ps.Z
Refering-URL: http://WWW.Ultimode.com/~wray/refs.html
Root-URL: 
Email: wray@kronos.arc.nasa.gov  
Title: LEARNING IN NETWORKS  
Author: Wray L. Buntine 
Date: August, 1995.  
Address: China,  Mail Stop 269-2 Moffett Field, CA 94035-1000, USA  
Affiliation: Statistical Institute, Beijing,  RIACS at NASA Ames Research Center  
Note: DRAFT Invited paper for 50th Session of the International  
Abstract: Intelligent systems require software incorporating probabilistic reasoning, and often times learning. Networks provide a framework and methodology for creating this kind of software. This paper introduces network models based on chain graphs with deterministic nodes. Chain graphs are defined as a hierarchical combination of Bayesian and Markov networks. To model learning, plates on chain graphs are introduced to model independent samples. The paper concludes by discussing various operations that can be performed on chain graphs with plates as a simplification process or to generate learning algorithms. Un systeme intelligent doit necessairement inclure un module de raisonement prob-abiliste et meme bien souvent des mechanismes d'apprentissage. Les reseaux offrent un cadre et une methodologie pour creer de tels logiciels. Ce papier introduit des modeles de reseaux bases sur les graphes en chaine avec noeuds deterministes. Un graphe en chaine est defini comme etant une combinaison hierarchique de reseaux Bayesiens et de reseaux de Markov. Afin de modeliser l'apprentissage, j'introduit des couches dans ces graphes en chaines pour modeliser des echantillons independants. Le papier conclue en discutant un certain nombre d'operations qui peuvent etre effectuees sur les graphes en chaine afin de les simplifier ou pour generer des algorithmes d'apprentissage.
Abstract-found: 1
Intro-found: 1
Reference: <author> Becker, R., Chambers, J., & Wilks, A. </author> <year> (1988). </year> <title> The New S Language. </title> <publisher> Wadsworth & Brooks/Cole, </publisher> <address> Pacific Grove, California. </address>
Reference-contexts: These problems are all non-trivial variations of well known techniques, and therefore require some additional programming|even in a statistically savvy language such as S <ref> (Becker, Chambers, & Wilks, 1988) </ref>. Our budget in general does not allow for it. The analysis was either not done or kludged together in an unsatisfactory fashion using existing tools.
Reference: <author> Besag, J., York, J., & Mollie, A. </author> <year> (1991). </year> <title> Bayesian image restoration with two applications in spatial statistics. </title> <journal> Ann. Inst. Statist. Math., </journal> <volume> 43 (1), </volume> <pages> 1-59. </pages>
Reference: <author> Boulton, D., & Wallace, C. </author> <year> (1970). </year> <title> A program for numerical classification. </title> <journal> The Computer Journal, </journal> <volume> 13 (1), </volume> <pages> 63-69. </pages>
Reference: <author> Buntine, W. </author> <year> (1994). </year> <title> Operations for learning with graphical models. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2, </volume> <pages> 159-225. </pages>
Reference-contexts: Let G i be the subgraph induced by G on U i [ parents (U i ). Then, make the variables in parents (U i ) all be shaded in G i and add extra arcs to make parents (U i ) into a clique (see <ref> (Buntine, 1994, Lemmas 2.1,2.2) </ref> for simplifications to these). <p> As a notational device to represent a sample|a group of like variables whose conditional distributions are independent and identical|plates are used on a chain graph <ref> (Buntine, 1994) </ref>. To introduce plates, consider the simplist version possible of a learning problem: there is a biased coin with an unknown bias for heads . That is, the long-run frequency of getting heads for this coin on a fair toss is . <p> By Lemma 3, this problem reduces to finding the finest partition whose elements have empty Markov boundary. This problem further simplifies because this need only be done for the graph with plates, not its expanded graph described above. This procedure <ref> (Buntine, 1994) </ref> goes as follows. Given a chain graph G with plates on variables X with known variables K. 1. Compute the element-wise Markov boundary relation for the graph G ignoring the plates, and thus compute the finest partition X 1 ; : : : ; X m . <p> In some cases, this allows the search for a MAP model to be improved considerably (Heckerman, Geiger, & Chickering, 1994), due to an incremental decomposition <ref> (Buntine, 1994) </ref>. 6 Some useful operations on chain graphs with plates By defining various operations on chain graphs with plates, such as conditioning and differentiation, useful algorithms can be pieced together for standard statistical procedures such as maximum likelihood or maximum a posteriori calculations, or the expectation maximization algorithm.
Reference: <author> Buntine, W., Kraft, R., Whitaker, K., Cooper, A., Powers, W., & Wallace, T. </author> <year> (1993). </year> <title> OPAD data analysis. </title> <booktitle> In AIAA/SAE/ASME/ASEE 29th Joint Propulsion Conference Monterey, </booktitle> <address> California. </address>
Reference-contexts: Similar non-standard examples from medicine are described in (Gilks, Clayton, Spiegelhalter, Best, 2 McNeil, Sharples, & Kirby, 1993). Aviation safety data is relational rather than tabular because it has groups of pilots and groups of aircraft in a single event <ref> (Kraft & Buntine, 1993) </ref>. Analyzing high resolution spectral data requires one-dimensional super-resolution to estimate the response function for the instrument. Super resolution on this problem is an integrated combination of registration, scaling, and one dimensional curve-fitting. <p> Super resolution on this problem is an integrated combination of registration, scaling, and one dimensional curve-fitting. Subsequent spectral analysis requires principle components analysis where some non-orthogonal metallic components were obtained separately from an analytic model of physical chemistry <ref> (Buntine, Kraft, Whitaker, Cooper, Powers, & Wallace, 1993) </ref>. These problems are all non-trivial variations of well known techniques, and therefore require some additional programming|even in a statistically savvy language such as S (Becker, Chambers, & Wilks, 1988). Our budget in general does not allow for it.
Reference: <author> Buntine, W., & Weigend, A. </author> <year> (1994). </year> <title> Computing second derivatives in feed-forward networks: a review. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 5 (3). </volume>
Reference-contexts: Let G i be the subgraph induced by G on U i [ parents (U i ). Then, make the variables in parents (U i ) all be shaded in G i and add extra arcs to make parents (U i ) into a clique (see <ref> (Buntine, 1994, Lemmas 2.1,2.2) </ref> for simplifications to these). <p> As a notational device to represent a sample|a group of like variables whose conditional distributions are independent and identical|plates are used on a chain graph <ref> (Buntine, 1994) </ref>. To introduce plates, consider the simplist version possible of a learning problem: there is a biased coin with an unknown bias for heads . That is, the long-run frequency of getting heads for this coin on a fair toss is . <p> By Lemma 3, this problem reduces to finding the finest partition whose elements have empty Markov boundary. This problem further simplifies because this need only be done for the graph with plates, not its expanded graph described above. This procedure <ref> (Buntine, 1994) </ref> goes as follows. Given a chain graph G with plates on variables X with known variables K. 1. Compute the element-wise Markov boundary relation for the graph G ignoring the plates, and thus compute the finest partition X 1 ; : : : ; X m . <p> In some cases, this allows the search for a MAP model to be improved considerably (Heckerman, Geiger, & Chickering, 1994), due to an incremental decomposition <ref> (Buntine, 1994) </ref>. 6 Some useful operations on chain graphs with plates By defining various operations on chain graphs with plates, such as conditioning and differentiation, useful algorithms can be pieced together for standard statistical procedures such as maximum likelihood or maximum a posteriori calculations, or the expectation maximization algorithm.
Reference: <author> Burnell, L., & Horvitz, E. </author> <year> (1995). </year> <title> Structure and chance: Melding logic and probability for software debugging. </title> <journal> Communications of the ACM, </journal> <volume> 37 (3). </volume>
Reference-contexts: In early artificial intelligence, deterministic grammars were used and only more recently have probabilistic methods taken a major role. As another example, only just recently have computer scientists come to realize that probabilities can be an important tool for debugging software <ref> (Burnell & Horvitz, 1995) </ref>. Software for intelligent systems is sufficiently complex that prototype-refinement cycles are used rather than a single design-implementation stage. This style of implementation requires three different skill sets: computer programming, the applications background, and probabilistic techniques.
Reference: <author> Cheeseman, P., Self, M., Kelly, J., Taylor, W., Freeman, D., & Stutz, J. </author> <year> (1988). </year> <title> Bayesian classification. </title> <booktitle> In Seventh National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 607-611 Saint Paul, Minnesota. </address> <booktitle> American Association for Artificial Intelligence. </booktitle> <volume> 18 Dawid, </volume> <editor> A. </editor> <year> (1979). </year> <title> Conditional independence in statistical theory. </title> <journal> SIAM Journal on Com--puting, </journal> <volume> 41, </volume> <pages> 1-31. </pages>
Reference: <author> Dawid, A., & Lauritzen, S. </author> <year> (1993). </year> <title> Hyper Markov laws in the statistical analysis of decomposable graphical models. </title> <journal> Annals of Statistics, </journal> <volume> 21 (3), </volume> <pages> 1272-1317. </pages>
Reference: <author> Dean, T., & Wellman, M. </author> <year> (1991). </year> <title> Planning and Control. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California. </address>
Reference-contexts: Efforts to date have largely focused on first-order probabilistic inference, for instance found in expert systems and diagnosis (Spiegelhalter, Dawid, Lauritzen, & Cowell, 1993; Heckerman, 1991), and planning and control <ref> (Dean & Wellman, 1991) </ref>.
Reference: <author> DeGroot, M. </author> <year> (1970). </year> <title> Optimal Statistical Decisions. </title> <publisher> McGraw-Hill. </publisher>
Reference: <author> Frydenberg, M. </author> <year> (1990). </year> <title> The chain graph Markov property. </title> <journal> Scandinavian Journal of Statistics, </journal> <volume> 17, </volume> <pages> 333-353. </pages>
Reference-contexts: In this paper, I define a chain graph as a Bayesian (directed) network whose components are Bayesian and Markov (undirected) networks. This form of definition allows the complex independence properties and functional form of a chain graph <ref> (Frydenberg, 1990) </ref> to be read off from knowledge of the simpler corresponding properties for directed and undirected networks. It also allows nodes to be deterministic.
Reference: <author> Geman, D. </author> <year> (1990). </year> <title> Random fields and inverse problems in imaging. </title> <editor> In Hennequin, P. (Ed.), Ecole d' Ete de Probabilites de Saint-Flour XVIII - 1988. </editor> <publisher> Springer-Verlag, </publisher> <address> Berlin. </address> <booktitle> In Lecture Notes in Mathematics, </booktitle> <volume> Volume 1427. </volume>
Reference: <author> Ghahramani, Z. </author> <year> (1994). </year> <title> Factorial learning and the EM algorithm. </title> <editor> In Tesauro, G., Touret-zky, D., & Leen, T. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 7 (NIPS*93). </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The bottom left graph, for instance, extends the standard model to include two independent hidden classes <ref> (Ghahramani, 1994) </ref>. 11 5 Chain graphs with plates To represent data analysis problems within a network language such as chain graphs, some additions are needed.
Reference: <author> Gilks, W., Clayton, D., Spiegelhalter, D., Best, N., McNeil, A., Sharples, L., & Kirby, A. </author> <year> (1993a). </year> <title> Modelling complexity: applications of Gibbs sampling in medicine. </title> <journal> Journal of the Royal Statistical Society B, </journal> <volume> 55, </volume> <pages> 39-102. </pages>
Reference: <author> Gilks, W., Thomas, A., & Spiegelhalter, D. </author> <year> (1993b). </year> <title> A language and program for complex Bayesian modelling. </title> <journal> The Statistician, </journal> <volume> 43, </volume> <pages> 169-178. </pages>
Reference: <author> Gill, P. E., Murray, W., & Wright, M. H. </author> <year> (1981). </year> <title> Practical Optimization. </title> <publisher> Academic Press, </publisher> <address> San Diego. </address>
Reference-contexts: This is useful after conditioning on the known data to do approximate inference. Numerical optimization <ref> (Gill, Murray, & Wright, 1981) </ref> using derivatives can be done to search for MAP values of parameters, or to apply the Laplace approximation to estimate moments. To use a gradient descent, conjugate gradient or Levenberg-Marquardt approach requires calculation of first derivatives.
Reference: <editor> Griewank, A., & Corliss, G. F. (Eds.). </editor> <year> (1991). </year> <title> Automatic Differentiation of Algorithms: Theory, Implementation, and Application, </title> <address> Breckenridge, Colorado. </address> <publisher> SIAM. </publisher>
Reference-contexts: To use a Newton-Raphson approach requires calculation of second derivatives, as well. While this could be done numerically by difference approximations, more accurate calculations exist. Methods for symbolically differentiating networks of functions, and piecing together the results to produce global derivatives are well understood <ref> (Griewank & Corliss, 1991) </ref>. For instance, software is available for taking a function defined in Fortran, C++ code, or some other language, to produce a second function that computes the exact derivative.
Reference: <author> Hanson, R., Stutz, J., & Cheeseman, P. </author> <year> (1991). </year> <title> Bayesian classification with correlation and inheritance. </title> <editor> In IJCAI91 (Ed.), </editor> <booktitle> International Joint Conference on Artificial Intelligence Sydney. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The other models in Figure 4 range from undirected networks, including the stochastic network at the bottom right, popular in neural networks (Hertz, Krogh, & Palmer, 1991), to networks matching the models discovered by systems allowing variable correlation, such as Autoclass IV <ref> (Hanson, Stutz, & Cheeseman, 1991) </ref>, and more general hybrids. The bottom left graph, for instance, extends the standard model to include two independent hidden classes (Ghahramani, 1994). 11 5 Chain graphs with plates To represent data analysis problems within a network language such as chain graphs, some additions are needed.
Reference: <author> Heckerman, D., Geiger, D., & Chickering, D. </author> <year> (1994). </year> <title> Learning Bayesian networks: The combination of knowledge and statistical data. </title> <type> Technical report MSR-TR-94-09 (Revised), </type> <institution> Microsoft Research, Advanced Technology Division. </institution> <note> To appear, Machine Learning Journal. </note>
Reference-contexts: For instance, in Figure 7 (b) the Bayes factor (Kass & Raftery, 1993) for this model versus a null model will take the form of a product over the subgraphs. In some cases, this allows the search for a MAP model to be improved considerably <ref> (Heckerman, Geiger, & Chickering, 1994) </ref>, due to an incremental decomposition (Buntine, 1994). 6 Some useful operations on chain graphs with plates By defining various operations on chain graphs with plates, such as conditioning and differentiation, useful algorithms can be pieced together for standard statistical procedures such as maximum likelihood or maximum
Reference: <author> Heckerman, D., Mamdani, A., & Wellman, M. </author> <year> (1995). </year> <title> Real-world applications of Bayesian networks: Introduction. </title> <journal> Communications of the ACM, </journal> <volume> 38 (3). </volume>
Reference-contexts: These tasks involve a high degree of uncertainty, and hence embedded probabilistic reasoning <ref> (Heckerman, Mamdani, & Well-man, 1995) </ref>. Often times, the techniques need adaptation using learning from data as well. The need for a probabilistic approach in some applications is not obvious from the outset. Consider natural language understanding, useful for document summarization, classification, and translation.
Reference: <author> Heckerman, D. </author> <year> (1991). </year> <title> Probabilistic Similarity Networks. </title> <publisher> MIT Press. </publisher>
Reference: <author> Hertz, J., Krogh, A., & Palmer, R. </author> <year> (1991). </year> <title> Introduction to the Theory of Neural Computation. </title> <publisher> Addison-Wesley. </publisher>
Reference-contexts: The other models in Figure 4 range from undirected networks, including the stochastic network at the bottom right, popular in neural networks <ref> (Hertz, Krogh, & Palmer, 1991) </ref>, to networks matching the models discovered by systems allowing variable correlation, such as Autoclass IV (Hanson, Stutz, & Cheeseman, 1991), and more general hybrids.
Reference: <author> Kass, R., & Raftery, A. </author> <year> (1993). </year> <title> Bayes factors and model uncertainty. </title> <type> Technical report #571, </type> <institution> Department of Statistics, Carnegie Mellon University, PA. </institution> <note> To appear, Journal of American Statistical Association. </note>
Reference-contexts: They cannot be further divided, however, because the Markov boundary of f 1 g is f 2 g. An important gain made by doing a decomposition is that learning can now procede independently for each of subgraphs. For instance, in Figure 7 (b) the Bayes factor <ref> (Kass & Raftery, 1993) </ref> for this model versus a null model will take the form of a product over the subgraphs. <p> We plug these pieces together for a particular novel problem, and then with a few commands, we can split the problem up into its independent components, reformulate the problem using conjugate distributions if they exist, and then generate routines for calculating derivatives useful for MAP calculations or the Laplace approximation <ref> (Kass & Raftery, 1993) </ref>, or generate sampling routines for a Gibbs sampler or some other Markov chain Monte Carlo sampler (Neal, 1994, 1993; Gilks et al., 1993b). 6.1 Analysis with conjugate distributions The following is a simple graphical reinterpretation of the Pitman-Koopman Theorem (DeG-root, 1970) for the exponential family of distributions.
Reference: <author> Kraft, R., & Buntine, W. </author> <year> (1993). </year> <title> Initial exploration of the ASRS database. </title> <booktitle> In Seventh 19 International Symposium on Aviation Psychology Columbus, </booktitle> <publisher> Ohio. </publisher>
Reference-contexts: Similar non-standard examples from medicine are described in (Gilks, Clayton, Spiegelhalter, Best, 2 McNeil, Sharples, & Kirby, 1993). Aviation safety data is relational rather than tabular because it has groups of pilots and groups of aircraft in a single event <ref> (Kraft & Buntine, 1993) </ref>. Analyzing high resolution spectral data requires one-dimensional super-resolution to estimate the response function for the instrument. Super resolution on this problem is an integrated combination of registration, scaling, and one dimensional curve-fitting. <p> Super resolution on this problem is an integrated combination of registration, scaling, and one dimensional curve-fitting. Subsequent spectral analysis requires principle components analysis where some non-orthogonal metallic components were obtained separately from an analytic model of physical chemistry <ref> (Buntine, Kraft, Whitaker, Cooper, Powers, & Wallace, 1993) </ref>. These problems are all non-trivial variations of well known techniques, and therefore require some additional programming|even in a statistically savvy language such as S (Becker, Chambers, & Wilks, 1988). Our budget in general does not allow for it.
Reference: <author> Lauritzen, S., Dawid, A., Larsen, B., & Leimer, H.-G. </author> <year> (1990). </year> <title> Independence properties of directed Markov fields. </title> <journal> Networks, </journal> <volume> 20, </volume> <pages> 491-505. </pages>
Reference-contexts: The lemma below shows that this definition is equivalent to a definition based in independence statements <ref> (Lauritzen, Dawid, Larsen, & Leimer, 1990) </ref>, related to (Pearl, 1988), with the notation due to (Dawid, 1979). Definition 1 A is independent of B given C, denoted A??BjC, when p (A [ BjC) = p (AjC)p (BjC) for all instantiations of the variables A; B; C.
Reference: <author> Lauritzen, S., & Spiegelhalter, D. </author> <year> (1988). </year> <title> Local computations with probabilities on graphical structures and their application to expert systems (with discussion). </title> <journal> Journal of the Royal Statistical Society B, </journal> <volume> 50 (2), </volume> <pages> 240-265. </pages>
Reference-contexts: Second-order inference on probabilistic networks was first suggested by Lauritzen and Spiegelhalter <ref> (Lauritzen & Spiegelhalter, 1988) </ref>, and has subsequently been developed by several groups (Shachter, Eddy, & Hasselblad, 1990; Gilks, Thomas, & Spiegelhalter, 1993b; Dawid & Lauritzen, 1993; Buntine, 1994). This paper uses chain graphs (Wermuth & Lauritzen, 1989) as a general probabilistic network model.
Reference: <author> McCullagh, P., & Nelder, J. </author> <year> (1989). </year> <title> Generalized Linear Models (second edition). </title> <publisher> Chapman and Hall, London. </publisher>
Reference-contexts: This applies to both the maximum likelihood and Bayesian frameworks for statistical inference. Software for processing networks based on chain graphs should supersede the technologies of generalized linear models <ref> (McCullagh & Nelder, 1989) </ref>, and many algorithms from neural networks. Of course, this does not simplify the critical tasks of modelling and choosing appropriate priors for a problem. These two tasks might be said to be an art form.
Reference: <author> Neal, R. M. </author> <year> (1994). </year> <title> Bayesian Learning for Neural Networks. </title> <type> Ph.D. thesis, </type> <institution> University of Toronto, Graduate Department of Computer Science. </institution> <note> Available via FTP from ftp://cs.toronto.edu/pub/radford/thesis.ps.Z. </note>
Reference: <author> Neal, R. </author> <year> (1993). </year> <title> Probabilistic inference using Markov chain Monte Carlo methods. </title> <type> Technical report CRG-TR-93-1, </type> <institution> Dept. of Computer Science, University of Toronto. </institution>
Reference: <author> Pearl, J. </author> <year> (1988). </year> <title> Probabilistic Reasoning in Intelligent Systems. </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The lemma below shows that this definition is equivalent to a definition based in independence statements (Lauritzen, Dawid, Larsen, & Leimer, 1990), related to <ref> (Pearl, 1988) </ref>, with the notation due to (Dawid, 1979). Definition 1 A is independent of B given C, denoted A??BjC, when p (A [ BjC) = p (AjC)p (BjC) for all instantiations of the variables A; B; C. The following definitions are used here. <p> The particular independence statements are based on set separation in the moralized graph, which is equivalent to another condition known as d-separation <ref> (Pearl, 1988) </ref>: Definition 3 The distribution p (X) satisfies the directed global Markov property relative to the directed graph G if A??BjS when S separates A and B in the graph H m where H is the subgraph of G restricted to ancestors (A [ B [ S). <p> As noted early, we can equally well mark all the deterministic nodes unshaded because their non-deterministic parents are all shaded, so this problem is side stepped. An important notion used in partitioning graphical models into independent subsets is the Markov blanket <ref> (Pearl, 1988) </ref>. I use the term Markov boundary here. The Markov boundary defines the region of local dependence for a node. To split a graphical model into its independent subgraphs, we then take the transitive closure of the Markov boundary relation.
Reference: <author> Ripley, B. </author> <year> (1994). </year> <title> Network methods in statistics. </title> <editor> In Kelly, F. (Ed.), </editor> <title> Probability, </title> <journal> Statistics and Optimization, </journal> <pages> pp. 241-253. </pages> <publisher> Wiley & Sons, </publisher> <address> New York. </address>
Reference-contexts: Chain graphs can represent many well known models as a special case including linear and logistic regression, various forms of clustering, feed-forward neural networks and various stochastic neural networks. This includes a large number of the more general network models now available <ref> (Ripley, 1994) </ref>. These many different models are formed by combining basic nodes in the network representing for instance, Gaussian variables, wavelet basis functions, or deterministic Sigmoid units. The framework of chain graphs offers a specification language for defining probabilistic models, and thus the input to a computer program.
Reference: <author> Shachter, R. </author> <year> (1990). </year> <title> An ordered examination of influence diagrams. </title> <journal> Networks, </journal> <volume> 20, </volume> <pages> 535-563. </pages>
Reference-contexts: They could also have been unshaded, since the fact that they are known can be deduced from their parents. The analysis of deterministic nodes in Bayesian networks and, more generally, in influence diagrams is considered by <ref> (Shachter, 1990) </ref>. Deterministic nodes cannot have any neighbors, meaning they do not occur in undirected subgraphs. To analyze these nodes, we need to extend the usual definition of a parent and a child for a graph.
Reference: <author> Shachter, R., Eddy, D., & Hasselblad, V. </author> <year> (1990). </year> <title> An influence diagram approach to medical technology assessment. </title> <editor> In Oliver, R., & Smith, J. (Eds.), </editor> <title> Influence Diagrams, Belief Nets and Decision Analysis, </title> <journal> pp. </journal> <pages> 321-350. </pages> <publisher> Wiley. </publisher>
Reference-contexts: They could also have been unshaded, since the fact that they are known can be deduced from their parents. The analysis of deterministic nodes in Bayesian networks and, more generally, in influence diagrams is considered by <ref> (Shachter, 1990) </ref>. Deterministic nodes cannot have any neighbors, meaning they do not occur in undirected subgraphs. To analyze these nodes, we need to extend the usual definition of a parent and a child for a graph.
Reference: <author> Shachter, R., & Heckerman, D. </author> <year> (1987). </year> <title> Thinking backwards for knowledge acquisition. </title> <journal> AI Magazine, </journal> <volume> 8 (Fall), </volume> <pages> 55-61. </pages>
Reference-contexts: Conditional networks are represented by introducing shaded variables in the graph. Shaded variables are assumed to have their values known, so the probability defined by the network is now conditional on the shaded variables. Figure 1 shows two conditional versions of a simple medical problem <ref> (Shachter & Heckerman, 1987) </ref>.
Reference: <author> Spiegelhalter, D., Dawid, A., Lauritzen, S., & Cowell, R. </author> <year> (1993). </year> <title> Bayesian analysis in expert systems. </title> <journal> Statistical Science, </journal> <volume> 8 (3), </volume> <pages> 219-283. </pages>
Reference-contexts: Unfortunately, most of the tasks I see do not fit into one of the standard statistical recipes, such as clustering or non-linear regression, for which excellent software is available. Similar non-standard examples from medicine are described in <ref> (Gilks, Clayton, Spiegelhalter, Best, 2 McNeil, Sharples, & Kirby, 1993) </ref>. Aviation safety data is relational rather than tabular because it has groups of pilots and groups of aircraft in a single event (Kraft & Buntine, 1993).
Reference: <author> Thomas, A., Spiegelhalter, D., & Gilks, W. </author> <year> (1992). </year> <title> BUGS: A program to perform Bayesian inference using Gibbs sampling. </title> <editor> In Bernardo, J., Berger, J., Dawid, A., & Smith, A. (Eds.), </editor> <booktitle> Bayesian Statistics 4, </booktitle> <pages> pp. 837-42. </pages> <publisher> Oxford University Press. </publisher>
Reference-contexts: An environment like S does not address the problem directly|many data analysis algorithms for S are written in C and linked in at runtime. The promise of software support for data analysis is illustrated by the BUGS software for Bayesian analysis using Gibbs sampling <ref> (Thomas, Spiegelhalter, & Gilks, 1992) </ref>. This software is a program generator: it allows a Gibbs sampler to be generated from a model specification. Software application and support for probability methods is the kind of area that the relatively new field of probabilistic networks is aimed at serving.
Reference: <author> Titterington, D., Smith, A., & Makov, U. </author> <year> (1985). </year> <title> Statistical Analysis of Finite Mixture Distributions. </title> <publisher> John Wiley & Sons, </publisher> <address> Chichester. </address>
Reference: <author> Werbos, P. J., McAvoy, T., & Su, T. </author> <year> (1992). </year> <title> Neural networks, system identification, and control in the chemical process industry. In White, </title> <editor> D. A., & Sofge, D. A. (Eds.), </editor> <booktitle> Handbook of Intelligent Control, </booktitle> <pages> pp. 283-356. </pages> <publisher> Van Nostrand Reinhold. </publisher>
Reference: <author> Wermuth, N., & Lauritzen, S. </author> <year> (1989). </year> <title> On substantive research hypotheses, conditional independence graphs and graphical chain models. </title> <journal> Journal of the Royal Statistical Society B, </journal> <volume> 51 (3). </volume>
Reference-contexts: Second-order inference on probabilistic networks was first suggested by Lauritzen and Spiegelhalter (Lauritzen & Spiegelhalter, 1988), and has subsequently been developed by several groups (Shachter, Eddy, & Hasselblad, 1990; Gilks, Thomas, & Spiegelhalter, 1993b; Dawid & Lauritzen, 1993; Buntine, 1994). This paper uses chain graphs <ref> (Wermuth & Lauritzen, 1989) </ref> as a general probabilistic network model. Chain graphs mix undirected and directed graphs (or networks) to give a probabilistic representation that includes Markov random fields and various Markov models.
Reference: <author> Whittaker, J. </author> <year> (1990). </year> <title> Graphical Models in Applied Multivariate Statistics. </title> <publisher> Wiley. </publisher> <pages> 20 </pages>
References-found: 41

