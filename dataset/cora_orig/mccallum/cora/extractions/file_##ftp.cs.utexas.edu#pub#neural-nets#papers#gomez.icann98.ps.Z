URL: file://ftp.cs.utexas.edu/pub/neural-nets/papers/gomez.icann98.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/nn/pages/publications/abstracts.html
Root-URL: http://www.cs.utexas.edu
Email: inaki@cs.utexas.edu  risto@cs.utexas.edu  
Title: 2-D Pole Balancing with Recurrent Evolutionary Networks  
Phone: 1998.  
Author: Faustino Gomez Risto Miikkulainen 
Address: New York: Elsevier,  Austin, TX 78712  Austin, TX 78712  
Affiliation: Sweden).  Department of Computer Sciences The University of Texas at Austin  Department of Computer Sciences The University of Texas at Austin  
Note: In Proceedings of the International Conference on Artificial Neural Networks (ICANN-98, Skovde,  
Abstract: The success of evolutionary methods on standard control learning tasks has created a need for new benchmarks. The classic pole balancing problem is no longer difficult enough to serve as a viable yardstick for measuring the learning efficiency of these systems. In this paper we present a more difficult version to the classic problem where the cart and pole can move in a plane. We demonstrate a neuroevolution system (Enforced Sub-Populations, or ESP) that can solve this difficult problem without velocity information.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. W. Anderson. </author> <title> Learning to control an inverted pendulum using neural networks. </title> <journal> IEEE Control Systems Magazine, </journal> <volume> 9 </volume> <pages> 31-37, </pages> <month> April </month> <year> 1989. </year>
Reference: [2] <author> D. Michie and R. A. Chambers. </author> <title> BOXES: An experiment in adaptive control. </title> <editor> In E. Dale and D. Michie, editors, </editor> <booktitle> Machine Intelligence. </booktitle> <publisher> Oliver and Boyd, Edinburgh, </publisher> <address> UK, </address> <year> 1968. </year>
Reference: [3] <author> J. Schaffer and R. Cannon. </author> <title> On the control of unstable mechanincal systems. In Automatic and Remote Control III: </title> <booktitle> Proceedings of the Third Congress of the International Federation of Automatic Control, </booktitle> <year> 1966. </year>
Reference: [4] <author> R. Sutton. </author> <title> Temporal Credit Assignment in Reinforcement Learning. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, </institution> <address> Amherst, MA, </address> <year> 1984. </year>
Reference-contexts: It is a real-world task that is easy to understand and visualize. It can be performed manually by humans and implemented on a physical robot. (2) It embodies many essential aspects of a whole class of learning tasks that involve temporal credit assignment <ref> [4] </ref>. In short, it is an elegant environment that is a good surrogate for more general problems. Despite this long history, the relatively recent success of modern reinforcement learning methods on control learning tasks has rendered the basic pole balancing problem obsolete.
Reference: [5] <author> F. Gomez and R. Miikkulainen. </author> <title> Incremental evolution of complex general behavior. </title> <booktitle> Adaptive Behavior, </booktitle> <volume> 5 </volume> <pages> 317-342, </pages> <year> 1997. </year>
Reference-contexts: It can now be solved so easily that it provides little or no insight about a system's ability. Neuroevolution (NE) systems (i.e. systems that evolve neural networks using genetic algorithms), for example, often find solutions in the initial random population <ref> [5, 6] </ref>. In response to this need for a new benchmark, a variety of ways to extend the basic pole-balancing task have been suggested. <p> The network is formed by randomly selecting one neuron from each subpopulation. culminating in a two pole version. In Gomez and Miikkulainen <ref> [5] </ref>, the Enforced Sub-Populations (ESP) method was shown to be significantly faster than other NE methods on this control problem. Here we present a different, more intuitive and realistic task, where the cart and pole can move in a plane instead of just a 1-D track. <p> The SANE approach has proven faster and more efficient than other reinforcement learning methods in the basic pole balancing task [6] and ESP has been shown to solve the double pole balancing problem very efficiently <ref> [5] </ref>. The two-dimensional problem examined here was found to be more difficult to evolve than the double pole problem.
Reference: [6] <author> D. E. Moriarty and R. Miikkulainen. </author> <title> Efficient reinforcement learning through symbiotic evolution. </title> <journal> Machine Learning, </journal> <volume> 22 </volume> <pages> 11-32, </pages> <year> 1996. </year>
Reference-contexts: It can now be solved so easily that it provides little or no insight about a system's ability. Neuroevolution (NE) systems (i.e. systems that evolve neural networks using genetic algorithms), for example, often find solutions in the initial random population <ref> [5, 6] </ref>. In response to this need for a new benchmark, a variety of ways to extend the basic pole-balancing task have been suggested. <p> Results are the average of 50 simulations. derivatives. This is a realistic version of the problem since only the positions can be observed easily in the real world. The SANE approach has proven faster and more efficient than other reinforcement learning methods in the basic pole balancing task <ref> [6] </ref> and ESP has been shown to solve the double pole balancing problem very efficiently [5]. The two-dimensional problem examined here was found to be more difficult to evolve than the double pole problem. <p> Many tasks from game-playing to robotics require memory to disambiguate states. The current task, therefore, can serve as a surrogate with which new methods can be tested. SANE has been shown effective in a variety of domains, including robot arm control, constraint satisfaction, and in controlling chaos <ref> [6, 8, 9] </ref>. The results presented in this paper show that ESP extends this powerful method by allowing the evolution of recurrent networks and therefore making it applicable to non-Markovian environments. In the future, we plan to apply this system to real-world tasks such as robot navigation and game playing.
Reference: [7] <author> A. Wieland. </author> <title> Evolving neural network controllers for unstable systems. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks (Seattle, WA), </booktitle> <volume> volume II, </volume> <pages> pages 667-673, </pages> <address> Piscataway, NJ, 1991. </address> <publisher> IEEE. </publisher>
Reference-contexts: Neuroevolution (NE) systems (i.e. systems that evolve neural networks using genetic algorithms), for example, often find solutions in the initial random population [5, 6]. In response to this need for a new benchmark, a variety of ways to extend the basic pole-balancing task have been suggested. Wieland <ref> [7] </ref> presented a series of increasingly difficult variations on the standard pole balancing task fl This research was supported in part by National Science Foundation under grant #IRI-9504317. circles. The network is formed by randomly selecting one neuron from each subpopulation. culminating in a two pole version. <p> The equations of motion for this system are an extension of those found in <ref> [7] </ref> for the single pole problem with one equation for each principal axis.
Reference: [8] <author> D. E. Moriarty. </author> <title> Symbiotic Evolution of Neural Networks in Sequential Decision Tasks. </title> <type> PhD thesis, </type> <institution> Department of Computer Sciences, The University of Texas at Austin, </institution> <year> 1997. </year> <note> Technical Report UT-AI97-257. </note>
Reference-contexts: We demonstrate how ESP can evolve a fully recurrent network to solve this difficult task even when no velocity information is provided. 2 Enforced Sub-Populations (ESP) ESP is a neuroevolution system based on Symbiotic, Adaptive Neuro-Evolution (SANE; <ref> [8] </ref>). Like SANE, it differs from other NE systems in that it evolves a population of neurons instead of complete networks (figure 1). These neurons are combined to form neural networks that are then evaluated on a given problem. <p> As a matter of fact, in the advanced stages of SANE evolution, instead of converging the population around a single individual like the standard GA approaches, the neuron population clusters into "species" or groups of individuals that perform specialized functions in the target behavior <ref> [8] </ref>. ESP builds on the SANE in two ways: (1) It accelerates evolution. The sub-populations that gradually form in SANE are already circumscribed by design in ESP. <p> Many tasks from game-playing to robotics require memory to disambiguate states. The current task, therefore, can serve as a surrogate with which new methods can be tested. SANE has been shown effective in a variety of domains, including robot arm control, constraint satisfaction, and in controlling chaos <ref> [6, 8, 9] </ref>. The results presented in this paper show that ESP extends this powerful method by allowing the evolution of recurrent networks and therefore making it applicable to non-Markovian environments. In the future, we plan to apply this system to real-world tasks such as robot navigation and game playing.
Reference: [9] <author> D. E. Moriarty and R. Miikkulainen. </author> <title> Evolving obstacle avoidance behavior in a robot arm. </title> <editor> In P. Maes, M. Mataric, J.-A. Meyer, and J. Pollack, editors, </editor> <booktitle> From Animals to Animats 4: Proceedings of the 4th International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pages 468-475, </pages> <address> Cambridge, MA, 1996. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Many tasks from game-playing to robotics require memory to disambiguate states. The current task, therefore, can serve as a surrogate with which new methods can be tested. SANE has been shown effective in a variety of domains, including robot arm control, constraint satisfaction, and in controlling chaos <ref> [6, 8, 9] </ref>. The results presented in this paper show that ESP extends this powerful method by allowing the evolution of recurrent networks and therefore making it applicable to non-Markovian environments. In the future, we plan to apply this system to real-world tasks such as robot navigation and game playing.
References-found: 9

