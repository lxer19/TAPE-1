URL: http://www.cs.rice.edu/MSCP/papers/CompLang.ps.gz
Refering-URL: http://www.cs.rice.edu/MSCP/publications.html
Root-URL: 
Title: A Methodology for Procedure Cloning  
Author: Keith D. Cooper Mary W. Hall Ken Kennedy 
Keyword: cloning, specialization, interprocedural data-flow analysis, interprocedural optimization  
Date: May 9, 1995  
Abstract: Procedure cloning is an interprocedural transformation where the compiler creates specialized copies of procedure bodies. The compiler divides incoming calls between the original procedure and its copies. By carefully partitioning the calls, the compiler ensures that each clone inherits an environment that allows for better code optimization. This paper presents a three-phase algorithm for deciding when to clone a procedure. The algorithm seeks to avoid unnecessary code growth by considering how the information exposed by cloning will be used during optimization. We present a set of assumptions that bound both the algorithm's running time and code expansion.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Ball, </author> <title> J.E. Predicting the effects of optimization on a procedure body. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 14(8) </volume> <pages> 214-220, </pages> <year> 1979. </year>
Reference-contexts: For example, it can be the number of positions that differ between a pair of cloning vectors. This stategy can be improved by taking into account execution frequency estimates and weighting the effects of each piece of information <ref> [1] </ref>. Given a method to compute the opportunity cost of merging two cloning vectors, the compiler can adopt a relatively simple rationing scheme. Assume that we set a quota for the total number of cloning vectors allowed during compilation and a quota for each procedure.
Reference: [2] <author> Banning, J.P. </author> <title> An efficient way to find the side effects of procedure calls and the aliases of variables. </title> <booktitle> In Proceedings of the Sixth Annual Symposium on Principles of Programming Languages, </booktitle> <pages> pages 29-41. </pages> <publisher> ACM, </publisher> <month> January </month> <year> 1979. </year> <month> 14 </month>
Reference-contexts: If two cloning vectors have unique constant values that produce the same effect on optimization, they are merged. Other interprocedural problems exist for which this merging phase is unnecessary. As an example, we briefly consider cloning based on alias analysis <ref> [2] </ref>. Two variable names are aliases in a procedure if they can refer to the same memory location. A compiler uses alias information to verify the safety of certain optimizations.
Reference: [3] <author> Briggs, P., Cooper, K.D., Hall, M.W., and Torczon, L. </author> <title> Goal-directed interprocedural optimization. </title> <type> Technical Report TR90-148, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> November </month> <year> 1990. </year>
Reference-contexts: We derive some key insights for the algorithm. 3.1 The Experiment Matrix300 computes eight variants on matrix multiplication, selectively transposing the input and output matrices. The goal of the experiment was to apply a series of transformations to the program to improve its execution time <ref> [3] </ref>. These transformations reorder the iteration space of a loop to expose reuse of values in registers and decrease cache misses. The most important of these optimizations, unroll and jam, has demonstrated dramatic improvements on linear algebra kernels [6]. <p> Thus, a good cloning technique should try to distinguish between facts that have an impact on code quality and those that do not. We can avoid unnecessary code growth by restricting cloning to those cases where important information is exposed. We describe such a strategy as goal-directed <ref> [3] </ref>. In the matrix300 example, we clone only to expose constants needed to improve the results of dependence analysis. These constants fall into three categories: (1) they specify the dimension size of an array parameter; (2) they determine control flow; or, (3) they appear in a subscript expression. <p> A bottom-up pass over the program propagates these variables, translating from formal to actual parameters at calls. This approach derives ImportantVariables (p), the variables of procedure p that, if constant, might improve dependence information in this procedure or one of its descendants <ref> [3, 12] </ref>. For other forward data-flow problems, a goal-directed approach depends both on the problem and the desired optimization effects. Designing a strategy for a specific compiler necessarily involves experimentation to understand how well the compiler takes advantage of the kind of facts that cloning can expose. <p> These applications come from diverse areas: compiling for scalar architectures, compiling for both shared-memory and distributed-memory parallel architectures, and instrumenting code for run-time detection of race conditions in shared-memory parallel programs. To date, we have effectively employed cloning in experiments with interprocedural constant propagation <ref> [3, 12] </ref> and interprocedural transformations for parallel code generation [13] through hand optimization and a partial implementation of the algorithm. ParaScope is devoted to high-performance Fortran programming, but the need for cloning arises in many other contexts such as those discussed in Section 2.
Reference: [4] <author> Bulyonkov, M.A. </author> <title> Polyvariant mixed computation for analyzer programs. </title> <journal> Acta Informatica, </journal> <volume> 21 </volume> <pages> 473-484, </pages> <year> 1984. </year>
Reference-contexts: Section 6 addresses how to further restrict cloning in pathological cases that exceed the assumed bounds. 2 Related Work Techniques and algorithms very closely related to ours appear in intraprocedural optimization [19] and partial evaluation <ref> [4, 18] </ref>. Similar techniques appear in dynamic compilation of APL [15] and compilation of the dynamically-typed, object-oriented language self [8]. Wegman's node distinction replicates basic blocks in a procedure's control flow graph based on intra-procedural data-flow solutions and incrementally propagates the more precise solutions [19]. <p> His algorithm uses heuristics to avoid some unnecessary replication. In partial evaluation, specialization involves replicating code in order to tailor copies to particular variable values or types <ref> [4] </ref>. Bulyonkov describes an abstract interpretation approach to locate program points where specialization improves analysis information, both in the interprocedural and intraprocedural settings. Ruf and Weise present an algorithm to reduce the amount of specialization in a partial evaluator [18].
Reference: [5] <author> Callahan, C.D., Cooper, K.D., Hood, R.T., Kennedy, K., and Torczon, L. </author> <title> ParaScope: a parallel programming environment. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 2(4) </volume> <pages> 84-89, </pages> <year> 1988. </year>
Reference-contexts: Nevertheless, in the event of pathological behavior, we suggest mechanisms to reduce the amount of cloning to a manageable size. The algorithm was designed in the context of the program compiler for the ParaScope programming environment the tool that manages interprocedural issues in compilation <ref> [5] </ref>. This general algorithm supports a number of emerging applications for cloning. These applications come from diverse areas: compiling for scalar architectures, compiling for both shared-memory and distributed-memory parallel architectures, and instrumenting code for run-time detection of race conditions in shared-memory parallel programs.
Reference: [6] <author> Callahan, D., Carr, S., and Kennedy, K. </author> <title> Improving register allocation for subscripted variables. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 25(6) </volume> <pages> 53-65, </pages> <year> 1990. </year>
Reference-contexts: These transformations reorder the iteration space of a loop to expose reuse of values in registers and decrease cache misses. The most important of these optimizations, unroll and jam, has demonstrated dramatic improvements on linear algebra kernels <ref> [6] </ref>. Unroll and jam cannot be applied directly to the key computational kernel of matrix300 because of the program's structure. Unroll and jam transforms a nest of two or more loops; in matrix300, each loop is in a different procedure. The leaf procedure, daxpy, only contains a single loop.
Reference: [7] <author> Callahan, D., Cooper, K.D., Kennedy, K., and Torczon, L. </author> <title> Interprocedural constant propagation. </title> <journal> ACM SIG-PLAN Notices, </journal> <volume> 21(7) </volume> <pages> 152-161, </pages> <year> 1986. </year>
Reference-contexts: We use jump functions to describe the value of each such expression as a function of the external variables for which constant propagation and cloning may potentially uncover a constant value <ref> [7] </ref>. A jump function, J E1 (f), describes the value of expression E1 as a function of external variable f . These jump functions can be constructed by examination of the procedure prior to any interprocedural analysis.
Reference: [8] <author> Chambers, C. and Ungar, D. </author> <title> Customization: Optimizing compiler technology for SELF, a dynamically-typed object-oriented programming language. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 24(7) </volume> <pages> 146-160, </pages> <year> 1989. </year>
Reference-contexts: Similar techniques appear in dynamic compilation of APL [15] and compilation of the dynamically-typed, object-oriented language self <ref> [8] </ref>. Wegman's node distinction replicates basic blocks in a procedure's control flow graph based on intra-procedural data-flow solutions and incrementally propagates the more precise solutions [19]. His algorithm uses heuristics to avoid some unnecessary replication. <p> Environments for subsequent invocations are compared to the compiled version, and if necessary, the code is generalized to accomodate both environments so that only a single compiled version exists for that statement. The self compiler combines dynamic and static compilation techniques <ref> [8] </ref>. Customization dynamically creates a specialized copy of a method based on the receiver type for a message invoking that method, which may generate copies for every possible receiver type in the program.
Reference: [9] <author> Cooper, K.D., Hall, M.W., and Torczon, L. </author> <title> An experiment with inline substitution. </title> <journal> Software | Practice and Experience, </journal> <volume> 21(6) </volume> <pages> 581-601, </pages> <year> 1991. </year>
Reference-contexts: It has been widely assumed that call overhead is the more significant effect; a recent study of Fortran suggests that call overhead may play less of a role in run-time performance than believed <ref> [9] </ref>. Traditionally, two approaches have emerged for breaking down the call-site barrier. The first, inline substitution, replaces call sites with distinct copies of the body of the called procedure. The code is then optimized in the context of the calling procedure. <p> This information is used, in turn, to optimize the individual procedures. Each technique has limitations. Inlining can lead to code growth, increased compile time, and degradation in code quality <ref> [9] </ref>. Using only interprocedural analysis lets the structure of the program constrain the compiler; it assumes that each procedure should be implemented once.
Reference: [10] <author> Cooper, K.D., Kennedy, K., and Torczon, L. </author> <title> The impact of interprocedural analysis and optimization in the R n programming environment. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 8(4) </volume> <pages> 491-523, </pages> <year> 1986. </year>
Reference-contexts: Using only interprocedural analysis lets the structure of the program constrain the compiler; it assumes that each procedure should be implemented once. To improve the latter approach, an aggressive compiler can consider procedure cloning | creating multiple implementations of a single procedure and partitioning the calls among them <ref> [10] </ref>. fl This research has been supported by the National Science Foundation, IBM Corporation, the Defense Advanced Research Projects Agency, and the State of Texas. y Department of Computer Science, Rice University, Houston, TX 77251 z Center for Integrated Systems, Stanford University, Stanford, CA 94305 1 * Cloning differs from straightforward
Reference: [11] <author> Cooper, K.D., Kennedy, K., Torczon, L., Weingarten, A., and Wolcott, M. </author> <title> Editing and compiling whole programs. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 22(1) </volume> <pages> 92-101, </pages> <year> 1987. </year>
Reference-contexts: It is related to the algorithm for minimizing the number of states in a Deterministic Finite Automaton (DFA) [14]. It is also similar to an algorithm used to minimize the number of implementations of a procedure required when multiple definitions of the same procedure occur in a program <ref> [11] </ref>. The algorithm partitions the cloning vectors for a procedure according to the values for their state vectors. It begins by assuming all cloning vectors for a procedure are equivalent.
Reference: [12] <author> Hall, M.W. </author> <title> Managing Interprocedural Optimization. </title> <type> PhD thesis, </type> <institution> Rice University, Department of Computer Science, Houston, TX, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: A bottom-up pass over the program propagates these variables, translating from formal to actual parameters at calls. This approach derives ImportantVariables (p), the variables of procedure p that, if constant, might improve dependence information in this procedure or one of its descendants <ref> [3, 12] </ref>. For other forward data-flow problems, a goal-directed approach depends both on the problem and the desired optimization effects. Designing a strategy for a specific compiler necessarily involves experimentation to understand how well the compiler takes advantage of the kind of facts that cloning can expose. <p> The original call graph has only n nodes and 2 (n 1) edges. Because cloning can exhibit exponential behavior, our algorithm must anticipate this possibility and impose restrictions when necessary. However, based on experience, the amount of useful cloning on a program is likely to be small <ref> [12] </ref>. For this reason, we expect that the restrictions on cloning will rarely be necessary. Nevertheless, the algorithm will perform well even in the event of pathological behavior. 4 Cloning Algorithm This section presents a polynomial-time algorithm for procedure cloning. The algorithm has three phases. <p> An ideal ordering of cloning decisions would also take into account how a decision would affect performance. A simple approach is to estimate the execution frequency of procedures and perform cloning along paths leading to the most frequently executed 11 procedures <ref> [12] </ref>. We could also use a strategy similar to the merging of vectors for an individual procedure (see Section 6). We need to do two things for the newly created clone. <p> These applications come from diverse areas: compiling for scalar architectures, compiling for both shared-memory and distributed-memory parallel architectures, and instrumenting code for run-time detection of race conditions in shared-memory parallel programs. To date, we have effectively employed cloning in experiments with interprocedural constant propagation <ref> [3, 12] </ref> and interprocedural transformations for parallel code generation [13] through hand optimization and a partial implementation of the algorithm. ParaScope is devoted to high-performance Fortran programming, but the need for cloning arises in many other contexts such as those discussed in Section 2.
Reference: [13] <author> Hall, M.W., Kennedy, K., and McKinley, </author> <title> K.S. Interprocedural transformations for parallel code generation. </title> <booktitle> In Proceedings of Supercomputing '91. IEEE Computer Society, </booktitle> <month> November </month> <year> 1991. </year>
Reference-contexts: To date, we have effectively employed cloning in experiments with interprocedural constant propagation [3, 12] and interprocedural transformations for parallel code generation <ref> [13] </ref> through hand optimization and a partial implementation of the algorithm. ParaScope is devoted to high-performance Fortran programming, but the need for cloning arises in many other contexts such as those discussed in Section 2.
Reference: [14] <author> Hopcroft, J. </author> <title> An nlogn algorithm for minimizing states in a finite automaton. </title> <editor> In Z. Kohavi and A. Paz, editors, </editor> <booktitle> Theory of Machines and Computations, </booktitle> <pages> pages 189-196. </pages> <address> New York: </address> <publisher> Academic Press, </publisher> <year> 1971. </year>
Reference-contexts: It is related to the algorithm for minimizing the number of states in a Deterministic Finite Automaton (DFA) <ref> [14] </ref>. It is also similar to an algorithm used to minimize the number of implementations of a procedure required when multiple definitions of the same procedure occur in a program [11]. The algorithm partitions the cloning vectors for a procedure according to the values for their state vectors. <p> If we test for equality by hashing the strings, the partitioning step for each procedure has an expected time linear in the number of its cloning vectors. (An approach based on state minimization would yield O (n log n) time, even for worst-case performance <ref> [14] </ref>.) Phase 3. The final phase of the cloning algorithm is accomplished by a single top-down pass over the call graph. The number of clones created is less than the total number of cloning vectors. Thus, Phase 3 is also bounded by the number of cloning vectors.
Reference: [15] <author> Johnston, </author> <title> R.L. The dynamic incremental compiler of APLn3000. </title> <booktitle> In Proceedings of the APL '79 Conference, </booktitle> <pages> pages 82-87. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1979. </year>
Reference-contexts: Section 6 addresses how to further restrict cloning in pathological cases that exceed the assumed bounds. 2 Related Work Techniques and algorithms very closely related to ours appear in intraprocedural optimization [19] and partial evaluation [4, 18]. Similar techniques appear in dynamic compilation of APL <ref> [15] </ref> and compilation of the dynamically-typed, object-oriented language self [8]. Wegman's node distinction replicates basic blocks in a procedure's control flow graph based on intra-procedural data-flow solutions and incrementally propagates the more precise solutions [19]. His algorithm uses heuristics to avoid some unnecessary replication. <p> Dynamic compilation also includes techniques similar to procedure cloning. Johnston presents the earliest such approach in his APLn3000 Dynamic Incremental Compiler <ref> [15] </ref>. When a statement is invoked for the first time, the compiler creates a compiled version specialized to the invocation environment.
Reference: [16] <author> Kuck, </author> <title> D.J. The Structure of Computers and Computations, </title> <booktitle> volume 1. </booktitle> <address> New York: </address> <publisher> John Wiley and Sons, </publisher> <year> 1978. </year>
Reference-contexts: Inlining daxpy translates the reference A (1,i) to the linearized form A (k+(i-1)*ii,1), where the value of ii is unknown at compile time. The multiplication by ii makes this subscript expression too complex for dependence analysis, upon which these transformations rely <ref> [16] </ref>. The memory optimizations rely on precise dependence information to locate reuse and to prove safety conditions. Thus, directly inlining the call creates the necessary loop structure, but leaves the code in a form where the transformations cannot be applied.
Reference: [17] <author> Marlowe, T.J. and Ryder, B.G. </author> <title> An efficient hybrid algorithm for incremental data flow analysis. </title> <booktitle> In Conference Record of the Seventeenth Annual Symposium on Principles of Programming Languages, </booktitle> <pages> pages 184-196. </pages> <publisher> ACM, </publisher> <month> January </month> <year> 1990. </year>
Reference-contexts: For call graphs containing cycles, usually representing recursion, we locate strongly-connected regions and replace the cycle with a representative node. When the algorithm reaches a representative node, it propagates each incoming cloning vector within the nodes in the cycle until the information stabilizes <ref> [17] </ref>. This approach to cycles has two important benefits. Most importantly, it prevents the algorithm from analyzing the clones generated by unrolling the recursion, which can generate an infinite number of possible clones.
Reference: [18] <author> Ruf, E. and Weise, D. </author> <title> Using types to avoid redundant specialization. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 26(9) </volume> <pages> 321-333, </pages> <year> 1991. </year>
Reference-contexts: Section 6 addresses how to further restrict cloning in pathological cases that exceed the assumed bounds. 2 Related Work Techniques and algorithms very closely related to ours appear in intraprocedural optimization [19] and partial evaluation <ref> [4, 18] </ref>. Similar techniques appear in dynamic compilation of APL [15] and compilation of the dynamically-typed, object-oriented language self [8]. Wegman's node distinction replicates basic blocks in a procedure's control flow graph based on intra-procedural data-flow solutions and incrementally propagates the more precise solutions [19]. <p> Bulyonkov describes an abstract interpretation approach to locate program points where specialization improves analysis information, both in the interprocedural and intraprocedural settings. Ruf and Weise present an algorithm to reduce the amount of specialization in a partial evaluator <ref> [18] </ref>. Their algorithm merges specialized copies if they generate the same result at every statement even though the analysis information for the two copies is different (similar to Phase 2 of the algorithm described in Section 4.2). Dynamic compilation also includes techniques similar to procedure cloning.

References-found: 18

