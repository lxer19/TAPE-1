URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3754/3754.ps.Z
Refering-URL: http://www.cs.bham.ac.uk/~anp/bibtex/kdd.bib.html
Root-URL: 
Email: fflip,labrinid,kotidis,christos,kaplunov,dejanpg@cs.umd.edu  
Title: Quantifiable Data Mining Using Principal Component Analysis proposed method con sistently achieves a "guessing error"
Author: Flip Korn, Alexandros Labrinidis, Yannis Kotidis, Christos Faloutsos Alex Kaplunovich, Dejan Perkovic 
Note: Experiments on real datasets (e.g., NBA statistics) demonstrate that the  
Address: College Park  
Affiliation: Department of Computer Science University of Maryland at  
Abstract: Association Rule Mining algorithms operate on a data matrix (e.g., customers fi products) to derive rules [2, 22]. We propose a single-pass algorithm for mining linear rules in such a matrix based on Principal Component Analysis. PCA detects correlated columns of the matrix, which correspond to, e.g., products that sell together. The first contribution of this work is that we propose to quantify the "goodness" of a set of discovered rules. We define the "guessing error": the root-mean-square error of the reconstructed values of the cells of the given matrix, when we pretend that they are unknown. The second contribution is a novel method to guess missing/hidden values from the linear rules that our method derives. For example, if somebody bought $10 of milk and $3 of bread, our rules can "guess" the amount spent on, say, butter. Thus, we can perform a variety of important tasks such as forecasting, `what-if' scenarios, outlier detection, and visualization. Moreover, we show that we can compute the principal components with a single pass over the dataset. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Rakesh Agrawal, Tomasz Imielinski, and Arun Swami. </author> <title> "Database Mining: A Performance Perspective". </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 5(6) </volume> <pages> 914-925, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: Section 4 introduces the proposed method. Section 5 gives the experimental results. Section 6 provides a discussion. Section 7 gives some conclusions and pointers to future work. 2 Related Work Agrawal et al. distinguish between three data mining problems: identifying classifications, finding sequential patterns, and discovering association rules <ref> [1] </ref>. We review only material relevant to the latter since it is the focus of this paper. See [7] for an excellent, recent survey of all three problems. The seminal work of [2] introduced the problem of discovering association rules and presented an efficient algorithm for mining them.
Reference: [2] <author> Rakesh Agrawal, Tomasz Imielinski, and Arun Swami. </author> <title> "Mining Association Rules between Sets of Items in Large Databases". </title> <booktitle> In Proc. of the 1993 ACM SIGMOD Conference, </booktitle> <pages> pages 207-216, </pages> <address> Washington D.C., USA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Data Mining has recently been receiving increasing interest [9], of which the quintessential problem is association rule mining <ref> [2] </ref>. Given a data matrix, with, e.g., customers for rows and products for columns, we want to find rules. Existing algorithms find rules of the form fbread; milkg ) butter, meaning that customers who buy "bread" and "milk" also tend to buy "butter". <p> What distinguishes database work from AI/Machine Learning and statistics work is its emphasis on large datasets. The initial association rule mining paper by Agrawal et al. <ref> [2] </ref>, as well as all the follow-up database work [4], proposed algorithms to minimize the time to extract these rules through clever record-keeping to avoid additional passes over the dataset, through parallelism, etc. fl This research was partially funded by the Institute for Systems Research (ISR), by the National Science Foundation <p> We review only material relevant to the latter since it is the focus of this paper. See [7] for an excellent, recent survey of all three problems. The seminal work of <ref> [2] </ref> introduced the problem of discovering association rules and presented an efficient algorithm for mining them. Since then, new serial algorithms [4, 15, 19] and parallel algorithms [14, 3, 10] have been proposed. In addition, generalized association rules has been the subject of recent work [21, 11]. <p> See Section 6.3 for a comparison of this method versus our proposed method. Traditional criteria for selecting association rules are based on the support-confidence framework <ref> [2] </ref>; recent alternative criteria include the chi-square test [6] and probability-based measures [20]. Related issues include outlier detection and forecasting. See [13] for a textbook treatment of both, and [5] and [12] for recent developments. 3 Principal Component Analysis The proposed method is based on principal component analysis. <p> Association Rules Since we propose a completely different paradigm of rules, namely, linear rules as opposed to association rules, it is important to discuss the qualitative differences between the two. Specifically, we are concerned with the following types of rules: * Boolean association rules <ref> [2] </ref>: e.g., fbread; milkg ) butter * quantitative association rules [22]: e.g., &lt; bread : [2 5] &gt; ) &lt; butter : [1 2] &gt; * linear rules: e.g., ratio of spendings bread:butter = 2:3 Boolean association rules have the advantages that they are easy to interpret and relatively easy to
Reference: [3] <author> Rakesh Agrawal and John C. Shafer. </author> <title> "Parallel Mining of Association Rules". </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 8(6) </volume> <pages> 962-969, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: See [7] for an excellent, recent survey of all three problems. The seminal work of [2] introduced the problem of discovering association rules and presented an efficient algorithm for mining them. Since then, new serial algorithms [4, 15, 19] and parallel algorithms <ref> [14, 3, 10] </ref> have been proposed. In addition, generalized association rules has been the subject of recent work [21, 11]. <p> Since typically the number of rows is in the hundreds of thousands (e.g., sales, or customers), and the number of columns in the hundreds (e.g., products, or patient symptoms), our algorithm of Fig. 2 is very efficient. Notice that the algorithms of <ref> [3] </ref> require more than one pass over the dataset in an attempt to find large itemsets.
Reference: [4] <author> Rakesh Agrawal and Ramakrishnan Srikant. </author> <title> "Fast Algorithms for Mining Association Rules". </title> <booktitle> In Proc. of the 20 th VLDB Conference, </booktitle> <pages> pages 487-499, </pages> <address> Santiago, Chile, </address> <month> September </month> <year> 1994. </year>
Reference-contexts: What distinguishes database work from AI/Machine Learning and statistics work is its emphasis on large datasets. The initial association rule mining paper by Agrawal et al. [2], as well as all the follow-up database work <ref> [4] </ref>, proposed algorithms to minimize the time to extract these rules through clever record-keeping to avoid additional passes over the dataset, through parallelism, etc. fl This research was partially funded by the Institute for Systems Research (ISR), by the National Science Foundation under Grants No. <p> See [7] for an excellent, recent survey of all three problems. The seminal work of [2] introduced the problem of discovering association rules and presented an efficient algorithm for mining them. Since then, new serial algorithms <ref> [4, 15, 19] </ref> and parallel algorithms [14, 3, 10] have been proposed. In addition, generalized association rules has been the subject of recent work [21, 11].
Reference: [5] <author> Andreas Arning, Rakesh Agrawal, and Prabhakar Raghavan. </author> <title> "A Linear Method for Deviation Detection in Large Databases". </title> <booktitle> In Proc. of 2 nd Int'l Conference on Knowledge Discovery and Data Mining, </booktitle> <address> Portland, Oregon, USA, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: Traditional criteria for selecting association rules are based on the support-confidence framework [2]; recent alternative criteria include the chi-square test [6] and probability-based measures [20]. Related issues include outlier detection and forecasting. See [13] for a textbook treatment of both, and <ref> [5] </ref> and [12] for recent developments. 3 Principal Component Analysis The proposed method is based on principal component analysis. PCA is a popular and powerful operation in statistical analysis [13].
Reference: [6] <author> Sergey Brin, Rajeev Motwani, and Craig Silverstein. </author> <title> "Beyond Market Baskets: Generalizing Association Rules to Correlations". </title> <booktitle> In Proc. of the 1997 ACM SIGMOD Conference (to appear), </booktitle> <address> Tucson, Arizona, USA, </address> <month> May </month> <year> 1997. </year>
Reference-contexts: See Section 6.3 for a comparison of this method versus our proposed method. Traditional criteria for selecting association rules are based on the support-confidence framework [2]; recent alternative criteria include the chi-square test <ref> [6] </ref> and probability-based measures [20]. Related issues include outlier detection and forecasting. See [13] for a textbook treatment of both, and [5] and [12] for recent developments. 3 Principal Component Analysis The proposed method is based on principal component analysis.
Reference: [7] <author> Ming-Syan Chen, Jiawei Han, and Philip S. Yu. </author> <title> "Data Mining: An Overview from a Database Perspective". </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 8(6) </volume> <pages> 866-883, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: Section 7 gives some conclusions and pointers to future work. 2 Related Work Agrawal et al. distinguish between three data mining problems: identifying classifications, finding sequential patterns, and discovering association rules [1]. We review only material relevant to the latter since it is the focus of this paper. See <ref> [7] </ref> for an excellent, recent survey of all three problems. The seminal work of [2] introduced the problem of discovering association rules and presented an efficient algorithm for mining them. Since then, new serial algorithms [4, 15, 19] and parallel algorithms [14, 3, 10] have been proposed.
Reference: [8] <author> R.O. Duda and P.E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: $2.50 on ham, how much will s/he spend on mayonnaise?'; * What-if scenarios and decision support: `We expect doubled demand of Cheerios; how much milk should we stock up on?'; * Outlier detection: `Which customers deviate from the typical sales pattern?'; * Visualization: PCA, being identical to the Karhunen-Loeve transform <ref> [8] </ref>, is the optimal dimen sionality reduction method, mapping the rows of the data matrix in to 2- or 3-dimensional points, that can be plotted to reveal the structure of the dataset (e.g., clusters, linear correlations, etc.); The paper is organized as follows: Section 2 discusses past work. <p> PCA is a popular and powerful operation in statistical analysis [13]. It is identical to the Karhunen-Loeve transform from pattern recogni tion <ref> [8] </ref>. 3.1 Intuition behind PCA In our running example, we have N customers and M products organized in an N fi M matrix X, where the entries are the dollar amount spent by customer i on product j. <p> Finally, we present a qualitative comparison of the linear rules of PCA versus general association rules that were the subject of [22]. 6.1 Visualization The application of PCA for visualization has been well-studied, and is well known in the pattern classification and image processing literature as the Karhunen-Loeve (KL) transform <ref> [8] </ref>. Recall that PCA identifies the axes of greatest variation. By projecting the points onto the top two or three of these axes (i.e., the eigenvectors associated with the largest eigenvalues), the points can be plotted to give an idea of the density and structure of the dataset.
Reference: [9] <editor> Usama Fayyad and Ramasamy Uthurusamy. </editor> <title> Data mining and knowledge discovery in databases. </title> <journal> Communications of the ACM: Data Mining and Knowledge Discovery (special issue), </journal> <volume> 39(11), </volume> <month> November </month> <year> 1996. </year>
Reference-contexts: 1 Introduction Data Mining has recently been receiving increasing interest <ref> [9] </ref>, of which the quintessential problem is association rule mining [2]. Given a data matrix, with, e.g., customers for rows and products for columns, we want to find rules.
Reference: [10] <author> Eui-Hong Han, George Karypis, and Vipin Kumar. </author> <title> "Scalable Parallel Data Mining for Association Rules". </title> <booktitle> In Proc. of the 1997 ACM SIGMOD Conference (to appear), </booktitle> <address> Tucson, Arizona, USA, </address> <month> May </month> <year> 1997. </year>
Reference-contexts: See [7] for an excellent, recent survey of all three problems. The seminal work of [2] introduced the problem of discovering association rules and presented an efficient algorithm for mining them. Since then, new serial algorithms [4, 15, 19] and parallel algorithms <ref> [14, 3, 10] </ref> have been proposed. In addition, generalized association rules has been the subject of recent work [21, 11].
Reference: [11] <author> Jiawei Han and Yongjian Fu. </author> <title> "Discovery of Multiple-Level Association Rules from Large Databases". </title> <booktitle> In Proc. of the 21 st VLDB Conference, </booktitle> <pages> pages 420-431, </pages> <address> Zurich, Switzerland, </address> <month> September </month> <year> 1995. </year>
Reference-contexts: The seminal work of [2] introduced the problem of discovering association rules and presented an efficient algorithm for mining them. Since then, new serial algorithms [4, 15, 19] and parallel algorithms [14, 3, 10] have been proposed. In addition, generalized association rules has been the subject of recent work <ref> [21, 11] </ref>. The vast majority of association rule discovery techniques are basically Boolean, since they discard the quantities of the items bought and only pay attention to whether something was bought or not.
Reference: [12] <author> Wen-Chi Hou. </author> <title> "Extraction and Applications of Statistical Relationships in Relational Databases.". </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 8(6) </volume> <pages> 939-945, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: Traditional criteria for selecting association rules are based on the support-confidence framework [2]; recent alternative criteria include the chi-square test [6] and probability-based measures [20]. Related issues include outlier detection and forecasting. See [13] for a textbook treatment of both, and [5] and <ref> [12] </ref> for recent developments. 3 Principal Component Analysis The proposed method is based on principal component analysis. PCA is a popular and powerful operation in statistical analysis [13].
Reference: [13] <author> I.T. Jolliffe. </author> <title> Principal Component Analysis. </title> <publisher> Springer Verlag, </publisher> <year> 1986. </year>
Reference-contexts: See Section 6.3 for a comparison of this method versus our proposed method. Traditional criteria for selecting association rules are based on the support-confidence framework [2]; recent alternative criteria include the chi-square test [6] and probability-based measures [20]. Related issues include outlier detection and forecasting. See <ref> [13] </ref> for a textbook treatment of both, and [5] and [12] for recent developments. 3 Principal Component Analysis The proposed method is based on principal component analysis. PCA is a popular and powerful operation in statistical analysis [13]. <p> Related issues include outlier detection and forecasting. See <ref> [13] </ref> for a textbook treatment of both, and [5] and [12] for recent developments. 3 Principal Component Analysis The proposed method is based on principal component analysis. PCA is a popular and powerful operation in statistical analysis [13]. It is identical to the Karhunen-Loeve transform from pattern recogni tion [8]. 3.1 Intuition behind PCA In our running example, we have N customers and M products organized in an N fi M matrix X, where the entries are the dollar amount spent by customer i on product j. <p> In the end, only the eigenvectors associated with the k largest eigen-values, namely the principal components, are kept. The goal here is to preserve most of the important information while discarding the redundancy. In order to choose the cutoff k of PCs to retain, the simplest textbook heuristic <ref> [13, p. 94] </ref> is to retain enough eigenvectors so that the sum of their eigenvalues 4 cover 85% of the grand total. <p> The advantage for the quantitative association rules are * They will be more suitable if the data points form clusters. * They have been applied to categorical data, although similar extensions of PCA are discussed in <ref> [13] </ref>. For the linear rules, the advantages are the following: 14 * They achieve more compact descriptions, if the data points are linearly correlated, as in Figure 10, or as in the real datasets that we saw earlier. <p> estimate one or more missing/hidden/corrupted values, when a new data record is given; thus, they can also be used in forecasting, for `what-if' scenarios, and for detecting outliers; * They are based on the time-tested tool of Principal Component Analysis (PCA), which is the optimal way to perform dimensionality reduction <ref> [13] </ref>; * They are easy to implement: the most difficult part of our method is the solution of an eigensystem for which reliable packages and/or source code are widely available; * They are fast and scalable, requiring a single pass over the data matrix, and growing linearly on the largest dimension
Reference: [14] <author> Andreas Mueller. </author> <title> "Fast Sequential and Parallel Algorithms for Association Rule Mining: A Comparison". </title> <type> Technical Report CS-TR-3515, </type> <institution> Department of Computer Science, University of Maryland at College Park, </institution> <month> August </month> <year> 1995. </year> <month> 20 </month>
Reference-contexts: See [7] for an excellent, recent survey of all three problems. The seminal work of [2] introduced the problem of discovering association rules and presented an efficient algorithm for mining them. Since then, new serial algorithms [4, 15, 19] and parallel algorithms <ref> [14, 3, 10] </ref> have been proposed. In addition, generalized association rules has been the subject of recent work [21, 11].
Reference: [15] <author> Jong Soo Park, Ming-Syan Chen, and Philip S. Yu. </author> <title> "An Effective Hash Based Algorithm for Mining Association Rules". </title> <booktitle> In Proc. of the 1995 ACM SIGMOD Conference, </booktitle> <pages> pages 175-186, </pages> <address> San Jose, California, USA, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: See [7] for an excellent, recent survey of all three problems. The seminal work of [2] introduced the problem of discovering association rules and presented an efficient algorithm for mining them. Since then, new serial algorithms <ref> [4, 15, 19] </ref> and parallel algorithms [14, 3, 10] have been proposed. In addition, generalized association rules has been the subject of recent work [21, 11].
Reference: [16] <author> William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery. </author> <title> Numerical Recipes in C. </title> <publisher> Cambridge University Press, </publisher> <year> 1992. </year> <note> 2nd Edition. </note>
Reference-contexts: Here the system has no solution, so we find a least-squares solution for x concept based on the Moore-Penrose pseudo-inverse of V 0 . This uses the singular value decomposition (see <ref> [16] </ref>) of V 0 : V 0 = R fi diag ( j ) fi S t (9) Since V 0 is singular, no inverse exists, but we can find a pseudo-inverse: [V 0 ] 1 = S fi diag (1= j ) fi R t (10) and, thus, x concept
Reference: [17] <author> John Ross Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann Publishers Inc., </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: The details of the choice of training and testing sets is orthogonal to our definition, and outside the scope of this work, since they have been extensively examined in the machine learning and classification 6 literature <ref> [17] </ref>. A reasonable choice is to use 90% of the original data matrix for training and the remaining 10% for testing. Another possibility is the use the entire data matrix for both training and testing.
Reference: [18] <author> G. Salton and M.J. McGill. </author> <title> Introduction to Modern Information Retrieval. </title> <publisher> McGraw-Hill, </publisher> <year> 1983. </year>
Reference-contexts: Of course, the proposed method is applicable to any N fi M matrix, with a variety of interpretations for the rows and columns, e.g., patients and medical-test-measurements (blood pressure, body weight, etc.); documents and terms (typical in Information Retrieval <ref> [18] </ref>), etc. Each row vector of the matrix can be thought of as an M -dimensional point. Given this set of N points, PCA identifies the axes (orthogonal directions) of greatest variance, after centering the points about the origin. Figure 1 illustrates an example of an axis that PCA finds.
Reference: [19] <author> Ashoka Savasere, Edward Omiecinski, and Shamkant B. Navathe. </author> <title> "An Efficient Algorithm for Mining Association Rules in Large Databases". </title> <booktitle> In Proc. of the 21 st VLDB Conference, </booktitle> <pages> pages 432-444, </pages> <address> Zurich, Switzerland, </address> <month> September </month> <year> 1995. </year>
Reference-contexts: See [7] for an excellent, recent survey of all three problems. The seminal work of [2] introduced the problem of discovering association rules and presented an efficient algorithm for mining them. Since then, new serial algorithms <ref> [4, 15, 19] </ref> and parallel algorithms [14, 3, 10] have been proposed. In addition, generalized association rules has been the subject of recent work [21, 11].
Reference: [20] <author> Abraham Silberschatz and Alexander Tuzhilin. </author> <title> "What Makes Patterns Interesting in Knowledge Discovery". </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 8(6) </volume> <pages> 970-974, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: See Section 6.3 for a comparison of this method versus our proposed method. Traditional criteria for selecting association rules are based on the support-confidence framework [2]; recent alternative criteria include the chi-square test [6] and probability-based measures <ref> [20] </ref>. Related issues include outlier detection and forecasting. See [13] for a textbook treatment of both, and [5] and [12] for recent developments. 3 Principal Component Analysis The proposed method is based on principal component analysis. PCA is a popular and powerful operation in statistical analysis [13].
Reference: [21] <author> Ramakrishnan Srikant and Rakesh Agrawal. </author> <title> "Mining Generalized Association Rules". </title> <booktitle> In Proc. of the 21 st VLDB Conference, </booktitle> <pages> pages 407-419, </pages> <address> Zurich, Switzerland, </address> <month> September </month> <year> 1995. </year>
Reference-contexts: The seminal work of [2] introduced the problem of discovering association rules and presented an efficient algorithm for mining them. Since then, new serial algorithms [4, 15, 19] and parallel algorithms [14, 3, 10] have been proposed. In addition, generalized association rules has been the subject of recent work <ref> [21, 11] </ref>. The vast majority of association rule discovery techniques are basically Boolean, since they discard the quantities of the items bought and only pay attention to whether something was bought or not.
Reference: [22] <author> Ramakrishnan Srikant and Rakesh Agrawal. </author> <title> "Mining Quantitative Association Rules in Large Relational Tables". </title> <booktitle> In Proc. of the 1996 ACM SIGMOD Conference, </booktitle> <pages> pages 1-12, </pages> <address> Montreal, Quebec, Canada, </address> <month> June </month> <year> 1996. </year> <month> 21 </month>
Reference-contexts: The vast majority of association rule discovery techniques are basically Boolean, since they discard the quantities of the items bought and only pay attention to whether something was bought or not. A notable exception is the work of Srikant and Agrawal <ref> [22] </ref>, where they address the problem of mining quantitative association rules. <p> Using the 10 PCA. `nba' dataset, we demonstrate how these linear rules can be interpreted, with references to the plots. Finally, we present a qualitative comparison of the linear rules of PCA versus general association rules that were the subject of <ref> [22] </ref>. 6.1 Visualization The application of PCA for visualization has been well-studied, and is well known in the pattern classification and image processing literature as the Karhunen-Loeve (KL) transform [8]. Recall that PCA identifies the axes of greatest variation. <p> Specifically, we are concerned with the following types of rules: * Boolean association rules [2]: e.g., fbread; milkg ) butter * quantitative association rules <ref> [22] </ref>: e.g., &lt; bread : [2 5] &gt; ) &lt; butter : [1 2] &gt; * linear rules: e.g., ratio of spendings bread:butter = 2:3 Boolean association rules have the advantages that they are easy to interpret and relatively easy to implement.
References-found: 22

