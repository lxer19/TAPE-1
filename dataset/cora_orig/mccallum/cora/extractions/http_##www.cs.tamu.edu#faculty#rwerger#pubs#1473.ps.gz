URL: http://www.cs.tamu.edu/faculty/rwerger/pubs/1473.ps.gz
Refering-URL: http://www.cs.tamu.edu/faculty/rwerger/pubs/
Root-URL: http://www.cs.tamu.edu
Email: padua@csrd.uiuc.edu  
Phone: (217) 333-4223  
Title: Advanced Program Restructuring for High-Performance Computers with Polaris  
Author: William Blume Ramon Doallo Rudolf Eigenmann John Grout Jay Hoeflinger Thomas Lawrence Jaejin Lee David Padua Yunheung Paek Bill Pottenger Lawrence Rauchwerger Peng Tu 
Keyword: Compilers, Program Restructuring, Automatic Parallelization, Parallel Computing.  
Note: The research described is supported by Army contract #DABT63-92-C-003. This work is not necessarily representative of the positions or policies of the Army or the Government.  
Address: 1308 W. Main Street, Urbana, IL 61801-2932, USA  
Affiliation: Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, 415 Computer and Systems Research Laboratory  
Abstract: Multiprocessor computers are rapidly becoming the norm. Parallel workstations are widely available today and it is likely that most PCs in the near future will also be parallel. To accommodate these changes, some classes of applications must be developed in explicitly parallel form. Yet, in order to avoid a substantial increase in software development costs, compilers that translate conventional programs into efficient parallel form clearly will be necessary. In the ideal case, multiprocessor parallelism should be as transparent to programmers as functional level parallelism is to programmers of today's superscalar machines. However, compiling for multiprocessors is substantially more complex than compiling for functional unit parallelism, in part because successful parallelization often requires a very accurate analysis of long sections of code. This article discusses recent experience at Illinois on the automatic parallelization of scientific codes using the Polaris restructurer. Also, the article presents several new analysis techniques that we have developed in recent years based on an extensive analysis of the characteristics of real Fortran codes. These techniques, which are based on both static and dynamic parallelization strategies, have been incorporated in the Polaris restructurer. Preliminary results on parallel workstations are encouraging and, once the implementation of the new techniques is complete, we expect that Polaris will be able to obtain good speedups for most scientific codes on parallel workstations. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Utpal Banerjee, Rudolf Eigenmann, Alexandru Nicolau, and David Padua. </author> <title> Automatic Program Paralleliza 20 tion. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 81(2), </volume> <month> February </month> <year> 1993. </year>
Reference-contexts: The discussion of the techniques is brief, due to space limitations, but includes references to papers describing them in more detail. Although we did not include an extensive list of references, also due to space limitations, a more complete list can be found in <ref> [1] </ref>, a recent survey paper. 2 2 Techniques Implemented in Polaris Before starting the Polaris project, our group conducted a study of the effectiveness of automatic parallelization of Fortran programs [4].
Reference: [2] <author> William Blume and Rudolf Eigenmann. </author> <title> The Range Test: A Dependence Test for Symbolic, Non-linear Expressions. </title> <booktitle> Proceedings of Supercomputing '94, </booktitle> <address> Washington D.C., </address> <pages> pages 528-537, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Most techniques, including those in the version of KAP used in our previous study, have similar requirements. In fact, several important loops in our study were not parallelized because the values of the coefficients in the subscript expressions were not known at compile-time. We have developed the Range Test <ref> [2, 3] </ref>, a dependence analysis algorithm with symbolic algebra capabilities. To determine when there are no cross-iteration dependences, this test makes use of information on the possibly symbolic range of values that a subscript expression may assume.
Reference: [3] <author> William Joseph Blume. </author> <title> Symbolic Analysis Techniques for Effective Automatice Parallelization. </title> <type> PhD thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, Cntr. for Supercomputing Res. & Dev., </institution> <month> June </month> <year> 1995. </year>
Reference-contexts: Most techniques, including those in the version of KAP used in our previous study, have similar requirements. In fact, several important loops in our study were not parallelized because the values of the coefficients in the subscript expressions were not known at compile-time. We have developed the Range Test <ref> [2, 3] </ref>, a dependence analysis algorithm with symbolic algebra capabilities. To determine when there are no cross-iteration dependences, this test makes use of information on the possibly symbolic range of values that a subscript expression may assume.
Reference: [4] <author> Rudolf Eigenmann, Jay Hoeflinger, Greg Jaxon, Zhiyuan Li, and David Padua. </author> <title> Restructuring Fortran Programs for Cedar. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 5(7) </volume> <pages> 553-573, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: not include an extensive list of references, also due to space limitations, a more complete list can be found in [1], a recent survey paper. 2 2 Techniques Implemented in Polaris Before starting the Polaris project, our group conducted a study of the effectiveness of automatic parallelization of Fortran programs <ref> [4] </ref>. The collection of analysis and transformation techniques now implemented in Polaris was primarily based on the conclusions of that study.
Reference: [5] <author> John Robert Grout. </author> <title> Inline Expansion for the Polaris Research Compiler. </title> <type> Master's thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, Cntr. for Supercomputing Res. & Dev., </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: In addition to the four techniques presented in the preceding section, Polaris also applies Autoinlining <ref> [5] </ref> and Interprocedural value Propagation (IPVP). Autoinlining expands all calls within loop bodies to subroutines with fewer than a given number of lines (50 for the experiments reported below). Interprocedural Value Propagation propagates the value of integer subroutine parameters across the entire program.
Reference: [6] <author> Bill Pottenger and Rudolf Eigenmann. </author> <title> Idiom Recognition in the Polaris Parallelizing Compiler. </title> <booktitle> Proceedings of the 9th ACM International Conference on Supercomputing, </booktitle> <address> Barcelona, Spain, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: END DO all occurrences of J after S could be replaced by I + I*(I-1)/2, and those after T within the inner loop by I + I*(I-1)/2+K. Techniques to identify and replace induction variables in multiply-nested loops have been implemented in Polaris <ref> [6] </ref>. The implementation uses information on the range of values of variables and is supported by symbolic algebra algorithms. 2.4 Reduction Variable Recognition Reduction variables are those incremented within the loop body, usually by floating-point values that change across iterations. <p> A (K) = A (K) + expression END DO Advanced techniques to transform loops like this into parallel form have been implemented in Polaris <ref> [6] </ref>. 3 Effectiveness of Polaris Techniques We now discuss the effectiveness of the six main analysis and transformation techniques implemented in Polaris. In addition to the four techniques presented in the preceding section, Polaris also applies Autoinlining [5] and Interprocedural value Propagation (IPVP).
Reference: [7] <author> Lawrence Rauchwerger, Nancy M. Amato, and David A. Padua. </author> <title> Run-Time Methods for Parallelizing Partially Parallel Loops. </title> <booktitle> Proceedings of the 9th ACM International Conference on Supercomputing, </booktitle> <address> Barcelona, Spain, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Furthermore, we believe that this type of access pattern is prevalent in other applications (not introduced in this article) such as modern, sparse simulations and integer programs. In <ref> [7] </ref> and [8] we have offered a solution for both the case just described, and for any case in which classical, static compile-time methods fail. In brief, we propose an analysis of the dependence pattern at run-time, when all necessary values become known.
Reference: [8] <author> Lawrence Rauchwerger and David Padua. </author> <title> The LRPD Test: Speculative Run-Time Parallelization of Loops with Privatization and Reduction Parallelization. </title> <booktitle> Proceedings of the SIGPLAN'95 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: Furthermore, we believe that this type of access pattern is prevalent in other applications (not introduced in this article) such as modern, sparse simulations and integer programs. In [7] and <ref> [8] </ref> we have offered a solution for both the case just described, and for any case in which classical, static compile-time methods fail. In brief, we propose an analysis of the dependence pattern at run-time, when all necessary values become known.
Reference: [9] <author> Lawrence Rauchwerger and David A. Padua. </author> <title> Parallelizing WHILE Loops for Multiprocessor Systems. </title> <booktitle> Proceedings for the 9th International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1995. </year>
Reference-contexts: A naive parallelization of such a loop could execute too many iterations, thereby modifying data that would not have been changed in a sequential execution. We believe that the problem can be solved in the near future by implementing some of the ideas presented in <ref> [9] </ref>. Briefly stated, we provide a mechanism through which to undo the modifications of unwanted iterations.
Reference: [10] <author> Peng Tu and David Padua. </author> <title> Automatic Array Privatization. </title> <editor> In Utpal BanerjeeDavid GelernterAlex Nicolau-David Padua, editor, </editor> <booktitle> Proc. Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR. </address> <booktitle> Lecture Notes in Computer Science., </booktitle> <volume> volume 768, </volume> <pages> pages 500-521, </pages> <month> August 12-14, </month> <year> 1993. </year>
Reference-contexts: Identifying private arrays is of great importance for parallelization. The lack of array privatization techniques was one of the primary causes of the low speedups obtained in our earlier study. In Polaris, we have implemented a privatization technique to deal both with scalars and arrays <ref> [10] </ref>. This algorithm is substantially more complex than the traditional scalar privatization algorithm because it is necessary at each point within the loop to keep track of the array sections assigned earlier in the iteration and of the array section that is read.

References-found: 10

