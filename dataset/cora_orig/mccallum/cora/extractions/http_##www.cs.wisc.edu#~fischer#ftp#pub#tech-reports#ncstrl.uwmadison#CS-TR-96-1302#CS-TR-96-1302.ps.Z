URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-96-1302/CS-TR-96-1302.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-96-1302/
Root-URL: http://www.cs.wisc.edu
Email: wwt@cs.wisc.edu  
Author: Babak Falsafi and David A. Wood 
Address: 1210 West Dayton Street Madison, WI 53706  
Affiliation: Computer Sciences Department University of WisconsinMadison  
Abstract: This work is supported in part by Wright Laboratory Avionics Directorate, Air Force Material Command, USAF, under grant #F33615-94-1-1525 and ARPA order no. B550, NSF PYI Award CCR-9157366, NSF Grant MIP-9225097, Digital Equipment Corporation, Sun Microsystems, Thinking Machines Corporation, and Xerox Corporation. Our Thinking Machines CM-5 was purchased through NSF Institutional Infrastructure Grant No. CDA-9024618 with matching funding from the University of Wisconsin Graduate School. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the Wright Laboratory Avionics Directorate or the U.S. Government. When does Dedicated Protocol Processing Make Sense? Abstract Distributed-memory parallel computers and networks of workstations (NOWs) both rely on efficient communication over increasingly high-speed networks. Software communication protocols from ow-control and reliable delivery to multicasting and coherent distributed shared memory are often the performance bottleneck. Several current and proposed parallel systemse.g., the Intel Paragonaddress this problem by dedicating one general-purpose processor (in a multiprocessor node) specifically for protocol processing. This operating system convention reduces communication latency and increases effective bandwidth, but also reduces the peak performance since the dedicated processor no longer performs useful computation. In this paper, we study a network of multiprocessor workstations and ask the question: when does it make sense to dedicate a processor specifically for protocol processing? We compare three protocol processing policies: Single, the baseline case with one processor that does everything; Fixed, which uses a dedicated protocol processor; and Floating, where all processors perform both computation and protocol processing. We use a simple analytic model of a general request/reply protocol to illustrate the trade-offs between the policies. The model shows that: i) adding a dedicated protocol processor to a uniprocessor node is unlikely to be cost-effective and even less likely to outperform the Floating policy; ii) a dedicated processor is more advantageous for light-weight protocols (e.g., active messages) than for heavy-weight protocols (e.g., TCP/IP), iii) the Fixed policy becomes advantageous when communication becomes the bottleneck, as when multiple compute processors and multithreading saturate the resource. The break-even point between Fixed and Floating is a function of the number of processors, protocol overheads, and application parallelism. We then evaluate these policies in the context of a fine-grain user-level distributed shared memory system. We present preliminary measurements from a dedicated network of Sun SparcStation-20s connected by a Myrinet network. The measured performance on four nodeseach with up to four processorsof three hybrid shared-memory parallel applications confirm the intuitive results from the model. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anant Agarwal. </author> <title> Performance Tradeoffs in Multithreaded Processors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(5):525539, </volume> <month> September </month> <year> 1992. </year>
Reference-contexts: For example, in multithreaded distributed shared memory systems [2,12], a remote miss causes the (compute) processor to switch to executing a new thread. If the context switch overhead (T cs ) is less than the remote miss latency, than multithreading should increase processor utilization <ref> [1] </ref>. Under the Fixed policy, the compute processors utilization can grow as high as C / (C + T cs ), if the application has sufficient parallelism and the protocol processor does not saturate.
Reference: [2] <author> Anant Agarwal, Beng-Hong Lim, David Kranz, and John Kubiatowicz. </author> <month> APRIL: </month> <title> A Processor Architecture for Multiprocessing. </title> <booktitle> In Proc. of the 17th Intll Symposium on Computer Architecture, </booktitle> <pages> pages 104114, </pages> <month> June </month> <year> 1990. </year>
Reference: [3] <author> Tom Anderson. </author> <title> NOW: Distributed Supercomputing on a Network of Workstations, September 1993. </title> <booktitle> Presentation at 1993 Fall ARPA HPC Software PIs meeting. </booktitle>
Reference: [4] <author> Tom Anderson, David Culler, and David Patterson. </author> <title> A Case for Networks of Workstations: NOW. </title> <type> Technical report, </type> <institution> Computer Science Division (EECS), University of California at Berkeley, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: 1 Introduction Distributed-memory parallel computers have become the supercomputers of the 1990s, providing impressive performance on many large and important applications. Networks of workstations (NOWs) promise to exploit economies-of-scale to make large-scale parallel computing cost-effective enough for every day use <ref> [4] </ref>. Both types of systems rely heavily on efficient communication over high-speed networks. While the underlying network hardware keeps improving rapidly, the overhead of software communications protocolsranging from ow-control and reliable delivery to multicasting and coherent distributed shared memoryhas increasingly become a bottleneck [18].
Reference: [5] <author> Boon Seong Ang, Arvind, and Derek Chiou. </author> <title> StarT the Next Generation: Integrating Global Caches and Dataflow Architecture. CSG Memo 354, </title> <publisher> MIT, </publisher> <month> February </month> <year> 1994. </year>
Reference-contexts: By reducing the frequency of system calls, interrupts, locking, and cache pollution, these processors reduce communication latency and increase effective bandwidth. A variation on this approach exploits the growing availability of bus-based shared-memory multiprocessors. The Intel Paragon [13], MIT StarT-NG <ref> [5] </ref>, and Wisconsin T-Zero [21] systems all dedicate one processor of a multiprocessor nodeby operating system conventionspecifically for protocol processing. Unfortunately, while a dedicated protocol processor can improve communications performance, it provides little benefit for compute-bound programs. These applications would rather use the dedicated processor for computation.
Reference: [6] <author> James Boyle, Ralph Butler, Terrence Disz, Barnett Glickfieldand Ewing Lusk, Ross Overbeek, James Patterson, and Rick Stevens. </author> <title> Portable Programs for Parallel Processors. </title> <publisher> Holt, Rinehart and Winston Inc., </publisher> <year> 1987. </year>
Reference-contexts: Table 1 lists the applications and input parameters used in this study. All programs are Tempest-compliant applications and communicate using a combination of active messages and transparent shared memory. Coherence is maintained using a sequentially-consistent protocol with 64-byte blocks. The applications use PARMACS directives <ref> [6] </ref> to create a process per node, allocate shared memory, and synchronize between nodes. Each PARMACS process is multithreaded using our locally-developed thread package.
Reference: [7] <author> Satish Chandra, James R. Larus, and Anne Rogers. </author> <booktitle> Where is Time Spent in Message-Passing and Shared-Memory Programs? In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VI), </booktitle> <pages> pages 6175, </pages> <month> October </month> <year> 1994. </year> <note> [8] et al. </note> <author> Craig Stunkel. </author> <title> The SP2 High-Performance Switch. </title> <journal> IBM System Journal, </journal> <volume> 34(2), </volume> <year> 1995. </year> <note> to appear. </note>
Reference-contexts: The last phase exhibits load imbalance and does not speedup as the number of threads increases. However, it does not account for a large fraction of the execution time <ref> [7] </ref>. Reading the maximum pivot row is a potential bottleneck since all threads read the row before computation begins. Thus the faster the protocol processor can deliver the data, the faster the program runs.
Reference: [9] <author> D. E. Culler, A. Dusseau, S. C. Goldstein, A. Krishnamurthy, S. Lumetta, T. von Eicken, and K. Yelick. </author> <title> Parallel Programming in Split-C. </title> <booktitle> In Proceedings of Supercomputing 93, </booktitle> <pages> pages 262273, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: push the bus to its limit, Fixed improves performance slightly by minimizing the bus traffic required to process protocol actions (since the protocol code and state are not ushed from the protocol processors cache by the computation). 6.3 Em3d Em3d models propagation of electromagnetic waves through objects in three dimensions <ref> [9] </ref>. The program iterates over a bipartite graph made up of directed edges between nodes representing electric and magnetic fields (E and H nodes, respectively). The graph is partitioned over all threads so that each thread gets an equal share of the computation.
Reference: [10] <author> J. Dongarra, G. A. Geist, R. Manchek, and V. S. Sunderam. </author> <title> Integrated PVM framework supports heterogeneous network computing. </title> <booktitle> Computers in Physics, </booktitle> <address> 7(2):166174, </address> <month> March-April </month> <year> 1993. </year>
Reference: [11] <author> Babak Falsafi, Alvin Lebeck, Steven Reinhardt, Ioannis Schoinas, Mark D. Hill, James Larus, Anne Rogers, and David Wood. </author> <title> Application-Specific Protocols for User-Level Shared Memory. </title> <booktitle> In Proceedings of Supercomputing 94, </booktitle> <pages> pages 380389, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: We vary the number of processors per node from one to four as well as the number of threads per processor. We examine the performance of three hybrid shared-memory applications <ref> [11] </ref> running on a fine-grain distributed shared memory system [23]. Measurements confirm the breakeven point predicted by the analytic model. Fixed performs better than Float for communication-intensive codes that saturate the protocol processor.
Reference: [12] <author> Vincent Freech, David W. Lowenthal, and Gregory R. Andrews. </author> <title> Distributed Filaments: Efficient Fine-Grain Parallelism on a Cluster of Workstations. </title> <booktitle> In First USENIX Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 201214, </pages> <month> November </month> <year> 1994. </year>
Reference: [13] <author> Intel Corporation. </author> <title> Paragon Technical Summary. Intel Supercomputer Systems Division, </title> <year> 1993. </year>
Reference-contexts: By reducing the frequency of system calls, interrupts, locking, and cache pollution, these processors reduce communication latency and increase effective bandwidth. A variation on this approach exploits the growing availability of bus-based shared-memory multiprocessors. The Intel Paragon <ref> [13] </ref>, MIT StarT-NG [5], and Wisconsin T-Zero [21] systems all dedicate one processor of a multiprocessor nodeby operating system conventionspecifically for protocol processing. Unfortunately, while a dedicated protocol processor can improve communications performance, it provides little benefit for compute-bound programs. These applications would rather use the dedicated processor for computation.
Reference: [14] <author> Randy Katz. </author> <title> ASPLOSVI Keynote Address, </title> <month> October </month> <year> 1994. </year>
Reference-contexts: These applications would rather use the dedicated processor for computation. In a recent experiment, researchers at Sandia demonstrated that using the Paragons protocol processor for computation (via a low-level cross-call mechanism under SUNMOS) nearly doubled the performance of Linpack <ref> [14] </ref>.
Reference: [15] <author> Jeffrey Kuskin et al. </author> <title> The Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 302313, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: To address this problem, many distributed-memory parallel machines employ a dedicated protocol processor to off-load the primary (computation) processor (s). For example, the Meiko CS-2 [17], IBM SP-2 [8], and proposed Stanford FLASH <ref> [15] </ref> all use embedded processors to accelerate communications performance. By reducing the frequency of system calls, interrupts, locking, and cache pollution, these processors reduce communication latency and increase effective bandwidth. A variation on this approach exploits the growing availability of bus-based shared-memory multiprocessors.
Reference: [16] <author> James R. Larus and Eric Schnarr. EEL: </author> <title> Machine-Independent Executable Editing. </title> <booktitle> In Proceedings of the SIG-PLAN 95 Conference on Programming Language Design and Implementation (PLDI), </booktitle> <month> June </month> <year> 1995. </year> <note> To Appear. </note>
Reference-contexts: The alternative is periodic polling, which requires instrumenting the computation thread to periodically check for messages. This can be done either via a compiler [30] or by directly editing the executable file <ref> [16] </ref>. This approach requires a trade-off between latency and overhead: frequent polls decrease message latency but increase overhead. 3.2 Fixed Protocol Processor The Fixed policy dedicates one processor of a multiprocessor node to perform only protocol processing. <p> These faults invoke a user-registered handler much like the UNIX signal interface. The study in this paper uses a software implementation of fine-grain access control similar to that in Blizzard-S [23]. We use EEL <ref> [16] </ref>, a tool for executing editable files, to instrument each shared memory operations with tag lookup code that enforces the Tempest access control semantics.
Reference: [17] <author> Meiko World Inc. </author> <title> Computing Surface 2: Overview Documentation Set, </title> <year> 1993. </year>
Reference-contexts: To address this problem, many distributed-memory parallel machines employ a dedicated protocol processor to off-load the primary (computation) processor (s). For example, the Meiko CS-2 <ref> [17] </ref>, IBM SP-2 [8], and proposed Stanford FLASH [15] all use embedded processors to accelerate communications performance. By reducing the frequency of system calls, interrupts, locking, and cache pollution, these processors reduce communication latency and increase effective bandwidth.
Reference: [18] <author> Alan Montz, David Mosberger, Sean OMalley, Larry L. Peterson, Todd A. Proebsting, and John H. Hartman. </author> <title> Scout: A Communications-Oriented Operating System. </title> <type> Technical Report TR 94-02, </type> <institution> Department of Computer Science, University of Arizona, </institution> <year> 1994. </year>
Reference-contexts: Both types of systems rely heavily on efficient communication over high-speed networks. While the underlying network hardware keeps improving rapidly, the overhead of software communications protocolsranging from ow-control and reliable delivery to multicasting and coherent distributed shared memoryhas increasingly become a bottleneck <ref> [18] </ref>. To address this problem, many distributed-memory parallel machines employ a dedicated protocol processor to off-load the primary (computation) processor (s). For example, the Meiko CS-2 [17], IBM SP-2 [8], and proposed Stanford FLASH [15] all use embedded processors to accelerate communications performance.
Reference: [19] <author> David Mosberger, Larry L. Peterson, and Sean OMalley. </author> <title> Protocol Latency: Mips and Reality. </title> <type> Technical Report TR 95-02, </type> <institution> Department of Computer Science, University of Arizona, </institution> <year> 1995. </year>
Reference: [20] <author> Scott Pakin, Vijay Karamcheti, and Andrew A. Chien. </author> <title> Myrinet Fast Messages, </title> <month> March </month> <year> 1995. </year>
Reference-contexts: The breakeven point is a function of the number of processors, protocol overheads, and application parallelism. We then present preliminary measurements from an implementation on four nodes of a network of Sun SparcStation-20s connected by a Myrinet network 1 running the light-weight Illinois Fast Message protocol <ref> [20] </ref>. We vary the number of processors per node from one to four as well as the number of threads per processor. We examine the performance of three hybrid shared-memory applications [11] running on a fine-grain distributed shared memory system [23]. <p> However, the results of this study are largely independent of this implementation decision. Messaging Layer On this system, the Tempest active message interface is implemented on top of the Illinois FM library, which provides low-latency communication through the Myrinet switch <ref> [20] </ref>. The library implements an active message interface and guarantees delivery of messages. The FM interface provides send calls to inject messages and an extract call to receive any pending messages and dispatches the corresponding handlers.
Reference: [21] <author> Steve Reinhardt, Robert Pfile, and David A. Wood. T-Zero: </author> <title> Prototyping distributed shared-memory on a network of workstations. </title> <type> Technical report, </type> <institution> Computer Sciences Department, University of WisconsinMadison, </institution> <year> 1995. </year>
Reference-contexts: By reducing the frequency of system calls, interrupts, locking, and cache pollution, these processors reduce communication latency and increase effective bandwidth. A variation on this approach exploits the growing availability of bus-based shared-memory multiprocessors. The Intel Paragon [13], MIT StarT-NG [5], and Wisconsin T-Zero <ref> [21] </ref> systems all dedicate one processor of a multiprocessor nodeby operating system conventionspecifically for protocol processing. Unfortunately, while a dedicated protocol processor can improve communications performance, it provides little benefit for compute-bound programs. These applications would rather use the dedicated processor for computation.
Reference: [22] <author> Steven K. Reinhardt, James R. Larus, and David A. Wood. Tempest and Typhoon: </author> <title> User-Level Shared Memory. </title> <booktitle> In Proc. of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 325337, </pages> <month> April </month> <year> 1994. </year> <month> 17 </month>
Reference-contexts: More general time sharing is of course possible, but is beyond the scope of this paper. A distributed shared memory system extends the coherent shared memory abstraction beyond the processors in a single node. This study assumes a fine-grain distributed shared memory system based on the Tempest interface <ref> [22] </ref>. This system allocates shared memory at the page granularity, like many other DSM systems, but maintains coherence at a finer grain (e.g., a cache block). Coherence is enforced via a fine-grain access control mechanism analogous to the ubiquitous page-level protection mechanism.
Reference: [23] <author> Ioannis Schoinas, Babak Falsafi, Alvin R. Lebeck, Steven K. Reinhardt, James R. Larus, and David A. Wood. </author> <title> Fine-grain Access Control for Distributed Shared Memory. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems , pages 297307, </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: We vary the number of processors per node from one to four as well as the number of threads per processor. We examine the performance of three hybrid shared-memory applications [11] running on a fine-grain distributed shared memory system <ref> [23] </ref>. Measurements confirm the breakeven point predicted by the analytic model. Fixed performs better than Float for communication-intensive codes that saturate the protocol processor. <p> Coherence is enforced via a fine-grain access control mechanism analogous to the ubiquitous page-level protection mechanism. While the results of this paper are largely independent of whether this mechanism is implemented in hardware or software, the results in Section 6 come from a software technique <ref> [23] </ref>. High-performance communication is performed via an active message abstraction [30]. Active messages are essentially very light-weight RPCs that are optimized for the case where processing nodes are co-scheduled; that is, where the destination node already has the correct context for the RPC. <p> Fine-grain access control Fine-grain access control provides the foundation of the distributed shared-memory system used in this study <ref> [23] </ref>. The Tempest interface defines a set of mechanisms to control sharing a region of memory at subpage granularities. These mechanisms allow user-level software to manipulate logical tags on 32-byte blocks of memory. These tags have possible values of Invalid, Readonly and 1. <p> Read or Write accesses to an Invalid block or Write accesses to a Readonly block incur block access faults. These faults invoke a user-registered handler much like the UNIX signal interface. The study in this paper uses a software implementation of fine-grain access control similar to that in Blizzard-S <ref> [23] </ref>. We use EEL [16], a tool for executing editable files, to instrument each shared memory operations with tag lookup code that enforces the Tempest access control semantics.
Reference: [24] <author> SPEC. </author> <title> SPEC Benchmark Suite Release 1.0, </title> <month> Winter </month> <year> 1990. </year>
Reference-contexts: Name Input Data Set em3d 8160 nodes, degree 5, 5% remote, distance span 2, 10 iterations gauss 672 x 672 matrix tomcatv 768 x 768 matrices, 10 iterations TABLE 1. Benchmark Input Parameters 12 6.2 Tomcatv Tomcatv is a parallel version of the well-known SPEC benchmark <ref> [24] </ref>. The program performs an iterative stencil computation on a pair of matrices allocated in shared memory and five local matrices. Work is partitioned by assigning a block of rows to each thread.
Reference: [25] <author> Mark S. Squillante and Edward D. Lazowska. </author> <title> Using Processor-Cache Affinity Information in Shared-Memory Multiprocessor Scheduling. </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> 4(2):131143, </volume> <month> April </month> <year> 1990. </year>
Reference: [26] <author> Chandramohan A. Thekkath and Henry M. Levy. </author> <title> Hardware and Software Support for Efficient Exception Handling. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 110119, </pages> <address> San Jose, California, </address> <year> 1994. </year>
Reference-contexts: Unfortunately, delivering user-level interrupts is slow in most current operating systems; Thekkath and Levy showed that a simple exception (which requires a similar path through the kernel) takes at least 60 to 200 microseconds for a round-trip <ref> [26] </ref>. The alternative is periodic polling, which requires instrumenting the computation thread to periodically check for messages. This can be done either via a compiler [30] or by directly editing the executable file [16].
Reference: [27] <author> Radhika Thekkath and Susan J. Eggers. </author> <title> The Effectiveness of Multiple Hardware Contexts. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 328337, </pages> <address> San Jose, California, </address> <year> 1994. </year>
Reference-contexts: Adding processors similarly allows for higher number of simultaneous requests, increasing the request rate. Figure 5 indicates that both multithreading and multiprocessing can utilize the available bandwidth effectively <ref> [27] </ref>. That is, the request bandwidth is not limited by the switch time. Saturation for both Floating and Fixed occurs quite rapidly as we increase the number of threads and/or processors; this follows from the relatively high protocol processing overheads of our remote miss handlers.
Reference: [28] <author> Andrew Tucker and Anoop Gupta. </author> <title> Process Control and Scheduling Issues for Multiprogrammed Shared-Me mory Multiprocessors. </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Operating System Principles (SOSP), </booktitle> <pages> pages 159166, </pages> <month> December </month> <year> 1989. </year>
Reference: [29] <author> Raj Vaswani and John Zahorjan. </author> <title> The Implications of Cache Affinity on Processor Scheduling for Multipro-grammed, Shared Memory Multiprocessors. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating System Principles (SOSP), </booktitle> <pages> pages 2640, </pages> <month> October </month> <year> 1991. </year>
Reference: [30] <author> Thorsten von Eicken, David E. Culler, Seth Copen Goldstein, and Klaus Erik Schauser. </author> <title> Active Messages: a Mechanism for Integrating Communication and Computation. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 256266, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: While the results of this paper are largely independent of whether this mechanism is implemented in hardware or software, the results in Section 6 come from a software technique [23]. High-performance communication is performed via an active message abstraction <ref> [30] </ref>. Active messages are essentially very light-weight RPCs that are optimized for the case where processing nodes are co-scheduled; that is, where the destination node already has the correct context for the RPC. <p> The alternative is periodic polling, which requires instrumenting the computation thread to periodically check for messages. This can be done either via a compiler <ref> [30] </ref> or by directly editing the executable file [16]. This approach requires a trade-off between latency and overhead: frequent polls decrease message latency but increase overhead. 3.2 Fixed Protocol Processor The Fixed policy dedicates one processor of a multiprocessor node to perform only protocol processing.
Reference: [31] <author> David A. Wood and Mark D. Hill. </author> <title> Cost-Effective Parallel Computing. </title> <journal> IEEE Computer, </journal> <volume> 28(2):6972, </volume> <month> Feb </month> <year> 1995. </year>
Reference-contexts: The advantage of Fixed decreases as the overhead saved by the protocol processor decreases (small 2 O pp + O reply ). Adding a dedicated protocol processor is only cost-effective if the performance gain exceeds the cost increment <ref> [31] </ref>. Thus if a two-processor workstation node costs one third more than a unipro FIGURE 3.
References-found: 30

