URL: http://www.cs.jhu.edu/salzberg/mlj93.ps.gz
Refering-URL: http://www.cs.jhu.edu/salzberg/home.html
Root-URL: 
Email: Internet: cost@cs.jhu.edu, salzberg@cs.jhu.edu  
Title: A Weighted Nearest Neighbor Algorithm for Learning with Symbolic Features  
Author: Scott Cost and Steven Salzberg 
Address: Baltimore, MD 21218  
Affiliation: Department of Computer Science Johns Hopkins University  
Abstract: In the past, nearest neighbor algorithms for learning from examples have worked best in domains in which all features had numeric values. In such domains, the examples can be treated as points and distance metrics can use standard definitions. In symbolic domains, a more sophisticated treatment of the feature space is required. We introduce a nearest neighbor algorithm for learning in domains with symbolic features. Our algorithm calculates distance tables that allow it to produce real-valued distances between instances, and attaches weights to the instances to further modify the structure of feature space. We show that this technique produces excellent classification accuracy on three problems that have been studied by machine learning researchers: predicting protein secondary structure, identifying DNA promoter sequences, and pronouncing English text. Direct experimental comparisons with the other learning algorithms show that our nearest neighbor algorithm is comparable or superior in all three domains. In addition, our algorithm has advantages in training speed, simplicity, and perspicuity. We conclude that experimental evidence favors the use and continued development of nearest neighbor algorithms for domains such as the ones studied here. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. </author> <title> (1989) Incremental, instance-based learning of independent and graded concept descriptions. </title> <booktitle> Proceedings of the Sixth International Workshop on Machine Learning (pp. </booktitle> <pages> 387-391). </pages> <address> Ithaca, NY: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Aha, D. and Kibler, D. </author> <booktitle> (1989) Noise-tolerant instace-based learning algorithms. Proceedings of the Eleventh International Joint Conference on 39 Artificial Intelligence (pp. </booktitle> <pages> 794-799). </pages> <address> Detroit, MI: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Aha, D. </author> <title> (1990) A Study of Instance-Based Algorithms for Supervised Learning Tasks. </title> <type> Ph.D. Thesis, Technical Report 90-42, </type> <institution> Department of Information and Computer Science, University of California, Irvine. </institution>
Reference: <author> Aha, D., Kibler, D., and Albert, M. </author> <title> (1991) Instance-Based Learning Algorithms. </title> <journal> Machine Learning, </journal> <volume> 6:1, </volume> <pages> 37-66. </pages>
Reference: <author> Chou, P. and Fasman, G. </author> <title> (1978) Prediction of the secondary structure of proteins from their amino acid sequence. </title> <booktitle> Advanced Enzymology 47, </booktitle> <pages> 45-148. </pages>
Reference: <author> Cohen, F., Abarbanel, R., Kuntz, I., and Fletterick, R. </author> <title> (1986) Turn Prediction in Proteins Using a Pattern Matching Approach. </title> <journal> Biochemistry, </journal> <volume> 25, </volume> <pages> 266-275. </pages>
Reference: <author> Cost, S. </author> <title> (1990) M.S. </title> <type> Thesis, </type> <institution> Department of Computer Science, Johns Hop-kins University. </institution>
Reference-contexts: Here, only two points are required to define a rule and an exception. The capability becomes even more important for IBL models that store only a subset of the training examples, because it further reduces 17 the number of points which must be stored <ref> (Cost and Salzberg, 1990) </ref>. Given the above discussion, it should be clear that all instances should not be initialized with weights of 1. Consider a system trained on n 1 instances, now training on the n th .
Reference: <author> Cost, S. and Salzberg, S. </author> <title> (1990) Exemplar-based Learning to Predict Protein Folding. </title> <booktitle> Proceedings of the Symposium on Computer Applications to Medical Care, </booktitle> <address> Washington, D.C., </address> <month> November </month> <year> 1990. </year>
Reference-contexts: Here, only two points are required to define a rule and an exception. The capability becomes even more important for IBL models that store only a subset of the training examples, because it further reduces 17 the number of points which must be stored <ref> (Cost and Salzberg, 1990) </ref>. Given the above discussion, it should be clear that all instances should not be initialized with weights of 1. Consider a system trained on n 1 instances, now training on the n th .
Reference: <author> Cover, T. and Hart, P. </author> <title> (1967) Nearest neighbor pattern classification. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 13:1, </volume> <pages> 21-27. </pages>
Reference-contexts: Despite many successes, there are some domains in which the task has proven very difficult, due either to the inherent difficulty of the domain or to the lack of sufficient data for learning. For example, instance-based learning programs (also called exemplar-based (Salzberg, 1990) or nearest neighbor <ref> (Cover and Hart, 1967) </ref> methods), which learn by storing examples as points in a feature space, require some means of measuring distance between examples (Aha, 1989; Aha and Kibler, 1989; Salzberg, 1989; Cost and Salzberg, 1990). An example is usually a vector of feature values plus a category label.
Reference: <author> Crick, F., and Asanuma, C. </author> <title> (1986) Certain aspects of the anatomy and physiology of the cerebral cortex. </title> <booktitle> In Parallel Distributed Processing: 40 explorations in the microstructure of cognition, </booktitle> <volume> vol II, </volume> <editor> J. McClelland, D. Rumelhart, </editor> <booktitle> and the PDP Research Group (Eds.), </booktitle> <pages> 170-215. </pages> <address> Cam-bridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Dietterich, T., Hild, H., and Bakiri, G. </author> <title> (1990) A comparative study of ID3 and backpropagation for English text-to-speech mapping. </title> <booktitle> Proceedings of the 7th International Conference on Machine Learning, </booktitle> <pages> 24-31. </pages>
Reference: <author> Fertig, S. and Gelernter, D. </author> <year> (1991) </year> <month> FGP: </month> <title> A virtual machine for acquiring knowledge from cases. </title> <booktitle> Proceedings of the 12th International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 796-802). </pages> <address> Los Altos, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: When the number of processors is as large as the training set, classification time may be reduced to O (d log n). We implemented our system on a set of four loosely-coupled transputers as well as on a conventional architecture, and other recent efforts such as FGP <ref> (Fertig and Gelernter, 1991) </ref> use larger numbers of parallel processors. The MBRtalk system of Waltz and Stanfill (1986) implemented a form of k-nearest-neighbor learning using a tightly-coupled massively parallel architecture, the 64,000-processor Connection Machine tm . Perspicuity.
Reference: <author> Fisher, D., and McKusick, K. </author> <title> (1989) An empirical comparison of ID3 and back-propagation. </title> <booktitle> Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <pages> 788-793. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Garnier, J., Osguthorpe, D., and Robson, B. </author> <title> (1978) Analysis of the accuracy and implication of simple methods for predicting the secondary structure of globular proteins. </title> <journal> Journal of Molecular Biology, </journal> <volume> 120, </volume> <pages> 97-120. </pages>
Reference-contexts: Attempts to predict secondary structure involve the classification of residues into three categories: ff helix, fi sheet, and coil. Three of the most widely used approaches to this problem are those of Robson <ref> (Garnier et al., 1978) </ref>, Chou and Fasman (1978), and Lim (1974), which produce classification accuracies ranging from 48% to 58%.
Reference: <author> Hanson, S. and Burr, D. </author> <title> (1990) What connectionist models learn: Learning and representation in connectionist networks. </title> <journal> Behavioral and Brain Sciences, </journal> <volume> 13, </volume> <pages> 471-518. </pages>
Reference-contexts: Neural nets do not yet provide any insight into why they made the classification they did, although some recent efforts have explored new methods for understanding the content of a trained network <ref> (Hanson and Burr, 1990) </ref>. It is also relatively easy to modify our algorithm to include domain specific knowledge: if the relative importance of the features is known, the features may be weighted accordingly in the distance formula (Salzberg, 1989).
Reference: <author> Holley, L. and Karplus, M. </author> <title> (1989) Protein Secondary Structure Prediction 41 with a Neural Network. </title> <booktitle> Proceedings of the National Academy of Sci--ences USA, </booktitle> <volume> 86, </volume> <pages> 152-156. </pages>
Reference: <author> Kabsch, W. and Sander, C. </author> <title> (1983) Dictionary of protein secondary structure: Pattern recognition of hydrogen-bonded and geometric features. </title> <journal> Biopolymers, </journal> <volume> 22 (1983), </volume> <pages> 2577-2637. </pages>
Reference: <author> Kontogiorgis, S. </author> <title> (1988) Automatic Letter-to-Phoneme Transcription for Speech Synthesis. </title> <type> Technical Report JHU-88/22, </type> <institution> Department of Computer Science, Johns Hopkins University. </institution>
Reference: <author> Lathrop, R., Webster, T. and Smith, T. </author> <year> (1987) </year> <month> ARIADNE: </month> <title> Pattern-directed Inference and Hierarchical Abstraction in Protein Structure Recognition. </title> <journal> Communications of the ACM, </journal> <volume> 30:11, </volume> <pages> 909-921. </pages>
Reference: <author> Lim, V. </author> <title> (1974) Algorithms for prediction of ff-helical and beta-structural regions in globular proteins. </title> <journal> Journal of Molecular Biology, </journal> <volume> 88, </volume> <pages> 873-894. </pages>
Reference: <author> Mathews, B. W. </author> <title> (1975) Comparison of the predicted and observed secondary structure of T4 phage lysozyme. </title> <journal> Biochim. Biophys. Acta., </journal> <volume> 405, </volume> <pages> 442-451. </pages>
Reference-contexts: Thus the exemplar weights did improve performance significantly. Another frequently used measure of performance in this domain are the correlation coefficients, which provide a measure of accuracy for each of the categories. They are defined by the following equation, from <ref> (Mathews, 1975) </ref>: p ff fi n ff u ff fi o ff (n ff + u ff ) (n ff + o ff ) (p ff + u ff ) (p ff + o ff ) where p ff is the number times ff was correctly predicted, n ff is the
Reference: <author> McClelland, J. and Rumelhart, D. </author> <title> (1986) A distributed model of human learning and memory. </title> <booktitle> In Parallel Distributed Processing: explorations in the microstructure of cognition, </booktitle> <volume> vol II, </volume> <editor> J. McClelland, D. Rumelhart, </editor> <booktitle> and the PDP Research Group (Eds.), </booktitle> <pages> 170-215. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. 42 Mooney, </publisher> <editor> R., Shavlik, J., Towell, G., and Gove, A. </editor> <title> (1989) An experimental comparison of symbolic and connectionist learning algorithms. </title> <booktitle> Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <pages> 775-780. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Medin, D. and Schaffer, M. </author> <title> (1978) Context theory of classification learning. </title> <journal> Psychological Review, </journal> <volume> 85:3, </volume> <pages> 207-238. </pages>
Reference: <author> Nosofsky, R. </author> <title> (1984) Choice, Similarity, and the Context Theory of Classification. Journal of Experimental Psychology: Learning, Memory, </title> <journal> and Cognition 10:1, </journal> <pages> 104-114. </pages>
Reference: <author> O'Neill, M. </author> <title> (1989) Escherichia coli promoters: I. Consensus as it relates to spacing class, specificity, repeat substructure, and three dimensional organization. </title> <journal> Journal of Biological Chemistry, </journal> <volume> 264, </volume> <pages> 5522-5530. </pages>
Reference: <author> Preparata, F. and Shamos, M. </author> <title> (1985) Computational Geometry: An Introduction. </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Qian, N. and Sejnowski, T. </author> <title> (1988) Predicting the secondary structure of globular proteins using neural network models. </title> <journal> Journal of Molecular Biology, </journal> <volume> 202, </volume> <pages> 865-884. </pages>
Reference: <author> Reed, S. </author> <title> (1972) Pattern Recognition and Categorization. </title> <journal> Cognitive Psychology, </journal> <volume> 3, </volume> <pages> 382-407. </pages>
Reference: <author> Rumelhart, D., Hinton, G., and Williams, R. </author> <title> (1986) Learning representations by back-propagating errors. </title> <booktitle> Nature 323:9, </booktitle> <month> October </month> <year> 1986, </year> <pages> 533-536. </pages> <note> 43 Rumelhart, </note> <author> D., Smolensky, P., McClelland, J., and Hinton, G. </author> <title> (1986) Schemata and sequential thought processes in PDP models. </title> <booktitle> In Parallel Distributed Processing: explorations in the microstructure of cognition, </booktitle> <volume> vol II, </volume> <editor> J. McClelland, D. Rumelhart, </editor> <booktitle> and the PDP Research Group (Eds.), </booktitle> <pages> 7-57. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <editor> Rumelhart, D., McClelland, J., </editor> <booktitle> and the PDP Research Group (1986) Parallel Distributed Processing: explorations in the microstructure of cognition, vol I. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Salzberg, S. </author> <title> (1989) Nested Hyper-rectangles for Exemplar-based Learning. </title>
Reference-contexts: It is also relatively easy to modify our algorithm to include domain specific knowledge: if the relative importance of the features is known, the features may be weighted accordingly in the distance formula <ref> (Salzberg, 1989) </ref>. Taken together, the advantages listed above make it clear that IBL algorithms have a number of benefits with respect to competing models. However, in order to be considered a realistic practical learning technique, IBL must still demonstrate good classification accuracy. <p> We call our method MVDM, for Modified Value Difference Metric. Our second component is a standard distance metric for measuring the distance between two examples in a multi-dimensional feature space. Finally, the distance is modified by a weighting scheme that weights instances in memory according to their performance history <ref> (Salzberg 1989, 1990) </ref>. These components of the distance calculation are described in sections 2.2 2 The parallelization of the algorithm was developed to speed up experimentation, and is of no theoretical importance to our learning model. 9 and 2.3. PEBLS requires two passes through the training set. <p> We accomplish this with the weight w X in our distance formula: reliable exemplars are given smaller weights, making them appear closer to a new example. Our weighting scheme was first adopted in the Each system <ref> (Salzberg 1989, 1990) </ref>, which assigned weights to exemplars according to their performance history. w X is the ratio of the number of uses of an exemplar to the number of correct uses of the exemplar; thus, accurate exemplars 14 will have w X 1.
Reference: <editor> In K.P. Jantke (ed.), </editor> <title> Analogical and Inductive Inference: </title> <booktitle> International Workshop AII '89, </booktitle> <pages> 184-201. </pages> <address> Berlin: </address> <publisher> Springer- Verlag. </publisher>
Reference: <author> Salzberg, S. </author> <title> (1990) Learning with Nested Generalized Exemplars. </title> <publisher> Norwell, </publisher> <address> MA: </address> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: Despite many successes, there are some domains in which the task has proven very difficult, due either to the inherent difficulty of the domain or to the lack of sufficient data for learning. For example, instance-based learning programs (also called exemplar-based <ref> (Salzberg, 1990) </ref> or nearest neighbor (Cover and Hart, 1967) methods), which learn by storing examples as points in a feature space, require some means of measuring distance between examples (Aha, 1989; Aha and Kibler, 1989; Salzberg, 1989; Cost and Salzberg, 1990). <p> Here, only two points are required to define a rule and an exception. The capability becomes even more important for IBL models that store only a subset of the training examples, because it further reduces 17 the number of points which must be stored <ref> (Cost and Salzberg, 1990) </ref>. Given the above discussion, it should be clear that all instances should not be initialized with weights of 1. Consider a system trained on n 1 instances, now training on the n th . <p> Finally, our experiments in the protein domain demonstrated that the use of weights attached to exemplars can improve the accuracy of nearest neighbor algorithms. In other domains, such as English pronunciation, weights did not make a significant difference. Based on these results, and our earlier results on real-valued domains <ref> (Salzberg, 1990, 1991) </ref>, we conclude that exemplar weights offer real potential for enhancing the power of practical learning algorithms. 6 Conclusion We have demonstrated, through a series of experiments, that an instance-based learning algorithm can perform exceptionally well on domains in which features values are symbolic.
Reference: <author> Salzberg, S. </author> <title> (1991) A nearest hyperrectangle learning method. </title> <journal> Machine Learning, </journal> <volume> 6:3, </volume> <pages> 251-276. </pages>
Reference-contexts: Our algorithm is implemented in a program called PEBLS, which stands for Parallel Exemplar-Based Learning System. 2 For clarity, we use the term "example" to mean a training or test example being shown to the system for the first time. We use the term "exemplar" (following the usage of <ref> (Salzberg, 1991) </ref>) to refer specifically to an instance that has been previously stored in computer memory. Such exemplars may have additional information attached to them (e.g., weights). The term "instance" covers both examples and exemplars. PEBLS was designed to process instances that have symbolic feature values.
Reference: <author> Sejnowski, T. and Rosenberg, C. </author> <year> (1987) </year> <month> NETtalk: </month> <title> A Parallel Network that Learns to Read Aloud. </title> <journal> Complex Systems, </journal> <volume> 1, </volume> <pages> 145-168. </pages> <note> Also Technical Report JHU/EECS-86/01, </note> <institution> Johns Hopkins University, Baltimore, MD, </institution> <year> 1986. </year>
Reference: <author> Shavlik, J., Mooney, R., and Towell, G. </author> <title> (1989) Symbolic and neural learning algorithms: an experimental comparison. </title> <type> Technical Report #857, </type> <institution> 44 Computer Sciences Department, University of Wisconsin, Madison, WI, </institution> <month> June </month> <year> 1989. </year>
Reference-contexts: This set consists of all instances drawn from the Brown Corpus, or the 1000 most commonly used words of the English language. We were unable to discern a difference between that training set and the somewhat more restricted set of Shav-lik <ref> (Shavlik et al., 1989) </ref>, so only one experimental design was used. After training on the Brown Corpus, PEBLS was tested on the entire 20,012 word Merriam Webster Pocket Dictionary. Results are presented in Table 9 for weighted and unweighted versions of the PEBLS algorithm. <p> On the other hand, the derivation of these distances is not perspicuous, being derived from global characteristics of the training data. For the English pronunciation task, distributed output encodings have been shown to produce superior performance to local encodings <ref> (Shavlik et al. 1989) </ref>. This result points out a weakness of PEBLS, and of the 1-nearest-neighbor method, in that they do not allow for distributed output encodings.
Reference: <author> Sigillito, V. </author> <type> (1989) Personal communication. </type>
Reference: <author> Stanfill, C. and Waltz, D. </author> <title> (1986) Toward memory-based reasoning. </title> <journal> Communications of the ACM, </journal> <volume> 29:12, </volume> <pages> 1213-1228. </pages>
Reference-contexts: In this paper, we present a more sophisticated instance-based algorithm designed for domains in which some or all of the feature values are sym 2 bolic. Our algorithm constructs modified "value difference" tables (in the style of <ref> (Stanfill and Waltz, 1986) </ref>) to produce a non-Euclidean distance metric, and we introduce the idea of "exception spaces" that result when weights are attached to individual examples. The combination of these two techniques results is a robust instance-based learning algorithm that works for any domain with symbolic feature values. <p> They applied their technique to the English pronunciation problem with impressive initial results <ref> (Stanfill and Waltz, 1986) </ref>. Their Value Difference Metric (VDM) takes into account the overall similarity of classification of all instances for each possible value of each feature.
Reference: <author> Towell, G., Shavlik, J., and Noordewier, M. </author> <title> (1990) Refinement of approximate domain theories by knowledge-based neural networks. </title> <booktitle> Proceedings Eighth National Conference on Artificial Intelligence, </booktitle> <pages> 861-866. </pages> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Towell notes that the negative examples in his database (the same data as used here) were derived by selecting substrings from a fragment of E. coli bacteriophage that is "believed not to contain any promoter sites" <ref> (Towell et al., 1990, p. 865) </ref>. We would suggest, based on our results, that four of the examples be re-examined.
Reference: <author> Waltz, D. </author> <booktitle> (1990) Massively parallel AI. Proceedings Eighth National Conference on Artificial Intelligence, </booktitle> <pages> 1117-1122. </pages> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Recently, Zhang and Waltz have investigated a hybrid learning method for protein structure prediction, combining nearest neighbor with neural net learning and statistical information. Figures have not yet been published, but their method also outperforms previous methods <ref> (Waltz, 1990) </ref>, although its accuracy does not exceed that of PEBLS. For the DNA promoter sequence prediction, Towell et al. (1990) report that KBANN, a technique that integrates neural nets and domain knowledge, is superior to standard back propagation with 99.95% certainty (t = 5:29, d.f.= 18).
Reference: <author> Weiss, S. and Kapouleas, I. </author> <title> (1989) An empirical comparison of pattern recognition, neural nets, and machine learning classification methods. </title> <booktitle> Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <pages> 781-787. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher> <pages> 45 </pages>
References-found: 41

