URL: http://www.isi.edu/~koehn/publications/encoding96.ps.gz
Refering-URL: http://www.isi.edu/~koehn/publications/index.html
Root-URL: http://www.isi.edu
Email: kohn@cs.utk.edu  
Title: Genetic Encoding Strategies for Neural Networks  
Author: Philipp Kohn 
Address: Dreibergstr. 5, 91056 Erlangen, Germany  
Affiliation: University of Tennessee Universitat Erlangen-Nurnberg  
Abstract: The application of genetic algorithms to neural network optimization (GANN) has produced an active field of research. This paper proposes a classification of the encoding strategies and it also gives a critical analysis of the current state of development. The idea of evolving artificial neural networks (NN) by genetic algorithms (GA) is based on a powerful metaphor: the evolution of the human brain. This mechanism has developed the highest form of intelligence known from scratch. The metaphor has inspired a great deal of research activities that can be traced to the late 1980s (for instance [15]). An increasing amount of research reports, journal papers and theses have been published on the topic, generating a conti-nously growing field. Researchers have devoloped a variety of different techniques to encode neural networks for the GA, with increasing complexity. This young field is driven mostly by small, independet research groups that scarcely cooperate with each other. This paper will attempt to analyse and to structure the already performed work, and to point out the shortcomings of the approaches. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Egber Boers and Herman Kuiper. </author> <title> Biological metaphors and the design of modular artificial neural networks. </title> <type> Master's thesis, </type> <institution> Leiden University, </institution> <address> the Netherlands, </address> <year> 1992. </year>
Reference-contexts: In this instance, the network is viewed as a set of paths from an input to an output node. Again, special GA operators are used. Indirect encoding This completely seperate strategy can be traced back to 1990 [12], with more recent approaches <ref> [1, 4] </ref> receiving great acclaim in the community. The basic idea is to encode a grammar-based construction program in the genome, instead of directly encoding properties of the NN. Boers and Kuiper used a grammar based on Lindenmayer systems, Gruau applied cellular encoding. <p> Most papers report good results on small-scale toy problems such as xor, sine, parity, or low-bit adding. The convergence time competes well with backpropagation [28, 13]. The largest problems that have been tackled successfully range from high-bit parity [4], pole-balancing [28, 10], and simplified pattern recognition <ref> [1] </ref> to high-order classification tasks [26]. If convergence times are reported at all, they are usually quite high: 3 days on 11 workstations [1] or one week [26, in conversation], [10, in conversation] for the mentioned tasks. <p> The largest problems that have been tackled successfully range from high-bit parity [4], pole-balancing [28, 10], and simplified pattern recognition <ref> [1] </ref> to high-order classification tasks [26]. If convergence times are reported at all, they are usually quite high: 3 days on 11 workstations [1] or one week [26, in conversation], [10, in conversation] for the mentioned tasks. These are tasks that can be solved faster by simple back-propagation on arbitrary NN chosen by rules of thumb.
Reference: [2] <author> Hugo de Garis. </author> <booktitle> Genetic programming. In International Joint Conference on Neural Networks, </booktitle> <pages> pages III 511-516. </pages> <publisher> IEEE, </publisher> <year> 1990. </year>
Reference-contexts: Another idea is to partially develop NNs for subtasks and compose them to a more complex structure. This was investigated for different parts of the nervous system of an artificial being <ref> [2, 3] </ref>.
Reference: [3] <author> Hugo de Garis. </author> <title> Circuits of production rule: </title> <booktitle> Gen-nets | the genetic programming of artificial nervous systems. In International Joint Conference on Neural Networks and Genetic Algorithms, </booktitle> <pages> pages 699-705, </pages> <address> Innsbruck, </address> <year> 1993. </year>
Reference-contexts: Another idea is to partially develop NNs for subtasks and compose them to a more complex structure. This was investigated for different parts of the nervous system of an artificial being <ref> [2, 3] </ref>.
Reference: [4] <author> Frederic Gruau. </author> <title> Neural Network Synthesis using Cellular Encoding and the Genetic Algorithm. </title> <type> PhD thesis, </type> <institution> Ecole Normale Su-perieure de Lyon, </institution> <year> 1994. </year> <note> ftp://lip.ens-lyon.fr /pub/Rapports/PhD/PhD94-01-E.ps.Z. </note>
Reference-contexts: In this instance, the network is viewed as a set of paths from an input to an output node. Again, special GA operators are used. Indirect encoding This completely seperate strategy can be traced back to 1990 [12], with more recent approaches <ref> [1, 4] </ref> receiving great acclaim in the community. The basic idea is to encode a grammar-based construction program in the genome, instead of directly encoding properties of the NN. Boers and Kuiper used a grammar based on Lindenmayer systems, Gruau applied cellular encoding. <p> Most papers report good results on small-scale toy problems such as xor, sine, parity, or low-bit adding. The convergence time competes well with backpropagation [28, 13]. The largest problems that have been tackled successfully range from high-bit parity <ref> [4] </ref>, pole-balancing [28, 10], and simplified pattern recognition [1] to high-order classification tasks [26]. If convergence times are reported at all, they are usually quite high: 3 days on 11 workstations [1] or one week [26, in conversation], [10, in conversation] for the mentioned tasks. <p> Lack of theoretical insight In general, the field lacks theoretical insight. It is rare that general criteria for encoding strategies are proposed, such as by Gruau <ref> [4] </ref>: completeness (every NN can be encoded) and closure (only meaningful NN are encoded) among others. These are fulfilled by almost all strategies. Also, there have been only a few properties of GANN systems to be identified and discussed.
Reference: [5] <author> Frederic Gruau and Darell Whitley. </author> <title> Adding learning to the celular development of neural networks: evolution and the Baldwin effect. </title> <journal> Evolutionary Computation, </journal> <volume> 3(1) </volume> <pages> 213-233, </pages> <year> 1993. </year>
Reference-contexts: These are fulfilled by almost all strategies. Also, there have been only a few properties of GANN systems to be identified and discussed. The argument over the Baldwin effect <ref> [5] </ref> or the permutation problem (see below) has just started. Permutation problem Almost always, the same (or similar) networks may be encoded in quite different ways. The crossover of two different encodings of similar networks can result in a completely divergent NN.
Reference: [6] <author> Peter J.B. Hancock. </author> <title> Genetic algorithms and permutation problem: a comparison of recombination operators for neural net structure specification. </title> <booktitle> In International Workshop on Combinations of Genetic Algorithms and Neural Networks, </booktitle> <pages> pages 108-122, </pages> <address> Baltimore, 1992. </address> <publisher> IEEE. </publisher>
Reference-contexts: Permutation problem Almost always, the same (or similar) networks may be encoded in quite different ways. The crossover of two different encodings of similar networks can result in a completely divergent NN. This broadly acknowledged problem is also known as structural-functional mapping problem [27] or competing conventions problem <ref> [6] </ref>. There have been a few proposals to solve it [25, 11, 6]. However, these studies also indicate that the problem has less impact than expected. <p> The crossover of two different encodings of similar networks can result in a completely divergent NN. This broadly acknowledged problem is also known as structural-functional mapping problem [27] or competing conventions problem [6]. There have been a few proposals to solve it <ref> [25, 11, 6] </ref>. However, these studies also indicate that the problem has less impact than expected.
Reference: [7] <author> Bart L.M. Happel and Jacob M.J. Murre. </author> <title> Design and evolution of modular neural network architectures. </title> <booktitle> Neural Networks, </booktitle> 7(6/7):985-1004, 1994. 
Reference-contexts: The empirical results, however, do not clearly indicate that more complex systems show a better performance. The fundamental problem of the poor performance of GANN systems on real-world scale tasks has been recognized [28]. One magic word for the solution is modularity <ref> [7] </ref>, but unfortunately encoding strategies with high modularity like indirect encoding, have not been a breakthrough. Another idea is to partially develop NNs for subtasks and compose them to a more complex structure. This was investigated for different parts of the nervous system of an artificial being [2, 3].
Reference: [8] <author> Steven Alex Harp and Tariq Smad. </author> <title> Towards the genetic synthesis of neural networks. </title> <booktitle> In Third International Conference on Genetic Algorithms, </booktitle> <pages> pages 360-369. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1989. </year>
Reference-contexts: The code for each node may include relative position, backward connectivity [24], weight values, threshold function [26] and more. Crossover and mutation is mostly re stricted to cuts between node information. Layer-based encoding With layer-based encoding one can obtain larger networks <ref> [8, 9, 16] </ref>. The encoding scheme is a very complicated system of descriptions of connectivity between a list of layers. Therefore, special GA operators are required. Pathway-based encoding Pathway-based encoding is proposed in [10] for recurrent NN.
Reference: [9] <author> Steven Alex Harp and Tariq Smad. </author> <title> Genetic synthesis of neural network architecture. </title> <booktitle> In Handbook of Genetic Algorithms, </booktitle> <pages> pages 202-221, </pages> <year> 1991. </year>
Reference-contexts: The code for each node may include relative position, backward connectivity [24], weight values, threshold function [26] and more. Crossover and mutation is mostly re stricted to cuts between node information. Layer-based encoding With layer-based encoding one can obtain larger networks <ref> [8, 9, 16] </ref>. The encoding scheme is a very complicated system of descriptions of connectivity between a list of layers. Therefore, special GA operators are required. Pathway-based encoding Pathway-based encoding is proposed in [10] for recurrent NN.
Reference: [10] <author> Christian Jacob and Jan Rehder. </author> <title> Evolution of neural network architectures by a hierarchical grammar-based genetic system. </title> <booktitle> In International Joint Conference on Neural Networks and Genetic Algorithms, </booktitle> <pages> pages 72-79, </pages> <address> Innsbruck, </address> <year> 1993. </year>
Reference-contexts: Or, the evolution of the NN can take place in two stages: One that finds the optimal architecture, the other that finds a weight setting for each architecture <ref> [10] </ref>. 2 Weight encoding A few different complexities of weight encoding have been investigated in this field. The simplest case is one-bit connectivity information [27]. Or, two bits are used for encoding if a connection is disconnected, inhibitory or excitatory [19]. <p> Layer-based encoding With layer-based encoding one can obtain larger networks [8, 9, 16]. The encoding scheme is a very complicated system of descriptions of connectivity between a list of layers. Therefore, special GA operators are required. Pathway-based encoding Pathway-based encoding is proposed in <ref> [10] </ref> for recurrent NN. In this instance, the network is viewed as a set of paths from an input to an output node. Again, special GA operators are used. <p> With increasing complexity and inhomogenity of the genoms, the classical GA operators mutation and crossover are replaced by parameter-oriented operators such as "randomly add neuron" [23], or "drop path" <ref> [10] </ref>. 4 Tasks for GANN systems Thanks to mutation operators, every optimal NN will be ultimately found by any GANN system. The quality of an encoding strategy, however, has to be measured by the speed of convergence to sufficiently good solutions. <p> Most papers report good results on small-scale toy problems such as xor, sine, parity, or low-bit adding. The convergence time competes well with backpropagation [28, 13]. The largest problems that have been tackled successfully range from high-bit parity [4], pole-balancing <ref> [28, 10] </ref>, and simplified pattern recognition [1] to high-order classification tasks [26]. If convergence times are reported at all, they are usually quite high: 3 days on 11 workstations [1] or one week [26, in conversation], [10, in conversation] for the mentioned tasks. <p> If convergence times are reported at all, they are usually quite high: 3 days on 11 workstations [1] or one week [26, in conversation], <ref> [10, in conversation] </ref> for the mentioned tasks. These are tasks that can be solved faster by simple back-propagation on arbitrary NN chosen by rules of thumb.
Reference: [11] <author> Nachimuthu Karunanithi, Rajarshi Das, and Darell Whitley. </author> <title> Genetic cascade learning for neural networks. </title> <booktitle> In International Workshop on Combinations of Genetic Algorithms and Neural Networks, </booktitle> <pages> pages 134-145, </pages> <address> Baltimore, 1992. </address> <publisher> IEEE. </publisher>
Reference-contexts: The crossover of two different encodings of similar networks can result in a completely divergent NN. This broadly acknowledged problem is also known as structural-functional mapping problem [27] or competing conventions problem [6]. There have been a few proposals to solve it <ref> [25, 11, 6] </ref>. However, these studies also indicate that the problem has less impact than expected.
Reference: [12] <author> Hiroaki Kitano. </author> <title> Designing neural networks using genetic algorithms with graph generation systems. </title> <journal> Complex Systems, </journal> <volume> 4 </volume> <pages> 461-476, </pages> <year> 1990. </year>
Reference-contexts: Pathway-based encoding Pathway-based encoding is proposed in [10] for recurrent NN. In this instance, the network is viewed as a set of paths from an input to an output node. Again, special GA operators are used. Indirect encoding This completely seperate strategy can be traced back to 1990 <ref> [12] </ref>, with more recent approaches [1, 4] receiving great acclaim in the community. The basic idea is to encode a grammar-based construction program in the genome, instead of directly encoding properties of the NN. Boers and Kuiper used a grammar based on Lindenmayer systems, Gruau applied cellular encoding.
Reference: [13] <author> Philipp Koehn. </author> <title> Combining genetic algorithms and neural networks: The encoding problem. </title> <type> Master's thesis, </type> <institution> University of Tennessee, Knoxville, </institution> <year> 1994. </year> <note> ftp://archive.cis.ohio-state.edu /pub/neuroprose/koehn.encoding.ps.Z. </note>
Reference-contexts: The simplest case is one-bit connectivity information [27]. Or, two bits are used for encoding if a connection is disconnected, inhibitory or excitatory [19]. Weights have been encoded as real numbers [21, 22], bit strings [20, 27], or bit strings using grey coding <ref> [13] </ref>. Maniezzo proposes that the number of encoding bits should increase during the evolution [17], allowing fine tuning at a later stage. A study suggests that a high number of encoding bits per weight improves GA weight training, whereas grey coding has no impact [13]. 3 Architecture encoding The task to <p> or bit strings using grey coding <ref> [13] </ref>. Maniezzo proposes that the number of encoding bits should increase during the evolution [17], allowing fine tuning at a later stage. A study suggests that a high number of encoding bits per weight improves GA weight training, whereas grey coding has no impact [13]. 3 Architecture encoding The task to encode the architecture of a network is far more complex than weight encoding. The classical view that the genome is a bit string of fixed length does not easily fit to the complex interconnected structure of a NN. <p> The classes are roughly ordered to the complexity of the strategies. Connection-based encoding The vast majority of early approaches is connection-based <ref> [13, 17, 19, 21, 27] </ref>. Here, the genome is a string of weight values or pure connectivity information. This requires a fixed maximal architecture, which is typ ically either fully-connected or layered. <p> The quality of an encoding strategy, however, has to be measured by the speed of convergence to sufficiently good solutions. Most papers report good results on small-scale toy problems such as xor, sine, parity, or low-bit adding. The convergence time competes well with backpropagation <ref> [28, 13] </ref>. The largest problems that have been tackled successfully range from high-bit parity [4], pole-balancing [28, 10], and simplified pattern recognition [1] to high-order classification tasks [26]. <p> Although many researchers describe the benefit of the application of GA to NN as the avoidance of rules of thumb in NN design, rules of thumbs and time-intensive experimental optimizations have to be used for these parameters <ref> [13] </ref>. Still, it is not at all clear, if these parameters are robust or if they depend on the specific problems.
Reference: [14] <author> John R. Koza and James P. Rice. </author> <title> Genetic generation of both the weight and architecture for a neural network. </title> <booktitle> In International Joint Conference on Neural Networks, </booktitle> <pages> pages II 397-404. </pages> <publisher> IEEE, </publisher> <year> 1991. </year>
Reference-contexts: This requires a fixed maximal architecture, which is typ ically either fully-connected or layered. Node-based encoding An advantage over connection-based encoding is that more flexibility can be obtained by using nodes as basic units. In this approach, the genome is a string or tree <ref> [14] </ref> of node information. The code for each node may include relative position, backward connectivity [24], weight values, threshold function [26] and more. Crossover and mutation is mostly re stricted to cuts between node information. Layer-based encoding With layer-based encoding one can obtain larger networks [8, 9, 16].
Reference: [15] <author> Steve Lehar and John Weaver. </author> <title> A developmental approach to neural network design. </title> <booktitle> In International Conference on Neural Networks, </booktitle> <pages> pages II 97-104. </pages> <publisher> IEEE, </publisher> <year> 1994. </year>
Reference: [16] <author> Martin Mandischer. </author> <title> Representation and evolution of neural networks. </title> <booktitle> In International Joint Conference on Neural Networks and Genetic Algorithms, </booktitle> <pages> pages 643-649. </pages> <address> Innsbruck, </address> <year> 1993. </year>
Reference-contexts: The code for each node may include relative position, backward connectivity [24], weight values, threshold function [26] and more. Crossover and mutation is mostly re stricted to cuts between node information. Layer-based encoding With layer-based encoding one can obtain larger networks <ref> [8, 9, 16] </ref>. The encoding scheme is a very complicated system of descriptions of connectivity between a list of layers. Therefore, special GA operators are required. Pathway-based encoding Pathway-based encoding is proposed in [10] for recurrent NN. <p> Still, it is not at all clear, if these parameters are robust or if they depend on the specific problems. In a few of the GANN systems, some of these parameters are encoded in the genome, for instance: the learning rate in <ref> [16] </ref> or number of encoding bits per weight in [18]. This encoding solves part of the problem at the cost of an increase of the search space and thus the convergence time. Comparison of encoding techniques The inexistence of a theoretical account makes the comparison of different GANN systems difficult.
Reference: [17] <author> Vittorio Maniezzo. </author> <title> Searching among search spaces: hastening the genetic evolution of feed-forward neural networks. </title> <booktitle> In International Joint Conference on Neural Networks and Genetic Algorithms, </booktitle> <pages> pages 635-642, </pages> <year> 1993. </year>
Reference-contexts: Or, two bits are used for encoding if a connection is disconnected, inhibitory or excitatory [19]. Weights have been encoded as real numbers [21, 22], bit strings [20, 27], or bit strings using grey coding [13]. Maniezzo proposes that the number of encoding bits should increase during the evolution <ref> [17] </ref>, allowing fine tuning at a later stage. A study suggests that a high number of encoding bits per weight improves GA weight training, whereas grey coding has no impact [13]. 3 Architecture encoding The task to encode the architecture of a network is far more complex than weight encoding. <p> The classes are roughly ordered to the complexity of the strategies. Connection-based encoding The vast majority of early approaches is connection-based <ref> [13, 17, 19, 21, 27] </ref>. Here, the genome is a string of weight values or pure connectivity information. This requires a fixed maximal architecture, which is typ ically either fully-connected or layered.
Reference: [18] <author> Vittorio Maniezzo. </author> <title> Genetic evolution of the topology and weight distribution of neural networks. </title> <journal> IEEE Transactions of Neural Networks, </journal> <volume> 5(1) </volume> <pages> 39-53, </pages> <year> 1994. </year>
Reference-contexts: In a few of the GANN systems, some of these parameters are encoded in the genome, for instance: the learning rate in [16] or number of encoding bits per weight in <ref> [18] </ref>. This encoding solves part of the problem at the cost of an increase of the search space and thus the convergence time. Comparison of encoding techniques The inexistence of a theoretical account makes the comparison of different GANN systems difficult.
Reference: [19] <author> Leonardo Marti. </author> <title> Genetically generated neural networks II: searching for the optimal representation. </title> <booktitle> In IEEE Joint Conference on Neural Networks, </booktitle> <pages> pages II 221-226, </pages> <year> 1992. </year>
Reference-contexts: The simplest case is one-bit connectivity information [27]. Or, two bits are used for encoding if a connection is disconnected, inhibitory or excitatory <ref> [19] </ref>. Weights have been encoded as real numbers [21, 22], bit strings [20, 27], or bit strings using grey coding [13]. Maniezzo proposes that the number of encoding bits should increase during the evolution [17], allowing fine tuning at a later stage. <p> The classes are roughly ordered to the complexity of the strategies. Connection-based encoding The vast majority of early approaches is connection-based <ref> [13, 17, 19, 21, 27] </ref>. Here, the genome is a string of weight values or pure connectivity information. This requires a fixed maximal architecture, which is typ ically either fully-connected or layered.
Reference: [20] <author> Geoffrey F. Miller, Peter M. Todd, and Shailesh U. Hedge. </author> <title> Designing neural networks using genetic algorithms. </title> <booktitle> In Third International Conference on Genetic Algorithms, </booktitle> <pages> pages 379-384. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1989. </year>
Reference-contexts: The simplest case is one-bit connectivity information [27]. Or, two bits are used for encoding if a connection is disconnected, inhibitory or excitatory [19]. Weights have been encoded as real numbers [21, 22], bit strings <ref> [20, 27] </ref>, or bit strings using grey coding [13]. Maniezzo proposes that the number of encoding bits should increase during the evolution [17], allowing fine tuning at a later stage.
Reference: [21] <author> D. Montana and L. Davis. </author> <title> Training feedforward neural networks using genetic algorithms. </title> <booktitle> In 11th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 762-767. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1989. </year>
Reference-contexts: The simplest case is one-bit connectivity information [27]. Or, two bits are used for encoding if a connection is disconnected, inhibitory or excitatory [19]. Weights have been encoded as real numbers <ref> [21, 22] </ref>, bit strings [20, 27], or bit strings using grey coding [13]. Maniezzo proposes that the number of encoding bits should increase during the evolution [17], allowing fine tuning at a later stage. <p> The classes are roughly ordered to the complexity of the strategies. Connection-based encoding The vast majority of early approaches is connection-based <ref> [13, 17, 19, 21, 27] </ref>. Here, the genome is a string of weight values or pure connectivity information. This requires a fixed maximal architecture, which is typ ically either fully-connected or layered.
Reference: [22] <author> David J. Montana. </author> <title> Automated parameter tuning for interpretation of synthetic images. </title> <booktitle> In Handbook for Genetic Algorithms, </booktitle> <pages> pages 282-311, </pages> <year> 1991. </year>
Reference-contexts: The simplest case is one-bit connectivity information [27]. Or, two bits are used for encoding if a connection is disconnected, inhibitory or excitatory [19]. Weights have been encoded as real numbers <ref> [21, 22] </ref>, bit strings [20, 27], or bit strings using grey coding [13]. Maniezzo proposes that the number of encoding bits should increase during the evolution [17], allowing fine tuning at a later stage.
Reference: [23] <author> Wolfram Schiffmann, Merten Joost, and Ran-dolf Werner. </author> <title> Synthesis and performance analysis of neural network architectures. </title> <type> Technical Report 16/1992, </type> <institution> Universitat Koblenz, Ger-many, </institution> <year> 1992. </year> <note> ftp://archive.cis.ohio-state.edu /pub/neuroprose/schiff.nnga.ps.Z. </note>
Reference-contexts: We can distinguish two seperate issues for the genetic algorithm: on the one hand architecture optimization, and on the other hand weight training. A few GANN systems focus solely on weight training [27], wheras others focus on architecture <ref> [23] </ref>. There are also many ways to combine the two issues: The GA may set initial weight information while weight training is left to established learning rules for NN, such as backpropagation [26]. <p> With increasing complexity and inhomogenity of the genoms, the classical GA operators mutation and crossover are replaced by parameter-oriented operators such as "randomly add neuron" <ref> [23] </ref>, or "drop path" [10]. 4 Tasks for GANN systems Thanks to mutation operators, every optimal NN will be ultimately found by any GANN system. The quality of an encoding strategy, however, has to be measured by the speed of convergence to sufficiently good solutions.
Reference: [24] <author> Wolfram Schiffmann, Merten Joost, and Randolf Werner. </author> <title> Application of genetic algorithms to the construction of topologies for multilayer perceptrons. </title> <booktitle> In International Joint Conference on Neural Networks and Genetic Algorithms, </booktitle> <pages> pages 675-682, </pages> <address> Innsbruck, </address> <year> 1993. </year>
Reference-contexts: Node-based encoding An advantage over connection-based encoding is that more flexibility can be obtained by using nodes as basic units. In this approach, the genome is a string or tree [14] of node information. The code for each node may include relative position, backward connectivity <ref> [24] </ref>, weight values, threshold function [26] and more. Crossover and mutation is mostly re stricted to cuts between node information. Layer-based encoding With layer-based encoding one can obtain larger networks [8, 9, 16]. The encoding scheme is a very complicated system of descriptions of connectivity between a list of layers.
Reference: [25] <author> M. Srinivas and L.M. Patnaik. </author> <title> Learning neural network weights using genetic algorithms | improving performance by search space reduction. </title> <booktitle> In International Joint Conference on Neural Networks, </booktitle> <pages> pages 2331-2336. </pages> <publisher> IEEE, </publisher> <year> 1991. </year>
Reference-contexts: The crossover of two different encodings of similar networks can result in a completely divergent NN. This broadly acknowledged problem is also known as structural-functional mapping problem [27] or competing conventions problem [6]. There have been a few proposals to solve it <ref> [25, 11, 6] </ref>. However, these studies also indicate that the problem has less impact than expected.
Reference: [26] <author> David W. White. GANNet: </author> <title> A Genetic Algorithm for Searching Toplogy and Weight Spaces in Neural Network Design. </title> <type> PhD thesis, </type> <institution> University of Maryland, </institution> <year> 1993. </year>
Reference-contexts: A few GANN systems focus solely on weight training [27], wheras others focus on architecture [23]. There are also many ways to combine the two issues: The GA may set initial weight information while weight training is left to established learning rules for NN, such as backpropagation <ref> [26] </ref>. Or, the evolution of the NN can take place in two stages: One that finds the optimal architecture, the other that finds a weight setting for each architecture [10]. 2 Weight encoding A few different complexities of weight encoding have been investigated in this field. <p> Thus, most recent researchers are using more complex data types. The different encoding strategies can be classified by their smallest defining unit, or the "allele" <ref> [26] </ref>: connections, nodes, layers or pathways, or they can be indirectly encoded which is a a completely different approach. The classes are roughly ordered to the complexity of the strategies. Connection-based encoding The vast majority of early approaches is connection-based [13, 17, 19, 21, 27]. <p> In this approach, the genome is a string or tree [14] of node information. The code for each node may include relative position, backward connectivity [24], weight values, threshold function <ref> [26] </ref> and more. Crossover and mutation is mostly re stricted to cuts between node information. Layer-based encoding With layer-based encoding one can obtain larger networks [8, 9, 16]. The encoding scheme is a very complicated system of descriptions of connectivity between a list of layers. <p> Boers and Kuiper used a grammar based on Lindenmayer systems, Gruau applied cellular encoding. Basically all encoding techniques developed so far, can be classified into one of these classes. If the genomes are too large, the efficiency of the GA decreases <ref> [26] </ref>, and this problem has to be addressed when larger networks are involved. More complex encoding strategies are motivated by this lack of scalability, because not every connection or node has to be individually represented in the genome. <p> The convergence time competes well with backpropagation [28, 13]. The largest problems that have been tackled successfully range from high-bit parity [4], pole-balancing [28, 10], and simplified pattern recognition [1] to high-order classification tasks <ref> [26] </ref>. If convergence times are reported at all, they are usually quite high: 3 days on 11 workstations [1] or one week [26, in conversation], [10, in conversation] for the mentioned tasks. <p> The largest problems that have been tackled successfully range from high-bit parity [4], pole-balancing [28, 10], and simplified pattern recognition [1] to high-order classification tasks [26]. If convergence times are reported at all, they are usually quite high: 3 days on 11 workstations [1] or one week <ref> [26, in conversation] </ref>, [10, in conversation] for the mentioned tasks. These are tasks that can be solved faster by simple back-propagation on arbitrary NN chosen by rules of thumb.
Reference: [27] <author> D. Whitley, T. Starkweather, and C. Bogart. </author> <title> Genetic algorithms and neural networks: optimizing connections and connectivity. </title> <journal> Parallel Computing, </journal> <volume> 14 </volume> <pages> 347-361, </pages> <year> 1990. </year>
Reference-contexts: We can distinguish two seperate issues for the genetic algorithm: on the one hand architecture optimization, and on the other hand weight training. A few GANN systems focus solely on weight training <ref> [27] </ref>, wheras others focus on architecture [23]. There are also many ways to combine the two issues: The GA may set initial weight information while weight training is left to established learning rules for NN, such as backpropagation [26]. <p> The simplest case is one-bit connectivity information <ref> [27] </ref>. Or, two bits are used for encoding if a connection is disconnected, inhibitory or excitatory [19]. Weights have been encoded as real numbers [21, 22], bit strings [20, 27], or bit strings using grey coding [13]. <p> The simplest case is one-bit connectivity information [27]. Or, two bits are used for encoding if a connection is disconnected, inhibitory or excitatory [19]. Weights have been encoded as real numbers [21, 22], bit strings <ref> [20, 27] </ref>, or bit strings using grey coding [13]. Maniezzo proposes that the number of encoding bits should increase during the evolution [17], allowing fine tuning at a later stage. <p> The classes are roughly ordered to the complexity of the strategies. Connection-based encoding The vast majority of early approaches is connection-based <ref> [13, 17, 19, 21, 27] </ref>. Here, the genome is a string of weight values or pure connectivity information. This requires a fixed maximal architecture, which is typ ically either fully-connected or layered. <p> Permutation problem Almost always, the same (or similar) networks may be encoded in quite different ways. The crossover of two different encodings of similar networks can result in a completely divergent NN. This broadly acknowledged problem is also known as structural-functional mapping problem <ref> [27] </ref> or competing conventions problem [6]. There have been a few proposals to solve it [25, 11, 6]. However, these studies also indicate that the problem has less impact than expected.
Reference: [28] <author> Darell Whitley, Stephen Dominic, Rajarshi Das, and Charles W. Anderson. </author> <title> Genetic reinforcement learning for neurocontrol problems. </title> <journal> Machine Learning, </journal> <volume> 13 </volume> <pages> 259-284, </pages> <year> 1993. </year>
Reference-contexts: The quality of an encoding strategy, however, has to be measured by the speed of convergence to sufficiently good solutions. Most papers report good results on small-scale toy problems such as xor, sine, parity, or low-bit adding. The convergence time competes well with backpropagation <ref> [28, 13] </ref>. The largest problems that have been tackled successfully range from high-bit parity [4], pole-balancing [28, 10], and simplified pattern recognition [1] to high-order classification tasks [26]. <p> Most papers report good results on small-scale toy problems such as xor, sine, parity, or low-bit adding. The convergence time competes well with backpropagation [28, 13]. The largest problems that have been tackled successfully range from high-bit parity [4], pole-balancing <ref> [28, 10] </ref>, and simplified pattern recognition [1] to high-order classification tasks [26]. If convergence times are reported at all, they are usually quite high: 3 days on 11 workstations [1] or one week [26, in conversation], [10, in conversation] for the mentioned tasks. <p> The empirical results, however, do not clearly indicate that more complex systems show a better performance. The fundamental problem of the poor performance of GANN systems on real-world scale tasks has been recognized <ref> [28] </ref>. One magic word for the solution is modularity [7], but unfortunately encoding strategies with high modularity like indirect encoding, have not been a breakthrough. Another idea is to partially develop NNs for subtasks and compose them to a more complex structure.
References-found: 28

