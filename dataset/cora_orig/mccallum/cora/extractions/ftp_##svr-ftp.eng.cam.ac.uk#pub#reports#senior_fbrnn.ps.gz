URL: ftp://svr-ftp.eng.cam.ac.uk/pub/reports/senior_fbrnn.ps.gz
Refering-URL: http://www.cs.cmu.edu/Web/Groups/NIPS/NIPS95/Papers.html
Root-URL: 
Title: Forward-backward retraining of recurrent neural networks  
Author: Andrew Senior Tony Robinson 
Address: Trumpington Street, Cambridge, England  
Affiliation: Cambridge University Engineering Department  
Abstract: This paper describes the training of a recurrent neural network as the letter posterior probability estimator for a hidden Markov model, off-line handwriting recognition system. The network estimates posterior distributions for each of a series of frames representing sections of a handwritten word. The supervised training algorithm, backpropagation through time, requires target outputs to be provided for each frame. Three methods for deriving these targets are presented. A novel method based upon the forward-backward algorithm is found to result in the recognizer with the lowest error rate.
Abstract-found: 1
Intro-found: 1
Reference: <author> Bourlard, H. and Morgan, N. </author> <title> (1993) Connectionist Speech Recognition: A Hybrid Approach. </title> <publisher> Kluwer. </publisher>
Reference-contexts: Further, the approximation P (fl i jx t 1 ) P (fl i jx t ) is made. The posteriors are scaled by the class priors P (fl i ) <ref> (Bourlard and Morgan 1993) </ref>, and these scaled posteriors are used in the hidden Markov model in place of data likelihoods since, by Bayes' rule, P (x t jfl i ) / P (fl i ) Table 1 shows word recognition error rates for three 80-unit networks trained to wards fixed targets
Reference: <author> Rabiner, L. R. and Juang, B. H. </author> <title> (1986) An introduction to hidden Markov models. </title> <journal> IEEE ASSP magazine 3 (1): </journal> <pages> 4-16. </pages>
Reference-contexts: Such a scheme has been used before in speech recognition using recurrent networks (Robinson 1994). This representation, is found to inadequately represent some frames which can represent two letters, or the ligatures between letters. Thus, by analogy with the forward-backward algorithm <ref> (Rabiner and Juang 1986) </ref> for HMM speech recognizers, we have developed a fl Now at IBM T.J.Watson Research Center, Yorktown Heights NY10598, USA. forward-backward method for retraining the recurrent neural network. <p> A `soft' classification would give a more accurate portrayal of the frame identities. Such a distribution, fl p (t) = P (S t = q p jx t 1 ; W ), can be calculated with the forward-backward algorithm <ref> (Rabiner and Juang 1986) </ref>. To obtain fl p (t), the forward probabilities ff p (t) = P (S t = q p ; x t 1 ) must be combined with the backward probabilities fi p (t) = P (S t = q p ; x t t+1 ).
Reference: <author> Robinson, A. </author> <title> (1994) The application of recurrent nets to phone probability estimation. </title> <journal> IEEE Transactions on Neural Networks. </journal>
Reference-contexts: Such a system could be used for a variety of purposes, from cheque processing and postal sorting to personal correspondence reading for the blind or historical document reading. In a previous publication (Senior 1994) we have described a system based on a recurrent neural network <ref> (Robinson 1994) </ref> which can transcribe a handwritten document. The recurrent neural network is used to estimate posterior probabilities for character classes, given frames of data which represent the handwritten word. <p> The second is a simple Viterbi-style segmentation method that assigns a single class label to each of the frames of data. Such a scheme has been used before in speech recognition using recurrent networks <ref> (Robinson 1994) </ref>. This representation, is found to inadequately represent some frames which can represent two letters, or the ligatures between letters.
Reference: <author> Rumelhart, D. E., Hinton, G. E. and Williams, R. J. </author> <title> (1986) Learning internal representations by error propagation. In Parallel Distributed Processing: Explorations in the Microstructure of Cognition, </title> <editor> ed. by D. E. Rumelhart and J. L. McClelland, </editor> <volume> volume 1, chapter 8, </volume> <pages> pp. 318-362. </pages> <publisher> Bradford Books. </publisher>
Reference-contexts: is the mean squared error, but here the relative entropy, G, of the target and output distributions is used: G = t j j (t) : (5) At the end of a word, the errors between the network's outputs and the targets are propagated back using the generalized delta rule <ref> (Rumelhart et al. 1986) </ref> and changes to the network weights are calculated. The network at successive time steps is treated as adjacent layers of a multi-layer network. This process is generally known as `back-propagation through time' (Werbos 1990).
Reference: <author> Santini, S. and Del Bimbo, A. </author> <title> (1995) Recurrent neural networks can be trained to be maximum a posteriori probability classifiers. </title> <booktitle> Neural Networks 8 (1): </booktitle> <pages> 25-29. </pages>
Reference-contexts: This cycle can be repeated until the segmentations do not change and performance ceases to improve. For speed, the network is not trained to convergence at each iteration. It can be shown <ref> (Santini and Del Bimbo 1995) </ref> that, assuming that the network has enough parameters, the network outputs after convergence will approximate the posterior probabilities P (fl i jx t 1 ). Further, the approximation P (fl i jx t 1 ) P (fl i jx t ) is made.
Reference: <author> Senior, A. W., </author> <title> (1994) Off-line Cursive Handwriting Recognition using Recurrent Neural Networks. </title> <institution> Cambridge University Engineering Department Ph.D. </institution> <type> thesis. </type> <address> URL: ftp://svr-ftp.eng.cam.ac.uk/pub/reports/senior thesis.ps.gz. </address>
Reference-contexts: Such a system could be used for a variety of purposes, from cheque processing and postal sorting to personal correspondence reading for the blind or historical document reading. In a previous publication <ref> (Senior 1994) </ref> we have described a system based on a recurrent neural network (Robinson 1994) which can transcribe a handwritten document. The recurrent neural network is used to estimate posterior probabilities for character classes, given frames of data which represent the handwritten word. <p> The other parts are summarized here and described in more detail in another publication <ref> (Senior 1994) </ref>. The first stage of processing converts the raw data into an invariant representation used as an input to the neural network. The network outputs are used to calculate word probabilities in a hidden Markov model. First, the scanned page image is automatically segmented into words and then normalized.
Reference: <author> Werbos, P. J. </author> <title> (1990) Backpropagation through time: What it does and how to do it. </title> <booktitle> Proceedings of the IEEE 78: </booktitle> <pages> 1550-60. </pages>
Reference-contexts: The network at successive time steps is treated as adjacent layers of a multi-layer network. This process is generally known as `back-propagation through time' <ref> (Werbos 1990) </ref>. After processing t frames of data with an input/output latency, the network is equivalent to a (t + latency) layer perceptron sharing weights between layers.
References-found: 7

