URL: http://http.cs.berkeley.edu/~vahdat/publications/c268_writeup.ps
Refering-URL: http://http.cs.berkeley.edu/~vahdat/publications/work.html
Root-URL: 
Title: The Suitability of an ATM Switch for a NOW Interconnect  
Author: Kim Man Liu Clifford Mather Amin Vahdat 
Date: May 10, 1994  
Pubnum: CS-268 Final Project Report  
Abstract: ATM technology is heralded as the future of both local area networking and telecommunication switching. An affordable high bandwidth, low latency interconnect will enable a number of exciting new applications. The Network of Workstations (NOW) project at UC Berkeley is investigating both software and hardware techniques to harvest idle cycles in workstation clusters to run parallel jobs in the background. One of the key components for the success of the project is a low latency network with scalable aggregate bandwidth. In this paper, we examine the performance characteristics of a Synoptics LatticeCell ATM switch to determine whether it may be a suitable interconnect for the NOW project. 
Abstract-found: 1
Intro-found: 1
Reference: [Alles 1993] <author> Alles, A. </author> <title> ATM in Private Networking. </title> <institution> Hughes LAN Systems, </institution> <year> 1993. </year>
Reference-contexts: High bandwidth is also demanded by other NOW features that access large blocks of remote memory, such as process migration, remote memory access, and remote file access. 2.3 ATM Overview The ATM protocol <ref> [Alles 1993] </ref> resides between the physical layer and transport layer in the OSI 7-layer model, using the physical layer (e.g. SONET) to provide services to a transport layer (e.g. TCP or UDP) through an ATM Adaptation Layer (AAL).
Reference: [Arpaci et al. 1993] <author> Arpaci, R., Dusseau, A., and Liu, L. T. </author> <title> The Effects of a Non-Dedicated Environment on Parallel Applications. </title> <institution> Computer Science Division, University of California, Berkeley, </institution> <year> 1993. </year>
Reference-contexts: Together, they will provide the NOW's parallel functionality: * Coscheduling. Preliminary research has shown that allowing each CPU to schedule parallel processes independently adversely affects performance; thus, parallel jobs will be coscheduled <ref> [Arpaci et al. 1993] </ref>. * Distributed shared memory. Global memory references will be translated into remote memory accesses. * Parallel file system. NOW intends to incorporate a prototype high-performance parallel file system, xFS [Dahlin et al. 1994]. <p> From the plot we find that the efficiency of parallel jobs depends heavily on communication performance. Since messages in a parallel job are often small [Cypher et al. 1993], this performance is dominated by latency. Other motivating factors for low latency include distributed shared memory and coscheduling skew minimization <ref> [Arpaci et al. 1993] </ref>. Our target latency goal for the NOW project is 10 s between machines: 2 s in the wire, 3 s in switch routing, and 2.5 s in each of the two drivers.
Reference: [Bubenik et al. 1989] <author> Bubenik, R. G. and Turner, J. S. </author> <title> Performance of a Broadcast Packet Switch. </title> <journal> IEEE Transactions on Communications, </journal> <volume> vol. 37, no. 1, ppg. </volume> <pages> 60-69, </pages> <month> January </month> <year> 1989. </year> <title> 14 poor aggregate bandwidth. </title>
Reference-contexts: Much of the information presented here is from the "St. Louis" switch <ref> [Bubenik et al. 1989] </ref>, upon which the Synoptics switch is based. We have confirmed that most of this information applies to the Synoptics switch as well, through private communications with various informed sources. 5 * The distribution network avoids pathological routing combinations by routing incoming cells to alternate outputs.
Reference: [Culler et al. 1993] <author> Culler, D. E., Dusseau, A., Goldstein, S. C., Krishnamurthy, A., Lumetta, S., von Eicken, T., and Yelick, K. </author> <title> Parallel Programming in Split-C. </title> <institution> Computer Science Division, University of California, Berkeley, </institution> <year> 1993. </year>
Reference-contexts: This requires the creation of a global layer in the operating system, GLUnix, that will support a communication layer, Active Messages [von Eicken et al. 1992], and a low-level programming language, Split-C <ref> [Culler et al. 1993] </ref>. Together, they will provide the NOW's parallel functionality: * Coscheduling. Preliminary research has shown that allowing each CPU to schedule parallel processes independently adversely affects performance; thus, parallel jobs will be coscheduled [Arpaci et al. 1993]. * Distributed shared memory.
Reference: [Clark et al. 1989] <author> Clark, D. D., Jacobson, V., Romkey, J., and Salwen, H. </author> <title> An Analysis of TCP Processing Overhead. </title> <journal> IEEE Communications Magazine, </journal> <volume> 6 </volume> <pages> 23-29, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: We identify the cost associated with each of these components and present them below. Traditionally, end-to-end communication in UNIX systems use either the UDP or TCP protocol stacks. There are commonly-held notions that the overhead associated with these protocols is very high <ref> [Clark et al. 1989] </ref>. This overhead can largely be attributed to multiple context switches (from user level to the kernel and back again), and multiple memory copies (from the network to the kernel, and from the kernel into user space).
Reference: [Cypher et al. 1993] <author> Cypher, R., Ho, A., Konstantinidou, S., and Messina, P. </author> <title> Architectural Requirements of Parallel Scientific Applications with Explicit Communication. </title> <booktitle> International Symposium on Computer Architecture, </booktitle> <year> 1993. </year>
Reference-contexts: Figure 1 shows a plot of efficiency versus the ratio m f , as well as reference points for the above two configurations. From the plot we find that the efficiency of parallel jobs depends heavily on communication performance. Since messages in a parallel job are often small <ref> [Cypher et al. 1993] </ref>, this performance is dominated by latency. Other motivating factors for low latency include distributed shared memory and coscheduling skew minimization [Arpaci et al. 1993].
Reference: [Dahlin et al. 1994] <author> Dahlin, M. D., Mather, C. J., Wang, R. Y., Anderson, T. E., and Patterson, D. A. </author> <title> A Quantitative Analysis of Cache Policies for Scalable Network File Systems. </title> <booktitle> To appear in Proc. ACM SIGMETRICS Conference, </booktitle> <month> May </month> <year> 1994. </year>
Reference-contexts: Global memory references will be translated into remote memory accesses. * Parallel file system. NOW intends to incorporate a prototype high-performance parallel file system, xFS <ref> [Dahlin et al. 1994] </ref>. Since parallel programmers are but a small fraction of all NOW users, in order to convince most users to provide their machines for parallel use, NOW must provide similar functionality and performance to sequential users as well.
Reference: [Demmel 1994] <author> Demmel, J. </author> <booktitle> CS-267 Lecture Notes. </booktitle> <institution> Computer Science Division, University of California, Berkeley, </institution> <year> 1994. </year>
Reference-contexts: For example, consider 2 f m workstation LAN 0.5 ms 5 ms Table 1: f and m values for two computation clusters. f , with reference points for two example computation clusters. the problem of modeling heat transfer through a metal bar <ref> [Demmel 1994] </ref>.
Reference: [ERL 1993] <author> Electronics Research Laboratory. </author> <title> System Support for Distributed Supercomputing on a Network of Workstations (NOW). </title> <institution> University of California, Berkeley, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: 1 Introduction This paper examines the performance of a Synoptics LatticeCell ATM switch in order to determine if it is feasible as an interconnect for the NOW project <ref> [ERL 1993] </ref>. The purpose of NOW is to run parallel programs on a workstation cluster, providing the same environment (i.e. communication API, compilers, tools) as a parallel machine, while maintaining or enhancing sequential performance.
Reference: [Feitelson et al. 1992] <author> Feitelson, D. G. and Rudolph, L. </author> <title> Gang Scheduling Performance Benefits for Fine-Grain Synchronization. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 16 </volume> <pages> 306-318, </pages> <year> 1992. </year>
Reference: [Gupta et al. 1991] <author> Gupta, A., Tucker, A., and Urushibara, S. </author> <title> The Impact of Operating System Scheduling Policies and Synchronization Methods on the Performance of Parallel Applications. </title> <booktitle> Proc. ACM SIGMETRICS Conference, </booktitle> <month> May </month> <year> 1991. </year>
Reference: [Leutenegger et al. 1990] <author> Leutenegger, S. and Vernon, M. </author> <title> The performance of Multiprogrammed Multiprocessor Scheduling Policies. </title> <booktitle> Proc. ACM SIGMETRICS Conference, </booktitle> <month> May </month> <year> 1990. </year> <month> 15 </month>
Reference: [Ousterhout 1982] <author> Ousterhout, J. K. </author> <title> Scheduling Techniques for Concurrent Systems. </title> <booktitle> Proc. 3rd International Conference on Distributed Computing Systems, ppg. </booktitle> <pages> 22-30, </pages> <month> October </month> <year> 1982. </year>
Reference: [Ousterhout 1993] <author> Ousterhout, J. K. </author> <title> Tcl and the Tk Toolkit. </title> <publisher> Addison-Wesley, </publisher> <year> 1993. </year>
Reference-contexts: In our network of eight Sparcstation 10 workstations connected by two Synoptics LatticeCell ATM switches, we measured latency, bandwidth, and drop rate of packets through a switch, using the Unix sockets interface [Stevens 1990]. Additionally, we built a user-interface to our benchmark programs, using Tcl/TK <ref> [Ousterhout 1993] </ref>, to facilitate our measurement and analysis. Our preliminary benchmarks have shown a latency through the switch of 42 s and packet drop rate of 50% during periods of heavy load, both unsatisfactory for current NOW goals.
Reference: [Schroeder et al. 1991] <author> Schroeder, M., Birrell, A., Burrows, M., Murray, H., Needham, R., Rode-heffer, T., Satterthwaite, E., and Thacker, C. Autonet: </author> <title> A High-Speed Self-Configuring Local Area Network Using Point-to-Point Links. </title> <journal> IEEE J. Selected Areas Comm., </journal> <month> Oc-tober </month> <year> 1991. </year>
Reference-contexts: Independent of the cause, it is discouraging to see contention on the network when there are only six machines connected to the switch. The problem with congestion will only become compounded once a full complement of sixteen machines is connected to the switch. A crossbar design <ref> [Schroeder et al. 1991] </ref> will not suffer from contention for internal nodes and may be more 12 appropriate than the Banyan design in the Synoptics switch. 5 Future Work One limitation to our experiments was the small number of machines available to us to place on the ATM network.
Reference: [Stevens 1990] <author> Stevens, W. R. </author> <title> UNIX network programming. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <note> 1990. </note> <author> [von Eicken et al. 1992] von Eicken, T., Culler, D. E., Goldstein, S. C., and Schauser, K. E. </author> <title> Active Messages: a Mechanism for Integrated Communication and Computation. </title> <booktitle> In Proc. of the 19th Int'l Symposium on Computer Architecture, </booktitle> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year> <note> (Also available as Technical Report UCB/CSD 92/675, </note> <institution> CS Div., University of California at Berkeley). </institution>
Reference-contexts: Furthermore, bandwidth should be scalable with the number of machines in the workstation cluster. In our network of eight Sparcstation 10 workstations connected by two Synoptics LatticeCell ATM switches, we measured latency, bandwidth, and drop rate of packets through a switch, using the Unix sockets interface <ref> [Stevens 1990] </ref>. Additionally, we built a user-interface to our benchmark programs, using Tcl/TK [Ousterhout 1993], to facilitate our measurement and analysis.
Reference: [Zajcew 1993] <author> Zajcew, R. et al. </author> <title> An OSF/1 UNIX for Massively Parallel Multicomputers. </title> <booktitle> Proc. 1993 Winter USENIX, ppg. </booktitle> <pages> 449-468, </pages> <month> January </month> <year> 1993. </year> <month> 16 </month>
References-found: 17

