URL: ftp://ftp-pubs.lcs.mit.edu/pub/lcs-pubs/tr.outbox/MIT-LCS-TR-664.ps.gz
Refering-URL: ftp://ftp-pubs.lcs.mit.edu/pub/lcs-pubs/listings/tr600.html
Root-URL: 
Title: Reactive Synchronization Algorithms for Multiprocessors  
Author: by Beng-Hong Lim Anant Agarwal 
Degree: 1991 Submitted to the DEPARTMENT OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCE in partial fulfillment of the requirements for the degree of DOCTOR OF PHILOSOPHY at the  All rights reserved Signature of Author  Certified by  Associate Professor of Electrical Engineering and Computer Science Thesis Supervisor Accepted by Frederic R. Morgenthaler Chairman, EECS Committee on Graduate Students  
Note: c 1995,  
Date: February 1995  October 28, 1994  
Address: 1986  
Affiliation: B.S., Electrical Engineering and Computer Science Massachusetts Institute of Technology,  M.S., Electrical Engineering and Computer Science Massachusetts Institute of Technology,  MASSACHUSETTS INSTITUTE OF TECHNOLOGY  Massachusetts Institute of Technology  Department of Electrical Engineering and Computer Science  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> A. Agarwal, B.-H. Lim, D. Kranz, and J. Kubiatowicz. </author> <month> APRIL: </month> <title> A Processor Architecture for Multiprocessing. </title> <booktitle> In Proceedings 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 104-114, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: It accomplishes this by rapidly switching the processor to a different thread whenever a high-latency operation is encountered. While previous multithreaded designs switch contexts at every cycle [53, 21], Alewife's multithreaded processor <ref> [1] </ref> switches contexts only on synchronization faults and remote cache misses. This style is called block multithreading [33] and has the advantage of high single thread performance. <p> End (id, CHANGE) FUNC IsValid (p) -&gt; BOOL = RET p.valid Figure B.2: A C-serial specification of a protocol object. 141 MODULE ProtocolManager [P1, P2, V] = TYPE PS = SEQ [PROTOCOL_OBJECT] PROC Create () -&gt; PS = VAR ps := PS- P1.Create (), P2.Create () - | Invalidate (ps <ref> [1] </ref>); RET p PROC DoSynchOp (ps) -&gt; V = VAR v : UNION [V, NULL] := nil | DO v = nil =&gt; v := DoProtocol (ps [0]) [] v := DoProtocol (ps [1]) OD; PROC DoChange (ps) = Invalidate (ps [0]) =&gt; Validate (ps [1]) [] Invalidate (ps [1]) =&gt; <p> Create () -&gt; PS = VAR ps := PS- P1.Create (), P2.Create () - | Invalidate (ps <ref> [1] </ref>); RET p PROC DoSynchOp (ps) -&gt; V = VAR v : UNION [V, NULL] := nil | DO v = nil =&gt; v := DoProtocol (ps [0]) [] v := DoProtocol (ps [1]) OD; PROC DoChange (ps) = Invalidate (ps [0]) =&gt; Validate (ps [1]) [] Invalidate (ps [1]) =&gt; Validate (ps [0]) Figure B.3: A protocol manager. <p> () - | Invalidate (ps <ref> [1] </ref>); RET p PROC DoSynchOp (ps) -&gt; V = VAR v : UNION [V, NULL] := nil | DO v = nil =&gt; v := DoProtocol (ps [0]) [] v := DoProtocol (ps [1]) OD; PROC DoChange (ps) = Invalidate (ps [0]) =&gt; Validate (ps [1]) [] Invalidate (ps [1]) =&gt; Validate (ps [0]) Figure B.3: A protocol manager. B.3 An Implementation of a Protocol Manager Figure B.3 provides essentially the same implementation of a protocol manager as in protocols to execute until it succeeds in executing a valid protocol. <p> (ps <ref> [1] </ref>); RET p PROC DoSynchOp (ps) -&gt; V = VAR v : UNION [V, NULL] := nil | DO v = nil =&gt; v := DoProtocol (ps [0]) [] v := DoProtocol (ps [1]) OD; PROC DoChange (ps) = Invalidate (ps [0]) =&gt; Validate (ps [1]) [] Invalidate (ps [1]) =&gt; Validate (ps [0]) Figure B.3: A protocol manager. B.3 An Implementation of a Protocol Manager Figure B.3 provides essentially the same implementation of a protocol manager as in protocols to execute until it succeeds in executing a valid protocol. It returns the result of the valid execution. <p> P.ReadLock, P.ReadUnlock, P.WriteLock, and P.WriteUnlock are the original reader-writer lock protocols. 144 MODULE MutexManager [P1, P2] = PROC Acquire (ps) = VAR b : BOOL := fail | DO b = fail =&gt; IsValid (ps [0]) =&gt; b := Acquire (ps [0]) [] IsValid (ps <ref> [1] </ref>) =&gt; b := Acquire (ps [1]) OD PROC Release (ps) = IsValid (ps [0]) =&gt; Release (ps [0]) [] IsValid (ps [1]) =&gt; Release (ps [1]) PROC DoChange (ps) = Invalidate (ps [0]) =&gt; Validate (ps [1]) [] Invalidate (ps [1]) =&gt; Validate (ps [0]) END MutexManager MODULE MutexObject [P] <p> P.ReadUnlock, P.WriteLock, and P.WriteUnlock are the original reader-writer lock protocols. 144 MODULE MutexManager [P1, P2] = PROC Acquire (ps) = VAR b : BOOL := fail | DO b = fail =&gt; IsValid (ps [0]) =&gt; b := Acquire (ps [0]) [] IsValid (ps <ref> [1] </ref>) =&gt; b := Acquire (ps [1]) OD PROC Release (ps) = IsValid (ps [0]) =&gt; Release (ps [0]) [] IsValid (ps [1]) =&gt; Release (ps [1]) PROC DoChange (ps) = Invalidate (ps [0]) =&gt; Validate (ps [1]) [] Invalidate (ps [1]) =&gt; Validate (ps [0]) END MutexManager MODULE MutexObject [P] = PROC Acquire (p) -&gt; BOOL <p> PROC Acquire (ps) = VAR b : BOOL := fail | DO b = fail =&gt; IsValid (ps [0]) =&gt; b := Acquire (ps [0]) [] IsValid (ps <ref> [1] </ref>) =&gt; b := Acquire (ps [1]) OD PROC Release (ps) = IsValid (ps [0]) =&gt; Release (ps [0]) [] IsValid (ps [1]) =&gt; Release (ps [1]) PROC DoChange (ps) = Invalidate (ps [0]) =&gt; Validate (ps [1]) [] Invalidate (ps [1]) =&gt; Validate (ps [0]) END MutexManager MODULE MutexObject [P] = PROC Acquire (p) -&gt; BOOL = PROC Release (p) = P.Lock (p); P.Unlock (p) p^.valid =&gt; RET success [*] P.Unlock (p); <p> VAR b : BOOL := fail | DO b = fail =&gt; IsValid (ps [0]) =&gt; b := Acquire (ps [0]) [] IsValid (ps <ref> [1] </ref>) =&gt; b := Acquire (ps [1]) OD PROC Release (ps) = IsValid (ps [0]) =&gt; Release (ps [0]) [] IsValid (ps [1]) =&gt; Release (ps [1]) PROC DoChange (ps) = Invalidate (ps [0]) =&gt; Validate (ps [1]) [] Invalidate (ps [1]) =&gt; Validate (ps [0]) END MutexManager MODULE MutexObject [P] = PROC Acquire (p) -&gt; BOOL = PROC Release (p) = P.Lock (p); P.Unlock (p) p^.valid =&gt; RET success [*] P.Unlock (p); RET fail PROC Validate <p> =&gt; IsValid (ps [0]) =&gt; b := Acquire (ps [0]) [] IsValid (ps <ref> [1] </ref>) =&gt; b := Acquire (ps [1]) OD PROC Release (ps) = IsValid (ps [0]) =&gt; Release (ps [0]) [] IsValid (ps [1]) =&gt; Release (ps [1]) PROC DoChange (ps) = Invalidate (ps [0]) =&gt; Validate (ps [1]) [] Invalidate (ps [1]) =&gt; Validate (ps [0]) END MutexManager MODULE MutexObject [P] = PROC Acquire (p) -&gt; BOOL = PROC Release (p) = P.Lock (p); P.Unlock (p) p^.valid =&gt; RET success [*] P.Unlock (p); RET fail PROC Validate (p) = PROC Invalidate (p) -&gt; BOOL = P.Lock (p); P.Lock <p> =&gt; b := Acquire (ps [0]) [] IsValid (ps <ref> [1] </ref>) =&gt; b := Acquire (ps [1]) OD PROC Release (ps) = IsValid (ps [0]) =&gt; Release (ps [0]) [] IsValid (ps [1]) =&gt; Release (ps [1]) PROC DoChange (ps) = Invalidate (ps [0]) =&gt; Validate (ps [1]) [] Invalidate (ps [1]) =&gt; Validate (ps [0]) END MutexManager MODULE MutexObject [P] = PROC Acquire (p) -&gt; BOOL = PROC Release (p) = P.Lock (p); P.Unlock (p) p^.valid =&gt; RET success [*] P.Unlock (p); RET fail PROC Validate (p) = PROC Invalidate (p) -&gt; BOOL = P.Lock (p); P.Lock (p); p^.valid =&gt; HAVOC <p> = PROC AcquireWrite (ps) = VAR b : BOOL := fail | VAR b : BOOL := fail | DO b = fail =&gt; DO b = fail =&gt; IsValid (ps [0]) =&gt; IsValid (ps [0]) =&gt; b := AcquireRead (ps [0]) b := AcquireWrite (ps [0]) [] IsValid (ps <ref> [1] </ref>) =&gt; [] IsValid (ps [1]) =&gt; b := AcquireRead (ps [1]) OD b := AcquireWrite (ps [1]) OD PROC ReleaseRead (ps) = PROC ReleaseWrite (ps) = IsValid (ps [0]) =&gt; IsValid (ps [0]) =&gt; ReleaseRead (ps [0]) ReleaseWrite (ps [0]) [] IsValid (ps [1]) =&gt; [] IsValid (ps [1]) =&gt; <p> VAR b : BOOL := fail | VAR b : BOOL := fail | DO b = fail =&gt; DO b = fail =&gt; IsValid (ps [0]) =&gt; IsValid (ps [0]) =&gt; b := AcquireRead (ps [0]) b := AcquireWrite (ps [0]) [] IsValid (ps <ref> [1] </ref>) =&gt; [] IsValid (ps [1]) =&gt; b := AcquireRead (ps [1]) OD b := AcquireWrite (ps [1]) OD PROC ReleaseRead (ps) = PROC ReleaseWrite (ps) = IsValid (ps [0]) =&gt; IsValid (ps [0]) =&gt; ReleaseRead (ps [0]) ReleaseWrite (ps [0]) [] IsValid (ps [1]) =&gt; [] IsValid (ps [1]) =&gt; ReleaseRead (ps [1]) ReleaseWrite (ps <p> | VAR b : BOOL := fail | DO b = fail =&gt; DO b = fail =&gt; IsValid (ps [0]) =&gt; IsValid (ps [0]) =&gt; b := AcquireRead (ps [0]) b := AcquireWrite (ps [0]) [] IsValid (ps <ref> [1] </ref>) =&gt; [] IsValid (ps [1]) =&gt; b := AcquireRead (ps [1]) OD b := AcquireWrite (ps [1]) OD PROC ReleaseRead (ps) = PROC ReleaseWrite (ps) = IsValid (ps [0]) =&gt; IsValid (ps [0]) =&gt; ReleaseRead (ps [0]) ReleaseWrite (ps [0]) [] IsValid (ps [1]) =&gt; [] IsValid (ps [1]) =&gt; ReleaseRead (ps [1]) ReleaseWrite (ps [1]) PROC DoChange (ps) = Invalidate <p> fail | DO b = fail =&gt; DO b = fail =&gt; IsValid (ps [0]) =&gt; IsValid (ps [0]) =&gt; b := AcquireRead (ps [0]) b := AcquireWrite (ps [0]) [] IsValid (ps <ref> [1] </ref>) =&gt; [] IsValid (ps [1]) =&gt; b := AcquireRead (ps [1]) OD b := AcquireWrite (ps [1]) OD PROC ReleaseRead (ps) = PROC ReleaseWrite (ps) = IsValid (ps [0]) =&gt; IsValid (ps [0]) =&gt; ReleaseRead (ps [0]) ReleaseWrite (ps [0]) [] IsValid (ps [1]) =&gt; [] IsValid (ps [1]) =&gt; ReleaseRead (ps [1]) ReleaseWrite (ps [1]) PROC DoChange (ps) = Invalidate (ps [0]) =&gt; Validate (ps [1]) <p> AcquireWrite (ps [0]) [] IsValid (ps <ref> [1] </ref>) =&gt; [] IsValid (ps [1]) =&gt; b := AcquireRead (ps [1]) OD b := AcquireWrite (ps [1]) OD PROC ReleaseRead (ps) = PROC ReleaseWrite (ps) = IsValid (ps [0]) =&gt; IsValid (ps [0]) =&gt; ReleaseRead (ps [0]) ReleaseWrite (ps [0]) [] IsValid (ps [1]) =&gt; [] IsValid (ps [1]) =&gt; ReleaseRead (ps [1]) ReleaseWrite (ps [1]) PROC DoChange (ps) = Invalidate (ps [0]) =&gt; Validate (ps [1]) [] Invalidate (ps [1]) =&gt; Validate (ps [0]) END RWLockManager MODULE RWLockObject [P] = PROC AcquireRead (p) -&gt; BOOL = PROC AcquireWrite (p) -&gt; BOOL = P.ReadLock <p> (ps <ref> [1] </ref>) =&gt; [] IsValid (ps [1]) =&gt; b := AcquireRead (ps [1]) OD b := AcquireWrite (ps [1]) OD PROC ReleaseRead (ps) = PROC ReleaseWrite (ps) = IsValid (ps [0]) =&gt; IsValid (ps [0]) =&gt; ReleaseRead (ps [0]) ReleaseWrite (ps [0]) [] IsValid (ps [1]) =&gt; [] IsValid (ps [1]) =&gt; ReleaseRead (ps [1]) ReleaseWrite (ps [1]) PROC DoChange (ps) = Invalidate (ps [0]) =&gt; Validate (ps [1]) [] Invalidate (ps [1]) =&gt; Validate (ps [0]) END RWLockManager MODULE RWLockObject [P] = PROC AcquireRead (p) -&gt; BOOL = PROC AcquireWrite (p) -&gt; BOOL = P.ReadLock (p); P.WriteLock (p); p^.valid =&gt; <p> IsValid (ps <ref> [1] </ref>) =&gt; b := AcquireRead (ps [1]) OD b := AcquireWrite (ps [1]) OD PROC ReleaseRead (ps) = PROC ReleaseWrite (ps) = IsValid (ps [0]) =&gt; IsValid (ps [0]) =&gt; ReleaseRead (ps [0]) ReleaseWrite (ps [0]) [] IsValid (ps [1]) =&gt; [] IsValid (ps [1]) =&gt; ReleaseRead (ps [1]) ReleaseWrite (ps [1]) PROC DoChange (ps) = Invalidate (ps [0]) =&gt; Validate (ps [1]) [] Invalidate (ps [1]) =&gt; Validate (ps [0]) END RWLockManager MODULE RWLockObject [P] = PROC AcquireRead (p) -&gt; BOOL = PROC AcquireWrite (p) -&gt; BOOL = P.ReadLock (p); P.WriteLock (p); p^.valid =&gt; RET success p^.valid =&gt; <p> =&gt; b := AcquireRead (ps <ref> [1] </ref>) OD b := AcquireWrite (ps [1]) OD PROC ReleaseRead (ps) = PROC ReleaseWrite (ps) = IsValid (ps [0]) =&gt; IsValid (ps [0]) =&gt; ReleaseRead (ps [0]) ReleaseWrite (ps [0]) [] IsValid (ps [1]) =&gt; [] IsValid (ps [1]) =&gt; ReleaseRead (ps [1]) ReleaseWrite (ps [1]) PROC DoChange (ps) = Invalidate (ps [0]) =&gt; Validate (ps [1]) [] Invalidate (ps [1]) =&gt; Validate (ps [0]) END RWLockManager MODULE RWLockObject [P] = PROC AcquireRead (p) -&gt; BOOL = PROC AcquireWrite (p) -&gt; BOOL = P.ReadLock (p); P.WriteLock (p); p^.valid =&gt; RET success p^.valid =&gt; RET success [*] <p> <ref> [1] </ref>) OD PROC ReleaseRead (ps) = PROC ReleaseWrite (ps) = IsValid (ps [0]) =&gt; IsValid (ps [0]) =&gt; ReleaseRead (ps [0]) ReleaseWrite (ps [0]) [] IsValid (ps [1]) =&gt; [] IsValid (ps [1]) =&gt; ReleaseRead (ps [1]) ReleaseWrite (ps [1]) PROC DoChange (ps) = Invalidate (ps [0]) =&gt; Validate (ps [1]) [] Invalidate (ps [1]) =&gt; Validate (ps [0]) END RWLockManager MODULE RWLockObject [P] = PROC AcquireRead (p) -&gt; BOOL = PROC AcquireWrite (p) -&gt; BOOL = P.ReadLock (p); P.WriteLock (p); p^.valid =&gt; RET success p^.valid =&gt; RET success [*] P.ReadUnlock (p); [*] P.WriteUnlock (p); RET fail RET fail PROC ReleaseRead <p> (ps) = PROC ReleaseWrite (ps) = IsValid (ps [0]) =&gt; IsValid (ps [0]) =&gt; ReleaseRead (ps [0]) ReleaseWrite (ps [0]) [] IsValid (ps <ref> [1] </ref>) =&gt; [] IsValid (ps [1]) =&gt; ReleaseRead (ps [1]) ReleaseWrite (ps [1]) PROC DoChange (ps) = Invalidate (ps [0]) =&gt; Validate (ps [1]) [] Invalidate (ps [1]) =&gt; Validate (ps [0]) END RWLockManager MODULE RWLockObject [P] = PROC AcquireRead (p) -&gt; BOOL = PROC AcquireWrite (p) -&gt; BOOL = P.ReadLock (p); P.WriteLock (p); p^.valid =&gt; RET success p^.valid =&gt; RET success [*] P.ReadUnlock (p); [*] P.WriteUnlock (p); RET fail RET fail PROC ReleaseRead (p) = PROC ReleaseWrite
Reference: [2] <author> Anant Agarwal and Mathews Cherian. </author> <title> Adaptive Backoff Synchronization Techniques. </title> <booktitle> In Proceedings 16th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 396-406, </pages> <address> New York, </address> <month> June </month> <year> 1989. </year> <note> IEEE. Also as MIT-LCS TM-396. </note>
Reference-contexts: The research demonstrates that test-and-set with randomized exponential backoff and queuing are the most promising spin-lock protocols. 28 Exponential Backoff Anderson [5] proposed exponential backoff as a way of reducing contention for spin locks. Agarwal and Cherian <ref> [2] </ref> independently proposed exponential backoff for reducing contention at barriers. The idea is to have each waiting process introduce some delay between lock accesses in the test-and-set or test-and-test-and-set algorithms.
Reference: [3] <author> Anant Agarwal, John Kubiatowicz, David Kranz, Beng-Hong Lim, Donald Yeung, Godfrey D'Souza, and Mike Parkin. Sparcle: </author> <title> An Evolutionary Processor Design for Multiprocessors. </title> <journal> IEEE Micro, </journal> <volume> 13(3) </volume> <pages> 48-61, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Commonly used waiting mechanisms are spinning and blocking: spinning is a polling mechanism, while blocking is a signaling mechanism. The Alewife multiprocessor provides additional polling and signaling mechanisms through Sparcle, its multithreaded processor <ref> [3] </ref>. Since a polling mechanism incurs a cost that is proportional to the waiting time, while a signaling mechanism incurs a fixed cost, the choice between a polling and a signaling mechanism depends on the length of the waiting time [47]. <p> Figure 2.2 illustrates the high-level organization of an Alewife node. Each node consists of a Sparcle processor <ref> [3] </ref>, an FPU, 64KB of cache memory, a 4MB portion of globally-addressable memory, the Caltech MRC network router, and the Alewife Communications and Memory Management Unit (CMMU) [32]. The current prototype is designed to run at 33MHz. Sparcle is a modified SPARC processor that supports multithreading.
Reference: [4] <author> A. Agarwal et al. </author> <title> The MIT Alewife Machine: A Large-Scale Distributed-Memory Multiprocessor. </title> <booktitle> In Proceedings of Workshop on Scalable Shared Memory Multiprocessors. </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year> <note> An extended version of this paper appears as MIT/LCS Memo TM-454, </note> <year> 1991. </year>
Reference-contexts: Anderson [5] observes that the choice between the two protocols depends on the level of lock contention. lock by Mellor-Crummey and Scott [43]. We measured the overhead incurred by these spin lock algorithms on a simulation of the Alewife multiprocessor <ref> [4] </ref>. Each data point represents the average overhead incurred by the synchronization algorithm for each critical section with P processors contending for the lock. One can view the overhead as the number of cycles the locking algorithm adds to the execution of each critical section. <p> It first overviews previous research on competitive on-line algorithms and shows how they relate to the design of reactive synchronization algorithms. It then describes the multiprocessing platform used for experimental evaluation of the reactive algorithms. Specifically, we describe relevant features of the Alewife multiprocessor <ref> [4] </ref> and the organization of the Alewife simulator on which we ran most of the experiments. 2.1 Competitive On-Line Algorithms An on-line problem is one in which an algorithm must process a sequence of requests without knowledge of future requests. <p> These experiments were run on a simulation of the Alewife multiprocessor. The Alewife multiprocessor is representative of a scalable shared-memory architecture based on distributed nodes that communicate via an interconnection network. 22 2.2.1 The Alewife Multiprocessor The MIT Alewife multiprocessor <ref> [4] </ref> is a cache-coherent, distributed-memory multiprocessor that supports the shared-memory programming abstraction. Figure 2.2 illustrates the high-level organization of an Alewife node.
Reference: [5] <author> Thomas E. Anderson. </author> <title> The Performance Implications of Spin Lock Alternatives for Shared-Memory Multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(1) </volume> <pages> 6-16, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: A remedy is a queuing protocol [43] that constructs a software queue of lock waiters to reduce memory contention. However, queuing comes at the price of a higher latency in the absence of contention. Anderson <ref> [5] </ref> observes that the choice between the two protocols depends on the level of lock contention. lock by Mellor-Crummey and Scott [43]. We measured the overhead incurred by these spin lock algorithms on a simulation of the Alewife multiprocessor [4]. <p> However a significant amount of communication traffic is still generated when the lock is released due to the ensuing cache invalidations and updates. With small critical sections, this transient behavior dominates and read-polling can generate as much communication traffic as polling with test&set <ref> [5] </ref>. Recent research has resulted in more sophisticated protocols that alleviate the detrimental effects of contention [5, 19, 43]. The research demonstrates that test-and-set with randomized exponential backoff and queuing are the most promising spin-lock protocols. 28 Exponential Backoff Anderson [5] proposed exponential backoff as a way of reducing contention for <p> With small critical sections, this transient behavior dominates and read-polling can generate as much communication traffic as polling with test&set [5]. Recent research has resulted in more sophisticated protocols that alleviate the detrimental effects of contention <ref> [5, 19, 43] </ref>. The research demonstrates that test-and-set with randomized exponential backoff and queuing are the most promising spin-lock protocols. 28 Exponential Backoff Anderson [5] proposed exponential backoff as a way of reducing contention for spin locks. Agarwal and Cherian [2] independently proposed exponential backoff for reducing contention at barriers. <p> can generate as much communication traffic as polling with test&set <ref> [5] </ref>. Recent research has resulted in more sophisticated protocols that alleviate the detrimental effects of contention [5, 19, 43]. The research demonstrates that test-and-set with randomized exponential backoff and queuing are the most promising spin-lock protocols. 28 Exponential Backoff Anderson [5] proposed exponential backoff as a way of reducing contention for spin locks. Agarwal and Cherian [2] independently proposed exponential backoff for reducing contention at barriers. The idea is to have each waiting process introduce some delay between lock accesses in the test-and-set or test-and-test-and-set algorithms. <p> Memory contention is reduced because each waiter spins on a different memory location and only one lock waiter is signaled when the lock is released. Queue locks have the additional advantage of providing fair access to the lock. Several queue lock protocols were developed independently by Anderson <ref> [5] </ref>, Graunke and Thakkar [19], and Mellor-Crummey and Scott [43]. All three protocols scale well with the level of contention. <p> This is due to the protocol cost of maintaining the queue of lock waiters. The results indicate that no single protocol has the best performance across contention levels. These measurements are consistent with previously reported results in <ref> [5, 43, 19] </ref>. It is clear that a reactive spin lock algorithm should select the test-and 32 test-and-set protocol when contention is low and the queue protocol when contention is high. The test-and-test-and-set protocol has the lowest overhead at low contention levels, and outperforms the test-and-set protocol. <p> Otherwise, a single processor may hog the spin lock when using the test-and-set or test-and-test-and-set protocols, unfairly favoring their performance. This test program is similar to that used by Anderson <ref> [5] </ref>. Each data point represents the average lock overhead per critical section with P processors contending for the lock. To arrive at this measure, we first compute the average elapsed time per critical section by dividing the actual elapsed time by the number of critical sections. <p> Second, the centralized nature of the algorithms removes any opportunity for parallelism by sequential-izing accesses. In response to this problem of scaling to high contention levels, a recent area of research focuses on designing scalable algorithms that perform well under high contention. Research on spin locks by Anderson <ref> [5] </ref>, Mellor-Crummey and Scott [43], and Graunke and Thakkar [19] show that the best approach to implementing spin locks under high contention is to enqueue lock waiters and service them one at a time.
Reference: [6] <author> Arvind, R. S. Nikhil, and K. K. Pingali. I-Structures: </author> <title> Data Structures for Parallel Computing. </title> <booktitle> In Proceedings of the Workshop on Graph Reduction, (Springer-Verlag Lecture Notes in Computer Science 279), </booktitle> <pages> pages 336-369, </pages> <month> September/October </month> <year> 1986. </year>
Reference-contexts: In research on waiting algorithms, we consider additional waiting mechanisms that are made possible through multithreading. We also consider the performance of waiting algorithms for several synchronization types, such as futures [22] and I-structures <ref> [6] </ref>, that are implemented with tagging and full-empty bits. 2.2.4 Run-Time System Assumptions The scheduling policy and the run-time overhead of thread management have a significant impact on the performance of waiting algorithms. <p> The Poisson assumption is a useful approximation of the behavior of many complex systems, and helps to make analysis tractable. Producer-Consumer Synchronization Producer-consumer synchronization is performed between one producer and one or more consumers of the data produced. Examples of this type of synchronization include futures [22] and I-structures <ref> [6] </ref>. This form of producer-consumer synchronization is different from another form where only one consumer is allowed to consume the data. This second form of producer-consumer synchronization can be modeled as mutual-exclusion synchronization. <p> A write to a full slot signals an error. We allow a J-structure slot to be reset. A reset empties the slot, permitting multiple assignments. Reusing J-structure slots in this way allows efficient cache performance. J-structures can be used to implement I-structure <ref> [6] </ref> semantics. Futures Futures [22] are a method for specifying control parallelism. The expression (future X) specifies that the expression X may be executed in parallel with the current thread.
Reference: [7] <author> J. Aspnes, M.P. Herlihy, and N. Shavit. </author> <title> Counting Networks and Multi-Processor Coordination. </title> <booktitle> In Proceedings of the 23rd Annual Symposium on Theory of Computing, </booktitle> <pages> pages 348-358, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Jacobi-Bar solves exactly the same problem as Jacobi, but uses a global barrier between iterations for synchronization instead of J-structures. Like in Jacobi, only nearest neighbor communication is necessary within an iteration. CountNet tests an implementation of a counting network <ref> [7] </ref>. Threads repeatedly try to increment the value of a counter through a bitonic counting network so as to reduce contention and allow parallelism.
Reference: [8] <author> Paul S. Barth, Rishiyur S. Nikhil, and Arvind. M-structures: </author> <title> Extending a parallel, non-strict, functional language with state. </title> <booktitle> In Proceedings of the 5th ACM Conference on Functional Programming Languages and Computer Architecture, </booktitle> <pages> pages 538-568, </pages> <month> August </month> <year> 1991. </year> <month> 157 </month>
Reference-contexts: A non-locking read returns the value found in a slot if full; otherwise it returns an invalid value. An L-structure therefore allows mutually exclusive access to each of its slots. The locking and unlocking L-structure reads and writes are sufficient to implement M-structures <ref> [8] </ref>. L-structures are different from M-structures in that they allow multiple non-locking readers. Semaphores Semaphores are used to implement mutual-exclusion.
Reference: [9] <author> Allan Borodin, Nathan Linial, and Michael Saks. </author> <title> An Optimal Online Algorithm for Metrical Task Systems. </title> <booktitle> In Proceedings of the 19th ACM Symposium on Theory of Computing, </booktitle> <pages> pages 373-382, </pages> <address> New York, </address> <month> May </month> <year> 1987. </year> <note> ACM. </note>
Reference-contexts: This places a limit on the complexity of the algorithms for detecting run-time conditions and deciding which protocol and waiting mechanism to use. Since dynamic protocol and waiting mechanism selection are instances of on-line problems, we rely on previous research on competitive algorithms <ref> [9, 41] </ref> to help design the reactive algorithms. An on-line problem is one in which an algorithm must process a sequence of requests without knowledge of future requests. <p> The amortized analysis of on-line algorithms has been extensively studied in the theory community in recent years <ref> [9, 41, 26] </ref>. The objective has been to design on-line algorithms that are within a small constant factor of the performance of an optimal off-line algorithm that has complete knowledge of future requests. Karlin et al. [27] coined the term c-competitive to describe such algorithms. <p> A c-competitive algorithm has a cost that is at most c times the cost of an optimal off-line algorithm plus a fixed constant term, for any sequence of requests. c is termed the competitive factor. 20 2.1.1 Task Systems Borodin, Linial and Saks <ref> [9] </ref> formalized the definition of an on-line problem and called it a task system. They also designed a 2n 1-competitive algorithm for task systems, where n is the number of states in the task system. <p> In <ref> [9] </ref>, Borodin et al. present an algorithm for such a task system that has a competitive factor of (2n 1) (D). They term their algorithm a nearly oblivious algorithm. <p> Thus, a request sequence is composed of contiguous sequences of wait requests, each followed by a proceed request, and can be described by the regular expression (waitjproceed) fl . The work by Borodin, Linial and Saks <ref> [9] </ref> presents a 3-competitive algorithm for this type of task system (See Chapter 2). Their work also shows that the competitive factor of 3 is a lower bound for a general two-state task system with unconstrained inputs.
Reference: [10] <author> Eric A. Brewer. </author> <title> Portable High-Performance Supercomputing: High-Level Architecture-Dependent Optimization. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <address> Cambridge, MA, </address> <month> September </month> <year> 1994. </year>
Reference-contexts: One approach is to model the performance of each protocol in terms of some relevant architectural parameters so that the tradeoffs between different protocols can be easily predicted for a given architecture <ref> [10] </ref>. It may also be possible to design more sophisticated policies for deciding when to switch protocols. The primary goal is to defend against the possibility of thrashing between protocols. In this thesis, we explored using hysteresis and competitive techniques for deciding when to switch protocols.
Reference: [11] <author> P.J. Burns et al. </author> <title> Vectorization of Monte-Carlo Particle Transport: An Architectural Study using the LANL Benchmark "Gamteb". </title> <booktitle> In Proc. Supercomputing '89, </booktitle> <address> New York, NY, </address> <month> November </month> <year> 1989. </year> <pages> IEEE/ACM. </pages>
Reference-contexts: They demonstrate the utility of having a reactive algorithm select the protocol to use. To better understand the results, we describe the characteristics of each application Gamteb Gamteb <ref> [11] </ref> is a photon transport simulation based on the Monte Carlo method. In this simulation, we used an input parameter of 2048 particles. Gamteb updates a set of nine interaction counters using fetch-and-increment.
Reference: [12] <author> David Chaiken, John Kubiatowicz, and Anant Agarwal. </author> <title> LimitLESS Directories: A Scalable Cache Coherence Scheme. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <pages> pages 224-234. </pages> <publisher> ACM, </publisher> <month> April </month> <year> 1991. </year>
Reference-contexts: It allows us to consider multithreading as providing additional waiting mechanisms in our research on waiting algorithms. The CMMU implements a cache-coherent globally-shared address space with the LimitLESS cache-coherence protocol <ref> [12] </ref>. The LimitLESS cache-coherence protocol maintains a small, fixed number of directory pointers in hardware, and relies on software trap handlers to handle cache-coherence actions when the number of read copies of a cache block exceeds the limited number of hardware directory pointers.
Reference: [13] <author> Manhoi Choy and Ambuj K. Singh. </author> <title> Adaptive Solutions to the Mutual Exclusion Problem. </title> <booktitle> In 12th Symposium on Principles of Distributed Computing (PODC), </booktitle> <address> Ithaca, NY, 1993. </address> <publisher> ACM. </publisher>
Reference-contexts: Reactive synchronization algorithms take a more general approach and deal with the harder problem of allowing the synchronization protocol itself to be changed. Adaptive Mutual Exclusion Recent research by Yang and Anderson [55], and Choy and Singh <ref> [13] </ref> designed adaptive algorithms for mutual exclusion in the context of shared-memory multiprocessors that provide only atomic read and write primitives. They tackle the classic mutual exclusion problem of reducing the time complexity of implementing mutual exclusion with only atomic reads and writes.
Reference: [14] <author> P.J. Courtois, F. Heymans, and D.L. Parnas. </author> <title> Concurrent Control with `Readers' and `Writers'. </title> <journal> Communications of the ACM, </journal> <volume> 14(10) </volume> <pages> 667-668, </pages> <month> October </month> <year> 1971. </year>
Reference-contexts: When designing a reactive algorithm for mutual-exclusion locks, the use of locks in 41 the protocol manager itself leads to the recursive problem of what protocols to use for these locks. We could solve the first problem of unnecessarily serializing protocol executions by using reader-writer locks <ref> [14] </ref> instead of mutual-exclusion locks. DoProtocol would acquire a read lock, while Invalidate and Validate would acquire write locks.
Reference: [15] <author> James R. Goodman, Mary K. Vernon, and Philip J. Woest. </author> <title> Efficient Synchronization Primitives for Large-Scale Cache-Coherent Multiprocessors. </title> <booktitle> In Third International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS III), </booktitle> <pages> pages 64-75, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: In this thesis, we use the software combining tree algorithm for fetch-and-op presented by Goodman, Vernon and Woest in <ref> [15] </ref>. <p> A variable protected by a test-and-test-and-set lock. 2. A variable protected by a queue lock. 3. A software combining tree by Goodman et al. <ref> [15] </ref>. In the first two protocols, the consensus objects are the locks protecting the centralized variables. In the combining tree, the consensus object is the root of the combining tree. Appendix C presents a pseudo-code listing of the reactive fetch-and-op algorithm. <p> The advantage of fetch-and-op is that concurrent fetch-and-op operations to a single variable can be combined and can proceed in parallel. Goodman et al. <ref> [15] </ref> present a combining tree algorithm to compute fetch-and-op in parallel. The price of using these scalable algorithms is that they typically have a higher protocol cost than simpler algorithms under low contention. In effect, these algorithms trade off performance at low contention for performance under high contention. <p> Reactive algorithms provide a solution by deferring the choice of protocols to run-time. The reactive spin lock algorithm removes the need for special hardware support for queue locks. For example, the Stanford DASH multiprocessor [38] and the Wisconsin Multicube <ref> [15] </ref> both include hardware support for queuing lock waiters. Software queuing algorithms provide the same scalable performance as hardware queue locks, but they come at a price of higher lock latency in the absence of contention. <p> Finally parent and children point to the parent and children of the node. The following is a modified excerpt from <ref> [15] </ref> describing their algorithm. In Part One, a process progresses up the combining tree, marking each FREE node as a COMBINE node. If the process finds a RESULT node, it must wait until the node becomes either FREE or COMBINE before continuing up the tree.
Reference: [16] <author> K. Gopinath, Krishna Narasimhan M. K., Beng-Hong Lim, and Anant Agarwal. </author> <title> Performance of Switch-Blocking in Multithreaded Processors. </title> <booktitle> In Proceedings of the 23rd International Conference on Parallel Processing, </booktitle> <address> Chicago, IL, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: Waiting mechanism Awaiting mechanism waits for synchronization conditions to be satisfied. Spinning and blocking are two common waiting mechanisms. A mul-tithreaded processor may provide alternative waiting mechanisms, such as switch spinning and switch-blocking <ref> [39, 16] </ref>. Waiting time Synchronization waiting time is the interval from when a thread begins waiting on a synchronization condition to when the synchronization condition is satisfied and the waiting thread is allowed to proceed. <p> We estimate the cost of switch-blocking in Alewife to be less than 100 cycles. We do not analyze the performance of switch-blocking as a waiting mechanism in this thesis. In <ref> [16] </ref>, Gopinath et al. present an analysis of switch-blocking on Alewife that shows that the use of switch-blocking as a waiting mechanism does not yield much advantage over switch-spinning, given the current parameters of the Alewife machine. 4.2 Polling versus Signaling A waiting algorithm can use any of the above waiting
Reference: [17] <author> A. Gottlieb, R. Grishman, C. P. Kruskal, K. P. McAuliffe, L. Rudolph, and M. Snir. </author> <title> The NYU Ultracomputer Designing a MIMD Shared-Memory Parallel Machine. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-32(2):175-189, </volume> <month> February </month> <year> 1983. </year>
Reference-contexts: When the operation is combinable [30], e.g., in fetch-and-add, combining techniques can be used to compute the operation in parallel. Fetch-and-op was deemed important enough for the designers of the NYU Ultracomputer <ref> [17] </ref> to include hardware support in its interconnection network for combining fetch-and-op requests to the same memory 30 location. The Stanford DASH multiprocessor [38] supports fetch-and-increment and fetch--and-decrement directly in its cache coherence protocol, although without combining. <p> Our reactive spin lock solves the latency problem, thus eliminating the incentive of providing queuing in hardware. The reactive fetch-and-op algorithm provides a viable alternative to hardware combining networks. For example, the NYU Ultracomputer <ref> [17] </ref> includes combining in its interconnection network. While software combining algorithms offer an alternative, they come at a price of extremely high latency at low contention levels. Our reactive fetch-and-op algorithm solves the latency problem at low contention levels and provides scalable throughput.
Reference: [18] <author> Allan Gottlieb, B. D. Lubachevsky, and Larry Rudolph. </author> <title> Basic Techniques for the Efficient Coordination of Very Large Numbers of Cooperating Sequential Processors. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 5(2) </volume> <pages> 164-189, </pages> <month> April </month> <year> 1983. </year>
Reference-contexts: In this simulation, TSP solves an 11-city tour. To ensure a deterministic amount of work, we seed the best path value with the optimal path. The global queue is based on an algorithm for a concurrent queue described in <ref> [18] </ref> that allows multiple processes simultaneous access to the queue. Fetch-and-increment operations synchronize access to the queue. Contention for the fetch-and-increment operation in this application depends on the number of processors. <p> Their results prescribe using a combining tree or butterfly network to combine arrival information and to signal barrier completion. Combining reduces contention and allows the algorithm to proceed in parallel. Observing that mutual exclusion has the undesired effect of serializing processes, Got-tlieb et al. <ref> [18] </ref> suggest a method of avoiding serialization by using a fetch-and-op operation. The advantage of fetch-and-op is that concurrent fetch-and-op operations to a single variable can be combined and can proceed in parallel. Goodman et al. [15] present a combining tree algorithm to compute fetch-and-op in parallel.
Reference: [19] <author> Gary Graunke and Shreekant Thakkar. </author> <title> Synchronization Algorithms for Shared-Memory Multiprocessors. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 60-70, </pages> <month> June </month> <year> 1990. </year> <month> 158 </month>
Reference-contexts: With small critical sections, this transient behavior dominates and read-polling can generate as much communication traffic as polling with test&set [5]. Recent research has resulted in more sophisticated protocols that alleviate the detrimental effects of contention <ref> [5, 19, 43] </ref>. The research demonstrates that test-and-set with randomized exponential backoff and queuing are the most promising spin-lock protocols. 28 Exponential Backoff Anderson [5] proposed exponential backoff as a way of reducing contention for spin locks. Agarwal and Cherian [2] independently proposed exponential backoff for reducing contention at barriers. <p> Queue locks have the additional advantage of providing fair access to the lock. Several queue lock protocols were developed independently by Anderson [5], Graunke and Thakkar <ref> [19] </ref>, and Mellor-Crummey and Scott [43]. All three protocols scale well with the level of contention. <p> This is due to the protocol cost of maintaining the queue of lock waiters. The results indicate that no single protocol has the best performance across contention levels. These measurements are consistent with previously reported results in <ref> [5, 43, 19] </ref>. It is clear that a reactive spin lock algorithm should select the test-and 32 test-and-set protocol when contention is low and the queue protocol when contention is high. The test-and-test-and-set protocol has the lowest overhead at low contention levels, and outperforms the test-and-set protocol. <p> In response to this problem of scaling to high contention levels, a recent area of research focuses on designing scalable algorithms that perform well under high contention. Research on spin locks by Anderson [5], Mellor-Crummey and Scott [43], and Graunke and Thakkar <ref> [19] </ref> show that the best approach to implementing spin locks under high contention is to enqueue lock waiters and service them one at a time. This prevents lock waiters from simultaneously recontending for the lock and reduces the detrimental effects of memory contention.
Reference: [20] <author> Rajiv Gupta and Charles R. Hill. </author> <title> A Scalable Implementation of Barrier Synchroniza--tion Using an Adaptive Combining Tree. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 18(3) </volume> <pages> 161-180, </pages> <year> 1989. </year>
Reference-contexts: However, if barrier arrival times are skewed, the use of a combining tree to accumulate barrier arrivals leads to a higher latency than a simple, centralized counter. This observation led Gupta and Hill <ref> [20] </ref> to propose an adaptive combining tree barrier that adapts the shape of the combining tree to the arrival patterns of the participating processes. They show that their algorithm leads to improved time complexities. However, their analysis of the algorithm ignores the run-time overhead of reconfiguring the combining tree. <p> If processes arrive skewed in time, the length of time in between barrier episodes will be sufficiently long that the reduction in latency for detecting the last process is insignificant. The adaptive combining tree does however show a performance advantage when used as a fuzzy barrier <ref> [20] </ref>. In a fuzzy barrier, a process waiting at the barrier can perform some useful computation that does not rely on completion of the barrier.
Reference: [21] <author> R.H. Halstead and T. Fujita. MASA: </author> <title> A Multithreaded Processor Architecture for Parallel Symbolic Computing. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 443-451, </pages> <address> New York, </address> <month> June </month> <year> 1988. </year> <note> IEEE. </note>
Reference-contexts: It accomplishes this by rapidly switching the processor to a different thread whenever a high-latency operation is encountered. While previous multithreaded designs switch contexts at every cycle <ref> [53, 21] </ref>, Alewife's multithreaded processor [1] switches contexts only on synchronization faults and remote cache misses. This style is called block multithreading [33] and has the advantage of high single thread performance.
Reference: [22] <author> Robert H. Halstead. </author> <title> Multilisp: A Language for Parallel Symbolic Computation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(4) </volume> <pages> 501-539, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: These instructions are directly supported by the Alewife CMMU, and are cache-coherent. * Alewife supports automatic detection of unresolved futures <ref> [22] </ref> by using the least significant bit of a data word as a tag bit, and by trapping on misaligned addresses in 24 25 memory access instructions. * Alewife allows software to directly access the underlying message layer [31], providing the opportunity for software to use messages to implement shared-memory synchronization <p> In research on waiting algorithms, we consider additional waiting mechanisms that are made possible through multithreading. We also consider the performance of waiting algorithms for several synchronization types, such as futures <ref> [22] </ref> and I-structures [6], that are implemented with tagging and full-empty bits. 2.2.4 Run-Time System Assumptions The scheduling policy and the run-time overhead of thread management have a significant impact on the performance of waiting algorithms. <p> The Poisson assumption is a useful approximation of the behavior of many complex systems, and helps to make analysis tractable. Producer-Consumer Synchronization Producer-consumer synchronization is performed between one producer and one or more consumers of the data produced. Examples of this type of synchronization include futures <ref> [22] </ref> and I-structures [6]. This form of producer-consumer synchronization is different from another form where only one consumer is allowed to consume the data. This second form of producer-consumer synchronization can be modeled as mutual-exclusion synchronization. <p> A write to a full slot signals an error. We allow a J-structure slot to be reset. A reset empties the slot, permitting multiple assignments. Reusing J-structure slots in this way allows efficient cache performance. J-structures can be used to implement I-structure [6] semantics. Futures Futures <ref> [22] </ref> are a method for specifying control parallelism. The expression (future X) specifies that the expression X may be executed in parallel with the current thread.
Reference: [23] <author> Maurice P. Herlihy and Jeanette M. Wing. </author> <title> Linearizability: A Correctness Condition for Concurrent Objects. </title> <type> Technical Report CMU-CS-88-120, </type> <institution> Carnegie-Mellon University, </institution> <month> March </month> <year> 1988. </year>
Reference-contexts: A concurrent execution consists of an interleaving of requests and responses from multiple processes at each object. However, only a subset of these possible interleavings represent correct executions. To aid in describing correct executions, we follow the example of Herlihy and Wing in <ref> [23] </ref>, and model a concurrent execution by a history. A history is a finite sequence of request and response events. A process history, HjP , of a history H is the subsequence of events in H whose process names are P . <p> There may be other means of satisfying C-serializability, and it would be useful to further develop the theory to allow a designer to verify whether his/her protocol satisfies it. A possible approach is to restrict our attention to linearizable implementations of concurrent objects <ref> [23] </ref>. Such objects can be specified using standard sequential axiomatic specifications. We can extend a sequential specification of a linearizable object in the following way. <p> We then specify two new operations to validate and invalidate the object. An implementation of this extended specification would need to serialize protocol changes, thus satisfying C-serializability. An algorithm designer can then use the methodology developed in <ref> [23] </ref> to determine if an implementation satisfies the extended specification. Policies for Switching Protocols Recall that a policy for deciding when to switch protocols first needs to monitor run-time conditions to decide which protocol is optimal for the current conditions.
Reference: [24] <author> Qin Huang. </author> <title> An Analysis of Concurrent Priority Queue Algorithms. </title> <type> Master's thesis, </type> <institution> EECS Department, Massachusetts Institute of Technology, </institution> <address> Cambridge, MA, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: Threads acquire and release mutexes at each network node as they traverse the network. 110 FibHeap tests an implementation of a scalable priority queue based on a Fibonacci heap <ref> [24] </ref>. Mutexes are used to ensure atomic updates to the heap. Scalability is achieved by distributing mutexes throughout the data structure. This avoids points of high lock contention and allows parallelism. The test involves repeatedly executing insert and extract-min operations on the priority queue.
Reference: [25] <author> Anna Karlin, Kai Li, Mark Manasse, and Susan Owicki. </author> <title> Empirical Studies of Competitive Spinning for A Shared-Memory Multiprocessor. </title> <booktitle> In 13th ACM Symposium on Operating Systems Principles (SOSP), </booktitle> <pages> pages 41-55, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: We observe that a shorter polling phase results in better performance than 2phase/1 in MGrid and Jacobi because producer arrival rates were low. Under such conditions, i.e., when &lt; 1=fiB, our theoretical analysis predicts that 2phase/0:5 will perform better than 2phase/1. Karlin et al. <ref> [25] </ref> also observe by analyzing measured waiting-time profiles that setting L poll to 0:5B can result in lower waiting costs. Surprisingly, 2phase/0:5 also performed better than signal. <p> They present a randomized two-phase waiting algorithm, where the length of the polling phase is randomly picked from a predetermined probability distribution. The randomized algorithm achieves an expected competitive factor of e=(e 1) 1:58. In a separate paper <ref> [25] </ref>, Karlin et al. performed an empirical study of several techniques for determining L poll in two-phase waiting algorithms for mutual exclusion locks.
Reference: [26] <author> Anna Karlin, Mark Manasse, Lyle McGeoch, and Susan Owicki. </author> <title> Competitive Randomized Algorithms for Non-Uniform Problems. </title> <booktitle> In Proceedings 1st Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 301-309, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: For example, setting L poll equal to the cost of blocking a thread yields a 2-competitive waiting algorithm. Karlin et al. <ref> [26] </ref> present a randomized algorithm for selecting L poll that achieves a competitive factor of 1.58. They also prove a lower bound of 1.58 on the competitive factor of any on-line waiting algorithm. This thesis investigates two-phase waiting algorithms in the context of a multiprocessing 16 system with lightweight threads. <p> The amortized analysis of on-line algorithms has been extensively studied in the theory community in recent years <ref> [9, 41, 26] </ref>. The objective has been to design on-line algorithms that are within a small constant factor of the performance of an optimal off-line algorithm that has complete knowledge of future requests. Karlin et al. [27] coined the term c-competitive to describe such algorithms. <p> In Chapter 3, we use an on-line algorithm by Borodin, Linial and Saks to design a 3-competitive algorithm for deciding when to change protocols. In Chapter 4, based on work by Karlin et al. <ref> [26] </ref>, we will use probabilistic analysis to design a 1.58-competitive algorithm for deciding between polling and signaling waiting mechanisms. 2.2 Experimental Platform To investigate the performance characteristics of reactive synchronization algorithms, we run a set of synthetic and application benchmarks that exercise the synchronization algorithms. <p> In <ref> [26] </ref>, Karlin et al. present randomized and adaptive methods for dynamically determining L poll . A drawback of these methods is that they incur a significant run-time overhead. Minimizing the run-time overhead for determining L poll is crucial in large-scale multiprocessors that support lightweight threads. <p> Using terminology in <ref> [26] </ref>, a strong adversary is one that chooses requests depending on the choices made by the algorithm in satisfying previous requests. A weak adversary is one that chooses requests without regard to the previous choices made by the algorithm. <p> If we weaken the adversary and consider expected costs, a dynamic two-phase waiting algorithm can achieve lower competitive factors. In <ref> [26] </ref>, Karlin et al. present a dynamic, randomized two-phase waiting algorithm with an expected competitive factor of e=(e1) 1:58 and prove this factor to be optimal for on-line algorithms against a weak adversary. <p> It follows that one cannot construct a two-phase algorithm with a competitive factor lower than e=(e 1). This competitive factor matches the lower bound obtained in <ref> [26] </ref> against a weak adversary. 2 In light of this lower bound, the natural question to ask is whether a single static value for ff can attain this lower bound under exponentially distributed waiting times. <p> They conclude that data dependence and multiprogramming does not significantly alter lock waiting times. However, for barrier synchronization, both data dependence and multiprogramming lead to sharply increased waiting times. Research by Karlin et al. <ref> [26] </ref> focuses on selecting L poll to optimize the performance of two-phase waiting. They present a randomized two-phase waiting algorithm, where the length of the polling phase is randomly picked from a predetermined probability distribution. The randomized algorithm achieves an expected competitive factor of e=(e 1) 1:58.
Reference: [27] <author> Anna Karlin, Mark Manasse, Larry Rudolph, and Daniel Sleator. </author> <title> Competitive Snoopy Caching. </title> <journal> Algorithmica, </journal> <volume> 3(1) </volume> <pages> 79-119, </pages> <year> 1988. </year>
Reference-contexts: The objective has been to design on-line algorithms that are within a small constant factor of the performance of an optimal off-line algorithm that has complete knowledge of future requests. Karlin et al. <ref> [27] </ref> coined the term c-competitive to describe such algorithms.
Reference: [28] <author> David Kranz, Beng-Hong Lim, Anant Agarwal, and Donald Yeung. </author> <title> Low-cost Support for Fine-Grain Synchronization in Multiprocessors. In Multithreaded Computer Architecture: A Summary of the State of the Art, </title> <booktitle> chapter 7, </booktitle> <pages> pages 139-166. </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1994. </year> <note> Also available as MIT Laboratory for Computer Science TM-470, </note> <month> June </month> <year> 1992. </year>
Reference-contexts: runs reliably at a clock speed of 20MHz. 2.2.3 Synchronization Support The Alewife multiprocessor was designed to provide a variety of mechanisms to help implement synchronization operations efficiently. * Alewife provides two basic hardware primitives for synchronization: an atomic fetch&store instruction, and a set of instructions that manipulate full-empty bits <ref> [53, 28] </ref> associated with each memory word. <p> It is implemented as a vector with full/empty bits associated with each vector slot. See <ref> [28] </ref> for further details. A reader of a J-structure slot waits until the slot is full before returning the value. A writer of a J-structure slot writes a value to the slot, sets it to full, and releases all waiters for the slot. <p> For example, barrier 127 synchronization is frequently used to enforce data-dependencies across phases of a pro-gram. However, barrier synchronization presents two major drawbacks: it requires global communication and it unnecessarily delays computation. Instead of barriers, programs can use data-level or point-to-point synchronization to enforce data dependencies. Kranz et al. <ref> [28] </ref> and Yeung and Agarwal [56] investigated the performance benefits of restructuring a program to use fine-grained synchronization. They also investigated the benefits of providing hardware support for efficient data-level synchronization.
Reference: [29] <author> David A. Kranz et al. </author> <title> ORBIT: An Optimizing Compiler for Scheme. </title> <booktitle> In Proceedings of SIGPLAN '86, Symposium on Compiler Construction, </booktitle> <month> June </month> <year> 1986. </year>
Reference-contexts: NWO is faithful enough to the hardware design that it exposed many Alewife hardware errors during the design phase. It also allowed us to implement Alewife's run-time system even before hardware was available. the ORBIT <ref> [29] </ref> compiler, an optimizing compiler for Scheme. The ORBIT compiler was extended for generating parallel code. The primary drawback of NWO is its slow simulation speed: it provides accuracy at the price of simulation speed. On a SPARCstation 10, it simulates approximately 2000 processor cycles per second.
Reference: [30] <author> Clyde Kruskal, Larry Rudolph, and Marc Snir. </author> <title> Efficient Synchronization on Multiprocessors with Shared Memory. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 10(4) </volume> <pages> 579-601, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: In our experiments, we use the version of the MCS lock that does not rely on compare-and-swap since Alewife does not have a compare&swap instruction. 3.1.2 Passive Fetch-and-Op Algorithms Fetch-and-op is a useful primitive for implementing higher-level synchronization operations. When the operation is combinable <ref> [30] </ref>, e.g., in fetch-and-add, combining techniques can be used to compute the operation in parallel. Fetch-and-op was deemed important enough for the designers of the NYU Ultracomputer [17] to include hardware support in its interconnection network for combining fetch-and-op requests to the same memory 30 location.
Reference: [31] <author> John Kubiatowicz and Anant Agarwal. </author> <title> Anatomy of a Message in the Alewife Multiprocessor. </title> <booktitle> In International Supercomputing Conference (ICS) 1993, </booktitle> <address> Tokyo, Japan, </address> <month> July </month> <year> 1993. </year> <journal> IEEE. </journal> <volume> 159 </volume>
Reference-contexts: The current implementation of the Alewife CMMU has 5 hardware directory pointers per cache line. The CMMU also interfaces the Sparcle processor to the interconnection network, allowing the use of an efficient message-passing interface for communication <ref> [31] </ref>. The LimitLESS protocol relies on this interface to handle coherence operations in software. <p> CMMU, and are cache-coherent. * Alewife supports automatic detection of unresolved futures [22] by using the least significant bit of a data word as a tag bit, and by trapping on misaligned addresses in 24 25 memory access instructions. * Alewife allows software to directly access the underlying message layer <ref> [31] </ref>, providing the opportunity for software to use messages to implement shared-memory synchronization operations. * Lastly, Alewife's processor implements a coarse-grained version of multithreading, called block multithreading [33], that can be used to lessen the cost of waiting for synchronization. <p> As in MP3D, we see that the higher latency of the MCS lock has a negligible impact on execution times. 3.6 Reactive Algorithms and Message-Passing Protocols In this section, we consider reactive algorithms that select between shared-memory and message-passing protocols. Recent architectures for scalable shared-memory multiprocessors <ref> [31, 34, 48] </ref> implement the shared-memory abstraction on top of a collection of processing nodes that communicate via messages through an interconnection network. They allow software to bypass the shared-memory abstraction and directly access the message layer .
Reference: [32] <author> John Kubiatowicz, David Chaiken, and Anant Agarwal. </author> <title> The Alewife CMMU: Ad--dressing the Multiprocessor Communications Gap. </title> <booktitle> In HOTCHIPS, </booktitle> <month> August </month> <year> 1994. </year>
Reference-contexts: Figure 2.2 illustrates the high-level organization of an Alewife node. Each node consists of a Sparcle processor [3], an FPU, 64KB of cache memory, a 4MB portion of globally-addressable memory, the Caltech MRC network router, and the Alewife Communications and Memory Management Unit (CMMU) <ref> [32] </ref>. The current prototype is designed to run at 33MHz. Sparcle is a modified SPARC processor that supports multithreading. Multiple threads can be resident on the processor at once, and the processor can context switch from one processor-resident thread to another in 14 cycles.
Reference: [33] <author> Kiyoshi Kurihara, David Chaiken, and Anant Agarwal. </author> <title> Latency Tolerance through Multithreading in Large-Scale Multiprocessors. </title> <booktitle> In Proceedings International Symposium on Shared Memory Multiprocessing, </booktitle> <address> Japan, April 1991. </address> <publisher> IPS Press. </publisher>
Reference-contexts: and by trapping on misaligned addresses in 24 25 memory access instructions. * Alewife allows software to directly access the underlying message layer [31], providing the opportunity for software to use messages to implement shared-memory synchronization operations. * Lastly, Alewife's processor implements a coarse-grained version of multithreading, called block multithreading <ref> [33] </ref>, that can be used to lessen the cost of waiting for synchronization. Except for fetch&store to serve as an atomic read-modify-write primitive, the contributions and conclusions of this research do not depend on the availability of these synchronization features of the Alewife multiprocessor. <p> It accomplishes this by rapidly switching the processor to a different thread whenever a high-latency operation is encountered. While previous multithreaded designs switch contexts at every cycle [53, 21], Alewife's multithreaded processor [1] switches contexts only on synchronization faults and remote cache misses. This style is called block multithreading <ref> [33] </ref> and has the advantage of high single thread performance. In this thesis, we considered multithreaded processors as providing additional waiting mechanisms to be selected by a waiting algorithm. 128 5.3 Concurrent Search Structure Algorithms A search structure algorithm implements the dictionary abstract data type.
Reference: [34] <author> J. Kuskin et al. </author> <title> The Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture (ISCA), </booktitle> <address> Chicago, IL, </address> <month> April </month> <year> 1994. </year> <note> IEEE. </note>
Reference-contexts: As in MP3D, we see that the higher latency of the MCS lock has a negligible impact on execution times. 3.6 Reactive Algorithms and Message-Passing Protocols In this section, we consider reactive algorithms that select between shared-memory and message-passing protocols. Recent architectures for scalable shared-memory multiprocessors <ref> [31, 34, 48] </ref> implement the shared-memory abstraction on top of a collection of processing nodes that communicate via messages through an interconnection network. They allow software to bypass the shared-memory abstraction and directly access the message layer .
Reference: [35] <author> Leslie Lamport. </author> <title> A Fast Mutual Exclusion Algorithm. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 5(1) </volume> <pages> 1-11, </pages> <month> February </month> <year> 1987. </year>
Reference-contexts: With this constraint, the best known mutual exclusion algorithms are either fast in the absence of contention but scale poorly, or slow in the absence of contention but scale well. Yang and Anderson designed an algorithm that adaptively selects between Lamport's fast mutual exclusion algorithm <ref> [35] </ref> and a scalable algorithm of their design. It selects Lamport's algorithm when there is absolutely no contention, and the scalable algorithm when any contention is detected. Choy and Singh use a filter as a building block for constructing mutual exclusion algorithms.
Reference: [36] <author> Butler Lampson, William Weihl, and Eric Brewer. </author> <booktitle> 6.826 Principles of Computer Systems. Research Seminar Series MIT/LCS/RSS 19, </booktitle> <institution> Massachusetts Institute of Technology, </institution> <month> July </month> <year> 1992. </year>
Reference-contexts: We use pseudo-code to provide readable descriptions of the specifications and implementations. However, since pseudo-code can be somewhat imprecise, we also provide more precise specifications and implementations of protocol objects and managers using the Spec language <ref> [36] </ref> in Appendix B. 3.2.2 Protocol Object Specification To allow a protocol manager to select and change protocols, we require that each protocol object supports operations to validate and invalidate itself, in addition to an operation to execute the synchronization protocol. <p> Appropriate feedback may allow the programmer or compiler to fix the choice of protocols for these two different types of locks in MP3D. 134 Appendix A An Overview of Spec This appendix gives a brief overview of the Spec language <ref> [36] </ref> that is used in Appendix B for specifying and describing several implementations of protocol selection algorithms. Spec is a language designed by Butler Lampson and William Weihl for writing specifications and the first few stages of successive refinement towards practical implementations of digital systems, all in a common syntax. <p> Spec provides a succinct notation for writing precise descriptions of sequential or concurrent systems, both sequential and concurrent. It is essentially a notation for describing allowable sequences of transitions of a state machine. A complete description of Spec's syntax and semantics is presented in <ref> [36] </ref>. This purpose of this overview is to aid the reader in understanding the Spec code. We concentrate on the features of Spec that are different from, or absent from conventional programming languages. The overview is largely derived from the handouts describing Spec in [36]. <p> syntax and semantics is presented in <ref> [36] </ref>. This purpose of this overview is to aid the reader in understanding the Spec code. We concentrate on the features of Spec that are different from, or absent from conventional programming languages. The overview is largely derived from the handouts describing Spec in [36]. Expressions and Commands The Spec language has two main constructs: an expression that describes how to compute a value as a function of other values (literal constants, or current values of state variables) without any side-effects, and a command that describes possible transitions of the state variables.
Reference: [37] <author> Charles E. Leiserson et al. </author> <title> The Network Architecture of the Connection Machine CM-5. </title> <booktitle> In The Fourth Annual ACM Symposium on Parallel Algorithms and Architectures. ACM, </booktitle> <year> 1992. </year>
Reference-contexts: The primary drawback of NWO is its slow simulation speed: it provides accuracy at the price of simulation speed. On a SPARCstation 10, it simulates approximately 2000 processor cycles per second. Fortunately, a parallel version of NWO runs on a Thinking Machines Corporation CM-5 <ref> [37] </ref>, allowing us to simulate a large number of processing nodes in a reasonable amount of time. A 16-node Alewife prototype recently became operational in June, 1994. We will present data from the real machine that validates some of the results gathered from the simulations.
Reference: [38] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. Lam. </author> <title> The Stanford Dash Multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Fetch-and-op was deemed important enough for the designers of the NYU Ultracomputer [17] to include hardware support in its interconnection network for combining fetch-and-op requests to the same memory 30 location. The Stanford DASH multiprocessor <ref> [38] </ref> supports fetch-and-increment and fetch--and-decrement directly in its cache coherence protocol, although without combining. In the absence of special hardware support, several software algorithms can be used to implement fetch-and-op. We consider the following in this paper. <p> Reactive algorithms provide a solution by deferring the choice of protocols to run-time. The reactive spin lock algorithm removes the need for special hardware support for queue locks. For example, the Stanford DASH multiprocessor <ref> [38] </ref> and the Wisconsin Multicube [15] both include hardware support for queuing lock waiters. Software queuing algorithms provide the same scalable performance as hardware queue locks, but they come at a price of higher lock latency in the absence of contention.
Reference: [39] <author> Beng-Hong Lim and Anant Agarwal. </author> <title> Waiting Algorithms for Synchronization in Large-Scale Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(3) </volume> <pages> 253-294, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: Waiting mechanism Awaiting mechanism waits for synchronization conditions to be satisfied. Spinning and blocking are two common waiting mechanisms. A mul-tithreaded processor may provide alternative waiting mechanisms, such as switch spinning and switch-blocking <ref> [39, 16] </ref>. Waiting time Synchronization waiting time is the interval from when a thread begins waiting on a synchronization condition to when the synchronization condition is satisfied and the waiting thread is allowed to proceed. <p> The uniform distribution is a reasonable model for barrier waiting times. Such waiting times would arise if inter-barrier thread execution lengths are uniformly distributed within some time interval. We also show in <ref> [39] </ref> that if barrier arrivals are generated by a Poisson process, then waiting times approach a uniform distribution. Mutual-Exclusion Mutual-exclusion synchronization provides exclusive access to data structures and critical sections of code. <p> Unfortunately, the resulting PDF of waiting times from such a model is sufficiently complex that it does not lend itself to a closed-form analysis. However, we note that under conditions of low lock contention, the queuing model predicts close to exponentially distributed waiting times. See <ref> [39] </ref> for a more detailed discussion. 4.5 Deriving Optimal Values for L poll The following analysis focuses on the exponential and uniform distributions as models for waiting time distributions. Section 4.6 presents empirical measurements of waiting times encountered in parallel applications that exhibit such waiting time distributions.
Reference: [40] <author> S. Lo and V. Gligor. </author> <title> A Comparative Analysis of Multiprocessor Scheduling Algorithms. </title> <booktitle> In 7th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 356-363. </pages> <publisher> IEEE, </publisher> <month> Sept. </month> <year> 1987. </year>
Reference-contexts: Ousterhout first proposed the two-phase waiting algorithm in his Medusa operating system [47]. The operating system implements two-phase waiting with a user-settable L poll . In a study of multiprocessor scheduling algorithms, Lo and Gligor <ref> [40] </ref> found that use of two-phase waiting improves the performance of group scheduling when L poll is set in between B and 2B, where B is the cost of blocking.
Reference: [41] <author> Mark S. Manasse, Lyle A. McGeoch, and Daniel D. Sleator. </author> <title> Competitive Algorithms for On-line Problems. </title> <booktitle> In Proceedings of the 20th Annual Symposium on Theory of Computing, </booktitle> <pages> pages 322-333, </pages> <address> Chicago, IL, </address> <month> May </month> <year> 1988. </year> <note> ACM. </note>
Reference-contexts: This places a limit on the complexity of the algorithms for detecting run-time conditions and deciding which protocol and waiting mechanism to use. Since dynamic protocol and waiting mechanism selection are instances of on-line problems, we rely on previous research on competitive algorithms <ref> [9, 41] </ref> to help design the reactive algorithms. An on-line problem is one in which an algorithm must process a sequence of requests without knowledge of future requests. <p> The amortized analysis of on-line algorithms has been extensively studied in the theory community in recent years <ref> [9, 41, 26] </ref>. The objective has been to design on-line algorithms that are within a small constant factor of the performance of an optimal off-line algorithm that has complete knowledge of future requests. Karlin et al. [27] coined the term c-competitive to describe such algorithms. <p> They also designed a 2n 1-competitive algorithm for task systems, where n is the number of states in the task system. To paraphrase the definition of a task system from <ref> [41] </ref>, A task system consists of a set of n states, a set of m tasks, an n by n state transition cost matrix D, where d ij is the cost of changing from state i to state j, and an n by m task cost matrix C, where c ij
Reference: [42] <author> Maurice Herlihy. </author> <title> Wait-Free Synchronization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 11(1) </volume> <pages> 124-149, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: We model a protocol selection algorithm as a protocol manager and a set of concurrent protocol objects, as illustrated in operations that allows it to be selected by a protocol manager. We define these operations further below. 1 These consensus objects are unrelated to Herlihy's wait-free consensus objects in <ref> [42] </ref>. 35 The protocol manager mediates concurrent access to the protocol objects and presents a conventional interface to the synchronizing processes. There are multiple instances of the protocol manager, one for each process. Informally, one can view the protocol manager as a procedure that is called by a synchronizing process.
Reference: [43] <author> John M. Mellor-Crummey and Michael L. Scott. </author> <title> Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(1) </volume> <pages> 21-65, </pages> <month> February </month> <year> 1991. </year> <month> 160 </month>
Reference-contexts: Although it is a simple and efficient protocol in the absence of contention, its performance degrades drastically under high contention. A remedy is a queuing protocol <ref> [43] </ref> that constructs a software queue of lock waiters to reduce memory contention. However, queuing comes at the price of a higher latency in the absence of contention. Anderson [5] observes that the choice between the two protocols depends on the level of lock contention. lock by Mellor-Crummey and Scott [43]. <p> <ref> [43] </ref> that constructs a software queue of lock waiters to reduce memory contention. However, queuing comes at the price of a higher latency in the absence of contention. Anderson [5] observes that the choice between the two protocols depends on the level of lock contention. lock by Mellor-Crummey and Scott [43]. We measured the overhead incurred by these spin lock algorithms on a simulation of the Alewife multiprocessor [4]. Each data point represents the average overhead incurred by the synchronization algorithm for each critical section with P processors contending for the lock. <p> Protocol A synchronization protocol implements a synchronization operation. For example, the MCS queue lock protocol <ref> [43] </ref> implements a mutual exclusion lock and a combining-tree protocol [57] implements a barrier. Waiting mechanism Awaiting mechanism waits for synchronization conditions to be satisfied. Spinning and blocking are two common waiting mechanisms. A mul-tithreaded processor may provide alternative waiting mechanisms, such as switch spinning and switch-blocking [39, 16]. <p> With small critical sections, this transient behavior dominates and read-polling can generate as much communication traffic as polling with test&set [5]. Recent research has resulted in more sophisticated protocols that alleviate the detrimental effects of contention <ref> [5, 19, 43] </ref>. The research demonstrates that test-and-set with randomized exponential backoff and queuing are the most promising spin-lock protocols. 28 Exponential Backoff Anderson [5] proposed exponential backoff as a way of reducing contention for spin locks. Agarwal and Cherian [2] independently proposed exponential backoff for reducing contention at barriers. <p> Queue locks have the additional advantage of providing fair access to the lock. Several queue lock protocols were developed independently by Anderson [5], Graunke and Thakkar [19], and Mellor-Crummey and Scott <ref> [43] </ref>. All three protocols scale well with the level of contention. However, the first two queue locks require space per lock proportional to the number of processors, and Anderson's queue lock has a high single-processor latency on machines that do not support atomic fetch&increment directly in hardware. <p> If so, it signals that successor, otherwise it empties the queue with a compare&swap instruction. An alternative version empties the queue with a fetch&store instruction, but requires more complicated code to handle a race condition. See <ref> [43] </ref> for a more complete description of the MCS lock. In our experiments, we use the version of the MCS lock that does not rely on compare-and-swap since Alewife does not have a compare&swap instruction. 3.1.2 Passive Fetch-and-Op Algorithms Fetch-and-op is a useful primitive for implementing higher-level synchronization operations. <p> This is due to the protocol cost of maintaining the queue of lock waiters. The results indicate that no single protocol has the best performance across contention levels. These measurements are consistent with previously reported results in <ref> [5, 43, 19] </ref>. It is clear that a reactive spin lock algorithm should select the test-and 32 test-and-set protocol when contention is low and the queue protocol when contention is high. The test-and-test-and-set protocol has the lowest overhead at low contention levels, and outperforms the test-and-set protocol. <p> In response to this problem of scaling to high contention levels, a recent area of research focuses on designing scalable algorithms that perform well under high contention. Research on spin locks by Anderson [5], Mellor-Crummey and Scott <ref> [43] </ref>, and Graunke and Thakkar [19] show that the best approach to implementing spin locks under high contention is to enqueue lock waiters and service them one at a time. This prevents lock waiters from simultaneously recontending for the lock and reduces the detrimental effects of memory contention. <p> This prevents lock waiters from simultaneously recontending for the lock and reduces the detrimental effects of memory contention. Mellor-Crummey and Scott measured the performance of a number of scalable spin-barrier algorithms in <ref> [43] </ref>. Their results prescribe using a combining tree or butterfly network to combine arrival information and to signal barrier completion. Combining reduces contention and allows the algorithm to proceed in parallel.
Reference: [44] <author> Eric Mohr, David A. Kranz, and Robert H. Halstead. </author> <title> Lazy Task Creation: A Technique for Increasing the Granularity of Parallel Programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 264-280, </pages> <month> Jul </month> <year> 1991. </year>
Reference-contexts: The synchronization structure of the program can be most easily viewed as a recursive function call tree with synchronization occurring at each node of the tree. The program was dynamically partitioned with lazy task creation <ref> [44] </ref>. Queens solves the n-queens problem: given an n fi n chess board, place n queens such that no two queens are on the same row, column, or diagonal. <p> This problem with deadlock is not present for unmatched Queens and Factor because they are dynamically partitioned with lazy task creation <ref> [44] </ref>. signal performs reasonably well except for matched Jacobi which has very short waiting times. Barrier Synchronization Table 4.4 summarizes the detailed simulation results for barrier synchronization.
Reference: [45] <author> Bodhisattwa Mukherjee and Karsten Schwan. </author> <title> Improving Performance by Use of Adaptive Objects: Experimentation with a Configurable Multiprocessor Thread Package. </title> <booktitle> In Proceedings of the 2nd International Symposium on High Performance Distributed Computing, </booktitle> <pages> pages 59-66, </pages> <month> July </month> <year> 1993. </year> <note> Also available as Technical Report GIT-CC-93/17, </note> <institution> Georgia Institute of Technology. </institution>
Reference-contexts: Here, we describe some recent research in using adaptivity to improve the performance of operating system functions and synchronization operations. Reconfigurable Operating Systems Mukherjee and Schwan <ref> [45] </ref> provide an overview of reconfigurable operating systems. The general idea is to provide hooks into an operating system so that application programs can dynamically control certain parameters of operating system services and improve performance.
Reference: [46] <author> John Nguyen. </author> <title> Compiler Analysis to Implement Point-To-Point Synchronization in Parallel Programs. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <address> Cambridge, MA, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: They found that restructuring the program to use fine-grained synchronization instead of barriers improves performance by a factor of three due to increased parallelism. Hardware support for fine-grained, data-level synchronization in the form of full/empty bits [53] yields an additional 40% performance improvement. Nguyen <ref> [46] </ref> used compiler analysis to transform statically partitioned DOALL loops to use point-to-point communication between processors instead of global barriers. Conventional implementations of DOALL loops use a barrier at the end of each DOALL loop to enforce data-dependencies across DOALL loops. However, barriers enforce unnecessary dependencies across all the processors.
Reference: [47] <author> John K. Ousterhout. </author> <title> Scheduling Techniques for Concurrent Systems. </title> <booktitle> In 3rd International Conference on Distributed Computing Systems, </booktitle> <pages> pages 22-30, </pages> <year> 1982. </year>
Reference-contexts: Since a polling mechanism incurs a cost that is proportional to the waiting time, while a signaling mechanism incurs a fixed cost, the choice between a polling and a signaling mechanism depends on the length of the waiting time <ref> [47] </ref>. Short waiting times favor polling mechanisms while long waiting times favor signaling mechanisms. For example, the cost of blocking a thread on the Alewife multiprocessor is about 500 cycles. <p> Since this is another instance of an on-line problem, competitive techniques can be used to bound the worst case cost of a waiting algorithm. A popular algorithm for selecting waiting mechanisms is the two-phase waiting algorithm <ref> [47] </ref>, where a waiting thread first polls until the cost of polling reaches a limit L poll . If further waiting is necessary, the thread resorts to a signaling mechanism and incurs a fixed cost. The choice of L poll is key to the performance of a two-phase waiting algorithm. <p> We place constraints on the distribution of waiting times, thus constraining the run-lengths of wait requests to be 93 selected from a probability distribution. 4.3 Two-Phase Waiting Algorithms An on-line algorithm that chooses between polling and signaling mechanisms is the two-phase waiting algorithm, first suggested by Ousterhout in <ref> [47] </ref>. In a two-phase waiting algorithm a waiting thread first uses a polling mechanism to wait until the cost of polling reaches a limit L poll . <p> Research in this area has designed waiting algorithms that make 124 intelligent run-time choices between spinning and blocking. Ousterhout first proposed the two-phase waiting algorithm in his Medusa operating system <ref> [47] </ref>. The operating system implements two-phase waiting with a user-settable L poll .
Reference: [48] <author> Steven K. Reinhardt, James R. Larus, and David A. Wood. Tempest and Typhoon: </author> <title> User-Level Shared Memory. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture (ISCA), </booktitle> <address> Chicago, IL, </address> <month> April </month> <year> 1994. </year> <note> IEEE. </note>
Reference-contexts: As in MP3D, we see that the higher latency of the MCS lock has a negligible impact on execution times. 3.6 Reactive Algorithms and Message-Passing Protocols In this section, we consider reactive algorithms that select between shared-memory and message-passing protocols. Recent architectures for scalable shared-memory multiprocessors <ref> [31, 34, 48] </ref> implement the shared-memory abstraction on top of a collection of processing nodes that communicate via messages through an interconnection network. They allow software to bypass the shared-memory abstraction and directly access the message layer .
Reference: [49] <author> Michael L. Scott and John M. Mellor-Crummey. </author> <title> Fast, Contention-Free Combining Tree Barriers. </title> <type> Technical Report TR-429, </type> <institution> University of Rochester, </institution> <month> June </month> <year> 1992. </year>
Reference-contexts: They show that their algorithm leads to improved time complexities. However, their analysis of the algorithm ignores the run-time overhead of reconfiguring the combining tree. In later work <ref> [49] </ref>, Scott and Mellor-Crummey investigated the performance of Gupta and Hill's adaptive combining-tree barriers and found that the adaptive combining tree fails to outperform conventional tree and dissemination barriers.
Reference: [50] <author> Z. Segall and L. Rudolph. </author> <title> Dynamic Decentralized Cache Schemes for MIMD Parallel Processors. </title> <booktitle> In 11th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 340-347. </pages> <publisher> IEEE, </publisher> <month> June </month> <year> 1984. </year>
Reference-contexts: The test-and-test-and-set Algorithm Segall and Rudolph <ref> [50] </ref> proposed the test-and-test-and-set algorithm for reducing bus or network traffic on cache-coherent machines.
Reference: [51] <author> Dennis Shasha and Nathan Goodman. </author> <title> Concurrent Search Structure Algorithms. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 13(1) </volume> <pages> 53-90, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: In this thesis, we considered multithreaded processors as providing additional waiting mechanisms to be selected by a waiting algorithm. 128 5.3 Concurrent Search Structure Algorithms A search structure algorithm implements the dictionary abstract data type. In <ref> [51] </ref>, Shasha and Goodman present a framework for designing and verifying concurrent search structure algorithms. They exploit the semantics of the dictionary abstract data type to design and verify highly concurrent search structure algorithms.
Reference: [52] <author> J.P. Singh, W.-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title> <type> Technical Report CSL-TR-92-526, </type> <institution> Stanford University, </institution> <month> June </month> <year> 1992. </year>
Reference-contexts: Nevertheless, the reactive spin lock achieves performance that is close to the best passive algorithm, and should be useful for applications that perform locking frequently and at a very fine grain such that lock latencies becomes a concern. 76 MP3D MP3D is part of the SPLASH parallel benchmark suite <ref> [52] </ref>. For this simulation, we use problem sizes of 3,000 and 10,000 particles and turn on the locking option in MP3D. We measure the time taken for 5 iterations. MP3D uses locks for atomic updating for cell parameters, where a cell represents a discretization of space.
Reference: [53] <author> B.J. Smith. </author> <title> Architecture and Applications of the HEP Multiprocessor Computer System. </title> <journal> Society of Photocoptical Instrumentation Engineers, </journal> <volume> 298 </volume> <pages> 241-248, </pages> <year> 1981. </year>
Reference-contexts: runs reliably at a clock speed of 20MHz. 2.2.3 Synchronization Support The Alewife multiprocessor was designed to provide a variety of mechanisms to help implement synchronization operations efficiently. * Alewife provides two basic hardware primitives for synchronization: an atomic fetch&store instruction, and a set of instructions that manipulate full-empty bits <ref> [53, 28] </ref> associated with each memory word. <p> They also investigated the benefits of providing hardware support for efficient data-level synchronization. They found that restructuring the program to use fine-grained synchronization instead of barriers improves performance by a factor of three due to increased parallelism. Hardware support for fine-grained, data-level synchronization in the form of full/empty bits <ref> [53] </ref> yields an additional 40% performance improvement. Nguyen [46] used compiler analysis to transform statically partitioned DOALL loops to use point-to-point communication between processors instead of global barriers. Conventional implementations of DOALL loops use a barrier at the end of each DOALL loop to enforce data-dependencies across DOALL loops. <p> It accomplishes this by rapidly switching the processor to a different thread whenever a high-latency operation is encountered. While previous multithreaded designs switch contexts at every cycle <ref> [53, 21] </ref>, Alewife's multithreaded processor [1] switches contexts only on synchronization faults and remote cache misses. This style is called block multithreading [33] and has the advantage of high single thread performance.
Reference: [54] <author> Thorsten von Eicken, David Culler, Seth Goldstein, and Klaus Schauser. </author> <title> Active Messages: A Mechanism for Integrated Communication and Computation. </title> <booktitle> In 19th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: The advantage of using message-passing to implement synchronization operations over shared-memory is that under high contention, message-passing results in more efficient communication patterns, and atomicity is easily provided by making message handlers atomic with respect to other message handlers <ref> [54] </ref>. For example, fetch-and-op can be implemented by allocating the fetch-and-op variable in a private memory location of some processing node. To perform a fetch-and-op, a process sends a message to the processor 77 associated with that memory location.
Reference: [55] <author> Jae-Heon Yang and James H. Anderson. </author> <title> Fast, Scalable Synchronization with Minimal Hardware Support. </title> <booktitle> In 12th Symposium on Principles of Distributed Computing (PODC), </booktitle> <address> Ithaca, NY, </address> <year> 1993. </year> <journal> ACM. </journal> <volume> 161 </volume>
Reference-contexts: Reactive synchronization algorithms take a more general approach and deal with the harder problem of allowing the synchronization protocol itself to be changed. Adaptive Mutual Exclusion Recent research by Yang and Anderson <ref> [55] </ref>, and Choy and Singh [13] designed adaptive algorithms for mutual exclusion in the context of shared-memory multiprocessors that provide only atomic read and write primitives. They tackle the classic mutual exclusion problem of reducing the time complexity of implementing mutual exclusion with only atomic reads and writes.
Reference: [56] <author> Donald Yeung and Anant Agarwal. </author> <title> Experience with Fine-Grain Synchronization in MIMD Machines for Preconditioned Conjugate Gradient. </title> <booktitle> In Symposium on Principles and Practice of Parallel Programming (PPoPP), </booktitle> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year> <note> ACM. </note>
Reference-contexts: However, barrier synchronization presents two major drawbacks: it requires global communication and it unnecessarily delays computation. Instead of barriers, programs can use data-level or point-to-point synchronization to enforce data dependencies. Kranz et al. [28] and Yeung and Agarwal <ref> [56] </ref> investigated the performance benefits of restructuring a program to use fine-grained synchronization. They also investigated the benefits of providing hardware support for efficient data-level synchronization. They found that restructuring the program to use fine-grained synchronization instead of barriers improves performance by a factor of three due to increased parallelism.
Reference: [57] <author> Pen-Chung Yew, Nian-Feng Tzeng, and Duncan H. Lawrie. </author> <title> Distributing Hot-Spot Addressing in Large-Scale Multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(4):388-395, </volume> <month> April </month> <year> 1987. </year>
Reference-contexts: Protocol A synchronization protocol implements a synchronization operation. For example, the MCS queue lock protocol [43] implements a mutual exclusion lock and a combining-tree protocol <ref> [57] </ref> implements a barrier. Waiting mechanism Awaiting mechanism waits for synchronization conditions to be satisfied. Spinning and blocking are two common waiting mechanisms. A mul-tithreaded processor may provide alternative waiting mechanisms, such as switch spinning and switch-blocking [39, 16]. <p> To execute a fetch-and-op, a process acquires the lock, updates the value of the fetch-and-op variable, and releases the lock. Software Combining Tree A drawback of a centralized, lock-based implementation of fetch-and-op is that it may unnecessarily serialize fetch-and-op operations. Software combining protocols <ref> [57] </ref> can be used to compute the fetch-and-op in parallel. The idea is to combine multiple operations from different processes into a single operation whenever possible. The fetch-and-op variable is stored in the root of a software combining tree, and combining takes place at the internal nodes of the tree. <p> Barriers Barriers ensure that all participating threads have reached a point in a program before proceeding. To avoid excessive traffic to a single location, and to distribute the enqueuing and release operations, we use software combining trees <ref> [57] </ref> to implement barriers. 4.6.2 Benchmarks The experiments use benchmarks that are representative of producer-consumer, barrier, and mutual-exclusion synchronization. Table 4.2 lists the benchmarks and indicates the synchronization types in each of the benchmarks. Blocking only makes sense if there is another runnable thread to execute.
Reference: [58] <author> J. Zahorjan and E. Lazowska. </author> <title> Spinning Versus Blocking in Parallel Systems with Uncertainty. </title> <type> Technical Report TR-88-03-01, </type> <institution> Dept. of Computer Science, University of Washington, </institution> <address> Seattle, WA, </address> <month> Mar </month> <year> 1988. </year> <month> 162 </month>
Reference-contexts: Therefore, with accurate information about U we can attain a competitive factor of 4=3 as illustrated in Figure 4.5. However, it is hard to predict U since barrier waiting times are highly dependent on run-time factors <ref> [58] </ref>. If we cannot reliably predict U , we should choose 2phase/0:62 to obtain the best competitive factor of 1.62 (the golden ratio), as prescribed by Theorem 4, and as illustrated in Figure 4.5. <p> This thesis shows that the effectiveness of two-phase waiting depends on both the distribution of waiting times and the setting of L poll . Zahorjan et al. <ref> [58] </ref> studied the effect of data dependence and multiprogramming on waiting times for locks and barrier synchronization, and showed that waiting times can be highly dependent on run-time factors. They conclude that data dependence and multiprogramming does not significantly alter lock waiting times.
References-found: 58

