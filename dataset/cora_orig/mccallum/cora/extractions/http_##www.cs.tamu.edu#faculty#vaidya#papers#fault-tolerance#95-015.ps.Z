URL: http://www.cs.tamu.edu/faculty/vaidya/papers/fault-tolerance/95-015.ps.Z
Refering-URL: http://www.cs.tamu.edu/faculty/vaidya/Vaidya-ftc.html
Root-URL: http://www.cs.tamu.edu
Email: E-mail: vaidya@cs.tamu.edu  
Phone: Phone: (409) 845-0512 FAX: (409) 847-8578  
Title: On Checkpoint Latency  
Author: Nitin H. Vaidya 
Address: College Station, TX 77843-3112  
Affiliation: Department of Computer Science Texas A&M University  
Abstract: Technical Report 95-015 March 1995 fl Abstract Checkpointing and rollback is a technique to minimize the loss of computation in the presence of failures. Two metrics can be used to characterize a checkpointing scheme: (i) checkpoint overhead (increase in the execution time of the application because of a checkpoint), and (ii) checkpoint latency (duration of time required to save the checkpoint). For many checkpointing methods, checkpoint latency is larger than checkpoint overhead. The contributions of this report are as follows: 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. M. Chandy, J. C. Browne, C. W. Dissly, and W. R. Uhrig, </author> <title> "Analytic models for rollback and recovery strategies in data base systems," </title> <journal> IEEE Trans. Softw. Eng., </journal> <volume> vol. 1, </volume> <pages> pp. 100-110, </pages> <month> March </month> <year> 1975. </year>
Reference-contexts: Such applications can encounter loss of a significant amount of computation if a failure occurs during the execution. Checkpointing and rollback recovery is a technique used to minimize the loss of computation in an environment subject to failures <ref> [1] </ref>. A checkpoint is a copy of the application's state stored on a stable storage a stable storage is not subject to failures. The application periodically saves checkpoints; the application recovers from a failure by rolling back to a recent checkpoint. <p> In many implementations, checkpoint latency is larger than the checkpoint overhead. (Section 2 illustrates this with an example.) We denote checkpoint latency as L. In the past, a large number of researchers have analyzed the checkpointing and rollback recovery scheme (for instance, to determine the optimal checkpoint interval) <ref> [1, 4, 5, 7, 8, 12, 16] </ref>. This report evaluates the impact of checkpoint latency on the performance of a checkpointing scheme. The contributions of this report are as follows: 1. <p> When a processor fails, its local state is corrupted. A processor can 1 Chandy et al. <ref> [1] </ref> present an analysis of checkpointing schemes that does not take checkpoint latency into account. However, for sequential checkpointing (with L = C), our analysis is similar to theirs with one exception. An assumption made by Chandy et al. [1] implies that a failure that occurs while checkpoint CP2 (in Figure <p> A processor can 1 Chandy et al. <ref> [1] </ref> present an analysis of checkpointing schemes that does not take checkpoint latency into account. However, for sequential checkpointing (with L = C), our analysis is similar to theirs with one exception. An assumption made by Chandy et al. [1] implies that a failure that occurs while checkpoint CP2 (in Figure 5) is being saved, only requires re-initiation of the checkpointing operation. <p> However, the analysis presented in the report can be used to obtain an approximate estimate of performance overhead of distributed consistent checkpointing schemes. Appendix The analysis presented below is similar to <ref> [1] </ref>. Consider the overhead ratio r as a function of T .
Reference: [2] <author> K. M. Chandy and L. Lamport, </author> <title> "Distributed snapshots: Determining global states in distributed systems," </title> <journal> ACM Trans. Comp. Syst., </journal> <volume> vol. 3, </volume> <pages> pp. 63-75, </pages> <month> February </month> <year> 1985. </year> <title> 5 e (T+C) = 1 + (T + C) + 2 (T + C) 2 =2 </title> + . 
Reference-contexts: Checkpointing can be used for sequential as well as parallel (or distributed) applications. When the application consists of more than one process, a consistent checkpointing algorithm can be used to save a consistent state of the multi-process application <ref> [2, 6, 11] </ref>. Two metrics can be used to characterize a checkpointing scheme: * Checkpoint overhead: Checkpoint overhead is the increase in the execution time of the application because of a checkpoint. <p> When an application consists of multiple processes in a distributed environment, we assume that that application periodically takes consistent checkpoints <ref> [2, 11, 15] </ref>. A consistent checkpoint consists of one checkpoint per process, and possibly a few messages logged on the stable storage. <p> In other words, in such a system, different processes may have different overhead and latency. Figure 12 illustrates an application that uses the Chandy-Lamport algorithm <ref> [2] </ref> for consistent checkpointing. (The "marker" messages used by the algorithm are not shown in the figure.) As shown in the figure, let us assume that the application consists of three processes P1, P2 and P3 that are located on three processors in a distributed system. <p> Just as there are different approaches for checkpointing uni-process applications, there are different schemes for consistent checkpointing of multi-process applications <ref> [2, 3, 6, 11, 15] </ref>, that 17 achieve different overhead and latency.
Reference: [3] <author> E. N. Elnozahy, D. B. Johnson, and W. Zwaenepoel, </author> <title> "The performance of consistent check--pointing," </title> <booktitle> in Symposium on Reliable Distributed Systems, </booktitle> <year> 1992. </year>
Reference-contexts: The report concludes with Section 7. 2 Checkpoint Latency We limit the initial discussion and analysis to uni-process applications. We will address multi-process applications in Section 6. For uni-process applications, checkpointing schemes have been proposed that achieve a low checkpoint overhead, while resulting in a large checkpoint latency <ref> [11, 3, 9] </ref>. In this section, we illustrate the distinction between checkpoint latency and checkpoint overhead with two examples of checkpointing schemes for uni-process applications. Sequential checkpointing is an approach for which checkpoint overhead is identical to checkpoint latency. <p> Just as there are different approaches for checkpointing uni-process applications, there are different schemes for consistent checkpointing of multi-process applications <ref> [2, 3, 6, 11, 15] </ref>, that 17 achieve different overhead and latency.
Reference: [4] <author> E. Gelenbe and D. Derochette, </author> <title> "Performance of rollback recovery systems under intermittent failures," </title> <journal> Comm. ACM, </journal> <volume> vol. 21, </volume> <pages> pp. 493-499, </pages> <month> June </month> <year> 1978. </year>
Reference-contexts: In many implementations, checkpoint latency is larger than the checkpoint overhead. (Section 2 illustrates this with an example.) We denote checkpoint latency as L. In the past, a large number of researchers have analyzed the checkpointing and rollback recovery scheme (for instance, to determine the optimal checkpoint interval) <ref> [1, 4, 5, 7, 8, 12, 16] </ref>. This report evaluates the impact of checkpoint latency on the performance of a checkpointing scheme. The contributions of this report are as follows: 1.
Reference: [5] <author> E. Gelenbe, </author> <title> "On the optimum checkpointing interval," </title> <journal> J. ACM, </journal> <volume> vol. 2, </volume> <pages> pp. 259-270, </pages> <month> April </month> <year> 1979. </year>
Reference-contexts: In many implementations, checkpoint latency is larger than the checkpoint overhead. (Section 2 illustrates this with an example.) We denote checkpoint latency as L. In the past, a large number of researchers have analyzed the checkpointing and rollback recovery scheme (for instance, to determine the optimal checkpoint interval) <ref> [1, 4, 5, 7, 8, 12, 16] </ref>. This report evaluates the impact of checkpoint latency on the performance of a checkpointing scheme. The contributions of this report are as follows: 1.
Reference: [6] <author> R. Koo and S. Toueg, </author> <title> "Checkpointing and rollback-recovery for distributed systems," </title> <journal> IEEE Trans. Softw. Eng., </journal> <volume> vol. 13, </volume> <pages> pp. 23-31, </pages> <month> January </month> <year> 1987. </year>
Reference-contexts: Checkpointing can be used for sequential as well as parallel (or distributed) applications. When the application consists of more than one process, a consistent checkpointing algorithm can be used to save a consistent state of the multi-process application <ref> [2, 6, 11] </ref>. Two metrics can be used to characterize a checkpointing scheme: * Checkpoint overhead: Checkpoint overhead is the increase in the execution time of the application because of a checkpoint. <p> Just as there are different approaches for checkpointing uni-process applications, there are different schemes for consistent checkpointing of multi-process applications <ref> [2, 3, 6, 11, 15] </ref>, that 17 achieve different overhead and latency.
Reference: [7] <author> V. G. Kulkarni, V. F. Nicola, and K. S. Trivedi, </author> <title> "Effects of checkpointing and queueing on program performance," </title> <journal> Commun. Statist.-Stochastic Models, </journal> <volume> vol. 4, no. 6, </volume> <pages> pp. 615-648, </pages> <year> 1990. </year>
Reference-contexts: In many implementations, checkpoint latency is larger than the checkpoint overhead. (Section 2 illustrates this with an example.) We denote checkpoint latency as L. In the past, a large number of researchers have analyzed the checkpointing and rollback recovery scheme (for instance, to determine the optimal checkpoint interval) <ref> [1, 4, 5, 7, 8, 12, 16] </ref>. This report evaluates the impact of checkpoint latency on the performance of a checkpointing scheme. The contributions of this report are as follows: 1.
Reference: [8] <author> P. L'Ecuyer and J. Malenfant, </author> <title> "Computing optimal checkpointing strategies for rollback and recovery systems," </title> <journal> IEEE Trans. Computers, </journal> <volume> vol. 37, </volume> <pages> pp. 491-496, </pages> <month> April </month> <year> 1988. </year>
Reference-contexts: In many implementations, checkpoint latency is larger than the checkpoint overhead. (Section 2 illustrates this with an example.) We denote checkpoint latency as L. In the past, a large number of researchers have analyzed the checkpointing and rollback recovery scheme (for instance, to determine the optimal checkpoint interval) <ref> [1, 4, 5, 7, 8, 12, 16] </ref>. This report evaluates the impact of checkpoint latency on the performance of a checkpointing scheme. The contributions of this report are as follows: 1.
Reference: [9] <author> K. Li, J. F. Naughton, and J. S. Plank, </author> <title> "Low-latency, concurrent checkpointing for parallel programs," </title> <journal> IEEE Trans. Par. Distr. Syst., </journal> <volume> vol. 5, </volume> <pages> pp. 874-879, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: For equi-distant checkpoints, the optimal checkpoint interval is shown to be independent of the checkpoint latency (L). 1 Related work: Plank et al. <ref> [11, 10, 9] </ref> present measurements of checkpoint latency and overhead for a few applications, however, they do not present any performance analysis. We also measured checkpoint latency and overhead for a few uni-process applications, and briefly analyzed the impact of checkpoint latency on performance of "two-level" recovery schemes [14]. <p> The report concludes with Section 7. 2 Checkpoint Latency We limit the initial discussion and analysis to uni-process applications. We will address multi-process applications in Section 6. For uni-process applications, checkpointing schemes have been proposed that achieve a low checkpoint overhead, while resulting in a large checkpoint latency <ref> [11, 3, 9] </ref>. In this section, we illustrate the distinction between checkpoint latency and checkpoint overhead with two examples of checkpointing schemes for uni-process applications. Sequential checkpointing is an approach for which checkpoint overhead is identical to checkpoint latency. <p> The "measured L" curve in Figure 11 plots checkpoint overhead and latency measured for a merge sort program using four different checkpointing schemes the data is borrowed from Li et al. <ref> [9] </ref>. One of the four schemes is sequential checkpointing with overhead C max = 31 seconds.
Reference: [10] <author> J. S. Plank, M. Beck, G. Kingsley, and K. Li, "Libckpt: </author> <title> Transparent checkpointing under Unix," </title> <booktitle> in Usenix Winter 1995 Technical Conference, </booktitle> <address> New Orleans, </address> <month> January </month> <year> 1995. </year>
Reference-contexts: For equi-distant checkpoints, the optimal checkpoint interval is shown to be independent of the checkpoint latency (L). 1 Related work: Plank et al. <ref> [11, 10, 9] </ref> present measurements of checkpoint latency and overhead for a few applications, however, they do not present any performance analysis. We also measured checkpoint latency and overhead for a few uni-process applications, and briefly analyzed the impact of checkpoint latency on performance of "two-level" recovery schemes [14]. <p> Sequential checkpointing is an approach for which checkpoint overhead is identical to checkpoint latency. In this approach, when an application process wants to take a checkpoint, it pauses and saves its state on the stable storage <ref> [10] </ref>. The process continues execution only after the state is completely saved on the stable storage. Therefore, the time required to save the checkpoint (i.e., checkpoint latency) is practically identical to the increase in the execution time of the process (i.e., checkpoint overhead). Figure 1 illustrates this approach. <p> However, it results in a larger checkpoint overhead as compared to other approaches. Forked checkpointing is an approach for which checkpoint overhead is usually much smaller than the checkpoint latency. In this approach, when a process wants to take a checkpoint, it forks a child process <ref> [10] </ref>. The state of the child process is identical to that of the parent process when fork is performed. After the fork, the parent process continues computation, while the child process saves its state on the stable storage. Figure 2 (a) illustrates this approach.
Reference: [11] <author> J. S. Plank, </author> <title> Efficient Checkpointing on MIMD Architectures. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Princeton University, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: Checkpointing can be used for sequential as well as parallel (or distributed) applications. When the application consists of more than one process, a consistent checkpointing algorithm can be used to save a consistent state of the multi-process application <ref> [2, 6, 11] </ref>. Two metrics can be used to characterize a checkpointing scheme: * Checkpoint overhead: Checkpoint overhead is the increase in the execution time of the application because of a checkpoint. <p> For equi-distant checkpoints, the optimal checkpoint interval is shown to be independent of the checkpoint latency (L). 1 Related work: Plank et al. <ref> [11, 10, 9] </ref> present measurements of checkpoint latency and overhead for a few applications, however, they do not present any performance analysis. We also measured checkpoint latency and overhead for a few uni-process applications, and briefly analyzed the impact of checkpoint latency on performance of "two-level" recovery schemes [14]. <p> The report concludes with Section 7. 2 Checkpoint Latency We limit the initial discussion and analysis to uni-process applications. We will address multi-process applications in Section 6. For uni-process applications, checkpointing schemes have been proposed that achieve a low checkpoint overhead, while resulting in a large checkpoint latency <ref> [11, 3, 9] </ref>. In this section, we illustrate the distinction between checkpoint latency and checkpoint overhead with two examples of checkpointing schemes for uni-process applications. Sequential checkpointing is an approach for which checkpoint overhead is identical to checkpoint latency. <p> When an application consists of multiple processes in a distributed environment, we assume that that application periodically takes consistent checkpoints <ref> [2, 11, 15] </ref>. A consistent checkpoint consists of one checkpoint per process, and possibly a few messages logged on the stable storage. <p> Just as there are different approaches for checkpointing uni-process applications, there are different schemes for consistent checkpointing of multi-process applications <ref> [2, 3, 6, 11, 15] </ref>, that 17 achieve different overhead and latency.
Reference: [12] <author> K. Shin, T.-H. Lin, and Y.-H. Lee, </author> <title> "Optimal checkpointing of real-time tasks," </title> <journal> IEEE Trans. Computers, </journal> <volume> vol. 36, </volume> <pages> pp. 1328-1341, </pages> <month> November </month> <year> 1987. </year>
Reference-contexts: In many implementations, checkpoint latency is larger than the checkpoint overhead. (Section 2 illustrates this with an example.) We denote checkpoint latency as L. In the past, a large number of researchers have analyzed the checkpointing and rollback recovery scheme (for instance, to determine the optimal checkpoint interval) <ref> [1, 4, 5, 7, 8, 12, 16] </ref>. This report evaluates the impact of checkpoint latency on the performance of a checkpointing scheme. The contributions of this report are as follows: 1.
Reference: [13] <author> K. S. Trivedi, </author> <title> Probability and Statistics with Reliability, Queueing and Computer Science Applications. </title> <publisher> Prentice-Hall, </publisher> <year> 1988. </year>
Reference-contexts: Let denote the expected (average) execution time of an interval. Then, it is easy to see that, overhead ratio r = lim t!1 t = T Expected execution time of a single interval can be evaluated using the 3-state discrete Markov chain <ref> [13, 17] </ref> presented in Figure 7. State 0 is the initial state, when an interval starts execution. A transition from state 0 to state 1 occurs if the interval is completed without a failure.
Reference: [14] <author> N. H. Vaidya, </author> <title> "Another two-level failure recovery scheme: Performance impact of checkpoint placement and checkpoint latency," </title> <type> Tech. Rep. 94-068, </type> <institution> Computer Science Department, Texas A&M University, College Station, </institution> <month> December </month> <year> 1994. </year> <note> Available via anonymous ftp from ftp.cs.tamu.edu in directory /pub/vaidya. </note>
Reference-contexts: We also measured checkpoint latency and overhead for a few uni-process applications, and briefly analyzed the impact of checkpoint latency on performance of "two-level" recovery schemes <ref> [14] </ref>. This report presents results on the impact of checkpoint latency on traditional checkpointing and rollback schemes. This report is organized as follows. Section 2 illustrates the difference between checkpoint overhead and latency by means of two checkpointing schemes. Section 3 discusses how to model latency.
Reference: [15] <author> N. H. Vaidya, </author> <title> "Consistent logical checkpointing," </title> <type> Tech. Rep. 94-051, </type> <institution> Computer Science Department, Texas A&M University, College Station, </institution> <month> July </month> <year> 1994. </year> <note> Available via anonymous ftp from ftp.cs.tamu.edu in directory /pub/vaidya. </note>
Reference-contexts: When an application consists of multiple processes in a distributed environment, we assume that that application periodically takes consistent checkpoints <ref> [2, 11, 15] </ref>. A consistent checkpoint consists of one checkpoint per process, and possibly a few messages logged on the stable storage. <p> Just as there are different approaches for checkpointing uni-process applications, there are different schemes for consistent checkpointing of multi-process applications <ref> [2, 3, 6, 11, 15] </ref>, that 17 achieve different overhead and latency.
Reference: [16] <author> J. W. Young, </author> <title> "A first order approximation to the optimum checkpoint interval," </title> <journal> Comm. ACM, </journal> <volume> vol. 17, </volume> <pages> pp. 530-531, </pages> <month> September </month> <year> 1974. </year>
Reference-contexts: In many implementations, checkpoint latency is larger than the checkpoint overhead. (Section 2 illustrates this with an example.) We denote checkpoint latency as L. In the past, a large number of researchers have analyzed the checkpointing and rollback recovery scheme (for instance, to determine the optimal checkpoint interval) <ref> [1, 4, 5, 7, 8, 12, 16] </ref>. This report evaluates the impact of checkpoint latency on the performance of a checkpointing scheme. The contributions of this report are as follows: 1. <p> We used this approximation when plotting g (C). Young <ref> [16] </ref> previously obtained this approximate expression for T c by a somewhat different analysis. 14 Consider the g (C) curve for C max = 25 in Figure 10 (a).
Reference: [17] <author> A. Ziv and J. Bruck, </author> <title> "Analysis of checkpointing schemes for multiprocessor systems," </title> <type> Tech. Rep. RJ 9593, </type> <institution> IBM Almaden Research Center, </institution> <month> November </month> <year> 1993. </year> <month> 20 </month>
Reference-contexts: Let denote the expected (average) execution time of an interval. Then, it is easy to see that, overhead ratio r = lim t!1 t = T Expected execution time of a single interval can be evaluated using the 3-state discrete Markov chain <ref> [13, 17] </ref> presented in Figure 7. State 0 is the initial state, when an interval starts execution. A transition from state 0 to state 1 occurs if the interval is completed without a failure.
References-found: 17

