URL: ftp://ftp.cs.wisc.edu/math-prog/tech-reports/93-ga.ps
Refering-URL: http://www.cs.wisc.edu/math-prog/tech-reports/
Root-URL: 
Email: eja2@phx.cam.ac.uk  ferris@cs.wisc.edu  
Title: Genetic Algorithms for Combinatorial Optimization: The Assembly Line Balancing Problem  
Author: Edward J. Anderson Michael C. Ferris 
Keyword: Subject classifications: Programming: combinatorial optimization, parallel processing. Production/Scheduling: assembly line balancing. Keywords: Genetic algorithms; combinatorial optimization; parallel processing; assembly line balancing.  
Date: August 1991 Revised June 1992 Revised January 1993  
Address: Mill Lane, Cambridge CB2 1RX, England  Madison, Wisconsin 53706  
Affiliation: University of Cambridge Judge Institute of Management Studies  Computer Sciences Department University of Wisconsin  
Abstract: Genetic algorithms are one example of the use of a random element within an algorithm for combinatorial optimization. We consider the application of the genetic algorithm to a particular problem, the Assembly Line Balancing Problem. A general description of genetic algorithms is given, and their specialized use on our test-bed problems is discussed. We carry out extensive computational testing to find appropriate values for the various parameters associated with this genetic algorithm. These experiments underscore the importance of the correct choice of a scaling parameter and mutation rate to ensure the good performance of a genetic algorithm. We also describe a parallel implementation of the genetic algorithm and give some comparisons between the parallel and serial implementations. Both versions of the algorithm are shown to be effective in producing good solutions for problems of this type (with appropriately chosen parameters). 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E.H.L. Aarts, P.J.M. van Laarhoven, and N.L.J. Ulder, </author> <year> 1991. </year> <title> Local-search-based algorithms for job shop scheduling. CQM-Not 106, Centre for Quantitative Methods, </title> <institution> Nederlandse Philips Bedrijven B.V., </institution> <address> P.O. Box 218, NL-5600 MD Eindhoven. </address>
Reference-contexts: Simulated Annealing [23, 9, 14] has this property, as does Tabu Search [12, 17] in some implementations. There has also been some recent work which has considered techniques which combine local search and the genetic algorithm ideas outlined above. For example Aarts et al. <ref> [1] </ref> have shown that such a technique is competitive with Simulated Annealing for some job shop scheduling problems. <p> Assign to each individual an interval proportional to its fitness, and scaled so that the total length of all the intervals is N . Consider the intervals laid end to end along the line from 0 to N . Choose a random number x uniformly 7 on <ref> [0; 1] </ref> and put the individuals corresponding to the intervals in which the points x; x + 1; x + 2; :::x + N lie into the mating pool. Pairs of individuals are then removed from the mating pool and recombined using crossover and mutation.
Reference: [2] <author> E.J. Anderson and M.C. Ferris, </author> <year> 1990. </year> <title> A genetic algorithm for the assembly line balancing problem. </title> <booktitle> In Proceedings of the Integer Programming / Combinatorial Optimization Conference, </booktitle> <address> Waterloo, Ontario, Canada, </address> <month> May 28-30. </month> <institution> University of Waterloo Press. </institution>
Reference-contexts: This may be dictated to some extent by the architecture of the parallel processor we will wish to avoid passing messages which require transmission through large numbers of processors. We have carried out experiments with a number of neighborhood structures <ref> [2] </ref> without coming to a firm conclusion. In the experiments that we report below we have used a neighborhood structure we call ring8. <p> Our strategy was to replace the current individual with its best offspring provided this offspring is better than the worst individual in the neighborhood. The question arises whether it is possible to use the other offspring? We have carried out some experiments in order to answer this <ref> [2] </ref> and it seems that the performance of the algorithm is not degraded by throwing the less fit offspring away. This is the policy which is adopted in the experiments reported below.
Reference: [3] <author> A.L. Arcus, </author> <year> 1966. </year> <title> COMSOAL: A computer method of sequencing operations for assembly lines. In E.S. </title> <editor> Buffa, editor, </editor> <booktitle> Readings in Production and Operations Management. </booktitle> <publisher> John Wiley & Sons, </publisher> <address> New York. </address>
Reference-contexts: The first scheme generated the initial population entirely at random. In order that one can effectively program a genetic code quickly, this would be the method of preference for generating initial solutions. In the second scheme we generated a set of initial solutions using a method due to Arcus <ref> [3] </ref> . This is extremely effective. In a significant proportion of cases the initial set of Arcus solutions contains at least one which is never improved upon by the GA and in the other cases the best of the Arcus solutions is never far from the best solution found.
Reference: [4] <author> J.D. Bagley, </author> <year> 1967. </year> <title> The Behaviour of Adaptive Systems which employ Genetic and Correlation Algorithms. </title> <type> PhD thesis, </type> <institution> University of Michigan. </institution>
Reference-contexts: 0 Introduction Algorithms based on genetic ideas were first used to solve optimization problems more than twenty years ago <ref> [4] </ref> following the development of the fundamental ideas of genetic algorithms by John Holland at the University of Michigan. During the 1970's this work continued, but was largely unknown.
Reference: [5] <author> J.E. Baker. </author> <title> Reducing bias and inefficiency in the selection algorithm. </title> <booktitle> In Grefenstette [15], </booktitle> <pages> pp. 14-21. </pages>
Reference-contexts: It is better to ensure that any individual with above average fitness is automatically selected. The method we use to achieve this is called Stochastic Universal Sampling and is described in detail by Baker <ref> [5] </ref> . In brief, the procedure can be described as follows. Take the members of the population and reorder them randomly. Assign to each individual an interval proportional to its fitness, and scaled so that the total length of all the intervals is N .
Reference: [6] <editor> R.K. Belew, editor, </editor> <booktitle> 1991. Proceedings of the Fourth International Conference on Genetic Algorithms, </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, California. </address>
Reference-contexts: During the 1970's this work continued, but was largely unknown. In the last few years, however, there has been increasing interest in genetic algorithms (see for example the conference proceedings <ref> [25, 6] </ref> and the books [7, 8, 13] A number of researchers have looked at the application of genetic algorithms to optimization of nonlinear functions; our interest, however, is in the application of this technique to combinatorial optimization problems.
Reference: [7] <editor> L.D. Davis, editor, </editor> <year> 1987. </year> <title> Genetic Algorithms and Simulated Annealing. </title> <publisher> Pitman, London. </publisher>
Reference-contexts: During the 1970's this work continued, but was largely unknown. In the last few years, however, there has been increasing interest in genetic algorithms (see for example the conference proceedings [25, 6] and the books <ref> [7, 8, 13] </ref> A number of researchers have looked at the application of genetic algorithms to optimization of nonlinear functions; our interest, however, is in the application of this technique to combinatorial optimization problems.
Reference: [8] <author> L.D. Davis, </author> <year> 1990. </year> <title> The Handbook of Genetic Algorithms. </title> <publisher> Van Nostrand Reinhold. </publisher>
Reference-contexts: During the 1970's this work continued, but was largely unknown. In the last few years, however, there has been increasing interest in genetic algorithms (see for example the conference proceedings [25, 6] and the books <ref> [7, 8, 13] </ref> A number of researchers have looked at the application of genetic algorithms to optimization of nonlinear functions; our interest, however, is in the application of this technique to combinatorial optimization problems.
Reference: [9] <author> R.W. Eglese, </author> <year> 1990. </year> <title> Simulated annealing: A tool for operational research. </title> <journal> European Journal of Operations Research 46, </journal> <pages> 271-281. </pages>
Reference-contexts: There has been an increasing interest in the use of search techniques which contain a stochastic element as methods for the solution of hard combinatorial optimization problems. Simulated Annealing <ref> [23, 9, 14] </ref> has this property, as does Tabu Search [12, 17] in some implementations. There has also been some recent work which has considered techniques which combine local search and the genetic algorithm ideas outlined above.
Reference: [10] <author> T.C. Fogarty, </author> <year> 1989. </year> <title> Varying the probability of mutation in the genetic algorithm. </title> <booktitle> In Schaeffer [25], </booktitle> <pages> pp. 422-427. </pages>
Reference-contexts: In effect our method benefits from a higher mutation rate early in the run, without this interfering with the convergence of the algorithm at the later stages. This approach thus lends support to the suggestion (see for example <ref> [10] </ref>) that a reduction in mutation during the course of a run may be beneficial, in a similar way to the reduction in "temperature" which is used in simulated annealing methods. 11 Table 1: p c = 0:6 | - 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 big 0.005 b
Reference: [11] <author> M. Georges-Schleuter, </author> <year> 1989. </year> <title> Asparagos: An asynchronous parallel genetic algorithm. </title> <booktitle> In Schaeffer [25], </booktitle> <pages> pp. 416-421. </pages>
Reference-contexts: For a parallel implementation, we assume that each individual in the population resides on a processor (with one or more individuals assigned to each processor) and communication is carried out by message passing. For descriptions of other implementations of parallel genetic algorithms see the references <ref> [11, 19, 24, 26, 28] </ref> We consider first the mechanisms that would be involved in an effective parallelization of the genetic algorithm we have described above. <p> Recently, this type of approach has been successfully used by a number of researchers, see for example the work of H. Muhlenbein and M. Gorges-Schleuter who have developed the ASPARAGOS system to implement an asynchronous parallel genetic algorithm <ref> [11, 24] </ref> A model algorithm for a scheme in which fitness information is only compared locally is displayed in Exhibit II. Exhibit II .
Reference: [12] <author> F. Glover, </author> <year> 1989. </year> <title> Tabu search part 1. </title> <journal> ORSA Journal on Computing 1, </journal> <pages> 190-206. </pages>
Reference-contexts: There has been an increasing interest in the use of search techniques which contain a stochastic element as methods for the solution of hard combinatorial optimization problems. Simulated Annealing [23, 9, 14] has this property, as does Tabu Search <ref> [12, 17] </ref> in some implementations. There has also been some recent work which has considered techniques which combine local search and the genetic algorithm ideas outlined above. For example Aarts et al. [1] have shown that such a technique is competitive with Simulated Annealing for some job shop scheduling problems.
Reference: [13] <author> D.E. Goldberg, </author> <year> 1989. </year> <title> Genetic Algorithms in Search, Optimization and Machine Learning. </title> <publisher> Addison-Wesley, </publisher> <address> Reading MA. </address>
Reference-contexts: During the 1970's this work continued, but was largely unknown. In the last few years, however, there has been increasing interest in genetic algorithms (see for example the conference proceedings [25, 6] and the books <ref> [7, 8, 13] </ref> A number of researchers have looked at the application of genetic algorithms to optimization of nonlinear functions; our interest, however, is in the application of this technique to combinatorial optimization problems.
Reference: [14] <author> B.L. Golden and C.C. Skiscim, </author> <year> 1989. </year> <title> Using simulated annealing to solve routing and location problems. </title> <journal> Naval Research Logistics Quarterly 33, </journal> <pages> 261-279. </pages>
Reference-contexts: There has been an increasing interest in the use of search techniques which contain a stochastic element as methods for the solution of hard combinatorial optimization problems. Simulated Annealing <ref> [23, 9, 14] </ref> has this property, as does Tabu Search [12, 17] in some implementations. There has also been some recent work which has considered techniques which combine local search and the genetic algorithm ideas outlined above.
Reference: [15] <editor> J.J. Grefenstette, editor, </editor> <booktitle> 1987. Genetic Algorithms and their Applications: Proceedings of the Second International Conference on Genetic Algorithms, </booktitle> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, New Jersey. </address>
Reference: [16] <author> J.J. Grefenstette, </author> <year> 1987. </year> <title> Incorporating problem specific knowledge into genetic algorithms. </title> <note> In Davis [7]. </note>
Reference-contexts: There has been a variety of work done in this area, much of it considering the application of genetic algorithms to the solution of the travelling salesman problem <ref> [19, 29, 16] </ref> This is not surprising given the importance of the travelling salesman problem and its frequent use as a vehicle for testing new methods in combinatorial optimization. <p> We have made some limited experiments with different crossover mechanisms with the aim of incorporating some problem specific knowledge. This has been shown to be effective in some previous studies <ref> [16] </ref> . For our problem, however, these approaches do not appear to lead to any substantial improvements over the more standard crossover mechanism described in the introduction.
Reference: [17] <author> A. Hertz and D. de Werra, </author> <year> 1987. </year> <title> Using Tabu search techniques for graph coloring. </title> <booktitle> Computing 29, </booktitle> <pages> 345-351. </pages>
Reference-contexts: There has been an increasing interest in the use of search techniques which contain a stochastic element as methods for the solution of hard combinatorial optimization problems. Simulated Annealing [23, 9, 14] has this property, as does Tabu Search <ref> [12, 17] </ref> in some implementations. There has also been some recent work which has considered techniques which combine local search and the genetic algorithm ideas outlined above. For example Aarts et al. [1] have shown that such a technique is competitive with Simulated Annealing for some job shop scheduling problems.
Reference: [18] <author> P. Jog, J.Y. Suh, and D. Van Gucht. </author> <title> The effects of population size, heuristic crossover and local improvement on a genetic algorithm for the travelling salesman problem. </title> <booktitle> In Schaeffer [25], </booktitle> <pages> pp. 110-115. </pages>
Reference-contexts: With these parameter settings we then achieved best solutions after 350 generations which were on average 32% away from the best ever found (see Table 5). We note that the results given above support the conclusions given in <ref> [18] </ref> that the combination of selection and crossover is far more effective for optimization than an exclusive use of either technique.
Reference: [19] <author> P. Jog, J.Y. Suh, and D. Van Gucht, </author> <year> 1991. </year> <title> Parallel genetic algorithms applied to the travelling salesman problem. </title> <journal> SIAM Journal on Optimization 1, </journal> <pages> 515-5291. </pages>
Reference-contexts: There has been a variety of work done in this area, much of it considering the application of genetic algorithms to the solution of the travelling salesman problem <ref> [19, 29, 16] </ref> This is not surprising given the importance of the travelling salesman problem and its frequent use as a vehicle for testing new methods in combinatorial optimization. <p> For a parallel implementation, we assume that each individual in the population resides on a processor (with one or more individuals assigned to each processor) and communication is carried out by message passing. For descriptions of other implementations of parallel genetic algorithms see the references <ref> [11, 19, 24, 26, 28] </ref> We consider first the mechanisms that would be involved in an effective parallelization of the genetic algorithm we have described above.
Reference: [20] <author> D.S. Johnson, C.R. Aragon, L.A. McGeoch, and C. Schevon, </author> <year> 1989. </year> <title> Optimization by simulated annealing: An experimental evaluation; part I, graph partitioning. </title> <journal> Operations Research 37, </journal> <pages> 865-892. </pages>
Reference-contexts: For example Aarts et al. [1] have shown that such a technique is competitive with Simulated Annealing for some job shop scheduling problems. Also, recent careful computational work by Johnson et al. <ref> [20] </ref> has demonstrated that simulated annealing, for example, can be competitive with the best available heuristic methods for certain very large combinatorial optimization problems. This is the context in which the genetic algorithm approach should be evaluated.
Reference: [21] <author> R. Johnson, </author> <year> 1988. </year> <title> Optimally balancing large assembly lines with FABLE. </title> <booktitle> Management Science 34, </booktitle> <pages> 240-253. </pages>
Reference-contexts: Stations 2 and 3 have total processing times of 42 and 33 minutes respectively. The ALBP has attracted the attention of many researchers. Both heuristic and exact methods have been proposed for its solution. For a review of some of these methods see the papers <ref> [27, 21] </ref>. Note that the ALBP is sometimes posed with the total operation time for each station constrained by some upper bound (the desired "cycle time") and the number of stations as the variable to be 4 minimized.
Reference: [22] <author> K.A. De Jong, </author> <year> 1975. </year> <title> An Analysis of the Behavior of a Class of Genetic Adaptive Systems. </title> <type> PhD thesis, </type> <institution> University of Michigan. </institution> <note> Dissertation Abstracts International 36(10), 5140B. </note>
Reference-contexts: Many previous implementations of this method have been guided by experiments initially carried out by <ref> [22] </ref> and later extended by a number of other authors (for example [25]). In the great majority of these experiments nonlinear function optimization problems have been solved rather than combinatorial optimization problems.
Reference: [23] <author> M. Lundy and A. Mees, </author> <year> 1986. </year> <title> Convergence of an annealing algorithm. </title> <booktitle> Mathematical Programming 34, </booktitle> <pages> 111-124. </pages>
Reference-contexts: There has been an increasing interest in the use of search techniques which contain a stochastic element as methods for the solution of hard combinatorial optimization problems. Simulated Annealing <ref> [23, 9, 14] </ref> has this property, as does Tabu Search [12, 17] in some implementations. There has also been some recent work which has considered techniques which combine local search and the genetic algorithm ideas outlined above.
Reference: [24] <author> H. Muhlenbein, </author> <year> 1989. </year> <title> Parallel genetic algorithms, population genetics and combinatorial optimization. </title> <booktitle> In Schaeffer [25], </booktitle> <pages> pp. 416-421. </pages>
Reference-contexts: For a parallel implementation, we assume that each individual in the population resides on a processor (with one or more individuals assigned to each processor) and communication is carried out by message passing. For descriptions of other implementations of parallel genetic algorithms see the references <ref> [11, 19, 24, 26, 28] </ref> We consider first the mechanisms that would be involved in an effective parallelization of the genetic algorithm we have described above. <p> Recently, this type of approach has been successfully used by a number of researchers, see for example the work of H. Muhlenbein and M. Gorges-Schleuter who have developed the ASPARAGOS system to implement an asynchronous parallel genetic algorithm <ref> [11, 24] </ref> A model algorithm for a scheme in which fitness information is only compared locally is displayed in Exhibit II. Exhibit II .
Reference: [25] <editor> J.D. Schaeffer, editor, </editor> <booktitle> 1989. Proceedings of the Third International Conference on Genetic Algorithms, </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, California. </address>
Reference-contexts: During the 1970's this work continued, but was largely unknown. In the last few years, however, there has been increasing interest in genetic algorithms (see for example the conference proceedings <ref> [25, 6] </ref> and the books [7, 8, 13] A number of researchers have looked at the application of genetic algorithms to optimization of nonlinear functions; our interest, however, is in the application of this technique to combinatorial optimization problems. <p> Many previous implementations of this method have been guided by experiments initially carried out by [22] and later extended by a number of other authors (for example <ref> [25] </ref>). In the great majority of these experiments nonlinear function optimization problems have been solved rather than combinatorial optimization problems. We believe that there is much more to be learnt about the best implementation of genetic algorithms for combinatorial problems. The paper consists of two parts.
Reference: [26] <author> J.Y. Suh and D. Van Gucht, </author> <year> 1987. </year> <title> Distributed genetic algorithms. </title> <type> Technical Report 225, </type> <institution> Computer Science Department, Indiana University, Bloomington. </institution>
Reference-contexts: For a parallel implementation, we assume that each individual in the population resides on a processor (with one or more individuals assigned to each processor) and communication is carried out by message passing. For descriptions of other implementations of parallel genetic algorithms see the references <ref> [11, 19, 24, 26, 28] </ref> We consider first the mechanisms that would be involved in an effective parallelization of the genetic algorithm we have described above.
Reference: [27] <author> F.B. Talbot, J.H. Patterson, and W.V. Gehrlein, </author> <year> 1986. </year> <title> A comparative evaluation of heuristic line balancing techniques. </title> <booktitle> Management Science 32, </booktitle> <pages> 430-454. 22 </pages>
Reference-contexts: Stations 2 and 3 have total processing times of 42 and 33 minutes respectively. The ALBP has attracted the attention of many researchers. Both heuristic and exact methods have been proposed for its solution. For a review of some of these methods see the papers <ref> [27, 21] </ref>. Note that the ALBP is sometimes posed with the total operation time for each station constrained by some upper bound (the desired "cycle time") and the number of stations as the variable to be 4 minimized.
Reference: [28] <author> R. Tanese, </author> <year> 1987. </year> <title> Parallel genetic algorithms for a hypercube. </title> <booktitle> In Grefenstette [15], </booktitle> <pages> pp. 177-183. </pages>
Reference-contexts: For a parallel implementation, we assume that each individual in the population resides on a processor (with one or more individuals assigned to each processor) and communication is carried out by message passing. For descriptions of other implementations of parallel genetic algorithms see the references <ref> [11, 19, 24, 26, 28] </ref> We consider first the mechanisms that would be involved in an effective parallelization of the genetic algorithm we have described above.
Reference: [29] <author> D. Whitley, T. Starkweather, and D. Fuquay, </author> <year> 1989. </year> <title> Scheduling problems and travelling salesmen: The genetic edge recombination operator. </title> <booktitle> In Schaeffer [25], </booktitle> <pages> pp. 133-140. 23 </pages>
Reference-contexts: There has been a variety of work done in this area, much of it considering the application of genetic algorithms to the solution of the travelling salesman problem <ref> [19, 29, 16] </ref> This is not surprising given the importance of the travelling salesman problem and its frequent use as a vehicle for testing new methods in combinatorial optimization.
References-found: 29

