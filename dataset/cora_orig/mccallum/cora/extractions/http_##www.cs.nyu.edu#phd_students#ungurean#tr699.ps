URL: http://www.cs.nyu.edu/phd_students/ungurean/tr699.ps
Refering-URL: http://www.cs.nyu.edu/phd_students/ungurean/index.html
Root-URL: http://www.cs.nyu.edu
Title: Run-time versus Compile-time Instruction Scheduling in Superscalar (RISC) Processors: Performance and Tradeoffs  
Author: Allen Leung Krishna V. Palem Cristian Ungureanu 
Keyword: Key words: Compile-time Optimizations, Dynamic Schedulers, Instruction Scheduling, Program Traces, Scope, Superscalar Processors  
Abstract: The RISC revolution has spurred the development of processors with increasing levels of instruction level parallelism (ILP). In order to realize the full potential of these processors, multiple instructions must be issued and executed in a single cycle. Consequently, instruction scheduling plays a crucial role as an optimization in this context. While early attempts at instruction scheduling were limited to compile-time approaches, the recent trend is to provide dynamic support in hardware. In this paper, we present the results of a detailed comparative study of the performance advantages to be derived by the spectrum of instruction scheduling approaches: from limited basic-block schedulers in the compiler, to novel and aggressive run-time schedulers in hardware. A significant portion of our experimental study via simulations, is devoted to understanding the performance advantages of run-time scheduling. Our results indicate it to be effective in extracting the ILP inherent to the program trace being scheduled, over a wide range of machine and program parameters. Furthermore, we also show that this effectiveness can be further enhanced by a simple basic-block scheduler in the compiler, which optimizes for the presence of the run-time scheduler in the target; current basic-block schedulers are not designed to take advantage of this feature. We demonstrate this fact by presenting a novel enhanced basic-block scheduler in this paper. Finally, we outline a simple analytical characterization of the performance advantage, that run-time schedulers have to offer. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Ramon D. Acosta and Jacob Kjelstrup and H. C. Torng. </author> <title> An instruction Issuing Approach to Enhancing Performance in Multiple Functional Unit Processors IEEE Transactions on Computers, </title> <address> C-35(9):815-828, </address> <year> 1986 </year>
Reference: [2] <editor> F. Allen, B. Rosen, and K. Zadeck (editors). </editor> <booktitle> Optimization in Compilers, </booktitle> <publisher> ACM Press and Addison-Wesley (to appear). </publisher>
Reference: [3] <author> D. Bernstein and I. Gertner. </author> <title> Scheduling Expressions on a Pipelined Processor with a Maximal Delay of One Cycle In ACM TOPLAS 11(1), </title> <journal> pp. </journal> <pages> 57-66, </pages> <year> 1989 </year>
Reference: [4] <author> D. Bernstein and M. Rodeh. </author> <title> Global Instruction Scheduling for Superscalar Machines. </title> <booktitle> In Proceedings of SIGPLAN'91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pp. 241-255, </pages> <year> 1991. </year>
Reference: [5] <author> E. G. Coffman Jr. and R. Graham. </author> <title> Optimal Scheduling for Two Processor Systems ActaInform. </title> <booktitle> 1 1971, </booktitle> <pages> pp. 200-213 </pages>
Reference: [6] <author> P. Dubey and G. B. Adams, III and M. Flynn. </author> <title> Instruction Window Size Trade-Offs and Characterization of program Parallelism In IEEE Transactions on Computers 43(4) 1994 </title>
Reference: [7] <author> R. Cohn and T. Gross and M. Lam and P. S. Tseng. </author> <title> Architecture and Compiler Trade-offs for a Long Instruction Word Microprocessor In Proceedings of ASPLOS III, </title> <month> Apr. </month> <year> 1989, </year> <pages> pp. 2-14 </pages>
Reference-contexts: In this case, the processors have special hardware support <ref> [27, 22, 7] </ref> to analyze the current program dependences "on-the-fly" and schedule as many independent instructions as possible on a given cycle. Consequently, in these modern processors and in the interest of finding multiple independent instructions that can be co-scheduled, invariably S &gt; m.
Reference: [8] <author> Keith Dieterdorf and Rich Oehler and Ron Hochsprung. </author> <title> Evolution of the PowerPC Architecture IEEE Micro pp. </title> <month> 34-49 </month> <year> 1994 </year>
Reference: [9] <author> J. Fisher. </author> <title> Trace Scheduling: A General Technique for Global Microcode Compaction. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-30(7):478-490, </volume> <year> 1981. </year>
Reference-contexts: In either case, and independent of whether the compiler or the hardware performs the code-motion, these side-effects require expensive repair, leading to overheads, as detailed in Figure 20; for a complete discussion of these issues, please see <ref> [9] </ref>. In this figure, the top row details code motion where the instruction goes from being executed sometimes since it is on one of the two conditional branches, to being executed always after the code-motion.
Reference: [10] <author> W. M. Johnson. </author> <title> Superscalar Microprocessor Design Prentice Hall, </title> <address> Engelwood Cliffs, NY, </address> <year> 1991 </year> <month> 26 </month>
Reference: [11] <author> Joseph Fisher. </author> <title> Global Code Generation for Instruction-level Parallelism: Trace Scheduling-2 1991 </title>
Reference-contexts: We will also permit similar freedom to global compile-time instruction scheduling on the trace. In contrast, branches (and merges) are treated differently since the corresponding overheads can be prohibitive <ref> [11] </ref>. Consequently, current approaches to scheduling in both the compiler as well as the run-time hardware, is to limit the relative reordering of these instructions completely.
Reference: [12] <author> J. Fisher. J. Ellis, J. Ruttenberg, and A. Nicolau. </author> <title> Parallel Processing: A Smart Compiler and a Dumb Machine. </title> <booktitle> In Proceedings of SIGPLAN`84 Symposium on Compiler Construction, </booktitle> <pages> pp. 37-47, </pages> <year> 1984. </year>
Reference: [13] <author> J. Goodman and W. Hsu. </author> <title> Code Scheduling and Register Allocation in Large Basic Blocks. </title> <booktitle> In Proceedings of ACM Conference on Supercomputing, </booktitle> <pages> pp. 442-452, </pages> <year> 1988. </year>
Reference: [14] <author> Linley Gwennap. </author> <title> 620 Fills Out PowerPC Product Line Microprocessor Report, </title> <month> October </month> <year> 1994 </year>
Reference-contexts: at time step 2, even though both b and t are ready and in the scope, only b can be issued because c is not ready. 4.4 Out-of-order Execution and Hardware Lookahead As noted before, the trend in modern superscalar RISC processors is to have varying levels of run-time scheduling <ref> [14, 15] </ref>. In this case, the processors have special hardware support [27, 22, 7] to analyze the current program dependences "on-the-fly" and schedule as many independent instructions as possible on a given cycle.
Reference: [15] <author> Linley Gwennap. </author> <title> MIPS R10000 Uses Decoupled Architecture Microprocessor Report, </title> <month> October </month> <year> 1994 </year>
Reference-contexts: at time step 2, even though both b and t are ready and in the scope, only b can be issued because c is not ready. 4.4 Out-of-order Execution and Hardware Lookahead As noted before, the trend in modern superscalar RISC processors is to have varying levels of run-time scheduling <ref> [14, 15] </ref>. In this case, the processors have special hardware support [27, 22, 7] to analyze the current program dependences "on-the-fly" and schedule as many independent instructions as possible on a given cycle.
Reference: [16] <author> J. Hennessy and J. Jouppi and J. Gill and F. Baskett and T. Gross and C. Rowen and J. Leonard. </author> <booktitle> The MIPS machine In Proc. </booktitle> <address> IEEE Coupcon San Francisco, CA, </address> <month> Feb. </month> <year> 1982 </year>
Reference-contexts: To describe the run-time behavior of the processor, we will adopt a simple idealization which is valid from the viewpoint of the instruction scheduler. In particular, we will consider a model that reflects the behavior of early RISC processors <ref> [21, 16, 35] </ref> that did not embody any dynamic support for instruction scheduling; instructions were executed strictly in the order specified by the compiled instruction-stream. In this setting, we have a program counter P C which always points to the next instruction i to be executed.
Reference: [17] <author> J. Hennessy and T. Gross. </author> <title> Postpass Code Optimization of Pipeline Constraints. </title> <journal> ACM TOPLAS, </journal> <volume> 5(3), </volume> <year> 1983. </year>
Reference: [18] <author> W.-M. W. Hwu et al. </author> <title> The Superblock: An Effective Technique for VLIW and Superscalar Compilation. </title> <journal> The Journal of Supercomputing, </journal> <volume> Vol. 7 (1993), </volume> <pages> 229-248. </pages>
Reference: [19] <author> W. Hwu and P. Chang. </author> <title> Achieving High Instruction Cache Performance with an Optimizing Compiler. </title> <booktitle> In Proceedings of 16th Annual Symposium on Computer Architecture, </booktitle> <pages> pp. 242-250, </pages> <year> 1989. </year>
Reference: [20] <author> Ray A. Kamin III and George B. Adams III and Pradeep K. Dubey. </author> <title> Dynamic Trace Analysis for Analytic Modeling of Superscalar Performance In Performance Evaluation 19 (1994) pp. </title> <type> 259-276, </type> <year> 1994. </year>
Reference: [21] <author> M. Katavenis. </author> <title> Reduced Instruction Set Computer Architecture for VLSI MIT Press, </title> <address> Cambridge, MA, </address> <year> 1984 </year>
Reference-contexts: To describe the run-time behavior of the processor, we will adopt a simple idealization which is valid from the viewpoint of the instruction scheduler. In particular, we will consider a model that reflects the behavior of early RISC processors <ref> [21, 16, 35] </ref> that did not embody any dynamic support for instruction scheduling; instructions were executed strictly in the order specified by the compiled instruction-stream. In this setting, we have a program counter P C which always points to the next instruction i to be executed.
Reference: [22] <author> D. Kuck and Y. Muraoka and S. Chen. </author> <title> On the Number of Simultaneously Executable in Fortran-like Programs and Their Resulting Speedup IEEE Transactions on Computers, </title> <booktitle> C-21, </booktitle> <pages> pp. 1293-1310, </pages> <month> Dec. </month> <year> 1972 </year>
Reference-contexts: In this case, the processors have special hardware support <ref> [27, 22, 7] </ref> to analyze the current program dependences "on-the-fly" and schedule as many independent instructions as possible on a given cycle. Consequently, in these modern processors and in the interest of finding multiple independent instructions that can be co-scheduled, invariably S &gt; m.
Reference: [23] <author> M. Lam. </author> <title> Software Pipelining: An Effective Method for VLIW Machines In Proc. </title> <booktitle> ACM SIGLPAN`88 Conference on Programming Language Design and Implementation, </booktitle> <pages> pp. 318-327, </pages> <year> 1988. </year>
Reference: [24] <author> S. McFarling and J. Hennessy. </author> <title> Reducing the Cost of Branches. </title> <booktitle> In Proceedings of 13th Annual Symposium on Computer Architecture, </booktitle> <pages> pp. 396-403, </pages> <year> 1986. </year>
Reference: [25] <author> S. McFarling. </author> <title> Procedure Merging with Instruction Caches. </title> <booktitle> In Proceedings of SIGPLAN`91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pp. 26-28, </pages> <year> 1991. </year>
Reference: [26] <author> S. Moon and K. Ebcioglu. </author> <title> An Efficient Resource-constrained Global Scheduling Technique for Superscalar and VLIW Processors In Proc. </title> <month> MICRO-25 </month> <year> 1992 </year>
Reference: [27] <author> Toshio Nakatani and Kemal Ebcioglu. </author> <title> Making Compaction-Based Parallelization Affordable IEEE Transactions on Parallel and Distributed Systems, </title> <note> 4(9) 1993 pp. :1014-1029 </note>
Reference-contexts: In this case, the processors have special hardware support <ref> [27, 22, 7] </ref> to analyze the current program dependences "on-the-fly" and schedule as many independent instructions as possible on a given cycle. Consequently, in these modern processors and in the interest of finding multiple independent instructions that can be co-scheduled, invariably S &gt; m.
Reference: [28] <author> K. Palem and V. Sarkar. </author> <title> Code Optimization in Modern Compilers Lecture Notes, </title> <institution> Western Institute of Computer Science, Stanford University, </institution> <year> 1995. </year>
Reference-contexts: In this measure, the run-time scheduler is assisted by a conventional basic-block scheduler L, which is typically a part of modern optimizing compilers for superscalar processors <ref> [28] </ref>. Both of these are compared relative to basic-block scheduling in the context of a processor with no hardware support for scheduling, i.e., the method V . 2.
Reference: [29] <author> K. Palem and B. Simons. </author> <title> Scheduling Time-critical Instructions on RISC Machines. </title> <journal> ACM TOPLAS, </journal> <volume> 5(3), </volume> <year> 1993. </year>
Reference-contexts: However, both phases differ from those existing in traditional local schedulers. 22 6.0.1 Phase 1: computing priorities The priority of a node has two components: a local priority and a global priority. For both components we will use the rank prioritizing function as presented in <ref> [29] </ref>. The way to compute the two components is described in the following. Consider each basic block A in the trace (except the last one), and its successor B. Compute ranks to all instructions in basic blocks A and B considered together.
Reference: [30] <author> K. Palem and B. Simons. </author> <title> Instruction Scheduling. In Optimization in Compilers, </title> <editor> (eds: F. Allen, B. Rosen and K. Zadeck). </editor> <publisher> ACM Press and Addison-Wesley (to appear). </publisher>
Reference: [31] <author> D. Paterson. </author> <title> Reduced Instruction Set Computers. </title> <journal> Communications of the ACM, </journal> <volume> 28(1) </volume> <pages> 8-21, </pages> <year> 1985. </year>
Reference: [32] <author> K. Pettis and R. Hansen. </author> <title> Profile Guided Code Positioning. </title> <booktitle> In Proceedings of SIGPLAN`90 Conference on Programming Language Design and Implementation, </booktitle> <pages> pp. 16-27, </pages> <year> 1990. </year> <month> 27 </month>
Reference: [33] <author> B. R. Rau. </author> <title> Iterative Modulo Scheduling: </title> <booktitle> An Algorithm for Software Pipelining Loops In Proceedings of the 27-th Annual Symposium on Microarchitecture (MICRO-27), </booktitle> <month> Nov. </month> <year> 1994 </year>
Reference: [34] <author> S. Pinter. </author> <title> Register Allocation with Instruction Scheduling. </title> <booktitle> In Proceedings of SIGPLAN`93 Conference on Programming Language Design and Implementation, </booktitle> <pages> pp. 248-257, </pages> <year> 1993. </year>
Reference: [35] <author> G. Radin. </author> <title> The 801 Minicomputer. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 27(3) </volume> <pages> 237-246, </pages> <year> 1983. </year>
Reference-contexts: To describe the run-time behavior of the processor, we will adopt a simple idealization which is valid from the viewpoint of the instruction scheduler. In particular, we will consider a model that reflects the behavior of early RISC processors <ref> [21, 16, 35] </ref> that did not embody any dynamic support for instruction scheduling; instructions were executed strictly in the order specified by the compiled instruction-stream. In this setting, we have a program counter P C which always points to the next instruction i to be executed.
Reference: [36] <author> G. Sohi and S. </author> <title> Vajapeyam. </title> <booktitle> Instruction Issue Logic in High-performance Interruptible Pipelined Processors In Proceedings 14th Annual ACM Symposium on Computer Architecture, </booktitle> <pages> pp. 27-34, </pages> <year> 1987. </year>
Reference: [37] <author> M. Smith, M. Johnson and M. Horowitz. </author> <booktitle> Limits to Multiple Instruction Issue In Proceedings of 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 27-34, </pages> <year> 1987. </year>
Reference: [38] <author> D. </author> <title> Wall. </title> <booktitle> Limits to Instruction Level Parallelism In Proceedings of 3rd International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 290-302, </pages> <year> 1989. </year>
Reference: [39] <author> D. Wall. </author> <title> Predicting Program Behavior Using Real or Estimated Profile. </title> <booktitle> In Proceedings of SIGPLAN`91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pp. 59-70, </pages> <year> 1991. </year>
Reference: [40] <author> H. Warren. </author> <title> Instruction Scheduling for the IBM RISC System/6K Processors. </title> <journal> IBM Journal of Research and Development, </journal> <pages> 85-92, </pages> <year> 1990. </year> <month> 28 </month>
References-found: 40

