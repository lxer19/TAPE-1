URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-221.ps.Z
Refering-URL: http://www.cs.gatech.edu/computing/classes/cs7322_98_spring/readings.html
Root-URL: 
Title: Layered Representation for Motion Analysis  
Author: John Y. A. Wang Edward H. Adelson 
Address: Cambridge, MA 02139 Cambridge, MA 02139  
Affiliation: The MIT Media Laboratory The MIT Media Laboratory Dept. Elec. Eng. and Comp. Sci. Dept. Brain and Cognitive Sciences Massachusetts Institute of Technology Massachusetts Institute of Technology  
Abstract: M.I.T. Media Laboratory Vision and Modeling Group, Technical Report No. 221, April 1993 Appears in Proceedings of the IEEE Computer Vision and Pattern Recognition Conference, pp. 361-366, New York, June 1993. Abstract Standard approaches to motion analysis assume that the optic flow is smooth; such techniques have trouble dealing with occlusion boundaries. The most popular solution is to allow discontinuities in the flow field, imposing the smoothness constraint in a piecewise fashion. But there is a sense in which the dis-continuities in flow are artifactual, resulting from the attempt to capture the motion of multiple overlapping objects in a single flow field. Instead we can decompose the image sequence into a set of overlapping layers, where each layer's motion is described by a smooth flow field. The discontinuities in the description are then attributed to object opacities rather than to the flow itself, mirroring the structure of the scene. We have devised a set of techniques for segmenting images into coherently moving regions using affine motion analysis and clustering techniques. We are able to decompose an image into a set of layers along with information about occlusion and depth ordering. We have applied the techniques to the "flower garden" sequence. We can analyze the scene into four layers, and then represent the entire 30-frame sequence with a single image of each layer, along with associated motion parameters. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E.H. Adelson, </author> <title> Layered representation for image coding, </title> <type> Technical Report No. 181, </type> <institution> Vision and Modeling Group, The MIT Media Lab, </institution> <month> December </month> <year> 1991. </year>
Reference-contexts: In addition, the description could be an important step on the way to a meaningful object-based description of the scene, rather than a mere description of a flow field. Adelson <ref> [1] </ref> has described a general framework for "layered image representation," in which image sequences are decomposed into a set of layers ordered in depth along with associated maps defining their motions, opacities, and intensities.
Reference: [2] <author> J. R. Bergen, P. J. Burt, R. Hingorani, and S. </author> <title> Peleg , Computing two motions from three frames, </title> <booktitle> International Conference on Computer Vision, </booktitle> <year> 1990. </year>
Reference-contexts: Without the knowledge of the object boundaries, motion estimation will incorrectly apply the image constraints across multiple objects. Likewise, object boundaries are difficult to determine without some estimation of motion. Recent works by <ref> [7, 2, 9] </ref> have shown that the affine motion model provides a good approximation of 3-D moving objects.
Reference: [3] <author> M. J. Black, P. Anandan, </author> <title> Robust dynamic motion estimation over time, </title> <booktitle> Proc. IEEE Computer Vision and Pattern Recognition91, </booktitle> <pages> pp. 296-302, </pages> <year> 1991. </year>
Reference-contexts: Estimation of these regions involve global estimation using a single motion model, and thus, often result in accumulating data from multiple objects. Our implementation of multiple motion estimation is similar to robust techniques presented by <ref> [3, 4, 5] </ref>. We use a gradual migration from a local motion representation to a global object motion representation. By performing optic flow estimation follow by affine estimation instead of a direct global affine motion estimation, we can minimize the problems of multiple objects within our analysis region.
Reference: [4] <author> T. Darrell, and Alex Pentland, </author> <title> Robust estimation of multi-layered motion representation, </title> <booktitle> IEEE Workshop on Visual Motion, </booktitle> <pages> pp. 173-178, </pages> <address> Princeton, </address> <year> 1991. </year>
Reference-contexts: Estimation of these regions involve global estimation using a single motion model, and thus, often result in accumulating data from multiple objects. Our implementation of multiple motion estimation is similar to robust techniques presented by <ref> [3, 4, 5] </ref>. We use a gradual migration from a local motion representation to a global object motion representation. By performing optic flow estimation follow by affine estimation instead of a direct global affine motion estimation, we can minimize the problems of multiple objects within our analysis region.
Reference: [5] <author> R. Depommier R., E. Dubois, </author> <title> Motion estimation with detection of occlusion areas, </title> <journal> Proc. IEEE ICASSP92, </journal> <volume> Vol. 3, </volume> <pages> pp. 269-273, </pages> <address> San Francisco, </address> <month> March </month> <year> 1992. </year>
Reference-contexts: Estimation of these regions involve global estimation using a single motion model, and thus, often result in accumulating data from multiple objects. Our implementation of multiple motion estimation is similar to robust techniques presented by <ref> [3, 4, 5] </ref>. We use a gradual migration from a local motion representation to a global object motion representation. By performing optic flow estimation follow by affine estimation instead of a direct global affine motion estimation, we can minimize the problems of multiple objects within our analysis region.
Reference: [6] <author> T. S. Huang and Y. P. Hsu, </author> <title> "Image Sequence Enhancement," Image Sequence Analysis, </title> <editor> Editor T. S. </editor> <booktitle> Huang, </booktitle> <pages> pp. 289-309., </pages> <publisher> Springer-Verlag, </publisher> <year> 1981. </year>
Reference-contexts: This operation can be easily seen as a temporal median filtering operation on the motion compensated sequence in regions defined by the region masks. Earlier studies have shown that motion compensation median filter can enhance noisy images and preserve edge information better than a temporal averaging filter <ref> [6] </ref>. Finally, we determine occlusion relationship. For each location of each layer, we tabulate the number of corresponding points used in the median filtering operation.
Reference: [7] <author> M. Irani, S. Peleg, </author> <title> Image sequence enhancement using multiple motions analysis, </title> <booktitle> Proc. IEEE Computer Vision and Pattern Recognition92, </booktitle> <pages> pp. 216-221, </pages> <address> Cham-paign, </address> <month> June, </month> <year> 1992. </year>
Reference-contexts: Without the knowledge of the object boundaries, motion estimation will incorrectly apply the image constraints across multiple objects. Likewise, object boundaries are difficult to determine without some estimation of motion. Recent works by <ref> [7, 2, 9] </ref> have shown that the affine motion model provides a good approximation of 3-D moving objects.
Reference: [8] <author> Lucas, B., Kanade, T., </author> <title> An iterative image registration technique with an application to stereo vision, </title> <booktitle> Image Understanding Workshop, </booktitle> <pages> pp. 121-130, </pages> <month> April, </month> <year> 1981. </year>
Reference-contexts: Our local motion estimation is obtained with a multi-scale coarse-to-fine algorithm based on a gradient approach described by <ref> [8] </ref>. Since only one motion is visible at any point when dealing with opaque objects, the single motion model assumed in the optic flow estimation is acceptable. The multi-scale implementation allows for estimation of large motions.
Reference: [9] <author> S. Nagahdaripour, S. Lee, </author> <title> Motion recovery from image sequences using first-order optical flow information, </title> <booktitle> Proc. IEEE Workshop on Visual Motion 91, </booktitle> <pages> pp. 132-139, </pages> <address> Princeton, </address> <year> 1991. </year>
Reference-contexts: Without the knowledge of the object boundaries, motion estimation will incorrectly apply the image constraints across multiple objects. Likewise, object boundaries are difficult to determine without some estimation of motion. Recent works by <ref> [7, 2, 9] </ref> have shown that the affine motion model provides a good approximation of 3-D moving objects.
Reference: [10] <author> M. Shizawa and K. Mase, </author> <title> A unified computational theory for motion transparency and motion boundaries based on eigenenergy analysis," </title> <booktitle> Proc. IEEE Computer Vision and Pattern Recognition91, </booktitle> <pages> pp. 296-302, </pages> <year> 1991. </year>
Reference-contexts: The multi-scale implementation allows for estimation of large motions. When analyzing scene exhibiting transparent phenomena, the motion estimation technique described by Shizawa and Mase <ref> [10] </ref> may be suitable. Motion segmentation is obtained by iteratively refining the estimates of affine motions and the corresponding regions. We estimate the affine parameters within each subregion of the image by standard regression techniques on local motion field.
Reference: [11] <author> C. W. Therrien, </author> <title> Decision Estimation and Classification, </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1989. </year> <pages> 5 6 </pages>
Reference-contexts: We identify these hypotheses by their large residual error and eliminate them from our analysis. However, motion estimates from patches that cover the same object will have similar parameters. These are grouped in the affine motion parameter space with a k-means clustering algorithm described in <ref> [11] </ref>. In the clustering process, we derive a representative model for each group of similar models. The model clustering produces a set of likely affine motion models that are exhibited by objects in the scene. Next, we use hypothesis testing with the motion models to reassign the regions.
References-found: 11

