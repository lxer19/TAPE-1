URL: http://www.cs.brown.edu/people/tld/postscript/DeanGivanandLeachUAI-97.ps
Refering-URL: http://www.cs.brown.edu/research/ai/publications/
Root-URL: http://www.cs.brown.edu
Email: sml]@cs.brown.edu  
Title: Model Reduction Techniques for Computing Approximately Optimal Solutions for Markov Decision Processes  
Author: Thomas Dean and Robert Givan and Sonia Leach 
Web: http://www.cs.brown.edu/people/  
Address: [tld, rlg,  
Affiliation: Department of Computer Science, Brown University  
Abstract: We present a method for solving implicit (factored) Markov decision processes (MDPs) with very large state spaces. We introduce a property of state space partitions which we call *-homogeneity. Intuitively, an *-homogeneous partition groups together states that behave approximately the same under all or some subset of policies. Borrowing from recent work on model minimization in computer-aided software verification, we present an algorithm that takes a factored representation of an MDP and an 0 * 1 and computes a factored *-homogeneous partition of the state space. This partition defines a family of related MDPs|those MDP's with state space equal to the blocks of the partition, and transition probabilities "approximately" like those of any (original MDP) state in the source block. To formally study such families of MDPs, we introduce the new notion of a "bounded parameter MDP" (BMDP), which is a family of (traditional) MDPs defined by specifying upper and lower bounds on the transition probabilities and rewards. We describe algorithms that operate on BMDPs to find policies that are approximately optimal with respect to the original MDP. In combination, our method for reducing a large implicit MDP to a possibly much smaller BMDP using an *-homogeneous partition, and our methods for selecting actions in BMDP's constitute a new approach for analyzing large implicit MDP's. Among its advantages, this new approach provides insight into existing algorithms to solving implicit MDPs, provides useful connections to work in automata theory and model minimization, and suggests methods, which involve varying *, to trade time and space (specifically in terms of the size of the corresponding state space) for solution quality. 
Abstract-found: 1
Intro-found: 1
Reference: [ Bellman, 1957 ] <author> Bellman, Richard 1957. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press. </publisher>
Reference-contexts: or policy specific) value bounds for M P apply also to M (by extending the policy and value functions to the state space of M according to P). 5 Interval Value Iteration We have developed a variant of the value iteration algorithm for computing the optimal policy for exact MDPs <ref> [ Bellman, 1957 ] </ref> that operates on bounded parameter MDPs. A BMDP M represents a family of MDPs F M , implying some degree of uncertainty as to which MDP in the family actions will actually be taken in.
Reference: [ Bertsekas and Casta~non, 1989 ] <author> Bertsekas, D. P. and Casta~non, D. A. </author> <year> 1989. </year> <title> Adaptive aggregation for infinite horizon dynamic programming. </title> <journal> IEEE Transactions on Automatic Control 34(6) </journal> <pages> 589-598. </pages>
Reference: [ Bouajjani et al., 1992 ] <author> Bouajjani, A.; Fernandez, J.- C.; Halbwachs, N.; Raymond, P.; and Ratel, C. </author> <year> 1992. </year> <title> Minimal state graph generation. </title> <booktitle> Science of Computer Programming 18 </booktitle> <pages> 247-269. </pages>
Reference: [ Boutilier and Dearden, 1994 ] <author> Boutilier, Craig and Dearden, Richard 1994. </author> <title> Using abstractions for decision theoretic planning with time constraints. </title> <booktitle> In Proceedings AAAI-94. AAAI. </booktitle> <pages> 1016-1022. </pages> [ <note> Boutilier et al., 1995a ] Boutilier, </note> <author> Craig; Dean, Thomas; and Hanks, </author> <title> Steve 1995a. Planning under uncertainty: Structural assumptions and computational leverage. </title> <booktitle> In Proceedings of the Third European Workshop on Planning. </booktitle>
Reference: [ Boutilier et al., 1995b ] <author> Boutilier, Craig; Dearden, Richard; and Goldszmidt, </author> <month> Moises </month> <year> 1995b. </year> <title> Exploiting structure in policy construction. </title> <booktitle> In Proceedings IJCAI 14. IJCAII. </booktitle> <pages> 1104-1111. </pages>
Reference-contexts: ) where Parents (X) denotes the parents of X in the 2TBN and each of the conditional probability distributions Pr (X i;t+1 jParents (X i;t+1 ); U t ) can be repre sented as a conditional probability table or as a decision tree|we choose the latter in this paper following <ref> [ Boutilier et al., 1995b ] </ref> . <p> Dean and Givan [ 1997 ] describe a model-minimization algorithm for solving factored MDPs which is asymptotically equivalent to the algorithm in <ref> [ Boutilier et al., 1995b ] </ref> . Boutilier and Dearden [ ? ] extend the work in [ Boutilier et al., 1995b ] to compute approximate solutions to factored MDPs by associating upper and lower bounds with symbolically represented blocks of states. <p> Dean and Givan [ 1997 ] describe a model-minimization algorithm for solving factored MDPs which is asymptotically equivalent to the algorithm in <ref> [ Boutilier et al., 1995b ] </ref> . Boutilier and Dearden [ ? ] extend the work in [ Boutilier et al., 1995b ] to compute approximate solutions to factored MDPs by associating upper and lower bounds with symbolically represented blocks of states.
Reference: [ Burch et al., 1994 ] <author> Burch, Jerry; Clarke, Ed-mund M.; Long, David; McMillan, Kenneth L.; and Dill, David L. </author> <year> 1994. </year> <title> Symbolic model checking for sequential circuit verification. </title> <booktitle> IEEE Transactions on Computer Aided Design 13(4) </booktitle> <pages> 401-424. </pages>
Reference-contexts: The basic idea of computing equivalent reduced pro cesses has its origins in automata theory [ Hartmanis and Stearns, 1966 ] and stochastic processes [ Kemeny and Snell, 1960 ] and has surfaced more recently in the work on model checking in computer-aided verification <ref> [ Burch et al., 1994 ] </ref>[ Lee and Yannakakis, 1992 ] .
Reference: [ Dean and Givan, 1997 ] <author> Dean, Thomas and Givan, Robert 1997. </author> <title> Model minimization in Markov decision processes. </title> <booktitle> In Proceedings AAAI-97. </booktitle> <publisher> AAAI. </publisher>
Reference-contexts: Building on the work of Lee and Yannakakis [ 1992 ] , we have shown <ref> [ Dean and Givan, 1997 ] </ref> that several existing algorithms are asymptotically equivalent to first constructing the minimal reduced MDP and then solving this MDP using traditional methods that operate on the flat (unfactored) representations. The minimal model may be exponentially larger than the original compact MDP.
Reference: [ Dean and Kanazawa, 1989 ] <author> Dean, Thomas and Kanazawa, </author> <title> Keiji 1989. A model for reasoning about persistence and causation. </title> <booktitle> Computational Intelligence 5(3) </booktitle> <pages> 142-150. </pages>
Reference-contexts: temporal Bayesian network (2TBN) is a directed acyclic graph consisting of two sets of variables fX i;t g and fX i;t+1 g in which directed arcs indicating dependence are allowed from the variables in the first set to variables in the second set and between variables in the second set. <ref> [ Dean and Kanazawa, 1989 ] </ref> The state-transition probabilities are now factored as Pr (X t+1 jX t ; U t ) = i=1 Pr (X i;t+1 jParents (X i;t+1 ); U t ) where Parents (X) denotes the parents of X in the 2TBN and each of the conditional probability
Reference: [ Dean et al., 1995 ] <author> Dean, Thomas; Kaelbling, Leslie; Kirman, Jak; and Nicholson, </author> <title> Ann 1995. Planning under time constraints in stochastic domains. </title> <journal> Artificial Intelligence 76(1-2):35-74. </journal>
Reference: [ Givan et al., 1997 ] <author> Givan, Robert; Leach, Sonia; and Dean, </author> <title> Thomas 1997. Bounded parameter markov decision processes. </title> <type> Technical Report CS-97-05, </type> <institution> Brown University, </institution> <address> Providence, Rhode Island. </address> [ <note> Hartmanis and Stearns, 1966 ] Hartmanis, </note> <author> J. and Stearns, R. E. </author> <year> 1966. </year> <title> Algebraic Structure Theory of Sequential Machines. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J. </address>
Reference-contexts: Although BMDPs are introduced here to represent approximate aggregations, they are interesting in their own right and are discussed in more detail in <ref> [ Givan et al., 1997 ] </ref> , The model reduction algorithms and bounded parameter MDP solution methods can be combined to find approximately optimal solutions to large factored MDPs, varying * to trade time and space for solution quality. The remainder of this paper is organized as follows. <p> We will write of bounding the (optimal or policy specific) value of a state in a BMDP|by this we mean providing an upper or lower bound on the corresponding state value over the entire family of MDPs F M . For a more thorough treatment of BMDPs, please see <ref> [ Givan et al., 1997 ] </ref> . <p> These new estimates can then be used to select a new minimizing (maximizing) MDP for the next iteration, and so forth. Bounded parameter MDPs are interesting objects and we explore them at greater length in <ref> [ Givan et al., 1997 ] </ref> . In that paper, we prove the following results about IV I.
Reference: [ Howard and Matheson, 1984 ] <author> Howard, Ronald A. and Matheson, James E. </author> <year> 1984. </year> <title> Influence diagrams. </title> <editor> In Howard, Ronald A. and Matheson, James E., editors 1984, </editor> <title> The Principles and Applications of Decision Analysis. Strategic Decisions Group, </title> <address> Menlo Park, CA 94025. </address>
Reference-contexts: We enhance the 2TBN representation to include actions and reward func tions; the resulting graph is called an influence diagram <ref> [ Howard and Matheson, 1984 ] </ref> . three state variables, X = fP; Q; Sg, and describes the transition probabilities and rewards for a particular action.
Reference: [ Howard, 1960 ] <author> Howard, Ronald A. </author> <year> 1960. </year> <title> Dynamic Programming and Markov Processes. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts. </address>
Reference: [ Kemeny and Snell, 1960 ] <author> Kemeny, J. G. and Snell, J. L. </author> <year> 1960. </year> <title> Finite Markov Chains. </title> <editor> D. Van Nos-trand, </editor> <address> New York. </address>
Reference-contexts: The original and reduced processes are equivalent in the sense that they yield the same solutions, i.e., the same optimal policies and state values. The basic idea of computing equivalent reduced pro cesses has its origins in automata theory [ Hartmanis and Stearns, 1966 ] and stochastic processes <ref> [ Kemeny and Snell, 1960 ] </ref> and has surfaced more recently in the work on model checking in computer-aided verification [ Burch et al., 1994 ][ Lee and Yannakakis, 1992 ] . <p> C 1 ; : : : ; C k such that each 4 For the case of * = 0, *-approximate stochastic bisim-ulation homogeneity is closely related to the substitution property for finite automata developed by Hartmanis and Stearns [ 1966 ] and the notion of lumpability for Markov chains <ref> [ Kemeny and Snell, 1960 ] </ref> . 5 There may be many such clusterings, we currently choose a coarsest one arbitrarily. 6 The term splitting refers to the process whereby a block of a partition is divided into two or more sub-blocks to obtain a refinement of the original partition. 7
Reference: [ Kushmerick et al., 1995 ] <author> Kushmerick, Nicholas; Hanks, Steve; and Weld, </author> <title> Daniel 1995. An algorithm for probabilistic planning. </title> <journal> Artificial Intelligence 76(1-2). </journal>
Reference-contexts: Factored Representations In the remainder of this paper, we make use of Bayesian networks [ Pearl, 1988 ] to encode implicit (or factored) representations; however, our methods apply to other factored representations such as probabilistic STRIPS opera tors <ref> [ Kushmerick et al., 1995 ] </ref> . Let X = fX 1 ; : : : ; X m g be a set of state variables. We assume the variables are boolean, and refer to them also as flu-ents.
Reference: [ Lee and Yannakakis, 1992 ] <author> Lee, David and Yan-nakakis, </author> <title> Mihalis 1992. Online minimization of transition systems. </title> <booktitle> In Proceedings of 24th Annual ACM Symposium on the Theory of Computing. </booktitle>
Reference: [ Lin and Dean, 1995 ] <author> Lin, Shieu-Hong and Dean, </author> <title> Thomas 1995. Generating optimal policies for high-level plans with conditional branches and loops. </title> <booktitle> In Proceedings of the Third European Workshop on Planning. </booktitle> <pages> 205-218. </pages>
Reference: [ Pearl, 1988 ] <author> Pearl, </author> <title> Judea 1988. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, Califor-nia. </address>
Reference-contexts: For a more thorough treatment of BMDPs, please see [ Givan et al., 1997 ] . Factored Representations In the remainder of this paper, we make use of Bayesian networks <ref> [ Pearl, 1988 ] </ref> to encode implicit (or factored) representations; however, our methods apply to other factored representations such as probabilistic STRIPS opera tors [ Kushmerick et al., 1995 ] . Let X = fX 1 ; : : : ; X m g be a set of state variables.
Reference: [ Puterman, 1994 ] <author> Puterman, Martin L. </author> <year> 1994. </year> <title> Markov Decision Processes. </title> <publisher> John Wiley & Sons, </publisher> <address> New York. </address>
Reference-contexts: 1 Introduction Markov decision processes (MDP) provide a formal basis for representing planning problems involving uncertainty [ Boutilier et al., 1995a ] . There exist algorithms for solving MDPs that are polynomial in the size of the state space <ref> [ Puterman, 1994 ] </ref> . In this paper, we are interested in MDPs in which the states are specified implicitly using a set of state variables. <p> The value function V ;M for a given policy maps states to their expected discounted cumulative reward given that you start in that state and act according the given policy: V ;M (p) = R (p) + fl q2Q where fl is the discount rate, 0 fl &lt; 1. <ref> [ Puterman, 1994 ] </ref> .
Reference: [ Schweitzer et al., 1985 ] <author> Schweitzer, Paul J.; Puter-man, Martin L.; and Kindle, Kyle W. </author> <year> 1985. </year> <title> Iterative aggregation-disaggregation procedures for discounted semi-Markov reward processes. </title> <journal> Operations Research 33(3) </journal> <pages> 589-605. </pages>
Reference: [ Schweitzer, 1984 ] <author> Schweitzer, Paul J. </author> <year> 1984. </year> <title> Aggregation methods for large Markov chains. </title> <editor> In Iazola, G.; Coutois, P. J.; and Hordijk, A., editors 1984, </editor> <title> Mathemaical Computer Performance and Reliability. </title> <publisher> Elsevier, Amsterdam, Holland. </publisher> <pages> 275-302. </pages>
References-found: 20

