URL: http://www.eecs.berkeley.edu/~varaiya/papers_ps.dir/andrea.6.ps
Refering-URL: http://www.eecs.berkeley.edu/~varaiya/
Root-URL: 
Title: Capacity of Fading Channels with Channel Side Information  
Author: Andrea J. Goldsmith and Pravin P. Varaiya 
Keyword: Index Terms: Capacity, fading channels, channel side information, power adaptation.  
Note: To Appear: IEEE Trans. Inform. Theory.  
Abstract: We obtain the Shannon capacity of a fading channel with channel side information at the transmitter and receiver, and at the receiver alone. The optimal power adaptation in the former case is "water-pouring" in time, analogous to water-pouring in frequency for time-invariant frequency-selective fading channels. Inverting the channel results in a large capacity penalty in severe fading. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R.G. Gallager, </author> <title> Information Theory and Reliable Communication. </title> <address> New York: </address> <publisher> Wiley, </publisher> <year> 1968. </year> <month> 11 </month>
Reference-contexts: The optimal power allocation is a "water-pouring" in time, analogous to the water-pouring used to achieve capacity on frequency-selective fading channels <ref> [1, 2] </ref>. We show that for i.i.d. fading, using receiver side information only has a lower complexity and the same approximate capacity as optimally adapting to the channel, for the three fading distributions we examine. <p> If fl [i] is below this cutoff then no data is transmitted over the ith time interval. Since fl is time-varying, the maximizing power adaptation policy of (5) is a "water-pouring" formula in time <ref> [1] </ref> that depends on the fading statistics p (fl) only through the cutoff value fl 0 . <p> Gaussian source with variance equal to the signal power. The maximum likelihood decoder then observes the channel output vector y [] and chooses the codeword x w j which minimizes the Euclidean distance jj (y <ref> [1] </ref>; : : : ; y [n]) (x w j [1]g [1]; : : : ; x w j [n]g [n])jj. <p> Gaussian source with variance equal to the signal power. The maximum likelihood decoder then observes the channel output vector y [] and chooses the codeword x w j which minimizes the Euclidean distance jj (y <ref> [1] </ref>; : : : ; y [n]) (x w j [1]g [1]; : : : ; x w j [n]g [n])jj. Thus, for i.i.d. fading and constant transmit power, side information at the transmitter has no capacity benefit, and the encoder/decoder pair based on receiver side information alone is simpler than the adaptive multiplexing technique shown in Figure 2. <p> The message index w 2 <ref> [1; : : : ; 2 nR n ] </ref> is transmitted over the N + 1 channels in Figure 2 as follows. <p> We assume that the codes are designed with a priori knowledge of the channel side information fl n = ffl <ref> [1] </ref>; : : : ; fl [n]g, since any code designed under this assumption will have at least as high a rate as if fl [i] is only known at time i. Assume that the message index W is uniformly distributed on f1; : : : ; 2 nR g.
Reference: [2] <author> S. Kasturia, J.T. Aslanis, and J.M. Cioffi, </author> <title> "Vector coding for partial response channels," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> Vol. IT-36, No. 4, </volume> <pages> pp. 741-762, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: The optimal power allocation is a "water-pouring" in time, analogous to the water-pouring used to achieve capacity on frequency-selective fading channels <ref> [1, 2] </ref>. We show that for i.i.d. fading, using receiver side information only has a lower complexity and the same approximate capacity as optimally adapting to the channel, for the three fading distributions we examine.
Reference: [3] <author> S.-G. Chua and A.J. Goldsmith, </author> <title> "Variable-rate variable-power MQAM for fading channels," </title> <booktitle> VTC'96 Conf. Rec., </booktitle> <pages> pp. 815-819, </pages> <month> April </month> <year> 1996. </year> <note> Also to appear in IEEE Trans. Commun. </note>
Reference-contexts: The suboptimal adaptive techniques reduce complexity at a cost of decreased capacity. This tradeoff between achievable data rates and complexity is examined for adaptive and nonadaptive modulation in <ref> [3] </ref>, where adaptive modulation achieves an average data rate within 7-10dB of the capacity derived herein (depending on the required error probability), while nonadaptive modulation exhibits a severe rate penalty. Trellis codes can be combined with the adaptive modulation to achieve higher rates [4].
Reference: [4] <author> S.-G. Chua and A.J. Goldsmith, </author> <title> "Adaptive coded modulation," </title> <booktitle> ICC'97 Conf. Rec. </booktitle> <month> June </month> <year> 1997. </year> <note> Also submitted to IEEE Trans. Commun. </note>
Reference-contexts: Trellis codes can be combined with the adaptive modulation to achieve higher rates <ref> [4] </ref>. We do not consider the case when the channel fade level is unknown to both the transmitter and receiver. Capacity under this assumption was obtained for the Gilbert-Elliot channel in [5] and for more general Markov channel models in [6]. <p> In general, Shannon capacity analysis does not give any indication how to design adaptive or nonadaptive techniques for real systems. Achievable rates for adaptive trellis-coded MQAM have been investigated in <ref> [4] </ref>, where a simple 4-state trellis code combined with adaptive six-constellation MQAM modulation was shown to achieve rates within 7dB of the capacity (4) in Figures 3 and 4.
Reference: [5] <author> M. Mushkin and I. Bar-David, </author> <title> "Capacity and coding for the Gilbert-Elliot channel," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> Vol. IT-35, No. 6, </volume> <pages> pp. 1277-1290, </pages> <month> Nov. </month> <year> 1989. </year>
Reference-contexts: Trellis codes can be combined with the adaptive modulation to achieve higher rates [4]. We do not consider the case when the channel fade level is unknown to both the transmitter and receiver. Capacity under this assumption was obtained for the Gilbert-Elliot channel in <ref> [5] </ref> and for more general Markov channel models in [6]. If the statistics of the channel variation are also unknown, then channels with deep fading will typically have a capacity close to zero.
Reference: [6] <author> A.J. Goldsmith and P.P. Varaiya, </author> <title> "Capacity, mutual information, and coding for finite-state Markov channels," </title> <journal> IEEE Trans. Inform. Theory. </journal> <pages> pp. 868-886, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: We do not consider the case when the channel fade level is unknown to both the transmitter and receiver. Capacity under this assumption was obtained for the Gilbert-Elliot channel in [5] and for more general Markov channel models in <ref> [6] </ref>. If the statistics of the channel variation are also unknown, then channels with deep fading will typically have a capacity close to zero. This is because the data must be decoded without error, which is difficult when the location of deep fades are random.
Reference: [7] <author> I. Csiszar and J. Korner, </author> <title> Information Theory: Coding Theorems for Discrete Memoryless Channels. </title> <address> New York: </address> <publisher> Academic Press, </publisher> <year> 1981. </year>
Reference-contexts: In particular, the capacity of a fading channel with arbitrary variation is at most the capacity of a time-invariant channel under the worst-case fading conditions. More details about the capacity of time-varying channels under these assumptions can be found in the literature on Arbitrarily Varying Channels <ref> [7, 8] </ref>. The remainder of this paper is organized as follows. The next section describes the system model. The capacity of the fading channel under the different side information conditions is obtained in Section 3.
Reference: [8] <author> I. Csiszar and P. Narayan, </author> <title> The capacity of the Arbitrarily Varying Channel," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> Vol. 37, No. 1, </volume> <pages> pp. 18-26, </pages> <month> Jan. </month> <year> 1991. </year>
Reference-contexts: In particular, the capacity of a fading channel with arbitrary variation is at most the capacity of a time-invariant channel under the worst-case fading conditions. More details about the capacity of time-varying channels under these assumptions can be found in the literature on Arbitrarily Varying Channels <ref> [7, 8] </ref>. The remainder of this paper is organized as follows. The next section describes the system model. The capacity of the fading channel under the different side information conditions is obtained in Section 3.
Reference: [9] <author> J. Wolfowitz, </author> <title> Coding Theorems of Information Theory. 2nd Ed. </title> <address> New York: </address> <publisher> Springer-Verlag, </publisher> <year> 1964. </year>
Reference-contexts: Let C s denotes the capacity of a particular channel s 2 S, and p (s) denote the probability, or fraction of time, that the channel is in state s. The capacity of this time-varying channel is then given by Theorem 4.6.1 of <ref> [9] </ref>: C = s2S We now consider the capacity of the fading channel shown in Figure 1. Specifically, assume an AWGN fading channel with stationary and ergodic channel gain g [i].
Reference: [10] <author> R.J. McEliece and W. E. Stark, </author> <title> "Channels with block interference," </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> Vol IT-30, No. 1, </volume> <pages> pp. 44-53, </pages> <month> Jan. </month> <year> 1984. </year>
Reference-contexts: This sketches the proof of the coding theorem. Details can be found in the appendix, along with the converse theorem that no other coding scheme can achieve a higher rate. 4 3.2 Side Information at the Receiver In <ref> [10] </ref> it was shown that if the channel variation satisfies a compatibility constraint then the capacity of the channel with side information at the receiver only is also given by the average capacity formula (2). <p> If the transmit power is fixed at S and g [i] is i.i.d. then the input distribution at time i which achieves capacity is an i.i.d. Gaussian distribution with average power S. Thus, without power adaptation, the fading AWGN channel satisfies the compatibility constraint of <ref> [10] </ref>. The channel capacity with i.i.d. fading and receiver side information only is thus given by C (S) = B log (1 + fl)p (fl)dfl; (8) which is the same as (2), the capacity with transmitter and receiver side information but no power adaptation.
Reference: [11] <author> K. S. Gilhousen, I. M. Jacobs, R. Padovani, A. J. Viterbi, L. A. Weaver, Jr., and C. E. Wheatley III, </author> <title> "On the capacity of a cellular CDMA system," </title> <journal> IEEE Trans. Vehic. Technol., </journal> <volume> Vol. VT-40, No. 2, </volume> <pages> pp. 303-312, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: The constant thus satisfies R = 1=E [1=fl]. The fading channel capacity with channel inversion is just the capacity of an AWGN channel with SNR : 1 Channel inversion is common in spread spectrum systems with near-far interference imbalances <ref> [11] </ref>. It is also very simple to implement, since the encoder and decoder are designed for an AWGN channel, independent of the fading statistics. However, it can exhibit a large capacity penalty in extreme fading environments.
Reference: [12] <author> C. E. Shannon and W. Weaver, </author> <title> A Mathematical Theory of Communication. </title> <address> Urbana, IL: </address> <publisher> Univ. Illinois Press, </publisher> <year> 1949. </year>
Reference-contexts: For a given n, let n j = bnp (fl j fl &lt; fl j+1 )c = np (fl j fl &lt; fl j+1 ) for n sufficiently large. From Shannon <ref> [12] </ref>, for R j = B log (1 + fl j j =S) = B log (fl j = 0 ), we can develop a sequence of (2 n j R j ) codes fx w j [k]g n j 1; : : : ; 2 n j R j with
Reference: [13] <author> P. Billingsley. </author> <title> Probability and Measure. 2nd Ed. </title> <address> New York: </address> <publisher> Wiley, </publisher> <year> 1986. </year>
Reference-contexts: So for fixed * there exists an M * such that Z 1 B log fl Moreover, for M fixed, the monotone convergence theorem <ref> [13] </ref> implies that lim mM1 X B log fl j m!1 j=0 fl j 0 p (fl)dfl = 0 0 p (fl)dfl: Thus, using the M * in (23) and combining (23) and (24) we see that for the given * there exists an m sufficiently large such that mM *
Reference: [14] <author> T. Cover and J. Thomas, </author> <title> Elements of Information Theory. </title> <address> New York: </address> <publisher> Wiley, </publisher> <year> 1991. </year> <month> 12 </month>
Reference-contexts: Assume that the message index W is uniformly distributed on f1; : : : ; 2 nR g. Then nR = H (W jfl n ) a b where a follows from the data processing theorem <ref> [14] </ref> and the side information assumption, and b follows from Fano's inequality. Let N fl denote the number of times over the interval [0; n] that the channel has fade level fl.
References-found: 14

