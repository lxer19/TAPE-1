URL: http://www.csc.ncsu.edu/faculty/mpsingh/papers/mas/cl-ecai-92.ps
Refering-URL: http://www.csc.ncsu.edu/faculty/mpsingh/papers/mas/
Root-URL: http://www.csc.ncsu.edu
Email: msingh@mcc.com, msingh@cs.utexas.edu  
Title: A Critical Examination of the Cohen-Levesque Theory of Intentions  
Author: Munindar P. Singh 
Address: 3500 W. Balcones Center Drive, Austin, TX 78759, USA  Austin, TX 78712, USA  
Affiliation: MCC,  Department of Computer Sciences, University of Texas,  
Date: 1992  
Note: In Proceedings of the European Conference on Artificial Intelligence (ECAI),  
Abstract: We examine the formal theory of intentions recently proposed by Cohen & Levesque [2]. We evaluate the assumptions made by this theory and their consequences relative to fairly general intuitions about intentions, especially as they are applied in AI domains. We show that the theory is conceptually problematic and that certain technical claims made by the authors are false or counterintuitive in most natural scenarios. Keywords: intentions, logic in AI. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Michael E. Bratman. </author> <title> Intention, Plans, and Practical Reason. </title> <publisher> Harvard University Press, </publisher> <address> Cambridge, MA, </address> <year> 1987. </year>
Reference-contexts: Intentions are involved in several subareas of AI: Planning, Natural Language, Multiagent Systems, and others. C&L's paper builds on Bratman's theory of intentions, from which they derive their main requirements for a theory of intentions <ref> [1] </ref>. These requirements include (a) the mutual consistency of intentions and beliefs, and (b) the tendency of intentions to persist. C&L's theory is quite complex. We can discuss only its major features in the space available. Most of our remarks are conceptual. <p> Now we evaluate it relative to some further conceptual aspects of a theory of intentions. 5.1 Commitment and Intention-Revision The commitment-based analysis of intentions of Brat-man <ref> [1, ch. 2] </ref> and Harman [4, p. 94] is a useful view of intentions for AI. In the above philosophical analysis, however, the commitment of agents to their intentions is merely a condition that holds in normal circumstances. <p> That is, when real-life agents reason about their intentions, they normally persist with them. Indeed, Bratman says that intentions "resist (to some extent) revision and reconsideration" and involve "characteristic processes of reasoning and intention retention and (non)reconsideration." <ref> [1, p. 108] </ref>. This is only reasonable in light of the functional roles of intentions that he has described. However, C&L merge the agents' persistence into the very semantics of intentions. Thus the important distinction between the semantics of intentions and the policies of intention-revision is lost. <p> No theorems are given for P-R-GOAL. Most importantly, the presence of an arbitrary condition, q, only serves to reduce the predictive power of the theory. The main motivation for a commitment-based view of intentions is that it yields intuitive properties in Bratman's theory <ref> [1] </ref>. By introducing a rela-tivizing condition to use as a "reason" (p. 254), C&L state that an agent should persist with an intention as long as there are reasons for having it. But this is a position that Bratman anticipates and attacks as contradicting his view [1, p. 107]. <p> By introducing a rela-tivizing condition to use as a "reason" (p. 254), C&L state that an agent should persist with an intention as long as there are reasons for having it. But this is a position that Bratman anticipates and attacks as contradicting his view <ref> [1, p. 107] </ref>. Thus, in effect, C&L regress from the main motivation of their theory. If the definition of P-R-GOAL is seen as incorporating a general policy of intention-revision (see x5.1), it only complicates the problem.
Reference: [2] <author> Philip R. Cohen and Hector J. Levesque. </author> <title> Intention is choice with commitment. </title> <journal> Artificial Intelligence, </journal> <volume> 42 </volume> <pages> 213-261, </pages> <year> 1990. </year>
Reference-contexts: Unfortunately, there is not as much evaluation of claims and theories in AI as in other, more classical, disciplines. This is changing now, as AI becomes established itself. This paper contributes to this trend by critically examining a theory of intentions proposed by Cohen & Levesque (henceforth, C&L) <ref> [2] </ref>. The importance of intentions and actions in AI is obvious, but relatively few significant attempts have been made to formalize them. Intentions are involved in several subareas of AI: Planning, Natural Language, Multiagent Systems, and others.
Reference: [3] <author> E. A. Emerson. </author> <title> Temporal and modal logic. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science, volume B. </booktitle> <publisher> North-Holland, </publisher> <address> Am-sterdam, </address> <year> 1990. </year>
Reference-contexts: This is because success cannot be guaranteed unless one additionally assumes that if an agent repeatedly attempts a goal, he will eventually succeed. But this is precisely a form of a fairness assumption common in standard distributed computing <ref> [3] </ref>. However, C&L explicitly reject fairness "in the computer science sense" (p. 233). Another way of properly formalizing the conditions under which agents succeed with their P-GOALs would require explicit consideration of the agents' ability to achieve the given goal.
Reference: [4] <author> Gilbert Harman. </author> <title> Change in View. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: Now we evaluate it relative to some further conceptual aspects of a theory of intentions. 5.1 Commitment and Intention-Revision The commitment-based analysis of intentions of Brat-man [1, ch. 2] and Harman <ref> [4, p. 94] </ref> is a useful view of intentions for AI. In the above philosophical analysis, however, the commitment of agents to their intentions is merely a condition that holds in normal circumstances. That is, when real-life agents reason about their intentions, they normally persist with them.
Reference: [5] <author> John McCarthy. </author> <title> Epistemological problems of ar-tificial intelligence. </title> <editor> In Matthew L. Ginsberg, editor, </editor> <booktitle> Readings in Nonmonotonic Reasoning, </booktitle> <pages> pages 46-52. </pages> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA, </address> <year> 1987. </year> <note> Reprinted from IJCAI-77. </note>
Reference-contexts: Besides leading to a loss of predictive power, this is conceptually problematic. The entire set of possible exceptions cannot be specified in advance (this is akin to the qualification problem <ref> [5] </ref>).
References-found: 5

