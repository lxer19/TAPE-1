URL: ftp://ftp.cs.umass.edu/pub/anw/pub/sutton/sutton-90.ps.gz
Refering-URL: http://www-anw.cs.umass.edu/~rich/publications.html
Root-URL: 
Email: sutton@gte.com  
Title: Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming  
Author: Richard S. Sutton 
Address: Waltham, MA 02254  
Affiliation: GTE Laboratories Incorporated  
Note: Appeared in Proceedings of the Seventh Int. Conf. on Machine Learning, pp. 216-224, Morgan Kaufmann, 1990.  
Abstract: This paper extends previous work with Dyna, a class of architectures for intelligent systems based on approximating dynamic programming methods. Dyna architectures integrate trial-and-error (reinforcement) learning and execution-time planning into a single process operating alternately on the world and on a learned model of the world. In this paper, I present and show results for two Dyna architectures. The Dyna-PI architecture is based on dynamic programming's policy iteration method and can be related to existing AI ideas such as evaluation functions and universal plans (reactive systems). Using a navigation task, results are shown for a simple Dyna-PI system that simultaneously learns by trial and error, learns a world model, and plans optimal routes using the evolving world model. The Dyna-Q architecture is based on Watkins's Q-learning, a new kind of reinforcement learning. Dyna-Q uses a less familiar set of data structures than does Dyna-PI, but is arguably simpler to implement and use. We show that Dyna-Q architectures are easy to adapt for use in changing environments.
Abstract-found: 1
Intro-found: 1
Reference: <author> Agre, P. E., & Chapman, D. </author> <year> (1987) </year> <month> Pengi: </month> <title> An implementation of a theory of activity. </title> <booktitle> Proceedings of AAAI-87, </booktitle> <pages> 268-272. </pages>
Reference: <author> Anderson, C. W. </author> <title> (1987) Strategy learning with multilayer connectionist representations. </title> <booktitle> Proceedings of the Fourth International Workshop on Machine Learning, </booktitle> <pages> 103-114. </pages> <publisher> Morgan Kaufmann, </publisher> <address> Irvine, CA. </address>
Reference: <author> Barto, A. G., Sutton R. S., & Anderson, C. W. </author> <title> (1983) Neuronlike elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics 13: </journal> <pages> 834-846. </pages>
Reference: <author> Barto, A. G., Sutton, R. S., & Watkins, C. J. C. H. </author> <title> (1989) Learning and sequential decision making. </title> <type> COINS Technical Report 89-95, </type> <institution> Dept. of Computer and Information Science, University of Massachusetts, </institution> <address> Amherst, MA 01003. </address> <note> Also to appear in Learning and Computational Neuroscience, </note> <editor> M. Gabriel and J.W. Moore (Eds.), </editor> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Since the real world can also be sampled, by actually taking actions and observing the result, the world can be used in place of the world model in these methods. In this case, the result is not relaxation planning, but a trial-and-error learning process much like reinforcement learning <ref> (see Barto, Sutton & Watkins, 1989, 1990) </ref>. In Dyna-PI, both of these are done at once. The same algorithm is applied both to real experience (resulting in learning) and to hypothetical experience generated by the world model (resulting in relaxation planning). <p> Watkins (1989) subsequently developed the relationships between the reinforcement-learning architecture and dynamic programming <ref> (see also Barto, Sutton & Watkins, 1989, 1990) </ref> and, moreover, proposed a slightly different kind of reinforcement learning called Q-learning. The Dyna-Q architecture is the combination of this new kind of learning with the Dyna idea of using a learned world model to generate hypothetical experience and achieve planning.
Reference: <author> Barto, A. G., Sutton, R. S., & Watkins, C. J. C. H. </author> <title> (1990) Sequential decision problems and neural networks. </title> <booktitle> In Advances in Neural Information Processing Systems 2, </booktitle> <editor> D. S. Touretzky, Ed. </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Bellman, R. E. </author> <title> (1957) Dynamic Programming. </title> <publisher> Prince-ton University Press, </publisher> <address> Princeton, NJ. </address>
Reference: <author> Craik, K. J. W. </author> <title> (1943) The Nature of Explanation. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, UK. </address>
Reference: <author> Dennett, D. C. </author> <title> (1978) Why the law of effect will not go away. </title> <editor> In Brainstorms, by D. C. </editor> <booktitle> Dennett, </booktitle> <pages> 71-89, </pages> <publisher> Bradford Books, </publisher> <address> Montgomery, Vermont. </address>
Reference: <author> Howard, R. A. </author> <title> (1960) Dynamic Programming and Markov Processes. </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference-contexts: et al., 1986) have presented results for the specific idea of augmenting a reinforcement learning system with a world model used for planning. 2 Dyna-PI: Dyna by Approximating Policy Iteration I call the first Dyna architecture Dyna-PI because it is based on approximating a DP method known as policy iteration <ref> (Howard, 1960) </ref>. The Dyna-PI architecture consists of four components interacting as shown in Figure 1.
Reference: <author> Kaelbling, L. </author> <title> (in preparation) Learning in Embedded Systems. </title> <institution> Stanford Computer Science Ph.D. Dissertation. </institution>
Reference: <author> Korf, R. E. </author> <title> (1990) Real-Time Heuristic Search. </title> <booktitle> Artificial Intelligence 42: </booktitle> <pages> 189-211. </pages>
Reference-contexts: The theory of Dyna is based on the theory of DP (e.g., Ross, 1983) and on DP's relationship to reinforcement learning (Watkins, 1989; Barto, Sutton & Watkins, 1989, 1990), to temporal-difference learning (Sutton, 1988), and to AI methods for planning and search <ref> (Korf, 1990) </ref>.
Reference: <author> Mozer, M. C., & Bachrach, J. </author> <title> (1990) Discovering the structure of a reactive environment by exploration. </title> <booktitle> In Advances in Neural Information Processing Systems 2, </booktitle> <editor> D. S. Touretzky, Ed. </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address> <note> See also Technical Report CU-CS-451-89, </note> <institution> Dept. of Computer Science, University of Colorado at Boulder 80309. </institution>
Reference: <author> Rivest, R. L., & Schapire, R. E. </author> <title> (1987) A new approach to unsupervised learning in deterministic environments. </title> <booktitle> Proceedings of the Fourth International Workshop on Machine Learning, </booktitle> <pages> 364-375. </pages> <publisher> Morgan Kaufmann, </publisher> <address> Irvine, CA. </address>
Reference: <author> Ross, S. </author> <title> (1983) Introduction to Stochastic Dynamic Programming. </title> <publisher> Academic Press, </publisher> <address> New York. </address>
Reference-contexts: Intuitively, Dyna is based on the old idea that planning is like trial-and-error learning from hypothetical experience (Craik, 1943; Dennett, 1978). The theory of Dyna is based on the theory of DP <ref> (e.g., Ross, 1983) </ref> and on DP's relationship to reinforcement learning (Watkins, 1989; Barto, Sutton & Watkins, 1989, 1990), to temporal-difference learning (Sutton, 1988), and to AI methods for planning and search (Korf, 1990). <p> For example, this theory guarantees convergence to a unique evaluation function satisfying (1) and that the corresponding policy is optimal <ref> (Ross, 1983) </ref>. The evaluation function and policy need not be tables, but can be more compact function approxima-tors such as decision trees, k-d trees, connectionist networks, or symbolic rules.
Reference: <author> Rumelhart, D. E., Smolensky, P., McClelland, J. L., & Hinton, G. E. </author> <title> (1986) Schemata and sequential thought processes in PDP models. In Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Volume II, </title> <editor> by J. L. McClelland, D. E. Rumel-hart, </editor> <booktitle> and the PDP research group, </booktitle> <pages> 7-57. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Russell, S. J. </author> <title> (1989) Execution architectures and compilation. </title> <booktitle> Proceedings of IJCAI-89, </booktitle> <pages> 15-20. </pages>
Reference-contexts: The world represents the task to be solved; prototypically it is the robot's external environment. The world receives actions from the policy and produces a next state output and a reward output. The overall task is defined as maximizing the long-term average reward per time step <ref> (cf. Russell, 1989) </ref>. The architecture also includes an explicit world model. The world model is intended to mimic the one-step input-output behavior of the real world. Finally, the Dyna-PI architecture includes an evaluation function that rapidly maps states to values, much as the policy rapidly maps states to actions.
Reference: <author> Sutton, R. S. </author> <title> (1984) Temporal credit assignment in reinforcement learning. </title> <type> Doctoral dissertation, </type> <institution> Department of Computer and Information Science, University of Massachusetts, </institution> <address> Amherst, MA 01003. </address>
Reference: <author> Sutton, </author> <title> R.S. (1988) Learning to predict by the methods of temporal differences. </title> <booktitle> Machine Learning 3: </booktitle> <pages> 9-44. </pages>
Reference-contexts: The theory of Dyna is based on the theory of DP (e.g., Ross, 1983) and on DP's relationship to reinforcement learning (Watkins, 1989; Barto, Sutton & Watkins, 1989, 1990), to temporal-difference learning <ref> (Sutton, 1988) </ref>, and to AI methods for planning and search (Korf, 1990).
Reference: <author> Sutton, </author> <title> R.S., Barto, A.G. (1981) An adaptive network that constructs and uses an internal model of its environment. </title> <journal> Cognition and Brain Theory Quarterly 4: </journal> <pages> 217-246. </pages>
Reference: <author> Sutton, R.S., Pinette, B. </author> <title> (1985) The learning of world models by connectionist networks. </title> <booktitle> Proceedings of the Seventh Annual Conf. of the Cognitive Science Society, </booktitle> <pages> 54-64. </pages> <publisher> Lawrence Erlbaum, </publisher> <address> Hillsdale, NJ. </address>
Reference: <author> Watkins, C. J. C. H. </author> <title> (1989) Learning with Delayed Rewards. </title> <type> PhD thesis, </type> <institution> Cambridge University Psychology Department. </institution>
Reference-contexts: Since the real world can also be sampled, by actually taking actions and observing the result, the world can be used in place of the world model in these methods. In this case, the result is not relaxation planning, but a trial-and-error learning process much like reinforcement learning <ref> (see Barto, Sutton & Watkins, 1989, 1990) </ref>. In Dyna-PI, both of these are done at once. The same algorithm is applied both to real experience (resulting in learning) and to hypothetical experience generated by the world model (resulting in relaxation planning). <p> Watkins (1989) subsequently developed the relationships between the reinforcement-learning architecture and dynamic programming <ref> (see also Barto, Sutton & Watkins, 1989, 1990) </ref> and, moreover, proposed a slightly different kind of reinforcement learning called Q-learning. The Dyna-Q architecture is the combination of this new kind of learning with the Dyna idea of using a learned world model to generate hypothetical experience and achieve planning.
Reference: <author> Werbos, P. J. </author> <title> (1987) Building and understanding adaptive systems: A statistical/numerical approach to factory automation and brain research. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <month> Jan-Feb. </month>
Reference: <author> Whitehead, S. D. </author> <title> (1989) Scaling reinforcement learning systems. </title> <type> Technical Report 304, </type> <institution> Dept. of Computer Science, University of Rochester, Rochester, </institution> <address> NY 14627. </address>
References-found: 23

