URL: ftp://dis.cs.umass.edu/pub/or-icmas96-naghi.ps
Refering-URL: http://dis.cs.umass.edu/research/roles.html
Root-URL: 
Email: fnagendra,lesserg@cs.umass.edu  lander@bbtech.com  
Phone: 2  
Title: Learning Organizational Roles in a Heterogeneous Multi-agent System  
Author: M. V. Nagendra Prasad Victor R. Lesser and Susan E. Lander 
Address: Amherst, MA 01003  401 Main Street Amherst, MA 0l002  
Affiliation: 1 Department of Computer Science University of Massachusetts  Blackboard Technology Group, Inc.  
Abstract: This paper presents studies in learning a form of organizational knowledge called organizational roles in a multi-agent agent system. It attempts to demonstrate the viability and utility of self-organization in an agent-based system involving complex interactions within the agent set. We present a multi-agent parametric design system called L-TEAM where a set of heterogeneous agents learn their organizational roles in negotiated search for mutually acceptable designs. We tested the system on a steam condenser design domain and empirically demonstrated its usefulness. L-TEAM produced better results than its non-learning predecessor, TEAM, which required elaborate knowledge engineering to hand-code organizational roles for its agent set. In addition, we discuss experiments with L-TEAM that highlight the importance of certain learning issues in multi-agent systems. 
Abstract-found: 1
Intro-found: 1
Reference: <author> A. Barto, R. S., and Watkins, C. </author> <year> 1990. </year> <title> Learning and sequential decision making. </title> <editor> In Gariel, M., and Moore, J. W., eds., </editor> <booktitle> Learning and Computational Neu-roscience. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Garland, A., and Alterman, R. </author> <year> 1996. </year> <title> Multi-agent learning through collective memory. </title> <booktitle> In Proceedings of the 1996 AAAI Spring Symposium on Adaptation, Co-evolution and Learning in Multiagent Systems. </booktitle>
Reference-contexts: Though Weiss (Weiss 1994) studies this system in the blocks world domain, it could represent an interesting alternative to the learning mechanism we proposed in this paper for learning organizational knowledge. Nagendra Prasad et al.(Nagendra Prasad, Lesser, & Lander 1996) and Garland and Alterman <ref> (Garland & Alterman 1996) </ref> discuss issues in knowledge reuse in multi-agent systems.
Reference: <author> Holland, J. H. </author> <year> 1985. </year> <title> Properties of bucket brigade algorithm. </title> <booktitle> In First International Conference on Genetic Algorithms and their Applications, </booktitle> <pages> 1-7. </pages>
Reference-contexts: In L-TEAM, the concept of potential leads to different organizations and better quality results and is not a just a speedup device. A related work using classifier systems for learning suitable multi-agent organizations is presented in Weiss (Weiss 1994). Multiple agents use a variant of Holland's <ref> (Holland 1985) </ref> bucket brigade algorithm to learn appropriate instantiations of hierarchical organizations. Though Weiss (Weiss 1994) studies this system in the blocks world domain, it could represent an interesting alternative to the learning mechanism we proposed in this paper for learning organizational knowledge.
Reference: <author> Lander, S. E., and Lesser, V. R. </author> <year> 1994. </year> <title> Sharing meta-information to guide cooperative search among heterogeneous reusable agents. </title> <institution> Computer Science Technical Report 94-48, University of Massachusetts. </institution> <note> To appear in IEEE Transactions on Knowledge and Data Engineering, </note> <year> 1996. </year>
Reference-contexts: Introduction Requirements like reusability of legacy systems and heterogeneity of agent representations lead to a number of challenging issues in Multi-agent Systems (MAS). Lander and Lesser <ref> (Lander & Lesser 1994) </ref> developed the TEAM framework to examine some of these issues in heterogeneous reusable agents in the context of parametric design. TEAM is an open system assembled through minimally customized integration of a dynamically selected subset of a catalogue of existing agents. <p> This decision is complicated by the fact that an agent has to achieve this choice within its local view of the problem-solving situation <ref> (Lander 1994) </ref>. The objective of this paper is to investigate the utility of machine learning techniques as an aid to such a decision process in situations where the set of agents involved in problem solving are not necessarily known to the designer of any single agent. <p> be the learning rate. (p+1) P ot k ij + ff (P ot T (p) P ot k where state n 2 situation j Experimental Results To demonstrate the effectiveness of the mechanisms in L-TEAM and compare them to those in TEAM, we used the same domain as in Lander <ref> (Lander 1994) </ref> | parametric design of steam condensers. The prototype multi-agent system for this domain, built on top of the TEAM framework, consists of seven agents: pump-agent, heat-exchanger-agent, motor-agent, vbelt-agent, shaft-agent, platform-agent, and frequency-critic. The problem solving process starts by placing a problem specification on a central blackboard (BB).
Reference: <author> Lander, S. E. </author> <year> 1994. </year> <title> Distributed Search in Heterogeneous and Reusable Multi-Agent Systems. </title> <type> Ph.D. Dissertation, </type> <institution> Dept. of Computer Sceince, University of Massachusetts, Amherst. </institution>
Reference-contexts: Introduction Requirements like reusability of legacy systems and heterogeneity of agent representations lead to a number of challenging issues in Multi-agent Systems (MAS). Lander and Lesser <ref> (Lander & Lesser 1994) </ref> developed the TEAM framework to examine some of these issues in heterogeneous reusable agents in the context of parametric design. TEAM is an open system assembled through minimally customized integration of a dynamically selected subset of a catalogue of existing agents. <p> This decision is complicated by the fact that an agent has to achieve this choice within its local view of the problem-solving situation <ref> (Lander 1994) </ref>. The objective of this paper is to investigate the utility of machine learning techniques as an aid to such a decision process in situations where the set of agents involved in problem solving are not necessarily known to the designer of any single agent. <p> be the learning rate. (p+1) P ot k ij + ff (P ot T (p) P ot k where state n 2 situation j Experimental Results To demonstrate the effectiveness of the mechanisms in L-TEAM and compare them to those in TEAM, we used the same domain as in Lander <ref> (Lander 1994) </ref> | parametric design of steam condensers. The prototype multi-agent system for this domain, built on top of the TEAM framework, consists of seven agents: pump-agent, heat-exchanger-agent, motor-agent, vbelt-agent, shaft-agent, platform-agent, and frequency-critic. The problem solving process starts by placing a problem specification on a central blackboard (BB).
Reference: <author> Lesser, V. R., and Erman, L. D. </author> <year> 1980. </year> <title> Distributed interpretation: A model and an experiment. </title> <journal> IEEE Transactions on Computers C-29(12):1144-1163. </journal>
Reference-contexts: On a few occasions, situation-specific-L-TEAM performed worse than non-situation-specific-L-TEAM. We attribute this observation to the phenomenon of distraction frequently observed in multi-agent systems <ref> (Lesser & Erman 1980) </ref>. In the context of role assignments, this phenomenon maps to the ability of the agents to judge whether it is effective to work on its own designs or respond to the designs generated by the other members of the agent set in the present situation.
Reference: <author> Mataric, M. J. </author> <year> 1994. </year> <title> Reward functions for accelerated learning. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning. </booktitle>
Reference-contexts: L-TEAM is one of the few multi-agent systems demonstrating the viability of learning problem solving control for realistic and complex domains. We believe that importance of concepts like "potential" become more apparent in such domains. Mataric <ref> (Mataric 1994) </ref> discusses the concept of progress estimators akin to the idea of potential. Potential differs from progress estimators in that the later was primarily used as a method of speeding up reinforcement learning whereas the former plays a more complex role.
Reference: <author> Nagendra Prasad, M. V.; Lesser, V. R.; and Lander, S. E. </author> <year> 1996. </year> <title> Retrieval and reasoning in distributed case bases. Journal of Visual Communication and Image Representation, </title> <booktitle> Special Issue on Digital Libraries 7(1) </booktitle> <pages> 74-87. </pages>
Reference: <author> Sandholm, T., and Crites, R. </author> <year> 1995. </year> <title> Multi-agent reinforcement learning in the repeated prisoner's dilemma. </title> <note> to appear in Biosystems. </note>
Reference-contexts: Identifying such actions and rewarding the learning system for them can lead to an enhanced performance. Related Work Previous work related to learning in multi-agent systems is limited. Tan (Tan 1993), Sandholm and Crites <ref> (Sandholm & Crites 1995) </ref>, and Sen and Sekaran (Sen, Sekaran, & Hale 1994) discuss multi-agent reinforcement learning systems. All these systems rely on reinforcement learning methods (A. Barto & Watkins 1990; Sutton 1988).
Reference: <author> Sen, S.; Sekaran, M.; and Hale, J. </author> <year> 1994. </year> <title> Learning to coordinate without sharing information. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> 426-431. </pages> <address> Seattle, WA: </address> <publisher> AAAI. </publisher>
Reference-contexts: Identifying such actions and rewarding the learning system for them can lead to an enhanced performance. Related Work Previous work related to learning in multi-agent systems is limited. Tan (Tan 1993), Sandholm and Crites (Sandholm & Crites 1995), and Sen and Sekaran <ref> (Sen, Sekaran, & Hale 1994) </ref> discuss multi-agent reinforcement learning systems. All these systems rely on reinforcement learning methods (A. Barto & Watkins 1990; Sutton 1988). While these works highlight interesting aspects of multi-agent learning systems, they are primarily centered around toy problems on a grid world.
Reference: <author> Sian, S. S. </author> <year> 1991. </year> <title> Extending learning to multiple agents: issues and a model for multi-agent machine learning. </title> <booktitle> In Proceedings of Machine Learning - EWSL 91, </booktitle> <pages> 440-456. </pages>
Reference-contexts: Certain multi-agent learning systems in the literature deal with a different task from that presented in this paper. Systems like ILS (Silver et al. 1990) and MALE <ref> (Sian 1991) </ref> use multi-agent techniques to build hybrid learners from multiple learning agents. On the other hand, L-TEAM learns problem-solving control for multi-agent systems. Implications and Conclusion Previous work in self-organization for efficient distributed search control has, for the most part, involved simple agents with simple interaction patterns.
Reference: <author> Silver, B.; Frawely, W.; Iba, G.; Vittal, J.; and Brad-ford, K. </author> <year> 1990. </year> <title> A framework for multi-paradigmatic learning. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> 348-358. </pages>
Reference-contexts: Certain multi-agent learning systems in the literature deal with a different task from that presented in this paper. Systems like ILS <ref> (Silver et al. 1990) </ref> and MALE (Sian 1991) use multi-agent techniques to build hybrid learners from multiple learning agents. On the other hand, L-TEAM learns problem-solving control for multi-agent systems.
Reference: <author> Sugawara, T., and Lesser, V. R. </author> <year> 1993. </year> <title> On-line learning of coordination plans. </title> <booktitle> In Proceedings of the Twelfth International Workshop on Distributed AI. </booktitle>
Reference-contexts: Nagendra Prasad et al.(Nagendra Prasad, Lesser, & Lander 1996) and Garland and Alterman (Garland & Alterman 1996) discuss issues in knowledge reuse in multi-agent systems. Sugawra and Lesser <ref> (Sugawara & Lesser 1993) </ref> discuss a distributed network-diagnosis system where each local segment of the network has an intelligent diagnosis agent called LODES that monitors traffic on the network and uses an explanation-based learning technique to develop coordination rules for the LODES agents.
Reference: <author> Sutton, R. </author> <year> 1988. </year> <title> Learning to predict by the methods of temporal differences. </title> <booktitle> Machine Learning 3 </booktitle> <pages> 9-44. </pages>
Reference-contexts: In L-TEAM, an agent choosing a role indexes into a database of UPC values using the situation vector to obtain the relevant UPC values for the roles applicable in the current state. We use the supervised-learning approach to prediction learning (see <ref> (Sutton 1988) </ref>) to learn estimates for the UPC vectors for each of the situations. The agents collectively explore the space of possible role assignments to identify good role assignments in each of the situations.
Reference: <author> Tan, M. </author> <year> 1993. </year> <title> Multi-agent reinforcement learning: Independent vs. </title> <booktitle> cooperative agents. In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> 330-337. </pages>
Reference-contexts: Identifying such actions and rewarding the learning system for them can lead to an enhanced performance. Related Work Previous work related to learning in multi-agent systems is limited. Tan <ref> (Tan 1993) </ref>, Sandholm and Crites (Sandholm & Crites 1995), and Sen and Sekaran (Sen, Sekaran, & Hale 1994) discuss multi-agent reinforcement learning systems. All these systems rely on reinforcement learning methods (A. Barto & Watkins 1990; Sutton 1988).
Reference: <author> Weiss, G. </author> <year> 1994. </year> <title> Some studies in distributed machine learning and organizational design. </title> <type> Technical Report FKI-189-94, </type> <institution> Institut fur Informatik, TU Munchen. </institution>
Reference-contexts: In L-TEAM, the concept of potential leads to different organizations and better quality results and is not a just a speedup device. A related work using classifier systems for learning suitable multi-agent organizations is presented in Weiss <ref> (Weiss 1994) </ref>. Multiple agents use a variant of Holland's (Holland 1985) bucket brigade algorithm to learn appropriate instantiations of hierarchical organizations. Though Weiss (Weiss 1994) studies this system in the blocks world domain, it could represent an interesting alternative to the learning mechanism we proposed in this paper for learning organizational <p> A related work using classifier systems for learning suitable multi-agent organizations is presented in Weiss <ref> (Weiss 1994) </ref>. Multiple agents use a variant of Holland's (Holland 1985) bucket brigade algorithm to learn appropriate instantiations of hierarchical organizations. Though Weiss (Weiss 1994) studies this system in the blocks world domain, it could represent an interesting alternative to the learning mechanism we proposed in this paper for learning organizational knowledge.
Reference: <author> Whitehair, R., and Lesser, V. R. </author> <year> 1993. </year> <title> A framework for the analysis of sophisticated control in interpretation systems. </title> <institution> Computer Science Technical Report 93-53, University of Massachusetts. </institution>
Reference-contexts: Treating problem solving control as situation-specific can be beneficial (discussed in detail in Sec tion ). The rest of the paper is organized as follows. Section discusses the characteristics of organizational roles in distributed search and Section presents our use of the UPC formalism <ref> (Whitehair & Lesser 1993) </ref> as a basis for learning organizational knowledge and discusses our learning setup. The following section discusses an implementation of L-TEAM based on this algorithm and presents the results of our empirical explorations. We conclude by discussing some related work and the implications of this work. <p> Learning Role Assignments Learning involves exploring the space of role assignments, developing rating measures for roles in various situations. The formal basis for learning role assignments is derived from the UPC formalism for search control (see Whitehair and Lesser <ref> (Whitehair & Lesser 1993) </ref>) that relies on the calculation and use of the Utility, Probability and Cost (UPC) values associated with each hstate; R; f inal statei tuple.
References-found: 17

