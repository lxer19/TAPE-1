URL: http://www.ics.uci.edu/~pazzani/Publications/An-Investigation-MLW-91.ps
Refering-URL: http://www.ics.uci.edu/~mlearn/MLPapers.html
Root-URL: 
Email: brunk@ics.uci.edu  
Title: An investigation of noise-tolerant relational concept learning algorithms  
Author: Clifford A. Brunk and Michael J. Pazzani 
Address: Irvine, CA 92717 USA  
Affiliation: Department of Information and Computer Science University of California, Irvine  
Abstract: We discuss the types of noise that may occur in relational learning systems and describe two approaches to addressing noise in a relational concept learning algorithm. We then evaluate each approach experimentally.
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D., & Kibler, D. </author> <year> (1989). </year> <title> Noise tolerant instance-based learning algorithms. </title> <booktitle> Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <address> Detroit, MI: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Cohen, W. </author> <year> (1990). </year> <title> Abductive explanation-based learning: A solution to the multiple explanation-problem (ML-TR-29). </title> <address> New Brunswick, NJ: </address> <institution> Rutgers University. </institution>
Reference-contexts: It could be applied to the results of inductive systems such as GOLEM (Muggleton & Feng, 1990); explanation-based learning systems (Mitchell, Keller, & Cedar-Kabelli, 1986) or systems that combine explanation-based and empirical learning such as FOCL (Pazzani & Kibler, 1990; Pazzani, Brunk, & Silverstein, 1991), A-EBL <ref> (Cohen, 1990) </ref> or IOE (Flann & Dietterich, 1989). However, the de le te-la st-l iter a l o p e r a t o r would need to be replaced by a del ete any-li ter al operator.
Reference: <author> Dolsak, B., & Muggleton, S. </author> <year> (1991). </year> <title> The application of inductive logic programming to finite element mesh design. </title> <booktitle> The First International Workshop on Inductive Logic Programming, Porto, </booktitle> <address> Portugal. </address>
Reference: <author> Flann, N., & Dietterich, T. </author> <year> (1989). </year> <title> A study of explanation-based methods for inductive learning. </title> <journal> Machine Learning, </journal> <volume> 4, </volume> <pages> 187226. </pages>
Reference-contexts: It could be applied to the results of inductive systems such as GOLEM (Muggleton & Feng, 1990); explanation-based learning systems (Mitchell, Keller, & Cedar-Kabelli, 1986) or systems that combine explanation-based and empirical learning such as FOCL (Pazzani & Kibler, 1990; Pazzani, Brunk, & Silverstein, 1991), A-EBL (Cohen, 1990) or IOE <ref> (Flann & Dietterich, 1989) </ref>. However, the de le te-la st-l iter a l o p e r a t o r would need to be replaced by a del ete any-li ter al operator. In FOIL, literals are learned in an order that makes the less expensive delete-last-literal operator feasible.
Reference: <author> Mitchell, T., Keller, R., & Kedar-Cabelli, S. </author> <year> (1986). </year> <title> Explanation-based learning: A unifying view. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 4780. </pages>
Reference-contexts: One advantage of reduced error pruning is that it is independent of the algorithm used to learn the concept description. It could be applied to the results of inductive systems such as GOLEM (Muggleton & Feng, 1990); explanation-based learning systems <ref> (Mitchell, Keller, & Cedar-Kabelli, 1986) </ref> or systems that combine explanation-based and empirical learning such as FOCL (Pazzani & Kibler, 1990; Pazzani, Brunk, & Silverstein, 1991), A-EBL (Cohen, 1990) or IOE (Flann & Dietterich, 1989).
Reference: <author> Mingers, J. </author> <title> (1989) An empirical comparison of pruning methods for decision tree induction. </title> <journal> Machine Learning, </journal> <volume> 4, </volume> <pages> 227243. </pages>
Reference: <author> Muggleton, S., & Feng, C. </author> <title> (1990) Efficient induction of logic programs. </title> <booktitle> Proceedings of the First Conference on Algorithmic Learning Theory, </booktitle> <address> Tokyo, Japan. </address> <publisher> Ohmsha. </publisher>
Reference-contexts: 1 INTRODUCTION Recently, there have been several advances in relational concept learning (Muggleton & Feng, 1990; Quinlan, 1990) that may enable large applications to be implemented <ref> (e.g., Dolsak & Muggleton, 1990) </ref>. A characteristic problem of large learning applications is that there will inevitably be some type of noise in the training data. In this paper, we first discuss learning when there is noise in the training data. <p> At this point pruning terminates and the pruned concept description is returned. One advantage of reduced error pruning is that it is independent of the algorithm used to learn the concept description. It could be applied to the results of inductive systems such as GOLEM <ref> (Muggleton & Feng, 1990) </ref>; explanation-based learning systems (Mitchell, Keller, & Cedar-Kabelli, 1986) or systems that combine explanation-based and empirical learning such as FOCL (Pazzani & Kibler, 1990; Pazzani, Brunk, & Silverstein, 1991), A-EBL (Cohen, 1990) or IOE (Flann & Dietterich, 1989).
Reference: <author> Pagallo, G., & Haussler, D. </author> <year> (1990). </year> <title> Boolean Feature Discovery in Empirical Learning. </title> <journal> Machine Learning, </journal> <volume> 5, </volume> <pages> 7199. </pages>
Reference-contexts: Next, we describe the FOIL algorithm, concentrating on how it uses an encoding length metric to prevent overfitting the data. We also review reduced error pruning, an approach to dealing with noise that has been successfully applied to decision trees (Quinlan, 1987) and decision lists <ref> (Pagallo & Haussler, 1990) </ref>. Finally, we report on a series of experiments in which we introduce noise into artificially generated training data for a king-rook-king board classification problem.
Reference: <author> Pazzani, M., Brunk, C., & Silverstein, G. </author> <year> (1991). </year> <title> A n information-based approach to integrating empirical and explanation-based learning (Technical Report No. </title> <type> 9038). </type> <institution> Irvine,CA: University of California, Department of Information & Computer Science. </institution>
Reference: <author> Pazzani, M., & Kibler, D. </author> <year> (1990). </year> <title> The utility of knowledge in inductive learning (Technical Report No. </title> <type> 9018). </type> <institution> Irvine,CA: University of California, Department of Information & Computer Science. </institution>
Reference-contexts: q u a l ( X , Y ) , a d j a c e n t ( X , Y ) and between (X,Y,Z) are used (where between (X,Y,Z) is defined as less_than (X,Y) & less_than (Y,Z)), FOIL can learn a definition of illegal that is 100% accurate <ref> (Pazzani & Kibler, 1990) </ref>. We believe this occurs because the 100% accurate definition makes use of not (between (E,A,C)).
Reference: <author> Quinlan, J.R. </author> <year> (1986a). </year> <title> The effect of noise on concept learning, </title> <editor> in R. Michalski, J. Carbonell, & T. Mitchell (Eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach (Vol. 2). </booktitle> <address> Los Altos, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Quinlan, J.R. </author> <year> (1986b). </year> <title> Induction of decision trees." </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 81106. </pages>
Reference: <author> Quinlan, J.R. </author> <year> (1987). </year> <title> Simplifying decision trees. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 27, 221 234. </volume>
Reference-contexts: Next, we describe the FOIL algorithm, concentrating on how it uses an encoding length metric to prevent overfitting the data. We also review reduced error pruning, an approach to dealing with noise that has been successfully applied to decision trees <ref> (Quinlan, 1987) </ref> and decision lists (Pagallo & Haussler, 1990). Finally, we report on a series of experiments in which we introduce noise into artificially generated training data for a king-rook-king board classification problem.
Reference: <author> Quinlan, J.R. </author> <year> (1990). </year> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5, </volume> <pages> 239266. </pages>
Reference-contexts: A first pass at a taxonomy of missing values can be arrived at by replacing "incorrect" with "unknown" in the previous discussion. However, in this paper we will restrict our attention to just noisy training data. 3 THE LEARNING ALGORITHM FOIL <ref> (Quinlan, 1990) </ref> is a relational learner which uses a separate and conquer approach guided by an information based heuristic to produce a concept description that covers all the positive examples and excludes all the negative examples.
References-found: 14

