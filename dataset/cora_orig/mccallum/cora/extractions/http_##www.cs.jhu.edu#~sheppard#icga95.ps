URL: http://www.cs.jhu.edu/~sheppard/icga95.ps
Refering-URL: http://www.cs.jhu.edu/~sheppard/pubs.html
Root-URL: 
Email: lastname@cs.jhu.edu  
Title: Combining Genetic Algorithms with Memory Based Reasoning  
Author: John W. Sheppard and Steven L. Salzberg 
Address: Baltimore, Maryland 21218  
Affiliation: Department of Computer Science The Johns Hopkins University  
Abstract: Combining different machine learning algorithms in the same system can produce benefits above and beyond what either method could achieve alone. This paper demonstrates that genetic algorithms can be used in conjunction with memory-based reasoning to solve a difficult class of delayed reinforcement learning problems that both methods have trouble solving individually. This class includes numerous important control problems that arise in robotics, planning, game playing, and other areas. Our experiments demonstrate that by using one learning technique, genetic algorithms, as a bootstrapping method for the second learning technique, memory-based reasoning, we can create a system that outperforms either method alone. The resulting joint system learns to solve a difficult reinforcement learning task with a high degree of accuracy and with relatively small memory requirements.
Abstract-found: 1
Intro-found: 1
Reference: <author> D. Aha and D. Kibler. </author> <title> Noise-tolerant instance-based learning algorithms. </title> <booktitle> In Proceedings of IJCAI-89, </booktitle> <pages> pages 794-799, </pages> <address> Detroit, MI, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Another possible reason for MBR's problems is the presence of irrelevant attributes, which is known to cause problems for nearest neighbor algorithms <ref> (Aha and Kibler 1989, Salzberg 1991) </ref>. But the most likely reason for MBR's troubles, we concluded, was that we were generating bad examples in the early phases of the game.
Reference: <author> D. Aha and S. Salzberg. </author> <title> Learning to catch: Applying nearest neighbor algorithms to dynamic control tasks. </title> <booktitle> In Proceedings of the Fourth International Workshop on AI and Statistics, </booktitle> <year> 1993. </year>
Reference: <author> C. Atkeson. </author> <title> Using local models to control movement. </title> <booktitle> In Neural Information Systems Conference, </booktitle> <year> 1989. </year>
Reference: <author> L. Booker, D. Goldberg, and J. Holland. </author> <title> Classifier systems and genetic algorithms. </title> <journal> Artificial Intelligence, </journal> <volume> 40 </volume> <pages> 235-282, </pages> <year> 1989. </year>
Reference-contexts: The GA is explained next. 5 THE GENETIC ALGORITHM Genetic algorithm implementations for games require, first, a representation of the domain in terms of production rules, which test some features of the domain and then take an action <ref> (Booker et al. 1989, Hol-land 1975) </ref>. The state variables in a pursuit game are primarily numeric variables measuring the relative positions and speeds of P and E, and the conclusion is E's decision about what action to take.
Reference: <author> A. Barto, R. Sutton, and C. Anderson. </author> <title> Neuronlike adaptive elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 13 </volume> <pages> 835-846, </pages> <year> 1983. </year>
Reference-contexts: Temporal difference methods apply delayed reinforcement to a sequence of actions to predict future reinforcement and appropriate actions in performing the task. Specifically, predictions are refined through a process of identifying differences between the results of temporally successive actions. Two popular temporal difference algorithms are ACE/ASE <ref> (Barto et al. 1983, Barto et al. 1990) </ref> and Q-learning (Watkins 1989). The original work by Barto et al. (1983) demonstrated that the cart and pole problem could be solved using this method. <p> Neither the GA nor MBR were able to obtain such a high success rate on their own, after any number of trials. 6.1 OTHER TEACHING STRATEGIES Little other research has addressed teaching strategies for reinforcement learning problems. One approach that was investigated was Barto's ACE/ASE <ref> (Barto et al. 1983) </ref>. This differs from the bootstrapping approach in that no feedback is provided to the GA to modify its learning algorithm. Further, ACE/ASE are both connectionist architectures whose weights are modified based on reinforcement received from experience. In our model, only the GA learns from reinforcement.
Reference: <author> A. Barto, R. Sutton, and C. Watkins. </author> <title> Learning and sequential decision making. </title> <editor> In Gabriel and Moore, editors, </editor> <booktitle> Learning and Computational Neuroscience, Cam-bridge, 1990. </booktitle> <publisher> MIT Press. </publisher>
Reference: <author> D. Chapman. </author> <title> Planning for conjunctive goals. </title> <journal> Artificial Intelligence, </journal> <volume> 32 </volume> <pages> 333-377, </pages> <year> 1987. </year>
Reference-contexts: The more complicated task, which is described further in section 3, also resembles complicated planning tasks in which an agent has to satisfy several goals at once <ref> (Chapman 1987) </ref>. Our study is an attempt to develop a learning strategy that will improve the overall performance of either MBR, GAs, or both. As our experiments will show, we were successful at developing a method to solve our difficult reinforcement learning task.
Reference: <author> J. Clouse and P. Utgoff. </author> <title> A teaching method for reinforcement learning. </title> <booktitle> In Proceedings of the Machine Learning Conference, </booktitle> <year> 1992. </year>
Reference-contexts: Further, ACE/ASE are both connectionist architectures whose weights are modified based on reinforcement received from experience. In our model, only the GA learns from reinforcement. Another related teaching method is that of Clouse and Utgoff <ref> (Clouse and Utgoff 1992) </ref>, who used ACE/ASE with a separate teacher. Their teacher monitored the overall progress of the learning agent, and "reset" the eligibility traces of the two learning elements when the performance failed to improve.
Reference: <author> M. Colombetti and M. Dorigo. </author> <title> Training agents to perform sequential behavior. Adaptive Behavior, </title> <publisher> MIT Press, </publisher> <pages> 2(3) 247-275, </pages> <year> 1994. </year>
Reference: <author> M. Dorigo and M. Colombetti. </author> <title> Robot shaping: Developing autonomous agents through learning. </title> <journal> Artificial Intelligence, </journal> <volume> 71(2) </volume> <pages> 321-370, </pages> <year> 1994. </year>
Reference: <author> A. Friedman. </author> <title> Differential Games. </title> <publisher> Wiley Interscience, </publisher> <address> New York, </address> <year> 1971. </year>
Reference-contexts: We can also interpret differential games to be an extension of optimal control theory in which players' positions develop continuously in time, and where the goal is to optimize competing control laws for the players <ref> (Friedman 1971) </ref>. One class of differential games is the pursuit game. A pursuit game has two players, called the pursuer (P) and the evader (E).
Reference: <author> D. Goldberg. </author> <title> Genetic Algorithms in Search, Optimization, and Machine Learning. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1989. </year>
Reference-contexts: Plan fitness is calculated by running each plan against a set of randomly generated games, and computing the mean payoff for the set of tests. After each game, a rule is selected for mutation, using fitness proportional selection <ref> (Goldberg 1989) </ref>. Once selected, the rule is mutated according to a fixed mutation rate, and each clause or action of the rule is mutated according to a fixed mutation probability.
Reference: <author> J. Grefenstette. </author> <title> Credit assignment in rule discovery systems based on genetic algorithms. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 225-245, </pages> <year> 1988. </year>
Reference: <editor> J. Grefenstette. </editor> <booktitle> Lamarckian learning in multi-agent environments. In Proceedings of the Fourth International Conference of Genetic Algorithms, </booktitle> <pages> pages 303-310. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference: <author> J. Grefenstette, C. Ramsey, and A. Schultz. </author> <title> Learning sequential decision rules using simulation models and competition. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 355-381, </pages> <year> 1990. </year>
Reference-contexts: We began by considering a reinforcement learning problem that involved one agent trying to pursue and capture another. Earlier research showed that at least one implementation of this task, known as evasive maneuvers <ref> (Grefenstette et al. 1990) </ref>, can be solved by a genetic algorithm (GA). We first developed a memory-based approach for the same task, and then made the task substantially harder, in order to study the limitations of both GAs and memory-based reasoning (MBR) methods on this class of problems. <p> After selecting the rule to be mutated, we decided whether to mutate the rule using a 1 While not a part of the genetic algorithm itself, the generalization and specialization operators are consistent with Grefenstette's implementation <ref> (Grefenstette et al. 1990) </ref>. mutation rate of 0.01. Each clause within a rule was considered for mutation with probability 0.1. Crossover operates between plans. After each game, two plans are selected for crossover using fitness proportional selection, with a likelihood determined by the crossover rate.
Reference: <author> J. Holland. </author> <title> Adaptation in Natural and Artificial Systems. </title> <publisher> University of Michigan Press, </publisher> <address> Ann Arbor, Michi-gan, </address> <year> 1975. </year>
Reference: <author> F. Imado and T. Ishihara. </author> <title> Pursuit-evasion geometry analysis between two missiles and an aircraft. </title> <journal> Computers and Mathematics with Applications, </journal> <volume> 26(3) </volume> <pages> 125-139, </pages> <year> 1993. </year>
Reference-contexts: We initially implemented the same pursuit game as Grefenstette et al., and later we extended it to make it substantially more difficult. First, we added a second pursuer. Unlike the single-pursuer problem, the two-pursuer problem has no known optimal strategy <ref> (Imado and Ishihara 1993) </ref>. Second, we gave the evader additional capabilities: in the one-pursuer game, the evader only controls its turn angle at each time step. Thus E basically zigzags back and forth or makes a series of hard turns into the path of P to escape.
Reference: <author> R. Isaacs. </author> <title> Differential games: A mathematical theory with applications to warfare and other topics. </title> <type> Technical Report Research Contribution No. 1, </type> <institution> Center for Naval Analysis, </institution> <address> Washington, DC, </address> <year> 1963. </year>
Reference-contexts: The class of RL problems studied here has also been studied in the field of differential game theory. Differential game theory is an extension of traditional game theory in which a game follows a sequence of actions through a continuous state space to achieve some payoff <ref> (Isaacs 1963) </ref>. This sequence can be modeled with a set of differential equations which are analyzed to determine optimal play by the players.
Reference: <author> L. Lin. </author> <title> Programming robots using reinforcement learning and teaching. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 781-786, </pages> <year> 1991. </year>
Reference: <author> J. Millan and C. Torras. </author> <title> A reinforcement connectionist approach to robot path finding in non-maze-like environments. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 363-395, </pages> <year> 1992. </year>
Reference: <author> J. Mingers. </author> <title> An empirical comparison of pruning methods for decision tree induction. </title> <journal> Machine Learning, </journal> <volume> 4(2) </volume> <pages> 227-243, </pages> <year> 1989. </year>
Reference-contexts: We call the resulting system GANNE (GA plus nearest neighbor plus editing). Early work by Wilson (Wilson 1972) showed that examples could be removed from a set used for classification, and that simply editing would frequently improve classification accuracy (in the same way that pruning improves decision trees <ref> (Mingers 1989) </ref>). Wil-son's algorithm was to classify each example in a data set with its own k nearest neighbors. Those points that are incorrectly classified are deleted from the example set, the idea being that such points probably represent noise.
Reference: <author> A. Moore. </author> <title> Efficient Memory-Based Learnign for Robot Control. </title> <type> PhD thesis, </type> <institution> Cambridge University, </institution> <address> Cam-bridge, England, </address> <year> 1992. </year>
Reference-contexts: In the pattern recognition literature, a variety of algorithms for doing this can be found under the term "editing" nearest neighbors. However, because MBR is not frequently applied to control tasks (except within the context of reinforcement learning; see, e.g., <ref> (Moore 1992) </ref> and (Schaal and Atkeson 1994)), we were not able to find any editing methods specifically tied to our type of problem. We therefore modified a known editing algorithm for our problem. We call the resulting system GANNE (GA plus nearest neighbor plus editing).
Reference: <author> A. Moore and C. Atkeson. </author> <title> Prioritized sweeping: Reinforcement learning with less data and less time. </title> <journal> Machine Learning, </journal> <volume> 13 </volume> <pages> 103-130, </pages> <year> 1993. </year>
Reference: <author> D. Nguyen and B. Widrow. </author> <title> The truck backer-upper: An example of self learning in neural networks. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, </booktitle> <volume> volume 2, </volume> <pages> pages 357-363, </pages> <year> 1989. </year>
Reference-contexts: For example, back-propagation has been used to solve the cart and pole problem (Widrow 1987), in which a pole must be balanced vertically on a wheeled cart. A similar algorithm was applied to learning strategies for backing a truck into a loading dock <ref> (Nguyen and Widrow 1989) </ref>. Both of these methods incorporated knowledge of the correct behavior during training. In addition, Millan and Torras (1992) demonstrated a technique for training a neural network using reinforcement learning in which the control variables are permitted to vary continuously.
Reference: <author> G. Ritter, H. Woodruff, S. Lowry, and T. Isenhour. </author> <title> An algorithm for a selective nearest neighbor decision rule. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 21(6) </volume> <pages> 665-669, </pages> <year> 1975. </year>
Reference: <author> S. Salzberg. </author> <title> Distance metrics for instance-based learning. </title> <booktitle> In Methodologies for Intelligent Systems: 6th International Symposium, ISMIS '91, </booktitle> <pages> pages 399-408, </pages> <year> 1991. </year>
Reference: <author> S. Salzberg, A. Delcher, D. Heath, and S. Kasif. </author> <title> Learning with a helpful teacher. </title> <booktitle> In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 705-711, </pages> <address> Sydney, Australia, August 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> S. Schaal and C. Atkeson. </author> <title> Robot juggling: An implementation of memory-based learning. </title> <journal> Control Systems Magazine, </journal> <month> February </month> <year> 1994. </year>
Reference-contexts: In the pattern recognition literature, a variety of algorithms for doing this can be found under the term "editing" nearest neighbors. However, because MBR is not frequently applied to control tasks (except within the context of reinforcement learning; see, e.g., (Moore 1992) and <ref> (Schaal and Atkeson 1994) </ref>), we were not able to find any editing methods specifically tied to our type of problem. We therefore modified a known editing algorithm for our problem. We call the resulting system GANNE (GA plus nearest neighbor plus editing).
Reference: <author> J. Sheppard and S. Salzberg. </author> <title> Sequential decision making: An empirical analysis of three learning algorithms. </title> <type> Technical Report JHU-93/94-02, </type> <institution> Dept. of Computer Science, Johns Hopkins University, Balti-more, Maryland, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: The difficulty here, then, is how to determine the correct action to store with each state. To illustrate the problems that MBR has with the two-player pursuit game, we briefly describe here some findings of our earlier study <ref> (Sheppard and Salzberg 1993) </ref>, which compared k-NN, GAs, and the temporal difference algorithm called Q-learning (Watkins 1989). In that study, we found that the MBR method, k-NN, was by far the worst method in its performance on this problem.
Reference: <author> R. Sutton. </author> <title> Learning to predict by methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44, </pages> <year> 1988. </year>
Reference-contexts: They addressed the problem of teaching a robot to navigate around obstacles. Finally, considerable research has been performed using a form of reinforcement learning called temporal difference learning <ref> (Sutton 1988) </ref>. Temporal difference methods apply delayed reinforcement to a sequence of actions to predict future reinforcement and appropriate actions in performing the task. Specifically, predictions are refined through a process of identifying differences between the results of temporally successive actions.
Reference: <author> I. Tomek. </author> <title> An experiment with the edited nearest-neighbor rule. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> SMC-6(6):448-452, </volume> <month> June </month> <year> 1976. </year>
Reference-contexts: Wil-son's algorithm was to classify each example in a data set with its own k nearest neighbors. Those points that are incorrectly classified are deleted from the example set, the idea being that such points probably represent noise. Tomek <ref> (Tomek 1976) </ref> modified this approach by taking a sample (&gt; 1) of the data and classifying the sample with the remaining examples. Editing then proceeds as in Wilson editing. Ritter et al. (1975) developed another editing method, which differs from Wilson in that points that are correctly classified are discarded.
Reference: <author> C. Watkins. </author> <title> Learning with Delayed Rewards. </title> <type> PhD thesis, </type> <institution> Cambridge University, </institution> <address> Cambridge, England, </address> <year> 1989. </year>
Reference-contexts: Specifically, predictions are refined through a process of identifying differences between the results of temporally successive actions. Two popular temporal difference algorithms are ACE/ASE (Barto et al. 1983, Barto et al. 1990) and Q-learning <ref> (Watkins 1989) </ref>. The original work by Barto et al. (1983) demonstrated that the cart and pole problem could be solved using this method. <p> To illustrate the problems that MBR has with the two-player pursuit game, we briefly describe here some findings of our earlier study (Sheppard and Salzberg 1993), which compared k-NN, GAs, and the temporal difference algorithm called Q-learning <ref> (Watkins 1989) </ref>. In that study, we found that the MBR method, k-NN, was by far the worst method in its performance on this problem. While Q-learning performed well, the GA was superior to both of the other two methods.
Reference: <author> B. Widrow. </author> <title> The original adaptive neural net broom-balancer. </title> <booktitle> In International Symposium on Circuits and Systems, </booktitle> <pages> pages 351-357, </pages> <year> 1987. </year>
Reference-contexts: One of the most popular approaches to reinforcement learning has been using neural network learning algorithms. For example, back-propagation has been used to solve the cart and pole problem <ref> (Widrow 1987) </ref>, in which a pole must be balanced vertically on a wheeled cart. A similar algorithm was applied to learning strategies for backing a truck into a loading dock (Nguyen and Widrow 1989). Both of these methods incorporated knowledge of the correct behavior during training.
Reference: <author> D. Wilson. </author> <title> Asymptotic properties of nearest neighbor rules using edited data. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 2(3) </volume> <pages> 408-421, </pages> <month> July </month> <year> 1972. </year>
Reference-contexts: We therefore modified a known editing algorithm for our problem. We call the resulting system GANNE (GA plus nearest neighbor plus editing). Early work by Wilson <ref> (Wilson 1972) </ref> showed that examples could be removed from a set used for classification, and that simply editing would frequently improve classification accuracy (in the same way that pruning improves decision trees (Mingers 1989)).
References-found: 34

