URL: ftp://ftp.cs.rochester.edu/pub/u/kthanasi/95.CAN.Efficient_Shared_Memory_Minimal_HW_Support.ps.gz
Refering-URL: http://www.cs.rochester.edu/research/cashmere/97-12_DEC_report/bib.html
Root-URL: 
Email: fkthanasi,scottg@cs.rochester.edu  
Title: Efficient Shared Memory with Minimal Hardware Support  
Author: Leonidas I. Kontothanassis and Michael L. Scott 
Date: July 1995  
Web: http://www.cs.rochester.edu/u/kthanasi/cashmere.html  
Address: Rochester, NY 14627-0226  
Affiliation: Department of Computer Science University of Rochester  
Abstract: Shared memory is widely regarded as a more intuitive model than message passing for the development of parallel programs. A shared memory model can be provided by hardware, software, or some combination of both. One of the most important problems to be solved in shared memory environments is that of cache coherence. Experience indicates, unsurprisingly, that hardware-coherent multiprocessors greatly outperform distributed shared-memory (DSM) emulations on message-passing hardware. Intermediate options, however, have received considerably less attention. We argue in this position paper that one such optiona multiprocessor or network that provides a global physical address space in which processors can make non-coherent accesses to remote memory without trapping into the kernel or interrupting remote processorscan provide most of the performance of hardware cache coherence at little more monetary or design cost than traditional DSM systems. To support this claim we have developed the Cashmere family of software coherence protocols for NCC-NUMA (Non-Cache-Coherent, Non-Uniform-Memory Access) systems, and have used execution-driven simulation to compare the performance of these protocols to that of full hardware coherence and distributed shared memory emulation. We have found that for a large class of applications the performance of NCC-NUMA multiprocessors rivals that of fully hardware-coherent designs, and significantly surpasses the performance realized on more traditional DSM systems.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Blumrich, K. Li, R. Alpert, C. Dubnicki, E. Felten, and J. Sandberg. </author> <title> Virtual Memory Mapped Network Interface for the SHRIMP Multicomputer. </title> <booktitle> In Proceedings of the Twenty-First International Symposium on Computer Architecture, </booktitle> <pages> pages 142-153, </pages> <address> Chicago, IL, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: Machines in this non-cache-coherent, non-uniform memory access (NCC-NUMA) class include both tightly-coupled (single chassis) multiprocessors (e.g. the BBN TC2000, the Toronto Hector [21], and the Cray Research T3D), and memory-mapped network interfaces for workstations (e.g. the Princeton Shrimp <ref> [1] </ref>, the DEC Memory Channel [5], and the HP Hamlyn [22]). In comparison to hardware-coherent machines, NCC-NUMA systems can more easily be built from commodity parts, and can follow improvements in microprocessors and other hardware technologies closely. <p> Our work borrows ideas from several other systems, including Munin [2], TreadMarks/ParaNet [7], Platinum [3], and the thesis work of Karin Petersen [15, 16]. It is also related to ongoing work on the Wind Tunnel [19] and the Princeton Shrimp <ref> [1] </ref> project and, less directly, to several other DSM and multiprocessor projects.
Reference: [2] <author> J. B. Carter, J. K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and Performance of Munin. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <address> Pacific Grove, CA, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: Unfortunately, the current state of the art in software coherence for networks and multicomputers provides acceptable performance on only a limited class of applications. To make software coherence efficient, one would need to overcome several fundamental problems with existing distributed shared memory (DSM) emulations <ref> [2, 7, 14, 23] </ref>: fl This work is supported in part by NSF Infrastructure grant no. CDA-94-01142 and ONR research grant no. <p> The resulting inconsistencies force these systems to compute diffs with older versions of a page in order to merge the changes made by different processors <ref> [2, 7] </ref>. Copying and diffing pages is expensive not only in terms of time, but also in terms of storage overhead, cache pollution, and the need to garbage-collect old page copies, diffs, and other bookkeeping information. <p> We have found that neither remote fills (cache-line-size transfers) nor full-page copies is superior in all cases. We are currently investigating hybrids; we believe we can choose the strategy that works best on a page-by-page basis, dynamically. Our work borrows ideas from several other systems, including Munin <ref> [2] </ref>, TreadMarks/ParaNet [7], Platinum [3], and the thesis work of Karin Petersen [15, 16]. It is also related to ongoing work on the Wind Tunnel [19] and the Princeton Shrimp [1] project and, less directly, to several other DSM and multiprocessor projects.
Reference: [3] <author> A. L. Cox and R. J. Fowler. </author> <title> The Implementation of a Coherent Memory Abstraction on a NUMA Multiprocessor: Experiences with PLATINUM. </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 32-44, </pages> <address> Litchfield Park, AZ, </address> <month> December </month> <year> 1989. </year>
Reference-contexts: We are currently investigating hybrids; we believe we can choose the strategy that works best on a page-by-page basis, dynamically. Our work borrows ideas from several other systems, including Munin [2], TreadMarks/ParaNet [7], Platinum <ref> [3] </ref>, and the thesis work of Karin Petersen [15, 16]. It is also related to ongoing work on the Wind Tunnel [19] and the Princeton Shrimp [1] project and, less directly, to several other DSM and multiprocessor projects.
Reference: [4] <author> M. J. Feeley, J. S. Chase, V. R. Narasayya, and H. M. Levy. </author> <title> Log-Based Distributed Shared Memory. </title> <booktitle> In Proceedings of the First Symposium on Operating Systems Design and Implementation, </booktitle> <address> Monterey, CA, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: This of course works well only for programs whose sharing is coarse-grained. A few systems maintain coherence at a finer grain using in-line checks on references <ref> [4, 19, 23] </ref>, but this appears to require a restricted programming model or very high message traffic. * In order to mitigate the effects of false sharing in page-size blocks, the fastest virtual-memory-based DSM systems permit multiple copies of a page to be writable simultaneously.
Reference: [5] <author> R. Gillett. </author> <title> Memory Channel: An Optimized Cluster Interconnect. </title> <note> In Digital Technical Journal, </note> <institution> Maynard, </institution> <address> MA, </address> <month> Fall </month> <year> 1995. </year> <institution> Digital Equipment Corporation. </institution>
Reference-contexts: Machines in this non-cache-coherent, non-uniform memory access (NCC-NUMA) class include both tightly-coupled (single chassis) multiprocessors (e.g. the BBN TC2000, the Toronto Hector [21], and the Cray Research T3D), and memory-mapped network interfaces for workstations (e.g. the Princeton Shrimp [1], the DEC Memory Channel <ref> [5] </ref>, and the HP Hamlyn [22]). In comparison to hardware-coherent machines, NCC-NUMA systems can more easily be built from commodity parts, and can follow improvements in microprocessors and other hardware technologies closely.
Reference: [6] <author> P. Keleher, A. L. Cox, and W. Zwaenepoel. </author> <title> Lazy Release Consistency for Software Distributed Shared Memory. </title> <booktitle> In Proceedings of the Nineteenth International Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: These uses of the global address space constitute performance optimizations unavailable to DSM systems on message-passing hardware. To maximize concurrency and minimize unnecessary invalidations, the Cashmere protocols employ a variant of invalidation-based lazy release consistency <ref> [6] </ref>. They postpone notifying other processors that a page has been written until the processor that made the modification (s) reaches a synchronization release point.
Reference: [7] <author> P. Keleher, A. L. Cox, S. Dwarkadas, and W. Zwaenepoel. ParaNet: </author> <title> Distributed Shared Memory on Standard Workstations and Operating Systems. </title> <booktitle> In Proceedings of the USENIX Winter '94 Technical Conference, </booktitle> <address> San Francisco, CA, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: Unfortunately, the current state of the art in software coherence for networks and multicomputers provides acceptable performance on only a limited class of applications. To make software coherence efficient, one would need to overcome several fundamental problems with existing distributed shared memory (DSM) emulations <ref> [2, 7, 14, 23] </ref>: fl This work is supported in part by NSF Infrastructure grant no. CDA-94-01142 and ONR research grant no. <p> The resulting inconsistencies force these systems to compute diffs with older versions of a page in order to merge the changes made by different processors <ref> [2, 7] </ref>. Copying and diffing pages is expensive not only in terms of time, but also in terms of storage overhead, cache pollution, and the need to garbage-collect old page copies, diffs, and other bookkeeping information. <p> We have found that neither remote fills (cache-line-size transfers) nor full-page copies is superior in all cases. We are currently investigating hybrids; we believe we can choose the strategy that works best on a page-by-page basis, dynamically. Our work borrows ideas from several other systems, including Munin [2], TreadMarks/ParaNet <ref> [7] </ref>, Platinum [3], and the thesis work of Karin Petersen [15, 16]. It is also related to ongoing work on the Wind Tunnel [19] and the Princeton Shrimp [1] project and, less directly, to several other DSM and multiprocessor projects.
Reference: [8] <author> L. I. Kontothanassis and M. L. Scott. </author> <title> Distributed Shared Memory for New Generation Networks. </title> <type> TR 578, </type> <institution> Computer Science Department, University of Rochester, </institution> <month> March </month> <year> 1995. </year> <note> Submitted for publication. </note>
Reference-contexts: Specifically, the protocols differ in terms of: * The mechanism used to implement write-through to remote memory. Some NCC-NUMA hardware will not write through to remote locations on ordinary store instructions. In such cases we can achieve write-through by editing program binaries to include special instruction sequences <ref> [8] </ref>, in a manner reminiscent of the Blizzard [19] and Midway [23] projects. * The existence of a write-merge buffer. <p> It is also related to ongoing work on the Wind Tunnel [19] and the Princeton Shrimp [1] project and, less directly, to several other DSM and multiprocessor projects. Full protocol details and comparisons to related work can be found in other papers <ref> [8, 9, 10] </ref>. 2 Results and Project Status We have evaluated the performance of NCC-NUMA hardware running Cashmere protocols using execution-driven simulation on a variety of applications. <p> The results indicate that one can achieve substantial performance improvements over more traditional DSM systems by exploiting the additional hardware capabilities of NCC-NUMA systems. A complete discussion and explanation of the results can be found in previous papers <ref> [8, 9, 10] </ref>. The best performance, clearly, will be obtained by systems that combine the speed and concurrency of existing hardware coherence mechanisms with the flexibility of software coherence. This goal may be achieved by a new generation of machines with programmable network controllers [12, 18].
Reference: [9] <author> L. I. Kontothanassis and M. L. Scott. </author> <title> Software Cache Coherence for Large Scale Multiprocessors. </title> <booktitle> In Proceedings of the First International Symposium on High Performance Computer Architecture, </booktitle> <pages> pages 286-295, </pages> <address> Raleigh, NC, </address> <month> January </month> <year> 1995. </year>
Reference-contexts: It is also related to ongoing work on the Wind Tunnel [19] and the Princeton Shrimp [1] project and, less directly, to several other DSM and multiprocessor projects. Full protocol details and comparisons to related work can be found in other papers <ref> [8, 9, 10] </ref>. 2 Results and Project Status We have evaluated the performance of NCC-NUMA hardware running Cashmere protocols using execution-driven simulation on a variety of applications. <p> The results indicate that one can achieve substantial performance improvements over more traditional DSM systems by exploiting the additional hardware capabilities of NCC-NUMA systems. A complete discussion and explanation of the results can be found in previous papers <ref> [8, 9, 10] </ref>. The best performance, clearly, will be obtained by systems that combine the speed and concurrency of existing hardware coherence mechanisms with the flexibility of software coherence. This goal may be achieved by a new generation of machines with programmable network controllers [12, 18].
Reference: [10] <author> L. I. Kontothanassis and M. L. Scott. </author> <title> High Performance Software Coherence for Current and Future Architectures. </title> <journal> In Journal of Parallel and Distributed Computing, </journal> <note> 1995 (to appear). </note>
Reference-contexts: It is also related to ongoing work on the Wind Tunnel [19] and the Princeton Shrimp [1] project and, less directly, to several other DSM and multiprocessor projects. Full protocol details and comparisons to related work can be found in other papers <ref> [8, 9, 10] </ref>. 2 Results and Project Status We have evaluated the performance of NCC-NUMA hardware running Cashmere protocols using execution-driven simulation on a variety of applications. <p> The results indicate that one can achieve substantial performance improvements over more traditional DSM systems by exploiting the additional hardware capabilities of NCC-NUMA systems. A complete discussion and explanation of the results can be found in previous papers <ref> [8, 9, 10] </ref>. The best performance, clearly, will be obtained by systems that combine the speed and concurrency of existing hardware coherence mechanisms with the flexibility of software coherence. This goal may be achieved by a new generation of machines with programmable network controllers [12, 18].
Reference: [11] <author> L. I. Kontothanassis, M. L. Scott, and R. Bianchini. </author> <title> Lazy Release Consistency for Hardware-Coherent Multiprocessors. </title> <booktitle> In Proceedings Supercomputing '95, </booktitle> <address> San Diego, CA, </address> <note> December 1995 (to appear). Earlier version available as TR 547, </note> <institution> Computer Science Department, University of Rochester, </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: This goal may be achieved by a new generation of machines with programmable network controllers [12, 18]. Our studies indicate that modestly-lazy multi-writer protocols for such machines will outperform eager single-writer protocols by 5 to 20 percent <ref> [11] </ref>. It is not yet clear whether the performance advantages of programmable controllers will justify their design time and cost. Our suspicion, based on current results, is that NCC-NUMA systems will remain more cost effective in the near to medium term.
Reference: [12] <author> J. Kuskin, D. Ofelt, M. Heinrich, J. Heinlein, R. Simoni, K. Gharachorloo, J. Chapin, D. Nakahira, J. Baxter, M. Horowitz, A. Gupta, M. Rosenblum, and J. Hennessy. </author> <title> The FLASH Multiprocessor. </title> <booktitle> In Proceedings of the Twenty-First International Symposium on Computer Architecture, </booktitle> <pages> pages 302-313, </pages> <address> Chicago, IL, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: The best performance, clearly, will be obtained by systems that combine the speed and concurrency of existing hardware coherence mechanisms with the flexibility of software coherence. This goal may be achieved by a new generation of machines with programmable network controllers <ref> [12, 18] </ref>. Our studies indicate that modestly-lazy multi-writer protocols for such machines will outperform eager single-writer protocols by 5 to 20 percent [11]. It is not yet clear whether the performance advantages of programmable controllers will justify their design time and cost.
Reference: [13] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. S. Lam. </author> <title> The Stanford Dash Multiprocessor. </title> <journal> Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference: [14] <author> B. Nitzberg and V. Lo. </author> <title> Distributed Shared Memory: A Survey of Issues and Algorithms. </title> <journal> Computer, </journal> <volume> 24(8) </volume> <pages> 52-60, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Unfortunately, the current state of the art in software coherence for networks and multicomputers provides acceptable performance on only a limited class of applications. To make software coherence efficient, one would need to overcome several fundamental problems with existing distributed shared memory (DSM) emulations <ref> [2, 7, 14, 23] </ref>: fl This work is supported in part by NSF Infrastructure grant no. CDA-94-01142 and ONR research grant no.
Reference: [15] <author> K. Petersen and K. Li. </author> <title> Cache Coherence for Shared Memory Multiprocessors Based on Virtual Memory Support. </title> <booktitle> In Proceedings of the Seventh International Parallel Processing Symposium, </booktitle> <address> Newport Beach, CA, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: We are currently investigating hybrids; we believe we can choose the strategy that works best on a page-by-page basis, dynamically. Our work borrows ideas from several other systems, including Munin [2], TreadMarks/ParaNet [7], Platinum [3], and the thesis work of Karin Petersen <ref> [15, 16] </ref>. It is also related to ongoing work on the Wind Tunnel [19] and the Princeton Shrimp [1] project and, less directly, to several other DSM and multiprocessor projects.
Reference: [16] <author> K. Petersen and K. Li. </author> <title> An Evaluation of Multiprocessor Cache Coherence Based on Virtual Memory Support. </title> <booktitle> In Proceedings of the Eighth International Parallel Processing Symposium, p. </booktitle> <pages> 158-164, </pages> <address> Cancun, Mexico, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: We are currently investigating hybrids; we believe we can choose the strategy that works best on a page-by-page basis, dynamically. Our work borrows ideas from several other systems, including Munin [2], TreadMarks/ParaNet [7], Platinum [3], and the thesis work of Karin Petersen <ref> [15, 16] </ref>. It is also related to ongoing work on the Wind Tunnel [19] and the Princeton Shrimp [1] project and, less directly, to several other DSM and multiprocessor projects.
Reference: [17] <author> S. K. Reinhardt, B. Falsafi, and D. A. Wood. </author> <title> Kernel Support for the Wisconsin Wind Tunnel. </title> <booktitle> In Second Usenix Symposium on Microkernels and Other Kernel Architectures, </booktitle> <address> San Diego, CA, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: This shortcoming can be addressed by copying pages to local memory in response to a post-invalidation page fault, and then fetching from local memory. Alternatively, on some machines, one can use an idea developed for the Wisconsin Wind Tunnel <ref> [17] </ref> to generate ECC protection faults on each cache mess, in which case the handler can use a special instruction sequence to fetch from remote memory explicitly. * The granularity of data transfer.
Reference: [18] <author> S. K. Reinhardt, J. R. Larus, and D. A. Wood. Tempest and Typhoon: </author> <title> User-level Shared-Memory. </title> <booktitle> In Proceedings of the Twenty-First International Symposium on Computer Architecture, </booktitle> <pages> pages 325-336, </pages> <address> Chicago, IL, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: The best performance, clearly, will be obtained by systems that combine the speed and concurrency of existing hardware coherence mechanisms with the flexibility of software coherence. This goal may be achieved by a new generation of machines with programmable network controllers <ref> [12, 18] </ref>. Our studies indicate that modestly-lazy multi-writer protocols for such machines will outperform eager single-writer protocols by 5 to 20 percent [11]. It is not yet clear whether the performance advantages of programmable controllers will justify their design time and cost.
Reference: [19] <author> I. Schoinas, B. Falsafi, A. R. Lebeck, S. K. Reinhardt, J. R. Larus, and D. A. Wood. </author> <title> Fine-grain Access Control for Distributed Shared Memory. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 297-306, </pages> <address> San Jose, CA, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: This of course works well only for programs whose sharing is coarse-grained. A few systems maintain coherence at a finer grain using in-line checks on references <ref> [4, 19, 23] </ref>, but this appears to require a restricted programming model or very high message traffic. * In order to mitigate the effects of false sharing in page-size blocks, the fastest virtual-memory-based DSM systems permit multiple copies of a page to be writable simultaneously. <p> Some NCC-NUMA hardware will not write through to remote locations on ordinary store instructions. In such cases we can achieve write-through by editing program binaries to include special instruction sequences [8], in a manner reminiscent of the Blizzard <ref> [19] </ref> and Midway [23] projects. * The existence of a write-merge buffer. If neither the processor nor the cache controller includes a write buffer capable of merging writes to a common cache line (with per-word dirty bits for merging into memory), then the message traffic due to write-through increases substantially. <p> Our work borrows ideas from several other systems, including Munin [2], TreadMarks/ParaNet [7], Platinum [3], and the thesis work of Karin Petersen [15, 16]. It is also related to ongoing work on the Wind Tunnel <ref> [19] </ref> and the Princeton Shrimp [1] project and, less directly, to several other DSM and multiprocessor projects.
Reference: [20] <author> J. P. Singh, W. Weber, and A. Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title> <journal> ACM SIGARCH Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: The applications include two programs from the SPLASH suite <ref> [20] </ref> (mp3d and water), two from the NASA parallel benchmarks suite (appbt and mgrid), one from the Berkeley Split-C group (em3d), and three locally-written kernels (Gauss, sor, and fft). The results appear in two figures, one comparing NCC-NUMA and CC-NUMA and the other comparing NCC-NUMA and DSM systems.
Reference: [21] <author> Z. G. Vranesic, M. Stumm, D. M. Lewis, and R. White. Hector: </author> <title> A Hierarchically Structured Shared-Memory Multiprocessor. </title> <journal> Computer, </journal> <volume> 24(1) </volume> <pages> 72-79, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: Machines in this non-cache-coherent, non-uniform memory access (NCC-NUMA) class include both tightly-coupled (single chassis) multiprocessors (e.g. the BBN TC2000, the Toronto Hector <ref> [21] </ref>, and the Cray Research T3D), and memory-mapped network interfaces for workstations (e.g. the Princeton Shrimp [1], the DEC Memory Channel [5], and the HP Hamlyn [22]).
Reference: [22] <author> J. Wilkes. </author> <title> Hamlyn An Interface for Sender-Based Communications. </title> <type> Technical Report HPL-OSR-92-13, </type> <institution> Hewllett Packard Laboratories, </institution> <address> Palo Alto, CA, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: Machines in this non-cache-coherent, non-uniform memory access (NCC-NUMA) class include both tightly-coupled (single chassis) multiprocessors (e.g. the BBN TC2000, the Toronto Hector [21], and the Cray Research T3D), and memory-mapped network interfaces for workstations (e.g. the Princeton Shrimp [1], the DEC Memory Channel [5], and the HP Hamlyn <ref> [22] </ref>). In comparison to hardware-coherent machines, NCC-NUMA systems can more easily be built from commodity parts, and can follow improvements in microprocessors and other hardware technologies closely. As part of the Cashmere 1 project we have developed a family of protocols for NCC-NUMA systems with various levels of hardware support.
Reference: [23] <author> M. J. Zekauskas, W. A. Sawdon, and B. N. Bershad. </author> <title> Software Write Detection for Distributed Shared Memory. </title> <booktitle> In Proceedings of the First Symposium on Operating Systems Design and Implementation, </booktitle> <address> Monterey, CA, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: Unfortunately, the current state of the art in software coherence for networks and multicomputers provides acceptable performance on only a limited class of applications. To make software coherence efficient, one would need to overcome several fundamental problems with existing distributed shared memory (DSM) emulations <ref> [2, 7, 14, 23] </ref>: fl This work is supported in part by NSF Infrastructure grant no. CDA-94-01142 and ONR research grant no. <p> This of course works well only for programs whose sharing is coarse-grained. A few systems maintain coherence at a finer grain using in-line checks on references <ref> [4, 19, 23] </ref>, but this appears to require a restricted programming model or very high message traffic. * In order to mitigate the effects of false sharing in page-size blocks, the fastest virtual-memory-based DSM systems permit multiple copies of a page to be writable simultaneously. <p> Some NCC-NUMA hardware will not write through to remote locations on ordinary store instructions. In such cases we can achieve write-through by editing program binaries to include special instruction sequences [8], in a manner reminiscent of the Blizzard [19] and Midway <ref> [23] </ref> projects. * The existence of a write-merge buffer. If neither the processor nor the cache controller includes a write buffer capable of merging writes to a common cache line (with per-word dirty bits for merging into memory), then the message traffic due to write-through increases substantially.
References-found: 23

