URL: http://www.cs.princeton.edu/prism/papers-ps/sasha-bn2o-uai.ps
Refering-URL: http://www.cs.princeton.edu/prism/html/all-papers.html
Root-URL: http://www.cs.princeton.edu
Title: Computational complexity reduction for BN2O networks using similarity of states  
Author: Alexander V. Kozlov Jaswinder Pal Singh 
Address: Stanford, CA 94305-4090 Princeton, NJ 08544  
Affiliation: Portland, Oregon, USA  Department of Applied Physics Department of Computer Science Stanford University Princeton University  
Note: Submitted to the Twelfth Conference on Uncertainty in Artificial Intelligence (UAI-96)  
Email: email: alexvk@cs.stanford.edu email: jps@cs.princeton.edu  
Phone: phone: (415) 725-8814 phone: (609) 258 5329 fax: (415) 725-1449 fax: (609) 258 1771  
Date: August 1-3, 1996,  
Abstract: Although probabilistic inference in a general Bayesian belief network is an NP-hard problem, inference computation time can be reduced in most practical cases by exploiting domain knowledge and by making appropriate approximations in the knowledge representation. In this paper we introduce the property of similarity of states and a new method for approximate knowledge representation which is based on this property. We define two or more states of a node to be similar when the likelihood ratio of their probabilities does not depend on the instantiations of the other nodes in the network. We show that the similarity of states exposes redundancies in the joint probability distribution which can be exploited to reduce the computational complexity of probabilistic inference in networks with multiple similar states. For example, we show that a BN2O network|a two layer networks often used in diagnostic problems|can be reduced to a very close network with multiple similar states. Probabilistic inference in the new network can be done in only polynomial time with respect to the size of the network, and the results for queries of practical importance are very close to the results that can be obtained in exponential time with the original network. The error introduced by our reduction converges to zero faster than exponentially with respect to the degree of the polynomial describing the resulting computational complexity. 
Abstract-found: 1
Intro-found: 1
Reference: [Cooper, 1990] <author> Cooper, G. </author> <year> (1990). </year> <title> The computational complexity of probabilistic inference using Bayesian belief networks. </title> <journal> Artificial Intelligence, </journal> <volume> 42:393 - 405. </volume>
Reference-contexts: Its potential for application is increasing rapidly as more and more areas incorporate the need to make decisions under uncertain circumstances. Probabilistic inference is NP-hard for a general network with an arbitrary structure <ref> [Cooper, 1990] </ref>. Furthermore, even approximating inference in a general belief network is NP-hard [Dagum and Luby, 1993]. However, the features of a problem domain can help to reduce computation time of probabilistic inference. For example, networks with noisy-OR interaction between nodes allow efficient algorithms for probabilistic inference.
Reference: [Dagum and Luby, 1993] <author> Dagum, P. and Luby, M. </author> <year> (1993). </year> <title> Approximating probabilistic inference in Bayesian belief networks is NP-hard. </title> <journal> Artificial Intelligence, </journal> <volume> 60:141 - 153. </volume>
Reference-contexts: Its potential for application is increasing rapidly as more and more areas incorporate the need to make decisions under uncertain circumstances. Probabilistic inference is NP-hard for a general network with an arbitrary structure [Cooper, 1990]. Furthermore, even approximating inference in a general belief network is NP-hard <ref> [Dagum and Luby, 1993] </ref>. However, the features of a problem domain can help to reduce computation time of probabilistic inference. For example, networks with noisy-OR interaction between nodes allow efficient algorithms for probabilistic inference.
Reference: [D'Ambrosio, 1993] <author> D'Ambrosio, B. </author> <year> (1993). </year> <title> Incremental probabilistic inference. </title> <editor> In Heckerman, D. E., editor, </editor> <booktitle> Proceedings of the Ninth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 301 - 308. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: groups of similar states in cluster trees, especially if we have some insights into the underlying problem. 6 Relation to Previous Work The only approximate methods for the probabilistic evaluation of BN2O models we know are the different variants of the stochastic simulation method [Henrion, 1988] or the search method <ref> [D'Ambrosio, 1993] </ref>. Here we present a completely deterministic method, which is different form the previous ones in that it focuses on simplifying the original model. Thus, we call it a model reduction method.
Reference: [D'Ambrosio, 1994] <author> D'Ambrosio, B. </author> <year> (1994). </year> <title> Symbolic probabilistic inference in large BN2O networks. </title> <editor> In Lopez de Montara, R. and Poole, D., editors, </editor> <booktitle> Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 128 - 135. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This topological decomposition can be used to reduce the number of parents for a node in a network and is equivalent to the evaluation of the above sum (7) computationally. Besides these simplifications, probabilistic inference in a general BN2O network (as well as in the practical QMR-DT network <ref> [D'Ambrosio, 1994] </ref>) is computationally intractable for a general case of positive evidence. In the practical case of QMR-DT network, users have to apply stochastic simulation methods to obtain approximate results. <p> Thus, the state space aggregation performs better in highly interconnected BN2O networks, i.e. exactly when other methods based on topological decomposition fail <ref> [D'Ambrosio, 1994, Heckerman and Breese, 1994] </ref>.
Reference: [Heckerman and Breese, 1994] <author> Heckerman, D. and Breese, J. S. </author> <year> (1994). </year> <title> A new look at causal independence. </title> <editor> In Lopez de Montara, R. and Poole, D., editors, </editor> <booktitle> Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 286 - 292. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: in two layer networks with noisy-OR interaction between nodes (BN2O networks) for negative evidence about nodes [Heckerman, 1989], and Heckerman and Breese showed that the noisy-OR interaction between nodes can be further simplified to reduce the number of parents per node which reduces computational complexity for networks with special structure <ref> [Heckerman and Breese, 1994] </ref>. Thus, the computational complexity of probabilistic inference can be managed in some special cases. In this paper we propose a new way of simplifying probabilistic inference in belief networks based on the similarity of states property. <p> On the topological level, the causal independence assumed in the definition of the noisy-OR interaction leads to an efficient decomposition algorithm <ref> [Heckerman and Breese, 1994] </ref>. This topological decomposition can be used to reduce the number of parents for a node in a network and is equivalent to the evaluation of the above sum (7) computationally. <p> Thus, the state space aggregation performs better in highly interconnected BN2O networks, i.e. exactly when other methods based on topological decomposition fail <ref> [D'Ambrosio, 1994, Heckerman and Breese, 1994] </ref>.
Reference: [Heckerman, 1989] <author> Heckerman, D. E. </author> <year> (1989). </year> <title> A tractable inference algorithm for diagnosing multiple diseases. </title> <booktitle> In Uncertainty in Artificial Intelligence: Proceedings of the Fifth Conference, </booktitle> <pages> pages 174 - 181. </pages>
Reference-contexts: For example, networks with noisy-OR interaction between nodes allow efficient algorithms for probabilistic inference. Heckerman showed that probabilistic inference is linear in two layer networks with noisy-OR interaction between nodes (BN2O networks) for negative evidence about nodes <ref> [Heckerman, 1989] </ref>, and Heckerman and Breese showed that the noisy-OR interaction between nodes can be further simplified to reduce the number of parents per node which reduces computational complexity for networks with special structure [Heckerman and Breese, 1994].
Reference: [Heckerman, 1990] <author> Heckerman, D. E. </author> <year> (1990). </year> <title> Probabilistic similarity networks. Networks, </title> <address> 20:607 - 636. </address>
Reference-contexts: This is valid even if we have different priors on different spark plugs. 5 Submitted to the Twelfth Conference on Uncertainty in Artificial Intelligence (UAI-96) August 1-3, 1996, Portland, Oregon, USA This independence information was brilliantly used for the construction of similarity networks <ref> [Heckerman, 1990] </ref>. A similarity network is a construction consisting of a similarity graph and a collection of local knowledge maps corresponding to each edge in the similarity graph. Similarity networks were developed to simplify the construction of large and complex belief networks.
Reference: [Henrion, 1988] <author> Henrion, M. </author> <year> (1988). </year> <title> Propagating uncertainty in bayesian networks by probabilistic logic sampling. </title> <booktitle> In Proceedings of the Second Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 149 - 163. </pages>
Reference-contexts: states that can be aggregated into groups of similar states in cluster trees, especially if we have some insights into the underlying problem. 6 Relation to Previous Work The only approximate methods for the probabilistic evaluation of BN2O models we know are the different variants of the stochastic simulation method <ref> [Henrion, 1988] </ref> or the search method [D'Ambrosio, 1993]. Here we present a completely deterministic method, which is different form the previous ones in that it focuses on simplifying the original model. Thus, we call it a model reduction method.
Reference: [Kozlov and Singh, 1995] <author> Kozlov, A. V. and Singh, J. P. </author> <year> (1995). </year> <title> Sensitivities: an alternative to conditional probabilities for Bayesian belief networks. </title> <editor> In Besnard, P. and Hanks, S., editors, </editor> <booktitle> Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 376 - 385. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Thus, we call it a model reduction method. The method is related to the state space abstraction method [Wellman and Liu, 1994] and to a general approach of complexity reduction proposed in <ref> [Kozlov and Singh, 1995] </ref>. In the previous work we suggested reducing the computational complexity of probabilistic inference by reducing the rank of the sensitivity matrices. It can be shown that the scheme presented here is directly related to the sensitivities.
Reference: [Parker and Miller, 1987] <author> Parker, R. C. and Miller, R. A. </author> <year> (1987). </year> <title> Using causal knowledge to create simulated patient cases: the cpcs project as an extension of Internist-1. </title> <booktitle> In Proceedings of the 11th Annual Symposium on Computer Applications in Medical Care, </booktitle> <pages> pages 473 - 480. </pages> <note> IEEE Comp Soc Press. 12 Submitted to the Twelfth Conference on Uncertainty in Artificial Intelligence (UAI-96) August 1-3, 1996, </note> <institution> Portland, Oregon, USA </institution>
Reference-contexts: Parker and R. Miller at Stanford Medical School <ref> [Parker and Miller, 1987] </ref>). First, we convert a BN2O network to a cluster tree. A general form of the cluster tree for the BN2O network is a "Naive" bayesian classifier with one large node representing all nodes in the first level and many children representing nodes in the second level.
Reference: [Pearl, 1988] <author> Pearl, J. </author> <year> (1988). </year> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The noisy-OR interaction between the diseases and findings describes a causal independence assumption, i.e. that the ability of any single disease to cause a given symptom does not depend on the presence of the other diseases <ref> [Pearl, 1988] </ref>. A noisy-OR dependence between a finding node x f i and its n parent disease nodes x d j can be characterized by (n + 1) real numbers, a leak and n coefficients.
Reference: [Pradhan et al., 1994] <author> Pradhan, M., Provan, G., Middleton, B., and Henrion, M. </author> <year> (1994). </year> <title> Knowledge engineering for large belief networks. </title> <editor> In Lopez de Montara, R. and Poole, D., editors, </editor> <booktitle> Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 484 - 490. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: An extension of the noisy-OR interaction to multiply-valued nodes is possible and is called noisy-MAX <ref> [Pradhan et al., 1994] </ref>. In the noisy-MAX interaction relations identical to (1) and (2) hold for the combined probability of the first k states of the child node.
Reference: [Rubinstein, 1981] <author> Rubinstein, R. Y. </author> <year> (1981). </year> <title> Simulation and the Monte Carlo Method. </title> <publisher> John Wiley & Sons, </publisher> <address> New York. </address>
Reference-contexts: In general, the precision of the reduced model depends on how similar these states were in the original problem and on how much these states contribute to the total probability mass. An extension of this method might be a well-known variance reduction technique known as control variates <ref> [Rubinstein, 1981] </ref>. If a good enough approximation to the answer can be computed efficiently in a small amount of time, we can decompose the problem into two parts.
Reference: [Wellman and Liu, 1994] <author> Wellman, M. P. and Liu, C.-L. </author> <year> (1994). </year> <title> State-space abstraction for anytime evaluation of probabilistic networks. </title> <editor> In Lopez de Montara, R. and Poole, D., editors, </editor> <booktitle> Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 567 - 574. </pages> <publisher> Morgan Kaufmann. </publisher> <pages> 13 </pages>
Reference-contexts: Although the resulting cluster node has exponentially many states (in the number of the first layer nodes), we can aggregate some of these states into a group of similar states (as opposed to eliminating them in the state space abstraction method <ref> [Wellman and Liu, 1994] </ref>). We show that the resulting model provides a good estimate of the probabilistic inference results as compared to the original BN2O model. <p> The latter states are the ones we make similar. While in the state space abstraction method <ref> [Wellman and Liu, 1994] </ref> we would eliminate the states with the number of diseases present more than d max completely, in our approach we always have a non-zero contribution from the subset of similar states. <p> Here we present a completely deterministic method, which is different form the previous ones in that it focuses on simplifying the original model. Thus, we call it a model reduction method. The method is related to the state space abstraction method <ref> [Wellman and Liu, 1994] </ref> and to a general approach of complexity reduction proposed in [Kozlov and Singh, 1995]. In the previous work we suggested reducing the computational complexity of probabilistic inference by reducing the rank of the sensitivity matrices. <p> The results for these networks quickly converge to the exact results when we gradually increase the number of non-similar states in the reduced model. The proposed method of complexity reduction is related to the state space reduction method used before, for instance in <ref> [Wellman and Liu, 1994] </ref>. The state space reduction method assigns a zero probability to a subset of nodes.
References-found: 14

