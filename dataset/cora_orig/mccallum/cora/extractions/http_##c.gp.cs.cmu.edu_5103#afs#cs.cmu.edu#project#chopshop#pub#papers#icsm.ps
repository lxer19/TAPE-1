URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/project/chopshop/pub/papers/icsm.ps
Refering-URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/project/chopshop/pub/www/home.html
Root-URL: http://www.cs.cmu.edu
Title: Semantic Diff: A Tool for Summarizing the Effects of Modifications  
Keyword: 2 Practical Semantic Diff  
Note: Second, the tool should be fully automatic, fast and easy to use. Clearly this rules out any kind of theorem-proving (which would call for lemmas to be provided by the user and searches that might not terminate). But it also  
Abstract: explains how the tool works, and reports on some pre-liminary experience applying it to the code of a large real-time system. A maintainer can benefit in several ways from our tool. By running the tool after having made a change, he can correlate the tools summary against his intent; discrepancies between the two are likely to indicate flaws. This might reduce the incidence of fix on fix, when a modifications sole purpose is to fix a bug introduced in an earlier modification. A summary of the effects of a change is most important later, however, when the modified code is being modified again. Heavily modified code is far harder to understand than fresh code, often because maintainers fail adequately to document the effects of their changes. Our tool can relieve this burden substantially; it can provide the structure of the documentation for a change (which a maintainer might subsequently embellish). What are the key features of a practical semantic differencing tool? F irst, the vocabulary of its report should be semantic and not syntactic. A procedures semantic effect is a relation between its inputs and outputs; the difference between two versions must thus be expressed in terms of input-output behaviour. This is not to deny the value of syntactic reports. A slice indicating affected lines of code is good for some applications, but does not tell a maintainer how clients of a procedure will be affected; it merely reduces the size of the program from which this information must be inferred. This paper describes a tool that takes two versions of a procedure and generates a report summarizing the semantic differences between them. Unlike existing tools based on comparison of program dependence graphs, our tool expresses its results in terms of the observable input-output behaviour of the procedure, rather than its syntactic structure. And because the analysis is truly semantic, it requires no prior matching of syntactic components, and generates fewer spurious differ ences, so that meaning-pr eserving transformations (such as r enaming local variables) ar e correctly determined to have no visible effect. A pr elimi-nary experiment on modifications applied to the code of a large real-time system suggests that the approach is practical. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Samuel Bates and Susan Horwitz, </author> <title> Incremental Program Testing Using Program Dependence Graphs, </title> <booktitle> Proc. ACM Conf. on Principles of Programming Languages, </booktitle> <year> 1993. </year>
Reference-contexts: For example, techniques to reduce the cost of regression testing by eliminating needless tests or by not retesting unaffected code (such as Binkleys [2] and Bates and Horwitzs <ref> [1] </ref>) can safely find differences where none exist, since this only results in unnecessary test executions, but must never miss differ - ences, which might lead to inadequate testing. When the output of the tool is intended for human consumption, a different kind of accuracy may be called for.
Reference: [2] <author> David Binkley, </author> <title> Using Semantic Differencing to R educe the Cost of Regression Testing, </title> <booktitle> Proc. International Conf. on Software Maintenance, </booktitle> <year> 1992. </year>
Reference-contexts: For example, techniques to reduce the cost of regression testing by eliminating needless tests or by not retesting unaffected code (such as Binkleys <ref> [2] </ref> and Bates and Horwitzs [1]) can safely find differences where none exist, since this only results in unnecessary test executions, but must never miss differ - ences, which might lead to inadequate testing.
Reference: [3] <author> S.G. Eick, J.L. Steffen, E. Sumner Jr., Seesoft: </author> <title> A Tool for Visualizing Line- Oriented Software Statistics, </title> <journal> IEEE Trans. on Softwar e Engineering , 18(11), </journal> <pages> pp. 957968, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: By looking at the abstracts written by the maintainers summarizing their changes, and by examining the code itself with the Seesoft visualization tool <ref> [3] </ref>, we sifted out about 20 suitable candidates (the remaining modifications being insertions of large amounts of fresh code). Our tool fails to report semantic differences in 2 cases, both of which were quite subtle; in one, for example, the order of application of two bitshifting operations was reversed. <p> The maintainers of the system we studied keep a modification database that records every line that is added or deleted; we used the Seesoft visualization tool <ref> [3] </ref> to display the code with each modification (whether an addition or a deletion) highlighted in a different colour . We could view a whole directory on a single screen, find interesting modifications by looking for various colours, and then zoom in to examine the code.
Reference: [4] <author> J. Ferrante, K. Ottenstein and J. Warren, </author> <title> The Program Dependence Graph and Its Use in Optimization, </title> <journal> ACM Trans. on Programming Languages and Systems, </journal> <volume> 9:3, </volume> <month> July </month> <year> 1987. </year>
Reference-contexts: Rather, it constructs a flow graph and propagates dependences iteratively until a fixed point is reached. Recently, we have devised a simple technique [11] for calculating dependences using a representation similar to the program dependence graph <ref> [4] </ref>, but generalized to handle procedural abstraction. 5 Preliminary Experiments Our tool was developed following a week-long study of a sample of modifications applied to a large real-time system, whose maintenance currently employs a few hundred developers.
Reference: [5] <author> J.E. Grass and Y.F. Chen, </author> <title> The C ++ Information Abstractor, </title> <booktitle> Proc. USENIX C++ Conf., </booktitle> <address> San Francisco, CA, </address> <pages> pp. 265278, </pages> <year> 1990. </year>
Reference-contexts: There are a number of tools that generate syntax tree representations of code; this suggests finding differences between versions by comparing their trees. Cdiff [7], for example, compares checksums of trees produced by CIA <ref> [5] </ref>. Since the smallest unit of executable code tracked by CIA is a function, the most Cdiff can say is whether or not a functions parse tree has changed.
Reference: [6] <author> W.G. Griswold and D. Notkin, </author> <title> Automated Assistance for Program Restructuring, </title> <journal> ACM Trans. on Software Engineering and Methodology, </journal> <volume> 2(3), </volume> <month> July </month> <year> 1993. </year>
Reference-contexts: Griswold and Notkin have pioneered a completely different approach to modification, focusing on tools for transforming code rather than analysing it after the fact <ref> [6] </ref>. The meaning-preserving transformations that confound graph comparison approaches can be performed automatically, ensuring no semantic effect by checking certain syntactic conditions. A renaming of a local variable, for example, would be allowed only when the new name does not clash with existing names.
Reference: [7] <author> J.E. Grass, Cdiff: </author> <title> A Syntax Directed Diff for C++ Programs, </title> <booktitle> Proc. USENIX C ++ Confer ence, </booktitle> <address> Portland, OR, </address> <pages> pp. 181193, </pages> <year> 1992. </year>
Reference-contexts: These tools are line-based, so even a lexical change with no syntactic effect will appear as a modification. There are a number of tools that generate syntax tree representations of code; this suggests finding differences between versions by comparing their trees. Cdiff <ref> [7] </ref>, for example, compares checksums of trees produced by CIA [5]. Since the smallest unit of executable code tracked by CIA is a function, the most Cdiff can say is whether or not a functions parse tree has changed.
Reference: [8] <author> Susan Horwitz, Phil Pfeiffer and Thomas Reps, </author> <title> Dependency Analysis for P ointer Variables, </title> <booktitle> ACM Conf . on Programming L anguage Design and Implementation , 1989. </booktitle>
Reference: [9] <author> Daniel Jackson, </author> <title> Aspect: A Formal Specification Language For Detecting Bugs, </title> <type> Technical Report MIT/LCS/TR-543, </type> <institution> Massachusetts Institute of T echnology, Laboratory for Computer Science, </institution> <month> June </month> <year> 1992. </year>
Reference-contexts: We have not come across many examples that call for alias analysis, but we believe it to be an essential part of a practical tool. W e have implemented similar tools that combine this style of dependence calculation with alias analysis <ref> [9] </ref>, and if carefully done, the loss of accuracy need not be too great. 4 How The Tool Works The key approximation of the tool is to ignore the actual state transitions of a procedure, and instead view its behaviour as a dependence relation between the inputs and outputs.
Reference: [10] <author> Daniel Jackson, </author> <title> Abstract Analysis with Aspect, </title> <booktitle> Proc. International Symposium on Software Testing and Analysis, </booktitle> <address> Cambridge, MA, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: Finally, these specifications could be compared to the code they purport to represent (by the method described in <ref> [10] </ref>), so that inconsistencies could be detected automatically. Acknowledgments Eric Sumner helped initiate this project. Brendan Cain provided his expertise with the modification databases and change management systems, and helped us find our way round the source code. David Weiss gave us helpful comments on a draft of the paper.
Reference: [11] <author> Daniel Jackson and Eugene J. Rollins, </author> <title> Chopping: A Generalization of Slicing, </title> <type> Technical Report CMU-CS-94-169, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: The implementation does not actually calculate dependences over the syntax tree; we gave the rules in this form to illustrate their simple compositional nature. Rather, it constructs a flow graph and propagates dependences iteratively until a fixed point is reached. Recently, we have devised a simple technique <ref> [11] </ref> for calculating dependences using a representation similar to the program dependence graph [4], but generalized to handle procedural abstraction. 5 Preliminary Experiments Our tool was developed following a week-long study of a sample of modifications applied to a large real-time system, whose maintenance currently employs a few hundred developers.
References-found: 11

