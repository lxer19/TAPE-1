URL: http://pine.cs.yale.edu:4201/home/focs94-proceedings.ps
Refering-URL: http://pine.cs.yale.edu:4201/home/focs94-abstract.html
Root-URL: http://www.cs.yale.edu
Title: A Theory of Competitive Analysis for Distributed Algorithms  
Author: Miklos Ajtai James Aspnes Cynthia Dwork Orli Waarts 
Abstract: We introduce a theory of competitive analysis for distributed algorithms. The first steps in this direction were made in the seminal papers of Bartal, Fiat, and Rabani [17], and of Awerbuch, Kutten, and Pe-leg [15], in the context of data management and job scheduling. In these papers, as well as in other subsequent work [14, 4, 18], the cost of a distributed algorithm is compared to the cost of an optimal global-control algorithm. Here we introduce a more refined notion of competitiveness for distributed algorithms, one that reflects the performance of distributed algorithms more accurately. In particular, our theory allows one to compare the cost of a distributed on-line algorithm to the cost of an optimal distributed algorithm. We demonstrate our method by studying the cooperative collect primitive, first abstracted by Saks, Shavit, and Woll [50]. We provide the first algorithms that allow processes to cooperate to finish their work in fewer steps. Specifically, we present two algorithms (with different strengths), and provide a competitive analysis for each one. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. Abrahamson. </author> <title> On achieving consensus using a shared memory. </title> <booktitle> In Proc. 7th ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pp. 291-302, </pages> <month> August </month> <year> 1988. </year>
Reference: [2] <author> Y. Afek, H. Attiya, D. Dolev, E. Gafni, M. Merritt, and N. Shavit. </author> <title> Atomic Snapshots of Shared Memory. </title> <booktitle> Proc. 9th ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pp. 1-13, </pages> <year> 1990. </year>
Reference: [3] <author> N. Alon. </author> <title> Generating pseudo-random permutations and maximum-flow algorithms. </title> <booktitle> In Infor. Proc. </booktitle> <volume> Letters 35, </volume> <pages> 201-204, </pages> <year> 1990. </year>
Reference: [4] <author> N. Alon, G. Kalai, M. Ricklin, and L. Stockmeyer. </author> <title> Lower bounds on the competitive ratio for mobile user tracking and distributed job scheduling. </title> <booktitle> In Proc. 33rd IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 334-343, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Bartal, Fiat, and Rabani [17], and Awerbuch, Kut-ten, and Peleg [15], took the first steps in this direction. Their work was in the context of job scheduling and data management. In these papers, and in subsequent work <ref> [4, 14, 18] </ref>, the cost of a distributed on-line Page 1 algorithm is compared to the cost of an optimal global--control algorithm 1 . (This is also done implicitly in the earlier work of Awerbuch and Peleg [16].) As has been observed elsewhere (see, e.g. [14], paraphrased here), this imposes an <p> This is the approach introduced in this paper. An algorithm that is k-competitive according to the competitive notion of all current distributed competitive literature <ref> [14, 15, 4, 17, 18] </ref>, is at most k competitive according to our notion, but may be much better. (A concrete example appears below.) Thus, the competitive notion in this paper captures the performance of distributed algorithms more accurately than does the definition used in the literature.
Reference: [5] <author> J. Anderson. </author> <title> Composite Registers. </title> <booktitle> Proc. 9th ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pp. 15-30, </pages> <month> August </month> <year> 1990. </year>
Reference: [6] <author> R. Anderson and H. Woll. </author> <title> Wait-free Parallel Algorithms for the Union-Find Problem. </title> <booktitle> In Proc. 23rd ACM Symposium on Theory of Computing, </booktitle> <pages> pp. 370-380, </pages> <year> 1991. </year> <pages> Page 9 </pages>
Reference-contexts: A first step in this direction was made by Anderson and Woll in their elegant work on the certified write-all problem <ref> [6] </ref> (see Section 2). Due to the requirement of freshness, our adversary is less constrained than the adversary in [6], where freshness is not an issue. Thus, we need additional insight into the combinatorial structure of the schedule. <p> A first step in this direction was made by Anderson and Woll in their elegant work on the certified write-all problem <ref> [6] </ref> (see Section 2). Due to the requirement of freshness, our adversary is less constrained than the adversary in [6], where freshness is not an issue. Thus, we need additional insight into the combinatorial structure of the schedule. In particular, for this part of the proof we prove that if the adversary has a short description, then there exists a good set of permutations. <p> This paper was followed by a number of others that consider variants of the basic problem (see, for example, <ref> [6, 21, 38, 39, 40, 41, 44, 45] </ref>). All of the work on the CWA assumes some sort of multi-writer registers. In a model that provides multi-writer registers, the cooperative collect would be equivalent to the certified write-all (CWA) problem, were it not for the issue of freshness. <p> If registers are re-used the problem becomes more complicated, particularly in a deterministic setting. We know of no work on deterministic algorithms for the CWA problem that addresses these issues in our model of computation. (For example, <ref> [6] </ref> assumes Compare&Swap and a tagged architecture, in which associated with each register is a tag indicating the last time that it was written.) In contrast, our algorithms are deterministic. <p> Page 5 5 The Speedy Collect Algorithm In this section we present a non-constructive algo rithm that is O ( p n log 2 n)-competitive with respect to latency. Our starting point is the Certified Write-All algorithm of Anderson and Woll <ref> [6] </ref>. In their algorithm every process p i has a fixed permutation i of the integers f1; : : : ; ng. When p i takes a step it writes to the first location in i that has not yet been written. <p> The total number of writes performed by each p i in this schedule is bounded above by the length of the longest greedy monotonic increasing subsequence of i with respect to . It was shown probabilistically in <ref> [6] </ref> that there exists a set of n permutations on the numbers f1; : : : ; ng such that the sum of the lengths of all longest greedy monotonic increasing subsequences on the set with respect to any ordering is O (n log n). Later, J. Naor and R. <p> Later, J. Naor and R. Roth [47] obtained an explicit construction in which this quantity is O (n (log n) 1+" ). This, then, is our starting point. We observe that the adversary scheduler in <ref> [6] </ref> can be described in n log n bits. Due to freshness considerations, our problem is harder, and our adversary has more flexibility, and therefore may require significantly more bits to describe. <p> The relation between the R i 's and the adversary scheduler is as follows. In the algorithm of <ref> [6] </ref>, R a i describes the order in which the cells 1 : : : n are first written; thus R a i = R a j for all i; j; a.
Reference: [7] <author> J. Aspnes. </author> <title> Time- and space-efficient randomized con-sensus. </title> <journal> Journal of Algorithms 14(3) </journal> <pages> 414-431, </pages> <month> May </month> <year> 1993. </year> <title> An earlier version appeared in Proc. </title> <booktitle> 9th ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pp. 325-331, </pages> <month> August </month> <year> 1990. </year>
Reference: [8] <author> J. Aspnes and M. Herlihy. </author> <title> Fast randomized consensus using shared memory. </title> <note> In Journal of Algorithms 11(3), pp.441-461, </note> <month> September </month> <year> 1990. </year>
Reference: [9] <author> J. Aspnes and M. P. Herlihy. </author> <title> Wait-Free Data Structures in the Asynchronous PRAM Model. </title> <booktitle> In Proceedings of the 2nd Annual Symposium on Parallel Algorithms and Architectures, </booktitle> <month> July </month> <year> 1990, </year> <pages> pp. 340-349, </pages> <address> Crete, Greece. </address>
Reference: [10] <author> J. Aspnes and O. Waarts. </author> <title> Randomized consensus in expected O(n log 2 n) operations per processor. </title> <booktitle> In Proc. 33rd IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pp. 137-146, </pages> <month> October </month> <year> 1992. </year>
Reference: [11] <author> H. Attiya, M. Herlihy, and O. Rachman. </author> <title> Efficient atomic snapshots using lattice agreement. </title> <type> Technical report, </type> <institution> Technion, Haifa, Israel, </institution> <year> 1992. </year> <note> A preliminary version appeared in proceedings of the 6th International Workshop on Distributed Algorithms, </note> <institution> Haifa, Is-rael, </institution> <month> November </month> <year> 1992, </year> <editor> (A. Segall and S. Zaks, eds.), </editor> <booktitle> Lecture Notes in Computer Science #647, </booktitle> <publisher> Springer-Verlag, </publisher> <pages> pp. 35-53. </pages>
Reference: [12] <author> H. Attiya, A. Herzberg, and S. Rajsbaum. </author> <title> Optimal Clock Synchronization under Different Delay Assumptions. </title> <booktitle> In Proc. 12th ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pp. 109-120, </pages> <month> Aug. </month> <year> 1993. </year>
Reference-contexts: Their results were recently extended by Prisco, Mayer and Yung [49]. There is a long history of interest in optimality of a distributed algorithm given certain conditions, such as a particular pattern of failures [25, 29, 34, 46], or a particular pattern of message delivery <ref> [12, 31, 48] </ref>. These and related works are in the spirit of our paper, but differ substantially in the details and applicability to distinct situations.
Reference: [13] <author> H. Attiya and O. Rachman. </author> <title> Atomic Snapshots in 0(n log n) Operations. </title> <booktitle> In Proc. 12th ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pp. 29-40, </pages> <month> Aug. </month> <year> 1993. </year>
Reference-contexts: Intuitively, p will trust any value tagged with a timestamp whose component for p equals collect-num p because these values are necessarily read after p's collect began. The views of processes in a group are read and updated using the atomic snapshot algorithm of At-tiya and Rachman <ref> [13] </ref>. The basic operation of the Attiya-Rachman algorithm on an array A is Scan-Update (v), where v can be null.
Reference: [14] <author> B. Awerbuch, Y. Bartal, and A. Fiat. </author> <title> Competitive distributed file allocation. </title> <booktitle> In Proc. 25th ACM Symposium on Theory of Computing, </booktitle> <pages> pp. 164-173, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Bartal, Fiat, and Rabani [17], and Awerbuch, Kut-ten, and Peleg [15], took the first steps in this direction. Their work was in the context of job scheduling and data management. In these papers, and in subsequent work <ref> [4, 14, 18] </ref>, the cost of a distributed on-line Page 1 algorithm is compared to the cost of an optimal global--control algorithm 1 . (This is also done implicitly in the earlier work of Awerbuch and Peleg [16].) As has been observed elsewhere (see, e.g. [14], paraphrased here), this imposes an <p> papers, and in subsequent work [4, 14, 18], the cost of a distributed on-line Page 1 algorithm is compared to the cost of an optimal global--control algorithm 1 . (This is also done implicitly in the earlier work of Awerbuch and Peleg [16].) As has been observed elsewhere (see, e.g. <ref> [14] </ref>, paraphrased here), this imposes an additional handicap on the distributed on-line algorithm in comparison to the optimal algorithm: In the distributed algorithm the decisions are made based solely on local information. <p> This is the approach introduced in this paper. An algorithm that is k-competitive according to the competitive notion of all current distributed competitive literature <ref> [14, 15, 4, 17, 18] </ref>, is at most k competitive according to our notion, but may be much better. (A concrete example appears below.) Thus, the competitive notion in this paper captures the performance of distributed algorithms more accurately than does the definition used in the literature.
Reference: [15] <author> B. Awerbuch, S. Kutten, and D. Peleg. </author> <title> Competitive distributed job scheduling. </title> <booktitle> In Proc. 24th ACM Symposium on Theory of Computing, </booktitle> <pages> pp. 571-580, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Due to the additional type of nondeterminism in the distributed setting, it is not obvious how to extend the notion of competitive analysis to this environment. Bartal, Fiat, and Rabani [17], and Awerbuch, Kut-ten, and Peleg <ref> [15] </ref>, took the first steps in this direction. Their work was in the context of job scheduling and data management. <p> This is the approach introduced in this paper. An algorithm that is k-competitive according to the competitive notion of all current distributed competitive literature <ref> [14, 15, 4, 17, 18] </ref>, is at most k competitive according to our notion, but may be much better. (A concrete example appears below.) Thus, the competitive notion in this paper captures the performance of distributed algorithms more accurately than does the definition used in the literature. <p> Under both the definition of Sleator and Tarjan and the one introduced by <ref> [15, 17] </ref>, one only has to show that the competitive algorithm performs well in comparison with any other algorithm that deals with one type of nondeterminism: the nondeterminism of 1 Because most distributed algorithms have an on-line flavor, we use the terms distributed algorithm and distributed on-line algorithm interchangeably. not knowing <p> Thus no distributed algorithm can be competitive against such an algorithm. Hence also the competitive measure of <ref> [15, 17] </ref> does not allow us to distinguish between the nave algorithm and algorithms that totally dominate it. The competitive measure presented here allows us such a distinction. To characterize the behavior of an algorithm over a range of possible schedules we define the competitive latency of an algorithm.
Reference: [16] <author> B. Awerbuch and D. Peleg. </author> <title> Sparse Partitions. </title> <booktitle> In Proc. 31st IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pp. 503-513, </pages> <month> November </month> <year> 1990. </year>
Reference-contexts: In these papers, and in subsequent work [4, 14, 18], the cost of a distributed on-line Page 1 algorithm is compared to the cost of an optimal global--control algorithm 1 . (This is also done implicitly in the earlier work of Awerbuch and Peleg <ref> [16] </ref>.) As has been observed elsewhere (see, e.g. [14], paraphrased here), this imposes an additional handicap on the distributed on-line algorithm in comparison to the optimal algorithm: In the distributed algorithm the decisions are made based solely on local information.
Reference: [17] <author> Y. Bartal, A. Fiat, and Y. Rabani. </author> <title> Competitive algorithms for distributed data management. </title> <booktitle> In Proc. 24th ACM Symposium on Theory of Computing, </booktitle> <pages> pp. 39-50, </pages> <year> 1992. </year>
Reference-contexts: Due to the additional type of nondeterminism in the distributed setting, it is not obvious how to extend the notion of competitive analysis to this environment. Bartal, Fiat, and Rabani <ref> [17] </ref>, and Awerbuch, Kut-ten, and Peleg [15], took the first steps in this direction. Their work was in the context of job scheduling and data management. <p> This is the approach introduced in this paper. An algorithm that is k-competitive according to the competitive notion of all current distributed competitive literature <ref> [14, 15, 4, 17, 18] </ref>, is at most k competitive according to our notion, but may be much better. (A concrete example appears below.) Thus, the competitive notion in this paper captures the performance of distributed algorithms more accurately than does the definition used in the literature. <p> Under both the definition of Sleator and Tarjan and the one introduced by <ref> [15, 17] </ref>, one only has to show that the competitive algorithm performs well in comparison with any other algorithm that deals with one type of nondeterminism: the nondeterminism of 1 Because most distributed algorithms have an on-line flavor, we use the terms distributed algorithm and distributed on-line algorithm interchangeably. not knowing <p> Thus no distributed algorithm can be competitive against such an algorithm. Hence also the competitive measure of <ref> [15, 17] </ref> does not allow us to distinguish between the nave algorithm and algorithms that totally dominate it. The competitive measure presented here allows us such a distinction. To characterize the behavior of an algorithm over a range of possible schedules we define the competitive latency of an algorithm.
Reference: [18] <author> Y. Bartal, and A. Rosen. </author> <title> The distributed k-server problem A competitive distributed translator for k-server algorithms. </title> <booktitle> In Proc. 33rd IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pp. 344-353, </pages> <month> Oc-tober </month> <year> 1992. </year>
Reference-contexts: Bartal, Fiat, and Rabani [17], and Awerbuch, Kut-ten, and Peleg [15], took the first steps in this direction. Their work was in the context of job scheduling and data management. In these papers, and in subsequent work <ref> [4, 14, 18] </ref>, the cost of a distributed on-line Page 1 algorithm is compared to the cost of an optimal global--control algorithm 1 . (This is also done implicitly in the earlier work of Awerbuch and Peleg [16].) As has been observed elsewhere (see, e.g. [14], paraphrased here), this imposes an <p> This is the approach introduced in this paper. An algorithm that is k-competitive according to the competitive notion of all current distributed competitive literature <ref> [14, 15, 4, 17, 18] </ref>, is at most k competitive according to our notion, but may be much better. (A concrete example appears below.) Thus, the competitive notion in this paper captures the performance of distributed algorithms more accurately than does the definition used in the literature.
Reference: [19] <author> G. Bracha and O. Rachman. </author> <title> Randomized consensus in expected O(n 2 log n) operations. </title> <booktitle> Proceedings of the Fifth International Workshop on Distributed Algorithms. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference: [20] <author> M. F. Bridgeland and R. J. Watro. </author> <title> Fault-Tolerant Decision Making in Totally Asynchronous Distributed Systems. </title> <booktitle> In Proc. 6th ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pp. 52-63, </pages> <year> 1987. </year>
Reference-contexts: In the asynchronous message-passing model, Bridgeland and Watro studied the problem of performing a number t of tasks in a system of n processors <ref> [20] </ref>. In their work, processors may fail by crashing and each processor can perform at most one unit of work. They provide tight bounds on the number of crash failures that can be tolerated by any solution to the problem.
Reference: [21] <author> J. Buss and P. Ragde. </author> <title> Certified Write-All on a Strongly Asynchronous PRAM. </title> <type> Manuscript, </type> <year> 1990. </year>
Reference-contexts: This paper was followed by a number of others that consider variants of the basic problem (see, for example, <ref> [6, 21, 38, 39, 40, 41, 44, 45] </ref>). All of the work on the CWA assumes some sort of multi-writer registers. In a model that provides multi-writer registers, the cooperative collect would be equivalent to the certified write-all (CWA) problem, were it not for the issue of freshness.
Reference: [22] <author> T. Chandra and C. Dwork. </author> <title> Using Consensus to solve Atomic Snapshots. </title> <note> Submitted for Publication </note>
Reference: [23] <author> B. Chor, A. Israeli, and M. Li. </author> <title> On processor coordination using asynchronous hardware. </title> <booktitle> In Proc. 6th ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pp. 86-97, </pages> <year> 1987. </year>
Reference: [24] <author> D. Dolev and N. Shavit. </author> <title> Bounded Concurrent Time-Stamp Systems are Constructible! In Proc. </title> <booktitle> 21st ACM Symposium on Theory of Computing, </booktitle> <pages> pp. 454-465, </pages> <year> 1989. </year> <note> An extended version appears in IBM Research Report RJ 6785, </note> <month> March </month> <year> 1990. </year>
Reference: [25] <author> D. Dolev, R. Reischuk, and H.R. </author> <title> Strong. Early Stopping in Byzantine Agreement. </title> <journal> JACM 34:7, </journal> <month> Oct. </month> <year> 1990, </year> <pages> pp. 720-741. </pages> <note> First appeared in: Eventual is Earlier than Immediate, </note> <institution> IBM RJ 3915, </institution> <year> 1983. </year>
Reference-contexts: Their results were recently extended by Prisco, Mayer and Yung [49]. There is a long history of interest in optimality of a distributed algorithm given certain conditions, such as a particular pattern of failures <ref> [25, 29, 34, 46] </ref>, or a particular pattern of message delivery [12, 31, 48]. These and related works are in the spirit of our paper, but differ substantially in the details and applicability to distinct situations.
Reference: [26] <author> C. Dwork, M. Herlihy, S. Plotkin, and O. Waarts. </author> <title> Time-lapse snapshots. </title> <booktitle> Proceedings of Israel Symposium on the Theory of Computing and Systems, </booktitle> <year> 1992. </year>
Reference: [27] <author> C. Dwork, J. Halpern, and O. Waarts. </author> <title> Accomplishing Work in the Presence of Failures. </title> <booktitle> In Proc. 11th ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pp. 91-102, </pages> <year> 1992. </year>
Reference-contexts: They provide tight bounds on the number of crash failures that can be tolerated by any solution to the problem. In the synchronous message-passing model, Dwork, Halpern, and Waarts studied essentially the same problem <ref> [27] </ref>. Their goal was to design algorithms that minimized the total amount of effort, defined as the sum of the work performed and messages sent, in order for each non-faulty process to ensure that all tasks have been performed. Their results were recently extended by Prisco, Mayer and Yung [49].
Reference: [28] <author> C. Dwork, M. Herlihy, and O. Waarts. </author> <title> Bounded Round Numbers. </title> <booktitle> In Proc. 12th ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pp. 53-64, </pages> <year> 1993. </year>
Reference: [29] <author> C. Dwork and Y. Moses. </author> <title> Knowledge and Common Knowledge in a Byzantine Environment: Crash Failures. </title> <booktitle> In Information and Computation 88(2) (1990), originally in Proc. TARK 1986. </booktitle>
Reference-contexts: Their results were recently extended by Prisco, Mayer and Yung [49]. There is a long history of interest in optimality of a distributed algorithm given certain conditions, such as a particular pattern of failures <ref> [25, 29, 34, 46] </ref>, or a particular pattern of message delivery [12, 31, 48]. These and related works are in the spirit of our paper, but differ substantially in the details and applicability to distinct situations.
Reference: [30] <author> C. Dwork and O. Waarts. </author> <title> Simple and Efficient Bounded Concurrent Timestamping or Bounded Concurrent Timestamp Systems are Comprehensible!, </title> <booktitle> In Proc. 24th ACM Symposium on Theory of Computing, </booktitle> <pages> pp. 655-666, </pages> <year> 1992. </year>
Reference: [31] <author> M. Fischer and A. Michael, </author> <title> Sacrificing Serializability to Attain High Availablity of Data in an Unreliable Network. </title> <type> Research Report 221, </type> <institution> Yale U., </institution> <month> Feb. </month> <year> 1982. </year>
Reference-contexts: Their results were recently extended by Prisco, Mayer and Yung [49]. There is a long history of interest in optimality of a distributed algorithm given certain conditions, such as a particular pattern of failures [25, 29, 34, 46], or a particular pattern of message delivery <ref> [12, 31, 48] </ref>. These and related works are in the spirit of our paper, but differ substantially in the details and applicability to distinct situations.
Reference: [32] <author> R. Gawlick, N. Lynch, and N. Shavit. </author> <title> Concurrent Timestamping Made Simple. </title> <booktitle> Proceedings of Israel Symposium on Theory of Computing and Systems, </booktitle> <year> 1992. </year> <pages> Page 10 </pages>
Reference: [33] <author> J. Y. Halpern and Y. Moses. </author> <title> Knowledge and Common Knowledge in a Distributed Environment, </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> Vol 37, No 3, </volume> <month> January </month> <year> 1990, </year> <pages> pp. 549-587. </pages> <note> A preliminary version appeared in Proc. 3rd ACM Symposium on Principles of Distributed Computing, </note> <year> 1984. </year>
Reference: [34] <author> J.Y. Halpern, Y. Moses, and O. Waarts. </author> <title> A Characterization of Eventual Byzantine Agreement. </title> <booktitle> In Proc. 9th ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pp. 333-346, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: Their results were recently extended by Prisco, Mayer and Yung [49]. There is a long history of interest in optimality of a distributed algorithm given certain conditions, such as a particular pattern of failures <ref> [25, 29, 34, 46] </ref>, or a particular pattern of message delivery [12, 31, 48]. These and related works are in the spirit of our paper, but differ substantially in the details and applicability to distinct situations.
Reference: [35] <author> M.P. Herlihy. </author> <title> Randomized wait-free concurrent objects. </title> <booktitle> In Proc. 10th ACM Symposium on Principles of Distributed Computing, </booktitle> <month> August </month> <year> 1991. </year>
Reference: [36] <author> A. Israeli and M. Li. </author> <title> Bounded Time Stamps. </title> <booktitle> In Proc. 28th IEEE Symposium on Foundations of Computer Science, </booktitle> <year> 1987. </year>
Reference: [37] <author> A. Israeli and M. Pinhasov. </author> <title> A Concurrent Time-Stamp Scheme which is Linear in Time and Space. </title> <type> Manuscript, </type> <year> 1991. </year>
Reference: [38] <author> P. Kanellakis and A. Shvartsman. </author> <title> Efficient Parallel Algorithms Can Be Made Robust. </title> <booktitle> In Proc. 8th ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pp. 211-222 </pages>
Reference-contexts: The cooperative collect resembles the problem of arranging for processes to collaborate in order to perform a set of tasks. The closest problem in the literature is the certified write-all problem (CWA). In this problem, the first variant of which was introduced by Kanellakis and Shvartsman <ref> [38] </ref>, a group of processes must together write to every register in some set, and every process must learn that every register has been written into. <p> This paper was followed by a number of others that consider variants of the basic problem (see, for example, <ref> [6, 21, 38, 39, 40, 41, 44, 45] </ref>). All of the work on the CWA assumes some sort of multi-writer registers. In a model that provides multi-writer registers, the cooperative collect would be equivalent to the certified write-all (CWA) problem, were it not for the issue of freshness.
Reference: [39] <author> P. Kanellakis and A. Shvartsman. </author> <title> Efficient Robust Parallel Computations. </title> <booktitle> In Proc. 10th ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pp. 23-36, </pages> <year> 1991. </year>
Reference-contexts: This paper was followed by a number of others that consider variants of the basic problem (see, for example, <ref> [6, 21, 38, 39, 40, 41, 44, 45] </ref>). All of the work on the CWA assumes some sort of multi-writer registers. In a model that provides multi-writer registers, the cooperative collect would be equivalent to the certified write-all (CWA) problem, were it not for the issue of freshness.
Reference: [40] <author> Z. Kedem, K. Palem, A. Raghunathan, and P. Spi-rakis. </author> <title> Combining Tentative and Definitie Algorithms for Very Fast Dependable Parallel Computing. </title> <booktitle> In Proc. 23rd ACM Symposium on Theory of Computing, </booktitle> <pages> pp. 381-390, </pages> <year> 1991. </year>
Reference-contexts: This paper was followed by a number of others that consider variants of the basic problem (see, for example, <ref> [6, 21, 38, 39, 40, 41, 44, 45] </ref>). All of the work on the CWA assumes some sort of multi-writer registers. In a model that provides multi-writer registers, the cooperative collect would be equivalent to the certified write-all (CWA) problem, were it not for the issue of freshness.
Reference: [41] <author> A. Kedem, K. Palem, and P. Spiriakis. </author> <title> Efficient Robust Parallel Computations. </title> <booktitle> In Proc. 22nd ACM Symposium on Theory of Computing, </booktitle> <pages> pp. 138-148, </pages> <year> 1990. </year>
Reference-contexts: This paper was followed by a number of others that consider variants of the basic problem (see, for example, <ref> [6, 21, 38, 39, 40, 41, 44, 45] </ref>). All of the work on the CWA assumes some sort of multi-writer registers. In a model that provides multi-writer registers, the cooperative collect would be equivalent to the certified write-all (CWA) problem, were it not for the issue of freshness.
Reference: [42] <author> L. M. Kirousis, P. Spirakis and P. Tsigas. </author> <title> Reading Many Variables in One Atomic Operation Solutions With Linear or Sublinear Complexity. </title> <booktitle> In Proceedings of the 5th International Workshop on Distributed Algorithms, </booktitle> <year> 1991. </year>
Reference: [43] <author> L. Lamport. </author> <title> On Interprocess Communication, Parts I and II. </title> <booktitle> Distributed Computing 1, </booktitle> <pages> pp. 77-101, </pages> <year> 1986. </year>
Reference-contexts: As 3 This is analogous to the regularity property for registers <ref> [43] </ref>: if a read operation R returns a value that was written in an update operation U 1 , there must be no update operation U 2 to the same register such that U 1 ! U 2 ! R. discussed above, we refine previous notions by requiring that this best
Reference: [44] <author> C. Martel and R. Subramonian. </author> <title> On the Complexity of Certified Write-All Algorithms. </title> <type> Manuscript, </type> <year> 1993. </year>
Reference-contexts: This paper was followed by a number of others that consider variants of the basic problem (see, for example, <ref> [6, 21, 38, 39, 40, 41, 44, 45] </ref>). All of the work on the CWA assumes some sort of multi-writer registers. In a model that provides multi-writer registers, the cooperative collect would be equivalent to the certified write-all (CWA) problem, were it not for the issue of freshness.
Reference: [45] <author> C. Martel, R. Subramonian, and A. Park. </author> <title> Asynchronous PRAMS are (Almost) as Good as Synchronous PRAMs. </title> <booktitle> In Proc. 32nd IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pp. 590-599, </pages> <year> 1990. </year>
Reference-contexts: This paper was followed by a number of others that consider variants of the basic problem (see, for example, <ref> [6, 21, 38, 39, 40, 41, 44, 45] </ref>). All of the work on the CWA assumes some sort of multi-writer registers. In a model that provides multi-writer registers, the cooperative collect would be equivalent to the certified write-all (CWA) problem, were it not for the issue of freshness.
Reference: [46] <author> Y. Moses and M.R. Tuttle. </author> <title> Programming Simultaneous Actions Using Common Knowledge. </title> <journal> Algorithmica 3(1), </journal> <pages> pp. 121-169, </pages> <booktitle> 1988 (Also appeared in Proc. 28th IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pp. 208-221, </pages> <year> 1986.) </year>
Reference-contexts: Their results were recently extended by Prisco, Mayer and Yung [49]. There is a long history of interest in optimality of a distributed algorithm given certain conditions, such as a particular pattern of failures <ref> [25, 29, 34, 46] </ref>, or a particular pattern of message delivery [12, 31, 48]. These and related works are in the spirit of our paper, but differ substantially in the details and applicability to distinct situations.
Reference: [47] <author> J. Naor and R. M. Roth. </author> <title> Constructions of permutation arrays for certain scheduling cost measures. </title> <type> Manuscript. </type>
Reference-contexts: Later, J. Naor and R. Roth <ref> [47] </ref> obtained an explicit construction in which this quantity is O (n (log n) 1+" ). This, then, is our starting point. We observe that the adversary scheduler in [6] can be described in n log n bits.
Reference: [48] <author> B. Patt-Shamir and S. Rajsbaum. </author> <title> A Theory of Clock Synchronization. </title> <booktitle> In Proc. 26th ACM Symposium on Theory of Computing, </booktitle> <pages> pp. 810-819, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Their results were recently extended by Prisco, Mayer and Yung [49]. There is a long history of interest in optimality of a distributed algorithm given certain conditions, such as a particular pattern of failures [25, 29, 34, 46], or a particular pattern of message delivery <ref> [12, 31, 48] </ref>. These and related works are in the spirit of our paper, but differ substantially in the details and applicability to distinct situations.
Reference: [49] <author> R. D. Prisco, A. Mayer, and M. Yung. </author> <title> Time-optimal message-efficient work performance in the presence of faults. </title> <booktitle> In Proc. 30th ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 161-172, </pages> <year> 1994. </year>
Reference-contexts: Their goal was to design algorithms that minimized the total amount of effort, defined as the sum of the work performed and messages sent, in order for each non-faulty process to ensure that all tasks have been performed. Their results were recently extended by Prisco, Mayer and Yung <ref> [49] </ref>. There is a long history of interest in optimality of a distributed algorithm given certain conditions, such as a particular pattern of failures [25, 29, 34, 46], or a particular pattern of message delivery [12, 31, 48].
Reference: [50] <author> M. Saks, N. Shavit, and H. Woll. </author> <title> Optimal time randomized consensus | making resilient algorithms fast in practice. </title> <booktitle> In Proceedings of the 2nd ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pp. 351-362, </pages> <year> 1991. </year>
Reference-contexts: Our measure is defined formally in Section 4 and is one of the central contributions of the paper. Cooperative Collect To demonstrate our technique we study the problem of having processes repeatedly collect values, by the cooperative collect primitive, first abstracted by Saks, Shavit, and Woll <ref> [50] </ref>. In many shared-memory applications processes repeatedly read all values stored in a set of registers. If each process reads every register itself, then the communication costs increase dramatically with the degree of concurrency, due to bus congestion and contention. <p> In the cooperative collect primitive, processes perform the collect operation an operation in which the process learns the values of a set of n registers, with the guarantee that each value learned is fresh: 2 An exception is the consensus algorithm of Saks, Shavit, and Woll <ref> [50] </ref>. We discuss their results in Section 2. Page 2 it was present in the register at some point during the collect. 3 If each process reads every register, then this condition is trivially satisfied. <p> of space, this abstract describes only the non-constructive result, and most proofs are omitted or only sketched. 2 Other Related Work Saks, Shavit, and Woll were the first to recognize the opportunity for improving the efficiency of shared-memory algorithms by finding a way for processes to cooperate during their collects <ref> [50] </ref>. They devised an elegant randomized solution, which they analyzed in the so-called big-step model. In this model, a time unit is the minimal interval in the execution of the algorithm during which each non-faulty process executes at least one step.
Reference: [51] <author> D. D. Sleator and R. E. Tarjan. </author> <title> Amortized efficiency of list update and paging rules. </title> <journal> Comm. of the ACM 28(2), </journal> <pages> pp. 202-208, </pages> <year> 1985. </year>
Reference-contexts: Box 208285, New Haven CT 06520-8285. E-mail: aspnes-james@cs.yale.edu z Computer Science Division, U. C. Berkeley. Work supported by an NSF postdoctoral fellowship. During part of this research the fourth author was at IBM Almaden. E-Mail: orli@cs.stanford.edu Tarjan <ref> [51] </ref> to study problems that arise in an on-line setting, where an algorithm is given an unpredictable sequence of requests to perform operations, and must make decisions about how to satisfy its current request that may affect how efficiently it can satisfy future requests.
Reference: [52] <author> P. M. B. Vitanyi and B. Awerbuch. </author> <title> Atomic Shared Register Access by Asynchronous Hardware. </title> <booktitle> In Proc. 27th IEEE Symposium on Foundations of Computer Science, </booktitle> <year> 1986. </year> <pages> Page 11 </pages>
References-found: 52

