URL: http://www.umiacs.umd.edu/users/vishkin/PUBLICATIONS/icalp91.ps
Refering-URL: http://www.umiacs.umd.edu/users/vishkin/PUBLICATIONS/papers.html
Root-URL: 
Title: Structural Parallel Algorithmics  
Author: Uzi Vishkin 
Affiliation: University of Maryland Tel Aviv University  
Abstract: Some of the collective knowledge-base on non-numerical parallel algorithms can be characterized in a structural way. Each structure relates a few problems and technique to one another from the basic to the more involved. The second half of the paper provides a bird's-eye view of such structures for: (1) list, tree and graph parallel algorithms; (2) very fast deterministic parallel algorithms; and (3) very fast randomized parallel algorithms.
Abstract-found: 1
Intro-found: 1
Reference: [ABI86] <author> N. Alon, L. Babai, and A. Itai. </author> <title> A fast and simple randomized parallel algorithm for the maximal independent set problem. </title> <journal> J. Algorithms, </journal> <volume> 7 </volume> <pages> 567-583, </pages> <year> 1986. </year>
Reference-contexts: [HT84] and [SV88]. 6 Randomized Fast Algorithms Randomization has shown to be very useful for both the simulation of PRAM-like shared memory models of parallel computation by other models of parallel machines (e.g., in [KU86], [KRS88], [MV84], [Ran87], [KPS90] and [MSP90]), and for the design of parallel algorithms (e.g., in <ref> [ABI86] </ref>, [AM90], [Gaz86], [GM91], [KR87], [Lub86], [MR85], [MV90], [MV91], All "target algorithms" in this section are randomized, and their running time is at the doubly-logarithmic level, or faster. By the doubly-logarithmic level, we mean O (f (n) log log n) where the function f (n) is o (log log n).
Reference: [ADKP87] <author> K. Abrahamson, N. Dadoun, D. A. Kirkpatrick, and T. Przytycka. </author> <title> A simple parallel tree con traction algorithm. </title> <type> Technical Report 87-30, </type> <institution> The University of British Columbia, </institution> <year> 1987. </year>
Reference-contexts: trees are reviewed next: (1) The Euler tour technique [TV85] reduces the computation of many tree problems to list ranking. (2) The tree contraction technique [MR85] led to a number of optimal randomized logarithmic-time algorithms for tree problems, including expression tree evaluation; optimal deterministic versions were also given [GR86], [CV88], <ref> [ADKP87] </ref> and [KD88]. Implicit use of tree contraction in a non-standard parallel algorithmic setting appeared in [Bre74]. (3) Centroid decomposition of a tree, as implicitly used in [Win75] for O (log 2 n) time computations. Accelerating centroid decomposition was the motivation for the tree contraction version of [CV88].
Reference: [AHU74] <author> A. V. Aho, J. E. Hopcroft, and J. D. Ullman. </author> <title> The design and analysis of computer algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1974. </year>
Reference-contexts: Books on the topic include [Akl89], [GR88], [JaJ91], [Par87] and [Rei91]. For sequential computation, it has been of considerable advantage to deal with an abstraction of the von-Neumann machine, namely the RAM or Random Access Machine (see a standard textbook, such as <ref> [AHU74] </ref>). Two major advantages of such an abstraction are that 3 it makes the algorithm designer's task less complex, and it eliminates obstacles to algorithm portability.
Reference: [AIS84] <author> B. Awerbuch, A. Israeli, and Y. Shiloach. </author> <title> Finding Euler circuits in logarithmic parallel time. </title> <booktitle> In Proc. of the 16th Ann. ACM Symp. on Theory of Computing, </booktitle> <pages> pages 249-257, </pages> <month> May </month> <year> 1984. </year>
Reference-contexts: The graph connectivity problem turned out to be the main obstacle to deriving optimal logarithmic time algorithms for several graph problems, including: biconnectivity [TV85], finding Euler tour in a graph [AV84], <ref> [AIS84] </ref> and orienting the edges of an undirected graph to get a strongly connected digraph ("strong orientation)" [Vis85a]. We also note some recent parallel algorithms for k (edge and vertex) connectivity problems [KS89] and [CT91].
Reference: [Akl89] <author> S.G. Akl. </author> <title> The Design and Analysis of Parallel Algorithms. </title> <publisher> Prentice Hall, </publisher> <address> Engelwood Cliffs, New Jersey, </address> <year> 1989. </year> <month> 12 </month>
Reference-contexts: The survey paper [Vis83] elaborates on the raison d'etre of the "PRAM approach". Survey papers specializing on the class NC are [Coo81] and [Coo85]. More recent review articles include [EG88], [KR88a], and [KRS88], as well as [Ata90b], which is devoted to parallel computational geometry. Books on the topic include <ref> [Akl89] </ref>, [GR88], [JaJ91], [Par87] and [Rei91]. For sequential computation, it has been of considerable advantage to deal with an abstraction of the von-Neumann machine, namely the RAM or Random Access Machine (see a standard textbook, such as [AHU74]).
Reference: [AKS83] <author> M. Ajtai, J. Komlos, and E. Szemeredi. </author> <title> An O(n log n) sorting network. </title> <booktitle> In Proc. of the 15th Ann. ACM Symp. on Theory of Computing, </booktitle> <pages> pages 1-9, </pages> <year> 1983. </year>
Reference-contexts: Several parallel deterministic and randomized algorithms, that run in time proportional to log n= log log n ("logarithmic level") or slower, were given for sorting <ref> [AKS83] </ref>, [Bat68], [BN89], [Col88], [Hir78] [Pre78], [RV87], and [SV81], and integer sorting [BDH + 89], [Hag87], [Hag91a], [MV90], [MV91], [RR89a], [Ram90] and [Ram91].
Reference: [AM88] <author> R.J. Anderson and G.L. Miller. </author> <title> Optimal parallel algorithms for list ranking. </title> <booktitle> In 3rd Aegean workshop on computing, Lecture Notes in Computer Science 319, 1988, </booktitle> <publisher> Springer-Verlag, </publisher> <pages> pages 81-90, </pages> <year> 1988. </year>
Reference-contexts: In fact, obtaining optimal algorithms for list ranking and (undirected) graph connectivity proved to be central to obtaining optimal algorithms for a considerable number of list, tree and graph problems. First randomized, and later deterministic, optimal parallel algorithms for list ranking were given [Vis84b], [CV86b], [CV86a], <ref> [AM88] </ref> and [CV89]. The deterministic algorithms are based on a deterministic arbitration technique, dubbed deterministic coin tossing [CV86b]. Extensions of this technique for sparse graphs and other applications were given [GPS87], [CZ90], and [HCD87].
Reference: [AM90] <author> N. Alon and N. Megiddo. </author> <title> Parallel linear programming almost surely in constant time. </title> <booktitle> In Proc. of the 31st IEEE Annual Symp. on Foundation of Computer Science, </booktitle> <pages> pages 574-582, </pages> <year> 1990. </year>
Reference-contexts: and [SV88]. 6 Randomized Fast Algorithms Randomization has shown to be very useful for both the simulation of PRAM-like shared memory models of parallel computation by other models of parallel machines (e.g., in [KU86], [KRS88], [MV84], [Ran87], [KPS90] and [MSP90]), and for the design of parallel algorithms (e.g., in [ABI86], <ref> [AM90] </ref>, [Gaz86], [GM91], [KR87], [Lub86], [MR85], [MV90], [MV91], All "target algorithms" in this section are randomized, and their running time is at the doubly-logarithmic level, or faster. By the doubly-logarithmic level, we mean O (f (n) log log n) where the function f (n) is o (log log n). <p> By the doubly-logarithmic level, we mean O (f (n) log log n) where the function f (n) is o (log log n). Several constant-time optimal randomized algorithms were given: (1) for finding the maximum among n elements [Rei81]; and its generalization (2) for linear programming in fixed dimension <ref> [AM90] </ref>; (3) for finding approximate median [Sen89]; (4) for the nearest one problem (as in [BV89] and [Rag90]), under the assumption that there is some upper bound on the number of ones, [Ram90].
Reference: [Ata90a] <author> M.J. Atallah. </author> <title> A faster algorithm for a parallel algorithm for a matrix searching problem. </title> <booktitle> In Proc. 2nd SWAT, </booktitle> <volume> volume LNCS 447, </volume> <pages> pages 192-200. </pages> <publisher> Springer-Verlag, </publisher> <year> 1990. </year>
Reference-contexts: More remotedly related to the doubly-logarithmic tree paradigm is a matrix searching algorithm <ref> [Ata90a] </ref> Remark. Some of the problems mentioned in this section, particularly from here on, may have a rather specific flavor. However, they are still interesting since improvement on the more general problem is either impossible or apparently difficult.
Reference: [Ata90b] <author> M.J. Atallah. </author> <title> Parallel techniques for computational geometry. </title> <type> Technical Report CS-1020, </type> <institution> Purdue University, </institution> <year> 1990. </year>
Reference-contexts: The survey paper [Vis83] elaborates on the raison d'etre of the "PRAM approach". Survey papers specializing on the class NC are [Coo81] and [Coo85]. More recent review articles include [EG88], [KR88a], and [KRS88], as well as <ref> [Ata90b] </ref>, which is devoted to parallel computational geometry. Books on the topic include [Akl89], [GR88], [JaJ91], [Par87] and [Rei91].
Reference: [AV84] <author> M.J. Atallah and U. Vishkin. </author> <title> Finding Euler tours in parallel. </title> <journal> J. Comp. Sys. Sci., </journal> <volume> 29,3:330-337, </volume> <year> 1984. </year>
Reference-contexts: The graph connectivity problem turned out to be the main obstacle to deriving optimal logarithmic time algorithms for several graph problems, including: biconnectivity [TV85], finding Euler tour in a graph <ref> [AV84] </ref>, [AIS84] and orienting the edges of an undirected graph to get a strongly connected digraph ("strong orientation)" [Vis85a]. We also note some recent parallel algorithms for k (edge and vertex) connectivity problems [KS89] and [CT91].
Reference: [Bat68] <author> K. Batcher. </author> <title> Sorting networks and their applications. </title> <booktitle> In AFIPS Spring Joint Computing Confer ence, </booktitle> <pages> pages 307-314, 32(1968). </pages>
Reference-contexts: Several parallel deterministic and randomized algorithms, that run in time proportional to log n= log log n ("logarithmic level") or slower, were given for sorting [AKS83], <ref> [Bat68] </ref>, [BN89], [Col88], [Hir78] [Pre78], [RV87], and [SV81], and integer sorting [BDH + 89], [Hag87], [Hag91a], [MV90], [MV91], [RR89a], [Ram90] and [Ram91].
Reference: [BBG + 89] <author> O. Berkman, D. Breslauer, Z. Galil, B. Schieber, and U. Vishkin. </author> <title> Highly-parallelizable problems. </title> <booktitle> In Proc. of the 21st Ann. ACM Symp. on Theory of Computing, </booktitle> <pages> pages 309-319, </pages> <year> 1989. </year>
Reference-contexts: Figure 2.1 discusses works that can be viewed as using the doubly-logarithmic tree paradigm, as per <ref> [BBG + 89] </ref> . Doubly-logarithmic trees are rooted trees with n = 2 2 i leaves for some integer i &gt; 0. The root has 2 2 (i1) children, each being the root of a doubly-logarithmic subtree with 2 2 (i1) leaves.
Reference: [BDH + 89] <author> P.C.P. Bhatt, K. Diks, T. Hagerup, V.C. Prasad, T. Radzik, and S. Saxena. </author> <title> Improved determin istic parallel integer sorting. </title> <type> Technical Report TR 15/1989, </type> <institution> Fachbereich Informatik, Universitat des Saarlandes, D-6600 Saarbrucken, W. Germany, </institution> <month> November </month> <year> 1989. </year>
Reference-contexts: Several parallel deterministic and randomized algorithms, that run in time proportional to log n= log log n ("logarithmic level") or slower, were given for sorting [AKS83], [Bat68], [BN89], [Col88], [Hir78] [Pre78], [RV87], and [SV81], and integer sorting <ref> [BDH + 89] </ref>, [Hag87], [Hag91a], [MV90], [MV91], [RR89a], [Ram90] and [Ram91].
Reference: [BG88] <author> D. Breslauer and Z. Galil. </author> <title> An optimal O(log log n) parallel string matching algorithm. </title> <note> To appear in SIAM J. Comput., </note> <year> 1988. </year>
Reference-contexts: String matching: For some family of parallel algorithms it is sufficient to consider only non-periodic patterns [Gal85]. A method for eliminating (at least) one among two potential occurrences of a non-periodic pattern string in a text string in [Vis85b] was observed in <ref> [BG88] </ref> to be similar to comparing two numbers in order to determine which one is larger and together with an algorithm for finding the maximum, led to an optimal doubly-logarithmic string matching algorithm; [BG91] showed recently a matching lower-bound for a parallel comparison model of computation.
Reference: [BG91] <author> D. Breslauer and Z. Galil. </author> <title> A lower bound for parallel string matching. </title> <booktitle> In Proc. of the 23rd Ann. ACM Symp. on Theory of Computing, </booktitle> <year> 1991. </year>
Reference-contexts: two potential occurrences of a non-periodic pattern string in a text string in [Vis85b] was observed in [BG88] to be similar to comparing two numbers in order to determine which one is larger and together with an algorithm for finding the maximum, led to an optimal doubly-logarithmic string matching algorithm; <ref> [BG91] </ref> showed recently a matching lower-bound for a parallel comparison model of computation.
Reference: [BH85] <author> A. Borodin and J.E. Hopcroft. </author> <title> Routing, merging, and sorting on parallel models of computation. </title> <journal> J. Computer and System Sciences, </journal> <volume> 30 </volume> <pages> 130-145, </pages> <year> 1985. </year>
Reference-contexts: paper, we mention here several lower-bound results whose circumvention provided motivation for much of the research in sections 5 and 6: (1) (log n= log log n) time using a polynomial number of processors for the parity problem [BH87]; (2) for finding the maximum among n elements [Val75], and merging <ref> [BH85] </ref> on a parallel comparison model of computation; and (3) for CREW PRAM computation of the OR function of n bits [CDR86]. <p> Such structure guides the computation in optimal doubly-logarithmic parallel algorithms for finding the maximum among n elements [SV81] (using [Val75]), finding the maximum relative to all prefixes of an array of elements [Sch87] and [BSV88] (the prefix-maxima problem), merging two sorted lists [Kru83] and <ref> [BH85] </ref>, finding the convex hull of a monotone polygon [BSV91], and finding all nearest neighbors in a convex polygon [SV90]. Note that all merging algorithms that are mentioned in this paper may be implemented on a CREW PRAM.
Reference: [BH87] <author> P. Beame and J. Hastad. </author> <title> Optimal bounds for decision problems on the CRCW PRAM. </title> <booktitle> In Proc. of the 19th Ann. ACM Symp. on Theory of Computing, </booktitle> <pages> pages 83-93, </pages> <year> 1987. </year>
Reference-contexts: While lower-bound techniques are not the focus of this paper, we mention here several lower-bound results whose circumvention provided motivation for much of the research in sections 5 and 6: (1) (log n= log log n) time using a polynomial number of processors for the parity problem <ref> [BH87] </ref>; (2) for finding the maximum among n elements [Val75], and merging [BH85] on a parallel comparison model of computation; and (3) for CREW PRAM computation of the OR function of n bits [CDR86]. <p> The lower-bound in <ref> [BH87] </ref> implies that faster algorithms are possible only by relaxing the definition of the problem: (1) [MS91] gave a doubly-logarithmic level result, assuming the input comes from a certain random source; the output is given in a "padded" representation; (2) [Hag91a] allows general integer inputs from the range [1::n]; the output
Reference: [BJK + 90] <author> O. Berkman, J. JaJa, S. Krishnamurthy, R. Thurimella, and U. Vishkin. </author> <title> Some triply-logarithmic parallel algorithms. </title> <booktitle> In Proc. of the 31st IEEE Annual Symp. on Foundation of Computer Science, </booktitle> <pages> pages 871-881, </pages> <year> 1990. </year>
Reference-contexts: A triply-logarithmic paradigm <ref> [BJK + 90] </ref> uses the surplus-log approach, in conjunction with doubly-logarithmic algorithms for the same problems, as part of a global strategy. <p> The running time obtained is O (log log log s) [BV90]. There are also similar triply-logarithmic results for the prefix-maxima problem <ref> [BJK + 90] </ref> (and thereby for finding the maximum among n elements). Optimal log-star time (i.e., O (log fl n)) time) parallel algorithms seem to be the hardest to fit into a strict structure of paradigms using presently available ideas. See Figure 2.2.
Reference: [BJK + 91] <author> O. Berkman, J. JaJa, S. Krishnamurthy, R. Thurimella, and U. Vishkin. </author> <title> Top-bottom routing is as easy as prefix minima. In preparation (a preliminary and partial version is part of Some Triply-logarithmic Parallel Algorithms, </title> <note> see above), </note> <year> 1991. </year>
Reference-contexts: See Figure 2.2. However, using the surplus-log approach, as a rule-of-thumb, was helpful for several problems: (1) String matching for a preprocessed pattern [Vis91]; (2) prefix-maxima <ref> [BJK + 91] </ref>; there, this prefix-maxima algorithm is also the most time consuming step in an algorithm for routing around a rectangle a VLSI routing problem ; and (3) for preprocessing a rooted tree, so that any level-ancestor query can be processed in constant-time [BV91b].
Reference: [BN89] <author> G. Bilardi and A. Nicolau. </author> <title> Adaptive bitonic sorting: an optimal parallel algorithm for shared memory machines. </title> <journal> SIAM J. Computing, </journal> <volume> 18 </volume> <pages> 216-228, </pages> <year> 1989. </year>
Reference-contexts: Several parallel deterministic and randomized algorithms, that run in time proportional to log n= log log n ("logarithmic level") or slower, were given for sorting [AKS83], [Bat68], <ref> [BN89] </ref>, [Col88], [Hir78] [Pre78], [RV87], and [SV81], and integer sorting [BDH + 89], [Hag87], [Hag91a], [MV90], [MV91], [RR89a], [Ram90] and [Ram91].
Reference: [Bre74] <author> R.P. Brent. </author> <title> The parallel evaluation of general arithmetic expressions. </title> <journal> J. Assoc. Comput. Mach., </journal> <volume> 21 </volume> <pages> 302-206, </pages> <year> 1974. </year>
Reference-contexts: Implicit use of tree contraction in a non-standard parallel algorithmic setting appeared in <ref> [Bre74] </ref>. (3) Centroid decomposition of a tree, as implicitly used in [Win75] for O (log 2 n) time computations. Accelerating centroid decomposition was the motivation for the tree contraction version of [CV88]. <p> to jobs is a typical concern in parallel algorithms; for instance, one of the most powerful methodologies for designing parallel algorithms is to have a first design in terms of total work and time; extending this first design into a "full PRAM" design is guided by a theorem due to <ref> [Bre74] </ref>; the problem, however, is that the theorem holds only for a non-standard model of parallel computation, where assignment of processors to jobs can be done free of charge; the methodology was first used for the design of a PRAM algorithm in [SV82b], and is elucidated in [Vis90] and [JaJ91], who
Reference: [BSV88] <author> O. Berkman, B. Schieber, and U. Vishkin. </author> <title> Some doubly logarithmic parallel algorithms based on finding all nearest smaller values. </title> <type> Technical Report UMIACS-TR-88-79, </type> <institution> Univ. of Maryland Inst. for Advanced Computer Studies, </institution> <year> 1988. </year>
Reference-contexts: Such structure guides the computation in optimal doubly-logarithmic parallel algorithms for finding the maximum among n elements [SV81] (using [Val75]), finding the maximum relative to all prefixes of an array of elements [Sch87] and <ref> [BSV88] </ref> (the prefix-maxima problem), merging two sorted lists [Kru83] and [BH85], finding the convex hull of a monotone polygon [BSV91], and finding all nearest neighbors in a convex polygon [SV90]. Note that all merging algorithms that are mentioned in this paper may be implemented on a CREW PRAM. <p> While generalizing two problems finding the maximum and merging an optimal doubly-logarithmic algorithm for ANSV was still possible <ref> [BSV88] </ref>.
Reference: [BSV91] <author> O. Berkman, B. Schieber, and U. Vishkin. </author> <title> The parallel complexity of finding the convex hull of a monotone polygon. </title> <note> In preparation, 1991. 13 </note>
Reference-contexts: the computation in optimal doubly-logarithmic parallel algorithms for finding the maximum among n elements [SV81] (using [Val75]), finding the maximum relative to all prefixes of an array of elements [Sch87] and [BSV88] (the prefix-maxima problem), merging two sorted lists [Kru83] and [BH85], finding the convex hull of a monotone polygon <ref> [BSV91] </ref>, and finding all nearest neighbors in a convex polygon [SV90]. Note that all merging algorithms that are mentioned in this paper may be implemented on a CREW PRAM. String matching: For some family of parallel algorithms it is sufficient to consider only non-periodic patterns [Gal85].
Reference: [BV89] <author> O. Berkman and U. Vishkin. </author> <title> Recursive *-tree parallel data-structure. </title> <booktitle> In Proc. of the 30th IEEE Annual Symp. on Foundation of Computer Science, </booktitle> <pages> pages 196-202, </pages> <year> 1989. </year>
Reference-contexts: Parallel 6 algorithms for this problem [SV88] and <ref> [BV89] </ref> use the Euler tour technique. Depth first search (DFS) is perceived by many as the most useful technique known for designing sequential algorithms for graph problems. Unfortunately, it is not known how to implement DFS efficiently in parallel. <p> Optimal inverse-Ackermann time (i.e., O (ff (n)) time , where ff is the inverse-Ackermann extremely slow growing function) parallel algorithms actually use the surplus-log approach in a methodological way, overviewed below. Benefiting from a construction on unbounded fan-in circuits in [CFL83], the inverse-Ackermann paradigm <ref> [BV89] </ref> works by designing a series of algorithms; the first in the series should run in O (1) time using n log n processors; then, in a certain way, slight increase in time implies significant decrease in the number of processors. <p> Several constant-time optimal randomized algorithms were given: (1) for finding the maximum among n elements [Rei81]; and its generalization (2) for linear programming in fixed dimension [AM90]; (3) for finding approximate median [Sen89]; (4) for the nearest one problem (as in <ref> [BV89] </ref> and [Rag90]), under the assumption that there is some upper bound on the number of ones, [Ram90].
Reference: [BV90] <author> O. Berkman and U. Vishkin. </author> <title> On parallel integer merging. </title> <type> Technical Report UMIACS-TR-90-15, </type> <institution> University of Maryland Inst. for Advanced Computer Studies, </institution> <year> 1990. </year>
Reference-contexts: The strategy leads to optimal parallel algorithms for several problems whose running time is triply-logarithmic in the following sense: consider, for instance, the problem of merging two sorted lists of integers drawn from the domain [1:::s]. The running time obtained is O (log log log s) <ref> [BV90] </ref>. There are also similar triply-logarithmic results for the prefix-maxima problem [BJK + 90] (and thereby for finding the maximum among n elements). <p> find the smallest j &gt; i such that a j = 1 and the largest j &lt; i such that a j = 1); this leads to: (4) merging two sorted lists; the nearest-one complementation and the merging algorithms are for a CREW PRAM; the last two results are in <ref> [BV90] </ref>.
Reference: [BV91a] <author> O. Berkman and U. Vishkin. </author> <title> Almost fully-parallel paretheses matching. </title> <note> In preparation, </note> <year> 1991. </year>
Reference-contexts: Problems for which optimal inverse-Ackerman algorithms were given include: (1) the all nearest smaller value (ANSV) problem; this leads to: (2) parentheses matching: given the level of nesting 9 for each parenthesis in a legal sequence of parentheses, find for each parenthesis its match; the last two results are in <ref> [BV91a] </ref>; (3) the nearest-one complementation problem: given is an array of bits (a 1 ; :::; a n ) and suppose for each a i = 1, the two nearest indices j and l, such that a j = a l = 1, are known; find for each a i =
Reference: [BV91b] <author> O. Berkman and U. Vishkin. </author> <title> Finding level-ancestors in trees. </title> <type> Technical Report UMIACS-TR 91-9, </type> <institution> University of Maryland Institute for Advanced Computer Studies, </institution> <year> 1991. </year>
Reference-contexts: a preprocessed pattern [Vis91]; (2) prefix-maxima [BJK + 91]; there, this prefix-maxima algorithm is also the most time consuming step in an algorithm for routing around a rectangle a VLSI routing problem ; and (3) for preprocessing a rooted tree, so that any level-ancestor query can be processed in constant-time <ref> [BV91b] </ref>.
Reference: [CDR86] <author> S.A. Cook, C. Dwork, and R. Reischuk. </author> <title> Upper and lower time bounds for parallel random access machines without simultaneous writes. </title> <journal> SIAM J. Comput., </journal> <volume> 15 </volume> <pages> 87-97, </pages> <year> 1986. </year>
Reference-contexts: (1) (log n= log log n) time using a polynomial number of processors for the parity problem [BH87]; (2) for finding the maximum among n elements [Val75], and merging [BH85] on a parallel comparison model of computation; and (3) for CREW PRAM computation of the OR function of n bits <ref> [CDR86] </ref>. As explained elsewhere (e.g., [KR88a], [KRS88] or [Vis83]), the PRAM should be viewed as a virtual design-space for a parallel machine and not as a parallel machine, and improvement in the parallel running time of a PRAM algorithm can benefit us in reducing the actual running time.
Reference: [CFL83] <author> A.K. Chandra, S. Fortune, and R.J. Lipton. </author> <title> Unbounded fan-in circuits and associative functions. </title> <booktitle> In Proc. of the 15th Ann. ACM Symp. on Theory of Computing, </booktitle> <pages> pages 52-60, </pages> <year> 1983. </year>
Reference-contexts: Optimal inverse-Ackermann time (i.e., O (ff (n)) time , where ff is the inverse-Ackermann extremely slow growing function) parallel algorithms actually use the surplus-log approach in a methodological way, overviewed below. Benefiting from a construction on unbounded fan-in circuits in <ref> [CFL83] </ref>, the inverse-Ackermann paradigm [BV89] works by designing a series of algorithms; the first in the series should run in O (1) time using n log n processors; then, in a certain way, slight increase in time implies significant decrease in the number of processors.
Reference: [Cha90] <author> S. Chaudhuri. </author> <title> Tight bounds for the chaining problem. </title> <type> preprint, </type> <month> December, </month> <year> 1990. </year>
Reference-contexts: Inverse-Ackermann time for chaining is best possible in an "oblivious" model of parallel computation, even with n processors <ref> [Cha90] </ref>.
Reference: [Col88] <author> R. Cole. </author> <title> Parallel merge sort. </title> <journal> SIAM J. Computing, </journal> <volume> 17(4) </volume> <pages> 770-785, </pages> <year> 1988. </year>
Reference-contexts: Several parallel deterministic and randomized algorithms, that run in time proportional to log n= log log n ("logarithmic level") or slower, were given for sorting [AKS83], [Bat68], [BN89], <ref> [Col88] </ref>, [Hir78] [Pre78], [RV87], and [SV81], and integer sorting [BDH + 89], [Hag87], [Hag91a], [MV90], [MV91], [RR89a], [Ram90] and [Ram91].
Reference: [Coo81] <author> S.A. Cook. </author> <title> Towards a complexity theory of synchronous parallel computation. </title> <journal> Ensign. Math., </journal> <volume> 27 </volume> <pages> 99-124, </pages> <year> 1981. </year>
Reference-contexts: Preference was given to domains of parallel algorithms where more structure, in a sense that is explained later, was found. Omitted is a review of general NC algorithms and the wealth of fundamental results they offer (e.g., for more on this work see <ref> [Coo81] </ref>, [Coo85] and [KR88a]) It was impossible to give a self-contained presentation within the space limitations. An introduction to parallelism, PRAMs, and PRAM algorithms is followed by a review of list, fl Partially supported by NSF grant CCR-8906949. tree and graph algorithms, most of which are not very recent. <p> The survey paper [Vis83] elaborates on the raison d'etre of the "PRAM approach". Survey papers specializing on the class NC are <ref> [Coo81] </ref> and [Coo85]. More recent review articles include [EG88], [KR88a], and [KRS88], as well as [Ata90b], which is devoted to parallel computational geometry. Books on the topic include [Akl89], [GR88], [JaJ91], [Par87] and [Rei91].
Reference: [Coo85] <author> S.A. Cook. </author> <title> A taxonomy of problems with fast parallel algorithms. </title> <journal> Information and Control, </journal> <volume> 64 </volume> <pages> 2-22, </pages> <year> 1985. </year>
Reference-contexts: Preference was given to domains of parallel algorithms where more structure, in a sense that is explained later, was found. Omitted is a review of general NC algorithms and the wealth of fundamental results they offer (e.g., for more on this work see [Coo81], <ref> [Coo85] </ref> and [KR88a]) It was impossible to give a self-contained presentation within the space limitations. An introduction to parallelism, PRAMs, and PRAM algorithms is followed by a review of list, fl Partially supported by NSF grant CCR-8906949. tree and graph algorithms, most of which are not very recent. <p> The survey paper [Vis83] elaborates on the raison d'etre of the "PRAM approach". Survey papers specializing on the class NC are [Coo81] and <ref> [Coo85] </ref>. More recent review articles include [EG88], [KR88a], and [KRS88], as well as [Ata90b], which is devoted to parallel computational geometry. Books on the topic include [Akl89], [GR88], [JaJ91], [Par87] and [Rei91].
Reference: [CT91] <author> J. Cheriyan and R. Thurimella. </author> <title> Algorithms for parallel k-vertex connectivity and sparse certifi cates. </title> <booktitle> In Proc. of the 23rd Ann. ACM Symp. on Theory of Computing, </booktitle> <year> 1991. </year>
Reference-contexts: We also note some recent parallel algorithms for k (edge and vertex) connectivity problems [KS89] and <ref> [CT91] </ref>.
Reference: [CV86a] <author> R. Cole and U. Vishkin. </author> <title> Approximate and exact parallel scheduling with applications to list, tree and graph problems. </title> <booktitle> In Proc. of the 27th IEEE Annual Symp. on Foundation of Computer Science, </booktitle> <pages> pages 478-491, </pages> <year> 1986. </year>
Reference-contexts: In fact, obtaining optimal algorithms for list ranking and (undirected) graph connectivity proved to be central to obtaining optimal algorithms for a considerable number of list, tree and graph problems. First randomized, and later deterministic, optimal parallel algorithms for list ranking were given [Vis84b], [CV86b], <ref> [CV86a] </ref>, [AM88] and [CV89]. The deterministic algorithms are based on a deterministic arbitration technique, dubbed deterministic coin tossing [CV86b]. Extensions of this technique for sparse graphs and other applications were given [GPS87], [CZ90], and [HCD87]. <p> Accelerating centroid decomposition was the motivation for the tree contraction version of [CV88]. Two logarithmic time connectivity algorithms were given: (1) a deterministic one which is optimal on all except very sparse graphs <ref> [CV86a] </ref>; (2) a randomized optimal one [Gaz86]. For problem, dubbed duration unknown task scheduling, and the Euler tour technique, as well as ideas from two previous connectivity algorithms [HCS79] and [SV82a].
Reference: [CV86b] <author> R. Cole and U. Vishkin. </author> <title> Deterministic coin tossing with applications to optimal parallel list ranking. </title> <journal> Information and Control, </journal> <volume> 70 </volume> <pages> 32-53, </pages> <year> 1986. </year>
Reference-contexts: In fact, obtaining optimal algorithms for list ranking and (undirected) graph connectivity proved to be central to obtaining optimal algorithms for a considerable number of list, tree and graph problems. First randomized, and later deterministic, optimal parallel algorithms for list ranking were given [Vis84b], <ref> [CV86b] </ref>, [CV86a], [AM88] and [CV89]. The deterministic algorithms are based on a deterministic arbitration technique, dubbed deterministic coin tossing [CV86b]. Extensions of this technique for sparse graphs and other applications were given [GPS87], [CZ90], and [HCD87]. <p> First randomized, and later deterministic, optimal parallel algorithms for list ranking were given [Vis84b], <ref> [CV86b] </ref>, [CV86a], [AM88] and [CV89]. The deterministic algorithms are based on a deterministic arbitration technique, dubbed deterministic coin tossing [CV86b]. Extensions of this technique for sparse graphs and other applications were given [GPS87], [CZ90], and [HCD87].
Reference: [CV88] <author> R. Cole and U. Vishkin. </author> <title> The accelerated centroid decomposition technique for optimal parallel tree evaluation in logarithmic time. </title> <journal> Algorithmica, </journal> <volume> 3 </volume> <pages> 329-348, </pages> <year> 1988. </year>
Reference-contexts: on trees are reviewed next: (1) The Euler tour technique [TV85] reduces the computation of many tree problems to list ranking. (2) The tree contraction technique [MR85] led to a number of optimal randomized logarithmic-time algorithms for tree problems, including expression tree evaluation; optimal deterministic versions were also given [GR86], <ref> [CV88] </ref>, [ADKP87] and [KD88]. Implicit use of tree contraction in a non-standard parallel algorithmic setting appeared in [Bre74]. (3) Centroid decomposition of a tree, as implicitly used in [Win75] for O (log 2 n) time computations. Accelerating centroid decomposition was the motivation for the tree contraction version of [CV88]. <p> given [GR86], <ref> [CV88] </ref>, [ADKP87] and [KD88]. Implicit use of tree contraction in a non-standard parallel algorithmic setting appeared in [Bre74]. (3) Centroid decomposition of a tree, as implicitly used in [Win75] for O (log 2 n) time computations. Accelerating centroid decomposition was the motivation for the tree contraction version of [CV88]. Two logarithmic time connectivity algorithms were given: (1) a deterministic one which is optimal on all except very sparse graphs [CV86a]; (2) a randomized optimal one [Gaz86].
Reference: [CV89] <author> R. Cole and U. Vishkin. </author> <title> Faster optimal parallel prefix sums and list ranking. </title> <journal> Information and Computation, </journal> <volume> 81 </volume> <pages> 334-352, </pages> <year> 1989. </year>
Reference-contexts: The fact that the prefix-sums problem appears at the bottom of Figure 1 is meant to convey the basic role of this problem. A faster CRCW algorithm for prefix-sums also exists <ref> [CV89] </ref>. A generalization of this problem to pointer structures, the list ranking problem, was identified in [Wyl79]; list ranking has proven to be a key subroutine in parallel algorithms. <p> In fact, obtaining optimal algorithms for list ranking and (undirected) graph connectivity proved to be central to obtaining optimal algorithms for a considerable number of list, tree and graph problems. First randomized, and later deterministic, optimal parallel algorithms for list ranking were given [Vis84b], [CV86b], [CV86a], [AM88] and <ref> [CV89] </ref>. The deterministic algorithms are based on a deterministic arbitration technique, dubbed deterministic coin tossing [CV86b]. Extensions of this technique for sparse graphs and other applications were given [GPS87], [CZ90], and [HCD87].
Reference: [CZ90] <author> R. Cole and O. Zajicek. </author> <title> An optimal parallel algorithm for building a data structure for planar point location. </title> <journal> J. Parallel and Distributed Computing, </journal> <volume> 8 </volume> <pages> 280-285, </pages> <year> 1990. </year>
Reference-contexts: First randomized, and later deterministic, optimal parallel algorithms for list ranking were given [Vis84b], [CV86b], [CV86a], [AM88] and [CV89]. The deterministic algorithms are based on a deterministic arbitration technique, dubbed deterministic coin tossing [CV86b]. Extensions of this technique for sparse graphs and other applications were given [GPS87], <ref> [CZ90] </ref>, and [HCD87].
Reference: [DM89] <editor> M. Dietzfelbinger and F. Meyer auf der Heide. </editor> <title> An optimal parallel dictionary. </title> <booktitle> In Proc. 1st ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 360-368, </pages> <year> 1989. </year>
Reference-contexts: Load balancing and hashing methods, including the ones in [GM91], led to a doubly-logarithmic level "dictionary" extension of hashing, where, insertion and deletion queries are also supported [GMV90]; an algorithm in <ref> [DM89] </ref> 11 solves the dictionary problem with running time of the form O (n * ). Routines for the prefix-sums problem play a major role in parallel algorithms, as indicated in Section 4.
Reference: [EG88] <author> D. Eppstein and Z. Galil. </author> <title> Parallel algorithmic techniques for combinatorial computation. </title> <journal> Ann. Rev. Comput. Sci., </journal> <volume> 3 </volume> <pages> 233-283, </pages> <year> 1988. </year>
Reference-contexts: The survey paper [Vis83] elaborates on the raison d'etre of the "PRAM approach". Survey papers specializing on the class NC are [Coo81] and [Coo85]. More recent review articles include <ref> [EG88] </ref>, [KR88a], and [KRS88], as well as [Ata90b], which is devoted to parallel computational geometry. Books on the topic include [Akl89], [GR88], [JaJ91], [Par87] and [Rei91]. <p> Most of this work was done between 1980 and 1988. We highlight structure-related issues of this work, primarily for background; an elaborate overview of most of this material can be found in <ref> [EG88] </ref> and [KR88a]. The algorithms for most "target problems" in this figure (these are the more involved and known problems; usually, they are at the top or slightly below the top of the figure) run in logarithmic time.
Reference: [FRT89] <author> D. Fussell, V.L. Ramachandran, and R. Thurimella. </author> <title> Finding triconnected components by local replacements. </title> <booktitle> In Proc. of 16th ICALP, </booktitle> <publisher> Springer LNCS 372, </publisher> <pages> pages 379-393, </pages> <year> 1989. </year>
Reference-contexts: More powerful applications were for finding an st-numbering of a graph, again in [MSV86], as well as for triconnectivity algorithms [MR87] and [RV88]. An st-numbering is used in the planarity testing algorithm of [KR88b]. The 7 most recent algorithms for triconnectivity <ref> [FRT89] </ref> and planarity testing [RR89b], are very nice examples of reaching target problems by building an even higher level in the structure of Figure 1, and using effectively many of the previous techniques. 5 Deterministic Fast Algorithms Structure that was found in optimal doubly-logarithmic time (or faster), parallel algorithms is highlighted.
Reference: [FW78] <author> S. Fortune and J. Wyllie. </author> <title> Parallelism in random access machines. </title> <booktitle> In Proceedings of the 10th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 114-118, </pages> <year> 1978. </year>
Reference-contexts: The PRAM was first proposed as a model for parallel computation in a joint complexity theoretic and algorithmic context in a 1979 thesis [Wyl79] and in a paper <ref> [FW78] </ref>; this original instance concerned the CREW PRAM model. [Sch80b] also advocated using a PRAM for studying the limits of parallel computation at around the same chronological time. [Gol82] was the first to propose the CRCW PRAM model in a complexity theoretic context (he called it a SIMDAG). [Pip79] identified and
Reference: [Gal85] <author> Z. Galil. </author> <title> Optimal parallel algorithms for string matching. </title> <journal> Information and Control, </journal> <volume> 67 </volume> <pages> 144-157, </pages> <year> 1985. </year>
Reference-contexts: Note that all merging algorithms that are mentioned in this paper may be implemented on a CREW PRAM. String matching: For some family of parallel algorithms it is sufficient to consider only non-periodic patterns <ref> [Gal85] </ref>.
Reference: [Gaz86] <author> H. Gazit. </author> <title> An optimal randomized parallel algorithm for finding connected components in a graph. </title> <booktitle> In Proc. of the 27th IEEE Annual Symp. on Foundation of Computer Science, </booktitle> <pages> pages 492-501, </pages> <year> 1986. </year>
Reference-contexts: Accelerating centroid decomposition was the motivation for the tree contraction version of [CV88]. Two logarithmic time connectivity algorithms were given: (1) a deterministic one which is optimal on all except very sparse graphs [CV86a]; (2) a randomized optimal one <ref> [Gaz86] </ref>. For problem, dubbed duration unknown task scheduling, and the Euler tour technique, as well as ideas from two previous connectivity algorithms [HCS79] and [SV82a]. <p> [SV88]. 6 Randomized Fast Algorithms Randomization has shown to be very useful for both the simulation of PRAM-like shared memory models of parallel computation by other models of parallel machines (e.g., in [KU86], [KRS88], [MV84], [Ran87], [KPS90] and [MSP90]), and for the design of parallel algorithms (e.g., in [ABI86], [AM90], <ref> [Gaz86] </ref>, [GM91], [KR87], [Lub86], [MR85], [MV90], [MV91], All "target algorithms" in this section are randomized, and their running time is at the doubly-logarithmic level, or faster. By the doubly-logarithmic level, we mean O (f (n) log log n) where the function f (n) is o (log log n).
Reference: [Gil90] <author> J. Gil. </author> <title> Fast load balancing on PRAM. Preliminary report; see also: Lower Bounds and Algorithms for Hashing and Parallel Processing, </title> <type> Ph.D. Thesis, </type> <institution> Hebrew University, Jerusalem, Israel, </institution> <year> 1990. </year>
Reference-contexts: Load balancing can be achieved by a simple application of a prefix-sums algorithm (e.g., [Vis84b]), with a logarithmic-level time overhead. A family of load balancing algorithms are treated in <ref> [Gil90] </ref>, with a doubly-logarithmic multiplicative overhead; [MV91] treats a more specific family, with log-star level additive overhead, using the LAC algorithm.
Reference: [GM91] <author> J. Gil and Y. Matias. </author> <title> Fast hashing on a PRAM. </title> <booktitle> In Proc. of the 2nd Second ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 271-280, </pages> <year> 1991. </year>
Reference-contexts: 6 Randomized Fast Algorithms Randomization has shown to be very useful for both the simulation of PRAM-like shared memory models of parallel computation by other models of parallel machines (e.g., in [KU86], [KRS88], [MV84], [Ran87], [KPS90] and [MSP90]), and for the design of parallel algorithms (e.g., in [ABI86], [AM90], [Gaz86], <ref> [GM91] </ref>, [KR87], [Lub86], [MR85], [MV90], [MV91], All "target algorithms" in this section are randomized, and their running time is at the doubly-logarithmic level, or faster. By the doubly-logarithmic level, we mean O (f (n) log log n) where the function f (n) is o (log log n). <p> This paradigm has been extended, within the same performance bounds, to cope with the more general and well-investigated problem of hashing: given a set of n input elements, build a linear size table that supports membership queries in constant-time. Logarithmic level hashing [MV90], and doubly-logarithmic level hashing <ref> [GM91] </ref> preceded this result. Some log-star level ideas for a non-standard algorithmic model, where cost of counting, as well as assignment of processors to jobs, are ignored were given in [GMW90]. <p> A family of load balancing algorithms are treated in [Gil90], with a doubly-logarithmic multiplicative overhead; [MV91] treats a more specific family, with log-star level additive overhead, using the LAC algorithm. Load balancing and hashing methods, including the ones in <ref> [GM91] </ref>, led to a doubly-logarithmic level "dictionary" extension of hashing, where, insertion and deletion queries are also supported [GMV90]; an algorithm in [DM89] 11 solves the dictionary problem with running time of the form O (n * ).
Reference: [GMV90] <author> Y. Gil, Y. Matias, and U. Vishkin. </author> <title> A fast parallel dictionary. </title> <note> In preparation, </note> <year> 1990. </year>
Reference-contexts: Load balancing and hashing methods, including the ones in [GM91], led to a doubly-logarithmic level "dictionary" extension of hashing, where, insertion and deletion queries are also supported <ref> [GMV90] </ref>; an algorithm in [DM89] 11 solves the dictionary problem with running time of the form O (n * ). Routines for the prefix-sums problem play a major role in parallel algorithms, as indicated in Section 4.
Reference: [GMW90] <author> Y. Gil, F. Meyer auf der Heide, and A. Wigderson. </author> <title> Not all keys can be hashed in constant time. </title> <booktitle> In Proc. of the 22nd Ann. ACM Symp. on Theory of Computing, </booktitle> <pages> pages 244-253, </pages> <year> 1990. </year>
Reference-contexts: Logarithmic level hashing [MV90], and doubly-logarithmic level hashing [GM91] preceded this result. Some log-star level ideas for a non-standard algorithmic model, where cost of counting, as well as assignment of processors to jobs, are ignored were given in <ref> [GMW90] </ref>. An (log fl n) time lower-bound using n processors is also given in [GMW90]; the lower bound is for a model of computation that admits the log-star level algorithm. <p> Some log-star level ideas for a non-standard algorithmic model, where cost of counting, as well as assignment of processors to jobs, are ignored were given in <ref> [GMW90] </ref>. An (log fl n) time lower-bound using n processors is also given in [GMW90]; the lower bound is for a model of computation that admits the log-star level algorithm.
Reference: [Gol82] <author> L.M. Goldschlager. </author> <title> A universal interconnection pattern for parallel computers. </title> <journal> J. Assoc. Comput. Mach., </journal> <volume> 29 </volume> <pages> 1073-1086, </pages> <year> 1982. </year>
Reference-contexts: a model for parallel computation in a joint complexity theoretic and algorithmic context in a 1979 thesis [Wyl79] and in a paper [FW78]; this original instance concerned the CREW PRAM model. [Sch80b] also advocated using a PRAM for studying the limits of parallel computation at around the same chronological time. <ref> [Gol82] </ref> was the first to propose the CRCW PRAM model in a complexity theoretic context (he called it a SIMDAG). [Pip79] identified and characterized the class NC. [SV81] suggested using the CRCW PRAM in an algorithmic context. Figures 1-3 are a focal point for this short tutorial paper.
Reference: [GPS87] <author> A. Goldberg, S. Plotkin, and G. Shannon. </author> <title> Parallel symmetry-breaking in sparse graphs. </title> <booktitle> In Proceedings 19th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 315-324, </pages> <year> 1987. </year>
Reference-contexts: First randomized, and later deterministic, optimal parallel algorithms for list ranking were given [Vis84b], [CV86b], [CV86a], [AM88] and [CV89]. The deterministic algorithms are based on a deterministic arbitration technique, dubbed deterministic coin tossing [CV86b]. Extensions of this technique for sparse graphs and other applications were given <ref> [GPS87] </ref>, [CZ90], and [HCD87].
Reference: [GR86] <author> A. Gibbons and W. Rytter. </author> <title> An optimal parallel algorithm for dynamic evaluation and its ap plications. </title> <booktitle> In Proceedings of the sixth Conference on Foundations of Software Technology and Theoretical Computer Science, Lecture Notes in Computer Science 241, </booktitle> <pages> pages 453-469. </pages> <publisher> Springer-Verlag, </publisher> <year> 1986. </year>
Reference-contexts: algorithms on trees are reviewed next: (1) The Euler tour technique [TV85] reduces the computation of many tree problems to list ranking. (2) The tree contraction technique [MR85] led to a number of optimal randomized logarithmic-time algorithms for tree problems, including expression tree evaluation; optimal deterministic versions were also given <ref> [GR86] </ref>, [CV88], [ADKP87] and [KD88]. Implicit use of tree contraction in a non-standard parallel algorithmic setting appeared in [Bre74]. (3) Centroid decomposition of a tree, as implicitly used in [Win75] for O (log 2 n) time computations. Accelerating centroid decomposition was the motivation for the tree contraction version of [CV88].
Reference: [GR88] <author> A. Gibbons and W. Rytter. </author> <title> Efficient Parallel Algorithms. </title> <publisher> Cambridge University Press, </publisher> <address> Cam bridge, </address> <year> 1988. </year>
Reference-contexts: Survey papers specializing on the class NC are [Coo81] and [Coo85]. More recent review articles include [EG88], [KR88a], and [KRS88], as well as [Ata90b], which is devoted to parallel computational geometry. Books on the topic include [Akl89], <ref> [GR88] </ref>, [JaJ91], [Par87] and [Rei91]. For sequential computation, it has been of considerable advantage to deal with an abstraction of the von-Neumann machine, namely the RAM or Random Access Machine (see a standard textbook, such as [AHU74]).
Reference: [Hag87] <author> T. Hagerup. </author> <title> Towards optimal parallel bucket sorting. </title> <journal> Information and Computation, </journal> <volume> 75 </volume> <pages> 39-51, </pages> <year> 1987. </year>
Reference-contexts: Several parallel deterministic and randomized algorithms, that run in time proportional to log n= log log n ("logarithmic level") or slower, were given for sorting [AKS83], [Bat68], [BN89], [Col88], [Hir78] [Pre78], [RV87], and [SV81], and integer sorting [BDH + 89], <ref> [Hag87] </ref>, [Hag91a], [MV90], [MV91], [RR89a], [Ram90] and [Ram91].
Reference: [Hag91a] <author> T. Hagerup. </author> <title> Constant-time parallel integer sorting. </title> <booktitle> In Proc. of the 23rd Ann. ACM Symp. on Theory of Computing, </booktitle> <year> 1991. </year>
Reference-contexts: Several parallel deterministic and randomized algorithms, that run in time proportional to log n= log log n ("logarithmic level") or slower, were given for sorting [AKS83], [Bat68], [BN89], [Col88], [Hir78] [Pre78], [RV87], and [SV81], and integer sorting [BDH + 89], [Hag87], <ref> [Hag91a] </ref>, [MV90], [MV91], [RR89a], [Ram90] and [Ram91]. The lower-bound in [BH87] implies that faster algorithms are possible only by relaxing the definition of the problem: (1) [MS91] gave a doubly-logarithmic level result, assuming the input comes from a certain random source; the output is given in a "padded" representation; (2) [Hag91a] <p> <ref> [Hag91a] </ref>, [MV90], [MV91], [RR89a], [Ram90] and [Ram91]. The lower-bound in [BH87] implies that faster algorithms are possible only by relaxing the definition of the problem: (1) [MS91] gave a doubly-logarithmic level result, assuming the input comes from a certain random source; the output is given in a "padded" representation; (2) [Hag91a] allows general integer inputs from the range [1::n]; the output is given in a linked list which is sorted in a non-decreasing order. We proceed to Figure 3, the main structure in this section.
Reference: [Hag91b] <author> T. Hagerup. </author> <title> Fast parallel generation of random permutations. </title> <booktitle> In Proc. of 18th ICALP, </booktitle> <year> 1991. </year>
Reference-contexts: Using the log-star-time deterministic algorithms for the nearest-one and prefix-maxima problems, mentioned earlier, as well as the LAC algorithm, an optimal log-star level for generating a random permutation was given. Other methods for this problem are at the logarithmic level [MR85] and [RR89a]; <ref> [Hag91b] </ref> gives a doubly-logarithmic level algorithm that produces random permutations in a non-standard representation. The LAC algorithm required a new algorithmic paradigm.
Reference: [HCD87] <author> T. Hagerup, M. Chrobak, and K. Diks. </author> <title> Parallel 5-coloring of planar graphs. </title> <booktitle> In Proc. of 14th ICALP, </booktitle> <pages> pages 304-313, </pages> <year> 1987. </year>
Reference-contexts: First randomized, and later deterministic, optimal parallel algorithms for list ranking were given [Vis84b], [CV86b], [CV86a], [AM88] and [CV89]. The deterministic algorithms are based on a deterministic arbitration technique, dubbed deterministic coin tossing [CV86b]. Extensions of this technique for sparse graphs and other applications were given [GPS87], [CZ90], and <ref> [HCD87] </ref>.
Reference: [HCS79] <author> D.S. Hirschberg, A.K. Chandra, and D.V. Sarwate. </author> <title> Computing connected components on parallel computers. </title> <journal> Comm. ACM, </journal> <volume> 22,8:461-464, </volume> <year> 1979. </year>
Reference-contexts: Two logarithmic time connectivity algorithms were given: (1) a deterministic one which is optimal on all except very sparse graphs [CV86a]; (2) a randomized optimal one [Gaz86]. For problem, dubbed duration unknown task scheduling, and the Euler tour technique, as well as ideas from two previous connectivity algorithms <ref> [HCS79] </ref> and [SV82a]. It should be pointed out that the logarithmic time version of the deterministic connectivity algorithm requires the use of expander graphs and thus is highly impractical at present; however, a slightly less parallel version involves much smaller constants.
Reference: [Hir78] <author> D. S. Hirschberg. </author> <title> Fast parallel sorting algorithms. </title> <journal> Comm. ACM, </journal> <volume> 21 </volume> <pages> 657-661, </pages> <year> 1978. </year>
Reference-contexts: Several parallel deterministic and randomized algorithms, that run in time proportional to log n= log log n ("logarithmic level") or slower, were given for sorting [AKS83], [Bat68], [BN89], [Col88], <ref> [Hir78] </ref> [Pre78], [RV87], and [SV81], and integer sorting [BDH + 89], [Hag87], [Hag91a], [MV90], [MV91], [RR89a], [Ram90] and [Ram91].
Reference: [HT84] <author> D. Harel and R.E. Tarjan. </author> <title> Fast algorithms for finding nearest common ancestors. </title> <journal> SIAM J. Comput., </journal> <volume> 13(2) </volume> <pages> 338-355, </pages> <month> May </month> <year> 1984. </year>
Reference-contexts: a rooted tree so that a query requesting the lowest-common-ancestor (LCA) of any pair of vertices can be quickly processed; results are similar to the ones for range-maxima, assuming that the Euler tour of the tree is given; the algorithm is new, and interestingly also simpler than previous LCA algorithms <ref> [HT84] </ref> and [SV88]. 6 Randomized Fast Algorithms Randomization has shown to be very useful for both the simulation of PRAM-like shared memory models of parallel computation by other models of parallel machines (e.g., in [KU86], [KRS88], [MV84], [Ran87], [KPS90] and [MSP90]), and for the design of parallel algorithms (e.g., in [ABI86],
Reference: [JaJ91] <author> J. JaJa. </author> <title> Introduction to Parallel Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1991. </year>
Reference-contexts: Survey papers specializing on the class NC are [Coo81] and [Coo85]. More recent review articles include [EG88], [KR88a], and [KRS88], as well as [Ata90b], which is devoted to parallel computational geometry. Books on the topic include [Akl89], [GR88], <ref> [JaJ91] </ref>, [Par87] and [Rei91]. For sequential computation, it has been of considerable advantage to deal with an abstraction of the von-Neumann machine, namely the RAM or Random Access Machine (see a standard textbook, such as [AHU74]). <p> to [Bre74]; the problem, however, is that the theorem holds only for a non-standard model of parallel computation, where assignment of processors to jobs can be done free of charge; the methodology was first used for the design of a PRAM algorithm in [SV82b], and is elucidated in [Vis90] and <ref> [JaJ91] </ref>, who call it the work-time framework; typical applications of this methodology solve the processor assignment problem in an ad-hoc manner; however, sometimes proper processor assignment can be achieved using general methods for balancing loads among processors.
Reference: [KD88] <author> S.R. Kosaraju and A.L. Delcher. </author> <title> Optimal parallel evaluation of tree-structured computations by ranking. </title> <booktitle> In Proc. of AWOC 88, Lecture Notes in Computer Science No. </booktitle> <volume> 319, </volume> <pages> pages 101-110. </pages> <publisher> Springer-Verlag, </publisher> <year> 1988. </year>
Reference-contexts: reviewed next: (1) The Euler tour technique [TV85] reduces the computation of many tree problems to list ranking. (2) The tree contraction technique [MR85] led to a number of optimal randomized logarithmic-time algorithms for tree problems, including expression tree evaluation; optimal deterministic versions were also given [GR86], [CV88], [ADKP87] and <ref> [KD88] </ref>. Implicit use of tree contraction in a non-standard parallel algorithmic setting appeared in [Bre74]. (3) Centroid decomposition of a tree, as implicitly used in [Win75] for O (log 2 n) time computations. Accelerating centroid decomposition was the motivation for the tree contraction version of [CV88].
Reference: [KM68] <author> R.M. Karp and W.L. Miranker. </author> <title> Parallel minimax search for a maximum. </title> <journal> J. of Combinatorial Theory, </journal> <volume> 4 </volume> <pages> 19-34, </pages> <year> 1968. </year>
Reference-contexts: We note that it had not been clear that the design of efficient parallel algorithms for "enough" problems is at all possible. Specifically, I recall a discussion with a colleague in 1979. In a thought-off support of a skeptical position, he quoted <ref> [KM68] </ref>, who proved that parallelism will be rather ineffective in the context of binary search; informally, they show that an increase in the number of processors from one to p may cut the time of binary search by a factor of at most log p.
Reference: [KPS90] <author> Z.M. Kedem, K.V. Palem, and P.G. Spirakis. </author> <title> Efficient robust parallel computations. </title> <booktitle> In Proc. of the 22nd Ann. ACM Symp. on Theory of Computing, </booktitle> <pages> pages 138-148, </pages> <year> 1990. </year>
Reference-contexts: the algorithm is new, and interestingly also simpler than previous LCA algorithms [HT84] and [SV88]. 6 Randomized Fast Algorithms Randomization has shown to be very useful for both the simulation of PRAM-like shared memory models of parallel computation by other models of parallel machines (e.g., in [KU86], [KRS88], [MV84], [Ran87], <ref> [KPS90] </ref> and [MSP90]), and for the design of parallel algorithms (e.g., in [ABI86], [AM90], [Gaz86], [GM91], [KR87], [Lub86], [MR85], [MV90], [MV91], All "target algorithms" in this section are randomized, and their running time is at the doubly-logarithmic level, or faster.
Reference: [KR87] <author> R.M. Karp and M.O. Rabin. </author> <title> Efficient randomized pattern-matching algorithms. </title> <journal> IBM J. of Research and Development, </journal> <volume> 31 </volume> <pages> 249-260, </pages> <year> 1987. </year> <month> 15 </month>
Reference-contexts: Randomized Fast Algorithms Randomization has shown to be very useful for both the simulation of PRAM-like shared memory models of parallel computation by other models of parallel machines (e.g., in [KU86], [KRS88], [MV84], [Ran87], [KPS90] and [MSP90]), and for the design of parallel algorithms (e.g., in [ABI86], [AM90], [Gaz86], [GM91], <ref> [KR87] </ref>, [Lub86], [MR85], [MV90], [MV91], All "target algorithms" in this section are randomized, and their running time is at the doubly-logarithmic level, or faster. By the doubly-logarithmic level, we mean O (f (n) log log n) where the function f (n) is o (log log n).
Reference: [KR88a] <author> R.M. Karp and V. Ramachandran. </author> <title> A survey of parallel algorithms for shared-memory machines. </title> <type> Technical Report UCB/CSD 88/408, </type> <institution> Computer Science Division (EECS) U. </institution> <address> C. Berkeley, </address> <year> 1988. </year> <note> also, in Handbook of Theoretical Computer Science, North-Holland, to appear. </note>
Reference-contexts: Preference was given to domains of parallel algorithms where more structure, in a sense that is explained later, was found. Omitted is a review of general NC algorithms and the wealth of fundamental results they offer (e.g., for more on this work see [Coo81], [Coo85] and <ref> [KR88a] </ref>) It was impossible to give a self-contained presentation within the space limitations. An introduction to parallelism, PRAMs, and PRAM algorithms is followed by a review of list, fl Partially supported by NSF grant CCR-8906949. tree and graph algorithms, most of which are not very recent. <p> The survey paper [Vis83] elaborates on the raison d'etre of the "PRAM approach". Survey papers specializing on the class NC are [Coo81] and [Coo85]. More recent review articles include [EG88], <ref> [KR88a] </ref>, and [KRS88], as well as [Ata90b], which is devoted to parallel computational geometry. Books on the topic include [Akl89], [GR88], [JaJ91], [Par87] and [Rei91]. <p> As explained elsewhere (e.g., <ref> [KR88a] </ref>, [KRS88] or [Vis83]), the PRAM should be viewed as a virtual design-space for a parallel machine and not as a parallel machine, and improvement in the parallel running time of a PRAM algorithm can benefit us in reducing the actual running time. <p> Most of this work was done between 1980 and 1988. We highlight structure-related issues of this work, primarily for background; an elaborate overview of most of this material can be found in [EG88] and <ref> [KR88a] </ref>. The algorithms for most "target problems" in this figure (these are the more involved and known problems; usually, they are at the top or slightly below the top of the figure) run in logarithmic time.
Reference: [KR88b] <author> P. Klein and J.H. Reif. </author> <title> An efficient parallel algorithm for planarity. </title> <journal> J. Comp. Sys. Sci., </journal> <volume> 37, </volume> <year> 1988. </year>
Reference-contexts: The EDS method implies alternative algorithms for biconnectivity and strong orientation. More powerful applications were for finding an st-numbering of a graph, again in [MSV86], as well as for triconnectivity algorithms [MR87] and [RV88]. An st-numbering is used in the planarity testing algorithm of <ref> [KR88b] </ref>.
Reference: [KRS88] <author> C.P. Kruskal, L. Rudolph, and M. Snir. </author> <title> A complexity theory of efficient parallel algorithms. </title> <booktitle> In Proc. of 15th ICALP, </booktitle> <publisher> Springer LNCS 317, </publisher> <pages> pages 333-346, </pages> <year> 1988. </year>
Reference-contexts: The survey paper [Vis83] elaborates on the raison d'etre of the "PRAM approach". Survey papers specializing on the class NC are [Coo81] and [Coo85]. More recent review articles include [EG88], [KR88a], and <ref> [KRS88] </ref>, as well as [Ata90b], which is devoted to parallel computational geometry. Books on the topic include [Akl89], [GR88], [JaJ91], [Par87] and [Rei91]. <p> As explained elsewhere (e.g., [KR88a], <ref> [KRS88] </ref> or [Vis83]), the PRAM should be viewed as a virtual design-space for a parallel machine and not as a parallel machine, and improvement in the parallel running time of a PRAM algorithm can benefit us in reducing the actual running time. <p> We suggest the following first step towards building such a knowledge-base: develop a core of problems that can be computed very fast, as well as very fast computational paradigms. Another line of additional justification follows <ref> [KRS88] </ref>, [Val90] and [Vis84a] that advocate slackness in processors. Let 4 us explain. Suppose we are given an efficient PRAM algorithm and a (real) parallel machine with p 1 processors, on which we wish to simulate the algorithm. <p> tree is given; the algorithm is new, and interestingly also simpler than previous LCA algorithms [HT84] and [SV88]. 6 Randomized Fast Algorithms Randomization has shown to be very useful for both the simulation of PRAM-like shared memory models of parallel computation by other models of parallel machines (e.g., in [KU86], <ref> [KRS88] </ref>, [MV84], [Ran87], [KPS90] and [MSP90]), and for the design of parallel algorithms (e.g., in [ABI86], [AM90], [Gaz86], [GM91], [KR87], [Lub86], [MR85], [MV90], [MV91], All "target algorithms" in this section are randomized, and their running time is at the doubly-logarithmic level, or faster.
Reference: [Kru83] <author> C.P. Kruskal. </author> <title> Searching, merging, and sorting in parallel computation. </title> <journal> IEEE Trans. on Comp, </journal> <volume> C-32:942-946, </volume> <year> 1983. </year>
Reference-contexts: Such structure guides the computation in optimal doubly-logarithmic parallel algorithms for finding the maximum among n elements [SV81] (using [Val75]), finding the maximum relative to all prefixes of an array of elements [Sch87] and [BSV88] (the prefix-maxima problem), merging two sorted lists <ref> [Kru83] </ref> and [BH85], finding the convex hull of a monotone polygon [BSV91], and finding all nearest neighbors in a convex polygon [SV90]. Note that all merging algorithms that are mentioned in this paper may be implemented on a CREW PRAM.
Reference: [KS89] <author> S. Khuller and B. Schieber. </author> <title> Efficient parallel algorithms for testing connectivity and finding disjoint s-t paths in graphs. </title> <booktitle> In Proc. of the 30th IEEE Annual Symp. on Foundation of Computer Science, </booktitle> <pages> pages 288-293, </pages> <year> 1989. </year>
Reference-contexts: We also note some recent parallel algorithms for k (edge and vertex) connectivity problems <ref> [KS89] </ref> and [CT91].
Reference: [KU86] <author> A. Karlin and E. Upfal. </author> <title> Parallel hashing an efficient implementation of shared memory. </title> <booktitle> In Proc. of the 18th Ann. ACM Symp. on Theory of Computing, </booktitle> <pages> pages 160-168, </pages> <year> 1986. </year>
Reference-contexts: the tree is given; the algorithm is new, and interestingly also simpler than previous LCA algorithms [HT84] and [SV88]. 6 Randomized Fast Algorithms Randomization has shown to be very useful for both the simulation of PRAM-like shared memory models of parallel computation by other models of parallel machines (e.g., in <ref> [KU86] </ref>, [KRS88], [MV84], [Ran87], [KPS90] and [MSP90]), and for the design of parallel algorithms (e.g., in [ABI86], [AM90], [Gaz86], [GM91], [KR87], [Lub86], [MR85], [MV90], [MV91], All "target algorithms" in this section are randomized, and their running time is at the doubly-logarithmic level, or faster.
Reference: [LF80] <author> R.E. Ladner and M.J. Fischer. </author> <title> Parallel prefix computation. </title> <journal> J. Assoc. Comput. Mach., </journal> <volume> 27:831 838, </volume> <year> 1980. </year>
Reference-contexts: was hard to anticipate the structure of Figure 1 beforehand, searching for a similar structure became one of the research goals for the later work. 4 List, Tree and Graph Algorithms A basic routine that is used most often in parallel algorithms is undoubtedly that for the prefix sums problem <ref> [LF80] </ref>. The fact that the prefix-sums problem appears at the bottom of Figure 1 is meant to convey the basic role of this problem. A faster CRCW algorithm for prefix-sums also exists [CV89].
Reference: [Lov85] <author> L. Lovasz. </author> <title> Computing ears and branching in parallel. </title> <booktitle> In Proc. of the 26th IEEE Annual Symp. on Foundation of Computer Science, </booktitle> <pages> pages 464-467, </pages> <year> 1985. </year>
Reference-contexts: Unfortunately, it is not known how to implement DFS efficiently in parallel. A technique called ear decomposition search (EDS) was suggested as a replacement for DFS in the context of efficient and fast parallel algorithms [MSV86] and [MR86], after an earlier suggestion in <ref> [Lov85] </ref> for computing EDS in parallel in a fast but inefficient manner. The EDS method implies alternative algorithms for biconnectivity and strong orientation. More powerful applications were for finding an st-numbering of a graph, again in [MSV86], as well as for triconnectivity algorithms [MR87] and [RV88].
Reference: [Lub86] <author> M. Luby. </author> <title> A simple parallel algorithm for the maximal independent set problem. </title> <journal> SIAM J. Comput., </journal> <volume> 15 </volume> <pages> 1036-1053, </pages> <year> 1986. </year>
Reference-contexts: Fast Algorithms Randomization has shown to be very useful for both the simulation of PRAM-like shared memory models of parallel computation by other models of parallel machines (e.g., in [KU86], [KRS88], [MV84], [Ran87], [KPS90] and [MSP90]), and for the design of parallel algorithms (e.g., in [ABI86], [AM90], [Gaz86], [GM91], [KR87], <ref> [Lub86] </ref>, [MR85], [MV90], [MV91], All "target algorithms" in this section are randomized, and their running time is at the doubly-logarithmic level, or faster. By the doubly-logarithmic level, we mean O (f (n) log log n) where the function f (n) is o (log log n).
Reference: [MR85] <author> G.L. Miller and J.H. Reif. </author> <title> Parallel tree contraction and its application. </title> <booktitle> In Proc. of the 26th IEEE Annual Symp. on Foundation of Computer Science, </booktitle> <pages> pages 478-489, </pages> <year> 1985. </year>
Reference-contexts: Extensions of this technique for sparse graphs and other applications were given [GPS87], [CZ90], and [HCD87]. Key techniques for parallel algorithms on trees are reviewed next: (1) The Euler tour technique [TV85] reduces the computation of many tree problems to list ranking. (2) The tree contraction technique <ref> [MR85] </ref> led to a number of optimal randomized logarithmic-time algorithms for tree problems, including expression tree evaluation; optimal deterministic versions were also given [GR86], [CV88], [ADKP87] and [KD88]. <p> Algorithms Randomization has shown to be very useful for both the simulation of PRAM-like shared memory models of parallel computation by other models of parallel machines (e.g., in [KU86], [KRS88], [MV84], [Ran87], [KPS90] and [MSP90]), and for the design of parallel algorithms (e.g., in [ABI86], [AM90], [Gaz86], [GM91], [KR87], [Lub86], <ref> [MR85] </ref>, [MV90], [MV91], All "target algorithms" in this section are randomized, and their running time is at the doubly-logarithmic level, or faster. By the doubly-logarithmic level, we mean O (f (n) log log n) where the function f (n) is o (log log n). <p> Using the log-star-time deterministic algorithms for the nearest-one and prefix-maxima problems, mentioned earlier, as well as the LAC algorithm, an optimal log-star level for generating a random permutation was given. Other methods for this problem are at the logarithmic level <ref> [MR85] </ref> and [RR89a]; [Hag91b] gives a doubly-logarithmic level algorithm that produces random permutations in a non-standard representation. The LAC algorithm required a new algorithmic paradigm.
Reference: [MR86] <author> G.L. Miller and V.L. Ramachandran. </author> <title> Efficient parallel ear decomposition and applications. </title> <type> unpublished manuscript, </type> <year> 1986. </year>
Reference-contexts: Unfortunately, it is not known how to implement DFS efficiently in parallel. A technique called ear decomposition search (EDS) was suggested as a replacement for DFS in the context of efficient and fast parallel algorithms [MSV86] and <ref> [MR86] </ref>, after an earlier suggestion in [Lov85] for computing EDS in parallel in a fast but inefficient manner. The EDS method implies alternative algorithms for biconnectivity and strong orientation.
Reference: [MR87] <author> G.L. Miller and V.L. Ramachandran. </author> <title> A new graph triconnectivity algorithm and its parallization. </title> <booktitle> In Proc. of the 19th Ann. ACM Symp. on Theory of Computing, </booktitle> <pages> pages 335-344, </pages> <year> 1987. </year>
Reference-contexts: The EDS method implies alternative algorithms for biconnectivity and strong orientation. More powerful applications were for finding an st-numbering of a graph, again in [MSV86], as well as for triconnectivity algorithms <ref> [MR87] </ref> and [RV88]. An st-numbering is used in the planarity testing algorithm of [KR88b].
Reference: [MS91] <author> P.D. MacKenzie and Q.F. Stout. </author> <title> Ultra-fast expected time parallel algorithms. </title> <booktitle> In Proc. of the 2nd Second ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 414-424, </pages> <year> 1991. </year>
Reference-contexts: The lower-bound in [BH87] implies that faster algorithms are possible only by relaxing the definition of the problem: (1) <ref> [MS91] </ref> gave a doubly-logarithmic level result, assuming the input comes from a certain random source; the output is given in a "padded" representation; (2) [Hag91a] allows general integer inputs from the range [1::n]; the output is given in a linked list which is sorted in a non-decreasing order.
Reference: [MSP90] <author> C. Martel, R. Subramonian, and A. Park. </author> <title> Asynchronous PRAMs are (almost) as good as syn chronous PRAMs. </title> <booktitle> In Proc. of the 31st IEEE Annual Symp. on Foundation of Computer Science, </booktitle> <pages> pages 590-599, </pages> <year> 1990. </year>
Reference-contexts: is new, and interestingly also simpler than previous LCA algorithms [HT84] and [SV88]. 6 Randomized Fast Algorithms Randomization has shown to be very useful for both the simulation of PRAM-like shared memory models of parallel computation by other models of parallel machines (e.g., in [KU86], [KRS88], [MV84], [Ran87], [KPS90] and <ref> [MSP90] </ref>), and for the design of parallel algorithms (e.g., in [ABI86], [AM90], [Gaz86], [GM91], [KR87], [Lub86], [MR85], [MV90], [MV91], All "target algorithms" in this section are randomized, and their running time is at the doubly-logarithmic level, or faster.
Reference: [MSV86] <author> Y. Maon, B. Schieber, and U. Vishkin. </author> <title> Parallel ear-decomposition search (EDS) and st-numbering in graphs. </title> <journal> Theoretical Computer Science, </journal> <volume> 47 </volume> <pages> 277-298, </pages> <year> 1986. </year>
Reference-contexts: Unfortunately, it is not known how to implement DFS efficiently in parallel. A technique called ear decomposition search (EDS) was suggested as a replacement for DFS in the context of efficient and fast parallel algorithms <ref> [MSV86] </ref> and [MR86], after an earlier suggestion in [Lov85] for computing EDS in parallel in a fast but inefficient manner. The EDS method implies alternative algorithms for biconnectivity and strong orientation. More powerful applications were for finding an st-numbering of a graph, again in [MSV86], as well as for triconnectivity algorithms <p> of efficient and fast parallel algorithms <ref> [MSV86] </ref> and [MR86], after an earlier suggestion in [Lov85] for computing EDS in parallel in a fast but inefficient manner. The EDS method implies alternative algorithms for biconnectivity and strong orientation. More powerful applications were for finding an st-numbering of a graph, again in [MSV86], as well as for triconnectivity algorithms [MR87] and [RV88]. An st-numbering is used in the planarity testing algorithm of [KR88b].
Reference: [MV84] <author> K. Mehlhorn and U. Vishkin. </author> <title> Randomized and deterministic simulations of PRAMs by parallel machines with restricted granularity of parallel memories. </title> <journal> Acta Informatica, </journal> <volume> 21 </volume> <pages> 339-374, </pages> <year> 1984. </year>
Reference-contexts: is given; the algorithm is new, and interestingly also simpler than previous LCA algorithms [HT84] and [SV88]. 6 Randomized Fast Algorithms Randomization has shown to be very useful for both the simulation of PRAM-like shared memory models of parallel computation by other models of parallel machines (e.g., in [KU86], [KRS88], <ref> [MV84] </ref>, [Ran87], [KPS90] and [MSP90]), and for the design of parallel algorithms (e.g., in [ABI86], [AM90], [Gaz86], [GM91], [KR87], [Lub86], [MR85], [MV90], [MV91], All "target algorithms" in this section are randomized, and their running time is at the doubly-logarithmic level, or faster.
Reference: [MV90] <author> Y. Matias and U. Vishkin. </author> <title> On parallel hashing and integer sorting. </title> <booktitle> In Proc. of 17th ICALP, </booktitle> <publisher> Springer LNCS 443, </publisher> <pages> pages 729-743, </pages> <year> 1990. </year> <note> Also, in UMIACS-TR-90-13, </note> <institution> Inst. for Advanced Computer Studies, Univ. of Maryland, </institution> <month> Aug. </month> <note> 1990 (revised), and J. Algorithms, to appear. </note>
Reference-contexts: Randomization has shown to be very useful for both the simulation of PRAM-like shared memory models of parallel computation by other models of parallel machines (e.g., in [KU86], [KRS88], [MV84], [Ran87], [KPS90] and [MSP90]), and for the design of parallel algorithms (e.g., in [ABI86], [AM90], [Gaz86], [GM91], [KR87], [Lub86], [MR85], <ref> [MV90] </ref>, [MV91], All "target algorithms" in this section are randomized, and their running time is at the doubly-logarithmic level, or faster. By the doubly-logarithmic level, we mean O (f (n) log log n) where the function f (n) is o (log log n). <p> Several parallel deterministic and randomized algorithms, that run in time proportional to log n= log log n ("logarithmic level") or slower, were given for sorting [AKS83], [Bat68], [BN89], [Col88], [Hir78] [Pre78], [RV87], and [SV81], and integer sorting [BDH + 89], [Hag87], [Hag91a], <ref> [MV90] </ref>, [MV91], [RR89a], [Ram90] and [Ram91]. <p> This paradigm has been extended, within the same performance bounds, to cope with the more general and well-investigated problem of hashing: given a set of n input elements, build a linear size table that supports membership queries in constant-time. Logarithmic level hashing <ref> [MV90] </ref>, and doubly-logarithmic level hashing [GM91] preceded this result. Some log-star level ideas for a non-standard algorithmic model, where cost of counting, as well as assignment of processors to jobs, are ignored were given in [GMW90]. <p> An (log fl n) time lower-bound using n processors is also given in [GMW90]; the lower bound is for a model of computation that admits the log-star level algorithm. We mention here only one application of hashing; see <ref> [MV90] </ref> for reference to several parallel algorithms with excessive space requirements that become space-efficient by using parallel hashing; the penalties are increase in time (as required by the hashing algorithm) and switching from a deterministic to a randomized algorithm.
Reference: [MV91] <author> Y. Matias and U. Vishkin. </author> <title> Converting high probability into nearly-constant time with appli cations to parallel hashing. </title> <booktitle> In Proc. of the 23rd Ann. ACM Symp. on Theory of Computing, </booktitle> <year> 1991. </year>
Reference-contexts: has shown to be very useful for both the simulation of PRAM-like shared memory models of parallel computation by other models of parallel machines (e.g., in [KU86], [KRS88], [MV84], [Ran87], [KPS90] and [MSP90]), and for the design of parallel algorithms (e.g., in [ABI86], [AM90], [Gaz86], [GM91], [KR87], [Lub86], [MR85], [MV90], <ref> [MV91] </ref>, All "target algorithms" in this section are randomized, and their running time is at the doubly-logarithmic level, or faster. By the doubly-logarithmic level, we mean O (f (n) log log n) where the function f (n) is o (log log n). <p> Several parallel deterministic and randomized algorithms, that run in time proportional to log n= log log n ("logarithmic level") or slower, were given for sorting [AKS83], [Bat68], [BN89], [Col88], [Hir78] [Pre78], [RV87], and [SV81], and integer sorting [BDH + 89], [Hag87], [Hag91a], [MV90], <ref> [MV91] </ref>, [RR89a], [Ram90] and [Ram91]. <p> The linear approximate compaction (LAC) problem is harder: using the same input, the items are to be inserted into an array whose size is linear in m, say 4m. An optimal randomized algorithm for LAC, whose running time is at the log-star level was given <ref> [MV91] </ref>. Unless mentioned otherwise, all log-star level results are from this paper. The algorithm uses the d-PAC algorithm. A somewhat similar use of the d-PAC algorithm for a different problem can be found in [Ram90]. <p> Load balancing can be achieved by a simple application of a prefix-sums algorithm (e.g., [Vis84b]), with a logarithmic-level time overhead. A family of load balancing algorithms are treated in [Gil90], with a doubly-logarithmic multiplicative overhead; <ref> [MV91] </ref> treats a more specific family, with log-star level additive overhead, using the LAC algorithm.
Reference: [Par87] <author> I. Parberry. </author> <title> Parallel Complexity Theory. </title> <publisher> Pitman, </publisher> <address> London, </address> <year> 1987. </year> <month> 16 </month>
Reference-contexts: Survey papers specializing on the class NC are [Coo81] and [Coo85]. More recent review articles include [EG88], [KR88a], and [KRS88], as well as [Ata90b], which is devoted to parallel computational geometry. Books on the topic include [Akl89], [GR88], [JaJ91], <ref> [Par87] </ref> and [Rei91]. For sequential computation, it has been of considerable advantage to deal with an abstraction of the von-Neumann machine, namely the RAM or Random Access Machine (see a standard textbook, such as [AHU74]).
Reference: [Pip79] <author> N. Pippenger. </author> <title> On simultaneous resource bounds. </title> <booktitle> In Proc. of the 20th IEEE Annual Symp. on Foundation of Computer Science, </booktitle> <pages> pages 307-311, </pages> <year> 1979. </year>
Reference-contexts: a paper [FW78]; this original instance concerned the CREW PRAM model. [Sch80b] also advocated using a PRAM for studying the limits of parallel computation at around the same chronological time. [Gol82] was the first to propose the CRCW PRAM model in a complexity theoretic context (he called it a SIMDAG). <ref> [Pip79] </ref> identified and characterized the class NC. [SV81] suggested using the CRCW PRAM in an algorithmic context. Figures 1-3 are a focal point for this short tutorial paper. The figures illustrate some of the structure of PRAM algorithmics.
Reference: [Pre78] <author> F. P. Preparata. </author> <title> New parallel sorting schemes. </title> <journal> IEEE trans. Computer, </journal> <volume> C-27:669-673, </volume> <year> 1978. </year>
Reference-contexts: Several parallel deterministic and randomized algorithms, that run in time proportional to log n= log log n ("logarithmic level") or slower, were given for sorting [AKS83], [Bat68], [BN89], [Col88], [Hir78] <ref> [Pre78] </ref>, [RV87], and [SV81], and integer sorting [BDH + 89], [Hag87], [Hag91a], [MV90], [MV91], [RR89a], [Ram90] and [Ram91].
Reference: [Rag90] <author> P. Ragde. </author> <title> The parallel simplicity of compaction and chaining. </title> <booktitle> In Proc. of 17th ICALP, </booktitle> <publisher> Springer LNCS 443, </publisher> <pages> pages 744-751, </pages> <year> 1990. </year>
Reference-contexts: See Figure 2.3 for the sequel. The most basic problem that was solved using the inverse-Ackermann paradigm is for the nearest-one problem (see also <ref> [Rag90] </ref>, who calls it the chaining problem): given an array of bits (a 1 ; :::; a n ), find for each 1 i n, the two nearest j and l such that a j = a l = 1 (that is, find the smallest j &gt; i such that a <p> Several constant-time optimal randomized algorithms were given: (1) for finding the maximum among n elements [Rei81]; and its generalization (2) for linear programming in fixed dimension [AM90]; (3) for finding approximate median [Sen89]; (4) for the nearest one problem (as in [BV89] and <ref> [Rag90] </ref>), under the assumption that there is some upper bound on the number of ones, [Ram90]. <p> A constant-time algorithm using n processors has been given for this fundamental problem in <ref> [Rag90] </ref>. The linear approximate compaction (LAC) problem is harder: using the same input, the items are to be inserted into an array whose size is linear in m, say 4m. An optimal randomized algorithm for LAC, whose running time is at the log-star level was given [MV91].
Reference: [Ram90] <author> R. Raman. </author> <title> The power of collision: Randomized parallel algorithms for chaining and integer sorting. </title> <note> Technical Report TR-336 (revised version, January 1991), </note> <institution> Computer Science Dept., Univ. of Rochester, </institution> <year> 1990. </year>
Reference-contexts: for finding the maximum among n elements [Rei81]; and its generalization (2) for linear programming in fixed dimension [AM90]; (3) for finding approximate median [Sen89]; (4) for the nearest one problem (as in [BV89] and [Rag90]), under the assumption that there is some upper bound on the number of ones, <ref> [Ram90] </ref>. Several parallel deterministic and randomized algorithms, that run in time proportional to log n= log log n ("logarithmic level") or slower, were given for sorting [AKS83], [Bat68], [BN89], [Col88], [Hir78] [Pre78], [RV87], and [SV81], and integer sorting [BDH + 89], [Hag87], [Hag91a], [MV90], [MV91], [RR89a], [Ram90] and [Ram91]. <p> the number of ones, <ref> [Ram90] </ref>. Several parallel deterministic and randomized algorithms, that run in time proportional to log n= log log n ("logarithmic level") or slower, were given for sorting [AKS83], [Bat68], [BN89], [Col88], [Hir78] [Pre78], [RV87], and [SV81], and integer sorting [BDH + 89], [Hag87], [Hag91a], [MV90], [MV91], [RR89a], [Ram90] and [Ram91]. <p> Unless mentioned otherwise, all log-star level results are from this paper. The algorithm uses the d-PAC algorithm. A somewhat similar use of the d-PAC algorithm for a different problem can be found in <ref> [Ram90] </ref>. Using the log-star-time deterministic algorithms for the nearest-one and prefix-maxima problems, mentioned earlier, as well as the LAC algorithm, an optimal log-star level for generating a random permutation was given.
Reference: [Ram91] <author> R. Raman. </author> <title> Optimal sub-logarithmic time integer sorting on a CRCW PRAM (note). </title> <type> manuscript, </type> <year> 1991. </year>
Reference-contexts: Several parallel deterministic and randomized algorithms, that run in time proportional to log n= log log n ("logarithmic level") or slower, were given for sorting [AKS83], [Bat68], [BN89], [Col88], [Hir78] [Pre78], [RV87], and [SV81], and integer sorting [BDH + 89], [Hag87], [Hag91a], [MV90], [MV91], [RR89a], [Ram90] and <ref> [Ram91] </ref>.
Reference: [Ran87] <author> A.G. Ranade. </author> <title> How to emulate shared memory. </title> <booktitle> In Proc. of the 28th IEEE Annual Symp. on Foundation of Computer Science, </booktitle> <pages> pages 185-194, </pages> <year> 1987. </year>
Reference-contexts: given; the algorithm is new, and interestingly also simpler than previous LCA algorithms [HT84] and [SV88]. 6 Randomized Fast Algorithms Randomization has shown to be very useful for both the simulation of PRAM-like shared memory models of parallel computation by other models of parallel machines (e.g., in [KU86], [KRS88], [MV84], <ref> [Ran87] </ref>, [KPS90] and [MSP90]), and for the design of parallel algorithms (e.g., in [ABI86], [AM90], [Gaz86], [GM91], [KR87], [Lub86], [MR85], [MV90], [MV91], All "target algorithms" in this section are randomized, and their running time is at the doubly-logarithmic level, or faster.
Reference: [Rei81] <author> R. Reischuk. </author> <title> A fast probabilistic parallel sorting algorithm. </title> <booktitle> In Proc. of the 22nd IEEE Annual Symp. on Foundation of Computer Science, </booktitle> <pages> pages 212-219, </pages> <month> October </month> <year> 1981. </year>
Reference-contexts: By the doubly-logarithmic level, we mean O (f (n) log log n) where the function f (n) is o (log log n). Several constant-time optimal randomized algorithms were given: (1) for finding the maximum among n elements <ref> [Rei81] </ref>; and its generalization (2) for linear programming in fixed dimension [AM90]; (3) for finding approximate median [Sen89]; (4) for the nearest one problem (as in [BV89] and [Rag90]), under the assumption that there is some upper bound on the number of ones, [Ram90].
Reference: [Rei91] <author> J.H. Reif, </author> <title> editor. Synthesis of Parallel Algorithms. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California, </address> <year> 1991. </year>
Reference-contexts: Survey papers specializing on the class NC are [Coo81] and [Coo85]. More recent review articles include [EG88], [KR88a], and [KRS88], as well as [Ata90b], which is devoted to parallel computational geometry. Books on the topic include [Akl89], [GR88], [JaJ91], [Par87] and <ref> [Rei91] </ref>. For sequential computation, it has been of considerable advantage to deal with an abstraction of the von-Neumann machine, namely the RAM or Random Access Machine (see a standard textbook, such as [AHU74]).
Reference: [RR89a] <author> S. Rajasekaran and J.H. Reif. </author> <title> Optimal and sublogarithmic time randomized parallel sorting algorithms. </title> <journal> SIAM J. Comput., </journal> <volume> 18 </volume> <pages> 594-607, </pages> <year> 1989. </year>
Reference-contexts: Several parallel deterministic and randomized algorithms, that run in time proportional to log n= log log n ("logarithmic level") or slower, were given for sorting [AKS83], [Bat68], [BN89], [Col88], [Hir78] [Pre78], [RV87], and [SV81], and integer sorting [BDH + 89], [Hag87], [Hag91a], [MV90], [MV91], <ref> [RR89a] </ref>, [Ram90] and [Ram91]. <p> Using the log-star-time deterministic algorithms for the nearest-one and prefix-maxima problems, mentioned earlier, as well as the LAC algorithm, an optimal log-star level for generating a random permutation was given. Other methods for this problem are at the logarithmic level [MR85] and <ref> [RR89a] </ref>; [Hag91b] gives a doubly-logarithmic level algorithm that produces random permutations in a non-standard representation. The LAC algorithm required a new algorithmic paradigm.
Reference: [RR89b] <author> V.L. Ramachandran and J.H. Reif. </author> <title> An optimal parallel algorithm for graph planarity. </title> <booktitle> In Proc. of the 30th IEEE Annual Symp. on Foundation of Computer Science, </booktitle> <pages> pages 282-287, </pages> <year> 1989. </year>
Reference-contexts: More powerful applications were for finding an st-numbering of a graph, again in [MSV86], as well as for triconnectivity algorithms [MR87] and [RV88]. An st-numbering is used in the planarity testing algorithm of [KR88b]. The 7 most recent algorithms for triconnectivity [FRT89] and planarity testing <ref> [RR89b] </ref>, are very nice examples of reaching target problems by building an even higher level in the structure of Figure 1, and using effectively many of the previous techniques. 5 Deterministic Fast Algorithms Structure that was found in optimal doubly-logarithmic time (or faster), parallel algorithms is highlighted.
Reference: [RS89] <author> J.H. Reif and S. Sen. </author> <title> Polling: a new random sampling technique for computational geometry. </title> <booktitle> In Proc. of the 21st Ann. ACM Symp. on Theory of Computing, </booktitle> <pages> pages 394-404, </pages> <year> 1989. </year>
Reference: [RV87] <author> J.H. Reif and L.G. Valiant. </author> <title> A logarithmic time sort for linear size networks. </title> <journal> J. Assoc. Comput. Mach., </journal> <volume> 34 </volume> <pages> 60-76, </pages> <year> 1987. </year>
Reference-contexts: Several parallel deterministic and randomized algorithms, that run in time proportional to log n= log log n ("logarithmic level") or slower, were given for sorting [AKS83], [Bat68], [BN89], [Col88], [Hir78] [Pre78], <ref> [RV87] </ref>, and [SV81], and integer sorting [BDH + 89], [Hag87], [Hag91a], [MV90], [MV91], [RR89a], [Ram90] and [Ram91].
Reference: [RV88] <author> V.L. Ramachandran and U. Vishkin. </author> <title> Efficient parallel triconnectivity in logarithmic parallel time. </title> <booktitle> In Proc. of AWOC 88, Lecture Notes in Computer Science No. </booktitle> <volume> 319, </volume> <pages> pages 33-42. </pages> <publisher> Springer-Verlag, </publisher> <year> 1988. </year>
Reference-contexts: The EDS method implies alternative algorithms for biconnectivity and strong orientation. More powerful applications were for finding an st-numbering of a graph, again in [MSV86], as well as for triconnectivity algorithms [MR87] and <ref> [RV88] </ref>. An st-numbering is used in the planarity testing algorithm of [KR88b].
Reference: [Sch80a] <author> J. Schwartz. </author> <title> Fast probabilistic algorithms for verification of polynomial identities. </title> <journal> JACM, </journal> <volume> 27(4) </volume> <pages> 701-717, </pages> <year> 1980. </year>
Reference: [Sch80b] <author> J. T. Schwartz. </author> <title> Ultracomputers. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 2(4) </volume> <pages> 484-521, </pages> <year> 1980. </year>
Reference-contexts: The PRAM was first proposed as a model for parallel computation in a joint complexity theoretic and algorithmic context in a 1979 thesis [Wyl79] and in a paper [FW78]; this original instance concerned the CREW PRAM model. <ref> [Sch80b] </ref> also advocated using a PRAM for studying the limits of parallel computation at around the same chronological time. [Gol82] was the first to propose the CRCW PRAM model in a complexity theoretic context (he called it a SIMDAG). [Pip79] identified and characterized the class NC. [SV81] suggested using the CRCW
Reference: [Sch87] <author> B. Schieber. </author> <title> Design and analysis of some parallel algorithms. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Tel Aviv Univ., </institution> <year> 1987. </year>
Reference-contexts: For i = 0 a doubly-logarithmic tree consists of a root and two children, which are leaves. Such structure guides the computation in optimal doubly-logarithmic parallel algorithms for finding the maximum among n elements [SV81] (using [Val75]), finding the maximum relative to all prefixes of an array of elements <ref> [Sch87] </ref> and [BSV88] (the prefix-maxima problem), merging two sorted lists [Kru83] and [BH85], finding the convex hull of a monotone polygon [BSV91], and finding all nearest neighbors in a convex polygon [SV90]. Note that all merging algorithms that are mentioned in this paper may be implemented on a CREW PRAM.
Reference: [Sen89] <author> S. Sen. </author> <title> Finding an approximate-median with high-probability in constant time. </title> <type> Manuscript, </type> <year> 1989. </year>
Reference-contexts: Several constant-time optimal randomized algorithms were given: (1) for finding the maximum among n elements [Rei81]; and its generalization (2) for linear programming in fixed dimension [AM90]; (3) for finding approximate median <ref> [Sen89] </ref>; (4) for the nearest one problem (as in [BV89] and [Rag90]), under the assumption that there is some upper bound on the number of ones, [Ram90].
Reference: [SV81] <author> Y. Shiloach and U. Vishkin. </author> <title> Finding the maximum, merging, and sorting in a parallel computa tion model. </title> <journal> J. Algorithms, </journal> <volume> 2 </volume> <pages> 88-102, </pages> <year> 1981. </year>
Reference-contexts: the CREW PRAM model. [Sch80b] also advocated using a PRAM for studying the limits of parallel computation at around the same chronological time. [Gol82] was the first to propose the CRCW PRAM model in a complexity theoretic context (he called it a SIMDAG). [Pip79] identified and characterized the class NC. <ref> [SV81] </ref> suggested using the CRCW PRAM in an algorithmic context. Figures 1-3 are a focal point for this short tutorial paper. The figures illustrate some of the structure of PRAM algorithmics. <p> For i = 0 a doubly-logarithmic tree consists of a root and two children, which are leaves. Such structure guides the computation in optimal doubly-logarithmic parallel algorithms for finding the maximum among n elements <ref> [SV81] </ref> (using [Val75]), finding the maximum relative to all prefixes of an array of elements [Sch87] and [BSV88] (the prefix-maxima problem), merging two sorted lists [Kru83] and [BH85], finding the convex hull of a monotone polygon [BSV91], and finding all nearest neighbors in a convex polygon [SV90]. <p> Several parallel deterministic and randomized algorithms, that run in time proportional to log n= log log n ("logarithmic level") or slower, were given for sorting [AKS83], [Bat68], [BN89], [Col88], [Hir78] [Pre78], [RV87], and <ref> [SV81] </ref>, and integer sorting [BDH + 89], [Hag87], [Hag91a], [MV90], [MV91], [RR89a], [Ram90] and [Ram91].
Reference: [SV82a] <author> Y. Shiloach and U. Vishkin. </author> <title> An O(log n) parallel connectivity algorithm. </title> <journal> J. Algorithms, </journal> <volume> 3 </volume> <pages> 57-67, </pages> <year> 1982. </year>
Reference-contexts: For problem, dubbed duration unknown task scheduling, and the Euler tour technique, as well as ideas from two previous connectivity algorithms [HCS79] and <ref> [SV82a] </ref>. It should be pointed out that the logarithmic time version of the deterministic connectivity algorithm requires the use of expander graphs and thus is highly impractical at present; however, a slightly less parallel version involves much smaller constants.
Reference: [SV82b] <author> Y. Shiloach and U. Vishkin. </author> <title> An O(n 2 log n) parallel Max-Flow algorithm. </title> <journal> J. Algorithms, </journal> <volume> 3:128 146, </volume> <year> 1982. </year>
Reference-contexts: design is guided by a theorem due to [Bre74]; the problem, however, is that the theorem holds only for a non-standard model of parallel computation, where assignment of processors to jobs can be done free of charge; the methodology was first used for the design of a PRAM algorithm in <ref> [SV82b] </ref>, and is elucidated in [Vis90] and [JaJ91], who call it the work-time framework; typical applications of this methodology solve the processor assignment problem in an ad-hoc manner; however, sometimes proper processor assignment can be achieved using general methods for balancing loads among processors.
Reference: [SV88] <author> B. Schieber and U. Vishkin. </author> <title> On finding lowest common ancestors: simplification and paralleliza tion. </title> <journal> SIAM Journal on Computing, </journal> <volume> 17(6) </volume> <pages> 1253-1262, </pages> <year> 1988. </year>
Reference-contexts: Parallel 6 algorithms for this problem <ref> [SV88] </ref> and [BV89] use the Euler tour technique. Depth first search (DFS) is perceived by many as the most useful technique known for designing sequential algorithms for graph problems. Unfortunately, it is not known how to implement DFS efficiently in parallel. <p> tree so that a query requesting the lowest-common-ancestor (LCA) of any pair of vertices can be quickly processed; results are similar to the ones for range-maxima, assuming that the Euler tour of the tree is given; the algorithm is new, and interestingly also simpler than previous LCA algorithms [HT84] and <ref> [SV88] </ref>. 6 Randomized Fast Algorithms Randomization has shown to be very useful for both the simulation of PRAM-like shared memory models of parallel computation by other models of parallel machines (e.g., in [KU86], [KRS88], [MV84], [Ran87], [KPS90] and [MSP90]), and for the design of parallel algorithms (e.g., in [ABI86], [AM90], [Gaz86],
Reference: [SV90] <author> B. Schieber and U. Vishkin. </author> <title> Finding all nearest neighbors for convex polygons in parallel: a new lower bounds technique and a matching algorithm. </title> <journal> Discrete Applied Math, </journal> <volume> 29 </volume> <pages> 97-111, </pages> <year> 1990. </year>
Reference-contexts: maximum among n elements [SV81] (using [Val75]), finding the maximum relative to all prefixes of an array of elements [Sch87] and [BSV88] (the prefix-maxima problem), merging two sorted lists [Kru83] and [BH85], finding the convex hull of a monotone polygon [BSV91], and finding all nearest neighbors in a convex polygon <ref> [SV90] </ref>. Note that all merging algorithms that are mentioned in this paper may be implemented on a CREW PRAM. String matching: For some family of parallel algorithms it is sufficient to consider only non-periodic patterns [Gal85]. <p> algorithm has been used to reduce a general version of the merging problem to the problem of finding all nearest neighbors (ANN) of vertices in a convex polygon; a consequence is that a doubly-logarithmic time lower-bound for merging extends to the ANN problem, resulting in a simpler proof than in <ref> [SV90] </ref>. Wherever reducibilities are more efficient than lower bounds they become promising tools for the theory of lower bounds.
Reference: [TV85] <author> R. E. Tarjan and U. Vishkin. </author> <title> Finding biconnected components and computing tree functions in logarithmic parallel time. </title> <journal> SIAM J. Computing, </journal> <volume> 14 </volume> <pages> 862-874, </pages> <year> 1985. </year>
Reference-contexts: The deterministic algorithms are based on a deterministic arbitration technique, dubbed deterministic coin tossing [CV86b]. Extensions of this technique for sparse graphs and other applications were given [GPS87], [CZ90], and [HCD87]. Key techniques for parallel algorithms on trees are reviewed next: (1) The Euler tour technique <ref> [TV85] </ref> reduces the computation of many tree problems to list ranking. (2) The tree contraction technique [MR85] led to a number of optimal randomized logarithmic-time algorithms for tree problems, including expression tree evaluation; optimal deterministic versions were also given [GR86], [CV88], [ADKP87] and [KD88]. <p> The graph connectivity problem turned out to be the main obstacle to deriving optimal logarithmic time algorithms for several graph problems, including: biconnectivity <ref> [TV85] </ref>, finding Euler tour in a graph [AV84], [AIS84] and orienting the edges of an undirected graph to get a strongly connected digraph ("strong orientation)" [Vis85a]. We also note some recent parallel algorithms for k (edge and vertex) connectivity problems [KS89] and [CT91].
Reference: [Val75] <author> L.G. Valiant. </author> <title> Parallelism in comparison problems. </title> <journal> SIAM J. Comput., </journal> <volume> 4 </volume> <pages> 348-355, </pages> <year> 1975. </year>
Reference-contexts: focus of this paper, we mention here several lower-bound results whose circumvention provided motivation for much of the research in sections 5 and 6: (1) (log n= log log n) time using a polynomial number of processors for the parity problem [BH87]; (2) for finding the maximum among n elements <ref> [Val75] </ref>, and merging [BH85] on a parallel comparison model of computation; and (3) for CREW PRAM computation of the OR function of n bits [CDR86]. <p> For i = 0 a doubly-logarithmic tree consists of a root and two children, which are leaves. Such structure guides the computation in optimal doubly-logarithmic parallel algorithms for finding the maximum among n elements [SV81] (using <ref> [Val75] </ref>), finding the maximum relative to all prefixes of an array of elements [Sch87] and [BSV88] (the prefix-maxima problem), merging two sorted lists [Kru83] and [BH85], finding the convex hull of a monotone polygon [BSV91], and finding all nearest neighbors in a convex polygon [SV90].
Reference: [Val90] <author> L.G. Valiant. </author> <title> A bridging model for parallel computation. </title> <journal> Comm. ACM, </journal> <volume> 33,8:103-111, </volume> <year> 1990. </year>
Reference-contexts: We suggest the following first step towards building such a knowledge-base: develop a core of problems that can be computed very fast, as well as very fast computational paradigms. Another line of additional justification follows [KRS88], <ref> [Val90] </ref> and [Vis84a] that advocate slackness in processors. Let 4 us explain. Suppose we are given an efficient PRAM algorithm and a (real) parallel machine with p 1 processors, on which we wish to simulate the algorithm.
Reference: [Vis83] <author> U. Vishkin. </author> <title> Synchronous parallel computation a survey. </title> <type> Technical Report TR 71, </type> <institution> Dept. of Computer Science, Courant Institute, </institution> <address> New York University, </address> <year> 1983. </year>
Reference-contexts: The survey paper <ref> [Vis83] </ref> elaborates on the raison d'etre of the "PRAM approach". Survey papers specializing on the class NC are [Coo81] and [Coo85]. More recent review articles include [EG88], [KR88a], and [KRS88], as well as [Ata90b], which is devoted to parallel computational geometry. <p> As explained elsewhere (e.g., [KR88a], [KRS88] or <ref> [Vis83] </ref>), the PRAM should be viewed as a virtual design-space for a parallel machine and not as a parallel machine, and improvement in the parallel running time of a PRAM algorithm can benefit us in reducing the actual running time.
Reference: [Vis84a] <author> U. Vishkin. </author> <title> A parallel-design distributed-implementation (PDDI) general purpose computer. </title> <journal> Theoretical Computer Science, </journal> <volume> 32 </volume> <pages> 157-172, </pages> <year> 1984. </year>
Reference-contexts: We suggest the following first step towards building such a knowledge-base: develop a core of problems that can be computed very fast, as well as very fast computational paradigms. Another line of additional justification follows [KRS88], [Val90] and <ref> [Vis84a] </ref> that advocate slackness in processors. Let 4 us explain. Suppose we are given an efficient PRAM algorithm and a (real) parallel machine with p 1 processors, on which we wish to simulate the algorithm. Suppose that the PRAM algorithm is efficient for up to p 2 PRAM processors.
Reference: [Vis84b] <author> U. Vishkin. </author> <title> Randomized speed-ups in parallel computations. </title> <booktitle> In Proc. of the 16th Ann. ACM Symp. on Theory of Computing, </booktitle> <pages> pages 230-239, </pages> <year> 1984. </year>
Reference-contexts: In fact, obtaining optimal algorithms for list ranking and (undirected) graph connectivity proved to be central to obtaining optimal algorithms for a considerable number of list, tree and graph problems. First randomized, and later deterministic, optimal parallel algorithms for list ranking were given <ref> [Vis84b] </ref>, [CV86b], [CV86a], [AM88] and [CV89]. The deterministic algorithms are based on a deterministic arbitration technique, dubbed deterministic coin tossing [CV86b]. Extensions of this technique for sparse graphs and other applications were given [GPS87], [CZ90], and [HCD87]. <p> Load balancing can be achieved by a simple application of a prefix-sums algorithm (e.g., <ref> [Vis84b] </ref>), with a logarithmic-level time overhead. A family of load balancing algorithms are treated in [Gil90], with a doubly-logarithmic multiplicative overhead; [MV91] treats a more specific family, with log-star level additive overhead, using the LAC algorithm.
Reference: [Vis85a] <author> U. Vishkin. </author> <title> On efficient parallel strong orientation. </title> <journal> Information Processing Letters, </journal> <volume> 20 </volume> <pages> 235-240, </pages> <year> 1985. </year>
Reference-contexts: The graph connectivity problem turned out to be the main obstacle to deriving optimal logarithmic time algorithms for several graph problems, including: biconnectivity [TV85], finding Euler tour in a graph [AV84], [AIS84] and orienting the edges of an undirected graph to get a strongly connected digraph ("strong orientation)" <ref> [Vis85a] </ref>. We also note some recent parallel algorithms for k (edge and vertex) connectivity problems [KS89] and [CT91].
Reference: [Vis85b] <author> U. Vishkin. </author> <title> Optimal parallel pattern matching in strings. </title> <journal> Information and Computation, </journal> <volume> 67,1 3 </volume> <pages> 91-113, </pages> <year> 1985. </year>
Reference-contexts: String matching: For some family of parallel algorithms it is sufficient to consider only non-periodic patterns [Gal85]. A method for eliminating (at least) one among two potential occurrences of a non-periodic pattern string in a text string in <ref> [Vis85b] </ref> was observed in [BG88] to be similar to comparing two numbers in order to determine which one is larger and together with an algorithm for finding the maximum, led to an optimal doubly-logarithmic string matching algorithm; [BG91] showed recently a matching lower-bound for a parallel comparison model of computation.
Reference: [Vis90] <author> U. Vishkin. </author> <title> A parallel blocking flow algorithm for acyclic networks. </title> <type> Technical Report UMIACS TR-90-11, </type> <institution> University of Maryland Inst. for Advanced Computer Studies, </institution> <year> 1990. </year>
Reference-contexts: theorem due to [Bre74]; the problem, however, is that the theorem holds only for a non-standard model of parallel computation, where assignment of processors to jobs can be done free of charge; the methodology was first used for the design of a PRAM algorithm in [SV82b], and is elucidated in <ref> [Vis90] </ref> and [JaJ91], who call it the work-time framework; typical applications of this methodology solve the processor assignment problem in an ad-hoc manner; however, sometimes proper processor assignment can be achieved using general methods for balancing loads among processors.
Reference: [Vis91] <author> U. Vishkin. </author> <title> Deterministic sampling anew technique for fast pattern matching. </title> <journal> SIAM J. Comput., </journal> <volume> 20(1) </volume> <pages> 22-40, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: See Figure 2.2. However, using the surplus-log approach, as a rule-of-thumb, was helpful for several problems: (1) String matching for a preprocessed pattern <ref> [Vis91] </ref>; (2) prefix-maxima [BJK + 91]; there, this prefix-maxima algorithm is also the most time consuming step in an algorithm for routing around a rectangle a VLSI routing problem ; and (3) for preprocessing a rooted tree, so that any level-ancestor query can be processed in constant-time [BV91b].
Reference: [Win75] <author> S. Winograd. </author> <title> On the evaluation of certain arithmetic expressions. </title> <journal> J. Assoc. Comput. Mach., </journal> <volume> 22,4:477-492, </volume> <year> 1975. </year>
Reference-contexts: Implicit use of tree contraction in a non-standard parallel algorithmic setting appeared in [Bre74]. (3) Centroid decomposition of a tree, as implicitly used in <ref> [Win75] </ref> for O (log 2 n) time computations. Accelerating centroid decomposition was the motivation for the tree contraction version of [CV88]. Two logarithmic time connectivity algorithms were given: (1) a deterministic one which is optimal on all except very sparse graphs [CV86a]; (2) a randomized optimal one [Gaz86].
Reference: [Wyl79] <author> J. C. Wyllie. </author> <title> The Complexity of Parallel Computations. </title> <type> PhD thesis, </type> <institution> Computer Science Depart ment, Conell University, </institution> <address> Ithaca, NY, </address> <year> 1979. </year> <month> 18 </month>
Reference-contexts: These techniques have led to efficient fast parallel algorithms in a diversity of areas, including computational geometry, graph problems, pattern matching, and comparison problems. The PRAM was first proposed as a model for parallel computation in a joint complexity theoretic and algorithmic context in a 1979 thesis <ref> [Wyl79] </ref> and in a paper [FW78]; this original instance concerned the CREW PRAM model. [Sch80b] also advocated using a PRAM for studying the limits of parallel computation at around the same chronological time. [Gol82] was the first to propose the CRCW PRAM model in a complexity theoretic context (he called it <p> The fact that the prefix-sums problem appears at the bottom of Figure 1 is meant to convey the basic role of this problem. A faster CRCW algorithm for prefix-sums also exists [CV89]. A generalization of this problem to pointer structures, the list ranking problem, was identified in <ref> [Wyl79] </ref>; list ranking has proven to be a key subroutine in parallel algorithms. In fact, obtaining optimal algorithms for list ranking and (undirected) graph connectivity proved to be central to obtaining optimal algorithms for a considerable number of list, tree and graph problems.
References-found: 119

