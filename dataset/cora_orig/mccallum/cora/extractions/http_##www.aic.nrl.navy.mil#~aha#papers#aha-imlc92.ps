URL: http://www.aic.nrl.navy.mil/~aha/papers/aha-imlc92.ps
Refering-URL: http://www.cs.indiana.edu/classes/b652/
Root-URL: 
Email: aha@cs.jhu.edu  
Title: Generalizing from Case Studies: A Case Study  
Author: David W. Aha 
Address: Laurel, MD 20723 USA  
Affiliation: Research Center, RMI Group Applied Physics Laboratory The Johns Hopkins University  
Abstract: Most empirical evaluations of machine learning algorithms are case studies evaluations of multiple algorithms on multiple databases. Authors of case studies implicitly or explicitly hypothesize that the pattern of their results, which often suggests that one algorithm performs significantly better than others, is not limited to the small number of databases investigated, but instead holds for some general class of learning problems. However, these hypotheses are rarely supported with additional evidence, which leaves them suspect. This paper describes an empirical method for generalizing results from case studies and an example application. This method yields rules describing when some algorithms significantly outperform others on some dependent measures. Advantages for generalizing from case studies and limitations of this particular approach are also described.
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. W. </author> <year> (1989). </year> <title> Incremental, instance-based learning of independent and graded concept descriptions. </title> <booktitle> In Proceedings of the Sixth International Workshop on Machine Learning (pp. </booktitle> <pages> 387-391). </pages> <address> Ithaca, NY: </address> <publisher> Mor-gan Kaufmann. </publisher>
Reference: <author> Aha, D. W. </author> <year> (1991). </year> <title> Incremental constructive induction: An instance-based approach. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning (pp. </booktitle> <pages> 117-121). </pages> <address> Evanston, IL: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The results of these tests are summarized in Table 3. The algorithms are: 1. Backpropagation (Rumelhart, McClelland, & the PDP Research Group, 1986), the well-known multi-layer connectionist algorithm, 2. IB1 <ref> (Aha, Kibler, & Albert, 1991) </ref>, a minor vari ant of the nearest neighbor algorithm, 3. CN2 2 (Clark & Niblett, 1989; Clark & Boswell, 1991), a set-covering rule-learner that employs a 2 CN2 was always evaluated using its ordered-rules op tion (Clark & Boswell, 1991). <p> 3 and IB1 attained significantly higher predictive accuracies than CN2 and C4. 4 This is somewhat surprising; previous comparisons showed that C4 usually outperformed IB1 or recorded similar accuracies, although IB1's accuracy was significantly higher for a densely-populated tic-tac-toe endgame database whose attributes contained little information for C4's splitting criterion <ref> (Aha, 1991) </ref>. However, the tic-tac-toe endgame database is quite different from the letter recognition database (e.g., its attributes are not numeric-valued, it has only two classes). Furthermore, whereas CN2 performed as well as IB1 on the tic-tac-toe endgame database, its accuracies were significantly lower here.
Reference: <author> Aha, D. W., Kibler, D., & Albert, M. K. </author> <year> (1991). </year> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> 37-66. </pages>
Reference-contexts: The results of these tests are summarized in Table 3. The algorithms are: 1. Backpropagation (Rumelhart, McClelland, & the PDP Research Group, 1986), the well-known multi-layer connectionist algorithm, 2. IB1 <ref> (Aha, Kibler, & Albert, 1991) </ref>, a minor vari ant of the nearest neighbor algorithm, 3. CN2 2 (Clark & Niblett, 1989; Clark & Boswell, 1991), a set-covering rule-learner that employs a 2 CN2 was always evaluated using its ordered-rules op tion (Clark & Boswell, 1991). <p> 3 and IB1 attained significantly higher predictive accuracies than CN2 and C4. 4 This is somewhat surprising; previous comparisons showed that C4 usually outperformed IB1 or recorded similar accuracies, although IB1's accuracy was significantly higher for a densely-populated tic-tac-toe endgame database whose attributes contained little information for C4's splitting criterion <ref> (Aha, 1991) </ref>. However, the tic-tac-toe endgame database is quite different from the letter recognition database (e.g., its attributes are not numeric-valued, it has only two classes). Furthermore, whereas CN2 performed as well as IB1 on the tic-tac-toe endgame database, its accuracies were significantly lower here.
Reference: <author> Benedict, P. </author> <year> (1990). </year> <title> The second data generation program - DGP/2. </title> <institution> University of Illinois, Urbana-Champaign, Inductive Learning Group, Beckman Institute for Advanced Technology and Sciences. </institution> <note> Unpublished. </note>
Reference: <author> Clark, P. E., & Boswell, R. </author> <year> (1991). </year> <title> Rule induction with CN2: Some recent improvements. </title> <booktitle> In Proceedings of the Fifth European Working Session on Learning (pp. </booktitle> <pages> 151-163). </pages> <address> Porto, Portugal: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: IB1 (Aha, Kibler, & Albert, 1991), a minor vari ant of the nearest neighbor algorithm, 3. CN2 2 (Clark & Niblett, 1989; Clark & Boswell, 1991), a set-covering rule-learner that employs a 2 CN2 was always evaluated using its ordered-rules op tion <ref> (Clark & Boswell, 1991) </ref>. Table 4: Average Accuracies and Standard Deviations (10 trials) When Using only the First Five Letters in Frey and Slate's (1991) Letter Recognition Database Algorithm Accuracy IB1 91:9 1:2% C4 86:3 1:9% noise-tolerant significance test to determine which rules to retain, and 4.
Reference: <author> Clark, P. E., & Niblett, T. </author> <year> (1989). </year> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 261-284. </pages>
Reference-contexts: They can be manually generated by noting the commonalities of the independent parameters' settings when the significant performance differences recurred. For more complex studies, rules can be generated by an appropriate rule-generating algorithm (e.g., CN2 <ref> (Clark & Niblett, 1989) </ref>), where instances are points in the database-characterization space classified according to whether the significant performance differences occur. The parameters held constant should be included as conditions to these rules. Both manual and CN2-generated rules are exemplified in Section 3.
Reference: <author> Dietterich, T. G., Hild, H., & Bakiri, G. </author> <year> (1990). </year> <title> A comparative study of ID3 and Backpropagation for English text-to-speech mapping. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning (pp. </booktitle> <pages> 24-31). </pages> <address> Austin, TX: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: A future research goal is to demonstrate that this method can also be used to help determine why algorithms have significant performance differences. For example, the performance of variants of the same algorithm can be compared to determine when the variations improve performance (e.g., as in <ref> (Dietterich, Hild, & Bakiri, 1990) </ref>). Acknowledgements Suggestions by Isaac Bankman, Pat Langley, Dennis Kibler, and the four anonymous reviewers greatly improved the content and presentation of this paper. Vince Sigillito and Fernando Pineda assisted with the design, implementation, and tuning of Backprop.
Reference: <author> Fogarty, T. C. </author> <title> (in press). First nearest neighbor classification on Frey and Slate's letter recognition problem. </title> <note> To appear in Machine Learning. </note>
Reference: <author> Frey, P. W., & Slate, D. J. </author> <year> (1991). </year> <title> Letter recognition using Holland-style adaptive classifiers. </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> 161-182. </pages>
Reference-contexts: Training on the first 16,000 instances, Frey and Slate's most Table 2: Characteristics of the Letter Recognition Database <ref> (Frey & Slate, 1991) </ref> 1. Number of training instances: 16,000 2. Number of test instances: 4000 3. Number of target classes: 26 4. Number of prototypes per class: 20 5. Number of attributes: 16 6. Type of attributes: Integer-valued 7. Range of each attribute's values: [1,16] 8. <p> Normal Table 3: Average Accuracies and Standard Deviations (10 trials) on Frey and Slate's (1991) Letter Recognition Database when Testing on its last 4000 Instances Algorithm Size of training sets 16,000 1600 Backpropagation - 81:9 0:6% IB1 95:7 0:4% 81:7 0:7% C4 86:4 0:7% 67:4 0:8% Classifier systems 82.7% - <ref> (Frey & Slate, 1991) </ref> accurate variant had an 82.7% predictive accuracy on the final 4000 instances. Fogarty (1992) discovered that the nearest neighbor algorithm's accuracy on this same task was 95.7%, an increase of 13%.
Reference: <author> Kelly, J. D., Jr., & Davis, L. </author> <year> (1991). </year> <title> A hybrid genetic algorithm for classification. </title> <booktitle> In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 645-650). </pages> <address> Sydney, Australia: </address> <publisher> Mor-gan Kaufmann. </publisher>
Reference: <author> Kibler, D., & Langley, P. </author> <year> (1988). </year> <title> Machine learning as an experimental science. </title> <booktitle> In Proceedings of the Third European Working Session on Learning (pp. </booktitle> <pages> 81-92). </pages> <address> Glasgow, Scotland: </address> <publisher> Pitman. </publisher>
Reference: <author> Pazzani, M. J., & Sarrett, W. E. </author> <year> (1990). </year> <title> Integrating empirical and explanation-based learning: Experimental and analytical results. </title> <booktitle> In Proceedings of the Seventh International Conference On Machine Learning (pp. </booktitle> <pages> 339-347). </pages> <address> Austin, TX: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Although formal mathematical analyses are preferred to detail these conditions in the form of average expected computational behavior <ref> (Pazzani & Sarrett, 1990) </ref>, such results are difficult to produce since the algorithms and/or databases are usually complex.
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> The effect of noise on concept learning. </title> <editor> In R. S. Michalski, J. G. Carbonell, & T. M. Mitchell (Eds.), </editor> <booktitle> Machine learning: An artificial intelligence approach (Vol. II). </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Artificially-created databases are required for this task since modifying the characteristics of a database with some unknown characteristics investigates only the relative rather than absolute behavior of the selected algorithms (e.g., as done in <ref> (Quinlan, 1986) </ref>). However, every effort must be made to ensure that the artificially created database is highly similar to the original database; they should share many characteristics and yield similar values for the selected dependent variables. 1 These example characteristics foreshadow that our experiments involve concept learning tasks. <p> Table 4: Average Accuracies and Standard Deviations (10 trials) When Using only the First Five Letters in Frey and Slate's (1991) Letter Recognition Database Algorithm Accuracy IB1 91:9 1:2% C4 86:3 1:9% noise-tolerant significance test to determine which rules to retain, and 4. C4 <ref> (Quinlan, 1986) </ref>, a decision-tree inducer that prunes trees to tolerate noise.
Reference: <author> Rendell, L., & Cho, H. H. </author> <year> (1990). </year> <title> The effect of data character on empirical concept learning. </title> <booktitle> In Machine Learning, </booktitle> <volume> 5, </volume> <pages> 267-298. </pages>
Reference-contexts: It yields rules detailing when these performance differences occur. 2.1 COLLECT CASE STUDY DETAILS These include the selected algorithms, the values for the dependent variables, and the characteristics of the database <ref> (c.f. Rendell & Cho, 1990) </ref>. Although often difficult to obtain, these characteristics are required for the subsequent attempt to mimic the case study results on an artificially-generated database.
Reference: <author> Rumelhart D. E., McClelland, J. L., </author> & <title> The PDP Research Group (Eds.), </title> <booktitle> (1986). Parallel distributed processing: Explorations in the microstructure of cognition (Vol. 1). </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: I tested four learning algorithms on the same training and test sets, and, at Fogarty's suggestion, also tested them on ten smaller-sized training sets whose union is the original training set. The results of these tests are summarized in Table 3. The algorithms are: 1. Backpropagation <ref> (Rumelhart, McClelland, & the PDP Research Group, 1986) </ref>, the well-known multi-layer connectionist algorithm, 2. IB1 (Aha, Kibler, & Albert, 1991), a minor vari ant of the nearest neighbor algorithm, 3.
References-found: 15

