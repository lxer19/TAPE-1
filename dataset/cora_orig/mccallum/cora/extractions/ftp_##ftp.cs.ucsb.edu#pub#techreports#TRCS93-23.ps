URL: ftp://ftp.cs.ucsb.edu/pub/techreports/TRCS93-23.ps
Refering-URL: http://www.cs.umd.edu/~keleher/bib/dsmbiblio/node8.html
Root-URL: 
Title: Evaluating Weak Memories with Maya  
Author: Divyakant Agrawal Manhoi Choy Hong Va Leong Ambuj K. Singh 
Keyword: distributed shared memory, memory consistency, parallel programming, weak memories  
Address: Santa Barbara, CA 93106  
Affiliation: Department of Computer Science University of California at Santa Barbara  
Abstract: Maya is a simulation platform for evaluating the performance of parallel programs on parallel architectures with different memory coherence protocols. Rapid prototyping of different memory protocols supporting varying degrees of coherence is possible and the impact of these protocols on the performance of application programs can be studied. Implementations of existing weak memories along with some new primitives using Maya are presented. The results of running some user applications are summarized and the impact of weak memories on the efficiency of parallel programs is discussed. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Agrawal, M. Choy, H.V. Leong, and A.K. Singh. Maya: </author> <title> A simulation platform for parallel archi-tectures and distributed shared memories. </title> <type> Technical Report TRCS-93-24, </type> <institution> University of California at Santa Barbara, Department of Computer Science, </institution> <year> 1993. </year>
Reference-contexts: The communication event scheduler is used for scheduling non-local simulation events such as transmissions and deliveries of messages. Further discussion of the simulation aspects of Maya can be found in <ref> [1] </ref>. 4 Implementation of Weak Memories and Synchronization Prim <p>- itives Maya manages the shared memory in terms of a set of pages that are allocated by the system during initialization.
Reference: [2] <author> Mustaque Ahamad, James E. Burns, Phillip W. Hutto, and Gil Neiger. </author> <title> Causal memory. </title> <booktitle> In Proceedings of the 5th International Workshop on Distributed Algorithms, </booktitle> <pages> pages 9-30. </pages> <publisher> LNCS, </publisher> <month> October </month> <year> 1991. </year>
Reference-contexts: In general, the degree of coupling is closely related to the latency of memory access and to the cost of maintaining consistency. Most definitions of shared memories abandon atomicity of access, the usual correctness condition in sequential systems. Examples of such non-atomic memories are causal memory <ref> [2] </ref> and pipelined random access memory [19], and memories based on the notions of release consistency [11], entry consistency [5], and hybrid consistency [4]. In addition, some of these definitions are specific to a particular architecture. <p> This paper describes the results of our experiments with distributed shared memory based on three different notions of consistency: sequential consistency [16], causal memory <ref> [2, 3] </ref>, and pipelined random access memory (PRAM) [19]. Since weak memories reduce the amount of implicit synchronization, explicit primitives are needed in order to synchronize user processes. Maya supports locks and barriers as explicit synchronization primitives. <p> Since weak memories reduce the amount of implicit synchronization, explicit primitives are needed in order to synchronize user processes. Maya supports locks and barriers as explicit synchronization primitives. We consider two different user applications: synchronous iterative linear equation solver <ref> [2] </ref> and Cholesky factorization of a sparse matrix [21]. The first problem is solved using a coordinator and a number of worker processes. Each worker is responsible for updating a block of the input array. <p> Note that this definition allows multiple copies of the same variable that reside at different processes to have different values. However, due to the FIFO channel assumption, all updates performed by a process are observed in the same order at any other process. Recently, Ahamad, Burns, Hutto, and Neiger <ref> [2, 3] </ref> have proposed causal memory based on the idea of causality of reads and writes [15]. In this definition, events belonging to the same process are causally related by the program order and a read event is causally related to the write whose value it returns. <p> However, we expect that most of our future experiments will focus on memory consistency conditions weaker than sequential consistency. 4.2 Causal Memory The causal memory protocol in Maya is a fully replicated non-blocking implementation <ref> [2] </ref>. Both read and write operations are never blocked. In this protocol, each node holds a copy of the shared memory. Read operations are served locally by returning the value of the local copy of the memory word. <p> The first application is a coordinator/worker-based synchronous iterative linear equation solver <ref> [2] </ref> and the second application is a Cholesky factorization program. The first program exhibits a static data sharing and updating pattern, whereas the second program shares data statically but updates data dynamically. The first application is slightly computation bound and the second application is communication bound.
Reference: [3] <author> Mustaque Ahamad, Phillip W. Hutto, and Ranjit John. </author> <title> Implementing and programming causal dis-tributed shared memory. </title> <booktitle> In Proceedings of the 11th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 274-281. </pages> <publisher> IEEE, </publisher> <month> May </month> <year> 1991. </year>
Reference-contexts: This paper describes the results of our experiments with distributed shared memory based on three different notions of consistency: sequential consistency [16], causal memory <ref> [2, 3] </ref>, and pipelined random access memory (PRAM) [19]. Since weak memories reduce the amount of implicit synchronization, explicit primitives are needed in order to synchronize user processes. Maya supports locks and barriers as explicit synchronization primitives. <p> Note that this definition allows multiple copies of the same variable that reside at different processes to have different values. However, due to the FIFO channel assumption, all updates performed by a process are observed in the same order at any other process. Recently, Ahamad, Burns, Hutto, and Neiger <ref> [2, 3] </ref> have proposed causal memory based on the idea of causality of reads and writes [15]. In this definition, events belonging to the same process are causally related by the program order and a read event is causally related to the write whose value it returns.
Reference: [4] <author> Hagit Attiya and Roy Friedman. </author> <title> A correctness condition for high-performance multiprocessors. </title> <booktitle> In Proceedings of the 24th Annual ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 679-690, </pages> <year> 1992. </year>
Reference-contexts: Most definitions of shared memories abandon atomicity of access, the usual correctness condition in sequential systems. Examples of such non-atomic memories are causal memory [2] and pipelined random access memory [19], and memories based on the notions of release consistency [11], entry consistency [5], and hybrid consistency <ref> [4] </ref>. In addition, some of these definitions are specific to a particular architecture. <p> A recent paper by Sandhu, Gamsa, and Zhou [22] advocates shared regions, an idea similar to entry consistency. Other definitions of memory consistency in the literature include processor consistency [12], weak ordering [8], and hybrid consistency <ref> [4] </ref>. 3 An Overview of Maya Maya is designed to be a versatile tool for parallel programming with a variety of features. First, it is capable of simulating a target parallel architecture on a different host machine.
Reference: [5] <author> B.N. Bershad and M.J. Zekauskas. Midway: </author> <title> Shared memory parallel programming with entry consis-tency for distributed memory multiprocessors. </title> <booktitle> In COMPCON, </booktitle> <year> 1993. </year>
Reference-contexts: Most definitions of shared memories abandon atomicity of access, the usual correctness condition in sequential systems. Examples of such non-atomic memories are causal memory [2] and pipelined random access memory [19], and memories based on the notions of release consistency [11], entry consistency <ref> [5] </ref>, and hybrid consistency [4]. In addition, some of these definitions are specific to a particular architecture. <p> We concentrate on this phase of computation during our experiments, since this is the most time consuming phase. We ran our experiments on a network of workstations and an Intel Paragon with similar results. Maya is closely related to Munin [7, 14] at Rice and Midway <ref> [5] </ref> at CMU. Munin is one of the first DSM systems that supports multiple coherence protocols within the same system. <p> Synchronization 4 operations are further classified into acquire and release operations. An acquire operation essentially signals that the shared data is needed and a release operation signals that the shared data is available. Consistency is maintained only across synchronization operations. Entry consistency <ref> [5] </ref> strengthens release consistency by associating a unique synchronization object (e.g., a semaphore) with each critical section, i.e., a matching pair of acquire and release operations. A recent paper by Sandhu, Gamsa, and Zhou [22] advocates shared regions, an idea similar to entry consistency. <p> We solve this problem by introducing freeze operations with which full replication can be avoided. This is discussed further in Section 5. 4.4 Entry Consistent Memory The notion of entry consistency was defined by Bershad, Zekauskas and Sawdon <ref> [5] </ref> and independently by Sandhu, Gamsa and Zhou [22]. In entry consistency, synchronization objects are used to protect a critical section of code accessing shared data. The consistency of shared data is only enforced when a critical section is entered. <p> A lock is associated uniquely with each disjoint set of shared variables accessed in a critical section. These locks are mapped to and managed by lock managers that are responsible for issuing grant messages. Communication of updated values is performed in a lazy manner <ref> [5, 14] </ref>: when a lock is acquired by a process, the current values of shared variables associated with the critical section are sent to the process together with the lock grant message. The acquirer then updates the local memory and enters the critical section.
Reference: [6] <author> K.P. Birman, T.A. Joseph, K. Kane, and F. Schmuck. </author> <title> ISIS a distributed programming environment user's guide and reference manual. </title> <type> Technical report, </type> <institution> Cornell University, Department of Computer Science, </institution> <month> March </month> <year> 1988. </year>
Reference-contexts: A system execution is acceptable if and only if the causal order is acyclic and the values read are consistent with the causal order. Causal memory is reminiscent of causal message delivery in distributed systems just as PRAM is reminiscent of ordered message delivery <ref> [6] </ref>. For the case of two processes, causal memory becomes equivalent to PRAM. The simple program fragment shown in Figure 1 can be used to distinguish between the above kinds of shared memory. There are three processes p; q; s and three shared variables x; y; z.
Reference: [7] <author> John B. Carter, John K. Bennett, and Willy Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Proceedings of the Symposium on Operating System Principles, </booktitle> <pages> pages 152-164. </pages> <publisher> ACM, </publisher> <year> 1991. </year>
Reference-contexts: We concentrate on this phase of computation during our experiments, since this is the most time consuming phase. We ran our experiments on a network of workstations and an Intel Paragon with similar results. Maya is closely related to Munin <ref> [7, 14] </ref> at Rice and Midway [5] at CMU. Munin is one of the first DSM systems that supports multiple coherence protocols within the same system.
Reference: [8] <author> Michael Dubois, Christoph Scheurich, and Faye A. Briggs. </author> <title> Memory access buffering in multiprocessors. </title> <booktitle> In Proceedings of the 13th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 434-442, </pages> <month> May </month> <year> 1986. </year>
Reference-contexts: A recent paper by Sandhu, Gamsa, and Zhou [22] advocates shared regions, an idea similar to entry consistency. Other definitions of memory consistency in the literature include processor consistency [12], weak ordering <ref> [8] </ref>, and hybrid consistency [4]. 3 An Overview of Maya Maya is designed to be a versatile tool for parallel programming with a variety of features. First, it is capable of simulating a target parallel architecture on a different host machine.
Reference: [9] <author> Michael Dubois, Christoph Scheurich, and Faye A. Briggs. </author> <title> Synchronization, coherence, and event ordering in multiprocessors. </title> <journal> IEEE Computer, </journal> <volume> 21(2) </volume> <pages> 9-21, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: In order to decrease the latency of accessing shared memory, processes keep local copies of shared variables. This leads to the cache coherence problem of keeping multiple copies of a shared variable consistent with each other <ref> [25, 9] </ref>. The protocol that maintains the consistency of replicated copies of the shared memory among different distributed processors is called the distributed shared memory protocol [18] and the shared memory viewed by the programmers is called distributed shared memory (DSM).
Reference: [10] <author> A. George and J. Liu. </author> <title> Computer Solution of Large Sparse Positive Definite Systems. </title> <publisher> Prentice Hall, </publisher> <year> 1981. </year>
Reference-contexts: In the case of Cholesky factorization, the input matrix is first symbolically factorized and an elimination tree of columns is then constructed <ref> [10] </ref>. Next, the columns are partitioned between the processes and the actual factorization is carried out. We concentrate on this phase of computation during our experiments, since this is the most time consuming phase. We ran our experiments on a network of workstations and an Intel Paragon with similar results. <p> It is intended to evaluate the effectiveness of object-oriented DSM. Here, the input sparse matrix is symbolically factorized and an elimination tree of columns is constructed <ref> [10, 21] </ref> in an initial phase. The time spent in the initial phase is usually negligible compared with the remaining factorization phase and is ignored here.
Reference: [11] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J.L. Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26. </pages> <publisher> IEEE, </publisher> <month> May </month> <year> 1990. </year>
Reference-contexts: Most definitions of shared memories abandon atomicity of access, the usual correctness condition in sequential systems. Examples of such non-atomic memories are causal memory [2] and pipelined random access memory [19], and memories based on the notions of release consistency <ref> [11] </ref>, entry consistency [5], and hybrid consistency [4]. In addition, some of these definitions are specific to a particular architecture. <p> However, the history satisfies the definition of PRAM. We end this section by briefly discussing some other consistency conditions. Release consistency <ref> [11] </ref> classifies the shared accesses into non-synchronization and synchronization operations. Synchronization 4 operations are further classified into acquire and release operations. An acquire operation essentially signals that the shared data is needed and a release operation signals that the shared data is available. Consistency is maintained only across synchronization operations. <p> In a similar vein, the concept of a C-barrier can be defined for causal memory. The concept of a P-lock ensures that when a process acquires the lock, it is informed of all updates performed since it last acquired the lock. This notion is closely related to release consistency <ref> [11] </ref>.
Reference: [12] <author> J.R. Goodman. </author> <title> Cache consistency and sequential consistency. </title> <type> Technical Report CS-1006, </type> <institution> University of Wisconsin-Madison, </institution> <month> February </month> <year> 1991. </year>
Reference-contexts: A recent paper by Sandhu, Gamsa, and Zhou [22] advocates shared regions, an idea similar to entry consistency. Other definitions of memory consistency in the literature include processor consistency <ref> [12] </ref>, weak ordering [8], and hybrid consistency [4]. 3 An Overview of Maya Maya is designed to be a versatile tool for parallel programming with a variety of features. First, it is capable of simulating a target parallel architecture on a different host machine.
Reference: [13] <author> Maurice P. Herlihy and Jeannette M. Wing. </author> <title> Linearizability: a correctness condition for concurrent objects. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 12(3) </volume> <pages> 463-492, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: In other words, it should be possible to construct a sequential global history by interleaving the individual process histories. A notion related to sequential consistency of memory is linearizability <ref> [13] </ref> or atomicity. Linearizability strengthens sequential consistency by requiring the preservation of operation orderings across different processes in the construction of the sequential global history. <p> It also leads to better performance for certain applications. For example, in Cholesky factorization, a considerable amount of locking/unlocking overhead can be eliminated by treating the array elements as abstract objects providing increment/decrement operations. Herlihy and Wing have extended the idea of atomic variables to abstract objects <ref> [13] </ref>; the resulting condition is called linearizability . This strong consistency requirement for objects can be weakened in the spirit of weak consistency conditions for shared variables. For example, based on the degree of desired coherence, a counter object can be implemented using sequential consistency, causal memory, or PRAM.
Reference: [14] <author> Pete Keleher, Alan L. Cox, and Willy Zwaenepoel. </author> <title> Lazy release consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 13-21. </pages> <publisher> IEEE, </publisher> <year> 1992. </year>
Reference-contexts: We concentrate on this phase of computation during our experiments, since this is the most time consuming phase. We ran our experiments on a network of workstations and an Intel Paragon with similar results. Maya is closely related to Munin <ref> [7, 14] </ref> at Rice and Midway [5] at CMU. Munin is one of the first DSM systems that supports multiple coherence protocols within the same system. <p> A lock is associated uniquely with each disjoint set of shared variables accessed in a critical section. These locks are mapped to and managed by lock managers that are responsible for issuing grant messages. Communication of updated values is performed in a lazy manner <ref> [5, 14] </ref>: when a lock is acquired by a process, the current values of shared variables associated with the critical section are sent to the process together with the lock grant message. The acquirer then updates the local memory and enters the critical section.
Reference: [15] <author> Leslie Lamport. </author> <title> Time, clocks, and the ordering of events in a distributed system. </title> <journal> Communications of the ACM, </journal> <volume> 21(7) </volume> <pages> 558-565, </pages> <month> July </month> <year> 1978. </year> <month> 14 </month>
Reference-contexts: However, due to the FIFO channel assumption, all updates performed by a process are observed in the same order at any other process. Recently, Ahamad, Burns, Hutto, and Neiger [2, 3] have proposed causal memory based on the idea of causality of reads and writes <ref> [15] </ref>. In this definition, events belonging to the same process are causally related by the program order and a read event is causally related to the write whose value it returns.
Reference: [16] <author> Leslie Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multiprocess programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 28(9) </volume> <pages> 690-691, </pages> <month> September </month> <year> 1979. </year>
Reference-contexts: Users of Maya write their programs by using an extension of C that enables processes to share variables and presents an abstraction of shared memory to the programmers. This paper describes the results of our experiments with distributed shared memory based on three different notions of consistency: sequential consistency <ref> [16] </ref>, causal memory [2, 3], and pipelined random access memory (PRAM) [19]. Since weak memories reduce the amount of implicit synchronization, explicit primitives are needed in order to synchronize user processes. Maya supports locks and barriers as explicit synchronization primitives. <p> The performance results are detailed in Section 6 and we conclude with a brief discussion in Section 7. 2 Consistency Requirements of Distributed Shared Memories The most widely accepted coherence requirement in the shared memory programming paradigm is the assumption of sequentially consistent memory <ref> [16] </ref>. It is the extension of the memory requirement from a single-processor environment to the multi-processor environment.
Reference: [17] <author> H.V. Leong and D. Agrawal. </author> <title> Type-specific coherence protocol for distributed shared memory. </title> <booktitle> In Proceedings of the 12th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 434-441. </pages> <publisher> IEEE, </publisher> <month> June </month> <year> 1992. </year>
Reference-contexts: a DSM system does not affect the memory consistency scheme because the responsibility of using bulk accesses lies with the user; the current implementation however allows bulk accesses only for data structures allocated contiguously. 5.4 Object-oriented Memory Object-oriented distributed shared memory is a natural generalization of distributed shared memory systems <ref> [17] </ref>. It also leads to better performance for certain applications. For example, in Cholesky factorization, a considerable amount of locking/unlocking overhead can be eliminated by treating the array elements as abstract objects providing increment/decrement operations. <p> The previously described implementation for read/write variables can then be used. Furthermore, in situations where a certain type of operation occurs far more often than others, protocols that are based on the semantics of the operations can be used to improve efficiency <ref> [17] </ref>. Causal and PRAM implementations of shared objects are similar to the analogous implementations of read/write variables. For example, object-oriented PRAM is implemented by replicating the objects and broadcasting any updates to other nodes along FIFO channels.
Reference: [18] <author> Kai Li and Paul Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: This leads to the cache coherence problem of keeping multiple copies of a shared variable consistent with each other [25, 9]. The protocol that maintains the consistency of replicated copies of the shared memory among different distributed processors is called the distributed shared memory protocol <ref> [18] </ref> and the shared memory viewed by the programmers is called distributed shared memory (DSM). Several definitions have been proposed for relaxing the degree of coupling between the copies of shared variables. <p> Next, we describe the implementations of the coherence protocols and the synchronization primitives. 4.1 Sequentially Consistent Memory The sequentially consistent memory protocol supported by the current Maya implementation is the directory- based dynamic ownership protocol using invalidations <ref> [18] </ref>. At any time, only a single process may have the write access privilege to a page; this process is called the owner of the page. Ownership of a page may change dynamically when the variables of the page are being accessed. <p> For example, in Li and Hudak's implementation <ref> [18] </ref>, a page of data is retrieved whenever a page fault occurs. However, choosing a proper page size is difficult. Using a large page size may lead to false sharing and using a small page size may not be efficient.
Reference: [19] <author> Richard J. Lipton and Jonathan S. Sandberg. </author> <title> PRAM: A scalable shared memory. </title> <type> Technical Report CS-TR-180-88, </type> <institution> Princeton University, Department of Computer Science, </institution> <month> September </month> <year> 1988. </year>
Reference-contexts: Most definitions of shared memories abandon atomicity of access, the usual correctness condition in sequential systems. Examples of such non-atomic memories are causal memory [2] and pipelined random access memory <ref> [19] </ref>, and memories based on the notions of release consistency [11], entry consistency [5], and hybrid consistency [4]. In addition, some of these definitions are specific to a particular architecture. <p> This paper describes the results of our experiments with distributed shared memory based on three different notions of consistency: sequential consistency [16], causal memory [2, 3], and pipelined random access memory (PRAM) <ref> [19] </ref>. Since weak memories reduce the amount of implicit synchronization, explicit primitives are needed in order to synchronize user processes. Maya supports locks and barriers as explicit synchronization primitives. We consider two different user applications: synchronous iterative linear equation solver [2] and Cholesky factorization of a sparse matrix [21]. <p> Thus, if there is only a single copy of every object and the read and write operations are implemented as remote procedure calls, then linearizability (and therefore, sequential consistency) is automatically enforced. Pipelined random access memory (PRAM) was introduced by Lipton and Sandberg <ref> [19] </ref>. In this model, processes maintain a complete copy of the globally shared memory. A process reads and updates shared vari <p>- ables in its local copy. The updates are later broadcast to other processes where they occur asynchronously. <p> Our implementation of causal memory timestamps each access instead of each variable or location. So, there is no storage overhead due to storing timestamps with variables or pages. 4.3 Pipelined Random Access Memory The PRAM protocol in Maya is also a fully replicated non-blocking implementation <ref> [19] </ref>. Read operations are served locally and write operations are performed by updating the local copy followed by broadcasting update messages to remote nodes in a FIFO manner. When an update message is received, the effect is incorporated into the local copy without any delay.
Reference: [20] <author> Friedemann Mattern. </author> <title> Virtual time and global states of distributed systems. </title> <editor> In M. Cosnard et. al., editor, </editor> <booktitle> Parallel and Distributed Algorithms: proceedings of the International Workshop on Parallel & Distributed Algorithms, </booktitle> <pages> pages 215-226. </pages> <publisher> Elsevier Science Publishers B. V., </publisher> <year> 1989. </year>
Reference-contexts: Write operations are performed by writing the new value to the local copy and broadcasting update messages to all other nodes along with a vector timestamp that provides a causal ordering on the messages <ref> [20] </ref>. In particular, this vector timestamp counts the number of those messages on each node that are causally preceding the current message. When a message is sent, the field corresponding to the sending node in the vector timestamp is incremented by one.
Reference: [21] <author> Edward Eric Rothberg. </author> <title> Exploiting the Memory Hierarchy in Sequential and Parallel Sparse Cholesky Factorization. </title> <type> PhD thesis, </type> <institution> Stanford University, Department of Computer Science, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: Since weak memories reduce the amount of implicit synchronization, explicit primitives are needed in order to synchronize user processes. Maya supports locks and barriers as explicit synchronization primitives. We consider two different user applications: synchronous iterative linear equation solver [2] and Cholesky factorization of a sparse matrix <ref> [21] </ref>. The first problem is solved using a coordinator and a number of worker processes. Each worker is responsible for updating a block of the input array. <p> It is intended to evaluate the effectiveness of object-oriented DSM. Here, the input sparse matrix is symbolically factorized and an elimination tree of columns is constructed <ref> [10, 21] </ref> in an initial phase. The time spent in the initial phase is usually negligible compared with the remaining factorization phase and is ignored here.
Reference: [22] <author> Harjinder S. Sandhu, Benjamin Gamsa, and Songnian Zhou. </author> <title> The shared regions approach to software cache coherence on multiprocessors. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 229-238. </pages> <publisher> ACM, </publisher> <year> 1993. </year>
Reference-contexts: Consistency is maintained only across synchronization operations. Entry consistency [5] strengthens release consistency by associating a unique synchronization object (e.g., a semaphore) with each critical section, i.e., a matching pair of acquire and release operations. A recent paper by Sandhu, Gamsa, and Zhou <ref> [22] </ref> advocates shared regions, an idea similar to entry consistency. Other definitions of memory consistency in the literature include processor consistency [12], weak ordering [8], and hybrid consistency [4]. 3 An Overview of Maya Maya is designed to be a versatile tool for parallel programming with a variety of features. <p> We solve this problem by introducing freeze operations with which full replication can be avoided. This is discussed further in Section 5. 4.4 Entry Consistent Memory The notion of entry consistency was defined by Bershad, Zekauskas and Sawdon [5] and independently by Sandhu, Gamsa and Zhou <ref> [22] </ref>. In entry consistency, synchronization objects are used to protect a critical section of code accessing shared data. The consistency of shared data is only enforced when a critical section is entered. In the Maya implementation of entry consistency, each critical section is guarded by a lock.
Reference: [23] <author> Ambuj K. Singh. </author> <title> A framework for programming using non-atomic variables. </title> <booktitle> In Proceedings of the 8th International Parallel Processing Symposium, </booktitle> <year> 1994, </year> <note> to appear. </note>
Reference-contexts: For the applications outlined here, these changes are minimal or non-existent. For example, for the linear equation solver, it is possible to show that causal memory leads to the same output as that obtained using sequentially consistent memory <ref> [23] </ref>. (The proof is essentially based on the commutativity of the actions of the processes.) Currently, we are investigating such well-formedness conditions for programs that use different consistency conditions such as causal memory, PRAM, and that rely on explicit synchronization primitives such as locks and barriers.
Reference: [24] <author> J.P. Singh, W.D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford parallel applications for shared memory. </title> <type> Technical report, </type> <institution> Stanford, Computer Systems Laboratory, </institution> <year> 1991. </year>
Reference-contexts: This can be explained by the fact that the communication network on the Intel Paragon is much faster than the Ethernet connecting the Sun workstations. 6.2 Cholesky Factorization Our second application, Cholesky factorization, is borrowed from the SPLASH suite <ref> [24] </ref>. It is intended to evaluate the effectiveness of object-oriented DSM. Here, the input sparse matrix is symbolically factorized and an elimination tree of columns is constructed [10, 21] in an initial phase.
Reference: [25] <author> A. J. Smith. </author> <title> Cache memories. </title> <journal> Computing Surveys, </journal> <volume> 14 </volume> <pages> 473-540, </pages> <year> 1982. </year>
Reference-contexts: In order to decrease the latency of accessing shared memory, processes keep local copies of shared variables. This leads to the cache coherence problem of keeping multiple copies of a shared variable consistent with each other <ref> [25, 9] </ref>. The protocol that maintains the consistency of replicated copies of the shared memory among different distributed processors is called the distributed shared memory protocol [18] and the shared memory viewed by the programmers is called distributed shared memory (DSM).
Reference: [26] <author> V. Sunderam. </author> <title> PVM: A framework for parallel distributed computing. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 2(4) </volume> <pages> 315-339, </pages> <month> December </month> <year> 1990. </year> <month> 15 </month>
Reference-contexts: The primary responsibility of the communication subsystem is to facilitate communication among the memory subsystems, which cooperate with one another to implement distributed shared memory. The communication subsystem in Maya is based on the message passing library PVM <ref> [26] </ref>. Since PVM is available on most distributed memory architectures, Maya can be supported on a variety of hardware platforms. Another advantage of using PVM is that it can be used in a network of heterogeneous Unix compatible machines since PVM uses an external message passing standard.
References-found: 26

