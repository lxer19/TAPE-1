URL: ftp://dimacs.rutgers.edu/pub/dimacs/TechnicalReports/TechReports/1997/97-01.ps.gz
Refering-URL: http://dimacs.rutgers.edu/TechnicalReports/1997.html
Root-URL: http://www.cs.rutgers.edu
Title: Improved Algorithms via Approximations of Probability Distributions 1  
Author: by Suresh Chari ; Pankaj Rohatgi Aravind Srinivasan ; 
Note: DIMACS is a partnership of Rutgers University, Princeton University, AT&T Research, Bellcore, and Bell Laboratories. DIMACS is an NSF Science and Technology Center, funded under contract STC-91-19999; and also receives support from the New Jersey Commission on Science and Technology.  
Address: Ithaca, NY 14853, USA  NY 10532, USA  Singapore 119260 Republic of Singapore  
Affiliation: Dept. of Computer Science Cornell University  IBM T. J. Watson Research Center Hawthorne,  Dept. of Information Systems Computer Science National University of Singapore  
Abstract: DIMACS Technical Report 97-01 on the Theory of Computing, pages 584-592, 1994. 2 Supported in part by NSF grant CCR-9123730. 3 Work done while at Cornell University, supported in part by NSF grant CCR-9123730. 4 Former Joint Postdoctoral Fellow 5 Parts of this work were done at: (i) the Dept. of Computer Science, Cornell University, Ithaca, NY 14853, USA, supported by an IBM Graduate Fellowship; (ii) the School of Mathematics, Institute for Advanced Study, Princeton, NJ 08540, USA, supported by grant 93-6-6 of the Alfred P. Sloan Foundation to the Institute for Advanced Study; (iii) DIMACS (NSF Center for Discrete Mathematics and Theoretical Computer Science), supported by NSF grant NSF-STC91-19999 to DIMACS and by support to DIMACS from the New Jersey Commission on Science and Technology, and (iv) the National University of Singapore. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> N. Alon, L. Babai, and A. Itai, </author> <title> A fast and simple randomized parallel algorithm for the maximal independent set problem, </title> <journal> Journal of Algorithms, </journal> <volume> 7 </volume> <pages> 567-583, </pages> <year> 1986. </year>
Reference-contexts: : :, we have P r ( V Q Efficient constructions of sample spaces S f0; 1g n of size usually much smaller than 2 n , such that the distribution induced on f0; 1g n by sampling uniformly at random from S is k-wise independent, are given, e.g., in <ref> [16, 19, 1] </ref>. The idea here is to analyze a given randomized algorithm and to show that its behavior is good enough if the X i s are k-wise independent for a suitably large k = k (n), rather than completely independent. <p> These two points yield the bounds for Luby's algorithm. (For this problem, P rof it and Cost actually lie in <ref> [0; 1] </ref>.) Using these two points in conjunction with Theorem 6 by making fl an arbitrarily small constant, we get Corollary 1 ( + 1)-vertex coloring is solvable by an ((n + m)n ffi ; log 2 n) algorithm, for any fixed ffi &gt; 0.
Reference: [2] <author> N. Alon, J. Bruck, J. Naor, M. Naor, and R. Roth, </author> <title> Construction of asymptotically good, low-rate error-correcting codes through pseudo-random graphs, </title> <journal> IEEE Trans. Info. Theory, </journal> <volume> 38 </volume> <pages> 509-516, </pages> <year> 1992. </year>
Reference-contexts: NC 1 constructions of sample spaces of size O (n=* 3 ) with Fourier coefficients bounded in magnitude by * were developed in <ref> [2] </ref>, improving on those of [22]. NC 1 constructions of three different sample spaces of size O (n 2 =* 2 ), were presented by [3]. <p> A k-wise *-biased space is k-wise *-approximate [22]. Sample spaces of size O (minf k logn * 3 ; * 2 g) which are k-wise *-biased can be constructed in NC 1 <ref> [22, 3, 2] </ref>. If S 0 is k-wise * 0 -biased in Lemma 1, then the resulting space S is k-wise *-biased. <p> : : : ; a jIj in [0; m 1], jP r ~ Y 2S ( i2I (Y i = a i )) i2I P r D (X i = a i )j *. (The case m = 2 with D being the uniform distribution is handled by small-bias spaces <ref> [22, 3, 2] </ref>.) We are interested in such approximating sample spaces that are "small" and efficiently constructible.
Reference: [3] <author> N. Alon, O. Goldreich, J. H-astad, and R. Peralta, </author> <title> Simple constructions of almost k-wise independent random variables, Random Structures and Algorithms, </title> <booktitle> 3(3) </booktitle> <pages> 289-303, </pages> <year> 1992. </year>
Reference-contexts: NC 1 constructions of sample spaces of size O (n=* 3 ) with Fourier coefficients bounded in magnitude by * were developed in [2], improving on those of [22]. NC 1 constructions of three different sample spaces of size O (n 2 =* 2 ), were presented by <ref> [3] </ref>. The basic idea behind our construction of approximate sample spaces is the following simple and well-known fact: Fact 1 Let X 1 ; : : : ; X k be independent binary random variables, with bias (X i ) = * i . <p> A k-wise *-biased space is k-wise *-approximate [22]. Sample spaces of size O (minf k logn * 3 ; * 2 g) which are k-wise *-biased can be constructed in NC 1 <ref> [22, 3, 2] </ref>. If S 0 is k-wise * 0 -biased in Lemma 1, then the resulting space S is k-wise *-biased. <p> Thus each stage can be implemented using (n + m)2 O (flq) jS 0 j processors and the running time is O (t + log n + flq) for each stage. Plugging in the size of S 0 from the standard construction of <ref> [3] </ref> and using the fact that there are fl 1 stages, we get - 11 - Theorem 6 For any parameter fl 2 (0; 1) and any fixed ffi &gt; 0, there is an ((n + m)2 ffiflq n ffi ; fl 1 (t + log n)+q) algorithm to find a <p> Therefore we can do with a log c n-wise n fi (polylog (n)) -biased space. However, it can be shown that the size of such a sample space must be superpolynomial <ref> [3] </ref>. We therefore suggest the following approach. <p> : : : ; a jIj in [0; m 1], jP r ~ Y 2S ( i2I (Y i = a i )) i2I P r D (X i = a i )j *. (The case m = 2 with D being the uniform distribution is handled by small-bias spaces <ref> [22, 3, 2] </ref>.) We are interested in such approximating sample spaces that are "small" and efficiently constructible.
Reference: [4] <author> N. Alon and M. Naor, Derandomization, </author> <title> witnesses for Boolean matrix multiplication and construction of perfect hash functions, </title> <journal> Algorithmica, </journal> <volume> 16 </volume> <pages> 434-449, </pages> <year> 1996. </year>
Reference-contexts: In contrast, our method yields an ((m + n)n ffi ; log n) algorithm for this problem, for any fixed ffi &gt; 0. We combine our method with some ideas of Alon & Naor <ref> [4] </ref> to derive improved algorithms for some other basic problems. <p> The parameter q is usually at most polylogarithmic in the input size N , in our applications. By combining our basic method with some ideas of <ref> [4] </ref> we get improved parallel algorithms for various problems. <p> However, the methods of the previous section require us to compute probabilities by first converting them to the appropriate bias terms; the large number of Fourier coefficients would then increase the processor complexity by a 2 2q factor. Instead, we adopt the following strategy, motivated by a technique of <ref> [4] </ref> (see also [5]). The idea is to set flq bits of each of the labels ^ `(v) all at once in each stage, and repeat fl 1 times. As said before, fl denotes a positive parameter that is at most one, whose value we choose based on the application. <p> Inequality (2) is established by induction on s, as in <ref> [4] </ref>; the basis s = 0 holds trivially. Assume that (2) is true for s; we now show how to pick w s+1 2 S 1 such that it holds for s + 1. Suppose we pick R s+1 according to BU s+1 .
Reference: [5] <author> N. Alon and A. Srinivasan, </author> <title> Improved parallel approximation of a class of integer programming problems, To appear in Algorithmica. A preliminary version of this work appears in the Proc. </title> <booktitle> International Colloquium on Automata, Languages and Programming, </booktitle> <pages> pages 562-573, </pages> <year> 1996. </year>
Reference-contexts: Instead, we adopt the following strategy, motivated by a technique of [4] (see also <ref> [5] </ref>). The idea is to set flq bits of each of the labels ^ `(v) all at once in each stage, and repeat fl 1 times. As said before, fl denotes a positive parameter that is at most one, whose value we choose based on the application.
Reference: [6] <author> J. Beck and W. Chen, </author> <title> Irregularities of distribution, </title> <publisher> Cambridge University Press, </publisher> <year> 1987. </year>
Reference-contexts: R. We define S (R k The study of such discrepancy problems from a different viewpoint, has also been conducted in discrepancy theory (see, e.g., Beck & Chen <ref> [6] </ref>). Such problems are also relevant to certain numerical integration problems.
Reference: [7] <author> B. Berger, </author> <title> Using randomness to design efficient deterministic algorithms, </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering and Computer Science, MIT, </institution> <month> May </month> <year> 1990. </year>
Reference-contexts: The original algorithm uses the above general framework as a subroutine and replacing the original algorithm with ours results in the following improvement. We refer the reader to <ref> [7, 8] </ref> for details. <p> The problem of finding a maximum-sized such ^ A is known to be NP-hard. Berger <ref> [7] </ref> derives an NC algorithm which finds a subset ^ A of size at least target : jAj + c 0 ( i=1 deg (i) + i=1 where n = jV j, d out (v) and d in (v) denote the out- and in-degrees of v in G, deg (v) = <p> This description in x4.4 of <ref> [7] </ref>, is an expanded version of the work of [9]. It is first shown in [7] that we may assume without loss of generality that G has no pair of anti-parallel arcs, i.e., no pair of the form f (u; v); (v; u)g. <p> This description in x4.4 of <ref> [7] </ref>, is an expanded version of the work of [9]. It is first shown in [7] that we may assume without loss of generality that G has no pair of anti-parallel arcs, i.e., no pair of the form f (u; v); (v; u)g. The essential idea in their algorithm is as follows. <p> The resulting set of arcs ^ A clearly defines an acyclic graph; using the fourth moment method, it is shown in <ref> [7] </ref> that if the labels are assigned even 5-wise independently, then E [j ^ Aj] is at least as high as the - 12 - above-defined target. Thus, the goal is to efficiently derandomize the above algorithm in NC, by computing appropriate values for the labels r v . <p> Thus, the goal is to efficiently derandomize the above algorithm in NC, by computing appropriate values for the labels r v . We now present only the results of <ref> [7] </ref> that are directly relevant to us; more details can be obtained from x4.4 of [7]. Given an event B, let I (B) denote the indicator variable for B, i.e., I (B) equals 1 if B holds, and is 0 if B does not hold. <p> Thus, the goal is to efficiently derandomize the above algorithm in NC, by computing appropriate values for the labels r v . We now present only the results of <ref> [7] </ref> that are directly relevant to us; more details can be obtained from x4.4 of [7]. Given an event B, let I (B) denote the indicator variable for B, i.e., I (B) equals 1 if B holds, and is 0 if B does not hold. <p> For any v 2 V , let N (v) = fw 2 V : (v; w) 2 A or (w; v) 2 Ag, and let = max v2V jN (v)j. Also let V = f1; 2; : : :; ng. The following theorem follows from x4.4 of <ref> [7] </ref>: Theorem 7 ([7]) (a) For any (random or deterministic) choice of the labels r v in the above algorithm, the set of arcs ^ A that is output, satisfies j ^ Aj f (r 1 ; r 2 ; : : : ; r n ), where f (r 1 <p> Let q = log = dlog ne, and let us denote each r v by a bit-string y v;1 y v;2 y v;q , with each y v;j 2 f0; 1g. To derandomize the above algorithm, the work of <ref> [7] </ref> makes a suitable choice for the vector z i = (y 1;i y 2;i y n;i ) in stages i = 1; 2; : : :; q, via the method of conditional probabilities; it is also shown in [7] that any term such as E [I (r u = r <p> To derandomize the above algorithm, the work of <ref> [7] </ref> makes a suitable choice for the vector z i = (y 1;i y 2;i y n;i ) in stages i = 1; 2; : : :; q, via the method of conditional probabilities; it is also shown in [7] that any term such as E [I (r u = r v )jz 1 ; z 2 ; : : : ; z i1 ] or E [I (r v &lt; minfr w : w 2 Sg)jz 1 ; z 2 ; : : : ; z i1 ] can <p> ; z 2 ; : : : ; z i1 ] can be computed in constant time using one processor. (Note that the problem on hand is an instance of the general framework considered in x3.3.) This results in an (n 4 ; log 3 n) algorithm for this problem <ref> [7] </ref>. We could replace their invocation of the general framework by ours and this would result in an algorithm of complexity (n 1+ffi 4 ; log 2 n) for any fixed ffi &gt; 0.
Reference: [8] <author> B. Berger and J. Rompel, </author> <title> Simulating (log c n)-wise independence in NC, </title> <journal> Journal of the ACM, </journal> <volume> 38 </volume> <pages> 1026-1046, </pages> <year> 1991. </year>
Reference-contexts: His method has been used and extended to derive NC algorithms for fundamental problems such as set discrepancy <ref> [8, 21] </ref>. The motivation for our first method is similar to that of [20]. <p> Spencer [29] gave a polynomial-time algorithm to find an assignment with disc () = O ( p NC 3 algorithms to find an assignment with disc () = O (s 1=2+ffi p log n), for any fixed ffi &gt; 0, were first given in <ref> [8, 21] </ref> and small-bias spaces were used to do this in NC 1 [22], but with (n 3+2=ffi ) processors. <p> We wish to find a sample point X 0 2 f0; 1g n deterministically such that X 0 E [X]. This general framework, which models the derandomization of many RNC algorithms, was introduced in <ref> [8] </ref> (see also [21]), where an (n a+b ; t log 3 n) algorithm is given (here we assume that each f i is computable in O (t) time with one processor). <p> A key example of this arises in solving set discrepancy in NC, with discrepancy O ( p s log n)-a challenging open question <ref> [8, 21] </ref>. We show that if a certain small-bias space S is efficiently constructible, then our method can solve the above class of problems in NC. This would be interesting since most current methods cannot tackle a superpolynomial number of terms. However, we do not yet know if S exists. <p> It is shown in <ref> [8, 21] </ref> that if we choose k = 2dlog (2n)=(ffi log s)e and choose the assignments (j); j 2 X, k-wise independently then P r ( (9i) j (S i ) 2 p n X E [((S i ) 2 ) k ] log n) k &lt; 1: (1) This is <p> fi (log n) ; log n) algorithm with the constant in the exponent of the processor complexity roughly half of what a direct implementation using regular *-biased spaces can yield. - 8 - 3.3 A general framework Set discrepancy has been captured as part of a more general framework in <ref> [8] </ref>, and we can extend the above method to this general framework to derive improved algorithms. <p> We wish to find a sample point X 0 with X 0 E [X], where a and b are constants. An (n a+b ; t log 3 n) algorithm is presented for this problem in <ref> [8] </ref> if each f i is computable in O (t) time with one processor. <p> If s = log O (1) n in the set discrepancy problem, then an (n 2 log O (1) n; s 2 log 3 n) algorithm is given in <ref> [8] </ref> to find with disc () = O ( p s log n). The original algorithm uses the above general framework as a subroutine and replacing the original algorithm with ours results in the following improvement. We refer the reader to [7, 8] for details. <p> The original algorithm uses the above general framework as a subroutine and replacing the original algorithm with ours results in the following improvement. We refer the reader to <ref> [7, 8] </ref> for details. <p> An obvious instance of this framework is the set discrepancy problem where we wish to find an assignment with disc () = O ( p s log n) using the O (log n)-th moment method, i.e., using variables which are O (log n) wise independent <ref> [8, 21] </ref>. For this application, we have a sum of n functions of this type, with c = 1. All the currently known methods fail to solve this problem in NC since they need one processor per term and there are n fi (log c n) terms.
Reference: [9] <author> B. Berger and P. W. Shor, </author> <title> Approximation algorithms for the maximum acyclic subgraph problem, </title> <booktitle> in Proc. ACM/SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 236-243, </pages> <year> 1990. </year> <month> - 19 </month> - 
Reference-contexts: This gives faster algorithms for other problems such as the much harder -vertex coloring problem [25]. We use similar ideas to derive a faster algorithm for approximating the maximum acyclic subgraph problem <ref> [9] </ref>, incurring a small processor penalty. Thus, our first method yields the fastest known NC algorithms for a large class of problems, without a big processor penalty. <p> This description in x4.4 of [7], is an expanded version of the work of <ref> [9] </ref>. It is first shown in [7] that we may assume without loss of generality that G has no pair of anti-parallel arcs, i.e., no pair of the form f (u; v); (v; u)g. The essential idea in their algorithm is as follows.
Reference: [10] <author> B. Chazelle and J. Matousek, </author> <title> On linear-time deterministic algorithms for optimization problems in fixed dimension, </title> <booktitle> in Proc. ACM/SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 281-290, </pages> <year> 1993. </year>
Reference-contexts: This yields improved algorithms for problems like vector-balancing [28], lattice approximation [26], and computing *-nets and *-approximations for range spaces with finite V C-dimension <ref> [10] </ref>. Set balancing is a special case of the following general problem. <p> The set discrepancy abstraction has been used to solve other problems such as vector balancing [28], lattice approximation [26] (see also the description in [21]) and for computing *-nets and *- approximations for range spaces with finite V C-dimension <ref> [10] </ref>. Theorem 4 implies improved NC algorithms for all these problems. Our method also yields improvements in processor complexity for algorithms which find an assignment with disc () = O ( p s log n).
Reference: [11] <author> G. Even, O. Goldreich, M. Luby, N. Nisan, and B. Velickovic, </author> <title> Approximations of general independent distributions, </title> <booktitle> in Proc. ACM Symposium on Theory of Computing, </booktitle> <pages> pages 10-16, </pages> <year> 1992. </year>
Reference-contexts: In this paper, we present two new techniques for derandomization. The first leads to improved NC algorithms for many basic problems such as finding large cuts in graphs, set discrepancy, ( + 1)-vertex coloring of graphs and others while the second improves the constructions due to <ref> [11] </ref> of small sample spaces for use in derandomization. We also show how the first method could potentially be used to solve some other open problems in parallel computation. The second method yields smaller sample spaces for the efficient derandomization of randomized algorithms. <p> However, we do not yet know if S exists. We now turn to the second type of derandomization technique presented. Given the utility of small spaces approximating the joint distribution of unbiased and independent random bits, the general problem of approximating arbitrary independent distributions was addressed in <ref> [11] </ref>. <p> Three constructions of such small sample spaces are presented in <ref> [11] </ref>, each suitable for different ranges of the input parameters. We provide a construction which is always better than or as good as all of their constructions. Our sample space is of size poly (log n; 1=*; (dk= log (1=*)e) log (1=*) ). As in [11], we reduce this problem to <p> sample spaces are presented in <ref> [11] </ref>, each suitable for different ranges of the input parameters. We provide a construction which is always better than or as good as all of their constructions. Our sample space is of size poly (log n; 1=*; (dk= log (1=*)e) log (1=*) ). As in [11], we reduce this problem to a geometric discrepancy problem; our improvement follows from our improved solution for this latter problem. As pointed out in x6.1, improved solutions for this geometric discrepancy problem also yield improved methods for certain classes of numerical integration problems. <p> As said before, such efficiently constructible spaces have become a key tool in derandomization. With this motivation, the work of <ref> [11] </ref> considers the general problem of approximating the distribution of independent multivalued random variables: - 14 - Definition 5 Let X 1 ; : : : ; X n 2 f0; : : :; m 1g be independent random variables with arbitrary individual distributions and joint distribution D. <p> Indeed, as pointed out in <ref> [11] </ref>, a simple probabilistic argument shows that there exists a (k; *)- approximation of cardinality as small as O (n log (n=*)=* 2 ) for any D and any k n. <p> We now improve on the cardinality of the previous-best constructions of (k; *)- approximations. 6.1 A geometric discrepancy problem The efficient construction of (k; *)-approximations can be reduced to the following discrepancy problem, as shown in <ref> [11] </ref>. <p> Small-discrepancy sets are obvious candidates to be such a good set of points, and, indeed, there are bounds on the error in the above process, that show that a small discrepancy implies a small error. The following lemma of <ref> [11] </ref> shows how the above geometric discrepancy problem is relevant for (k; *)-approximations: Lemma 2 ([11]) Suppose, for integers k; n and for some parameter * &gt; 0, a finite set S [0; 1) n satisfies S (R k n ) *. <p> The major goal here is to construct S such that jSj = poly (k; log n; 1=*); (5) even a construction with cardinality poly (n; 1=*) will be very interesting. However, these seem to be difficult problems for now. The work of <ref> [11] </ref> describes efficient constructions of three sample spaces S 1 ; S 2 ; S 3 [0; 1) n to satisfy (4) with jS 1 j = poly (log n; 2 k ; 1=*); jS 2 j = (n=*) O (log (1=*)) ; and jS 3 j = (n=*) O (logn) <p> We now use the fact that maxfb 1 ; b 2 g jb 1 b 2 j + b 2 to complete the proof. 2 We next present a useful lemma, whose proof closely follows that of Theorem 2 of <ref> [11] </ref> (see also Theorem 2.6 in [27]): Lemma 3 Let X 1 ; X 2 ; : : : ; X t be independent binary random variables with P r (X i = 1) = p i ; let Z 1 ; Z 2 ; : : : ; Z t <p> For any positive integer ` t, let B ` = A [t]:jAj=` ^ (Z i = 1)); and C ` = A [t]:jAj=` ^ (X i = 1)): Case I: P i2 [t] p i k=(2e). One consequence of the proof of Theorem 2 of <ref> [11] </ref> is that in this case, C k 2 k : (7) b 2 = C k . <p> [t 0 ] p i k=(2e), it follows as in Case I that fi ^ (Z i = 0))P r ( i2 [t 0 ] fi k X X fi ^ (Z i = 1))P r ( i2A fi (8) It also follows from the proof of Theorem 2 of <ref> [11] </ref> that since P i2 [t 0 ] p i &gt; k=(2e) 1, we have P r ( i2 [t 0 ] (k=2e) 1 t 0 This fact, combined with (8), and the observation that 0 P r ( i2 [t] ^ (Z i = 0)); 0 P r ( i2 <p> For any i, 0 i r, we have t i x i . Thus, P r t P t i (xr=t) i = 6.3 The construction We present a construction whose size either matches or is smaller than those of <ref> [11] </ref>. We start with a useful lemma which, in conjunction with Lemma 3, will help validate our construction. Lemma 4 Suppose that a finite set S [0; 1) n satisfies S (R k 0 n ) * 0 , for some k 0 and * 0 . <p> Thus, since (13) is established, the proof is complete. 2 Thus, our construction either matches or is better in size than all three of <ref> [11] </ref>. For instance if k = log b n and * = n fi (1) with b 2, our construction has size n O (loglog n) , while those of [11] have size n O (logn) . - 18 - 7 Discussion We have devised a new probabilistic technique: the construction <p> established, the proof is complete. 2 Thus, our construction either matches or is better in size than all three of <ref> [11] </ref>. For instance if k = log b n and * = n fi (1) with b 2, our construction has size n O (loglog n) , while those of [11] have size n O (logn) . - 18 - 7 Discussion We have devised a new probabilistic technique: the construction of a sample space whose Fourier coefficients are close to those of a independent random source on n random variables. <p> An obvious generalization is to study such approximations for n independent multi-valued random variables that have arbitrary marginal distributions. Such constructions again have pseudorandom generators and derandomiza-tion as their main motivations. For this problem, we have presented a construction that improves on the previous-best results, due to <ref> [11] </ref>. Acknowledgments. We thank Juris Hartmanis, Ronitt Rubinfeld and David Shmoys for their guidance and support, and Noga Alon, Bernard Chazelle and Moni Naor for valuable discussions.
Reference: [12] <author> A. M. Ferrenberg, D. P. Landau, and Y. J. Wong, </author> <title> Monte Carlo simulations: Hidden errors from "good" random number generators, </title> <journal> Physical Review Letters, </journal> <volume> 69(23) </volume> <pages> 3382-3384, </pages> <year> 1992. </year>
Reference-contexts: There have also been reports of Monte-Carlo simulations giving quite different results under different random-number generators <ref> [12] </ref>, and direct implementations of certain RNC algorithms taking longer time than expected due to the pseudorandom nature of computer-generated "random" bits [15, 14]. In this paper, we present two new techniques for derandomization.
Reference: [13] <author> Y. Han, </author> <title> A fast derandomization scheme and its applications, </title> <journal> SIAM J. Comput., </journal> <volume> 25 </volume> <pages> 52-82, </pages> <year> 1996. </year>
Reference-contexts: a ( + 1)-vertex coloring of the graph can be computed by an ((n + m); log 3 n log log n) algorithm [20]; an improved ((n + m)= log log n; log 2 n log log n) algorithm that runs on the CREW PRAM, has been devised by Han <ref> [13] </ref>. We improve these time complexities by presenting an ((n + m)n ffi ; log 2 n) algorithm where ffi &gt; 0 is any constant. This gives faster algorithms for other problems such as the much harder -vertex coloring problem [25].
Reference: [14] <author> T.-s. Hsu, </author> <title> Graph augmentation and related problems: theory and practice, </title> <type> PhD thesis, </type> <institution> Department of Computer Sciences, University of Texas at Austin, </institution> <month> October </month> <year> 1993. </year>
Reference-contexts: There have also been reports of Monte-Carlo simulations giving quite different results under different random-number generators [12], and direct implementations of certain RNC algorithms taking longer time than expected due to the pseudorandom nature of computer-generated "random" bits <ref> [15, 14] </ref>. In this paper, we present two new techniques for derandomization.
Reference: [15] <author> T.-s. Hsu, V. Ramachandran, and N. Dean, </author> <title> Parallel implementation of algorithms for finding connected components, </title> <booktitle> in DIMACS International Algorithm Implementation Challenge, </booktitle> <pages> pages 1-14, </pages> <year> 1994. </year>
Reference-contexts: There have also been reports of Monte-Carlo simulations giving quite different results under different random-number generators [12], and direct implementations of certain RNC algorithms taking longer time than expected due to the pseudorandom nature of computer-generated "random" bits <ref> [15, 14] </ref>. In this paper, we present two new techniques for derandomization.
Reference: [16] <author> A. Joffe, </author> <title> On a set of almost deterministic k-independent random variables, </title> <journal> The Annals of Probability, </journal> <volume> 2(1) </volume> <pages> 161-162, </pages> <year> 1974. </year>
Reference-contexts: : :, we have P r ( V Q Efficient constructions of sample spaces S f0; 1g n of size usually much smaller than 2 n , such that the distribution induced on f0; 1g n by sampling uniformly at random from S is k-wise independent, are given, e.g., in <ref> [16, 19, 1] </ref>. The idea here is to analyze a given randomized algorithm and to show that its behavior is good enough if the X i s are k-wise independent for a suitably large k = k (n), rather than completely independent.
Reference: [17] <author> H. J. Karloff and P. Raghavan, </author> <title> Randomized algorithms and pseudorandom numbers, </title> <journal> Journal of the ACM, </journal> <volume> 40(3) </volume> <pages> 454-476, </pages> <year> 1993. </year>
Reference-contexts: The fact that computers do not use "real" random sources prevents randomized algorithms from having a sound footing; it has been shown that if algorithms such as randomized Quicksort are not implemented carefully when used with some existing pseudorandom generators, their expected running times can be high <ref> [17] </ref>. There have also been reports of Monte-Carlo simulations giving quite different results under different random-number generators [12], and direct implementations of certain RNC algorithms taking longer time than expected due to the pseudorandom nature of computer-generated "random" bits [15, 14].
Reference: [18] <author> R. M. Karp and A. Wigderson, </author> <title> A fast parallel algorithm for the maximal independent set problem, </title> <journal> Journal of the ACM, </journal> <volume> 32 </volume> <pages> 762-773, </pages> <year> 1985. </year>
Reference: [19] <author> M. Luby, </author> <title> A simple parallel algorithm for the maximal independent set problem, </title> <journal> SIAM J. Comput., </journal> <volume> 15(4) </volume> <pages> 1036-1053, </pages> <year> 1986. </year>
Reference-contexts: : :, we have P r ( V Q Efficient constructions of sample spaces S f0; 1g n of size usually much smaller than 2 n , such that the distribution induced on f0; 1g n by sampling uniformly at random from S is k-wise independent, are given, e.g., in <ref> [16, 19, 1] </ref>. The idea here is to analyze a given randomized algorithm and to show that its behavior is good enough if the X i s are k-wise independent for a suitably large k = k (n), rather than completely independent.
Reference: [20] <author> M. Luby, </author> <title> Removing randomness in parallel computation without a processor penalty, </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 47(2) </volume> <pages> 250-286, </pages> <year> 1993. </year>
Reference-contexts: This ensures that when we have assigned all n components we have found an ~ X 0 such that f ( ~ X 0 ) E (f ( ~ X )). In <ref> [20] </ref>, Luby observed that using limited independence directly in some parallel algorithms leads to a high processor complexity, and introduced a way of combining limited independence with the method of conditional probabilities which led to processor-efficient algorithms. <p> His method has been used and extended to derive NC algorithms for fundamental problems such as set discrepancy [8, 21]. The motivation for our first method is similar to that of <ref> [20] </ref>. We observe that for a large class of problems, direct use of small-bias spaces leads to inefficient NC algorithms, and present a way to combine them with the method of conditional probabilities to get significantly better NC algorithms. <p> NC algorithms of complexity ((n + m); log 2 n) are known for this problem (see, e.g., <ref> [20] </ref>). Small-bias spaces can be used to obtain an NC 1 algorithm but with a processor complexity of fi (m 3 log 2 n). Similarly, direct use of pairwise independent sample spaces leads to an NC 1 algorithm, but with a processor complexity of mn. <p> Given a graph G = (V; E) with jV j = n, jEj = m and having a maximum degree , a ( + 1)-vertex coloring of the graph can be computed by an ((n + m); log 3 n log log n) algorithm <ref> [20] </ref>; an improved ((n + m)= log log n; log 2 n log log n) algorithm that runs on the CREW PRAM, has been devised by Han [13]. <p> Some of these problems can also be solved using the above-seen general framework; however, the solutions we give here are better than those obtained as instances of the general framework. 4.1 The Profit/Cost problem The first application we describe is related to the "General Pairs Benefit Problem" of <ref> [20] </ref>, which was called the "General PROFIT/COST Problem" in the conference version of [20] (Proc. IEEE Symposium on Foundations of Computer Science, pages 162-173, 1988). This general problem, which models problems such as ( + 1)-vertex coloring of graphs [20], requires a few details that are not pertinent to our work <p> general framework; however, the solutions we give here are better than those obtained as instances of the general framework. 4.1 The Profit/Cost problem The first application we describe is related to the "General Pairs Benefit Problem" of <ref> [20] </ref>, which was called the "General PROFIT/COST Problem" in the conference version of [20] (Proc. IEEE Symposium on Foundations of Computer Science, pages 162-173, 1988). This general problem, which models problems such as ( + 1)-vertex coloring of graphs [20], requires a few details that are not pertinent to our work ( here refers to the maximum degree of a given graph). <p> we describe is related to the "General Pairs Benefit Problem" of <ref> [20] </ref>, which was called the "General PROFIT/COST Problem" in the conference version of [20] (Proc. IEEE Symposium on Foundations of Computer Science, pages 162-173, 1988). This general problem, which models problems such as ( + 1)-vertex coloring of graphs [20], requires a few details that are not pertinent to our work ( here refers to the maximum degree of a given graph). It is shown in [20] that such details, e.g., the existence of suitable "pessimistic estimators", can be abstracted away, leading to the following problem, which we also focus <p> This general problem, which models problems such as ( + 1)-vertex coloring of graphs <ref> [20] </ref>, requires a few details that are not pertinent to our work ( here refers to the maximum degree of a given graph). It is shown in [20] that such details, e.g., the existence of suitable "pessimistic estimators", can be abstracted away, leading to the following problem, which we also focus upon. Our presentation is slightly different from, but essentially equivalent to, that of [20]. <p> It is shown in <ref> [20] </ref> that such details, e.g., the existence of suitable "pessimistic estimators", can be abstracted away, leading to the following problem, which we also focus upon. Our presentation is slightly different from, but essentially equivalent to, that of [20]. We are given a graph G = (V; E), with jV j = n and jEj = m. <p> Note that we can assume that the labels `(v) are uniform and pairwise independent over f0; 1g q , to compute avgbenefit. The strategy used in <ref> [20] </ref> is to set one bit of each of the labels ^ `(v) per stage without using too many processors. This results in an NC algorithm using O (n + m) processors and running in O (q (t + log 2 n)) time. <p> As instantiations of this general framework we can derive the following as corollaries, via Theorem 6 and some results from <ref> [20] </ref> and [25]. Recall that any graph with maximum degree can be colored using + 1 colors. The first linear-processor NC algorithm for ( + 1)-coloring was presented in [20], with a running time of O (log 3 n log log n). It is shown in [20] that: (a) the General <p> As instantiations of this general framework we can derive the following as corollaries, via Theorem 6 and some results from <ref> [20] </ref> and [25]. Recall that any graph with maximum degree can be colored using + 1 colors. The first linear-processor NC algorithm for ( + 1)-coloring was presented in [20], with a running time of O (log 3 n log log n). It is shown in [20] that: (a) the General Profit/Cost problem can be solved using O (n + m) processors in O (q (t + log n log log n) + log 2 n log log n) time, <p> and some results from <ref> [20] </ref> and [25]. Recall that any graph with maximum degree can be colored using + 1 colors. The first linear-processor NC algorithm for ( + 1)-coloring was presented in [20], with a running time of O (log 3 n log log n). It is shown in [20] that: (a) the General Profit/Cost problem can be solved using O (n + m) processors in O (q (t + log n log log n) + log 2 n log log n) time, and that: (b) ( + 1)-coloring can be solved by a (P (n; m); log n (log
Reference: [21] <author> R. Motwani, J. Naor, and M. Naor, </author> <title> The probabilistic method yields deterministic parallel algorithms, </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 49 </volume> <pages> 478-516, </pages> <year> 1994. </year>
Reference-contexts: His method has been used and extended to derive NC algorithms for fundamental problems such as set discrepancy <ref> [8, 21] </ref>. The motivation for our first method is similar to that of [20]. <p> Spencer [29] gave a polynomial-time algorithm to find an assignment with disc () = O ( p NC 3 algorithms to find an assignment with disc () = O (s 1=2+ffi p log n), for any fixed ffi &gt; 0, were first given in <ref> [8, 21] </ref> and small-bias spaces were used to do this in NC 1 [22], but with (n 3+2=ffi ) processors. <p> We wish to find a sample point X 0 2 f0; 1g n deterministically such that X 0 E [X]. This general framework, which models the derandomization of many RNC algorithms, was introduced in [8] (see also <ref> [21] </ref>), where an (n a+b ; t log 3 n) algorithm is given (here we assume that each f i is computable in O (t) time with one processor). <p> A key example of this arises in solving set discrepancy in NC, with discrepancy O ( p s log n)-a challenging open question <ref> [8, 21] </ref>. We show that if a certain small-bias space S is efficiently constructible, then our method can solve the above class of problems in NC. This would be interesting since most current methods cannot tackle a superpolynomial number of terms. However, we do not yet know if S exists. <p> It is shown in <ref> [8, 21] </ref> that if we choose k = 2dlog (2n)=(ffi log s)e and choose the assignments (j); j 2 X, k-wise independently then P r ( (9i) j (S i ) 2 p n X E [((S i ) 2 ) k ] log n) k &lt; 1: (1) This is <p> The set discrepancy abstraction has been used to solve other problems such as vector balancing [28], lattice approximation [26] (see also the description in <ref> [21] </ref>) and for computing *-nets and *- approximations for range spaces with finite V C-dimension [10]. Theorem 4 implies improved NC algorithms for all these problems. Our method also yields improvements in processor complexity for algorithms which find an assignment with disc () = O ( p s log n). <p> An obvious instance of this framework is the set discrepancy problem where we wish to find an assignment with disc () = O ( p s log n) using the O (log n)-th moment method, i.e., using variables which are O (log n) wise independent <ref> [8, 21] </ref>. For this application, we have a sum of n functions of this type, with c = 1. All the currently known methods fail to solve this problem in NC since they need one processor per term and there are n fi (log c n) terms.
Reference: [22] <author> J. Naor and M. Naor, </author> <title> Small-bias probability spaces: efficient constructions and applications, </title> <journal> SIAM J. Comput., </journal> <volume> 22(4) </volume> <pages> 838-856, </pages> <year> 1993. </year>
Reference-contexts: Motivated by the fact that randomized algorithms are usually robust to small changes in the probabilities, Naor & Naor observed that it often suffices if the sample space is almost k-wise independent <ref> [22] </ref>, captured by Definition 2 (a) A sample space S f0; 1g n is said to be *-approximate if when ~ X = (X 1 ; : : :; X n ) is sampled uniformly from S, for all I [n] and for all b 1 ; : : :; b <p> The important work of <ref> [22] </ref> actually presents efficient constructions of "*-biased" sample spaces S f0; 1g n (to be defined in x2), and shows that they are *-approximate; thus we have efficient constructions of almost k-wise independent spaces that are often much smaller than their k-wise independent counterparts. <p> find an assignment with disc () = O ( p NC 3 algorithms to find an assignment with disc () = O (s 1=2+ffi p log n), for any fixed ffi &gt; 0, were first given in [8, 21] and small-bias spaces were used to do this in NC 1 <ref> [22] </ref>, but with (n 3+2=ffi ) processors. We derive an NC 1 algorithm with processor complexity O (n 1+ffi 0 +2=ffi ) for any fixed ffi 0 &gt; 0 which is an improvement of (n 2ffi 0 ) in the processor complexity. <p> can be used here to obtain O (t log n) time NC algorithms, but the processor complexity would be (n 3a+2b log 2 n). - 3 - The problem of finding a heavy codeword, which generalizes the classical problem of finding a large cut in a graph, was introduced in <ref> [22] </ref>. Given a matrix A 2 Z mfin 2 , no row of which has only zeroes, the problem is to find an x 2 Z n 2 with Ax (over Z 2 ) having at least m 2 ones. Small-bias sample spaces are used in [22] to obtain a (minfm <p> graph, was introduced in <ref> [22] </ref>. Given a matrix A 2 Z mfin 2 , no row of which has only zeroes, the problem is to find an x 2 Z n 2 with Ax (over Z 2 ) having at least m 2 ones. Small-bias sample spaces are used in [22] to obtain a (minfm 4 n 2 ; m 3 n 3 g, log (m + n)) algorithm, thus placing the problem in NC. <p> The biases of a sample space S with respect to the vectors ff 2 Z n 2 are the Fourier coefficients of the probability distribution D S induced on f0; 1g n by sampling uniformly from S <ref> [22] </ref>. The Fourier bases are the functions f ff , where f ff (x) = (1) &lt; ff; x &gt; for x 2 f0; 1g n . <p> NC 1 constructions of sample spaces of size O (n=* 3 ) with Fourier coefficients bounded in magnitude by * were developed in [2], improving on those of <ref> [22] </ref>. NC 1 constructions of three different sample spaces of size O (n 2 =* 2 ), were presented by [3]. <p> Similar definitions can be made for sample spaces that are k-wise *-approximate. A sample space S is said to be k-wise *-biased if jbias S (ff)j * for all nonzero ff 2 Z n 2 with wt (ff) k. A k-wise *-biased space is k-wise *-approximate <ref> [22] </ref>. Sample spaces of size O (minf k logn * 3 ; * 2 g) which are k-wise *-biased can be constructed in NC 1 [22, 3, 2]. If S 0 is k-wise * 0 -biased in Lemma 1, then the resulting space S is k-wise *-biased. <p> A k-wise *-biased space is k-wise *-approximate [22]. Sample spaces of size O (minf k logn * 3 ; * 2 g) which are k-wise *-biased can be constructed in NC 1 <ref> [22, 3, 2] </ref>. If S 0 is k-wise * 0 -biased in Lemma 1, then the resulting space S is k-wise *-biased. <p> on n vertices and m edges, with maximum degree . ffi; ffi 0 ; ffi 00 ; ffi 1 ; ffi 2 etc. denote arbitrarily small positive constants. 3.1 Cuts in Graphs and the heavy codeword problem As a straightforward application of our method, we consider the heavy codeword problem <ref> [22] </ref>: given A 2 Z mfin 2 , find an x 2 Z n 2 such that Ax (over Z 2 ) has at least m 2 ones. In the following a i denotes the ith row of A, and we assume that the a i 's are nonzero. <p> In the following a i denotes the ith row of A, and we assume that the a i 's are nonzero. As pointed out in <ref> [22] </ref>, if x is picked from an *-biased space S with * = 1 2m , then E [wt (Ax)] = i=1 m X 1 bias (&lt; a i ; x &gt;) 2 m (1=2 *=2) and thus, there exists an x 2 S with wt (Ax) dm (1=2 *=2)e = <p> It is shown in <ref> [22] </ref> that (1) holds even if the assignments (j); j 2 S are k-wise *-biased for * &lt; 1=(2n 1+1=ffi ), leading to an ((n 3+ 2 ffi ); log n) algorithm. <p> : : : ; a jIj in [0; m 1], jP r ~ Y 2S ( i2I (Y i = a i )) i2I P r D (X i = a i )j *. (The case m = 2 with D being the uniform distribution is handled by small-bias spaces <ref> [22, 3, 2] </ref>.) We are interested in such approximating sample spaces that are "small" and efficiently constructible.
Reference: [23] <author> N. Nisan, </author> <title> Extracting Randomness: How and Why, </title> <booktitle> in Proc. IEEE Conference on Computational Complexity (formerly "Structure in Complexity Theory"), </booktitle> <pages> pages 44-58, </pages> <year> 1996. </year>
Reference-contexts: For instance, another major derandomization approach, which uses dispersers and extractors that we do not define here, also uses small-bias spaces: see, e.g., the survey of Nisan <ref> [23] </ref>. A third key approach to derandomization, the method of conditional probabilities, can be described informally as follows. Suppose a randomized algorithm chooses n random bits ~ X = (X 1 ; : : : ; X n ) and outputs a function f ( ~ X ).
Reference: [24] <author> N. Nisan, E. Szemeredi, and A. Wigderson, </author> <title> Undirected connectivity in O(log 1:5 n) space, </title> <booktitle> in Proc. IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 24-29, </pages> <year> 1992. </year>
Reference-contexts: 1 Introduction Derandomization, the development of general tools to derive efficient deterministic algorithms from their randomized counterparts, has blossomed greatly in the last decade. The best-known deterministic solutions for several classical problems such as testing undirected graph connectivity within limited space bounds, take this approach <ref> [24] </ref>. Two motivations for research in this area are the nonavailability of "perfect" random sources, and the need for absolute (non-probabilistic) guarantees of correctness in, say, critical applications.
Reference: [25] <author> A. Panconesi and A. Srinivasan, </author> <title> The local nature of -colorings and its algorithmic applications, </title> <journal> Combinatorica, </journal> <volume> 15 </volume> <pages> 255-280, </pages> <year> 1995. </year>
Reference-contexts: We improve these time complexities by presenting an ((n + m)n ffi ; log 2 n) algorithm where ffi &gt; 0 is any constant. This gives faster algorithms for other problems such as the much harder -vertex coloring problem <ref> [25] </ref>. We use similar ideas to derive a faster algorithm for approximating the maximum acyclic subgraph problem [9], incurring a small processor penalty. Thus, our first method yields the fastest known NC algorithms for a large class of problems, without a big processor penalty. <p> As instantiations of this general framework we can derive the following as corollaries, via Theorem 6 and some results from [20] and <ref> [25] </ref>. Recall that any graph with maximum degree can be colored using + 1 colors. The first linear-processor NC algorithm for ( + 1)-coloring was presented in [20], with a running time of O (log 3 n log log n). <p> Recall that a connected graph is vertex-colorable with colors iff it is neither an odd cycle nor a complete graph. The proof of Theorem 4 of <ref> [25] </ref> shows that if there is a P (n; m)-processor NC algorithm for ( + 1)-coloring that runs in O (T (n; m)) time, then we can find a -coloring in NC using P (n; m) processors in O ((log n + T (n; m)) log 2 n= log ) time; <p> there is a P (n; m)-processor NC algorithm for ( + 1)-coloring that runs in O (T (n; m)) time, then we can find a -coloring in NC using P (n; m) processors in O ((log n + T (n; m)) log 2 n= log ) time; see x5.1 in <ref> [25] </ref>.
Reference: [26] <author> P. Raghavan, </author> <title> Probabilistic construction of deterministic algorithms: approximating packing integer programs, </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 37 </volume> <pages> 130-143, </pages> <year> 1988. </year>
Reference-contexts: We derive an NC 1 algorithm with processor complexity O (n 1+ffi 0 +2=ffi ) for any fixed ffi 0 &gt; 0 which is an improvement of (n 2ffi 0 ) in the processor complexity. This yields improved algorithms for problems like vector-balancing [28], lattice approximation <ref> [26] </ref>, and computing *-nets and *-approximations for range spaces with finite V C-dimension [10]. Set balancing is a special case of the following general problem. <p> The set discrepancy abstraction has been used to solve other problems such as vector balancing [28], lattice approximation <ref> [26] </ref> (see also the description in [21]) and for computing *-nets and *- approximations for range spaces with finite V C-dimension [10]. Theorem 4 implies improved NC algorithms for all these problems.
Reference: [27] <author> J. P. Schmidt, A. Siegel, and A. Srinivasan, </author> <title> Chernoff-Hoeffding bounds for applications with limited independence, </title> <journal> SIAM J. Discrete Math., </journal> <volume> 8 </volume> <pages> 223-250, </pages> <year> 1995. </year> <month> - 20 </month> - 
Reference-contexts: We now use the fact that maxfb 1 ; b 2 g jb 1 b 2 j + b 2 to complete the proof. 2 We next present a useful lemma, whose proof closely follows that of Theorem 2 of [11] (see also Theorem 2.6 in <ref> [27] </ref>): Lemma 3 Let X 1 ; X 2 ; : : : ; X t be independent binary random variables with P r (X i = 1) = p i ; let Z 1 ; Z 2 ; : : : ; Z t be arbitrary binary random variables.
Reference: [28] <author> J. H. Spencer, </author> <title> Balancing vectors in the max norm, </title> <journal> Combinatorica, </journal> <volume> 6 </volume> <pages> 55-65, </pages> <year> 1986. </year>
Reference-contexts: We derive an NC 1 algorithm with processor complexity O (n 1+ffi 0 +2=ffi ) for any fixed ffi 0 &gt; 0 which is an improvement of (n 2ffi 0 ) in the processor complexity. This yields improved algorithms for problems like vector-balancing <ref> [28] </ref>, lattice approximation [26], and computing *-nets and *-approximations for range spaces with finite V C-dimension [10]. Set balancing is a special case of the following general problem. <p> The set discrepancy abstraction has been used to solve other problems such as vector balancing <ref> [28] </ref>, lattice approximation [26] (see also the description in [21]) and for computing *-nets and *- approximations for range spaces with finite V C-dimension [10]. Theorem 4 implies improved NC algorithms for all these problems.
Reference: [29] <author> J. H. Spencer, </author> <title> Ten Lectures on the Probabilistic Method, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1987. </year>
Reference-contexts: Spencer <ref> [29] </ref> gave a polynomial-time algorithm to find an assignment with disc () = O ( p NC 3 algorithms to find an assignment with disc () = O (s 1=2+ffi p log n), for any fixed ffi &gt; 0, were first given in [8, 21] and small-bias spaces were used to
References-found: 29

