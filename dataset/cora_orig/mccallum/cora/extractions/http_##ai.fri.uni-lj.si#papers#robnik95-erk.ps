URL: http://ai.fri.uni-lj.si/papers/robnik95-erk.ps
Refering-URL: http://ai.fri.uni-lj.si/papers/index.html
Root-URL: 
Email: fMarko.Robnik, Igor.Kononenkog@fer.uni-lj.si  
Phone: tel.: +386 61 1768-386, fax: +386 61 264-990  
Title: Discretization of continuous attributes using ReliefF  
Author: Marko Robnik Sikonja Igor Kononenko 
Address: Trzaska 25, 61001 Ljubljana, Slovenia  
Affiliation: University of Ljubljana Faculty of Electrical Engineering and Computer Science  
Abstract: Many existing learning algorithms expect the attributes to be discrete. Discretization of continuous attributes might be difficult task even for domain experts. We have tried the non-myopic heuristic measure ReliefF for discretization and compared it with well known dissimilarity measure and discretizations by experts. An extensive testing with several learning algorithms on six real world databases has shown that none of the discretizations has clear advantage over the others. 
Abstract-found: 1
Intro-found: 1
Reference: [2] <author> Cestnik, B., Kononenko, I., Bratko, I.: </author> <title> ASSISTANT 86: A Knowledge-Elicitation Tool for Sophisticated Users. </title> <editor> In: Ivan Bratko, Nada Lavrac (eds.): </editor> <booktitle> Progress in Machine Learning, Proceedings of EWSL 87. </booktitle> <address> Wilmslow, </address> <publisher> Sigma Press, </publisher> <year> 1987. </year>
Reference-contexts: All the algorithms except the Semi-naive Bayesian classifier are top-down decision tree builders equipped with pruning and handling of missing values similar to Assistant`s <ref> [2] </ref>. We have reimplemented [12] LFC (Lookahead Feature Construction) [10], an algorithm who, at each node of the tree, joins attributes with conjunction, disjunction and negation and uses these constructs instead of original attributes to split the learning set.
Reference: [3] <author> Cestnik, B.: </author> <title> Informativity-Based Splitting of Numerical Attributes into Intervals. </title> <booktitle> In Hamza, M.H.(ed):Expert Systems Theory & Applications, Proc. of the IASTED International Symposium,, </booktitle> <publisher> Acta Press, </publisher> <year> 1989 </year>
Reference-contexts: We have used ReliefF to guide the discretization process and compared the results with another dis-cretization which uses attribute dissimilarity <ref> [9, 3] </ref>. <p> We have tested both measures with several machine learning algorithms and databases. 2 Discretization algorithm We have used the following greedy algorithm for discretization of attributes (similar algorithm was used by <ref> [3] </ref>): BestDiscretization = fg SetOfBoundaries = fg repeat add the split which maximizes the heuristic measure to the SetOfBoundaries if SetOfBoundaries is best so far then BestDiscretization = SetOfBoundaries until n times the heuristic is worse then in previous step At each step the algorithm searches for the value which, when
Reference: [4] <author> Kira K., Rendell L.: </author> <title> A practical approach to feature selection. </title> <booktitle> In Proceedings of the Machine Learning Conference, </booktitle> <address> Aberdeen, </address> <year> 1992, </year> <pages> pp. 250-256 </pages>
Reference-contexts: They differ about the measure they use to estimate the closeness. The heuristic measure we use is ReliefF [6] (an extension of Relief <ref> [4] </ref>) which is a powerful attribute estimator performing well on artificial as well as real world problems [8]. We have used ReliefF to guide the discretization process and compared the results with another dis-cretization which uses attribute dissimilarity [9, 3]. <p> We compared performance of two heuristic estimates. ReliefF [6] is an extension of Relief <ref> [4] </ref> which can deal with missing values and multivalued classes and also extends the power of estimation.
Reference: [5] <author> Kononenko, I.: </author> <title> Semi-Naive Bayesian Classifier, </title> <booktitle> Proc. of EWSL 91, Porto, </booktitle> <address> Portugal, March 6-8, </address> <year> 1991, </year> <month> pp.206-213 </month>
Reference-contexts: It has an additional constructive operator (equality). The naive Bayesian classifier presumes independence of attributes and therefore uses the naive Bayesian formula to predict classes of unseen examples. The Semi-Naive Bayesian classifier <ref> [5] </ref> is its extension: it tries to join certain values of attributes into new attributes, hoping to capture crucial dependencies. Formation of new attributes is a kind of constructive induction.
Reference: [6] <author> Kononenko, I.: </author> <title> Estimating attributes: analysis and extensions of RELIEF. </title> <booktitle> Proc. of European Conf. on Machine Learning, </booktitle> <publisher> Springer Verlag, </publisher> <year> 1994. </year>
Reference-contexts: They differ about the measure they use to estimate the closeness. The heuristic measure we use is ReliefF <ref> [6] </ref> (an extension of Relief [4]) which is a powerful attribute estimator performing well on artificial as well as real world problems [8]. We have used ReliefF to guide the discretization process and compared the results with another dis-cretization which uses attribute dissimilarity [9, 3]. <p> We compared performance of two heuristic estimates. ReliefF <ref> [6] </ref> is an extension of Relief [4] which can deal with missing values and multivalued classes and also extends the power of estimation.
Reference: [7] <author> Kononenko, I., Bratko, I.: </author> <title> Information based evaluation criterion for classifier's performance. </title> <booktitle> Machine Learning 6, </booktitle> <year> 1991, </year> <pages> pp. 67-80 </pages>
Reference-contexts: For each split we have used the training set to perform discretization and learning, then we tested on testing set. The results are averaged over ten runs. We compared discretizations and learning methods in terms of the prediction accuracy and the information score <ref> [7] </ref>.

Reference: [9] <author> Mantaras R.L.: </author> <title> ID3 revisited: A distance based criterion for attribute selection. </title> <booktitle> Proc. Int. Symp. Methodologies for Intelligent Systems, </booktitle> <address> Charlotte, North Carolina, USA, </address> <month> October </month> <year> 1989 </year>
Reference-contexts: We have used ReliefF to guide the discretization process and compared the results with another dis-cretization which uses attribute dissimilarity <ref> [9, 3] </ref>. <p> These two probabilities for different near surroundings are combined into a single estimate with range from -1 to 1. The larger the estimate the more significant the attribute. For comparison to our novel approach we have used a well known measure of dissimilarity between attribute and class <ref> [9] </ref>. It is defined as: D (A; C) = H (A&C) where H (X) represents entropy of variable X. It is defined as H (X) = x where x runs over all possible values of X.
Reference: [10] <author> Ragavan H., Rendell L.: </author> <title> Lookahead Feature Construction for Learning Hard Concepts. </title> <booktitle> Proceedings of the Tenth International Machine Learning Conference, </booktitle> <year> 1993, </year> <pages> pp. 252-259 </pages>
Reference-contexts: All the algorithms except the Semi-naive Bayesian classifier are top-down decision tree builders equipped with pruning and handling of missing values similar to Assistant`s [2]. We have reimplemented [12] LFC (Lookahead Feature Construction) <ref> [10] </ref>, an algorithm who, at each node of the tree, joins attributes with conjunction, disjunction and negation and uses these constructs instead of original attributes to split the learning set.
Reference: [11] <author> Richeldi M., Rossotto M.: </author> <title> Class-Driven Statistical Discretization of Continuous Attributes. </title> <editor> In Lavrac N., Wrobel S.: </editor> <booktitle> Machine Learning: </booktitle> <address> ECML-95, </address> <publisher> Springer 1995 </publisher>
Reference-contexts: As they ignore class of exam ples when partitioning them into different intervals, the classification algorithm might no longer be able to separate examples of different classes. The essential information may be lost and consequently the method performs poorly in many situations <ref> [11] </ref>. * Class aware methods try to partition the examples into intervals that most closely correspond to the concept. They differ about the measure they use to estimate the closeness.
Reference: [12] <author> Robnik, M.: Konstruktivna in-dukcija z odlocitvenimi drevesi. Diplomska naloga. Univerza v Ljubljani, FER, Ljubljana, </author> <year> 1993 </year>
Reference-contexts: All the algorithms except the Semi-naive Bayesian classifier are top-down decision tree builders equipped with pruning and handling of missing values similar to Assistant`s [2]. We have reimplemented <ref> [12] </ref> LFC (Lookahead Feature Construction) [10], an algorithm who, at each node of the tree, joins attributes with conjunction, disjunction and negation and uses these constructs instead of original attributes to split the learning set.
Reference: [13] <editor> Robnik, M.: Konstruktivna indukcija v strojnem ucenju. Elektrotehniski vestnik, </editor> <volume> Vol. 62(1) </volume> <pages> 43-49 </pages>
Reference-contexts: With the help of automatic procedure the task becomes easier. Discretization of attributes can reduce the learning complexity and help to understand the dependence between attributes and the target concept. Also, in the constructive induction process, the discretized attributes are needed for most operators <ref> [13] </ref>. There are several methods we can use to discretize an attribute. We can divide them into two groups. * Class blind techniques (equal width, uniform frequency, maximal marginal entropy) are easy to implement, but require user to specify the number of intervals.
References-found: 11

