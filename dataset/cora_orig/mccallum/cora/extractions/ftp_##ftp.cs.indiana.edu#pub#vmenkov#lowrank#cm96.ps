URL: ftp://ftp.cs.indiana.edu/pub/vmenkov/lowrank/cm96.ps
Refering-URL: http://www.cs.indiana.edu/hyplan/vmenkov/resume.html
Root-URL: http://www.cs.indiana.edu
Title: SOLVING BLOCK LINEAR SYSTEMS WITH LOW-RANK OFF-DIAGONAL BLOCKS IS EASILY PARALLELIZABLE  
Author: V. ME NKOV 
Address: BLOOMINGTON  
Affiliation: DEPARTMENT OF COMPUTER SCIENCE INDIANA UNIVERSITY  
Abstract: An easily and efficiently parallelizable direct method is given for solving a block linear system Bx = y, where B = D + Q is the sum of a non-singular block diagonal matrix D and a matrix Q with low-rank blocks. This implicitly defines a new preconditioning method with an operation count close to the cost of calculating a matrix-vector product Qw for some w, plus at most twice the cost of calculating D 1 w for some w. When implemented on a parallel machine the processor utilization can be as good as that of those operations. Order estimates are given for the general case, and an implementation is compared to block SSOR preconditioning. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> F. Alvarado, </author> <title> Ordering schemes for partitioned sparse inverses, 1989. SIAM Symposium on Sparse Matrices, </title> <address> Salishan Lodge, Gleneden Beach, OR, </address> <month> May 22-24 </month> <year> 1989. </year>
Reference-contexts: One way to recover parallelism is to increase the number of blocks in the partitioning of A and use a level scheduling <ref> [1, 2] </ref>. however, this also causes the quality of preconditioning to fall, and in the limiting case becomes pointwise symmetrized Gauss-Seidel, which typically provides poor quality of preconditioning. In this paper we borrow an idea from the solution of integral equations, and use low-rank approximations (LRA) of the off-diagonal blocks.
Reference: [2] <author> E. Anderson, </author> <title> Parallel implementation of preconditioned conjugate gradient methods for solving sparse systems of linear equations, </title> <type> Master's thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomputing Res. & Dev., </institution> <year> 1988. </year>
Reference-contexts: One way to recover parallelism is to increase the number of blocks in the partitioning of A and use a level scheduling <ref> [1, 2] </ref>. however, this also causes the quality of preconditioning to fall, and in the limiting case becomes pointwise symmetrized Gauss-Seidel, which typically provides poor quality of preconditioning. In this paper we borrow an idea from the solution of integral equations, and use low-rank approximations (LRA) of the off-diagonal blocks.
Reference: [3] <author> R. Bramley and V. </author> <title> Me ~ nkov, Low rank off-diagonal block preconditioners for solving sparse linear systems on parallel computers, </title> <type> Tech. Rep. 446, </type> <institution> Department of Computer Science, Indiana University, Bloomington, </institution> <note> IN, 1996. (To be submitted). </note>
Reference-contexts: In this paper we borrow an idea from the solution of integral equations, and use low-rank approximations (LRA) of the off-diagonal blocks. In <ref> [3] </ref> the characteristics of such an approximation are analyzed; this paper shows how such a preconditioner can be implemented with parallelism almost as good as that of block diagonal preconditioning. <p> BSSOR LOB (C = (Q L + D)D 1 (Q U + D). Blocks of the strict lower and upper triangular Q L and Q U are obtained by a lumping-based low-rank ap proximation method <ref> [3] </ref> of the respective blocks of A, with rank Q kl 3). The preconditioner is applied using a conventional LU-solve method. 2. Direct LOB (C = D + (Q L + Q U ), with the same Q L and Q U as above), applied using the SMW method. 3.
Reference: [4] <author> H. V. der Vorst, </author> <title> Bi-CGSTAB: A fast and smoothly converging variant of Bi-CG for the solution of nonsymmetric linear systems, </title> <journal> SIAM Journal of Scientific and Statistical Computing, </journal> <volume> 13 (1992), </volume> <pages> pp. 631-644. </pages>
Reference-contexts: Preliminary computational results. The direct LOB preconditioner, ap plied using the SMW-based algorithm in conjugate gradients stabilized <ref> [4] </ref> iteration was implemented in pC++ [5] on an SGI Power Challenge. Only sequential solving of (I + G)s = t was implemented in this version. Two test problems with p = 8, coming from a 2D finite element problem in CFD, were solved using a number of preconditioners: 1.
Reference: [5] <author> D. Gannon, S. X. Yang, and P. Beckman, </author> <title> User Guide for a Portable Parallel C++ Programming System, pC++, </title> <institution> Department of Computer Science and CICA, Indiana University, Bloomington, </institution> <note> IN, 1994. Available via World Wide Web at http://www.extreme.indiana.edu/sage/pcxx ug/pcxx ug.html. </note>
Reference-contexts: Preliminary computational results. The direct LOB preconditioner, ap plied using the SMW-based algorithm in conjugate gradients stabilized [4] iteration was implemented in pC++ <ref> [5] </ref> on an SGI Power Challenge. Only sequential solving of (I + G)s = t was implemented in this version. Two test problems with p = 8, coming from a 2D finite element problem in CFD, were solved using a number of preconditioners: 1.
Reference: [6] <author> A. George and W.-H. Liu, </author> <title> The Computer Solution of Large Sparse Positive Definite Systems, </title> <publisher> Prentice-Hall, </publisher> <address> Engelwood Cliffs, NJ, </address> <year> 1981. </year>
Reference-contexts: The equation for H jkl shows that the block G kl can be non-zero only if U kl is non-zero, which, in its turn, can be non-zero only if Q kl is non-zero. 2 If we use no-cancellation assumption <ref> [6, Section 2.2.2] </ref> (i.e., disregard the possibility that a potentially non-zero element becomes zero simply because a given dot product happens to be zero), it follows from Proposition 4.1 that G has the same non-zero block structure as Q. Corollary 4.2.
Reference: [7] <author> B. Hendrickson and R. Leland, </author> <title> The Chaco User's Guide Version 1.0, </title> <type> Tech. Rep. SAND 93-2339, </type> <institution> Sandia National Laboratories, </institution> <address> Albuquerque, N.M., </address> <month> October </month> <year> 1993. </year> <title> [8] , An Improved Spectral Graph Partitioning Algorithm for Mapping Parallel Computations, </title> <journal> SIAM Journal on Scientific Computing, </journal> <year> (1995), </year> <pages> pp. 452-469. </pages>
Reference-contexts: In all preconditioners, blocks of D were obtained by an incomplete LU-factorization [9] of the diagonal blocks of A with 3 levels of fill allowed. The system was first 8 partitioned into eight subdomains using a spectral method from Chaco <ref> [7, 8] </ref>.
Reference: [9] <author> J. Meijerink and H. van der Vorst, </author> <title> An iterative solution method for linear systems of which the coefficient matrix is a symmetric M-matrix, </title> <journal> Math. Comp., </journal> <volume> 31 (1977), </volume> <pages> pp. 148-162. </pages>
Reference-contexts: Direct LOB preconditioner with original ODBs (C = D + (L + U ) with the same L and U as above), applied using the SMW-based method. In all preconditioners, blocks of D were obtained by an incomplete LU-factorization <ref> [9] </ref> of the diagonal blocks of A with 3 levels of fill allowed. The system was first 8 partitioned into eight subdomains using a spectral method from Chaco [7, 8].
Reference: [10] <author> J. M. Ortega and W. C. Rheinboldt, </author> <title> Iterative Solution of Nonlinear Equations in Several Variables, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1970. </year>
Reference-contexts: Block columns in (7) and (8) corresponding to zero blocks Q kl are absent (have no columns). Since B = D + U V T , the Sherman-Morrison-Woodbury formula <ref> [10] </ref> gives B 1 = (D + U V T ) 1 = D 1 D 1 U (I + G) 1 V T D 1 ;(9) with G = V T D 1 U of order M .

References-found: 9

