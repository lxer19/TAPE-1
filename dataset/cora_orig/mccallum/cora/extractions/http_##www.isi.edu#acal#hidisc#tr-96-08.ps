URL: http://www.isi.edu/acal/hidisc/tr-96-08.ps
Refering-URL: http://www.isi.edu/acal/hidisc/papers.html
Root-URL: http://www.isi.edu
Email: crago@isi.edu  despain@isi.edu  
Phone: (310) 822-1511 ext. 713  (310) 822-1511 ext. 377  
Title: Improving the Performance of Loop-Based Programs Using a Prefetch Processor  
Author: Stephen P. Crago Alvin M. Despain 
Date: November 8, 1996  
Address: 4676 Admiralty Way Marina del Rey, CA 90292-6695  
Affiliation: University of Southern California Information Sciences Institute  
Abstract-found: 0
Intro-found: 1
Reference: [AgCo87] <author> T.Y. Agawala and J. Cocke. </author> <title> High Performance Reduced Instruction Set Processors. </title> <institution> IBM T.J. Watson Research Center, </institution> <type> Technical Report #55845, </type> <month> March </month> <year> 1987. </year>
Reference-contexts: As transistors continue to shrink and die sizes get larger, computer architects must determine how to best use the larger number of available transistors [FaTP94]. Processors have recently used transistors to provide multiple functional units, using the VLIW [Fish83] or superscalar <ref> [AgCo87] </ref> paradigm, and are deeply pipelined to increase the clock speed. While superscalar, VLIW, and deep pipelining can all provide additional microparallelism, processor performance is limited by memory performance, diminishing the returns of additional micro-parallelism within uniprocessors.
Reference: [BaCh91] <author> J.-L. Baer and T.-F. Chen. </author> <title> An effective on-chip preloading scheme to reduce data access penalty. </title> <booktitle> Proceedings of Supercomputing 91, </booktitle> <year> 1991. </year>
Reference: [Brig74] <author> E.O. Brigham. </author> <title> The Fast Fourier Transform, </title> <publisher> Prentice-Hall, Inc., </publisher> <year> 1974, </year> <note> p. 165. </note>
Reference-contexts: The simulator does not model cache or memory contention. If sufficient memory bandwidth is not provided, no processor architecture will provide significant speedup. The benchmarks we used are C versions of the first five Livermore Loops [McMa72], discrete convolution, and the bit-reverse access pattern used in the FFT <ref> [Brig74] </ref>. The benchmarks were compiled on a MIPS R5000-based SGI Indy running IRIX 5.3. We compiled using the cc compiler with the -O3 -sopt and -non_shared options. Loop unrolling was performed on all loops. We use, as points for comparison, a single-issue uniprocessor with and without prefetching.
Reference: [Chen95] <author> T.-F. Chen. </author> <title> An effective programmable prefetch engine for on-chip caches. </title> <address> Micro-28, </address> <month> November </month> <year> 1995, </year> <pages> pp. 237-242. </pages>
Reference: [Denn74] <author> R. Dennard et al. </author> <title> Design of Ion-Implanted MOSFETs with very small physical dimensions. </title> <journal> IEEE Journal of Solid-State Circuits, v. CS-9, </journal> <volume> No. 5, </volume> <month> Oct. </month> <year> 1974. </year>
Reference-contexts: 1. Introduction The gap between processor speeds and DRAM speeds is increasing at an exponential rate [WuMc94]. Transistor sizes have been shrinking and die sizes have been growing steadily for many years [Sano94]. These trends have allowed processors to get faster <ref> [Denn74] </ref> and more complex. Processors have used these additional transistors by adding multiple functional units and highly pipelined modules. As transistors continue to shrink and die sizes get larger, computer architects must determine how to best use the larger number of available transistors [FaTP94].
Reference: [FaTP94] <author> M. Farrens, G. Tyson, and A.R. Pleszkun. </author> <title> A Study of Single-Chip Processor/ Cache Organizations for Large Number of Transistors. </title> <booktitle> Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <year> 1994, </year> <pages> pp. 338-347. </pages>
Reference-contexts: Processors have used these additional transistors by adding multiple functional units and highly pipelined modules. As transistors continue to shrink and die sizes get larger, computer architects must determine how to best use the larger number of available transistors <ref> [FaTP94] </ref>. Processors have recently used transistors to provide multiple functional units, using the VLIW [Fish83] or superscalar [AgCo87] paradigm, and are deeply pipelined to increase the clock speed.
Reference: [Fish83] <author> Fisher, J.A. </author> <title> Very long instruction word architectures and ELSI-512. </title> <booktitle> Proceedings of the Tenth Symposium on Computer Architecture, </booktitle> <year> 1993, </year> <pages> pp. 140-150. </pages>
Reference-contexts: As transistors continue to shrink and die sizes get larger, computer architects must determine how to best use the larger number of available transistors [FaTP94]. Processors have recently used transistors to provide multiple functional units, using the VLIW <ref> [Fish83] </ref> or superscalar [AgCo87] paradigm, and are deeply pipelined to increase the clock speed. While superscalar, VLIW, and deep pipelining can all provide additional microparallelism, processor performance is limited by memory performance, diminishing the returns of additional micro-parallelism within uniprocessors.
Reference: [Joup90] <author> N.P. Jouppi. </author> <title> Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers. </title> <type> Technical Note TN-14, </type> <institution> Digital Western Research Laboratory, </institution> <month> March </month> <year> 1990. </year>
Reference-contexts: The data caches are 2-way set associative (except for LLL5 for which we used 4-way) to reduce the effect of prefetches conflicting with useful data, which could also be reduced using a victim cache <ref> [Joup90] </ref>. The cache sizes are small because they allow us to simulate the effects of large data sets while keeping the simulation time low. We believe that similar results would be attained with larger caches and larger data sets.
Reference: [KuHC94] <author> L. Kurian, P.T. Hulina, and L.D. Caraor. </author> <title> Memory latency effects in decoupled architectures. </title> <journal> IEEE Transactions on Computers, v. </journal> <volume> 43, no. </volume> <month> 10 (October </month> <year> 1994), </year> <pages> pp. 1129-1139. </pages>
Reference-contexts: Many applications do not have enough locality to reap the benefit of a cache. Many signal processing applications, for example, have been shown to perform worse with a cache than without one because there is a penalty associated with cache misses <ref> [KuHC94] </ref>. When each datum is used only once, the penalty is often not offset by a sufficient benefit. Software prefetching has been proposed for hiding memory latency with less hardware support and allows more flexibility than hardware prefetching.
Reference: [Lam88] <author> M.S. Lam. </author> <title> Software pipelining: an effective scheduling technique for VLIW machines. </title> <booktitle> Proceedings of the SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <year> 1988, </year> <pages> pp. 318-328. </pages>
Reference-contexts: Techniques such as tiling [Wolf92] try to increase locality by making compile time transformations. These techniques are limited to scientific programs which exhibit simple access behavior. In our architecture we can use these techniques as well as others which would not be possible by complier analysis alone. Software pipelining <ref> [Lam88] </ref> is a technique to expose more parallelism so that computation can be overlapped. Our architecture also overlaps computation but is not limited to statically determined instructions in loops as it is in software pipelining. Compiler techniques are static in nature and therefore work only in a limited domain.
Reference: [McMa72] <author> F.H. McMahon. </author> <title> Fortran CPU Performance Analysis. </title> <institution> Lawrence Livermore Laboratories, </institution> <year> 1972. </year>
Reference-contexts: The simulator models a prefetch buffer that can hold 20 entries. The simulator does not model cache or memory contention. If sufficient memory bandwidth is not provided, no processor architecture will provide significant speedup. The benchmarks we used are C versions of the first five Livermore Loops <ref> [McMa72] </ref>, discrete convolution, and the bit-reverse access pattern used in the FFT [Brig74]. The benchmarks were compiled on a MIPS R5000-based SGI Indy running IRIX 5.3. We compiled using the cc compiler with the -O3 -sopt and -non_shared options. Loop unrolling was performed on all loops.
Reference: [Mips95] <institution> MIPS Technologies, Inc. </institution> <note> MIPS R10000 Microprocessor Users Manual, 1995. 16 </note>
Reference-contexts: The simulator allows overlapped execution of integer and floating point instructions and models interlocks for integer multiply and divide and the latencies and issue rates of floating point operations of the MIPS R10000 <ref> [Mips95] </ref>. The simulator models a prefetch buffer that can hold 20 entries. The simulator does not model cache or memory contention. If sufficient memory bandwidth is not provided, no processor architecture will provide significant speedup.
Reference: [MoLG92] <author> T.C. Mowry, M.S. Lam, and A. Gupta. </author> <title> Design and evaluation of a compiler algorithm for prefetching. </title> <booktitle> Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <year> 1992, </year> <pages> pp. 62-73. </pages>
Reference-contexts: Software schemes, such as software prefetching and software controlled caches, expose the memory hierarchy to the compiler. The compiler can adapt the memory hierarchy behavior to the individual program. However, software schemes incur an instruction overhead and cannot adapt to dynamic run-time behavior. Software prefetching <ref> [MoLG92] </ref> allows prefetch instructions to be inserted into the instruction stream and hides memory latency by loading data into the cache before it is needed. <p> In all cases, only prefetch instruc 11 tions were added. No additional address calculation was necessary, in contrast with many other benchmarks in which prefetching adds a significant (but not prohibitive) instruction overhead <ref> [MoLG92] </ref>. with and without prefetching. The results in Figure 5 were obtained using a data cache size of 8 KB (except for convolution and bit-reverse for which we used 1 KB).
Reference: [Papo80] <author> A. Papoulis. </author> <title> Circuits and Systems, </title> <publisher> Holt, Rinehart and Winston, Inc., </publisher> <year> 1980, </year> <note> p. 146. </note>
Reference-contexts: Table 2 shows the instruction extensions of the prefetch proces sor. 3.4 Sample Program In this section, we use the inner loop of the discrete convolution algorithm <ref> [Papo80] </ref> to illustrate Table 1: Main Processor Instruction Extension Instruction Mnemonic Description Get Slip Token GET_SLIP Consume a token from the Slip Control Queue Table 2: Prefetch Processor Instruction Extensions Instruction Mnemonic Description Prefetch PREF Prefetch data into the cache Put Slip Token PUT_SLIP Produce a token for the Slip Control
Reference: [SaPa96] <author> R.H. Saavedra and D. Park. </author> <title> Improving the effectiveness of software prefetching with adaptive execution. 1996 Parallel Architectures and Compilation Techniques, </title> <month> October </month> <year> 1996. </year>
Reference-contexts: Prefetching initiates data accesses a fixed number of instructions before the data is needed and works well only when memory behavior is predictable. <ref> [SaPa96] </ref> introduces adaptive prefetching, which allows the prefetch distance to be adjusted as a program executes. However, prefetches should not be inserted unless cache misses can be predicted because the instruction overhead will reduce performance when cache hit rates are high.
Reference: [Sano94] <author> B.J. Sano. </author> <title> Microparallel Processors. </title> <type> Ph.D. Thesis, </type> <institution> University of Southern Cali-fornia, </institution> <year> 1994. </year>
Reference-contexts: 1. Introduction The gap between processor speeds and DRAM speeds is increasing at an exponential rate [WuMc94]. Transistor sizes have been shrinking and die sizes have been growing steadily for many years <ref> [Sano94] </ref>. These trends have allowed processors to get faster [Denn74] and more complex. Processors have used these additional transistors by adding multiple functional units and highly pipelined modules.
Reference: [WaRa95] <author> I. Watson and A. Rawsthorne. </author> <title> Decoupled Pre-Fetching for distributed shared memory. </title> <booktitle> Proceedings of the 28th Annual Hawaii International Conference on System Sciences, </booktitle> <month> January </month> <year> 1995, </year> <pages> pp. 252-261. </pages>
Reference-contexts: These techniques involve solving indexing functions at compile time for regular computation on arrays in scientific programs which in practice can be very difficult. These techniques do not extend well to programs with irregular data structures such as dynamically allocated structures in symbolic pro 5 grams. <ref> [WaRa95] </ref> proposed the use of a prefetch processor to prefetch pages in the context of virtual shared memory. However, this technique was proposed for prefetching pages, not cache blocks. This paper also does not suggest an adaptive method for controlling the amount that the prefetch processor runs ahead. 3.
Reference: [WeGu89] <author> W.D. Weber and A. Gupta. </author> <title> Exploring the benefits of multiple hardware contexts in a multiprocessor architecture: preliminary results. </title> <booktitle> Proceedings of the 16th Annual International Symposium on Computer Architecture, </booktitle> <year> 1989, </year> <pages> pp. 273-280. </pages>
Reference-contexts: Hardware prefetching [BaCh91][Chen95] tries to reduce the miss ratio of the cache by prefetching additional data when a miss occurs. Stride-based accesses can be accommodated by hardware prefetching because the memory access pattern is static over the course of a program. Multithreading <ref> [WeGu89] </ref> has also been proposed to hide memory latency from the processor by switching threads when cache misses occur. Multithreading places a heavy burden on the compiler because the compiler has to find enough independent threads to hide the memory latency.
Reference: [Wolf92] <author> M.E. Wolf. </author> <title> Improving Locality and Parallelism in Nested Loops. </title> <type> Ph.D. Thesis, </type> <institution> Stanford University, </institution> <year> 1992. </year>
Reference-contexts: However, prefetches should not be inserted unless cache misses can be predicted because the instruction overhead will reduce performance when cache hit rates are high. Techniques such as tiling <ref> [Wolf92] </ref> try to increase locality by making compile time transformations. These techniques are limited to scientific programs which exhibit simple access behavior. In our architecture we can use these techniques as well as others which would not be possible by complier analysis alone.
Reference: [WuMc94] <author> W.A. Wulf and S.A. McKee. </author> <title> Hitting the Memory Wall: Implications of the Obvious. Computer Architecture News, </title> <editor> v. </editor> <volume> 23, no. </volume> <month> 1 (December </month> <year> 1994), </year> <pages> pp. 20-24. </pages>
Reference-contexts: 1. Introduction The gap between processor speeds and DRAM speeds is increasing at an exponential rate <ref> [WuMc94] </ref>. Transistor sizes have been shrinking and die sizes have been growing steadily for many years [Sano94]. These trends have allowed processors to get faster [Denn74] and more complex. Processors have used these additional transistors by adding multiple functional units and highly pipelined modules.
References-found: 20

