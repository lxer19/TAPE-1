URL: ftp://ftp.cs.uu.nl/pub/RUU/CS/techreps/CS-1997/1997-19.ps.gz
Refering-URL: http://www.cs.ruu.nl/docs/research/publication/TechList1.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Motion Planning in Environments with Low Obstacle Density  
Author: A. Frank van der Stappen Mark H. Overmars Mark de Berg Jules Vleugels 
Address: P.O. Box 80.089, 3508 TB Utrecht, The Netherlands.  
Affiliation: Department of Computer Science, Utrecht University,  
Abstract: We present a simple and efficient paradigm for computing the exact solution of the motion planning problem in environments with a low obstacle density. Such environments frequently occur in practical instances of the motion planning problem. The complexity of the free space for such environments is known to be linear in the number of obstacles. Our paradigm is a new cell decomposition approach to motion planning and exploits properties that follow from the low density of the obstacles in the robot's workspace. These properties allow us to decompose the workspace, subject to some constraints, rather than to decompose the higher-dimensional free configuration space directly. A sequence of uniform steps transforms the workspace decomposition into a free space decomposition of asymptotically the same size. The approach leads to nearly-optimal O(n log n) motion planning algorithms for free-flying robots with any fixed number of degrees of freedom in workspaces with low obstacle density.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P.K. Agarwal, M.J. Katz, and M. Sharir, </author> <title> Computing depth orders for fat objects and related problems, Computational Geometry: </title> <booktitle> Theory and Applications 5 (1995), </booktitle> <pages> pp. 187-206. </pages>
Reference-contexts: A collision-free path or motion for a robot B from an initial placement Z 0 to a final placement Z 1 is a continuous map: t : <ref> [0; 1] </ref> ! FP, with t (0) = Z 0 and t (1) = Z 1 . Hence, solving the motion planning problem boils down to finding a continuous curve in FP connecting Z 0 and Z 1 . <p> Several papers present surprising improvements in combinatorial complexity bounds [2, 9, 13, 16, 30] and efficiency gains for algorithms <ref> [1, 4, 11, 19, 20] </ref> if the objects under consideration are fat. Fat objects are `compact' to some extent, rather than long and thin. Fatness is a realistic assumption, since in many practical instances of geometric problems the considered objects are fat. <p> near-linear bounds on the complexity of the union of certain fat figures (e.g. triangles, wedges) in the plane [2, 9, 13, 16], a linear bound on the complexity of the free space for motion planning amidst fat obstacles [30], and efficient algorithms for computing depth orders on certain fat objects <ref> [1] </ref>, binary space partitions for sets of fat objects [4], hidden surface removal for fat horizontal triangles [11], and range searching and point location among fat objects [19, 20]. <p> Contrary to many other definitions of fatness in literature <ref> [1, 2, 9, 11, 13, 16] </ref>, the notion introduced in [30], and recaptured below, applies to general objects in arbitrary dimension d. The definition involves a parameter k, supplying a qualitative measure of the fatness of an object: the smaller the value of k, the fatter the object must be.
Reference: [2] <author> H. Alt, R. Fleischer, M. Kaufmann, K. Mehlhorn, S. N aher, S. Schirra, and C. Uhrig, </author> <title> Approximate motion planning and the complexity of the boundary of the union of simple geometric figures, </title> <booktitle> Algorithmica 8 (1992), </booktitle> <pages> pp. 391-406. </pages>
Reference-contexts: Simple queries are queries that are either easily seen to yield no solution|because there exists no path for a simple inscribed shape of the robot|or easily solvable |because there exists a path for an outer approximation of the robot with fewer degrees of freedom. Alt et al. <ref> [2] </ref> introduce the tightness of a motion planning problem for a rectangle among polygonal obstacles as a measure for its complexity. The tightness of a problem is closely related to the scaling factor for the rectangular robot to make a solvable problem unsolvable, or an unsolvable problem solvable. <p> An immediate consequence of the preceding results will be that complexity of the free space is linear in the number of obstacles. 2.2 Fatness Fatness has turned out to be an interesting phenomenon in computational geometry. Several papers present surprising improvements in combinatorial complexity bounds <ref> [2, 9, 13, 16, 30] </ref> and efficiency gains for algorithms [1, 4, 11, 19, 20] if the objects under consideration are fat. Fat objects are `compact' to some extent, rather than long and thin. <p> The achievements of the study of fatness so far include near-linear bounds on the complexity of the union of certain fat figures (e.g. triangles, wedges) in the plane <ref> [2, 9, 13, 16] </ref>, a linear bound on the complexity of the free space for motion planning amidst fat obstacles [30], and efficient algorithms for computing depth orders on certain fat objects [1], binary space partitions for sets of fat objects [4], hidden surface removal for fat horizontal triangles [11], and <p> Contrary to many other definitions of fatness in literature <ref> [1, 2, 9, 11, 13, 16] </ref>, the notion introduced in [30], and recaptured below, applies to general objects in arbitrary dimension d. The definition involves a parameter k, supplying a qualitative measure of the fatness of an object: the smaller the value of k, the fatter the object must be.
Reference: [3] <author> F. Avnaim, J.-D. Boissonnat, and B. Faverjon, </author> <title> A practical exact motion planning algorithm for polygonal objects amidst polygonal obstacles, </title> <booktitle> Proc. Geometry and Robotics Workshop (J.-D. </booktitle> <editor> Boissonnat and J.-P. Laumond Eds.), </editor> <booktitle> Lecture Notes in Computer Science 391 (1988), </booktitle> <pages> pp. 67-86. </pages>
Reference-contexts: Some algorithms have a hidden sensitivity to the complexity of the free space [33]. For example, the boundary cell decomposition algorithm by Avnaim, Boissonnat, and Faverjon <ref> [3] </ref>, running in time O (n 3 log n) for a constant complexity polygonal robot amidst arbitrary polygonal obstacles, can be shown to run in O (n log n) time in the low obstacle density setting.
Reference: [4] <author> M. de Berg, </author> <title> Linear size binary space partitions for fat objects, </title> <type> Technical Report, </type> <institution> Dept. of Computer Science, Utrecht University, </institution> <note> to appear. </note>
Reference-contexts: The realistic low obstacle density and bounded robot size assumptions also guarantee the existence of a workspace partition of (optimal) O (n) size, based on the binary space partition by De Berg <ref> [4] </ref>. The computation of the partition takes O (n log n) time. As a result, motion planning problems in low obstacle density environments can be solved in O (n log n) time. <p> Several papers present surprising improvements in combinatorial complexity bounds [2, 9, 13, 16, 30] and efficiency gains for algorithms <ref> [1, 4, 11, 19, 20] </ref> if the objects under consideration are fat. Fat objects are `compact' to some extent, rather than long and thin. Fatness is a realistic assumption, since in many practical instances of geometric problems the considered objects are fat. <p> certain fat figures (e.g. triangles, wedges) in the plane [2, 9, 13, 16], a linear bound on the complexity of the free space for motion planning amidst fat obstacles [30], and efficient algorithms for computing depth orders on certain fat objects [1], binary space partitions for sets of fat objects <ref> [4] </ref>, hidden surface removal for fat horizontal triangles [11], and range searching and point location among fat objects [19, 20]. <p> This section discusses a cc-partition of linear size, which can be computed in nearly linear time. We obtain a cc-partition of the workspace by applying a simplified version of the first stage of De Berg's binary space partition algorithm <ref> [4] </ref> to the set fG (E; B )jE 2 Eg of grown obstacles. <p> Assume that all bounding boxes are enclosed by a large axis-parallel hypercube, which contains all vertices of . Our variant of the first stage of the binary space partition algorithm by De Berg <ref> [4] </ref> recursively subdivides this large hypercube until all resulting regions R satisfy (R) = ;. <p> The nodes of the tree can be rearranged in O (n log n) time into a second tree of height O (log n) (see <ref> [4] </ref> for details). <p> The condition is true for our decomposition, 19 an L-shaped region and a square. because its regions are either hypercubes or L-shapes. We will use the point location structure during the computation of the region adjacencies. De Berg <ref> [4] </ref> shows that, under the weak condition of unclutteredness, the number of objects from S intersecting each of the boxes of his decomposition is constant.
Reference: [5] <author> M. de Berg, M. Katz, A.F. van der Stappen, and J. Vleugels, </author> <title> Realistic input models for geometric algorithms, </title> <booktitle> Proc. 13th Ann. ACM Symp. on Computational Geometry (1997), </booktitle> <pages> pp. 294-303. </pages>
Reference-contexts: The set S satisfies the unclutteredness condition if there is a constant such that any hypercube whose interior does not contain a vertex of one of the bounding boxes of S is intersected by at most objects of S. In <ref> [5] </ref>, De Berg et al. study the relations between various realistic assumptions on sets of geometric objects, including fatness, unclutteredness, and low density. Among their results is the following relation. Lemma 4.2 [5] Let IR d with a set S of objects satisfy the low object density property. <p> In <ref> [5] </ref>, De Berg et al. study the relations between various realistic assumptions on sets of geometric objects, including fatness, unclutteredness, and low density. Among their results is the following relation. Lemma 4.2 [5] Let IR d with a set S of objects satisfy the low object density property. Then IR d with S satisfies the unclutteredness condition. Let us now apply the recursive decomposition scheme to the set fG (E; B )jE 2 Eg.
Reference: [6] <author> R.-P. Berretty, M. Overmars, and A.F. van der Stappen, </author> <title> Dynamic motion planning in low obstacle density environments, </title> <booktitle> Proc. Workshop on Algorithms and Data Structures (WADS'97) (1997). </booktitle>
Reference-contexts: The results from this paper are almost immediately applicable to this problem [33]. Other possible extensions include motion planning with moving obstacles, multiple robots, and anchored robot arms. The dynamic version of the low density motion planning problem has been studied by Berretty et al. <ref> [6] </ref>. They consider a setting in which the obstacles in the workspace move at constant speed along polyline paths. The workspace is assumed to satisfy the low density property at any time.
Reference: [7] <author> J.F. Canny, </author> <title> The complexity of robot motion planning, </title> <publisher> MIT Press, </publisher> <address> Cambridge MA (1988). </address>
Reference-contexts: As a result, the size of the query structure|the connectivity graph CG|and the time to compute it depend on the complexity of FP. Retraction methods (see e.g. <ref> [7, 14, 17, 18, 29] </ref>) aim at capturing the structure and connectivity of the free space in some one-dimensional network of curves in the free space, the roadmap. <p> General approaches to motion planning (e.g. by Schwartz and Sharir [23] with running time O (n 2 f+6 ) and Canny <ref> [7] </ref> with running time O (n f log n)) are computationally expensive, even under our beneficial circumstances. These rigorous methods do not take advantage of any accidental structure of the free space (as is present in our case). <p> It has been noted that the existing planar motion planning algorithms are not easily extendible towards other|in particular spatial|problems. Moreover, the existing general approaches to motion planning (like those by Schwartz and Sharir [23] and Canny <ref> [7] </ref>) are computationally expensive, even for problems from the special class that we consider here. Motion planning problems in Euclidean workspaces of dimension three normally imply at least three-dimensional configuration spaces.
Reference: [8] <author> B. Chazelle and L. Guibas, </author> <title> Fractional cascading I: A data structuring technique, </title> <booktitle> Algorithmica 1 (1986), </booktitle> <pages> pp. 133-162. </pages>
Reference: [9] <author> A. Efrat, G. Rote, and M. Sharir, </author> <title> On the union of fat wedges and separating a collection of segments by a line, Computational Geometry: </title> <booktitle> Theory and Applications 3 (1993), </booktitle> <pages> pp. 277-288. </pages>
Reference-contexts: An immediate consequence of the preceding results will be that complexity of the free space is linear in the number of obstacles. 2.2 Fatness Fatness has turned out to be an interesting phenomenon in computational geometry. Several papers present surprising improvements in combinatorial complexity bounds <ref> [2, 9, 13, 16, 30] </ref> and efficiency gains for algorithms [1, 4, 11, 19, 20] if the objects under consideration are fat. Fat objects are `compact' to some extent, rather than long and thin. <p> The achievements of the study of fatness so far include near-linear bounds on the complexity of the union of certain fat figures (e.g. triangles, wedges) in the plane <ref> [2, 9, 13, 16] </ref>, a linear bound on the complexity of the free space for motion planning amidst fat obstacles [30], and efficient algorithms for computing depth orders on certain fat objects [1], binary space partitions for sets of fat objects [4], hidden surface removal for fat horizontal triangles [11], and <p> Contrary to many other definitions of fatness in literature <ref> [1, 2, 9, 11, 13, 16] </ref>, the notion introduced in [30], and recaptured below, applies to general objects in arbitrary dimension d. The definition involves a parameter k, supplying a qualitative measure of the fatness of an object: the smaller the value of k, the fatter the object must be.
Reference: [10] <author> D. Halperin and M.H. Overmars, </author> <title> Spheres, molecules, and hidden surface removal, </title> <booktitle> Proc. 10th Ann. ACM Symp. on Computational Geometry (1994), </booktitle> <pages> pp. 113-122. </pages>
Reference-contexts: So, under the circumstances sketched in Theorem 2.4, the boundary of the union of all wrappings has complexity O (n). Theorems 2.4 and 2.9 are, for example, applicable to the molecule model in the paper by Halperin and Overmars <ref> [10] </ref>. The atoms that constitute a molecule are assumed to satisfy the hard sphere model. The hard sphere model describes atoms by spheres and forbids any sphere center to get too close to another sphere center.
Reference: [11] <author> M.J. Katz, M.H. Overmars, and M. Sharir, </author> <title> Efficient hidden surface removal for objects with small union size, Computational Geometry: </title> <booktitle> Theory and Applications 2 (1992), </booktitle> <pages> pp. 223-234. </pages>
Reference-contexts: Several papers present surprising improvements in combinatorial complexity bounds [2, 9, 13, 16, 30] and efficiency gains for algorithms <ref> [1, 4, 11, 19, 20] </ref> if the objects under consideration are fat. Fat objects are `compact' to some extent, rather than long and thin. Fatness is a realistic assumption, since in many practical instances of geometric problems the considered objects are fat. <p> plane [2, 9, 13, 16], a linear bound on the complexity of the free space for motion planning amidst fat obstacles [30], and efficient algorithms for computing depth orders on certain fat objects [1], binary space partitions for sets of fat objects [4], hidden surface removal for fat horizontal triangles <ref> [11] </ref>, and range searching and point location among fat objects [19, 20]. Contrary to many other definitions of fatness in literature [1, 2, 9, 11, 13, 16], the notion introduced in [30], and recaptured below, applies to general objects in arbitrary dimension d. <p> Contrary to many other definitions of fatness in literature <ref> [1, 2, 9, 11, 13, 16] </ref>, the notion introduced in [30], and recaptured below, applies to general objects in arbitrary dimension d. The definition involves a parameter k, supplying a qualitative measure of the fatness of an object: the smaller the value of k, the fatter the object must be.
Reference: [12] <author> Y. Ke and J. O'Rourke, </author> <title> Moving a ladder in three dimensions: upper and lower bounds, </title> <booktitle> Proc. 3rd Ann. ACM Symp. on Computational Geometry (1987), </booktitle> <pages> pp. 136-145. </pages>
Reference-contexts: Although there essentially exist two different approaches to exact motion planning (cell decomposition and retraction), the time spent in processing the free space and the size of the resulting query structure clearly depend on the complexity of the free space. Cell decomposition algorithms (see e.g. <ref> [12, 15, 22, 23, 24, 25, 28] </ref>) partition the free space into a finite number of simple connected subcells, such that planning a motion between two placements in a single subcell is straightforward and such that uniform crossing rules can be defined for B crossing from one cell into another.
Reference: [13] <author> M. van Kreveld, </author> <title> On fat partitioning, fat covering and the union size of polygons, </title> <type> Technical Report RUU-CS-93-36, </type> <institution> Dept. of Computer Science, Utrecht University (1993). </institution>
Reference-contexts: An immediate consequence of the preceding results will be that complexity of the free space is linear in the number of obstacles. 2.2 Fatness Fatness has turned out to be an interesting phenomenon in computational geometry. Several papers present surprising improvements in combinatorial complexity bounds <ref> [2, 9, 13, 16, 30] </ref> and efficiency gains for algorithms [1, 4, 11, 19, 20] if the objects under consideration are fat. Fat objects are `compact' to some extent, rather than long and thin. <p> The achievements of the study of fatness so far include near-linear bounds on the complexity of the union of certain fat figures (e.g. triangles, wedges) in the plane <ref> [2, 9, 13, 16] </ref>, a linear bound on the complexity of the free space for motion planning amidst fat obstacles [30], and efficient algorithms for computing depth orders on certain fat objects [1], binary space partitions for sets of fat objects [4], hidden surface removal for fat horizontal triangles [11], and <p> Contrary to many other definitions of fatness in literature <ref> [1, 2, 9, 11, 13, 16] </ref>, the notion introduced in [30], and recaptured below, applies to general objects in arbitrary dimension d. The definition involves a parameter k, supplying a qualitative measure of the fatness of an object: the smaller the value of k, the fatter the object must be.
Reference: [14] <author> D. Leven and M. Sharir, </author> <title> Planning a purely translational motion for a convex object in two-dimensional space using generalized Voronoi diagrams, </title> <booktitle> Discrete & Computational Geometry 2 (1987), </booktitle> <pages> pp. 9-31. </pages>
Reference-contexts: As a result, the size of the query structure|the connectivity graph CG|and the time to compute it depend on the complexity of FP. Retraction methods (see e.g. <ref> [7, 14, 17, 18, 29] </ref>) aim at capturing the structure and connectivity of the free space in some one-dimensional network of curves in the free space, the roadmap.
Reference: [15] <author> D. Leven and M. Sharir, </author> <title> An efficient and simple motion planning algorithm for a ladder amidst polygonal barriers, </title> <booktitle> Journal of Algorithms 8 (1987), </booktitle> <pages> pp. 192-215. </pages>
Reference-contexts: Although there essentially exist two different approaches to exact motion planning (cell decomposition and retraction), the time spent in processing the free space and the size of the resulting query structure clearly depend on the complexity of the free space. Cell decomposition algorithms (see e.g. <ref> [12, 15, 22, 23, 24, 25, 28] </ref>) partition the free space into a finite number of simple connected subcells, such that planning a motion between two placements in a single subcell is straightforward and such that uniform crossing rules can be defined for B crossing from one cell into another.
Reference: [16] <author> J. Matou sek, J. Pach, M. Sharir, S. Sifrony, and E. Welzl, </author> <title> Fat triangles determine linearly many holes, </title> <journal> SIAM Journal on Computing 23 (1994), </journal> <pages> pp. 154-169. </pages>
Reference-contexts: An immediate consequence of the preceding results will be that complexity of the free space is linear in the number of obstacles. 2.2 Fatness Fatness has turned out to be an interesting phenomenon in computational geometry. Several papers present surprising improvements in combinatorial complexity bounds <ref> [2, 9, 13, 16, 30] </ref> and efficiency gains for algorithms [1, 4, 11, 19, 20] if the objects under consideration are fat. Fat objects are `compact' to some extent, rather than long and thin. <p> The achievements of the study of fatness so far include near-linear bounds on the complexity of the union of certain fat figures (e.g. triangles, wedges) in the plane <ref> [2, 9, 13, 16] </ref>, a linear bound on the complexity of the free space for motion planning amidst fat obstacles [30], and efficient algorithms for computing depth orders on certain fat objects [1], binary space partitions for sets of fat objects [4], hidden surface removal for fat horizontal triangles [11], and <p> Contrary to many other definitions of fatness in literature <ref> [1, 2, 9, 11, 13, 16] </ref>, the notion introduced in [30], and recaptured below, applies to general objects in arbitrary dimension d. The definition involves a parameter k, supplying a qualitative measure of the fatness of an object: the smaller the value of k, the fatter the object must be.
Reference: [17] <author> C. O'D unlaing, M. Sharir, and C.-K. Yap, Retraction: </author> <title> A new approach to motion planning, </title> <booktitle> Proc. 15th Ann. ACM Symp. on the Theory of Computing (1983), </booktitle> <pages> pp. 207-220. </pages>
Reference-contexts: As a result, the size of the query structure|the connectivity graph CG|and the time to compute it depend on the complexity of FP. Retraction methods (see e.g. <ref> [7, 14, 17, 18, 29] </ref>) aim at capturing the structure and connectivity of the free space in some one-dimensional network of curves in the free space, the roadmap.
Reference: [18] <author> C. O'D unlaing and C.-K. Yap, </author> <title> A retraction method for planning the motion of a disc, </title> <booktitle> Journal of Algorithms 6 (1985), </booktitle> <pages> pp. 104-111. </pages>
Reference-contexts: As a result, the size of the query structure|the connectivity graph CG|and the time to compute it depend on the complexity of FP. Retraction methods (see e.g. <ref> [7, 14, 17, 18, 29] </ref>) aim at capturing the structure and connectivity of the free space in some one-dimensional network of curves in the free space, the roadmap.
Reference: [19] <author> M.H. Overmars, </author> <title> Point location in fat subdivisions, </title> <booktitle> Information Processing Letters 44 (1992), </booktitle> <pages> pp. 261-265. </pages>
Reference-contexts: Several papers present surprising improvements in combinatorial complexity bounds [2, 9, 13, 16, 30] and efficiency gains for algorithms <ref> [1, 4, 11, 19, 20] </ref> if the objects under consideration are fat. Fat objects are `compact' to some extent, rather than long and thin. Fatness is a realistic assumption, since in many practical instances of geometric problems the considered objects are fat. <p> complexity of the free space for motion planning amidst fat obstacles [30], and efficient algorithms for computing depth orders on certain fat objects [1], binary space partitions for sets of fat objects [4], hidden surface removal for fat horizontal triangles [11], and range searching and point location among fat objects <ref> [19, 20] </ref>. Contrary to many other definitions of fatness in literature [1, 2, 9, 11, 13, 16], the notion introduced in [30], and recaptured below, applies to general objects in arbitrary dimension d.
Reference: [20] <author> M.H. Overmars and A.F. van der Stappen, </author> <title> Range searching and point location among fat objects, </title> <booktitle> Journal of Algorithms 21 (1996), </booktitle> <pages> pp. 629-656. </pages>
Reference-contexts: Several papers present surprising improvements in combinatorial complexity bounds [2, 9, 13, 16, 30] and efficiency gains for algorithms <ref> [1, 4, 11, 19, 20] </ref> if the objects under consideration are fat. Fat objects are `compact' to some extent, rather than long and thin. Fatness is a realistic assumption, since in many practical instances of geometric problems the considered objects are fat. <p> complexity of the free space for motion planning amidst fat obstacles [30], and efficient algorithms for computing depth orders on certain fat objects [1], binary space partitions for sets of fat objects [4], hidden surface removal for fat horizontal triangles [11], and range searching and point location among fat objects <ref> [19, 20] </ref>. Contrary to many other definitions of fatness in literature [1, 2, 9, 11, 13, 16], the notion introduced in [30], and recaptured below, applies to general objects in arbitrary dimension d.
Reference: [21] <author> Ph. Pignon, </author> <title> Structuration de l' espace pour une planification hierarchisee des trajec-toires de robots mobiles, </title> <type> Ph.D. Thesis, </type> <institution> LAAS-CNRS and Universite Paul Sabatier de Toulouse, </institution> <note> Rapport LAAS N o 93395 (1993) (in French). </note>
Reference-contexts: It is though trivial to extend it to sets that satisfy the low obstacle density property.) Circumstances that resemble the low obstacle density have also been studied by Schwartz and Sharir [26] who refer to it as bounded local complexity and by Pignon <ref> [21] </ref> who calls it sparsity. A question that immediately comes to mind when considering the combinatorial result of [30] is whether this reduced complexity opens the way to efficient motion planning algorithms for such realistic environments. <p> Any (imaginary) ball with radius r in such a workspace intersects no more than a constant number of obstacles. The property resembles our notion of low obstacle density. The authors give directions on how to solve the motion planning problem in such workspaces. Pignon <ref> [21] </ref> structures two-dimensional workspaces with polygonal obstacles and a polygonal robot (using Minkowski differences) to efficiently detect and solve simple path-finding queries.
Reference: [22] <author> J.T. Schwartz and M. Sharir, </author> <title> On the piano movers' problem: I. The case of a two-dimensional rigid polygonal body moving amidst polygonal boundaries, </title> <journal> Communications on Pure and Applied Mathematics 36 (1983), </journal> <pages> pp. 345-398. </pages>
Reference-contexts: A direction of research in computational geometry, initiated by a series of papers|known as the Piano Movers' series <ref> [22, 23, 24, 25, 28] </ref>|by Schwartz and Sharir in the early 80s, studies the exact solution of the motion planning problem. Exact methods for solving the motion planning problem are guaranteed to find a path if one exists, and report failure if no path exists. <p> Although there essentially exist two different approaches to exact motion planning (cell decomposition and retraction), the time spent in processing the free space and the size of the resulting query structure clearly depend on the complexity of the free space. Cell decomposition algorithms (see e.g. <ref> [12, 15, 22, 23, 24, 25, 28] </ref>) partition the free space into a finite number of simple connected subcells, such that planning a motion between two placements in a single subcell is straightforward and such that uniform crossing rules can be defined for B crossing from one cell into another. <p> The O (n 5 ) algorithm by Schwartz and Sharir <ref> [22] </ref> for planning the motion of a ladder or a polygonal robot amidst polygonal obstacles can be shown 4 to run, unmodified, in time O (n 2 ) if the obstacle density is low, whereas a minor modification improves the efficiency to a running time of O (n log n) (see
Reference: [23] <author> J.T. Schwartz and M. Sharir, </author> <title> On the piano movers' problem: II. General techniques for computing topological properties of real algebraic manifolds, </title> <booktitle> Advances in Applied Mathematics 4 (1983), </booktitle> <pages> pp. 298-351. </pages>
Reference-contexts: A direction of research in computational geometry, initiated by a series of papers|known as the Piano Movers' series <ref> [22, 23, 24, 25, 28] </ref>|by Schwartz and Sharir in the early 80s, studies the exact solution of the motion planning problem. Exact methods for solving the motion planning problem are guaranteed to find a path if one exists, and report failure if no path exists. <p> Although there essentially exist two different approaches to exact motion planning (cell decomposition and retraction), the time spent in processing the free space and the size of the resulting query structure clearly depend on the complexity of the free space. Cell decomposition algorithms (see e.g. <ref> [12, 15, 22, 23, 24, 25, 28] </ref>) partition the free space into a finite number of simple connected subcells, such that planning a motion between two placements in a single subcell is straightforward and such that uniform crossing rules can be defined for B crossing from one cell into another. <p> Algorithms for efficient motion planning in 3D workspaces are scarce: approaches in contact space, like the algorithms mentioned above by Sifrony and Sharir, and by Avnaim, Boissonnat, and Faverjon, were never shown to generalize to higher dimensions. General approaches to motion planning (e.g. by Schwartz and Sharir <ref> [23] </ref> with running time O (n 2 f+6 ) and Canny [7] with running time O (n f log n)) are computationally expensive, even under our beneficial circumstances. These rigorous methods do not take advantage of any accidental structure of the free space (as is present in our case). <p> It has been noted that the existing planar motion planning algorithms are not easily extendible towards other|in particular spatial|problems. Moreover, the existing general approaches to motion planning (like those by Schwartz and Sharir <ref> [23] </ref> and Canny [7]) are computationally expensive, even for problems from the special class that we consider here. Motion planning problems in Euclidean workspaces of dimension three normally imply at least three-dimensional configuration spaces. <p> The first for-loop computes a decomposition of FP " (R fi D) into O (1) (constant-complexity) subcells and gathers these in a set V C . One possible way to perform this computation in constant time is by applying (in step 2) the rigorous techniques by Schwartz and Sharir <ref> [23] </ref> to the constant number of constraint hypersurfaces intersecting the cylinder RfiD. The output is a subdivision of the arrangement cells into constant-complexity subcells. Restriction of these subcells to R fiD and subsequently filtering out the forbidden ones results in an appropriate cell decomposition of FP " (R fi D). <p> The techniques by Schwartz and Sharir from <ref> [23] </ref> may be useful to compute a decomposition of the free part FP " (R fi D) of a cylinder R fi D. The refinement of step 1 of the first for-loop verifies the running time of O (jV W j) for the first for-loop of the transformation.
Reference: [24] <author> J.T. Schwartz and M. Sharir, </author> <title> On the piano movers' problem: III. Coordinating the motion of several independent bodies: the special case of circular bodies moving amidst polygonal barriers, </title> <journal> International Journal of Robotics Research 2 (1983), </journal> <pages> pp. 46-75. 26 </pages>
Reference-contexts: A direction of research in computational geometry, initiated by a series of papers|known as the Piano Movers' series <ref> [22, 23, 24, 25, 28] </ref>|by Schwartz and Sharir in the early 80s, studies the exact solution of the motion planning problem. Exact methods for solving the motion planning problem are guaranteed to find a path if one exists, and report failure if no path exists. <p> Although there essentially exist two different approaches to exact motion planning (cell decomposition and retraction), the time spent in processing the free space and the size of the resulting query structure clearly depend on the complexity of the free space. Cell decomposition algorithms (see e.g. <ref> [12, 15, 22, 23, 24, 25, 28] </ref>) partition the free space into a finite number of simple connected subcells, such that planning a motion between two placements in a single subcell is straightforward and such that uniform crossing rules can be defined for B crossing from one cell into another.
Reference: [25] <author> J.T. Schwartz and M. Sharir, </author> <title> On the piano movers' problem: V. The case of a rod moving in three-dimensional space amidst polyhedral obstacles, </title> <journal> Communications on Pure and Applied Mathematics 37 (1984), </journal> <pages> pp. 815-848. </pages>
Reference-contexts: A direction of research in computational geometry, initiated by a series of papers|known as the Piano Movers' series <ref> [22, 23, 24, 25, 28] </ref>|by Schwartz and Sharir in the early 80s, studies the exact solution of the motion planning problem. Exact methods for solving the motion planning problem are guaranteed to find a path if one exists, and report failure if no path exists. <p> Although there essentially exist two different approaches to exact motion planning (cell decomposition and retraction), the time spent in processing the free space and the size of the resulting query structure clearly depend on the complexity of the free space. Cell decomposition algorithms (see e.g. <ref> [12, 15, 22, 23, 24, 25, 28] </ref>) partition the free space into a finite number of simple connected subcells, such that planning a motion between two placements in a single subcell is straightforward and such that uniform crossing rules can be defined for B crossing from one cell into another.
Reference: [26] <author> J.T. Schwartz and M. Sharir, </author> <title> Efficient motion planning algorithms in environments of bounded local complexity, </title> <type> Report 164, </type> <institution> Department of Computer Science, Courant Inst. Math. Sci., </institution> <address> New York NY (1985). </address>
Reference-contexts: It is though trivial to extend it to sets that satisfy the low obstacle density property.) Circumstances that resemble the low obstacle density have also been studied by Schwartz and Sharir <ref> [26] </ref> who refer to it as bounded local complexity and by Pignon [21] who calls it sparsity. A question that immediately comes to mind when considering the combinatorial result of [30] is whether this reduced complexity opens the way to efficient motion planning algorithms for such realistic environments. <p> This number gives some idea of how cluttered the obstacles in the workspace are and is closely related to the complexity of the free space. Schwartz and Sharir <ref> [26] </ref> consider workspaces with obstacles of so-called bounded local complexity. Any (imaginary) ball with radius r in such a workspace intersects no more than a constant number of obstacles. The property resembles our notion of low obstacle density.
Reference: [27] <author> M. Sharir, </author> <title> Efficient algorithms for planning purely translational collision-free motion in two and three dimensions, </title> <booktitle> Proc. of the IEEE Int. Conf. on Robotics and Automation, </booktitle> <address> Raleigh NC (1987), </address> <pages> pp. 1326-1331. </pages>
Reference-contexts: These bounds generally remain close to one order of magnitude, i.e., a factor n, below the (n f ) bound (see e.g. <ref> [27, 36] </ref>). Hence, even in such more specific cases, the theoretical worst-case bounds are high.
Reference: [28] <author> M. Sharir and E. Ariel-Sheffi, </author> <title> On the piano movers' problem: IV. Various decomposable two-dimensional motion planning problems, </title> <journal> Communications on Pure and Applied Mathematics 37 (1984), </journal> <pages> pp. 479-493. </pages>
Reference-contexts: A direction of research in computational geometry, initiated by a series of papers|known as the Piano Movers' series <ref> [22, 23, 24, 25, 28] </ref>|by Schwartz and Sharir in the early 80s, studies the exact solution of the motion planning problem. Exact methods for solving the motion planning problem are guaranteed to find a path if one exists, and report failure if no path exists. <p> Although there essentially exist two different approaches to exact motion planning (cell decomposition and retraction), the time spent in processing the free space and the size of the resulting query structure clearly depend on the complexity of the free space. Cell decomposition algorithms (see e.g. <ref> [12, 15, 22, 23, 24, 25, 28] </ref>) partition the free space into a finite number of simple connected subcells, such that planning a motion between two placements in a single subcell is straightforward and such that uniform crossing rules can be defined for B crossing from one cell into another.
Reference: [29] <author> S. Sifrony and M. Sharir, </author> <title> A new efficient motion planning algorithm for a rod in two-dimensional polygonal space, </title> <booktitle> Algorithmica 2 (1987), </booktitle> <pages> pp. 367-402. </pages>
Reference-contexts: As a result, the size of the query structure|the connectivity graph CG|and the time to compute it depend on the complexity of FP. Retraction methods (see e.g. <ref> [7, 14, 17, 18, 29] </ref>) aim at capturing the structure and connectivity of the free space in some one-dimensional network of curves in the free space, the roadmap. <p> The vast majority of motion planning algorithms have no reported sensitivity to the complexity of the free space. A clear exception is the boundary-vertices retraction algorithm by Sifrony and Sharir <ref> [29] </ref> for a ladder moving in a planar workspace with polygonal obstacles. The algorithm runs in time O (K log n), where K is the number of pairs of obstacle corners that lie less than the length of the ladder apart. <p> We are aware of only few (related) results on exact motion planning methods with provable efficiency or free space complexity-sensitive behavior for realistic motion planning problems (with low complexity workspaces or free spaces). The running time of Sifrony and Sharir's algorithm <ref> [29] </ref> depends on the number of pairs of obstacle corners that lie less than the length of the ladder apart. This number gives some idea of how cluttered the obstacles in the workspace are and is closely related to the complexity of the free space.
Reference: [30] <author> A.F. van der Stappen, D. Halperin, M.H. Overmars, </author> <title> The complexity of the free space for a robot moving amidst fat obstacles, Computational Geometry: </title> <booktitle> Theory and Applications 3 (1993), </booktitle> <pages> pp. 353-373. </pages>
Reference-contexts: What natural mild assumptions would for example lead to the relative low obstacle density of the above example, in which the robot is unable to touch more than a constant number of obstacles simultaneously? Van der Stappen, Halperin, and Overmars <ref> [30] </ref> show that the combinatorial complexity of the free space is linear in the number of obstacles if the robot is not too large compared to the obstacles and if any workspace region intersects no more than a constant number of obstacles that are at least as large as the region. <p> We shall refer to the latter property as the low obstacle density property of the workspace. (Actually, in <ref> [30] </ref> the linear bound is only proven for the more restricted assumption of fatness of the obstacles. <p> A question that immediately comes to mind when considering the combinatorial result of <ref> [30] </ref> is whether this reduced complexity opens the way to efficient motion planning algorithms for such realistic environments. The vast majority of motion planning algorithms have no reported sensitivity to the complexity of the free space. <p> The results are basically reformulations of results previously reported in <ref> [30] </ref>, but we repeat them because they are fundamental to this paper. As the relative sizes of the robot and the obstacles play a crucial role throughout the paper, we first give convenient measures for the size of an obstacle and a robot. <p> The reader is referred to <ref> [30] </ref> for a proof. 1 The arrangement of a set is the subdivision of space into connected pieces of any dimension induced by that set. 7 Theorem 2.5 Let W with a set E of n constant-complexity obstacles with minimal enclosing hypersphere radii at least be a low obstacle density workspace. <p> An immediate consequence of the preceding results will be that complexity of the free space is linear in the number of obstacles. 2.2 Fatness Fatness has turned out to be an interesting phenomenon in computational geometry. Several papers present surprising improvements in combinatorial complexity bounds <ref> [2, 9, 13, 16, 30] </ref> and efficiency gains for algorithms [1, 4, 11, 19, 20] if the objects under consideration are fat. Fat objects are `compact' to some extent, rather than long and thin. <p> The achievements of the study of fatness so far include near-linear bounds on the complexity of the union of certain fat figures (e.g. triangles, wedges) in the plane [2, 9, 13, 16], a linear bound on the complexity of the free space for motion planning amidst fat obstacles <ref> [30] </ref>, and efficient algorithms for computing depth orders on certain fat objects [1], binary space partitions for sets of fat objects [4], hidden surface removal for fat horizontal triangles [11], and range searching and point location among fat objects [19, 20]. <p> Contrary to many other definitions of fatness in literature [1, 2, 9, 11, 13, 16], the notion introduced in <ref> [30] </ref>, and recaptured below, applies to general objects in arbitrary dimension d. The definition involves a parameter k, supplying a qualitative measure of the fatness of an object: the smaller the value of k, the fatter the object must be. <p> An intuitive explanation lies in the observation that it is impossible to have a large number of fat obstacles of a certain minimum size intersecting a small region. A more formal proof follows from <ref> [30] </ref>. Here, we confine ourselves to reporting the result. <p> The mild assumptions provide a realistic framework for many practical motion planning problems. The complexity of the free space for problems that satisfy the assumptions was proven to be O (n) <ref> [30] </ref>, whereas the complexity can easily be as high as (n f ) when both assumptions are dropped. Besides having a low combinatorial complexity, the free space for a motion planning problem that fits in our framework also has a beneficial structure.
Reference: [31] <author> A.F. van der Stappen, </author> <title> The complexity of the free space for motion planning amidst fat obstacles, </title> <journal> Journal of Intelligent and Robotic Systems 11 (1994), </journal> <pages> pp. 21-44. </pages>
Reference-contexts: planning the motion of a ladder or a polygonal robot amidst polygonal obstacles can be shown 4 to run, unmodified, in time O (n 2 ) if the obstacle density is low, whereas a minor modification improves the efficiency to a running time of O (n log n) (see also <ref> [31] </ref>). Hence, there exist (planar) motion planning algorithms that do benefit from low free space complexities, even though several other algorithms do not.
Reference: [32] <author> A.F. van der Stappen and M.H. Overmars, </author> <title> Motion planning amidst fat obstacles, </title> <booktitle> Proc. 10th Ann. ACM Symp. on Computational Geometry (1994), </booktitle> <pages> pp. 31-40. </pages>
Reference-contexts: Unfortunately, the partition does not suit our purposes, because the d-cells themselves may have more than constant complexity. Hence, it is not a cc-partition. In the case of a planar workspace, the partition is easily refined into a cc-partition of O (n) size by means of a vertical decomposition <ref> [32, 33] </ref>. The decomposition procedure discussed in Section 4, however, gives a cc-partition of linear size for any dimension. In summary, we have found that a cc-partition of a low obstacle density workspace always exists.
Reference: [33] <author> A.F. van der Stappen, </author> <title> Motion planning amidst fat obstacles, </title> <type> Ph.D. Thesis, </type> <institution> Dept. of Computer Science, Utrecht University (1994). </institution>
Reference-contexts: The low obstacle density causes K to be only O (n), whereas it could be fi (n 2 ) in the worst case for arbitrary workspaces with obstacles. Some algorithms have a hidden sensitivity to the complexity of the free space <ref> [33] </ref>. For example, the boundary cell decomposition algorithm by Avnaim, Boissonnat, and Faverjon [3], running in time O (n 3 log n) for a constant complexity polygonal robot amidst arbitrary polygonal obstacles, can be shown to run in O (n log n) time in the low obstacle density setting. <p> Unfortunately, the partition does not suit our purposes, because the d-cells themselves may have more than constant complexity. Hence, it is not a cc-partition. In the case of a planar workspace, the partition is easily refined into a cc-partition of O (n) size by means of a vertical decomposition <ref> [32, 33] </ref>. The decomposition procedure discussed in Section 4, however, gives a cc-partition of linear size for any dimension. In summary, we have found that a cc-partition of a low obstacle density workspace always exists. <p> The constant number of additional constraint hypersurfaces induced by the self-collisions of the constant-complexity robot does not increase the asymptotic complexity of the arrangement inside any cylinder R fi D. Therefore, the combinatorial and algorithmic considerations of this section apply without restrictions. The reader is referred to <ref> [33] </ref> for further details. 18 4 A linear size base partition In Subsection 3.2, we have reduced the low density motion planning problem for a free-flying robot to the problem of finding a cc-partition of the workspace. <p> We have already mentioned (see Subsection 3.2) the motion planning problem for a robot moving in a three-dimensional world while its motion is confined to a plane, e.g. a factory floor. The results from this paper are almost immediately applicable to this problem <ref> [33] </ref>. Other possible extensions include motion planning with moving obstacles, multiple robots, and anchored robot arms. The dynamic version of the low density motion planning problem has been studied by Berretty et al. [6].
Reference: [34] <author> J. Vleugels, </author> <title> On fatness and fitness|realistic input models for geometric algorithms, </title> <type> Ph.D. Thesis, </type> <institution> Dept. of Computer Science, Utrecht University (1997). </institution>
Reference-contexts: Assuming that the minimal enclosing hypersphere radii of all obstacles in the workspace are at least , the restriction we impose is that the reach B of the robot B is bounded by b , for some constant b 0. Definition 2.2 (see also <ref> [34] </ref>) defines a class of (work)spaces that, in combination with the 6 bound on the relative size of the robot and the obstacles, give rise to a linear complexity free space and allow for efficient motion planning. <p> The ideas of a cylindrical decomposition of the free space seem applicable if the workspace W is a projective subspace of each of the spaces C i ; in that case, W c is a valid base space. Vleugels <ref> [34] </ref> has shown how to restrict the search for a free path for two robots in their composite configuration space C to a collection 24 of lower-dimensional subspaces of C, such that a path exists in the subspaces whenever one exists in C.
Reference: [35] <author> C. Wentink and O. Schwarzkopf, </author> <title> Motion planning for vacuum cleaner robots, </title> <booktitle> Proc. 6th Canadian Conf. on Computational Geometry (1994), </booktitle> <pages> pp. 51-56. </pages>
Reference-contexts: At first sight, this may seem like an impractical generalization. Imagine, however, a vacuum-cleaning robot which moves in a three-dimensional workspace although its motion is restricted to a plane (the floor) <ref> [35] </ref>. Here, a point p B 2 B = IR 2 is sufficient to describe the position of the vacuum cleaner's reference point.
Reference: [36] <author> A. Wiernik and M. Sharir, </author> <title> Planar realizations of nonlinear Davenport-Schinzel sequences by segments, </title> <booktitle> Discrete & Computational Geometry 3 (1988), </booktitle> <pages> pp. 15-47. 27 </pages>
Reference-contexts: These bounds generally remain close to one order of magnitude, i.e., a factor n, below the (n f ) bound (see e.g. <ref> [27, 36] </ref>). Hence, even in such more specific cases, the theoretical worst-case bounds are high.
References-found: 36

