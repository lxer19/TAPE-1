URL: ftp://ftp.cs.toronto.edu/pub/radford/bmm.ps.Z
Refering-URL: http://www.cs.toronto.edu/~radford/bmm-maxent.abstract.html
Root-URL: 
Email: e-mail: radford@ai.toronto.edu  
Title: Bayesian Mixture Modeling by Monte Carlo Simulation  
Author: Radford M. Neal 
Date: June, 1991  
Address: Toronto  
Affiliation: Department of Computer Science University of  
Pubnum: Technical Report CRG-TR-91-2  
Abstract: It is shown that Bayesian inference from data modeled by a mixture distribution can feasibly be performed via Monte Carlo simulation. This method exhibits the true Bayesian predictive distribution, implicitly integrating over the entire underlying parameter space. An infinite number of mixture components can be accommodated without difficulty, using a prior distribution for mixing proportions that selects a reasonable subset of components to explain any finite training set. The need to decide on a "correct" number of components is thereby avoided. The feasibility of the method is shown empirically for a simple classification task. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Ackley, D. H., Hinton, G. E., and Sejnowski, T. J. </author> <title> (1985) A learning algorithm for Boltzmann machines, </title> <journal> Cognitive Science, </journal> <volume> vol. 9, </volume> <pages> pp. 147-169. </pages>
Reference-contexts: This problem arises in the context of statistical physics [16], image restoration [10], stochastic neural networks <ref> [1, 17] </ref>, belief networks for expert systems [18], and general statistical calculation [12, 9]. In each case, the problem has been solved by a Monte Carlo simulation method, under names such as the "Metropolis algorithm", the "Boltzmann Machine", and the "Gibbs sampler". <p> The situation is analogous to the problem of local maxima with deterministic optimization procedures, except that the stochastic aspect ensures that the simulation will escape the local maximum eventually. The time required for this can be reduced by the method of "simulated annealing", as is done in <ref> [1] </ref>, but such elaborations are not considered in this paper. Simulating the Bayesian predictive distribution. This Monte Carlo simulation technique can be used to solve the Bayesian prediction problem for mixture models. <p> C [g i ] C [g i ] + 1 od Select a new value for v n1 , updating the occurrence and frequency counts accordingly. A <ref> [g n ; 1; v n1 ] </ref> A [g n ; 1; v n1 ] 1 v n1 Value in the range 1::N 1 picked according to the distribution Y A [g n ; 1; v n1 ] A [g n ; 1; v n1 ] + 1 od Calculate the <p> C [g i ] C [g i ] + 1 od Select a new value for v n1 , updating the occurrence and frequency counts accordingly. A <ref> [g n ; 1; v n1 ] </ref> A [g n ; 1; v n1 ] 1 v n1 Value in the range 1::N 1 picked according to the distribution Y A [g n ; 1; v n1 ] A [g n ; 1; v n1 ] + 1 od Calculate the observed distribution for the category attribute of the <p> A <ref> [g n ; 1; v n1 ] </ref> A [g n ; 1; v n1 ] 1 v n1 Value in the range 1::N 1 picked according to the distribution Y A [g n ; 1; v n1 ] A [g n ; 1; v n1 ] + 1 od Calculate the observed distribution for the category attribute of the test item, v n1 . for c 1::N 1 do q c F [c] = T od 8 test item. <p> A <ref> [g n ; 1; v n1 ] </ref> A [g n ; 1; v n1 ] 1 v n1 Value in the range 1::N 1 picked according to the distribution Y A [g n ; 1; v n1 ] A [g n ; 1; v n1 ] + 1 od Calculate the observed distribution for the category attribute of the test item, v n1 . for c 1::N 1 do q c F [c] = T od 8 test item. <p> The following quantities were calculated with respect to a test item, e V fl , as the simulation was run: p c = T t=1 g=1 n + ff A <ref> [g; 1; c] </ref> + fi 1 =N 1 j=2 C [g] + fi j One can then obtain the desired category probabilities: P (V fl1 = c j e V i = ev i ; V flj = v flj : 1 i n; 2 j m) p c = c
Reference: [2] <author> Andersen, E. B. </author> <title> (1982) Latent structure analysis: A survey, </title> <journal> Scandinavian Journal of Statistics, </journal> <volume> vol. 9, </volume> <pages> pp. 1-12. 21 </pages>
Reference-contexts: Introduction Mixture distributions [8, 20] are an appropriate tool for modeling processes whose output is thought to be generated by several different underlying mechanisms, or to come from several different populations. One aim of a mixture model analysis may be to identify and characterize these underlying "latent classes" <ref> [2, 7] </ref>, either for some scientific purpose, or as one realization of "unsupervised learning" in artificial intelligence. In other cases, prediction of future observations is the objective.
Reference: [3] <author> Celeux, G. and Cedex, C. </author> <title> (1986) Validity tests in cluster analysis using a probabilistic teacher algorithm, </title> <booktitle> COMPSTAT 1986: Proceedings in Computational Statistics, </booktitle> <address> Heidel-berg: </address> <publisher> Physica-Verlag. </publisher>
Reference-contexts: In the limit as the size of the sample taken grows, this stochastic form of the EM algorithm will give the same results as the exact form. At the other extreme, one might generate only a single value for each unobserved variable. This is the procedure used in <ref> [3] </ref> in the context of maximum likelihood estimation for a mixture of Gaussians.
Reference: [4] <author> Cheeseman, P., Kelly, J., Self, M., Stutz, J., Taylor, W., and Freeman, D. </author> <year> (1988) </year> <month> Auto-Class: </month> <title> A Bayesian classification system, </title> <booktitle> Proceedings of the Fifth International Conference on Machine Learning. </booktitle>
Reference-contexts: MAP estimation is algorithmically identical to the non-Bayesian "penalized likelihood" method. This method has been applied to latent class analysis using mixture models in the early versions of the AutoClass system of <ref> [4] </ref>. In this system, a conjugate prior is used for the model parameters (as in equation (3) here), allowing the MAP estimate to be found using the EM algorithm.
Reference: [5] <author> Dempster, A. P., Laird, N. M., and Rubin, D. B. </author> <title> (1977) Maximum likelihood from incomplete data via the EM algorithm (with discussion), </title> <journal> Journal of the Royal Statistical Society B, </journal> <volume> vol. 39, </volume> <pages> pp. 1-38. </pages>
Reference-contexts: Computation of the maximum likelihood parameters for a mixture model can conveniently be carried out by the EM algorithm <ref> [5, 8, 20] </ref>. In this iterative procedure, expec 9 tations for which unobserved mixture component underlies each data item are first computed using the current estimate of parameters (the E step), after which a new parameter estimate is computed using these expectations for the unobserved data (the M step).
Reference: [6] <author> Duda, R. O. and Hart, P. E. </author> <title> (1973) Pattern Classification and Scene Analysis, </title> <address> New York: </address> <publisher> John Wiley. </publisher>
Reference-contexts: One aim of a mixture model analysis may be to identify and characterize these underlying "latent classes" [2, 7], either for some scientific purpose, or as one realization of "unsupervised learning" in artificial intelligence. In other cases, prediction of future observations is the objective. In a "classification" application <ref> [6] </ref>, for example, we are interested in predicting the category attribute of an item on the basis of various indicator attributes.
Reference: [7] <author> Everitt, B. S. </author> <title> (1984) An Introduction to Latent Variable Models, </title> <publisher> London: Chapman and Hall. </publisher>
Reference-contexts: Introduction Mixture distributions [8, 20] are an appropriate tool for modeling processes whose output is thought to be generated by several different underlying mechanisms, or to come from several different populations. One aim of a mixture model analysis may be to identify and characterize these underlying "latent classes" <ref> [2, 7] </ref>, either for some scientific purpose, or as one realization of "unsupervised learning" in artificial intelligence. In other cases, prediction of future observations is the objective.
Reference: [8] <author> Everitt, B. S. and Hand, D. J. </author> <title> (1981) Finite Mixture Distributions, </title> <publisher> London: Chapman and Hall. </publisher>
Reference-contexts: Introduction Mixture distributions <ref> [8, 20] </ref> are an appropriate tool for modeling processes whose output is thought to be generated by several different underlying mechanisms, or to come from several different populations. <p> Computation of the maximum likelihood parameters for a mixture model can conveniently be carried out by the EM algorithm <ref> [5, 8, 20] </ref>. In this iterative procedure, expec 9 tations for which unobserved mixture component underlies each data item are first computed using the current estimate of parameters (the E step), after which a new parameter estimate is computed using these expectations for the unobserved data (the M step). <p> With some difficulty, classical hypothesis testing criteria can be used to decide between models with different numbers of components <ref> [8, 20] </ref>. The method of cross-validation [19] should also be applicable. Maximum a posteriori probability (MAP) estimation. Rather than use the parameters which maximize the likelihood, one can instead use the parameters with maximum posterior probability density | the product of likelihood and prior probability density.
Reference: [9] <author> Gelfand, A. E. and Smith, A. F. M. </author> <title> (1990) Sampling-based approaches to calculating marginal densities, </title> <journal> Journal of the American Statistical Association, </journal> <volume> vol. 85, </volume> <pages> pp. 398-409. </pages>
Reference-contexts: This problem arises in the context of statistical physics [16], image restoration [10], stochastic neural networks [1, 17], belief networks for expert systems [18], and general statistical calculation <ref> [12, 9] </ref>. In each case, the problem has been solved by a Monte Carlo simulation method, under names such as the "Metropolis algorithm", the "Boltzmann Machine", and the "Gibbs sampler". The simulation starts with arbitrary values ha 0 1 ; . . . ; a 0 n i.
Reference: [10] <author> Geman, S. and Geman, D. </author> <title> (1984) Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images, </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> vol. 6, </volume> <pages> pp. 721-741. </pages>
Reference-contexts: This problem arises in the context of statistical physics [16], image restoration <ref> [10] </ref>, stochastic neural networks [1, 17], belief networks for expert systems [18], and general statistical calculation [12, 9]. In each case, the problem has been solved by a Monte Carlo simulation method, under names such as the "Metropolis algorithm", the "Boltzmann Machine", and the "Gibbs sampler".
Reference: [11] <author> Hanson, R., Stutz, J., and Cheeseman, P. </author> <title> (1991) Bayesian classification with correlation and inheritance, </title> <booktitle> to be presented at the 12th International Joint Conference on Artificial Intelligence, </booktitle> <address> Sydney, Australia, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: Furthermore, if several local maxima are found, selecting between them on the basis of their posterior probability density will not, in general, give the result which is most likely to be close to the best value. Local approximations to the Bayesian solution. In later versions of the AutoClass system <ref> [11] </ref>, the EM algorithm is run from several starting points, with the number of mixture components set to various values.
Reference: [12] <author> Hastings, W. K. </author> <title> (1970) Monte Carlo sampling methods using Markov chains and their applications, </title> <journal> Biometrika, </journal> <volume> vol. 57, no. 1, </volume> <pages> pp. 97-109. </pages>
Reference-contexts: This problem arises in the context of statistical physics [16], image restoration [10], stochastic neural networks [1, 17], belief networks for expert systems [18], and general statistical calculation <ref> [12, 9] </ref>. In each case, the problem has been solved by a Monte Carlo simulation method, under names such as the "Metropolis algorithm", the "Boltzmann Machine", and the "Gibbs sampler". The simulation starts with arbitrary values ha 0 1 ; . . . ; a 0 n i.
Reference: [13] <author> Jain, A. K. </author> <title> (1988) Algorithms for Clustering Data, </title> <address> Englewood Cliffs, New Jersey: </address> <publisher> Prentice Hall. </publisher>
Reference-contexts: The items could then be clustered on the basis of this similarity matrix by any of several methods <ref> [13] </ref>. In this paper, I have assumed that the user's prior knowledge can be adequately captured by a choice of values for ff and the fi j in the prior distribution (equation (3)). Often, the real prior will not be so specific.
Reference: [14] <author> Kai-Tai, F. and Yao-Ting, Z. </author> <title> (1990) Generalized Multivariate Analysis, </title> <publisher> Berlin: Springer-Verlag. </publisher>
Reference-contexts: The integrals in the last formula are taken over the simplexes of valid probability distributions | S D = f hx 1 ; . . . ; x D i : x i 0; P These integrals are standard for Dirichlet distributions (see <ref> [14, section 2.4] </ref>, for example).
Reference: [15] <author> Levinson, S. E., Rabiner, L. R., and Sondhi, M. M. </author> <title> (1983) An introduction to the application of the theory of probabilistic functions of a Markov process to automatic speech recognition, </title> <journal> Bell System Technical Journal, </journal> <volume> vol. 62, no. </volume> <pages> 4. </pages>
Reference-contexts: Although the development in this paper is confined to discrete data, the technique should also be applicable when items have real-valued attributes modeled by independent Gaussian distributions if an appropriate conjugate prior is used. The method can also be extended to the Hidden Markov Models used in speech recognition <ref> [15] </ref>, and to the "noisy-OR" form of the stochastic neural networks described in [17]. Acknowledgements I thank David MacKay, Geoff Hinton, and the members of the Connectionist Research Group at the University of Toronto for helpful comments on earlier versions of this paper.
Reference: [16] <author> Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., and Teller, E. </author> <title> (1953) Equation of state calculations by fast computing machines, </title> <journal> Journal of Chemical Physics, </journal> <volume> vol. 21, no. 6, </volume> <pages> pp. 1087-1092. </pages>
Reference-contexts: This problem arises in the context of statistical physics <ref> [16] </ref>, image restoration [10], stochastic neural networks [1, 17], belief networks for expert systems [18], and general statistical calculation [12, 9]. In each case, the problem has been solved by a Monte Carlo simulation method, under names such as the "Metropolis algorithm", the "Boltzmann Machine", and the "Gibbs sampler".
Reference: [17] <author> Neal, R. M. </author> <title> (1990) Learning stochastic feedforward networks, </title> <institution> University of Toronto, Department of Computer Science, Connectionist Research Group, </institution> <note> Technical Report CRG-TR-90-7. </note>
Reference-contexts: This problem arises in the context of statistical physics [16], image restoration [10], stochastic neural networks <ref> [1, 17] </ref>, belief networks for expert systems [18], and general statistical calculation [12, 9]. In each case, the problem has been solved by a Monte Carlo simulation method, under names such as the "Metropolis algorithm", the "Boltzmann Machine", and the "Gibbs sampler". <p> The method can also be extended to the Hidden Markov Models used in speech recognition [15], and to the "noisy-OR" form of the stochastic neural networks described in <ref> [17] </ref>. Acknowledgements I thank David MacKay, Geoff Hinton, and the members of the Connectionist Research Group at the University of Toronto for helpful comments on earlier versions of this paper.
Reference: [18] <author> Pearl, J. </author> <title> (1987) Evidential reasoning using stochastic simulation of causal models, </title> <journal> Artificial 22 Intelligence, </journal> <volume> vol. 32, no. 2, </volume> <pages> pp. 245-257. </pages>
Reference-contexts: This problem arises in the context of statistical physics [16], image restoration [10], stochastic neural networks [1, 17], belief networks for expert systems <ref> [18] </ref>, and general statistical calculation [12, 9]. In each case, the problem has been solved by a Monte Carlo simulation method, under names such as the "Metropolis algorithm", the "Boltzmann Machine", and the "Gibbs sampler".
Reference: [19] <author> Stone, M. </author> <title> (1974) Cross-validatory choice and assessment of statistical predictions (with discussion), </title> <journal> Journal of the Royal Statistical Society B, </journal> <volume> vol. 36, </volume> <pages> pp. 111-147. </pages>
Reference-contexts: With some difficulty, classical hypothesis testing criteria can be used to decide between models with different numbers of components [8, 20]. The method of cross-validation <ref> [19] </ref> should also be applicable. Maximum a posteriori probability (MAP) estimation. Rather than use the parameters which maximize the likelihood, one can instead use the parameters with maximum posterior probability density | the product of likelihood and prior probability density.
Reference: [20] <author> Titterington, D. M., Smith, A. F. M., and Makov, U. E. </author> <title> (1985) Statistical Analysis of Finite Mixture Distributions, </title> <address> Chichester, New York: </address> <publisher> Wiley. </publisher>
Reference-contexts: Introduction Mixture distributions <ref> [8, 20] </ref> are an appropriate tool for modeling processes whose output is thought to be generated by several different underlying mechanisms, or to come from several different populations. <p> Computation of the maximum likelihood parameters for a mixture model can conveniently be carried out by the EM algorithm <ref> [5, 8, 20] </ref>. In this iterative procedure, expec 9 tations for which unobserved mixture component underlies each data item are first computed using the current estimate of parameters (the E step), after which a new parameter estimate is computed using these expectations for the unobserved data (the M step). <p> With some difficulty, classical hypothesis testing criteria can be used to decide between models with different numbers of components <ref> [8, 20] </ref>. The method of cross-validation [19] should also be applicable. Maximum a posteriori probability (MAP) estimation. Rather than use the parameters which maximize the likelihood, one can instead use the parameters with maximum posterior probability density | the product of likelihood and prior probability density.
Reference: [21] <author> Wei, G. C. G. and Tanner, M. A. </author> <title> (1990) A Monte Carlo implementation of the EM algorithm and the Poor Man's Data Augmentation algorithms, </title> <journal> Journal of the American Statistical Association, </journal> <volume> vol. 85, </volume> <pages> pp. 699-704. </pages>
Reference-contexts: Stochastic forms of the EM algorithm. In the E step of the EM algorithm, rather than calculate exact expectations for the unobserved data, one can instead generate a sample from the distribution for these variables conditional on the observed data and the current estimate of the model parameters <ref> [21] </ref>. In some situations, this procedure will be computationally more efficient than the standard E step (though this is not so for the mixture models considered in this paper).
Reference: [22] <author> Young, A. S. </author> <title> (1977) A Bayesian approach to prediction using polynomials, </title> <journal> Biometrika, </journal> <volume> vol. 64, no. 2, </volume> <pages> pp. 309-317. 23 </pages>
Reference-contexts: Infinite mixture models are thus an attractive option whenever the true number of components is unknown, since one thereby avoids the problem of selecting between models with different numbers of components. A similar idea has been applied to the problem of modeling data by polynomials of indefinite degree in <ref> [22] </ref>. Below, I will define the Bayesian formulation of the mixture modeling problem, and describe how it can be solved by Monte Carlo simulation.
References-found: 22

