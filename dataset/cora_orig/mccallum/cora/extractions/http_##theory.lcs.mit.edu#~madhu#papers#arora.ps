URL: http://theory.lcs.mit.edu/~madhu/papers/arora.ps
Refering-URL: http://theory.lcs.mit.edu/~madhu/papers.html
Root-URL: 
Title: Improved low-degree testing and its applications with high probability. Such a strong self-corrector is a
Author: Sanjeev Arora Madhu Sudan 
Date: 1  
Note: t 0:5, then the tester/corrector determines ffi and generates O(  input,  arora@cs.princeton.edu. Supported by an NSF CAREER award, an Alfred P. Sloan Fellowship, and a Packard Fellowship. madhu@lcs.mit.edu. Laboratory for Computer Science, MIT, Cambridge, MA 02139. Part of this work was done when this author was at the IBM Thomas J. Watson Research Center.  
Affiliation: Princeton University  MIT  
Abstract: NP = PCP(log n; 1) and related results crucially depend upon the close connection between the probability with which a function passes a low degree test and the distance of this function to the nearest degree d polynomial. In this paper we study a test proposed by Rubinfeld and Sudan [RS93]. The strongest previously known connection for this test states that a function passes the test with probability ffi for some ffi &gt; 7=8 iff the function has agreement ffi with a polynomial of degree d. We present a new, and surprisingly strong, analysis which shows that the preceding statement is true for arbitrarily small ffi, provided the field size is polynomially larger than d=ffi. The analysis uses a version of Hilbert irreducibility, a tool of algebraic geometry. As a consequence we obtain an alternate construction for the following proof system: A constant prover 1-round proof system for NP languages in which the verifier uses O(log n) random bits, receives answers of size O(log n) bits, and has an error prob ability of at most 2 log 1* n . Such a proof system, which implies the NP-hardness of approximating Set Cover to within (log n) factors, has already been obtained by Raz and Safra [RazS96]. Raz and Safra obtain their result by giving a strong analysis, in the sense described above, of a new low-degree test that they present. A second consequence of our analysis is a self tester/corrector for any buggy program that (supposedly) computes a polynomial over a finite field. If the program is correct only on ffi fraction of inputs where ffi = 1= jFj * ffi ) values for every input, such that one of them is the correct output. In fact, our results yield something stronger: Given the buggy program, we can construct O( 1 ffi ) randomized programs such that one of them is correct on every
Abstract-found: 1
Intro-found: 1
Reference: [ALRS92] <author> S. Ar, R. Lipton, R. Rubinfeld and M. Sudan. </author> <title> Reconstructing algebraic functions from noisy data. </title> <booktitle> Proceedings of the 33rd Symposium on Foundations of Computer Science, IEEE, </booktitle> <year> 1992. </year>
Reference-contexts: As for correction, note 2 that its meaning is not clear when ffi &lt; 1=2, since as many as O (1=ffi) polynomials could have agreement ffi with the program. Two notions of correction are possible, as noted in <ref> [ALRS92] </ref>. The weaker one is that for each input, the corrector outputs O (1=ffi) values, one of which is correct. Such a corrector is given in [Su96]. The stronger notion is that the corrector creates O ( 1 ffi ) programs (polynomials) such that w.h.p. one of them is correct. <p> ij y 2 a i : : : = 0 i=0 j=0 n a i Since (1 + d x )(1 + d y ), the number of variables, exceeds n, the number of constraints, a nontrivial solution exists. 2 Then Sudan uses the following lemma from Ar et al. <ref> [ALRS92] </ref>. <p> This task was left as an open problem in Ar et al. <ref> [ALRS92] </ref>, and no polynomial (in m, d and 1 ffi ) time algorithm was known for this problem. Goldreich et al. [GRS] solve this task when ffi 2 p d=q in time exponential in d.
Reference: [A93] <author> S. Arora Unpublished, </author> <year> 1993. </year>
Reference-contexts: We remark that a similar statement had earlier been proved for really large fields, satisfying jFj &gt; 2 O (m+d+1=ffi) <ref> [A93, T93] </ref>. However, most applications require the field size to be poly (m; d; 1=ffi). We also prove a related result, Theorem 6, which is more useful for constructing efficient PCP-style verifiers.
Reference: [A94] <author> S. Arora. </author> <title> Probabilistic Checking of Proofs and Hardness of Approximation Problems. </title> <type> PhD thesis, </type> <institution> U.C. Berkeley, </institution> <year> 1994. </year> <note> Available from http://www.cs.princeton.edu/~ arora </note> . 
Reference-contexts: Paper organization. We state and explain our main theorem (Theorem 1) and its corollaries (Theorems 6 and 7) in Section 2. We prove the theorem in Section 3. This proof resembles proofs of earlier results <ref> [RS93, ALMSS92, A94, FS95] </ref>, in that it has two parts. <p> Then in Section 3.3 we "bootstrap" to allow larger m. This part uses probabilistic arguments and relies upon the cases m = 2; 3 (including Theorems 6 and 7 for the cases m = 2; 3). It is inspired by the "symmetry-based" approach of Arora <ref> [A94] </ref>. Finally, the appendix contains the construction of constant prover 1-round proof systems and proofs of many lemmas. 2 The Low-degree Test Let F be a finite field and m; d be integers. <p> We rely on symmetry-based arguments similar to those in <ref> [A94] </ref>. These use the notion of a k-dimensional subspace of F m . Definition 3 Let m; k 2 Z + and k &lt; m. <p> The two phases are based on the analysis of the bootstrapping method described in Section 3.3 which is in turn based on the work of Arora <ref> [A94] </ref>. Both phases use the algorithm of Sudan [Su96].
Reference: [ABSS93] <author> S. Arora, L. Babai, J. Stern and Z. Sweedyk. </author> <title> The hardness of approximating problems defined by linear constraints. </title> <booktitle> Proceedings of the 34th Symposium on Foundations of Computer Science, IEEE, </booktitle> <year> 1993. </year>
Reference-contexts: For most of these problems it implies NP-hardness, but for some |most notably the problem of approximating set cover within a factor O (log n) and an entire set of problems in <ref> [ABSS93] </ref> | it is only known to imply quasi-NP-hardness (a quasi-NP-hard problem has a polynomial-time algorithm only if NP Time (n polylog (n) )). Plugging our improved analysis of the low degree test into known constructions leads to very efficient constant-prover 1-round proof systems for NP. <p> So long as we use the verifier composition idea of [AS98], 3 provers appears to be the best possible. Reducing the number of provers to 2 would imply the NP-hardness of approximation problems dealt with in <ref> [ABSS93] </ref>. Thanks Sanjeev Arora thanks Laci Babai and Kati Friedl for introducing him to "symmetry-based" arguments for the low degree test in summer 1993. We thank Dick Lipton for saving us from fruitless labor on an incorrect conjecture on irreducibility (he provided a counterexample).
Reference: [ALMSS92] <author> S. Arora, C. Lund, R. Motwani, M. Sudan and M. Szegedy. </author> <title> Proof verification and the hardness of approximation problems. </title> <booktitle> Proceedings of the 33rd Symposium on Foundations of Computer Science, IEEE, </booktitle> <year> 1992. </year>
Reference-contexts: 1 Introduction The use of algebraic techniques has recently led to new (probabilistic) characterizations of traditional complexity classes. These characterizations involve an interaction between an untrustworthy prover (or many provers) and a probabilistic polynomial-time verifier. In MIP= NEXPTIME [BFL91], and NP = PCP (log n; 1) <ref> [AS98, ALMSS92] </ref> the verifier has to probabilistically verify the satisfiability of a boolean formula by reading very few bits in a "proof string" presented by a prover. In IP=PSPACE [LFKN92, Sh92] the verifier has to probabilistically verify tautologyhood of a quantified boolean formulae by interacting with a prover. <p> Recall that NP=PCP (log n; 1) implies the hardness of computing approximate solutions to many optimization problems such as clique [FGLSS91, AS98], chromatic number and set cover [LY93], and max-3sat <ref> [ALMSS92] </ref>. <p> Paper organization. We state and explain our main theorem (Theorem 1) and its corollaries (Theorems 6 and 7) in Section 2. We prove the theorem in Section 3. This proof resembles proofs of earlier results <ref> [RS93, ALMSS92, A94, FS95] </ref>, in that it has two parts. <p> In particular, there exists at least one line for which the two events in the preceding paragraph happen. Let l 0 be such a line. The existing analysis of the low degree test <ref> [ALMSS92] </ref> implies that for each ffi 2 [0:9; 1], every function with d-success-rate at least 1 ffi=24 has agreement at least 1 ffi=12 with some degree d polynomial. Let g be this polynomial for f . <p> Furthermore, the circuit size of the verifier is polynomial in a (n). To construct very efficient O (1)-prover 1-round proof systems for SAT we use two standard techniques. First we plug our low degree test into a construction of <ref> [ALMSS92] </ref> to get a proof system with 3 provers that uses O (log n) random bits but the oracles in the proof have answer size 2 log fi n for some fi &lt; 1 (see Lemma 34).
Reference: [AS98] <author> S. Arora and S. Safra. </author> <title> Probabilistic checking of proofs: a new characterization of NP. </title> <journal> Journal of the ACM. </journal> <note> Preliminary version in Proceedings of the 33rd Symposium on Foundations of Computer Science, IEEE, </note> <year> 1992. </year>
Reference-contexts: 1 Introduction The use of algebraic techniques has recently led to new (probabilistic) characterizations of traditional complexity classes. These characterizations involve an interaction between an untrustworthy prover (or many provers) and a probabilistic polynomial-time verifier. In MIP= NEXPTIME [BFL91], and NP = PCP (log n; 1) <ref> [AS98, ALMSS92] </ref> the verifier has to probabilistically verify the satisfiability of a boolean formula by reading very few bits in a "proof string" presented by a prover. In IP=PSPACE [LFKN92, Sh92] the verifier has to probabilistically verify tautologyhood of a quantified boolean formulae by interacting with a prover. <p> Recall that NP=PCP (log n; 1) implies the hardness of computing approximate solutions to many optimization problems such as clique <ref> [FGLSS91, AS98] </ref>, chromatic number and set cover [LY93], and max-3sat [ALMSS92]. <p> Existing analyses of all low degree tests cannot say anything meaningful about f if ffi &lt; 1=2; in fact the analyses of <ref> [FGLSS91, GLRSW91, RS93, AS98] </ref> require ffi &gt; 1 O (1=d). A crucial ingredient of NP=PCP (log n; 1) was an analysis (actually just a combination of the analyses of [AS98, RS93]) of the above test that worked for any ffi &gt; 1 *, where * &gt; 0 is fixed. <p> A crucial ingredient of NP=PCP (log n; 1) was an analysis (actually just a combination of the analyses of <ref> [AS98, RS93] </ref>) of the above test that worked for any ffi &gt; 1 *, where * &gt; 0 is fixed. <p> Now we claim that we can find a nontrivial solution Q that in addition is in F [y][z; x] and satisfies deg y (Q) dl 3=2 . The reason is that Q is obtained by Cramer's Rule (see Fact 25 in <ref> [AS98] </ref>) on a system of l constraints, which calls for computing determinants of (l 1) fi (l 1) submatrices. The determinant of such a submatrix is a polynomial of degree l1 in the matrix entries. <p> Then we use "verifier composition," a technique from <ref> [AS98] </ref>, to reduce the answer size to O (log n) (the number of oracles stays O (1)). By composing our verifier with a verifier of Bellare et al. [BGLR93] we obtain a 5-prover proof system with error 2 log 1=3 n . <p> This concludes the proof of the correctness of the self-correction algorithm. 2 5 Conclusions We do not know how to reduce the number of provers in our constructions of constant prover protocols to 2. So long as we use the verifier composition idea of <ref> [AS98] </ref>, 3 provers appears to be the best possible. Reducing the number of provers to 2 would imply the NP-hardness of approximation problems dealt with in [ABSS93]. Thanks Sanjeev Arora thanks Laci Babai and Kati Friedl for introducing him to "symmetry-based" arguments for the low degree test in summer 1993.
Reference: [BFL91] <author> L. Babai, L. Fortnow, and C. Lund. </author> <title> Non-deterministic exponential time has two-prover interactive protocols. </title> <journal> Computational Complexity, </journal> <volume> 1 </volume> <pages> 3-40, </pages> <year> 1991. </year> <month> 25 </month>
Reference-contexts: 1 Introduction The use of algebraic techniques has recently led to new (probabilistic) characterizations of traditional complexity classes. These characterizations involve an interaction between an untrustworthy prover (or many provers) and a probabilistic polynomial-time verifier. In MIP= NEXPTIME <ref> [BFL91] </ref>, and NP = PCP (log n; 1) [AS98, ALMSS92] the verifier has to probabilistically verify the satisfiability of a boolean formula by reading very few bits in a "proof string" presented by a prover. <p> If this is the case, accept. This test is similar in flavor to all other known low degree tests, such as the original test in <ref> [BFL91] </ref> and later ones in [BFLS91, FGLSS91, GLRSW91]. (Many of those tests check the degree of the polynomial in each variable, whereas the test we described checks the total degree.) Let ffi denote the probability with which f passes the low-degree test.
Reference: [BFLS91] <author> L. Babai, L. Fortnow, L. Levin, and M. Szegedy. </author> <title> Checking computations in polylogarithmic time. </title> <booktitle> Proceedings of the 23rd Annual Symposium on Theory of Computing, ACM, </booktitle> <year> 1991. </year>
Reference-contexts: If this is the case, accept. This test is similar in flavor to all other known low degree tests, such as the original test in [BFL91] and later ones in <ref> [BFLS91, FGLSS91, GLRSW91] </ref>. (Many of those tests check the degree of the polynomial in each variable, whereas the test we described checks the total degree.) Let ffi denote the probability with which f passes the low-degree test.
Reference: [BGS95] <author> M. Bellare, O. Goldreich and M. Sudan. </author> <title> Free bits, PCPs and non-approximability | towards tight results. </title> <booktitle> Proceedings of the 36th Symposium on Foundations of Computer Science, IEEE, 1995. TR95-024 of ECCC, the Electronic Colloquium on Computational Complexity, </booktitle> <address> http://www.eccc.uni-trier.de/eccc/. </address>
Reference-contexts: By the Chebyshev inequality, Pr [jX C j ffffi] V [X C ] We show next (using an argument from <ref> [BGS95] </ref>) that V [X C ] 2= jFj, thus proving the lemma.
Reference: [BGLR93] <author> M. Bellare, S. Goldwasser, C. Lund, and A. Russell. </author> <title> Efficient probabilistically checkable proofs. </title> <booktitle> Proceedings of the 25th Annual Symposium on Theory of Computing, ACM, 1993. (See also Errata sheet in Proceedings of the 26th Annual Symposium on Theory of Computing, ACM, </booktitle> <year> 1994). </year>
Reference-contexts: Such systems imply the NP-hardness of approximating Set Cover to within a factor of O (ln n) (see the reduction of [LY93], adapted for more than 2 provers in <ref> [BGLR93] </ref>). Raz and Safra [RazS96] had before us constructed such systems; so our construction can be viewed as an alternative proof of their result. <p> Finding such a corrector was an open problem. We provide such a corrector in Section 4.2. Past work. The first construction of a nontrivial constant prover 1-round proof system for NP appeared in [LS91]; others appeared in <ref> [FL92, BGLR93, T94, FK94, R95] </ref>. These systems could not reduce the error probability to below a constant while using O (log n) random bits (the best construction needs O (k log n) random bits to make the error probability 2 k ; see [R95]). <p> Then we use "verifier composition," a technique from [AS98], to reduce the answer size to O (log n) (the number of oracles stays O (1)). By composing our verifier with a verifier of Bellare et al. <ref> [BGLR93] </ref> we obtain a 5-prover proof system with error 2 log 1=3 n . We describe the result below and prove it in Section B. Theorem 21 NP MIP [5; O (log n); O (log n); 2 log 1=3 n ].
Reference: [BLR90] <author> M. Blum, M. Luby, and R. Rubinfeld. </author> <title> Self-testing/correcting with applications to numerical problems. </title> <booktitle> In Proc. 22nd ACM Symp. on Theory of Computing, </booktitle> <pages> pages 73-83, </pages> <year> 1990. </year>
Reference-contexts: Also, a future paper will contain full details of that construction.) Application to program testing/correcting. Suppose we are given a potentially buggy program that purportedly computes an (unknown) m-variate polynomial over a finite field F. Program testing/correcting <ref> [BLR90] </ref> concerns the following problems: (i) testing: determine ffi, the fraction of points at which this program is correct and (ii) correction: for each input, correct the output of the program in case it is incorrect.
Reference: [F96] <author> U. Feige. </author> <title> A threshold of ln n for Set Cover. </title> <booktitle> Proceedings of the 28th Annual Symposium on Theory of Computing, ACM, </booktitle> <year> 1996. </year>
Reference: [FGLSS91] <author> U. Feige, S. Goldwasser, L. L ovasz, S. Safra and M. Szegedy. </author> <title> Interactive proofs and the hardness of approximating cliques. </title> <journal> Journal of the ACM, </journal> <volume> 43(2) </volume> <pages> 268-292, </pages> <year> 1996. </year>
Reference-contexts: Recall that NP=PCP (log n; 1) implies the hardness of computing approximate solutions to many optimization problems such as clique <ref> [FGLSS91, AS98] </ref>, chromatic number and set cover [LY93], and max-3sat [ALMSS92]. <p> If this is the case, accept. This test is similar in flavor to all other known low degree tests, such as the original test in [BFL91] and later ones in <ref> [BFLS91, FGLSS91, GLRSW91] </ref>. (Many of those tests check the degree of the polynomial in each variable, whereas the test we described checks the total degree.) Let ffi denote the probability with which f passes the low-degree test. <p> Existing analyses of all low degree tests cannot say anything meaningful about f if ffi &lt; 1=2; in fact the analyses of <ref> [FGLSS91, GLRSW91, RS93, AS98] </ref> require ffi &gt; 1 O (1=d). A crucial ingredient of NP=PCP (log n; 1) was an analysis (actually just a combination of the analyses of [AS98, RS93]) of the above test that worked for any ffi &gt; 1 *, where * &gt; 0 is fixed.
Reference: [FK94] <author> U. Feige and J. Kilian. </author> <title> Two prover protocols Low error at affordable rates. </title> <booktitle> Proceedings of the 26th Annual Symposium on Theory of Computing, ACM, </booktitle> <year> 1994. </year>
Reference-contexts: Finding such a corrector was an open problem. We provide such a corrector in Section 4.2. Past work. The first construction of a nontrivial constant prover 1-round proof system for NP appeared in [LS91]; others appeared in <ref> [FL92, BGLR93, T94, FK94, R95] </ref>. These systems could not reduce the error probability to below a constant while using O (log n) random bits (the best construction needs O (k log n) random bits to make the error probability 2 k ; see [R95]).
Reference: [FK95] <author> U. Feige and J. Kilian. </author> <title> Impossibility results for recycling random bits in two-prover proof systems. </title> <booktitle> Proceedings of the 27th Annual Symposium on Theory of Computing, ACM, </booktitle> <year> 1995. </year>
Reference-contexts: These systems could not reduce the error probability to below a constant while using O (log n) random bits (the best construction needs O (k log n) random bits to make the error probability 2 k ; see [R95]). It was also known <ref> [FK95] </ref> that some obvious ideas (such as "recycling randomness") cannot let us get around this. Slightly prior to our work, Raz and Safra [RazS96] found a construction of a proof system achieving subconstant error.
Reference: [FL92] <author> U. Feige and L. L ovasz. </author> <title> Two-prover one-round proof systems: Their power and their problems. </title> <booktitle> Proceedings of the 24th Annual Symposium on Theory of Computing, ACM, </booktitle> <year> 1992. </year>
Reference-contexts: Finding such a corrector was an open problem. We provide such a corrector in Section 4.2. Past work. The first construction of a nontrivial constant prover 1-round proof system for NP appeared in [LS91]; others appeared in <ref> [FL92, BGLR93, T94, FK94, R95] </ref>. These systems could not reduce the error probability to below a constant while using O (log n) random bits (the best construction needs O (k log n) random bits to make the error probability 2 k ; see [R95]).
Reference: [FS95] <author> K. Friedl and M. Sudan. </author> <title> Some improvements to low-degree tests. </title> <booktitle> Proceedings of the Third Israel Symposium on Theory and Computing Systems, IEEE, </booktitle> <year> 1995. </year>
Reference-contexts: This analysis showed that if a function f passes the test with probability ffi &gt; 1 *, then there exists a degree d polynomial that has agreement ffi with f . (The value of * for which this is true was later improved to 1=8 <ref> [FS95] </ref>.) In this paper we present an analysis (see Theorem 7) that continues to say something meaningful about f even when ffi is fairly close to 0. <p> Paper organization. We state and explain our main theorem (Theorem 1) and its corollaries (Theorems 6 and 7) in Section 2. We prove the theorem in Section 3. This proof resembles proofs of earlier results <ref> [RS93, ALMSS92, A94, FS95] </ref>, in that it has two parts. <p> The converse can also be shown to be true: if on every line in F m , the values of f are described by a univariate degree-d polynomial and F is sufficiently large (q (d + 1)( p p1 ), where p is the characteristic of the field <ref> [FS95] </ref>), then f must be a degree-d polynomial. The low degree test is presented with f : F m ! F, and an integer d. It is also presented a table that is meant to be a "proof" that f is a degree d polynomial.
Reference: [GLRSW91] <author> P. Gemmell, R. Lipton, R. Rubinfeld, M. Sudan and A. Wigderson. </author> <title> Self-testing/correcting for polynomials and for approximate functions. </title> <booktitle> Proceedings of the 23rd Annual Symposium on Theory of Computing, ACM, </booktitle> <year> 1991. </year>
Reference-contexts: If this is the case, accept. This test is similar in flavor to all other known low degree tests, such as the original test in [BFL91] and later ones in <ref> [BFLS91, FGLSS91, GLRSW91] </ref>. (Many of those tests check the degree of the polynomial in each variable, whereas the test we described checks the total degree.) Let ffi denote the probability with which f passes the low-degree test. <p> Existing analyses of all low degree tests cannot say anything meaningful about f if ffi &lt; 1=2; in fact the analyses of <ref> [FGLSS91, GLRSW91, RS93, AS98] </ref> require ffi &gt; 1 O (1=d). A crucial ingredient of NP=PCP (log n; 1) was an analysis (actually just a combination of the analyses of [AS98, RS93]) of the above test that worked for any ffi &gt; 1 *, where * &gt; 0 is fixed.
Reference: [GRS] <author> O. Goldreich, R. Rubinfeld and M. Sudan. </author> <title> Learning polynomials with queries: The highly noisy case. </title> <booktitle> Proceedings of the 36th Symposium on Foundations of Computer Science, IEEE, </booktitle> <year> 1995. </year>
Reference-contexts: This task was left as an open problem in Ar et al. [ALRS92], and no polynomial (in m, d and 1 ffi ) time algorithm was known for this problem. Goldreich et al. <ref> [GRS] </ref> solve this task when ffi 2 p d=q in time exponential in d. We now describe our solution that works when ffi (md=q) * , for some positive *, and is the first polynomial time-bounded solution for any ffi &lt; 1=2.
Reference: [K85] <author> E. Kaltofen. </author> <title> Polynomial-time reductions from multivariate to bi- and univariate integral polynomial factorization. </title> <journal> SIAM Journal on Computing, </journal> <volume> 14(2) </volume> <pages> 469-489, </pages> <year> 1985. </year>
Reference-contexts: First in Section 3.1 we prove the theorem when m is constant (specifically, m = 2; 3); this uses algebraic arguments inspired by Sudan's [Su96] work on reconstructing polynomials from very noisy data and Kaltofen's work on "Effective Hilbert Irreducibility" <ref> [K85, K95] </ref>. Then in Section 3.3 we "bootstrap" to allow larger m. This part uses probabilistic arguments and relies upon the cases m = 2; 3 (including Theorems 6 and 7 for the cases m = 2; 3). It is inspired by the "symmetry-based" approach of Arora [A94].
Reference: [K85] <author> E. Kaltofen. </author> <title> Effective Hilbert irreducibility. </title> <journal> Information and Control, </journal> <volume> 66 </volume> <pages> 123-137, </pages> <year> 1985. </year>
Reference-contexts: First in Section 3.1 we prove the theorem when m is constant (specifically, m = 2; 3); this uses algebraic arguments inspired by Sudan's [Su96] work on reconstructing polynomials from very noisy data and Kaltofen's work on "Effective Hilbert Irreducibility" <ref> [K85, K95] </ref>. Then in Section 3.3 we "bootstrap" to allow larger m. This part uses probabilistic arguments and relies upon the cases m = 2; 3 (including Theorems 6 and 7 for the cases m = 2; 3). It is inspired by the "symmetry-based" approach of Arora [A94].
Reference: [K95] <author> E. Kaltofen. </author> <title> Effective Noether irreducibility forms and applications. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 50(2) </volume> <pages> 274-295, </pages> <year> 1995. </year>
Reference-contexts: First in Section 3.1 we prove the theorem when m is constant (specifically, m = 2; 3); this uses algebraic arguments inspired by Sudan's [Su96] work on reconstructing polynomials from very noisy data and Kaltofen's work on "Effective Hilbert Irreducibility" <ref> [K85, K95] </ref>. Then in Section 3.3 we "bootstrap" to allow larger m. This part uses probabilistic arguments and relies upon the cases m = 2; 3 (including Theorems 6 and 7 for the cases m = 2; 3). It is inspired by the "symmetry-based" approach of Arora [A94].
Reference: [LS91] <author> D. Lapidot and A. Shamir. </author> <title> Fully Parallelized Multi-prover protocols for NEXP-time. </title> <booktitle> Proceedings of the 32nd Symposium on Foundations of Computer Science, IEEE, </booktitle> <year> 1991. </year>
Reference-contexts: Finding such a corrector was an open problem. We provide such a corrector in Section 4.2. Past work. The first construction of a nontrivial constant prover 1-round proof system for NP appeared in <ref> [LS91] </ref>; others appeared in [FL92, BGLR93, T94, FK94, R95]. These systems could not reduce the error probability to below a constant while using O (log n) random bits (the best construction needs O (k log n) random bits to make the error probability 2 k ; see [R95]).
Reference: [LFKN92] <author> C. Lund, L. Fortnow, H. Karloff, and N. Nisan. </author> <title> Algebraic methods for interactive proof systems. </title> <journal> Journal of the ACM, </journal> <volume> 39(4) </volume> <pages> 859-868, </pages> <year> 1992. </year> <month> 26 </month>
Reference-contexts: In MIP= NEXPTIME [BFL91], and NP = PCP (log n; 1) [AS98, ALMSS92] the verifier has to probabilistically verify the satisfiability of a boolean formula by reading very few bits in a "proof string" presented by a prover. In IP=PSPACE <ref> [LFKN92, Sh92] </ref> the verifier has to probabilistically verify tautologyhood of a quantified boolean formulae by interacting with a prover. <p> All these results fundamentally rely on the same idea: the verifier first arithmetizes (or algebraizes) the boolean formula, which involves viewing a boolean assignment not as a sequence of bits but as values of a polynomial <ref> [LFKN92] </ref>. From then on, verifying satisfiability or tautologyhood involves verifying | using some efficient algebraic procedures | specific properties of a polynomial that has been provided by the prover.
Reference: [LY93] <author> C. Lund and M. Yannakakis. </author> <title> On the hardness of approximating minimization prob-lems. </title> <booktitle> Proceedings of the 25th Annual Symposium on Theory of Computing, ACM, </booktitle> <year> 1993. </year>
Reference-contexts: Recall that NP=PCP (log n; 1) implies the hardness of computing approximate solutions to many optimization problems such as clique [FGLSS91, AS98], chromatic number and set cover <ref> [LY93] </ref>, and max-3sat [ALMSS92]. <p> Plugging our improved analysis of the low degree test into known constructions leads to very efficient constant-prover 1-round proof systems for NP. Such systems imply the NP-hardness of approximating Set Cover to within a factor of O (ln n) (see the reduction of <ref> [LY93] </ref>, adapted for more than 2 provers in [BGLR93]). Raz and Safra [RazS96] had before us constructed such systems; so our construction can be viewed as an alternative proof of their result.
Reference: [PS94] <author> A. Polishchuk and D. Spielman. </author> <title> Nearly Linear Sized Holographic Proofs. </title> <booktitle> Proceedings of the 26th Annual Symposium on Theory of Computing, ACM, </booktitle> <year> 1994. </year>
Reference: [R95] <author> R. Raz. </author> <title> A parallel repetition theorem. </title> <booktitle> Proceedings of the 27th Annual Symposium on Theory of Computing, ACM, </booktitle> <year> 1995. </year>
Reference-contexts: Finding such a corrector was an open problem. We provide such a corrector in Section 4.2. Past work. The first construction of a nontrivial constant prover 1-round proof system for NP appeared in [LS91]; others appeared in <ref> [FL92, BGLR93, T94, FK94, R95] </ref>. These systems could not reduce the error probability to below a constant while using O (log n) random bits (the best construction needs O (k log n) random bits to make the error probability 2 k ; see [R95]). <p> These systems could not reduce the error probability to below a constant while using O (log n) random bits (the best construction needs O (k log n) random bits to make the error probability 2 k ; see <ref> [R95] </ref>). It was also known [FK95] that some obvious ideas (such as "recycling randomness") cannot let us get around this. Slightly prior to our work, Raz and Safra [RazS96] found a construction of a proof system achieving subconstant error.
Reference: [RazS96] <author> R. Raz and S. Safra. </author> <type> Personal communication. </type> <month> March </month> <year> 1996. </year> <type> Manuscript. </type> <month> September </month> <year> 1996. </year>
Reference-contexts: Such systems imply the NP-hardness of approximating Set Cover to within a factor of O (ln n) (see the reduction of [LY93], adapted for more than 2 provers in [BGLR93]). Raz and Safra <ref> [RazS96] </ref> had before us constructed such systems; so our construction can be viewed as an alternative proof of their result. <p> It was also known [FK95] that some obvious ideas (such as "recycling randomness") cannot let us get around this. Slightly prior to our work, Raz and Safra <ref> [RazS96] </ref> found a construction of a proof system achieving subconstant error. Their analysis also relies on a low degree test, albeit a new one and with a very different correctness proof than ours.
Reference: [RS93] <author> R. Rubinfeld and M. Sudan. </author> <title> Robust characterizations of polynomials with applications to program testing. </title> <journal> SIAM Journal on Computing 25:2, </journal> <pages> pp. 252-271, </pages> <year> 1996. </year>
Reference-contexts: Both d and are inputs to the test.) The low degree test is allowed to be probabilistic and it has to read as few values of f as possible. 1 We are interested in a test described in <ref> [RS93] </ref> that works roughly as follows: Pick a random "line" in F m and verify that the restriction of f to this line agrees significantly with some univariate degree d polynomial. If this is the case, accept. <p> Existing analyses of all low degree tests cannot say anything meaningful about f if ffi &lt; 1=2; in fact the analyses of <ref> [FGLSS91, GLRSW91, RS93, AS98] </ref> require ffi &gt; 1 O (1=d). A crucial ingredient of NP=PCP (log n; 1) was an analysis (actually just a combination of the analyses of [AS98, RS93]) of the above test that worked for any ffi &gt; 1 *, where * &gt; 0 is fixed. <p> A crucial ingredient of NP=PCP (log n; 1) was an analysis (actually just a combination of the analyses of <ref> [AS98, RS93] </ref>) of the above test that worked for any ffi &gt; 1 *, where * &gt; 0 is fixed. <p> Paper organization. We state and explain our main theorem (Theorem 1) and its corollaries (Theorems 6 and 7) in Section 2. We prove the theorem in Section 3. This proof resembles proofs of earlier results <ref> [RS93, ALMSS92, A94, FS95] </ref>, in that it has two parts.
Reference: [Sch80] <author> J. T. Schwartz. </author> <title> Probabilistic algorithms for verification of polynomial identities. </title> <journal> Journal of the ACM, </journal> <volume> 27 </volume> <pages> 701-717, </pages> <year> 1980. </year>
Reference-contexts: Before going on to establish these two forms, we need some elementary results about polynomials. We start with the basic distance property of polynomials. Lemma 2 (Schwartz <ref> [Sch80] </ref>) A non-zero m-variate degree D polynomial can be zero at no more than D= jF j fraction of points in F m .
Reference: [Sh92] <author> A. Shamir. </author> <title> IP = PSPACE. </title> <journal> Journal of the ACM, </journal> <volume> 39(4) </volume> <pages> 869-877, </pages> <year> 1992. </year>
Reference-contexts: In MIP= NEXPTIME [BFL91], and NP = PCP (log n; 1) [AS98, ALMSS92] the verifier has to probabilistically verify the satisfiability of a boolean formula by reading very few bits in a "proof string" presented by a prover. In IP=PSPACE <ref> [LFKN92, Sh92] </ref> the verifier has to probabilistically verify tautologyhood of a quantified boolean formulae by interacting with a prover.
Reference: [Su96] <author> M. Sudan. </author> <title> Maximum likelihood decoding of Reed Solomon codes. </title> <booktitle> Proceedings of the 37th Symposium on Foundations of Computer Science, IEEE, </booktitle> <year> 1996. </year>
Reference-contexts: Two notions of correction are possible, as noted in [ALRS92]. The weaker one is that for each input, the corrector outputs O (1=ffi) values, one of which is correct. Such a corrector is given in <ref> [Su96] </ref>. The stronger notion is that the corrector creates O ( 1 ffi ) programs (polynomials) such that w.h.p. one of them is correct. Finding such a corrector was an open problem. We provide such a corrector in Section 4.2. Past work. <p> We prove the theorem in Section 3. This proof resembles proofs of earlier results [RS93, ALMSS92, A94, FS95], in that it has two parts. First in Section 3.1 we prove the theorem when m is constant (specifically, m = 2; 3); this uses algebraic arguments inspired by Sudan's <ref> [Su96] </ref> work on reconstructing polynomials from very noisy data and Kaltofen's work on "Effective Hilbert Irreducibility" [K85, K95]. Then in Section 3.3 we "bootstrap" to allow larger m. <p> Step 2 depends on a fairly difficult technical fact, Theorem 24, 8 which will be proved separately in Section A in the appendix. Step 1 is motivated by Sudan's <ref> [Su96] </ref> technique for reconstructing univariate polynomials from very noisy data, which we describe next. Sudan makes the following observation. <p> The two phases are based on the analysis of the bootstrapping method described in Section 3.3 which is in turn based on the work of Arora [A94]. Both phases use the algorithm of Sudan <ref> [Su96] </ref>. Preprocessing Phase: Pick a random line l through the m-dimensional space F m and reconstruct (using the algorithm of [Su96]) a list of all polynomials p 1 ; : : : ; p k agreeing with P on ffi=4 fraction of the inputs. <p> Both phases use the algorithm of Sudan <ref> [Su96] </ref>. Preprocessing Phase: Pick a random line l through the m-dimensional space F m and reconstruct (using the algorithm of [Su96]) a list of all polynomials p 1 ; : : : ; p k agreeing with P on ffi=4 fraction of the inputs. For each polynomial p i create a new program P i which works as follows: Query phase: Let x 2 F m be the query. <p> Reconstruct a list of all 4-variate polynomials g 1 ; : : : ; g k that agree with P on at least ffi=2 fraction of the points in D (again using the algorithm of <ref> [Su96] </ref>). If there exists a unique polynomial g j such that g j j l equals p i , then output the value of g j at x, else output FAIL.
Reference: [T93] <author> G. Tardos. </author> <type> Personal Communication, </type> <year> 1993. </year>
Reference-contexts: We remark that a similar statement had earlier been proved for really large fields, satisfying jFj &gt; 2 O (m+d+1=ffi) <ref> [A93, T93] </ref>. However, most applications require the field size to be poly (m; d; 1=ffi). We also prove a related result, Theorem 6, which is more useful for constructing efficient PCP-style verifiers.
Reference: [T94] <author> G. Tardos. </author> <title> Multi-prover encoding schemes and three-prover proof systems. </title> <booktitle> Proceedings of the 9th Annual Conference on Structure in Complexity Theory, IEEE, </booktitle> <year> 1994. </year>
Reference-contexts: Finding such a corrector was an open problem. We provide such a corrector in Section 4.2. Past work. The first construction of a nontrivial constant prover 1-round proof system for NP appeared in [LS91]; others appeared in <ref> [FL92, BGLR93, T94, FK94, R95] </ref>. These systems could not reduce the error probability to below a constant while using O (log n) random bits (the best construction needs O (k log n) random bits to make the error probability 2 k ; see [R95]).
Reference: [T96] <author> G. Tardos. </author> <type> Personal Communication, </type> <year> 1996. </year>
Reference-contexts: The number of provers in our construction grows as O (1=*). If we are willing to increase the error probability to 2 log 1=3 n then the number of provers is 5. (The number of provers can be reduced to 3 using a technique of Tardos <ref> [T96] </ref>). Whether or not the number of provers can be reduced to to 2 remains an open problem. Now we briefly describe low degree tests; see Section 2 for more details.
References-found: 35

