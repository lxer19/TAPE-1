URL: ftp://rtcl.eecs.umich.edu/outgoing/ashish/selfparam.ps
Refering-URL: http://www.eecs.umich.edu/~ashish/
Root-URL: http://www.cs.umich.edu
Email: mehraa@watson.ibm.com fzqwang,kgshing@eecs.umich.edu  
Phone: 704  
Title: Self-Parameterizing Protocol Stacks for Quality-of-Service Guarantees  
Author: Ashish Mehray, Zhiqun Wangz, and Kang Shinz 
Address: P.O. Box  Yorktown Heights, NY 10598  Ann Arbor, MI 48109  
Affiliation: yIBM T J Watson Research Center zReal-Time Computing Lab  Department of EECS  The University of Michigan  
Abstract: Recent trends in communication subsystem design reflect a continued thrust towards support for high-performance and high-function data transfer. One important type of value-added functionality is support for quality of service (QoS) guarantees within end host communication subsystems (and operating systems) and network elements such as routers. QoS guarantees on end-to-end communication are potentially useful for a large class of distributed multimedia and real-time applications. Building QoS-sensitive host communication subsystem software requires, in addition to significant architectural enhancements, an intimate knowledge of the overheads and costs associated with the host hardware and software platform. This dependency makes it extremely difficult to develop portable QoS-sensitive communication software that can be readily deployed across heterogeneous computing and network platforms while accommodating subsequent hardware or software platform upgrades. Suitable architectural mechanisms are needed to address these issues. We propose self-parameterizing protocol stacks as the basis for the design and development of QoS-sensitive communication subsystems. Self-parameterizing protocol stacks are designed with the ability to capture and represent communication subsystem performance as a first-class abstraction within the communication subsystem. Such an abstraction of the underlying communication subsystem permits reasoning about communication resource allocation and usage costs when admitting new connections for service. Coupled with built-in support for profiling and parameterization, it facilitates rapid and efficient re-targeting of QoS-sensitive communication subsystem software to new host platforms. We develop an architecture for efficient self-parameterization and describe a prototype implementation of a guaranteed-QoS communication service on Pentium-based PCs running the OSF Mach MK operating system. We evaluate the implementation experimentally to compare and contrast self-parameterization with manual profiling/parameterization. Using quantitative as well as qualitative arguments, we conclude that self-parameterizing protocol stacks significantly enhance the portability of QoS-sensitive communication subsystems. The work reported in this paper was performed at the University of Michigan and was supported in part by the National Science Foundation under grant MIP-9203895 and the Office of Naval Research under grants N00014-94-1-0229. Any opinions, findings, and conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of NSF or ONR. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Armando Fox, Steven D. Gribble, Eric A. Brewer, and Elan Amir, </author> <title> "Adapting to network and client variability via on-demand dynamic distillation," </title> <booktitle> in Proc. Int'l Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1996, </year> <pages> pp. 160-170. </pages>
Reference-contexts: Also, Web proxies and caches may perform intelligent data transformation and distillation functions to accommodate the capabilities of the clients requesting the data <ref> [1] </ref>. Similarly, high-function communication subsystems are being realized via communication middleware such as CORBA, further stressing the performance of the communication subsystem [2]. Not only does this trend increase protocol stack depth (and hence protocol processing latency), it also introduces additional data manipulations that impact communication subsystem performance.
Reference: [2] <author> Aniruddha Gokhale and Douglas C. Schmidt, </author> <title> "Measuring the performance of communication middleware on high-speed networks," </title> <booktitle> in Proc. of ACM SIGCOMM, </booktitle> <month> August </month> <year> 1996. </year>
Reference-contexts: Also, Web proxies and caches may perform intelligent data transformation and distillation functions to accommodate the capabilities of the clients requesting the data [1]. Similarly, high-function communication subsystems are being realized via communication middleware such as CORBA, further stressing the performance of the communication subsystem <ref> [2] </ref>. Not only does this trend increase protocol stack depth (and hence protocol processing latency), it also introduces additional data manipulations that impact communication subsystem performance.
Reference: [3] <author> Author1, Author2, Author3, </author> <note> "Reference omitted to preserve anonymity," </note> <year> 1997. </year>
Reference-contexts: Provision of QoS guarantees necessitates new architectural approaches for QoS-sensitive design of the host communication subsystem and the operating system <ref> [3, 4, 5] </ref>, and architectural enhancements to network elements such as routers. <p> As mentioned in Section 2.1, the run-time resource management in the service architecture is based in large part on the architecture proposed in <ref> [3] </ref>, with enhancements to accommodate the specific requirements of the implementation environment. Details on the internals of the service components and their interaction are provided in [18]. The CORDS-based protocol stack for our service is shown in Figure 5 (b).
Reference: [4] <author> R. Gopalakrishnan and G. M. Parulkar, </author> <title> "A real-time upcall facility for protocol processing with QoS guarantees," </title> <booktitle> in Proc. ACM Symp. on Operating Systems Principles, </booktitle> <month> December </month> <year> 1995, </year> <note> p. 231. </note>
Reference-contexts: Provision of QoS guarantees necessitates new architectural approaches for QoS-sensitive design of the host communication subsystem and the operating system <ref> [3, 4, 5] </ref>, and architectural enhancements to network elements such as routers.
Reference: [5] <author> David K. Y. Yau and Simon S. Lam, </author> <title> "An architecture towards efficient OS support for distributed multimedia," </title> <booktitle> in Proc. IS&T/SPIE Multimedia Computing and Networking, </booktitle> <month> January </month> <year> 1996. </year>
Reference-contexts: Provision of QoS guarantees necessitates new architectural approaches for QoS-sensitive design of the host communication subsystem and the operating system <ref> [3, 4, 5] </ref>, and architectural enhancements to network elements such as routers.
Reference: [6] <author> Caglan M. Aras, Jame F. Kurose, Douglas S. Reeves, and Henning Schulzrinne, </author> <title> "Real-time communication in packet-switched networks," </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> vol. 82, no. 1, </volume> <pages> pp. 122-139, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: For example, new QoS components for traffic enforcement and resource management (admission control and scheduling) are needed within the communication subsystem to allocate and share communication resources (such as CPU and network bandwidth, packet buffers) according to application QoS requirements <ref> [6] </ref>. Support for QoS guarantees on end-to-end communication represents an important set of functional and architectural enhancements being made to host communication subsystems.
Reference: [7] <author> Peter Druschel, Larry L. Peterson, and Bruce S. Davie, </author> <title> "Experiences with a high-speed network adaptor: A software perspective," </title> <booktitle> in Proc. of ACM SIGCOMM, </booktitle> <month> August </month> <year> 1994, </year> <pages> pp. 2-13. </pages>
Reference-contexts: A number of recent research efforts have developed efficient architectures and performance optimizations for the components constituting the communication subsystem, namely, the protocol stack and its interaction with the attached network interfaces <ref> [7, 8, 9, 10, 11, 12] </ref>. The primary thrust of these efforts has been to improve the average latency and throughput delivered by the communication subsystem to applications. <p> The primary thrust of these efforts has been to improve the average latency and throughput delivered by the communication subsystem to applications. The issues explored include optimizations to improve protocol processing latency [9, 10, 11], techniques to minimize data copies <ref> [7, 12] </ref>, and high-performance network interface design [8, 7]. However, while improving communication subsystem performance, these approaches are insufficient for the design and development of QoS-sensitive communication subsystems, as explained 2 later. <p> The primary thrust of these efforts has been to improve the average latency and throughput delivered by the communication subsystem to applications. The issues explored include optimizations to improve protocol processing latency [9, 10, 11], techniques to minimize data copies [7, 12], and high-performance network interface design <ref> [8, 7] </ref>. However, while improving communication subsystem performance, these approaches are insufficient for the design and development of QoS-sensitive communication subsystems, as explained 2 later. It is important to note that in traditional communication subsystem design the desired functionality is largely decoupled from performance.
Reference: [8] <author> Peter A. Steenkiste, </author> <title> "A systematic approach to host interface design for high-speed networks," </title> <booktitle> IEEE Computer, </booktitle> <pages> pp. 47-57, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: A number of recent research efforts have developed efficient architectures and performance optimizations for the components constituting the communication subsystem, namely, the protocol stack and its interaction with the attached network interfaces <ref> [7, 8, 9, 10, 11, 12] </ref>. The primary thrust of these efforts has been to improve the average latency and throughput delivered by the communication subsystem to applications. <p> The primary thrust of these efforts has been to improve the average latency and throughput delivered by the communication subsystem to applications. The issues explored include optimizations to improve protocol processing latency [9, 10, 11], techniques to minimize data copies [7, 12], and high-performance network interface design <ref> [8, 7] </ref>. However, while improving communication subsystem performance, these approaches are insufficient for the design and development of QoS-sensitive communication subsystems, as explained 2 later. It is important to note that in traditional communication subsystem design the desired functionality is largely decoupled from performance.
Reference: [9] <author> David Mosberger, Larry L. Peterson, Patrick G. Bridges, and Sean O'Malley, </author> <title> "Analysis of techniques to improve protocol processing latency," </title> <booktitle> in Proc. of ACM SIGCOMM, </booktitle> <month> October </month> <year> 1996, </year> <pages> pp. 73-84. </pages>
Reference-contexts: A number of recent research efforts have developed efficient architectures and performance optimizations for the components constituting the communication subsystem, namely, the protocol stack and its interaction with the attached network interfaces <ref> [7, 8, 9, 10, 11, 12] </ref>. The primary thrust of these efforts has been to improve the average latency and throughput delivered by the communication subsystem to applications. <p> The primary thrust of these efforts has been to improve the average latency and throughput delivered by the communication subsystem to applications. The issues explored include optimizations to improve protocol processing latency <ref> [9, 10, 11] </ref>, techniques to minimize data copies [7, 12], and high-performance network interface design [8, 7]. However, while improving communication subsystem performance, these approaches are insufficient for the design and development of QoS-sensitive communication subsystems, as explained 2 later. <p> This has significant implications for system parameterization since it highlights the difficulty in measuring various processing overheads accurately. Cache predictability may be improved via appropriate protocol implementation and compilation techniques <ref> [9] </ref>, or via cache partitioning and appropriate OS support [25]. Any worst-case processing estimates are likely to be overly conservative. We note that this problem relates to memory subsystem design for modern processors, and is not related to the actual mechanism employed to profile communication subsystems.
Reference: [10] <author> Robbert van Renesse, </author> <title> "Masking the overhead of protocol layering," </title> <booktitle> in Proc. of ACM SIGCOMM, </booktitle> <month> October </month> <year> 1996, </year> <pages> pp. 96-104. </pages>
Reference-contexts: A number of recent research efforts have developed efficient architectures and performance optimizations for the components constituting the communication subsystem, namely, the protocol stack and its interaction with the attached network interfaces <ref> [7, 8, 9, 10, 11, 12] </ref>. The primary thrust of these efforts has been to improve the average latency and throughput delivered by the communication subsystem to applications. <p> The primary thrust of these efforts has been to improve the average latency and throughput delivered by the communication subsystem to applications. The issues explored include optimizations to improve protocol processing latency <ref> [9, 10, 11] </ref>, techniques to minimize data copies [7, 12], and high-performance network interface design [8, 7]. However, while improving communication subsystem performance, these approaches are insufficient for the design and development of QoS-sensitive communication subsystems, as explained 2 later.
Reference: [11] <author> Trevor Blackwell, </author> <title> "Speeding up protocols for small messages," </title> <booktitle> in Proc. of ACM SIGCOMM, </booktitle> <month> October </month> <year> 1996, </year> <pages> pp. 85-95. </pages>
Reference-contexts: A number of recent research efforts have developed efficient architectures and performance optimizations for the components constituting the communication subsystem, namely, the protocol stack and its interaction with the attached network interfaces <ref> [7, 8, 9, 10, 11, 12] </ref>. The primary thrust of these efforts has been to improve the average latency and throughput delivered by the communication subsystem to applications. <p> The primary thrust of these efforts has been to improve the average latency and throughput delivered by the communication subsystem to applications. The issues explored include optimizations to improve protocol processing latency <ref> [9, 10, 11] </ref>, techniques to minimize data copies [7, 12], and high-performance network interface design [8, 7]. However, while improving communication subsystem performance, these approaches are insufficient for the design and development of QoS-sensitive communication subsystems, as explained 2 later. <p> Similarly, the importance of cache performance for small messages such as those found in typical signalling protocols is highlighted in <ref> [11] </ref>. This has significant implications for system parameterization since it highlights the difficulty in measuring various processing overheads accurately. Cache predictability may be improved via appropriate protocol implementation and compilation techniques [9], or via cache partitioning and appropriate OS support [25].
Reference: [12] <author> Jose Carlos Brustoloni and Peter Steenkiste, </author> <title> "Effects of buffering semantics on I/O performance," </title> <booktitle> in Proc. USENIX Symp. on Operating Systems Design and Implementation, </booktitle> <month> October </month> <year> 1996, </year> <pages> pp. 277-291. </pages>
Reference-contexts: A number of recent research efforts have developed efficient architectures and performance optimizations for the components constituting the communication subsystem, namely, the protocol stack and its interaction with the attached network interfaces <ref> [7, 8, 9, 10, 11, 12] </ref>. The primary thrust of these efforts has been to improve the average latency and throughput delivered by the communication subsystem to applications. <p> The primary thrust of these efforts has been to improve the average latency and throughput delivered by the communication subsystem to applications. The issues explored include optimizations to improve protocol processing latency [9, 10, 11], techniques to minimize data copies <ref> [7, 12] </ref>, and high-performance network interface design [8, 7]. However, while improving communication subsystem performance, these approaches are insufficient for the design and development of QoS-sensitive communication subsystems, as explained 2 later. <p> The negative impact of data-touching overheads such as checksumming has also been studied extensively, and a number of techniques devised to improve data-copying performance [21, 22]. Similarly, much attention has been focused recently on appropriate buffer management for data copy elimination <ref> [12, 23, 24] </ref>. In contrast, our goal is to explicitly account for any copying cost incurred during data movement to/from applications, and measure this cost via appropriate profiling.
Reference: [13] <author> Larry McVoy and C. Staelin, "lmbench: </author> <title> Portable tools for performance analysis," </title> <booktitle> in Proc. USENIX Winter Conference, </booktitle> <month> January </month> <year> 1996, </year> <pages> pp. 279-295. </pages>
Reference-contexts: More importantly, while performance-related considerations may influence the design substantially, exact knowledge or estimates of communication subsystem performance are typically not necessary when designing for some desired communication subsystem functionality. Communication subsystem performance is measured externally via tools such as lmbench <ref> [13] </ref> or via detailed and explicit profiling (assuming source code availability), but the performance thus measured is only utilized to compare competing implementations or design approaches. Accordingly, no provision is made for capturing and representing performance as a first-class abstraction within the communication subsystem. <p> The first such study focused on the reasons behind the failure of operating system performance improvements to track performance improvements in hardware technology [30]. More recently, efforts have focused on developing a suite of portable operating system benchmarks for cross-platform performance comparisons, such as lmbench <ref> [13] </ref>, as well as for detailed system analysis, such as hbench:OS [31]. In all these efforts, however, the performance profiling is geared towards performance comparisons and the impact of OS-hardware interactions on operating system primitives.
Reference: [14] <author> Author1, Author2, Author3, </author> <note> "Reference omitted to preserve anonymity," </note> <year> 1996. </year>
Reference-contexts: We subsequently argue that a detailed manual profiling of the communication subsystem is not only complex and time-consuming, it may also be insufficient due to its static nature. We then make the case for an automated approach to profiling and system parameterization. 2.1 QoS-Sensitive Communication Subsystem In earlier work <ref> [14] </ref>, we have proposed and evaluated a QoS-sensitive communication subsystem architecture realizing real-time channels [15, 16], a paradigm for guaranteed-QoS communication in packet-switched networks. <p> Details of the communication subsystem architecture and its components are beyond the scope of this paper and can be found in <ref> [14] </ref>. Of immediate relevance, however, is the admission control extensions adopted by the architecture to realize the real-time channel paradigm in practice [17, 18].
Reference: [15] <author> Domenico Ferrari and Dinesh C. Verma, </author> <title> "A scheme for real-time channel establishment in wide-area networks," </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> vol. 8, no. 3, </volume> <pages> pp. 368-379, </pages> <month> April </month> <year> 1990. </year> <month> 23 </month>
Reference-contexts: We then make the case for an automated approach to profiling and system parameterization. 2.1 QoS-Sensitive Communication Subsystem In earlier work [14], we have proposed and evaluated a QoS-sensitive communication subsystem architecture realizing real-time channels <ref> [15, 16] </ref>, a paradigm for guaranteed-QoS communication in packet-switched networks. The architecture delivers contracted QoS to individual connections, provides isolation between connections, and manages communication resources (e.g., CPU protocol processing, network transmission/reception, and memory buffers) in accordance with priorities derived from per-connection QoS specifications. <p> As mentioned, the service is geared towards the real-time channel <ref> [15, 16] </ref> model for guaranteed-QoS communication in packet-switched networks. This service has been realized on 133 MHz Pentium-based PCs running the OSF (now the Open Group) Mach MK microkernel operating system, using OSF's CORDS (Communication Objects for Real-time Dependable Systems) framework [40].
Reference: [16] <author> D. D. Kandlur, K. G. Shin, and D. Ferrari, </author> <title> "Real-time communication in multi-hop networks," </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> vol. 5, no. 10, </volume> <pages> pp. 1044-1056, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: We then make the case for an automated approach to profiling and system parameterization. 2.1 QoS-Sensitive Communication Subsystem In earlier work [14], we have proposed and evaluated a QoS-sensitive communication subsystem architecture realizing real-time channels <ref> [15, 16] </ref>, a paradigm for guaranteed-QoS communication in packet-switched networks. The architecture delivers contracted QoS to individual connections, provides isolation between connections, and manages communication resources (e.g., CPU protocol processing, network transmission/reception, and memory buffers) in accordance with priorities derived from per-connection QoS specifications. <p> As mentioned, the service is geared towards the real-time channel <ref> [15, 16] </ref> model for guaranteed-QoS communication in packet-switched networks. This service has been realized on 133 MHz Pentium-based PCs running the OSF (now the Open Group) Mach MK microkernel operating system, using OSF's CORDS (Communication Objects for Real-time Dependable Systems) framework [40].
Reference: [17] <author> Author1, Author2, Author3, </author> <note> "Reference omitted to preserve anonymity," </note> <year> 1996. </year>
Reference-contexts: Details of the communication subsystem architecture and its components are beyond the scope of this paper and can be found in [14]. Of immediate relevance, however, is the admission control extensions adopted by the architecture to realize the real-time channel paradigm in practice <ref> [17, 18] </ref>. For each connection requesting QoS guarantees, the admission control procedure estimates the connection's communication resource requirements by computing the request service and wait times for messages generated on the connection. <p> is utilized by the admission control procedure to reason about communication resource usage and availability. 4 2.2 Nature of System Parameters: Costs and Overheads To re-target a QoS-sensitive communication subsystem for a given platform, the system parameters (costs and overheads) must be determined accurately for effective and correct admission control <ref> [17] </ref>. Table 1 classifies various costs and overheads according to the associated communication resource, and highlights the key attributes that impact them. Note that one or more attributes may affect each cost component, i.e., no one-to-one correspondence is implied. <p> Moreover, the parameters defining the granularity at which communication resources are multiplexed are typically platform dependent. Examples include the maximum packet size (as derived from the MTU of the attached network and protocol stack headers) and the number of packets processed between successive preemption points <ref> [17] </ref>. These parameters cannot be fixed a priori and must be determined for the attached network and host platform, respectively. It is clear from the above observations that the values taken by each of the system parameters utilized in admission control is highly platform-specific and determined by myriad platform-dependent attributes. <p> The request wait time in turn is derived from the CPU wait time and the link wait time. The request processing and wait times are computed according to the extensions mentioned in Section 2.1 and described in <ref> [17] </ref>. The type of the channel at a host determines whether sender or receiver processing and wait times are computed. This computation is performed using the parameter variables that constitute the system parameter database.
Reference: [18] <author> Author1, </author> <note> "Reference omitted to preserve anonymity," </note> <year> 1997. </year>
Reference-contexts: Details of the communication subsystem architecture and its components are beyond the scope of this paper and can be found in [14]. Of immediate relevance, however, is the admission control extensions adopted by the architecture to realize the real-time channel paradigm in practice <ref> [17, 18] </ref>. For each connection requesting QoS guarantees, the admission control procedure estimates the connection's communication resource requirements by computing the request service and wait times for messages generated on the connection. <p> This is true for us since the unpredictability of the underlying operating system may introduce occasional preemption between the prologue and epilogue processing in each module. An example of such preemption is the execution of the device input thread during execution of a communication handler thread <ref> [18] </ref>. The collected samples 14 (a) Service architecture (b) CORDS-based protocol stack must be sorted to eliminate the "outliers" and consider only the remaining samples for computing parameter statistics. 5 Prototype Implementation and Evaluation In this section we describe our prototype implementation and evaluation of a self-parameterizing guaranteed-QoS communication service. <p> As mentioned in Section 2.1, the run-time resource management in the service architecture is based in large part on the architecture proposed in [3], with enhancements to accommodate the specific requirements of the implementation environment. Details on the internals of the service components and their interaction are provided in <ref> [18] </ref>. The CORDS-based protocol stack for our service is shown in Figure 5 (b). The protocols comprising this stack include RTC API ANCHOR, RTCOP, RTROUTER, IP, ETH, and ETHDRV; communication resources are managed by CLIPS.
Reference: [19] <author> Erich Nahum, David Yates, Jim Kurose, and Don Towsley, </author> <title> "Cache behavior of network protocols," </title> <booktitle> in Proc. of ACM SIGMETRICS, </booktitle> <month> June </month> <year> 1997, </year> <pages> pp. 169-180. </pages>
Reference-contexts: A faster CPU or higher memory copy bandwidth would reduce protocol processing latency. Similarly, different processor and cache architectures would generate different context switching overheads and cache miss penalties, respectively. The cache performance of communication subsystems is also determined in large part by the composition of the protocol stack <ref> [19] </ref>. The I/O bus bandwidth in part determines the available DMA bandwidth to/from system memory. Since this affects the time spent moving data between memory buffers and the network interface, it also determines, along with link bandwidth, the transmission time for outgoing packets. <p> A study of the cache behavior of network protocols such as TCP and UDP reports widely variable effects on processing latency, depending on whether the cache is cold or hot <ref> [19] </ref>. Similarly, the importance of cache performance for small messages such as those found in typical signalling protocols is highlighted in [11]. This has significant implications for system parameterization since it highlights the difficulty in measuring various processing overheads accurately. <p> Similar considerations apply for the parameter PROT SEND PROC, since the first packet of a message typically has a higher processing cost (e.g., due to cold cache misses <ref> [19] </ref>) compared to the subsequent packets. In the second step, once the sample vector has been reduced to two sample estimates, these estimates are then averaged across the collected samples and the average values thus computed associated with the corresponding parameter.
Reference: [20] <author> Jonathan Kay and Joseph Pasquale, </author> <title> "The importance of non-data touching processing overheads in TCP/IP," </title> <booktitle> in Proc. of ACM SIGCOMM, </booktitle> <month> September </month> <year> 1993, </year> <pages> pp. 259-268. </pages>
Reference-contexts: More germane to our focus, several recent efforts have also focused on experimentally quantifying, and identifying factors that influence, protocol stack performance. A detailed study of the non-data touching processing overheads in TCP/IP protocol stacks is presented in <ref> [20] </ref>. The factors contributing to these overheads include network buffer management, protocol-specific processing, operating system functions, data structure manipulations, and error checking. This study reports an extensive breakdown of the overheads incurred at each layer of the protocol stack for a DECstation 5000/200 running the Ultrix 4.2a operating system.
Reference: [21] <author> Jonathan Kay and Joseph Pasquale, </author> <title> "Measurement, analysis, and improvement of UDP/IP throughput for the DECstation 5000," </title> <booktitle> in Proc. USENIX Winter Conference, </booktitle> <month> January </month> <year> 1993, </year> <pages> pp. 249-258. </pages>
Reference-contexts: The negative impact of data-touching overheads such as checksumming has also been studied extensively, and a number of techniques devised to improve data-copying performance <ref> [21, 22] </ref>. Similarly, much attention has been focused recently on appropriate buffer management for data copy elimination [12, 23, 24]. In contrast, our goal is to explicitly account for any copying cost incurred during data movement to/from applications, and measure this cost via appropriate profiling.
Reference: [22] <author> Mark B. Abbott and Larry L. Peterson, </author> <title> "Increasing network throughput by integrating protocol layers," </title> <journal> IEEE/ACM Trans. Networking, </journal> <volume> vol. 5, no. 1, </volume> <pages> pp. 600-610, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: The negative impact of data-touching overheads such as checksumming has also been studied extensively, and a number of techniques devised to improve data-copying performance <ref> [21, 22] </ref>. Similarly, much attention has been focused recently on appropriate buffer management for data copy elimination [12, 23, 24]. In contrast, our goal is to explicitly account for any copying cost incurred during data movement to/from applications, and measure this cost via appropriate profiling.
Reference: [23] <author> Jose Carlos Brustoloni and Peter Steenkiste, </author> <title> "Evaluation of data passing and scheduling avoidance," </title> <booktitle> in Proc. Intl. Workshop on Network and Operating System Support for Digital Audio and Video, </booktitle> <month> May </month> <year> 1997. </year>
Reference-contexts: The negative impact of data-touching overheads such as checksumming has also been studied extensively, and a number of techniques devised to improve data-copying performance [21, 22]. Similarly, much attention has been focused recently on appropriate buffer management for data copy elimination <ref> [12, 23, 24] </ref>. In contrast, our goal is to explicitly account for any copying cost incurred during data movement to/from applications, and measure this cost via appropriate profiling.
Reference: [24] <author> B. Murphy, S. Zeadally, and C. J. Adams, </author> <title> "An analysis of process and memory models to support high-speed networking in a UNIX environment," </title> <booktitle> in Proc. USENIX Winter Conference, </booktitle> <month> January </month> <year> 1996. </year>
Reference-contexts: The negative impact of data-touching overheads such as checksumming has also been studied extensively, and a number of techniques devised to improve data-copying performance [21, 22]. Similarly, much attention has been focused recently on appropriate buffer management for data copy elimination <ref> [12, 23, 24] </ref>. In contrast, our goal is to explicitly account for any copying cost incurred during data movement to/from applications, and measure this cost via appropriate profiling.
Reference: [25] <author> Jochen Liedtke, Hermann Hartig, and Michael Hohmuth, </author> <title> "OS-controlled cache predictability for real-time systems," </title> <booktitle> in Proc. Real-Time Technology and Applications Symposium, </booktitle> <month> June </month> <year> 1997, </year> <pages> pp. 213-223. </pages>
Reference-contexts: This has significant implications for system parameterization since it highlights the difficulty in measuring various processing overheads accurately. Cache predictability may be improved via appropriate protocol implementation and compilation techniques [9], or via cache partitioning and appropriate OS support <ref> [25] </ref>. Any worst-case processing estimates are likely to be overly conservative. We note that this problem relates to memory subsystem design for modern processors, and is not related to the actual mechanism employed to profile communication subsystems.
Reference: [26] <author> Peter Sjodin, Per Gunningberg, Erik Nordmark, and Stephen Pink, </author> <title> "Towards protocol benchmarks," in Protocols for High-Speed Networks, </title> <editor> H. Rudin and R. Williamson, </editor> <booktitle> Eds., </booktitle> <pages> pp. 57-67. </pages> <publisher> North-Holland, </publisher> <year> 1989. </year>
Reference-contexts: In contrast, we propose that traditional protocol stacks be extended to dynamically determine the performance of various components and maintain this information in terms of well-defined system parameters. 3.2 Protocol Benchmarking The notion of protocol benchmarks was proposed in <ref> [26] </ref> for a comparative evaluation of different implementations of communication protocols and protocol stacks. To capture the communication behavior 7 of real applications, a three-level model is proposed that comprises basic operations, basic applications, and compound applications. <p> This model has been used in a tool to compare different protocols and protocol implementations for varying levels of background load. In particular, it has been used to evaluate the performance of VMTP [27, 28], FTP and SunRPC [29], and to compare the performance of OSI TP4 and TCP <ref> [26] </ref>. This tool also allows specification of variable message sizes and the delay between successive messages sent on a given connection. The primary focus of such protocol benchmarks and the above-mentioned tool is to compare the end-to-end performance of different protocols.
Reference: [27] <author> David R. Cheriton and Carey L. Williamson, </author> <title> "VMTP as the transport layer for high-performance distributed systems," </title> <journal> IEEE Communication Magazine, </journal> <pages> pp. 37-44, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: This model has been used in a tool to compare different protocols and protocol implementations for varying levels of background load. In particular, it has been used to evaluate the performance of VMTP <ref> [27, 28] </ref>, FTP and SunRPC [29], and to compare the performance of OSI TP4 and TCP [26]. This tool also allows specification of variable message sizes and the delay between successive messages sent on a given connection.
Reference: [28] <author> Erik Nordmark and David R. Cheriton, </author> <title> "Experiences from VMTP: How to achieve low response time," in Protocols for High-Speed Networks, </title> <editor> H. Rudin and R. Williamson, </editor> <booktitle> Eds., </booktitle> <pages> pp. 43-54. </pages> <publisher> North-Holland, </publisher> <year> 1989. </year>
Reference-contexts: This model has been used in a tool to compare different protocols and protocol implementations for varying levels of background load. In particular, it has been used to evaluate the performance of VMTP <ref> [27, 28] </ref>, FTP and SunRPC [29], and to compare the performance of OSI TP4 and TCP [26]. This tool also allows specification of variable message sizes and the delay between successive messages sent on a given connection.
Reference: [29] <author> P. Gunningberg, M. Bjorkman, E. Nordmark, S. Pink, P. Sjodin, and J.-E. Stromquist, </author> <title> "Application protocols and performance benchmarks," </title> <journal> IEEE Communication Magazine, </journal> <pages> pp. 30-36, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: This model has been used in a tool to compare different protocols and protocol implementations for varying levels of background load. In particular, it has been used to evaluate the performance of VMTP [27, 28], FTP and SunRPC <ref> [29] </ref>, and to compare the performance of OSI TP4 and TCP [26]. This tool also allows specification of variable message sizes and the delay between successive messages sent on a given connection.
Reference: [30] <author> John K. Ousterhout, </author> <title> "Why aren't operating systems getting faster as fast as hardware?," </title> <booktitle> in Summer USENIX Conference, </booktitle> <month> June </month> <year> 1990, </year> <pages> pp. 1-10. </pages>
Reference-contexts: The first such study focused on the reasons behind the failure of operating system performance improvements to track performance improvements in hardware technology <ref> [30] </ref>. More recently, efforts have focused on developing a suite of portable operating system benchmarks for cross-platform performance comparisons, such as lmbench [13], as well as for detailed system analysis, such as hbench:OS [31].
Reference: [31] <author> Aaron Brown and Margo Seltzer, </author> <title> "Operating system benchmarking in the wake of lmbench: A case study of the performance of NetBSD on the Intel x86 architecture," </title> <booktitle> in Proc. of ACM SIGMETRICS, </booktitle> <month> June </month> <year> 1997, </year> <pages> pp. 214-224. </pages>
Reference-contexts: More recently, efforts have focused on developing a suite of portable operating system benchmarks for cross-platform performance comparisons, such as lmbench [13], as well as for detailed system analysis, such as hbench:OS <ref> [31] </ref>. In all these efforts, however, the performance profiling is geared towards performance comparisons and the impact of OS-hardware interactions on operating system primitives. <p> For example, for our platform timestamps using the real-time clock cost 15 s. While this overhead is relatively high, it can be reduced drastically by using hardware performance counters provided in several modern processors <ref> [31] </ref>. Hardware cycle counters provide timestamps with resolutions of the order of just a few nanoseconds. The kernel-level profiling described in [39] is performed using such hardware counters. For platforms with significant timestamp cost, the generated profile sample must be adjusted appropriately. <p> The processing performed on the collected samples may include calculation of simple averages and additional statistics such as the standard deviation. Often it is more accurate to discard some of the highest and the lowest values before computing parameter statistics <ref> [31] </ref>. This is true for us since the unpredictability of the underlying operating system may introduce occasional preemption between the prologue and epilogue processing in each module. An example of such preemption is the execution of the device input thread during execution of a communication handler thread [18].
Reference: [32] <author> Margo Seltzer and Christopher Small, </author> <title> "Self-monitoring and self-adapting operating systems," </title> <booktitle> in Proc. Workshop on HoTOS, </booktitle> <month> May </month> <year> 1997. </year>
Reference-contexts: Moreover, the performance profiling is not geared towards on-line system parameterization, since the grain at which profiling is done may be incompatible with that required by the admission control procedure. Self-parameterizing protocol stacks come closest in flavor to the idea of self-monitoring and self-adapting operating systems <ref> [32] </ref>, although the two differ greatly in goals, scope and the approach adopted. It is proposed in [32] to perform continuous monitoring of operating system activity to construct a database of performance statistics, classify this data appropriately, and perform off-line analysis to construct a characterization of the system under normal behavior <p> Self-parameterizing protocol stacks come closest in flavor to the idea of self-monitoring and self-adapting operating systems <ref> [32] </ref>, although the two differ greatly in goals, scope and the approach adopted. It is proposed in [32] to perform continuous monitoring of operating system activity to construct a database of performance statistics, classify this data appropriately, and perform off-line analysis to construct a characterization of the system under normal behavior and detect anomalous behavior.
Reference: [33] <author> Jennifer Anderson, Lance M. Berc, et al., </author> <title> "Continuous profiling: Where have all the cycles gone?," </title> <booktitle> in Proc. ACM Symp. on Operating Systems Principles, </booktitle> <month> October </month> <year> 1997. </year>
Reference-contexts: Continuous on-line profiling of operating systems with fine-grain performance-based feedback is described in <ref> [33] </ref>, which also reports impressive performance improvements due to the continuous 8 profiling methodology adopted. In contrast, we focus exclusively on the communication subsystem, with the primary goal of parameterizing it via on-line profiling and making this information available to the admission control module.
Reference: [34] <author> Clifford W. Mercer and Ragunathan Rajkumar, </author> <title> "An interactive interface and RT-Mach support for monitoring and controlling resource management," </title> <booktitle> in Proc. Real-Time Technology and Applications Symposium, </booktitle> <month> May </month> <year> 1995. </year> <month> 24 </month>
Reference-contexts: Moreover, due to the requirement of capturing constituent system overheads accurately, our on-line profiling operates at a finer time scale and may be disabled once appropriate measurements have been completed. Several other efforts have also focused on operating system support for resource monitoring and application adaptation <ref> [34, 35, 36] </ref>. Such support becomes necessary in order to accommodate inaccurate or changing estimates of application resource requirements, and is geared primary towards adaptive multimedia applications.
Reference: [35] <author> Tatsuo Nakajima, </author> <title> "A dynamic QOS control based on optimistic processor reservation," </title> <booktitle> in Proc. Intl. Conf. on Multimedia Computing and Systems, </booktitle> <year> 1996. </year>
Reference-contexts: Moreover, due to the requirement of capturing constituent system overheads accurately, our on-line profiling operates at a finer time scale and may be disabled once appropriate measurements have been completed. Several other efforts have also focused on operating system support for resource monitoring and application adaptation <ref> [34, 35, 36] </ref>. Such support becomes necessary in order to accommodate inaccurate or changing estimates of application resource requirements, and is geared primary towards adaptive multimedia applications.
Reference: [36] <author> Lakshman Krishnamurthy, </author> <title> AQUA: An Adaptive Quality of Service Architecture for Distributed Multimedia Applications, </title> <type> Ph.D. thesis, </type> <institution> University of Kentucky, </institution> <year> 1997. </year>
Reference-contexts: Moreover, due to the requirement of capturing constituent system overheads accurately, our on-line profiling operates at a finer time scale and may be disabled once appropriate measurements have been completed. Several other efforts have also focused on operating system support for resource monitoring and application adaptation <ref> [34, 35, 36] </ref>. Such support becomes necessary in order to accommodate inaccurate or changing estimates of application resource requirements, and is geared primary towards adaptive multimedia applications.
Reference: [37] <author> Xiaolan Zhang, Zheng Wang, Nicholas Gloy, J. Bradley Chen, and Michael D. Smith, </author> <title> "Operating system support for automatic profiling and optimization," </title> <booktitle> in Proc. ACM Symp. on Operating Systems Principles, </booktitle> <month> October </month> <year> 1997. </year>
Reference-contexts: We note that while our primary focus is on provision of deterministic guarantees, self-parameterizing protocol stacks are desirable for provision of looser forms of QoS guarantees as well. System support for automatic profiling and optimization of applications is described in <ref> [37] </ref>, where the focus is primarily on improving application execution performance via statistical profiling and profile-based optimizations.
Reference: [38] <author> Samuel J. Le*er, Marshall Kirk McKusick, Michael J. Karels, and John S. Quarterman, </author> <title> The Design and Implementation of the 4.3BSD Unix Operating System, </title> <publisher> Addison Wesley, </publisher> <month> May </month> <year> 1989. </year>
Reference-contexts: The API data buffering module refers to the API layer of the communication subsystem, such as the sockets layer in BSD Unix <ref> [38] </ref> and the RTC API ANCHOR layer described in Section 5. The message classification and queueing module sits just below the API data buffering layer and corresponds to the per-channel message queues processed by the communication handlers.
Reference: [39] <author> Tsipora Barzilai, Dilip Kandlur, Ashish Mehra, Debanjan Saha, and Steve Wise, </author> <title> "Design and implementation of an RSVP-based quality of service architecture for integrated services Internet," </title> <booktitle> in Proc. Int'l Conf. on Distributed Computing Systems, </booktitle> <month> May </month> <year> 1997. </year>
Reference-contexts: While this overhead is relatively high, it can be reduced drastically by using hardware performance counters provided in several modern processors [31]. Hardware cycle counters provide timestamps with resolutions of the order of just a few nanoseconds. The kernel-level profiling described in <ref> [39] </ref> is performed using such hardware counters. For platforms with significant timestamp cost, the generated profile sample must be adjusted appropriately.
Reference: [40] <author> F. Travostino, E. Menze, and F. Reynolds, </author> <title> "Paths: Programming with system resources in support of real-time distributed applications," </title> <booktitle> in Proc. IEEE Workshop on Object-Oriented Real-Time Dependable Systems, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: This service has been realized on 133 MHz Pentium-based PCs running the OSF (now the Open Group) Mach MK microkernel operating system, using OSF's CORDS (Communication Objects for Real-time Dependable Systems) framework <ref> [40] </ref>. The CORDS framework is based on the x-kernel [41], and provides support for allocating communication resources (such as memory and CPU threads) to individual communication protocols on nodes along a connection path from source to destination.
Reference: [41] <author> Norman C. Hutchinson and Larry L. Peterson, </author> <title> "The x-Kernel: An architecture for implementing network protocols," </title> <journal> IEEE Trans. Software Engineering, </journal> <volume> vol. 17, no. 1, </volume> <pages> pp. 1-13, </pages> <month> January </month> <year> 1991. </year> <month> 25 </month>
Reference-contexts: This service has been realized on 133 MHz Pentium-based PCs running the OSF (now the Open Group) Mach MK microkernel operating system, using OSF's CORDS (Communication Objects for Real-time Dependable Systems) framework [40]. The CORDS framework is based on the x-kernel <ref> [41] </ref>, and provides support for allocating communication resources (such as memory and CPU threads) to individual communication protocols on nodes along a connection path from source to destination.
References-found: 41

