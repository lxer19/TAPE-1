URL: http://www.cs.rpi.edu/~zaki/PS/THESIS.ps.gz
Refering-URL: http://www.cs.rpi.edu/~zaki/papers.html
Root-URL: http://www.cs.rpi.edu
Title: Scalable Data Mining for Rules  
Author: by Mohammed Javeed Zaki Dr. Wei Li 
Degree: Submitted in Partial Fulfillment of the Requirements for the Degree Doctor of Philosophy Supervised by  
Date: 1998  
Address: Rochester, New York  
Affiliation: Department of Computer Science The College Arts and Sciences University of Rochester  
Abstract-found: 0
Intro-found: 1
Reference: [Aggarwal and Yu, 1998] <author> C. Aggarwal and P. Yu, </author> <title> "Online Generation of Association Rules," </title> <booktitle> In 14th Intl. Conf. on Data Engineering, </booktitle> <month> February </month> <year> 1998. </year>
Reference: [Agrawal et al., 1992] <author> R. Agrawal, S. Ghosh, T. Imielinski, B. Iyer, and A. Swami, </author> <title> "An Interval Classifier for Database Mining Applications," </title> <booktitle> In 18th VLDB Conference, </booktitle> <month> August </month> <year> 1992. </year>
Reference-contexts: Decision 122 trees can be constructed relatively fast compared to other methods. Another advantage is that decision tree models are simple and easy to understand [Quinlan, 1993]. Moreover, trees can be easily converted into SQL statements that can be used to access databases efficiently <ref> [Agrawal et al., 1992] </ref>. Finally, decision tree classifiers obtain similar, and sometimes better, accuracy when compared with other classification methods [Michie et al., 1994]. We have therefore focused on building scalable and parallel decision-tree classifiers. <p> Incremental learning methods, where the data is classified in batches, have also been studied [Quinlan, 1979; Wirth and Catlett, 1988]. However, the cumulative cost of classifying data incrementally can sometimes exceed the cost of classifying the entire training set once. In <ref> [Agrawal et al., 1992] </ref>, a classifier built with database considerations, the size of the training set was overlooked. Instead, the focus was on building a classifier that could use database indices to improve the retrieval efficiency while classifying test data. <p> Its largest dataset contains about 57,000 records. In our performance study we are interested in evaluating the SMP algorithms on large out-of-core data. We therefore use the synthetic benchmark proposed in <ref> [Agrawal et al., 1992] </ref> and used in several past studies. Example tuples in this benchmark have both continuous and categorical attributes. The benchmark gives several classification functions of varying complexity to generate synthetic databases.
Reference: [Agrawal et al., 1993a] <author> R. Agrawal, T. Imielinski, and A. Swami, </author> <title> "Database Mining: A Performance Perspective," </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 5(6) </volume> <pages> 914-925, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: The induced model consists of patterns that are useful in class discrimination. Once induced, the model can help in the automatic prediction of new unclassified data. Classification has been identified as an important problem in the emerging field of data mining <ref> [Agrawal et al., 1993a] </ref>. It has important applications in diverse domains such as retail target marketing, customer retention, fraud detection and medical diagnosis [Michie et al., 1994]. <p> Among these models, decision trees are particularly suited for data mining <ref> [Agrawal et al., 1993a; Mehta et al., 1996] </ref>. Decision 122 trees can be constructed relatively fast compared to other methods. Another advantage is that decision tree models are simple and easy to understand [Quinlan, 1993].
Reference: [Agrawal et al., 1993b] <author> R. Agrawal, T. Imielinski, and A. Swami, </author> <title> "Mining association rules between sets of items in large databases," </title> <booktitle> In ACM SIGMOD Conf. Management of Data, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: The typical discovery-driven tasks include: * Association Rules: Given a database of transactions, where each transaction consists of a set of items, association discovery finds all the item sets that frequently occur together, and also the rules among them <ref> [Agrawal et al., 1993b; Agrawal et al., 1996] </ref>. <p> Section 2.5 describes our new algorithms. Some previous methods, used for experimental comparison, are described in more detail in Section 2.6. An experimental study is presented in Section 2.7, and we conclude in Section 2.8. 2.2 Problem Statement The association mining task, first introduced in <ref> [Agrawal et al., 1993b] </ref>, can be stated as follows: Let I be a set of items, and D a database of transactions, where each transaction has a unique identifier (tid) and contains a set of items. A set of items is also called an itemset. <p> In our example database, there are only 7 closed frequent itemsets versus 19 frequent itemsets. 4.5 Rule Generation Association rules were originally proposed in <ref> [Agrawal et al., 1993b] </ref>. However, we will show below that association rules are exactly the partial implications, satisfying support and confidence constraints, proposed in an earlier paper [Luxenburger, 1991]. Let (G; M; I) be a context.
Reference: [Agrawal et al., 1996] <author> R. Agrawal, H. Mannila, R. Srikant, H. Toivonen, and A. Inkeri Verkamo, </author> <title> "Fast Discovery of Association Rules," </title> <editor> In U. Fayyad and et al, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 307-328. </pages> <publisher> AAAI Press, </publisher> <address> Menlo Park, CA, </address> <year> 1996. </year>
Reference-contexts: The typical discovery-driven tasks include: * Association Rules: Given a database of transactions, where each transaction consists of a set of items, association discovery finds all the item sets that frequently occur together, and also the rules among them <ref> [Agrawal et al., 1993b; Agrawal et al., 1996] </ref>. <p> The data mining task is to generate all association rules in the database, which have a support greater than min sup, i.e., the rules are frequent. The rules must also have confidence greater than min conf, i.e., the rules are strong. This task can be broken into two steps <ref> [Agrawal et al., 1996] </ref>: 1. Find all frequent itemsets. This step is computationally and I/O intensive. Given m items, there can be potentially 2 m frequent itemsets. Efficient methods are needed to traverse this exponential itemset search space to enumerate all the frequent itemsets. <p> The Apriori algorithm <ref> [Agrawal et al., 1996] </ref> is the best known previous algorithm, and it uses an efficient candidate generation procedure, such that only the frequent itemsets at a level are used to construct candidates at the next level. However, it requires multiple database scans. <p> AprClique: It uses maximal-clique-based pseudo-equivalence relation OE 1 . However, unlike the algorithms described above, it uses horizontal data layout. It has two main steps: i) All possible subsets of the maximum element in each sub-lattice are generated and inserted in hash trees <ref> [Agrawal et al., 1996] </ref>, avoiding duplicates. There is one hash tree for each length, i.e., a k-subset is inserted in the tree C k . <p> The pseudo-code is shown in Figure 2.18. 2.6 The Apriori and Partition Algorithms We now discuss Apriori and Partition in some more detail, since we will experimen tally compare our new algorithms against them. Apriori Algorithm Apriori <ref> [Agrawal et al., 1996] </ref> is an iterative algorithm that counts itemsets of a specific length in a given database pass. The process starts by 35 scanning all transactions in the database and computing the frequent items. Next, a set of potentially frequent candidate 2-itemsets is formed from the frequent items. <p> Another database scan is made to obtain their supports. The frequent 2-itemsets are retained for the next pass, and the process is repeated until all frequent itemsets have been enumerated. The complete algorithm is shown in figure 2.19. We refer the reader to <ref> [Agrawal et al., 1996] </ref> for additional details. There are three main steps in the algorithm: 1. Generate candidates of length k from the frequent (k 1) length itemsets, by a self join on F k1 . For example, if F 2 = fAB; AC; AD; AE; BC; BD; BEg. <p> The dataset generation procedure is described in <ref> [Agrawal et al., 1996] </ref>, and the code is publicly available from IBM [IBMa]. These datasets mimic the transactions in a retailing environment, where people tend to buy sets of items together, the so called potential maximal frequent set. <p> The MEDD and MSDD algorithms [Oates et al., 1997] discover patterns in multiple event sequences. However, they only find sequences of length two with a given window size and a time-gap. Sequence discovery can essentially be thought of as association discovery <ref> [Agrawal et al., 1996] </ref> over a temporal database. While association rules discover only intra-transaction patterns (itemsets), we now also have to discover inter-transaction patterns (sequences). The set of all frequent sequences is a superset of the set of frequent itemsets. <p> The set of all frequent sequences is a superset of the set of frequent itemsets. Due to this similarity sequence mining algorithms like AprioriAll, GSP, etc., utilize some of the ideas initially proposed for the discovery of association rules <ref> [Agrawal et al., 1996; Savasere et al., 1995] </ref>. Our new algorithm is based on the fast association mining techniques presented by us in [Zaki et al., 1997c]. Nevertheless, the sequence search space is much more complex and challenging than the itemset space, and thus warrants specific algorithms.
Reference: [Agrawal and Shafer, 1996] <author> R. Agrawal and J. Shafer, </author> <title> "Parallel Mining of Association Rules," </title> <journal> IEEE Trans. on Knowledge and Data Engg., </journal> <volume> 8(6) </volume> <pages> 962-969, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: Section 3.4 describes the design and implementation of the new parallel algorithms. Our experimental study is presented in Section 3.5, and our conclusions in Section 3.6. 3.2 Related Work Distributed-Memory Machines Three different parallelizations of Apriori on IBM-SP2, a distributed-memory machine, were presented in <ref> [Agrawal and Shafer, 1996] </ref>. The Count Distribution algorithm is a straight-forward parallelization of Apriori. Each processor generates the partial support of all candidate itemsets from its local database partition. At the end of each iteration the global supports are generated by exchanging the partial supports among all the processors. <p> It thus suffers from huge communication overhead. The Candidate Distribution algorithm also partitions the candidates, but it selectively replicates the database, so that each processor proceeds independently. The local database portion is still scanned in every iteration. Count Distribution was shown to have superior performance among these three algorithms <ref> [Agrawal and Shafer, 1996] </ref>. Other parallel algorithms im 48 proving upon these ideas in terms of communication efficiency, or aggregate memory utilization have also been proposed [Cheung et al., 1996b; Cheung et al., 1996a; Han et al., 1997]. <p> The PDM algorithm [Park et al., 1995b] presents a parallelization of the DHP algorithm [Park et al., 1995a]. The hash based parallel algorithms NPA, SPA, HPA, and HPA-ELD, proposed in [Shintani and Kitsuregawa, 1996] are similar to those in <ref> [Agrawal and Shafer, 1996] </ref>. Essentially NPA corresponds to Count Distribution, SPA to Data Distribution, and HPA to Candidate Distribution. The HPA-ELD algorithm is the best among NPA, SPA, and HPA, since it eliminates the effect of data skew, and reduces communication by replicating candidates with high support on all processors. <p> These algorithms assume that the database is partitioned among all the processors in equal-sized blocks, which reside on the local disk of each processor. The Count Distribution algorithm <ref> [Agrawal and Shafer, 1996] </ref> is a simple paralleliza 49 tion of Apriori. All processors generate the entire candidate hash tree from F k1 . Each processor can thus independently get partial supports of the candidates from its local database partition. <p> The second step performs the reduction among the hosts. We also utilize some optimization techniques such as hash-tree balancing and short-circuited subset counting [Zaki et al., 1996] to further improve the performance of Count Distribution. The Data Distribution algorithm <ref> [Agrawal and Shafer, 1996] </ref> was designed to utilize the total system memory by generating disjoint candidate sets on each processor. However to generate the global support each processor must scan the entire database (its local partition, and all the remote partitions) in all iterations. <p> However to generate the global support each processor must scan the entire database (its local partition, and all the remote partitions) in all iterations. It thus suffers from high communication overhead, and performs poorly when compared to Count Distribution. The Candidate Distribution algorithm <ref> [Agrawal and Shafer, 1996] </ref> uses a property of frequent itemsets [Agrawal and Shafer, 1996; Zaki et al., 1996] to partition the candidates during iteration l, so that each processor can generate disjoint candidates independent of other processors. <p> It thus suffers from high communication overhead, and performs poorly when compared to Count Distribution. The Candidate Distribution algorithm [Agrawal and Shafer, 1996] uses a property of frequent itemsets <ref> [Agrawal and Shafer, 1996; Zaki et al., 1996] </ref> to partition the candidates during iteration l, so that each processor can generate disjoint candidates independent of other processors. At the same time the database is selectively replicated so that a processor can generate global counts independently. <p> This pruning information is used if it arrives in time, otherwise it is used in the next iteration. Note that each processor must still scan its local data once per iteration. Even though it uses problem-specific information, it performs worse than Count Distribution <ref> [Agrawal and Shafer, 1996] </ref>. <p> We further assume that the database of tid-lists is initially partitioned among all the hosts. This partitioning is done off-line, similar to the assumption made in Count Distribution <ref> [Agrawal and Shafer, 1996] </ref>. The tid-lists are partitioned so that the total length of all tid-lists in the local portions on each host are roughly equal. This is achieved using a greedy algorithm. <p> Frequent Itemsets Frequent Itemset Size (k) Frequent Itemsets at Support = 0.25% T20.I6.D1137K T10.I4.D2084K 62 3.5.1 Performance Comparison In this section we will compare the performance of our new algorithms with Count Distribution (henceforth referred to as CD), which was shown to be superior to both Data and Candidate Distribution <ref> [Agrawal and Shafer, 1996] </ref>. <p> Once in the set-up phase, and once when processing all the itemset classes. We implemented the algorithms on a 32 processor Digital cluster interconnected with the Memory Channel network, and compared them against a well known parallel algorithm Count Distribution <ref> [Agrawal and Shafer, 1996] </ref>. Experimental results indicate that our best parallel algorithm Par-MaxClique outperformed Count Distribution by upto an order of magnitude. 73 4 Theoretical Foundations of Association Rules 4.1 Introduction In the previous chapters we developed efficient algorithms for mining association rules.
Reference: [Agrawal and Srikant, 1994] <author> R. Agrawal and R. Srikant, </author> <title> "Fast Algorithms for Mining Association Rules," </title> <booktitle> In 20th VLDB Conference, </booktitle> <month> September </month> <year> 1994. </year> <month> 167 </month>
Reference-contexts: The data is generated using the following procedure. We first generate L maximal itemsets of average size I, by choosing from the N items. We next generate D transactions of average size T by choosing from 37 the L maximal itemsets. We refer the reader to <ref> [Agrawal and Srikant, 1994] </ref> for more detail on the database generation. In our experiments we set N = 1000 and L = 2000. Experiments are conducted on databases with different values of D, T , and I. The database parameters are shown in Table 2.1. <p> The average transaction size is denoted as T , and the average maximal potentially frequent itemset size as I. The number of maximal potentially frequent itemsets was L = 2000, and the number of items was N = 1000. We refer the reader to <ref> [Agrawal and Srikant, 1994] </ref> for more detail on the database generation. All the experiments were performed with a minimum support value of 0.25%.
Reference: [Agrawal and Srikant, 1995] <author> R. Agrawal and R. Srikant, </author> <title> "Mining Sequential Patterns," </title> <booktitle> In 11th Intl. Conf. on Data Engg., </booktitle> <year> 1995. </year>
Reference-contexts: that, "40% of people who buy Jane Austen's Pride and Prejudice also buy Sense and Sensibility." Potential application areas include catalog design, store layout, customer segmentation, telecommunication alarm diagnosis, etc. 3 * Sequential Patterns: Sequence discovery aims at extracting sets of events that commonly occur over a period of time <ref> [Agrawal and Srikant, 1995] </ref>. An example of a sequential pattern could be that "70% of the people who buy Jane Austen's Pride and Prejudice also buy Emma within a month". <p> 1997a; Zaki et al., 1997b; Zaki et al., 1997e; Zaki and Ogihara, 1998; Parthasarathy et al., 1998]. 1.1.2 Mining Sequence Rules Existing solutions to this problem share most of the features and limitations of association mining, namely that they tend to make repeated database scans, and use complex hash structures <ref> [Agrawal and Srikant, 1995; Srikant and Agrawal, 1996b] </ref>. In this thesis we develop a fast new algorithm for sequence discovery. As in the association case, the new algorithm partitions the search space into small independent pieces, which are solved in main memory. <p> Since the rule generation step is quite straightforward, in the rest of the chapter we will only concentrate on the frequent sequence discovery phase. 5.3 Related Work The problem of mining sequential patterns was introduced in <ref> [Agrawal and Srikant, 1995] </ref>. They also presented three algorithms for solving this problem. The AprioriAll algorithm was shown to perform equal to or better than the other two approaches. <p> SPADE (min sup; D): F 1 = f frequent items or 1-sequences g; F 2 = f frequent 2-sequences g; E = f equivalence classes [X] 1 g; for all [X] 2 E do Enumerate-Frequent-Seq ([X]); 106 5.5.1 Computing Frequent 1-Sequences and 2-Sequences Most of the current sequence mining algorithms <ref> [Agrawal and Srikant, 1995; Srikant and Agrawal, 1996b] </ref> assume a horizontal database layout such as the one shown in customer has a set of transactions, along with the items contained in the transaction. <p> Like [Srikant and Agrawal, 1996b] we set N S = 5000, N I = 25000 and N = 10000. The number of data-sequences was set to D = 200; 000. Table 5.2 shows the datasets with their parameter settings. We refer the reader to <ref> [Agrawal and Srikant, 1995] </ref> for additional details on the dataset generation. Plan Dataset The real-life dataset was obtained from a Natural Language Planning domain. The planner generates plans for routing commodities from one city to another.
Reference: [Agrawal et al., 1993c] <author> Rakesh Agrawal, Christos Faloutsos, and Arun Swami, </author> <title> "Efficient Similarity Search in Sequence Databases," </title> <booktitle> In Proc. of the Fourth Int'l Conference on Foundations of Data Organization and Algorithms, </booktitle> <month> October </month> <year> 1993. </year>
Reference-contexts: new types of stars in datasets of stellar objects, and so on. * Similarity Search: Similarity search is performed on a database of objects to find the object (s) that are within a user-defined distance from the queried object, or to find all pairs within some distance of each other <ref> [Agrawal et al., 1993c; 4 Faloutsos et al., 1994] </ref>. This kind of search is especially applicable to temporal and spatial databases.
Reference: [Bayardo, 1998] <author> R. J. Bayardo, </author> <title> "Efficiently mining long patterns from Databases," </title> <booktitle> In ACM SIGMOD Conf. Management of Data, </booktitle> <month> June </month> <year> 1998. </year>
Reference-contexts: The Pincer-Search algorithm [Lin and Kedem, 1998] not only constructs the candidates in a bottom-up manner like Apriori, but also starts a top-down search at the same time. This can help in reducing the number of database scans. MaxMiner <ref> [Bayardo, 1998] </ref> is another algorithm for finding the maximal elements. It uses efficient pruning techniques to quickly narrow the search space.
Reference: [Berge, 1989] <author> C. Berge, </author> <title> Hypergraphs: Combinatorics of Finite Sets, </title> <publisher> North-Holland, </publisher> <year> 1989. </year>
Reference: [Bitton et al., 1984] <author> Dina Bitton, David DeWitt, David K. Hsiao, and Jaishankar Menon, </author> <title> "A Taxonomy of Parallel Sorting," </title> <journal> ACM Computing Surveys, </journal> <volume> 16(3) </volume> <pages> 287-318, </pages> <month> September </month> <year> 1984. </year>
Reference-contexts: For simple datasets such as F2, it can be significant, whereas it is negligible for complex datasets such as F7. We have not focussed on parallelizing these phases, concentrating instead on the more challenging build phase. There is much existing research in parallel sorting on 151 SMP machines <ref> [Bitton et al., 1984] </ref>. The creation of attribute lists can be speeded up by essentially using multiple input streams and merging this phase with the sort phase. In our implementation, the data scan to create attribute lists is sequential, although we write attribute lists in parallel.
Reference: [Breiman et al., 1984] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone, </author> <title> Classification and Regression Trees, </title> <publisher> Wadsworth, </publisher> <address> Belmont, </address> <year> 1984. </year>
Reference-contexts: Since the field being predicted is pre-labeled, classification is also known as supervised induction. While there are several classification methods including neural networks [Lippmann, 1987] and genetic algorithms [Goldberg, 1989], decision trees <ref> [Breiman et al., 1984; Quinlan, 1993] </ref> are particularly suited to data mining, since they can be constructed relatively quickly, and are simple and easy to understand. While classification predicts a categorical value, regression is applied if the field being predicted comes from a real-valued domain. <p> Classification is a well-studied problem (see [Weiss and Kulikowski, 1991; Michie et al., 1994] for excellent overviews) and several models have been proposed over the years, which include neural networks [Lippmann, 1987], statistical models like linear/quadratic discriminants [James, 1985], decision trees <ref> [Breiman et al., 1984; Quinlan, 1993] </ref> and genetic algorithms [Goldberg, 1989]. Among these models, decision trees are particularly suited for data mining [Agrawal et al., 1993a; Mehta et al., 1996]. Decision 122 trees can be constructed relatively fast compared to other methods. <p> The algorithms presented there also require processor communication to evaluate any given split point, limiting the number of possible partitioning schemes the algorithms can efficiently consider for each leaf. The Darwin toolkit from Thinking Machines also contained a parallel implementation of the decision-tree classifier CART <ref> [Breiman et al., 1984] </ref>; however, details of this paralleliza-tion are not available in published literature. The recently proposed SLIQ classification algorithm [Mehta et al., 1996] addressed several issues in building a fast scalable classifier. SLIQ gracefully handles disk-resident data that is too large to fit in memory. <p> The decision tree can be used to screen future insurance applicants by classifying them into the High or Low risk categories. A decision tree classifier is usually built in two phases <ref> [Breiman et al., 1984; Quinlan, 1993] </ref>: a growth phase and a prune phase. The tree is grown using an elegantly simple divide and conquer approach. It takes as input a set of training examples S. There are basically two cases to be considered. <p> It builds the tree breadth-first and uses a one-time pre-sorting technique to reduce the cost of continuous attribute evaluation. In contrast to this, the well-known CART <ref> [Breiman et al., 1984] </ref> and C4.5 [Quinlan, 1993] classifiers, grow trees depth-first and repeatedly sort the data at every node of the tree to arrive at the best splits for continuous attributes. Attribute lists SPRINT initially creates an disk-based attribute list for each attribute in the data. <p> The "goodness" of the split obviously depends on how well it separates the classes. Several splitting indices have been proposed in the past to evaluate the goodness of the split. SPRINT uses the gini index <ref> [Breiman et al., 1984] </ref> for this task. For a data set S containing examples from n classes, gini (S) is defined as gini (S) = 1 X j where p j is the relative frequency of class j in S. <p> The accuracy and tree size characteristics of our SMP classifier are identical to SLIQ and SPRINT since they consider the same splits and use the same pruning algorithm. SLIQ's accuracy, execution time, and tree size have been compared with other classifiers such as CART <ref> [Breiman et al., 1984] </ref> and C4 (a predecessor of C4.5 [Quinlan, 1993]). This performance evaluation, available in [Mehta et al., 1996], shows that compared to other classifiers SLIQ achieves comparable or better classification accuracy, but produces small decision trees and has small execution times.
Reference: [Brin et al., 1997] <author> S. Brin, R. Motwani, J. Ullman, and S. Tsur, </author> <title> "Dynamic Itemset Counting and Implication Rules for Market Basket Data," </title> <booktitle> In ACM SIGMOD Conf. Management of Data, </booktitle> <month> May </month> <year> 1997. </year>
Reference-contexts: It generates frequent itemsets via logical AND operations on the bit-vectors. However, DLG assumes that the bit vectors fit in memory, and thus scalability could be a problem for databases with millions of transactions. The DIC algorithm <ref> [Brin et al., 1997] </ref> dynamically counts candidates of varying length as the database scan progresses, and thus is able to reduce the number of scans. Another way to minimize the I/O overhead is to work with only a small sample of the database. <p> CCPD uses additional optimization such as candidate balancing, hash-tree balancing and short-circuited subset counting to speed up performance [Zaki et al., 1996]. APM [Cheung et al., 1998] is an asynchronous parallel algorithm for shared-memory machines based on the DIC algorithm <ref> [Brin et al., 1997] </ref>. 3.3 Apriori-based Parallel Algorithms In this section we will look at some previous parallel algorithms. These algorithms assume that the database is partitioned among all the processors in equal-sized blocks, which reside on the local disk of each processor.
Reference: [Bron and Kerbosch, 1973] <author> C. Bron and J. Kerbosch, </author> <title> "Finding all cliques of an undi-rected graph," </title> <journal> Communications of the ACM, </journal> <volume> 16(9) </volume> <pages> 575-577, </pages> <year> 1973. </year>
Reference: [Carpineto and Romano, 1993] <author> C. Carpineto and G. Romano, </author> <title> "GALOIS: An order-theoretic approach to conceptual clustering," </title> <booktitle> In 10th Intl. Conf. on Machine Learning, </booktitle> <month> June </month> <year> 1993. </year>
Reference: [Carpineto and Romano, 1996] <author> C. Carpineto and G. Romano, </author> <title> "A lattice conceptual clustering system and its application to browsing retrieval," </title> <journal> Machine Learning, </journal> <volume> 24 </volume> <pages> 95-112, </pages> <year> 1996. </year>
Reference-contexts: A lot of algorithms have been proposed for generating the Galois lattice of concepts [Carpineto and Romano, 1993; Ganter, 1987; Godin et al., 1991; Guenoche, 1990; Kuznetsov, 1993]. An incremental approach for building the concepts was studied in <ref> [Carpineto and Romano, 1996; Godin et al., 1991] </ref>. These algorithms will have to be adapted to enumerate only the frequent concepts. Further, they have only been studied on small datasets. It remains to be seen how scalable these approaches are compared to the association mining algorithms.
Reference: [Catlett, 1991] <author> Jason Catlett, </author> <title> Megainduction: Machine Learning on Very Large Databases, </title> <type> PhD thesis, </type> <institution> University of Sydney, </institution> <year> 1991. </year> <month> 168 </month>
Reference-contexts: Recent work has targeted the massive databases usual in data mining. Classifying larger datasets can enable the development of higher accuracy models. Various studies have confirmed this hypothesis <ref> [Catlett, 1991; Chan and Stolfo, 1993a; Chan and Stolfo, 1993b] </ref>. Examples of fast scalable classification systems include SLIQ [Mehta et al., 1996], which for the first time was successful in handling disk-resident data. However, it did require some hashing information to be maintained in memory, restricting its scalability. <p> We give experimental results in Section 6.5 and conclude with a summary in Section 6.6. 6.2 Related Work Random sampling is often used to handle large datasets when building a classifier. Previous work on building tree-classifiers from large datasets includes Catlett's study of two methods <ref> [Catlett, 1991; Wirth and Catlett, 1988] </ref> for improving the time 124 taken to develop a classifier. The first method used data sampling at each node of the decision tree, and the second discretized continuous attributes.
Reference: [Chan and Stolfo, 1993a] <author> Philip K. Chan and Salvatore J. Stolfo, </author> <title> "Experiments on Multistrategy Learning by Meta-Learning," </title> <booktitle> In Proc. Second Intl. Conference on Info. and Knowledge Mgmt., </booktitle> <pages> pages 314-323, </pages> <year> 1993. </year>
Reference-contexts: Recent work has targeted the massive databases usual in data mining. Classifying larger datasets can enable the development of higher accuracy models. Various studies have confirmed this hypothesis <ref> [Catlett, 1991; Chan and Stolfo, 1993a; Chan and Stolfo, 1993b] </ref>. Examples of fast scalable classification systems include SLIQ [Mehta et al., 1996], which for the first time was successful in handling disk-resident data. However, it did require some hashing information to be maintained in memory, restricting its scalability. <p> The first method used data sampling at each node of the decision tree, and the second discretized continuous attributes. However, Catlett only considered datasets that could fit in memory; the largest training data had only 32,000 examples. Chan and Stolfo <ref> [Chan and Stolfo, 1993a; Chan and Stolfo, 1993b] </ref> considered partitioning the data into subsets that fit in memory and then developing a classifier on each subset in parallel. The output of multiple classifiers is combined using various algorithms to reach the final classification.
Reference: [Chan and Stolfo, 1993b] <author> Philip K. Chan and Salvatore J. Stolfo, </author> <title> "Meta-learning for Multistrategy and Parallel Learning," </title> <booktitle> In Proc. Second Intl. Workshop on Multistrat-egy Learning, </booktitle> <pages> pages 150-165, </pages> <year> 1993. </year>
Reference-contexts: Recent work has targeted the massive databases usual in data mining. Classifying larger datasets can enable the development of higher accuracy models. Various studies have confirmed this hypothesis <ref> [Catlett, 1991; Chan and Stolfo, 1993a; Chan and Stolfo, 1993b] </ref>. Examples of fast scalable classification systems include SLIQ [Mehta et al., 1996], which for the first time was successful in handling disk-resident data. However, it did require some hashing information to be maintained in memory, restricting its scalability. <p> The first method used data sampling at each node of the decision tree, and the second discretized continuous attributes. However, Catlett only considered datasets that could fit in memory; the largest training data had only 32,000 examples. Chan and Stolfo <ref> [Chan and Stolfo, 1993a; Chan and Stolfo, 1993b] </ref> considered partitioning the data into subsets that fit in memory and then developing a classifier on each subset in parallel. The output of multiple classifiers is combined using various algorithms to reach the final classification.
Reference: [Cheeseman et al., 1988] <author> P. Cheeseman, James Kelly, Matthew Self, et al., </author> <note> "AutoClass: </note>
References-found: 21

