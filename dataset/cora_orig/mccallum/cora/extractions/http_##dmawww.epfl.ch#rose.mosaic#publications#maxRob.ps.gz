URL: http://dmawww.epfl.ch/rose.mosaic/publications/maxRob.ps.gz
Refering-URL: http://dmawww.epfl.ch/rose.mosaic/publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Maximizing the Robustness of a Linear Threshold Classifier with Discrete Weights  
Author: Eddy Mayoraz and Vincent Robert 
Keyword: Perceptron, learning, robustness, weight quantization, combinatorial optimization, tabu search.  
Note: First revision  Second revision  
Date: December 1992  August 1993  January 94  
Address: CH-1015 Lausanne  
Affiliation: Department of Mathematics, Swiss Federal Institute of Technology  
Abstract: Quantization of the parameters of a Perceptron is a central problem in hardware implementation of neural networks using a numerical technology. An interesting property of neural networks used as classifiers is their ability to provide some robustness on input noise. This paper presents efficient learning algorithms for the maximization of the robustness of a Perceptron and especially designed to tackle the combinatorial problem arising from the discrete weights. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Amaldi, E. Mayoraz, and D. de Werra, </author> <title> A review of combinatorial problems arising in feedforward neural network design, </title> <type> tech. rep., </type> <institution> Swiss Federal Institute of Technology, Department of Mathematics, </institution> <year> 1991. </year> <note> to appear in Discrete Applied Math. </note>
Reference-contexts: Bouten, A. Komoda and R. Serneels is of particular interest for our work since it deals with parameters of values in TT [3]. They mainly established the critical storage capacity ff c (f ) of a perceptron with a fixed rate f 2 <ref> [0; 1] </ref> of non zero weights. It turns out that ff c attains its maximum value 1:17 when the dilution of the parameters is f = 0:63. In our case, the situation is slightly different since the dilution rate is not fixed a priori but chosen by the learning process.
Reference: [2] <author> E. Amaldi and S. Nicolis, </author> <title> Stability-capacity diagram of a neural network with ising bonds, </title> <journal> J. Physique, </journal> <volume> 50 (1989), </volume> <pages> pp. 2333-2345. </pages>
Reference-contexts: Many papers dedicated to the study of processing units with binary activations (i.e. 1) and with discrete parameters focus on the case of binary parameters x i = 1 <ref> [2, 21, 20, 33, 12, 3, 18, 29] </ref>. The analogy with spin glasses in physics, the simplicity of the model as well as the interest to have activations and weights taking the same values motivated this choice. <p> So, it illustrates the efficiency of each algorithm and the influence on the capacity of the introduction of the weight value 0. In 1989, E. Amaldi and S. Nicolis compared TS and SA techniques for the binary case <ref> [2] </ref>. One year later, H. Kohler tackled the same problem using a genetic algorithm [18]. In 1992, C. Perez et al. solved this problem using interior penalty technique (IP) [29] and H. Horner improves substantially the results obtained in [2] with SA [16]. 15 (w) p n ff = 0:3 ff <p> Nicolis compared TS and SA techniques for the binary case <ref> [2] </ref>. One year later, H. Kohler tackled the same problem using a genetic algorithm [18]. In 1992, C. Perez et al. solved this problem using interior penalty technique (IP) [29] and H. Horner improves substantially the results obtained in [2] with SA [16]. 15 (w) p n ff = 0:3 ff = 0:7 n = The best stability found by our method based on dynamic gain function is normalized by 1= n plotted against 1 n . <p> We did some experiments with IP for ternary weights [30] but apparently this method required a great expertise since the results we obtained were rather poor. By a private communication with authors of [29] we know that they ran ff c w i 2 IB, SA, <ref> [2] </ref> w i 2 IB, Genetic, [18] w i 2 IB, IP, [29] w i 2 TT , IP, authors of [29] w i 2 IB, SA, [16] w i 2 TT , TS, g ffi The critical storage capacity obtained by different optimization techniques is plotted against n.
Reference: [3] <author> M. Bouten, A. Komoda, and R. Serneels, </author> <title> Storage capacity of a diluted neural network with ising couplings, </title> <journal> J. Phys. A: Math. Gen., </journal> <volume> 23 (1990), </volume> <pages> pp. 2605-2611. </pages>
Reference-contexts: Many papers dedicated to the study of processing units with binary activations (i.e. 1) and with discrete parameters focus on the case of binary parameters x i = 1 <ref> [2, 21, 20, 33, 12, 3, 18, 29] </ref>. The analogy with spin glasses in physics, the simplicity of the model as well as the interest to have activations and weights taking the same values motivated this choice. <p> Amongst the numerous theoretical studies of the stability maximization problem achieved with statistical mechanic tools, the paper of M. Bouten, A. Komoda and R. Serneels is of particular interest for our work since it deals with parameters of values in TT <ref> [3] </ref>. They mainly established the critical storage capacity ff c (f ) of a perceptron with a fixed rate f 2 [0; 1] of non zero weights. It turns out that ff c attains its maximum value 1:17 when the dilution of the parameters is f = 0:63.
Reference: [4] <author> V. Chvatal, </author> <title> Linear Programming, </title> <publisher> Freeman and Co., </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: stability can then be expressed as a linear programming problem P P : u.c. x &gt; a k 8k (2) and can be solved either by the simplex algorithm or by more sophisticated techniques that ensures to find the optimum in a polynomial time (Khachian algorithm or Karmarkar's method in <ref> [4, 28] </ref>). The tightness of the lower bound of expressed in (1) depends on the number of values x i a k i close to kxk 1 for the points a k s of lowest potentials [22].
Reference: [5] <author> W. Cook, A. M. H. Gerards, A. Schrijver, and E. Tardos, </author> <title> Sensitivity theorems in integer linear programming, </title> <journal> Mathematical Programming, </journal> <volume> 34 (1986), </volume> <pages> pp. 251-264. </pages>
Reference-contexts: The problem of determining upper bounds on the deviation between w fl an optimum of an integer linear 4 problem and x fl an optimum of the associated linear problem has been addressed in a general context by Cook, Gerards, Schrijver and Tardos <ref> [5] </ref>. Unfortunately, their results are not applicable in our case, since their bound for kx fl w fl k 1 is much larger than 2, the maximum deviation.
Reference: [6] <author> D. Costa, </author> <title> An evolutionary tabu search algorithm and the NHL scheduling problem, </title> <type> ORWP 92/11, </type> <institution> Swiss Federal Institute of Technology, Department of Mathematics, </institution> <year> 1992. </year>
Reference-contexts: In order to improve the lower bound of the optimum stability, we also tested memetic algorithms <ref> [6, 27] </ref> based on a genetic algorithm principle and improved by TS which is applied on each member of the population before each crossing-over.
Reference: [7] <institution> CPLEX Optimization, Inc., CPLEX 1.2, 7710-T Cherry Park, </institution> <address> Suite 124 Houston, TX 77095, </address> <year> 1990. </year>
Reference-contexts: In comparison, two other curves are 12 comparative CPU-time c) g # , w (0)random d) g ffi , w (0)random a) g # in TT N b) g # , w (0) is x fl truncated x fl The resolution of the linear program was achieved by CPLEX <ref> [7] </ref>. The times of the simplex is added to the time of the iterative search, when the latter is based on information given by the optimum of the linear program. superposed on that graph.
Reference: [8] <author> M. Frean, </author> <title> A thermal perceptron learning rule, </title> <booktitle> Neural Computation, 4 (1992), </booktitle> <pages> pp. 946-957. </pages>
Reference-contexts: For example, the adaline [35] is very efficient for training a single linear threshold unit used for control or adaptive systems. However, when the goal is to maximize the number of correct classifications amongst a given set of points separated into two distinct classes, the thermal perceptron <ref> [8] </ref>, the Ho-Kachyap adaptive process [13] or even the simple pocket algorithm [9] are much more appropriate than the adaline.
Reference: [9] <author> S. I. Gallant, </author> <title> Perceptron-based learning algorithms, </title> <journal> IEEE Trans. on Neural Networks, </journal> <volume> 1 (1990), </volume> <pages> pp. 179-191. </pages>
Reference-contexts: However, when the goal is to maximize the number of correct classifications amongst a given set of points separated into two distinct classes, the thermal perceptron [8], the Ho-Kachyap adaptive process [13] or even the simple pocket algorithm <ref> [9] </ref> are much more appropriate than the adaline. Indeed, if the distribution of the points into the two classes is unbalanced, the solution minimizing the quadratic error criterion can missclassify a part of the data even if the two sets of points are linearly separable.
Reference: [10] <author> F. Glover, </author> <title> "tabu search" part I, </title> <journal> ORSA J. Computing, </journal> <volume> 1 (1989), </volume> <pages> pp. 190-206. </pages>
Reference-contexts: The above description of TS is summarized and simplified and the reader who needs more information will find it in <ref> [10, 15, 11] </ref>. Tabu search is well suited for maximizing the stability. For a problem with n variables, the most general set of feasible solutions is TT n the set of 3 n weight vectors with components in TT .
Reference: [11] <author> F. Glover, M. Laguna, E. Taillard, and D. de Werra, eds., </author> <title> Tabu Search, </title> <note> vol. 41, No. 1-4 of Annals of Operation Research, </note> <editor> J. C. </editor> <publisher> Baltzer AG, Science Publishers, </publisher> <address> Basel-Switzerland, </address> <year> 1993. </year>
Reference-contexts: The above description of TS is summarized and simplified and the reader who needs more information will find it in <ref> [10, 15, 11] </ref>. Tabu search is well suited for maximizing the stability. For a problem with n variables, the most general set of feasible solutions is TT n the set of 3 n weight vectors with components in TT .
Reference: [12] <author> H. Gutfreund and Y. Stein, </author> <title> Capacity of neural networks with discrete synaptic couplings, </title> <journal> J. Phys. A: Math. Gen., </journal> <volume> 23 (1990), </volume> <pages> pp. 2613-2630. </pages>
Reference-contexts: Many papers dedicated to the study of processing units with binary activations (i.e. 1) and with discrete parameters focus on the case of binary parameters x i = 1 <ref> [2, 21, 20, 33, 12, 3, 18, 29] </ref>. The analogy with spin glasses in physics, the simplicity of the model as well as the interest to have activations and weights taking the same values motivated this choice.
Reference: [13] <author> M. H. Hassoun and J. Song, </author> <title> Adaptive Ho-Kashyap rules for perceptron training, </title> <journal> IEEE Trans. on Neural Networks, </journal> <volume> 3 (1992), </volume> <pages> pp. 51-61. 18 </pages>
Reference-contexts: However, when the goal is to maximize the number of correct classifications amongst a given set of points separated into two distinct classes, the thermal perceptron [8], the Ho-Kachyap adaptive process <ref> [13] </ref> or even the simple pocket algorithm [9] are much more appropriate than the adaline.
Reference: [14] <author> A. Hertz and D. de Werra, </author> <title> Using tabu search techniques for graph coloring, </title> <journal> Computing, </journal> <volume> 29 (1987), </volume> <pages> pp. </pages> <month> 345-351. </month> <title> [15] , The tabu search metaheuristic: How we used it, </title> <journal> Annals of Math. and Artificial Intelligence, </journal> <volume> 1 (1991), </volume> <pages> pp. 111-121. </pages>
Reference-contexts: The latter function is thus less constrained than the former. Note that the underlying idea of those two functions is derived from some very efficient objective functions designed for graph coloring problems <ref> [14] </ref>. The last function presented in this paper illustrates another way to take into account the set of all potentials.
Reference: [16] <author> H. Horner, </author> <title> Dynamics of learning for the binary perceptron problem, </title> <journal> Zeitschrift fur Physik B, Condensed Matter 86 (1992), </journal> <pages> pp. 291-308. </pages>
Reference-contexts: One year later, H. Kohler tackled the same problem using a genetic algorithm [18]. In 1992, C. Perez et al. solved this problem using interior penalty technique (IP) [29] and H. Horner improves substantially the results obtained in [2] with SA <ref> [16] </ref>. 15 (w) p n ff = 0:3 ff = 0:7 n = The best stability found by our method based on dynamic gain function is normalized by 1= n plotted against 1 n . <p> By a private communication with authors of [29] we know that they ran ff c w i 2 IB, SA, [2] w i 2 IB, Genetic, [18] w i 2 IB, IP, [29] w i 2 TT , IP, authors of [29] w i 2 IB, SA, <ref> [16] </ref> w i 2 TT , TS, g ffi The critical storage capacity obtained by different optimization techniques is plotted against n. The increase due to the consideration of ternary instead of binary weights is also presented. <p> In reading figure 8 we should keep in mind the fact that our goal was quite different than that in <ref> [18, 16] </ref>. The aim of the latter works was to develop algorithms able to bring the stability as far as possible in order to corroborate the theoretical previsions.
Reference: [17] <author> S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi, </author> <title> Optimization by simulated annealing, </title> <note> Science, 220 (1983), p. 671. </note>
Reference-contexts: At step k, the choice of the neighbor is guided by the best value of g in N (s k ). Amongst the other heuristic techniques suitable for the resolution of the same kind of problems, one of the most popular is Simulated Annealing <ref> [17] </ref>. The difference between TS and SA lies in the way a move is accepted or rejected.
Reference: [18] <author> H. M. K ohler, </author> <title> Adaptive genetic algorithm for the binary perceptron problem, </title> <journal> J. Phys. A: Math. Gen., </journal> <volume> 23 (1990), </volume> <pages> pp. L1265-L1271. </pages>
Reference-contexts: Many papers dedicated to the study of processing units with binary activations (i.e. 1) and with discrete parameters focus on the case of binary parameters x i = 1 <ref> [2, 21, 20, 33, 12, 3, 18, 29] </ref>. The analogy with spin glasses in physics, the simplicity of the model as well as the interest to have activations and weights taking the same values motivated this choice. <p> In 1989, E. Amaldi and S. Nicolis compared TS and SA techniques for the binary case [2]. One year later, H. Kohler tackled the same problem using a genetic algorithm <ref> [18] </ref>. In 1992, C. Perez et al. solved this problem using interior penalty technique (IP) [29] and H. <p> By a private communication with authors of [29] we know that they ran ff c w i 2 IB, SA, [2] w i 2 IB, Genetic, <ref> [18] </ref> w i 2 IB, IP, [29] w i 2 TT , IP, authors of [29] w i 2 IB, SA, [16] w i 2 TT , TS, g ffi The critical storage capacity obtained by different optimization techniques is plotted against n. <p> In reading figure 8 we should keep in mind the fact that our goal was quite different than that in <ref> [18, 16] </ref>. The aim of the latter works was to develop algorithms able to bring the stability as far as possible in order to corroborate the theoretical previsions.
Reference: [19] <author> W. Krauth and M. M ezard, </author> <title> Learning algorithms with optimal stability in neural networks, </title> <journal> Journal of Physics A: Math. Gen., </journal> <volume> 20 (1987), </volume> <pages> pp. </pages> <month> L745-L752. </month>
Reference-contexts: This property is obtained in an optimum way by maximizing the criterion of the stability defined by Krauth and Mezard <ref> [19] </ref> and presented in further details in section 2. <p> W. Krauth and M. Mezard <ref> [19] </ref> suggest to normalize x with kxk 1 = 1 and to maximize the stability (T; x) in order to improve the robustness of the classifier. <p> The Euclidian normalization of x seems to provide a better lower bound: 2kxk 2 However, the resulting optimization problem is no more linear and by the way, much more difficult to solve <ref> [19] </ref>. From now on we will be interested in the weights limited to the set TT composed of the three values 1, 0 and +1.
Reference: [20] <author> W. Krauth and M. M ezard, </author> <title> Storage capacity of memory networks with binary couplings, </title> <institution> J. Phys. France, </institution> <month> 50 </month> <year> (1989), </year> <pages> pp. 3057-3066. </pages>
Reference-contexts: Many papers dedicated to the study of processing units with binary activations (i.e. 1) and with discrete parameters focus on the case of binary parameters x i = 1 <ref> [2, 21, 20, 33, 12, 3, 18, 29] </ref>. The analogy with spin glasses in physics, the simplicity of the model as well as the interest to have activations and weights taking the same values motivated this choice.
Reference: [21] <author> W. Krauth and M. Opper, </author> <title> Critical storage capacity of the j = 1 neural networks, </title> <journal> J. Phys. A, </journal> <volume> 22 (1989), </volume> <pages> pp. </pages> <month> L519-L523. </month>
Reference-contexts: Many papers dedicated to the study of processing units with binary activations (i.e. 1) and with discrete parameters focus on the case of binary parameters x i = 1 <ref> [2, 21, 20, 33, 12, 3, 18, 29] </ref>. The analogy with spin glasses in physics, the simplicity of the model as well as the interest to have activations and weights taking the same values motivated this choice.
Reference: [22] <author> E. Mayoraz, </author> <title> Benchmark of some learning algorithms for single layer and Hopfield networks, </title> <journal> Complex Systems, </journal> <volume> 4 (1990), </volume> <pages> pp. </pages> <month> 477-490. </month> <title> [23] , On the power of networks of majority functions, </title> <booktitle> in Lecture Notes in Computer Science 540, </booktitle> <editor> A. Prieto, ed., IWANN'91, </editor> <publisher> Springer-Verlag, </publisher> <year> 1991, </year> <pages> pp. </pages> <month> 78-85. </month> <title> [24] , Maximizing the stability of a majority perceptron using tabu search, </title> <booktitle> in Proceedings of IJCNN'92 Baltimore, </booktitle> <year> 1992, </year> <pages> pp. </pages> <month> II254-II259. </month> <title> [25] , On the power of democratic networks, </title> <type> Tech. Rep. </type> <institution> ORWP 93/2, Swiss Federal Institute of Technology, Department of Mathematics, </institution> <year> 1993. </year> <note> submitted for publication. </note>
Reference-contexts: The tightness of the lower bound of expressed in (1) depends on the number of values x i a k i close to kxk 1 for the points a k s of lowest potentials <ref> [22] </ref>. The Euclidian normalization of x seems to provide a better lower bound: 2kxk 2 However, the resulting optimization problem is no more linear and by the way, much more difficult to solve [19].
Reference: [26] <author> E. Mayoraz and F. Aviolat, </author> <title> Constructive training methods for feedforward neural networks with binary weights, </title> <institution> orwp, Swiss Federal Institute of Technology, Department of Mathematics, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: In the present work, we tend to produce algorithms that run fast enough to be used at each iteration of a general training algorithm for multilayer feedforward neural networks (see <ref> [26] </ref>). Thus, in our experiment with binary weights, for each problem TS executed around 1500 steps and for problems of size n = 255, the computational time was below 3 minutes per problem on a Silicon Graphics MIPS R4400 at 50Mhz.
Reference: [27] <author> P. Moscato, </author> <title> An introduction to population approaches for optimization and hierarchical objective functions: A discussion on the role of tabu search, </title> <journal> in Annals of Operation Research, </journal> <volume> Vol 41, No. </volume> <month> 1-4, </month> <title> Tabu Search, </title> <editor> F. Glover, M. Laguna, E. Taillard, and D. de Werra, eds., J. C. </editor> <publisher> Baltzer AG, Science Publishers, </publisher> <address> Basel-Switzerland, </address> <month> May </month> <year> 1993, </year> <pages> pp. 85-121. </pages>
Reference-contexts: In order to improve the lower bound of the optimum stability, we also tested memetic algorithms <ref> [6, 27] </ref> based on a genetic algorithm principle and improved by TS which is applied on each member of the population before each crossing-over.
Reference: [28] <author> C. H. Papadimitriou and K. Steiglitz, </author> <title> Combinatorial Optimization: Algorithms and Complexity, </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1982. </year>
Reference-contexts: stability can then be expressed as a linear programming problem P P : u.c. x &gt; a k 8k (2) and can be solved either by the simplex algorithm or by more sophisticated techniques that ensures to find the optimum in a polynomial time (Khachian algorithm or Karmarkar's method in <ref> [4, 28] </ref>). The tightness of the lower bound of expressed in (1) depends on the number of values x i a k i close to kxk 1 for the points a k s of lowest potentials [22].
Reference: [29] <author> C. J. P erez Vincent, J. Carrabina, and E. Valderrama, </author> <title> Study of learning algorithm for neural network with discrete synaptic couplings, Network, </title> <booktitle> 3 (1992), </booktitle> <pages> pp. 165-176. </pages>
Reference-contexts: Many papers dedicated to the study of processing units with binary activations (i.e. 1) and with discrete parameters focus on the case of binary parameters x i = 1 <ref> [2, 21, 20, 33, 12, 3, 18, 29] </ref>. The analogy with spin glasses in physics, the simplicity of the model as well as the interest to have activations and weights taking the same values motivated this choice. <p> In 1989, E. Amaldi and S. Nicolis compared TS and SA techniques for the binary case [2]. One year later, H. Kohler tackled the same problem using a genetic algorithm [18]. In 1992, C. Perez et al. solved this problem using interior penalty technique (IP) <ref> [29] </ref> and H. Horner improves substantially the results obtained in [2] with SA [16]. 15 (w) p n ff = 0:3 ff = 0:7 n = The best stability found by our method based on dynamic gain function is normalized by 1= n plotted against 1 n . <p> Each point is averaged over 100 random problems and each curve corresponds to a different ratio ff = p=n, namely from up to down, 0:1, 0:3, 0:5 and 0:7. In <ref> [29] </ref>, the authors claimed that their approach can be easily generalized to other discrete sets of weight values. We did some experiments with IP for ternary weights [30] but apparently this method required a great expertise since the results we obtained were rather poor. <p> We did some experiments with IP for ternary weights [30] but apparently this method required a great expertise since the results we obtained were rather poor. By a private communication with authors of <ref> [29] </ref> we know that they ran ff c w i 2 IB, SA, [2] w i 2 IB, Genetic, [18] w i 2 IB, IP, [29] w i 2 TT , IP, authors of [29] w i 2 IB, SA, [16] w i 2 TT , TS, g ffi The critical <p> By a private communication with authors of <ref> [29] </ref> we know that they ran ff c w i 2 IB, SA, [2] w i 2 IB, Genetic, [18] w i 2 IB, IP, [29] w i 2 TT , IP, authors of [29] w i 2 IB, SA, [16] w i 2 TT , TS, g ffi The critical storage capacity obtained by different optimization techniques is plotted against n. <p> By a private communication with authors of <ref> [29] </ref> we know that they ran ff c w i 2 IB, SA, [2] w i 2 IB, Genetic, [18] w i 2 IB, IP, [29] w i 2 TT , IP, authors of [29] w i 2 IB, SA, [16] w i 2 TT , TS, g ffi The critical storage capacity obtained by different optimization techniques is plotted against n. The increase due to the consideration of ternary instead of binary weights is also presented.
Reference: [30] <author> V. </author> <type> Robert, </type> <institution> Determination d'une fonction majorite maximisant la stabilite, travail de diplome, Ecole Polytechnique Federale de Lausanne, Departement de Mathematiques, </institution> <year> 1991. </year>
Reference-contexts: In [29], the authors claimed that their approach can be easily generalized to other discrete sets of weight values. We did some experiments with IP for ternary weights <ref> [30] </ref> but apparently this method required a great expertise since the results we obtained were rather poor.
Reference: [31] <author> F. Rosenblatt, </author> <title> The perceptron: a probabilistic model for information storage and organization in the brain, </title> <journal> Psychological Review, </journal> <volume> 63 (1958), </volume> <pages> pp. 386-408. 19 </pages>
Reference-contexts: The optimization tools used to solve the problem are described in section 4. Numerical results are reported in section 5. 2 Definition of the model The neural model considered in this study is based on the perceptron of Rosenblatt <ref> [31] </ref>, with binary input and output activations, so the neural function is simply a linear threshold Boolean function. The set of Boolean values is noted IB and we choose the numerical 2 representation 1 and +1 for the values False and True respectively.
Reference: [32] <author> E. Taillard, </author> <title> Recherche Iterative Dirigee Parallele, </title> <type> PhD thesis, </type> <institution> Ecole Polytechnique Federale de Lausanne, Departement de Mathematiques, </institution> <year> 1993. </year>
Reference-contexts: This situation seems to be particular to the robustness maximizing problem because in other applications of TS the tabu list length is a sensitive parameter <ref> [32] </ref>. Our main objective in this work was to produce an algorithm able to find a good solution in a short time because in a further work it will be applied iteratively on each unit of a full multilayer network by a global algorithm.
Reference: [33] <author> S. Venkatesh, </author> <title> Directed drift: A new linear threshold algorithm for learning binary weights on-line, </title> <journal> Journal of Computer Science and Systems, </journal> <year> (1989). </year>
Reference-contexts: Many papers dedicated to the study of processing units with binary activations (i.e. 1) and with discrete parameters focus on the case of binary parameters x i = 1 <ref> [2, 21, 20, 33, 12, 3, 18, 29] </ref>. The analogy with spin glasses in physics, the simplicity of the model as well as the interest to have activations and weights taking the same values motivated this choice.
Reference: [34] <author> M. Verleysen, B. Sirletti, A. Vandermeulebroecke, and P. Jespers, </author> <title> A high-storage capacity content-addressable memory and its learning algorithm, </title> <journal> IEEE Trans. on Circuits and Systems, </journal> <volume> 36 (1989), </volume> <pages> pp. 762-766. </pages>
Reference-contexts: We can find in the literature <ref> [34] </ref> examples of resolution of the stability maximization problem with bipolar or ternary weights, consisting of truncating a real optimum of P described in (2). (x fl ) (w) w = x fl on 3 values w = x fl on 5 values w = x fl on 7 values w

References-found: 30

