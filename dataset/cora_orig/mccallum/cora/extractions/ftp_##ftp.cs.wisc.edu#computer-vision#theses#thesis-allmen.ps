URL: ftp://ftp.cs.wisc.edu/computer-vision/theses/thesis-allmen.ps
Refering-URL: http://www.cs.wisc.edu/computer-vision/pubs.html
Root-URL: 
Title: IMAGE SEQUENCE DESCRIPTION USING SPATIOTEMPORAL FLOW CURVES: TOWARD MOTION-BASED RECOGNITION  
Author: By MARK C. ALLMEN 
Degree: A thesis submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy (Computer Sciences) at the  
Date: 1991  
Address: WISCONSIN MADISON  
Affiliation: UNIVERSITY OF  
Abstract-found: 0
Intro-found: 1
Reference: [Adel85] <author> E. H. Adelson and J. R. Bergen. </author> <title> Spatiotemporal energy models for the perception of motion. </title> <journal> J. Optical Society of America, </journal> <volume> 2:284 - 299, </volume> <month> February, </month> <year> 1985. </year>
Reference-contexts: The psychophysical work includes that of Watson, Barlow, VanSanten, Fleet and Heeger <ref> [Heeg87, Wats85, Barl65, VanS85, Adel85, Flee84a, Flee84b] </ref>. Computational models include those developed by Marr, Buxton, and Liou [Marr81, Buxt83, Buxt84, Liou89]. According to Liou and Jain [Liou89], filter models suffer from several fundamental problems. First, most filter models assume constant velocity.
Reference: [Agga88] <author> J. K. Aggarwal and N. Nandhakumar. </author> <title> On the computation of motion from sequences of images A review. </title> <journal> Proc. IEEE, </journal> <volume> 76 </volume> <pages> 917-935, </pages> <year> 1988. </year>
Reference-contexts: Edge operators detect edge points based on this definition, recovering edge points only where spatial coherence does not exist [Cann86]. Temporal coherence, while used in short image sequences for the computation of optical flow <ref> [Agga88] </ref>, ST surface flow [Allm90a] and simplifying the correspondence problem [Grzy89], has not been used previously as a constraint for motion understanding over long image sequences. In addition to temporal and spatial coherence of intensity values, there is temporal and spatial coherence of motion. <p> One approach to quantifying this structure would be to use partial derivatives to examine how the surface changes. This is the approach taken in many optical flow algorithms <ref> [Agga88] </ref>. <p> Eq. (3.2) and dividing through by ffit we have ffix ffit @E + ffiy ffit @E + @t 29 In the limit as ffi ! 0 dx @E + dt @y @E = 0 (3:3) This is known as the optical flow constraint equation and, along with additional con straints <ref> [Agga88] </ref>, has been widely used to compute optical flow. The optical flow constraint equation contains two unknowns, dx dt and dy dt , and is there fore insufficient to uniquely determine the motion. The system is under constrained since ffiE is affected by motion along both axes. <p> with cases where it exists, is also briefly addressed. 62 3.5.1 Comparison with Gradient-Based Optical Flow Algo- rithms Gradient-based optical flow algorithms typically use Eq. (3.3), the optical flow constraint equation, along with additional assumptions in order to solve for the motion dx dt and dy These additional assumptions include <ref> [Agga88] </ref>: * optical flow is smooth and neighboring points have similar velocities * optical flow is constant over some segment of the image * objects in the scene undergo restricted motion By examining what, if any, of these three assumption our method makes we can better understand the relationship between our <p> A common paradigm when examining image sequences is to recover 3D structure or 3D motion immediately after optical flow is computed <ref> [Agga88] </ref>. Recently, some work has even completely skipped the flow recovery step and computed structure and motion directly [Heel90, Taal90, Faug90]. However, a great deal of information exists in the flow field and therefore our approach is to recover a qualitative description of image motion from ST surface flow.
Reference: [Akit84] <author> K. Akita. </author> <title> Image sequence analysis of real world human motion. </title> <journal> Pattern Recognition, </journal> <volume> 17 </volume> <pages> 73-83, </pages> <year> 1984. </year>
Reference-contexts: High-level motion analysis, i.e., recognizing a coordinated sequence of events such as walking and throwing, has been formulated previously as a process that follows high-level object 2 recognition <ref> [Hogg83, Akit84, Leun87a, Leun87b] </ref>. Only recently have some researchers considered high-level motion analysis as a process that does not depend on complex object descriptions [Godd88b, Godd88a, Godd89, Goul89]. <p> Goddard [Godd88a] noted that there are two ways that motion in a scene can be used. The first paradigm used motion information to first recover the rigid structure of the objects. The recovered structure was then used to index into models <ref> [Hogg83, Akit84, Leun87a, Leun87b] </ref>. The second paradigm used motion information to index into the models. Gould and Shah [Goul89] and Goddard [Godd88a] have asserted that it is possible to recognize an object and its high-level motion without first recovering its structure. <p> The system successfully recognized and tracked a human in the image sequence. The system, however, had only one object model, a human, and walking is the only high-level motion that the system could track. Akita <ref> [Akit84] </ref> also worked with a model of a human. His model was similar to Marr and Vaina's [Marr80] in that it used generalized cylinders to represent solid parts and it stored relations between the cylinders to represent the joints. <p> This is demonstrated by the fact that a model is matched to one frame of a sequence <ref> [Hogg83, Akit84] </ref> before the motion of the object is used.
Reference: [Alle89] <author> P. K. Allen. </author> <title> Real-time motion tracking using spatio-temporal filters. </title> <booktitle> In Proc. Image Understanding Workshop, </booktitle> <pages> pages 695-701, </pages> <year> 1989. </year>
Reference-contexts: However, in many visual motion tasks, exact localization of motion boundaries is not required. For example, if 74 one is interested in tracking an object, the exact location of the object boundary is not necessarily as important as, say, where the centroid of the object moves <ref> [Alle89] </ref>. An automated surveillance system may need to detect only that motion occurred, and track the centroid of the motion [Burt88]. In obstacle avoidance, unless the margin for error is small, the exact boundaries of objects in relative motion are not needed [Nels88].
Reference: [Allm90a] <author> M. Allmen and C. R. Dyer. </author> <title> Computing spatiotemporal surface flow. </title> <booktitle> In Proc. 3rd Int. Conf. Computer Vision, </booktitle> <pages> pages 47-50, </pages> <year> 1990. </year>
Reference-contexts: Edge operators detect edge points based on this definition, recovering edge points only where spatial coherence does not exist [Cann86]. Temporal coherence, while used in short image sequences for the computation of optical flow [Agga88], ST surface flow <ref> [Allm90a] </ref> and simplifying the correspondence problem [Grzy89], has not been used previously as a constraint for motion understanding over long image sequences. In addition to temporal and spatial coherence of intensity values, there is temporal and spatial coherence of motion. <p> An ST cube, constructed by stacking a sequence of temporally-close images to 6 gether, has temporal motion coherence (as well as temporal gray level coherence) and is therefore an ideal structure for studying image sequences using temporal coherence <ref> [Allm90a, Jain88, Bake88b] </ref>. A first step in understanding the motion in an ST cube is to determine the instantaneous motion of each point in the cube, called the ST surface flow (see Chapter 3). <p> Given a starting point, an orientation, and the curvature and torsion values at every point on a curve, that curve is uniquely defined [DoCa76]. Therefore, the torsion and curvature are good measures of the shape of curves <ref> [Allm90a, Mokh88, Hoff88] </ref>. By classifying the different types of motion that can occur in the scene, and the resulting projected motion, one can gain an understanding of the types of flow curves that can be produced.
Reference: [Allm90b] <author> M. Allmen and C. R. Dyer. </author> <title> Cyclic motion detection using spatiotemporal surfaces and curves. </title> <booktitle> In Proc. 10th Int. Conf. Pattern. Recognition, </booktitle> <pages> pages 365-370, </pages> <year> 1990. </year>
Reference-contexts: A complete description of ST surfaces includes global as well as local information. That is, not only is the flow at each point needed, but higher level descriptions such as long-range periodic motion are also desired <ref> [Allm91, Allm90b] </ref>. In order to recover more global descriptions of ST surfaces, temporal as well as spatial coherence is needed. Optical flow uses spatial coherence, but the traditional approach which solves for the flow field at one instant of time, does not maintain temporal coherence. <p> Our work can be viewed as an extension of that problem into the temporal dimension, taking advantage of temporal motion coherence over long image sequences. This allows the recognition of higher-level motions, e.g., cyclic motion <ref> [Allm90b] </ref>, which occur over long image sequences. By clustering flow curves, a qualitative grouping of regions in the ST cube is recovered as opposed to a segmentation of the ST cube. Segmentation requires that every pixel in the cube be classified into some set. <p> Therefore, because torsion adds nothing and is a third derivative operation making it very susceptible to noise, only curvature and velocity are used. Other work has also used only curvature for similar reasons <ref> [Allm90b] </ref>. From these five primitive cases of 3D motion, we have shown how curvature and velocity are sufficient for describing and classifying flow curves. When objects undergo a combination of these types of motion, accelerate, or there is camera motion, the flow curves become more complicated of course.
Reference: [Allm91] <author> M. Allmen and C. R. Dyer. </author> <title> Long-range spatiotemporal motion understanding using spatiotemporal flow curves. </title> <booktitle> In Proc. Computer Vision and Pattern Recognition Conf., </booktitle> <pages> pages 303-309, </pages> <year> 1991. </year>
Reference-contexts: A complete description of ST surfaces includes global as well as local information. That is, not only is the flow at each point needed, but higher level descriptions such as long-range periodic motion are also desired <ref> [Allm91, Allm90b] </ref>. In order to recover more global descriptions of ST surfaces, temporal as well as spatial coherence is needed. Optical flow uses spatial coherence, but the traditional approach which solves for the flow field at one instant of time, does not maintain temporal coherence.
Reference: [Aloi88] <author> J. Aloimonos. </author> <title> Visual shape computation. </title> <journal> Proc. IEEE, </journal> <volume> 76 </volume> <pages> 899-916, </pages> <year> 1988. </year>
Reference-contexts: Many optical flow [Horn81] and structure-from-motion [Ullm79] algorithms are similar in that they also use only a few frames to accomplish their goal. Recently, there has been a trend toward using more than a few frames to examine an image sequence <ref> [Jain88, Aloi88] </ref>. Jain notes: "Most techniques try to do too much using too few frames. <p> They then defined a motion detector using these hypersurfaces. Most work on spatiotemporal surfaces has used a 3D volume rather than the 4D space of Liou and Jain. The fourth dimension is eliminated by storing the gray level at each space-time point. Aloimonos <ref> [Aloi88] </ref> introduced flow lines to represent the trajectories of point features through the spatiotemporal volume. Peng and Medioni [Peng89] worked with 2D slices of a spatiotemporal volume. For every edge point in the first frame, four slices, centered at the point, were taken.
Reference: [Ande85] <author> C. H. Anderson, P. J. Burt, and G. S. VanDerWal. </author> <title> Change detection and tracking using pyramid transform techniques. </title> <booktitle> Proc. SPIE Intelligent Robots and Computer Vision, </booktitle> <volume> 579:72 - 78, </volume> <year> 1985. </year>
Reference-contexts: Most previous work has focused on low-level motion analysis. Frame differencing of temporally-adjacent frames is one common method of low-level motion analysis <ref> [Jain79b, Jain79a, Jain81, Jain83, Jaya83, Ande85, Lee88a] </ref>. Recovering orientation of a surface or the position within the scene of points on an object is commonly referred to as structure-from-motion. Structure-from-motion has been performed using correspondences between points [Ullm79] as well as without using point correspondences [Terz88]. <p> They perform poorly if motion is not in a steady state. Much of the temporal examination of image sequences consists of looking at only a few frames at a time. Frame differencing has been a popular method of temporal examination of two frames <ref> [Jain79b, Jain79a, Jain81, Jain83, Lee88b, Jaya83, Ande85] </ref>. With this method, the difference of two temporally-adjacent frames is calculated and where the difference is nonzero, there has been motion. Lee and Anderson [Lee88b, Ande85] constructed spatial pyramids from difference images in order to track objects. <p> Frame differencing has been a popular method of temporal examination of two frames [Jain79b, Jain79a, Jain81, Jain83, Lee88b, Jaya83, Ande85]. With this method, the difference of two temporally-adjacent frames is calculated and where the difference is nonzero, there has been motion. Lee and Anderson <ref> [Lee88b, Ande85] </ref> constructed spatial pyramids from difference images in order to track objects. But this spatial pyramid must be rebuilt for every pair of frames. Many optical flow [Horn81] and structure-from-motion [Ullm79] algorithms are similar in that they also use only a few frames to accomplish their goal.
Reference: [Badl79] <author> N. I. Badler and S. W. Smoliar. </author> <title> Digital representations of human movement. </title> <journal> Computing Surveys, </journal> <volume> 11 </volume> <pages> 19-38, </pages> <month> March, </month> <year> 1979. </year>
Reference-contexts: Using spatiotemporal surfaces, Bolles and Baker [Boll87, Bake88b] examined how to recover camera motion. Baker [Bake88a] showed how to construct these spatiotemporal surfaces. Most of this work has concentrated on recovering camera motion rather than a description of high-level motion in the scene. Badler and Smoliar <ref> [Badl79] </ref> presented a survey of representations of human movement. <p> O'Rourke and Badler [O'Ro80] used constraints and a model of the human body to analyze human walking. The model is the same as the one developed by Badler and Toltzis <ref> [Badl79] </ref>. The model consists of overlapping spheres and the constraints arise from two sources: 1) the structure of the human body and 2) properties of the function that form images by projection of the model.
Reference: [Bake88a] <author> H. H. Baker. </author> <title> Building surfaces of evolution: the weaving wall. </title> <booktitle> Image Understanding Workshop, </booktitle> <pages> pages 1031-1040, </pages> <year> 1988. </year>
Reference-contexts: High-level motion is concerned with using these low-level motion descriptions in order to recover a coordinated sequence of events that can then be used to index into models of high-level motion. Using spatiotemporal surfaces, Bolles and Baker [Boll87, Bake88b] examined how to recover camera motion. Baker <ref> [Bake88a] </ref> showed how to construct these spatiotemporal surfaces. Most of this work has concentrated on recovering camera motion rather than a description of high-level motion in the scene. Badler and Smoliar [Badl79] presented a survey of representations of human movement. <p> These errors were caused because edge points from separate ST surfaces were recovered as one surface. In general, the problem of segmenting edge points into coherent ST surfaces is a difficult problem <ref> [Bake88a] </ref> which we have not focused on in this research. Gray level images are preferable in this respect since the intermediate steps of detecting edge points and segmenting them into ST surfaces are avoided. 46 (a) pixels/frame. (b) ST surface flow for the middle frame.
Reference: [Bake88b] <author> H. H. Baker and R. C. Bolles. </author> <title> Generalizing epipolar-plane image analysis on the spatiotemporal surface. </title> <booktitle> Proc. Computer Vision and Pattern Recognition Conf., </booktitle> <pages> pages 2-9, </pages> <year> 1988. </year> <month> 137 </month>
Reference-contexts: An ST cube, constructed by stacking a sequence of temporally-close images to 6 gether, has temporal motion coherence (as well as temporal gray level coherence) and is therefore an ideal structure for studying image sequences using temporal coherence <ref> [Allm90a, Jain88, Bake88b] </ref>. A first step in understanding the motion in an ST cube is to determine the instantaneous motion of each point in the cube, called the ST surface flow (see Chapter 3). <p> High-level motion is concerned with using these low-level motion descriptions in order to recover a coordinated sequence of events that can then be used to index into models of high-level motion. Using spatiotemporal surfaces, Bolles and Baker <ref> [Boll87, Bake88b] </ref> examined how to recover camera motion. Baker [Bake88a] showed how to construct these spatiotemporal surfaces. Most of this work has concentrated on recovering camera motion rather than a description of high-level motion in the scene. Badler and Smoliar [Badl79] presented a survey of representations of human movement. <p> These considerations suggest a low-level representation that coherently represents many frames, i.e., an ST cube. The concise, coherent nature of this representation has also lead others to use it to examine long sequences of frames <ref> [Bake88b] </ref>. Because of the coherent nature of the cube, ST surface flow and ST flow curves can be computed. Since ST flow curves represent the motion over long sequences, it is an ideal representation from which to detect cyclic motion.
Reference: [Bake89] <author> H. H. Baker and R. C. Bolles. </author> <title> Generalizing epipolar-plane image analysis on the spatiotemporal surface. </title> <journal> Int. J. of Computer Vision, </journal> <volume> 3 </volume> <pages> 33-49, </pages> <year> 1989. </year>
Reference-contexts: We are concerned with recovering an interpretation of ST surfaces which can give information about motion in the scene without first recovering depth information. Baker and Bolles <ref> [Boll87, Bake89] </ref> examined ST surfaces in order to recover camera motion but objects in the scene were stationary. Jain [Jain88] described a spatiotemporal volume segmentation approach using the signs of the three principle curvatures of the ST 3-surface.
Reference: [Barl65] <author> H. B. Barlow and W. R. Levick. </author> <title> The mechanism of directionally selective units in rabbit's retina. </title> <journal> J. Physiol. London, </journal> <volume> 178 </volume> <pages> 477-504, </pages> <year> 1965. </year>
Reference-contexts: The psychophysical work includes that of Watson, Barlow, VanSanten, Fleet and Heeger <ref> [Heeg87, Wats85, Barl65, VanS85, Adel85, Flee84a, Flee84b] </ref>. Computational models include those developed by Marr, Buxton, and Liou [Marr81, Buxt83, Buxt84, Liou89]. According to Liou and Jain [Liou89], filter models suffer from several fundamental problems. First, most filter models assume constant velocity.
Reference: [Barr81] <author> H. G. Barrow and J. M. Tenenbaum. </author> <title> Computational vision. </title> <journal> Proc. IEEE, </journal> <volume> 69 </volume> <pages> 572-595, </pages> <month> May </month> <year> 1981. </year>
Reference-contexts: Other results also indicate that the perception of intermediate descriptions is accomplished prior to recognition of the stimulus. For example, Barrow and Tenenbaum <ref> [Barr81] </ref> used photomicrographs of pollen grains to argue that the HVS can recover surface shape and orientation without recognition. When Gabrielsson [Gabr73] presented subjects with songs, they could detect rhythm within the music even without having 101 heard the song previously.
Reference: [Bert84] <author> B. I. Bertenthal, D. R. Proffitt, and J. E. </author> <title> Cutting. Infant sensitivity to figural coherence in biomechanical motions. </title> <journal> Journal of Experiment Child Psychology, </journal> <volume> 37 </volume> <pages> 213-230, </pages> <year> 1984. </year>
Reference-contexts: As will be pointed out in Chapter 2, psychological models of the human visual system have shown that it is feasible to first recover the rigid parts of an object using motion and then use this rigid structure information to recognize the object <ref> [Joha73, Joha76, Bert84, Bert85] </ref>. All of the models presented in Section 2.2.2 also recover rigid parts and then use this information to index into the models of objects to be recognized. <p> The three processes are: 1) connectivity: the lights are connected into arms, legs, etc.; 2) depth order: the lights are ordered in depth; 3) implicit contour: an implicit contour consistent with the edges of the human form is perceived. Bertenthal, Proffitt and Cutting <ref> [Bert84] </ref> presented a model that consists of five levels: individual motions, organization of figural coherence, biomechanically appropriate motions, causal (kinetic) relations, and identification of human form. The individual motions level refers to the perception of the absolute motion of a single moving light.
Reference: [Bert85] <author> B. Bertenthal, D. Proffitt, N. Spetner, and A. Thomas. </author> <title> The development of infant sensitivity to biomechanical motions. Child Development, </title> <booktitle> 56 </booktitle> <pages> 531-543, </pages> <year> 1985. </year>
Reference-contexts: As will be pointed out in Chapter 2, psychological models of the human visual system have shown that it is feasible to first recover the rigid parts of an object using motion and then use this rigid structure information to recognize the object <ref> [Joha73, Joha76, Bert84, Bert85] </ref>. All of the models presented in Section 2.2.2 also recover rigid parts and then use this information to index into the models of objects to be recognized. <p> Other studies [Cutt81b, MacA83] found evidence that adults are capable of recogniz ing friends and the gender of a person from only the joint motions in MLD's. 16 There have been numerous psychological models to explain how the HVS is able to correctly interpret MLD's. Bertenthal <ref> [Bert85] </ref> presented three properties that must be perceived in order to perceive a moving light display as biomechanical motion.
Reference: [Besl86] <author> P. J. Besl and R. C. Jain. </author> <title> Invariant surface characteristics for 3D object recognition in range images. </title> <journal> Comp. Vision, Graphics, and Image Proc., </journal> <volume> 33 </volume> <pages> 33-80, </pages> <year> 1986. </year>
Reference-contexts: There have been numerous suggestions as to which measures are appropriate to describe a surface. Medioni and Navatia [Medi84] suggested using Gaussian curvature and the maximum principle curvature. Brady et al. [Brad85] used principle curvature directions and lines of curvature to describe surfaces. Besl and Jain <ref> [Besl86] </ref> suggested using Gaussian and mean curvature together as surface descriptors. Haralik [Hara83] et al. proposed the topographic primal sketch which labels each pixel in a range image as one of ten possible topographic labels. Gradients, Hessians, and first and second directional derivatives were used. <p> Therefore the result of the previous section will apply. Any parameterized surface can be represented locally by a function [DoCa76]. The map is called a Monge patch and f is the "height" as a function of s and t. See Figure 3.5. See <ref> [Besl86] </ref> for a complete discussion of Monge patches. For each point in an ST volume, a quadratic patch is fit using that point and the 40 X (p) s X (s, t) = (s, t, f (s, t)) Surface in R 3 temporal and spatial neighboring edge points. <p> X s X s = 1 + f 2 E s = 2f s (s; t)f ss (s; t) E t = 2f s (s; t)f st (s; t) (3:17) f s (s; t), f t (s; t), f ss (s; t) and f st (s; t) are easily computable <ref> [Besl86] </ref>. We will now show that E in a Monge patch parameterized surface is equal to E in the model parameterization of that surface. <p> recovering the motion of a point on a contour using the Monge patch parameterization can now be given: 42 * Detect points on ST surfaces in a given ST volume using a spatiotemporal edge operator, e.g., three-dimensional zero-crossings of the Laplacian * Fit a quadratic surface to the edge points <ref> [Besl86] </ref> * Compute X s , X t and E, F and G of the patch (Eqs. (3.5), (3.15) and (3.16)) [Besl86] * Compute E s and E t (Eq. (3.17)) * Compute rE on the patch (Eq. (3.13)) * Compute the direction in the tangent plane of the point that <p> points on ST surfaces in a given ST volume using a spatiotemporal edge operator, e.g., three-dimensional zero-crossings of the Laplacian * Fit a quadratic surface to the edge points <ref> [Besl86] </ref> * Compute X s , X t and E, F and G of the patch (Eqs. (3.5), (3.15) and (3.16)) [Besl86] * Compute E s and E t (Eq. (3.17)) * Compute rE on the patch (Eq. (3.13)) * Compute the direction in the tangent plane of the point that is perpendicular to the gradient direction In order to find the direction perpendicular to the gradient, the following procedure is used. <p> A Monge patch and its partial derivatives are now defined as X (r; s; t) = (r; s; t; f (r; s; t)) X s (r; s; t) = (0; 1; 0; f g (r; s; t)) The quadratic surface fit described by Besl and Jain <ref> [Besl86] </ref> can easily be extended to describe 3-surfaces and used to compute the Monge patch partial derivatives. It is no longer necessary to detect ST surfaces using an edge operator. All the temporal and spatial neighbors of a point in the ST volume are used to fit the quadratic surface. <p> A separate curve segment, centered at each point, is fit for every point making up a flow curve. This is done using a 1D version of the quadratic surface fitting procedure described by Besl and Jain <ref> [Besl86] </ref>.
Reference: [Besl88] <author> P. J. Besl and R. C. Jain. </author> <title> Segmentation through variable-order surface fitting. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 10 </volume> <pages> 167-192, </pages> <year> 1988. </year>
Reference-contexts: But this assumption is also made by gradient-based optical flow approaches since the partial derivatives of the ST volume must be computed there as well. This assumption is not a severe limitation since higher order surfaces can always be used <ref> [Besl88] </ref>. Since it is not necessary to assume neighboring points have similar velocities, we also do not assume that the optical flow is constant over an image region. Thus, our approach does not make the second assumption given above.
Reference: [Boll87] <author> R. C. Bolles, H. H. Baker, and D. H. Marimont. </author> <title> Epipolar-plane image analysis: An approach to determining structure from motion. </title> <journal> Int. J. Computer Vision, </journal> <volume> 1 </volume> <pages> 7-55, </pages> <year> 1987. </year>
Reference-contexts: To test our theory that coordinated sequences of events do not have to be computed after a complete static description of the objects has been recovered, we develop in this thesis a new, specific computational theory and implement a system that tests it. We propose, as others have <ref> [Boll87, Jain88, Liou89] </ref>, that an ST volume is the proper low-level representation of the input sequence. <p> Spatiotemporal volumes are three-dimensional structures that are built by "stacking" a sequence of densely-sampled image frames. The spatiotemporal volume is a 23 viewer-centered, three-dimensional (x-y-time) description. If we sample densely enough in time and an edge operator is applied <ref> [Boll87] </ref>, the spatiotemporal volume will contain surfaces and volumes created by the surfaces. These surfaces and volumes represent object motion swept out through time. Liou and Jain [Liou89] considered using spatiotemporal filters but, because of the drawbacks, opted for a variation of the spatiotemporal surface model just presented. <p> High-level motion is concerned with using these low-level motion descriptions in order to recover a coordinated sequence of events that can then be used to index into models of high-level motion. Using spatiotemporal surfaces, Bolles and Baker <ref> [Boll87, Bake88b] </ref> examined how to recover camera motion. Baker [Bake88a] showed how to construct these spatiotemporal surfaces. Most of this work has concentrated on recovering camera motion rather than a description of high-level motion in the scene. Badler and Smoliar [Badl79] presented a survey of representations of human movement. <p> We are concerned with recovering an interpretation of ST surfaces which can give information about motion in the scene without first recovering depth information. Baker and Bolles <ref> [Boll87, Bake89] </ref> examined ST surfaces in order to recover camera motion but objects in the scene were stationary. Jain [Jain88] described a spatiotemporal volume segmentation approach using the signs of the three principle curvatures of the ST 3-surface.
Reference: [Bolt94] <author> T. L. </author> <title> Bolton. </title> <journal> Rhythm. American J. of Psychology, </journal> <volume> 6 </volume> <pages> 145-238, 1894. </pages>
Reference-contexts: The fact that people perceive these motions as repeating demonstrates that the HVS is capable of detecting cyclic motion. The study of humans' ability to perceive a stimulus as cyclic 1 dates back to the late 19th century <ref> [Bolt94] </ref> and has continued as an active area of research [Frai78]. While the ability of the HVS to detect cyclic stimuli has been recognized [Frai78], there is little research in the area. Most of the research concentrates on the human perceptual system's ability to perceive auditory rhythm.
Reference: [Bosc67] <author> C. Bosche. </author> <title> Computer-generated random-dot images. </title> <editor> In M. Krampen and Pl Seitz, editors, </editor> <booktitle> Design and Planning: 2. Computers in design and communication, </booktitle> <pages> pages 87-91. </pages> <address> Hastings House, </address> <year> 1967. </year>
Reference-contexts: Lettvin, et al. [Lett59, Lett61], and Henn and Grusser [Henn69] developed a mo 15 tion detector such that proximate, similarly moving points are conceptually grouped. Bosche <ref> [Bosc67] </ref> produced images of random dots where some dots moved to the left and others moved to the right. This was perceived as rigid, transparent motion. Similarly, Ullman [Ullm79] presented two dotted, transparent cylinders, one inside the other, rotating in opposite directions.
Reference: [Brad85] <author> M. Brady, J. Ponce, A. Yuille, and H. Asada. </author> <title> Describing surfaces. </title> <booktitle> Proc. 2nd Int. Symp. on Robotics Research, </booktitle> <pages> pages 5-16, </pages> <year> 1985. </year>
Reference-contexts: There has been relatively little work using differential geometry for ST surface interpretation. There have been numerous suggestions as to which measures are appropriate to describe a surface. Medioni and Navatia [Medi84] suggested using Gaussian curvature and the maximum principle curvature. Brady et al. <ref> [Brad85] </ref> used principle curvature directions and lines of curvature to describe surfaces. Besl and Jain [Besl86] suggested using Gaussian and mean curvature together as surface descriptors. Haralik [Hara83] et al. proposed the topographic primal sketch which labels each pixel in a range image as one of ten possible topographic labels.
Reference: [Brus83] <author> A. R. Bruss and B. K. P. Horn. </author> <title> Passive navigation. Computer Vision, </title> <journal> Graphics, and Image Processing, </journal> <volume> 21 </volume> <pages> 3-20, </pages> <year> 1983. </year>
Reference-contexts: And high-level object and motion refers to abstract configurations and sequences such as people and walking, respectively. In most previous research, motion analysis consisted primarily of using a low-level motion description to recover rigid structure. Higher-level object recognition was then recovered from this structure. Discrete structure-from-motion <ref> [Brus83] </ref> algorithms use little more than the temporal disparity of a few points over a short interval leading to a very primitive description consisting of points' movements from one frame to the next. <p> The third phased assigned motion verbs to the movement. Phases one and two, which correspond to recovering low-level structure, were addressed by the authors but the third phase, which is high-level motion recognition, was left as future work. Most previous work on structure-from-motion <ref> [Ullm79, Webb82, Spet87, Brus83] </ref> can be classified as detecting low-level structure.
Reference: [Burt88] <author> P. J. Burt. </author> <title> Smart sensing within a pyramid vision machine. </title> <journal> Proc. IEEE, </journal> <volume> 76 </volume> <pages> 1006-1015, </pages> <year> 1988. </year>
Reference-contexts: An automated surveillance system may need to detect only that motion occurred, and track the centroid of the motion <ref> [Burt88] </ref>. In obstacle avoidance, unless the margin for error is small, the exact boundaries of objects in relative motion are not needed [Nels88]. So rather than attempt to classify pixels based on inaccurate data, exact localization of motion boundaries is sacrificed.
Reference: [Buxt83] <author> B. F. Buxton and H. Buxton. </author> <title> Monocular depth perception from optical flow by space-time signal processing. </title> <journal> Proc. Roy. Soc. London B, </journal> <volume> 218 </volume> <pages> 27-47, </pages> <year> 1983. </year>
Reference-contexts: The psychophysical work includes that of Watson, Barlow, VanSanten, Fleet and Heeger [Heeg87, Wats85, Barl65, VanS85, Adel85, Flee84a, Flee84b]. Computational models include those developed by Marr, Buxton, and Liou <ref> [Marr81, Buxt83, Buxt84, Liou89] </ref>. According to Liou and Jain [Liou89], filter models suffer from several fundamental problems. First, most filter models assume constant velocity. Second, filter models suffer from the scale problem, i.e., they have problems separating events at different scales.
Reference: [Buxt84] <author> B. F. Buxton and H. Buxton. </author> <title> Computation of optical flow from the motion of edge features in image sequences. </title> <journal> Image and Vision Computing, </journal> <volume> 2 </volume> <pages> 59-75, </pages> <year> 1984. </year>
Reference-contexts: The psychophysical work includes that of Watson, Barlow, VanSanten, Fleet and Heeger [Heeg87, Wats85, Barl65, VanS85, Adel85, Flee84a, Flee84b]. Computational models include those developed by Marr, Buxton, and Liou <ref> [Marr81, Buxt83, Buxt84, Liou89] </ref>. According to Liou and Jain [Liou89], filter models suffer from several fundamental problems. First, most filter models assume constant velocity. Second, filter models suffer from the scale problem, i.e., they have problems separating events at different scales.
Reference: [Cann86] <author> J. Canny. </author> <title> A computational approach to edge detection. </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence, </journal> <volume> 8 </volume> <pages> 679-698, </pages> <year> 1986. </year>
Reference-contexts: Points where the gray level varies smoothly, i.e., there exists spatial coherence, are not edge points. Edge operators detect edge points based on this definition, recovering edge points only where spatial coherence does not exist <ref> [Cann86] </ref>. Temporal coherence, while used in short image sequences for the computation of optical flow [Agga88], ST surface flow [Allm90a] and simplifying the correspondence problem [Grzy89], has not been used previously as a constraint for motion understanding over long image sequences.
Reference: [Carl88] <author> S. Carlsson. </author> <title> Information in the geometric structure of retinal flow field. </title> <booktitle> In Proc. 2nd Int. Conf. Computer Vision, </booktitle> <pages> pages 629-633, </pages> <year> 1988. </year>
Reference-contexts: Qualitative analysis of image motion is not new. In the past, this usually meant a qualitative study of optical flow or short image sequences <ref> [Fran90, Carl88, Verr87, Koen75] </ref>. Our work can be viewed as an extension of that problem into the temporal dimension, taking advantage of temporal motion coherence over long image sequences. This allows the recognition of higher-level motions, e.g., cyclic motion [Allm90b], which occur over long image sequences.
Reference: [Coxe61] <author> H. S. M. Coxeter. </author> <title> Introduction to Geometry. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1961. </year>
Reference-contexts: Proof Sketch Any instantaneous motion of a rigid body in three dimensions can be uniquely described by a twist <ref> [Coxe61] </ref>. A twist is described by an axis l, an angular velocity !, and a translational velocity v along l. Since a twist describes the 3D motion of any rigid object, it can be applied to an entire rigid object or a point on the rigid object.
Reference: [Cutt81a] <author> J. </author> <title> Cutting. Coding theory adapted to gate perception. </title> <journal> Journal of Experimental Psychology: Human Perception and Performance, </journal> <volume> 7 </volume> <pages> 71-87, </pages> <year> 1981. </year>
Reference-contexts: In order to prevent throwing motion from being recognized, relative spatial information should also be recovered. Cutting showed that relative spatial information is a necessary component of MLDs in order for the HVS to perceive walking <ref> [Cutt81a] </ref>. He presented subjects with a "walking" MLD, where the initial positions of the lights were altered while keeping their individual 10 absolute motions correct. None of the subjects detected the walking motion. Note that relative spatial information is not only a dynamic property but a static property.
Reference: [Cutt81b] <author> J. Cutting and D. Proffitt. </author> <title> Gait perception as an example of how we may perceive events. </title> <editor> In R. Walk and H. L. Pick, editors, </editor> <booktitle> Intersensory perception and sensory integration, </booktitle> <pages> pages 249-273. </pages> <publisher> Plenum, </publisher> <address> New York, </address> <year> 1981. </year>
Reference-contexts: Runeson [Rune81] made MLD's that had points of light not only on the joints of a person but also at the corners of a box that the person was lifting. Observers of this MLD were able to judge the weight of the box. Other studies <ref> [Cutt81b, MacA83] </ref> found evidence that adults are capable of recogniz ing friends and the gender of a person from only the joint motions in MLD's. 16 There have been numerous psychological models to explain how the HVS is able to correctly interpret MLD's.
Reference: [Cutt82] <author> J. E Cutting and D. R. Proffitt. </author> <title> The minimum principle and the perception of absolute, common, and relative motions. </title> <journal> Cognitive Psychology, </journal> <volume> 14 </volume> <pages> 211-246, </pages> <year> 1982. </year>
Reference-contexts: Rather the HVS perceives the common motion of translation and the relative motion of rotation about the center of the wheel. The absolute motion of the cycloid is generally not perceived. Even though the absolute cycloidal motion is not perceived, the following equation holds <ref> [Cutt82] </ref>: common motion + relative motion = absolute motion (6:1) We can show how this equation holds true for the case of the rolling wheel. <p> Therefore, we will examine how the HVS perceives these motions and use that as an additional constraint to solve Eq. (6.1). Psychologists have long recognized this perceptual phenomenon [Joha50, Hoch57, Wall65], but have failed to developed a complete model of how the HVS recovers relative and common motion <ref> [Joha73, Cutt82] </ref>. A common component of those models is to minimize the complexity of the relative and/or common motion [Hoch53, Hoch60, Cutt82]. Minimizing the motion can be viewed as making it as simple as possible. Unfortunately, how and what is being minimized is usually very poorly defined. <p> Psychologists have long recognized this perceptual phenomenon [Joha50, Hoch57, Wall65], but have failed to developed a complete model of how the HVS recovers relative and common motion [Joha73, Cutt82]. A common component of those models is to minimize the complexity of the relative and/or common motion <ref> [Hoch53, Hoch60, Cutt82] </ref>. Minimizing the motion can be viewed as making it as simple as possible. Unfortunately, how and what is being minimized is usually very poorly defined.
Reference: [DoCa76] <author> M. DoCarmo. </author> <title> Differential Geometry of Curves and Surfaces. </title> <publisher> Prentice-Hall, </publisher> <year> 1976. </year>
Reference-contexts: So in addition to examining the shape, for straight flow curves the slope of the curve is also required. A space curve is completely defined, up to a rigid translation and rotation, by its curvature and torsion <ref> [DoCa76] </ref>. By applying standard clustering techniques using curvature, torsion and slope, three distinct clusters are formed. Each cluster represents the motion of a single object or the background over a long period of time. <p> See <ref> [DoCa76] </ref>, for example, for more details. Subscripts on mappings indicate the partial derivative with respect to the subscript. &lt; a; b &gt; p indicates the inner product of vectors a and b evaluated at the point p. The inner product is also indicated by ab. <p> The gradient direction of a differentiable function f : S ! &lt; is a differential map rf : S ! &lt; 3 which assigns to each point p 2 S a vector rf (p) 2 T p (S) &lt; 3 . rf is given by <ref> [DoCa76] </ref>: rf = EG F 2 X s + EG F 2 X t (3:13) Since we are only concerned with the direction and not the magnitude of the gradient, the denominators can be eliminated in Eq. (3.13). Let f = E. <p> The Monge patches recovered at each point on an ST surface will be parameterized such that E in a parameterized Monge patch equals E in the model parameterizations. Therefore the result of the previous section will apply. Any parameterized surface can be represented locally by a function <ref> [DoCa76] </ref>. The map is called a Monge patch and f is the "height" as a function of s and t. See Figure 3.5. See [Besl86] for a complete discussion of Monge patches. <p> So the curvature is a measure of curving in the plane and the torsion is a measure of twisting out of the plane. Given a starting point, an orientation, and the curvature and torsion values at every point on a curve, that curve is uniquely defined <ref> [DoCa76] </ref>. Therefore, the torsion and curvature are good measures of the shape of curves [Allm90a, Mokh88, Hoff88]. By classifying the different types of motion that can occur in the scene, and the resulting projected motion, one can gain an understanding of the types of flow curves that can be produced. <p> Curvature and torsion along the path of movement of an object represents movement and is invariant to position. In fact, the curvature and torsion along the path uniquely defines the path up to a rigid transformation <ref> [DoCa76] </ref>. However, for our purposes, curvature alone will be sufficient to describe how an object 103 moves. We choose this over torsion because it is possible that some object's motion is restricted to a plane, e.g., a swinging arm. <p> This is equivalent to showing that translation and rotation of a space curve does not affect its curvature <ref> [DoCa76] </ref>.
Reference: [Enge86] <author> S. A. Engel and J. M. Rubin. </author> <title> Detecting visual motion boundaries. </title> <booktitle> Proc. IEEE Workshop on Motion, </booktitle> <pages> pages 107-111, </pages> <year> 1986. </year>
Reference-contexts: That is, while some interactions are real interactions, e.g., objects colliding, and some are apparent, e.g., one object partially occluding another, all are manifested as viewer-centered motion features in the ST cube. Rubin and Richards, and Engel <ref> [Rubi85, Enge86] </ref> proposed that there are four important motion boundaries: force impulses, starts, stops and pauses. These boundaries also describe some real, as opposed to apparent, interactions between objects. For example, when two rigid objects collide there is a force impulse applied to the objects. <p> The greater the slope of the line in the slice, the slower the point was moving in the image. The slopes of the lines in each group of four slices were examined and the normal component of velocity of the point was computed. Engel and Rubin <ref> [Enge86] </ref> presented a system that detects visual motion boundaries. This was an implementation of Rubin and Richards earlier work [Rubi85]. Rubin and Richards argued that force impulses, starts, stops and pauses, are important temporal events. Starts, stops, and pauses were detected by examining the velocity of a moving point.
Reference: [Faug90] <author> O. Faugeras. </author> <title> On the motion of 3D curves and its relationship to optical flow. </title> <booktitle> In Proc. 1st European Conf. Computer Vision, </booktitle> <pages> pages 107-117. </pages> <publisher> Springer-Verlag, </publisher> <year> 1990. </year>
Reference-contexts: A common paradigm when examining image sequences is to recover 3D structure or 3D motion immediately after optical flow is computed [Agga88]. Recently, some work has even completely skipped the flow recovery step and computed structure and motion directly <ref> [Heel90, Taal90, Faug90] </ref>. However, a great deal of information exists in the flow field and therefore our approach is to recover a qualitative description of image motion from ST surface flow. Qualitative analysis of image motion is not new.
Reference: [Flee84a] <author> D. J. </author> <title> Fleet. The early processing of spatio-temporal visual information. </title> <type> Technical report, </type> <institution> University of Toronto, </institution> <year> 1984. </year>
Reference-contexts: The psychophysical work includes that of Watson, Barlow, VanSanten, Fleet and Heeger <ref> [Heeg87, Wats85, Barl65, VanS85, Adel85, Flee84a, Flee84b] </ref>. Computational models include those developed by Marr, Buxton, and Liou [Marr81, Buxt83, Buxt84, Liou89]. According to Liou and Jain [Liou89], filter models suffer from several fundamental problems. First, most filter models assume constant velocity.
Reference: [Flee84b] <author> D. J. Fleet, A. D. Jepson, and P. E. Hallet. </author> <title> A spatio-temporal model for early visual processing. </title> <type> Technical Report RCBV-TR-84-1, </type> <institution> University of Toronto, </institution> <year> 1984. </year>
Reference-contexts: The psychophysical work includes that of Watson, Barlow, VanSanten, Fleet and Heeger <ref> [Heeg87, Wats85, Barl65, VanS85, Adel85, Flee84a, Flee84b] </ref>. Computational models include those developed by Marr, Buxton, and Liou [Marr81, Buxt83, Buxt84, Liou89]. According to Liou and Jain [Liou89], filter models suffer from several fundamental problems. First, most filter models assume constant velocity.
Reference: [Flin81] <author> B. E. Flinchbaugh and B. Chandrasekaran. </author> <title> A theory of spatio-temporal aggregation for vision. </title> <journal> Artificial Intelligence, </journal> <volume> 17 </volume> <pages> 387-407, </pages> <year> 1981. </year>
Reference-contexts: Flinchbaugh and Chandrasekaran segmented a sequence of images; but rather than stress the importance of using the motion of objects over long sequences, the instantaneous motion was used for clustering <ref> [Flin81] </ref>. Further, the common and relative motion of points was recovered prior to clustering the points.
Reference: [Frai78] <author> P. Fraisse. </author> <title> Time and rhythm perception. </title> <editor> In E. C. Carterette and M. P. Fried-man, editors, </editor> <booktitle> Handbook of Perception, Volume VIII, </booktitle> <pages> pages 203-254. </pages> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: The fact that people perceive these motions as repeating demonstrates that the HVS is capable of detecting cyclic motion. The study of humans' ability to perceive a stimulus as cyclic 1 dates back to the late 19th century [Bolt94] and has continued as an active area of research <ref> [Frai78] </ref>. While the ability of the HVS to detect cyclic stimuli has been recognized [Frai78], there is little research in the area. Most of the research concentrates on the human perceptual system's ability to perceive auditory rhythm. In this chapter we will examine the detection of cyclic stimuli in vision. <p> The study of humans' ability to perceive a stimulus as cyclic 1 dates back to the late 19th century [Bolt94] and has continued as an active area of research <ref> [Frai78] </ref>. While the ability of the HVS to detect cyclic stimuli has been recognized [Frai78], there is little research in the area. Most of the research concentrates on the human perceptual system's ability to perceive auditory rhythm. In this chapter we will examine the detection of cyclic stimuli in vision. Clearly, the perception of rhythm can be accompanied by a recognition of the rhythm.
Reference: [Fran90] <author> E. Francois and P. Bouthemy. </author> <title> The derivation of qualitative information in motion analysis. </title> <booktitle> In Proc. 1st European Conf. Computer Vision, </booktitle> <pages> pages 226-230. </pages> <publisher> Springer-Verlag, </publisher> <year> 1990. </year>
Reference-contexts: Qualitative analysis of image motion is not new. In the past, this usually meant a qualitative study of optical flow or short image sequences <ref> [Fran90, Carl88, Verr87, Koen75] </ref>. Our work can be viewed as an extension of that problem into the temporal dimension, taking advantage of temporal motion coherence over long image sequences. This allows the recognition of higher-level motions, e.g., cyclic motion [Allm90b], which occur over long image sequences.
Reference: [Fu68] <author> K. S. Fu. </author> <title> Sequential Methods in Pattern Recognition and Machine Learning. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1968. </year> <month> 139 </month>
Reference-contexts: Once a texel is known, its shape under projection can be computed. Using this information it is possible to recover the surface orientation of a surface containing the texel. Conversely, the structure of a surface can first be recovered or assumed, and then used in recovering the texels <ref> [Fu68, Fuku72, Tou74] </ref>. The issue of representation sequences in the human visual system has also been addressed in the psychology community. For example, it was once thought that the human visual system performed stereopsis at a high-level, after monocular object recognition took place.
Reference: [Fuku72] <author> K. Fukunaga. </author> <title> Introduction to Statistical Pattern Recognition. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1972. </year>
Reference-contexts: Once a texel is known, its shape under projection can be computed. Using this information it is possible to recover the surface orientation of a surface containing the texel. Conversely, the structure of a surface can first be recovered or assumed, and then used in recovering the texels <ref> [Fu68, Fuku72, Tou74] </ref>. The issue of representation sequences in the human visual system has also been addressed in the psychology community. For example, it was once thought that the human visual system performed stereopsis at a high-level, after monocular object recognition took place.
Reference: [Gabr73] <author> A. Gabrielsson. </author> <title> Adjective ratings and dimension analysis of auditory rhythm patterns. </title> <journal> Scandinavian J. of Psychology, </journal> <volume> 14 </volume> <pages> 244-260, </pages> <year> 1973. </year>
Reference-contexts: Most of the research concentrates on the human perceptual system's ability to perceive auditory rhythm. In this chapter we will examine the detection of cyclic stimuli in vision. Clearly, the perception of rhythm can be accompanied by a recognition of the rhythm. For example, when Gabrielsson <ref> [Gabr73] </ref> presented subjects with complete musical patterns and measured their perception of rhythm, the subjects could also recognize the pattern if they had heard it before. The analog of this in the visual domain was shown by Johansson [Joha73] using moving light displays (MLD). <p> Other results also indicate that the perception of intermediate descriptions is accomplished prior to recognition of the stimulus. For example, Barrow and Tenenbaum [Barr81] used photomicrographs of pollen grains to argue that the HVS can recover surface shape and orientation without recognition. When Gabrielsson <ref> [Gabr73] </ref> presented subjects with songs, they could detect rhythm within the music even without having 101 heard the song previously. These psychophysical results suggest that intermediate-level descriptions can be recovered prior to object recognition.
Reference: [Gear71] <author> C. W. Gear. </author> <title> Numerical Initial Value Problems in Ordinary Differential Equations. </title> <publisher> Prentice-Hall, </publisher> <year> 1971. </year>
Reference-contexts: How ever, since F is only defined at coordinate points and must be interpolated at intermediate pixels, the relatively simple Runge-Kutta method <ref> [Pres88, Gear71] </ref> is appropriate. More sophisticated methods where the increment in t varies depending upon the complexity of 77 F are not used since there is no reason not to use the smallest increment in t available, namely 1.
Reference: [Godd88a] <author> N. H. Goddard. </author> <title> Recognizing animal motion. </title> <booktitle> Proceedings of Image Understanding Workshop, </booktitle> <pages> pages 938-944, </pages> <year> 1988. </year>
Reference-contexts: Only recently have some researchers considered high-level motion analysis as a process that does not depend on complex object descriptions <ref> [Godd88b, Godd88a, Godd89, Goul89] </ref>. When the objects in the scene are common, complex objects such as people, it is clear what is meant by "object recognition." The object to be recognized is a person and the motion is walking, for example. <p> A significant computational issue for a complete theory of vision is whether to first identify the objects undergoing motion and then recognize the motion or, recognize the motion before identifying the objects. Goddard <ref> [Godd88a] </ref> noted that there are two ways that motion in a scene can be used. The first paradigm used motion information to first recover the rigid structure of the objects. The recovered structure was then used to index into models [Hogg83, Akit84, Leun87a, Leun87b]. <p> The first paradigm used motion information to first recover the rigid structure of the objects. The recovered structure was then used to index into models [Hogg83, Akit84, Leun87a, Leun87b]. The second paradigm used motion information to index into the models. Gould and Shah [Goul89] and Goddard <ref> [Godd88a] </ref> have asserted that it is possible to recognize an object and its high-level motion without first recovering its structure. The motion of a few representative points, rather than the low-level structure, was used to recognize the object [Goul89]. This implies high-level models that are rich in temporal information [Godd88a]. <p> Goddard <ref> [Godd88a] </ref> have asserted that it is possible to recognize an object and its high-level motion without first recovering its structure. The motion of a few representative points, rather than the low-level structure, was used to recognize the object [Goul89]. This implies high-level models that are rich in temporal information [Godd88a]. For example, the fundamental event in Goddard's system was a change of angular velocity of a joint. A high-level model consists of a sequence of angular velocity changes. <p> All of the models presented in Section 2.2.2 also recover rigid parts and then use this information to index into the models of objects to be recognized. Gould and Shah [Goul89] and Goddard <ref> [Godd88a] </ref> both have argued that it is possible to recognize an object and its high-level motion without first recovering its high-level geometric structure. The motion of a few representative points, rather than the low-level structure, e.g., rigidity, can be used to recognize an object [Goul89]. <p> The motion of a few representative points, rather than the low-level structure, e.g., rigidity, can be used to recognize an object [Goul89]. Their approaches require high-level models that are rich in temporal information <ref> [Godd88a] </ref>. The fundamental event in one of Goddard's scenarios is a change of angular velocity. A scenario becomes active when the correct sequence of these changes occur. In other words, the motions of joints is used to recognize, or index into, the high-level motion models. <p> Geometric structure has no role in the indexing. In order to index using motion information, obviously our models have to be able to be indexed using motion. As will be shown in Chapter 2, all the high-level structure models are indexed using structure rather than motion. Goddard <ref> [Godd88b, Godd88a, Godd89] </ref> 9 presented the only model to date that indexes directly from motion. But Goddard's model uses only angular velocity change of joints as the motion information for indexing. Our goal is to first recover a high-level motion description starting with an image sequence. <p> The system was given the high-level motion being performed and then predicted and verified where the body parts would appear. In this respect this work is very similar to Hogg's work [Hogg83]. Goddard <ref> [Godd88b, Godd88a, Godd89] </ref> developed a general paradigm for recognizing high-level motion. He assumed that the scene contains an articulated stick figure moving parallel to the image plane. <p> Given an intermediate level description of motion as computed here, the high level, or coordinated sequences of events needs to be recovered. How this should be performed is an interesting and useful problem to address. One approach is currently being examined by Goddard <ref> [Godd89, Godd88a] </ref>. The following subsection lays out an approach for computing additional intermediate-level motion descriptions based on the HVS. 6.1.1 Computing Relative and Common Motion Given a segmentation of an ST cube, we would like to use that segmentation to recognize high-level motions. <p> This claim has been made by only a few other researchers <ref> [Godd88a, Goul89] </ref>, and this thesis is one of the first thorough investigations of the claim.
Reference: [Godd88b] <author> N. H. Goddard. </author> <title> Representing and recognizing event sequences. </title> <booktitle> Proc. CMU Summer Connectionist Workshop, </booktitle> <year> 1988. </year>
Reference-contexts: Only recently have some researchers considered high-level motion analysis as a process that does not depend on complex object descriptions <ref> [Godd88b, Godd88a, Godd89, Goul89] </ref>. When the objects in the scene are common, complex objects such as people, it is clear what is meant by "object recognition." The object to be recognized is a person and the motion is walking, for example. <p> Geometric structure has no role in the indexing. In order to index using motion information, obviously our models have to be able to be indexed using motion. As will be shown in Chapter 2, all the high-level structure models are indexed using structure rather than motion. Goddard <ref> [Godd88b, Godd88a, Godd89] </ref> 9 presented the only model to date that indexes directly from motion. But Goddard's model uses only angular velocity change of joints as the motion information for indexing. Our goal is to first recover a high-level motion description starting with an image sequence. <p> The system was given the high-level motion being performed and then predicted and verified where the body parts would appear. In this respect this work is very similar to Hogg's work [Hogg83]. Goddard <ref> [Godd88b, Godd88a, Godd89] </ref> developed a general paradigm for recognizing high-level motion. He assumed that the scene contains an articulated stick figure moving parallel to the image plane.
Reference: [Godd89] <author> N. H. Goddard. </author> <title> The interpretation of visual motion: Recognizing moving light displays. </title> <booktitle> Proc. IEEE Workshop on Motion, </booktitle> <pages> pages 212-220, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: Only recently have some researchers considered high-level motion analysis as a process that does not depend on complex object descriptions <ref> [Godd88b, Godd88a, Godd89, Goul89] </ref>. When the objects in the scene are common, complex objects such as people, it is clear what is meant by "object recognition." The object to be recognized is a person and the motion is walking, for example. <p> Geometric structure has no role in the indexing. In order to index using motion information, obviously our models have to be able to be indexed using motion. As will be shown in Chapter 2, all the high-level structure models are indexed using structure rather than motion. Goddard <ref> [Godd88b, Godd88a, Godd89] </ref> 9 presented the only model to date that indexes directly from motion. But Goddard's model uses only angular velocity change of joints as the motion information for indexing. Our goal is to first recover a high-level motion description starting with an image sequence. <p> Work on high-level motion is concerned with using lower-level motion descriptions in order to recover a coordinated sequence of events that can then be used to index into models of high-level motion <ref> [Godd89] </ref>. high-level motion fit into this characterization. The top node in Figure 2.1 represents all computational vision work. This work can be divided into that which deals with a single image, Static, and multiple images, Dynamic. <p> The system was given the high-level motion being performed and then predicted and verified where the body parts would appear. In this respect this work is very similar to Hogg's work [Hogg83]. Goddard <ref> [Godd88b, Godd88a, Godd89] </ref> developed a general paradigm for recognizing high-level motion. He assumed that the scene contains an articulated stick figure moving parallel to the image plane. <p> In this degenerate case, a point displays cyclic motion with an arbitrary period. Similarly, a point moving in a circle with constant speed displays cyclic motion with an arbitrary period since the curvature is constant. This definition fails to define a cycle as one revolution of the circle. Goddard <ref> [Godd89] </ref> used change in angular velocity as a primitive to detect movements such as walking. A point moving in a circle with constant speed does not change its angular velocity. So Goddard's primitive would also fail to define the period of cyclic motion as one revolution. <p> Given an intermediate level description of motion as computed here, the high level, or coordinated sequences of events needs to be recovered. How this should be performed is an interesting and useful problem to address. One approach is currently being examined by Goddard <ref> [Godd89, Godd88a] </ref>. The following subsection lays out an approach for computing additional intermediate-level motion descriptions based on the HVS. 6.1.1 Computing Relative and Common Motion Given a segmentation of an ST cube, we would like to use that segmentation to recognize high-level motions.
Reference: [Gold88] <author> D. B. Goldgof, H. Lee, and T. S. Huang. </author> <title> Motion analysis of nonrigid surfaces. </title> <booktitle> Proc. Computer Vision and Pattern Recognition Conf., </booktitle> <pages> pages 375-380, </pages> <year> 1988. </year>
Reference-contexts: Similar to lines of curvature, the directional derivative of an ST surface gives incomplete information about a contour and its motion except in very restrictive cases. Goldgof, Lee and Huang <ref> [Gold88] </ref> used a curvature-based approach to analyze the motion of nonrigid surfaces. But their approach works with the actual 3D structure of the surfaces, i.e., the depth of each point of the surface has to be recovered a priori.
Reference: [Goul89] <author> K. Gould and M. Shah. </author> <title> The trajectory primal sketch: A multi-scale scheme for representing motion characteristics. </title> <booktitle> Proc. Computer Vision and Pattern Recognition Conf., </booktitle> <pages> pages 79-85, </pages> <year> 1989. </year>
Reference-contexts: Only recently have some researchers considered high-level motion analysis as a process that does not depend on complex object descriptions <ref> [Godd88b, Godd88a, Godd89, Goul89] </ref>. When the objects in the scene are common, complex objects such as people, it is clear what is meant by "object recognition." The object to be recognized is a person and the motion is walking, for example. <p> The first paradigm used motion information to first recover the rigid structure of the objects. The recovered structure was then used to index into models [Hogg83, Akit84, Leun87a, Leun87b]. The second paradigm used motion information to index into the models. Gould and Shah <ref> [Goul89] </ref> and Goddard [Godd88a] have asserted that it is possible to recognize an object and its high-level motion without first recovering its structure. The motion of a few representative points, rather than the low-level structure, was used to recognize the object [Goul89]. <p> Gould and Shah <ref> [Goul89] </ref> and Goddard [Godd88a] have asserted that it is possible to recognize an object and its high-level motion without first recovering its structure. The motion of a few representative points, rather than the low-level structure, was used to recognize the object [Goul89]. This implies high-level models that are rich in temporal information [Godd88a]. For example, the fundamental event in Goddard's system was a change of angular velocity of a joint. A high-level model consists of a sequence of angular velocity changes. <p> All of the models presented in Section 2.2.2 also recover rigid parts and then use this information to index into the models of objects to be recognized. Gould and Shah <ref> [Goul89] </ref> and Goddard [Godd88a] both have argued that it is possible to recognize an object and its high-level motion without first recovering its high-level geometric structure. The motion of a few representative points, rather than the low-level structure, e.g., rigidity, can be used to recognize an object [Goul89]. <p> Gould and Shah <ref> [Goul89] </ref> and Goddard [Godd88a] both have argued that it is possible to recognize an object and its high-level motion without first recovering its high-level geometric structure. The motion of a few representative points, rather than the low-level structure, e.g., rigidity, can be used to recognize an object [Goul89]. Their approaches require high-level models that are rich in temporal information [Godd88a]. The fundamental event in one of Goddard's scenarios is a change of angular velocity. A scenario becomes active when the correct sequence of these changes occur. <p> Starts, stops, and pauses were detected by examining the velocity of a moving point. Assuming mass remains constant, a force impulse, or force discontinuity, was detected by examining the acceleration of a moving point. Gould and Shah <ref> [Goul89] </ref> used motion information to categorize the motion of objects. They defined a trajectory primal sketch as a low-level representation for the movement of points. Using scale space, important events were detected and primitive movements 24 or trajectories, including translation, rotation, projectile and cycloid were described. <p> A point moving in a circle with constant speed does not change its angular velocity. So Goddard's primitive would also fail to define the period of cyclic motion as one revolution. Similarly, the Trajectory Primal Sketch of Gould and Shah <ref> [Goul89] </ref> does not directly represent position information so it would also fail to define the period as one revolution. A definition of cyclic motion that is sensitive to such cases would only obscure the type of motion of the point. <p> This claim has been made by only a few other researchers <ref> [Godd88a, Goul89] </ref>, and this thesis is one of the first thorough investigations of the claim.
Reference: [Grzy89] <author> N. M. Grzywacz, J. A. Smith, and A. L. Yuille. </author> <title> A common theoretical framework for visual motion's spatial and temporal coherence. </title> <booktitle> In Proc. Workshop Visual Motion, </booktitle> <pages> pages 148-155, </pages> <year> 1989. </year>
Reference-contexts: Edge operators detect edge points based on this definition, recovering edge points only where spatial coherence does not exist [Cann86]. Temporal coherence, while used in short image sequences for the computation of optical flow [Agga88], ST surface flow [Allm90a] and simplifying the correspondence problem <ref> [Grzy89] </ref>, has not been used previously as a constraint for motion understanding over long image sequences. In addition to temporal and spatial coherence of intensity values, there is temporal and spatial coherence of motion.
Reference: [Hans78] <author> A. R. Hanson and E. M. Riseman. </author> <title> Visions: A computer system for interpreting scenes. </title> <editor> In Allen R. Hanson and Edward M. Riseman, editors, </editor> <booktitle> Computer Vision Systems, </booktitle> <pages> pages 303-333. </pages> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: Introduction Recovering a hierarchical motion description of an image sequence is one way to recognize objects undergoing motion. With many hierarchical representations, higher levels are recovered from lower levels. For example, in VISIONS <ref> [Hans78] </ref> the hierarchy is built in the following order with each level recovered from the previous level: vertices, segments, regions, surfaces, volumes, objects, and schemas.
Reference: [Hara83] <author> R. M. Haralik, L. T. Watson, and T. J. Laffey. </author> <title> The topographic primal sketch. </title> <journal> Int. J. Robotics Research, </journal> <volume> 2 </volume> <pages> 50-72, </pages> <year> 1983. </year>
Reference-contexts: Medioni and Navatia [Medi84] suggested using Gaussian curvature and the maximum principle curvature. Brady et al. [Brad85] used principle curvature directions and lines of curvature to describe surfaces. Besl and Jain [Besl86] suggested using Gaussian and mean curvature together as surface descriptors. Haralik <ref> [Hara83] </ref> et al. proposed the topographic primal sketch which labels each pixel in a range image as one of ten possible topographic labels. Gradients, Hessians, and first and second directional derivatives were used.
Reference: [Hart75] <author> J. A. Hartigan. </author> <title> Clustering Algorithms. </title> <publisher> Wiley, </publisher> <year> 1975. </year>
Reference-contexts: In many applications it is reasonable to expect that this information is supplied. However, in our case this would amount to specifying how many objects are moving in the scene. Clearly this is an undesirable assumption. Therefore we will use heuristic techniques for automatically determining how many clusters exist <ref> [Hart75, Rome84] </ref>. A clustering algorithm needs a function that measures the difference between two items. <p> If this difference is greater than the difference of the flow curve and another cluster's mean, the flow curve is moved into the other cluster. This is called the K-Means clustering method <ref> [Hart75, Rome84] </ref>. Note that the number of clusters does not change, but it allows flow curves to move between existing clusters.
Reference: [Heeg87] <author> D. J. Heeger. </author> <title> Optical flow from spatiotemporal filters. </title> <booktitle> Proc. First Int. Conf. on Computer Vision, </booktitle> <pages> pages 181-190, </pages> <year> 1987. </year>
Reference-contexts: The psychophysical work includes that of Watson, Barlow, VanSanten, Fleet and Heeger <ref> [Heeg87, Wats85, Barl65, VanS85, Adel85, Flee84a, Flee84b] </ref>. Computational models include those developed by Marr, Buxton, and Liou [Marr81, Buxt83, Buxt84, Liou89]. According to Liou and Jain [Liou89], filter models suffer from several fundamental problems. First, most filter models assume constant velocity.
Reference: [Heel90] <author> J. Heel. </author> <title> Direct estimation of structure and motion from multiple frames. </title> <journal> A.I. </journal> <volume> Memo 1190, </volume> <publisher> MIT, </publisher> <year> 1990. </year>
Reference-contexts: A common paradigm when examining image sequences is to recover 3D structure or 3D motion immediately after optical flow is computed [Agga88]. Recently, some work has even completely skipped the flow recovery step and computed structure and motion directly <ref> [Heel90, Taal90, Faug90] </ref>. However, a great deal of information exists in the flow field and therefore our approach is to recover a qualitative description of image motion from ST surface flow. Qualitative analysis of image motion is not new.
Reference: [Henn69] <author> V. Henn and O. J. Grusser. </author> <title> The summation of excitation in the receptive fields of movement-sensitive neurons of the frog's retina. </title> <journal> Vision Research, </journal> <volume> 9 </volume> <pages> 57-69, </pages> <year> 1969. </year>
Reference-contexts: After briefly describing experiments using MLD-like stimuli of low-level motion, we will review the MLD literature and show that an intermediate-level motion representation, namely relative temporal information, is lacking. Lettvin, et al. [Lett59, Lett61], and Henn and Grusser <ref> [Henn69] </ref> developed a mo 15 tion detector such that proximate, similarly moving points are conceptually grouped. Bosche [Bosc67] produced images of random dots where some dots moved to the left and others moved to the right. This was perceived as rigid, transparent motion.
Reference: [Hoch53] <author> J. Hochberg and E. McAlister. </author> <title> A quantitative approach to figural goodness. </title> <journal> Journal of Experimental Psychology, </journal> <volume> 46 </volume> <pages> 361-364, </pages> <year> 1953. </year> <month> 140 </month>
Reference-contexts: Psychologists have long recognized this perceptual phenomenon [Joha50, Hoch57, Wall65], but have failed to developed a complete model of how the HVS recovers relative and common motion [Joha73, Cutt82]. A common component of those models is to minimize the complexity of the relative and/or common motion <ref> [Hoch53, Hoch60, Cutt82] </ref>. Minimizing the motion can be viewed as making it as simple as possible. Unfortunately, how and what is being minimized is usually very poorly defined.
Reference: [Hoch57] <author> J. Hochberg. </author> <title> Effects of the gestalt revolution: The Cornell symposium on perception. </title> <journal> Psychological Review, </journal> <volume> 64 </volume> <pages> 73-84, </pages> <year> 1957. </year>
Reference-contexts: There are no "correct" solutions to Eq. (6.1), only solutions that are similar to the HVS's perception. Therefore, we will examine how the HVS perceives these motions and use that as an additional constraint to solve Eq. (6.1). Psychologists have long recognized this perceptual phenomenon <ref> [Joha50, Hoch57, Wall65] </ref>, but have failed to developed a complete model of how the HVS recovers relative and common motion [Joha73, Cutt82]. A common component of those models is to minimize the complexity of the relative and/or common motion [Hoch53, Hoch60, Cutt82].
Reference: [Hoch60] <author> J. Hochberg and V. Brooks. </author> <title> The psychophysics of form: Reversible perspective drawing of spatial objects. </title> <journal> American Journal of Psychology, </journal> <volume> 73 </volume> <pages> 337-354, </pages> <year> 1960. </year>
Reference-contexts: Psychologists have long recognized this perceptual phenomenon [Joha50, Hoch57, Wall65], but have failed to developed a complete model of how the HVS recovers relative and common motion [Joha73, Cutt82]. A common component of those models is to minimize the complexity of the relative and/or common motion <ref> [Hoch53, Hoch60, Cutt82] </ref>. Minimizing the motion can be viewed as making it as simple as possible. Unfortunately, how and what is being minimized is usually very poorly defined.
Reference: [Hoff88] <author> D. D. Hoffman and W. Richards. </author> <title> Representing smooth plane curves for recognition: Implications for figure-ground reversal. </title> <editor> In W. Richards, editor, </editor> <booktitle> Natural Computation, </booktitle> <pages> pages 76-82. </pages> <publisher> MIT Press, </publisher> <year> 1988. </year>
Reference-contexts: Given a starting point, an orientation, and the curvature and torsion values at every point on a curve, that curve is uniquely defined [DoCa76]. Therefore, the torsion and curvature are good measures of the shape of curves <ref> [Allm90a, Mokh88, Hoff88] </ref>. By classifying the different types of motion that can occur in the scene, and the resulting projected motion, one can gain an understanding of the types of flow curves that can be produced.
Reference: [Hogg83] <author> D. Hogg. </author> <title> Model-based vision: A program to see a walking person. </title> <booktitle> Image and Computer Vision Computing, </booktitle> <volume> 1 </volume> <pages> 5-20, </pages> <year> 1983. </year>
Reference-contexts: High-level motion analysis, i.e., recognizing a coordinated sequence of events such as walking and throwing, has been formulated previously as a process that follows high-level object 2 recognition <ref> [Hogg83, Akit84, Leun87a, Leun87b] </ref>. Only recently have some researchers considered high-level motion analysis as a process that does not depend on complex object descriptions [Godd88b, Godd88a, Godd89, Goul89]. <p> Goddard [Godd88a] noted that there are two ways that motion in a scene can be used. The first paradigm used motion information to first recover the rigid structure of the objects. The recovered structure was then used to index into models <ref> [Hogg83, Akit84, Leun87a, Leun87b] </ref>. The second paradigm used motion information to index into the models. Gould and Shah [Goul89] and Goddard [Godd88a] have asserted that it is possible to recognize an object and its high-level motion without first recovering its structure. <p> That is, structure-from-motion work is usually concerned with finding the structure of rigid parts and not the composition of rigid parts that forms more complex objects. 2.2.2 High-Level Structure High-level structure analysis is concerned with how to construct high-level structures of non-rigid objects. The most common high-level structure representations <ref> [Marr80, Hogg83] </ref> have complex objects stored as a hierarchy of rigid parts with constraints between the parts. Many representations also store how the parts move while performing some high-level motion such as walking [Marr80, Hogg83]. <p> The most common high-level structure representations <ref> [Marr80, Hogg83] </ref> have complex objects stored as a hierarchy of rigid parts with constraints between the parts. Many representations also store how the parts move while performing some high-level motion such as walking [Marr80, Hogg83]. Marr and Vaina [Marr80] extended Marr and Nishihara's model [Marr78] so that the representation includes how rigid parts move in relation to each other. The new, augmented representation is object-centered as in Marr and Nishihara's original model [Marr78]. <p> In the SMS representation a behavior is described as a sequence of motion segments. A motion segment of an object M consists of the interval between two adjacent, overall-rest states of M. Hogg <ref> [Hogg83] </ref> used a model similar to Marr and Vaina's [Marr80] to recognize a walking person. Hogg's model of a human is hierarchical with constraints specifying how the parts can move relative to one another. <p> Each key frame represented a different phase of body posture from the point of view of occlusion. In other words, a key frame was stored for every topologically-distinct view of the body in motion. As in Hogg's work <ref> [Hogg83] </ref>, Akita matched one frame of the sequence to the human model and then used the key frames to predict where occlusion would occur in other frames. All the computational models presented in this section are more spatially based than temporally based. <p> This is demonstrated by the fact that a model is matched to one frame of a sequence <ref> [Hogg83, Akit84] </ref> before the motion of the object is used. <p> The system was given the high-level motion being performed and then predicted and verified where the body parts would appear. In this respect this work is very similar to Hogg's work <ref> [Hogg83] </ref>. Goddard [Godd88b, Godd88a, Godd89] developed a general paradigm for recognizing high-level motion. He assumed that the scene contains an articulated stick figure moving parallel to the image plane.
Reference: [Horn81] <author> B. K. P. Horn and B. G. Schunck. </author> <title> Determining optical flow. </title> <journal> Artificial Intelligence, </journal> <volume> 17 </volume> <pages> 185-203, </pages> <year> 1981. </year>
Reference-contexts: Two other examples of the representation sequence issue are illustrated by approaches used for optical flow and texture analysis. Optical flow can be viewed as a correspondence problem and can be performed using two temporally-adjacent frames <ref> [Horn81] </ref>. When computed in this manner, the sequence of representations consists of the original images followed by an optical flow description. Alternatively, it is possible to compute optical flow after tokens, such as corners, have been identified based on the matching of these 1 tokens. <p> Lee and Anderson [Lee88b, Ande85] constructed spatial pyramids from difference images in order to track objects. But this spatial pyramid must be rebuilt for every pair of frames. Many optical flow <ref> [Horn81] </ref> and structure-from-motion [Ullm79] algorithms are similar in that they also use only a few frames to accomplish their goal. Recently, there has been a trend toward using more than a few frames to examine an image sequence [Jain88, Aloi88].
Reference: [Jain79a] <author> R. Jain, W. N. Martin, and J. K. Aggarwal. </author> <title> Segmentation through the detection of changes due to motion. Computer Vision, </title> <journal> Graphics and Image Processing, </journal> <volume> 11 </volume> <pages> 13-34, </pages> <year> 1979. </year>
Reference-contexts: Most previous work has focused on low-level motion analysis. Frame differencing of temporally-adjacent frames is one common method of low-level motion analysis <ref> [Jain79b, Jain79a, Jain81, Jain83, Jaya83, Ande85, Lee88a] </ref>. Recovering orientation of a surface or the position within the scene of points on an object is commonly referred to as structure-from-motion. Structure-from-motion has been performed using correspondences between points [Ullm79] as well as without using point correspondences [Terz88]. <p> They perform poorly if motion is not in a steady state. Much of the temporal examination of image sequences consists of looking at only a few frames at a time. Frame differencing has been a popular method of temporal examination of two frames <ref> [Jain79b, Jain79a, Jain81, Jain83, Lee88b, Jaya83, Ande85] </ref>. With this method, the difference of two temporally-adjacent frames is calculated and where the difference is nonzero, there has been motion. Lee and Anderson [Lee88b, Ande85] constructed spatial pyramids from difference images in order to track objects.
Reference: [Jain79b] <author> R. Jain and H. H. Nagel. </author> <title> On the analysis of accumulative difference pictures from image sequences of real world scenes. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 1 </volume> <pages> 206-214, </pages> <year> 1979. </year>
Reference-contexts: Most previous work has focused on low-level motion analysis. Frame differencing of temporally-adjacent frames is one common method of low-level motion analysis <ref> [Jain79b, Jain79a, Jain81, Jain83, Jaya83, Ande85, Lee88a] </ref>. Recovering orientation of a surface or the position within the scene of points on an object is commonly referred to as structure-from-motion. Structure-from-motion has been performed using correspondences between points [Ullm79] as well as without using point correspondences [Terz88]. <p> They perform poorly if motion is not in a steady state. Much of the temporal examination of image sequences consists of looking at only a few frames at a time. Frame differencing has been a popular method of temporal examination of two frames <ref> [Jain79b, Jain79a, Jain81, Jain83, Lee88b, Jaya83, Ande85] </ref>. With this method, the difference of two temporally-adjacent frames is calculated and where the difference is nonzero, there has been motion. Lee and Anderson [Lee88b, Ande85] constructed spatial pyramids from difference images in order to track objects.
Reference: [Jain81] <author> R. Jain. </author> <title> Dynamic scene analysis using pixel-based processes. </title> <journal> IEEE Computer, </journal> <volume> 14 </volume> <pages> 12-19, </pages> <year> 1981. </year>
Reference-contexts: Most previous work has focused on low-level motion analysis. Frame differencing of temporally-adjacent frames is one common method of low-level motion analysis <ref> [Jain79b, Jain79a, Jain81, Jain83, Jaya83, Ande85, Lee88a] </ref>. Recovering orientation of a surface or the position within the scene of points on an object is commonly referred to as structure-from-motion. Structure-from-motion has been performed using correspondences between points [Ullm79] as well as without using point correspondences [Terz88]. <p> They perform poorly if motion is not in a steady state. Much of the temporal examination of image sequences consists of looking at only a few frames at a time. Frame differencing has been a popular method of temporal examination of two frames <ref> [Jain79b, Jain79a, Jain81, Jain83, Lee88b, Jaya83, Ande85] </ref>. With this method, the difference of two temporally-adjacent frames is calculated and where the difference is nonzero, there has been motion. Lee and Anderson [Lee88b, Ande85] constructed spatial pyramids from difference images in order to track objects.
Reference: [Jain83] <author> R. Jain. </author> <title> Extraction of motion information from peripheral processes. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 3 </volume> <pages> 489-503, </pages> <year> 1983. </year>
Reference-contexts: Most previous work has focused on low-level motion analysis. Frame differencing of temporally-adjacent frames is one common method of low-level motion analysis <ref> [Jain79b, Jain79a, Jain81, Jain83, Jaya83, Ande85, Lee88a] </ref>. Recovering orientation of a surface or the position within the scene of points on an object is commonly referred to as structure-from-motion. Structure-from-motion has been performed using correspondences between points [Ullm79] as well as without using point correspondences [Terz88]. <p> They perform poorly if motion is not in a steady state. Much of the temporal examination of image sequences consists of looking at only a few frames at a time. Frame differencing has been a popular method of temporal examination of two frames <ref> [Jain79b, Jain79a, Jain81, Jain83, Lee88b, Jaya83, Ande85] </ref>. With this method, the difference of two temporally-adjacent frames is calculated and where the difference is nonzero, there has been motion. Lee and Anderson [Lee88b, Ande85] constructed spatial pyramids from difference images in order to track objects.
Reference: [Jain88] <author> R. Jain. </author> <title> Dynamic vision. </title> <booktitle> In Proc. 9th Int. Conf. Pattern Recognition, </booktitle> <pages> pages 226-235, </pages> <year> 1988. </year>
Reference-contexts: To test our theory that coordinated sequences of events do not have to be computed after a complete static description of the objects has been recovered, we develop in this thesis a new, specific computational theory and implement a system that tests it. We propose, as others have <ref> [Boll87, Jain88, Liou89] </ref>, that an ST volume is the proper low-level representation of the input sequence. <p> An ST cube, constructed by stacking a sequence of temporally-close images to 6 gether, has temporal motion coherence (as well as temporal gray level coherence) and is therefore an ideal structure for studying image sequences using temporal coherence <ref> [Allm90a, Jain88, Bake88b] </ref>. A first step in understanding the motion in an ST cube is to determine the instantaneous motion of each point in the cube, called the ST surface flow (see Chapter 3). <p> Many optical flow [Horn81] and structure-from-motion [Ullm79] algorithms are similar in that they also use only a few frames to accomplish their goal. Recently, there has been a trend toward using more than a few frames to examine an image sequence <ref> [Jain88, Aloi88] </ref>. Jain notes: "Most techniques try to do too much using too few frames. <p> We are concerned with recovering an interpretation of ST surfaces which can give information about motion in the scene without first recovering depth information. Baker and Bolles [Boll87, Bake89] examined ST surfaces in order to recover camera motion but objects in the scene were stationary. Jain <ref> [Jain88] </ref> described a spatiotemporal volume segmentation approach using the signs of the three principle curvatures of the ST 3-surface. While using curvature values is suitable for segmenting an ST volume, it does not give complete information about the structure, i.e., direction of motion, of the ST surfaces in the volume. <p> As stated earlier, our approach does not deal with these areas, but rather uses areas where the flow is well defined. Jain segmented an ST cube by examining the signs of three principle curvatures of an ST 3-surface <ref> [Jain88] </ref>. These signs were determined by the type of the motion that generated the surface, translational for example. As such, only the type of motion was used and not the direction of motion, as we do when using ST surface flow. <p> Recently, there has been a trend toward using more than a few frames to analyze an image sequence <ref> [Jain88] </ref>. Cyclic motion description requires long sequences of frames since many frames are required before a motion repeats. These considerations suggest a low-level representation that coherently represents many frames, i.e., an ST cube.
Reference: [Jaya83] <author> S. N. Jayaramamurthy and R. Jain. </author> <title> An approach to the segmentation of textured dynamic scenes. Computer Vision, </title> <journal> Graphics and Image Processing, </journal> <volume> 21 </volume> <pages> 239-261, </pages> <year> 1983. </year>
Reference-contexts: Most previous work has focused on low-level motion analysis. Frame differencing of temporally-adjacent frames is one common method of low-level motion analysis <ref> [Jain79b, Jain79a, Jain81, Jain83, Jaya83, Ande85, Lee88a] </ref>. Recovering orientation of a surface or the position within the scene of points on an object is commonly referred to as structure-from-motion. Structure-from-motion has been performed using correspondences between points [Ullm79] as well as without using point correspondences [Terz88]. <p> They perform poorly if motion is not in a steady state. Much of the temporal examination of image sequences consists of looking at only a few frames at a time. Frame differencing has been a popular method of temporal examination of two frames <ref> [Jain79b, Jain79a, Jain81, Jain83, Lee88b, Jaya83, Ande85] </ref>. With this method, the difference of two temporally-adjacent frames is calculated and where the difference is nonzero, there has been motion. Lee and Anderson [Lee88b, Ande85] constructed spatial pyramids from difference images in order to track objects.
Reference: [Joha50] <author> G. Johansson. </author> <title> Configurations in event perception. </title> <publisher> Almqvist & Wiksell, </publisher> <year> 1950. </year>
Reference-contexts: There are no "correct" solutions to Eq. (6.1), only solutions that are similar to the HVS's perception. Therefore, we will examine how the HVS perceives these motions and use that as an additional constraint to solve Eq. (6.1). Psychologists have long recognized this perceptual phenomenon <ref> [Joha50, Hoch57, Wall65] </ref>, but have failed to developed a complete model of how the HVS recovers relative and common motion [Joha73, Cutt82]. A common component of those models is to minimize the complexity of the relative and/or common motion [Hoch53, Hoch60, Cutt82].
Reference: [Joha73] <author> G. Johansson. </author> <title> Visual perception of biological motion and a model for its analysis. </title> <journal> Perception and Psychophysics, </journal> <volume> 14 </volume> <pages> 201-211, </pages> <year> 1973. </year>
Reference-contexts: Structure had no role in the indexing. We propose a variation of the second paradigm. Goddard used only low-level motion information to index into high-level motion models. We propose that intermediate-level and high-level motion descriptions should be recovered and used to index into the models. Johansson <ref> [Joha73] </ref> showed that people can immediately recognize moving light displays of simple motions such as swinging pendulums and rolling wheels, as well more complex common motions such as walking and cycling. <p> As will be pointed out in Chapter 2, psychological models of the human visual system have shown that it is feasible to first recover the rigid parts of an object using motion and then use this rigid structure information to recognize the object <ref> [Joha73, Joha76, Bert84, Bert85] </ref>. All of the models presented in Section 2.2.2 also recover rigid parts and then use this information to index into the models of objects to be recognized. <p> This results because most MLD experiments are of complex, real world objects, people for example. Chapter 1 showed why relative temporal information is important in a model of motion-based recognition. However, it will be clear that none of the psychological models contain an explicit representation of time. Johansson <ref> [Joha73] </ref> used MLDs consisting of ten lights to examine human performance at interpreting articulated objects. MLD's were used so that only point motion information at the joints was available. In the first demonstration adults watched an MLD of human walking, as viewed from the side of the walker. <p> At the causal (kinetic) relations level, observers see motion as having a causal role. For example, motions in one part are seen as causing certain motion in other parts. At the final level, the identification of human form, observers perceive a human walking. Johansson <ref> [Joha73] </ref> proposed three principles that may be used in order to perceive correctly MLDs of biomechanical motion. <p> So rather than attempt to classify pixels based on inaccurate data, exact localization of motion boundaries is sacrificed. Related work in psychology supports the view that localization of occlusion boundaries is not required in order to recover a useful description of motion. Johansson <ref> [Joha73] </ref> attached lights to the joints of a person and then filmed the person undergoing motion, such as walking, in the dark. Only the motion of the lights was visible in these moving light displays (MLD). When people observed these MLD's they almost immediately recognize the motion. <p> With these representative flow curves, motion recognition can be performed without prior knowledge of the objects undergoing motion. Johansson's results with MLD's suggests that this is possible and that using flow curves is a promising approach <ref> [Joha73] </ref>. The next chapter will present an intermediate-level motion problem, cyclic motion detection, and show how ST flow curves can be used to detect cyclic motion without prior recognition of the objects undergoing the motion. <p> For example, when Gabrielsson [Gabr73] presented subjects with complete musical patterns and measured their perception of rhythm, the subjects could also recognize the pattern if they had heard it before. The analog of this in the visual domain was shown by Johansson <ref> [Joha73] </ref> using moving light displays (MLD). While the recognition of the rhythms was not central to those studies, one can extrapolate from the results and conclude that the perception of rhythm is accompanied by recognition. This chapter addresses the detection of cyclic motion within the context of motion recognition. <p> There has been considerable work examining the HVS's ability to recognize movement 1 The psychology community refers to a repeating stimulus as "rhythmic" rather than "cyclic". 100 that contains cyclic motion, but the detection of the cycles was not the focus of the studies <ref> [Joha73] </ref>. Johansson's MLDs contained cyclic motion in the form of human walking, running, cycling and dancing. When subjects were presented with an MLD of walking, they always recognized the motion after the first one or two steps. <p> Therefore, we will examine how the HVS perceives these motions and use that as an additional constraint to solve Eq. (6.1). Psychologists have long recognized this perceptual phenomenon [Joha50, Hoch57, Wall65], but have failed to developed a complete model of how the HVS recovers relative and common motion <ref> [Joha73, Cutt82] </ref>. A common component of those models is to minimize the complexity of the relative and/or common motion [Hoch53, Hoch60, Cutt82]. Minimizing the motion can be viewed as making it as simple as possible. Unfortunately, how and what is being minimized is usually very poorly defined.
Reference: [Joha76] <author> G. Johansson. </author> <title> Spatio-temporal differentiation and integration in visual motion perception. </title> <journal> Psychological Research, </journal> <volume> 38 </volume> <pages> 379-393, </pages> <year> 1976. </year>
Reference-contexts: As will be pointed out in Chapter 2, psychological models of the human visual system have shown that it is feasible to first recover the rigid parts of an object using motion and then use this rigid structure information to recognize the object <ref> [Joha73, Joha76, Bert84, Bert85] </ref>. All of the models presented in Section 2.2.2 also recover rigid parts and then use this information to index into the models of objects to be recognized. <p> The three principles state: 1) the HVS groups moving points together, 2) moving points that keep the same relative positions are grouped into rigid parts, and 3) motions of multiple rigid parts are organized into a single common motion, such as walking, if possible. Later, Johansson <ref> [Joha76] </ref> presented an additional theory using a hierarchy of coordinate systems to explain this human performance. None of these psychological theories contains an explicit temporal description of motion.
Reference: [Jule71] <author> B. Julesz. </author> <title> Foundations of Cyclopean Perception. </title> <publisher> The University of Chicago Press, </publisher> <address> Chicago, </address> <year> 1971. </year> <month> 141 </month>
Reference-contexts: The issue of representation sequences in the human visual system has also been addressed in the psychology community. For example, it was once thought that the human visual system performed stereopsis at a high-level, after monocular object recognition took place. Julesz <ref> [Jule71] </ref> concluded from his studies with random dot stereograms that stereopsis can take place before monocular object recognition. When the human visual system views a single random dot stereogram monocularly, no structure is found in the image.
Reference: [Kana81] <author> T. Kanade. </author> <title> Recovery of the three-dimensional shape of an object from a single view. </title> <journal> Artificial Intelligence, </journal> <volume> 17 </volume> <pages> 409-460, </pages> <year> 1981. </year>
Reference-contexts: Each cluster represents a temporally-coherent structure in the ST cube, i.e., structures that result from an object or surface in the image undergoing similar image motion over long intervals of time. In order to recover 3D shape from a single image, Kanade used the heuristic of nonaccidental regularities <ref> [Kana81] </ref>: "Regularities observable in the picture are not by accident, but are some projection of real regularities." to recover 3D shape from a single image." Similarly, we assume that similar image motion is the projection of similar scene motion.
Reference: [Koen75] <author> J. J. Koenderink and J. J. Van Doorn. </author> <title> Invariant properties of the motion parallax field due to the movement of rigid bodies relative to an observer. </title> <journal> Optica Acta, </journal> <volume> 22 </volume> <pages> 773-791, </pages> <year> 1975. </year>
Reference-contexts: Qualitative analysis of image motion is not new. In the past, this usually meant a qualitative study of optical flow or short image sequences <ref> [Fran90, Carl88, Verr87, Koen75] </ref>. Our work can be viewed as an extension of that problem into the temporal dimension, taking advantage of temporal motion coherence over long image sequences. This allows the recognition of higher-level motions, e.g., cyclic motion [Allm90b], which occur over long image sequences.
Reference: [Lee88a] <author> C. H. Lee. </author> <title> Structure and motion from two perspective views via planar patch. </title> <booktitle> Proc. ICCV, </booktitle> <pages> pages 158 - 164, </pages> <year> 1988. </year>
Reference-contexts: Most previous work has focused on low-level motion analysis. Frame differencing of temporally-adjacent frames is one common method of low-level motion analysis <ref> [Jain79b, Jain79a, Jain81, Jain83, Jaya83, Ande85, Lee88a] </ref>. Recovering orientation of a surface or the position within the scene of points on an object is commonly referred to as structure-from-motion. Structure-from-motion has been performed using correspondences between points [Ullm79] as well as without using point correspondences [Terz88].
Reference: [Lee88b] <author> J. S. Lee and C. Lin. </author> <title> A novel approach to real-time motion detection. </title> <booktitle> Proc. Computer Vision and Pattern Recognition, </booktitle> <pages> pages 730 - 735, </pages> <year> 1988. </year>
Reference-contexts: They perform poorly if motion is not in a steady state. Much of the temporal examination of image sequences consists of looking at only a few frames at a time. Frame differencing has been a popular method of temporal examination of two frames <ref> [Jain79b, Jain79a, Jain81, Jain83, Lee88b, Jaya83, Ande85] </ref>. With this method, the difference of two temporally-adjacent frames is calculated and where the difference is nonzero, there has been motion. Lee and Anderson [Lee88b, Ande85] constructed spatial pyramids from difference images in order to track objects. <p> Frame differencing has been a popular method of temporal examination of two frames [Jain79b, Jain79a, Jain81, Jain83, Lee88b, Jaya83, Ande85]. With this method, the difference of two temporally-adjacent frames is calculated and where the difference is nonzero, there has been motion. Lee and Anderson <ref> [Lee88b, Ande85] </ref> constructed spatial pyramids from difference images in order to track objects. But this spatial pyramid must be rebuilt for every pair of frames. Many optical flow [Horn81] and structure-from-motion [Ullm79] algorithms are similar in that they also use only a few frames to accomplish their goal.
Reference: [Lett59] <author> J. Y. Lettvin, R. R. Maturana, W. S. McCulloch, and W. H. Pitts. </author> <title> What the frog's eye tells the frog's brain. </title> <journal> Proc. Inst. Radio Engrs., </journal> <volume> 47 </volume> <pages> 1940-1951, </pages> <year> 1959. </year>
Reference-contexts: That is, we are not primarily reviewing psychological models of low-level motion. After briefly describing experiments using MLD-like stimuli of low-level motion, we will review the MLD literature and show that an intermediate-level motion representation, namely relative temporal information, is lacking. Lettvin, et al. <ref> [Lett59, Lett61] </ref>, and Henn and Grusser [Henn69] developed a mo 15 tion detector such that proximate, similarly moving points are conceptually grouped. Bosche [Bosc67] produced images of random dots where some dots moved to the left and others moved to the right. This was perceived as rigid, transparent motion.
Reference: [Lett61] <author> J. Y. Lettvin, R. R. Maturana, , and W. H. Pitts W. S. McCulloch. </author> <title> Two remarks on the visual system of the frog. </title> <editor> In W. A Rosenblith, editor, </editor> <title> Sensory Communication. </title> <publisher> MIT Press, </publisher> <year> 1961. </year>
Reference-contexts: That is, we are not primarily reviewing psychological models of low-level motion. After briefly describing experiments using MLD-like stimuli of low-level motion, we will review the MLD literature and show that an intermediate-level motion representation, namely relative temporal information, is lacking. Lettvin, et al. <ref> [Lett59, Lett61] </ref>, and Henn and Grusser [Henn69] developed a mo 15 tion detector such that proximate, similarly moving points are conceptually grouped. Bosche [Bosc67] produced images of random dots where some dots moved to the left and others moved to the right. This was perceived as rigid, transparent motion.
Reference: [Leun87a] <author> M. K. Leung and Y. H. Yang. </author> <title> Human body motion segmentation in a complex scene. </title> <journal> Pattern Recognition, </journal> <volume> 20 </volume> <pages> 55-64, </pages> <year> 1987. </year>
Reference-contexts: High-level motion analysis, i.e., recognizing a coordinated sequence of events such as walking and throwing, has been formulated previously as a process that follows high-level object 2 recognition <ref> [Hogg83, Akit84, Leun87a, Leun87b] </ref>. Only recently have some researchers considered high-level motion analysis as a process that does not depend on complex object descriptions [Godd88b, Godd88a, Godd89, Goul89]. <p> Goddard [Godd88a] noted that there are two ways that motion in a scene can be used. The first paradigm used motion information to first recover the rigid structure of the objects. The recovered structure was then used to index into models <ref> [Hogg83, Akit84, Leun87a, Leun87b] </ref>. The second paradigm used motion information to index into the models. Gould and Shah [Goul89] and Goddard [Godd88a] have asserted that it is possible to recognize an object and its high-level motion without first recovering its structure. <p> Webb and Aggarwal used this property to recover the structure of rigid objects as well as the rigid parts of jointed objects. 20 Leung and Yang <ref> [Leun87a, Leun87b] </ref> developed a system for human body motion analysis. They used a model of the human body and divided the problem into three phases. The first phase separated moving body parts from the background. The second phase labelled the body parts.
Reference: [Leun87b] <author> M. K. Leung and Y. H. Yang. </author> <title> A region based approach for human body motion analysis. </title> <journal> Pattern Recognition, </journal> <volume> 20 </volume> <pages> 321-339, </pages> <year> 1987. </year>
Reference-contexts: High-level motion analysis, i.e., recognizing a coordinated sequence of events such as walking and throwing, has been formulated previously as a process that follows high-level object 2 recognition <ref> [Hogg83, Akit84, Leun87a, Leun87b] </ref>. Only recently have some researchers considered high-level motion analysis as a process that does not depend on complex object descriptions [Godd88b, Godd88a, Godd89, Goul89]. <p> Goddard [Godd88a] noted that there are two ways that motion in a scene can be used. The first paradigm used motion information to first recover the rigid structure of the objects. The recovered structure was then used to index into models <ref> [Hogg83, Akit84, Leun87a, Leun87b] </ref>. The second paradigm used motion information to index into the models. Gould and Shah [Goul89] and Goddard [Godd88a] have asserted that it is possible to recognize an object and its high-level motion without first recovering its structure. <p> Webb and Aggarwal used this property to recover the structure of rigid objects as well as the rigid parts of jointed objects. 20 Leung and Yang <ref> [Leun87a, Leun87b] </ref> developed a system for human body motion analysis. They used a model of the human body and divided the problem into three phases. The first phase separated moving body parts from the background. The second phase labelled the body parts.
Reference: [Limb75] <author> J. O. Limb and J. A. Murphy. </author> <title> Estimating velocity of motion images in television signals. </title> <journal> Computer Graphics and Image Processing, </journal> <volume> 4 </volume> <pages> 311-327, </pages> <year> 1975. </year>
Reference-contexts: The system is under constrained since ffiE is affected by motion along both axes. Motion along the x axis affects @E @y and motion along the y axis affects @E @x . If this were not the case, as assumed by Limb and Murphy <ref> [Limb75] </ref>, the motion could be solved using just the simplified constraint equations dx = @E=@x dt @E=@t Not only do the changes of the ST surface along x and y affect each other, but they do not have the uniformity we desire.
Reference: [Liou89] <author> S. P. Liou and R. C. Jain. </author> <title> Motion detection in spatio-temporal space. Computer Vision, </title> <journal> Graphics, and Image Processing, </journal> <volume> 45 </volume> <pages> 227-250, </pages> <year> 1989. </year>
Reference-contexts: To test our theory that coordinated sequences of events do not have to be computed after a complete static description of the objects has been recovered, we develop in this thesis a new, specific computational theory and implement a system that tests it. We propose, as others have <ref> [Boll87, Jain88, Liou89] </ref>, that an ST volume is the proper low-level representation of the input sequence. <p> It is because of this spatial over temporal emphasis that these models are classified as addressing high-level structure analysis rather than high-level motion. 2.2.3 Low-Level Motion Liou and Jain <ref> [Liou89] </ref> presented past work on spatiotemporal representations that included a survey of filter-type spatiotemporal models. Typically, spatiotemporal filters work in the frequency domain and are sensitive to "motion energy" that is present in the data. <p> Typically, spatiotemporal filters work in the frequency domain and are sensitive to "motion energy" that is present in the data. The essential concepts of filter models are described by Liou and Jain, and Watson 22 and Ahumada <ref> [Liou89, Wats85] </ref>. Let c (x; y; t) define the gray level at each point x, y, t over some interval. The Fourier transform is denoted by ~c (u; v; w). <p> The psychophysical work includes that of Watson, Barlow, VanSanten, Fleet and Heeger [Heeg87, Wats85, Barl65, VanS85, Adel85, Flee84a, Flee84b]. Computational models include those developed by Marr, Buxton, and Liou <ref> [Marr81, Buxt83, Buxt84, Liou89] </ref>. According to Liou and Jain [Liou89], filter models suffer from several fundamental problems. First, most filter models assume constant velocity. Second, filter models suffer from the scale problem, i.e., they have problems separating events at different scales. <p> The psychophysical work includes that of Watson, Barlow, VanSanten, Fleet and Heeger [Heeg87, Wats85, Barl65, VanS85, Adel85, Flee84a, Flee84b]. Computational models include those developed by Marr, Buxton, and Liou [Marr81, Buxt83, Buxt84, Liou89]. According to Liou and Jain <ref> [Liou89] </ref>, filter models suffer from several fundamental problems. First, most filter models assume constant velocity. Second, filter models suffer from the scale problem, i.e., they have problems separating events at different scales. <p> If we sample densely enough in time and an edge operator is applied [Boll87], the spatiotemporal volume will contain surfaces and volumes created by the surfaces. These surfaces and volumes represent object motion swept out through time. Liou and Jain <ref> [Liou89] </ref> considered using spatiotemporal filters but, because of the drawbacks, opted for a variation of the spatiotemporal surface model just presented. Their model used a hypersurface in a 4D volume.
Reference: [Liou90] <author> S. P. Liou and R. C. Jain. </author> <title> A parallel technique for three-dimensional image segmentation. </title> <booktitle> In Proc. 10th Int. Conf. Pattern Recognition, </booktitle> <pages> pages 201-203, </pages> <year> 1990. </year>
Reference-contexts: The features in the slices are then examined to recover the normal component of motion. Since only slices of the ST volume are used, there is no coherent treatment of the volume. Liou and Jain <ref> [Liou90] </ref> defined a motion detector based on the angle between the gradient direction in an ST volume and the time axis. Both these methods suffer from being able to detect only the normal component of motion even when the true motion is recoverable. <p> As such, only the type of motion was used and not the direction of motion, as we do when using ST surface flow. Later, Liou and Jain presented a volume-growing algorithm based on gray level <ref> [Liou90] </ref>. While both 75 of these approaches address the problem of ST cube segmentation, they used temporal and spatial gray-level coherence as opposed to temporal and spatial motion coherence. Peng and Medioni presented an algorithm that analyzed oriented slices of an ST cube [Peng89].
Reference: [Litt87] <author> J. J. Little, G. Blelloch, and T. Cass. </author> <title> Parallel algorithms for computer vision on the connection machine. </title> <booktitle> In Proc. 1st Int. Conf. Computer Vision, </booktitle> <pages> pages 587-591, </pages> <year> 1987. </year>
Reference-contexts: In addition to working with an instant in time, most optical flow segmentation approaches attempt to localize discontinuities in the flow <ref> [Litt87, Thom85, Reic83] </ref> even though the image flow is in error in these areas [Schu86]. As stated earlier, our approach does not deal with these areas, but rather uses areas where the flow is well defined.
Reference: [MacA83] <author> L. MacArther and R. Baron. </author> <title> Toward an ecological theory of social perception. </title> <journal> Psychological Review, </journal> <volume> 90, No. 3 </volume> <pages> 215-238, </pages> <year> 1983. </year>
Reference-contexts: Runeson [Rune81] made MLD's that had points of light not only on the joints of a person but also at the corners of a box that the person was lifting. Observers of this MLD were able to judge the weight of the box. Other studies <ref> [Cutt81b, MacA83] </ref> found evidence that adults are capable of recogniz ing friends and the gender of a person from only the joint motions in MLD's. 16 There have been numerous psychological models to explain how the HVS is able to correctly interpret MLD's.
Reference: [Mack88] <author> A. K. Mackworth and F. Mokhtarian. </author> <title> The renormalized curvature scale space and the evolution properties of planar curves. </title> <booktitle> Proc. Computer Vision and Pattern Recognition, </booktitle> <pages> pages 318-326, </pages> <year> 1988. </year> <month> 142 </month>
Reference-contexts: 00 z fi fi fi fi fi This is equivalent to: fi fi fi fi fi fi X u Y u Z u fi fi fi fi fi jk (u)j 1 This proof is based on a similar proof showing the same property for curvature scale space of planar curves <ref> [Mack88] </ref>. 133 On any contour in torsion scale space k (u; ) = 0 And the fact that all are in C 1 this is equivalent to: X uuu [Y u Z uu Y uu Z u ] Y uuu [X u Z uu X uu Z u ] Z uuu
Reference: [Marr78] <author> D. Marr and H. K. Nishihara. </author> <title> Representation and recognition of the spatial organization of three-dimensional shapes. </title> <journal> Proceedings of the Royal Society of London B, </journal> <volume> 200 </volume> <pages> 269-294, </pages> <year> 1978. </year>
Reference-contexts: Many representations also store how the parts move while performing some high-level motion such as walking [Marr80, Hogg83]. Marr and Vaina [Marr80] extended Marr and Nishihara's model <ref> [Marr78] </ref> so that the representation includes how rigid parts move in relation to each other. The new, augmented representation is object-centered as in Marr and Nishihara's original model [Marr78]. Also, the representation retains a hierarchical form by allowing the specification of how parts move throughout the hierarchy. <p> Marr and Vaina [Marr80] extended Marr and Nishihara's model <ref> [Marr78] </ref> so that the representation includes how rigid parts move in relation to each other. The new, augmented representation is object-centered as in Marr and Nishihara's original model [Marr78]. Also, the representation retains a hierarchical form by allowing the specification of how parts move throughout the hierarchy. Marr and Vaina [Marr80] also developed a State-Motion-State (SMS) representation to describe how articulated objects move over time.
Reference: [Marr80] <author> D. Marr and L. Vaina. </author> <title> Representation and recognition of the movements of shapes. </title> <type> AI Memo 597, </type> <month> October, </month> <year> 1980. </year>
Reference-contexts: That is, structure-from-motion work is usually concerned with finding the structure of rigid parts and not the composition of rigid parts that forms more complex objects. 2.2.2 High-Level Structure High-level structure analysis is concerned with how to construct high-level structures of non-rigid objects. The most common high-level structure representations <ref> [Marr80, Hogg83] </ref> have complex objects stored as a hierarchy of rigid parts with constraints between the parts. Many representations also store how the parts move while performing some high-level motion such as walking [Marr80, Hogg83]. <p> The most common high-level structure representations <ref> [Marr80, Hogg83] </ref> have complex objects stored as a hierarchy of rigid parts with constraints between the parts. Many representations also store how the parts move while performing some high-level motion such as walking [Marr80, Hogg83]. Marr and Vaina [Marr80] extended Marr and Nishihara's model [Marr78] so that the representation includes how rigid parts move in relation to each other. The new, augmented representation is object-centered as in Marr and Nishihara's original model [Marr78]. <p> The most common high-level structure representations [Marr80, Hogg83] have complex objects stored as a hierarchy of rigid parts with constraints between the parts. Many representations also store how the parts move while performing some high-level motion such as walking [Marr80, Hogg83]. Marr and Vaina <ref> [Marr80] </ref> extended Marr and Nishihara's model [Marr78] so that the representation includes how rigid parts move in relation to each other. The new, augmented representation is object-centered as in Marr and Nishihara's original model [Marr78]. <p> The new, augmented representation is object-centered as in Marr and Nishihara's original model [Marr78]. Also, the representation retains a hierarchical form by allowing the specification of how parts move throughout the hierarchy. Marr and Vaina <ref> [Marr80] </ref> also developed a State-Motion-State (SMS) representation to describe how articulated objects move over time. In the SMS representation a behavior is described as a sequence of motion segments. A motion segment of an object M consists of the interval between two adjacent, overall-rest states of M. <p> In the SMS representation a behavior is described as a sequence of motion segments. A motion segment of an object M consists of the interval between two adjacent, overall-rest states of M. Hogg [Hogg83] used a model similar to Marr and Vaina's <ref> [Marr80] </ref> to recognize a walking person. Hogg's model of a human is hierarchical with constraints specifying how the parts can move relative to one another. High-level motions such as walking 21 were stored as sequences of instantaneous descriptions of the 3D structure of the human model. <p> The system, however, had only one object model, a human, and walking is the only high-level motion that the system could track. Akita [Akit84] also worked with a model of a human. His model was similar to Marr and Vaina's <ref> [Marr80] </ref> in that it used generalized cylinders to represent solid parts and it stored relations between the cylinders to represent the joints. High-level motions such as walking and throwing were represented as sequences of key frames.
Reference: [Marr81] <author> D. Marr and S. Ullman. </author> <title> Directional selectivity and its use in early visual processing. </title> <journal> Proc. Roy. Soc London B, </journal> <volume> 211 </volume> <pages> 151-180, </pages> <year> 1981. </year>
Reference-contexts: The psychophysical work includes that of Watson, Barlow, VanSanten, Fleet and Heeger [Heeg87, Wats85, Barl65, VanS85, Adel85, Flee84a, Flee84b]. Computational models include those developed by Marr, Buxton, and Liou <ref> [Marr81, Buxt83, Buxt84, Liou89] </ref>. According to Liou and Jain [Liou89], filter models suffer from several fundamental problems. First, most filter models assume constant velocity. Second, filter models suffer from the scale problem, i.e., they have problems separating events at different scales.
Reference: [Marr82] <author> D. Marr. </author> <title> Vision. </title> <publisher> Freeman, </publisher> <address> San Francisco, </address> <year> 1982. </year>
Reference-contexts: We propose that an intermediate-level motion representation is required to bridge the gap between low-level and high-level motion descriptions. We will construct this intermediate level non-purposively, i.e., without depending on the high-level models, analogous to the way Marr developed the primal sketch and 2 1 2 sketch <ref> [Marr82] </ref>. As in any vision system, the knowledge representation used has a great effect on the way the system performs.
Reference: [Medi84] <author> G. Medioni and R. Nevatia. </author> <title> Description of 3-d surfaces using curvature properties. </title> <booktitle> Proc. Image Understanding Workshop, </booktitle> <pages> pages 291-299, </pages> <year> 1984. </year>
Reference-contexts: There has been relatively little work using differential geometry for ST surface interpretation. There have been numerous suggestions as to which measures are appropriate to describe a surface. Medioni and Navatia <ref> [Medi84] </ref> suggested using Gaussian curvature and the maximum principle curvature. Brady et al. [Brad85] used principle curvature directions and lines of curvature to describe surfaces. Besl and Jain [Besl86] suggested using Gaussian and mean curvature together as surface descriptors.
Reference: [Mokh86] <author> F. Mokhtarian and A. Mackworth. </author> <title> Scale-based description and recognition of planar curves and two-dimensional shapes. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 8 </volume> <pages> 34-43, </pages> <month> January </month> <year> 1986. </year>
Reference-contexts: Note that the contours do not extend to the finest scale as in traditional 1D scale-space images but the features still resemble arches. To detect repeating patterns in the features, we used a uniform cost algorithm that is similar to Mokhtarian's <ref> [Mokh86] </ref>. The algorithm begins by creating a node for every possible pair of features in scale-space. A match cost is computed for each node, measuring how different the two features are.
Reference: [Mokh88] <author> F. Mokhtarian. </author> <title> Multi-scale description of space curves and three-dimensional objects. </title> <booktitle> In Proc. Computer Vision and Pattern Recognition Conf., </booktitle> <pages> pages 298-303, </pages> <year> 1988. </year>
Reference-contexts: Given a starting point, an orientation, and the curvature and torsion values at every point on a curve, that curve is uniquely defined [DoCa76]. Therefore, the torsion and curvature are good measures of the shape of curves <ref> [Allm90a, Mokh88, Hoff88] </ref>. By classifying the different types of motion that can occur in the scene, and the resulting projected motion, one can gain an understanding of the types of flow curves that can be produced. <p> ST curves that track other point features, such as surface markings or vertices, will also have the same period of cyclic motion. We propose using curvature scale-space to detect repeating patterns in ST curves. Mokhtarian <ref> [Mokh88] </ref> showed how to construct curvature scale-space images for a space curve 2 . It is constructed much like the traditional scale-space image of a 1D signal. A space curve is one dimensional with curvature values defined at every point along the curve. <p> As the curve is smoothed, the curvature level-crossings are recomputed. The horizontal axis of the curvature scale-space image is arc length and the vertical axis is the amount of smoothing. Curvature scale-space has many properties that make it desirable, regardless of the application Mokhtarian <ref> [Mokh88] </ref>. Two properties, however, are particularly relevant to the problem of cyclic motion detection. Cyclic motion can occur at many scales. Since curvature scale-space represents curvature over many scales, it is a natural representation to use.
Reference: [Nels88] <author> R. C. Nelson and J. Y. Aloimonos. </author> <title> Using flow field divergence for obstacle avoidance: Towards qualitative vision. </title> <booktitle> In Proc. 2nd Int. Conf. Computer Vision, </booktitle> <pages> pages 188-196, </pages> <year> 1988. </year>
Reference-contexts: An automated surveillance system may need to detect only that motion occurred, and track the centroid of the motion [Burt88]. In obstacle avoidance, unless the margin for error is small, the exact boundaries of objects in relative motion are not needed <ref> [Nels88] </ref>. So rather than attempt to classify pixels based on inaccurate data, exact localization of motion boundaries is sacrificed. Related work in psychology supports the view that localization of occlusion boundaries is not required in order to recover a useful description of motion.
Reference: [O'Ro80] <author> J. O'Rourke and N. I. Badler. </author> <title> Model-based image analysis of human motion using constraint propagation. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 2 </volume> <pages> 522-536, </pages> <month> November, </month> <year> 1980. </year>
Reference-contexts: Badler and Smoliar concluded that animation by simulation is preferable because 25 simulation systems model the interactions between movements in an environment. It will be shown in the next section how object interactions, e.g., collisions, can be a useful intermediate-level motion description. O'Rourke and Badler <ref> [O'Ro80] </ref> used constraints and a model of the human body to analyze human walking. The model is the same as the one developed by Badler and Toltzis [Badl79].
Reference: [Peng89] <author> S. L. Peng and G. Medioni. </author> <title> Interpretation of image sequences by spatio-temporal analysis. </title> <booktitle> In Proc. Workshop Visual Motion, </booktitle> <pages> pages 344-351, </pages> <year> 1989. </year>
Reference-contexts: The fourth dimension is eliminated by storing the gray level at each space-time point. Aloimonos [Aloi88] introduced flow lines to represent the trajectories of point features through the spatiotemporal volume. Peng and Medioni <ref> [Peng89] </ref> worked with 2D slices of a spatiotemporal volume. For every edge point in the first frame, four slices, centered at the point, were taken. Each slice had one spatial and one temporal dimension. <p> While using curvature values is suitable for segmenting an ST volume, it does not give complete information about the structure, i.e., direction of motion, of the ST surfaces in the volume. Peng and Medioni <ref> [Peng89] </ref> presented an algorithm which takes oriented slices of an ST volume. The features in the slices are then examined to recover the normal component of motion. Since only slices of the ST volume are used, there is no coherent treatment of the volume. <p> While both 75 of these approaches address the problem of ST cube segmentation, they used temporal and spatial gray-level coherence as opposed to temporal and spatial motion coherence. Peng and Medioni presented an algorithm that analyzed oriented slices of an ST cube <ref> [Peng89] </ref>. An edge operator was applied to each slice and the resulting edge slices were then examined to recover the normal component of motion. and Y junctions, indicators of occlusion and disocclusion respectively, were located in each slice.
Reference: [Pres88] <author> W. H. Press, B. P. Flannery, S. A. Teukolsky, and W. T. Vetterling. </author> <title> Numerical Recipes in C. </title> <publisher> Cambridge University Press, </publisher> <year> 1988. </year>
Reference-contexts: How ever, since F is only defined at coordinate points and must be interpolated at intermediate pixels, the relatively simple Runge-Kutta method <ref> [Pres88, Gear71] </ref> is appropriate. More sophisticated methods where the increment in t varies depending upon the complexity of 77 F are not used since there is no reason not to use the smallest increment in t available, namely 1.
Reference: [Rash80] <author> R. F. Rashid. </author> <title> Towards a system for the interpretation of moving light displays. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 2, No. 6 </volume> <pages> 574-581, </pages> <month> November, </month> <year> 1980. </year>
Reference-contexts: This assumption says that if a group of pixels, or some other low-level feature, can be interpreted as points on a rigid object, then assume that the pixels are in fact points on a rigid object. Rashid <ref> [Rash80] </ref> used this assumption to recover rigid parts from an MLD. The correspondence of the points between frames was recovered and the 2D velocity of each point calculated.
Reference: [Reic83] <author> W. Reichardt, T. Poggio, and K. Hausen. </author> <title> Figure-ground discrimination by relative movements in the visual system of the fly. Part ii: Towards the neural circuitry. </title> <journal> Biological Cybernetics, </journal> <volume> 46 </volume> <pages> 1-30, </pages> <year> 1983. </year>
Reference-contexts: In addition to working with an instant in time, most optical flow segmentation approaches attempt to localize discontinuities in the flow <ref> [Litt87, Thom85, Reic83] </ref> even though the image flow is in error in these areas [Schu86]. As stated earlier, our approach does not deal with these areas, but rather uses areas where the flow is well defined.
Reference: [Rome84] <author> H. C. Romesburg. </author> <title> Cluster Analysis for Researchers. Lifetime Learning, </title> <year> 1984. </year>
Reference-contexts: In many applications it is reasonable to expect that this information is supplied. However, in our case this would amount to specifying how many objects are moving in the scene. Clearly this is an undesirable assumption. Therefore we will use heuristic techniques for automatically determining how many clusters exist <ref> [Hart75, Rome84] </ref>. A clustering algorithm needs a function that measures the difference between two items. <p> The difference between two adjacent nodes in the tree is defined by the difference between the two associated clusters. See Figure 4.2. Clearly, in most cases, clustering should stop before only one cluster remains. One heuristic, suggested by Romesburg <ref> [Rome84] </ref>, determines the number of clusters such that the decision is least sensitive to error in the difference measure. Figure 4.2 (b) shows the number of clusters obtained for different widths of the ranges of difference between the clusters in Figure 4.2 (a). <p> If this difference is greater than the difference of the flow curve and another cluster's mean, the flow curve is moved into the other cluster. This is called the K-Means clustering method <ref> [Hart75, Rome84] </ref>. Note that the number of clusters does not change, but it allows flow curves to move between existing clusters.
Reference: [Rubi85] <author> J. Rubin and W.A. Richards. </author> <title> Boundaries of visual motion. </title> <type> Technical Report AI Memo 835, </type> <institution> MIT, </institution> <year> 1985. </year> <month> 143 </month>
Reference-contexts: That is, while some interactions are real interactions, e.g., objects colliding, and some are apparent, e.g., one object partially occluding another, all are manifested as viewer-centered motion features in the ST cube. Rubin and Richards, and Engel <ref> [Rubi85, Enge86] </ref> proposed that there are four important motion boundaries: force impulses, starts, stops and pauses. These boundaries also describe some real, as opposed to apparent, interactions between objects. For example, when two rigid objects collide there is a force impulse applied to the objects. <p> The slopes of the lines in each group of four slices were examined and the normal component of velocity of the point was computed. Engel and Rubin [Enge86] presented a system that detects visual motion boundaries. This was an implementation of Rubin and Richards earlier work <ref> [Rubi85] </ref>. Rubin and Richards argued that force impulses, starts, stops and pauses, are important temporal events. Starts, stops, and pauses were detected by examining the velocity of a moving point. Assuming mass remains constant, a force impulse, or force discontinuity, was detected by examining the acceleration of a moving point.
Reference: [Rune81] <author> S. Runeson and G. Frykholm. </author> <title> Visual perception of lifted weight. </title> <journal> Journal of Experimental Psychology: Human Perception and Performance, </journal> <volume> 7, No. 4 </volume> <pages> 733-740, </pages> <year> 1981. </year>
Reference-contexts: The results with this display were similar to the results in the first demonstration. Johansson also tested displays for running, cycling, climbing, dancing in couples, various types of gymnastic motion, and others. In all cases adults spontaneously and correctly identified the motion. Runeson <ref> [Rune81] </ref> made MLD's that had points of light not only on the joints of a person but also at the corners of a box that the person was lifting. Observers of this MLD were able to judge the weight of the box.
Reference: [Schu84] <author> B. G. Schunck. </author> <title> The motion constraint equation for optical flow. </title> <booktitle> Proc. 7th Int. Conf. on Pattern Recognition, </booktitle> <pages> pages 20-22, </pages> <year> 1984. </year>
Reference-contexts: Thus, our approach does not make the second assumption given above. Our model parameterization assumes motion of a contour is a combination of translational and rotational motion parallel to the image plane, i.e., the motion is planar. However, it was shown by Schunck <ref> [Schu84] </ref> that the optical flow constraint equation is valid only for translation, i.e., our model parameterization is no more restrictive. 63 3.5.2 Approximating General Motion The model parameterization assumes that motion in the scene is parallel to the image plane, i.e., there is no rotation in depth.
Reference: [Schu86] <author> B. G. Schunck. </author> <title> The image flow constraint equation. Computer Vision, </title> <journal> Graphics, and Image Processing, </journal> <volume> 35 </volume> <pages> 20-46, </pages> <year> 1986. </year>
Reference-contexts: In addition to working with an instant in time, most optical flow segmentation approaches attempt to localize discontinuities in the flow [Litt87, Thom85, Reic83] even though the image flow is in error in these areas <ref> [Schu86] </ref>. As stated earlier, our approach does not deal with these areas, but rather uses areas where the flow is well defined. Jain segmented an ST cube by examining the signs of three principle curvatures of an ST 3-surface [Jain88].
Reference: [Spet87] <author> M. Spetsakis and J. Aloimonos. </author> <title> Closed form solution to the structure from motion problem from line correspondences. </title> <booktitle> Proc. Sixth National Conference on Artificial Intelligence, </booktitle> <pages> pages 738-743, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: The third phased assigned motion verbs to the movement. Phases one and two, which correspond to recovering low-level structure, were addressed by the authors but the third phase, which is high-level motion recognition, was left as future work. Most previous work on structure-from-motion <ref> [Ullm79, Webb82, Spet87, Brus83] </ref> can be classified as detecting low-level structure.
Reference: [Stev79] <author> K. A. Stevens. </author> <title> Representing and analyzing surface orientation. </title> <editor> In P. H. Win-ston and R. H. Brown, editors, </editor> <booktitle> Artificial Intelligence: An MIT Perspective, </booktitle> <volume> volume 2. </volume> <publisher> MIT Press, </publisher> <year> 1979. </year>
Reference-contexts: In this case, the sequence of representations includes the original images, the final optical flow description, and object descriptions. Similarly, many approaches to texture and structure analysis first recover texels and then use them to recover object structure <ref> [Stev79] </ref>. Once a texel is known, its shape under projection can be computed. Using this information it is possible to recover the surface orientation of a surface containing the texel.
Reference: [Taal90] <author> M. A. Taalebinezhaad. </author> <title> Direct recovery of motion and shape in the general case by fixation. </title> <booktitle> In Proc. Int. Conf. Computer Vision, </booktitle> <pages> pages 451-455, </pages> <year> 1990. </year>
Reference-contexts: A common paradigm when examining image sequences is to recover 3D structure or 3D motion immediately after optical flow is computed [Agga88]. Recently, some work has even completely skipped the flow recovery step and computed structure and motion directly <ref> [Heel90, Taal90, Faug90] </ref>. However, a great deal of information exists in the flow field and therefore our approach is to recover a qualitative description of image motion from ST surface flow. Qualitative analysis of image motion is not new.
Reference: [Terz88] <author> D. Terzopoulos, A. Witkin, and M. Kass. </author> <title> Constraints on deformable models: Recovering 3d shape and nonrigid motion. </title> <journal> Artificial Intelligence, </journal> <volume> 36 </volume> <pages> 91-123, </pages> <year> 1988. </year>
Reference-contexts: Recovering orientation of a surface or the position within the scene of points on an object is commonly referred to as structure-from-motion. Structure-from-motion has been performed using correspondences between points [Ullm79] as well as without using point correspondences <ref> [Terz88] </ref>. Our approach is based on constructing a coordinated hierarchy of not only low-level motion description, but also intermediate-level and high-level motion description. <p> They defined a trajectory primal sketch as a low-level representation for the movement of points. Using scale space, important events were detected and primitive movements 24 or trajectories, including translation, rotation, projectile and cycloid were described. A different approach was taken by Terzopoulos, Witkin and Kass <ref> [Terz88] </ref>. They built a dynamic 3D model from image data. These models are dynamic since they change to best match the image data. A model can be conceptualized as a deformable 3D space curve spine with a deformable sheet rolled around the spine.
Reference: [Thom85] <author> W. B. Thompson, K. M. Mutch, and V. A. Berzins. </author> <title> Dynamic occlusion analysis in optical flow fields. </title> <journal> IEEE Trans. Patt. Analysis Machine Intell., </journal> <volume> 7 </volume> <pages> 374-383, </pages> <year> 1985. </year>
Reference-contexts: In addition to working with an instant in time, most optical flow segmentation approaches attempt to localize discontinuities in the flow <ref> [Litt87, Thom85, Reic83] </ref> even though the image flow is in error in these areas [Schu86]. As stated earlier, our approach does not deal with these areas, but rather uses areas where the flow is well defined.
Reference: [Tou74] <author> J. T. Tou and R. C. Gonzalez. </author> <title> Pattern Recognition Principles. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1974. </year>
Reference-contexts: Once a texel is known, its shape under projection can be computed. Using this information it is possible to recover the surface orientation of a surface containing the texel. Conversely, the structure of a surface can first be recovered or assumed, and then used in recovering the texels <ref> [Fu68, Fuku72, Tou74] </ref>. The issue of representation sequences in the human visual system has also been addressed in the psychology community. For example, it was once thought that the human visual system performed stereopsis at a high-level, after monocular object recognition took place.
Reference: [Ullm79] <author> S. Ullman. </author> <title> The Interpretation of Visual Motion. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1979. </year>
Reference-contexts: Recovering orientation of a surface or the position within the scene of points on an object is commonly referred to as structure-from-motion. Structure-from-motion has been performed using correspondences between points <ref> [Ullm79] </ref> as well as without using point correspondences [Terz88]. Our approach is based on constructing a coordinated hierarchy of not only low-level motion description, but also intermediate-level and high-level motion description. <p> Bosche [Bosc67] produced images of random dots where some dots moved to the left and others moved to the right. This was perceived as rigid, transparent motion. Similarly, Ullman <ref> [Ullm79] </ref> presented two dotted, transparent cylinders, one inside the other, rotating in opposite directions. The correct perception of two counter-rotating cylinders was perceived. Chapter 1 described high-level motion as abstract sequences such as walking. <p> The third phased assigned motion verbs to the movement. Phases one and two, which correspond to recovering low-level structure, were addressed by the authors but the third phase, which is high-level motion recognition, was left as future work. Most previous work on structure-from-motion <ref> [Ullm79, Webb82, Spet87, Brus83] </ref> can be classified as detecting low-level structure. <p> Lee and Anderson [Lee88b, Ande85] constructed spatial pyramids from difference images in order to track objects. But this spatial pyramid must be rebuilt for every pair of frames. Many optical flow [Horn81] and structure-from-motion <ref> [Ullm79] </ref> algorithms are similar in that they also use only a few frames to accomplish their goal. Recently, there has been a trend toward using more than a few frames to examine an image sequence [Jain88, Aloi88]. Jain notes: "Most techniques try to do too much using too few frames.
Reference: [VanS85] <author> J. P. H. VanSanten and G. Sperling. </author> <title> Elaborated reichardt detectors. </title> <journal> J. Optical Society of America, </journal> <volume> 2:300 - 321, </volume> <month> February, </month> <year> 1985. </year>
Reference-contexts: The psychophysical work includes that of Watson, Barlow, VanSanten, Fleet and Heeger <ref> [Heeg87, Wats85, Barl65, VanS85, Adel85, Flee84a, Flee84b] </ref>. Computational models include those developed by Marr, Buxton, and Liou [Marr81, Buxt83, Buxt84, Liou89]. According to Liou and Jain [Liou89], filter models suffer from several fundamental problems. First, most filter models assume constant velocity.
Reference: [Verr87] <author> A. Verri and T. Poggio. </author> <title> Against quantitative optical flow. </title> <booktitle> In Proc. 1st Int. Conf. Computer Vision, </booktitle> <pages> pages 171-180, </pages> <year> 1987. </year>
Reference-contexts: Qualitative analysis of image motion is not new. In the past, this usually meant a qualitative study of optical flow or short image sequences <ref> [Fran90, Carl88, Verr87, Koen75] </ref>. Our work can be viewed as an extension of that problem into the temporal dimension, taking advantage of temporal motion coherence over long image sequences. This allows the recognition of higher-level motions, e.g., cyclic motion [Allm90b], which occur over long image sequences.
Reference: [Wall65] <author> H. Wallach. </author> <title> Visual perception of motion. </title> <editor> In G. Kepes, editor, </editor> <booktitle> The nature and art of motion. </booktitle> <address> Braziller, </address> <year> 1965. </year> <title> (Revised in H. Wallach, </title> <journal> On perception. </journal> <volume> Quadrangle, </volume> <year> 1976). </year>
Reference-contexts: There are no "correct" solutions to Eq. (6.1), only solutions that are similar to the HVS's perception. Therefore, we will examine how the HVS perceives these motions and use that as an additional constraint to solve Eq. (6.1). Psychologists have long recognized this perceptual phenomenon <ref> [Joha50, Hoch57, Wall65] </ref>, but have failed to developed a complete model of how the HVS recovers relative and common motion [Joha73, Cutt82]. A common component of those models is to minimize the complexity of the relative and/or common motion [Hoch53, Hoch60, Cutt82].
Reference: [Wats85] <author> A. B. Watson and A. J. Ahumada. </author> <title> Model of human visual-motion sensing. </title> <journal> J. Optical Society of America, </journal> <volume> 2:322 - 342, </volume> <month> February </month> <year> 1985. </year>
Reference-contexts: Typically, spatiotemporal filters work in the frequency domain and are sensitive to "motion energy" that is present in the data. The essential concepts of filter models are described by Liou and Jain, and Watson 22 and Ahumada <ref> [Liou89, Wats85] </ref>. Let c (x; y; t) define the gray level at each point x, y, t over some interval. The Fourier transform is denoted by ~c (u; v; w). <p> The psychophysical work includes that of Watson, Barlow, VanSanten, Fleet and Heeger <ref> [Heeg87, Wats85, Barl65, VanS85, Adel85, Flee84a, Flee84b] </ref>. Computational models include those developed by Marr, Buxton, and Liou [Marr81, Buxt83, Buxt84, Liou89]. According to Liou and Jain [Liou89], filter models suffer from several fundamental problems. First, most filter models assume constant velocity.
Reference: [Webb82] <author> J. A. Webb and J. K. Aggarwal. </author> <title> Structure from motion of rigid and jointed objects. </title> <journal> Artificial Intelligence, </journal> <volume> 19 </volume> <pages> 107-130, </pages> <year> 1982. </year> <month> 144 </month>
Reference-contexts: Next, the minimal spanning tree was found. The resulting tree appears as clusters of edges with the clusters connected by long edges. These long edges are cut and the resulting clusters represent all distinct, rigid parts or objects. Webb and Aggarwal <ref> [Webb82] </ref> used a fixed-axis assumption that states: the motion of every rigid object consists of a translation plus a rotation about an axis that is fixed in direction for short periods of time. <p> The third phased assigned motion verbs to the movement. Phases one and two, which correspond to recovering low-level structure, were addressed by the authors but the third phase, which is high-level motion recognition, was left as future work. Most previous work on structure-from-motion <ref> [Ullm79, Webb82, Spet87, Brus83] </ref> can be classified as detecting low-level structure.
Reference: [Yama89] <author> M. Yamamoto. </author> <title> A general aperture problem for direct estimation of 3-d mo-tion parameters. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 11 </volume> <pages> 528-536, </pages> <year> 1989. </year>
Reference-contexts: Although not visible in Figure 3.20 (b), most of the sky region except the center contains clouds. In the cloud areas there was sufficient gray level variation for the correct result to be achieved, but in the non-cloud area there was not enough variation. 3.5.3 The Aperture Problem Yamamoto <ref> [Yama89] </ref> showed that for translational and rotational motion parallel to the image plane, there is an aperture problem when the contour is a straight line and when there is rotation about the center of a circle.
References-found: 118

