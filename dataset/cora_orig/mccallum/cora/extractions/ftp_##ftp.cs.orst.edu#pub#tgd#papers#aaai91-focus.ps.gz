URL: ftp://ftp.cs.orst.edu/pub/tgd/papers/aaai91-focus.ps.gz
Refering-URL: http://www.cs.orst.edu/~tgd/cv/pubs.html
Root-URL: 
Email: almualh@cs.orst.edu tgd@cs.orst.edu  
Title: Learning With Many Irrelevant Features  
Author: Hussein Almuallim and Thomas G. Dietterich Dearborn Hall 
Date: 1  
Note: ln  given to ID3 or FRINGE.  
Address: Corvallis, OR 97331-3202  
Affiliation: Department of Computer Science Oregon State University  
Abstract: In many domains, an appropriate inductive bias is the MIN-FEATURES bias, which prefers consistent hypotheses definable over as few features as possible. This paper defines and studies this bias. First, it is shown that any learning algorithm implementing the MIN-FEATURES bias requires fi( 1 * [2 p + p ln n]) training examples to guarantee PAC-learning a concept having p relevant features out of n available features. This bound is only logarithmic in the number of irrelevant features. The paper also presents a quasi-polynomial time algorithm, FOCUS, which implements MIN-FEATURES. Experimental studies are presented that compare FOCUS to the ID3 and FRINGE algorithms. These experiments show that| contrary to expectations|these algorithms do not implement good approximations of MIN-FEATURES. The coverage, sample complexity, and generalization performance of FOCUS is substantially better than either ID3 or FRINGE on learning problems where the MIN-FEATURES bias is appropriate. This suggests that, in practical applications, training data should be preprocessed to remove irrelevant features before being
Abstract-found: 1
Intro-found: 1
Reference: <author> Buntine, W. L. </author> <year> 1990. </year> <title> Myths and Legends in Learning Classification Rules. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence (AAAI-90), </booktitle> <pages> 736-742. </pages> <address> Boston, MA: </address> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: Often, it is difficult even to state the bias in any simple way. Consequently, it is difficult to tell in advance whether the bias is appropriate for a new learning problem. Recently, a few authors <ref> (Buntine 1990, Wolpert 1990) </ref> have advocated a different procedure: (i) adopt a bias over some space of hypotheses (or, equivalently, select a prior probability distribution over the space), (ii) select a scheme for representing hypotheses in the space, and (iii) design an algorithm that implements this bias, at least approximately.
Reference: <author> Blumer, A.; Ehrenfeucht, A.; Haussler, D.; and War-muth, M. </author> <year> 1987a. </year> <title> Learnability and the Vapnik-Chervonenkis Dimension, </title> <type> Technical Report UCSC-CRL-87-20, </type> <institution> Department of Computer and Information Sciences, University of California, Santa Cruz, </institution> <month> Nov. </month> <year> 1987. </year> <note> Also in Journal of ACM, 36(4) 929-965. </note>
Reference-contexts: This is quasi-polynomial in n and s, but clearly it will be impractical for large values of s. According to the definition of learnability given in <ref> (Blumer et al. 1987a) </ref>, this says that the class of Boolean concepts, under our complexity measure, is learnable using a polynomial number of examples in quasi-polynomial time.
Reference: <author> Blumer, A.; Ehrenfeucht, A.; Haussler, D.; and War-muth, M. </author> <year> 1987b. </year> <title> Occam's Razor. </title> <journal> Information Processing Letters, </journal> <volume> 24 </volume> <pages> 377-380. </pages>
Reference-contexts: Proof (Sketch): For any target concept of complexity at most s, the hypothesis space for any algorithm that implements the MIN-FEATURES bias is contained in C n;s . We argue that jC n;s j n The result follows immediately by applying the lemma of <ref> (Blumer et al. 1987b) </ref>. 2 It is interesting to note that the number of examples sufficient for learning grows only logarithmically in the number of irrelevant features and linearly in the complexity of the concept.
Reference: <author> Dietterich, T. G. </author> <year> 1989. </year> <title> Limitations on Inductive Learning. </title> <booktitle> In Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <pages> 124-128. </pages> <address> Ithaca, NY: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: If, for fraction 1 ffi of the training samples, L outputs a function that is *-close to the correct concept, then we say that L frequently-approximately correctly (FAC) learns c <ref> (Dietterich 1989) </ref>. The coverage of an algorithm, given m, *; and ffi, is the number of concepts that can be FAC-learned by the algorithm.
Reference: <author> Ehrenfeucht, A.; Haussler, D.; Kearns, M.; and Valiant, L.G. </author> <year> 1988. </year> <title> A General Lower Bound on the Number of Examples Needed for Learning. </title> <booktitle> In Proceedings of the First Workshop on Computational Learning Theory, </booktitle> <pages> 139-154. </pages> <address> Boston, MA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: It is shown that the number of examples needed for learning any class of concepts strongly depends on the VC-dimension of the class. Specifically, <ref> (Ehrenfeucht et al. 1988) </ref> prove the following: Theorem 2 Let C be a class of concepts and 0 &lt; *; ffi &lt; 1.
Reference: <author> Harrison, M. </author> <year> 1965. </year> <title> Introduction to Switching and Automata Theory. </title> <publisher> McGraw Hill, Inc. </publisher>
Reference: <author> Littlestone, N. </author> <year> 1988. </year> <title> Learning Quickly When Irrelevant Attributes Abound: A New Linear-threshold Algorithm. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 285-318. </pages>
Reference-contexts: This applies, for example, to the task of learning diagnosis rules for several different diseases from the medical records of a large number of patients. These records usually contain more information than is actually required for describing each disease. Another example <ref> (given in Littlestone, 1988) </ref> involves pattern recognition tasks in which feature detectors automatically extract a large number of features for the learner's consideration, not knowing which might prove useful.
Reference: <author> Mitchell, T. M. </author> <year> 1982. </year> <title> Generalization as Search. </title> <journal> Artificial Intelligence, </journal> <volume> 18 </volume> <pages> 203-226. </pages>
Reference: <author> Pagallo, G.; and Haussler, D. </author> <year> 1990. </year> <title> Boolean Feature Discovery in Empirical Learning. </title> <journal> Machine Learning, </journal> <volume> 5(1) </volume> <pages> 71-100. </pages>
Reference-contexts: For example, ID3 (Quinlan 1986) has a bias in favor of small decision trees, and small trees would seem to test only a subset of the input features. In the final section of the paper, we present experiments comparing FOCUS to ID3 and FRINGE <ref> (Pagallo & Haussler 1990) </ref>. These demonstrate that ID3 and FRINGE are not good implementations of the MIN-FEATURES bias|these algorithms often produce hypotheses as output that are much more complex (in terms of the number of input features used) than the hypotheses found by FOCUS. <p> In this section, we test these algorithms to see how well they implement the MIN-FEATURES bias. In particular, we compare three algorithms: (i) ID3: As described in (Quinlan 1986), but resolving ties randomly when two or more features look equally good. (ii) FRINGE: As given in <ref> (Pagallo & Haussler 1990) </ref>, with the maximum number of iterations set to 10. (iii) FOCUSed-ID3: First, a minimum set of features sufficient to produce a consistent hypothesis is obtained as in FOCUS. After finding a minimal-size subset of relevant features, the training examples are filtered to remove all irrelevant features.
Reference: <author> Quinlan, J. R. </author> <year> 1986. </year> <title> Induction of Decision Trees, </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 81-106. </pages>
Reference-contexts: It then applies a straightforward learning procedure that focuses on just those p features. At first glance, it may appear that there are already many algorithms that approximate this bias. For example, ID3 <ref> (Quinlan 1986) </ref> has a bias in favor of small decision trees, and small trees would seem to test only a subset of the input features. In the final section of the paper, we present experiments comparing FOCUS to ID3 and FRINGE (Pagallo & Haussler 1990). <p> However, their result is only shown for the uniform distribution case, while ours applies to all distributions. Experimental Work Several learning algorithms appear to have biases similar to the MIN-FEATURES bias. In particular, algorithms related to ID3 <ref> (Quinlan 1986) </ref> attempt to construct "small" decision trees. These algorithms construct the decision tree top-down (i.e., starting at the root), and they terminate as soon as they find a tree consistent with the training examples. <p> Features tested at each node are chosen according to their estimated relevance to the target concept, measured using the mutual information criterion. In this section, we test these algorithms to see how well they implement the MIN-FEATURES bias. In particular, we compare three algorithms: (i) ID3: As described in <ref> (Quinlan 1986) </ref>, but resolving ties randomly when two or more features look equally good. (ii) FRINGE: As given in (Pagallo & Haussler 1990), with the maximum number of iterations set to 10. (iii) FOCUSed-ID3: First, a minimum set of features sufficient to produce a consistent hypothesis is obtained as in FOCUS.
Reference: <author> Slepian, D. </author> <year> 1953. </year> <title> On the Number of Symmetry Types of Boolean Functions of n Variables. Can. </title> <journal> J. Math., </journal> <volume> 5(2) </volume> <pages> 185-193. </pages>
Reference: <author> Verbeurgt, K. </author> <year> 1990. </year> <title> Learning DNF Under the Uniform Distribution in Quasi-polynomial Time. </title> <booktitle> In Proceedings of the Third Workshop on Computational Learning Theory, </booktitle> <pages> 314-326. </pages> <address> Rochester, NY: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: According to the definition of learnability given in (Blumer et al. 1987a), this says that the class of Boolean concepts, under our complexity measure, is learnable using a polynomial number of examples in quasi-polynomial time. An analogous result is given in <ref> (Verbeurgt 1990) </ref> where, taking the minimum number of terms needed to encode the concept as a DNF for-mula as the complexity measure, they obtain a learn-ability result using a polynomial sample size and quasi-polynomial time.
Reference: <author> Wolpert, D. </author> <year> 1990. </year> <title> A Mathematical Theory of Generalization: Parts I and II. </title> <journal> Complex Systems, </journal> <volume> 4 </volume> (2):151-249. 
Reference-contexts: Often, it is difficult even to state the bias in any simple way. Consequently, it is difficult to tell in advance whether the bias is appropriate for a new learning problem. Recently, a few authors <ref> (Buntine 1990, Wolpert 1990) </ref> have advocated a different procedure: (i) adopt a bias over some space of hypotheses (or, equivalently, select a prior probability distribution over the space), (ii) select a scheme for representing hypotheses in the space, and (iii) design an algorithm that implements this bias, at least approximately.
References-found: 13

