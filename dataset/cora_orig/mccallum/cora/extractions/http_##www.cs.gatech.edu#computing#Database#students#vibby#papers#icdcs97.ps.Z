URL: http://www.cs.gatech.edu/computing/Database/students/vibby/papers/icdcs97.ps.Z
Refering-URL: http://www.cs.gatech.edu/computing/Database/students/vibby/vibby.html
Root-URL: 
Email: fvibbyg@watson.ibm.com  fedwardo,ramag@cc.gatech.edu  
Title: Database Storage Management in a Shared Virtual Memory Environment  
Author: Vibby Gottemukkala Edward Omiecinski Umakishore Ramachandran 
Affiliation: IBM T.J.Watson Research Center  College of Computing Georgia Institute of Technology  
Abstract: Shared Virtual Memory (SVM) provides a shared memory abstraction in systems that do not have a physically shared memory. Several architectures that provide this sharing abstraction without the scalability limitations of physical sharing have been proposed. However, there has not been much work in the area of database systems to exploit this scalable abstraction. In earlier work we proposed a scalable sharing database architecture that exploits the SVM abstraction for better scalability and resource utilization. In this paper we propose a database storage management technique that exploits the SVM abstraction to better utilize the global memory available. The technique uses remote memory as a part of the overall storage hierarchy. Through this technique the database system reduces the overall amount of disk I/O, thus improving performance by about 20%. Furthermore, we also discuss how this storage management technique is implemented in existing systems and how it can improve even uniprocessor database performance. 
Abstract-found: 1
Intro-found: 1
Reference: [B + 92] <author> H. Burkhardt et al. </author> <title> Overview of the KSR1 Computer System. </title> <type> Technical Report KSR-TR-9202001, </type> <institution> Kendall Square Research, </institution> <address> Boston, </address> <year> 1992. </year>
Reference-contexts: In recent years there has been a great deal of focus in designing and building scalable SVM systems <ref> [S + 95, TP96, B + 92, SN95] </ref>. Database systems are the primary commercial applications for large scale parallel systems. Our research focus in in designing parallel database systems that are highly scalable and efficient in resource utilization. <p> However, the key difference between the two schemes is that in the SS-MMSM scheme there is no central server that knows the system-wide caching status of all the data, but is distributed <ref> [B + 92, S + 95] </ref>. In the scheme used by Franklin et al., if a client buffer manager does not find the requested data in its cache it always contacts the server.
Reference: [Bac86] <author> M. J. Bach. </author> <title> The Design of the UNIX Operating System. </title> <publisher> Prentice-Hall, </publisher> <year> 1986. </year>
Reference-contexts: [Cor91] the I/O performance difference between BM-R and BM-F could be around 50%, the same range as we observe through our experiments. 5.3 Discussion Most Unix DBMS (e.g., DB2, Informix, Oracle, Sybase) allow the database to be stored as part of the Unix file system or, on raw disk partitions <ref> [Bac86, L + 88] </ref> that are explicitly managed by the DBMS or, on both. Storing data on raw disk partitions significantly increases the complexity of the DBMS but correspondingly allows the DBMS to exercise greater control and provide better 21 performance.
Reference: [BHT90] <author> M. Bellew, M. Hsu, and V. Tam. </author> <title> Update Propagation in Distributed Memory Hierarchy. </title> <booktitle> In Proceedings of the 6th International Conference on Data Engineering, </booktitle> <pages> pages 521-28, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: Later in this chapter we discuss the system support needed in order to efficiently implement an MMSM. 2.4 Related work There has not been much work in using shared virtual memory (SVM) for database systems and global memory management. Bellew et al. <ref> [BHT90] </ref> have used SVM to share data between clients in a client-server database system. The thrust of this work is efficient coherence maintenance and concurrency control. Shatdal and Naughton [SN93] assume a multiprocessor 7 system with a software SVM layer to enhance join strategies that efficiently handle data skew.
Reference: [Cor91] <author> Oracle Corp. </author> <title> Oracle for Unix: Technical Reference Guide. </title> <address> Redwood City, CA, </address> <note> version 6.0.30 edition, </note> <year> 1991. </year>
Reference-contexts: A lack of better understanding of how the SunOS implements and handles I/Os to large files prevents us from being able to completely explain this anomalous behavior. However, it has been claimed that in the Oracle DBMS (a Unix-based DBMS) <ref> [Cor91] </ref> the I/O performance difference between BM-R and BM-F could be around 50%, the same range as we observe through our experiments. 5.3 Discussion Most Unix DBMS (e.g., DB2, Informix, Oracle, Sybase) allow the database to be stored as part of the Unix file system or, on raw disk partitions [Bac86, <p> Storing data on raw disk partitions significantly increases the complexity of the DBMS but correspondingly allows the DBMS to exercise greater control and provide better 21 performance. It has been suggested in commercial database system manuals and performance tuning guides <ref> [IBM95, Cor91, Rod90] </ref> that data requiring high-performance access be placed on the raw disk partitions and data such as catalogs and secondary indexes be placed in files managed by the operating system.
Reference: [DG92] <author> D. DeWitt and J. Gray. </author> <title> Parallel Database Systems: The Future of High Performance Database Systems. </title> <journal> Communications of the ACM, </journal> <volume> 35(6) </volume> <pages> 85-98, </pages> <month> June </month> <year> 1992. </year> <month> 23 </month>
Reference-contexts: In earlier work [GOR94] we proposed and evaluated a database architecture, called the scalable sharing (SS) architecture, that assumes SVM support in the underlying system. The scalable sharing (SS) architecture is a hybrid between the well known shared nothing (SN) and shared everything (SE) parallel database architectures <ref> [DG92] </ref>. One of the main goals of the SS architecture is to improve resource utilization and thus improve scalability of the database architecture.
Reference: [FCL92] <author> M. J. Franklin, M. J. Carey, and M. Livny. </author> <title> Global Memory Management in Client--Server DBMS Architectures. </title> <booktitle> In Proceedings of the 18th VLDB Conference, </booktitle> <pages> pages 596-609, </pages> <year> 1992. </year>
Reference-contexts: Shatdal and Naughton [SN93] assume a multiprocessor 7 system with a software SVM layer to enhance join strategies that efficiently handle data skew. However, neither of these studies have examined the use of SVM for better memory utilization. Franklin et al. <ref> [FCL92] </ref> have proposed a global memory management scheme in the context of a client-server database. The main thrust of this work is that the server, in co-ordination with the clients, tries to maximize the amount of database that is memory resident in the system by eliminating duplicate caching. <p> Thus, in concept the SS-MMSM scheme is very similar to the work done by Franklin et al. <ref> [FCL92] </ref>. However, the key difference between the two schemes is that in the SS-MMSM scheme there is no central server that knows the system-wide caching status of all the data, but is distributed [B + 92, S + 95].
Reference: [GL92] <author> V. Gottemukkala and T. J. Lehman. </author> <title> Locking and Latching in a Memory-Resident Database System. </title> <booktitle> In Proceedings of the VLDB Conference, </booktitle> <month> August </month> <year> 1992. </year>
Reference-contexts: The advantages of structuring the database in such a manner have been described in the literature <ref> [GL92, J + 94, LSC92] </ref>. It has been shown by Lehman et al. [LSC92] that for data already in memory, a memory-resident database can process queries between 1.5 to 5 times faster than a disk-resident database.
Reference: [GOR94] <author> V. Gottemukkala, E. Omiecinski, and U. Ramachandran. </author> <title> A Scalable Sharing Architecture for a Parallel Database System. </title> <booktitle> In Proceedings of the IEEE Symposium on Parallel and Distributed Processing, </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: Database systems are the primary commercial applications for large scale parallel systems. Our research focus in in designing parallel database systems that are highly scalable and efficient in resource utilization. In earlier work <ref> [GOR94] </ref> we proposed and evaluated a database architecture, called the scalable sharing (SS) architecture, that assumes SVM support in the underlying system. The scalable sharing (SS) architecture is a hybrid between the well known shared nothing (SN) and shared everything (SE) parallel database architectures [DG92]. <p> The scalable sharing (SS) architecture is a hybrid between the well known shared nothing (SN) and shared everything (SE) parallel database architectures [DG92]. One of the main goals of the SS architecture is to improve resource utilization and thus improve scalability of the database architecture. In <ref> [GOR94] </ref> we examined how the SS architecture allows better utilization of the available resources such as the CPU, the network, and the disks through our data partitioning and dynamic scheduling strategies (which exploit SVM). <p> We have shown that the data partitioning and dynamic scheduling in the SS architecture provides the scalability of the SN architecture while providing better performance and utilization <ref> [GOR94] </ref>. 3 2.2 Managing memory in the SS architecture In order to better understand our storage management technique, we first describe how storage management (often referred to as buffer management) is typically done in database systems and its applicability to the SS architecture. <p> Furthermore, we assume that the buffer managers and transactions are cognizant of the SVM support available in the SS architecture and use it to transfer data between the nodes as described below. Earlier we outlined the scheduling framework for the SS architecture <ref> [GOR94] </ref>. Under this framework, units-of-work/sub-queries are scheduled to be processed where the data resides. However, idle nodes can pick-up and process sub-queries assigned to other nodes. <p> For both the SS schemes - SS-MMSM and SS-BM (the traditional buffer 10 management approach described in Section 2), and the SN scheme we have used a simple LRU replacement policy to select the pages to be replaced. Furthermore, as discussed in <ref> [GOR94] </ref>, the SN architecture tries to handle data skew through sophisticated (and, complex and static) partitioning schemes. On the other hand, the SS architecture uses the simple diagonal assignment strategy for partitioning and the handling of the effects of the resultant data skew is deferred to the dynamic scheduling strategy.
Reference: [GR93] <author> J. Gray and A. Reuter. </author> <title> Transaction Processing: Concepts and Techniques. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: Furthermore, experimental studies have shown that for reasonably large buffer sizes, replacement policies such as random replacement or the clock algorithm (commonly used by operating systems) perform as well as the more sophisticated replacement schemes <ref> [GR93] </ref>. 6 Conclusion In this paper we have presented a memory-mapped storage management (MMSM) technique for database systems that exploits shared virtual memory. The MMSM technique allows the database to view remote memory as part of the storage hierarchy, between local memory and disk.
Reference: [HLH92] <author> E. Hagersten, A. Landin, and S. Haridi. </author> <title> DDM A Cache-Only Memory Architecture. </title> <journal> IEEE Computer, </journal> <volume> 25(9), </volume> <month> September </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Parallel systems that support a shared address space allow database systems to better utilize the system resources. The shared address space can be supported either through shared physical memory or through shared virtual memory (SVM) <ref> [LH89, HLH92] </ref>. Although both approaches provide global address space to the application, the performance characteristics of the systems are significantly different. <p> Although both approaches provide global address space to the application, the performance characteristics of the systems are significantly different. More importantly, SVM based systems, especially those that use hardware support, tend to be more scalable than SMP systems because they have no/fewer centralized resources that limit scalability <ref> [HLH92, RSRM93] </ref>.
Reference: [HM93] <author> A. Hosking and E. Moss. </author> <title> Object Fault Handling for Persistent Programming Languages: A Performance Evaluation. </title> <booktitle> In Proceedings of the International Conference on Object-Oriented Programming Systems, Languages, and Applications, </booktitle> <year> 1993. </year>
Reference-contexts: This technique has been used in Cricket [SZ90] which uses Mach's external pagers, the ObjectStore OODBMS [LLOW91], the Texas Persistent Store [SKW92], and others <ref> [HM93, RLPG95, WD94] </ref>. 3 Using MMSM in the SS Architecture In the SS architecture the database is partitioned across the nodes. Therefore, when MMSM is used in the SS architecture (SS-MMSM), the shared virtual memory is partitioned among all the nodes in the system.
Reference: [IBM95] <institution> IBM. DB2 Administration Guide. Toronto, </institution> <note> version 2.1 - s20h-4580-01 edition, </note> <year> 1995. </year>
Reference-contexts: Storing data on raw disk partitions significantly increases the complexity of the DBMS but correspondingly allows the DBMS to exercise greater control and provide better 21 performance. It has been suggested in commercial database system manuals and performance tuning guides <ref> [IBM95, Cor91, Rod90] </ref> that data requiring high-performance access be placed on the raw disk partitions and data such as catalogs and secondary indexes be placed in files managed by the operating system.
Reference: [J + 94] <author> H. V. Jagadish et al. </author> <title> Dali: A High Performance Main Memory Storage Manager. </title> <booktitle> In Proceedings of the VLDB Conference, </booktitle> <month> September </month> <year> 1994. </year>
Reference-contexts: Much of the memory-mapped storage management work reported in this paper is based on these discussions and has been adapted to currently available operating system support. Dali <ref> [J + 94] </ref> is a main-memory storage manager that maps the database of interest, in its entirety, into virtual memory. However, in Dali it is assumed that any database mapped into virtual memory can be entirely memory-resident. The Dali storage architecture views the mapped database as a single heap. <p> The advantages of structuring the database in such a manner have been described in the literature <ref> [GL92, J + 94, LSC92] </ref>. It has been shown by Lehman et al. [LSC92] that for data already in memory, a memory-resident database can process queries between 1.5 to 5 times faster than a disk-resident database.
Reference: [L + 88] <author> S. J. Leffler et al. </author> <title> The Design and Implementation of the 4.3BSD UNIX Operating System. </title> <publisher> Addison Wesley, </publisher> <year> 1988. </year>
Reference-contexts: The features we have discussed above are widely supported by current systems. Memory-mapping and memory-residence manipulation is supported by most Unix-based operating systems through system calls such as mmap, mlock, munlock, msync <ref> [L + 88] </ref>. In addition, processors and systems that support wide address spaces are becoming increasingly available (e.g. DEC-Alpha, PA-RISC, PowerPC). Next, we describe an implementation of the MMSM using standard operating system support. The MMSM allows transactions to read from and write to the database using virtual memory addresses. <p> On the other hand, the buffer manager uses read, write, lseek, fsync system calls to provide access to the database. We have implemented two versions of BM one that uses the Unix file system interface to the disk and the other uses the Unix raw disk partition interface <ref> [L + 88] </ref> (referred to in the following as BM-F and BM-R, respectively). <p> [Cor91] the I/O performance difference between BM-R and BM-F could be around 50%, the same range as we observe through our experiments. 5.3 Discussion Most Unix DBMS (e.g., DB2, Informix, Oracle, Sybase) allow the database to be stored as part of the Unix file system or, on raw disk partitions <ref> [Bac86, L + 88] </ref> that are explicitly managed by the DBMS or, on both. Storing data on raw disk partitions significantly increases the complexity of the DBMS but correspondingly allows the DBMS to exercise greater control and provide better 21 performance.
Reference: [LH89] <author> K. Li and P. Hudak. </author> <title> Memory Coherence in Shared Virtual Memory Systems. </title> <journal> ACM TOCS, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: 1 Introduction Parallel systems that support a shared address space allow database systems to better utilize the system resources. The shared address space can be supported either through shared physical memory or through shared virtual memory (SVM) <ref> [LH89, HLH92] </ref>. Although both approaches provide global address space to the application, the performance characteristics of the systems are significantly different.
Reference: [LLOW91] <author> C. Lamb, G. Landis, J. Orenstein, and D. Weinreb. </author> <title> The ObjectStore Database System. </title> <journal> Communications of the ACM, </journal> <volume> 34(10) </volume> <pages> 50-63, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: This technique has been used in Cricket [SZ90] which uses Mach's external pagers, the ObjectStore OODBMS <ref> [LLOW91] </ref>, the Texas Persistent Store [SKW92], and others [HM93, RLPG95, WD94]. 3 Using MMSM in the SS Architecture In the SS architecture the database is partitioned across the nodes.
Reference: [LSC92] <author> T. Lehman, E. Shekita, and L. Cabrera. </author> <title> An Evaluation of the Starburst Memory-Resident Storage Component. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <month> December </month> <year> 1992. </year>
Reference-contexts: In fact, transactions do not even have to interact with the storage manager for read accesses. * Reduces the concurrent data structures that have to be maintained by the storage manager and thus reduces the concurrency overhead for transactions <ref> [LSC92] </ref>. * Allows the DBMS to structure the data as if it were memory-resident since all data can be accessed through unique virtual addresses. The advantages of structuring the database in such a manner have been described in the literature [GL92, J + 94, LSC92]. <p> The advantages of structuring the database in such a manner have been described in the literature <ref> [GL92, J + 94, LSC92] </ref>. It has been shown by Lehman et al. [LSC92] that for data already in memory, a memory-resident database can process queries between 1.5 to 5 times faster than a disk-resident database. <p> The advantages of structuring the database in such a manner have been described in the literature [GL92, J + 94, LSC92]. It has been shown by Lehman et al. <ref> [LSC92] </ref> that for data already in memory, a memory-resident database can process queries between 1.5 to 5 times faster than a disk-resident database.
Reference: [RLPG95] <author> B. Reinwald, T. J. Lehman, H. Pirahesh, and V. Gottemukkala. </author> <title> Storing C++ Objects in Relational Databases. </title> <note> submitted for publication, </note> <month> February </month> <year> 1995. </year>
Reference-contexts: This technique has been used in Cricket [SZ90] which uses Mach's external pagers, the ObjectStore OODBMS [LLOW91], the Texas Persistent Store [SKW92], and others <ref> [HM93, RLPG95, WD94] </ref>. 3 Using MMSM in the SS Architecture In the SS architecture the database is partitioned across the nodes. Therefore, when MMSM is used in the SS architecture (SS-MMSM), the shared virtual memory is partitioned among all the nodes in the system.
Reference: [Rod90] <author> U. Rodgers. </author> <title> Unix Database Management Systems. </title> <publisher> Yourdon Press, </publisher> <year> 1990. </year>
Reference-contexts: Storing data on raw disk partitions significantly increases the complexity of the DBMS but correspondingly allows the DBMS to exercise greater control and provide better 21 performance. It has been suggested in commercial database system manuals and performance tuning guides <ref> [IBM95, Cor91, Rod90] </ref> that data requiring high-performance access be placed on the raw disk partitions and data such as catalogs and secondary indexes be placed in files managed by the operating system.
Reference: [RSRM93] <author> U. Ramachandran, G. Shah, S. Ravikumar, and J. Muthukumarasamy. </author> <title> Scalability Study of the KSR-1. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <year> 1993. </year> <month> 24 </month>
Reference-contexts: Although both approaches provide global address space to the application, the performance characteristics of the systems are significantly different. More importantly, SVM based systems, especially those that use hardware support, tend to be more scalable than SMP systems because they have no/fewer centralized resources that limit scalability <ref> [HLH92, RSRM93] </ref>.
Reference: [S + 95] <author> A. Saulsbury et al. </author> <title> An Argument for Simple COMA. </title> <booktitle> In Proceedings of High Perfor--mance Computer Architectures - 1, </booktitle> <month> January </month> <year> 1995. </year>
Reference-contexts: In recent years there has been a great deal of focus in designing and building scalable SVM systems <ref> [S + 95, TP96, B + 92, SN95] </ref>. Database systems are the primary commercial applications for large scale parallel systems. Our research focus in in designing parallel database systems that are highly scalable and efficient in resource utilization. <p> However, the key difference between the two schemes is that in the SS-MMSM scheme there is no central server that knows the system-wide caching status of all the data, but is distributed <ref> [B + 92, S + 95] </ref>. In the scheme used by Franklin et al., if a client buffer manager does not find the requested data in its cache it always contacts the server.
Reference: [Sch90] <author> H. Schwetman. </author> <title> CSIM Users' Guide, </title> <month> March </month> <year> 1990. </year>
Reference-contexts: This reduction in I/O is the primary benefit of using MMSM in the SS architecture. 3.1 Evaluation of MMSM in the SS architecture 3.1.1 Experimental model We have developed a process-oriented simulation model, based on CSIM <ref> [Sch90] </ref>, to model and evaluate the SS and SN architectures. In this section we first present the simulation model, and our assumptions about the database and workload models. In the simulation model, the activities in the system (e.g., transaction generation, transactions, processing daemons) are modeled as processes.
Reference: [SKW92] <author> V. Singhal, S. V. Kakkad, and P. R. Wilson. </author> <title> Texas: An Efficient, Portable Persistent Store. </title> <booktitle> In Proceedings of the International Workshop on Persistent Object Systems, </booktitle> <month> September </month> <year> 1992. </year>
Reference-contexts: This technique has been used in Cricket [SZ90] which uses Mach's external pagers, the ObjectStore OODBMS [LLOW91], the Texas Persistent Store <ref> [SKW92] </ref>, and others [HM93, RLPG95, WD94]. 3 Using MMSM in the SS Architecture In the SS architecture the database is partitioned across the nodes. Therefore, when MMSM is used in the SS architecture (SS-MMSM), the shared virtual memory is partitioned among all the nodes in the system.
Reference: [SN93] <author> A. Shatdal and J. F. Naughton. </author> <title> Using Shared Virtual Memory for Parallel Join Processing. </title> <booktitle> In Proceedings of the 1993 ACM SIGMOD Conference, </booktitle> <pages> pages 119-128, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Bellew et al. [BHT90] have used SVM to share data between clients in a client-server database system. The thrust of this work is efficient coherence maintenance and concurrency control. Shatdal and Naughton <ref> [SN93] </ref> assume a multiprocessor 7 system with a software SVM layer to enhance join strategies that efficiently handle data skew. However, neither of these studies have examined the use of SVM for better memory utilization.
Reference: [SN95] <author> A. Saulsbury and A. Nowatzyk. </author> <title> Simple COMA on S3.MP. </title> <booktitle> In Proceedings of the ISCA Shared Memory Workshop, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: In recent years there has been a great deal of focus in designing and building scalable SVM systems <ref> [S + 95, TP96, B + 92, SN95] </ref>. Database systems are the primary commercial applications for large scale parallel systems. Our research focus in in designing parallel database systems that are highly scalable and efficient in resource utilization.
Reference: [SZ90] <author> E. Shekita and M. Zwilling. Cricket: </author> <title> A Mapped, Persistent Object Store. </title> <booktitle> In Proceedings of the Fourth International Workshop on Persistent Object Systems, </booktitle> <pages> pages 85-98, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: This technique has been used in Cricket <ref> [SZ90] </ref> which uses Mach's external pagers, the ObjectStore OODBMS [LLOW91], the Texas Persistent Store [SKW92], and others [HM93, RLPG95, WD94]. 3 Using MMSM in the SS Architecture In the SS architecture the database is partitioned across the nodes.
Reference: [TP96] <author> J. Torrellas and D. Padua. </author> <title> The Illinois Aggressive Coma Multiprocessor Project (I-ACOMA). </title> <booktitle> In Proceedings of the 6th Symposium on the Frontiers of Massively Parallel Computing, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: In recent years there has been a great deal of focus in designing and building scalable SVM systems <ref> [S + 95, TP96, B + 92, SN95] </ref>. Database systems are the primary commercial applications for large scale parallel systems. Our research focus in in designing parallel database systems that are highly scalable and efficient in resource utilization.
Reference: [Tra82] <author> I. L. Traiger. </author> <title> Virtual Memory Management for Database Systems. </title> <journal> Operating Systems Review, </journal> <volume> 16(4), </volume> <month> October </month> <year> 1982. </year>
Reference-contexts: All of the global memory management is done by the DBMS/storage manager in software. Over the years there has been some interest in exploiting the underlying operating system's virtual memory manager to perform most of the database storage management. Traiger <ref> [Tra82] </ref> presents a detailed discussion on the operating system support required to implement a transactional storage manager using virtual memory mapping. The author has studied the variations required to support different recovery protocols.
Reference: [WD94] <author> S. J. White and D. J. DeWitt. </author> <title> QuickStore: A High Performance Mapped Object Store. </title> <booktitle> In Proceedings of the ACM SIGMOD Conference, </booktitle> <month> May </month> <year> 1994. </year> <month> 25 </month>
Reference-contexts: This technique has been used in Cricket [SZ90] which uses Mach's external pagers, the ObjectStore OODBMS [LLOW91], the Texas Persistent Store [SKW92], and others <ref> [HM93, RLPG95, WD94] </ref>. 3 Using MMSM in the SS Architecture In the SS architecture the database is partitioned across the nodes. Therefore, when MMSM is used in the SS architecture (SS-MMSM), the shared virtual memory is partitioned among all the nodes in the system.
References-found: 29

