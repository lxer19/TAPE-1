URL: http://www.icsi.berkeley.edu/~amnon/Papers/ext_sur.ps
Refering-URL: http://www.icsi.berkeley.edu/~amnon/
Root-URL: http://www.icsi.berkeley.edu
Title: Extracting Randomness A Survey and New Constructions  
Author: Noam Nisan Amnon Ta-Shma 
Abstract: In this paper we do two things. First, we survey extractors and dispersers: what they are, how they can be designed, and some of their applications. The work described in the survey is due to a long list of research papers by various authors most notably by David Zuckerman. Then, we present a new tool for constructing explicit extractors, and give two new constructions that greatly improve upon previous results. The new tool we devise, a "merger", is a function that accepts d strings, one of which is uniformly distributed, and outputs a single string that is guaranteed to be uniformly distributed. We show how to build good explicit mergers, and how mergers can be used to build better extractors. Using this, we present two new constructions. The first construction succeeds in extracting all of the randomness from any somewhat random source. This improves upon previous extractors that extract only some of the randomness from somewhat random sources with "enough" randomness. The amount of truly random bits used by this extractor, however, is not optimal. The second extractor we build extracts only some of the randomness, and works only for sources with enough randomness, but uses a near-optimal amount of truly random bits. Extractors and dispersers have many applications in "removing randomness" in various settings, and in making randomized constructions explicit. We survey some of these applications, and note whenever our new constructions yield better results, e.g., plugging our new extractors into a previous construction we achieve the first explicit N - superconcentrators of linear size and polyloglog(N ) depth. fl This paper is a combination of the paper "On Extracting Randomness From Weak Random Sources" [Ta-96] and the paper "Refining Randomness: Why and How" [Nis96]. This work was supported by BSF grant 92-00043 and by a Wolfeson award administered by the Israeli Academy of Sciences. y Institute of computer science, Hebrew University, Jerusalem, Israel z Institute of computer science, Hebrew University, Jerusalem, Israel 
Abstract-found: 1
Intro-found: 1
Reference: [ACRT97] <author> A. E. Andreev, A. E.F. Clementi, J. D.P. Rolim, and L. Trevisan. </author> <title> Weak random sources, hitting sets, and bpp simulations. Technical report, </title> <booktitle> Electronic Colloquium on Computational Complexity, </booktitle> <year> 1997. </year>
Reference-contexts: A variable that appears twice (or more) in the same expression has the same value in all occurrences, i.e. 4 Previous result [SZ94] was for ffi &gt; 1=2, and required n O (log (n)) time. Recently an optimal poly (n) simu lation was presented in <ref> [ACRT97] </ref> using different techniques. 8 A ffi A denotes a random variable with values a ffi a.
Reference: [AGHP92] <author> Alon, Goldreich, Hastad, and Peralta. </author> <title> Simple constructions of almost k-wise independent random variables. Random Structures & Algorithms, </title> <type> 3, </type> <year> 1992. </year>
Reference-contexts: This construction had d = polylog (n) for k n=polylog (n). An efficient extractor working for small k's, k = fi (log (n)), was obtained by [GW94, SZ94] using tiny families of hash functions <ref> [NN93, AGHP92] </ref>. This was used in [SZ94] to improve upon the [NZ93] extractor, and get it work for any k &gt; p n. In [SSZ95] a disperser with d = O (log n) was obtained for any k = n (1) .
Reference: [AKS87] <author> Ajtai, Komlos, and Szemeredi. </author> <title> Deterministic simulation in LOGSPACE. </title> <booktitle> In ACM Symposium on Theory of Computing (STOC), </booktitle> <year> 1987. </year>
Reference-contexts: This problem, known as the "deterministic amplification" problem, was extensively studied [CG89, IZ89, CW89]. Using expanders, this can be done using only n + O (t) random bits 6 Previous result [SZ94] was for ffi &gt; 1=2, and required n O (log (n)) time. 39 <ref> [AKS87, IZ89, CW89] </ref>. Sipser [Sip88] defined dispersers as a tool which implies stronger RP amplification. Using extractors, we obtain BP P amplification. <p> The generator can use any good extractor for high min-entropies, and our new constructions do not improve its operation. The following theorem of [NZ93] improves on previous results of <ref> [AKS87] </ref>. 43 Theorem: [NZ93] There exists a pseudo-random generator which converts O (S) truly ran-dom bits into poly (S) bits which look random to any algorithm which runs in space S. The generator runs in O (S) space and poly (S) time.
Reference: [AKSS89] <author> M. Ajtai, J. Komlos, W. Steiger, and E. Szemeredi. </author> <title> Almost sorting in one round. </title> <booktitle> In Advances in Computer Research, </booktitle> <volume> volume 5, </volume> <pages> pages 117-125, </pages> <year> 1989. </year>
Reference-contexts: This corollary has applications on sorting [Pip87a, WZ93] and selecting <ref> [AKSS89, WZ93] </ref> in k rounds. Corollary 1.2 There are explicit algorithms for sorting in k rounds using O (n 1+ 1 k 2 polyloglog (n) ) comparisons, and for selecting in k rounds using O (n 1+ 1 2 k 1 2 polyloglog (n) ) comparisons.
Reference: [ALM + 92] <author> S. Arora, C. Lund, R. Motwani, M. Sudan, and M. Szegedy. </author> <title> Proof verification and hardness of approximation problems. </title> <booktitle> In Proceedings of the 33rd Annual IEEE Symposium on the Foundations of Computer Science, IEEE, </booktitle> <pages> pages 14-23, </pages> <year> 1992. </year>
Reference-contexts: For the precise statement see [Zuc93, SZ94]. Our new construction can derandomize Zuckerman's result. The proof itself builds on the known hardness results for approximating clique <ref> [AS92b, ALM + 92, FGL + 91] </ref>, and is best understood when viewing it as deterministic amplification for P CP systems. 6.2.4 Time vs. Space Sipser [Sip88] defined dispersers in order to obtain the following theorem (which became a theorem only with the recent constructions of [SSZ95]).
Reference: [AS92a] <author> N. Alon and J. H. Spencer. </author> <title> The Probabilistic Method. </title> <publisher> John Wiley and Sons, </publisher> <year> 1992. </year>
Reference-contexts: The "probabilistic method" is many times used to non-constructively prove the existence of these sought after objects. Again, in many cases it is known how to "derandomize" these probabilistic proofs, and achieve an explicit construction. We refer the reader to <ref> [AS92a] </ref> for a survey of the probabilistic method. Derandomization Techniques It is possible to roughly categorize the techniques used for derandomization according to their generality. On one extreme are techniques which relate very strongly to the problem and algorithm at hand. <p> Pairwise (and k-wise) independence and Hashing. 2. Small Bias Spaces. 3. Expanders. 4. Extractors and Dispersers. We refer the reader, again, to <ref> [AS92a, MR95] </ref> for further information as well as for references. Dispersers and Extractors In this paper we deal with the fourth general type of tool: a family of graphs called Dispersers and Extractors.
Reference: [AS92b] <author> S. Arora and S. Safra. </author> <title> Probabilistic checking of proofs; a new characterization of NP. </title> <booktitle> In Proceedings of the 33rd Annual IEEE Symposium on the Foundations of Computer Science, </booktitle> <pages> pages 2-13, </pages> <year> 1992. </year>
Reference-contexts: For the precise statement see [Zuc93, SZ94]. Our new construction can derandomize Zuckerman's result. The proof itself builds on the known hardness results for approximating clique <ref> [AS92b, ALM + 92, FGL + 91] </ref>, and is best understood when viewing it as deterministic amplification for P CP systems. 6.2.4 Time vs. Space Sipser [Sip88] defined dispersers in order to obtain the following theorem (which became a theorem only with the recent constructions of [SSZ95]).
Reference: [Blu86] <author> M. Blum. </author> <title> Independent unbiased coin flips from a correlated biased source: a finite markov chain. </title> <journal> Combinatorica, </journal> <volume> 6(2) </volume> <pages> 97-108, </pages> <year> 1986. </year>
Reference-contexts: A natural idea is to, deterministically, convert this source into truly random bits. For certain types of sources this can indeed be done. E.g. <ref> [Blu86] </ref> shows how it can be done if the source is a (known) Markov chain. For more general sources it can be shown that this cannot 37 be done [SV86]. Instead, we may use the somewhat random source to indirectly simulate a given randomized algorithm.
Reference: [BR94] <author> Bellare and Rompel. </author> <title> Randomness-efficient oblivious sampling. </title> <booktitle> In IEEE Symposium on Foundations of Computer Science (FOCS), </booktitle> <year> 1994. </year>
Reference-contexts: These are sampling procedures that give a good estimate for the expected value of a real valued function on some finite domain. Definition 6.2 <ref> [BR94] </ref> An oblivious (ffi; *)-sampler is a deterministic function that for each x 2 [N ] produces a set (x) = fz 1 :::z D g [M ] such that for every function f : [M ] ! [0; 1], (where [0; 1] is the real interval between 0 and 1), <p> Oblivious samplers were constructed in <ref> [BR94] </ref> who used them for interactive proof systems. The best known results to date, which use extractors, appear in [Zuc]. Theorem: There exist explicitly constructible (ffi; *) oblivious samplers where ffi = 2K=N .
Reference: [CG88] <author> B. Chor and O. Goldreich. </author> <title> Unbiased bits from sources of weak randomness and probabilistic communication complexity. </title> <journal> SIAM Journal on Computing, </journal> <volume> 17(2) </volume> <pages> 230-261, </pages> <year> 1988. </year>
Reference-contexts: The roots of the research on extractors lie mostly in the work on "somewhat random sources" done in the late 1980's, 5 by Vazirani, Santha and Vazirani, Vazirani and Vazirani, Chor and Goldreich, and others <ref> [SV86, Vaz87a, Vaz86, Vaz87b, VV85, CG88] </ref>. The direct development of the constructions and applications of extractors and dispersers came first in papers written by Zuckerman in the early 1990's [Zuc90, Zuc91], and then in a sequence of papers by various authors [NZ93, WZ93, SZ94, SSZ95, Zuc93, Zuc]. <p> If they are allowed to be correlated then the previous statement may not hold. The following key definition, first formulated in <ref> [CG88] </ref>, turns out to suffice instead of total independence. Definition 3.4 Let X 1 ; X 2 be (possibly correlated) random variables taking values, respectively, in [N 1 ] and [N 2 ]. <p> Thus, the requirement that is needed for the output to be close to uniform is: Definition 3.5 (extending <ref> [CG88] </ref>) Let B 1 ; :::; B t be correlated random variables taking values, respectively, in [N 1 ]; :::; [N t ]. <p> A useful way to view this is that G 1 ffi ffi G t multiplies the number of random bits by the product of the "randomness-multiplying" capabilities of each extractor. Of course, this multiplication requires a block-wise source, and not just any source with enough min-entropy. Lemma 3.7 <ref> [CG88, NZ93] </ref> Let X = X 1 ffi X 2 : : : ffi X t be a (k 1 ; : : : ; k t ) block-wise source to within * where k t = (log (n)) and k i1 = c tiny k i . <p> Several models of "somewhat random" sources were considered in the literature. Santha and Vazirani [SV86], Vazirani [Vaz87a, Vaz86, Vaz87b] and Vazirani and Vazirani [VV85] studied a class of sources, called "slightly random sources", and showed that BP P can be simulated given such a source. Chor and Goldreich <ref> [CG88] </ref> generalized this model, and showed that BP P can be simulated even using the more general source. Many authors studied other restricted classes of random sources (e.g. [CW89, CGH + 85, LLS89]). Finally, Zuckerman [Zuc90] suggested a general model generalizing all the previous models.
Reference: [CG89] <author> Chor and Goldreich. </author> <title> On the power of two-point based sampling. </title> <journal> Journal of Complexity, </journal> <volume> 5, </volume> <year> 1989. </year>
Reference-contexts: We want to achieve this using as few random bits as possible. This problem, known as the "deterministic amplification" problem, was extensively studied <ref> [CG89, IZ89, CW89] </ref>. Using expanders, this can be done using only n + O (t) random bits 6 Previous result [SZ94] was for ffi &gt; 1=2, and required n O (log (n)) time. 39 [AKS87, IZ89, CW89]. Sipser [Sip88] defined dispersers as a tool which implies stronger RP amplification.
Reference: [CGH + 85] <author> B. Chor, O. Goldreich, J. Hastad, J. Friedman, S. Rudich, and R. Smolensky. </author> <title> The bit extraction problem and t-resilient functions. </title> <booktitle> In Proceedings of the 26th Annual IEEE Symposium on the Foundations of Computer Science, </booktitle> <pages> pages 396-407, </pages> <year> 1985. </year>
Reference-contexts: Chor and Goldreich [CG88] generalized this model, and showed that BP P can be simulated even using the more general source. Many authors studied other restricted classes of random sources (e.g. <ref> [CW89, CGH + 85, LLS89] </ref>). Finally, Zuckerman [Zuc90] suggested a general model generalizing all the previous models. His model was simply all random sources having high min-entropy. Later [Zuc91] he showed that BP P can be simulated given such a source (with enough min-entropy).
Reference: [CW89] <author> A. Cohen and A. Wigderson. Dispersers, </author> <title> deterministic amplification, and weak random sources. </title> <booktitle> In Proceedings of the 30th Annual IEEE Symposium on the Foundations of Computer Science, </booktitle> <pages> pages 14-19, </pages> <year> 1989. </year>
Reference-contexts: Chor and Goldreich [CG88] generalized this model, and showed that BP P can be simulated even using the more general source. Many authors studied other restricted classes of random sources (e.g. <ref> [CW89, CGH + 85, LLS89] </ref>). Finally, Zuckerman [Zuc90] suggested a general model generalizing all the previous models. His model was simply all random sources having high min-entropy. Later [Zuc91] he showed that BP P can be simulated given such a source (with enough min-entropy). <p> We want to achieve this using as few random bits as possible. This problem, known as the "deterministic amplification" problem, was extensively studied <ref> [CG89, IZ89, CW89] </ref>. Using expanders, this can be done using only n + O (t) random bits 6 Previous result [SZ94] was for ffi &gt; 1=2, and required n O (log (n)) time. 39 [AKS87, IZ89, CW89]. Sipser [Sip88] defined dispersers as a tool which implies stronger RP amplification. <p> This problem, known as the "deterministic amplification" problem, was extensively studied [CG89, IZ89, CW89]. Using expanders, this can be done using only n + O (t) random bits 6 Previous result [SZ94] was for ffi &gt; 1=2, and required n O (log (n)) time. 39 <ref> [AKS87, IZ89, CW89] </ref>. Sipser [Sip88] defined dispersers as a tool which implies stronger RP amplification. Using extractors, we obtain BP P amplification.
Reference: [FGL + 91] <author> U. Feige, S. Goldwasser, L. Lovasz, S. Safra, and M. Szegedy. </author> <title> Approximating clique is almost NP-complete. </title> <booktitle> In Proceedings of the 32nd Annual IEEE Symposium on the Foundations of Computer Science, IEEE, </booktitle> <pages> pages 2-12, </pages> <year> 1991. </year> <month> 45 </month>
Reference-contexts: For the precise statement see [Zuc93, SZ94]. Our new construction can derandomize Zuckerman's result. The proof itself builds on the known hardness results for approximating clique <ref> [AS92b, ALM + 92, FGL + 91] </ref>, and is best understood when viewing it as deterministic amplification for P CP systems. 6.2.4 Time vs. Space Sipser [Sip88] defined dispersers in order to obtain the following theorem (which became a theorem only with the recent constructions of [SSZ95]).
Reference: [GG81] <author> Gabber and Galil. </author> <title> Explicit constructions of linear-sized superconcentrators. </title> <journal> Jour--nal of Computer and System Sciences, </journal> <volume> 22, </volume> <year> 1981. </year>
Reference-contexts: Gabber and Galil <ref> [GG81] </ref> constructed, using expanders, the first explicit linear size super concentrators. The graph they construct has O (log (N )) depth. For depth 2, non-constructive proofs show that super concentrators of depth 2 and size O (N log 2 (N )) exists [Pip82].
Reference: [GW94] <author> O. Goldreich and A. Wigderson. </author> <title> Tiny families of functions with random properties: A quality-size trade-off for hashing. </title> <booktitle> In Proceedings of the 26th Annual ACM Symposium on the Theory of Computing, ACM, </booktitle> <pages> pages 574-583, </pages> <year> 1994. </year>
Reference-contexts: The first explicit construction of extractors came in [NZ93], and relied on techniques developed in [Zuc90, Zuc91]. This construction had d = polylog (n) for k n=polylog (n). An efficient extractor working for small k's, k = fi (log (n)), was obtained by <ref> [GW94, SZ94] </ref> using tiny families of hash functions [NN93, AGHP92]. This was used in [SZ94] to improve upon the [NZ93] extractor, and get it work for any k &gt; p n. In [SSZ95] a disperser with d = O (log n) was obtained for any k = n (1) . <p> On the other hand, our purposes do not quite require 0-collision error but only small collision error. This will allow us to reduce the size of the family of hash functions, and hence, reduce the extractor degree. It turns out that "tiny families of hash functions" <ref> [SZ94, GW94] </ref> can be built using small-biased distributions. Lemma 3.2 [SZ94, GW94] For all 1 L N (L = 2 l ; N = 2 n ) and * &gt; 0, there exist explicit families H of hash functions from [N ] to [L] with * collision error, and size jHj <p> This will allow us to reduce the size of the family of hash functions, and hence, reduce the extractor degree. It turns out that "tiny families of hash functions" <ref> [SZ94, GW94] </ref> can be built using small-biased distributions. Lemma 3.2 [SZ94, GW94] For all 1 L N (L = 2 l ; N = 2 n ) and * &gt; 0, there exist explicit families H of hash functions from [N ] to [L] with * collision error, and size jHj = poly (n; *; L).
Reference: [ILL89] <author> R. Impagliazzo, L. Levin, and M. Luby. </author> <title> Pseudo-random generation from one-way functions. </title> <booktitle> In Proceedings of the 21st Annual ACM Symposium on the Theory of Computing, ACM, </booktitle> <pages> pages 12-24, </pages> <year> 1989. </year>
Reference-contexts: The following lemma is a variant of the of leftover hash lemma of Impagliazzo, Levin, and Luby <ref> [ILL89] </ref>, stated in our terms: Lemma 3.1 [ILL89] Let H be a family of hash functions from [N ] to [L] with collision error ffi. Then, the extractor defined from H is a (k; *)-extractor for K = 2 k = O (L=ffi) and * = O ( ffi). <p> The following lemma is a variant of the of leftover hash lemma of Impagliazzo, Levin, and Luby <ref> [ILL89] </ref>, stated in our terms: Lemma 3.1 [ILL89] Let H be a family of hash functions from [N ] to [L] with collision error ffi. Then, the extractor defined from H is a (k; *)-extractor for K = 2 k = O (L=ffi) and * = O ( ffi).
Reference: [IZ89] <author> R. Impagliazzo and D. Zuckerman. </author> <title> How to recycle random bits. </title> <booktitle> In Proceedings of the 30th Annual IEEE Symposium on the Foundations of Computer Science, IEEE, </booktitle> <pages> pages 248-253, </pages> <year> 1989. </year>
Reference-contexts: We want to achieve this using as few random bits as possible. This problem, known as the "deterministic amplification" problem, was extensively studied <ref> [CG89, IZ89, CW89] </ref>. Using expanders, this can be done using only n + O (t) random bits 6 Previous result [SZ94] was for ffi &gt; 1=2, and required n O (log (n)) time. 39 [AKS87, IZ89, CW89]. Sipser [Sip88] defined dispersers as a tool which implies stronger RP amplification. <p> This problem, known as the "deterministic amplification" problem, was extensively studied [CG89, IZ89, CW89]. Using expanders, this can be done using only n + O (t) random bits 6 Previous result [SZ94] was for ffi &gt; 1=2, and required n O (log (n)) time. 39 <ref> [AKS87, IZ89, CW89] </ref>. Sipser [Sip88] defined dispersers as a tool which implies stronger RP amplification. Using extractors, we obtain BP P amplification.
Reference: [LLS89] <author> Lichtenstein, Linial, and Saks. </author> <title> Some extremal problems arising from discrete control processes. </title> <journal> Combinatorica, </journal> <volume> 9, </volume> <year> 1989. </year>
Reference-contexts: Chor and Goldreich [CG88] generalized this model, and showed that BP P can be simulated even using the more general source. Many authors studied other restricted classes of random sources (e.g. <ref> [CW89, CGH + 85, LLS89] </ref>). Finally, Zuckerman [Zuc90] suggested a general model generalizing all the previous models. His model was simply all random sources having high min-entropy. Later [Zuc91] he showed that BP P can be simulated given such a source (with enough min-entropy).
Reference: [Mes84] <author> R. Meshulam. </author> <title> A geometric construction of a superconcentrator of depth 2. </title> <journal> Theoretical Computer Science, </journal> <volume> 32 </volume> <pages> 215-219, </pages> <year> 1984. </year>
Reference-contexts: Gabber and Galil [GG81] constructed, using expanders, the first explicit linear size super concentrators. The graph they construct has O (log (N )) depth. For depth 2, non-constructive proofs show that super concentrators of depth 2 and size O (N log 2 (N )) exists [Pip82]. Meshulam <ref> [Mes84] </ref> showed an explicit depth 2, super concentrator of size O (N 1+1=2 ). [WZ93] give a construction based on extractors. <p> The work done in Section 4 imply depth 2 super concentrators of size N 2 polyloglog (N) . Proof: Meshulam <ref> [Mes84] </ref> showed that H is a super concentrator of depth 2, iff for any 1 K 0 N and any two sets W I,Z O of size K 0 each, there are at least K 0 common neighbors. We will build a depth 2 graph with this property.
Reference: [MR95] <author> Rajeev Motwani and Prabhakar Raghavan. </author> <title> Randomized Algorithms. </title> <publisher> Cambridge University Press, </publisher> <year> 1995. </year>
Reference-contexts: There are many examples of randomized algorithms for various problems which are better than any known deterministic algorithm for the problem. The randomized algorithms may be faster, more space-efficient, use less communication, allow parallelization, or may be simply simpler than the deterministic counterparts. We refer the reader e.g. to <ref> [MR95] </ref> for a textbook on randomized algorithms. Despite the wide spread use of randomized algorithms, in almost all cases it is not at all clear whether randomization is really necessary. <p> Pairwise (and k-wise) independence and Hashing. 2. Small Bias Spaces. 3. Expanders. 4. Extractors and Dispersers. We refer the reader, again, to <ref> [AS92a, MR95] </ref> for further information as well as for references. Dispersers and Extractors In this paper we deal with the fourth general type of tool: a family of graphs called Dispersers and Extractors.
Reference: [Nis96] <author> N. Nisan. </author> <title> Refining randomness: Why and how. </title> <booktitle> In Annual Conference on Structure in Complexity Theory, </booktitle> <year> 1996. </year>
Reference: [NN93] <author> Naor and Naor. </author> <title> Small-bias probability spaces: Efficient constructions and applications. </title> <journal> SIAM Journal on Computing, </journal> <volume> 22, </volume> <year> 1993. </year>
Reference-contexts: This construction had d = polylog (n) for k n=polylog (n). An efficient extractor working for small k's, k = fi (log (n)), was obtained by [GW94, SZ94] using tiny families of hash functions <ref> [NN93, AGHP92] </ref>. This was used in [SZ94] to improve upon the [NZ93] extractor, and get it work for any k &gt; p n. In [SSZ95] a disperser with d = O (log n) was obtained for any k = n (1) .
Reference: [NZ93] <author> N. Nisan and D. Zuckerman. </author> <title> More deterministic simulation in logspace. </title> <booktitle> In Proceedings of the 25th Annual ACM Symposium on the Theory of Computing, ACM, </booktitle> <pages> pages 235-244, </pages> <year> 1993. </year>
Reference-contexts: Survey of Previous Results Dispersers were first defined (with somewhat different parameters) by Sipser [Sip88], while extractors were defined by Nisan and Zuckerman <ref> [NZ93] </ref>. The roots of the research on extractors lie mostly in the work on "somewhat random sources" done in the late 1980's, 5 by Vazirani, Santha and Vazirani, Vazirani and Vazirani, Chor and Goldreich, and others [SV86, Vaz87a, Vaz86, Vaz87b, VV85, CG88]. <p> The direct development of the constructions and applications of extractors and dispersers came first in papers written by Zuckerman in the early 1990's [Zuc90, Zuc91], and then in a sequence of papers by various authors <ref> [NZ93, WZ93, SZ94, SSZ95, Zuc93, Zuc] </ref>. The first explicit construction of extractors came in [NZ93], and relied on techniques developed in [Zuc90, Zuc91]. This construction had d = polylog (n) for k n=polylog (n). <p> The direct development of the constructions and applications of extractors and dispersers came first in papers written by Zuckerman in the early 1990's [Zuc90, Zuc91], and then in a sequence of papers by various authors [NZ93, WZ93, SZ94, SSZ95, Zuc93, Zuc]. The first explicit construction of extractors came in <ref> [NZ93] </ref>, and relied on techniques developed in [Zuc90, Zuc91]. This construction had d = polylog (n) for k n=polylog (n). An efficient extractor working for small k's, k = fi (log (n)), was obtained by [GW94, SZ94] using tiny families of hash functions [NN93, AGHP92]. <p> This construction had d = polylog (n) for k n=polylog (n). An efficient extractor working for small k's, k = fi (log (n)), was obtained by [GW94, SZ94] using tiny families of hash functions [NN93, AGHP92]. This was used in [SZ94] to improve upon the <ref> [NZ93] </ref> extractor, and get it work for any k &gt; p n. In [SSZ95] a disperser with d = O (log n) was obtained for any k = n (1) . In [Zuc], d = O (log n) was obtained for k = (n). <p> H 1 (X ) = k as a generalization of being uniform over a set of size 2 k . 2.2 Extractors and Dispersers Extractors and dispersers are very similar to each other, yet, in the literature, dispersers have been usually defined as graphs [Sip88, SSZ95], while extractors as functions <ref> [NZ93, SZ94, Zuc] </ref>. We will define both extractors and dispersers both as functions and as graphs, taking the view that it is the same combinatorial object, viewed in two different, useful, ways. Graph Definitions Extractors and dispersers are certain types of bipartite (multi-)graphs. <p> Remark 2.1 This definition is slightly different from the one in <ref> [NZ93, SZ94] </ref>, who require that the random choice of the edge originating at x is is also almost independent from (X ). The difference is minor, though, and we prefer this definition. <p> This calculation was done, for certain dispersers, in [Sip88]. For tight bounds see [RTS]. Lower Bounds A trivial lower bound, for any n; m and * &lt; 1=2, is d m k 1. We will mostly consider the case where k m so this does not help us. In <ref> [NZ93] </ref> a lower bound of d min (m; (log (n k) + log * 1 )) was proved for all n; m; k n 1; * &lt; 1=2. <p> The lemma follows. Notice how we overcome the delicate issue of independence. Because this question is so central to our discussion we formalize this in: Lemma 3.6 <ref> [NZ93] </ref> Let X and Y be two correlated random variables. Let B be a distribution, and call an x "bad" if (Y j X = x) is not * close to B. If Pr x2X (x is bad) then X ffi Y is * + close to X fi B. <p> A useful way to view this is that G 1 ffi ffi G t multiplies the number of random bits by the product of the "randomness-multiplying" capabilities of each extractor. Of course, this multiplication requires a block-wise source, and not just any source with enough min-entropy. Lemma 3.7 <ref> [CG88, NZ93] </ref> Let X = X 1 ffi X 2 : : : ffi X t be a (k 1 ; : : : ; k t ) block-wise source to within * where k t = (log (n)) and k i1 = c tiny k i . <p> What we need is for B to have enough min-entropy, even though it is much shorter than the original string (so that enough min-entropy is left for further blocks.) In <ref> [NZ93] </ref> it is shown that choosing a subset of the bits of X in a pairwise independent way suffices for this. Other ways of sampling a subset of the bits of X behave similarly. <p> Given an n bit string x 1 :::x n , and a subset S f1:::ng, denote x S to be the string obtained by concatenating the bits x i for all i 2 S (in the natural order). Lemma 3.8 <ref> [NZ93] </ref> For every distribution X and for for almost all choices of S, the distribution of X S is close to some distribution W with H 1 (W ) ~ ( l n H 1 (X)). <p> The actual proof of this lemma proceeds by essentially carrying out the above argument separately for each possible value of x, and then combining all the x's back together. Unfor tunately, this argument is quite cumbersome and delicate. We summarize this as: Lemma 3.9 <ref> [NZ93, SZ94] </ref> Let X be a random source over f0; 1g n . <p> Actually, as long as l 1 + : : : + l i1 &lt;< m, we can go on and extract another block B i , which has high min-entropy even conditioned on the history. Thus we get: Lemma 3.10 <ref> [NZ93] </ref> Let X be a distribution on [N ] with H 1 (X) k. <p> Let us see several ways by which these ingredients can be put together in order to get good extractors. 17 We first discuss the basic strategy used in <ref> [Zuc90, Zuc91, NZ93, SZ94] </ref> and present it essentially in the form found in [SZ94]. Choose t = O (log n) blocks each of size O (k= log n) as described in Section 3.3. This gives a (near) block-wise source. <p> We also saw (Corollary 4.4) that it is enough to find a (k; *) extractor E : (2k) fi (d) 7! (m) with m very close to k. To be more specific, we need m k O ( k log (n) ). By <ref> [NZ93, SZ94, Zuc] </ref> we know to find such extractors with m = (k). Using a simple idea due to Wigderson and Zuckerman [WZ93] we can get m much closer to k. <p> We show that combining the extractor of Theorem 6 with the <ref> [NZ93] </ref> block extractor, we can extract randomness from sources having n 1 min-entropy using less random bits. The idea behind the construction is the following: since the given source X has H 1 (X) n 1 , we can use the [NZ93] block extraction to extract t = O (log (f <p> that combining the extractor of Theorem 6 with the <ref> [NZ93] </ref> block extractor, we can extract randomness from sources having n 1 min-entropy using less random bits. The idea behind the construction is the following: since the given source X has H 1 (X) n 1 , we can use the [NZ93] block extraction to extract t = O (log (f (n))) blocks that together form a block-wise source with each block containing some n (1) min-entropy. Then, by investing O (log (n)) bits, we can extract some log (n) 2 (t) = log (n) f (n) random bits. <p> The generator can use any good extractor for high min-entropies, and our new constructions do not improve its operation. The following theorem of <ref> [NZ93] </ref> improves on previous results of [AKS87]. 43 Theorem: [NZ93] There exists a pseudo-random generator which converts O (S) truly ran-dom bits into poly (S) bits which look random to any algorithm which runs in space S. The generator runs in O (S) space and poly (S) time. <p> The generator can use any good extractor for high min-entropies, and our new constructions do not improve its operation. The following theorem of <ref> [NZ93] </ref> improves on previous results of [AKS87]. 43 Theorem: [NZ93] There exists a pseudo-random generator which converts O (S) truly ran-dom bits into poly (S) bits which look random to any algorithm which runs in space S. The generator runs in O (S) space and poly (S) time.
Reference: [Pip77] <author> Pippenger. </author> <title> Superconcentrators. </title> <journal> SICOMP: SIAM Journal on Computing, </journal> <year> 1977. </year>
Reference-contexts: In these cases, the "random-like" properties of extractors and dispersers suffice as a replacement for using random graphs, and thus convert a non-constructive proof to a construction. 6.3.1 Super Concentrators Definition 6.3 <ref> [Pip77] </ref> Let H = (V; E) be a directed graph with a specified subset I V of nodes called input nodes, and a disjoint subset, O V , called output nodes. Assume jIj = jOj = N .
Reference: [Pip82] <author> Pippenger. </author> <title> Superconcentrators of depth 2. </title> <journal> JCSS: Journal of Computer and System Sciences, </journal> <volume> 24, </volume> <year> 1982. </year>
Reference-contexts: Gabber and Galil [GG81] constructed, using expanders, the first explicit linear size super concentrators. The graph they construct has O (log (N )) depth. For depth 2, non-constructive proofs show that super concentrators of depth 2 and size O (N log 2 (N )) exists <ref> [Pip82] </ref>. Meshulam [Mes84] showed an explicit depth 2, super concentrator of size O (N 1+1=2 ). [WZ93] give a construction based on extractors.
Reference: [Pip87a] <author> N. Pippenger. </author> <title> Sorting and selecting in rounds. </title> <journal> SIAM Journal on Computing, </journal> <volume> 16 </volume> <pages> 1032-1038, </pages> <year> 1987. </year>
Reference-contexts: This corollary has applications on sorting <ref> [Pip87a, WZ93] </ref> and selecting [AKSS89, WZ93] in k rounds. Corollary 1.2 There are explicit algorithms for sorting in k rounds using O (n 1+ 1 k 2 polyloglog (n) ) comparisons, and for selecting in k rounds using O (n 1+ 1 2 k 1 2 polyloglog (n) ) comparisons.
Reference: [Pip87b] <author> N. Pippenger. </author> <title> Sorting and selecting in rounds. </title> <journal> SIAM Journal on Computing, </journal> <volume> 16 </volume> <pages> 1032-1038, </pages> <year> 1987. </year>
Reference-contexts: The number of edges is bounded from above D 2DN=M . Using the extractor of Section 4: Corollary 6.5 For any N and 1 a N there is an explicitly constructible a-expanding graph with N vertices, and maximum degree O ( N a 2 polyloglog (N) ) . Pippenger <ref> [Pip87b] </ref> showed that good explicit highly expanding graphs, yield good algo rithms for "sorting in rounds" and for "selecting in rounds". 6.4 Pseudo-random Generators Extractors can also be used to construct pseudo-random generators which fool certain classes of algorithms.
Reference: [RTS] <author> J. Radhakrishnan and A. Ta-Shma. </author> <title> Tight bounds for depth-two superconcentra-tors. </title> <note> To appear in FOCS 1997. </note>
Reference-contexts: This calculation was done, for certain dispersers, in [Sip88]. For tight bounds see <ref> [RTS] </ref>. Lower Bounds A trivial lower bound, for any n; m and * &lt; 1=2, is d m k 1. We will mostly consider the case where k m so this does not help us. <p> In [NZ93] a lower bound of d min (m; (log (n k) + log * 1 )) was proved for all n; m; k n 1; * &lt; 1=2. Better (and tight) lower bounds can be found in <ref> [RTS] </ref>. 11 3 Survey of Previous Constructions In this section we survey the main ingredients used in previous constructions and indicate how they are put together. We describe the constructions precisely, but provide only sketches of proofs for their validity.
Reference: [Sip88] <author> Sipser. Expanders, </author> <title> randomness, or time versus space. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 36, </volume> <year> 1988. </year> <month> 46 </month>
Reference-contexts: Survey of Previous Results Dispersers were first defined (with somewhat different parameters) by Sipser <ref> [Sip88] </ref>, while extractors were defined by Nisan and Zuckerman [NZ93]. <p> think of a distribution X with H 1 (X ) = k as a generalization of being uniform over a set of size 2 k . 2.2 Extractors and Dispersers Extractors and dispersers are very similar to each other, yet, in the literature, dispersers have been usually defined as graphs <ref> [Sip88, SSZ95] </ref>, while extractors as functions [NZ93, SZ94, Zuc]. We will define both extractors and dispersers both as functions and as graphs, taking the view that it is the same combinatorial object, viewed in two different, useful, ways. Graph Definitions Extractors and dispersers are certain types of bipartite (multi-)graphs. <p> This calculation was done, for certain dispersers, in <ref> [Sip88] </ref>. For tight bounds see [RTS]. Lower Bounds A trivial lower bound, for any n; m and * &lt; 1=2, is d m k 1. We will mostly consider the case where k m so this does not help us. <p> This problem, known as the "deterministic amplification" problem, was extensively studied [CG89, IZ89, CW89]. Using expanders, this can be done using only n + O (t) random bits 6 Previous result [SZ94] was for ffi &gt; 1=2, and required n O (log (n)) time. 39 [AKS87, IZ89, CW89]. Sipser <ref> [Sip88] </ref> defined dispersers as a tool which implies stronger RP amplification. Using extractors, we obtain BP P amplification. <p> Our new construction can derandomize Zuckerman's result. The proof itself builds on the known hardness results for approximating clique [AS92b, ALM + 92, FGL + 91], and is best understood when viewing it as deterministic amplification for P CP systems. 6.2.4 Time vs. Space Sipser <ref> [Sip88] </ref> defined dispersers in order to obtain the following theorem (which became a theorem only with the recent constructions of [SSZ95]). Again, the dispersers are needed for deterministic amplification. The [SSZ95] dispersers are good enough, and our new constructions do not improve this result.
Reference: [SSZ95] <author> M. Saks, A. Srinivasan, and S. Zhou. </author> <title> Explicit dispersers with polylog degree. </title> <booktitle> In Proceedings of the 26th Annual ACM Symposium on the Theory of Computing, ACM, </booktitle> <year> 1995. </year>
Reference-contexts: The direct development of the constructions and applications of extractors and dispersers came first in papers written by Zuckerman in the early 1990's [Zuc90, Zuc91], and then in a sequence of papers by various authors <ref> [NZ93, WZ93, SZ94, SSZ95, Zuc93, Zuc] </ref>. The first explicit construction of extractors came in [NZ93], and relied on techniques developed in [Zuc90, Zuc91]. This construction had d = polylog (n) for k n=polylog (n). <p> An efficient extractor working for small k's, k = fi (log (n)), was obtained by [GW94, SZ94] using tiny families of hash functions [NN93, AGHP92]. This was used in [SZ94] to improve upon the [NZ93] extractor, and get it work for any k &gt; p n. In <ref> [SSZ95] </ref> a disperser with d = O (log n) was obtained for any k = n (1) . In [Zuc], d = O (log n) was obtained for k = (n). <p> think of a distribution X with H 1 (X ) = k as a generalization of being uniform over a set of size 2 k . 2.2 Extractors and Dispersers Extractors and dispersers are very similar to each other, yet, in the literature, dispersers have been usually defined as graphs <ref> [Sip88, SSZ95] </ref>, while extractors as functions [NZ93, SZ94, Zuc]. We will define both extractors and dispersers both as functions and as graphs, taking the view that it is the same combinatorial object, viewed in two different, useful, ways. Graph Definitions Extractors and dispersers are certain types of bipartite (multi-)graphs. <p> Next we shortly mention a different idea which works even for smaller values of k. An Alternative Method This alternative method for effectively getting a block-wise source was introduced in <ref> [SSZ95] </ref> and will be further extended in Section 4. The intuition can again be best obtained by considering the entropy H (X) instead of the min-entropy. <p> However, it turns out that for dispersers we can simply try all the choices, and combine them together (i.e. take the union of edges that each choice implies). As long as t is relatively small, the total number of choices is manageable. Two more tricks can be used. In <ref> [SSZ95] </ref> a "universal" set of choices for i 1 ; : : :; i t with smaller (for the interesting cases polynomial) size is presented, thus achieving good dispersers (for certain ranges of parameters). <p> A similar argument for BPP uses an extractor. With current constructions, we get a polynomial time (in n) simulation (i.e. D = poly (n)) for RP as long as H 1 (X) n ffi for any ffi &gt; 0 <ref> [SSZ95] </ref>; a polynomial time simulation for BP P as long as H 1 (X) = (n) (or even slightly less) [Zuc93]; and using our new results presented in Section 5, a slightly quasi-polynomial (n log (c) n for any constant c) time simulation for BP P for any H 1 (X): <p> Space Sipser [Sip88] defined dispersers in order to obtain the following theorem (which became a theorem only with the recent constructions of <ref> [SSZ95] </ref>). Again, the dispersers are needed for deterministic amplification. The [SSZ95] dispersers are good enough, and our new constructions do not improve this result. <p> Space Sipser [Sip88] defined dispersers in order to obtain the following theorem (which became a theorem only with the recent constructions of <ref> [SSZ95] </ref>). Again, the dispersers are needed for deterministic amplification. The [SSZ95] dispersers are good enough, and our new constructions do not improve this result.
Reference: [SV86] <author> M. Santha and U. Vazirani. </author> <title> Generating quasi-random sequences from slightly random sources. </title> <journal> J. of Computer and System Sciences, </journal> <volume> 33 </volume> <pages> 75-87, </pages> <year> 1986. </year>
Reference-contexts: The roots of the research on extractors lie mostly in the work on "somewhat random sources" done in the late 1980's, 5 by Vazirani, Santha and Vazirani, Vazirani and Vazirani, Chor and Goldreich, and others <ref> [SV86, Vaz87a, Vaz86, Vaz87b, VV85, CG88] </ref>. The direct development of the constructions and applications of extractors and dispersers came first in papers written by Zuckerman in the early 1990's [Zuc90, Zuc91], and then in a sequence of papers by various authors [NZ93, WZ93, SZ94, SSZ95, Zuc93, Zuc]. <p> For certain types of sources this can indeed be done. E.g. [Blu86] shows how it can be done if the source is a (known) Markov chain. For more general sources it can be shown that this cannot 37 be done <ref> [SV86] </ref>. Instead, we may use the somewhat random source to indirectly simulate a given randomized algorithm. <p> Clearly, physicists should try to produce physical source where this property is as strong as possible, while computer scientists should aim to rely on a property which is as week as possible. Several models of "somewhat random" sources were considered in the literature. Santha and Vazirani <ref> [SV86] </ref>, Vazirani [Vaz87a, Vaz86, Vaz87b] and Vazirani and Vazirani [VV85] studied a class of sources, called "slightly random sources", and showed that BP P can be simulated given such a source.
Reference: [SZ94] <author> A. Srinivasan and D. Zuckerman. </author> <title> Computing with very weak random sources. </title> <booktitle> In Proceedings of the 35th Annual IEEE Symposium on the Foundations of Computer Science, </booktitle> <year> 1994. </year>
Reference-contexts: The direct development of the constructions and applications of extractors and dispersers came first in papers written by Zuckerman in the early 1990's [Zuc90, Zuc91], and then in a sequence of papers by various authors <ref> [NZ93, WZ93, SZ94, SSZ95, Zuc93, Zuc] </ref>. The first explicit construction of extractors came in [NZ93], and relied on techniques developed in [Zuc90, Zuc91]. This construction had d = polylog (n) for k n=polylog (n). <p> The first explicit construction of extractors came in [NZ93], and relied on techniques developed in [Zuc90, Zuc91]. This construction had d = polylog (n) for k n=polylog (n). An efficient extractor working for small k's, k = fi (log (n)), was obtained by <ref> [GW94, SZ94] </ref> using tiny families of hash functions [NN93, AGHP92]. This was used in [SZ94] to improve upon the [NZ93] extractor, and get it work for any k &gt; p n. In [SSZ95] a disperser with d = O (log n) was obtained for any k = n (1) . <p> This construction had d = polylog (n) for k n=polylog (n). An efficient extractor working for small k's, k = fi (log (n)), was obtained by [GW94, SZ94] using tiny families of hash functions [NN93, AGHP92]. This was used in <ref> [SZ94] </ref> to improve upon the [NZ93] extractor, and get it work for any k &gt; p n. In [SSZ95] a disperser with d = O (log n) was obtained for any k = n (1) . In [Zuc], d = O (log n) was obtained for k = (n). <p> The previous upper bound <ref> [WZ93, SZ94] </ref> was O ( N a 2 log (N) 1=2+o (1) 2 This improves the previous upper bound of O (N 2 log (N) 1=2+o (1) ) achieved using the [WZ93] technique and the [SZ94] extractor. 3 This improves the previous upper bound of O (log (N) 1=2+o (1) ). <p> The previous upper bound [WZ93, SZ94] was O ( N a 2 log (N) 1=2+o (1) 2 This improves the previous upper bound of O (N 2 log (N) 1=2+o (1) ) achieved using the [WZ93] technique and the <ref> [SZ94] </ref> extractor. 3 This improves the previous upper bound of O (log (N) 1=2+o (1) ). 7 We can also prove a deterministic version of the hardness of approximating the iterated log of MaxClique. See [Zuc93] for more details. <p> We denote by A ffi B the concatenation of two random variables A and B. A variable that appears twice (or more) in the same expression has the same value in all occurrences, i.e. 4 Previous result <ref> [SZ94] </ref> was for ffi &gt; 1=2, and required n O (log (n)) time. Recently an optimal poly (n) simu lation was presented in [ACRT97] using different techniques. 8 A ffi A denotes a random variable with values a ffi a. <p> H 1 (X ) = k as a generalization of being uniform over a set of size 2 k . 2.2 Extractors and Dispersers Extractors and dispersers are very similar to each other, yet, in the literature, dispersers have been usually defined as graphs [Sip88, SSZ95], while extractors as functions <ref> [NZ93, SZ94, Zuc] </ref>. We will define both extractors and dispersers both as functions and as graphs, taking the view that it is the same combinatorial object, viewed in two different, useful, ways. Graph Definitions Extractors and dispersers are certain types of bipartite (multi-)graphs. <p> Remark 2.1 This definition is slightly different from the one in <ref> [NZ93, SZ94] </ref>, who require that the random choice of the edge originating at x is is also almost independent from (X ). The difference is minor, though, and we prefer this definition. <p> On the other hand, our purposes do not quite require 0-collision error but only small collision error. This will allow us to reduce the size of the family of hash functions, and hence, reduce the extractor degree. It turns out that "tiny families of hash functions" <ref> [SZ94, GW94] </ref> can be built using small-biased distributions. Lemma 3.2 [SZ94, GW94] For all 1 L N (L = 2 l ; N = 2 n ) and * &gt; 0, there exist explicit families H of hash functions from [N ] to [L] with * collision error, and size jHj <p> This will allow us to reduce the size of the family of hash functions, and hence, reduce the extractor degree. It turns out that "tiny families of hash functions" <ref> [SZ94, GW94] </ref> can be built using small-biased distributions. Lemma 3.2 [SZ94, GW94] For all 1 L N (L = 2 l ; N = 2 n ) and * &gt; 0, there exist explicit families H of hash functions from [N ] to [L] with * collision error, and size jHj = poly (n; *; L). <p> A useful way to view this is that we manage to multiply the number of truly random bits from d to m = (1 + c)d for some constant c &gt; 0, using a source with enough min-entropy to supply this increase. Lemma 3.4 <ref> [SZ94] </ref> There is some constant c &gt; 1 s.t. for any k = (log (n)) there is an explicit (2k; 2 k=5 ) extractor A k : (n) fi (k) 7! (ck). <p> The actual proof of this lemma proceeds by essentially carrying out the above argument separately for each possible value of x, and then combining all the x's back together. Unfor tunately, this argument is quite cumbersome and delicate. We summarize this as: Lemma 3.9 <ref> [NZ93, SZ94] </ref> Let X be a random source over f0; 1g n . <p> Let us see several ways by which these ingredients can be put together in order to get good extractors. 17 We first discuss the basic strategy used in <ref> [Zuc90, Zuc91, NZ93, SZ94] </ref> and present it essentially in the form found in [SZ94]. Choose t = O (log n) blocks each of size O (k= log n) as described in Section 3.3. This gives a (near) block-wise source. <p> Let us see several ways by which these ingredients can be put together in order to get good extractors. 17 We first discuss the basic strategy used in [Zuc90, Zuc91, NZ93, SZ94] and present it essentially in the form found in <ref> [SZ94] </ref>. Choose t = O (log n) blocks each of size O (k= log n) as described in Section 3.3. This gives a (near) block-wise source. We now compose t extractors each which multiplies the number of random bits by a constant factor as presented in Section 3.1. <p> All together this gives d = O (log 2 n). As remarked in Section 3.3. this can work as long as k = H 1 (X) ~ ( n) (which also reflects in the bound m = ~ (k 2 =n)). Thus, we get: Lemma 3.11 <ref> [SZ94] </ref> 5 Let k (n) n 1=2+fl for some constant fl &gt; 0, then for any * there is an explicit (k (n); *) extractor E : (n) fi (O (log 2 n log ( 1 * ))) 7! ( n ). <p> For k = H 1 (X) = (n), we can choose the sizes of the t blocks more carefully, making sure that the final block has length (n), and thus we can extract m = (n) bits. Reducing d We now show a simple idea from <ref> [SZ94] </ref>, showing that by composing two of the extractors just designed we can reduce d even further. <p> The error parameter * of this construction depends on the smaller block and is 1=poly (n 0 ), which is larger than 1=poly (n). By carefully controlling the composition of many extractors of differing lengths, <ref> [SZ94, Zuc] </ref> show that d = O (log n + log * 1 ) suffices for any * as long as k = (n) (and even slightly less). A different way of decreasing d is given in the construction of our second extractor in Section 5. <p> These m 2 bits are independent from the first m 1 bits since, even conditioned on 5 The parameters here are simplified. The real parameters appearing in <ref> [SZ94] </ref> are somewhat better. 18 any particular value of the first m 1 bits, X (almost always) still has minentropy of at least k m 1 , and thus the extractor produces random bits. This idea is implicit in [WZ93]. <p> We also saw (Corollary 4.4) that it is enough to find a (k; *) extractor E : (2k) fi (d) 7! (m) with m very close to k. To be more specific, we need m k O ( k log (n) ). By <ref> [NZ93, SZ94, Zuc] </ref> we know to find such extractors with m = (k). Using a simple idea due to Wigderson and Zuckerman [WZ93] we can get m much closer to k. <p> Then for any k there is an explicit (k; poly (n) *) extractor E : (n) fi (O ( k log (n) log ( 1 * ) + log 2 (n) d)) 7! Mergers That Do Not Lose Much Min-entropy The <ref> [SZ94] </ref> extractor of Lemma 3.11 works for any source with H 1 (X) n 1=2+fl . Thus, using Lemma 4.6 by repeatedly using the [SZ94] extractor, we can extract at least n 2 n 1=2+fl quasi-random bits from a source having H 1 (X) n 2 . <p> (n) fi (O ( k log (n) log ( 1 * ) + log 2 (n) d)) 7! Mergers That Do Not Lose Much Min-entropy The <ref> [SZ94] </ref> extractor of Lemma 3.11 works for any source with H 1 (X) n 1=2+fl . Thus, using Lemma 4.6 by repeatedly using the [SZ94] extractor, we can extract at least n 2 n 1=2+fl quasi-random bits from a source having H 1 (X) n 2 . Thus, we have a 2-merger that does not lose much randomness in the merging process. Applying Theorem 5 we get a good n-merger. <p> Notice that Theorem 5 and Corollary 4.10 take advantage of the simple structure of somewhere random sources, giving us an explicit somewhere random merger that works even for sources with very small min-entropy to which the <ref> [SZ94] </ref> extractor of Lemma 3.11 does not apply. <p> The extractor B in Corollary 4.11 uses O (2 p log (n) polylog (n) log ( 1 * )) truly random bits to extract all the randomness in the given source. Although 2 p log (n) is quite a large amount of truly random bits, we can use the <ref> [SZ94] </ref> extractor to extract n 1=3 bits from n 2=3 min entropy, and then use these n 1=3 &gt;> O (2 p log (n) polylog (n) log ( 1 * )) bits to further extract all the remaining min-entropy. <p> The extractor works only for sources having at least n (1) min-entropy, and extracts only some small fraction of the min-entropy present in the original source. Yet, this extractor improves upon the previous construction of <ref> [SZ94] </ref> in two ways: first it uses much less truly random bits (almost linear), and second, it works for sources having less than n 1=2 min-entropy. <p> We want to achieve this using as few random bits as possible. This problem, known as the "deterministic amplification" problem, was extensively studied [CG89, IZ89, CW89]. Using expanders, this can be done using only n + O (t) random bits 6 Previous result <ref> [SZ94] </ref> was for ffi &gt; 1=2, and required n O (log (n)) time. 39 [AKS87, IZ89, CW89]. Sipser [Sip88] defined dispersers as a tool which implies stronger RP amplification. Using extractors, we obtain BP P amplification. <p> In fact Zuckerman showed that, for any constant j, approximating the j'th iterated log of clique to within any constant is ~ N P -hard. For the precise statement see <ref> [Zuc93, SZ94] </ref>. Our new construction can derandomize Zuckerman's result. The proof itself builds on the known hardness results for approximating clique [AS92b, ALM + 92, FGL + 91], and is best understood when viewing it as deterministic amplification for P CP systems. 6.2.4 Time vs.
Reference: [Ta-96] <author> Ta-Shma. </author> <title> On extracting randomness from weak random sources. </title> <booktitle> In ACM Symposium on Theory of Computing (STOC), </booktitle> <year> 1996. </year>
Reference: [Vaz86] <author> U. Vazirani. </author> <title> Randomness, Adversaries and Computation. </title> <type> PhD thesis, </type> <institution> University of California, Berkeley, </institution> <year> 1986. </year>
Reference-contexts: The roots of the research on extractors lie mostly in the work on "somewhat random sources" done in the late 1980's, 5 by Vazirani, Santha and Vazirani, Vazirani and Vazirani, Chor and Goldreich, and others <ref> [SV86, Vaz87a, Vaz86, Vaz87b, VV85, CG88] </ref>. The direct development of the constructions and applications of extractors and dispersers came first in papers written by Zuckerman in the early 1990's [Zuc90, Zuc91], and then in a sequence of papers by various authors [NZ93, WZ93, SZ94, SSZ95, Zuc93, Zuc]. <p> Clearly, physicists should try to produce physical source where this property is as strong as possible, while computer scientists should aim to rely on a property which is as week as possible. Several models of "somewhat random" sources were considered in the literature. Santha and Vazirani [SV86], Vazirani <ref> [Vaz87a, Vaz86, Vaz87b] </ref> and Vazirani and Vazirani [VV85] studied a class of sources, called "slightly random sources", and showed that BP P can be simulated given such a source. Chor and Goldreich [CG88] generalized this model, and showed that BP P can be simulated even using the more general source.
Reference: [Vaz87a] <author> U. Vazirani. </author> <title> Efficiency considerations in using semi-random sources. </title> <booktitle> In Proceedings of the 19th Annual ACM Symposium on the Theory of Computing, ACM, </booktitle> <pages> pages 160-168, </pages> <year> 1987. </year>
Reference-contexts: The roots of the research on extractors lie mostly in the work on "somewhat random sources" done in the late 1980's, 5 by Vazirani, Santha and Vazirani, Vazirani and Vazirani, Chor and Goldreich, and others <ref> [SV86, Vaz87a, Vaz86, Vaz87b, VV85, CG88] </ref>. The direct development of the constructions and applications of extractors and dispersers came first in papers written by Zuckerman in the early 1990's [Zuc90, Zuc91], and then in a sequence of papers by various authors [NZ93, WZ93, SZ94, SSZ95, Zuc93, Zuc]. <p> Clearly, physicists should try to produce physical source where this property is as strong as possible, while computer scientists should aim to rely on a property which is as week as possible. Several models of "somewhat random" sources were considered in the literature. Santha and Vazirani [SV86], Vazirani <ref> [Vaz87a, Vaz86, Vaz87b] </ref> and Vazirani and Vazirani [VV85] studied a class of sources, called "slightly random sources", and showed that BP P can be simulated given such a source. Chor and Goldreich [CG88] generalized this model, and showed that BP P can be simulated even using the more general source.
Reference: [Vaz87b] <author> U. Vazirani. </author> <title> Strong communication complexity or generating quasi-random sequences from two communicating semi-random sources. </title> <journal> Combinatorica, </journal> <volume> 7(4) </volume> <pages> 375-392, </pages> <year> 1987. </year>
Reference-contexts: The roots of the research on extractors lie mostly in the work on "somewhat random sources" done in the late 1980's, 5 by Vazirani, Santha and Vazirani, Vazirani and Vazirani, Chor and Goldreich, and others <ref> [SV86, Vaz87a, Vaz86, Vaz87b, VV85, CG88] </ref>. The direct development of the constructions and applications of extractors and dispersers came first in papers written by Zuckerman in the early 1990's [Zuc90, Zuc91], and then in a sequence of papers by various authors [NZ93, WZ93, SZ94, SSZ95, Zuc93, Zuc]. <p> Clearly, physicists should try to produce physical source where this property is as strong as possible, while computer scientists should aim to rely on a property which is as week as possible. Several models of "somewhat random" sources were considered in the literature. Santha and Vazirani [SV86], Vazirani <ref> [Vaz87a, Vaz86, Vaz87b] </ref> and Vazirani and Vazirani [VV85] studied a class of sources, called "slightly random sources", and showed that BP P can be simulated given such a source. Chor and Goldreich [CG88] generalized this model, and showed that BP P can be simulated even using the more general source.
Reference: [VV85] <author> U. Vazirani and V. Vazirani. </author> <title> Random polynomial time is equal to slightly-random polynomial time. </title> <booktitle> In Proceedings of the 26th Annual IEEE Symposium on the Foundations of Computer Science, IEEE, </booktitle> <pages> pages 417-428, </pages> <year> 1985. </year>
Reference-contexts: The roots of the research on extractors lie mostly in the work on "somewhat random sources" done in the late 1980's, 5 by Vazirani, Santha and Vazirani, Vazirani and Vazirani, Chor and Goldreich, and others <ref> [SV86, Vaz87a, Vaz86, Vaz87b, VV85, CG88] </ref>. The direct development of the constructions and applications of extractors and dispersers came first in papers written by Zuckerman in the early 1990's [Zuc90, Zuc91], and then in a sequence of papers by various authors [NZ93, WZ93, SZ94, SSZ95, Zuc93, Zuc]. <p> Several models of "somewhat random" sources were considered in the literature. Santha and Vazirani [SV86], Vazirani [Vaz87a, Vaz86, Vaz87b] and Vazirani and Vazirani <ref> [VV85] </ref> studied a class of sources, called "slightly random sources", and showed that BP P can be simulated given such a source. Chor and Goldreich [CG88] generalized this model, and showed that BP P can be simulated even using the more general source.
Reference: [WZ93] <author> A. Wigderson and D. Zuckerman. </author> <title> Expanders that beat the eigenvalue bound: Explicit construction and applications. </title> <booktitle> In Proceedings of the 25th Annual ACM Symposium on the Theory of Computing, ACM, </booktitle> <pages> pages 245-251, </pages> <year> 1993. </year>
Reference-contexts: The direct development of the constructions and applications of extractors and dispersers came first in papers written by Zuckerman in the early 1990's [Zuc90, Zuc91], and then in a sequence of papers by various authors <ref> [NZ93, WZ93, SZ94, SSZ95, Zuc93, Zuc] </ref>. The first explicit construction of extractors came in [NZ93], and relied on techniques developed in [Zuc90, Zuc91]. This construction had d = polylog (n) for k n=polylog (n). <p> Our new constructions improve some of them. In the following we state the results we improve. Formal definitions and proofs are given in Section 6. The first application is constructing explicit a-expanding graphs, obtained by plugging our first extractor into the <ref> [WZ93] </ref> construction: Corollary 1.1 For any N and 1 a N there is an explicitly constructible a-expanding graph with N vertices, and maximum degree O ( N a 2 polyloglog (N ) ) 1 . This corollary has applications on sorting [Pip87a, WZ93] and selecting [AKSS89, WZ93] in k rounds. <p> This corollary has applications on sorting <ref> [Pip87a, WZ93] </ref> and selecting [AKSS89, WZ93] in k rounds. Corollary 1.2 There are explicit algorithms for sorting in k rounds using O (n 1+ 1 k 2 polyloglog (n) ) comparisons, and for selecting in k rounds using O (n 1+ 1 2 k 1 2 polyloglog (n) ) comparisons. <p> This corollary has applications on sorting [Pip87a, WZ93] and selecting <ref> [AKSS89, WZ93] </ref> in k rounds. Corollary 1.2 There are explicit algorithms for sorting in k rounds using O (n 1+ 1 k 2 polyloglog (n) ) comparisons, and for selecting in k rounds using O (n 1+ 1 2 k 1 2 polyloglog (n) ) comparisons. <p> Another corollary is for the construction of explicit small-depth superconcentrators. It is again obtained by plugging our first extractor into a previous construction by <ref> [WZ93] </ref>: Corollary 1.4 For every N there is an efficiently constructible depth 2 superconcentrator over N vertices with size O (N 2 polyloglog (N ) ). 2 Wigderson and Zuckerman [WZ93] prove that a direct corollary of this is: Corollary 1.5 For any N there is an explicitly constructible superconcentrator over <p> It is again obtained by plugging our first extractor into a previous construction by <ref> [WZ93] </ref>: Corollary 1.4 For every N there is an efficiently constructible depth 2 superconcentrator over N vertices with size O (N 2 polyloglog (N ) ). 2 Wigderson and Zuckerman [WZ93] prove that a direct corollary of this is: Corollary 1.5 For any N there is an explicitly constructible superconcentrator over N vertices, with linear size and polyloglog (N) depth 3 . 1 The obvious lower bound is N a . <p> The previous upper bound <ref> [WZ93, SZ94] </ref> was O ( N a 2 log (N) 1=2+o (1) 2 This improves the previous upper bound of O (N 2 log (N) 1=2+o (1) ) achieved using the [WZ93] technique and the [SZ94] extractor. 3 This improves the previous upper bound of O (log (N) 1=2+o (1) ). <p> The previous upper bound [WZ93, SZ94] was O ( N a 2 log (N) 1=2+o (1) 2 This improves the previous upper bound of O (N 2 log (N) 1=2+o (1) ) achieved using the <ref> [WZ93] </ref> technique and the [SZ94] extractor. 3 This improves the previous upper bound of O (log (N) 1=2+o (1) ). 7 We can also prove a deterministic version of the hardness of approximating the iterated log of MaxClique. See [Zuc93] for more details. <p> The real parameters appearing in [SZ94] are somewhat better. 18 any particular value of the first m 1 bits, X (almost always) still has minentropy of at least k m 1 , and thus the extractor produces random bits. This idea is implicit in <ref> [WZ93] </ref>. The above idea uses d 1 + d 2 bits to extract m 1 + m 2 bits and can of course be repeated until k P m i is too small. <p> To be more specific, we need m k O ( k log (n) ). By [NZ93, SZ94, Zuc] we know to find such extractors with m = (k). Using a simple idea due to Wigderson and Zuckerman <ref> [WZ93] </ref> we can get m much closer to k. More Bits Using The Same Extractor Suppose we have an extractor E that extracts randomness from any source having at least k min-entropy. <p> More Bits Using The Same Extractor Suppose we have an extractor E that extracts randomness from any source having at least k min-entropy. How much randomness can we extract from sources having K min-entropy when K &gt;> k ? The following algorithm is implicit in <ref> [WZ93] </ref>: use the same extractor E many times over the same string x, each time with a fresh truly random string r i , until you get K k output bits. <p> The graph they construct has O (log (N )) depth. For depth 2, non-constructive proofs show that super concentrators of depth 2 and size O (N log 2 (N )) exists [Pip82]. Meshulam [Mes84] showed an explicit depth 2, super concentrator of size O (N 1+1=2 ). <ref> [WZ93] </ref> give a construction based on extractors. Theorem: (following [WZ93]) There is an explicit super concentrator of depth 2 and size N O ( k=1 D k 2 k =M k ), where D k ; M k are the parameters for dispersers with K = 2 k . <p> For depth 2, non-constructive proofs show that super concentrators of depth 2 and size O (N log 2 (N )) exists [Pip82]. Meshulam [Mes84] showed an explicit depth 2, super concentrator of size O (N 1+1=2 ). <ref> [WZ93] </ref> give a construction based on extractors. Theorem: (following [WZ93]) There is an explicit super concentrator of depth 2 and size N O ( k=1 D k 2 k =M k ), where D k ; M k are the parameters for dispersers with K = 2 k . <p> We would like to explicitly build K-expanding graphs with degree as close as possible to N=K. The eigenvalue methods for constructing expanders give such expanders for K p give anything for smaller values of K. <ref> [WZ93] </ref> show how dispersers can be used to construct expanders with small values of K. Theorem: (following [WZ93]) There are constructible K-expanding graph with N vertices, and maximum degree O (N D 2 =M ). <p> The eigenvalue methods for constructing expanders give such expanders for K p give anything for smaller values of K. <ref> [WZ93] </ref> show how dispersers can be used to construct expanders with small values of K. Theorem: (following [WZ93]) There are constructible K-expanding graph with N vertices, and maximum degree O (N D 2 =M ). Using the extractors of Section 4 we get maximum degree of N K exp (polyloglogN ) for any value of K.
Reference: [Zuc] <author> D. Zuckerman. </author> <title> Randomness-optimal sampling, extractors, and constructive leader election. </title> <type> Private Communication. </type>
Reference-contexts: The direct development of the constructions and applications of extractors and dispersers came first in papers written by Zuckerman in the early 1990's [Zuc90, Zuc91], and then in a sequence of papers by various authors <ref> [NZ93, WZ93, SZ94, SSZ95, Zuc93, Zuc] </ref>. The first explicit construction of extractors came in [NZ93], and relied on techniques developed in [Zuc90, Zuc91]. This construction had d = polylog (n) for k n=polylog (n). <p> This was used in [SZ94] to improve upon the [NZ93] extractor, and get it work for any k &gt; p n. In [SSZ95] a disperser with d = O (log n) was obtained for any k = n (1) . In <ref> [Zuc] </ref>, d = O (log n) was obtained for k = (n). <p> H 1 (X ) = k as a generalization of being uniform over a set of size 2 k . 2.2 Extractors and Dispersers Extractors and dispersers are very similar to each other, yet, in the literature, dispersers have been usually defined as graphs [Sip88, SSZ95], while extractors as functions <ref> [NZ93, SZ94, Zuc] </ref>. We will define both extractors and dispersers both as functions and as graphs, taking the view that it is the same combinatorial object, viewed in two different, useful, ways. Graph Definitions Extractors and dispersers are certain types of bipartite (multi-)graphs. <p> The error parameter * of this construction depends on the smaller block and is 1=poly (n 0 ), which is larger than 1=poly (n). By carefully controlling the composition of many extractors of differing lengths, <ref> [SZ94, Zuc] </ref> show that d = O (log n + log * 1 ) suffices for any * as long as k = (n) (and even slightly less). A different way of decreasing d is given in the construction of our second extractor in Section 5. <p> Repeating the previous construction O (1) times allows extracting m = k (1 ) random bits using only d = O (log n) truly random bits for any constant &gt; 0 and k = (n) <ref> [Zuc] </ref>. Decreasing k This is done using the "alternative method" sketched in Section 3.3, and will be presented in Section 4. 4 The First Construction: An Extractor for Any Min-entropy! In this section we present our first new extractor. <p> We also saw (Corollary 4.4) that it is enough to find a (k; *) extractor E : (2k) fi (d) 7! (m) with m very close to k. To be more specific, we need m k O ( k log (n) ). By <ref> [NZ93, SZ94, Zuc] </ref> we know to find such extractors with m = (k). Using a simple idea due to Wigderson and Zuckerman [WZ93] we can get m much closer to k. <p> It can easily be seen that if we want to amplify an RP algorithm we can use dispersers instead of extractors. Using current constructions we can use only n = (1 + ff)(m + t) bits to get error 2 t , for any fixed ff &gt; 0 <ref> [Zuc93, Zuc] </ref>. Using the construction of Section 4, we can use only m + t random bits, but the running time becomes quasi-polynomial. 6.2.2 Oblivious Sampling The above simulation may be generalized to give what is called an oblivious sampler. <p> Oblivious samplers were constructed in [BR94] who used them for interactive proof systems. The best known results to date, which use extractors, appear in <ref> [Zuc] </ref>. Theorem: There exist explicitly constructible (ffi; *) oblivious samplers where ffi = 2K=N . Using the best constructions of extractors [Zuc], we get oblivious samplers with n = (1 + ff)(m + log ffi 1 ) and D = poly (m; * 1 ; log ffi 1 ) (for any <p> Oblivious samplers were constructed in [BR94] who used them for interactive proof systems. The best known results to date, which use extractors, appear in <ref> [Zuc] </ref>. Theorem: There exist explicitly constructible (ffi; *) oblivious samplers where ffi = 2K=N . Using the best constructions of extractors [Zuc], we get oblivious samplers with n = (1 + ff)(m + log ffi 1 ) and D = poly (m; * 1 ; log ffi 1 ) (for any ff &gt; 0). Proof: Take an extractor G and use the (x) from the extractor.
Reference: [Zuc90] <author> D. Zuckerman. </author> <title> General weak random sources. </title> <booktitle> In Proceedings of the 31st Annual IEEE Symposium on the Foundations of Computer Science, </booktitle> <pages> pages 534-543, </pages> <year> 1990. </year>
Reference-contexts: The direct development of the constructions and applications of extractors and dispersers came first in papers written by Zuckerman in the early 1990's <ref> [Zuc90, Zuc91] </ref>, and then in a sequence of papers by various authors [NZ93, WZ93, SZ94, SSZ95, Zuc93, Zuc]. The first explicit construction of extractors came in [NZ93], and relied on techniques developed in [Zuc90, Zuc91]. This construction had d = polylog (n) for k n=polylog (n). <p> and applications of extractors and dispersers came first in papers written by Zuckerman in the early 1990's <ref> [Zuc90, Zuc91] </ref>, and then in a sequence of papers by various authors [NZ93, WZ93, SZ94, SSZ95, Zuc93, Zuc]. The first explicit construction of extractors came in [NZ93], and relied on techniques developed in [Zuc90, Zuc91]. This construction had d = polylog (n) for k n=polylog (n). An efficient extractor working for small k's, k = fi (log (n)), was obtained by [GW94, SZ94] using tiny families of hash functions [NN93, AGHP92]. <p> Let us see several ways by which these ingredients can be put together in order to get good extractors. 17 We first discuss the basic strategy used in <ref> [Zuc90, Zuc91, NZ93, SZ94] </ref> and present it essentially in the form found in [SZ94]. Choose t = O (log n) blocks each of size O (k= log n) as described in Section 3.3. This gives a (near) block-wise source. <p> Chor and Goldreich [CG88] generalized this model, and showed that BP P can be simulated even using the more general source. Many authors studied other restricted classes of random sources (e.g. [CW89, CGH + 85, LLS89]). Finally, Zuckerman <ref> [Zuc90] </ref> suggested a general model generalizing all the previous models. His model was simply all random sources having high min-entropy. Later [Zuc91] he showed that BP P can be simulated given such a source (with enough min-entropy).
Reference: [Zuc91] <author> D. Zuckerman. </author> <title> Simulating BPP using a general weak random source. </title> <booktitle> In Proceedings of the 32nd Annual IEEE Symposium on the Foundations of Computer Science, </booktitle> <pages> pages 79-89, </pages> <year> 1991. </year>
Reference-contexts: The direct development of the constructions and applications of extractors and dispersers came first in papers written by Zuckerman in the early 1990's <ref> [Zuc90, Zuc91] </ref>, and then in a sequence of papers by various authors [NZ93, WZ93, SZ94, SSZ95, Zuc93, Zuc]. The first explicit construction of extractors came in [NZ93], and relied on techniques developed in [Zuc90, Zuc91]. This construction had d = polylog (n) for k n=polylog (n). <p> and applications of extractors and dispersers came first in papers written by Zuckerman in the early 1990's <ref> [Zuc90, Zuc91] </ref>, and then in a sequence of papers by various authors [NZ93, WZ93, SZ94, SSZ95, Zuc93, Zuc]. The first explicit construction of extractors came in [NZ93], and relied on techniques developed in [Zuc90, Zuc91]. This construction had d = polylog (n) for k n=polylog (n). An efficient extractor working for small k's, k = fi (log (n)), was obtained by [GW94, SZ94] using tiny families of hash functions [NN93, AGHP92]. <p> Let us see several ways by which these ingredients can be put together in order to get good extractors. 17 We first discuss the basic strategy used in <ref> [Zuc90, Zuc91, NZ93, SZ94] </ref> and present it essentially in the form found in [SZ94]. Choose t = O (log n) blocks each of size O (k= log n) as described in Section 3.3. This gives a (near) block-wise source. <p> Many authors studied other restricted classes of random sources (e.g. [CW89, CGH + 85, LLS89]). Finally, Zuckerman [Zuc90] suggested a general model generalizing all the previous models. His model was simply all random sources having high min-entropy. Later <ref> [Zuc91] </ref> he showed that BP P can be simulated given such a source (with enough min-entropy). In a very basic sense, Zuckerman's model is the most general source we can think off.
Reference: [Zuc93] <author> D. Zuckerman. </author> <title> NP-complete problems have a version that's hard to approximate. </title> <booktitle> In Proceedings of the 8th Structures in Complexity Theory, IEEE, </booktitle> <pages> pages 305-312, </pages> <year> 1993. </year>
Reference-contexts: The direct development of the constructions and applications of extractors and dispersers came first in papers written by Zuckerman in the early 1990's [Zuc90, Zuc91], and then in a sequence of papers by various authors <ref> [NZ93, WZ93, SZ94, SSZ95, Zuc93, Zuc] </ref>. The first explicit construction of extractors came in [NZ93], and relied on techniques developed in [Zuc90, Zuc91]. This construction had d = polylog (n) for k n=polylog (n). <p> See <ref> [Zuc93] </ref> for more details. <p> D = poly (n)) for RP as long as H 1 (X) n ffi for any ffi &gt; 0 [SSZ95]; a polynomial time simulation for BP P as long as H 1 (X) = (n) (or even slightly less) <ref> [Zuc93] </ref>; and using our new results presented in Section 5, a slightly quasi-polynomial (n log (c) n for any constant c) time simulation for BP P for any H 1 (X): Corollary 6.2 For any ffi &gt; 0 and constant k &gt; 0, BP P can be simulated in time n <p> It can easily be seen that if we want to amplify an RP algorithm we can use dispersers instead of extractors. Using current constructions we can use only n = (1 + ff)(m + t) bits to get error 2 t , for any fixed ff &gt; 0 <ref> [Zuc93, Zuc] </ref>. Using the construction of Section 4, we can use only m + t random bits, but the running time becomes quasi-polynomial. 6.2.2 Oblivious Sampling The above simulation may be generalized to give what is called an oblivious sampler. <p> This cannot be true since an *-close to uniform distribution on the z's gives an *-close estimate of e, in contradiction to each element in S &lt; (resp, S &gt; ) erring on e by at least *. 6.2.3 Approximating Clique Zuckerman <ref> [Zuc93] </ref> showed how deterministic amplification obtained by extractors implies that Theorem: [Zuc93] Approximating log (Clique (G)) to within an constant factor is ~ N P -hard. <p> true since an *-close to uniform distribution on the z's gives an *-close estimate of e, in contradiction to each element in S &lt; (resp, S &gt; ) erring on e by at least *. 6.2.3 Approximating Clique Zuckerman <ref> [Zuc93] </ref> showed how deterministic amplification obtained by extractors implies that Theorem: [Zuc93] Approximating log (Clique (G)) to within an constant factor is ~ N P -hard. In fact Zuckerman showed that, for any constant j, approximating the j'th iterated log of clique to within any constant is ~ N P -hard. For the precise statement see [Zuc93, SZ94]. <p> In fact Zuckerman showed that, for any constant j, approximating the j'th iterated log of clique to within any constant is ~ N P -hard. For the precise statement see <ref> [Zuc93, SZ94] </ref>. Our new construction can derandomize Zuckerman's result. The proof itself builds on the known hardness results for approximating clique [AS92b, ALM + 92, FGL + 91], and is best understood when viewing it as deterministic amplification for P CP systems. 6.2.4 Time vs.
References-found: 43

