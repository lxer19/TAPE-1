URL: http://www.neci.nj.nec.com/homepages/giles/papers/UMD-CS-TR-3396.fsm-rnn.dynamics.systems.ps.Z
Refering-URL: http://www.neci.nj.nec.com/homepages/giles/html/CLG_pub.html
Root-URL: 
Email: Email: tino@decef.elf.stuba.sk  Email: p.c.collingwood@shu.ac.uk  Email: ftino,horne,gilesg@research.nj.nec.com  
Title: Finite State Machines and Recurrent Neural Networks Automata and Dynamical Systems Approaches  
Author: Peter Tino a;c Bill G. Horne c C. Lee Giles c;d Pete C. Collingwood b 
Address: Ilkovicova 3, 812 19 Bratislava, Slovakia  Park 100 Napier St., Sheffield, S11 8HD, UK  4 Independence Way Princeton, NJ 08540  College Park, MD 20742  College Park, MD 20742  
Affiliation: a Department of Computer Science and Engineering Slovak Technical University  b School of Computing Man. Sci. Sheffield Hallam University Hallam Business  c NEC Research Institute  d Institute for Advanced Computer Studies University of Maryland  Institute for Advanced Computer Studies University of Maryland  
Pubnum: Technical Report UMIACS-TR-95-1 and CS-TR-3396  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> N. Alon, A.K. Dewdney, and T.J. Ott. </author> <title> Efficient simulation of finite automata by neural nets. </title> <journal> Journal of the Association of Computing Machinery, </journal> <volume> 38(2) </volume> <pages> 495-514, </pages> <year> 1991. </year>
Reference-contexts: In the context of mealy machines and threshold networks a similar problem was attacked by Alon et al. <ref> [1] </ref> and Horne and Hush [23]. An attempt to predict the minimal second-order RNN size so that the network can learn to accept a given regular language is presented in [33]. The predicted numbers of neurons were shown to correlate well with the experimental findings.
Reference: [2] <author> R.D. Beer. </author> <title> On the dynamics of small continuous-time recurrent networks. </title> <type> Technical Report CES-94-18, </type> <institution> Case Western Reserve University, Cleveland, OH, </institution> <year> 1994. </year>
Reference-contexts: Literature dealing with the relationship between RNNs and DSs is quite rich: [20], [3], [16], [6], [7] [24], [26], [35], [36], [34], <ref> [2] </ref>, [21], for example. However, as it has been already mentioned, the task of complete understanding of the global dynamical behaviour of a given DS is not at all an easy one. <p> Note that as ffi increases attractive fixed points move closer to vertices f0; 1g 2 . A similar approach to determining the number and stability types of fixed points of the underlying dynamical systems in continuous-time recurrent neural networks can be found in <ref> [2] </ref>. 21 At the same time, jflj has to be also appropriately increased so as to compensate for the increase in ffi so that the "bended" part of f ffi;fl does not move radically to higher values of u. 29 8 Experiments Learning loops of FSM A RNN with two state
Reference: [3] <author> E.K. Blum and X. Wang. </author> <title> Stability of fixed points and periodic orbits and bifurcations in analog neural networks. </title> <booktitle> Neural Networks, </booktitle> (5):577-587, 1992. 
Reference-contexts: Literature dealing with the relationship between RNNs and DSs is quite rich: [20], <ref> [3] </ref>, [16], [6], [7] [24], [26], [35], [36], [34], [2], [21], for example. However, as it has been already mentioned, the task of complete understanding of the global dynamical behaviour of a given DS is not at all an easy one. <p> These rather strict conditions were weakened in [7], where a more easily satisfied conditions are 20 21 formulated. Blum and Wang <ref> [3] </ref> globally analyze networks with nonsymmetrical connectivity patterns of special types. In case of two recurrent neurons with sigmoidal activation function g, they give results for weight matrices with diagonal elements equal to zero 17 .
Reference: [4] <author> F. Brglez, D. Bryan, and K. Kozminski. </author> <title> Combinational Profiles of Sequential Benchmark Circuits. </title> <booktitle> In Proceedings of the International Symposium on Circuits and Systems, </booktitle> <address> Portland, Oregon, </address> <month> May </month> <year> 1989. </year> <title> 25 Instead of the center with greatest posterior probability given a pattern of state units' activation, a linear combination of centers is used, where each center is weighted by its posterior probability given current network state. </title> <type> 45 </type>
Reference-contexts: As another example, Consider a FSM M in figure 24. It is a FSM taken from the database of the International Symposium on Circuits and Systems (Portland, Oregon, 1989) <ref> [4] </ref>. In each of its 7 states there is an a-loop with output 0 except for a-loops in states 4 and 7. The training set consists of 3500 training strings 24 of input string length 3-35 and is ordered according to their length starting with the shortest ones.
Reference: [5] <author> M.P. Casey. </author> <title> Computation dynamics in discrete-time recurrent neural networks. </title> <booktitle> In Proceed--ings of the Annual Research Symposium, </booktitle> <volume> volume 3, </volume> <pages> pages 78-95, </pages> <address> UCSD, La Jolla, CA, </address> <year> 1993. </year> <title> Institute for Neural Computation. </title>
Reference-contexts: The expected relationship among their basins of attraction has to be taken into account at the same time <ref> [5] </ref>. As an example consider the FSMs M 1 and M 2 presented in figures 26, and 27 respectively. Apparently, the the internal structure of a regular language accepted by M 2 is "more complex" than that of accepted by M 1 .
Reference: [6] <author> M.P. Casey. </author> <title> Computation in Discrete-Time Dynamical Systems. </title> <type> PhD thesis, </type> <institution> University of California, San Diego, Department of Mathematics, </institution> <month> March </month> <year> 1995. </year>
Reference-contexts: Literature dealing with the relationship between RNNs and DSs is quite rich: [20], [3], [16], <ref> [6] </ref>, [7] [24], [26], [35], [36], [34], [2], [21], for example. However, as it has been already mentioned, the task of complete understanding of the global dynamical behaviour of a given DS is not at all an easy one. <p> As mentioned in the introduction, the approach presented in <ref> [6] </ref> addresses representational issues concerning recurrent neural networks trained to act as regular language recognizers. Recurrent neural networks are assumed to operate in a noisy environment. <p> Such an assumption can be supported by an argument that in any system implemented on a digital computer there is a finite amount of noise due to round-off errors and "we are only interested in solutions wich work in spite of round-off errors" <ref> [6] </ref>. Orbits of points under a map f and attractive sets of f are substituted for by the notions of *-pseudo-orbit of points under f and *-pseudo-attractor of f .
Reference: [7] <author> M.P. Casey. </author> <title> Relaxing the symmetric weight condition for convergent dynamics in discrete-time recurrent networks. </title> <type> Technical Report INC-9504, </type> <institution> Institute for Neural Computation, University of California, </institution> <address> San Diego, 9500 Gilman Drive, La Jolla, CA 92093-0112, </address> <year> 1995. </year>
Reference-contexts: Literature dealing with the relationship between RNNs and DSs is quite rich: [20], [3], [16], [6], <ref> [7] </ref> [24], [26], [35], [36], [34], [2], [21], for example. However, as it has been already mentioned, the task of complete understanding of the global dynamical behaviour of a given DS is not at all an easy one. <p> For example, symmetric connectivity and absence of self-interactions enabled Hopfield [22] to interpret the network as a physical system having energy minima in attractive fixed points of the network. These rather strict conditions were weakened in <ref> [7] </ref>, where a more easily satisfied conditions are 20 21 formulated. Blum and Wang [3] globally analyze networks with nonsymmetrical connectivity patterns of special types. In case of two recurrent neurons with sigmoidal activation function g, they give results for weight matrices with diagonal elements equal to zero 17 .
Reference: [8] <author> A. Cleeremans, D. Servan-Schreiber, and J.L. McClelland. </author> <title> Finite state automata and simple recurrent networks. </title> <journal> Neural Computation, </journal> <volume> 1(3) </volume> <pages> 372-381, </pages> <year> 1989. </year>
Reference-contexts: @ i0 x ) 1 (N x ) A : (12) Similar bounds can be found for (q 2 ) N ; :::; (q m ) N , in particular t m " (t im y j Some researchers attempted to extract learned automaton from a trained recurrent network [17], <ref> [8] </ref>, [37], [31]. Extraction procedures rely on the assumption that equivalent network states are grouped together in well-separated regions in the recurrent neurons' activation space.
Reference: [9] <author> F. Cummins. </author> <title> Representation of temporal patterns in recurrent networks. </title> <booktitle> Submitted to the 15th Annual Conference of the Cognitive Science Society, </booktitle> <year> 1993. </year>
Reference-contexts: If the network is supposed to operate as a FSM, its state space must have multiple attractor basins to store distinct internal states. The network solves the task of FSM simulation by location of point and periodic attractors and the shaping of their respective basins of attraction <ref> [9] </ref>. Before training, the connection weights are set to small random values and as a consequence, the network has only one attractor basin. This implies that the network must undergo several bifurcations [13].
Reference: [10] <author> S. Das, C.L. Giles, and G.Z. Sun. </author> <title> Learning context-free grammars: Capabilities and limitations of a recurrent neural network with an external stack memory. </title> <booktitle> In Proceedings of The Fourteenth Annual Conference of Cognitive Science Society. </booktitle> <institution> Indiana University, </institution> <year> 1992. </year>
Reference: [11] <author> S. Das and M.C. Mozer. </author> <title> A unified gradient-descent/clustering architecture for finite state machine induction. </title> <editor> In J.D. Cowen, G. Tesauro, and J. Alspector, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 19-26. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: state of the FSM even for short input strings. 43 fa; bg m a; m 2 f1; 3g. 3 , where L 3 = b fl ab fl [ (b fl a) 3 b + [ (b fl a) 4 . 44 Zeng et al. [39] and Das and Mozer <ref> [11] </ref> view the RNN state space quantization as an integral part of the learning process in which the network is trained to mimic a finite state machine. <p> In particular, in [39] state units' activation pattern is mapped at each time step to the nearest corner of a hypercube as if state neurons had a hard threshold activation function. Das and Mozer <ref> [11] </ref> used a "soft" version of the gaussian mixture model 25 in a supervised mode as a clustering tool. The mixture model parameters were adjusted so as to minimize the overall performance error of the whole system (recurrent network + clustering tool).
Reference: [12] <author> R.L. Devaney. </author> <title> An Introduction to Chaotic Dynamical Systems. </title> <publisher> Benjamin/Cummings Publishing Company, Inc., </publisher> <address> Menlo Park, California, </address> <year> 1986. </year>
Reference-contexts: Attractive fixed points and periodic orbits are trivial examples of attractive sets. Much more complicated attractive sets can be found in dynamical systems literature under the name strange attractors 6 <ref> [12] </ref>. As in the case of an attractive fixed point, the basin of attraction of an attractive set ~ B is the set of all points whose orbits converge to ~ B.
Reference: [13] <author> K. Doya. </author> <title> Bifurcations in the learning of recurrent neural networks. </title> <booktitle> In Proc. of 1992 IEEE Int. Symposium on Circuits and Systems, </booktitle> <pages> pages 2777-2780, </pages> <year> 1992. </year>
Reference-contexts: Before training, the connection weights are set to small random values and as a consequence, the network has only one attractor basin. This implies that the network must undergo several bifurcations <ref> [13] </ref>. This can have an undesirable effect on the training process, since the gradient descent learning may get into trouble. At bifurcations points, the output of a network can change discontinuously with the change of parameters and therefore convergence of gradient descent algorithms is not guaranteed [14].
Reference: [14] <author> K. Doya. </author> <title> Bifurcations of recurrent neural networks in gradient descent learning. </title> <journal> Submitted to IEEE Transactions on Neural Networks, </journal> <year> 1993. </year>
Reference-contexts: This can have an undesirable effect on the training process, since the gradient descent learning may get into trouble. At bifurcations points, the output of a network can change discontinuously with the change of parameters and therefore convergence of gradient descent algorithms is not guaranteed <ref> [14] </ref>. In the following a possible application of these ideas to the problem of determination of the complexity of language recognition by neural networks will be discussed briefly. Any FSM with binary output alphabet f0; 1g can function as a recognizer of a regular language.
Reference: [15] <author> J.L. Elman. </author> <title> Finding structure in time. </title> <journal> Cognitive Science, </journal> <volume> 14 </volume> <pages> 179-211, </pages> <year> 1990. </year> <month> 46 </month>
Reference: [16] <author> M. Garzon and S. Franklin. </author> <title> Global dynamics in neural networks. </title> <journal> Complex Systems, </journal> (3):29-36, 1989. 
Reference-contexts: Literature dealing with the relationship between RNNs and DSs is quite rich: [20], [3], <ref> [16] </ref>, [6], [7] [24], [26], [35], [36], [34], [2], [21], for example. However, as it has been already mentioned, the task of complete understanding of the global dynamical behaviour of a given DS is not at all an easy one.
Reference: [17] <author> C.L. Giles, C.B. Miller, D. Chen, H.H. Chen, G.Z. Sun, and Y.C. Lee. </author> <title> Learning and extracting finite state automata with second-order recurrent neural networks. </title> <journal> Neural Computation, </journal> <volume> 4(3) </volume> <pages> 393-405, </pages> <year> 1992. </year>
Reference-contexts: 1 @ i0 x ) 1 (N x ) A : (12) Similar bounds can be found for (q 2 ) N ; :::; (q m ) N , in particular t m " (t im y j Some researchers attempted to extract learned automaton from a trained recurrent network <ref> [17] </ref>, [8], [37], [31]. Extraction procedures rely on the assumption that equivalent network states are grouped together in well-separated regions in the recurrent neurons' activation space. <p> After training, the network state space is partitioned into clusters using some clustering tool and for each q 2 Q, the region (q) N is approximated by (possibly) several of such obtained clusters. For example, in <ref> [17] </ref> the network state neurons' activation space is divided into several equal hypercubes. When the number of hypercubes is sufficiently high, each hypercube is believed to contain only mutually equal states. After training, Tino and Sajda [31] present a large number of input words to the network input. <p> However, it is possible that a correct automaton is extracted from trained RNN even though the network is known to generalize poorly on long, unseen input words <ref> [17] </ref>. This is discussed in section 8. 5.1 Experiments Number of experiments were performed in which RNNs with two or three state neurons were trained simple FSMs. <p> Further study needs to be devoted to that matter. However, at least empirically and for simple tasks, our use of the Kohonen Map as a clustering tool [31], as well as the use of simple clustering technique introduced in <ref> [17] </ref> are supported. 6 RNN as a Collection of Dynamical Systems RNNs can be viewed as discrete-time DSs. Literature dealing with the relationship between RNNs and DSs is quite rich: [20], [3], [16], [6], [7] [24], [26], [35], [36], [34], [2], [21], for example. <p> N max = 100. of word's last symbol is 1. Hence, the network output is used to decide whether a word does belong to the language, or not. One of the most promising neural acceptors of regular languages [32] is the second-order RNN introduced by Giles et al. <ref> [17] </ref>. However, the practical aspects of the acceptance issue are still unclear [33]. The difficulty of acceptance of a given language by a neural network (the neural complexity of the language) can be quantified by the minimal number of neurons needed to recognize the language. <p> It was shown that even in such cases a correct state transition diagram of the FSM can potentially be extracted even though the network performs badly on longer input strings (as reported by Giles et al. <ref> [17] </ref>). The state degradation diagram for an input symbol x illustrates how regions of network state space, initially acting as if they assumed the role of states of the FSM in which there is an x-loop, gradually degradate upon repeated presentation of x.
Reference: [18] <author> C.L. Giles, C.B. Miller, D. Chen, G.Z. Sun, H.H. Chen, and Y.C. Lee. </author> <title> Extracting and learning an unknown grammar with recurrent neural networks. </title> <editor> In J.E. Moody, S.J. Hanson, and R.P. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 317-324, </pages> <year> 1992. </year>
Reference: [19] <author> J. Guckenheimer and P. Holmes. </author> <title> Nonlinear Oscilations, Dynamical Systems, and Bifurcations of Vector Fields. </title> <publisher> Springer-Verlag, </publisher> <year> 1982. </year>
Reference-contexts: B may be an attracting set of f , or it may contain an attractive set of f 7 , or none of the two 8 . To learn more about the theory of DSs, see for example <ref> [19] </ref>. 4 in sense of inclusion 5 B denotes the closure of B 6 Loosely speaking, strange attractors are attractive sets that are topologically distinct from (i.e. cannot be transformed by a homeomorphism to) trivial attractive sets mentioned above. 7 Note that this does not necessarily imply that B is part
Reference: [20] <author> M.W. Hirsch. </author> <title> Convergent activation dynamics in continuous time networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 2(5) </volume> <pages> 331-349, </pages> <year> 1989. </year>
Reference-contexts: Literature dealing with the relationship between RNNs and DSs is quite rich: <ref> [20] </ref>, [3], [16], [6], [7] [24], [26], [35], [36], [34], [2], [21], for example. However, as it has been already mentioned, the task of complete understanding of the global dynamical behaviour of a given DS is not at all an easy one.
Reference: [21] <author> M.W. Hirsch. </author> <title> Saturation at high gain in discrete time recurrent networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 7(3) </volume> <pages> 449-453, </pages> <year> 1994. </year>
Reference-contexts: Literature dealing with the relationship between RNNs and DSs is quite rich: [20], [3], [16], [6], [7] [24], [26], [35], [36], [34], [2], <ref> [21] </ref>, for example. However, as it has been already mentioned, the task of complete understanding of the global dynamical behaviour of a given DS is not at all an easy one. <p> of (14) lie in the "-neighborhood of vertices of unit square, where " = (0:5 (ff)) 2 + (0:5 (ffi)) 2 : The tendency of attractive fixed points in discrete-time RNNs with exclusively self-exciting recur rent neurons to move towards saturation values as neural gain grows is also discussed in <ref> [21] </ref>. 26 So far, we have confined the areas of the network state space (0; 1) 2 where (under some assump-tions on weights) fixed points of (14) of particular stability types can lie.
Reference: [22] <author> J.J. </author> <title> Hopfield. Neurons with a graded response have collective computational properties like those of two-state neurons. </title> <booktitle> Proceedings of the National Academy of Science USA, </booktitle> <volume> 81 </volume> <pages> 3088-3092, </pages> <month> May </month> <year> 1984. </year>
Reference-contexts: No plotting occurred during the pre-iterations. 7 RNN with Two State Neurons Usually, studies of the asymptotic behaviour of recurrent neural networks assume some form of structure in the weight matrix describing connectivity pattern among recurrent neurons. For example, symmetric connectivity and absence of self-interactions enabled Hopfield <ref> [22] </ref> to interpret the network as a physical system having energy minima in attractive fixed points of the network. These rather strict conditions were weakened in [7], where a more easily satisfied conditions are 20 21 formulated.
Reference: [23] <author> B.G. Horne and D.R. Hush. </author> <title> Bounds on the complexity of recurrent neural network implementations of finite state machines. </title> <editor> In J.D. Cowen, G. Tesauro, and J. Alspector, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 359-366. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year> <note> Also submitted to Neural Networks. </note>
Reference-contexts: In the context of mealy machines and threshold networks a similar problem was attacked by Alon et al. [1] and Horne and Hush <ref> [23] </ref>. An attempt to predict the minimal second-order RNN size so that the network can learn to accept a given regular language is presented in [33]. The predicted numbers of neurons were shown to correlate well with the experimental findings.
Reference: [24] <author> S. Hui and S.H. Zak. </author> <title> Dynamical analysis of the brain-state-in-a-box neural models. </title> <journal> IEEE Transactions on Neural Networks, </journal> (1):86-94, 1992. 
Reference-contexts: Literature dealing with the relationship between RNNs and DSs is quite rich: [20], [3], [16], [6], [7] <ref> [24] </ref>, [26], [35], [36], [34], [2], [21], for example. However, as it has been already mentioned, the task of complete understanding of the global dynamical behaviour of a given DS is not at all an easy one.
Reference: [25] <author> L. Jin, P.N. Nikiforuk, and M.M. Gupta. </author> <title> Absolute stability conditions for discrete-time recurrent neural networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> (6):954-963, 1994. <volume> 47 </volume>
Reference-contexts: Blum and Wang [3] globally analyze networks with nonsymmetrical connectivity patterns of special types. In case of two recurrent neurons with sigmoidal activation function g, they give results for weight matrices with diagonal elements equal to zero 17 . Recently, Jin, Nikifiruk and Gupta <ref> [25] </ref> reported new results on the absolute stability for a rather general class of recurrent neural networks. Conditions under which all fixed points of the network are attractive were determined by the weight matrix of the network.
Reference: [26] <author> M.I. Jordan. </author> <title> Attractor dynamics and parallelism in a connectionist sequential machine. </title> <booktitle> In Proceedings of the Eighth Conference of the Cognitive Science Society, </booktitle> <pages> pages 531-546. </pages> <publisher> Erlbaum, </publisher> <year> 1986. </year>
Reference-contexts: Literature dealing with the relationship between RNNs and DSs is quite rich: [20], [3], [16], [6], [7] [24], <ref> [26] </ref>, [35], [36], [34], [2], [21], for example. However, as it has been already mentioned, the task of complete understanding of the global dynamical behaviour of a given DS is not at all an easy one.
Reference: [27] <author> L.K. Li. </author> <title> Fixed point analysis for discrete-time recurrent neural networks. </title> <booktitle> In Proceedings of IJCNN, </booktitle> <volume> volume 4, </volume> <pages> pages 134-139, </pages> <address> Baltimore, USA, </address> <year> 1992. </year>
Reference-contexts: The map t x is transformed to the map t x by multiplying weights W iln by a scalar &gt; 0, i.e. t x (s) = t x (s). is also called the neuron gain. The following Lemma was proved by Li <ref> [27] </ref>. It is stated for maps t x and accommodated with our notation. It tells us under what conditions one may expect an attractive fixed point of t x to exist "near" a vertex v 2 f0; 1g L .
Reference: [28] <author> P. Manolios and R. Fanelli. </author> <title> First order recurrent neural networks and deterministic finite state automata. </title> <journal> Neural Computation, </journal> <volume> 6(6) </volume> <pages> 1155-1173, </pages> <year> 1994. </year>
Reference: [29] <author> R.C. Minnick. </author> <title> Linear-input logic. </title> <journal> IRE Transactions on Electronic Computers, </journal> <volume> EC-13:6-16, </volume> <year> 1961. </year>
Reference: [30] <author> P. Tino, I.E. Jelly, and V. Vojtek. </author> <title> Non-standard topologies of neuron field in self-organizing feature maps. </title> <booktitle> In Proceedings of the AIICSR'94 conference, Slovakia, </booktitle> <pages> pages 391-396. </pages> <publisher> World Scientific Publishing Company, </publisher> <year> 1994. </year>
Reference-contexts: Such a topology helped to reduce great sensitivity to initial conditions found in vector-coding algorithms using independent cluster centers, while avoiding time consuming approximation of input space topology typical of classical regular-grid topologies of Kohonen Map <ref> [30] </ref>. Other approaches to RNN state space clustering are discussed in [31].
Reference: [31] <author> P. Tino and J. Sajda. </author> <title> Learning and extracting initial mealy machines with a modular neural network model. </title> <journal> Neural Computation, </journal> <volume> 7(4), </volume> <year> 1995. </year>
Reference-contexts: introductions to state machines and dynamical systems respectively. Section 4 is devoted to the model of RNN <ref> [31] </ref> used for learning FSMs. 2 State Machines This section introduces the concept of state machine, which is a generalized finite state machine with possibly uncountable number of states. When viewed as automata, RNNs can be described in terms of state machines. <p> Think of attractive periodic orbit inside B that encircles a repelling fixed point. 8 Identity map constitutes a simple example 6 4 Recurrent Neural Network The RNN presented in figure 1 was shown to be able to learn mappings that can be described by finite state machines <ref> [31] </ref>. A binary input vector I (t) = (I (t) (t) N ) corresponds to the activations of N input neurons. <p> Training is performed via optimization with respect to the error function E = 2 m m O (t) where T (t) m 2f0; 1g is the desired response value for the m-th output neuron at the time step t. For a more detailed explanation of the training procedure see <ref> [31] </ref>. 5 RNN as a State Machine In this section we assume that a RNN N of the type described above has learned to exactly mimic the behaviour of a reduced, connected FSM M = (X; Y; Q; ffi; ; s o ) it was trained with. <p> for don't know output of the net. 11 In practical terms, during learning phase, the network is trained to respond to a special "reset" input symbol # (# =2 X) by changing its state to a state equivalent to s 0 , the initial state of M (more details in <ref> [31] </ref>). <p> x ) 1 (N x ) A : (12) Similar bounds can be found for (q 2 ) N ; :::; (q m ) N , in particular t m " (t im y j Some researchers attempted to extract learned automaton from a trained recurrent network [17], [8], [37], <ref> [31] </ref>. Extraction procedures rely on the assumption that equivalent network states are grouped together in well-separated regions in the recurrent neurons' activation space. <p> For example, in [17] the network state neurons' activation space is divided into several equal hypercubes. When the number of hypercubes is sufficiently high, each hypercube is believed to contain only mutually equal states. After training, Tino and Sajda <ref> [31] </ref> present a large number of input words to the network input. All states the network passes through during the presentation are saved. <p> Such a topology helped to reduce great sensitivity to initial conditions found in vector-coding algorithms using independent cluster centers, while avoiding time consuming approximation of input space topology typical of classical regular-grid topologies of Kohonen Map [30]. Other approaches to RNN state space clustering are discussed in <ref> [31] </ref>. <p> Generally, in our experiments, regions approximating (q) N were observed to be connected and of "simple shape". Further study needs to be devoted to that matter. However, at least empirically and for simple tasks, our use of the Kohonen Map as a clustering tool <ref> [31] </ref>, as well as the use of simple clustering technique introduced in [17] are supported. 6 RNN as a Collection of Dynamical Systems RNNs can be viewed as discrete-time DSs.
Reference: [32] <author> M.W. Shields. </author> <title> An Introduction to Automata Theory. </title> <publisher> Blackwell Scientific Publications, </publisher> <address> London, UK, </address> <year> 1987. </year>
Reference-contexts: Automata theory provides us with the ability to connect structural and behavioural equivalence of automata <ref> [32] </ref>. <p> N max = 100. of word's last symbol is 1. Hence, the network output is used to decide whether a word does belong to the language, or not. One of the most promising neural acceptors of regular languages <ref> [32] </ref> is the second-order RNN introduced by Giles et al. [17]. However, the practical aspects of the acceptance issue are still unclear [33].
Reference: [33] <editor> H.T. Siegelmann, E.D. Sontag, and C.L. Giles. </editor> <title> The complexity of language recognition by neural networks. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Algorithms, Software, Architecture (Proceedings of IFIP 12 th World Computer Congress), </booktitle> <pages> pages 329-335, </pages> <address> Amsterdam, 1992. </address> <publisher> North-Holland. </publisher>
Reference-contexts: One of the most promising neural acceptors of regular languages [32] is the second-order RNN introduced by Giles et al. [17]. However, the practical aspects of the acceptance issue are still unclear <ref> [33] </ref>. The difficulty of acceptance of a given language by a neural network (the neural complexity of the language) can be quantified by the minimal number of neurons needed to recognize the language. <p> In the context of mealy machines and threshold networks a similar problem was attacked by Alon et al. [1] and Horne and Hush [23]. An attempt to predict the minimal second-order RNN size so that the network can learn to accept a given regular language is presented in <ref> [33] </ref>. The predicted numbers of neurons were shown to correlate well with the experimental findings. Essentially, a good starting point for the estimation of neural complexity of a given regular language is the representation of the language with the reduced recognizer. <p> Essentially, a good starting point for the estimation of neural complexity of a given regular language is the representation of the language with the reduced recognizer. The most usual, very rough, approach to the neural complexity estimation takes into account only the number of states of such a recognizer <ref> [33] </ref>.
Reference: [34] <author> M. Vidyasagar. </author> <title> Location and stability of the high-gain equilibria of nonlinear neural networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 4(4) </volume> <pages> 660-672, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: Literature dealing with the relationship between RNNs and DSs is quite rich: [20], [3], [16], [6], [7] [24], [26], [35], [36], <ref> [34] </ref>, [2], [21], for example. However, as it has been already mentioned, the task of complete understanding of the global dynamical behaviour of a given DS is not at all an easy one.
Reference: [35] <author> X. Wang. </author> <title> Period-doublings to chaos in a simple neural network: An analytical proof. </title> <journal> Complex Systems, </journal> (5):425-441, 1991. 
Reference-contexts: Literature dealing with the relationship between RNNs and DSs is quite rich: [20], [3], [16], [6], [7] [24], [26], <ref> [35] </ref>, [36], [34], [2], [21], for example. However, as it has been already mentioned, the task of complete understanding of the global dynamical behaviour of a given DS is not at all an easy one.
Reference: [36] <author> X. Wang and E.K. Blum. </author> <title> Discrete-time versus continuous-time models of neural networks. </title> <journal> Journal of Computer and Systems Sciences, </journal> <volume> 45 </volume> <pages> 1-19, </pages> <year> 1990. </year> <month> 48 </month>
Reference-contexts: Literature dealing with the relationship between RNNs and DSs is quite rich: [20], [3], [16], [6], [7] [24], [26], [35], <ref> [36] </ref>, [34], [2], [21], for example. However, as it has been already mentioned, the task of complete understanding of the global dynamical behaviour of a given DS is not at all an easy one. In [36] it is shown that networks with just two recurrent neurons can exhibit chaos and hence <p> between RNNs and DSs is quite rich: [20], [3], [16], [6], [7] [24], [26], [35], <ref> [36] </ref>, [34], [2], [21], for example. However, as it has been already mentioned, the task of complete understanding of the global dynamical behaviour of a given DS is not at all an easy one. In [36] it is shown that networks with just two recurrent neurons can exhibit chaos and hence the asymptotic network dynamical behaviour (on a chaotic attractor) can be very complex.
Reference: [37] <author> R.L. Watrous and G.M. Kuhn. </author> <title> Induction of finite-state automata using second-order recurrent networks. </title> <editor> In J.E. Moody, S.J. Hanson, and R.P. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 309-316, </pages> <year> 1992. </year>
Reference-contexts: i0 x ) 1 (N x ) A : (12) Similar bounds can be found for (q 2 ) N ; :::; (q m ) N , in particular t m " (t im y j Some researchers attempted to extract learned automaton from a trained recurrent network [17], [8], <ref> [37] </ref>, [31]. Extraction procedures rely on the assumption that equivalent network states are grouped together in well-separated regions in the recurrent neurons' activation space.
Reference: [38] <author> R.L. Watrous and G.M. Kuhn. </author> <title> Induction of finite-state languages using second-order recurrent networks. </title> <journal> Neural Computation, </journal> <volume> 4(3) </volume> <pages> 406-414, </pages> <year> 1992. </year>
Reference: [39] <author> Z. Zeng, R.M. Goodman, and P. Smyth. </author> <title> Learning finite state machines with self-clustering recurrent networks. </title> <journal> Neural Computation, </journal> <volume> 5(6) </volume> <pages> 976-990, </pages> <year> 1993. </year> <month> 49 </month>
Reference-contexts: network state not representing any state of the FSM even for short input strings. 43 fa; bg m a; m 2 f1; 3g. 3 , where L 3 = b fl ab fl [ (b fl a) 3 b + [ (b fl a) 4 . 44 Zeng et al. <ref> [39] </ref> and Das and Mozer [11] view the RNN state space quantization as an integral part of the learning process in which the network is trained to mimic a finite state machine. In particular, in [39] state units' activation pattern is mapped at each time step to the nearest corner of <p> fl a) 3 b + [ (b fl a) 4 . 44 Zeng et al. <ref> [39] </ref> and Das and Mozer [11] view the RNN state space quantization as an integral part of the learning process in which the network is trained to mimic a finite state machine. In particular, in [39] state units' activation pattern is mapped at each time step to the nearest corner of a hypercube as if state neurons had a hard threshold activation function. Das and Mozer [11] used a "soft" version of the gaussian mixture model 25 in a supervised mode as a clustering tool.
References-found: 39

