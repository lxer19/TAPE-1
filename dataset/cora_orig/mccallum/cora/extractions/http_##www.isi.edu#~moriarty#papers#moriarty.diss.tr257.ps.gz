URL: http://www.isi.edu/~moriarty/papers/moriarty.diss.tr257.ps.gz
Refering-URL: http://www.isi.edu/~moriarty/mypapers.html
Root-URL: http://www.isi.edu
Title: Symbiotic Evolution of Neural Networks in Sequential Decision Tasks  
Author: David Eric Moriarty 
Address: Austin, TX 78712  
Affiliation: Artificial Intelligence Laboratory The University of Texas at Austin  
Web: http://www.cs.utexas.edu/users/moriarty/  
Note: moriarty@cs.utexas.edu  
Date: January 1997  
Pubnum: Report AI97-257  
Abstract-found: 0
Intro-found: 1
Reference: <author> Anderson, C. W. </author> <year> (1987). </year> <title> Strategy learning with multilayer connectionist representations. </title> <type> Technical Report TR87-509.3, </type> <institution> GTE Labs, </institution> <address> Waltham, MA. </address>
Reference-contexts: In the SANE implementation, however, randomness is unnecessary in the decision process since there is a large amount of state space sampling through multiple combinations of neurons. AHC Two different AHC implementations were tested: A single-layer version (Barto et al. 1983) and a two-layer version <ref> (Anderson 1987) </ref>. Table 5.1 lists the parameters for each method. Both implementations were run directly from simulation code written by Sutton and Anderson, respectively. The learning parameters, network architectures, and control strategy were thus chosen by Sutton and Anderson and presumably reflect parameters that have been found effective.
Reference: <author> Anderson, C. W. </author> <year> (1989). </year> <title> Learning to control an inverted pendulum using neural networks. </title> <journal> IEEE Control Systems Magazine, </journal> <volume> 9 </volume> <pages> 31-37. </pages>
Reference-contexts: Both implementations were run directly from simulation code written by Sutton and Anderson, respectively. The learning parameters, network architectures, and control strategy were thus chosen by Sutton and Anderson and presumably reflect parameters that have been found effective. Since the state evaluation function to be learned is non-monotonic <ref> (Anderson 1989) </ref> and single-layer networks can only learn linearly-separable tasks, Barto et al. (1983) discretized the input space into 162 nonoverlapping regions or "boxes" for the single-layer AHC. <p> Each input unit is connected to every hidden unit and to the single output unit. The two-layer networks are trained using a variant of backpropagation <ref> (Anderson 1989) </ref>. The output of the action network is interpreted as the probability of choosing that action (push left or right) in both the single and two-layer AHC implementations. <p> Small populations are used to discourage different, competing neural network "species" from forming within the population. Whitley et al. (1993) argue that speciation leads to competing conventions and produces poor offspring when two dissimilar networks are recombined. Whitley et al. (1993) compared GENITOR to the Adaptive Heuristic Critic <ref> (Anderson 1989, 87 AHC) </ref>, which uses the TD method of reinforcement learning. In several different versions of the common benchmark of balancing an pole on a cart, GENITOR was found to be comparable to the AHC in both learning rate and generalization.
Reference: <author> Barto, A. G., Sutton, R. S., and Anderson, C. W. </author> <year> (1983). </year> <title> Neuronlike adaptive elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, SMC-13:834-846. </journal>
Reference-contexts: TD learning is by far the most popular reinforcement learning method and has therefore become a standard for which alternative methods must be compared. The general idea of temporal 8 difference learning is first described followed by a description of its two most prominent implemen-tations: the Adaptive Heuristic Critic <ref> (Barto et al. 1983) </ref> and Q-learning (Watkins 1989; Watkins and Dayan 1992). 2.2.1 Learning Through Temporal Differences In temporal difference learning (Sutton 1988), an evaluation or value function maintains predictions of current and future rewards. <p> In the SANE implementation, however, randomness is unnecessary in the decision process since there is a large amount of state space sampling through multiple combinations of neurons. AHC Two different AHC implementations were tested: A single-layer version <ref> (Barto et al. 1983) </ref> and a two-layer version (Anderson 1987). Table 5.1 lists the parameters for each method. Both implementations were run directly from simulation code written by Sutton and Anderson, respectively.
Reference: <author> Barto, A. G., Sutton, R. S., and Watkins, C. J. C. H. </author> <year> (1990). </year> <title> Learning and sequential decision making. </title> <editor> In Gabriel, M., and Moore, J. W., editors, </editor> <booktitle> Learning and Computational Neuroscience. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Belew, R. K., McInerney, J., and Schraudolph, N. N. </author> <year> (1991). </year> <title> Evolving networks: Using genetic algorithm with connectionist learning. </title> <editor> In Farmer, J. D., Langton, C., Rasmussen, S., and Taylor, C., editors, </editor> <booktitle> Artificial Life II. </booktitle> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference: <author> Bellman, R. E. </author> <year> (1957). </year> <title> Dynamic Programming. </title> <publisher> Princeton, </publisher> <address> NJ: </address> <publisher> Princeton University Press. </publisher>
Reference: <author> Bertsekas, D. P. </author> <year> (1987). </year> <title> Dynamic Programming: Deterministic and Stochastic Models. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall. </publisher>
Reference: <author> Billman, D., and Shaman, D. </author> <year> (1990). </year> <title> Strategy knowledge and strategy change in skilled performance: A study of the game othello. </title> <journal> American Journal of Psychology, </journal> <volume> 103 </volume> <pages> 145-166. </pages>
Reference-contexts: However, the desired behavior of a championship Othello program is just the opposite. The best Othello players keep their piece count quite low during most of the game to maximize the number of move options <ref> (Billman and Shaman 1990) </ref>. If populations of game-playing individuals are evolved against a top Othello program from the start, they will likely never experience a winning game and will not evolve the complex strategies necessary for winning.
Reference: <author> Boyan, J. A., and Moore, A. W. </author> <year> (1995). </year> <title> Generalization in reinforcement learning: Safely approximating the value function. </title> <editor> In Tesauro, G., Touretzky, D., and Leen, T. K., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Chrisman, L. </author> <year> (1992). </year> <title> Reinforcement learning with perceptual aliasing: The perceptual distinctions approach. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> 183-188. </pages> <address> San Jose, CA. </address>
Reference: <author> Collins, R. J., and Jefferson, D. R. </author> <year> (1991). </year> <title> Selection in massively parallel genetic algorithms. </title> <booktitle> In Proceedings of the Fourth International Conference on Genetic Algorithms, </booktitle> <pages> 249-256. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The system developed in this dissertation will demonstrate that aggressive selection and recombination strategies can work well if tempered with effective diversity pressures. Several more intelligent methods have been developed to enforce population diversity, including fitness sharing (Goldberg and Richardson 1987), crowding (De Jong 1975), and local mating <ref> (Collins and Jefferson 1991) </ref>. Each of these techniques relies on external genetic functions that prevent convergence of the genetic material. The diversity assurances, however, are achieved through very expensive operations.
Reference: <author> Davidor, Y. </author> <year> (1991). </year> <title> Genetic Algorithms and Robotics. </title> <address> Teaneck, NJ: </address> <publisher> World Scientific. 108 De Jong, </publisher> <editor> K. A. </editor> <year> (1975). </year> <title> An Analysis of the Behavior of a Class of Genetic Adaptive Systems. </title> <type> PhD thesis, </type> <institution> The University of Michigan, </institution> <address> Ann Arbor, MI. </address>
Reference: <author> Edwards, D., and Hart, T. </author> <year> (1963). </year> <title> The alpha-beta heuristic. </title> <type> Technical Report 30, </type> <institution> MIT. </institution>
Reference: <author> Fahlman, S. E. </author> <year> (1988). </year> <title> An empirical study of learning speed in backpropagation networks. </title> <type> Technical Report CMU-CS-88-162, </type> <institution> Computer Science Department, Carnegie Mellon University, </institution> <address> Pittsburgh, PA. </address>
Reference-contexts: Montana and Davis report no clear advantage using the hybrid approach over the evolutionary algorithm alone and report superior results in the number of training iterations using the evolutionary algorithm over backpropagation. Potter (1992) used an evolutionary algorithm in place of the quickprop learning method <ref> (Fahlman 1988) </ref> in the cascade correlation architecture (Fahlman and Lebiere 1990), which is one 24 of the fastest known neural network training algorithms.
Reference: <author> Fahlman, S. E., and Lebiere, C. </author> <year> (1990). </year> <title> The cascade-correlation learning architecture. </title> <editor> In Touretzky, D. S., editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <pages> 524-532. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: GA/NN combinations offer many important advantages over the more traditional neural network learning methods like backpropagation (Rumelhart et al. 1986) and cascade correlation <ref> (Fahlman and Lebiere 1990) </ref>. This section describes and motivates the evolutionary approach for forming effective neural networks. <p> Potter (1992) used an evolutionary algorithm in place of the quickprop learning method (Fahlman 1988) in the cascade correlation architecture <ref> (Fahlman and Lebiere 1990) </ref>, which is one 24 of the fastest known neural network training algorithms. The genetic form of cascade correlation required fewer epochs in the two-spirals benchmark and slightly more epochs in the eight-bit parity problem than the standard quickprop cascade correlation. <p> Unlike SANE, the LCS/NN is a pure "Michigan" approach where the entire population of neurons represents the final solution. In SANE, subpopulations represent the solution. The LCS/NN implementation uses a variant of the cascade correlation algorithm <ref> (Fahlman and Lebiere 1990) </ref> to compute fitness levels for each neuron. Neuron fitness levels are increased if their activations correlate with correct output from the neural network. However, by basing credit assignment on the known correct behavior, the current LCS/NN implementation cannot be used for reinforcement learning.
Reference: <author> Feddema, J. T., and Lee, G. C. S. </author> <year> (1990). </year> <title> Adaptive image feature prediction and control for visual tracking with a hand-eye coordinated camera. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 20(5). </volume>
Reference: <author> Fogel, L. J., Owens, A. J., and Walsh, M. J. </author> <year> (1966). </year> <title> Artificial Intelligence through Simulated Evolution. </title> <address> New York: </address> <publisher> Wiley Publishing. </publisher>
Reference-contexts: This section describes the evolutionary algorithm approach and gives some examples of how decision policies could be represented and operated on within an EA. ERL methods are not restricted to a specific evolutionary algorithm. Methods from genetic algorithms (Holland 1975; Goldberg 1989), evolutionary programming <ref> (Fogel et al. 1966) </ref>, genetic programming (Koza 1992), or evolutionary strategies (Rechenberg 1964) could all be used in this framework to form effective decision-making agents.
Reference: <author> Goldberg, D. E. </author> <year> (1989). </year> <title> Genetic Algorithms in Search, Optimization and Machine Learning. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: Convergence The second research issue in neuro-evolution concerns convergence of the population. Because evolutionary algorithms continually select and breed the best individuals in the population, populations normally lose diversity and eventually converge around a single "type" of individual <ref> (Goldberg 1989) </ref>. Such convergence is undesirable for two reasons: (1) populations often converge on suboptimal peaks and (2) converged populations cannot adapt well to changes in the task environment. <p> A neuron-level evolution explicitly promotes genetic building blocks in the population that may be useful in building other networks. A network-level evolution does so only implicitly, along with various other sub- and superstructures <ref> (Goldberg 1989) </ref>.
Reference: <author> Goldberg, D. E., and Deb, K. </author> <year> (1991). </year> <title> A comparative analysis of selection schemes used in genetic algorithms. </title> <editor> In Rawlins, G., editor, </editor> <booktitle> Foundations of Genetic Algorithms, </booktitle> <pages> 69-93. </pages> <publisher> Morgan-Kaufmann. </publisher>
Reference-contexts: Recent research has shown tournament selection to be the preferred method of genetic selection in terms of its growth ratios for discouraging premature convergence <ref> (Goldberg and Deb 1991) </ref>. Comparisons of SANE to the standard tournament neuro-evolution approach demonstrate the performance of the symbiotic search relative to a more "state of the art" genetic search strategy. The fourth evolutionary approach is a symbiotic search without the higher-level blueprint evolution. This approach is called Neuron SANE.
Reference: <author> Goldberg, D. E., and Richardson, J. </author> <year> (1987). </year> <title> Genetic algorithms with sharing for multimodal function optimization. </title> <booktitle> In Proceedings of the Second International Conference on Genetic Algorithms, </booktitle> <pages> 148-154. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The system developed in this dissertation will demonstrate that aggressive selection and recombination strategies can work well if tempered with effective diversity pressures. Several more intelligent methods have been developed to enforce population diversity, including fitness sharing <ref> (Goldberg and Richardson 1987) </ref>, crowding (De Jong 1975), and local mating (Collins and Jefferson 1991). Each of these techniques relies on external genetic functions that prevent convergence of the genetic material. The diversity assurances, however, are achieved through very expensive operations.
Reference: <author> Gomez, F., and Miikkulainen, R. </author> <year> (1996). </year> <title> Incremental evolution of complex general behavior. </title> <type> Technical Report AI96-248, </type> <institution> Department of Computer Sciences, The University of Texas at Austin. </institution>
Reference: <author> Grefenstette, J., and Schultz, A. </author> <year> (1994). </year> <title> An evolutionary approach to learning in robots. </title> <booktitle> In Proceedings of the Machine Learning Workshop on Robot Learning, Eleventh International Conference on Machine Learning. </booktitle> <address> New Brunswick, NJ. </address>
Reference-contexts: SAMUEL has been applied to several small problems including the evasive maneuvers problem (Grefenstette et al.1990) and the game of cat-and-mouse (Grefenstette 1992). In more recent work, SAMUEL has been extended to the task of mobile robot navigation <ref> (Grefenstette and Schultz 1994) </ref>. The key difference between SAMUEL and SANE is the choice of representation. SAMUEL uses a rule-based production system, while SANE uses neural networks.
Reference: <author> Grefenstette, J. J. </author> <year> (1991). </year> <title> Lamarckian learning in multi-agent environments. </title> <booktitle> In Proceedings of the Fourth International Conference on Genetic Algorithms, </booktitle> <pages> 303-310. </pages> <address> San Diego, CA: </address> <publisher> Morgan Kaufman. </publisher>
Reference: <author> Grefenstette, J. J. </author> <year> (1992). </year> <title> An approach to anytime learning. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning (ML92), </booktitle> <pages> 189-195. </pages>
Reference-contexts: Each plan is evaluated by testing its performance in the task. New plans are created using genetic operators such as selection, crossover, and mutation. SAMUEL has been applied to several small problems including the evasive maneuvers problem (Grefenstette et al.1990) and the game of cat-and-mouse <ref> (Grefenstette 1992) </ref>. In more recent work, SAMUEL has been extended to the task of mobile robot navigation (Grefenstette and Schultz 1994). The key difference between SAMUEL and SANE is the choice of representation. SAMUEL uses a rule-based production system, while SANE uses neural networks.
Reference: <author> Grefenstette, J. J., Ramsey, C. L., and Schultz, A. C. </author> <year> (1990). </year> <title> Learning sequential decision rules using simulation models and competition. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 355-381. </pages> <note> 109 Gruau, </note> <author> F., and Whitley, D. </author> <year> (1993). </year> <title> Adding learning to the cellular development of neural networks. </title> <journal> Evolutionary Computation, </journal> <volume> 1(3) </volume> <pages> 213-233. </pages>
Reference-contexts: One difference, however, is that ERL methods do not use gradient descent algorithms, but rather allow the evolutionary algorithm to search for useful weights. In addition to neural networks, ERL decision policies have been successfully represented in symbolic rule sets <ref> (Grefenstette et al. 1990) </ref> and as Lisp S-expressions (Koza 1992). <p> This section highlights three of the most well-known systems. 7.1.1 SAMUEL The SAMUEL system uses evolutionary algorithms to form a production system to solve sequential decision problems <ref> (Grefenstette et al. 1990) </ref>. SAMUEL consists of three major components: a problem-specific module, a performance module, and a learning module. The problem-specific module consists of the environment and its interfaces. The performance module is a production system made up of several if-then rules that represent the decision policy. <p> The learning module uses an evolutionary algorithm to develop new tactical plans. Each plan is evaluated by testing its performance in the task. New plans are created using genetic operators such as selection, crossover, and mutation. SAMUEL has been applied to several small problems including the evasive maneuvers problem <ref> (Grefenstette et al.1990) </ref> and the game of cat-and-mouse (Grefenstette 1992). In more recent work, SAMUEL has been extended to the task of mobile robot navigation (Grefenstette and Schultz 1994). The key difference between SAMUEL and SANE is the choice of representation. <p> There are several key differences between GENITOR and SANE. The first difference is that in GENITOR, each network is only evaluated once. While this can reduce the number of network evaluations, it is very susceptible to sample error often referred to as noisy fitness evaluation <ref> (Grefenstette et al. 1990) </ref>. In SANE, each neuron is reevaluated in several networks per generation and in subsequent generations. Such variance results in a greater sampling of the solution space for each neuron and averages out the fitness evaluation noise.
Reference: <author> Hansson, O., and Mayer, A. </author> <year> (1989). </year> <title> Heuristic search as evidential reasoning. </title> <booktitle> In Proceedings of the Fifth Workshop on Uncertainty in AI. </booktitle>
Reference: <author> Hansson, O., and Mayer, A. </author> <year> (1990). </year> <title> Probabilistic heuristic estimates. </title> <journal> Annals of Mathematics and Artificial Intelligence, </journal> <volume> 2 </volume> <pages> 209-220. </pages>
Reference: <author> Haykin, S. </author> <year> (1994). </year> <title> Neural Networks a Comprehensive Foundation. </title> <address> New York: </address> <publisher> Macmillan College Publishing Company. </publisher>
Reference-contexts: The output of the activation function represents the neuron's output. The most popular activation function is the sigmoid function because it is simple mathematically and is biologically plausible <ref> (Haykin 1994) </ref>. A common sigmoid function is defined by F (v) = 1 + e v where v is the sum of the neuron's input. The communication between neurons is amplified or reduced by weighting the neural connections.
Reference: <author> Holland, J. H. </author> <year> (1975). </year> <title> Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology, </title> <booktitle> Control and Artificial Intelligence. </booktitle> <address> Ann Arbor, MI: </address> <publisher> University of Michigan Press. </publisher>
Reference: <author> Holland, J. H. </author> <year> (1986). </year> <title> Escaping brittleness: The possibilities of general-purpose learning algorithms applied to parallel rule-based systems. </title> <booktitle> In Machine Learning: An Artificial Intelligence Approach, </booktitle> <volume> vol. </volume> <pages> 2. </pages> <address> Los Altos, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: An LCS uses an evolutionary algorithm to evolve symbolic if-then rules called classifiers that map sensory input to an appropriate decision. Figure 7.1 outlines Holland's LCS framework <ref> (Holland 1986) </ref>. When sensory input is received, it is posted on the message list. If the left hand side of a classifier matches a message on the message list, its right hand side is posted on the message list.
Reference: <author> Holland, J. H. </author> <year> (1987). </year> <title> Genetic algorithms and classifier systems: </title> <booktitle> Foundations and future directions. In Proceedings of the Second International Conference on Genetic Algorithms, </booktitle> <pages> 82-89. </pages> <address> Hillsdale, New Jersey. </address>
Reference: <author> Holland, J. H., and Reitman, J. S. </author> <year> (1978). </year> <title> Cognitive systems based on adaptive algorithms. In Pattern-directed Inference Systems. </title> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference: <author> Horn, J., Goldberg, D. E., and Deb, K. </author> <year> (1994). </year> <title> Implicit niching in a learning classifier system: Nature's way. </title> <journal> Evolutionary Computation, </journal> <volume> 2(1) </volume> <pages> 37-66. </pages>
Reference: <author> Jefferson, D., Collins, R., Cooper, C., Dyer, M., Flowers, M., Korf, R., Taylor, C., and Wang, A. </author> <year> (1991). </year> <title> Evolution as a theme in artificial life: The genesys/tracker system. </title> <editor> In Farmer, J. D., Langton, C., Rasmussen, S., and Taylor, C., editors, </editor> <booktitle> Artificial Life II. </booktitle> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: The system developed in this dissertation will demonstrate that aggressive selection and recombination strategies can work well if tempered with effective diversity pressures. Several more intelligent methods have been developed to enforce population diversity, including fitness sharing (Goldberg and Richardson 1987), crowding (De Jong 1975), and local mating <ref> (Collins and Jefferson 1991) </ref>. Each of these techniques relies on external genetic functions that prevent convergence of the genetic material. The diversity assurances, however, are achieved through very expensive operations.
Reference: <author> Jolliffe, I. T. </author> <year> (1986). </year> <title> Principal Component Analysis. </title> <address> New York, NY: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: The experiments described in this section illustrate this process and, in the context of the Khepera simulator, describe how and why certain neuron specializations emerge. 4.3.1 Principal Component Analysis Principal component analysis (PCA) (see e.g. <ref> (Jolliffe 1986) </ref>) is a useful tool for visualizing the relative distances between high-dimensional vectors. PCA performs a coordinate transformation on a set of data points. The first dimension is chosen along the direction with the most variance in the data.
Reference: <author> Kaelbling, L. P., Littman, M. L., and Moore, A. W. </author> <year> (1996). </year> <title> Reinforcement learning: A survey. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4 </volume> <pages> 237-285. </pages>
Reference-contexts: Such learning requires little a priori information or implementation effort. As reinforcement learning methods are scaled up, however, it is becoming increasingly clear that they will need aid from existing domain knowledge <ref> (Kaelbling et al. 1996) </ref>. SANE has no direct mechanism to incorporate such knowledge into its initial population. Currently, the most effective way to encode domain information in SANE is through more descriptive input units or a better tuned fitness evaluation function.
Reference: <author> Kawato, M. </author> <year> (1990). </year> <title> Computational schemes and neural network models for formation and control of multijoint arm trajectory. In Neural Networks for Control. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Kitano, H. </author> <year> (1990). </year> <title> Designing neural networks using genetic algorithms with graph generation system. </title> <journal> Complex Systems, </journal> <volume> 4 </volume> <pages> 461-476. </pages>
Reference: <author> Knuth, D. E., and Moore, R. W. </author> <year> (1975). </year> <title> An analysis of alpha-beta pruning. </title> <journal> Artificial Intelligence, </journal> <volume> 6 </volume> <pages> 293-326. </pages>
Reference: <author> Korf, R. E. </author> <year> (1988). </year> <title> Search: A survey of recent results. </title> <editor> In Shrobe, H. E., editor, </editor> <booktitle> Exploring Artificial Intelligence. </booktitle> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Heuristic evaluation functions, therefore, are used to approximate the payoff of a state. Unfortunately, heuristics create errors that propagate up the search tree, and can greatly diminish the effectiveness of minimax <ref> (Korf 1988) </ref>. In other words, minimax is only as strong as the state evaluation function, since it always assumes that the heuristic estimates are accurate. A second drawback is that minimax also assumes that the opponent will always make the best move. It does not promote risk taking.
Reference: <author> Korf, R. E., and Chickering, D. M. </author> <year> (1994). </year> <title> Best-first minimax search: Othello results. </title> <booktitle> In AAAI-94. </booktitle>
Reference-contexts: It is conceivable that eventually the network's diminishing defense will leave it vulnerable to a powerful opponent, however that was never observed in these experiments. In the implementation described here, focus networks searched only through uniform-depth trees. Focus networks could also be implemented with algorithms such as best-first minimax <ref> (Korf and Chickering 1994) </ref>, where the tree is grown in non-uniform depths allowing more promising moves to be searched deeper.
Reference: <author> Koza, J. R. </author> <year> (1992). </year> <title> Genetic Programming: On the Programming of Computers by Means of Natural Selection. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: ERL methods are not restricted to a specific evolutionary algorithm. Methods from genetic algorithms (Holland 1975; Goldberg 1989), evolutionary programming (Fogel et al. 1966), genetic programming <ref> (Koza 1992) </ref>, or evolutionary strategies (Rechenberg 1964) could all be used in this framework to form effective decision-making agents. <p> One difference, however, is that ERL methods do not use gradient descent algorithms, but rather allow the evolutionary algorithm to search for useful weights. In addition to neural networks, ERL decision policies have been successfully represented in symbolic rule sets (Grefenstette et al. 1990) and as Lisp S-expressions <ref> (Koza 1992) </ref>.
Reference: <author> Koza, J. R., and Rice, J. P. </author> <year> (1991). </year> <title> Genetic generalization of both the weights and architecture for a neural network. </title> <booktitle> In International Joint Conference on Neural Networks, </booktitle> <volume> vol. 2, </volume> <pages> 397-404. </pages> <address> New York, NY: </address> <publisher> IEEE. </publisher>
Reference: <author> Kuperstein, M. </author> <year> (1991). </year> <title> INFANT neural controller for adaptive sensory-motor coordination. Neural Networks, </title> <type> 4(2). </type>
Reference: <author> Lee, K.-F., and Mahajan, S. </author> <year> (1990). </year> <title> The development of a world class Othello program. </title> <journal> Artificial Intelligence, </journal> <volume> 43 </volume> <pages> 21-36. </pages>
Reference-contexts: Both players were allowed to search through the 73 second level and used the same evaluation function. To optimize ff-fi pruning, node ordering was implemented based on the values of the evaluation function (Pearl 1984). The evaluation function implemented was the Bayes-optimized function used in Bill <ref> (Lee and Mahajan 1990) </ref>, which is composed of enormous lookup tables gathered from expert games. Bill was at one time the world-champion program and is still believed to be one of the best in the world. Any improvement over the current Bill evaluation function would thus be a significant result.
Reference: <author> Lin, L.-J. </author> <year> (1992). </year> <title> Self-improving reactive agents based on reinforcement learning, planning, and teaching. </title> <journal> Machine Learning, </journal> <volume> 8(3) </volume> <pages> 293-321. </pages>
Reference-contexts: The decision that returns the highest Q-value is the optimal choice. Q-learning has been shown to perform comparably to the AHC approach in terms of training time in toy domains <ref> (Lin 1992) </ref> and preliminary research has been done to scale up Q-learning to practical tasks (Lin 1993; Littman and Boyan 1993). However, there are several research issues like perceptual aliasing and generalization that must be resolved before Q-learning can be considered effective for large-scale tasks. <p> Note that the Q-function can be represented efficiently as a look-up table only when the state space is small. In a real-world application, the enormous state space would make explicit representation of each state impossible. Larger applications of Q-learning are likely to use neural networks <ref> (Lin 1992) </ref>, which can learn from continuous input values in an infinite state space. Instead of representing each state explicitly, neural networks form internal representations of the state space through their connections and weights, which allows them to generalize well to unobserved states.
Reference: <author> Lin, L.-J. </author> <year> (1993). </year> <title> Scaling up reinforcement learning for robot control. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning (ML93), </booktitle> <pages> 182-189. </pages> <address> Amherst, MA. </address>
Reference: <author> Lin, L.-J., and Mitchell, T. M. </author> <year> (1992). </year> <title> Memory approaches to reinforcement learning in non-markovian domains. </title> <type> Technical Report CMU-CS-92-138, </type> <institution> Carnegie Mellon University, School of Computer Science. </institution>
Reference-contexts: The decision that returns the highest Q-value is the optimal choice. Q-learning has been shown to perform comparably to the AHC approach in terms of training time in toy domains <ref> (Lin 1992) </ref> and preliminary research has been done to scale up Q-learning to practical tasks (Lin 1993; Littman and Boyan 1993). However, there are several research issues like perceptual aliasing and generalization that must be resolved before Q-learning can be considered effective for large-scale tasks. <p> Note that the Q-function can be represented efficiently as a look-up table only when the state space is small. In a real-world application, the enormous state space would make explicit representation of each state impossible. Larger applications of Q-learning are likely to use neural networks <ref> (Lin 1992) </ref>, which can learn from continuous input values in an infinite state space. Instead of representing each state explicitly, neural networks form internal representations of the state space through their connections and weights, which allows them to generalize well to unobserved states.
Reference: <author> Littman, M. L. </author> <year> (1995). </year> <title> Simulations combining evolution and learning. In Adaptive Individuals in Evolving Populations: Models and Algorithms: </title> <booktitle> Santa Fe Institute Studies in the Sciences of Complexity, </booktitle> <volume> vol. XXVI, </volume> <pages> 465-477. </pages> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference: <author> Littman, M. L. </author> <year> (1996). </year> <title> Algorithms for Sequential Decision Making. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Brown University. </institution>
Reference: <author> Littman, M. L., and Boyan, J. A. </author> <year> (1993). </year> <title> A distributed reinforcement learning scheme for network routing. </title> <type> Technical Report CMU-CS-93-165, </type> <institution> School of Computer Science, Carnegie Mellon University. </institution>
Reference: <author> Liu, Y., and Yao, X. </author> <year> (1996). </year> <title> A population-based learning algorithm which learns both architectures and weights of neural networks. </title> <journal> Chinese Journal of Advanced Software Research, </journal> <volume> 3(1). </volume>
Reference: <author> Lumelsky, V. J. </author> <year> (1987). </year> <title> Algorithmic and complexity issues of robot motion in an uncertain environment. </title> <journal> Journal of Complexity, </journal> <volume> 3 </volume> <pages> 146-182. </pages>
Reference-contexts: To produce such behavior using a supervised learning approach, training examples must demonstrate movement to intermediate arm positions (e.g. above the block). It is unclear how such examples could be generated without a path-planning algorithm <ref> (Lumelsky 1987) </ref>. Path-planning is an analytical approach performed o*ine in a complete mathematical model of the robot and its environment. Thus path-planning requires significant domain knowledge and computational resources, which may not be available in many situations.
Reference: <author> McAllester, D. A. </author> <year> (1988). </year> <title> Conspiracy numbers for min-max search. </title> <journal> Artificial Intelligence, </journal> <volume> 35 </volume> <pages> 287-310. </pages>
Reference: <author> McCallum, A. K. </author> <year> (1995). </year> <title> Reinforcement Learning with Selective Perception and Hidden State. </title> <type> PhD thesis, </type> <institution> The University of Rochester. </institution> <address> 111 Michel, O. </address> <year> (1995). </year> <note> Khepera simulator version 1.0 user manual. http://wwwi3s.unice.fr/ om/khep--sim.html. </note>
Reference: <author> Michie, D., and Chambers, R. A. </author> <year> (1968). </year> <title> BOXES: An experiment in adaptive control. </title> <editor> In Dale, E., and Michie, D., editors, </editor> <booktitle> Machine Intelligence. </booktitle> <address> Edinburgh, UK: </address> <publisher> Oliver and Boyd. </publisher>
Reference: <author> Miller, W. T. </author> <year> (1989). </year> <title> Real-time application of neural networks for sensor-based control of robots with vision. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 19(4) </volume> <pages> 825-831. </pages>
Reference: <author> Minsky, M. </author> <year> (1963). </year> <title> Steps toward artificial intelligence. </title> <editor> In Feigenbaum, E. A., and Feldman, J. A., editors, </editor> <booktitle> Computers and Thought, </booktitle> <pages> 406-450. </pages> <address> New York: </address> <publisher> McGraw-Hill. </publisher>
Reference-contexts: The crux of the problem is how to apportion credit to individual decisions based on an evaluation of a sequence of decisions and has been termed the credit assignment problem <ref> (Minsky 1963) </ref>. Game playing is just one example of the genre of problems that have been termed sequential decision tasks (Barto et al. 1990; Grefenstette et al. 1990; Littman 1996).
Reference: <author> Mondada, F., Franzi, E., and Ienne, P. </author> <year> (1993). </year> <title> Mobile robot miniaturization: A tool for investigation in control algorithms. </title> <booktitle> In Proceedings of the Third International Symposium on Experimental Robotics, </booktitle> <pages> 501-513. </pages> <address> Kyoto, Japan. </address>
Reference-contexts: The second experiments illustrate the specializations within SANE's population and uncover some of the different roles that the neurons assume in the networks. 4.1 The Khepera Robot Simulator The domain chosen for initial evaluation of the SANE method was mobile robotics, or more specifically, controlling the Khepera mobile robot <ref> (Mondada et al.1993) </ref>. 1 Robotics is a natural application for SANE. Many robot tasks such as navigation, sensory mapping, and kinematics approximation are very difficult to learn because the domain knowledge is normally not sufficient to provide targets for each action. <p> Despite its size, the robot is not easy to control. Khepera provides real world sensory information and requires a strong grounding to the motor outputs to effectively maneuver the robot <ref> (Mondada et al. 1993) </ref>. The I/O resources of the simulator were designed to accurately reflect those of the real robot. The eight infrared sensors detect the proximity of objects by light reflection and return values between 0 and 1023 depending on the color level.
Reference: <author> Montana, D. J., and Davis, L. </author> <year> (1989). </year> <title> Training feedforward neural networks using genetic algorithms. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Aritificial Intelligence, </booktitle> <pages> 762-767. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Moriarty, D. E., and Miikkulainen, R. </author> <year> (1994a). </year> <title> Evolutionary neural networks for value ordering in constraint satisfaction problems. </title> <type> Technical Report AI94-218, </type> <institution> Department of Computer Sciences, The University of Texas at Austin. </institution>
Reference-contexts: This section describes two such applications that have been completed during my tenure at Texas. 8.1.1 Value Ordering in Constraint Satisfaction Problems One of the first applications of SANE was for search control in a difficult class of artificial intelligence problems called constraint satisfaction problems <ref> (Moriarty and Miikkulainen 1994a) </ref>. Constraint satisfaction problems are common in many areas of computer science such as machine vision, scheduling, and planning. A CSP generally consists of a set of variables and a set of possible values for them.
Reference: <author> Moriarty, D. E., and Miikkulainen, R. </author> <year> (1994b). </year> <title> Evolving neural networks to focus minimax search. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI-94), </booktitle> <pages> 1371-1377. </pages> <address> Seattle, WA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: The second demonstrates SANE's performance in a continuous state and action space problem. Conjunctively, the two applications demonstrate both the flexibility and scope of the SANE decision learning system. Portions of this chapter are taken from <ref> (Moriarty and Miikkulainen 1994b, 1996b) </ref>. 6.1 Focusing Minimax Search Almost all current game programs rely on the minimax search algorithm (Shannon 1950) to return the best move. Minimax operates by searching from the current game situation through all possible moves.
Reference: <author> Moriarty, D. E., and Miikkulainen, R. </author> <year> (1995). </year> <title> Discovering complex Othello strategies through evolutionary neural networks. </title> <journal> Connection Science, </journal> <volume> 7(3) </volume> <pages> 195-209. </pages>
Reference-contexts: The individuals will likely succumb to the piece maximization strategy to minimize the overall damage at the end of the game. Previously, I showed that complex Othello strategies could be evolved in an incremental fashion <ref> (Moriarty and Miikkulainen 1995) </ref>. Populations were first evolved against a simple opponent to learn some good overall winning strategies. The opponent's skill level was then increased, and the population adapted this strategy into the difficult to master mobility strategy.
Reference: <author> Moriarty, D. E., and Miikkulainen, R. </author> <year> (1996a). </year> <title> Efficient reinforcement learning through symbiotic evolution. </title> <journal> Machine Learning, </journal> <volume> 22 </volume> <pages> 11-32. </pages>
Reference-contexts: The goal of this chapter is to familiarize the reader with neuro-evolution and to present the neuro-evolution system developed in this dissertation to solve difficult sequential decision tasks. Portions of this chapter are taken from <ref> (Moriarty and Miikkulainen 1996a, 1996c) </ref>. 3.1 Neural Computation for Sequential Decision Tasks Research in artificial neural networks (ANN) and their applications has exploded in the past decade. The scope of ANNs has extended far beyond artificial intelligence or even computer science. <p> Previous results have shown that a version of SANE without the blueprint population is quite effective in the pole balancing task <ref> (Moriarty and Miikkulainen 1996a) </ref>. The neuron-only version of SANE, however, was ineffective when scaled up to more difficult tasks, and the blueprint population was added to focus the search on the best combination of neurons.
Reference: <author> Moriarty, D. E., and Miikkulainen, R. </author> <year> (1996b). </year> <title> Evolving neuro-controllers for hand-eye coordination and obstacle avoidance in a robot arm. </title> <booktitle> In From Animals to Animats: Proceedings of the Fourth International Conference on Simulation of Adaptive Behavior (SAB-96). </booktitle> <address> Cape Cod, MA. </address>
Reference: <author> Moriarty, D. E., and Miikkulainen, R. </author> <year> (1996c). </year> <title> Hierarchical evolution of neural networks. </title> <type> Technical Report AI96-242, </type> <institution> Department of Computer Science, The University of Texas at Austin. </institution>
Reference: <author> Nolfi, S., and Parisi, D. </author> <year> (1992). </year> <title> Growing neural networks. </title> <booktitle> In Artificial Life III. </booktitle> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference: <author> Nolfi, S., and Parisi, D. </author> <year> (1995). </year> <title> Learning to adapt to changing environments in evolving neural networks. </title> <type> Technical Report 95-15, </type> <institution> Department of Neural Systems and Artificial Life, Institute of Psychology, CNR - Rome. </institution>
Reference: <author> Ourston, D., and Mooney, R. J. </author> <year> (1994). </year> <title> Theory refinement combining analytical and emprical methods. </title> <journal> Artificial Intelligence, </journal> <volume> 66 </volume> <pages> 311-344. </pages> <note> 112 Papanikolopoulos, </note> <author> N. P., and Khosla, P. K. </author> <year> (1993). </year> <title> Adaptive robotic visual tracking: Theory and experiments. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 38(3) </volume> <pages> 429-444. </pages>
Reference-contexts: Shared concepts, however, are advantageous because they can increase the classification accuracy for each category by applying general knowledge attained about one category to a related, but possibly more unfamiliar category <ref> (Ourston and Mooney 1994) </ref>. Symbiotic evolution, however, is capable of forming shared intermediate concepts by simultaneously evolving rules which are used to classify multiple categories. From an initially random rule base, subpopulations of rules could be selected to form a domain theory. <p> Symbiotic evolution, however, is capable of forming shared intermediate concepts by simultaneously evolving rules which are used to classify multiple categories. From an initially random rule base, subpopulations of rules could be selected to form a domain theory. The domain theory could then be evaluated through theory refinement <ref> (Ourston and Mooney 1994) </ref> which measures both the accuracy of the domain theory and the amount of refinement necessary. The evaluation score of the domain theory would be given to each participating rule and the process of selecting and evaluating random subpopulations would repeat.
Reference: <author> Parrello, B. D., Kabat, W. C., and Wos, L. </author> <year> (1986). </year> <title> Job-shop scheduling using automated reasoning: A case study of the car-sequencing problem. </title> <journal> Journal of Automated Reasoning, </journal> <volume> 2 </volume> <pages> 1-42. </pages>
Reference: <author> Pearl, J. </author> <year> (1984). </year> <title> Heuristics: Intelligent Search Strategies for Computer Problem Solving. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: Both players were allowed to search through the 73 second level and used the same evaluation function. To optimize ff-fi pruning, node ordering was implemented based on the values of the evaluation function <ref> (Pearl 1984) </ref>. The evaluation function implemented was the Bayes-optimized function used in Bill (Lee and Mahajan 1990), which is composed of enormous lookup tables gathered from expert games. Bill was at one time the world-champion program and is still believed to be one of the best in the world.
Reference: <author> Pendrith, M. </author> <year> (1994). </year> <title> On reinforcement learning of control actions in noisy and non-markovian domans. </title> <type> Technical Report UNSW-CSE-TR-9410, </type> <institution> School of Computer Science and Engineering, The University of New South Wales. </institution>
Reference: <author> Potter, M. A. </author> <year> (1992). </year> <title> A genetic cascade-correlation learning algorithm. </title> <booktitle> In Proceedings of the International Workshop on Combinations of Genetic Algorithms and Neural Networks (COGANN-92), </booktitle> <pages> 123-133. </pages> <address> Baltimore, MD. </address>
Reference: <author> Potter, M. A., and De Jong, K. A. </author> <year> (1995). </year> <title> Evolving neural networks with collaborative species. </title> <booktitle> In Proceedings of the 1995 Summer Computer Simulation Conference. </booktitle> <address> Ottawa, Canada. </address>
Reference: <author> Potter, M. A., De Jong, K. A., and Grefenstette, J. </author> <year> (1995). </year> <title> A coevolutionary approach to learning sequential decision rules. </title> <editor> In Eshelman, L., editor, </editor> <booktitle> Proceedings of the Sixth International Conference on Genetic Algorithms. </booktitle> <address> Pittsburgh, PA. </address>
Reference: <author> Puterman, M. L. </author> <year> (1994). </year> <title> Markov Decision Processes Discrete Stochastic Dynamic Programming. </title> <address> New York, NY: </address> <publisher> John Wiley and Sons Inc. </publisher>
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106. </pages>
Reference-contexts: Such signals provide only a general measure of proficiency in the task and do not explicitly direct the agent towards any course of action. This environment differs greatly from those found in applications of the more common supervised learning methods <ref> (Quinlan 1986) </ref>. In supervised learning, the agent has access to examples of correct behavior and learns from errors between its decisions and the correct decisions. In reinforcement learning, the correct course of action is not known.
Reference: <author> Rechenberg, I. </author> <year> (1964). </year> <title> Cybernetic solution path of an experimental problem. In Library Translation 1122. </title> <address> Farnborough, Hants, </address> <month> Aug. </month> <year> 1965: </year> <institution> Royal Aircraft Establishment. </institution>
Reference-contexts: ERL methods are not restricted to a specific evolutionary algorithm. Methods from genetic algorithms (Holland 1975; Goldberg 1989), evolutionary programming (Fogel et al. 1966), genetic programming (Koza 1992), or evolutionary strategies <ref> (Rechenberg 1964) </ref> could all be used in this framework to form effective decision-making agents. However, I do restrict ERL methods to those implementations of evolutionary algorithms that solve reinforcement learning problems. 2.3.1 Overview Evolutionary algorithms are global search techniques patterned after Darwin's theory of natural evolution.
Reference: <author> Ring, M. B. </author> <year> (1994). </year> <title> Continual Learning in Reinforcement Environments. </title> <type> PhD thesis, </type> <institution> The University of Texas at Austin. </institution>
Reference-contexts: It is conceivable that eventually the network's diminishing defense will leave it vulnerable to a powerful opponent, however that was never observed in these experiments. In the implementation described here, focus networks searched only through uniform-depth trees. Focus networks could also be implemented with algorithms such as best-first minimax <ref> (Korf and Chickering 1994) </ref>, where the tree is grown in non-uniform depths allowing more promising moves to be searched deeper.
Reference: <author> Rivest, R. L. </author> <year> (1987). </year> <title> Game tree searching by min/max approximation. </title> <journal> Artificial Intelligence, </journal> <volume> 34 </volume> <pages> 77-96. </pages>
Reference: <author> Rosenbloom, P. </author> <year> (1982). </year> <title> A world championship-level Othello program. </title> <journal> Artificial Intelligence, </journal> <volume> 19 </volume> <pages> 279-320. </pages>
Reference: <author> Rumelhart, D. E., Hinton, G. E., and Williams, R. J. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart, D. E., and McClelland, J. L., editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Volume 1: Foundations, </booktitle> <pages> 318-362. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Since the decision agent receives constant feedback from the critic, policy modifications can be made through a number of different hill-climbing search methods. The most common approach is to use a variant of the backpropagation <ref> (Rumelhart et al. 1986) </ref> method. 9 from which the action agent is trained. 2.2.3 Q-learning Q-learning (Watkins 1989; Watkins and Dayan 1992) is closely related to the AHC and is currently the most widely-studied reinforcement learning approach. <p> Consequently, an update from a single state observation influences all other policy decisions and effectively generalizes actions in observed states to unobserved states. Network weights are normally updated using a gradient descent algorithm such as backpropagation <ref> (Rumelhart et al. 1986) </ref>. Similarly, ERL methods often employ techniques such as neural networks to generalize the control policy. One difference, however, is that ERL methods do not use gradient descent algorithms, but rather allow the evolutionary algorithm to search for useful weights. <p> GA/NN combinations offer many important advantages over the more traditional neural network learning methods like backpropagation <ref> (Rumelhart et al. 1986) </ref> and cascade correlation (Fahlman and Lebiere 1990). This section describes and motivates the evolutionary approach for forming effective neural networks.
Reference: <author> Russell, S. J., and Norvig, P. </author> <year> (1994). </year> <title> Artificial Intelligence: A Modern Approach. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall. 113 Sammut, </publisher> <editor> C., and Cribb, J. </editor> <year> (1990). </year> <title> Is learning rate a good performance criterion for learning? In Proceedings of the Seventh International Conference on Machine Learning, </title> <address> 170-178. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Samuel, A. L. </author> <year> (1959). </year> <title> Some studies in machine learning using the game of checkers. </title> <journal> IBM Journal, </journal> <volume> 3 </volume> <pages> 210-229. </pages>
Reference: <author> Sanderson, A. C., and Weiss, L. E. </author> <year> (1983). </year> <title> Adaptive visual servo control of robots. </title> <editor> In Pugh, A., editor, </editor> <title> Robot Vision, </title> <address> 107-116. New York: </address> <publisher> Springer-Verlag. </publisher>
Reference: <editor> Schaffer, J. D., Whitley, D., and Eshelman, L. J. </editor> <year> (1992). </year> <title> Combinations of genetic algorithms and neural networks: A survey of the state of the art. </title> <booktitle> In Proceedings of the International Workshop on Combinations of Genetic Algorithms and Neural Networks (COGANN-92). </booktitle> <address> Baltimore, MD. </address>
Reference: <author> Shannon, C. E. </author> <year> (1950). </year> <title> Programming a computer for playing chess. </title> <journal> Philisophical Magazine, </journal> <volume> 41 </volume> <pages> 256-275. </pages>
Reference-contexts: Conjunctively, the two applications demonstrate both the flexibility and scope of the SANE decision learning system. Portions of this chapter are taken from (Moriarty and Miikkulainen 1994b, 1996b). 6.1 Focusing Minimax Search Almost all current game programs rely on the minimax search algorithm <ref> (Shannon 1950) </ref> to return the best move. Minimax operates by searching from the current game situation through all possible moves. In most games to find the best move, minimax must search search through several turns (player and opponent moves), which can be characterized as search levels.
Reference: <author> Smith, R. E., and Cribbs, H. B. </author> <year> (1994). </year> <title> Is a learning classifier system a type of neural network? Evolutionary Computation, </title> <type> 2(1). </type>
Reference: <author> Smith, R. E., Forrest, S., and Perelson, A. S. </author> <year> (1993). </year> <title> Searching for diverse, cooperative populations with genetic algorithms. </title> <journal> Evolutionary Computation, </journal> <volume> 1(2) </volume> <pages> 127-149. </pages>
Reference-contexts: Sharing requires O (n 2 ) similarity comparisons each generation, where n is the size of the population. In large populations with large chromosomes, comparison-based diversity methods such as sharing, crowding, and local mating are simply not practical <ref> (Smith et al. 1993) </ref>. A more recent technique for ensuring diversity has been termed implicit fitness sharing (Horn et al. 1994; Smith et al. 1993). In implicit fitness sharing, no comparisons are made between individuals.
Reference: <author> Smith, R. E., and Gray, B. </author> <year> (1993). </year> <title> Co-adaptive genetic algorithms: An example in othello strategy. </title> <type> Technical Report TCGA 94002, </type> <institution> Department of Engineering Science and Mechanics, The University of Alabama. </institution>
Reference-contexts: Sharing requires O (n 2 ) similarity comparisons each generation, where n is the size of the population. In large populations with large chromosomes, comparison-based diversity methods such as sharing, crowding, and local mating are simply not practical <ref> (Smith et al. 1993) </ref>. A more recent technique for ensuring diversity has been termed implicit fitness sharing (Horn et al. 1994; Smith et al. 1993). In implicit fitness sharing, no comparisons are made between individuals.
Reference: <author> Steetskamp, R. </author> <year> (1995). </year> <title> Explorations in symbiotic neuro-evolution search spaces. </title> <type> Masters Stage Report, </type> <institution> Department of Computer Science, University of Twente, The Netherlands. </institution>
Reference-contexts: Symbiosis emerges naturally in the current representation of neural networks as collections of hidden neurons, but preliminary experiments with other types of encodings, such as populations of individual network connections, have been unsuccessful <ref> (Steetskamp 1995) </ref>. An important facet of SANE's neurons is that they form complete input to output mappings, which makes every neuron a primitive solution in its own right.
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the methods of temproal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44. </pages>
Reference-contexts: To learn effective decision strategies in such tasks, a learning system must be capable of learning under very general and often infrequent reinforcements. This type of learning has become known as reinforcement learning <ref> (Sutton 1988) </ref>. Kaelbling et al. (1996) identified two main branches of research in reinforcement learning: methods that search the space of behaviors and methods that search the space of value functions that assess the utility of behaviors. <p> The problems discussed in this dissertation reflect the first maze, because in many interesting and important decision tasks very little domain information is available to the decision making agent. 2.2 Temporal Difference Reinforcement Learning This section describes the temporal difference (TD) approach to reinforcement learning <ref> (Sutton 1988) </ref>. TD learning is by far the most popular reinforcement learning method and has therefore become a standard for which alternative methods must be compared. <p> The general idea of temporal 8 difference learning is first described followed by a description of its two most prominent implemen-tations: the Adaptive Heuristic Critic (Barto et al. 1983) and Q-learning (Watkins 1989; Watkins and Dayan 1992). 2.2.1 Learning Through Temporal Differences In temporal difference learning <ref> (Sutton 1988) </ref>, an evaluation or value function maintains predictions of current and future rewards. More specifically, the value function predicts the expected return from the environment given the current state of the world and the current decision policy.
Reference: <author> Syswerda, G. </author> <year> (1991). </year> <title> A study of reproduction in generational and steady-state genetic algorithms. </title> <editor> In Rawlings, G., editor, </editor> <booktitle> Foundations of Genetic Algorithms, </booktitle> <pages> 94-101. </pages> <address> San Mateo, CA: </address> <publisher> Morgan-Kaufmann. </publisher>
Reference-contexts: Otherwise, it is increased. Whitley et. al. refer to this technique as adaptive mutation which tends to increase the mutation rate as populations converge. Essentially, this method promotes diversity within the population to encourage continual exploration of the solution space. GENITOR is considered a "steady-state" genetic algorithm <ref> (Syswerda 1991) </ref>, which differs considerably from the traditional function-optimization GA. In traditional GA's, genetic operators are applied after the entire population of individuals have been evaluated. In a steady-state GA, rather than following the synchronous generation model, genetic operators are applied asynchronously often after each solution is evaluated.
Reference: <author> Tanenbaum, A. </author> <year> (1989). </year> <title> Computer Networks. Prentice-Hall. </title> <note> second edition. </note> <author> van der Smagt, P. </author> <year> (1994). </year> <title> Simderella: A robot simulator for neuro-controller design. </title> <journal> Neurocom-puting, </journal> <volume> 6(2). </volume> <editor> van der Smagt, P. </editor> <year> (1995). </year> <title> Visual Robot Arm Guidance using Neural Networks. </title> <type> PhD thesis, </type> <institution> The University of Amsterdam, </institution> <address> Amsterdam, The Netherlands. </address>
Reference-contexts: These policies are normally problem-general and do not take advantage of problem-specific knowledge inherent in each domain. For example, in a communication network, packet routing is normally decided by a shortest path strategy <ref> (Tanenbaum 1989) </ref>, which is a problem-general policy. However, Littman and Boyan (1993) showed that better routing policies can be achieved using more domain-specific knowledge such as the specific network topology and traffic patterns.
Reference: <author> Van Hentenryck, P., Simonis, H., and Dincbas, M. </author> <year> (1992). </year> <title> Constraint satisfaction using constraint logic programming. </title> <journal> Artificial Intelligence, </journal> <note> 58:113. 114 Walter, </note> <author> J. A., Martinez, T. M., and Schulten, K. J. </author> <year> (1991). </year> <title> Industrial robot learns visuo-motor co-ordination by means of neural-gas network. </title> <editor> In Kohonen, T., editor, </editor> <booktitle> Artfificial Neural Networks, </booktitle> <volume> vol. </volume> <pages> 1. </pages> <address> Amsterdam. </address>
Reference: <author> Watkins, C. J. C. H. </author> <year> (1989). </year> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> University of Cambridge, </institution> <address> England. </address>
Reference: <author> Watkins, C. J. C. H., and Dayan, P. </author> <year> (1992). </year> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8(3) </volume> <pages> 279-292. </pages>
Reference: <author> Weeks, E. R., and Burgess, J. M. </author> <year> (1996). </year> <title> Evolving artificial neural networks to control chaos. </title> <journal> Phys. </journal>
Reference: <author> Rev. E. </author> <note> (submitted). </note>
Reference: <author> Weiss, L. E., Sanderson, A. C., and Neumann, C. P. </author> <year> (1987). </year> <title> Dynamic sensor-based control of robots with visual feedback. </title> <journal> Journal of Robotics and Automation, RA-3. </journal>
Reference: <author> Werbos, P. J. </author> <year> (1992). </year> <title> Neurocontrol and supervised learning: An overview and evaluation. </title> <booktitle> In Handbook of Intelligent Control, </booktitle> <pages> 65-89. </pages> <address> New York: </address> <publisher> Van Nostrand Reinhold. </publisher>
Reference-contexts: As in any supervised learning application, it is crucial that the training corpus contains a good representative sample of the desired behavior. The most common approach for generating training examples is to flail the arm and record the resulting joint and hand positions <ref> (Werbos 1992) </ref>. For example, if the joints are initially in position ~ J and a random rotation ~ R results in hand position ~ H, a training example of the form Input : ~ J; ~ H; Output : ~ R can be constructed. <p> They found that problems that could not be solved through direct evolution could often be solved using an incremental approach. 8.2.3 Online or Local Learning All of the neural controllers presented in this dissertation can be characterized as fixed adaptive controllers <ref> (Werbos 1992) </ref>. That is, once the controller is evolved and placed in its task it does not change. Evolution may create new controllers, but these merely replace the existing controller.
Reference: <author> Werner, G. M., and Dyer, M. G. </author> <year> (1991). </year> <title> Evolution of communication in artificial organisms. </title> <editor> In Farmer, J. D., Langton, C., Rasmussen, S., and Taylor, C., editors, </editor> <booktitle> Artificial Life II. </booktitle> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference: <author> Whitley, D. </author> <year> (1989). </year> <title> The GENITOR algorithm and selective pressure. </title> <booktitle> In Proceedings of the Third International Conference on Genetic Algorithms, </booktitle> <pages> 116-121. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufman. </publisher>
Reference: <author> Whitley, D., Dominic, S., Das, R., and Anderson, C. W. </author> <year> (1993). </year> <title> Genetic reinforcement learning for neurocontrol problems. </title> <journal> Machine Learning, </journal> <volume> 13 </volume> <pages> 259-284. </pages>
Reference-contexts: The original programs written by Sutton and Anderson were used for the AHC implementations, and the simulation code developed by Pendrith (1994) was used for the Q-learning implementation. For GENITOR, the system was reimplemented as described in <ref> (Whitley et al. 1993) </ref>. A control strategy was deemed successful if it could balance a pole for 120,000 time steps. SANE SANE was implemented to evolve a 2-layer network with 5 input, 8 hidden, and 2 output units. <p> For example, a decision of "move right" with activation 0.9 would move right only 90% of the time. Probabilistic output units allow the network to visit more of the state space during training, and thus incorporate a more global view of the problem into the control policy <ref> (Whitley et al. 1993) </ref>. In the SANE implementation, however, randomness is unnecessary in the decision process since there is a large amount of state space sampling through multiple combinations of neurons. <p> Comparisons between GENITOR's and SANE's search efficiency thus test the hypothesis that the symbiotic evolution produces an efficient search without reliance on additional randomness. Since GENITOR has been shown to be effective in evolving neural networks for the inverted pendulum problem <ref> (Whitley et al. 1993) </ref>, it also provides a state-of-the-art neuro-evolution comparison. GENITOR was implemented as detailed by Whitley et al. (1993) to evolve the weights in a fully-connected 2-layer network, with additional connections from each input unit to the output layer. <p> Such variance results in a greater sampling of the solution space for each neuron and averages out the fitness evaluation noise. Another difference between SANE and Whitley's approach lies in the network architectures. In the current implementation of GENITOR <ref> (Whitley et al. 1993) </ref>, the network architecture is fixed and only the weights are evolved. The topology of the network must be resolved a priori by the implementor. <p> Perhaps the major difference between SANE and GENITOR is that SANE does not require any extra randomness to maintain diverse populations. GENITOR achieves diversity through unusually high mutation rates that produce a random point within a specific radius of the parent network <ref> (Whitley et al. 1993) </ref>. Reliance on high randomness will create diversity, however, at the expense of many disruptions to important genetic building blocks. 7.1.3 Learning Classifier Systems Learning Classifier Systems (LCS) enjoy the longest history of the ERL methods (Holland and Reitman 1978; Holland 1987; Wilson 1994).
Reference: <author> Whitley, D., and Kauth, J. </author> <year> (1988). </year> <title> GENITOR: A different genetic algorithm. </title> <booktitle> In Proceedings of the Rocky Mountain Conference on Artificial Intelligence, </booktitle> <pages> 118-130. </pages> <address> Denver, </address> <publisher> CO. </publisher>
Reference: <author> Whitley, D., Starkweather, T., and Bogart, C. </author> <year> (1990). </year> <title> Genetic algorithms and neural networks: Optimizing connections and connectivity. </title> <journal> Parallel Computing, </journal> <volume> 14 </volume> <pages> 347-361. </pages>
Reference: <author> Wijesoma, S. W., Wolfe, D. F. H., and Richards, R. J. </author> <year> (1993). </year> <title> Eye-to-hand coordination for vision-guided robot control applications. </title> <journal> The International Journal of Robotics Research, </journal> <volume> 12(1) </volume> <pages> 65-78. </pages>
Reference: <author> Williams, R. J., and Zipser, D. </author> <year> (1989). </year> <title> Experimental analysis of the real-time recurrent learning algorithm. </title> <journal> Connection Science, </journal> <volume> 1 </volume> <pages> 87-111. </pages>
Reference-contexts: Since evolutionary algorithms do not use derivatives for credit assignment, other activation functions such as linear thresholds, splines, or product units may be used just as easily. In more complex architectures such as networks with recurrent connections, computing the gradient information necessary for hill-climbing is very costly <ref> (Williams and Zipser 1989) </ref>. Since evolutionary algorithms do not rely on backpropagating error signals, evolving recurrent networks requires no extra computation over networks with no recurrent connections. The primary motivation for neuro-evolution over the more standard techniques like backpropagation, however, is the ability to train under sparse reinforcement. <p> In contrast, forming a recurrent network using temporal difference methods may not be feasible, because training recurrent connections from gradient information (i.e. backpropagation) requires many error propagations through various network states, which is very costly <ref> (Williams and Zipser 1989) </ref>. Neuro-evolution does not require error propagation procedures and can thus form recurrent neural networks without additional overhead to the learning algorithm. Recurrent networks, however, do bring up several interesting issues within SANE's symbiotic populations.
Reference: <author> Wilson, S. W. </author> <year> (1994). </year> <title> Zcs: A zeroth level classifier system. </title> <journal> Evolutionary Computation, </journal> <volume> 2(1) </volume> <pages> 1-18. </pages>
Reference: <author> Wilson, S. W. </author> <year> (1995). </year> <title> Classifier fitness based on accuracy. </title> <booktitle> Evolutionary Computation, </booktitle> <pages> 3(2). </pages>
Reference-contexts: Such classifiers are the most responsible for the good behavior of the LCS and should be selected for by evolution. Recent work, however, has suggested that a separation of classifier strength and fitness may be more appropriate to build smaller, but important niches <ref> (Wilson 1995) </ref>. Unfortunately, the progress of the LCS has been somewhat disappointing. Despite the long history of work in LCS, there are very few successful applications. Wilson and Goldberg (1989) give an excellent historical perspective of the LCS and discuss many problems that have prevented its practical implementations.
Reference: <author> Wilson, S. W., and Goldberg, D. E. </author> <year> (1989). </year> <title> A critical review of classifier systems. </title> <booktitle> In Proceedings of the Third International Conference on Genetic Algorithms. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Yao, X., and Liu, Y. </author> <year> (1996). </year> <title> Evolving artificial neural networks through evolutionary programming. </title> <booktitle> In Proceedings of the Fifth Annual Conference on Evolutionary Programming. </booktitle> <address> San Diega, CA. </address> <month> 115 </month>
References-found: 112

