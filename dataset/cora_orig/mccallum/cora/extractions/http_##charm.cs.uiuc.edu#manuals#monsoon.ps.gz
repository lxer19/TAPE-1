URL: http://charm.cs.uiuc.edu/manuals/monsoon.ps.gz
Refering-URL: http://charm.cs.uiuc.edu/manuals/
Root-URL: http://www.cs.uiuc.edu
Title: Multithreading: A Revisionist View of Dataflow Architectures Computation Structures Group  
Author: Gregory M. Papadopoulos Kenneth R. Traub 
Date: March, 1991  
Pubnum: Memo 330  
Abstract: This report describes research done at the Laboratory for Computer Science of the Massachusetts Institute of Technology. Funding for the Laboratory is provided in part by the Advanced Research Projects Agency of the Department of Defense under the Office of Naval Research contract N00014-89-J-1988. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal, Ben-Hong Lim, D. Kranz, and J. Kubi-atowicz. </author> <month> APRIL: </month> <title> A Processor Architecture for Multiprocessing. </title> <booktitle> In Proceedings of the 17th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 104-114, </pages> <address> Sealt-tle, Washington, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: The Denelcor HEP [14] interleaved up to 64 threads per processing element in order to hide the latency of remote memory references. More recently, researchers have suggested rapid context switching among fewer threads to mitigate the cost of occasional cache misses <ref> [15, 1] </ref>. While useful for dealing with unpredictable memory latency, these approaches do not fundamentally add to the basic von Neu-mann programming model because only a small number of threads can be managed efficiently by a processor.
Reference: [2] <author> Arvind, D. E. Culler, and K. Ekanadham. </author> <title> The Price of Asynchronous Parallelism: an Analysis of Dataflow Architectures. </title> <booktitle> In Proceedings of CONPAR 88, </booktitle> <institution> Univ. of Manchester, </institution> <month> September </month> <year> 1988. </year> <journal> British Computer Society | Parallel Processing Specialists. </journal> <note> (also CSG Memo No. 278, </note> <institution> MIT Laboratory for Computer Science). </institution>
Reference-contexts: In careful comparative studies of scientific codes, we have found that, for a given algorithm, a dataflow machine tends to execute two or three times as many instructions as a von Neumann uniprocessor <ref> [2] </ref>. Unfortunately, the overhead is incurred even when a good static schedule is known at compile-time.
Reference: [3] <author> Arvind, D. E. Culler, and G. K. Maa. </author> <title> Assessing the Benefits of Fine-Grain Parallelism in Dataflow Programs. </title> <journal> The International Journal of Supercomputer Applications, </journal> <volume> 2(3), </volume> <month> November </month> <year> 1988. </year>
Reference-contexts: 1 Introduction The ability of dataflow machines to expose ample amounts of all sorts of parallelism|instruction, loop, procedure, producer/consumer, unstructured|is well documented <ref> [3] </ref>, and continues to be an attractive feature of the approach. This ability derives from fundamental properties of dynamic data flow graphs, which are directly executed as a dataflow processor's machine language.
Reference: [4] <author> Arvind and R. A. </author> <title> Iannucci. Two Fundamental Issues in Multiprocessing. </title> <booktitle> In Proceedings of DFVLR Conference 1987 on Parallel Processing in Science and Engineering, </booktitle> <address> Bonn-Bad Godesberg, </address> <publisher> W. </publisher> <address> Germany, </address> <month> June </month> <year> 1987. </year>
Reference-contexts: Computed, global memory references are performed with split-phase transactions <ref> [4] </ref>, in which a thread issues a memory request and continues while the request is processed concurrently. This is the means by which the Monsoon architecture tolerates long memory latency.
Reference: [5] <author> Arvind, R. S. Nikhil, and K. K. Pingali. I-Structures: </author> <title> Data Structures for Parallel Computing. </title> <type> Technical Report Computation Structures Group Memo 269, </type> <institution> MIT Laboratory for Computer Science, 545 Technology Square, </institution> <address> Cambridge, MA, </address> <month> February </month> <year> 1987. </year> <title> (Also appears in Proceedings of the Graph Reduction Workshop, </title> <address> Santa Fe, NM. </address> <month> October </month> <year> 1986.). </year>
Reference-contexts: More simply, the hardware supports strong sequential consistency. Monsoon supports a number of synchronizing memory transactions in addition to the imperative Fetch and Store operations already discussed. The most common of these are the I-Structure operations I-Fetch and I-Store <ref> [5] </ref>. These use presence bits to implement a synchronizing write-once protocol: an I-Fetch to an empty location is deferred by saving the return continuation in the location, and a subsequent I-Store causes the fetch to be satisfied.
Reference: [6] <author> V. G. Grafe and J. E. Hoch. </author> <title> The Epsilon-2 Hybrid Dataflow Architecture. </title> <booktitle> In Proceedings of Compcon90, </booktitle> <pages> pages 88-93, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: Moreover, given sufficient parallelism, dynamic instruction scheduling has the added pragmatic benefit of being resilient to long and unpredictable communication latency. The most recent generation of dataflow machines (e.g., MIT's Monsoon [11, 12], ETL's EM-4 [13], and Sandia's Epsilon-2 <ref> [6] </ref>) have shown how operand matching can be accomplished with simple hardware structures in two machine cycles. There does seem to be an unavoidable price of purely dynamic instruction scheduling, however. <p> That is, we would like to be able to create a sequence of instructions which get executed, in order, once the first instruction in the sequence has been dynamically scheduled. This value of this ability has been recognized by several researchers <ref> [9, 13, 6] </ref>. In the case of synchronous circular pipelines, such as Monsoon, this capability is surprisingly simple to implement: an instruction merely demands that one of its successor tokens re-enter the pipeline immediately following its execution.
Reference: [7] <author> J. L. Hennessy and T. Gross. </author> <title> Postpass Code Optimization of Pipeline Constraints. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 5(3) </volume> <pages> 422-448, </pages> <month> July </month> <year> 1983. </year>
Reference-contexts: From another perspective, multithreading is an acknowledgment of the empirical fact that instruction-level parallelism is probably better exploited in the context of a sequential thread executing in a well-engineered pipeline <ref> [7] </ref> (e.g., superscalar RISC), rather than from the dynamic execution of the equivalent dataflow graph. However, we must caution against extending this line of reasoning too far.
Reference: [8] <author> R. A. </author> <title> Iannucci. A Dataflow/von Neumann Hybrid Architecture. </title> <type> Technical Report TR-418, </type> <institution> MIT Laboratory for Computer Science, 545 Technology Square, </institution> <address> Cam-bridge, MA, </address> <month> May </month> <year> 1988. </year> <type> (PhD Thesis, </type> <institution> Dept. of EECS, MIT). </institution>
Reference-contexts: The VNDF hybrid machine maintains presence information on every memory location, and a thread can suspend or proceed as a function of the presence state <ref> [8] </ref>. P-RISC [10] attempts to reduce the implementation complexity of the hybrid approach by eliminating the presence bits in favor of explicit counting semaphores which are manipulated only at the beginning of a thread.
Reference: [9] <author> R. A. </author> <title> Iannucci. Toward a Dataflow/von Neumann Hybrid Architecture. </title> <booktitle> In Proc. 15th Int. Symp. on Computer Architecture, </booktitle> <pages> pages 131-140, </pages> <year> 1988. </year>
Reference-contexts: In this case, intermediate values could be communicated among instructions in the sequence via temporary registers, rather than asynchronously propagating values on tokens <ref> [9, 13] </ref>. Another objection to pure dataflow graphs concerns the coding of low-level operating system and resource management functions, such as trap handlers and storage allocation routines. These operations require frequent, guaranteed exclusive access to processor state and need critical sections around the updating of shared data structures. <p> That is, we would like to be able to create a sequence of instructions which get executed, in order, once the first instruction in the sequence has been dynamically scheduled. This value of this ability has been recognized by several researchers <ref> [9, 13, 6] </ref>. In the case of synchronous circular pipelines, such as Monsoon, this capability is surprisingly simple to implement: an instruction merely demands that one of its successor tokens re-enter the pipeline immediately following its execution.
Reference: [10] <author> R. S. Nikhil and Arvind. </author> <booktitle> Can Dataflow Subsume von Neumann Computing? In Proceedings of the 16th Annual International Symposium on Computer Architecture, </booktitle> <address> Jerusalem, Israel, </address> <month> May </month> <year> 1989. </year> <note> To appear. </note>
Reference-contexts: The VNDF hybrid machine maintains presence information on every memory location, and a thread can suspend or proceed as a function of the presence state [8]. P-RISC <ref> [10] </ref> attempts to reduce the implementation complexity of the hybrid approach by eliminating the presence bits in favor of explicit counting semaphores which are manipulated only at the beginning of a thread.
Reference: [11] <author> G. M. Papadopoulos. </author> <title> Implementation of a General Purpose Dataflow Multiprocessor. </title> <type> Technical Report TR432, </type> <institution> MIT Laboratory for Computer Science, 545 Technology Square, </institution> <address> Cambridge, MA, </address> <month> September </month> <year> 1988. </year> <type> (PhD Thesis, </type> <institution> Dept. of EECS, MIT). </institution>
Reference-contexts: Moreover, given sufficient parallelism, dynamic instruction scheduling has the added pragmatic benefit of being resilient to long and unpredictable communication latency. The most recent generation of dataflow machines (e.g., MIT's Monsoon <ref> [11, 12] </ref>, ETL's EM-4 [13], and Sandia's Epsilon-2 [6]) have shown how operand matching can be accomplished with simple hardware structures in two machine cycles. There does seem to be an unavoidable price of purely dynamic instruction scheduling, however. <p> A more detailed explanation, including what happens when more than one I-Fetch request arrives before the corresponding I-Store, may be found in <ref> [11] </ref>. Monsoon also supports a pair of mutual exclusion operations called Take and Put.
Reference: [12] <author> G. M. Papadopoulos and D. E. Culler. Monsoon: </author> <title> an Explicit Token-Store Architecture. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Archtecture, </booktitle> <year> 1990. </year>
Reference-contexts: Moreover, given sufficient parallelism, dynamic instruction scheduling has the added pragmatic benefit of being resilient to long and unpredictable communication latency. The most recent generation of dataflow machines (e.g., MIT's Monsoon <ref> [11, 12] </ref>, ETL's EM-4 [13], and Sandia's Epsilon-2 [6]) have shown how operand matching can be accomplished with simple hardware structures in two machine cycles. There does seem to be an unavoidable price of purely dynamic instruction scheduling, however.
Reference: [13] <author> S. Sakai, Y. Yamaguchi, K. Hiraki, Y. Kodama, and T. Yuba. </author> <title> An Architecture of a Dataflow Single Chip Processor. </title> <booktitle> In Proceedings of the 16th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 46-53, </pages> <address> Jerusalem, Israel, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: Moreover, given sufficient parallelism, dynamic instruction scheduling has the added pragmatic benefit of being resilient to long and unpredictable communication latency. The most recent generation of dataflow machines (e.g., MIT's Monsoon [11, 12], ETL's EM-4 <ref> [13] </ref>, and Sandia's Epsilon-2 [6]) have shown how operand matching can be accomplished with simple hardware structures in two machine cycles. There does seem to be an unavoidable price of purely dynamic instruction scheduling, however. <p> In this case, intermediate values could be communicated among instructions in the sequence via temporary registers, rather than asynchronously propagating values on tokens <ref> [9, 13] </ref>. Another objection to pure dataflow graphs concerns the coding of low-level operating system and resource management functions, such as trap handlers and storage allocation routines. These operations require frequent, guaranteed exclusive access to processor state and need critical sections around the updating of shared data structures. <p> That is, we would like to be able to create a sequence of instructions which get executed, in order, once the first instruction in the sequence has been dynamically scheduled. This value of this ability has been recognized by several researchers <ref> [9, 13, 6] </ref>. In the case of synchronous circular pipelines, such as Monsoon, this capability is surprisingly simple to implement: an instruction merely demands that one of its successor tokens re-enter the pipeline immediately following its execution.
Reference: [14] <author> B. J. Smith. </author> <title> A Pipelined, Shared Resource MIMD Computer. </title> <booktitle> In Proceedings of the 1978 International Conference on Parallel Processing, </booktitle> <pages> pages 6-8, </pages> <year> 1978. </year>
Reference-contexts: The von Neumann architecture can be extended into a multithreaded model by replicating the program counter and register set, and by providing primitives to synchronize among the several threads of control. The Denelcor HEP <ref> [14] </ref> interleaved up to 64 threads per processing element in order to hide the latency of remote memory references. More recently, researchers have suggested rapid context switching among fewer threads to mitigate the cost of occasional cache misses [15, 1].
Reference: [15] <author> W. Weber and A. Gupta. </author> <title> Exploring the Benefits of Multiple Hardware Contexts in a Multiprocessor Architecture: Preliminary Results. </title> <booktitle> In Proceedings of the 1989 International Symposium on Computer Architecture, </booktitle> <pages> pages 273-280, </pages> <address> Jerusalem, Israel, </address> <month> May </month> <year> 1989. </year>
Reference-contexts: The Denelcor HEP [14] interleaved up to 64 threads per processing element in order to hide the latency of remote memory references. More recently, researchers have suggested rapid context switching among fewer threads to mitigate the cost of occasional cache misses <ref> [15, 1] </ref>. While useful for dealing with unpredictable memory latency, these approaches do not fundamentally add to the basic von Neu-mann programming model because only a small number of threads can be managed efficiently by a processor.
References-found: 15

