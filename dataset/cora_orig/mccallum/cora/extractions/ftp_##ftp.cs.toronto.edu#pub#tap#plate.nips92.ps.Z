URL: ftp://ftp.cs.toronto.edu/pub/tap/plate.nips92.ps.Z
Refering-URL: http://www.cs.utoronto.ca/~tap/
Root-URL: 
Title: In  Holographic Recurrent Networks  of ordinary recurrent networks on these sequence generation tasks.  
Author: C. L. Giles, S. J. Hanson, and J. D. Cowan, Tony A. Plate 
Affiliation: Department of Computer Science University of Toronto  
Date: 1993  
Address: San Mateo, CA,  Toronto, M5S 1A4 Canada  
Note: editors, Advances in Neural Information Processing Systems 5 (NIPS*92), Morgan Kaufmann,  that  
Abstract:  
Abstract-found: 1
Intro-found: 1
Reference: [ Elman, 1991 ] <author> J. Elman. </author> <title> Distributed representations, simple recurrent networks and grammatical structure. </title> <journal> Machine Learning, </journal> 7(2/3):195-226, 1991. 
Reference-contexts: This paper introduces Holographic Recurrent Networks (HRNs), which are recurrent nets that incorporate these techniques for generating sequences of symbols or trajectories through continuous space. The recurrent component of these networks uses convolution operations rather than the logistic-of-matrix-vector-product traditionally used in simple recurrent networks (SRNs) <ref> [ Elman, 1991, Servan-Schreiber et al., 1991 ] </ref> . <p> memory techniques in networks trained by gradient descent; (2) to see whether adapting representations can improve the capacity of HRRs; and (3) to compare performance of HRNs with SRNs. 34 1.1 RECURRENT NETWORKS & SEQUENTIAL PROCESSING SRNs have been used successfully to process sequential input and induce finite state grammars <ref> [ Elman, 1991, Servan-Schreiber et al., 1991 ] </ref> . However, training times were extremely long, even for very simple grammars. This appeared to be due to the difficulty of finding a recurrent operation that preserved sufficient context [ Maskara and Noetzel, 1992 ] . <p> Nonetheless this figure provides a useful benchmark against which to compare the capacity of HRNs which adapt vectors using gradient descent. 36 3 TRAJECTORY ASSOCIATION & RECURRENT NETS HRNs incorporate the trajectory-association scheme in recurrent networks. HRNs are very similar to SRNs, such as those used by <ref> [ Elman, 1991 ] </ref> and [ Servan-Schreiber et al., 1991 ] . However, the task used in this paper is different: the generation of target sequences at the output units, with inputs that do not vary in time.
Reference: [ Maskara and Noetzel, 1992 ] <author> Arun Maskara and Andrew Noetzel. </author> <title> Forcing simple recurrent neural networks to encode context. </title> <booktitle> In Proceedings of the 1992 Long Island Conference on Artificial Intelligence and Computer Graphics, </booktitle> <year> 1992. </year>
Reference-contexts: However, training times were extremely long, even for very simple grammars. This appeared to be due to the difficulty of finding a recurrent operation that preserved sufficient context <ref> [ Maskara and Noetzel, 1992 ] </ref> . In the work reported in this paper the task is reversed to be one of generating sequential output. Furthermore, in order to focus on the context retention aspect, no grammar induction is required. 1.2 CIRCULAR CONVOLUTION Circular convolution is an associative memory operator.
Reference: [ Plate, 1991a ] <author> Tony A. </author> <title> Plate. Holographic Reduced Representations. </title> <type> Technical Report CRG-TR-91-1, </type> <institution> Department of Computer Science, University of Toronto, </institution> <year> 1991. </year>
Reference-contexts: Multiple associations can be summed. The result can be decoded by convolving with the exact inverse or approximate inverse, though the latter generally gives more stable results. Holographic Reduced Representations <ref> [ Plate, 1991a, Plate, 1991b ] </ref> use circular convolution for associating elements of a structure in a way that can embody hierarchical structure. <p> The most similar item in clean up memory will probably be c. The clean up memory should recognize this and output the clean version of c. 2.3 CAPACITY OF TRAJECTORY-ASSOCIATION In <ref> [ Plate, 1991a ] </ref> the capacity of circular-convolution based associative memory was calculated. It was assumed that the elements of all vectors (dimension n) were chosen randomly from a gaussian distribution with mean zero and variance 1=n (giving an expected Euclidean length of 1.0). <p> HRNs appear to be more suited to this task than SRNs, though SRNs did surprisingly well. The relatively high generative capacity of HRNs shows that the capacity of circular convolution associative memory <ref> [ Plate, 1991a ] </ref> can be greatly improved by adapting representations of vectors.
Reference: [ Plate, 1991b ] <author> Tony A. </author> <title> Plate. Holographic Reduced Representations: Convolution algebra for compositional distributed representations. </title> <editor> In John Mylopoulos and Ray Reiter, editors, </editor> <booktitle> Proceedings of the 12th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 30-35, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: 1 INTRODUCTION The representation and processing of data with complex structure in neural networks remains a challenge. In a previous paper <ref> [ Plate, 1991b ] </ref> I described Holographic Reduced Representations (HRRs) which use circular-convolution associative-memory to embody sequential and recursive structure in fixed-width distributed representations. This paper introduces Holographic Recurrent Networks (HRNs), which are recurrent nets that incorporate these techniques for generating sequences of symbols or trajectories through continuous space. <p> Multiple associations can be summed. The result can be decoded by convolving with the exact inverse or approximate inverse, though the latter generally gives more stable results. Holographic Reduced Representations <ref> [ Plate, 1991a, Plate, 1991b ] </ref> use circular convolution for associating elements of a structure in a way that can embody hierarchical structure.
Reference: [ Rumelhart et al., 1986 ] <author> D. E. Rumelhart, G. E. Hinton, and Williams R. J. </author> <title> Learning internal representations by error propagation. </title> <booktitle> In Parallel distributed processing: Explorations in the microstructure of cognition, </booktitle> <volume> volume 1, </volume> <pages> pages 318-362. </pages> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: This penalty term helped the HRN considerably but did not noticeably improve the performance of the SRN. The partial derivatives for the activations were computed by the unfolding in time method <ref> [ Rumelhart et al., 1986 ] </ref> .
Reference: [ Sejnowski and Rosenberg, 1986 ] <author> T. J. Sejnowski and C. R. Rosenberg. NETtalk: </author> <title> A parallel network that learns to read aloud. </title> <type> Technical report 86-01, </type> <institution> Department of Electrical Engineering and Computer Science, Johns Hopkins University, Baltimore, MD., </institution> <year> 1986. </year>
Reference-contexts: One approach has been to use a fixed window on the sequence, e.g., as in NETtalk <ref> [ Sejnowski and Rosenberg, 1986 ] </ref> . A disadvantage of this is any fixed size of window may not be large enough in some situations. Another approach is to use a recurrent net to retain information about previous inputs.
Reference: [ Servan-Schreiber et al., 1991 ] <author> David Servan-Schreiber, Axel Cleeremans, and James L. McClelland. </author> <title> Graded state machines: The representation of temporal contingencies in simple recurrent networks. </title> <journal> Machine Learning, </journal> 7(2/3):161-194, 1991. 
Reference-contexts: This paper introduces Holographic Recurrent Networks (HRNs), which are recurrent nets that incorporate these techniques for generating sequences of symbols or trajectories through continuous space. The recurrent component of these networks uses convolution operations rather than the logistic-of-matrix-vector-product traditionally used in simple recurrent networks (SRNs) <ref> [ Elman, 1991, Servan-Schreiber et al., 1991 ] </ref> . <p> memory techniques in networks trained by gradient descent; (2) to see whether adapting representations can improve the capacity of HRRs; and (3) to compare performance of HRNs with SRNs. 34 1.1 RECURRENT NETWORKS & SEQUENTIAL PROCESSING SRNs have been used successfully to process sequential input and induce finite state grammars <ref> [ Elman, 1991, Servan-Schreiber et al., 1991 ] </ref> . However, training times were extremely long, even for very simple grammars. This appeared to be due to the difficulty of finding a recurrent operation that preserved sufficient context [ Maskara and Noetzel, 1992 ] . <p> HRNs are very similar to SRNs, such as those used by [ Elman, 1991 ] and <ref> [ Servan-Schreiber et al., 1991 ] </ref> . However, the task used in this paper is different: the generation of target sequences at the output units, with inputs that do not vary in time.
Reference: [ Simard and LeCun, 1992 ] <author> P. Simard and Y. LeCun. </author> <title> Reverse TDNN: an architecture for trajectory generation. </title> <editor> In J. M. Moody, S. J. Hanson, and R. P. Lippman, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4 (NIPS*91), </booktitle> <address> Denver, CO, 1992. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 41 </pages>
References-found: 8

