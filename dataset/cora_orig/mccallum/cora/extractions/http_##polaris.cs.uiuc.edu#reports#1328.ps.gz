URL: http://polaris.cs.uiuc.edu/reports/1328.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: The parallel solution of nonsymmetric sparse linear systems using the H* reordering and an associated factorization.  
Author: K. A. Gallivan B. A. Marsolf H. A. G. Wijshoff 
Address: Urbana, Illinois 61801  Leiden, The Netherlands  
Affiliation: Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign  High Performance Computing Division Department of Computer Science Leiden University  
Abstract: In this paper, a nonsymmetric sparse linear system solver based on the exploitation of multilevel parallelism is proposed. One of the main issues addressed is the application of tearing techniques to enhance large grain parallelism in a manner that maintains reasonable stability. This is accomplished by a combination of a novel reordering technique (H*) and pivoting strategy. The large grain parallelism exposed by the reordering is combined with medium (various parallel row updates strategies) and fine grain (vector-ization) parallelism to allow adaptation to a wide range of multiprocessor architectures. Experimental results are presented which show the effectiveness of the reordering, as well as the stability and efficiency of the solver. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Alaghband, G. </author> <title> A parallel pivoting algorithm on a shared memory multiprocessor. </title> <booktitle> In Proceedings of the 1988 International Conference on Parallel Processing, Volume3: Applications and Algorithms (1988), </booktitle> <volume> vol. 3, </volume> <pages> pp. 177-180. </pages>
Reference: [2] <author> Arioli, M., and Duff, I. S. </author> <title> Experiments in tearing large sparse systems. In Reliable Numerical Computation, </title> <editor> M. G. Cox and S. Hammarling, Eds. </editor> <publisher> Oxford University Press, </publisher> <address> New York, </address> <year> 1990, </year> <pages> pp. 207-226. </pages>
Reference-contexts: Finally, the complexity of the synchronization during the factorization and the application of the factorization to subsequent right-hand side vectors is nontrivial compared to other ways of handling the problem. Other strategies discussed previously in the literature have resulted in solvers with either unacceptable cost or stability control, e.g., <ref> [2, 3, 4] </ref>. We would like to develop a strategy that preserves the overall structure of the matrix while allowing the implementation of a global pivoting strategy which yields a factorization with stability similar to more conventional nonsymmetric solvers, such as ma28 [8].
Reference: [3] <author> Arioli, M., Duff, I. S., Gould, N. I. M., and Reid, J. K. </author> <title> The practical use of the Hellerman-Rarick P 4 algorithm and the P 5 variant of Erisman et al. </title> <type> Tech. Rep. Report CSS 213, </type> <institution> CSS Division, Harwell Laboratory, </institution> <address> England, </address> <year> 1987. </year>
Reference-contexts: Finally, the complexity of the synchronization during the factorization and the application of the factorization to subsequent right-hand side vectors is nontrivial compared to other ways of handling the problem. Other strategies discussed previously in the literature have resulted in solvers with either unacceptable cost or stability control, e.g., <ref> [2, 3, 4] </ref>. We would like to develop a strategy that preserves the overall structure of the matrix while allowing the implementation of a global pivoting strategy which yields a factorization with stability similar to more conventional nonsymmetric solvers, such as ma28 [8].
Reference: [4] <author> Arioli, M., Duff, I. S., Gould, N. I. M., and Reid, J. K. </author> <title> Use of the P 4 and P 5 algorithms for in-core factorization of sparse matrices. </title> <journal> SIAM J. Sci. Stat. Comput. </journal> <volume> 11, </volume> <month> 5 (September </month> <year> 1990), </year> <pages> 913-927. </pages>
Reference-contexts: Finally, the complexity of the synchronization during the factorization and the application of the factorization to subsequent right-hand side vectors is nontrivial compared to other ways of handling the problem. Other strategies discussed previously in the literature have resulted in solvers with either unacceptable cost or stability control, e.g., <ref> [2, 3, 4] </ref>. We would like to develop a strategy that preserves the overall structure of the matrix while allowing the implementation of a global pivoting strategy which yields a factorization with stability similar to more conventional nonsymmetric solvers, such as ma28 [8]. <p> This compares favorably to results in <ref> [4] </ref> where the augmentation of their limited pivoting strategy with iterative refinement did not produce satisfactory results. Matrix Relative Error Fill-in (fi10 3 ) mcsparse ma28 mcsp. ma28 W/out With u = u = u = u = I.R.
Reference: [5] <author> Ashcraft, C. C., Grimes, R. G., Lewis, J. G., Pey-ton, B. W., and Simon, H. D. </author> <title> Progress in sparse matrix methods for large linear systems on vector supercomputers. </title> <journal> Intl. J. Supercomputing Appl. </journal> <volume> 1, </volume> <month> 4 (Winter </month> <year> 1987), </year> <pages> 10-30. </pages>
Reference: [6] <author> Cheung, L. K., and Kuh, E. S. </author> <title> The bordered triangular matrix and minimum essential sets of a digraph. </title> <journal> I.E.E.E. Transactions on Circuits and Systems CAS-21, </journal> <month> 5 (September </month> <year> 1974), </year> <pages> 633-639. </pages>
Reference: [7] <author> Davis, T. </author> <title> A parallel algorithm for sparse unsymmetric LU factorization. </title> <type> Tech. Rep. CSRD Report No. 907, </type> <institution> Center for Supercomputing Research and Development, University of Illinois, Urbana, IL, </institution> <year> 1989. </year> <type> PhD. thesis. </type>
Reference-contexts: This row permutation also destroys the structure of the matrix. Row permutations with the border, at the appropriate point in the factorization, do in fact preserve the bordered block upper triangular structure. For example, pairwise pivoting could be used to eliminate the rows of the border in parallel <ref> [7] </ref>.
Reference: [8] <author> Duff, I. S. </author> <title> Ma28- a set of fortran subroutines for sparse unsymmetric linear equations. </title> <type> Tech. Rep. </type> <institution> Report AERE R8730, </institution> <address> HMSO, London, </address> <year> 1977. </year> <month> 11 </month>
Reference-contexts: We would like to develop a strategy that preserves the overall structure of the matrix while allowing the implementation of a global pivoting strategy which yields a factorization with stability similar to more conventional nonsymmetric solvers, such as ma28 <ref> [8] </ref>. We will, however, allow the size of the border to increase during the factorization. <p> For a more detailed look at the Cedar performance results for mcsparse see [21]. 6.1 Stability Results To determine if mcsparse was stable, a comparison was made between the stability of mcsparse and ma28, <ref> [8] </ref>, a standard sparse system solver. <p> can be combined to determine the standard speed up comparing one processor to 32 processors.) 6.4 Performance Comparison for mcsparse and ma28 In this section we give performance results for mcsparse on one cluster of Cedar, an Alliant FX/8, and compare its effectiveness against a known sequential sparse solver, ma28 <ref> [8] </ref>. The solution times of the large matrices from the RUA collection for both the mcsparse and ma28 solvers are presented in Table 5. This table contains the wall clock times for the solutions as collected in single-user mode on the one cluster of Cedar.
Reference: [9] <author> Duff, I. S. </author> <title> Algorithm 575. permutations for a zero-free diagonal. </title> <journal> ACM Trans. Math. </journal> <volume> Software 7, </volume> <month> 3 (September </month> <year> 1981), </year> <pages> 387-390. </pages>
Reference: [10] <author> Duff, I. S. </author> <title> On algorithms for obtaining a maximum transversal. </title> <journal> ACM Trans. Math. </journal> <volume> Software 7, 3 (Septem-ber 1981), </volume> <pages> 315-330. </pages>
Reference: [11] <author> Duff, I. S. </author> <title> Parallel implementation of multifrontal schemes. </title> <booktitle> Parallel Computing 3 (1986), </booktitle> <pages> 193-204. </pages>
Reference: [12] <author> Duff, I. S., Erisman, A. M., and Reid, J. K. </author> <title> Direct Methods for Sparse Matrices. </title> <publisher> Oxford University Press, </publisher> <address> New York, </address> <year> 1986. </year>
Reference: [13] <author> Duff, I. S., and Reid, J. K. </author> <title> The multifrontal solution of indefinite sparse symmetric linear equations. </title> <journal> ACM Trans. Math. </journal> <volume> Software 9 (1983), </volume> <pages> 302-325. </pages>
Reference: [14] <author> Erisman, A. M., Grimes, R. G., Lewis, J. G., and Poole, W. G. </author> <title> A structurally stable modification of Hellerman-Rarick's P 4 algorithm for reordering unsym-metric sparse matrices. </title> <journal> SIAM J. Numer. Anal. </journal> <volume> 22, </volume> <month> 2 (April </month> <year> 1985), </year> <pages> 369-385. </pages>
Reference: [15] <author> Erisman, A. M., Grimes, R. G., Lewis, J. G., Poole, W. G., and Simon, H. D. </author> <title> Evaluation of or-derings for unsymmetric sparse matrices. </title> <journal> SIAM J. Sci. Stat. Comput. </journal> <volume> 8, </volume> <month> 4 (July </month> <year> 1987), </year> <pages> 600-624. </pages>
Reference: [16] <author> Gallivan, K., Marsolf, B., and Wijshoff, H. </author> <title> A large-grain parallel sparse system solver. </title> <booktitle> In Proc. Fourth SIAM Conf. on Parallel Proc. for Scient. Comp. </booktitle> <address> (Chicago, IL, </address> <year> 1989), </year> <pages> pp. 23-28. </pages>
Reference: [17] <author> Gallivan, K., Sameh, A., and Zlatev, Z. </author> <title> Parallel hybrid sparse linear system solver. </title> <booktitle> Computing systems in engineering 1 (1990), </booktitle> <pages> 183-195. </pages>
Reference-contexts: Initial results, [40], indicate that mcsparse can be adapted to use a combination of positional dropping, i.e., ignoring a fill-in element due to its position in the matrix, and numerical dropping, i.e., ignoring a fill-in element because of its relative magnitude <ref> [18, 17] </ref>, to produce a preconditioner for conjugate gradient-like algorithms. Finally, the techniques used in mcsparse should be considered for use with more conventional approaches to solving systems with tearing techniques, e.g., exploiting the Sherman-Morrison-Woodbury formula.
Reference: [18] <author> Gallivan, K., Sameh, A., and Zlatev, Z. </author> <title> Solving general sparse linear systems using conjugate gradient-type methods. </title> <booktitle> In Proceedings of the 1990 International Conference on Supercomputing (New York, 1990), </booktitle> <publisher> ACM Press, </publisher> <pages> pp. 132-139. </pages> <address> June 11-15, 1990, Am-sterdam, The Netherlands. </address>
Reference-contexts: Initial results, [40], indicate that mcsparse can be adapted to use a combination of positional dropping, i.e., ignoring a fill-in element due to its position in the matrix, and numerical dropping, i.e., ignoring a fill-in element because of its relative magnitude <ref> [18, 17] </ref>, to produce a preconditioner for conjugate gradient-like algorithms. Finally, the techniques used in mcsparse should be considered for use with more conventional approaches to solving systems with tearing techniques, e.g., exploiting the Sherman-Morrison-Woodbury formula.
Reference: [19] <author> Gallivan, K., Sameh, A., and Zlatev, Z. </author> <title> Parallel direct method codes for general sparse matrices. </title> <type> Tech. Rep. CSRD Report No. 1143, </type> <institution> Center for Supercomputing Research and Development, University of Illi-nois, Urbana, IL, </institution> <year> 1991. </year> <note> To appear in Proceedings of NATO ASI on Linear Systems. </note> <editor> Bertocchi, Spedicato and Vespucci, eds., </editor> <year> 1991. </year>
Reference-contexts: Nevertheless, the superiority of mcsparse is often large enough to indicate any performance increase via a redesign of ma28 to apply parallel pivots might still fall short. In any case, mcsparse often compares favorably with such a parallel pivots code for nonsymmetric systems. The interested reader should see <ref> [19] </ref> for the performance of the nonsymmetric sparse code y12m2. 7 Conclusions A parallel solver for nonsymmetric linear systems of equations, mcsparse, was introduced, which combines different granularities of parallelism.
Reference: [20] <author> Gallivan, K. A., Marsolf, B. A., and Wijshoff, H. A. G. </author> <title> MCSPARSE: A parallel sparse unsymmet-ric linear system solver. </title> <type> Tech. Rep. CSRD Report No. 1142, </type> <institution> Center for Supercomputing Research and Development, University of Illinois, Urbana, IL, </institution> <year> 1991. </year>
Reference-contexts: For further details about this estimate see <ref> [20] </ref>. D i 8i; i &lt; j. Therefore, when a pivot is selected in diagonal D i it will not perform any updates on the rows in block D j . Nor will the pivots in the block D j update any of the rows in block D i . <p> There are, of course, many combinations of casting that can be done. The combination used here and its parameters have been tuned via many experiments. The interested reader is directed to <ref> [20] </ref> for the details of the casting comparisons and tuning. Also, mcsparse was run both with and without iterative refinement (I.R.). Table 3 compares the stability for fourteen of the large RUA matrices.
Reference: [21] <author> Gallivan, K. A., Marsolf, B. A., and Wijshoff, H. A. G. </author> <title> Solving large nonsymmetric sparse linear systems on Cedar. </title> <type> Tech. Rep. CSRD Report No. 1313, </type> <institution> Center for Supercomputing Research and Development, University of Illinois, Urbana, IL, </institution> <year> 1993. </year>
Reference-contexts: For a more detailed look at the Cedar performance results for mcsparse see <ref> [21] </ref>. 6.1 Stability Results To determine if mcsparse was stable, a comparison was made between the stability of mcsparse and ma28, [8], a standard sparse system solver.
Reference: [22] <author> George, A. </author> <title> Nested dissection of a regular finite element mesh. </title> <journal> SIAM J. Numer. Anal. </journal> <volume> 10, </volume> <month> 2 (April </month> <year> 1973), </year> <pages> 345-363. </pages>
Reference: [23] <author> George, A. </author> <title> An automatic one-way dissection algorithm for irregular finite element problems. </title> <journal> SIAM J. Numer. Anal. </journal> <volume> 17, </volume> <month> 6 (December </month> <year> 1980), </year> <pages> 740-751. </pages>
Reference: [24] <author> George, A., and Liu, J. </author> <title> Computer Solution of Large Sparse Positive Definite Systems. </title> <publisher> Prentice Hall, </publisher> <year> 1981. </year>
Reference: [25] <author> George, A., and Liu, J. W. H. </author> <title> An automatic nested dissection algorithm for irregular finite-element problems. </title> <journal> SIAM J. Numer. Anal. </journal> <volume> 15 (1978), </volume> <pages> 1053-1069. </pages>
Reference: [26] <author> Gustavson, F. </author> <title> Finding the block lower triangular form of a matrix. In Sparse Matrix Computations, </title> <editor> J. Bunch and D. Rose, Eds. </editor> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1976. </year>
Reference: [27] <author> Hall, Jr., M. </author> <title> An algorithm for distinct representatives. </title> <journal> The American Mathematical Monthly 63, </journal> <volume> 10 (De-cember 1956), </volume> <pages> 716-717. </pages>
Reference: [28] <author> Hellerman, E., and Rarick, D. C. </author> <title> The partitioned preassigned pivot procedure (P 4 ). In Sparse Matrices and their Applications, </title> <editor> D. J. Rose and R. A. Willoughby, Eds. </editor> <publisher> Plenum, </publisher> <address> New York, </address> <year> 1972. </year>
Reference: [29] <author> Hopcroft, J. E., and Karp, R. M. </author> <title> An n 5 2 algorithm for maximum matchings in bipartite graphs. </title> <journal> SIAM J. Comput. </journal> <volume> 2, </volume> <month> 4 (December </month> <year> 1973), </year> <pages> 225-231. </pages>
Reference: [30] <author> Kuhn, H. W. </author> <title> The hungarian method for the assignment problem. </title> <journal> Naval Research Logistics Quarterly 2, </journal> <month> 1 (March </month> <year> 1955), </year> <pages> 83-97. </pages>
Reference: [31] <author> Leiserson, C., and Lewis, J. </author> <title> Orderings for parallel sparse symmetric factorization. </title> <booktitle> In Proc. Third SIAM Conf. on Parallel Proc. for Scient. Comp. </booktitle> <address> (Los Angeles, CA., </address> <year> 1987), </year> <pages> pp. 27-31. </pages>
Reference: [32] <author> Lin, T. D., and Mah, R. S. H. </author> <title> Hierarchical partition anew optimal pivoting algorithm. </title> <booktitle> Mathematical Programming 12 (1977), </booktitle> <pages> 260-278. </pages>
Reference: [33] <author> Lipton, R., Rose, D., and Tarjan, R. </author> <title> Generalized nested dissection. </title> <journal> SIAM J. Numer. Anal. </journal> <volume> 16 (1979), </volume> <pages> 346-358. </pages>
Reference: [34] <author> Markowitz, H. M. </author> <title> The elimination form of the inverse and its application to linear programming. </title> <booktitle> Management Science 3 (April 1957), </booktitle> <pages> 255-269. </pages>
Reference: [35] <author> Osterby, O., and Zlatev, Z. </author> <title> Direct methods for sparse matrices. </title> <publisher> Springer, </publisher> <address> Berlin, </address> <year> 1983. </year>
Reference: [36] <author> Sangiovanni-Vincentelli, A. </author> <title> An optimization problem arising from tearing methods. In Sparse Matrix Computations, </title> <editor> J. R. Bunch and D. J. Rose, Eds. </editor> <publisher> Academic Press Inc., </publisher> <address> New York, </address> <year> 1976, </year> <pages> pp. 97-110. </pages>
Reference: [37] <author> Staff. </author> <title> The Cedar Project. </title> <type> Tech. Rep. CSRD Report No. 1122, </type> <institution> Center for Supercomputing Research and Development, University of Illinois, Urbana, IL, </institution> <year> 1991. </year>
Reference-contexts: In fact, each cluster represents an Alliant FX/8 with increased cache size and the four clusters share a global memory accessed by an omega interconnection network <ref> [37] </ref>. Since mcsparse was designed to exploit large grain parallelism, the multi-cluster performance is examined first, followed by the one cluster performance. The times given in this section are wall clock times, in seconds, for the code running in single user mode.
Reference: [38] <author> Steward, D. V. </author> <title> Partitioning and tearing systems of equations. </title> <journal> SIAM Journal Numerical Analysis 2, </journal> <volume> 2 (1965), </volume> <pages> 345-365. </pages>
Reference: [39] <author> Tarjan, R. E. </author> <title> Depth-first search and linear graph algorithms. </title> <journal> SIAM J. Computing 1 (1972), </journal> <pages> 146-160. </pages>
Reference: [40] <author> Wang, X. </author> <title> private communication, </title> <month> April </month> <year> 1991. </year>
Reference-contexts: A parallel implementation of the H* ordering would improve further the overall performance of mcsparse. The code could be adapted to map its multilevel parallelism onto other multi-vector processors and to exploit their architectures efficiently. Initial results, <ref> [40] </ref>, indicate that mcsparse can be adapted to use a combination of positional dropping, i.e., ignoring a fill-in element due to its position in the matrix, and numerical dropping, i.e., ignoring a fill-in element because of its relative magnitude [18, 17], to produce a preconditioner for conjugate gradient-like algorithms.
Reference: [41] <author> Wijshoff, H. A. G. </author> <title> Symmetric orderings for unsym-metric sparse matrices. </title> <type> Tech. Rep. CSRD Report No. 901, </type> <institution> Center for Supercomputing Research and Development, University of Illinois, Urbana, IL, </institution> <year> 1989. </year> <month> 12 </month>
References-found: 41

