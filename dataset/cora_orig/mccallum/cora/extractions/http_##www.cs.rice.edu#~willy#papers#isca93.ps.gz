URL: http://www.cs.rice.edu/~willy/papers/isca93.ps.gz
Refering-URL: http://www.cs.rice.edu/~willy/TreadMarks/papers.html
Root-URL: 
Title: Evaluation of Release Consistent Software Distributed Shared Memory on Emerging Network Technology  
Author: Sandhya Dwarkadas, Pete Keleher, Alan L. Cox, and Willy Zwaenepoel 
Affiliation: Department of Computer Science Rice University  
Abstract: We evaluate the effect of processor speed, network characteristics, and software overhead on the performance of release-consistent software distributed shared memory. We examine five different protocols for implementing release consistency: eager update, eager invalidate, lazy update, lazy invalidate, and a new protocol called lazy hybrid. This lazy hybrid protocol combines the benefits of both lazy update and lazy invalidate. Our simulations indicate that with the processors and networks that are becoming available, coarse-grained applications such as Jacobi and TSP perform well, more or less independent of the protocol used. Medium-grained applications, such as Water, can achieve good performance, but the choice of protocol is critical. For sixteen processors, the best protocol, lazy hybrid, performed more than three times better than the worst, the eager update. Fine-grained applications such as Cholesky achieve little speedup regardless of the protocol used because of the frequency of synchronization operations and the high latency involved. While the use of relaxed memory models, lazy implementations, and multiple-writer protocols has reduced the impact of false sharing, synchronization latency remains a serious problem for software distributed shared memory systems. These results suggest that future work on software DSMs should concentrate on reducing the amount of synchronization or its effect. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. V. Adve and M. D. Hill. </author> <title> A unified formaliza tion of four shared-memory models. </title> <type> Technical Report CS-1051, </type> <institution> University of Wisconsin, Madison, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: At that time, the last releaser piggybacks a set of write notices on the lock grant message sent to the acquirer. These write notices describe the shared data modifications that precede the acquire according to the happened-before-1 partial order <ref> [1] </ref>. The happened-before-1 partial order is essentially the union of the total processor order of the memory accesses on each individual processor and the partial order of release-acquire pairs. The happened-before-1 partial order can be represented efficiently by tagging write notices with vector timestamps [11].
Reference: [2] <author> M. Ahamad, P.W. Hutto, and R. John. </author> <title> Im plementing and programming causal distributed shared memory. </title> <booktitle> In Proceedings of the 11th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 274-281, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: While reducing the page size has a limited effect on performance, restructuring the program may prove more beneficial. 7 Related Work This work draws on the large body of research in relaxed memory consistency models (e.g., <ref> [2, 4, 8, 9] </ref>). We Pr. Spd (MHz) Jacobi TSP Water Chol. 80 13.7 10.5 7.7 0.9 20 13.4 10.0 8.6 1.4 Table 4 Speedups with Different Processor Speeds (LH, 16 processors) Procs Page Size Jac. TSP Wat.
Reference: [3] <author> H.E. Bal and A.S. Tanenbaum. </author> <title> Distributed pro gramming with shared data. </title> <booktitle> In Proceedings of the 1988 International Conference on Computer Languages, </booktitle> <pages> pages 82-91, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: The few performance results that have been published fl This work was supported in part by NSF Grants CCR-9116343 and CCR-9211004, Texas ATP Grant No. 0036404013 and by a NASA Graduate Fellowship. consist of measurements of a particular implementation in a particular hardware and software environment <ref> [3, 5, 6, 13] </ref>. Since the cost of communication is very important to the performance of a DSM, these results are highly sensitive to the implementation of the communication software. Furthermore, the hardware environments of many of these implementations are by now obsolete.
Reference: [4] <author> B.N. Bershad and M.J. Zekauskas. Midway: </author> <title> Shared memory parallel programming with entry consistency for distributed memory multiprocessors. </title> <type> Technical Report CMU-CS-91-170, </type> <institution> Carnegie-Mellon University, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: While reducing the page size has a limited effect on performance, restructuring the program may prove more beneficial. 7 Related Work This work draws on the large body of research in relaxed memory consistency models (e.g., <ref> [2, 4, 8, 9] </ref>). We Pr. Spd (MHz) Jacobi TSP Water Chol. 80 13.7 10.5 7.7 0.9 20 13.4 10.0 8.6 1.4 Table 4 Speedups with Different Processor Speeds (LH, 16 processors) Procs Page Size Jac. TSP Wat. <p> An interesting alternative is entry consistency (EC), defined by Bershad and Zekauskas <ref> [4] </ref>. EC differs from RC because it requires all shared data to be explicitly associated with some synchronization variable. On a lock acquisition EC only needs to propagate the shared data associated with the lock.
Reference: [5] <author> J.B. Carter, J.K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: The few performance results that have been published fl This work was supported in part by NSF Grants CCR-9116343 and CCR-9211004, Texas ATP Grant No. 0036404013 and by a NASA Graduate Fellowship. consist of measurements of a particular implementation in a particular hardware and software environment <ref> [3, 5, 6, 13] </ref>. Since the cost of communication is very important to the performance of a DSM, these results are highly sensitive to the implementation of the communication software. Furthermore, the hardware environments of many of these implementations are by now obsolete. <p> Regardless of the considerable bandwidth available on these networks, Cholesky's performance is constrained by the very high number of synchronization operations. Among the protocols for implementing software release consistency, we distinguish between eager and lazy protocols. Eager protocols push modifications to all cachers at synchronization variable releases <ref> [5] </ref>. In contrast, lazy protocols [11] pull the modifications at synchronization variable acquires, and communicate only with the acquirer. Both eager and lazy release consistency can be implemented using either invalidate or update protocols. <p> Section 5 discusses our simulation methodology, and Section 6 presents the simulation results. We briefly survey related work in Section 7 and conclude in Section 8. 2 Release Consistency For completeness, we reiterate in this section the main concepts behind release consistency (RC) [9], eager release consistency (ERC) <ref> [5] </ref>, and lazy release consistency (LRC) [11]. RC [9] is a form of relaxed memory consistency that allows the effects of shared memory accesses to be delayed until selected synchronization accesses occur. <p> While this strategy masks latency, in a software implementation it is also important to reduce the number of messages sent because of the high per message cost. In an eager software implementation of RC such as Munin's multiple-writer protocol <ref> [5] </ref>, a processor delays propagating its modifications of shared data until it executes a release (see Figures 1 and 2). Lazy implementations of RC further delay the propagation of modifications until the acquire. <p> This contrasts with the exclusive-writer protocol used, for instance, in DASH [9], where a processor must obtain exclusive access to a cache line before it can be modified. Experience with Munin <ref> [5] </ref> indicates that multiple-writer protocols perform well in software DSMs, because they can handle false sharing without generating large amounts of message traffic between synchronization points. All of the protocols support the use of exclusive locks and global barriers to synchronize access to shared memory. <p> In this table, the concurrent last modifiers for a page are the processors that created modifications that do not precede, according to happened-before-1, any other known modifications to that page. 4.1 The Eager Protocols 4.1.1 Locks We base our eager RC algorithms on Munin's multiple-writer protocol <ref> [5] </ref>. A processor delays propagating its modifications of shared data until it comes to a release. <p> Although all messages are simulated, protocol-specific consistency information is not reflected in the amount of data sent. Only the actual shared data moved by the protocols is included in message lengths. 6 Simulation Results 6.1 DSM on an Ethernet Although prior work <ref> [5] </ref> showed that Ethernet-based software DSMs can achieve significant speedups, we find that for modern processors the Ethernet is no longer a viable option. Figure 6 shows the speedup of Ja-cobi, a coarse-grained program. Jacobi's speedup peaks at 5.2 for eight processors, and declines rapidly thereafter. <p> Prior work has developed implementations of relaxed memory consistency models for DSM that reduce but do not totally eliminate the effects of false sharing. For example, Munin's eager implementation of release consistency eliminates the "ping-pong" effect of a page bouncing between two writing processors <ref> [5] </ref>. However, modifications to falsely shared pages still have to be distributed to all processors caching the page at a release. The lazy hybrid protocol further reduces the effect of false sharing because data movement only occurs between synchronizing processors. <p> On a lock acquisition EC only needs to propagate the shared data associated with the lock. EC, however, requires the programmer to insert additional synchronization in shared memory programs to execute correctly on an EC memory. Typically, RC does not require additional synchronization. Ivy [13] and Munin <ref> [5] </ref> are two implementations of software DSMs for which performance measurements have been published. Both achieve good speedups on many of the applications studied. The slow processors used in the implementations prevented the network from becoming a bottleneck in achieving these speedups.
Reference: [6] <author> J.S. Chase, F.G. Amador, E.D. Lazowska, H.M. Levy, and R.J. Littlefield. </author> <title> The Amber system: Parallel programming on a network of multiprocessors. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 147-158, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: The few performance results that have been published fl This work was supported in part by NSF Grants CCR-9116343 and CCR-9211004, Texas ATP Grant No. 0036404013 and by a NASA Graduate Fellowship. consist of measurements of a particular implementation in a particular hardware and software environment <ref> [3, 5, 6, 13] </ref>. Since the cost of communication is very important to the performance of a DSM, these results are highly sensitive to the implementation of the communication software. Furthermore, the hardware environments of many of these implementations are by now obsolete.
Reference: [7] <author> R. G. Covington, S. Dwarkadas, J. R. Jump, S. Madala, and J. B. Sinclair. </author> <title> The Efficient Simulation of Parallel Computer Systems. </title> <journal> International Journal in Computer Simulation, </journal> <volume> 1 </volume> <pages> 31-58, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: The evaluation is done by execution-driven simulation <ref> [7] </ref>. The application programs we use have been written for (hardware) shared memory multiprocessors.
Reference: [8] <author> M. Dubois and C. Scheurich. </author> <title> Memory access dependencies in shared-memory multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 16(6) </volume> <pages> 660-673, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: While reducing the page size has a limited effect on performance, restructuring the program may prove more beneficial. 7 Related Work This work draws on the large body of research in relaxed memory consistency models (e.g., <ref> [2, 4, 8, 9] </ref>). We Pr. Spd (MHz) Jacobi TSP Water Chol. 80 13.7 10.5 7.7 0.9 20 13.4 10.0 8.6 1.4 Table 4 Speedups with Different Processor Speeds (LH, 16 processors) Procs Page Size Jac. TSP Wat.
Reference: [9] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gib bons, A. Gupta, and J. Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Furthermore, the hardware environments of many of these implementations are by now obsolete. Much faster processors are commonplace, and much faster networks are becoming available. We are focusing on DSMs that support release consistency <ref> [9] </ref>, i.e., where memory is guaranteed to be consistent only following certain synchronization operations. <p> Section 5 discusses our simulation methodology, and Section 6 presents the simulation results. We briefly survey related work in Section 7 and conclude in Section 8. 2 Release Consistency For completeness, we reiterate in this section the main concepts behind release consistency (RC) <ref> [9] </ref>, eager release consistency (ERC) [5], and lazy release consistency (LRC) [11]. RC [9] is a form of relaxed memory consistency that allows the effects of shared memory accesses to be delayed until selected synchronization accesses occur. <p> We briefly survey related work in Section 7 and conclude in Section 8. 2 Release Consistency For completeness, we reiterate in this section the main concepts behind release consistency (RC) <ref> [9] </ref>, eager release consistency (ERC) [5], and lazy release consistency (LRC) [11]. RC [9] is a form of relaxed memory consistency that allows the effects of shared memory accesses to be delayed until selected synchronization accesses occur. Simplifying matters somewhat, shared memory accesses are labeled either as ordinary or as synchronization accesses, with the latter category further divided into acquire and release accesses. <p> All five are multiple-writer protocols. Multiple processors can concurrently write to their own copy of a page with their separate modifications being merged at a subsequent release, in accordance with the RC model. This contrasts with the exclusive-writer protocol used, for instance, in DASH <ref> [9] </ref>, where a processor must obtain exclusive access to a cache line before it can be modified. Experience with Munin [5] indicates that multiple-writer protocols perform well in software DSMs, because they can handle false sharing without generating large amounts of message traffic between synchronization points. <p> While reducing the page size has a limited effect on performance, restructuring the program may prove more beneficial. 7 Related Work This work draws on the large body of research in relaxed memory consistency models (e.g., <ref> [2, 4, 8, 9] </ref>). We Pr. Spd (MHz) Jacobi TSP Water Chol. 80 13.7 10.5 7.7 0.9 20 13.4 10.0 8.6 1.4 Table 4 Speedups with Different Processor Speeds (LH, 16 processors) Procs Page Size Jac. TSP Wat.
Reference: [10] <author> D.B. Johnson and W. Zwaenepoel. </author> <title> The Peregrine high-performance RPC system. </title> <journal> Software: Practice and Experience, </journal> <volume> 23(2) </volume> <pages> 201-221, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: This cost is set at (1000 + message length fl 1:5=4) processor cycles at both the destination and source of each message. These figures were modeled after the Peregrine <ref> [10] </ref> implementation overheads. Peregrine is an RPC system that provides performance close to optimal by avoiding intermediate copying. The lazy implementation's extra complexity is modeled by doubling the per-byte message overhead both at the sender and at the receiver.
Reference: [11] <author> P. Keleher, A. L. Cox, and W. Zwaenepoel. </author> <title> Lazy release consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Among the protocols for implementing software release consistency, we distinguish between eager and lazy protocols. Eager protocols push modifications to all cachers at synchronization variable releases [5]. In contrast, lazy protocols <ref> [11] </ref> pull the modifications at synchronization variable acquires, and communicate only with the acquirer. Both eager and lazy release consistency can be implemented using either invalidate or update protocols. <p> We briefly survey related work in Section 7 and conclude in Section 8. 2 Release Consistency For completeness, we reiterate in this section the main concepts behind release consistency (RC) [9], eager release consistency (ERC) [5], and lazy release consistency (LRC) <ref> [11] </ref>. RC [9] is a form of relaxed memory consistency that allows the effects of shared memory accesses to be delayed until selected synchronization accesses occur. <p> The happened-before-1 partial order is essentially the union of the total processor order of the memory accesses on each individual processor and the partial order of release-acquire pairs. The happened-before-1 partial order can be represented efficiently by tagging write notices with vector timestamps <ref> [11] </ref>. At acquire time, the acquiring processor determines the pages for which the incoming write notices contain vector timestamps larger than the timestamp of its copy of that page in memory. <p> However, unlike in the lazy update protocol, the acquirer does not make any attempt to obtain any other modifications. Instead, it invalidates the pages for which it received write notices but for which no modifications were included in the lock grant message. Previous simulations <ref> [11] </ref> indicate that (1) the lazy protocols send fewer messages and less data than the eager protocols, and (2) the lazy update protocol send fewer messages in most cases than the lazy invalidate protocol, while the lazy invalidate protocol sends less data than the lazy update protocol.
Reference: [12] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The directory-based cache coherence protocol for the DASH multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Essentially, RC requires ordinary shared memory accesses to be performed only when a subsequent release by the same processor is performed. RC implementations can delay the effects of shared memory accesses as long as they meet this constraint. For instance, the DASH <ref> [12] </ref> implementation of RC buffers and pipelines writes without blocking the processor. A subsequent release is not allowed to perform (i.e., the corresponding lock cannot be granted to another processor) until acknowledgments have been received for all outstanding invalidations. <p> 1.0 4 1024 3.7 2.6 3.1 1.2 8 1024 7.2 5.1 5.1 1.4 16 1024 13.7 8.5 8.7 0.9 Table 5 Effect on Speedup of Reducing the Page Size to 1024 bytes (LH) have chosen as our basic model the release consistency model introduced by the DASH project at Stan-ford <ref> [12] </ref>, because it requires little or no change to existing shared memory programs. An interesting alternative is entry consistency (EC), defined by Bershad and Zekauskas [4]. EC differs from RC because it requires all shared data to be explicitly associated with some synchronization variable.
Reference: [13] <author> K. Li and P. Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: The few performance results that have been published fl This work was supported in part by NSF Grants CCR-9116343 and CCR-9211004, Texas ATP Grant No. 0036404013 and by a NASA Graduate Fellowship. consist of measurements of a particular implementation in a particular hardware and software environment <ref> [3, 5, 6, 13] </ref>. Since the cost of communication is very important to the performance of a DSM, these results are highly sensitive to the implementation of the communication software. Furthermore, the hardware environments of many of these implementations are by now obsolete. <p> The new diffs are then merged into the page and the processor is allowed to proceed. The lazy protocols determine the location of a page or updates to the page entirely on the basis of local information. No additional messages are required, unlike in other DSM systems <ref> [13] </ref>. 5 Methodology 5.1 Application Suite We simulated four programs, from three different classes of applications. Jacobi and TSP are coarse-grained programs with a large amount of computation relative to synchronization (323,840 and 18,092,000 cycles per processor between off-node synchronization operations, respectively, at 16 processors). <p> On a lock acquisition EC only needs to propagate the shared data associated with the lock. EC, however, requires the programmer to insert additional synchronization in shared memory programs to execute correctly on an EC memory. Typically, RC does not require additional synchronization. Ivy <ref> [13] </ref> and Munin [5] are two implementations of software DSMs for which performance measurements have been published. Both achieve good speedups on many of the applications studied. The slow processors used in the implementations prevented the network from becoming a bottleneck in achieving these speedups.
Reference: [14] <author> J.P. Singh, W.-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford parallel applications for shared-memory. </title> <type> Technical Report CSL-TR-91-469, </type> <institution> Stanford University, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: The application programs are Jacobi, Traveling Salesman Problem (TSP), and Water and Cholesky from the SPLASH benchmark suite <ref> [14] </ref>. Jacobi and TSP exhibit coarse-grained parallelism, with little synchronization relative to the amount of computation, whereas Water may be characterized as medium-grained, and Cholesky as fine-grained. <p> Our Ja-cobi program is a simple Successive Over-Relaxation program that works on grids of 512 by 512 elements. TSP solves the traveling salesman problem for 18-city tours. Water, from the SPLASH suite <ref> [14] </ref>, is a medium grained molecular dynamics simulation (19200 cycles per processor between off-node synchronization operations). We ran Water with the default parameters: 288 molecules for 2 steps.
References-found: 14

