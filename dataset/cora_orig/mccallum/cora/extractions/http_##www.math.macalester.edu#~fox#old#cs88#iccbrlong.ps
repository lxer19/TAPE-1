URL: http://www.math.macalester.edu/~fox/old/cs88/iccbrlong.ps
Refering-URL: http://www.math.macalester.edu/~fox/old/cs88/cbr.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Phone: (812) 855-8702, (812) 855-9756  
Title: Learning to Refine Indexing by Introspective Reasoning  
Author: Susan Fox and David B. Leake 
Address: Bloomington, IN 47405, USA  
Affiliation: Computer Science Department Indiana University  
Abstract: A significant problem for case-based reasoning (CBR) systems is deciding what features to use in judging case similarity for retrieval. We describe research that addresses the feature selection problem by using introspective reasoning to learn new features for indexing. Our method augments the CBR system with an introspective reasoning component which monitors system performance to detect poor retrievals, identifies features which would lead to retrieving cases requiring less adaptation, and refines the indices to include such features in order to avoid similar future failures. We explore the benefit of introspective reasoning by performing empirical tests on the implemented system. These tests examine the benefits of introspective index refinement and the effects of problem order on case and index learning, and show that introspective learning of new index features improves overall performance across the range of different problem orders.
Abstract-found: 1
Intro-found: 1
Reference: 1. <editor> D. Aha, editor. </editor> <booktitle> Proceedings of the AAAI-94 Workshop on Case-Based Reasoning, </booktitle> <address> Seattle, WA, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: the features used in indexing cases, and re-indexing the cases in memory to include the new feature. 3 Empirical Evaluation There has been growing interest recently in evaluation of case-based reasoning systems to identify to what extent they succeed and what components of their processing are responsible for their success <ref> [1] </ref>. However, little work has been done to quantify the benefits of introspective reasoning for improving reasoning methods, and the relationship between case presentation order and learning is largely unaddressed.
Reference: 2. <author> R. Alterman. </author> <title> An adaptive planner. </title> <booktitle> In Proceedings of the Fifth National Conference on Artificial Intelligence, </booktitle> <pages> pages 65-69, </pages> <address> Philadelphia, PA, </address> <month> August </month> <year> 1986. </year> <note> AAAI. </note>
Reference-contexts: The system has two main parts, as shown in Fig. 1: a planner which develops plans through case-based reasoning <ref> [2, 15, 16] </ref> and applies them through execution in a simulated world, and an introspective model-based reasoner which detects, explains, and repairs reasoning failures caused by poor indexing criteria. Fig. 1.
Reference: 3. <author> J. Arcos and E. </author> <title> Plaza. A reflective architecture for integrated memory-based learning and reasoning. </title> <editor> In S. Wess, K.D. Altoff, and M. Richter, editors, </editor> <booktitle> Topics in Case-Based Reasoning. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Kaiserslautern, Germany, </address> <year> 1993. </year>
Reference-contexts: IULIAN [18] also performs introspective learning, but instead of using an explicit introspective model, it stores plans for repairing its reasoning which are recalled when failures occur. <ref> [3] </ref> create a unified architecture for describing case-based and meta-level problem solving tasks by describing each process by decomposition into tasks and sub-tasks.
Reference: 4. <author> W.M. Bain. </author> <title> Case-based Reasoning: A Computer Model of Subjective Assessment. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <year> 1986. </year> <note> Computer Science Department Technical Report 470. </note>
Reference-contexts: It is acknowledged that problem order can strongly affect learning in general, and learning of CBR systems in particular (e.g. <ref> [4] </ref>). Redmond [19] suggests early problems presented should cover the range of possible situations, to lay a foundation for later learning.
Reference: 5. <author> S. Bhatta and A. Goel. </author> <title> Model-based learning of structural indices to design cases. </title> <booktitle> In Proceedings of the IJCAI-93 Workshop on Reuse of Design, </booktitle> <address> Chambery, France, </address> <month> September </month> <year> 1993. </year> <pages> IJCAI. </pages>
Reference-contexts: IDEAL <ref> [5] </ref> investigates index learning, but uses a model of its domain, not the reasoning task, as well as cases from its domain, in order to learn new indices.
Reference: 6. <author> L. Birnbaum, G. Collins, M. Brand, M. Freed, B. Krulwich, and L. Pryor. </author> <title> A model-based approach to the construction of adaptive case-based planning systems. </title> <editor> In R. Bareiss, editor, </editor> <booktitle> Proceedings of the Case-Based Reasoning Workshop, </booktitle> <pages> pages 215-224, </pages> <address> San Mateo, </address> <year> 1991. </year> <title> DARPA, </title> <publisher> Morgan Kaufmann, Inc. </publisher>
Reference-contexts: A discrepancy indicates a reasoning failure; the failure is corrected by explaining the failure and revising the reasoning process for constructing indices. Using introspective learning to improve the case-based reasoning process itself was first suggested by Birnbaum et al. <ref> [6] </ref>, deriving from their work using model to improve self-debugging planners [7]. Introspective improvement of reasoning processes is a relatively new approach, and there have been few evaluations of its effect on the performance of a system using it. <p> As the planner reasons about its task, the introspective component monitors its reasoning process by comparing it to the declarative model describing the planner's ideal Fig. 2. map of streets ROBBIE travels reasoning processes <ref> [6, 10, 13] </ref>. Reasoning failures occur when the model's expectations about the reasoning process are not fulfilled by the actual reasoning. Expectations are encoded as "assertions;" facts that would be ideally true of particular points in the case-based reasoning process. <p> A proposal by Birnbaum et al. <ref> [6] </ref> inspired ROBBIE's framework, but that proposal was not implemented and did not use a hierarchical model. Meta-AQUA [8] performs failure-driven introspective learning, but contains no explicit model of correct behavior. Instead, it uses a set of template descriptions for reasoning failures which provide diagnosis and repair information.
Reference: 7. <author> L. Birnbaum, G. Collins, M. Freed, and B. Krulwich. </author> <title> Model-based diagnosis of planning failures. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 318-323, </pages> <address> Boston, MA, </address> <month> July </month> <year> 1990. </year> <note> AAAI. </note>
Reference-contexts: Using introspective learning to improve the case-based reasoning process itself was first suggested by Birnbaum et al. [6], deriving from their work using model to improve self-debugging planners <ref> [7] </ref>. Introspective improvement of reasoning processes is a relatively new approach, and there have been few evaluations of its effect on the performance of a system using it.
Reference: 8. <author> M. Cox and A. Ram. </author> <title> Managing learning goals in strategy-selection problems. </title> <booktitle> In Proceedings of the Second European Workshop on Case-Based Reasoning, </booktitle> <pages> pages 85-93, </pages> <address> Chantilly, France, </address> <year> 1994. </year>
Reference-contexts: A proposal by Birnbaum et al. [6] inspired ROBBIE's framework, but that proposal was not implemented and did not use a hierarchical model. Meta-AQUA <ref> [8] </ref> performs failure-driven introspective learning, but contains no explicit model of correct behavior. Instead, it uses a set of template descriptions for reasoning failures which provide diagnosis and repair information.
Reference: 9. <author> R. J. Firby. </author> <title> Adaptive Execution in Complex Dynamic Worlds. </title> <type> PhD thesis, </type> <institution> Yale University, Computer Science Department, </institution> <year> 1989. </year> <type> Technical Report 672. </type>
Reference-contexts: ROBBIE is given a goal location and must create a plan for getting from its current location to the goal. Plan creation involves retrieving and adapting an old case. Because adaptation is not always guaranteed to be correct, ROBBIE executes the newly created plan using reactive planning <ref> [9] </ref> to fill in missing details and recover from failures. Execution evaluates the quality of the CBR-created plan, and permits ROBBIE to arrive at a solution despite flaws in the plan.
Reference: 10. <author> S. Fox and D. Leake. </author> <title> Using introspective reasoning to guide index refinement in case-based reasoning. </title> <booktitle> In Proceedings of the Sixteenth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 324-329, </pages> <address> Atlanta, GA, 1994. </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference-contexts: As the planner reasons about its task, the introspective component monitors its reasoning process by comparing it to the declarative model describing the planner's ideal Fig. 2. map of streets ROBBIE travels reasoning processes <ref> [6, 10, 13] </ref>. Reasoning failures occur when the model's expectations about the reasoning process are not fulfilled by the actual reasoning. Expectations are encoded as "assertions;" facts that would be ideally true of particular points in the case-based reasoning process. <p> Detecting retrieval problems: Failure detection for the introspective reasoner involves noticing discrepancies between assertions and actual behavior. At each point in the reasoning process, the introspective reasoner compares the actual reasoning to the ideal. When they fail to match, a reasoning failure has occurred (See <ref> [10] </ref> for a more extensive discussion of ROBBIE's failure detection). Detecting failures due to faulty indexing criteria requires monitoring the entire reasoning process: the failure may not be discovered until after the inappropriately retrieved case has been adapted and executed. <p> In addition, introspective reasoning for self-improvement is a general approach that could be extended to other portions of the CBR process: altering retrieval, adaptation, evaluation, and plan storage (See <ref> [10] </ref> for a more general discussion). Little empirical evaluation has been done of the effects of introspective reasoning on behavior of the systems which incorporate them.
Reference: 11. <author> S. Fox and D. Leake. </author> <title> An introspective reasoning method for index refinement. </title> <booktitle> In Proceedings of 14th international Joint Conference on Artificial Intelligence. IJCAI, </booktitle> <year> 1995. </year>
Reference-contexts: By putting runs of a sequence together on one set of axes, we see trends in the retrieval performance. For a more detailed discussion of these results see <ref> [11] </ref>. to a much lower level than those without: 30-45% instead of 60-70%. A similar pattern appeared for every sequence except the anomalous one. In some cases the decrease with re-indexing was much more dramatic.
Reference: 12. <author> S. Fox and D. Leake. </author> <title> Modeling case-based planning for repairing reasoning failures. </title> <booktitle> In Proceedings of the 1995 AAAI Spring Symposium on Representing Mental States and Mechanisms, </booktitle> <address> Stanford, CA, March 1995. </address> <publisher> AAAI. </publisher>
Reference-contexts: The assertions are clustered together by the part of the reasoning process to which they refer, and by how abstract or specific they are. Each assertion also contains links to other causally related assertions inside or outside its cluster. Fox and Leake <ref> [12] </ref> contains a detailed description of the model and its representation. Detecting retrieval problems: Failure detection for the introspective reasoner involves noticing discrepancies between assertions and actual behavior. At each point in the reasoning process, the introspective reasoner compares the actual reasoning to the ideal.
Reference: 13. <author> M. Freed and G. Collins. </author> <title> Adapting routines to improve task coordination. </title> <booktitle> In Proceedings of the 1994 Conference on AI Planning Systems, </booktitle> <pages> pages 255-259, </pages> <year> 1994. </year>
Reference-contexts: As the planner reasons about its task, the introspective component monitors its reasoning process by comparing it to the declarative model describing the planner's ideal Fig. 2. map of streets ROBBIE travels reasoning processes <ref> [6, 10, 13] </ref>. Reasoning failures occur when the model's expectations about the reasoning process are not fulfilled by the actual reasoning. Expectations are encoded as "assertions;" facts that would be ideally true of particular points in the case-based reasoning process.
Reference: 14. <author> A. Goel, K. Ali, and Andres Gomez de Silva Garza. </author> <title> Computational tradeoffs in experience-based reasoning. </title> <booktitle> In Proceedings of the AAAI-94 workshop on Case-Based Reasoning, </booktitle> <pages> pages 55-61, </pages> <address> Seattle, WA, </address> <year> 1994. </year>
Reference-contexts: Our testbed computer system, ROBBIE 2 , is a case-based route planner. Its CBR component learns by adding successful plans to its memory, as in other case-based route planners such as ROUTER <ref> [14] </ref>. However, our research focus is not the route planning task itself. Instead, the focus is on how introspective reasoning can be used by the CBR system to refine its indexing criteria. ROBBIE combines a case-based planner with an introspective component.
Reference: 15. <author> C. Hammond. </author> <title> Case-Based Planning: Viewing Planning as a Memory Task. </title> <publisher> Academic Press, </publisher> <address> San Diego, </address> <year> 1989. </year>
Reference-contexts: The system has two main parts, as shown in Fig. 1: a planner which develops plans through case-based reasoning <ref> [2, 15, 16] </ref> and applies them through execution in a simulated world, and an introspective model-based reasoner which detects, explains, and repairs reasoning failures caused by poor indexing criteria. Fig. 1.
Reference: 16. <author> J. Kolodner. </author> <title> Case-Based Reasoning. </title> <publisher> Morgan Kaufman, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: 1 Introduction Selecting the best set of features to use in indexing a case-based reasoning (CBR) system's memory may be difficult; determining which features are most appropriate may require experience <ref> [16] </ref>. An appealing alternative is to permit the system itself to learn what features are relevant in response to its experiences. <p> The system has two main parts, as shown in Fig. 1: a planner which develops plans through case-based reasoning <ref> [2, 15, 16] </ref> and applies them through execution in a simulated world, and an introspective model-based reasoner which detects, explains, and repairs reasoning failures caused by poor indexing criteria. Fig. 1.
Reference: 17. <author> D. Leake. </author> <title> Constructive similarity assessment: Using stored cases to define new situations. </title> <booktitle> In Proceedings of the Fourteenth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 313-318, </pages> <address> Bloomington, IN, 1992. </address> <publisher> Cognitive Science Society. </publisher>
Reference-contexts: Because the goodness of any retrieval scheme depends on whether it retrieves cases that are easy to adapt, it is desirable to achieve more direct connections between similarity/retrieval criteria and case adaptation ability in CBR systems (e.g., <ref> [17, ?] </ref>); our research is one way to make indexing criteria better reflect adaptability. In addition, introspective reasoning for self-improvement is a general approach that could be extended to other portions of the CBR process: altering retrieval, adaptation, evaluation, and plan storage (See [10] for a more general discussion).
Reference: 18. <author> R. Oehlmann, P. Edwards, and D. Sleeman. </author> <title> Changing the viewpoint: Re-indexing by introspective questioning. </title> <booktitle> In Proceedings of the Sixteenth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 675-680. </pages> <publisher> Lawrence Erlbaum Associates, </publisher> <year> 1994. </year>
Reference-contexts: Instead, it uses a set of template descriptions for reasoning failures which provide diagnosis and repair information. Autognostic [20] applies an existing framework for device modeling (SBF) to introspective modeling of the route planner ROUTER; their approach focuses on explanation of failures without strongly addressing failure detection. IULIAN <ref> [18] </ref> also performs introspective learning, but instead of using an explicit introspective model, it stores plans for repairing its reasoning which are recalled when failures occur. [3] create a unified architecture for describing case-based and meta-level problem solving tasks by describing each process by decomposition into tasks and sub-tasks.
Reference: 19. <author> M. </author> <title> Redmond. Learning by Observing and Understanding Expert Problem Solving. </title> <type> PhD thesis, </type> <institution> College of Computing, Georgia Institute of Technology, </institution> <year> 1992. </year> <note> Technical report GIT-CC-92/43. </note>
Reference-contexts: It is acknowledged that problem order can strongly affect learning in general, and learning of CBR systems in particular (e.g. [4]). Redmond <ref> [19] </ref> suggests early problems presented should cover the range of possible situations, to lay a foundation for later learning. In addition it seems clear that each problem should push the boundaries of the system's capabilities without extending so far that the problem is impossible for the system to solve.
Reference: 20. <author> E. Stroulia and A. Goel. </author> <title> Task structures: What to learn? In M. </title> <editor> desJardins and A. Ram, editors, </editor> <booktitle> Proceedings of the 1994 AAAI Spring Symposium on Goal-driven Learning, </booktitle> <pages> pages 112-121. </pages> <publisher> AAAI Press, </publisher> <year> 1994. </year> <title> This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: Meta-AQUA [8] performs failure-driven introspective learning, but contains no explicit model of correct behavior. Instead, it uses a set of template descriptions for reasoning failures which provide diagnosis and repair information. Autognostic <ref> [20] </ref> applies an existing framework for device modeling (SBF) to introspective modeling of the route planner ROUTER; their approach focuses on explanation of failures without strongly addressing failure detection.
References-found: 20

