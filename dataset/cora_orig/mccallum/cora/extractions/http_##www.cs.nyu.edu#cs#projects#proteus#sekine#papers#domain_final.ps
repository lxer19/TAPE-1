URL: http://www.cs.nyu.edu/cs/projects/proteus/sekine/papers/domain_final.ps
Refering-URL: http://www.cs.nyu.edu/cs/projects/proteus/sekine/index.html
Root-URL: http://www.cs.nyu.edu
Title: Domain Project Final  
Author: Satoshi Sekine and Ralph Grishman 
Address: New York University 715 Broadway, 7th floor New York, NY 10003, USA  
Affiliation: Computer Science Department  
Note: March  
Pubnum: Report  
Email: sekine,grishman@cs.nyu.edu  
Web: http://cs.nyu.edu/cs/project/proteus/sekine http://cs.nyu.edu/cs/faculty/grishman  
Date: 18, 1997  
Abstract: This is the final report of `domain project'. It covers all of our three intermediate reports, published June, August and November 1996. The short version of this report is published in Proceedings of the Fifth Conference on Applied Natural Language Processing (1997) 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S.Sekine, </author> <title> R.Grishman "Domain project intermediate report -" (1996 June) </title>
Reference: [2] <author> S.Sekine, </author> <title> R.Grishman "Domain project intermediate report 2 -" (1996 August) </title>
Reference: [3] <author> S.Sekine, </author> <title> R.Grishman "Domain project intermediate report 3 -" (1996 November) </title>
Reference: [4] <author> W.N.Francis, </author> <title> H.Kucera "Manual of information to accompany A Standard Corpus of Present-Day Edited American English" Brown University, </title> <institution> Department of Linguistics (1964/1979) </institution>
Reference-contexts: In order to acquire grammar rules in our experiment, we need a syntactically tagged corpus consisting of different domains, and the tagging has to be uniform throughout the corpus. To meet these requirements, the Brown Corpus <ref> [4] </ref> on the distribution of PennTreeBank version 1 [6] is used in our experiments. In order to give a general idea about the Brown corpus, the following description is quoted from the manual of the Brown corpus.
Reference: [5] <author> M.Marcus, B.Santorini, </author> <title> M.Marcinkiewicz "Building a large annotated corpus of English: the Penn Tree bank" in the distributed Penn Tree Bank Project CD-ROM, Linguistic Data Consortium, </title> <institution> University of Pennsylva-nia. </institution>
Reference: [6] <author> Mitchell P. Marcus, Beatrice Santorini and Mary A Marcinkiewicz: </author> <title> "Building a Large Annotated Corpus of English: The Penn TreeBank." </title> <booktitle> Computational Linguistics, </booktitle> <address> 19.1, pp313-330. </address> <year> (1993) </year>
Reference-contexts: In order to acquire grammar rules in our experiment, we need a syntactically tagged corpus consisting of different domains, and the tagging has to be uniform throughout the corpus. To meet these requirements, the Brown Corpus [4] on the distribution of PennTreeBank version 1 <ref> [6] </ref> is used in our experiments. In order to give a general idea about the Brown corpus, the following description is quoted from the manual of the Brown corpus.
Reference: [7] <author> Richard Kittredge, Lynette Hirschman: </author> <title> "Sublanguage: Studies of Language in Restricted Semantic domains." </title> <booktitle> Series of Foundations of Communications, </booktitle> <publisher> Walter de Gruyter, </publisher> <address> Berlin (1983) </address>
Reference-contexts: In particular, we are interested in taking advantage of domain dependencies of structural or syntactic knowledge. There has been considerable research in this area <ref> [7] </ref> [8]. For example, the domain dependence of lexical semantics is widely known. It is easy to observe that usage of the word `bank' is different between the `economic document' domain and the `geographic' domain. Also, there are surveys of domain dependencies concerning syntax or syntax-related features [9] [10] [11].
Reference: [8] <author> Ralph Grishman and Richard Kittredge: </author> <title> "Analyzing Language in Restricted Domains: Sublanguage Description and Processing." </title> <publisher> Lawrence Erl-baum Associates, Publishers (1986) </publisher>
Reference-contexts: In particular, we are interested in taking advantage of domain dependencies of structural or syntactic knowledge. There has been considerable research in this area [7] <ref> [8] </ref>. For example, the domain dependence of lexical semantics is widely known. It is easy to observe that usage of the word `bank' is different between the `economic document' domain and the `geographic' domain. Also, there are surveys of domain dependencies concerning syntax or syntax-related features [9] [10] [11].
Reference: [9] <author> Jonathan Slocum: </author> <title> "How One Might Automatically Identify and Adapt to a Sublanguage: An Initial Exploration." Analyzing Language in Restricted Domains, </title> <address> pp195-210. </address> <year> (1986) </year>
Reference-contexts: For example, the domain dependence of lexical semantics is widely known. It is easy to observe that usage of the word `bank' is different between the `economic document' domain and the `geographic' domain. Also, there are surveys of domain dependencies concerning syntax or syntax-related features <ref> [9] </ref> [10] [11]. It is intuitively conceivable that there are syntactic differences between `telegraphic messages' and `press report', or between `weather forecast sentences' and `romance and love story'.
Reference: [10] <author> Douglas Biber: </author> <title> "Using Register-Diversified Corpora for General Language Studies." </title> <journal> Journal of Computer Linguistics Vol.19, </journal> <note> Num 2, pp219-241. </note> <year> (1993) </year>
Reference-contexts: For example, the domain dependence of lexical semantics is widely known. It is easy to observe that usage of the word `bank' is different between the `economic document' domain and the `geographic' domain. Also, there are surveys of domain dependencies concerning syntax or syntax-related features [9] <ref> [10] </ref> [11]. It is intuitively conceivable that there are syntactic differences between `telegraphic messages' and `press report', or between `weather forecast sentences' and `romance and love story'.
Reference: [11] <author> Jussi Karlgren and Douglass Cutting: </author> <title> "Recognizing Text Genres with Sim--ple Metrics Using Discriminant Analysis." </title> <booktitle> The 15th International Conference on Computational Linguistics, </booktitle> <address> pp1071-1075. </address> <year> (1994) </year>
Reference-contexts: For example, the domain dependence of lexical semantics is widely known. It is easy to observe that usage of the word `bank' is different between the `economic document' domain and the `geographic' domain. Also, there are surveys of domain dependencies concerning syntax or syntax-related features [9] [10] <ref> [11] </ref>. It is intuitively conceivable that there are syntactic differences between `telegraphic messages' and `press report', or between `weather forecast sentences' and `romance and love story'.
Reference: [12] <author> Satoshi Sekine: </author> <title> "A New Direction for Sublanguage NLP" International Conference on New Methods in Language Processing (1994) </title>
Reference: [13] <author> Ezra Black, et.al: </author> <title> "A procedure for Quantitatively Comparing the Syntactic Coverage of English Grammars." </title> <booktitle> Proc. of Fourth DARPA Speech and Natural Language Workshop (1991) </booktitle>
Reference-contexts: Also we try to see the relationship of the size of the training corpus. We use the domains of `Press Reportage' and `Romance and Love Story' in this intensive experiment. In order to measure the accuracy of parsing, recall and precision measures are used <ref> [13] </ref>. 5.1 Individual Experiment Table 2 shows the parsing performance for domain A, B, E, J, K, L, N and P with four types of grammars. In the table, results are shown in the form of `recall/precision'.
Reference: [14] <author> John Carroll and Ted Briscoe: </author> <title> "Apportioning development effort in a probabilistic LR parsing system through evaluation." </title> <booktitle> Proceedings of Conference on Empirical Methods in Natural Language Processing. </booktitle> <year> (1996) </year>
Reference-contexts: When we try to parse a text in a particular domain, we should prepare a grammar which suits that domain. This idea naturally contrasts with the idea of robust broad-coverage parsing <ref> [14] </ref>, in which a single grammar should be prepared for parsing any kind of text.
Reference: [15] <author> Satoshi Sekine and Ralph Grishman: </author> <title> "A Corpus-based Probabilistic Grammar with Only Two Non-terminals." </title> <booktitle> International Workshop on Parsing Technologies, </booktitle> <address> pp216-223. </address> <year> (1995) </year>
Reference: [16] <author> Satoshi Sekine: </author> <note> "Apple Pie Parser homepage" http://cs.nyu.edu/cs/projects/proteus/app. (1996) </note>
Reference: [17] <author> E.Charniak: </author> <title> "Statistical Language Learning" The MIT Press (1993) </title>
Reference-contexts: CE (T; M ) = i The smaller the value, the more accurately the model represents the test set. If the model is identical to the test set, the value is equal to the entropy of the set itself. <ref> [17] </ref> For each pair of the probability data, we compute cross entropy. Figure 2 shows the cross entropy of grammar across domains.
Reference: [18] <author> R.Gnanadesikan: </author> <title> "Methods for statistical Data Analysis of Multivariate Observations" (1977) 24 APPENDIX </title>
Reference-contexts: Also, there are several methods to compute distance between two clusters (three major methods are shortest distance, average distance and longest distance). Here, for simplicity, we choose non-overlapping clusters and average-distance clustering metric. <ref> [18] </ref> Clustering makes it easier to see the huge matrix (Figure 2 and 3). We cluster the set of the domains based on the cross entropy data shown in the previous subsection. The distance between two domains is calculated by the average of two cross-entropies in both directions.
References-found: 18

