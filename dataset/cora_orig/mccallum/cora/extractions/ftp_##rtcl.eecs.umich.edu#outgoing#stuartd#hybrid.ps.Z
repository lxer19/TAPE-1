URL: ftp://rtcl.eecs.umich.edu/outgoing/stuartd/hybrid.ps.Z
Refering-URL: http://www.eecs.umich.edu/RTCL/routing/
Root-URL: http://www.cs.umich.edu
Email: E-Mail Address: fkgshin,stuartdg@eecs.umich.edu  
Title: Analysis and Implementation of Hybrid Switching  
Author: Kang G. Shin and Stuart W. Daniel 
Address: Ann Arbor, Michigan 48109-2122  
Affiliation: Real-Time Computing Laboratory Department of Electrical Engineering and Computer Science The University of Michigan  
Date: 1995  
Note: To be presented at the International Symposium on Computer Architecture,  
Abstract: The switching scheme of a point-to-point network determines how packets flow through each node, and is a primary element in determining the network's performance. In this paper, we present and evaluate a new switching scheme called hybrid switching. Hybrid switching dynamically combines both virtual cut-through and wormhole switching to provide higher achievable throughput than wormhole alone, while significantly reducing the buffer space required at intermediate nodes when compared to virtual cut-through. This scheme is motivated by a comparison of virtual cut-through and wormhole switching through cycle-level simulations, and then evaluated using the same methods. To show the feasibility of hybrid switching, as well as to provide a common base for simulating and implementing a variety of switching schemes, we have designed SPIDER, a communication adapter built around a custom ASIC, the Programmable Routing Controller (PRC). 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> W. J. Dally and C. L. Seitz, </author> <title> "The torus routing chip," </title> <journal> Journal of Distributed Computing, </journal> <volume> vol. 1, no. 3, </volume> <pages> pp. 187-196, </pages> <year> 1986. </year>
Reference-contexts: In addition, the burden placed on the host to handle communication-related activities should be minimized. One of the key factors that determines how well a point-to-point network meets applications' requirements in these areas is its switching scheme (s). Wormhole <ref> [1] </ref> and virtual cut-through [2] switching are two common schemes for forwarding packets through a point-to-point interconnection network. Both are "cut-through" switching schemes that decrease packet latencies by immediately forwarding incoming packets to idle output links. <p> SPIDER is micro-programmable with a wide range of routing and switching schemes, providing an ideal platform for experimenting with and comparing routing and switching schemes. 2.1 Existing Router Architectures Several routers that use wormhole switching have been developed <ref> [1, 7-9] </ref>. In general, the design of these routers has emphasized speed and simplicity, with the routing algorithm hardwired into the system. Each router only supports a small number of links, allowing a crossbar to be used to transfer data without internal blocking.
Reference: [2] <author> P. Kermani and L. Kleinrock, </author> <title> "Virtual cut-through: A new computer communication switching technique," </title> <journal> Computer Networks, </journal> <volume> vol. 3, </volume> <pages> pp. 267-286, </pages> <month> September </month> <year> 1979. </year>
Reference-contexts: In addition, the burden placed on the host to handle communication-related activities should be minimized. One of the key factors that determines how well a point-to-point network meets applications' requirements in these areas is its switching scheme (s). Wormhole [1] and virtual cut-through <ref> [2] </ref> switching are two common schemes for forwarding packets through a point-to-point interconnection network. Both are "cut-through" switching schemes that decrease packet latencies by immediately forwarding incoming packets to idle output links.
Reference: [3] <author> J. Ngai and C. Seitz, </author> <title> "A framework for adaptive routing in multicomputer networks," </title> <booktitle> in Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 1-9, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: As the traffic load increases, however, the probability of blocking increases, as does the likelihood of blocking other packets. Consequently, networks that use wormhole switching generally saturate from contention well before they exhaust their bandwidth <ref> [3, 4] </ref>. The effects of this contention can be reduced by increasing the number of virtual channels per physical link [4].
Reference: [4] <author> W. Dally, </author> <title> "Virtual-channel flow control," </title> <journal> IEEE Trans. Parallel and Distributed Systems, </journal> <volume> vol. 3, </volume> <pages> pp. 194-205, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: As the traffic load increases, however, the probability of blocking increases, as does the likelihood of blocking other packets. Consequently, networks that use wormhole switching generally saturate from contention well before they exhaust their bandwidth <ref> [3, 4] </ref>. The effects of this contention can be reduced by increasing the number of virtual channels per physical link [4]. <p> Consequently, networks that use wormhole switching generally saturate from contention well before they exhaust their bandwidth [3, 4]. The effects of this contention can be reduced by increasing the number of virtual channels per physical link <ref> [4] </ref>. Since either wormhole or virtual cut-through switching may yield shorter packet latencies, depending on the network traffic and the number of hops the packet must travel, it is advantageous to support both switching schemes in order to adapt to a wider range of circumstances. <p> Access to the bus is regulated by a binary priority-tree arbiter [13, 14]. 2.3 SPIDER Components As shown in Figure 1, SPIDER manages bidirectional communication with up to four neighboring nodes, with three virtual channels <ref> [4] </ref> on each unidirectional link. The programmable routing controller (PRC), a 231-pin, 1:3fi1:5 cm custom integrated circuit, is the cornerstone of SPIDER [5, 6, 13]. The 12 Transmitter Fetch Units (TFUs) control packet transmission, while the 4 micro-programmable routing engines coordinate packet reception. <p> Its peak throughput is dependent upon the link load and not upon packet distance. The maximum throughput of a network using wormhole switching can be increased by adding virtual channels <ref> [4] </ref>, or by significantly enlarging the number of flits buffered at each node. Adding virtual channels on each link, on the other hand, improves throughput by allowing packets to "bypass" stalled packets.
Reference: [5] <author> J. Dolter, S. Daniel, A. Mehra, J. Rexford, W. Feng, and K. Shin, "SPIDER: </author> <title> Flexible and efficient communication support for point-to-point distributed systems," </title> <booktitle> in Proc. Int'l Conf. on Distributed Computing Systems, </booktitle> <pages> pp. 574-580, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: The paper concludes with Section 5, which summarizes our main contributions and future directions. 2 A Flexible Router Architecture In order to isolate and take advantage of the differences in performance between cut-through switching schemes, we have developed SPIDER (Scalable Point-to-Point Interface DrivER) <ref> [5, 6] </ref>, a communication adapter that implements multiple switching schemes. SPIDER is micro-programmable with a wide range of routing and switching schemes, providing an ideal platform for experimenting with and comparing routing and switching schemes. 2.1 Existing Router Architectures Several routers that use wormhole switching have been developed [1, 7-9]. <p> The programmable routing controller (PRC), a 231-pin, 1:3fi1:5 cm custom integrated circuit, is the cornerstone of SPIDER <ref> [5, 6, 13] </ref>. The 12 Transmitter Fetch Units (TFUs) control packet transmission, while the 4 micro-programmable routing engines coordinate packet reception. Each routing engine performs low-level routing and switching operations for a single incoming link, with the three virtual channels sharing the custom processor.
Reference: [6] <author> S. Daniel, J. Rexford, J. Dolter, and K. Shin, </author> <title> "A programmable routing controller for flexible communications in point-to-point networks." </title> <booktitle> Submitted to International Conference on Computer Design, </booktitle> <year> 1995. </year>
Reference-contexts: The paper concludes with Section 5, which summarizes our main contributions and future directions. 2 A Flexible Router Architecture In order to isolate and take advantage of the differences in performance between cut-through switching schemes, we have developed SPIDER (Scalable Point-to-Point Interface DrivER) <ref> [5, 6] </ref>, a communication adapter that implements multiple switching schemes. SPIDER is micro-programmable with a wide range of routing and switching schemes, providing an ideal platform for experimenting with and comparing routing and switching schemes. 2.1 Existing Router Architectures Several routers that use wormhole switching have been developed [1, 7-9]. <p> The programmable routing controller (PRC), a 231-pin, 1:3fi1:5 cm custom integrated circuit, is the cornerstone of SPIDER <ref> [5, 6, 13] </ref>. The 12 Transmitter Fetch Units (TFUs) control packet transmission, while the 4 micro-programmable routing engines coordinate packet reception. Each routing engine performs low-level routing and switching operations for a single incoming link, with the three virtual channels sharing the custom processor.
Reference: [7] <author> S. Borkar, R. Cohn, et al., </author> <title> "Supporting systolic and memory communication in iWarp," </title> <booktitle> in Proc. Int'l Symposium on Computer Architecture, </booktitle> <pages> pp. 70-81, </pages> <year> 1990. </year>
Reference: [8] <author> W. J. Dally, J. A. S. Fiske, J. S. Keen, R. A. Lethin, M. D. Noakes, P. R. Nuth, R. E. Davison, and G. A. Fyler, </author> <title> "The Message-Driven Processor: A multicomputer processing node with efficient mechanisms," </title> <booktitle> IEEE Micro, </booktitle> <pages> pp. 23-39, </pages> <month> April </month> <year> 1992. </year>
Reference: [9] <author> D. Smitley, F. Hady, and D. Burns, </author> <title> "Hnet: A high-performance network evaluation testbed," </title> <type> Tech. Rep. </type> <institution> SRC-TR-91-049, Supercomputing Research Center, Institute for Defense Analyses, </institution> <month> December </month> <year> 1991. </year>
Reference: [10] <author> C. B. Stunkel, D. G. Shea, B. Abali, M. M. Den-neau, P. H. Hochschild, D. J. Joseph, B. J. Nathan-son, M. Tsao, and P. R. Varker, </author> <booktitle> "Architecture and implementation of Vulcan," in Proc. International Parallel Processing Symposium, </booktitle> <pages> pp. 268-274, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Each router only supports a small number of links, allowing a crossbar to be used to transfer data without internal blocking. Furthermore, the short internode distances allow flow control and parallel internode links to be efficiently implemented. The Vulcan Switch chip <ref> [10] </ref> uses an interesting variation, by adding a central, dynamically allocated queue to the switching element. This queue improves throughput by buffering "chunks" of packets in the blocking switch, rather than buffering the flits in several different switches and blocking those channels.
Reference: [11] <author> A. L. Davis, "Mayfly: </author> <title> A general-purpose, scalable, </title> <booktitle> parallel processing architecture," Lisp and Symbolic Computation, </booktitle> <volume> vol. 5, </volume> <pages> pp. 7-47, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: This queue improves throughput by buffering "chunks" of packets in the blocking switch, rather than buffering the flits in several different switches and blocking those channels. Virtual cut-through routers typically provide better throughput under heavy loads at the cost of increased buffer requirements. The Mayfly Post Office <ref> [11] </ref>, uses several (hardwired) routing algorithms and provides an internal buffer for packets that cannot cut through, but only supports virtual cut-through switching. It uses a shared internal bus to transfer packets between ports and also to and from the buffer pool.
Reference: [12] <author> K. Bolding, S.-C. Cheung, S.-E. Choi, C. Ebeling, S. Hassoun, T. A. Ngo, and R. Wille, </author> <title> "The Chaos router chip: Design and implementation of an adaptive router," </title> <booktitle> in Proc. VLSI, </booktitle> <month> September </month> <year> 1993. </year>
Reference-contexts: The Mayfly Post Office [11], uses several (hardwired) routing algorithms and provides an internal buffer for packets that cannot cut through, but only supports virtual cut-through switching. It uses a shared internal bus to transfer packets between ports and also to and from the buffer pool. The Chaos router <ref> [12] </ref> also provides an internal buffer for packets, but this buffer is much smaller | the router deroutes packets to avoid blocking or dropping them. 2.2 SPIDER SPIDER is designed to support multiple switching schemes, including store-and-forward, virtual cut-through, and wormhole switching.
Reference: [13] <author> J. Dolter, </author> <title> A Programmable Routing Controller Supporting Multi-mode Routing and Switching in Distributed Real-Time Systems. </title> <type> PhD thesis, </type> <institution> University of Michigan, </institution> <month> September </month> <year> 1993. </year>
Reference-contexts: Similarly, cut-through switching schemes require a high-bandwidth switch for transferring data between incoming and outgoing channels. In SPIDER, this is provided by a demand-slotted, time division-multiplexed (TDM) bus with bandwidth equal to the physical links. Access to the bus is regulated by a binary priority-tree arbiter <ref> [13, 14] </ref>. 2.3 SPIDER Components As shown in Figure 1, SPIDER manages bidirectional communication with up to four neighboring nodes, with three virtual channels [4] on each unidirectional link. The programmable routing controller (PRC), a 231-pin, 1:3fi1:5 cm custom integrated circuit, is the cornerstone of SPIDER [5, 6, 13]. <p> The programmable routing controller (PRC), a 231-pin, 1:3fi1:5 cm custom integrated circuit, is the cornerstone of SPIDER <ref> [5, 6, 13] </ref>. The 12 Transmitter Fetch Units (TFUs) control packet transmission, while the 4 micro-programmable routing engines coordinate packet reception. Each routing engine performs low-level routing and switching operations for a single incoming link, with the three virtual channels sharing the custom processor. <p> the host schedules the packet for transmission to the subsequent node in its route. 3 Comparing Wormhole and Vir tual Cut-through Switching To evaluate the performance of SPIDER and also to more accurately compare the performance of the various routing and switching schemes, we have developed a cycle-level discrete-event simulator <ref> [13, 16] </ref>. Written in C++, this simulator accurately models the flow of the individual bytes of packets through SPIDER. This captures features such as the low-level flow control, bus arbitration delays, and microcode execution time.
Reference: [14] <author> A. Kovaleski, S. Ratheal, and F. Lombardi, </author> <title> "An architecture and interconnection scheme for time-sliced buses in real-time processing," </title> <booktitle> Proc. Real-Time Systems Symposium, </booktitle> <pages> pp. 20-27, </pages> <year> 1986. </year>
Reference-contexts: Similarly, cut-through switching schemes require a high-bandwidth switch for transferring data between incoming and outgoing channels. In SPIDER, this is provided by a demand-slotted, time division-multiplexed (TDM) bus with bandwidth equal to the physical links. Access to the bus is regulated by a binary priority-tree arbiter <ref> [13, 14] </ref>. 2.3 SPIDER Components As shown in Figure 1, SPIDER manages bidirectional communication with up to four neighboring nodes, with three virtual channels [4] on each unidirectional link. The programmable routing controller (PRC), a 231-pin, 1:3fi1:5 cm custom integrated circuit, is the cornerstone of SPIDER [5, 6, 13].
Reference: [15] <institution> Advanced Micro Devices, </institution> <type> 901 Thompson Place, </type> <address> P.O. Box 3453, Sunnyvale CA 94088-3453, </address> <note> Am79168/Am79169 TAXI tm -275 Technical Manual, ban-0.1m-1/93/0 17490a ed. </note>
Reference-contexts: The Network Interface Transmitters (NI TXs) and Network Interface Receivers (NI RXs) perform the necessary interleaving of virtual channels to and from the physical links, on a word-by-word basis 1 . The network interface (NI) performs the media access and flow control on four pairs of AMD TAXI chips <ref> [15] </ref>; these TAXI transmitters and receivers control the physical links, providing a low-cost fiber-optic communication fabric. SPIDER treats outbound virtual channels (NI TXs) as individually reservable resources, allowing the device to support a variety of routing and switching schemes through flexible control over channel allocation policies.
Reference: [16] <author> J. Rexford, J. Dolter, W. Feng, and K. G. Shin, </author> <title> "PP-MESS-SIM: A simulator for evaluating mul-ticomputer interconnection networks." </title> <booktitle> To appear in Proc. Annual Simulation Symposium, </booktitle> <month> April </month> <year> 1995. </year>
Reference-contexts: the host schedules the packet for transmission to the subsequent node in its route. 3 Comparing Wormhole and Vir tual Cut-through Switching To evaluate the performance of SPIDER and also to more accurately compare the performance of the various routing and switching schemes, we have developed a cycle-level discrete-event simulator <ref> [13, 16] </ref>. Written in C++, this simulator accurately models the flow of the individual bytes of packets through SPIDER. This captures features such as the low-level flow control, bus arbitration delays, and microcode execution time.
Reference: [17] <author> W. J. Dally and C. L. Seitz, </author> <title> "Deadlock-free message routing in multiprocessor interconnection networks," </title> <journal> IEEE Trans. Computers, </journal> <volume> vol. C-36, no. 5, </volume> <pages> pp. 547-553, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: Packet destinations were uniformly distributed across all of the nodes (except where otherwise specified). The simulations also used a fixed packet size of 64 bytes (except where specified). To focus the experiments on the switching scheme, all packets use a static, dimension-ordered routing scheme <ref> [17] </ref>. Furthermore, most of the simulations use an unwrapped square mesh topology where only one virtual channel per link is required to prevent deadlock under wormhole switching. This allows the switching schemes to be compared with the same number of virtual channels.
Reference: [18] <author> A. A. Chien, </author> <title> "A cost and speed model for k-ary n-cube wormhole routers," </title> <booktitle> in Proc. Hot Interconnects, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: The primary cost is in the increased complexity of the crossbar connecting the reception channels to the transmission channels | either the size of the crossbar must be increased, or the arbitration becomes more complex <ref> [18] </ref>. Giving each virtual channel a flit buffer large enough to hold one packet should significantly improve throughput | each blocked packet only stalls a single link. Similarly, buffers capable of holding half of a packet's flits will prevent blocked packets from stalling more than two links.
References-found: 18

