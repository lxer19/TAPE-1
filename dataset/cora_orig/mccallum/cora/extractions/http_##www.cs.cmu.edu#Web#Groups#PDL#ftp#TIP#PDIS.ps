URL: http://www.cs.cmu.edu/Web/Groups/PDL/ftp/TIP/PDIS.ps
Refering-URL: http://c.gp.cs.cmu.edu:5103/afs/cs/project/multiC-sys-sw/WWW/publications.html
Root-URL: 
Title: subsystems exploit data striping to provide high throughput: high data rate for large parallel transfers
Author: R. Hugo Patterson, Garth A. Gibson* 
Address: Pittsburgh PA 15213  
Affiliation: Department of Electrical and Computer Engineering *School of Computer Science Carnegie Mellon University,  
Note: 1: Introduction RAID  This work was supported in part by the National Science Foundation under grant number ECD-8907068 and by an IBM Graduate Fellowship.  As presented previously,  Appears in Proc. Third International Conf. on Parallel and Distributed Information Systems, Austin, TX, Sept. 28-30, 1994, pp. 7-16  
Pubnum: [Sandberg85, Satya85].  
Abstract: Informed prefetching provides a simple mechanism for I/O-intensive, cache-ineffective applications to efficiently exploit highly-parallel I/O subsystems such as disk arrays. This mechanism, dynamic disclosure of future accesses, yields substantial benefits over sequential readahead mechanisms found in current file systems for non-sequential workloads. This paper reports the performance of the Transparent Informed Prefetching system (TIP), a minimal prototype implemented in a Mach 3.0 system with up to four disks. We measured reductions by factors of up to 1.9 and 3.7 in the execution time of two example applications: multi-file text search and scientific data visualization. Reducing program execution time is commonly the reason for purchasing new, faster processors. However, for programs that process stored data, faster processors do not linearly decrease execution time unless they are coupled with proportionately faster storage systems. Because storage performance is increasing more slowly than processor performance, data-intensive programs do not benefit as much as one might expect from a faster processor. To directly combat this limitation, new storage systems are increasing disk parallelism, usually in the form of Redun- dant Arrays of Inexpensive Disks (RAID) [Patterson88, Gibson91]. Caching recently used file blocks can provide fast access when a programs workload is small or has high locality. But, growing file sizes and the large volume of read-once data limit the effectiveness of file caching [Baker91]. Beyond caching, prefetching soon-to-be- needed file blocks is the best method of reducing storage access time [Feiertag71, McKusick84]. To be most successful, prefetching should be based on the knowledge of future accesses often available within applications. By passing hints to the file system, applications can disclose this information to lower levels of the system. There, it may be combined with global knowledge of the competing demands for system resources. Thus informed, a file system can transparently prefetch needed data and optimize resource utilization. We call this informed prefetching. 
Abstract-found: 1
Intro-found: 0
Reference: [Accetta86] <editor> Accetta, M.J., et al, </editor> <title> Mach: A New Kernel Foundation for Unix Development, </title> <booktitle> Proc. of the Summer 1986 USENIX Conference, </booktitle> <year> 1986, </year> <pages> pp. 93-113. </pages>
Reference-contexts: Section 3 describes our test applications in detail and reports their performance in our system. Sections 4 and 5 discuss related work and conclusions. 2: The TIP prototype As Figure 1 shows, we built our prototype Transparent Informed Prefetching system (TIP), in a Mach 3.0 system <ref> [Accetta86, Golub90] </ref> augmented with disk striping software (UX version 42, MK version 83). We installed TIP in the file buffer cache of the 4.3BSD Unix Fast File System (FFS) [McKusick84] in the UX server where it has the opportunity to execute whenever a buffer or disk was accessed.
Reference: [Baker91] <author> Baker, M.G., Hartman, J.H., Kupfer, M.D., Shirriff, K.W., and Ousterhout, J.K., </author> <title> Measurements of a Distributed File System, </title> <booktitle> Proc. of the 13th Symp. on Operating System Principles, </booktitle> <address> Pacific Grove, CA, </address> <month> October </month> <year> 1991, </year> <pages> pp. 198-212. </pages>
Reference-contexts: Caching recently used file blocks can provide fast access when a programs workload is small or has high locality. But, growing file sizes and the large volume of read-once data limit the effectiveness of file caching <ref> [Baker91] </ref>. Beyond caching, prefetching soon-to-be- needed file blocks is the best method of reducing storage access time [Feiertag71, McKusick84]. To be most successful, prefetching should be based on the knowledge of future accesses often available within applications.
Reference: [Cao94] <author> Cao, P., Felten, E.W., Li, K., </author> <title> User Level File Caching Policies, </title> <booktitle> Proc.of the Summer 1994 USENIX Conference, </booktitle> <address> Boston, MA, </address> <year> 1994, </year> <pages> pp. 171-182. </pages>
Reference-contexts: Informed cache management: Knowledge of future I/O requests can be used to hold on to needed blocks and avoid disk accesses altogether. Thus, an informed cache can outperform a standard LRU cache even without prefetching <ref> [Chou85, Korner90, Cao94] </ref>. To obtain the full benefit of these mechanisms, application hints to an informed prefetching system should not advise particular lower-level policies or actions. Instead, they should disclose knowledge of future accesses using the same abstractions and semantics that the application later uses for I/O requests.
Reference: [Chen90] <author> Chen, P. M., Gibson, G. A., Katz, R. H., Patterson, D. A., </author> <title> An Evaluation of Redundant Arrays of Disks Using an Amdahl 5890, </title> <booktitle> Proc. of the 1990 ACM Conf. on Measurement and Modeling of Computer Systems (SIGMETRICS), </booktitle> <address> Boulder CO, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: RAID subsystems exploit data striping to provide high throughput: high data rate for large parallel transfers and I/O concurrency and disk load balancing for large numbers of small accesses [Kim86, Livny87]. Unfortunately, RAID subsystems cannot reduce the access latency of isolated small reads and can increase small write latencies <ref> [Chen90, Stodolsky93] </ref>.
Reference: [Chen93] <author> Chen, J.B., Bershad, </author> <title> B.N., The Impact of Operating System Structure on Memory System Performance, </title> <booktitle> Proc. of the 14th Symp. on Operating System Principles, </booktitle> <year> 1993, </year> <pages> pp. 120-133. </pages>
Reference-contexts: The Striper process striped data across the disks with a stripe unit of 128 512-byte sectors, or eight 8 Kbyte file blocks. There were 400 8 Kbyte buffers in the file cache. Although Mach 3.0 may be inefficient in terms of instruction counts and memory cache behavior <ref> [Chen93] </ref> particularly when an applications primary activity is moving lots of data [Druschel93], it allowed us to add TIP and the disk striping functionality outside of the kernel.
Reference: [Chou85] <author> Chou, H. T., DeWitt, D. J., </author> <title> An Evaluation of Buffer Management Strategies for Relational Database Systems, </title> <booktitle> Proc. of the 11th Int. Conf. on Very Large Data Bases, </booktitle> <address> Stockholm, </address> <year> 1985, </year> <pages> pp. 127-141. </pages>
Reference-contexts: Informed cache management: Knowledge of future I/O requests can be used to hold on to needed blocks and avoid disk accesses altogether. Thus, an informed cache can outperform a standard LRU cache even without prefetching <ref> [Chou85, Korner90, Cao94] </ref>. To obtain the full benefit of these mechanisms, application hints to an informed prefetching system should not advise particular lower-level policies or actions. Instead, they should disclose knowledge of future accesses using the same abstractions and semantics that the application later uses for I/O requests. <p> Database systems researchers have long recognized the opportunity to accurately prefetch based on application level knowledge [Stonebraker81]. They have also extensively examined the opportunity to apply this knowledge through advice to buffer management algorithms <ref> [Sacco82, Chou85, Cornell89, Ng91] </ref> and for I/O optimizations [Selinger79]. We hope to extend these techniques to informed prefetching. Many researchers have looked into prefetching based on access patterns inferred from the stream of user I/O requests [Curewitz93, Kotz91, Tait91, Palmer91, Kor- ner90].
Reference: [Cornell89] <author> Cornell, D. W., Yu, P. S., </author> <title> Integration of Buffer Management and Query Optimization in Relational Database Environment, </title> <booktitle> Proc. of the 15th Int. Conf. on Very Large Data Bases, </booktitle> <address> Amsterdam, </address> <month> Aug. </month> <year> 1989, </year> <pages> pp. 247-255. </pages>
Reference-contexts: Database systems researchers have long recognized the opportunity to accurately prefetch based on application level knowledge [Stonebraker81]. They have also extensively examined the opportunity to apply this knowledge through advice to buffer management algorithms <ref> [Sacco82, Chou85, Cornell89, Ng91] </ref> and for I/O optimizations [Selinger79]. We hope to extend these techniques to informed prefetching. Many researchers have looked into prefetching based on access patterns inferred from the stream of user I/O requests [Curewitz93, Kotz91, Tait91, Palmer91, Kor- ner90].
Reference: [Curewitz93] <author> Curewitz, K.M., Krishnan, P., Vitter, J.S., </author> <title> Practical Prefetching via Data Compression, </title> <booktitle> Proc. of the 1993 ACM Conf. on Management of Data (SIGMOD), </booktitle> <address> Washington, DC, </address> <month> May, </month> <year> 1993, </year> <pages> pp. 257-66. </pages>
Reference-contexts: We hope to extend these techniques to informed prefetching. Many researchers have looked into prefetching based on access patterns inferred from the stream of user I/O requests <ref> [Curewitz93, Kotz91, Tait91, Palmer91, Kor- ner90] </ref>. Kotz looked at intelligent prefetching for MIMD multiprocessors with scientific workloads. He extended the applicability of readahead to non-sequential, but regular, accesses within one file by predicting future accesses based on previously observed access patterns.
Reference: [Druschel93] <author> Druschel, P., Peterson, L.L., Fbufs: </author> <title> A High-Bandwidth Cross-Domain Transfer Facility, </title> <booktitle> Proc. of the 14th Symp. on Operating System Principles, </booktitle> <year> 1993, </year> <pages> pp. 189-202. </pages>
Reference-contexts: There were 400 8 Kbyte buffers in the file cache. Although Mach 3.0 may be inefficient in terms of instruction counts and memory cache behavior [Chen93] particularly when an applications primary activity is moving lots of data <ref> [Druschel93] </ref>, it allowed us to add TIP and the disk striping functionality outside of the kernel. More- over, operating system overheads have little impact on the implemented in the Mach 3.0 operating system which is decomposed into a user-level Unix server, UX, and a microkernel, MK.
Reference: [Feiertag71] <author> Feiertag, R. J., Organisk, E. I., </author> <title> The Multics Input/- Output System, </title> <booktitle> Proc. of the 3rd Symp. on Operating System Principles, </booktitle> <year> 1971, </year> <pages> pp 35-41. </pages>
Reference-contexts: But, growing file sizes and the large volume of read-once data limit the effectiveness of file caching [Baker91]. Beyond caching, prefetching soon-to-be- needed file blocks is the best method of reducing storage access time <ref> [Feiertag71, McKusick84] </ref>. To be most successful, prefetching should be based on the knowledge of future accesses often available within applications. By passing hints to the file system, applications can disclose this information to lower levels of the system.
Reference: [Gibson91] <author> Gibson, G. A., </author> <title> Redundant Disk Arrays: Reliable, Parallel Secondary Storage, </title> <type> Ph.D. dissertation, </type> <institution> University of California, Berkeley, </institution> <type> technical report UCB/CSD 91/613, </type> <month> April </month> <year> 1991. </year> <note> Available in MIT Press 1991 ACM distinguished dissertation series, </note> <year> 1992. </year>
Reference-contexts: Because storage performance is increasing more slowly than processor performance, data-intensive programs do not benefit as much as one might expect from a faster processor. To directly combat this limitation, new storage systems are increasing disk parallelism, usually in the form of Redun- dant Arrays of Inexpensive Disks (RAID) <ref> [Patterson88, Gibson91] </ref>. RAID subsystems exploit data striping to provide high throughput: high data rate for large parallel transfers and I/O concurrency and disk load balancing for large numbers of small accesses [Kim86, Livny87].
Reference: [Golub90] <author> Golub, D., Dean, R., Forin, A., Rashid, R., </author> <title> Unix as an Application Program, </title> <booktitle> Proc.of the Summer 1990 USENIX Conference, </booktitle> <year> 1990, </year> <pages> pp. 87-95. </pages>
Reference-contexts: Section 3 describes our test applications in detail and reports their performance in our system. Sections 4 and 5 discuss related work and conclusions. 2: The TIP prototype As Figure 1 shows, we built our prototype Transparent Informed Prefetching system (TIP), in a Mach 3.0 system <ref> [Accetta86, Golub90] </ref> augmented with disk striping software (UX version 42, MK version 83). We installed TIP in the file buffer cache of the 4.3BSD Unix Fast File System (FFS) [McKusick84] in the UX server where it has the opportunity to execute whenever a buffer or disk was accessed.
Reference: [Grimshaw91] <author> Grimshaw, A.S., Loyot Jr., </author> <title> E.C., ELFS: ObjectOriented Extensible File Systems, </title> <institution> Computer Science Report No. TR-91-14, University of Virginia, </institution> <month> July 8, </month> <year> 1991. </year>
Reference-contexts: However, Korner uses traces of file system activity to predict future access patterns. Researchers have also proposed an object-oriented file system layered on top of the Unix file system called ELFS <ref> [Grimshaw91] </ref>. ELFS has knowledge of file structure and high-level file operations that allow it to help prefetch and cache operations. However, ELFS emphasizes user control over file activity.
Reference: [Kiczales92] <author> Kiczales, G., </author> <title> Towards a New Model of Abstraction in the Engineering of Software, </title> <booktitle> Proc. of the IMSA 92 Workshop on Reection and Meta-level Architectures, </booktitle> <year> 1992. </year>
Reference-contexts: A much better solution is to formally incorporate a path for the disclosure of optimization information into the interface to the H layer of the library <ref> [Kiczales92] </ref>. We have done this by adding a Hhint () routine. It accepts hints from higher layers of the library in the language used by the rest of H layer: offsets and lengths within data objects.
Reference: [Kim86] <author> Kim, M.Y., </author> <title> Synchronized Disk Interleaving, </title> <journal> IEEE Trans. on Computers, V. </journal> <volume> C-35 (11), </volume> <month> November </month> <year> 1986. </year>
Reference-contexts: RAID subsystems exploit data striping to provide high throughput: high data rate for large parallel transfers and I/O concurrency and disk load balancing for large numbers of small accesses <ref> [Kim86, Livny87] </ref>. Unfortunately, RAID subsystems cannot reduce the access latency of isolated small reads and can increase small write latencies [Chen90, Stodolsky93].
Reference: [Korner90] <author> Korner, K., </author> <title> Intelligent Caching for Remote File Service, </title> <booktitle> Proc. of the Tenth Int. Conf. on Distributed Computing Systems, </booktitle> <year> 1990, </year> <month> pp.220-226. </month>
Reference-contexts: Informed cache management: Knowledge of future I/O requests can be used to hold on to needed blocks and avoid disk accesses altogether. Thus, an informed cache can outperform a standard LRU cache even without prefetching <ref> [Chou85, Korner90, Cao94] </ref>. To obtain the full benefit of these mechanisms, application hints to an informed prefetching system should not advise particular lower-level policies or actions. Instead, they should disclose knowledge of future accesses using the same abstractions and semantics that the application later uses for I/O requests.
Reference: [Kotz91] <author> Kotz, D., Ellis, C.S., </author> <title> Practical Prefetching Techniques for Parallel File Systems, </title> <booktitle> Proc. First International Conf. on Parallel and Distributed Information Systems, </booktitle> <address> Miami Beach, Florida, </address> <month> Dec. </month> <pages> 4-6, </pages> <year> 1991, </year> <pages> pp. 182-189. </pages>
Reference-contexts: We hope to extend these techniques to informed prefetching. Many researchers have looked into prefetching based on access patterns inferred from the stream of user I/O requests <ref> [Curewitz93, Kotz91, Tait91, Palmer91, Kor- ner90] </ref>. Kotz looked at intelligent prefetching for MIMD multiprocessors with scientific workloads. He extended the applicability of readahead to non-sequential, but regular, accesses within one file by predicting future accesses based on previously observed access patterns.
Reference: [Lefer89] <author> Lefer, S.J., McKusick, M.K., Karels, M.J., Quarterman, J.S., </author> <title> The Design and Implementation of the 4.3BSD Unix Operating System, </title> <publisher> Addison-Wesley, </publisher> <address> New York, </address> <year> 1989. </year>
Reference: [Livny87] <author> Livny, M., Khoshafian, S., Boral, H., </author> <booktitle> Multidisk Management Algorithms, Proc. of the 1987 ACM Conf. on Measurement and Modeling of Computer Systems (SIGMETRICS), </booktitle> <month> May </month> <year> 1987. </year>
Reference-contexts: RAID subsystems exploit data striping to provide high throughput: high data rate for large parallel transfers and I/O concurrency and disk load balancing for large numbers of small accesses <ref> [Kim86, Livny87] </ref>. Unfortunately, RAID subsystems cannot reduce the access latency of isolated small reads and can increase small write latencies [Chen90, Stodolsky93].
Reference: [McKusick84] <author> McKusick, M. K., Joy, W. J., Lefer, S. J., Fabry, R. S., </author> <title> A Fast File System for Unix, </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> V 2 (3), </volume> <month> August </month> <year> 1984, </year> <pages> pp. 181-197. </pages>
Reference-contexts: But, growing file sizes and the large volume of read-once data limit the effectiveness of file caching [Baker91]. Beyond caching, prefetching soon-to-be- needed file blocks is the best method of reducing storage access time <ref> [Feiertag71, McKusick84] </ref>. To be most successful, prefetching should be based on the knowledge of future accesses often available within applications. By passing hints to the file system, applications can disclose this information to lower levels of the system. <p> We installed TIP in the file buffer cache of the 4.3BSD Unix Fast File System (FFS) <ref> [McKusick84] </ref> in the UX server where it has the opportunity to execute whenever a buffer or disk was accessed. The system ran on a DECstation 5000/200 with 32 megabytes of RAM, two SCSI strings and up to four IBM 0661 Lightning disks formatted with a file block size of 8Kbytes.
Reference: [Ng91] <author> Ng, R., Faloutsos, C., Sellis, T., </author> <title> Flexible Buffer Allocation Based on Marginal Gains, </title> <booktitle> Proc. of the 1991 ACM Conf. on Management of Data (SIGMOD), </booktitle> <pages> pp. 387-396. 10 </pages>
Reference-contexts: Database systems researchers have long recognized the opportunity to accurately prefetch based on application level knowledge [Stonebraker81]. They have also extensively examined the opportunity to apply this knowledge through advice to buffer management algorithms <ref> [Sacco82, Chou85, Cornell89, Ng91] </ref> and for I/O optimizations [Selinger79]. We hope to extend these techniques to informed prefetching. Many researchers have looked into prefetching based on access patterns inferred from the stream of user I/O requests [Curewitz93, Kotz91, Tait91, Palmer91, Kor- ner90].
Reference: [Palmer91] <author> Palmer, </author> <title> M.L., Zdonik, S.B., FIDO: A Cache that Learns to Fetch, </title> <institution> Brown University Technical Report CS-9015, </institution> <year> 1991. </year>
Reference-contexts: We hope to extend these techniques to informed prefetching. Many researchers have looked into prefetching based on access patterns inferred from the stream of user I/O requests <ref> [Curewitz93, Kotz91, Tait91, Palmer91, Kor- ner90] </ref>. Kotz looked at intelligent prefetching for MIMD multiprocessors with scientific workloads. He extended the applicability of readahead to non-sequential, but regular, accesses within one file by predicting future accesses based on previously observed access patterns.
Reference: [Patterson88] <author> Patterson, D., Gibson, G., Katz, R., </author> <title> A, A Case for Redundant Arrays of Inexpensive Disks (RAID), </title> <booktitle> Proc. of the 1988 ACM Conf. on Management of Data (SIGMOD), </booktitle> <address> Chicago, IL, </address> <month> June </month> <year> 1988, </year> <pages> pp. 109-116. </pages>
Reference-contexts: Because storage performance is increasing more slowly than processor performance, data-intensive programs do not benefit as much as one might expect from a faster processor. To directly combat this limitation, new storage systems are increasing disk parallelism, usually in the form of Redun- dant Arrays of Inexpensive Disks (RAID) <ref> [Patterson88, Gibson91] </ref>. RAID subsystems exploit data striping to provide high throughput: high data rate for large parallel transfers and I/O concurrency and disk load balancing for large numbers of small accesses [Kim86, Livny87].
Reference: [Patterson93] <author> Patterson, R.H., Gibson, G.A., Satyanarayanan, M., </author> <title> A Status Report on Research in Transparent Informed Prefetching, </title> <journal> Operating Systems Review, </journal> <volume> V 27 (2), </volume> <month> April, </month> <year> 1993, </year> <pages> pp. 21-35. </pages>
Reference-contexts: There, it may be combined with global knowledge of the competing demands for system resources. Thus informed, a file system can transparently prefetch needed data and optimize resource utilization. We call this informed prefetching. As presented previously, informed prefetching reduces application execution time through three mechanisms <ref> [Patterson93] </ref>. Exposure of an applications I/O concurrency: The primary advantage of informed prefetching is its ability to perform multiple I/O accesses in parallel so that applications and users spend less time waiting for these accesses to complete.
Reference: [Sacco82] <author> Sacco, G.M., Schkolnick, M., </author> <title> A Mechanism for Managing the Buffer Pool in a Relational Database System Using the Hot Set Model, </title> <booktitle> Proc. of the Eighth Int. Conf. on Very Large Data Bases, </booktitle> <month> September, </month> <year> 1982, </year> <pages> pp. 257-262. </pages>
Reference-contexts: Database systems researchers have long recognized the opportunity to accurately prefetch based on application level knowledge [Stonebraker81]. They have also extensively examined the opportunity to apply this knowledge through advice to buffer management algorithms <ref> [Sacco82, Chou85, Cornell89, Ng91] </ref> and for I/O optimizations [Selinger79]. We hope to extend these techniques to informed prefetching. Many researchers have looked into prefetching based on access patterns inferred from the stream of user I/O requests [Curewitz93, Kotz91, Tait91, Palmer91, Kor- ner90].
Reference: [Sandberg85] <author> Sandberg, R., Goldberg, D., Kleiman, S., Walsh, D., Lyon, B., </author> <title> Design and Implementation of the Sun Network File System, </title> <booktitle> Proc. of the Summer 1985 USENIX Conference, </booktitle> <address> Portland, OR, </address> <month> June </month> <year> 1985, </year> <pages> pp. 119-30. </pages>
Reference-contexts: Since serial streams of small accesses dominate many important workloads, access latency is an increasingly important component of overall system performance. Distributed file systems further increase its importance by adding transfer and server overheads to the storage access time <ref> [Sandberg85, Satya85] </ref>. Caching recently used file blocks can provide fast access when a programs workload is small or has high locality. But, growing file sizes and the large volume of read-once data limit the effectiveness of file caching [Baker91].
Reference: [Satya85] <author> Satyanarayanan, M., Howard, J. Nichols, D., Sidebotham, R., Spector, A., West, M., </author> <title> The ITC Distributed File System: </title> <booktitle> Principles and Design, Proc. of the Tenth Symp. on Operating Systems Principles, ACM, </booktitle> <month> December </month> <year> 1985, </year> <pages> pp. 3550. </pages>
Reference-contexts: Since serial streams of small accesses dominate many important workloads, access latency is an increasingly important component of overall system performance. Distributed file systems further increase its importance by adding transfer and server overheads to the storage access time <ref> [Sandberg85, Satya85] </ref>. Caching recently used file blocks can provide fast access when a programs workload is small or has high locality. But, growing file sizes and the large volume of read-once data limit the effectiveness of file caching [Baker91].
Reference: [Selinger79] <author> Selinger, P.G., Astrahan, </author> <title> M.M., Chamberlin, D.D., Lorie, R.A., Price, T.G., Access Path Selection in a Relational Database Management System, </title> <booktitle> Proc. of the 1979 ACM Conf. on Management of Data (SIGMOD), </booktitle> <address> Boston, MA, </address> <month> May, </month> <year> 1979, </year> <pages> pp. 23-34. </pages>
Reference-contexts: Database systems researchers have long recognized the opportunity to accurately prefetch based on application level knowledge [Stonebraker81]. They have also extensively examined the opportunity to apply this knowledge through advice to buffer management algorithms [Sacco82, Chou85, Cornell89, Ng91] and for I/O optimizations <ref> [Selinger79] </ref>. We hope to extend these techniques to informed prefetching. Many researchers have looked into prefetching based on access patterns inferred from the stream of user I/O requests [Curewitz93, Kotz91, Tait91, Palmer91, Kor- ner90]. Kotz looked at intelligent prefetching for MIMD multiprocessors with scientific workloads.
Reference: [Seltzer90] <author> Seltzer, M. I., Chen, P. M., Ousterhout, J. K., </author> <title> Disk Scheduling Revisited, </title> <booktitle> Proc. of the Winter 1990 USENIX Technical Conf., </booktitle> <address> Washington DC, </address> <month> January </month> <year> 1990. </year>
Reference-contexts: Third International Conf. on Parallel and Distributed Information Systems, Austin, TX, Sept. 28-30, 1994, pp. 7-16 2 Increased storage efficiency: Because informed prefetching systems prefetch aggressively, they can fill otherwise empty I/O queues with low-priority accesses and create opportunities for storage subsystem optimizations <ref> [Seltzer90] </ref>. Informed cache management: Knowledge of future I/O requests can be used to hold on to needed blocks and avoid disk accesses altogether. Thus, an informed cache can outperform a standard LRU cache even without prefetching [Chou85, Korner90, Cao94].
Reference: [Smith85] <author> Smith, A.J., </author> <title> Disk Cache--Miss Ratio Analysis and Design Considerations, </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> V 3 (3), </volume> <month> August </month> <year> 1985, </year> <pages> pp. 161-203. </pages>
Reference-contexts: He achieved significant performance improvements for stride access patterns in large scientific datasets. A drawback to speculative prefetching based on prior observations is that it risks hurting, rather than helping, performance <ref> [Smith85] </ref>. When a prediction is wrong, prefetching the unneeded data consumes valuable resources which could have been used for accessing needed data or storing recently used and soon-to-be-reused data.
Reference: [Steere94] <author> Steere, D., Satyanarayanan, M., </author> <title> A Case for Dynamic Sets, </title> <note> in preparation. </note>
Reference-contexts: While we are aware of the hint language richness and prefetching control problems that could be explored with synthetic workloads, we prefer to develop primitive mechanisms and allow higher level layers to build richer functionality on these primitives <ref> [Steere94] </ref>. Accordingly, an important goal in the development of our TIP prototype was to provide a platform for experimenting with I/O- intensive applications. For ease of implementation, hints are passed to a pseudo-device named /dev/tip through the Unix I/O control (ioctl) mechanism.
Reference: [Stodolsky93] <author> Stodolsky, D, Gibson, G., </author> <title> Parity Logging Disk Arrays, </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> V 12 (3), </volume> <month> August </month> <year> 1994. </year>
Reference-contexts: RAID subsystems exploit data striping to provide high throughput: high data rate for large parallel transfers and I/O concurrency and disk load balancing for large numbers of small accesses [Kim86, Livny87]. Unfortunately, RAID subsystems cannot reduce the access latency of isolated small reads and can increase small write latencies <ref> [Chen90, Stodolsky93] </ref>.
Reference: [Stonebraker81] <author> Stonebraker, Michael, </author> <title> Operating System Support for Database Management, </title> <journal> Communications of the ACM, </journal> <volume> V 24 (7), </volume> <month> July </month> <year> 1981, </year> <pages> pp. 412-418. </pages>
Reference-contexts: Hints are now widely enough understood that they appear in various existing implementations. For example, Sun Microsystems operating system provides two advise system calls that instruct the virtual memory systems policy decisions [Sun88]. Database systems researchers have long recognized the opportunity to accurately prefetch based on application level knowledge <ref> [Stonebraker81] </ref>. They have also extensively examined the opportunity to apply this knowledge through advice to buffer management algorithms [Sacco82, Chou85, Cornell89, Ng91] and for I/O optimizations [Selinger79]. We hope to extend these techniques to informed prefetching.
Reference: [Sun88] <author> Sun Microsystems, Inc., </author> <title> Sun OS Reference Manual, Part Number 800-1751-10, Revision A, </title> <month> May 9, </month> <year> 1988. </year>
Reference-contexts: For example, Trivedi suggested using programmer or compiler generated hints for prepaging [Trivedi79]. Hints are now widely enough understood that they appear in various existing implementations. For example, Sun Microsystems operating system provides two advise system calls that instruct the virtual memory systems policy decisions <ref> [Sun88] </ref>. Database systems researchers have long recognized the opportunity to accurately prefetch based on application level knowledge [Stonebraker81]. They have also extensively examined the opportunity to apply this knowledge through advice to buffer management algorithms [Sacco82, Chou85, Cornell89, Ng91] and for I/O optimizations [Selinger79].
Reference: [Tait91] <author> Tait, C.D., Duchamp, D., </author> <title> Detection and Exploitation of File Working Sets, </title> <booktitle> Proc. of the 11th Int. Conf. on Distributed Computing Systems, </booktitle> <address> Arlington, TX, </address> <month> May, </month> <year> 1991, </year> <pages> pp. 2-9. </pages>
Reference-contexts: We hope to extend these techniques to informed prefetching. Many researchers have looked into prefetching based on access patterns inferred from the stream of user I/O requests <ref> [Curewitz93, Kotz91, Tait91, Palmer91, Kor- ner90] </ref>. Kotz looked at intelligent prefetching for MIMD multiprocessors with scientific workloads. He extended the applicability of readahead to non-sequential, but regular, accesses within one file by predicting future accesses based on previously observed access patterns.
Reference: [Trivedi79] <author> Trivedi, </author> <title> K.S., An Analysis of Prepaging, </title> <journal> Computing, </journal> <volume> V 22 (3), </volume> <year> 1979, </year> <pages> pp. 191-210. </pages>
Reference-contexts: For example, Trivedi suggested using programmer or compiler generated hints for prepaging <ref> [Trivedi79] </ref>. Hints are now widely enough understood that they appear in various existing implementations. For example, Sun Microsystems operating system provides two advise system calls that instruct the virtual memory systems policy decisions [Sun88].
References-found: 36

