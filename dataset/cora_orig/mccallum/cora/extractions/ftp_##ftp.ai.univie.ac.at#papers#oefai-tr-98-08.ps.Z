URL: ftp://ftp.ai.univie.ac.at/papers/oefai-tr-98-08.ps.Z
Refering-URL: http://www.ai.univie.ac.at/cgi-bin/tr-online/?number+98-08
Root-URL: 
Phone: 2  3  
Title: Predicting Ordinal Classes in ILP  
Author: Gerhard Widmer ; Stefan Kramer Bernhard Pfahringer and Michael de Groeve 
Address: Vienna, Freyung 6/2, A-1010 Vienna  Schottengasse 3, A-1010 Vienna, Austria  Celestijnenlaan 200A, B-3001 Heverlee, Belgium  
Affiliation: 1 Department of Medical Cybernetics and Artificial Intelligence, University of  Austrian Research Institute for Artificial Intelligence,  Department of Computer Science, Katholieke Universiteit Leuven,  
Abstract: This paper is devoted to the problem of learning to predict ordinal (i.e., ordered discrete) classes in an ILP setting. We start with a relational regression algorithm named SRT (Structural Regression Trees) and study various ways of transforming it into a first-order learner for ordinal classification tasks. Combinations of these algorithm variants with several data preprocessing methods are compared on two ILP benchmark data sets to verify the relative strengths and weaknesses of the strategies and to study the trade-off between optimal categorical classification accuracy (hit rate) and minimum distance-based error. Preliminary results indicate that this is a promising avenue towards algorithms that combine aspects of classification and regression in relational learning.
Abstract-found: 1
Intro-found: 1
Reference: [Breiman et al., 1984] <author> Breiman, L., Friedman, J.H., Olshen, R.A., & Stone, C.J. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <address> Belmont, CA: </address> <publisher> Wadsworth International Group. </publisher> <pages> 10 </pages>
Reference-contexts: The algorithm constructs a tree containing a positive literal (an atomic formula) or a conjunction of literals in each node, and assigns a numerical value to each leaf. Since the original publication, SRT has been turned into a full-fledged relational version of CART <ref> [Breiman et al., 1984] </ref>. After the tree growing phase, the tree is pruned using so-called error-complexity pruning for regression or cost-complexity pruning for classification [Breiman et al., 1984]. These types of pruning are based on a separate "prune set" of examples. <p> Since the original publication, SRT has been turned into a full-fledged relational version of CART <ref> [Breiman et al., 1984] </ref>. After the tree growing phase, the tree is pruned using so-called error-complexity pruning for regression or cost-complexity pruning for classification [Breiman et al., 1984]. These types of pruning are based on a separate "prune set" of examples. For the construction of a tree, SRT follows the general procedure of top-down decision tree induction algorithms [Quinlan, 1993]. <p> SRTClassif chooses the most frequent class in a node as center value and uses the Gini index of diversity <ref> [Breiman et al., 1984] </ref> as evaluation measure; it does not pay attention to the distance between classes. <p> Finally, although our algorithms are based on a first-order learning algorithm and were tested in relational domains, the general methods should be valuable for propositional regression algorithms like CART <ref> [Breiman et al., 1984] </ref> or M5 [Quinlan, 1992] as well. Acknowledgments This research is part of the project "Carcinogenicity Detection by Machine Learning", supported by the Austrian Federal Ministry of Science and Transport. Michael de Groeve was supported by a SOCRATES (ERASMUS) grant.
Reference: [Dolsak & Muggleton, 1992] <author> Dolsak, B. & Muggleton, S. </author> <year> (1992). </year> <title> The Application of Inductive Logic Programming to Finite-Element Mesh Design. </title> <editor> In S. Muggleton (ed.), </editor> <booktitle> Inductive Logic Programming. </booktitle> <publisher> Academic Press. </publisher>
Reference-contexts: The problem consists in predicting the optimal granularity of a finite element (FE) model of a given physical structure. More precisely, the task is to predict the appropriate number of FEs along a given edge. In the widely used data set first described in <ref> [Dolsak & Muggleton, 1992] </ref>, there are 13 classes (1, : : :, 12 and 17 FEs). It seems obvious that in this domain, classification error should be regarded as a gradual phenomenon. <p> The first is the well-known Mesh Design data set <ref> [Dolsak & Muggleton, 1992] </ref>. In the original version of this data set, there are 278 instances in 13 classes (1 FEs per edge, 2FEs, : : :, 12 FEs, 17FEs).
Reference: [Dolsak et al., 1998] <author> Dolsak, B., Bratko, I., & Jezernik, A. </author> <year> (1998). </year> <title> Application of Machine Learning in Finite Element Computation. In R.S. </title> <editor> Michalski, I. Bratko & M. Kubat (eds.), </editor> <booktitle> Machine Learning and Data Mining: Methods and Applications. </booktitle> <address> Chich-ester, UK: </address> <publisher> Wiley. </publisher>
Reference-contexts: However, in almost all experiments with this data set described in the literature, classification learners (mostly first-order) were applied and only classification accuracies (as percentages of correct predictions) were reported. A summary of the results reported by various researchers is given in <ref> [Dolsak et al., 1998] </ref>. Current machine learning research that seems most relevant to the problem of predicting ordinal classes is work on cost-sensitive learning. In the domain of propositional learning, some induction algorithms have been proposed that can take into account matrices of misclassification costs (e.g., [Schiffers, 1997,Turney, 1995].
Reference: [Dzeroski, personal communication] <author> Dzeroski, S. </author> <title> Biodegradability data set. </title> <type> Personal communication. </type>
Reference-contexts: The second dataset, which we will call biodegradability data set in the following, describes 62 chemical substances in the familiar `atoms and bonds' representation [Srinivasan et al., 1995]. The task is to predict the half-rate of surface water aerobic aqueous biodegradation in hours <ref> [Dzeroski, personal communication] </ref>. For previous experiments, we had already discretized this quantity and mapped it to the four classes fast, moderate, slow, and resistant, represented as 1, 2, 3, and 4.
Reference: [Dzeroski & Bratko, 1992] <author> Dzeroski, S. and Bratko, I. </author> <year> (1992). </year> <title> Handling noise in Inductive Logic Programming. </title> <booktitle> In Proceedings ILP-92, </booktitle> <address> Tokyo, Japan. </address>
Reference: [Furnkranz, 1994] <author> Furnkranz, J. </author> <year> (1994). </year> <title> Fossil: A robust relational learner". </title> <booktitle> In Proceedings of the 7th European Conference on Machine Learning (ECML-94). </booktitle> <address> Berlin: </address> <publisher> Springer Verlag. </publisher>
Reference-contexts: 1 Introduction Learning to predict discrete classes from preclassified examples has long been, and continues to be, a central research topic in Inductive Logic Programming (e.g., <ref> [Quinlan, 1990, Pazzani & Kibler, 1992, Muggleton, 1995, Furnkranz, 1994] </ref>). Recently, there has also been increased interest in relational regression, i.e., the task of learning to predict continuous numeric variables from relational data (e.g., [Karalic, 1995,Karalic & Bratko, 1997,Kramer, 1996]).
Reference: [Karalic, 1995] <author> Karalic, A. </author> <year> (1995). </year> <title> First Order Regression. </title> <type> Ph.D. Thesis, </type> <institution> University of Ljubljana. </institution>
Reference: [Karalic & Bratko, 1997] <author> Karalic, A. & Bratko, I. </author> <year> (1997). </year> <title> First Order Regression. </title> <booktitle> Machine Learning 26(2/3), </booktitle> <pages> 147-177. </pages>
Reference: [Kramer, 1996] <author> Kramer, S. </author> <year> (1996). </year> <title> Structural Regression Trees. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI-96). </booktitle> <address> Cambridge, MA: </address> <publisher> AAAI Press/MIT Press. </publisher>
Reference-contexts: This seems a natural strategy because regression algorithms by definition have a notion of relative distance of target values, while classification algorithms usually do not. More precisely, we start with the algorithm SRT (Structural Regression Trees <ref> [Kramer, 1996] </ref> and study several modifications of the basic algorithm that turn it into a distance-sensitive classification learner. <p> of these algorithm variants with several pre-processing methods are compared on two data sets to verify the relative strengths and weaknesses of the strategies and to study the trade-off between optimal categorical classification accuracy (hit rate) and minimum distance-based error. 2 Relational Regression: The SRT Algorithm Structural Regression Trees (SRT) <ref> [Kramer, 1996] </ref> is an algorithm that learns a first-order theory for the prediction of numerical values from examples and relational background knowledge. The algorithm constructs a tree containing a positive literal (an atomic formula) or a conjunction of literals in each node, and assigns a numerical value to each leaf.
Reference: [Mathieson, 1996] <author> Mathieson, M. </author> <year> (1996). </year> <title> Ordered Classes and Incomplete Examples in Classification. </title> <editor> In M. Mozer et al. (eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 9. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Such cost matrices can be used to express relative distances between classes. In the area of statistics, there are methods directly relevant to our problem (e.g., Ordinal Logistic Regression [McCullagh, 1980]); some of these have also been studied in the field of neural networks (e.g., <ref> [Mathieson, 1996] </ref>). However, our goal is to develop induction algorithms that (1) produce interpretable, symbolic models and (2) are applicable to relational, first-order learning tasks like the mesh design problem.
Reference: [McCullagh, 1980] <author> McCullagh, P. </author> <year> (1980). </year> <title> Regression Models for Ordinal Data. </title> <journal> Journal of the Royal Statistical Society Series B 42, </journal> <pages> 109-142. </pages>
Reference-contexts: Such cost matrices can be used to express relative distances between classes. In the area of statistics, there are methods directly relevant to our problem (e.g., Ordinal Logistic Regression <ref> [McCullagh, 1980] </ref>); some of these have also been studied in the field of neural networks (e.g., [Mathieson, 1996]). However, our goal is to develop induction algorithms that (1) produce interpretable, symbolic models and (2) are applicable to relational, first-order learning tasks like the mesh design problem.
Reference: [Mosteller & Tukey, 1977] <author> Mosteller, F. & Tukey, J.W. </author> <year> (1977). </year> <title> Data Analysis and Regression A Second Course in Statistics. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: The basic idea underlying different data transformations is that numbers may represent fundamentally different types of measurements. <ref> [Mosteller & Tukey, 1977] </ref> distinguish, among others, the broad classes of amounts and counts (which cannot be negative), ranks (e.g., 1 = smallest, 2 = next-to-smallest, : : :), and grades (ordered labels, as in A, B, C, D, E).
Reference: [Muggleton, 1995] <author> Muggleton, S. </author> <year> (1995). </year> <title> Inverse Entailment and Progol. </title> <booktitle> New Generation Computing 13, </booktitle> <pages> 245-286. </pages>
Reference-contexts: 1 Introduction Learning to predict discrete classes from preclassified examples has long been, and continues to be, a central research topic in Inductive Logic Programming (e.g., <ref> [Quinlan, 1990, Pazzani & Kibler, 1992, Muggleton, 1995, Furnkranz, 1994] </ref>). Recently, there has also been increased interest in relational regression, i.e., the task of learning to predict continuous numeric variables from relational data (e.g., [Karalic, 1995,Karalic & Bratko, 1997,Kramer, 1996]).
Reference: [Pazzani & Kibler, 1992] <author> Pazzani, M. & Kibler, D. </author> <title> (192). The Utility of Knowledge in Inductive Learning. </title> <booktitle> Machine Learning 9(1), </booktitle> <pages> 57-94. </pages>
Reference-contexts: 1 Introduction Learning to predict discrete classes from preclassified examples has long been, and continues to be, a central research topic in Inductive Logic Programming (e.g., <ref> [Quinlan, 1990, Pazzani & Kibler, 1992, Muggleton, 1995, Furnkranz, 1994] </ref>). Recently, there has also been increased interest in relational regression, i.e., the task of learning to predict continuous numeric variables from relational data (e.g., [Karalic, 1995,Karalic & Bratko, 1997,Kramer, 1996]).
Reference: [Quinlan, 1990] <author> Quinlan, J.R. </author> <year> (1990). </year> <title> Learning logical definitions from relations. </title> <booktitle> Machine Learning 5, </booktitle> <pages> 239-266. </pages>
Reference-contexts: 1 Introduction Learning to predict discrete classes from preclassified examples has long been, and continues to be, a central research topic in Inductive Logic Programming (e.g., <ref> [Quinlan, 1990, Pazzani & Kibler, 1992, Muggleton, 1995, Furnkranz, 1994] </ref>). Recently, there has also been increased interest in relational regression, i.e., the task of learning to predict continuous numeric variables from relational data (e.g., [Karalic, 1995,Karalic & Bratko, 1997,Kramer, 1996]).
Reference: [Quinlan, 1992] <author> Quinlan, J.R. </author> <year> (1992). </year> <title> Learning with Continuous Classes. </title> <booktitle> In Proceedings AI'92. </booktitle> <address> Singapore: </address> <publisher> World Scientific. </publisher>
Reference-contexts: Finally, although our algorithms are based on a first-order learning algorithm and were tested in relational domains, the general methods should be valuable for propositional regression algorithms like CART [Breiman et al., 1984] or M5 <ref> [Quinlan, 1992] </ref> as well. Acknowledgments This research is part of the project "Carcinogenicity Detection by Machine Learning", supported by the Austrian Federal Ministry of Science and Transport. Michael de Groeve was supported by a SOCRATES (ERASMUS) grant. Thanks to Saso Dzeroski for providing the biodegradability dataset.
Reference: [Quinlan, 1993] <author> Quinlan, J.R. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Ma-teo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: These types of pruning are based on a separate "prune set" of examples. For the construction of a tree, SRT follows the general procedure of top-down decision tree induction algorithms <ref> [Quinlan, 1993] </ref>. It recursively builds a binary 3 tree, selecting a positive literal or a conjunction of literals (as defined by user-defined schemata [Silverstein & Pazzani, 1991]) in each node of the tree until a stopping criterion is fulfilled.
Reference: [Schiffers, 1997] <author> Schiffers, J. </author> <year> (1997). </year> <title> A Classification Approach Incorporating Misclas-sification Costs. Intelligent Data Analysis 1(1). </title>
Reference: [Silverstein & Pazzani, 1991] <author> Silverstein, G. & Pazzani, M.J. </author> <year> (1991). </year> <title> Relational Cliches: Constraining Constructive Induction During Relational Learning. </title> <booktitle> In Proceedings of the 8th International Workshop on Machine Learning (ML-91). </booktitle> <address> San Ma-teo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: For the construction of a tree, SRT follows the general procedure of top-down decision tree induction algorithms [Quinlan, 1993]. It recursively builds a binary 3 tree, selecting a positive literal or a conjunction of literals (as defined by user-defined schemata <ref> [Silverstein & Pazzani, 1991] </ref>) in each node of the tree until a stopping criterion is fulfilled. The algorithm keeps track of the examples in each node and the positive literals or conjunctions of literals in each path leading to the respective nodes.
Reference: [Srinivasan et al., 1995] <author> Srinivasan, A., Muggleton, S., and King, R.D. </author> <year> (1995). </year> <title> Comparing the use of background knowledge by Inductive Logic Programming systems. </title> <booktitle> In Proceedings ILP-95, </booktitle> <institution> Katholieke Universiteit Leuven, Belgium. </institution>
Reference-contexts: The second dataset, which we will call biodegradability data set in the following, describes 62 chemical substances in the familiar `atoms and bonds' representation <ref> [Srinivasan et al., 1995] </ref>. The task is to predict the half-rate of surface water aerobic aqueous biodegradation in hours [Dzeroski, personal communication]. For previous experiments, we had already discretized this quantity and mapped it to the four classes fast, moderate, slow, and resistant, represented as 1, 2, 3, and 4.
Reference: [Turney, 1995] <author> Turney, P.D. </author> <year> (1995). </year> <title> Cost-Sensitive Classification: Empirical Evaluation of a Hybrid Genetic Decision Tree Induction Algorithm. </title> <journal> Journal of Artificial Intelligence Research 2, </journal> <pages> 369-409. </pages>
References-found: 21

