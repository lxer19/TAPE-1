URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/usr/skoenig/www/sven/papers/book97.ps.gz
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/usr/skoenig/www/sven/abstracts/book97.html
Root-URL: 
Email: fskoenig, reidsg@cs.cmu.edu  
Title: Xavier: A Robot Navigation Architecture Based on Partially Observable Markov Decision Process Models  
Author: Sven Koenig and Reid G. Simmons 
Address: Pittsburgh, PA 15213-3890  
Affiliation: Carnegie Mellon University School of Computer Science  
Abstract: Autonomous mobile robots need very reliable navigation capabilities in order to operate unattended for long periods of time. We present a technique for achieving this goal that uses partially observable Markov decision process models (POMDPs) to explicitly model navigation uncertainty, including actuator and sensor uncertainty and approximate knowledge of the environment. This allows the robot to maintain a probability distribution over its current pose. Thus, while the robot rarely knows exactly where it is, it always has some belief as to what its true pose is, and is never completely lost. We present a navigation architecture based on POMDPs that provides a uniform framework with an established theoretical foundation for pose estimation, path planning, robot control during navigation, and learning. Our experiments show that this architecture indeed leads to robust corridor navigation for an actual indoor mobile robot.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <year> 1957. </year>
Reference-contexts: Such an optimal policy can be determined by solving the following system of jSj equations for the variables v (s), that is known as Bellman's Equation <ref> [1] </ref>: v (s) = max [r (s; a) + fl s 0 2S v (s) is the expected total reward if the process starts in state s and the decision maker acts optimally. <p> The optimal action to execute in state s is a (s) = arg max a2A (s) [r (s; a) + fl P The system of equations can be solved in polynomial time using dynamic programming methods [20]. A popular dynamic programming method is value iteration <ref> [1] </ref> (we leave the termination criterion unspecified): 1. Set v 1 (s) := 0 for all s 2 S. Set t := 1. P 3. Go to 2. Then, for all s 2 S, v (s) = lim t!1 v t (s).
Reference: [2] <author> A. Cassandra, L. Kaelbling, and J. Kurien. </author> <title> Acting under uncertainty: Discrete Bayesian models for mobile robot navigation. </title> <booktitle> In Proceedings of the International Conference on Intelligent Robots and Systems (IROS), </booktitle> <pages> pages 963-972, </pages> <year> 1996. </year>
Reference-contexts: Nourbakhsh et. al. [23] use Markov models that do not assume that the location of the robot is known with certainty, but do not utilize any metric information (the states of the robot are either at a topological node or somewhere in a connecting corridor). Cassandra et. al. <ref> [2] </ref> build on our work, and consequently use Markov models similar to ours, including modeling distance information, but assume that the distances are known with certainty. 3. <p> The experiments demonstrate that for the office navigation problems considered here, the efficient voting strategy performs very well. For an empirical comparison of several greedy policy generation and directive selection strategies in more complex environments, but using simpler POMDPs than we use here, see <ref> [2] </ref>. 5.1. Experiments with the Robot The robot experiments were performed on Xavier (Figure 10).
Reference: [3] <author> A. Cassandra, L. Kaelbling, and M. Littman. </author> <title> Acting optimally in partially observable stochastic domains. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence (AAAI), </booktitle> <pages> pages 1023-1028, </pages> <year> 1994. </year>
Reference-contexts: In Artificial Intelligence and Robotics, POMDPs have been applied to speech and handwriting recognition [11] and the interpretation of tele-operation commands [10, 37]. They have also gained popularity in the Artificial Intelligence community as a formal model for planning under uncertainty <ref> [3, 12] </ref>. Consequently, standard algorithms are available to solve tasks that are typically encountered by observers and decision makers. In the following, we describe some of these algorithms. 3.1. State Estimation: Determining the Current State Assume that an observer wants to determine the current state of a POMDP process.
Reference: [4] <author> L. Chrisman. </author> <title> Reinforcement learning with perceptual aliasing: The perceptual distinctions approach. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence (AAAI), </booktitle> <pages> pages 183-188, </pages> <year> 1992. </year>
Reference-contexts: is assigned to the most likely state, that is, the action a (arg max s2S ff (s)). * The "Voting" Strategy [31] executes the action with the highest probability mass according to ff, that is, the action arg max a2A P * The "Completely Observable after the First Step" Strategy <ref> [4, 35] </ref> executes the action arg max a2A s2S [ff (s)(r (s; a)+fl s 0 2S [p (s 0 js; a)v (s 0 )])]. <p> This POMDP is the one that best fits the empirical data. In our application, this corresponds to fine-tuning the navigation model from experience, including the map, actuator, and sensor models. While there is no known technique for doing this efficiently, there exist efficient algorithms that approximate the optimal POMDP <ref> [4, 22, 34] </ref>. The Baum-Welch algorithm [26] is one such algorithm. This iterative expectation-maximization algorithm does not require control of the POMDP process and thus can be used by an observer to learn the POMDP.
Reference: [5] <author> T. Dean, K. Basye, R. Chekaluk, S. Hyun, M. Lejter, and M. Randazza. </author> <title> Coping with uncertainty in a control system for navigation and exploration. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence (AAAI), </booktitle> <pages> pages 1010-1015, </pages> <year> 1990. </year>
Reference-contexts: Previously reported approaches that maintain pose distributions often use either Kalman filters [16, 32] or temporal Bayesian networks <ref> [5] </ref>. Both approaches can utilize motion and sensor reports to update the pose distribution. Kalman filters model only restricted pose distributions in continuous pose space. In the simplest case, these are Gaussians. While Gaussians are efficient to encode and update, they are not ideally suited for office navigation.
Reference: [6] <author> T. Dean, L. Kaelbling, J. Kirman, and A. Nicholson. </author> <title> Planning with deadlines in stochastic domains. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence (AAAI), </booktitle> <pages> pages 574-579, </pages> <year> 1993. </year>
Reference-contexts: There have been several other approaches that use Markov models for robot navigation: Dean et. al. <ref> [6] </ref> use Markov models, but, different from our approach, assume that the location of the robot is always known precisely.
Reference: [7] <author> P. Devijver. </author> <title> Baum's forward backward algorithm revisited. </title> <journal> Pattern Recognition Letters, </journal> <volume> 3 </volume> <pages> 369-373, </pages> <year> 1985. </year>
Reference-contexts: The Baum-Welch algorithm estimates the improved POMDP in three steps. First Step: A dynamic programming approach ("forward-backward algorithm") is used that applies Bayes rule repeatedly <ref> [7] </ref>. The forward phase calculates scaling factors scale t and alpha values ff t (s) = p (s t = sjo 1:::t ; a 1:::t1 ) for all s 2 S and t = 1 : : : T .
Reference: [8] <author> A. Elfes. </author> <title> Using occupancy grids for mobile robot perception and navigation. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 46-57, </pages> <month> 6 </month> <year> 1989. </year>
Reference-contexts: Table 1 lists the sensors that we currently use, together with the features that they report on. The sensor reports are derived from the raw sensor data by using a small occupancy grid <ref> [8] </ref> in the coordinates of the robot that is centered around the robot (Figure 3). The occupancy grid combines the raw data from all sonar sensors and integrates them over the recent past.
Reference: [9] <author> K. Haigh and M. Veloso. </author> <title> Interleaving planning and robot execution for asynchronous user requests. </title> <booktitle> In Proceedings of the International Conference on Intelligent Robots and Systems (IROS), </booktitle> <pages> pages 148-155, </pages> <year> 1996. </year>
Reference-contexts: smoothly in a goal direction while avoiding static and dynamic obstacles [29], a path planning layer that reasons about uncertainty to choose paths that have high expected utility [13], and a multiple-task planning layer that uses PRODIGY, a symbolic, non-linear planner, to integrate and schedule delivery requests that arrive asynchronously <ref> [9] </ref>. The layers, which are implemented as a number of distributed, concurrent processes operating on several processors, are integrated using the Task Control Architecture. The Task Control Architecture provides facilities for interprocess communication, task decomposition and sequencing, execution monitoring and exception handling, and resource management [28].
Reference: [10] <author> B. Hannaford and P. Lee. </author> <title> Hidden Markov model analysis of force/torque information in telema-nipulation. </title> <journal> The International Journal of Robotics Research, </journal> <volume> 10(5) </volume> <pages> 528-539, </pages> <year> 1991. </year> <month> 22 </month>
Reference-contexts: Properties of POMDPs have been studied extensively in Operations Research. In Artificial Intelligence and Robotics, POMDPs have been applied to speech and handwriting recognition [11] and the interpretation of tele-operation commands <ref> [10, 37] </ref>. They have also gained popularity in the Artificial Intelligence community as a formal model for planning under uncertainty [3, 12]. Consequently, standard algorithms are available to solve tasks that are typically encountered by observers and decision makers. In the following, we describe some of these algorithms. 3.1.
Reference: [11] <author> X. Huang, Y. Ariki, and M. Jack. </author> <title> Hidden Markov models for speech recognition. </title> <publisher> Edinburgh University Press, </publisher> <year> 1990. </year>
Reference-contexts: Properties of POMDPs have been studied extensively in Operations Research. In Artificial Intelligence and Robotics, POMDPs have been applied to speech and handwriting recognition <ref> [11] </ref> and the interpretation of tele-operation commands [10, 37]. They have also gained popularity in the Artificial Intelligence community as a formal model for planning under uncertainty [3, 12]. Consequently, standard algorithms are available to solve tasks that are typically encountered by observers and decision makers.
Reference: [12] <author> S. Koenig. </author> <title> Optimal probabilistic and decision-theoretic planning using Markovian decision theory. </title> <type> Master's thesis, </type> <institution> Computer Science Department, University of California at Berkeley, Berkeley (California), </institution> <year> 1991. </year> <note> (Available as Technical Report UCB/CSD 92/685). </note>
Reference-contexts: In Artificial Intelligence and Robotics, POMDPs have been applied to speech and handwriting recognition [11] and the interpretation of tele-operation commands [10, 37]. They have also gained popularity in the Artificial Intelligence community as a formal model for planning under uncertainty <ref> [3, 12] </ref>. Consequently, standard algorithms are available to solve tasks that are typically encountered by observers and decision makers. In the following, we describe some of these algorithms. 3.1. State Estimation: Determining the Current State Assume that an observer wants to determine the current state of a POMDP process.
Reference: [13] <author> S. Koenig, R. Goodwin, and R. Simmons. </author> <title> Robot navigation with Markov models: A framework for path planning and learning with limited computational resources. </title> <editor> In L. Dorst, M. van Lambalgen, and R. Voorbraak, editors, </editor> <booktitle> Reasoning with Uncertainty in Robotics, volume 1093 of Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 322-337. </pages> <publisher> Springer, </publisher> <year> 1996. </year>
Reference-contexts: the system include a servo-control layer that controls the motors of the robot, an obstacle avoidance layer that keeps the robot moving smoothly in a goal direction while avoiding static and dynamic obstacles [29], a path planning layer that reasons about uncertainty to choose paths that have high expected utility <ref> [13] </ref>, and a multiple-task planning layer that uses PRODIGY, a symbolic, non-linear planner, to integrate and schedule delivery requests that arrive asynchronously [9]. The layers, which are implemented as a number of distributed, concurrent processes operating on several processors, are integrated using the Task Control Architecture. <p> It uses a generate, evaluate, and refine strategy to determine a path in the topological map that minimizes the expected travel time of the robot <ref> [13] </ref>. The navigation layer then converts this path to a complete policy. For states corresponding to a topological node on the path, directives are assigned to head the robot towards the next node (except for the states corresponding to the last node on the path, which are assigned "stop" directives).
Reference: [14] <author> S. Koenig and R. Simmons. </author> <title> Passive distance learning for robot navigation. </title> <booktitle> In Proceedings of the International Conference on Machine Learning (ICML), </booktitle> <pages> pages 266-274, </pages> <year> 1996. </year>
Reference-contexts: In addition, we have extended the Baum Welch algorithm to address the issues of limited memory and the cost of collecting training data [15] and augmented it so that it is able to change the structure of the POMDP <ref> [14] </ref>. 3.4. Most Likely Path: Determining the State Sequence from Observations Assume that an observer wants to determine the most likely sequence of states that the POMDP process was in. This corresponds to determining the path that the robot most likely took to get to its destination. <p> Then, we describe the POMDP and the POMDP compilation component in detail. Finally, we explain how the POMDP is used by the pose estimation, policy generation, and directive selection components. The model learning component is described in <ref> [14, 15] </ref>. 4.1. Interface to the Obstacle Avoidance Layer An advantage of our layered robot system is that the navigation layer is insulated from many details of the actuators, sensors, and the environment (such as stationary and moving obstacles).
Reference: [15] <author> S. Koenig and R. Simmons. </author> <title> Unsupervised learning of probabilistic models for robot navigation. </title> <booktitle> In Proceedings of the International Conference on Robotics and Automation (ICRA), </booktitle> <pages> pages 2301-2308, </pages> <year> 1996. </year>
Reference-contexts: P P A10. Set p (ojs) := t=1:::T jo t =o fl t (s)= t=1:::T fl t (s) for all s 2 S and all o 2 O. To apply the Baum-Welch algorithm to real-world problems, there exist standard techniques for dealing with the following issues <ref> [15] </ref>: when to stop iterating the algorithm, with which initial POMDP to start the algorithm and how often to apply it to different initial POMDPs, how to handle transition or observation probabilities that are zero or one (the Baum-Welch algorithm does not change these probabilities), and how to deal with short <p> In addition, we have extended the Baum Welch algorithm to address the issues of limited memory and the cost of collecting training data <ref> [15] </ref> and augmented it so that it is able to change the structure of the POMDP [14]. 3.4. Most Likely Path: Determining the State Sequence from Observations Assume that an observer wants to determine the most likely sequence of states that the POMDP process was in. <p> Then, we describe the POMDP and the POMDP compilation component in detail. Finally, we explain how the POMDP is used by the pose estimation, policy generation, and directive selection components. The model learning component is described in <ref> [14, 15] </ref>. 4.1. Interface to the Obstacle Avoidance Layer An advantage of our layered robot system is that the navigation layer is insulated from many details of the actuators, sensors, and the environment (such as stationary and moving obstacles).
Reference: [16] <author> A. Kosaka and A. Kak. </author> <title> Fast vision-guided mobile robot navigation using model-based reasoning and prediction of uncertainties. </title> <booktitle> In Proceedings of the International Conference on Intelligent Robots and Systems (IROS), </booktitle> <pages> pages 2177-2186, </pages> <year> 1992. </year>
Reference-contexts: Previously reported approaches that maintain pose distributions often use either Kalman filters <ref> [16, 32] </ref> or temporal Bayesian networks [5]. Both approaches can utilize motion and sensor reports to update the pose distribution. Kalman filters model only restricted pose distributions in continuous pose space. In the simplest case, these are Gaussians.
Reference: [17] <author> B. Kuipers and Y.-T. Byun. </author> <title> A robust, qualitative method for robot spatial learning. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence (AAAI), </booktitle> <pages> pages 774-779, </pages> <year> 1988. </year>
Reference-contexts: While some landmark-based approaches use motion reports, mostly to resolve topological ambiguities, and some metric-based approaches use sensor reports to continuously realign the robot with the map <ref> [17, 21] </ref>, the two sources of information are treated differently. We want an approach that seamlessly integrates both sources of information, and is amenable to adding new sources such as a-priori information about which doorways are likely to be open or closed.
Reference: [18] <author> M. Littman. </author> <title> Algorithms for Sequential Decision Making. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Brown University, </institution> <address> Providence (Rhode Island), </address> <year> 1996. </year> <note> (Available as Technical Report CS-96-09). </note>
Reference-contexts: The SPOVA-RL algorithm [25], for example, can determine approximate policies for POMDPs with about a hundred states in a reasonable amount of time. Further performance improvements are anticipated, since POMDP planning algorithms are the object of current research <ref> [18] </ref> and researchers are starting to investigate, for example, how to exploit the restricted topology of some POMDPs. We describe here three greedy POMDP planning approaches that can find policies for large POMDPs fast, but still yield reasonable robot navigation behavior.
Reference: [19] <author> M. Littman, A. Cassandra, and L. Kaelbling. </author> <title> Learning policies for partially observable environments: Scaling up. </title> <booktitle> In Proceedings of the International Conference on Machine Learning (ICML), </booktitle> <pages> pages 362-370, </pages> <year> 1995. </year>
Reference-contexts: In fact, the POMDP planning problem is PSPACE-complete in general [?]. However, there are POMDP planning algorithms that trade off solution quality for speed, but that usually do not provide quality guarantees <ref> [19, 25] </ref>. The SPOVA-RL algorithm [25], for example, can determine approximate policies for POMDPs with about a hundred states in a reasonable amount of time.
Reference: [20] <author> M. Littman, T. Dean, and L. Kaelbling. </author> <title> On the complexity of solving Markov decision problems. </title> <booktitle> In Proceedings of the Annual Conference on Uncertainty in Artificial Intelligence (UAI), </booktitle> <pages> pages 394-402, </pages> <year> 1995. </year>
Reference-contexts: The optimal action to execute in state s is a (s) = arg max a2A (s) [r (s; a) + fl P The system of equations can be solved in polynomial time using dynamic programming methods <ref> [20] </ref>. A popular dynamic programming method is value iteration [1] (we leave the termination criterion unspecified): 1. Set v 1 (s) := 0 for all s 2 S. Set t := 1. P 3. Go to 2.
Reference: [21] <author> M. Mataric. </author> <title> Environment learning using a distributed representation. </title> <booktitle> In Proceedings of the International Conference on Robotics and Automation (ICRA), </booktitle> <pages> pages 402-406, </pages> <year> 1990. </year>
Reference-contexts: While some landmark-based approaches use motion reports, mostly to resolve topological ambiguities, and some metric-based approaches use sensor reports to continuously realign the robot with the map <ref> [17, 21] </ref>, the two sources of information are treated differently. We want an approach that seamlessly integrates both sources of information, and is amenable to adding new sources such as a-priori information about which doorways are likely to be open or closed.
Reference: [22] <author> A. McCallum. </author> <title> Instance-based state identification for reinforcement learning. </title> <booktitle> In Advances in Neural Information Processing Systems 7, </booktitle> <pages> pages 377-384, </pages> <year> 1995. </year>
Reference-contexts: This POMDP is the one that best fits the empirical data. In our application, this corresponds to fine-tuning the navigation model from experience, including the map, actuator, and sensor models. While there is no known technique for doing this efficiently, there exist efficient algorithms that approximate the optimal POMDP <ref> [4, 22, 34] </ref>. The Baum-Welch algorithm [26] is one such algorithm. This iterative expectation-maximization algorithm does not require control of the POMDP process and thus can be used by an observer to learn the POMDP.
Reference: [23] <author> I. Nourbakhsh, R. Powers, and S. Birchfield. Dervish: </author> <title> An office-navigating robot. </title> <journal> AI Magazine, </journal> <volume> 16(2) </volume> <pages> 53-60, </pages> <year> 1995. </year>
Reference-contexts: There have been several other approaches that use Markov models for robot navigation: Dean et. al. [6] use Markov models, but, different from our approach, assume that the location of the robot is always known precisely. Nourbakhsh et. al. <ref> [23] </ref> use Markov models that do not assume that the location of the robot is known with certainty, but do not utilize any metric information (the states of the robot are either at a topological node or somewhere in a connecting corridor). <p> We call this transformation "completing" the policy because the mapping from states to actions is completed to a mapping from state distributions to actions. Given the current state distribution ff, the approaches greedily complete the policy as follows: * The "Most Likely State" Strategy <ref> [23] </ref> executes the action that is assigned to the most likely state, that is, the action a (arg max s2S ff (s)). * The "Voting" Strategy [31] executes the action with the highest probability mass according to ff, that is, the action arg max a2A P * The "Completely Observable after
Reference: [24] <author> C. Papadimitriou and J. Tsitsiklis. </author> <title> The complexity of Markov decision processes. </title> <journal> Mathematics of Operations Research, </journal> <volume> 12(3) </volume> <pages> 441-450, </pages> <year> 1987. </year>
Reference: [25] <author> R. Parr and S. Russell. </author> <title> Approximating optimal policies for partially observable stochastic domains. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), </booktitle> <pages> pages 1088-1094, </pages> <year> 1995. </year>
Reference-contexts: In fact, the POMDP planning problem is PSPACE-complete in general [?]. However, there are POMDP planning algorithms that trade off solution quality for speed, but that usually do not provide quality guarantees <ref> [19, 25] </ref>. The SPOVA-RL algorithm [25], for example, can determine approximate policies for POMDPs with about a hundred states in a reasonable amount of time. <p> In fact, the POMDP planning problem is PSPACE-complete in general [?]. However, there are POMDP planning algorithms that trade off solution quality for speed, but that usually do not provide quality guarantees [19, 25]. The SPOVA-RL algorithm <ref> [25] </ref>, for example, can determine approximate policies for POMDPs with about a hundred states in a reasonable amount of time.
Reference: [26] <author> L. Rabiner. </author> <title> An introduction to hidden Markov models. </title> <journal> IEEE ASSP Magazine, </journal> <pages> pages 4-16, </pages> <month> 1 </month> <year> 1986. </year>
Reference-contexts: In our application, this corresponds to fine-tuning the navigation model from experience, including the map, actuator, and sensor models. While there is no known technique for doing this efficiently, there exist efficient algorithms that approximate the optimal POMDP [4, 22, 34]. The Baum-Welch algorithm <ref> [26] </ref> is one such algorithm. This iterative expectation-maximization algorithm does not require control of the POMDP process and thus can be used by an observer to learn the POMDP.
Reference: [27] <author> R. Simmons. </author> <title> Becoming increasingly reliable. </title> <booktitle> In Proceedings of the International Conference on Artificial Intelligence Planning Systems (AIPS), </booktitle> <pages> pages 152-157, </pages> <year> 1994. </year> <month> 23 </month>
Reference-contexts: Our experiments show that the architecture leads to robust long-term autonomous navigation in office environments (with corridors, foyers, and rooms) for an actual indoor mobile robot, significantly outperforming the landmark-based navigation technique that we used previously <ref> [27] </ref>. The POMDP-based navigation architecture uses a compiler that automatically produces POMDPs from topological maps, actuator and sensor models, and uncertain knowledge of the environment. <p> This success rate of 93 percent compares favorably with the 80 percent success rate that we obtained when using a landmark-based navigation technique in place of the POMDP-based navigation layer on the same robot with an otherwise unchanged robot system <ref> [27] </ref>. Thus, the difference in performance can be directly attributed to the different navigation techniques. 5.2.
Reference: [28] <author> R. Simmons. </author> <title> Structured control for autonomous robots. </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> 10(1) </volume> <pages> 34-43, </pages> <year> 1994. </year>
Reference-contexts: The layers, which are implemented as a number of distributed, concurrent processes operating on several processors, are integrated using the Task Control Architecture. The Task Control Architecture provides facilities for interprocess communication, task decomposition and sequencing, execution monitoring and exception handling, and resource management <ref> [28] </ref>. Finally, interaction with the robot is via the World Wide Web, which provides pages for both commanding the robot and monitoring its progress. In the following, Section 2 contrasts our POMDP-based navigation architecture with more traditional approaches to robot navigation. <p> It is a separate, concurrent process and communicates with the other layers of the robot system via message passing, supported by the Task Control Architecture <ref> [28] </ref>. In this section, we report on experiments that we performed in two environments for which the Markov property is only an approximation: an actual mobile robot navigating in our building, and a realistic simulation of the robot.
Reference: [29] <author> R. Simmons. </author> <title> The curvature-velocity method for local obstacle avoidance. </title> <booktitle> In Proceedings of the International Conference on Robotics and Automation (ICRA), </booktitle> <pages> pages 3375-3382, </pages> <year> 1996. </year>
Reference-contexts: Besides the navigation layer described here, the layers of the system include a servo-control layer that controls the motors of the robot, an obstacle avoidance layer that keeps the robot moving smoothly in a goal direction while avoiding static and dynamic obstacles <ref> [29] </ref>, a path planning layer that reasons about uncertainty to choose paths that have high expected utility [13], and a multiple-task planning layer that uses PRODIGY, a symbolic, non-linear planner, to integrate and schedule delivery requests that arrive asynchronously [9]. <p> This results in the robot making smooth turns at corridor junctions. The robot uses the Curvature Velocity Method <ref> [29] </ref> to do local obstacle avoidance. The Curvature Velocity Method formulates the problem as one of constrained optimization in velocity space. Constraints are placed on the translational and rotational velocities of the robot that stem from physical limitations (velocities and accelerations) and the environment (the configuration of obstacles).
Reference: [30] <author> R. Simmons, R. Goodwin, K. Haigh, S. Koenig, and J. O'Sullivan. </author> <title> A layered architecture for office delivery robots. </title> <booktitle> In Proceedings of the International Conference on Autonomous Agents, </booktitle> <year> 1997. </year>
Reference-contexts: In addition, both the POMDP and the generated information (such as the pose information and the plans) can be utilized by higher-level planning modules, such as task planners. The POMDP-based navigation architecture is one layer of our office delivery system (Figure 1) <ref> [30] </ref>. <p> In the period from December 1, 1995 to May 31, 1996 Xavier attempted 1,571 navigation requests and reached its intended destination in 1,467 cases, where each job required it to move 40 meters on average (see Table 3 and <ref> [30] </ref> for more details). Most failures are due to problems with our hardware (boards shaking loose) and the wireless communication between the on-board robot system and the off-board user interface (which includes the statistics-gathering software), and thus are unrelated to the POMDP-based navigation architecture.
Reference: [31] <author> R. Simmons and S. Koenig. </author> <title> Probabilistic robot navigation in partially observable environments. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), </booktitle> <pages> pages 1080-1087, </pages> <year> 1995. </year>
Reference-contexts: Given the current state distribution ff, the approaches greedily complete the policy as follows: * The "Most Likely State" Strategy [23] executes the action that is assigned to the most likely state, that is, the action a (arg max s2S ff (s)). * The "Voting" Strategy <ref> [31] </ref> executes the action with the highest probability mass according to ff, that is, the action arg max a2A P * The "Completely Observable after the First Step" Strategy [4, 35] executes the action arg max a2A s2S [ff (s)(r (s; a)+fl s 0 2S [p (s 0 js; a)v (s
Reference: [32] <author> R. Smith and P. Cheeseman. </author> <title> On the representation and estimation of spatial uncertainty. </title> <journal> The International Journal of Robotics Research, </journal> <volume> 5 </volume> <pages> 56-68, </pages> <year> 1986. </year>
Reference-contexts: Previously reported approaches that maintain pose distributions often use either Kalman filters <ref> [16, 32] </ref> or temporal Bayesian networks [5]. Both approaches can utilize motion and sensor reports to update the pose distribution. Kalman filters model only restricted pose distributions in continuous pose space. In the simplest case, these are Gaussians.
Reference: [33] <author> E. Sondik. </author> <title> The optimal control of partially observable Markov processes over the infinite horizon: Discounted costs. </title> <journal> Operations Research, </journal> <volume> 26(2) </volume> <pages> 282-304, </pages> <year> 1978. </year>
Reference-contexts: o (s) = P P P for all s 2 S: The transition probabilities are p (ff 0 X [q (ojs) s 0 2S Any mapping from states to actions that is optimal for this completely observable Markov decision process model is also optimal for the POMDP (under reasonable assumptions) <ref> [33] </ref>. This means that for POMDPs, there always exists a POMDP policy (a mapping from state distributions to actions) that maximizes the expected total reward. This policy can be pre-computed.
Reference: [34] <author> A. Stolcke and S. Omohundro. </author> <title> Hidden Markov model induction by Bayesian model merging. </title> <booktitle> In Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 11-18, </pages> <year> 1993. </year>
Reference-contexts: This POMDP is the one that best fits the empirical data. In our application, this corresponds to fine-tuning the navigation model from experience, including the map, actuator, and sensor models. While there is no known technique for doing this efficiently, there exist efficient algorithms that approximate the optimal POMDP <ref> [4, 22, 34] </ref>. The Baum-Welch algorithm [26] is one such algorithm. This iterative expectation-maximization algorithm does not require control of the POMDP process and thus can be used by an observer to learn the POMDP.
Reference: [35] <author> J. Tenenberg, J. Karlsson, and S. Whitehead. </author> <title> Learning via task decomposition. </title> <booktitle> In Proceedings of the "From Animals to Animats" Conference, </booktitle> <pages> pages 337-343, </pages> <year> 1992. </year>
Reference-contexts: is assigned to the most likely state, that is, the action a (arg max s2S ff (s)). * The "Voting" Strategy [31] executes the action with the highest probability mass according to ff, that is, the action arg max a2A P * The "Completely Observable after the First Step" Strategy <ref> [4, 35] </ref> executes the action arg max a2A s2S [ff (s)(r (s; a)+fl s 0 2S [p (s 0 js; a)v (s 0 )])].
Reference: [36] <author> A. </author> <title> Viterbi. Error bounds for convolutional codes and an asymptotically optimum decoding algorithm. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-13(2):260-269, </volume> <year> 1967. </year>
Reference-contexts: While the techniques from Section 3.3 can determine the most likely state of the POMDP process at each point in time, merely connecting these states might not result in a continuous path. The Viterbi algorithm uses dynamic programming to compute the most likely path efficiently <ref> [36] </ref>. Its first three steps are Steps A1. to A3. in Section 3.3, except that the summations on Lines A3.(a) and A3.(b) are replaced by maximizations: B1.
Reference: [37] <author> J. Yang, Y. Xu, and C. Chen. </author> <title> Hidden Markov model approach to skill learning and its application to telerobotics. </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> 10(5) </volume> <pages> 621-631, </pages> <year> 1994. </year> <month> 24 </month>
Reference-contexts: Properties of POMDPs have been studied extensively in Operations Research. In Artificial Intelligence and Robotics, POMDPs have been applied to speech and handwriting recognition [11] and the interpretation of tele-operation commands <ref> [10, 37] </ref>. They have also gained popularity in the Artificial Intelligence community as a formal model for planning under uncertainty [3, 12]. Consequently, standard algorithms are available to solve tasks that are typically encountered by observers and decision makers. In the following, we describe some of these algorithms. 3.1.
References-found: 37

