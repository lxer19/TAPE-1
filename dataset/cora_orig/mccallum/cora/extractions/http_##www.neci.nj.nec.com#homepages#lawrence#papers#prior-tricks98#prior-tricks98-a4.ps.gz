URL: http://www.neci.nj.nec.com/homepages/lawrence/papers/prior-tricks98/prior-tricks98-a4.ps.gz
Refering-URL: http://www.neci.nj.nec.com/homepages/lawrence/papers/prior-tricks98/
Root-URL: http://www.neci.nj.nec.com
Email: flawrence,gilesg@research.nj.nec.com, ian.burns@oa.com.au, back@zoo.riken.go.jp,  Tsoi@uow.edu.au  
Phone: 1  2  3  
Title: Neural Network Classification and Prior Class Probabilities  
Author: G. Orr and K.-R. Muller and R. Caruana, Steve Lawrence Ian Burns Andrew Back Ah Chung Tsoi C. Lee Giles ? Ah Chung 
Address: 4 Independence Way, Princeton, NJ 08540  2, 7-9 Albany St, St. Leonards, NSW 2065, Australia  Northfields Ave, Wollongong, NSW 2522, Australia  
Affiliation: NEC Research Institute  Open Access Pty Ltd, Level  Brain Information Processing Group, The Institute of Physical and Chemical Research (RIKEN), Japan 4 Faculty of Informatics, University of Wollongong,  
Note: To appear in: Tricks of the Trade, Lecture Notes in Computer Science State-of-the-Art Surveys, edited by  Springer Verlag, pp. 299-314, 1998.  
Abstract: A commonly encountered problem in MLP (multi-layer perceptron) classification problems is related to the prior probabilities of the individual classes if the number of training examples that correspond to each class varies significantly between the classes, then it may be harder for the network to learn the rarer classes in some cases. Such practical experience does not match theoretical results which show that MLPs approximate Bayesian a posteriori probabilities (independent of the prior class probabilities). Our investigation of the problem shows that the difference between the theoretical and practical results lies with the assumptions made in the theory (accurate estimation of Bayesian a posteriori probabilities requires the network to be large enough, training to converge to a global minimum, infinite training data, and the a priori class probabilities of the test set to be correctly represented in the training set). Specifically, the problem can often be traced to the fact that efficient MLP training mechanisms lead to sub-optimal solutions for most practical problems. In this chapter, we demonstrate the problem, discuss possible methods for alleviating it, and introduce new heuristics that are shown to perform well on a sample ECG classification problem. The heuristics may also be used as a simple means of adjusting for unequal misclassification costs. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> AAMI. </author> <title> Testing and reporting performance results of ventricular Arrhythmia detection algorithms. Association for the Advancement of Medical Instrumentation, </title> <address> Arlington, VA, </address> <year> 1987. ECAR-1987. </year>
Reference-contexts: For example, a model may always predict the most common class and still provide relatively high performance. Statistics such as the Sensitivity, Positive Predictivity, and False Positive Rate can provide more meaningful results <ref> [1] </ref>. These are defined on a class by class basis as follows: The Sensitivity of a class is the proportion of events labelled as that class which are correctly detected.
Reference: 2. <author> Rangachari Anand, Kishan G. Mehrotra, Chilukuri K. Mohan, and Sanjay Ranka. </author> <title> An improved algorithm for neural network classification of imbalanced training sets. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 4(6) </volume> <pages> 962-969, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Others have considered the case of different class probabilities between the training and test sets, e.g. [23]. 4 Anand et al. <ref> [2] </ref> have also presented an algorithm related to unequal prior class probabilities. However, their algorithm aims only to improve convergence speed.
Reference: 3. <author> Etienne Barnard and Elizabeth C. Botha. </author> <title> Back-propagation uses prior information efficiently. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 4(5) </volume> <pages> 794-802, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: If the number of training examples for each class varies significantly between classes then there may be a bias towards predicting the more common classes <ref> [3, 4] </ref>, leading to worse classification performance for the rarer classes. In [5] it was observed that classes with low a priori probability in a speech application were "ignored" (no samples were classified as these classes after training). <p> E = 2 k=1 j=1 x (d kj y kj ) 2 (5) 5 A cost function with similar motivation, the "classification figure-of-merit" (CFM) proposed by Hampshire and Waibel [13], has been suggested as a possible improvement when prior class probabilities vary <ref> [3] </ref>. In [13], the CFM cost function leads to networks which make different errors to those trained with the MSE criterion, and can therefore be useful for improving performance by combining classifiers trained with the CFM and the MSE. <p> not the case when fitting the function sech (x) using two tanh sigmoids [8] (because sech (x) = lim d!0 (tanh (x + d) tanh (x))=d, i.e. the weights become indefinitely large as the approximation improves). 10 In relation to the representational capacity (size of the network), Barnard and Botha <ref> [3] </ref> have observed that MLP networks have a tendency to guess higher probability classes when a network is too small to approximate the decision boundaries reason ably well. 11 Lyon and Yaeger [20] find that their frequency balancing technique reduces the effect of the prior class probabilities on the network and
Reference: 4. <author> Etienne Barnard and David Casasent. </author> <title> A comparison between criterion functions for linear classifiers, with an application to neural nets. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 19(5) </volume> <pages> 1030-1041, </pages> <year> 1989. </year>
Reference-contexts: If the number of training examples for each class varies significantly between classes then there may be a bias towards predicting the more common classes <ref> [3, 4] </ref>, leading to worse classification performance for the rarer classes. In [5] it was observed that classes with low a priori probability in a speech application were "ignored" (no samples were classified as these classes after training).
Reference: 5. <author> Etienne Barnard, R.A. Cole, and L. Hou. </author> <title> Location and classification of plosive constants using expert knowledge and neural-net classifiers. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 84 Supp 1:S60, </volume> <year> 1988. </year>
Reference-contexts: If the number of training examples for each class varies significantly between classes then there may be a bias towards predicting the more common classes [3, 4], leading to worse classification performance for the rarer classes. In <ref> [5] </ref> it was observed that classes with low a priori probability in a speech application were "ignored" (no samples were classified as these classes after training).
Reference: 6. <author> H.A. Bourlard and N. Morgan. </author> <title> Links between Markov models and multilayer perceptrons. </title> <editor> In D. S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems 1, </booktitle> <volume> volume 1, </volume> <pages> pages 502-510. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1989. </year>
Reference-contexts: 1 Introduction It has been shown theoretically that MLPs approximate Bayesian a posteriori probabilities when the desired network outputs are 1 of M and squared-error or cross-entropy cost functions are used <ref> [6, 11, 12, 15, 23, 25, 26, 28, 29, 32] </ref>.
Reference: 7. <author> H.A. Bourlard and N. Morgan. </author> <title> Connnectionist Speech Recognition: A Hybrid Approach. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, MA, </address> <year> 1994. </year>
Reference-contexts: Such problems indicate that either the estimation of Bayesian a posteriori probabilities is inaccurate, or that such estimation may not be desired (e.g. due to varying misclassification costs (this is explained further in section 4)). Bourlard and Morgan <ref> [7] </ref> have demonstrated inaccurate estimation of Bayesian a posteriori probabilities in speech recognition. This chapter discusses how the problem may occur along with methods of dealing with the problem. 2 The Trick This section describes the tricks for alleviating the aforementioned problem. <p> Rather than arguing for either backpropagation or conjugate gradient here (neither training algorithm is expected to always find a global minima in general), we simply note that our experience and the experience of others <ref> [7, 18, 19, 27] </ref> suggests that conjugate gradient is not superior for many problems - i.e. backpropagation works better on one class of problems and conjugate gradient works better on another class. Conjugate gradient resulted in significantly worse performance when tested on the ECG problem.
Reference: 8. <author> N. Scott Cardell, Wayne Joerding, and Ying Li. </author> <title> Why some feedforward networks cannot learn some polynomials. </title> <journal> Neural Computation, </journal> <volume> 6(4) </volume> <pages> 761-766, </pages> <year> 1994. </year>
Reference-contexts: We have used a modified implementation of the algorithm from Fletcher [9]. 9 In general, smaller weights correspond to smoother functions, however this is not always true. For example, this is not the case when fitting the function sech (x) using two tanh sigmoids <ref> [8] </ref> (because sech (x) = lim d!0 (tanh (x + d) tanh (x))=d, i.e. the weights become indefinitely large as the approximation improves). 10 In relation to the representational capacity (size of the network), Barnard and Botha [3] have observed that MLP networks have a tendency to guess higher probability classes
Reference: 9. <author> R. Fletcher. </author> <title> Practical Methods of Optimization, Second Edition. </title> <publisher> John Wiley & Sons, </publisher> <year> 1987. </year>
Reference-contexts: It should be noted that there are many options when implementing a conjugate gradient training algorithm and that poor performance may be attributed to the implementation used. We have used a modified implementation of the algorithm from Fletcher <ref> [9] </ref>. 9 In general, smaller weights correspond to smoother functions, however this is not always true.
Reference: 10. <author> S. Geman, E. Bienenstock, and R. Doursat. </author> <title> Neural networks and the bias/variance dilemma. </title> <journal> Neural Computation, </journal> <volume> 4(1) </volume> <pages> 1-58, </pages> <year> 1992. </year>
Reference-contexts: In practice, MLPs have also been shown to accurately estimate Bayesian a posteriori probabilities for certain experiments <ref> [10] </ref>. However, a commonly encountered problem in MLP classification is related to the case when the frequency of the classes in the training set varies significantly 3 .
Reference: 11. <author> H. Gish. </author> <title> A probabilistic approach to the understanding and training of neural network classifiers. </title> <booktitle> In Proceedings of the IEEE Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 1361-1364. </pages> <publisher> IEEE Press, </publisher> <year> 1990. </year>
Reference-contexts: 1 Introduction It has been shown theoretically that MLPs approximate Bayesian a posteriori probabilities when the desired network outputs are 1 of M and squared-error or cross-entropy cost functions are used <ref> [6, 11, 12, 15, 23, 25, 26, 28, 29, 32] </ref>.
Reference: 12. <author> J.B. Hampshire and Barak Pearlmutter. </author> <title> Equivalence proofs for multilayer perceptron classifiers and the Bayesian discriminant function. </title> <editor> In D. S. Touretzky, J. L. Elman, T. J. Sejnowski, and G. E. Hinton, editors, </editor> <booktitle> Proceedings of the 1990 Connectionist Models Summer School. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: 1 Introduction It has been shown theoretically that MLPs approximate Bayesian a posteriori probabilities when the desired network outputs are 1 of M and squared-error or cross-entropy cost functions are used <ref> [6, 11, 12, 15, 23, 25, 26, 28, 29, 32] </ref>.
Reference: 13. <author> J.B. Hampshire and Alex H. Waibel. </author> <title> A novel objective function for improved phoneme recognition using time delay neural networks. </title> <booktitle> In International Joint Conference on Neural Networks, </booktitle> <pages> pages 235-241, </pages> <address> Washington, DC, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: Prior scaling in this form can be expressed as training with the following alternative cost function 5 : Definition 1. E = 2 k=1 j=1 x (d kj y kj ) 2 (5) 5 A cost function with similar motivation, the "classification figure-of-merit" (CFM) proposed by Hampshire and Waibel <ref> [13] </ref>, has been suggested as a possible improvement when prior class probabilities vary [3]. In [13], the CFM cost function leads to networks which make different errors to those trained with the MSE criterion, and can therefore be useful for improving performance by combining classifiers trained with the CFM and the <p> E = 2 k=1 j=1 x (d kj y kj ) 2 (5) 5 A cost function with similar motivation, the "classification figure-of-merit" (CFM) proposed by Hampshire and Waibel <ref> [13] </ref>, has been suggested as a possible improvement when prior class probabilities vary [3]. In [13], the CFM cost function leads to networks which make different errors to those trained with the MSE criterion, and can therefore be useful for improving performance by combining classifiers trained with the CFM and the MSE. <p> However, networks trained with the CFM criterion do not result in higher classification performance than networks trained with the MSE criterion for the experiments reported in <ref> [13] </ref>. where the network has one output for each of the N c classes, N p is the number of patterns, d is the desired or target output, y is the predicted output, and x is the class of pattern k.
Reference: 14. <author> S. Haykin. </author> <title> Neural Networks, A Comprehensive Foundation. </title> <publisher> Macmillan, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: These techniques tend to result in networks with smaller weights. 4. A commonly recommended technique with MLP classification is to set the training targets away from the bounds of the activation function (e.g. (-0.8, 0.8) instead of (-1, 1) for the tanh activation function) <ref> [14] </ref>. These four situations can all lead to a bias towards smaller weights, or "smoother" models 9 .
Reference: 15. <author> F. Kanaya and S. Miyake. </author> <title> Bayes statistical behavior and valid generalization of pattern classifying neural networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 2(1):471, </volume> <year> 1991. </year>
Reference-contexts: 1 Introduction It has been shown theoretically that MLPs approximate Bayesian a posteriori probabilities when the desired network outputs are 1 of M and squared-error or cross-entropy cost functions are used <ref> [6, 11, 12, 15, 23, 25, 26, 28, 29, 32] </ref>.
Reference: 16. <author> A. Krogh and J.A. Hertz. </author> <title> A simple weight decay can improve generalization. </title> <editor> In J.E. Moody, S. J. Hanson, and R. P. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 4, </volume> <pages> pages 950-957. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1992. </year>
Reference-contexts: Often, a result of attaining a sub-optimal solution is that not all of the network resources are efficiently used. Experiments with a controlled task have indicated that the sub-optimal solutions often have smaller weights on average [17]. 3. Weight decay <ref> [16] </ref> or weight elimination [30] are often used in MLP training and aim to minimize a cost function which penalizes large weights. These techniques tend to result in networks with smaller weights. 4.
Reference: 17. <author> Steve Lawrence, C. Lee Giles, and A.C. Tsoi. </author> <title> Lessons in neural network training: Overfitting may be harder than expected. </title> <booktitle> In Proceedings of the Fourteenth National Conference on Artificial Intelligence, AAAI-97, </booktitle> <pages> pages 540-545. </pages> <publisher> AAAI Press, </publisher> <address> Menlo Park, California, </address> <year> 1997. </year>
Reference-contexts: Often, a result of attaining a sub-optimal solution is that not all of the network resources are efficiently used. Experiments with a controlled task have indicated that the sub-optimal solutions often have smaller weights on average <ref> [17] </ref>. 3. Weight decay [16] or weight elimination [30] are often used in MLP training and aim to minimize a cost function which penalizes large weights. These techniques tend to result in networks with smaller weights. 4.
Reference: 18. <author> Y. Le Cun. </author> <title> Efficient learning and second order methods. </title> <booktitle> Tutorial presented at Neural Information Processing Systems 5, </booktitle> <year> 1993. </year>
Reference-contexts: Rather than arguing for either backpropagation or conjugate gradient here (neither training algorithm is expected to always find a global minima in general), we simply note that our experience and the experience of others <ref> [7, 18, 19, 27] </ref> suggests that conjugate gradient is not superior for many problems - i.e. backpropagation works better on one class of problems and conjugate gradient works better on another class. Conjugate gradient resulted in significantly worse performance when tested on the ECG problem.
Reference: 19. <author> Y. Le Cun and Yoshua Bengio. </author> <title> Pattern recognition. </title> <editor> In Michael A. Arbib, editor, </editor> <booktitle> The Handbook of Brain Theory and Neural Networks, </booktitle> <pages> pages 711-715. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1995. </year>
Reference-contexts: Rather than arguing for either backpropagation or conjugate gradient here (neither training algorithm is expected to always find a global minima in general), we simply note that our experience and the experience of others <ref> [7, 18, 19, 27] </ref> suggests that conjugate gradient is not superior for many problems - i.e. backpropagation works better on one class of problems and conjugate gradient works better on another class. Conjugate gradient resulted in significantly worse performance when tested on the ECG problem.
Reference: 20. <author> R. Lyon and L. Yaeger. </author> <title> On-line hand-printing recognition with neural networks. </title> <booktitle> In Fifth International Conference on Microelectronics for Neural Networks and Fuzzy Systems, </booktitle> <address> Lausanne, Switzerland, 1996. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: as the approximation improves). 10 In relation to the representational capacity (size of the network), Barnard and Botha [3] have observed that MLP networks have a tendency to guess higher probability classes when a network is too small to approximate the decision boundaries reason ably well. 11 Lyon and Yaeger <ref> [20] </ref> find that their frequency balancing technique reduces the effect of the prior class probabilities on the network and effectively forces the network to -1 0 1 Output Input Network Outputs without Probabilistic Sampling Output 1 Output 2 -1 0 1 Output Input Network Outputs using Probabilistic Sampling Output 1 Output
Reference: 21. <author> MIT-BIH. </author> <note> MIT-BIH Arrhythmia database directory. Technical Report BMEC TR010 (Revised), </note> <institution> Massachusetts Institute of Technology and Beth Israel Hospital, </institution> <year> 1988. </year>
Reference-contexts: The database used is the MIT-BIH Arrhythmia database <ref> [21] </ref> a common publicly available ECG database which contains a large number of ECG records that have been carefully annotated by experts. Detection of the following four beat types is considered: Normal (N), Premature Ventricular Contraction (PVC), Supraventricular Contraction (S), and Fusion (F) [21], i.e. there are four output classes. <p> database used is the MIT-BIH Arrhythmia database <ref> [21] </ref> a common publicly available ECG database which contains a large number of ECG records that have been carefully annotated by experts. Detection of the following four beat types is considered: Normal (N), Premature Ventricular Contraction (PVC), Supraventricular Contraction (S), and Fusion (F) [21], i.e. there are four output classes. The four classes are denoted 1 (N), 2 (PVC), 3 (S), and 4 (F). An autoregressive model is calculated for a window of 200 samples centered over the peak of the R-wave of each beat.
Reference: 22. <author> Alan F. Murray and Peter J. Edwards. </author> <title> Enhanced MLP performance and fault tolerance resulting from synaptic weight noise during training. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 5(5) </volume> <pages> 792-802, </pages> <year> 1994. </year>
Reference-contexts: For the two class confusion matrix shown in table 1 the positive predictivity of class 1 is c 11 c 11 +c 21 . 6 The heuristic of adding noise during training <ref> [22] </ref> could be useful here as with the other techniques in this chapter. The False Positive Rate of a class is the proportion of all patterns for other classes which were incorrectly classified as that class.
Reference: 23. <author> M.D. Richard and R.P. Lippmann. </author> <title> Neural network classifiers estimate Bayesian a posteriori probabilities. </title> <journal> Neural Computation, </journal> <volume> 3(4) </volume> <pages> 461-483, </pages> <year> 1991. </year>
Reference-contexts: 1 Introduction It has been shown theoretically that MLPs approximate Bayesian a posteriori probabilities when the desired network outputs are 1 of M and squared-error or cross-entropy cost functions are used <ref> [6, 11, 12, 15, 23, 25, 26, 28, 29, 32] </ref>. <p> Others have considered the case of different class probabilities between the training and test sets, e.g. <ref> [23] </ref>. 4 Anand et al. [2] have also presented an algorithm related to unequal prior class probabilities. However, their algorithm aims only to improve convergence speed. <p> classes can be less accurate than that of the higher frequency classes [24] (the deviations of the network outputs from the true values in regions with a higher number of data points influence the squared error cost function more than the deviations in regions with a lower number of points <ref> [23] </ref>). The post scaling technique introduced here can also be used to optimize a given criterion, e.g. the outputs may be scaled so that the probability of predicting each class matches the prior probabilities in the training set as closely as possible. <p> For example, word frequency information can be obtained from text databases and the frequency of various diseases can be obtained from health statistics <ref> [23] </ref>. be constant throughout the input space, e.g. scaling may be helpful in one region but detrimental in another. 2. Nonlinear calibration.
Reference: 24. <author> B.D. Ripley. </author> <title> Pattern Recognition and Neural Networks. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, UK, </address> <year> 1996. </year>
Reference-contexts: Experiments with this technique alone show that it is not always as successful as prior scaling of the weight updates. This may be because the estimation of the lower frequency classes can be less accurate than that of the higher frequency classes <ref> [24] </ref> (the deviations of the network outputs from the true values in regions with a higher number of data points influence the squared error cost function more than the deviations in regions with a lower number of points [23]). <p> performed using a simple hill-climbing algorithm which adjusts a scaling factor associated with each of the outputs of the network. 2.4 Equalizing Class Membership A simple method for alleviating difficulty with unequal prior class probabilities is to adjust (e.g. equalize) the number of patterns in each class, either by subsam-pling <ref> [24] </ref> (removing patterns from higher frequency classes), or by duplication (of patterns in lower frequency classes) 6 . For subsampling, patterns can be removed randomly, or heuristics may be used to remove patterns in regions of low ambiguity. Subsampling involves a loss of information which can be detrimental. <p> a particular class) 13 , or ii) the distribution of the classes does not represent their relative importance, e.g. in a medical classification problem the cost of misclassifying a diseased case as normal may be much higher than the cost of classifying a normal case as a (possibly) diseased case <ref> [24] </ref>. The importance of each class may be independent of the class prior probabilities. Note that scaling such that lower frequency classes are made to be artificially more important can be useful when considering a higher level problem.
Reference: 25. <author> Raul Rojas. </author> <title> A short proof of the posterior probability property of classifier neural networks. </title> <journal> Neural Computation, </journal> <volume> 8 </volume> <pages> 41-43, </pages> <year> 1996. </year>
Reference-contexts: 1 Introduction It has been shown theoretically that MLPs approximate Bayesian a posteriori probabilities when the desired network outputs are 1 of M and squared-error or cross-entropy cost functions are used <ref> [6, 11, 12, 15, 23, 25, 26, 28, 29, 32] </ref>.
Reference: 26. <author> D.W. Ruck, S.K. Rogers, K. Kabrisky, M.E. Oxley, and B.W. Suter. </author> <title> The multilayer perceptron as an approximation to an optimal Bayes estimator. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 1(4) </volume> <pages> 296-298, </pages> <year> 1990. </year>
Reference-contexts: 1 Introduction It has been shown theoretically that MLPs approximate Bayesian a posteriori probabilities when the desired network outputs are 1 of M and squared-error or cross-entropy cost functions are used <ref> [6, 11, 12, 15, 23, 25, 26, 28, 29, 32] </ref>.
Reference: 27. <author> W. Schiffman, M. Joost, and R. Werner. </author> <title> Optimization of the backpropagation algorithm for training multilayer perceptrons. </title> <type> Technical report, </type> <institution> University of Koblenz, </institution> <year> 1994. </year>
Reference-contexts: Rather than arguing for either backpropagation or conjugate gradient here (neither training algorithm is expected to always find a global minima in general), we simply note that our experience and the experience of others <ref> [7, 18, 19, 27] </ref> suggests that conjugate gradient is not superior for many problems - i.e. backpropagation works better on one class of problems and conjugate gradient works better on another class. Conjugate gradient resulted in significantly worse performance when tested on the ECG problem.
Reference: 28. <author> P.A. Shoemaker. </author> <title> A note on least-squares learning procedures and classification by neural network models. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 2(1) </volume> <pages> 158-160, </pages> <year> 1991. </year>
Reference-contexts: 1 Introduction It has been shown theoretically that MLPs approximate Bayesian a posteriori probabilities when the desired network outputs are 1 of M and squared-error or cross-entropy cost functions are used <ref> [6, 11, 12, 15, 23, 25, 26, 28, 29, 32] </ref>.
Reference: 29. <author> E. Wan. </author> <title> Neural network classification: A Bayesian interpretation. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 1(4) </volume> <pages> 303-305, </pages> <year> 1990. </year>
Reference-contexts: 1 Introduction It has been shown theoretically that MLPs approximate Bayesian a posteriori probabilities when the desired network outputs are 1 of M and squared-error or cross-entropy cost functions are used <ref> [6, 11, 12, 15, 23, 25, 26, 28, 29, 32] </ref>.
Reference: 30. <author> A.S. Weigend, D.E. Rumelhart, and B.A. Huberman. </author> <title> Generalization by weight-elimination with application to forecasting. </title> <editor> In R. P. Lippmann, J.E. Moody, and D. S. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 3, </volume> <pages> pages 875-882. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: Often, a result of attaining a sub-optimal solution is that not all of the network resources are efficiently used. Experiments with a controlled task have indicated that the sub-optimal solutions often have smaller weights on average [17]. 3. Weight decay [16] or weight elimination <ref> [30] </ref> are often used in MLP training and aim to minimize a cost function which penalizes large weights. These techniques tend to result in networks with smaller weights. 4.
Reference: 31. <author> N.A. Weiss and M.J. Hassett. </author> <title> Introductory Statistics. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <note> second edition, </note> <year> 1987. </year>
Reference-contexts: The whiskers extend from the ends of the box to the minimum and maximum values. The median and the IQR are simple statistics which are not as sensitive to outliers as the mean and the standard deviation <ref> [31] </ref>. The median is the value in the middle when arranging the distribution in order from the smallest to the largest value. If the data is divided into two equal groups about the median, then the IQR is the difference between the medians of these groups.
Reference: 32. <author> H. White. </author> <title> Learning in artificial neural networks: A statistical perspective. </title> <journal> Neural Computation, </journal> <volume> 1(4) </volume> <pages> 425-464, </pages> <year> 1989. </year>
Reference-contexts: 1 Introduction It has been shown theoretically that MLPs approximate Bayesian a posteriori probabilities when the desired network outputs are 1 of M and squared-error or cross-entropy cost functions are used <ref> [6, 11, 12, 15, 23, 25, 26, 28, 29, 32] </ref>.
Reference: 33. <author> L. Yaeger, R. Lyon, and B. Webb. </author> <title> Effective training of a neural network character classifier for word recognition. </title> <editor> In M.C. Mozer, M.I. Jordan, and T. Petsche, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 9, </booktitle> <address> Cambridge, MA, 1997. </address> <publisher> MIT Press. </publisher>
Reference-contexts: However, a disadvantage of repeating patterns is that the effective training set would be larger, resulting in longer training times for the same number of epochs. Such a technique could be done probabilistically, and this is the subject of the next technique. 2.2 Probabilistic Sampling Yaeger et al. <ref> [33] </ref> have proposed a method called frequency balancing which is similar to the prior scaling method above. <p> Note that scaling such that lower frequency classes are made to be artificially more important can be useful when considering a higher level problem. For example, the training data from natural English words and phrases exhibit very non-uniform priors for different characters. Yaeger et al. <ref> [33] </ref> find that reducing the effect of these priors on the network using frequency balancing improves the performance of the higher level word recognition training. <p> We introduced algorithms which a) scale weight updates on a class by class basis according to the prior class probabilities, b) alter class frequencies probabilistically (very similar to the frequency balancing technique of Yaeger et al. <ref> [33] </ref>), and c) scale outputs after training in order to maximize a given performance criterion.
References-found: 33

