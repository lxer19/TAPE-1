URL: http://www.research.att.com/~mkearns/papers/malicious.ps.Z
Refering-URL: http://www.research.att.com/~mkearns/
Root-URL: 
Title: Learning in the Presence of Malicious Errors  
Author: Michael Kearns Ming Li 
Affiliation: AT&T Bell Laboratories  University of Waterloo  
Abstract: In this paper we study an extension of the distribution-free model of learning introduced by Valiant [23] (also known as the probably approximately correct or PAC model) that allows the presence of malicious errors in the examples given to a learning algorithm. Such errors are generated by an adversary with unbounded computational power and access to the entire history of the learning algorithm's computation. Thus, we study a worst-case model of errors. Our results include general methods for bounding the rate of error tolerable by any learning algorithm, efficient algorithms tolerating nontrivial rates of malicious errors, and equivalences between problems of learning with errors and standard combinatorial optimization problems.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Angluin, P. Laird. </author> <title> Learning from noisy examples. </title> <journal> Machine Learning, </journal> <volume> 2(4), </volume> <year> 1988, </year> <pages> pp. 343-370. </pages>
Reference-contexts: Several of our results also apply to a more benign model of classification noise defined by Angluin and Laird <ref> [1] </ref>, in which the underlying target distributions are unaltered, but there is some probability that a positive example is incorrectly classified as being negative, and vice-versa. Several themes are brought out. One is that error tolerance need not come at the expense of efficiency or simplicity. <p> On the other hand, some of our algorithms provide hope that if one can somehow reliably control the rate of error to a small amount, then errors of an arbitrary nature can be compensated for by the learning process. Angluin and Laird <ref> [1] </ref> subsequently modified Valiant's definitions to study a non-malicious model of errors, defined in Section 2.3 as the classification noise model. Their results demonstrate that under stronger assumptions on the nature of the errors, large rates of error can be tolerated by polynomial-time algorithms for nontrivial representation classes. <p> Oracles with classification noise. Some of our results will also apply to a more benign model of errors defined by Angluin and Laird <ref> [1] </ref>, which we will call the classification noise model. Here we have oracles POS fi fi CN that behave as follows: as before, with probability 1 fi, POS CN returns a point drawn randomly according to the target distribution D + . <p> As opposed to the malicious case, the input fi 0 is relevant here, even in the case of arbitrary target distributions, since classification noise rates approaching 1=2 can be tolerated by polynomial-time algorithms for some nontrivial representation classes <ref> [1] </ref>. Optimal classification noise rates. <p> Our first such result states that the malicious error rate must be smaller than the desired accuracy *. This is in sharp contrast to the classification noise model, where Angluin and Laird <ref> [1] </ref> proved, for example, E poly CN (kDNF n ) c 0 for all n and any constant c 0 &lt; 1=2. <p> Thus, for classification noise we see that the power of using both positive and negative examples may be dramatic: for kCNF we have E poly CN (kCNF n ) c 0 for any c 0 &lt; 1=2 due to Angluin and Laird <ref> [1] </ref> but E CN ;+ (kCNF n ) = O (*=n k ) by Theorem 2. (Kearns et al. [13] show that kCNF is not learnable in polynomial time from negative examples even in the error-free model.) In fact, we can give a bound on E CN;+ and E CN; that
Reference: [2] <author> D. Angluin, L.G. Valiant. </author> <title> Fast probabilistic algorithms for Hamiltonian circuits and matchings. </title> <journal> Journal of Computer and Systems Sciences, </journal> <volume> 18, </volume> <year> 1979, </year> <pages> pp. 155-193. 32 </pages>
Reference-contexts: Then for 0 ff 1, Fact CB1. LE (p; m; (1 ff)mp) e ff 2 mp=2 and Fact CB2. GE (p; m; (1 + ff)mp) e ff 2 mp=3 These bounds in the form they are stated are from the paper of Angluin and Valiant <ref> [2] </ref>; see also Chernoff [6]. Although we will make frequent use of Fact CB1 and Fact CB2, we will do so in varying levels of detail, depending on the complexity of the calculation involved.
Reference: [3] <author> A. Blum. </author> <title> Learning in an infinite attribute space. </title> <booktitle> Proceedings of the 22nd A.C.M. Symposium on the Theory of Computing, </booktitle> <year> 1990, </year> <pages> pp. 64-72. </pages>
Reference-contexts: Our algorithm will tolerate a larger rate of error when the number s of relevant attributes is considerably smaller than the total number of variables n. Other improvements in the performance of learning algorithms in the presence of many irrelevant attributes are investigated by Littlestone [16] and Blum <ref> [3] </ref>. We note that by applying Theorem 2 we can show that even for M 1 n , the class of monomials of length 1, the positive-only and negative-only malicious error rates are bounded by *=(n 1).
Reference: [4] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, M. Warmuth. </author> <title> Occam's razor. </title> <journal> Information Processing Letters, </journal> <volume> 24, </volume> <year> 1987, </year> <pages> pp. 377-380. </pages>
Reference-contexts: In this section we give efficient algorithms for several representation classes and analyze their tolerance to malicious errors. We begin by giving a generalization of Occam's Razor <ref> [4] </ref> for the case when errors are present in the examples. 16 Let C and H be representation classes over X. Let A be an algorithm accessing POS fi and NEG MAL , and taking inputs 0 &lt; *; ffi &lt; 1.
Reference: [5] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, M. Warmuth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the A.C.M., </journal> <volume> 36(4), </volume> <year> 1989, </year> <pages> pp. 929-965. </pages>
Reference-contexts: Another theme is the importance of using both positive and negative examples whenever errors (either malicious errors or classification noise errors) are present. Several existing learning algorithms use only positive examples or only negative examples (see e.g. Valiant [23] and Blumer et al. <ref> [5] </ref>). We demonstrate strong upper bounds on the tolerable error rate when only one type is used, and show that this rate can be provably increased when both types are used. <p> The Vapnik-Chervonenkis was originally introduced in the paper of Vapnik and Chervonenkis [25] and was first studied in the context of the distribution-free model by Blumer et al. <ref> [5] </ref>. Notational conventions. Let E (x) be an event and (x) a random variable that depend on a parameter x that takes on values in a set X. <p> On some run of A, *=r c 0 =2m, and for this run we have cost MD (h A ; S) (r + 1)opt MD (S) 2ropt MD (S), as desired. The first direction of this equivalence is also given by Blumer et al. <ref> [5] </ref>. Note that this equivalence as it is stated is representation-based, in the sense that it relies on the learning algorithm representing its hypothesis as a monomial.
Reference: [6] <author> H. Chernoff. </author> <title> A measure of asymptotic efficiency for tests of a hypothesis based on the sum of observations. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 23, </volume> <year> 1952, </year> <pages> pp. 493-509. </pages>
Reference-contexts: Then for 0 ff 1, Fact CB1. LE (p; m; (1 ff)mp) e ff 2 mp=2 and Fact CB2. GE (p; m; (1 + ff)mp) e ff 2 mp=3 These bounds in the form they are stated are from the paper of Angluin and Valiant [2]; see also Chernoff <ref> [6] </ref>. Although we will make frequent use of Fact CB1 and Fact CB2, we will do so in varying levels of detail, depending on the complexity of the calculation involved.
Reference: [7] <author> V. Chvatal. </author> <title> A greedy heuristic for the set covering problem. </title> <journal> Mathematics of Operations Research, </journal> <volume> 4(3), </volume> <year> 1979, </year> <pages> pp. 233-235. </pages>
Reference-contexts: We give a reduction from learning monomials with errors to a generalization of the weighted set cover problem, and give an approximation algorithm for 2 this problem (generalizing the greedy algorithm analyzed by several authors <ref> [7, 11, 18] </ref>) that is of independent interest. This approximation algorithm is used as a subroutine in a learning algorithm that tolerates an improved error rate for monomials. <p> This suggests that there are classes for which the optimal error rate that can be tolerated efficiently may be considerably smaller than the optimal information-theoretic rate. The best approximation known for the set cover problem remains the greedy algorithm analyzed by Chvatal <ref> [7] </ref>, Johnson [11], Lovasz [17], and Nigmatullin [18]. Finally, we give a canonical reduction that allows many learning with errors problems to be studied as equivalent optimization problems, thus allowing one to sidestep some of the difficulties of analysis in the distribution-free model. <p> This approximation algorithm is of independent interest and has found application in other learning algorithms [14, 26]. Our analysis and notation rely heavily on the work of Chvatal <ref> [7] </ref>; the reader may find it helpful to read his paper first. The Partial Cover Problem: Input: Finite sets S 1 ; : : :; S n with positive real costs c 1 ; : : : ; c n , and a positive fraction 0 &lt; p 1. <p> Output: J fl J such that j j2J fl (we call such a J fl a p-cover of the S i ) and such that cost PC (J fl ) = P j2J fl c j is minimized. Following Chvatal <ref> [7] </ref>, for notational convenience we identify a partial cover fS j 1 ; : : : ; S j s g with the index set fj 1 ; : : : ; j s g. <p> Step 4. Find a k minimizing the ratio c k =jS k j. Add k to J fl , and replace each S j by S j S k . Return to Step 2. Chvatal <ref> [7] </ref> shows that the greedy algorithm for the set cover problem cannot do better than H (m) times the cost of an optimal cover, where H (m) = P m i=1 1=i = fi (log m). <p> opt y i = i2T flfl 0 T opt X i2T flfl 1 T opt opt PC (I) + c j (H (m) + 1) (Lemma 16) Lemma 17 X i2T flfl "T opt y i (H (m) + 1)opt PC (I): Proof: We generalize the idea used by Chvatal <ref> [7] </ref>. <p> We solve this by preprocessing: before running the described simulation, A 0 runs the greedy approximation algorithm analyzed by Chvatal <ref> [7] </ref> on the set cover instance I, and removes any set whose cost is larger than the entire cost of the greedy cover. Then for the new (smaller) instance I 0 , every cost is within a multiplicative factor of log m of every other cost. <p> time algorithm A for M n (using hypothesis space M n ) tolerating E poly MAL (M n ; A) *=r (n) would imply a significant breakthrough in approximation algorithms for set cover, since the best algorithm for this problem remains the 30 greedy method analyzed by Chvatal and others <ref> [7, 11, 17, 18] </ref>. Note that the proof of Theorem 19 in fact shows the result holds for the class of monotone monomials.
Reference: [8] <author> A. Ehrenfeucht, D. Haussler, M. Kearns. L.G. Valiant. </author> <title> A general lower bound on the number of examples needed for learning. </title> <journal> Information and Computation, </journal> <volume> 82(3), </volume> <year> 1989, </year> <pages> pp. 247-261. </pages>
Reference-contexts: We can give a dual of the above algorithm to prove E poly MAL; (SF n ) = fi (*=n) as well. The number of examples required by the 18 algorithm of Theorem 10 is a factor of n larger than the lower bound given by Ehrenfeucht et al. <ref> [8] </ref> for the error-free case; whether this increase is necessary for positive-only algorithms in the presence of malicious errors is an open problem. The next theorem demonstrates that using both positive and negative examples can significantly increase the tolerated error rate in the malicious model. <p> Furthermore, the sample complexity of algorithm A above meets the lower bound (within a constant factor) for the error-free case given by Ehrenfeucht et al. <ref> [8] </ref>; thus we have an algorithm with optimal sample complexity that tolerates the largest possible malicious error rate. This also demonstrates that it may be difficult to prove general theorems providing hard trade-offs between sample size and error rate. <p> This technique has also been noted in the error-free setting by Haussler et al. [10]. 20 It is shown by Ehrenfeucht et al. <ref> [8] </ref> that any learning algorithm A for a representation class C must have sample complexity S A (*; ffi) = 1 1 + vcd (C) : Suppose that a learning algorithm A achieves this optimal sample complexity. <p> Similarly, a slight improvement over the malicious error rate given in Theorem 9 for kDNF can also be shown. For decision lists, we can apply the algorithm of Rivest [19] and the sample size bounds given by Ehrenfeucht et al. <ref> [8] </ref> to obtain the following: Corollary 14 Let kDL n be the class of k-decision lists over x 1 ; : : :; x n .
Reference: [9] <author> M. Garey, D. Johnson. </author> <title> Computers and intractability: a guide to the theory of NP-completeness. </title> <publisher> Freeman, </publisher> <year> 1979. </year>
Reference-contexts: We assume that domain points x 2 X and representations c 2 C are efficiently encoded using any of the standard schemes (see Garey and Johnson <ref> [9] </ref>), and denote by jxj and jcj the length of these encodings measured in bits. Parameterized representation classes. In this paper we will study parameterized classes of representations. Here we have a stratified domain X = S n1 X n and representation class C = n1 C n . <p> The partial cover problem is NP-hard, since it contains the set cover problem as a special case (p = 1) <ref> [9] </ref>. We now give a greedy approximation algorithm G for the partial cover problem. Algorithm G: Step 1. Initialize J fl = ;. Step 2. If j S j2J fl S j j pm then halt and output J fl , since J fl is a p-cover. Step 3.
Reference: [10] <author> D. Haussler, M. Kearns, N. Littlestone, M. Warmuth. </author> <title> Equivalence of models for polynomial learnability. </title> <booktitle> Proceedings of the 1988 Workshop on Computational Learning Theory, </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1988, </year> <pages> pp. 42-55, </pages> <institution> and University of California at Santa Cruz Information Sciences Department, </institution> <type> technical report number UCSC-CRL-88-06, </type> <year> 1988. </year>
Reference-contexts: Finally, we give a canonical reduction that allows many learning with errors problems to be studied as equivalent optimization problems, thus allowing one to sidestep some of the difficulties of analysis in the distribution-free model. Similar results are given for the error-free model by Haussler et al. <ref> [10] </ref>. We now give a brief survey of other studies of error in the distribution-free model. Valiant [24] modified his initial definitions of learnability to include the presence of errors in the examples. <p> We choose to explicitly separate the distributions over the positive and negative examples to facilitate the study of algorithms that learn using only positive examples or only negative examples. These models, however, are equivalent with respect to polynomial-time computation, as is shown by Haussler et al. <ref> [10] </ref>. Given a fixed target representation c 2 C, and given fixed target distributions D + c and D there is a natural measure of the error (with respect to c, D + c and D c ) of a representation h from a representation class H . <p> This technique has also been noted in the error-free setting by Haussler et al. <ref> [10] </ref>. 20 It is shown by Ehrenfeucht et al. [8] that any learning algorithm A for a representation class C must have sample complexity S A (*; ffi) = 1 1 + vcd (C) : Suppose that a learning algorithm A achieves this optimal sample complexity.
Reference: [11] <author> D. Johnson. </author> <title> Approximation algorithms for combinatorial problems. </title> <journal> Journal of Computer and Systems Sciences, </journal> <volume> 9, </volume> <year> 1974, </year> <pages> pp. 256-276. </pages>
Reference-contexts: We give a reduction from learning monomials with errors to a generalization of the weighted set cover problem, and give an approximation algorithm for 2 this problem (generalizing the greedy algorithm analyzed by several authors <ref> [7, 11, 18] </ref>) that is of independent interest. This approximation algorithm is used as a subroutine in a learning algorithm that tolerates an improved error rate for monomials. <p> This suggests that there are classes for which the optimal error rate that can be tolerated efficiently may be considerably smaller than the optimal information-theoretic rate. The best approximation known for the set cover problem remains the greedy algorithm analyzed by Chvatal [7], Johnson <ref> [11] </ref>, Lovasz [17], and Nigmatullin [18]. Finally, we give a canonical reduction that allows many learning with errors problems to be studied as equivalent optimization problems, thus allowing one to sidestep some of the difficulties of analysis in the distribution-free model. <p> time algorithm A for M n (using hypothesis space M n ) tolerating E poly MAL (M n ; A) *=r (n) would imply a significant breakthrough in approximation algorithms for set cover, since the best algorithm for this problem remains the 30 greedy method analyzed by Chvatal and others <ref> [7, 11, 17, 18] </ref>. Note that the proof of Theorem 19 in fact shows the result holds for the class of monotone monomials.
Reference: [12] <author> M. Kearns, M. Li. </author> <title> Learning in the presence of malicious errors. </title> <booktitle> Proceedings of the 20th A.C.M. Symposium on the Theory of Computing, </booktitle> <year> 1988, </year> <pages> pp. 267-280. </pages>
Reference: [13] <author> M. Kearns, M. Li, L. Pitt, L.G. Valiant. </author> <title> On the learnability of Boolean formulae. </title> <booktitle> Proceedings of the 19th A.C.M. Symposium on the Theory of Computing, </booktitle> <year> 1987, </year> <pages> pp. 285-295. </pages>
Reference-contexts: As examples, we state such results as corollaries for a few of the representation classes studied here. Even in cases where the representation class is known to be not learnable from only positive or only negative examples in polynomial time (for example, it is shown in Kearns et al. <ref> [13] </ref> that monomials are not polynomially learnable from negative examples), the bounds on E MAL;+ and E MAL; are relevant since they also hold for algorithms that do not run in polynomial time. <p> both positive and negative examples may be dramatic: for kCNF we have E poly CN (kCNF n ) c 0 for any c 0 &lt; 1=2 due to Angluin and Laird [1] but E CN ;+ (kCNF n ) = O (*=n k ) by Theorem 2. (Kearns et al. <ref> [13] </ref> show that kCNF is not learnable in polynomial time from negative examples even in the error-free model.) In fact, we can give a bound on E CN;+ and E CN; that is weaker but more general, and applies to almost any representation class. <p> Note that the class of monomials (respectively, kDNF) is not polynomially learnable even in the error-free case from negative (respectively, positive) examples <ref> [13] </ref> Combining Corollaries 8 and 9 with Corollaries 3 and 4 we have E poly MAL;+ (M n ) = fi (*=n) and poly MAL; (kDNF n ) = fi (*=n k ), thus proving that the algorithms of Valiant [24] tolerate the optimal malicious error rate with respect to positive-only <p> We may decide at run time which algorithm will tolerate the larger error rate, thus giving E poly MAL (M s * ln (1 + n s ln (1 + n ; s log s log n !! By using transformation techniques similar to those described Kearns et al. <ref> [13] </ref> it can be shown that the algorithm of Theorem 18 (as well as that obtained from Theorem 12) can be used to obtain an improvement in the error rate over the negative-only algorithm of Valiant [24] for the class kDNF n;s of kDNF formulae with at most s terms.
Reference: [14] <author> M. Kearns, L. Pitt. </author> <title> A polynomial-time algorithm for learning k-variable pattern languages from examples. </title> <booktitle> Proceedings of the 1989 Workshop on Computational Learning Theory, </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1989, </year> <pages> pp. 57-71. 33 </pages>
Reference-contexts: This approximation algorithm is of independent interest and has found application in other learning algorithms <ref> [14, 26] </ref>. Our analysis and notation rely heavily on the work of Chvatal [7]; the reader may find it helpful to read his paper first.
Reference: [15] <author> P. Laird. </author> <title> Learning from good and bad data. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1988. </year>
Reference-contexts: Their results demonstrate that under stronger assumptions on the nature of the errors, large rates of error can be tolerated by polynomial-time algorithms for nontrivial representation classes. Shackelford and Volper [21] investigate a model of random noise in the instances rather than the labels, and Sloan [22] and Laird <ref> [15] </ref> discuss a number of variants of both the malicious error and classification noise models. 2 Definitions for Distribution-free Learning In this section we give definitions and motivation for the model of machine learning we study.
Reference: [16] <author> N. Littlestone. </author> <title> Learning quickly when irrelevant attributes abound: a new linear threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2(4), </volume> <year> 1988, </year> <pages> pp. 245-318, </pages> <booktitle> and Proceedings of the 28th I.E.E.E. Symposium on the Foundations of Computer Science, </booktitle> <year> 1987, </year> <pages> pp. 68-77. </pages>
Reference-contexts: Our algorithm will tolerate a larger rate of error when the number s of relevant attributes is considerably smaller than the total number of variables n. Other improvements in the performance of learning algorithms in the presence of many irrelevant attributes are investigated by Littlestone <ref> [16] </ref> and Blum [3]. We note that by applying Theorem 2 we can show that even for M 1 n , the class of monomials of length 1, the positive-only and negative-only malicious error rates are bounded by *=(n 1). <p> As an example, if s = p n then Theorem 18 gives E poly MAL (M n * n log n ) as opposed to the the bound of (*=n ln *=n) of Theorem 13. Littlestone <ref> [16] </ref> shows that the Vapnik-Chervonenkis dimension of M s n is fi (s ln (1 + n=s)).
Reference: [17] <author> L. Lovasz. </author> <title> On the ratio of optimal integral and fractional covers. </title> <journal> Discrete Math, </journal> <volume> 13, </volume> <year> 1975, </year> <pages> pp. 383-390. </pages>
Reference-contexts: This suggests that there are classes for which the optimal error rate that can be tolerated efficiently may be considerably smaller than the optimal information-theoretic rate. The best approximation known for the set cover problem remains the greedy algorithm analyzed by Chvatal [7], Johnson [11], Lovasz <ref> [17] </ref>, and Nigmatullin [18]. Finally, we give a canonical reduction that allows many learning with errors problems to be studied as equivalent optimization problems, thus allowing one to sidestep some of the difficulties of analysis in the distribution-free model. <p> time algorithm A for M n (using hypothesis space M n ) tolerating E poly MAL (M n ; A) *=r (n) would imply a significant breakthrough in approximation algorithms for set cover, since the best algorithm for this problem remains the 30 greedy method analyzed by Chvatal and others <ref> [7, 11, 17, 18] </ref>. Note that the proof of Theorem 19 in fact shows the result holds for the class of monotone monomials.
Reference: [18] <author> R. Nigmatullin. </author> <title> The fastest descent method for covering problems. Proceedings of a Symposium on Questions of Precision and Efficiency of Computer Algorithms, </title> <booktitle> Kiev, 1969 (in Russian). </booktitle>
Reference-contexts: We give a reduction from learning monomials with errors to a generalization of the weighted set cover problem, and give an approximation algorithm for 2 this problem (generalizing the greedy algorithm analyzed by several authors <ref> [7, 11, 18] </ref>) that is of independent interest. This approximation algorithm is used as a subroutine in a learning algorithm that tolerates an improved error rate for monomials. <p> This suggests that there are classes for which the optimal error rate that can be tolerated efficiently may be considerably smaller than the optimal information-theoretic rate. The best approximation known for the set cover problem remains the greedy algorithm analyzed by Chvatal [7], Johnson [11], Lovasz [17], and Nigmatullin <ref> [18] </ref>. Finally, we give a canonical reduction that allows many learning with errors problems to be studied as equivalent optimization problems, thus allowing one to sidestep some of the difficulties of analysis in the distribution-free model. Similar results are given for the error-free model by Haussler et al. [10]. <p> time algorithm A for M n (using hypothesis space M n ) tolerating E poly MAL (M n ; A) *=r (n) would imply a significant breakthrough in approximation algorithms for set cover, since the best algorithm for this problem remains the 30 greedy method analyzed by Chvatal and others <ref> [7, 11, 17, 18] </ref>. Note that the proof of Theorem 19 in fact shows the result holds for the class of monotone monomials.
Reference: [19] <author> R. Rivest. </author> <title> Learning decision lists. </title> <journal> Machine Learning, </journal> <volume> 2(3), </volume> <year> 1987, </year> <pages> pp. 229-246. </pages>
Reference-contexts: We denote by SF n the class of all such representations. Decision Lists: A decision list <ref> [19] </ref> is a list L = &lt; (T 1 ; b 1 ); : : : ; (T l ; b l ) &gt;, where each T i is a monomial over the Boolean variables x 1 ; : : :; x n and each b i 2 f0; 1g. <p> Similarly, a slight improvement over the malicious error rate given in Theorem 9 for kDNF can also be shown. For decision lists, we can apply the algorithm of Rivest <ref> [19] </ref> and the sample size bounds given by Ehrenfeucht et al. [8] to obtain the following: Corollary 14 Let kDL n be the class of k-decision lists over x 1 ; : : :; x n .
Reference: [20] <author> R. Schapire. </author> <title> On the strength of weak learnability. </title> <booktitle> Proceedings of the 30th I.E.E.E. Symposium on the Foundations of Computer Science, </booktitle> <year> 1989, </year> <pages> pp. 28-33. </pages>
Reference-contexts: Note that a class being polynomially evaluatable simply means that it contains only "small" representations, that is, representations that can be written down in polynomial time. All representation classes considered here are polynomially evaluatable. It is worth mentioning at this point that Schapire <ref> [20] </ref> has shown that if a representation class is not polynomially evaluatable, then it is not efficiently learnable in our model. Thus, perhaps not surprisingly we see that classes that are not polynomially evaluatable constitute "unfair" learning problems. Samples.
Reference: [21] <author> G. Shackelford, D. Volper. </author> <title> Learning k-DNF with noise in the attributes. </title> <booktitle> Proceedings of the 1988 Workshop on Computational Learning Theory, </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1988, </year> <pages> pp. 97-105. </pages>
Reference-contexts: Their results demonstrate that under stronger assumptions on the nature of the errors, large rates of error can be tolerated by polynomial-time algorithms for nontrivial representation classes. Shackelford and Volper <ref> [21] </ref> investigate a model of random noise in the instances rather than the labels, and Sloan [22] and Laird [15] discuss a number of variants of both the malicious error and classification noise models. 2 Definitions for Distribution-free Learning In this section we give definitions and motivation for the model of
Reference: [22] <author> R. Sloan. </author> <title> Types of noise in data for concept learning. </title> <booktitle> Proceedings of the 1988 Workshop on Computational Learning Theory, </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1988, </year> <pages> pp. 91-96. </pages>
Reference-contexts: Their results demonstrate that under stronger assumptions on the nature of the errors, large rates of error can be tolerated by polynomial-time algorithms for nontrivial representation classes. Shackelford and Volper [21] investigate a model of random noise in the instances rather than the labels, and Sloan <ref> [22] </ref> and Laird [15] discuss a number of variants of both the malicious error and classification noise models. 2 Definitions for Distribution-free Learning In this section we give definitions and motivation for the model of machine learning we study.
Reference: [23] <author> L.G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the A.C.M., </journal> <volume> 27(11), </volume> <year> 1984, </year> <pages> pp. 1134-1142. </pages>
Reference-contexts: Another theme is the importance of using both positive and negative examples whenever errors (either malicious errors or classification noise errors) are present. Several existing learning algorithms use only positive examples or only negative examples (see e.g. Valiant <ref> [23] </ref> and Blumer et al. [5]). We demonstrate strong upper bounds on the tolerable error rate when only one type is used, and show that this rate can be provably increased when both types are used. <p> This model was first defined by Valiant <ref> [23] </ref> in 1984; he then went on to generalize his definitions to allow errors in 1985 [24]. <p> Then applying Theorem 12, we immediately obtain an algorithm for C that tolerates a malicious error rate of * ln * : This rate is also the best that can be obtained by applying Theorem 12. By applying this technique to the algorithm of Valiant <ref> [23] </ref> for the class of monomials in the error-free model, we obtain the following corollary: Corollary 13 Let M n be the class of monomials over x 1 ; : : : ; x n . <p> Littlestone [16] shows that the Vapnik-Chervonenkis dimension of M s n is fi (s ln (1 + n=s)). Since the algorithm of Valiant <ref> [23] </ref> can be modified to have optimal sample complexity for M s n , by applying Theorem 12 to this modified algorithm we obtain E poly MAL (M s * ln (1 + n s ln (1 + n ! This lower bound on E poly MAL (M s n )
Reference: [24] <author> L.G. Valiant. </author> <title> Learning disjunctions of conjunctions. </title> <booktitle> Proceedings of the 9th International Joint Conference on Artificial Intelligence, </booktitle> <year> 1985, </year> <pages> pp. 560-566. </pages>
Reference-contexts: Thus, we study a worst-case or malicious model of errors, in which the errors are generated by an adversary whose goal is to foil the learning algorithm. The study of learning from examples with malicious errors was initiated by Valiant <ref> [24] </ref>, where it is assumed that there is a fixed probability fi of an error occurring independently on each request for an example. <p> We show that there are representation classes for which the optimal malicious error rate can be achieved by algorithms that run in polynomial time and are easily coded. For example, we show that a polynomial-time algorithm for learning monomials with errors due to Valiant <ref> [24] </ref> tolerates the largest malicious error rate possible for any algorithm that uses only positive examples, polynomial-time or otherwise. We give an efficient learning algorithm for the class of symmetric functions that tolerates the optimal malicious error rate and uses an optimal number of examples. <p> In addition to proving this for the class of symmetric functions, we give an efficient algorithm that provides a strict increase in the malicious error rate over the positive-only algorithm of Valiant <ref> [24] </ref> for the class of monomials. A third theme is that there are strong ties between learning with errors and more traditional problems in combinatorial optimization. <p> Similar results are given for the error-free model by Haussler et al. [10]. We now give a brief survey of other studies of error in the distribution-free model. Valiant <ref> [24] </ref> modified his initial definitions of learnability to include the presence of errors in the examples. He also gave a generalization of his algorithm for learning monomials from positive examples, and analyzed the rate of malicious error tolerated by this algorithm. <p> This model was first defined by Valiant [23] in 1984; he then went on to generalize his definitions to allow errors in 1985 <ref> [24] </ref>. In addition to the basic definitions and notation, we give the form of Chernoff bounds we use, define the Vapnik-Chervonenkis dimension, and define a number of classes of representations whose error-tolerant learnability we will study. 2.1 Representing subsets of a domain Concept classes and their representation. <p> Thus, in the presence of errors of any kind, finding an *=2-good hypothesis is as good as learning, provided that fi &lt; *=4. This fact can be used to prove the correctness of the learning algorithms of the following two theorems due to Valiant. 17 Theorem 8 (Valiant <ref> [24] </ref>) Let M n be the class of monomials over x 1 ; : : : ; x n . Then E poly MAL;+ (M n ) = * Theorem 9 (Valiant [24]) For fixed k, let kDNF n be the class of kDNF formulae over x 1 ; : : <p> the correctness of the learning algorithms of the following two theorems due to Valiant. 17 Theorem 8 (Valiant <ref> [24] </ref>) Let M n be the class of monomials over x 1 ; : : : ; x n . Then E poly MAL;+ (M n ) = * Theorem 9 (Valiant [24]) For fixed k, let kDNF n be the class of kDNF formulae over x 1 ; : : : ; x n . <p> even in the error-free case from negative (respectively, positive) examples [13] Combining Corollaries 8 and 9 with Corollaries 3 and 4 we have E poly MAL;+ (M n ) = fi (*=n) and poly MAL; (kDNF n ) = fi (*=n k ), thus proving that the algorithms of Valiant <ref> [24] </ref> tolerate the optimal malicious error rate with respect to positive-only and negative-only learning. The algorithm given in the following theorem, similar to those of Valiant [24], proves an analogous result for efficiently learning symmetric functions from only one type of examples in the presence of errors. <p> (M n ) = fi (*=n) and poly MAL; (kDNF n ) = fi (*=n k ), thus proving that the algorithms of Valiant <ref> [24] </ref> tolerate the optimal malicious error rate with respect to positive-only and negative-only learning. The algorithm given in the following theorem, similar to those of Valiant [24], proves an analogous result for efficiently learning symmetric functions from only one type of examples in the presence of errors. Theorem 10 Let SF n be the class of symmetric functions over x 1 ; : : : ; x n . <p> Then E poly MAL (M n ) = * ln * : This improves the malicious error rate tolerated by the polynomial-time algorithm of Valiant <ref> [24] </ref> in Theorem 8 by a logarithmic factor. Furthermore, since E poly MAL;+ (M ) = fi (*=n) this proves that, as in the case of symmetric functions, using both oracles improves the tolerable error rate. <p> This is again an absolute bound, holding regardless of the computational complexity of the learning algorithm. Thus, the positive-only algorithm of Valiant <ref> [24] </ref> in Theorem 8 cannot exhibit an improved error rate when restricted to the subclass M s n for any value of s. 21 Our error-tolerant learning algorithm for monomials is based on an approximation algorithm for a generalization of the set cover problem that we call the partial cover problem, <p> log n !! By using transformation techniques similar to those described Kearns et al. [13] it can be shown that the algorithm of Theorem 18 (as well as that obtained from Theorem 12) can be used to obtain an improvement in the error rate over the negative-only algorithm of Valiant <ref> [24] </ref> for the class kDNF n;s of kDNF formulae with at most s terms.
Reference: [25] <author> V.N. Vapnik, A.Ya. Chervonenkis. </author> <title> On the uniform convergence of relative frequencies of events to their probabilities. </title> <journal> Theory of Probability and its Applications, </journal> <volume> 16(2), </volume> <year> 1971, </year> <pages> pp. 264-280. 34 </pages>
Reference-contexts: Then we define vcd (C) = maxfjY j : Y is shattered by Cg: 9 If this maximum does not exist, then vcd (C) is infinite. The Vapnik-Chervonenkis was originally introduced in the paper of Vapnik and Chervonenkis <ref> [25] </ref> and was first studied in the context of the distribution-free model by Blumer et al. [5]. Notational conventions. Let E (x) be an event and (x) a random variable that depend on a parameter x that takes on values in a set X.

References-found: 25

