URL: http://www.iscs.nus.sg/~rudys/nl.ps
Refering-URL: 
Root-URL: 
Email: Email:frudys,liuhg@iscs.nus.sg  
Title: NeuroLinear: From neural networks to oblique decision rules  
Author: Rudy Setiono and Huan Liu 
Keyword: Rule extraction, oblique-rule, pruning, discretization.  
Address: Ridge, Singapore 119260 Republic of Singapore  
Affiliation: Department of Information Systems and Computer Science National University of Singapore Kent  
Note: To appear in Neurocomputing  
Abstract: We present NeuroLinear, a system for extracting oblique decision rules from neural networks that have been trained for classification of patterns. Each condition of an oblique decision rule corresponds to a partition of the attribute space by a hyperplane that is not necessarily axis-parallel. Allowing a set of such hyperplanes to form the boundaries of the decision regions leads to a significant reduction in the number of rules generated while maintaining the accuracy rates of the networks. We describe the components of NeuroLinear in detail by way of two examples using artificial datasets. Our experimental results on real-world datasets show that the system is effective in extracting compact and comprehensible rules with high predictive accuracy from neural networks. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> K.P. Bennett and O.L. Mangasarian, </editor> <title> Neural network training via linear programming, in: P.M. </title> <editor> Pardalos, ed., </editor> <booktitle> Advances in Optimization and Parallel Computing, </booktitle> <publisher> (Elsevier Science Publishers B.V., </publisher> <address> Amsterdam, </address> <year> 1992) </year> <month> 56-67. </month>
Reference-contexts: Chi2: Discretization of Hidden Unit Activation Values The range of the activation values of the network's hidden units is the interval <ref> [1; 1] </ref>, since they have been computed as the hyperbolic tangent of the weighted inputs (cf. Eqn. 4). In order to extract rules from the network, it is necessary that these values be grouped into a few clusters while preserving the accuracy of the network. <p> A. Example 1 The patterns for the first example were generated randomly. Each patterns has two inputs (x 1 ; x 2 ), each of which is uniformly distributed in the interval <ref> [0; 1] </ref>. The target value t of each pattern is defined as follows (Fig. 1 (a)): If ((x 2 + 1:5 x 1 0:25) AND (x 2 x 1 )), then t = 1, Else t = 0. Two thousand patterns were generated for the experiment as the training data. <p> The 3rd input served as hidden unit threshold, its values for all patterns were 1. All initial weights of these networks were randomly and uniformly generated in the interval <ref> [1; 1] </ref>. After the network training reached a local minimum solution, the pruning process was initiated. Pruning was terminated as soon as the accuracy of the network dropped below 98 %. The smallest network with at least 98% accuracy on the training data was saved for possible rule extraction. <p> Hidden unit Subintervals 1 <ref> [1; 0:0); [0:0; 1] </ref> 3 [1; 1] 99.60 % and it has only 9 connections. the pruning algorithm is shown by the small number of connections left in the networks. <p> Hidden unit Subintervals 1 [1; 0:0); <ref> [0:0; 1] </ref> 3 [1; 1] 99.60 % and it has only 9 connections. the pruning algorithm is shown by the small number of connections left in the networks. <p> Hidden unit Subintervals 1 [1; 0:0); [0:0; 1] 3 <ref> [1; 1] </ref> 99.60 % and it has only 9 connections. the pruning algorithm is shown by the small number of connections left in the networks. <p> One of the pruned networks was selected for generation of piecewise-linear separators for the patterns. This network is shown in Fig. 2. The predicted network outputs, the activation of hidden units 1 and 2 for points in the square <ref> [0; 1] </ref> fi [0; 1] are shown in Fig. 1 (b-d). The Chi2 algorithm was applied to the activation values of hidden units 1 and 2. <p> One of the pruned networks was selected for generation of piecewise-linear separators for the patterns. This network is shown in Fig. 2. The predicted network outputs, the activation of hidden units 1 and 2 for points in the square <ref> [0; 1] </ref> fi [0; 1] are shown in Fig. 1 (b-d). The Chi2 algorithm was applied to the activation values of hidden units 1 and 2. <p> The results of Chi2 on hidden units 1 and 2 and the decision regions obtained were as follows: 1. Hidden unit 1. There were 2 clusters found. All activation values in the interval [1; 0) formed the first cluster and all values in the interval <ref> [0; 1] </ref> formed the second cluster. Referring to Fig.2, the activation values at hidden unit 1 are determined by 12 the two inputs, x 1 and x 2 . The weights of the two connections from the input units are -16.69 and 16.67, respectively. <p> Hidden unit 2. There were also 2 clusters found. All activation values in the interval [1; 0:09) formed the first cluster and all values in the interval <ref> [:09; 1] </ref> formed the second cluster. The common boundary of these two intervals is -0.09. Two inputs and a bias value determine the activation values of this hidden unit. The weights are 17.93, -11.90 and 2.85. <p> Experimental evidence can be found in [7] (Fig. 6). B. Example 2 (left) and the output of a neural network trained with 2000 patterns (right). The patterns in this example also have 2 inputs. The values for these inputs were generated randomly and uniformly in the interval <ref> [0; 1] </ref>. The training and testing set each consisted of 2000 patterns. <p> Hidden unit Subintervals 1 <ref> [1; 0:09); [0:09; 1] </ref> 3 [1; 0:98); [0:98; 1] 5 [1; 0:69); [0:69; 1] 4 Experimental results Table 5 Datasets used in the experiments. <p> Hidden unit Subintervals 1 [1; 0:09); <ref> [0:09; 1] </ref> 3 [1; 0:98); [0:98; 1] 5 [1; 0:69); [0:69; 1] 4 Experimental results Table 5 Datasets used in the experiments. <p> Hidden unit Subintervals 1 [1; 0:09); [0:09; 1] 3 <ref> [1; 0:98); [0:98; 1] </ref> 5 [1; 0:69); [0:69; 1] 4 Experimental results Table 5 Datasets used in the experiments. <p> Hidden unit Subintervals 1 [1; 0:09); [0:09; 1] 3 [1; 0:98); <ref> [0:98; 1] </ref> 5 [1; 0:69); [0:69; 1] 4 Experimental results Table 5 Datasets used in the experiments. <p> Hidden unit Subintervals 1 [1; 0:09); [0:09; 1] 3 [1; 0:98); [0:98; 1] 5 <ref> [1; 0:69); [0:69; 1] </ref> 4 Experimental results Table 5 Datasets used in the experiments. <p> Hidden unit Subintervals 1 [1; 0:09); [0:09; 1] 3 [1; 0:98); [0:98; 1] 5 [1; 0:69); <ref> [0:69; 1] </ref> 4 Experimental results Table 5 Datasets used in the experiments. <p> The datasets and the characteristics of their attributes are given in Table 5. Following Towell and Shavlik [21], for each dataset, ten repetitions of ten-fold cross validation were performed using NeuroLinear. Each neural network was given a set of initial weights randomly generated in the interval <ref> [1; 1] </ref>. For all networks, the following 17 Table 6 Accuracy rates (%) of C4.5rules and Neurolinear. <p> We describe next in detail how the rules are extracted by NeuroLinear on two datasets. A. Detailed analysis 1: The University of Wisconsin Breast Cancer Dataset. This data set has been used as the test data for several studies on pattern classification methods using linear programming techniques <ref> [1, 13] </ref> and statistical techniques [23]. Each pattern is described by nine attributes. The nine measurements taken from fine needle aspirates from human breast tissues correspond to cytological characteristics of a benign or of a malignant pattern. They are summarized in Table 8. <p> Due to their ordinal nature, we treated the attribute values as continuous. The values were normalized such that they ranged in the interval <ref> [0; 1] </ref>. The training set consisted of 350 randomly selected patterns, 121 of which are malignant patterns and the remaining 229 benign patterns. The testing set consisted of 120 malignant patterns and 229 benign patterns. <p> The activation values of all 343 patterns that were correctly classified by the pruned network were discretized by Chi2. It found 4 clusters in the first hidden unit and 3 clusters in the second hidden unit. The subintervals that define the 4 clusters of the first hidden unit are <ref> [1; 0:804); [0:804; 0:988); [0:988; 0:998); [0:998; 1] </ref>. The subintervals at the second hidden unit are [1; 0:376); [0:376; 0:602); [0:602; 1]. A new data set with 2 columns of discrete values was generated. <p> It found 4 clusters in the first hidden unit and 3 clusters in the second hidden unit. The subintervals that define the 4 clusters of the first hidden unit are [1; 0:804); <ref> [0:804; 0:988); [0:988; 0:998); [0:998; 1] </ref>. The subintervals at the second hidden unit are [1; 0:376); [0:376; 0:602); [0:602; 1]. A new data set with 2 columns of discrete values was generated. <p> It found 4 clusters in the first hidden unit and 3 clusters in the second hidden unit. The subintervals that define the 4 clusters of the first hidden unit are [1; 0:804); [0:804; 0:988); <ref> [0:988; 0:998); [0:998; 1] </ref>. The subintervals at the second hidden unit are [1; 0:376); [0:376; 0:602); [0:602; 1]. A new data set with 2 columns of discrete values was generated. <p> It found 4 clusters in the first hidden unit and 3 clusters in the second hidden unit. The subintervals that define the 4 clusters of the first hidden unit are [1; 0:804); [0:804; 0:988); [0:988; 0:998); <ref> [0:998; 1] </ref>. The subintervals at the second hidden unit are [1; 0:376); [0:376; 0:602); [0:602; 1]. A new data set with 2 columns of discrete values was generated. <p> It found 4 clusters in the first hidden unit and 3 clusters in the second hidden unit. The subintervals that define the 4 clusters of the first hidden unit are [1; 0:804); [0:804; 0:988); [0:988; 0:998); [0:998; 1]. The subintervals at the second hidden unit are <ref> [1; 0:376); [0:376; 0:602); [0:602; 1] </ref>. A new data set with 2 columns of discrete values was generated. All patterns with discretized activation values at hidden unit i located in the j-th subinterval were given a value equal to j in their i-th column. <p> It found 4 clusters in the first hidden unit and 3 clusters in the second hidden unit. The subintervals that define the 4 clusters of the first hidden unit are [1; 0:804); [0:804; 0:988); [0:988; 0:998); [0:998; 1]. The subintervals at the second hidden unit are [1; 0:376); <ref> [0:376; 0:602); [0:602; 1] </ref>. A new data set with 2 columns of discrete values was generated. All patterns with discretized activation values at hidden unit i located in the j-th subinterval were given a value equal to j in their i-th column. <p> The subintervals that define the 4 clusters of the first hidden unit are [1; 0:804); [0:804; 0:988); [0:988; 0:998); [0:998; 1]. The subintervals at the second hidden unit are [1; 0:376); [0:376; 0:602); <ref> [0:602; 1] </ref>. A new data set with 2 columns of discrete values was generated. All patterns with discretized activation values at hidden unit i located in the j-th subinterval were given a value equal to j in their i-th column. <p> An activation value of a pattern will be in this interval if and only if the weighted sum of its input is 20 less than tanh 1 (0:804) = 1:110. Similarly, the second hidden unit activation values of a pattern will fall in the third subinterval <ref> [0:602; 1] </ref> if and only if its weighted sum is greater than or equal to tanh 1 (0:602) = 0:696. <p> The network achieves 96.86 % accuracy rate on the training set and has only 5 connections. Hidden unit Subintervals 1 <ref> [1; 0:273); [0:273; 0:404); [0:404; 0:520); [0:520; 1] </ref> Rules that classify a pattern to be either benign or malignant are first generated in terms of its discretized hidden unit activation values. <p> The network achieves 96.86 % accuracy rate on the training set and has only 5 connections. Hidden unit Subintervals 1 [1; 0:273); <ref> [0:273; 0:404); [0:404; 0:520); [0:520; 1] </ref> Rules that classify a pattern to be either benign or malignant are first generated in terms of its discretized hidden unit activation values. <p> The network achieves 96.86 % accuracy rate on the training set and has only 5 connections. Hidden unit Subintervals 1 [1; 0:273); [0:273; 0:404); <ref> [0:404; 0:520); [0:520; 1] </ref> Rules that classify a pattern to be either benign or malignant are first generated in terms of its discretized hidden unit activation values. <p> The network achieves 96.86 % accuracy rate on the training set and has only 5 connections. Hidden unit Subintervals 1 [1; 0:273); [0:273; 0:404); [0:404; 0:520); <ref> [0:520; 1] </ref> Rules that classify a pattern to be either benign or malignant are first generated in terms of its discretized hidden unit activation values. <p> The attributes are summarized in Table 10. As can be seen from the table, one network input unit is assigned to each continuous attribute. Values of the continuous 23 attributes are normalized such that their range is <ref> [0; 1] </ref>. Each value of a discrete attribute with n possible values is represented by an n-bit binary coding, except when n = 2 where only 1 bit is used. With an addition of 1 input unit to represent hidden unit bias values, the number of input units was 26. <p> A network with 4 hidden units was trained and pruned with a minimum accuracy on the training set of 85 %. This figure for minimum accuracy was based on reported experimental results on this dataset <ref> [1] </ref>. The pruned network had only 2 hidden units and 13 connections left. Seven inputs I 1 ; I 5 ; I 7 ; I 9 ; I 13 ; I 14 and I 16 were connected to the first hidden unit. <p> The results are as follows: 1. Hidden unit 1. There were 3 subintervals, <ref> [1; 0:58); [0:58; 0:86) and [0:86; 1] </ref>. 2. Hidden unit 2. There were also 3 subintervals, [1; 0:54); [0:54; 0:24) and [0:24; 1]. Of the 9 possible combinations of the discretized activation values, 4 have target equal to 1, while the remaining 5 have target value equal to 0. <p> The results are as follows: 1. Hidden unit 1. There were 3 subintervals, [1; 0:58); <ref> [0:58; 0:86) and [0:86; 1] </ref>. 2. Hidden unit 2. There were also 3 subintervals, [1; 0:54); [0:54; 0:24) and [0:24; 1]. Of the 9 possible combinations of the discretized activation values, 4 have target equal to 1, while the remaining 5 have target value equal to 0. <p> The results are as follows: 1. Hidden unit 1. There were 3 subintervals, [1; 0:58); [0:58; 0:86) and <ref> [0:86; 1] </ref>. 2. Hidden unit 2. There were also 3 subintervals, [1; 0:54); [0:54; 0:24) and [0:24; 1]. Of the 9 possible combinations of the discretized activation values, 4 have target equal to 1, while the remaining 5 have target value equal to 0. They are summarized in Table 11. <p> The results are as follows: 1. Hidden unit 1. There were 3 subintervals, [1; 0:58); [0:58; 0:86) and [0:86; 1]. 2. Hidden unit 2. There were also 3 subintervals, <ref> [1; 0:54); [0:54; 0:24) and [0:24; 1] </ref>. Of the 9 possible combinations of the discretized activation values, 4 have target equal to 1, while the remaining 5 have target value equal to 0. They are summarized in Table 11. <p> The results are as follows: 1. Hidden unit 1. There were 3 subintervals, [1; 0:58); [0:58; 0:86) and [0:86; 1]. 2. Hidden unit 2. There were also 3 subintervals, [1; 0:54); <ref> [0:54; 0:24) and [0:24; 1] </ref>. Of the 9 possible combinations of the discretized activation values, 4 have target equal to 1, while the remaining 5 have target value equal to 0. They are summarized in Table 11. <p> The results are as follows: 1. Hidden unit 1. There were 3 subintervals, [1; 0:58); [0:58; 0:86) and [0:86; 1]. 2. Hidden unit 2. There were also 3 subintervals, [1; 0:54); [0:54; 0:24) and <ref> [0:24; 1] </ref>. Of the 9 possible combinations of the discretized activation values, 4 have target equal to 1, while the remaining 5 have target value equal to 0. They are summarized in Table 11.
Reference: [2] <author> L. Breiman, J.H. Friedman, R.A. Olshen and C.J. Stone, </author> <title> Classification and Regression Trees (Wadsworth & Brooks/Cole Advanced Books & Software, </title> <address> Belmont, CA, </address> <year> 1984). </year>
Reference-contexts: Classification rules are obtained by merging the rules from these two steps. Unlike the decision tree algorithms <ref> [15, 2] </ref> which consider smaller and smaller subsets of the data to improve the accuracy of the rules, the neural network approach for rule generation considers the entire training set as a whole.
Reference: [3] <author> T.G. Dietterich, H. Hild and G. Bakiri, </author> <title> A comparative study of ID3 and backpropagation for english text-to-speech mapping, </title> <booktitle> in Machine Learning: Proceedings of the Seventh International Conference, </booktitle> <address> Austin, Texas (1990) </address>
Reference-contexts: 1 Introduction Neural networks have been widely applied to solve classification problems. Comparisons between neural networks and decision trees algorithms for these problems have shown that in general neural networks can produce better accuracy rates <ref> [3, 4, 16, 20] </ref>. Recent developments in algorithms that extract rules from neural networks have made neural network techniques even more attractive. The extracted rules allow one to explain the decision process of a neural network.
Reference: [4] <author> D.H. Fisher and K.B. McKusick. </author> <title> "An empirical comparison of ID3 and back-propagation," </title> <booktitle> in Proceedings of 11th Int. Joint Conf. on AI, </booktitle> <year> (1989) </year> <month> 788-793. </month>
Reference-contexts: 1 Introduction Neural networks have been widely applied to solve classification problems. Comparisons between neural networks and decision trees algorithms for these problems have shown that in general neural networks can produce better accuracy rates <ref> [3, 4, 16, 20] </ref>. Recent developments in algorithms that extract rules from neural networks have made neural network techniques even more attractive. The extracted rules allow one to explain the decision process of a neural network.
Reference: [5] <author> L. Fu, </author> <title> Rule learning by searching on adapted nets, </title> <booktitle> in Proc. of the Ninth National Conference on Artificial Intelligence, </booktitle> <year> (1991) </year> <month> 590-595. </month>
Reference-contexts: These systems, however, do not actually extract rules from the networks; instead, they try to generate an explanation for each particular outcome of the networks. The KT algorithm developed by Fu <ref> [5] </ref> extracts rules from a trained network. It searches for subsets of connections to a network's unit with summed weight exceeding the bias of that unit. It is assumed that the unit's activation value is close to either 0 or 1.
Reference: [6] <author> S. Gallant, </author> <title> Connectionist expert systems, </title> <journal> Comm. of the ACM, </journal> <note> 31 (2) (1988) 152-169. </note>
Reference-contexts: The extracted rules allow one to explain the decision process of a neural network. It is not surprising that in the past few years great efforts 1 have been devoted to finding effective algorithms for extracting rules from a trained neural network. Gallant's connectionist expert systems <ref> [6] </ref> and Saito and Nakano's RN method [17] are two early works that attempt to generate rules from neural networks. These systems, however, do not actually extract rules from the networks; instead, they try to generate an explanation for each particular outcome of the networks.
Reference: [7] <author> H. Guo and S.B. Gelfand, </author> <title> Classification trees with neural network feature extraction, </title> <journal> IEEE Trans. on Neural Networks, </journal> <note> 3 (6) (1992) 923-933. </note>
Reference-contexts: For this kind of problems, intuitively, DT methods generate trees of large size proportional to the sample size. Experimental evidence can be found in <ref> [7] </ref> (Fig. 6). B. Example 2 (left) and the output of a neural network trained with 2000 patterns (right). The patterns in this example also have 2 inputs. The values for these inputs were generated randomly and uniformly in the interval [0; 1].
Reference: [8] <author> J. Hertz, A. Krogh and R.G. Palmer, </author> <title> Introduction to the theory of neural computation, </title> <publisher> (Addison Wesley, </publisher> <address> Redwood City, CA, </address> <year> 1991). </year>
Reference-contexts: following function: (w; v) = F (w; v) + P (w; v); where F (w; v) is the cross entropy function [22]: F (w; v) = i=1 p=1 t i p + (1 t i p ) : and P (w; v) is a penalty term used for weight decay <ref> [8] </ref>: P (w; v) = * 1 @ m=1 `=1 ` ) 2 ` ) 2 + m=1 p=1 p ) 2 p ) 2 A + * 2 @ m=1 `=1 ` ) 2 + m=1 p=1 v m 2 A ; (6) where * 1 ; * 2 ,
Reference: [9] <author> E. D. Karnin, </author> <title> A simple procedure for pruning back-propagation trained neural networks, </title> <journal> IEEE Trans. </journal> <note> on Neural Networks 1 (2) (1990) 239-242. </note>
Reference-contexts: One of the advantages of having a neural network with only the relevant connections is that the behavior of the net can be explained by a simple set of rules <ref> [9] </ref>. Our pruning algorithn removes connections in the network based on their magnitude. The details of this algorithm and the experimental results on a number of well known classification problems are given in [19].
Reference: [10] <author> R. Kerber, ChiMerge: </author> <title> Discretization of numeric attributes, </title> <booktitle> in The Proc. of the Ninth National Conference on AI, </booktitle> <publisher> (AAAI Press/The MIT Press, </publisher> <year> 1992) </year> <month> 123-128. </month>
Reference-contexts: Eqn. 4). In order to extract rules from the network, it is necessary that these values be grouped into a few clusters while preserving the accuracy of the network. Chi2 [11], an improved and automated version of ChiMerge <ref> [10] </ref>, is the algorithm used for this purpose. Given a dataset where each pattern is described by the values of the continuous attributes A 1 ; A 2 ; : : : and the class label of the pattern is known, Chi2 finds discrete representations of the dataset.
Reference: [11] <author> H. Liu and R. Setiono, Chi2: </author> <title> Feature selection and discretization of numeric attributes, </title> <booktitle> in Proceedings of the 7th IEEE International Conference on Tools with Artificial Intelligence (1995) 388-391. </booktitle>
Reference-contexts: Eqn. 4). In order to extract rules from the network, it is necessary that these values be grouped into a few clusters while preserving the accuracy of the network. Chi2 <ref> [11] </ref>, an improved and automated version of ChiMerge [10], is the algorithm used for this purpose.
Reference: [12] <author> H. Liu and S.T. Tan, X2R: </author> <title> A fast rule generator, </title> <booktitle> in Proceedings of IEEE International Conference on Systems, Man and Cybernetics (IEEE Press, </booktitle> <year> 1995). </year> <month> 31 </month>
Reference-contexts: Second, rules that describe each discretized hidden unit activation values are obtained in terms of the original attributes of the dataset. For the first phase, we have implemented an efficient rule generator called X2R <ref> [12] </ref>. It generates a set of rules which cover all the data with an error rate not exceeding the inconsistency rate present in the data. It is particularly suitable for moderate sized datasets with discrete attribute values.
Reference: [13] <author> O.L. Mangasarian, R. Setiono and W.H. Wolberg, </author> <title> Pattern recognition via linear programming: theory and application to medical diagnosis, </title> <editor> in: T.F. Coleman and Y. Li, eds., </editor> <title> Large-scale Numerical Optimization, </title> <publisher> (SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1990) </year> <month> 22-31. </month>
Reference-contexts: We describe next in detail how the rules are extracted by NeuroLinear on two datasets. A. Detailed analysis 1: The University of Wisconsin Breast Cancer Dataset. This data set has been used as the test data for several studies on pattern classification methods using linear programming techniques <ref> [1, 13] </ref> and statistical techniques [23]. Each pattern is described by nine attributes. The nine measurements taken from fine needle aspirates from human breast tissues correspond to cytological characteristics of a benign or of a malignant pattern. They are summarized in Table 8.
Reference: [14] <author> P.M. Murphy and D.W. Aha, </author> <title> UCI repository of machine learning databases [machine-readable data repository], </title> <institution> Department of Information and Computer Science, University of California, Irvine, </institution> <year> 1992. </year>
Reference-contexts: Five datasets from the machine learning data repository at the University of California, Irvine <ref> [14] </ref> are used to compare the performance of NeuroLinear to that of C4.5rules [15]. C4.5rules is chosen because the code is widely available and it has been commonly used in the machine learning community. The datasets and the characteristics of their attributes are given in Table 5.
Reference: [15] <author> J.R. Quinlan. C4.5: </author> <title> Programs for Machine Learning, </title> <publisher> (Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993). </year>
Reference-contexts: The rules generation step is similar to Fu's KT algorithm. A simple rule extraction algorithm is presented by Setiono and Liu [18]. The rules extracted from neural networks are comparable to those generated by decision trees <ref> [15] </ref> in terms of accuracy and comprehensibility. The basic idea behind the algorithm is the fact that it is generally possible to replace the continuous activations of the hidden units by a small number of discrete ones. Rule extraction is realized in two steps. <p> Classification rules are obtained by merging the rules from these two steps. Unlike the decision tree algorithms <ref> [15, 2] </ref> which consider smaller and smaller subsets of the data to improve the accuracy of the rules, the neural network approach for rule generation considers the entire training set as a whole. <p> Section V analyzes the merits of generating a set of classification rules with NeuroLinear. It also highlights the differences between the rules generated from a network and those from the decision-tree method C4.5rules <ref> [15] </ref>. Section VI gives a brief conclusion of the paper. 2 Rule extraction with Neurolinear The steps of extracting oblique decision rules from a neural network are as follows: 1. Select and train a network to meet a prespecified accuracy requirement. <p> Five datasets from the machine learning data repository at the University of California, Irvine [14] are used to compare the performance of NeuroLinear to that of C4.5rules <ref> [15] </ref>. C4.5rules is chosen because the code is widely available and it has been commonly used in the machine learning community. The datasets and the characteristics of their attributes are given in Table 5.
Reference: [16] <author> J.R. Quinlan. </author> <title> Comparing connectionist and symbolic learning methods, </title> <editor> in: S.J. Hanson, G.A. Drastall, and R.L. Rivest, eds., </editor> <title> Computational Learning Theory and Natural Learning Systems (1) (A Bradford Book, </title> <publisher> The MIT Press, </publisher> <year> 1994) </year> <month> 445-456. </month>
Reference-contexts: 1 Introduction Neural networks have been widely applied to solve classification problems. Comparisons between neural networks and decision trees algorithms for these problems have shown that in general neural networks can produce better accuracy rates <ref> [3, 4, 16, 20] </ref>. Recent developments in algorithms that extract rules from neural networks have made neural network techniques even more attractive. The extracted rules allow one to explain the decision process of a neural network.
Reference: [17] <author> K. Saito and R. Nakano, </author> <title> Medical diagnosis expert system based on PDP model, </title> <booktitle> Proc. IEEE Intl. Conf. on Neural Networks (IEEE Press, </booktitle> <address> New York) (1988) I255-I266. </address>
Reference-contexts: It is not surprising that in the past few years great efforts 1 have been devoted to finding effective algorithms for extracting rules from a trained neural network. Gallant's connectionist expert systems [6] and Saito and Nakano's RN method <ref> [17] </ref> are two early works that attempt to generate rules from neural networks. These systems, however, do not actually extract rules from the networks; instead, they try to generate an explanation for each particular outcome of the networks.
Reference: [18] <author> R. Setiono and H. Liu, </author> <title> Symbolic representation of neural networks, </title> <note> IEEE Computer (March 1996) 71-77. </note>
Reference-contexts: The complexity of the network is further reduced by replacing the weights in all remaining clusters by the average weight of the individual cluster. The rules generation step is similar to Fu's KT algorithm. A simple rule extraction algorithm is presented by Setiono and Liu <ref> [18] </ref>. The rules extracted from neural networks are comparable to those generated by decision trees [15] in terms of accuracy and comprehensibility.
Reference: [19] <author> R. Setiono. </author> <title> A penalty function approach for pruning feedforward neural networks, </title> <booktitle> Neural Computation, </booktitle> <month> 9 (1) </month> <year> (1997) </year> <month> 185-204. </month>
Reference-contexts: Our pruning algorithn removes connections in the network based on their magnitude. The details of this algorithm and the experimental results on a number of well known classification problems are given in <ref> [19] </ref>. The effectiveness of the pruning algorithm is shown by the fact that for the many problems tested, the final pruned networks have only the relevant connections left regardless of the number of initial hidden units in the networks. 2.2.
Reference: [20] <author> J.W. Shavlik, R.J. Mooney and G.G. Towell, </author> <title> Symbolic and neural learning algorithms: An experimental comparison, </title> <booktitle> Machine Learning, </booktitle> <month> 6 (2) </month> <year> (1991) </year> <month> 111-143. </month>
Reference-contexts: 1 Introduction Neural networks have been widely applied to solve classification problems. Comparisons between neural networks and decision trees algorithms for these problems have shown that in general neural networks can produce better accuracy rates <ref> [3, 4, 16, 20] </ref>. Recent developments in algorithms that extract rules from neural networks have made neural network techniques even more attractive. The extracted rules allow one to explain the decision process of a neural network.
Reference: [21] <author> G.G. Towell and J.W. Shavlik, </author> <title> Extracting refined rules from knowledge-based neural networks, </title> <note> Machine Learning 13 (1) (1993) 71-101. </note>
Reference-contexts: By searching for the proper subsets of the input connections, sets of rules are generated to describe under what conditions the unit's activation will take one of the two values. The MofN algorithm of Towell and Shavlik <ref> [21] </ref> clusters the weights of the trained network into equivalence classes. Clusters that do not significantly affect the unit's activation are eliminated. The complexity of the network is further reduced by replacing the weights in all remaining clusters by the average weight of the individual cluster. <p> C4.5rules is chosen because the code is widely available and it has been commonly used in the machine learning community. The datasets and the characteristics of their attributes are given in Table 5. Following Towell and Shavlik <ref> [21] </ref>, for each dataset, ten repetitions of ten-fold cross validation were performed using NeuroLinear. Each neural network was given a set of initial weights randomly generated in the interval [1; 1]. For all networks, the following 17 Table 6 Accuracy rates (%) of C4.5rules and Neurolinear.
Reference: [22] <author> A. van Ooyen and B. Nienhuis, </author> <title> Improving the convergence of the backpropagation algorithm, </title> <booktitle> Neural Networks, </booktitle> <month> 5 </month> <year> (1992) </year> <month> 465-471. </month>
Reference-contexts: The backpropagation algorithm is applied to update the weights (w; v) and minimize the following function: (w; v) = F (w; v) + P (w; v); where F (w; v) is the cross entropy function <ref> [22] </ref>: F (w; v) = i=1 p=1 t i p + (1 t i p ) : and P (w; v) is a penalty term used for weight decay [8]: P (w; v) = * 1 @ m=1 `=1 ` ) 2 ` ) 2 + m=1 p=1 p ) 2
Reference: [23] <author> W.H. Wolberg, M.A. Tanner and W.Y. Loh, </author> <title> Diagnostic schemes for fine needle aspirates of breast masses, Analytical and Quantitative Cytology and Histology, </title> <note> 10 (1988) 225-228. 32 </note>
Reference-contexts: A. Detailed analysis 1: The University of Wisconsin Breast Cancer Dataset. This data set has been used as the test data for several studies on pattern classification methods using linear programming techniques [1, 13] and statistical techniques <ref> [23] </ref>. Each pattern is described by nine attributes. The nine measurements taken from fine needle aspirates from human breast tissues correspond to cytological characteristics of a benign or of a malignant pattern. They are summarized in Table 8. Table 8 The 9 attributes of the Wisconsin Breast Cancer dataset.
References-found: 23

