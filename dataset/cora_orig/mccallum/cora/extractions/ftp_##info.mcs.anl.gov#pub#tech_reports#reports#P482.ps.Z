URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P482.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts94.htm
Root-URL: http://www.mcs.anl.gov
Title: Tensor-Krylov Methods for Large Nonlinear Equations  
Author: Ali Bouaricha 
Keyword: Key words. tensor-Krylov methods, Newton-Krylov methods, nonlinear equations, sparse problems, ill-conditioned and singular problems  
Note: Part of this work was performed while the author was research associate  Research supported in part by the Office of Scientific Computing, U.S. Department of Energy, under Contract W-31-109-Eng-38.  
Address: Argonne, IL 60439, USA  
Affiliation: MCS Division, Argonne National Laboratory,  at CERFACS (Centre Europeen de Recherche et de Formation Avancee en Calcul Scientifique).  
Abstract: In this paper, we describe tensor methods for large systems of nonlinear equations based on Krylov subspace techniques for approximately solving the linear systems that are required in each tensor iteration. We refer to a method in this class as a tensor-Krylov algorithm. We describe comparative testing for a tensor-Krylov implementation versus an analogous implementation based on a Newton-Krylov method. The test results show that tensor-Krylov methods are much more efficient and robust than Newton-Krylov methods on hard nonlinear equations problems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Bouaricha. </author> <title> Solving large sparse systems of nonlinear equations and nonlinear least squares problems using tensor methods on sequential and parallel computers. </title> <type> Ph.D. thesis, </type> <institution> Computer Science Department, University of Colorado at Boulder, </institution> <year> 1992. </year>
Reference-contexts: Experimental results in [2], [3] have shown that the tensor method solves a considerable number of problems that the standard Newton method does not, and the reverse is virtually never true. The tensor methods for large, sparse nonlinear equations described in Bouaricha <ref> [1] </ref> use sparse direct methods for solving the linear systems of equations that arise in each tensor iteration. Unfortunately, for problems where the factorization of the Jacobian matrix is too expensive, the slow asymptotic performance and large storage requirements of direct methods make them 2 impractical. <p> Newton-Krylov methods for nonlinear equations have been studied by Brown and Saad ([5], [4]). They show that their methods are remarkably effective on large systems of partial differential equations. Their success has motivated us to use Krylov subspace algorithms in conjunction with the tensor methods developed in <ref> [1] </ref>. The tensor methods developed in [1] require at least p + 1 back solves, where p is the number of interpolated function values from previous iterations. Usually, the cost of these solves is relatively small compared with the cost of the Jacobian factorization. <p> They show that their methods are remarkably effective on large systems of partial differential equations. Their success has motivated us to use Krylov subspace algorithms in conjunction with the tensor methods developed in <ref> [1] </ref>. The tensor methods developed in [1] require at least p + 1 back solves, where p is the number of interpolated function values from previous iterations. Usually, the cost of these solves is relatively small compared with the cost of the Jacobian factorization. Unfortunately, this is not the case when iterative methods are used. <p> Unfortunately, this is not the case when iterative methods are used. If p = 1, say, then it appears that the cost of a tensor-Krylov iteration will be approximately twice that of a Newton-Krylov iteration. However, by reformulating the tensor step computation in <ref> [1] </ref>, we show in x5 that the cost of one of the two solves can be made cheaper, as a result of a good initial estimate of the solution. <p> The remainder of this paper is organized as follows. In x2 we review tensor methods for large, sparse nonlinear equations that were introduced in <ref> [1] </ref>. In x3 we review Krylov subspace methods and their main properties. In particular, we review the generalized minimum residual method (GMRES) [10], which will be used in conjunction with our tensor methods. In x4 we review the Newton-Krylov methods for large nonlinear equations that were described in [5]. <p> The total cost of this process is the factorization of the sparse matrix J, p + 1 back solves using this factorization, the unconstrained minimization of a function of p variables, and some lower-order (O (n)) costs. For further information we refer to <ref> [1] </ref>. 3. Krylov Subspace Methods The underlying idea of Krylov subspace methods when applied to a linear system of equations, Ax = b, is to generate an approximate solution to the original problem from the Krylov subspace Spanfb; Ab; ; A m1 bg. <p> The iteration limit is set to 500. We ran the tensor-Krylov method with p = 1. The reasons for this choice are that previous computational results obtained in <ref> [1] </ref> showed that the tensor method with p = 1 is generally about as effective as the tensor method that allows p 1, and that the method is considerably simpler and cheaper to implement in this case. <p> This is exactly the case in which the tensor method is intended to improve upon the performance of Newton's method due to the faster local convergence properties of the tensor method <ref> [1, 7] </ref>. Clearly, the tensor-GMRES method outperforms the Newton-GMRES method on this test problem for large values of in nonlinear iterations, function evaluations, and GMRES iterations. When the default Krylov subspace is used, both methods have failed to solve test problems 2 and 3 for high Reynolds and Raleigh numbers. <p> Then, the tensor-Krylov method becomes much more efficient and robust than the Newton-Krylov method as the test problems become increasingly harder to solve. These results are consistent with those obtained in <ref> [1] </ref>. We anticipate that similar conclusions would be obtained on other test problems as well. Therefore, our recommendation is to use tensor-Krylov methods for difficult problems to solve, and Newton Krylov methods otherwise. 15 7. <p> In order to firmly establish the conclusion above, additional testing is required, including rank-deficient test problems. We expect the efficiency advantage of tensor-Krylov methods to be much larger on such problems <ref> [1] </ref>. Several issues remain to be investigated. Among them is the possibility of combining tensor-Krylov and Newton-Krylov methods with the two-dimensional trust region global strategy developed in [2]. This global approach was shown to be much more robust than line search methods [2].
Reference: [2] <author> A. Bouaricha and R. B. Schnabel. TENSOLVE: </author> <title> A software package for solving systems of nonlinear equations and nonlinear least squares problems using tensor methods. </title> <type> Preprint MCS-P463-0894, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1994. </year>
Reference-contexts: In tests reported in [3], the tensor method virtually never is less efficient than a standard method based upon a linear (Newton) model, and usually is more efficient. One of the major contributions of tensor methods has been its great robustness. Experimental results in <ref> [2] </ref>, [3] have shown that the tensor method solves a considerable number of problems that the standard Newton method does not, and the reverse is virtually never true. <p> We expect the efficiency advantage of tensor-Krylov methods to be much larger on such problems [1]. Several issues remain to be investigated. Among them is the possibility of combining tensor-Krylov and Newton-Krylov methods with the two-dimensional trust region global strategy developed in <ref> [2] </ref>. This global approach was shown to be much more robust than line search methods [2]. Thus, its integration with tensor-Krylov methods is expected to make these methods even more robust. Finally, we intend to implement the algorithms discussed in this paper in a software package. <p> Several issues remain to be investigated. Among them is the possibility of combining tensor-Krylov and Newton-Krylov methods with the two-dimensional trust region global strategy developed in <ref> [2] </ref>. This global approach was shown to be much more robust than line search methods [2]. Thus, its integration with tensor-Krylov methods is expected to make these methods even more robust. Finally, we intend to implement the algorithms discussed in this paper in a software package.

Reference: [4] <author> P. N. Brown and Y. Saad. </author> <title> Globally convergent techniques in nonlinear Newton-Krylov algo-rithms. </title> <type> Technical Report L-316, </type> <institution> Computing and Mathematics Research Division, Lawrence Livermore National Laboratory, </institution> <year> 1989. </year>
Reference-contexts: Thus, for these systems robust and fast iterative solvers such as Krylov algorithms must be used. Newton-Krylov methods for nonlinear equations have been studied by Brown and Saad ([5], <ref> [4] </ref>). They show that their methods are remarkably effective on large systems of partial differential equations. Their success has motivated us to use Krylov subspace algorithms in conjunction with the tensor methods developed in [1]. <p> The residual r k represents the amount by which ffi k fails to satisfy the Newton equation J (x k )ffi k = F (x k ). The forcing sequence j k 2 [0; 1) is used to control the level of accuracy. Brown and Saad <ref> [4] </ref> show that if the sequence j k ! 0, then if J (x fl ) is nonsingular and j k j max &lt; 1, the iterates generated by Algorithm 4.1 converge to the solution superlinearly; the convergence is quadratic if j k = O (jjF (x k )jj). 9 5.
Reference: [5] <author> P. N. Brown and Y. Saad. </author> <title> Hybrid Krylov methods for nonlinear systems of equations. </title> <journal> SIAM J. Stat. Comp., </journal> <volume> 11 </volume> <pages> 450-481, </pages> <year> 1990. </year>
Reference-contexts: In x3 we review Krylov subspace methods and their main properties. In particular, we review the generalized minimum residual method (GMRES) [10], which will be used in conjunction with our tensor methods. In x4 we review the Newton-Krylov methods for large nonlinear equations that were described in <ref> [5] </ref>. In x5 we introduce tensor-Krylov algorithms for large nonlinear equations. We present some numerical results in x6. Finally, in x7 we make some concluding remarks. 2. <p> This procedure does not always guarantee convergence but does appear to work well in practice. 4. Newton-Krylov Methods In this section we review the nonlinear version of the GMRES method described in Brown and Saad <ref> [5] </ref>, which combines it with a Newton iteration, for solving nonlinear systems of equations. A method in this class is referred to as a Newton-Krylov algorithm. The Newton-GMRES method has the following general form: Algorithm 4.1. Newton-GMRES Method 1. <p> Numerical Testing In this section, we present some results of testing tensor-Krylov methods on a set of nonlinear partial differential equations. We also compare these results with those obtained by Newton-Krylov methods <ref> [5] </ref> on the same problems. 11 All our computations were performed on a Sun SPARCstation 10, using double-precision arithmetic. The tensor-Krylov and Newton-Krylov programs terminate successfully if jj F (x + ) jj 1 is less than 10 9 . <p> The default size of the Krylov subspace is set to 10. Test Problem 1. The Bratu problem <ref> [5] </ref> u + ffu x + e u = f (6:1) We solve (6.1) over the unit square of R 2 with Dirichlet boundary conditions. <p> We run this problem with N = 1024 (n x = 32), with ff = 10:0, and with different values of ranging from -5.0 to 10 12 . Test Problem 2. The driven cavity problem (given in stream-vorticity formulation) <ref> [5] </ref> = ! in (6:3) @n ( 0 if 0 x 2 &lt; 1 Here = f (x 1 ; x 2 ) : 0 &lt; x 1 &lt; 1; 0 &lt; x 2 &lt; 1g, and the viscosity is the reciprocal of the Reynolds number R e .
Reference: [6] <author> J. E. Dennis and R. B. Schnabel. </author> <title> Numerical methods for unconstrained optimization and nonlinear equations. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1983. </year>
Reference-contexts: 2 if (slope &lt; 0) then if f (x t + ) &lt; f (x c ) + 10 4 slope then x + = x t else Find an acceptable x n + in the Newton direction d n , using a line search algorithm (Algorithm A6.3.1, page 325 <ref> [6] </ref>) Find an acceptable x t + in the tensor direction d t , using a line search algorithm (Algorithm A6.3.1, page 325 [6]) if f (x n + ) then x + = x n else x + = x t endif endif else Find an acceptable x n + <p> + = x t else Find an acceptable x n + in the Newton direction d n , using a line search algorithm (Algorithm A6.3.1, page 325 <ref> [6] </ref>) Find an acceptable x t + in the tensor direction d t , using a line search algorithm (Algorithm A6.3.1, page 325 [6]) if f (x n + ) then x + = x n else x + = x t endif endif else Find an acceptable x n + in the Newton direction d n , using a line search algorithm (Algorithm A6.3.1, page 325 [6]) x + = x n endif <p> search algorithm (Algorithm A6.3.1, page 325 <ref> [6] </ref>) if f (x n + ) then x + = x n else x + = x t endif endif else Find an acceptable x n + in the Newton direction d n , using a line search algorithm (Algorithm A6.3.1, page 325 [6]) x + = x n endif 6. Numerical Testing In this section, we present some results of testing tensor-Krylov methods on a set of nonlinear partial differential equations.
Reference: [7] <author> D. Feng, P. Frank, and R. B. Schnabel. </author> <title> Local convergence analysis of tensor methods for nonlinear equations. </title> <journal> Math. Prog., </journal> <volume> 62 </volume> <pages> 427-459, </pages> <year> 1993. </year>
Reference-contexts: This is exactly the case in which the tensor method is intended to improve upon the performance of Newton's method due to the faster local convergence properties of the tensor method <ref> [1, 7] </ref>. Clearly, the tensor-GMRES method outperforms the Newton-GMRES method on this test problem for large values of in nonlinear iterations, function evaluations, and GMRES iterations. When the default Krylov subspace is used, both methods have failed to solve test problems 2 and 3 for high Reynolds and Raleigh numbers.
Reference: [8] <author> Y. Saad. </author> <title> Krylov subspace methods for solving unsymmetric linear systems. </title> <journal> Math. Comp., </journal> <volume> 37 </volume> <pages> 105-126, </pages> <year> 1981. </year>
Reference-contexts: For further information we refer to <ref> [8] </ref>, [10], and [9]. 7 3.1. GMRES Method We first review Arnoldi's algorithm for building an orthonormal basis of the Krylov subspace K m , on which the GMRES method is based.
Reference: [9] <author> Y. Saad and M. H. Schultz. </author> <title> Conjugate gradient-like algorithms for solving nonsymmetric linear systems. </title> <journal> Math. Comp., </journal> <volume> 44 </volume> <pages> 417-424, </pages> <year> 1985. </year>
Reference-contexts: For further information we refer to [8], [10], and <ref> [9] </ref>. 7 3.1. GMRES Method We first review Arnoldi's algorithm for building an orthonormal basis of the Krylov subspace K m , on which the GMRES method is based.
Reference: [10] <author> Y. Saad and M. H. Schultz. </author> <title> GMRES: a generalized minimal residual algorithm for solving nonsymmetric linear systems. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 7 </volume> <pages> 856-869, </pages> <year> 1986. </year>
Reference-contexts: The remainder of this paper is organized as follows. In x2 we review tensor methods for large, sparse nonlinear equations that were introduced in [1]. In x3 we review Krylov subspace methods and their main properties. In particular, we review the generalized minimum residual method (GMRES) <ref> [10] </ref>, which will be used in conjunction with our tensor methods. In x4 we review the Newton-Krylov methods for large nonlinear equations that were described in [5]. In x5 we introduce tensor-Krylov algorithms for large nonlinear equations. We present some numerical results in x6. <p> For further information we refer to [8], <ref> [10] </ref>, and [9]. 7 3.1. GMRES Method We first review Arnoldi's algorithm for building an orthonormal basis of the Krylov subspace K m , on which the GMRES method is based. <p> Thus, they are of a great practical value in applications involving several right sides but they are not as well studied from the theoretical point of view. Consequently, we used standard iterative methods in this paper. We experimented with three Krylov solvers: the generalized minimum residual (GMRES) <ref> [10] </ref>, conjugate gradient squared (CGS) [16] , and the Bi-CGSTAB [17] methods. There appears to be no significant difference in performance among the three methods on the test problems described above.
Reference: [11] <author> R. B. Schnabel and P. D. Frank. </author> <title> Tensor methods for nonlinear equations. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 21 </volume> <pages> 815-843, </pages> <year> 1984. </year>
Reference-contexts: In practice, p is usually 1 or 2. The procedure of finding linearly independent directions is implemented easily by using a modified Gram-Schmidt method. After selecting the linearly independent past directions, we form the tensor term. Schnabel and Frank <ref> [11] </ref> choose T c to be the smallest matrix that satisfies the interpolation conditions; 3 that is, min T c 2R nfinfin jj T c jj F (2:1) subject to T c s k s k = 2 (F (x k ) F (x c ) F 0 (x c )s
Reference: [12] <author> R. B. Schnabel, J. E. Koontz, and B. E. Weiss. </author> <title> A modular system of algorithms of unconstrained minimization. </title> <journal> ACM Trans. Math. Softw., </journal> <volume> 11 </volume> <pages> 419-440, </pages> <year> 1985. </year>
Reference-contexts: Perform a Cholesky decomposition of W (i.e., W = LL T ) resulting in L 2 R pfip , a lower triangular matrix. 6 4. Use UNCMIN <ref> [12] </ref>, an unconstrained minimization software package, to solve min 2 or solve (2.12) in closed form if p = 1. 5.
Reference: [13] <author> J. N. Shadid and R. S. Tuminaro. </author> <title> Sparse iterative algorithm software for large-scale MIMD machines: An initial discussion and implementation. </title> <type> Technical Report Sand91-0059, </type> <institution> Sandia National Laboratories, </institution> <address> Albuquerque, N.M., </address> <year> 1991. </year>
Reference-contexts: In this test we chose N = 3969 (n x = 63). We tried six different values for R e : 500, 1000, 1500, 2000, 3000, and 5000. Test Problem 3. The incompressible Navier-Stokes and thermal energy problem (given in stream-vorticity formulation) <ref> [13] </ref> @ 2 @y 2 = 1 @x @ (6:6) @x 2 + Ax 2 @ 2 Ax 2 (6:7) @ @ Ax @x @y @ 2 @y 2 (6:8) Equations (6.6)-(6.8) describe buoyancy induced natural convection in an inclined two-dimensional rectangular cavity over the computational domain (1; 1) fi (1;
Reference: [14] <author> V. Simonsini and E. Gallopoulos. </author> <title> Convergence properties of block GMRES for solving systems with multiple right-hand sides. </title> <type> Technical Report 1316, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <year> 1992. </year>
Reference-contexts: We used central difference approximations to obtain a system of nonlinear equations. We ran this problem with N = 1024. Since the tensor method requires two solves per iteration, the use of block iterative algorithms (see e.g., <ref> [15, 14] </ref>) may be preferable than standard iterative methods in this case. A clear advantage is that under certain conditions of the residual block and the Jacobian matrix, block iterative methods achieve finite termination in [n=2] iterations in solving the two tensor solves. This makes block iterative algorithms mathematically attractive.
Reference: [15] <author> V. Simonsini and E. Gallopoulos. </author> <title> An iterative method for nonsymmetric systems with multiple right hand-hand sides. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 16 </volume> <pages> 917-933, </pages> <year> 1995. </year>
Reference-contexts: We used central difference approximations to obtain a system of nonlinear equations. We ran this problem with N = 1024. Since the tensor method requires two solves per iteration, the use of block iterative algorithms (see e.g., <ref> [15, 14] </ref>) may be preferable than standard iterative methods in this case. A clear advantage is that under certain conditions of the residual block and the Jacobian matrix, block iterative methods achieve finite termination in [n=2] iterations in solving the two tensor solves. This makes block iterative algorithms mathematically attractive.
Reference: [16] <author> P. Sonneveld. </author> <title> CGS: A fast Lanczos-type solver for nonsymmetric linear systems. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 10 </volume> <pages> 36-52, </pages> <year> 1989. </year>
Reference-contexts: Consequently, we used standard iterative methods in this paper. We experimented with three Krylov solvers: the generalized minimum residual (GMRES) [10], conjugate gradient squared (CGS) <ref> [16] </ref> , and the Bi-CGSTAB [17] methods. There appears to be no significant difference in performance among the three methods on the test problems described above. As a result, we discuss the test results only when the GMRES method was used in conjunction with the tensor and Newton methods.
Reference: [17] <author> H. Van Der Vost. </author> <title> Bi-CGSTAB: A fast and smoothly converging variant of Bi-CG for the solution of nonsymmetric linear systems. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 13 </volume> <pages> 631-644, </pages> <year> 1992. </year> <month> 25 </month>
Reference-contexts: Consequently, we used standard iterative methods in this paper. We experimented with three Krylov solvers: the generalized minimum residual (GMRES) [10], conjugate gradient squared (CGS) [16] , and the Bi-CGSTAB <ref> [17] </ref> methods. There appears to be no significant difference in performance among the three methods on the test problems described above. As a result, we discuss the test results only when the GMRES method was used in conjunction with the tensor and Newton methods.
References-found: 16

