URL: http://www.cs.umn.edu/Users/dept/users/kumar/survey-scalability.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/kumar/
Root-URL: http://www.cs.umn.edu
Email: kumar@cs.umn.edu and agupta@cs.umn.edu  
Title: Analyzing Scalability of Parallel Algorithms and Architectures  
Author: Vipin Kumar and Anshul Gupta 
Date: TR 91-18, November 1991 (Revised July 1993)  
Address: Minneapolis, MN 55455  
Affiliation: Department of Computer Science University of Minnesota  
Note: To appear in Journal of Parallel and Distributed Computing, 1994.  
Abstract: The scalability of a parallel algorithm on a parallel architecture is a measure of its capacity to effectively utilize an increasing number of processors. Scalability analysis may be used to select the best algorithm-architecture combination for a problem under different constraints on the growth of the problem size and the number of processors. It may be used to predict the performance of a parallel algorithm and a parallel architecture for a large number of processors from the known performance on fewer processors. For a fixed problem size, it may be used to determine the optimal number of processors to be used and the maximum possible speedup that can be obtained. The objective of this paper is to critically assess the state of the art in the theory of scalability analysis, and motivate further research on the development of new and more comprehensive analytical tools to study the scalability of parallel algorithms and architectures. We survey a number of techniques and formalisms that have been developed for studying scalability issues, and discuss their interrelationships. For example, we derive an important relationship between time-constrained scaling and the isoefficiency function. We point out some of the weaknesses of the existing schemes for measuring scalability, and discuss possible ways of extending them. fl This work was supported by IST/SDIO through the Army Research Office grant # 28408-MA-SDI to the University of Minnesota and by the Army High Performance Computing Research Center at the University of Minnesota. An earlier version of this paper appears in the Proceedings of the 1991 International Conference on Supercomputing, Cologne, Germany, June 1991. A short version also appeared as an invited paper in the Proceedings of the 29th Annual Allerton Conference on Communication, Control and Computing, Urbana, IL, October 1991 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. G. Akl. </author> <title> The Design and Analysis of Parallel Algorithms. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year>
Reference-contexts: For example, for multiplying two N fi N matrices using Fox's parallel matrix multiplication algorithm [12], W = N 3 and (W ) = N 2 = W 2=3 . It is easily seen that if the processor-time product <ref> [1] </ref> is fi (W ) (i.e., the algorithm is cost-optimal), then (W ) fi (W ). 3 Scalability Metrics for Parallel Systems It is a well known fact that given a parallel architecture and a problem instance of a fixed size, the speedup of a parallel algorithm does not continue to <p> This metric can favor parallel systems for which the processor-time product is worse as long as they run faster. For example, for the all-pairs shortest path problem, this metric will favor the parallel algorithm <ref> [1] </ref> based upon an inefficient fi (N 3 log N ) serial algorithm over the parallel algorithms [34, 25] that are based on Floyd's fi (N 3 ) algorithm. <p> The model considered in [53] is one for which T o = fi (p), as they consider the case of a constant serial component. As an example of a parallel system where their conclusion is not applicable, consider the matrix multiplication on a SIMD mesh architecture <ref> [1] </ref>. Here T o = t w N 2 p p for multiplying N fi N matrices.
Reference: [2] <author> G. M. </author> <title> Amdahl. Validity of the single processor approach to achieving large scale computing capabilities. </title> <booktitle> In AFIPS Conference Proceedings, </booktitle> <pages> pages 483-485, </pages> <year> 1967. </year>
Reference-contexts: The speedup tends to saturate or peak at a certain value. As early as in 1967, Amdahl <ref> [2] </ref> made the observation that if s is the serial fraction in an algorithm, then its speedup is bounded by 1 s , no matter how many processors are used.
Reference: [3] <author> M. L. Barton and G. R. Withers. </author> <title> Computing performance as a function of the speed, quantity, and the cost of processors. </title> <booktitle> In Supercomputing '89 Proceedings, </booktitle> <pages> pages 759-764, </pages> <year> 1989. </year>
Reference-contexts: Another issue in the cost vs. performance analysis of parallel systems is to determine the tradeoffs between the speed of processors and the number of processors to be employed for a given budget. 16 From the analysis in <ref> [3] </ref> and [53], it may appear that higher performance is always obtained by using fewer and faster processors. It can be shown that this is true only for those parallel systems in which the overall overhead T o grows faster than or equal to fi (p). <p> Some analysis of the cost-effectiveness of parallel computers is given by Barton and Withers in <ref> [3] </ref>. They define cost C of an ensemble of p processors to be equal to dpV b , where V is the speed of each individual processor in FLOPS (floating point operations per second), d and b are constants, b is typically greater than 1. <p> From this expression, it can be shown that for a given cost and a fixed problem instance, the delivered FLOPS peaks for a certain number of processors. As noted in <ref> [3] </ref>, this analysis is valid only for a fixed problem size. Actually, the value of p for peak performance also increases as the problem size is increased.
Reference: [4] <author> Krishna P. Belkhale and Prithviraj Banerjee. </author> <title> Approximate algorithms for the partitionable independent task scheduling problem. </title> <booktitle> In Proceedings of the 1990 International Conference on Parallel Processing, </booktitle> <pages> pages I72-I75, </pages> <year> 1990. </year>
Reference-contexts: The problem of partitioning becomes more complex as the number of different computations is increased. Some early work on this topic has been reported in <ref> [4, 44, 37, 38] </ref>. 18
Reference: [5] <author> D. P. Bertsekas and J. N. Tsitsiklis. </author> <title> Parallel and Distributed Computation: Numerical Methods. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year>
Reference-contexts: The latter is true when any algorithm with a global 13 operation (such as broadcast, and one-to-all and all-to-all personalized communication <ref> [5, 26] </ref>) is implemented on a parallel architecture that has a message passing latency or message startup time.
Reference: [6] <author> E. A. Carmona and M. D. Rice. </author> <title> A model of parallel performance. </title> <type> Technical Report AFWL-TR-89-01, </type> <institution> Air Force Weapons Laboratory, </institution> <year> 1989. </year>
Reference-contexts: However, if the overhead due to communication grows faster or slower than fi (p), as is the case with many parallel systems, the models based solely upon sequential vs. parallel component are not adequate. Carmona and Rice <ref> [6, 7] </ref> provide new and improved interpretations for the parameters commonly used in the literature such as serial fraction, and the portion of time spent on performing serial work on a parallel system, etc.
Reference: [7] <author> E. A. Carmona and M. D. Rice. </author> <title> Modeling the serial and parallel fractions of a parallel algorithm. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <year> 1991. </year>
Reference-contexts: However, if the overhead due to communication grows faster or slower than fi (p), as is the case with many parallel systems, the models based solely upon sequential vs. parallel component are not adequate. Carmona and Rice <ref> [6, 7] </ref> provide new and improved interpretations for the parameters commonly used in the literature such as serial fraction, and the portion of time spent on performing serial work on a parallel system, etc.
Reference: [8] <author> S. Chandran and Larry S. Davis. </author> <title> An approach to parallel vision algorithms. </title> <editor> In R. Porth, editor, </editor> <booktitle> Parallel Processing. </booktitle> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1987. </year>
Reference-contexts: For example, if W serial = W S and W parallel = 0, (p) = T P t c (W serial +W parallel =p) = 1 t c = fi (1) (i.e., the parallel system is perfectly scalable!). Chandran and Davis <ref> [8] </ref> defined processor efficiency function (PEF) as the upper limit on the number of processors p than can be used to solve a problem of input size N such that the execution time on p processors is of the same order as the ratio of the sequential time to p; i.e., <p> Clearly, this can be done only for scalable parallel systems, which are exactly the ones for which a fixed efficiency can be maintained for arbitrarily large p by simply increasing the problem size. For such systems, it is natural to use isoefficiency function or related metrics <ref> [31, 28, 8] </ref>. The analyses in [60, 61, 11, 38, 42, 52, 9] also attempt to study the behavior of a parallel system with some concern for overall efficiency. Although different scalability measures are appropriate for rather different situations, many of them are related to each other.
Reference: [9] <author> D. L. Eager, J. Zahorjan, and E. D. Lazowska. </author> <title> Speedup versus efficiency in parallel systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(3) </volume> <pages> 408-423, </pages> <year> 1989. </year>
Reference-contexts: Thus, the concept of isoefficiency helps us in further sub-classifying the problems in class PE. It helps us in identifying more scalable algorithms for problems in the PE class by distinguishing between parallel systems whose isoefficiency functions are small or large polynomials in p. Eager, Zahorajan and Lazowska <ref> [9] </ref> introduced the average parallelism measure to characterize the scalability of a parallel software system that consists of an acyclic directed graph of subtasks with a possibility of precedence constraints among them. <p> The reason is that the properties of the parallel hardware can have a substantial effect on the performance of a parallel system. As an 7 example, they point out the limitations of the approach of Eager et al. <ref> [9] </ref> in which a parallel system is characterized with the average parallelism of the software as a parameter. <p> The reader should note that there are many important natural parallel systems where the per-processor overhead t o is smaller than fi (log p). For instance, in two of the three benchmark problems used in the Sandia experiment [22], the per-processor overhead was constant. Eager et al. <ref> [9] </ref> use the the average parallelism measure to locate the position (in terms of number of processors) of the knee in the execution time-efficiency curve. The knee occurs at the same value of p for which the ratio of efficiency to execution time is maximized. <p> For such systems, it is natural to use isoefficiency function or related metrics [31, 28, 8]. The analyses in <ref> [60, 61, 11, 38, 42, 52, 9] </ref> also attempt to study the behavior of a parallel system with some concern for overall efficiency. Although different scalability measures are appropriate for rather different situations, many of them are related to each other. <p> Several researchers have proposed to use an operating point where the value of p (T P ) r is minimized for some constant r and for a given problem size W <ref> [11, 9, 52] </ref>. It can be shown [52] that this corresponds to the point where ES r1 is maximized for a given problem size.
Reference: [10] <author> Horace P. Flatt. </author> <title> Further applications of the overhead model for parallel systems. </title> <type> Technical Report G320-3540, </type> <institution> IBM Corporation, Palo Alto Scientific Center, </institution> <address> Palo Alto, CA, </address> <year> 1990. </year>
Reference-contexts: A number of researchers have analyzed the optimal number of processors required to minimize parallel execution time for a variety of problems [11, 42, 52, 38]. Flatt and Kennedy <ref> [11, 10] </ref> derive some important upper-bounds related to the performance of parallel computers in the presence of synchronization and communication overheads.
Reference: [11] <author> Horace P. Flatt and Ken Kennedy. </author> <title> Performance of parallel processors. </title> <journal> Parallel Computing, </journal> <volume> 12 </volume> <pages> 1-20, </pages> <year> 1989. </year>
Reference-contexts: In this context, this work can be categorized with that of other researchers who have investigated the minimum execution time as a function of the size of the problem on an architecture with an unlimited number of processors available <ref> [58, 16, 11] </ref>. According to Nussbaum and Agarwal, the scalability of an algorithm is captured in its performance on the PRAM. <p> A number of researchers have analyzed the optimal number of processors required to minimize parallel execution time for a variety of problems <ref> [11, 42, 52, 38] </ref>. Flatt and Kennedy [11, 10] derive some important upper-bounds related to the performance of parallel computers in the presence of synchronization and communication overheads. <p> A number of researchers have analyzed the optimal number of processors required to minimize parallel execution time for a variety of problems [11, 42, 52, 38]. Flatt and Kennedy <ref> [11, 10] </ref> derive some important upper-bounds related to the performance of parallel computers in the presence of synchronization and communication overheads. <p> Thus the theoretical value of p 0 and the efficiency at this point may not be useful in studying many parallel algorithms. Flatt and Kennedy <ref> [11] </ref> also discuss scaling up the problem size with the number of processors. They define scaled speedup as k times the ratio of T P (W; p) and T P (kW; kp). <p> Note that for any fixed problem size W , the speedup on a parallel system will saturate or peak at some value S max (W ), which can also be used as a metric. Scalability issues for the fixed problem size case are addressed in <ref> [11, 27, 16, 42, 52, 58] </ref>. Another possible scenario is that in which a parallel computer with a fixed number of processors is being used and the best parallel algorithm needs to be chosen for solving a particular problem. <p> For such systems, it is natural to use isoefficiency function or related metrics [31, 28, 8]. The analyses in <ref> [60, 61, 11, 38, 42, 52, 9] </ref> also attempt to study the behavior of a parallel system with some concern for overall efficiency. Although different scalability measures are appropriate for rather different situations, many of them are related to each other. <p> Several researchers have proposed to use an operating point where the value of p (T P ) r is minimized for some constant r and for a given problem size W <ref> [11, 9, 52] </ref>. It can be shown [52] that this corresponds to the point where ES r1 is maximized for a given problem size.
Reference: [12] <author> G. C. Fox, M. Johnson, G. Lyzenga, S. W. Otto, J. Salmon, and D. Walker. </author> <title> Solving Problems on Concurrent Processors: Volume 1. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year>
Reference-contexts: Clearly, for a given W , the parallel algorithm cannot use more than (W ) processors. (W ) depends only on the parallel algorithm, and is independent of the architecture. For example, for multiplying two N fi N matrices using Fox's parallel matrix multiplication algorithm <ref> [12] </ref>, W = N 3 and (W ) = N 2 = W 2=3 . <p> In the last decade, there has been a growing realization that for a variety of parallel systems, given any number of processors p, speedup arbitrarily close to p can be obtained by simply executing the parallel algorithm on big enough problem instances <ref> [47, 31, 36, 45, 22, 12, 38, 41, 40, 58] </ref>. Kumar and Rao [31] developed a scalability metric relating the problem size to the number of processors necessary for an increase in speedup in proportion to the number of processors. This metric is known as the isoefficiency function.
Reference: [13] <author> Ananth Grama, Anshul Gupta, and Vipin Kumar. Isoefficiency: </author> <title> Measuring the scalability of parallel algorithms and architectures. </title> <journal> IEEE Parallel and Distributed Technology, </journal> <volume> 1(3) </volume> <pages> 12-21, </pages> <month> August, </month> <year> 1993. </year> <note> Also available as Technical Report TR 93-24, </note> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN. </institution>
Reference-contexts: We survey a number of techniques and formalisms that have been developed for studying the scalability issues, and discuss their interrelationships. We show some interesting relationships between the technique of isoefficiency analysis <ref> [29, 13, 31] </ref> and many other methods for scalability analysis. We point out some of the weaknesses of the existing schemes, and discuss possible ways of extending them. The organization of this paper is as follows. Section 2 describes the terminology that is followed in the rest of the paper. <p> By performing isoefficiency analysis, one can test the performance of a parallel program on a few processors, and then predict its performance on a larger number of processors. For a tutorial introduction to the isoefficiency function and its applications, one is referred to <ref> [29, 13] </ref>. Gustafson, Montry and Benner [22, 20] were the first to experimentally demonstrate that by scaling up the problem size one can obtain near-linear speedup on as many as 1024 processors. Gustafson et al. introduced a new metric called scaled speedup to evaluate the performance on practically feasible architectures.
Reference: [14] <author> Ananth Grama, Vipin Kumar, and V. Nageshwara Rao. </author> <title> Experimental evaluation of load balancing techniques for the hypercube. </title> <booktitle> In Proceedings of the Parallel Computing '91 Conference, </booktitle> <pages> pages 497-514, </pages> <year> 1991. </year>
Reference: [15] <author> Anshul Gupta and Vipin Kumar. </author> <title> The scalability of matrix multiplication algorithms on parallel computers. </title> <type> Technical Report TR 91-54, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN, </institution> <year> 1991. </year> <note> A short version appears in Proceedings of 1993 International Conference on Parallel Processing, pages III-115-III-119, </note> <year> 1993. </year>
Reference-contexts: This class of algorithms includes some fairly important algorithms such as matrix multiplication (all-to-all/one-to-all broadcast) <ref> [15] </ref>, vector dot products (single node accumulation) [19], shortest paths (one-to-all broadcast) [34], and FFT (all-to-all personalized communication) [18], etc. <p> Thus, technology dependent constants like t c and t w can have a dramatic impact on the scalability of a parallel system. In [48] and <ref> [15] </ref>, the impact of these parameters is discussed on the scalability of parallel shortest path algorithms, and matrix multiplication algorithms, respectively. <p> In these algorithms, the isoeffi-ciency function does not grow exponentially (as in the case of FFT) with respect to the t w t c ratio, but is a polynomial function of this ratio. For example, in the best-scalable parallel implementations of matrix multiplication algorithm on the mesh <ref> [15] </ref>, the isoefficiency function is proportional to ( t w t c ) 3 . The implication of this is that for the same communication speed, using ten times faster processors in the parallel computer will require a 1000 fold increase in the problem size to maintain the same efficiency.
Reference: [16] <author> Anshul Gupta and Vipin Kumar. </author> <title> Performance properties of large scale parallel systems. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 19 </volume> <pages> 234-244, </pages> <year> 1993. </year> <note> Also available as Technical Report TR 92-32, </note> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN. </institution>
Reference-contexts: For some other problems, Worley found time-constrained speedup to be close to linear, thus indicating that arbitrarily large instances of these problems can be solved in fixed time by simply increasing p. In <ref> [16] </ref>, Gupta and Kumar identify the classes of parallel systems which yield linear and sublinear time-constrained speedup curves. Karp and Flatt [27] introduced experimentally determined serial fraction f as a new metric for measuring the performance of a parallel system on a fix-sized problem. <p> In this context, this work can be categorized with that of other researchers who have investigated the minimum execution time as a function of the size of the problem on an architecture with an unlimited number of processors available <ref> [58, 16, 11] </ref>. According to Nussbaum and Agarwal, the scalability of an algorithm is captured in its performance on the PRAM. <p> A major assumption in their analysis is that the per-processor overhead t o (W; p) (t o (W; p) = T o (W;p) p ) grows faster than fi (p). As discussed in <ref> [16] </ref>, this assumption limits the range of the applicability of the analysis. Further, the 4 They later suggest maximizing a combination of the number of processors and the parallel execution time. They consider the weighted geometric mean F x of E and S. <p> They consider the weighted geometric mean F x of E and S. For a given problem size, F x (p) = (E (p)) x (S (p)) 2x , where 0 &lt; x &lt; 2. analysis in <ref> [16] </ref> reveals that the better a parallel algorithm is (i.e., the slower t o grows with p), the higher the value of p 0 . For many algorithm architecture combinations, p 0 exceeds the limit imposed by the degree of concurrency on the number of processors that can be used [16]. <p> <ref> [16] </ref> reveals that the better a parallel algorithm is (i.e., the slower t o grows with p), the higher the value of p 0 . For many algorithm architecture combinations, p 0 exceeds the limit imposed by the degree of concurrency on the number of processors that can be used [16]. Thus the theoretical value of p 0 and the efficiency at this point may not be useful in studying many parallel algorithms. Flatt and Kennedy [11] also discuss scaling up the problem size with the number of processors. <p> Consequently, the system utilizes more processors, thereby operating at a low efficiency. A low value of r means greater emphasis on improving the efficiency than on reducing T P ; hence, the operating point of the system will use fewer processors and yield a high efficiency. In <ref> [16] </ref>, for a fairly general class of overhead functions, Gupta and Kumar analytically determine the optimal number of processors to minimize T P as well as the more general metric p (T P ) r . <p> Note that for any fixed problem size W , the speedup on a parallel system will saturate or peak at some value S max (W ), which can also be used as a metric. Scalability issues for the fixed problem size case are addressed in <ref> [11, 27, 16, 42, 52, 58] </ref>. Another possible scenario is that in which a parallel computer with a fixed number of processors is being used and the best parallel algorithm needs to be chosen for solving a particular problem. <p> Let us call this T iso P (W ). Clearly, T iso P (W ) can be no better than T min P (W ). In <ref> [16] </ref>, it is shown that for a fairly wide class of overhead functions, the relation between problem size W and the number of processors p at which the parallel run time T P is minimized, is given by the isoefficiency function for some value of efficiency. <p> Note that the location of the minima of p (T P ) r (with respect to p) for two different algorithm-architecture combinations can be used to choose one between the two. In <ref> [16] </ref>, it is also shown that for a fairly large class of overhead functions, the relation between the problem size and p for which the expression p (T P ) r is minimum for that problem size, is given by the isoefficiency function (to maintain a particular 14 efficiency E which <p> If more processors are used, the problem might be solved faster, but the parallel system will not be utilized efficiently. For a wide class of parallel systems identified in <ref> [16] </ref>, using more processors than determined by the isoefficiency function does not help in reducing the parallel time complexity of the algorithm. The introduction of hardware cost factors (in addition to speedup and efficiency) in the scalability analysis is important so that the overall cost-effectiveness can be determined.
Reference: [17] <author> Anshul Gupta and Vipin Kumar. </author> <title> A scalable parallel algorithm for sparse matrix factorization. </title> <type> Technical Report 94-19, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN, </institution> <year> 1994. </year> <note> A short version appears in Supercomputing '94 Proceedings. TR available in users/kumar at anonymous FTP site ftp.cs.umn.edu. </note>
Reference: [18] <author> Anshul Gupta and Vipin Kumar. </author> <title> The scalability of FFT on parallel computers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(8) </volume> <pages> 922-932, </pages> <month> August </month> <year> 1993. </year> <note> A detailed version available as Technical Report TR 90-53, </note> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN. </institution>
Reference-contexts: This happens for some fairly common algorithms such as parallel FFT on a SIMD hypercube <ref> [18] </ref> 3 . Also, the parallel parallel algorithms for which f increases with p for a fixed W are not uncommon. For instance, for computing vector dot products on a hypercube [19] T o &gt; fi (p) and hence f increases with p if W is fixed. <p> For large p, f = 1 S 1 pT S 1 pT S . 3 The analysis in <ref> [18] </ref> can be adapted for SIMD hypercube by making the message startup time equal to zero. 6 on the degree of scalability of an ideally scalable system. On the other hand, the isoefficiency function provides no information on the degree of unscalability of an unscalable parallel system. <p> An important consequence of this result is that an algorithm with a polynomial isoefficiency on one architecture will have a polynomial isoefficiency on many other architectures as well. There can however be exceptions; for instance, it is shown in <ref> [18] </ref> that the FFT algorithm has a polynomial isoefficiency on a hypercube but an exponential isoefficiency on a mesh. As shown in [18, 48, 19], isoefficiency functions for a parallel algorithm can vary across architectures and understanding of this variation is of considerable practical importance. <p> There can however be exceptions; for instance, it is shown in [18] that the FFT algorithm has a polynomial isoefficiency on a hypercube but an exponential isoefficiency on a mesh. As shown in <ref> [18, 48, 19] </ref>, isoefficiency functions for a parallel algorithm can vary across architectures and understanding of this variation is of considerable practical importance. Thus, the concept of isoefficiency helps us in further sub-classifying the problems in class PE. <p> This class of algorithms includes some fairly important algorithms such as matrix multiplication (all-to-all/one-to-all broadcast) [15], vector dot products (single node accumulation) [19], shortest paths (one-to-all broadcast) [34], and FFT (all-to-all personalized communication) <ref> [18] </ref>, etc. The readers should note that the presence of a global communication operation in an algorithm is a sufficient but not a necessary condition for non-linear isoefficiency on an architecture with message passing latency. <p> But unlike a sequential computer, in a parallel system, a k-fold increase in the CPU speed may not necessarily result in a k-fold reduction in execution time for the same problem size. For instance, consider the implementation of the FFT algorithm on a SIMD hypercube <ref> [18] </ref>. The efficiency of a N -point FFT computation on a p-processor cube is given by E = 1 1+ t w log p . <p> In other words, increasing the speed of the processors alone without improving the communication speed will result in diminishing returns in terms of overall speedup and efficiency. The decrease in performance is highly problem dependent. For some problems it is dramatic (e.g., parallel FFT <ref> [18] </ref>), while for others (e.g., parallel matrix multiplication) it is less severe. 7 Study of Cost Effectiveness of Parallel Systems So far we have confined the study of scalability of a parallel system to investigating the capability of the parallel algorithm to effectively utilize an increasing number of processors on the <p> For a given amount of resources, the aim is to maximize the overall performance which is proportional to the number of processors and the efficiency obtained on them. It is shown in <ref> [18] </ref> that under certain assumptions, it is more cost-effective to implement the FFT algorithm on a hypercube rather than a mesh despite the fact that large scale meshes are cheaper to construct than large hypercubes.
Reference: [19] <author> Anshul Gupta, Vipin Kumar, and A. H. Sameh. </author> <title> Performance and scalability of preconditioned conjugate gradient methods on parallel computers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <note> 1995 (To Appear). Also available as Technical Report TR 92-64, </note> <institution> Department of Computer Science, University of Minnesota, </institution> <address> Minneapolis, MN. </address> <booktitle> A short version appears in Proceedings of the Sixth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 664-674, </pages> <year> 1993. </year> <month> 19 </month>
Reference-contexts: This happens for some fairly common algorithms such as parallel FFT on a SIMD hypercube [18] 3 . Also, the parallel parallel algorithms for which f increases with p for a fixed W are not uncommon. For instance, for computing vector dot products on a hypercube <ref> [19] </ref> T o &gt; fi (p) and hence f increases with p if W is fixed. But as shown in [19], this algorithm-architecture combination has an isoefficiency function of fi (p log p) and can be considered quite scalable. <p> Also, the parallel parallel algorithms for which f increases with p for a fixed W are not uncommon. For instance, for computing vector dot products on a hypercube <ref> [19] </ref> T o &gt; fi (p) and hence f increases with p if W is fixed. But as shown in [19], this algorithm-architecture combination has an isoefficiency function of fi (p log p) and can be considered quite scalable. Zorbas et al. [61] developed the following framework to characterize the scalability of parallel systems. <p> There can however be exceptions; for instance, it is shown in [18] that the FFT algorithm has a polynomial isoefficiency on a hypercube but an exponential isoefficiency on a mesh. As shown in <ref> [18, 48, 19] </ref>, isoefficiency functions for a parallel algorithm can vary across architectures and understanding of this variation is of considerable practical importance. Thus, the concept of isoefficiency helps us in further sub-classifying the problems in class PE. <p> This class of algorithms includes some fairly important algorithms such as matrix multiplication (all-to-all/one-to-all broadcast) [15], vector dot products (single node accumulation) <ref> [19] </ref>, shortest paths (one-to-all broadcast) [34], and FFT (all-to-all personalized communication) [18], etc. The readers should note that the presence of a global communication operation in an algorithm is a sufficient but not a necessary condition for non-linear isoefficiency on an architecture with message passing latency.
Reference: [20] <author> John L. Gustafson. </author> <title> Reevaluating Amdahl's law. </title> <journal> Communications of the ACM, </journal> <volume> 31(5) </volume> <pages> 532-533, </pages> <year> 1988. </year>
Reference-contexts: By performing isoefficiency analysis, one can test the performance of a parallel program on a few processors, and then predict its performance on a larger number of processors. For a tutorial introduction to the isoefficiency function and its applications, one is referred to [29, 13]. Gustafson, Montry and Benner <ref> [22, 20] </ref> were the first to experimentally demonstrate that by scaling up the problem size one can obtain near-linear speedup on as many as 1024 processors. Gustafson et al. introduced a new metric called scaled speedup to evaluate the performance on practically feasible architectures. <p> For parallel systems with much worse isoefficiencies, the results provided by the two metrics may be quite different. In this case, the scaled-speedup vs. number of processors curve is sublinear. Two generalized notions of scaled speedup were considered by Gustafson et al. <ref> [20] </ref>, Worley [58] and Sun and Ni [50]. They differ in the methods by which the problem size is scaled up with the number of processors. In one method, the size of the problem is increased to fill the available memory on the parallel computer. <p> The scalability issues for such problems have been 12 explored by Worley [58], Gustafson <ref> [22, 20] </ref>, and Sun and Ni [50]. Another extreme in scaling the problem size is to try as big problems as can be handled in the memory. This is investigated by Worley [57, 58, 59], Gustafson [22, 20] and by Sun and Ni [50], and is called the memory-constrained case. <p> The scalability issues for such problems have been 12 explored by Worley [58], Gustafson <ref> [22, 20] </ref>, and Sun and Ni [50]. Another extreme in scaling the problem size is to try as big problems as can be handled in the memory. This is investigated by Worley [57, 58, 59], Gustafson [22, 20] and by Sun and Ni [50], and is called the memory-constrained case. Since the total memory of a parallel computer increases with increasing p, it is possible to solve bigger problems on parallel computer with bigger p.
Reference: [21] <author> John L. Gustafson. </author> <title> The consequences of fixed time performance measurement. </title> <booktitle> In Proceedings of the 25th Hawaii International Conference on System Sciences: Volume III, </booktitle> <pages> pages 113-124, </pages> <year> 1992. </year>
Reference-contexts: From these results, it is inferred that it is better to have a parallel computer with fewer faster processors than one with many slower processors. We discuss this issue further in Section 7 and show that this inference is valid only for a certain class of parallel systems. In <ref> [49, 21] </ref>, Sun and Gustafson argue that traditional speedup is an unfair measure, as it favors slow processors and poorly coded programs. They derive some fair performance measures for parallel programs to correct this. <p> They show that M 1 will attain poorer speedups (even if the communication overheads are ignored), but according to sizeup, M 1 will be considered better. Based on the concept of fixed time sizeup, Gustafson <ref> [21] </ref> has developed the SLALOM benchmark for a distortion-free comparison of computers of widely varying speeds.
Reference: [22] <author> John L. Gustafson, Gary R. Montry, and Robert E. Benner. </author> <title> Development of parallel methods for a 1024-processor hypercube. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 9(4) </volume> <pages> 609-638, </pages> <year> 1988. </year>
Reference-contexts: In the last decade, there has been a growing realization that for a variety of parallel systems, given any number of processors p, speedup arbitrarily close to p can be obtained by simply executing the parallel algorithm on big enough problem instances <ref> [47, 31, 36, 45, 22, 12, 38, 41, 40, 58] </ref>. Kumar and Rao [31] developed a scalability metric relating the problem size to the number of processors necessary for an increase in speedup in proportion to the number of processors. This metric is known as the isoefficiency function. <p> By performing isoefficiency analysis, one can test the performance of a parallel program on a few processors, and then predict its performance on a larger number of processors. For a tutorial introduction to the isoefficiency function and its applications, one is referred to [29, 13]. Gustafson, Montry and Benner <ref> [22, 20] </ref> were the first to experimentally demonstrate that by scaling up the problem size one can obtain near-linear speedup on as many as 1024 processors. Gustafson et al. introduced a new metric called scaled speedup to evaluate the performance on practically feasible architectures. <p> The reader should note that there are many important natural parallel systems where the per-processor overhead t o is smaller than fi (log p). For instance, in two of the three benchmark problems used in the Sandia experiment <ref> [22] </ref>, the per-processor overhead was constant. Eager et al. [9] use the the average parallelism measure to locate the position (in terms of number of processors) of the knee in the execution time-efficiency curve. <p> The scalability issues for such problems have been 12 explored by Worley [58], Gustafson <ref> [22, 20] </ref>, and Sun and Ni [50]. Another extreme in scaling the problem size is to try as big problems as can be handled in the memory. This is investigated by Worley [57, 58, 59], Gustafson [22, 20] and by Sun and Ni [50], and is called the memory-constrained case. <p> The scalability issues for such problems have been 12 explored by Worley [58], Gustafson <ref> [22, 20] </ref>, and Sun and Ni [50]. Another extreme in scaling the problem size is to try as big problems as can be handled in the memory. This is investigated by Worley [57, 58, 59], Gustafson [22, 20] and by Sun and Ni [50], and is called the memory-constrained case. Since the total memory of a parallel computer increases with increasing p, it is possible to solve bigger problems on parallel computer with bigger p.
Reference: [23] <author> Mark D. Hill. </author> <title> What is scalability? Computer Architecture News, </title> <type> 18(4), </type> <year> 1990. </year>
Reference-contexts: of parallel program characteristics like speedup and efficiency are then presented as a function of these parameters for several theoretical and empirical examples. 5 Relation Between Some Scalability Measures After reviewing these various measures of scalability, one may ask whether there exists one measure that is better than all others <ref> [23] </ref>? The answer to this question is no, as different measures are suitable for different situations. One situation arises when the problem at hand is fixed and one is trying to use an increasing number of processors to solve it.
Reference: [24] <author> Kai Hwang. </author> <title> Advanced Computer Architecture: Parallelism, Scalability, Programmability. </title> <publisher> McGraw-Hill, </publisher> <address> New York, NY, </address> <year> 1993. </year>
Reference: [25] <author> Jing-Fu Jenq and Sartaj Sahni. </author> <title> All pairs shortest paths on a hypercube multiprocessor. </title> <booktitle> In Proceedings of the 1987 International Conference on Parallel Processing, </booktitle> <pages> pages 713-716, </pages> <year> 1987. </year>
Reference-contexts: For example, for the all-pairs shortest path problem, this metric will favor the parallel algorithm [1] based upon an inefficient fi (N 3 log N ) serial algorithm over the parallel algorithms <ref> [34, 25] </ref> that are based on Floyd's fi (N 3 ) algorithm.
Reference: [26] <author> S. L. Johnsson and C.-T. Ho. </author> <title> Optimum broadcasting and personalized communication in hypercubes. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(9) </volume> <pages> 1249-1268, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: The latter is true when any algorithm with a global 13 operation (such as broadcast, and one-to-all and all-to-all personalized communication <ref> [5, 26] </ref>) is implemented on a parallel architecture that has a message passing latency or message startup time.
Reference: [27] <author> Alan H. Karp and Horace P. Flatt. </author> <title> Measuring parallel processor performance. </title> <journal> Communications of the ACM, </journal> <volume> 33(5) </volume> <pages> 539-543, </pages> <year> 1990. </year>
Reference-contexts: In [16], Gupta and Kumar identify the classes of parallel systems which yield linear and sublinear time-constrained speedup curves. Karp and Flatt <ref> [27] </ref> introduced experimentally determined serial fraction f as a new metric for measuring the performance of a parallel system on a fix-sized problem. If S is the speedup on a p-processor system, then f is defined as 1=S1=p 11=p . <p> Smaller values of f are considered better. If f increases with the number of processors, then it is considered as an indicator of rising communication overhead, and thus an indicator of poor scalability. If the value of f decreases with increasing p, then Karp and Flatt <ref> [27] </ref> consider it to be an anomaly to be explained by phenomena such as superlinear speedup effects or cache effects. On the contrary, our investigation shows that f can decrease for perfectly normal programs. <p> Note that for any fixed problem size W , the speedup on a parallel system will saturate or peak at some value S max (W ), which can also be used as a metric. Scalability issues for the fixed problem size case are addressed in <ref> [11, 27, 16, 42, 52, 58] </ref>. Another possible scenario is that in which a parallel computer with a fixed number of processors is being used and the best parallel algorithm needs to be chosen for solving a particular problem.
Reference: [28] <author> Clyde P. Kruskal, Larry Rudolph, and Marc Snir. </author> <title> A complexity theory of efficient parallel algorithms. </title> <type> Technical Report RC13572, </type> <institution> IBM T. J. Watson Research Center, </institution> <address> Yorktown Heights, NY, </address> <year> 1988. </year>
Reference-contexts: An inverse of this function called data efficiency function (DEF) is defined as the smallest problem size on a given number of processors such that the above relation holds. The concept of data efficiency function is very similar to that of the isoefficiency function. Kruskal et al. <ref> [28] </ref> defined the concept of Parallel Efficient (PE) problems which is related to the concept of isoefficiency function. The PE class of problems have algorithms with a polynomial isoefficiency function for some efficiency. <p> Clearly, this can be done only for scalable parallel systems, which are exactly the ones for which a fixed efficiency can be maintained for arbitrarily large p by simply increasing the problem size. For such systems, it is natural to use isoefficiency function or related metrics <ref> [31, 28, 8] </ref>. The analyses in [60, 61, 11, 38, 42, 52, 9] also attempt to study the behavior of a parallel system with some concern for overall efficiency. Although different scalability measures are appropriate for rather different situations, many of them are related to each other.
Reference: [29] <author> Vipin Kumar, Ananth Grama, Anshul Gupta, and George Karypis. </author> <title> Introduction to Parallel Computing: Design and Analysis of Algorithms. </title> <address> Benjamin/Cummings, Redwood City, CA, </address> <year> 1994. </year>
Reference-contexts: We survey a number of techniques and formalisms that have been developed for studying the scalability issues, and discuss their interrelationships. We show some interesting relationships between the technique of isoefficiency analysis <ref> [29, 13, 31] </ref> and many other methods for scalability analysis. We point out some of the weaknesses of the existing schemes, and discuss possible ways of extending them. The organization of this paper is as follows. Section 2 describes the terminology that is followed in the rest of the paper. <p> By performing isoefficiency analysis, one can test the performance of a parallel program on a few processors, and then predict its performance on a larger number of processors. For a tutorial introduction to the isoefficiency function and its applications, one is referred to <ref> [29, 13] </ref>. Gustafson, Montry and Benner [22, 20] were the first to experimentally demonstrate that by scaling up the problem size one can obtain near-linear speedup on as many as 1024 processors. Gustafson et al. introduced a new metric called scaled speedup to evaluate the performance on practically feasible architectures.
Reference: [30] <author> Vipin Kumar, Ananth Grama, and V. Nageshwara Rao. </author> <title> Scalable load balancing techniques for parallel computers. </title> <type> Technical Report 91-55, </type> <institution> Computer Science Department, University of Minnesota, </institution> <year> 1991. </year> <note> To appear in Journal of Distributed and Parallel Computing, </note> <year> 1994. </year>
Reference: [31] <author> Vipin Kumar and V. N. Rao. </author> <title> Parallel depth-first search, part II: Analysis. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 16(6) </volume> <pages> 501-519, </pages> <year> 1987. </year>
Reference-contexts: We survey a number of techniques and formalisms that have been developed for studying the scalability issues, and discuss their interrelationships. We show some interesting relationships between the technique of isoefficiency analysis <ref> [29, 13, 31] </ref> and many other methods for scalability analysis. We point out some of the weaknesses of the existing schemes, and discuss possible ways of extending them. The organization of this paper is as follows. Section 2 describes the terminology that is followed in the rest of the paper. <p> In the last decade, there has been a growing realization that for a variety of parallel systems, given any number of processors p, speedup arbitrarily close to p can be obtained by simply executing the parallel algorithm on big enough problem instances <ref> [47, 31, 36, 45, 22, 12, 38, 41, 40, 58] </ref>. Kumar and Rao [31] developed a scalability metric relating the problem size to the number of processors necessary for an increase in speedup in proportion to the number of processors. This metric is known as the isoefficiency function. <p> Kumar and Rao <ref> [31] </ref> developed a scalability metric relating the problem size to the number of processors necessary for an increase in speedup in proportion to the number of processors. This metric is known as the isoefficiency function. <p> Clearly, this can be done only for scalable parallel systems, which are exactly the ones for which a fixed efficiency can be maintained for arbitrarily large p by simply increasing the problem size. For such systems, it is natural to use isoefficiency function or related metrics <ref> [31, 28, 8] </ref>. The analyses in [60, 61, 11, 38, 42, 52, 9] also attempt to study the behavior of a parallel system with some concern for overall efficiency. Although different scalability measures are appropriate for rather different situations, many of them are related to each other.
Reference: [32] <author> Vipin Kumar and V. N. Rao. </author> <title> Load balancing on the hypercube architecture. </title> <booktitle> In Proceedings of the Fourth Conference on Hypercubes, Concurrent Computers, and Applications, </booktitle> <pages> pages 603-608, </pages> <year> 1989. </year>
Reference: [33] <author> Vipin Kumar and V. N. Rao. </author> <title> Scalable parallel formulations of depth-first search. </title> <editor> In Vipin Kumar, P. S. Gopalakr-ishnan, and Laveen N. Kanal, editors, </editor> <booktitle> Parallel Algorithms for Machine Intelligence and Vision. </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York, NY, </address> <year> 1990. </year>
Reference: [34] <author> Vipin Kumar and Vineet Singh. </author> <title> Scalability of parallel algorithms for the all-pairs shortest path problem. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13(2) </volume> <pages> 124-138, </pages> <month> October </month> <year> 1991. </year> <note> A short version appears in the Proceedings of the International Conference on Parallel Processing, </note> <year> 1990. </year>
Reference-contexts: The reason is that on these parallel systems, it is difficult to obtain good speedups for a large number of processors unless the problem size is enormous. On the other hand, if W needs to 1 For some parallel systems ( e.g., some of those discussed in [48] and <ref> [34] </ref>), the maximum obtainable efficiency E max is less than 1. Even such parallel systems are considered scalable if the efficiency can be maintained at a desirable value between 0 and E max . 4 grow only linearly with respect to p, then the parallel system is highly scalable. <p> For example, for the all-pairs shortest path problem, this metric will favor the parallel algorithm [1] based upon an inefficient fi (N 3 log N ) serial algorithm over the parallel algorithms <ref> [34, 25] </ref> that are based on Floyd's fi (N 3 ) algorithm. <p> This class of algorithms includes some fairly important algorithms such as matrix multiplication (all-to-all/one-to-all broadcast) [15], vector dot products (single node accumulation) [19], shortest paths (one-to-all broadcast) <ref> [34] </ref>, and FFT (all-to-all personalized communication) [18], etc. The readers should note that the presence of a global communication operation in an algorithm is a sufficient but not a necessary condition for non-linear isoefficiency on an architecture with message passing latency.
Reference: [35] <author> H. T. Kung. </author> <title> Memory requirements for balanced computer architectures. </title> <booktitle> In Proceedings of the 1986 IEEE Symposium on Computer Architecture, </booktitle> <pages> pages 49-54, </pages> <year> 1986. </year>
Reference-contexts: They also show that improving only the bus speed by a factor of k reduces the optimal execution time by a factor of k 2 15 Kung <ref> [35] </ref> studied the impact of increasing CPU speed (while keeping the I/O speed fixed) on the memory requirement for a variety of problems.
Reference: [36] <author> J. Lee, E. Shragowitz, and S. Sahni. </author> <title> A hypercube algorithm for the 0/1 knapsack problem. </title> <booktitle> In Proceedings of 1987 International Conference on Parallel Processing, </booktitle> <pages> pages 699-706, </pages> <year> 1987. </year>
Reference-contexts: In the last decade, there has been a growing realization that for a variety of parallel systems, given any number of processors p, speedup arbitrarily close to p can be obtained by simply executing the parallel algorithm on big enough problem instances <ref> [47, 31, 36, 45, 22, 12, 38, 41, 40, 58] </ref>. Kumar and Rao [31] developed a scalability metric relating the problem size to the number of processors necessary for an increase in speedup in proportion to the number of processors. This metric is known as the isoefficiency function.
Reference: [37] <author> Michael R. Leuze, Lawrence W. Dowdy, and Kee Hyun Park. </author> <title> Multiprogramming a distributed-memory multiprocessor. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 1(1) </volume> <pages> 19-33, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: The problem of partitioning becomes more complex as the number of different computations is increased. Some early work on this topic has been reported in <ref> [4, 44, 37, 38] </ref>. 18
Reference: [38] <author> Y. W. E. Ma and Denis G. Shea. </author> <title> Downward scalability of parallel architectures. </title> <booktitle> In Proceedings of the 1988 International Conference on Supercomputing, </booktitle> <pages> pages 109-120, </pages> <year> 1988. </year>
Reference-contexts: In the last decade, there has been a growing realization that for a variety of parallel systems, given any number of processors p, speedup arbitrarily close to p can be obtained by simply executing the parallel algorithm on big enough problem instances <ref> [47, 31, 36, 45, 22, 12, 38, 41, 40, 58] </ref>. Kumar and Rao [31] developed a scalability metric relating the problem size to the number of processors necessary for an increase in speedup in proportion to the number of processors. This metric is known as the isoefficiency function. <p> A number of researchers have analyzed the optimal number of processors required to minimize parallel execution time for a variety of problems <ref> [11, 42, 52, 38] </ref>. Flatt and Kennedy [11, 10] derive some important upper-bounds related to the performance of parallel computers in the presence of synchronization and communication overheads. <p> For such systems, it is natural to use isoefficiency function or related metrics [31, 28, 8]. The analyses in <ref> [60, 61, 11, 38, 42, 52, 9] </ref> also attempt to study the behavior of a parallel system with some concern for overall efficiency. Although different scalability measures are appropriate for rather different situations, many of them are related to each other. <p> The problem of partitioning becomes more complex as the number of different computations is increased. Some early work on this topic has been reported in <ref> [4, 44, 37, 38] </ref>. 18
Reference: [39] <author> Dan C. Marinescu and John R. Rice. </author> <title> On high level characterization of parallelism. </title> <type> Technical Report CSD-TR-1011, </type> <institution> CAPO Report CER-90-32, Computer Science Department, Purdue University, West Lafayette, </institution> <note> IN, Revised June 1991. To appear in Journal of Parallel and Distributed Computing, </note> <year> 1993. </year>
Reference-contexts: It is quite possible that a parallel algorithm with a large degree of concurrency is substantially worse than one with smaller concurrency and minimal communication overheads. Speedup and efficiency can be arbitrarily poor if communication overheads are present. Marinescu and Rice <ref> [39] </ref> argue that a single parameter that depends solely on the nature of the parallel software is not sufficient to analyze a parallel system. The reason is that the properties of the parallel hardware can have a substantial effect on the performance of a parallel system.
Reference: [40] <author> Paul Messina. </author> <title> Emerging supercomputer architectures. </title> <type> Technical Report C3P 746, </type> <institution> Concurrent Computation Program, California Institute of Technology, Pasadena, </institution> <address> CA, </address> <year> 1987. </year>
Reference-contexts: In the last decade, there has been a growing realization that for a variety of parallel systems, given any number of processors p, speedup arbitrarily close to p can be obtained by simply executing the parallel algorithm on big enough problem instances <ref> [47, 31, 36, 45, 22, 12, 38, 41, 40, 58] </ref>. Kumar and Rao [31] developed a scalability metric relating the problem size to the number of processors necessary for an increase in speedup in proportion to the number of processors. This metric is known as the isoefficiency function.
Reference: [41] <author> Cleve Moler. </author> <title> Another look at Amdahl's law. </title> <type> Technical Report TN-02-0587-0288, </type> <institution> Intel Scientific Computers, </institution> <year> 1987. </year> <month> 20 </month>
Reference-contexts: In the last decade, there has been a growing realization that for a variety of parallel systems, given any number of processors p, speedup arbitrarily close to p can be obtained by simply executing the parallel algorithm on big enough problem instances <ref> [47, 31, 36, 45, 22, 12, 38, 41, 40, 58] </ref>. Kumar and Rao [31] developed a scalability metric relating the problem size to the number of processors necessary for an increase in speedup in proportion to the number of processors. This metric is known as the isoefficiency function. <p> We call such systems scalable 1 parallel systems. This definition of scalable parallel algorithms is similar to the definition of parallel effective algorithms given by Moler <ref> [41] </ref>. For different parallel systems, W should be increased at different rates with respect to p in order to maintain a fixed efficiency. For instance, in some cases, W might need to grow as an exponential function of p to keep the efficiency from dropping as p increases.
Reference: [42] <author> David M. Nicol and Frank H. Willard. </author> <title> Problem size, parallel architecture, and optimal speedup. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5 </volume> <pages> 404-420, </pages> <year> 1988. </year>
Reference-contexts: A number of researchers have analyzed the optimal number of processors required to minimize parallel execution time for a variety of problems <ref> [11, 42, 52, 38] </ref>. Flatt and Kennedy [11, 10] derive some important upper-bounds related to the performance of parallel computers in the presence of synchronization and communication overheads. <p> Note that for any fixed problem size W , the speedup on a parallel system will saturate or peak at some value S max (W ), which can also be used as a metric. Scalability issues for the fixed problem size case are addressed in <ref> [11, 27, 16, 42, 52, 58] </ref>. Another possible scenario is that in which a parallel computer with a fixed number of processors is being used and the best parallel algorithm needs to be chosen for solving a particular problem. <p> For such systems, it is natural to use isoefficiency function or related metrics [31, 28, 8]. The analyses in <ref> [60, 61, 11, 38, 42, 52, 9] </ref> also attempt to study the behavior of a parallel system with some concern for overall efficiency. Although different scalability measures are appropriate for rather different situations, many of them are related to each other. <p> Hence, for parallel matrix multiplication, it is better to have a parallel computer with k-fold as many processors rather than one with the same number of processors, each k-fold as fast (assuming that the communication network and the bandwidth etc. remain the same). Nicol and Willard <ref> [42] </ref> consider the impact of CPU and communication speeds on the peak performance of parallel systems. They study the performance of various parallel architectures for solving Elliptic Partial Differential Equations.
Reference: [43] <author> Daniel Nussbaum and Anant Agarwal. </author> <title> Scalability of parallel machines. </title> <journal> Communications of the ACM, </journal> <volume> 34(3) </volume> <pages> 57-61, </pages> <year> 1991. </year>
Reference-contexts: Between two efficient parallel algorithms in this class, obviously the one that is able to use more processors is considered superior in terms of scalability. Nussbaum and Agarwal <ref> [43] </ref> defined scalability of a parallel architecture for a given algorithm as the ratio of the algorithm's asymptotic speedup when run on the architecture in question to its corresponding asymptotic speedup when run on an EREW PRAM. <p> Informally, it reflects "the fraction of the parallelism inherent in a given algorithm that 9 can be exploited by any machine of that architecture as a function of problem size" <ref> [43] </ref>. Note that this metric cannot be used to compare two algorithm-architecture pairs for solving the same problem or to characterize the scalability (or unscalability) of a parallel algorithm. It can only be used to compare parallel architectures for a given parallel algorithm.
Reference: [44] <author> Kee Hyun Park and Lawrence W. Dowdy. </author> <title> Dynamic partitioning of multiprocessor systems. </title> <journal> International Journal of Parallel Processing, </journal> <volume> 18(2) </volume> <pages> 91-120, </pages> <year> 1989. </year>
Reference-contexts: The problem of partitioning becomes more complex as the number of different computations is increased. Some early work on this topic has been reported in <ref> [4, 44, 37, 38] </ref>. 18
Reference: [45] <author> Michael J. Quinn and Year Back Yoo. </author> <title> Data structures for the efficient solution of graph theoretic problems on tightly-coupled MIMD computers. </title> <booktitle> In Proceedings of the 1984 International Conference on Parallel Processing, </booktitle> <pages> pages 431-438, </pages> <year> 1984. </year>
Reference-contexts: In the last decade, there has been a growing realization that for a variety of parallel systems, given any number of processors p, speedup arbitrarily close to p can be obtained by simply executing the parallel algorithm on big enough problem instances <ref> [47, 31, 36, 45, 22, 12, 38, 41, 40, 58] </ref>. Kumar and Rao [31] developed a scalability metric relating the problem size to the number of processors necessary for an increase in speedup in proportion to the number of processors. This metric is known as the isoefficiency function.
Reference: [46] <author> S. Ranka and S. Sahni. </author> <title> Hypercube Algorithms for Image Processing and Pattern Recognition. </title> <publisher> Springer-Verlag, </publisher> <address> New York, NY, </address> <year> 1990. </year>
Reference: [47] <author> V. N. Rao and Vipin Kumar. </author> <title> Parallel depth-first search, part I: Implementation. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 16(6) </volume> <pages> 479-499, </pages> <year> 1987. </year>
Reference-contexts: In the last decade, there has been a growing realization that for a variety of parallel systems, given any number of processors p, speedup arbitrarily close to p can be obtained by simply executing the parallel algorithm on big enough problem instances <ref> [47, 31, 36, 45, 22, 12, 38, 41, 40, 58] </ref>. Kumar and Rao [31] developed a scalability metric relating the problem size to the number of processors necessary for an increase in speedup in proportion to the number of processors. This metric is known as the isoefficiency function.
Reference: [48] <author> Vineet Singh, Vipin Kumar, Gul Agha, and Chris Tomlinson. </author> <title> Scalability of parallel sorting on mesh multicom-puters. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 20(2), </volume> <year> 1991. </year>
Reference-contexts: The reason is that on these parallel systems, it is difficult to obtain good speedups for a large number of processors unless the problem size is enormous. On the other hand, if W needs to 1 For some parallel systems ( e.g., some of those discussed in <ref> [48] </ref> and [34]), the maximum obtainable efficiency E max is less than 1. <p> There can however be exceptions; for instance, it is shown in [18] that the FFT algorithm has a polynomial isoefficiency on a hypercube but an exponential isoefficiency on a mesh. As shown in <ref> [18, 48, 19] </ref>, isoefficiency functions for a parallel algorithm can vary across architectures and understanding of this variation is of considerable practical importance. Thus, the concept of isoefficiency helps us in further sub-classifying the problems in class PE. <p> Thus, technology dependent constants like t c and t w can have a dramatic impact on the scalability of a parallel system. In <ref> [48] </ref> and [15], the impact of these parameters is discussed on the scalability of parallel shortest path algorithms, and matrix multiplication algorithms, respectively.
Reference: [49] <author> Xian-He Sun and John L. Gustafson. </author> <title> Toward a better parallel performance metric. </title> <journal> Parallel Computing, </journal> <volume> 17 </volume> <pages> 1093-1109, </pages> <month> December </month> <year> 1991. </year> <note> Also available as Technical Report IS-5053, </note> <institution> UC-32, Ames Laboratory, Iowa State University, Ames, IA. </institution>
Reference-contexts: From these results, it is inferred that it is better to have a parallel computer with fewer faster processors than one with many slower processors. We discuss this issue further in Section 7 and show that this inference is valid only for a certain class of parallel systems. In <ref> [49, 21] </ref>, Sun and Gustafson argue that traditional speedup is an unfair measure, as it favors slow processors and poorly coded programs. They derive some fair performance measures for parallel programs to correct this.
Reference: [50] <author> Xian-He Sun and L. M. Ni. </author> <title> Another view of parallel speedup. </title> <booktitle> In Supercomputing '90 Proceedings, </booktitle> <pages> pages 324-333, </pages> <year> 1990. </year>
Reference-contexts: For parallel systems with much worse isoefficiencies, the results provided by the two metrics may be quite different. In this case, the scaled-speedup vs. number of processors curve is sublinear. Two generalized notions of scaled speedup were considered by Gustafson et al. [20], Worley [58] and Sun and Ni <ref> [50] </ref>. They differ in the methods by which the problem size is scaled up with the number of processors. In one method, the size of the problem is increased to fill the available memory on the parallel computer. <p> The scalability issues for such problems have been 12 explored by Worley [58], Gustafson [22, 20], and Sun and Ni <ref> [50] </ref>. Another extreme in scaling the problem size is to try as big problems as can be handled in the memory. This is investigated by Worley [57, 58, 59], Gustafson [22, 20] and by Sun and Ni [50], and is called the memory-constrained case. <p> been 12 explored by Worley [58], Gustafson [22, 20], and Sun and Ni <ref> [50] </ref>. Another extreme in scaling the problem size is to try as big problems as can be handled in the memory. This is investigated by Worley [57, 58, 59], Gustafson [22, 20] and by Sun and Ni [50], and is called the memory-constrained case. Since the total memory of a parallel computer increases with increasing p, it is possible to solve bigger problems on parallel computer with bigger p.
Reference: [51] <author> Xian-He Sun and Diane Thiede Rover. </author> <title> Scalability of parallel algorithm-machine combinations. </title> <type> Technical Report IS-5057, </type> <institution> Ames Laboratory, Iowa State University, Ames, IA, </institution> <year> 1991. </year> <note> To appear in IEEE Transactions on Parallel and Distributed Systems. </note>
Reference-contexts: Based on the concept of fixed time sizeup, Gustafson [21] has developed the SLALOM benchmark for a distortion-free comparison of computers of widely varying speeds. Sun and Rover <ref> [51] </ref> define a scalability metric called the isospeed measure, which is the factor by which the problem size has to be increased so that the average unit speed of computation remains constant if the number of processors is raised from p to p 0 .
Reference: [52] <author> Zhimin Tang and Guo-Jie Li. </author> <title> Optimal granularity of grid iteration problems. </title> <booktitle> In Proceedings of the 1990 International Conference on Parallel Processing, </booktitle> <pages> pages I111-I118, </pages> <year> 1990. </year>
Reference-contexts: A number of researchers have analyzed the optimal number of processors required to minimize parallel execution time for a variety of problems <ref> [11, 42, 52, 38] </ref>. Flatt and Kennedy [11, 10] derive some important upper-bounds related to the performance of parallel computers in the presence of synchronization and communication overheads. <p> Determining the location of the knee is important when the same parallel computer is being used for many applications so that the processors can be partitioned among the applications in an optimal fashion. Tang and Li <ref> [52] </ref> prove that maximizing the efficiency to T P ratio is equivalent to minimizing p (T P ) 2 . They propose minimization of a more general expression; i.e., p (T P ) r . Here r determines the relative importance of efficiency and the parallel execution time. <p> Note that for any fixed problem size W , the speedup on a parallel system will saturate or peak at some value S max (W ), which can also be used as a metric. Scalability issues for the fixed problem size case are addressed in <ref> [11, 27, 16, 42, 52, 58] </ref>. Another possible scenario is that in which a parallel computer with a fixed number of processors is being used and the best parallel algorithm needs to be chosen for solving a particular problem. <p> For such systems, it is natural to use isoefficiency function or related metrics [31, 28, 8]. The analyses in <ref> [60, 61, 11, 38, 42, 52, 9] </ref> also attempt to study the behavior of a parallel system with some concern for overall efficiency. Although different scalability measures are appropriate for rather different situations, many of them are related to each other. <p> Several researchers have proposed to use an operating point where the value of p (T P ) r is minimized for some constant r and for a given problem size W <ref> [11, 9, 52] </ref>. It can be shown [52] that this corresponds to the point where ES r1 is maximized for a given problem size. <p> Several researchers have proposed to use an operating point where the value of p (T P ) r is minimized for some constant r and for a given problem size W [11, 9, 52]. It can be shown <ref> [52] </ref> that this corresponds to the point where ES r1 is maximized for a given problem size. Note that the location of the minima of p (T P ) r (with respect to p) for two different algorithm-architecture combinations can be used to choose one between the two.
Reference: [53] <author> Fredric A. Van-Catledge. </author> <title> Towards a general model for evaluating the relative performance of computer systems. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 3(2) </volume> <pages> 100-108, </pages> <year> 1989. </year>
Reference-contexts: For many parallel algorithms, it is a function of p and W . Hence for these algorithms, the expression for p smax derived under the assumption of a constant might yield a result quite different from the actual p smax . Van-Catledge <ref> [53] </ref> developed a model to determine the relative performance of parallel computers for an application w.r.t the performance on selected supercomputer configurations for the same application. The author presented a measure for evaluating the performance of a parallel computer called the equal performance condition. <p> As a refinement of the model of Gustafson et al., Zhou [60] and Van Catledge <ref> [53] </ref> present models that predict performance as a function of the serial fraction of the parallel algorithm. They conclude that by increasing the problem size, one can obtain speedups arbitrarily close to the number of processors. <p> The analysis in [60] and <ref> [53] </ref> does not take the communication overheads into account. The serial component W serial of an algorithm contributes W serial (p 1) pW serial to the total overhead cost of parallel execution using p processors. <p> Another issue in the cost vs. performance analysis of parallel systems is to determine the tradeoffs between the speed of processors and the number of processors to be employed for a given budget. 16 From the analysis in [3] and <ref> [53] </ref>, it may appear that higher performance is always obtained by using fewer and faster processors. It can be shown that this is true only for those parallel systems in which the overall overhead T o grows faster than or equal to fi (p). The model considered in [53] is one <p> [3] and <ref> [53] </ref>, it may appear that higher performance is always obtained by using fewer and faster processors. It can be shown that this is true only for those parallel systems in which the overall overhead T o grows faster than or equal to fi (p). The model considered in [53] is one for which T o = fi (p), as they consider the case of a constant serial component. As an example of a parallel system where their conclusion is not applicable, consider the matrix multiplication on a SIMD mesh architecture [1].
Reference: [54] <author> Jeffrey Scott Vitter and Roger A. Simons. </author> <title> New classes for parallel complexity: A study of unification and other complete problems for P. </title> <journal> IEEE Transactions on Computers, </journal> <month> May </month> <year> 1986. </year>
Reference-contexts: For a perfectly parallelizable algorithm with no communication, isospeed (p; p 0 0 p W p . For more realistic parallel systems, isospeed (p; p 0 0 0 p . The class PC* of problems defined by Vitter and Simons <ref> [54] </ref> captures a class of problems with efficient parallel algorithms on a PRAM.
Reference: [55] <author> Jinwoon Woo and Sartaj Sahni. </author> <title> Hypercube computing: Connected components. </title> <journal> Journal of Supercomputing, </journal> <note> 1991. Also available as TR 88-50 from the Department of Computer Science, </note> <institution> University of Minnesota, Minneapolis, MN. </institution>
Reference: [56] <author> Jinwoon Woo and Sartaj Sahni. </author> <title> Computing biconnected components on a hypercube. </title> <journal> Journal of Supercomputing, </journal> <month> June </month> <year> 1991. </year> <note> Also available as Technical Report TR 89-7 from the Department of Computer Science, </note> <institution> University of Minnesota, Minneapolis, MN. </institution>
Reference: [57] <author> Patrick H. Worley. </author> <title> Information Requirements and the Implications for Parallel Computation. </title> <type> PhD thesis, </type> <institution> Stanford University, Department of Computer Science, </institution> <address> Palo Alto, CA, </address> <year> 1988. </year>
Reference-contexts: The scalability issues for such problems have been 12 explored by Worley [58], Gustafson [22, 20], and Sun and Ni [50]. Another extreme in scaling the problem size is to try as big problems as can be handled in the memory. This is investigated by Worley <ref> [57, 58, 59] </ref>, Gustafson [22, 20] and by Sun and Ni [50], and is called the memory-constrained case. Since the total memory of a parallel computer increases with increasing p, it is possible to solve bigger problems on parallel computer with bigger p. <p> A direct corollary of the above result is that if the isoefficiency function is greater than fi (p), then the minimum parallel execution time will increase even if the problem size is increased as slowly as linearly with the number of processors. Worley <ref> [57, 58, 59] </ref> has shown that for many algorithms used in the scientific domain, for any given T P , there will exist a problem size large enough so that it cannot be solved in time T P , no matter how many processors are used.
Reference: [58] <author> Patrick H. Worley. </author> <title> The effect of time constraints on scaled speedup. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 11(5) </volume> <pages> 838-858, </pages> <year> 1990. </year>
Reference-contexts: In the last decade, there has been a growing realization that for a variety of parallel systems, given any number of processors p, speedup arbitrarily close to p can be obtained by simply executing the parallel algorithm on big enough problem instances <ref> [47, 31, 36, 45, 22, 12, 38, 41, 40, 58] </ref>. Kumar and Rao [31] developed a scalability metric relating the problem size to the number of processors necessary for an increase in speedup in proportion to the number of processors. This metric is known as the isoefficiency function. <p> For parallel systems with much worse isoefficiencies, the results provided by the two metrics may be quite different. In this case, the scaled-speedup vs. number of processors curve is sublinear. Two generalized notions of scaled speedup were considered by Gustafson et al. [20], Worley <ref> [58] </ref> and Sun and Ni [50]. They differ in the methods by which the problem size is scaled up with the number of processors. In one method, the size of the problem is increased to fill the available memory on the parallel computer. <p> In this context, this work can be categorized with that of other researchers who have investigated the minimum execution time as a function of the size of the problem on an architecture with an unlimited number of processors available <ref> [58, 16, 11] </ref>. According to Nussbaum and Agarwal, the scalability of an algorithm is captured in its performance on the PRAM. <p> Note that for any fixed problem size W , the speedup on a parallel system will saturate or peak at some value S max (W ), which can also be used as a metric. Scalability issues for the fixed problem size case are addressed in <ref> [11, 27, 16, 42, 52, 58] </ref>. Another possible scenario is that in which a parallel computer with a fixed number of processors is being used and the best parallel algorithm needs to be chosen for solving a particular problem. <p> The scalability issues for such problems have been 12 explored by Worley <ref> [58] </ref>, Gustafson [22, 20], and Sun and Ni [50]. Another extreme in scaling the problem size is to try as big problems as can be handled in the memory. <p> The scalability issues for such problems have been 12 explored by Worley [58], Gustafson [22, 20], and Sun and Ni [50]. Another extreme in scaling the problem size is to try as big problems as can be handled in the memory. This is investigated by Worley <ref> [57, 58, 59] </ref>, Gustafson [22, 20] and by Sun and Ni [50], and is called the memory-constrained case. Since the total memory of a parallel computer increases with increasing p, it is possible to solve bigger problems on parallel computer with bigger p. <p> A direct corollary of the above result is that if the isoefficiency function is greater than fi (p), then the minimum parallel execution time will increase even if the problem size is increased as slowly as linearly with the number of processors. Worley <ref> [57, 58, 59] </ref> has shown that for many algorithms used in the scientific domain, for any given T P , there will exist a problem size large enough so that it cannot be solved in time T P , no matter how many processors are used.
Reference: [59] <author> Patrick H. Worley. </author> <title> Limits on parallelism in the numerical solution of linear PDEs. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 12 </volume> <pages> 1-35, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: The scalability issues for such problems have been 12 explored by Worley [58], Gustafson [22, 20], and Sun and Ni [50]. Another extreme in scaling the problem size is to try as big problems as can be handled in the memory. This is investigated by Worley <ref> [57, 58, 59] </ref>, Gustafson [22, 20] and by Sun and Ni [50], and is called the memory-constrained case. Since the total memory of a parallel computer increases with increasing p, it is possible to solve bigger problems on parallel computer with bigger p. <p> A direct corollary of the above result is that if the isoefficiency function is greater than fi (p), then the minimum parallel execution time will increase even if the problem size is increased as slowly as linearly with the number of processors. Worley <ref> [57, 58, 59] </ref> has shown that for many algorithms used in the scientific domain, for any given T P , there will exist a problem size large enough so that it cannot be solved in time T P , no matter how many processors are used.
Reference: [60] <author> Xiaofeng Zhou. </author> <title> Bridging the gap between Amdahl's law and Sandia laboratory's result. </title> <journal> Communications of the ACM, </journal> <volume> 32(8) </volume> <pages> 1014-5, </pages> <year> 1989. </year>
Reference-contexts: As a refinement of the model of Gustafson et al., Zhou <ref> [60] </ref> and Van Catledge [53] present models that predict performance as a function of the serial fraction of the parallel algorithm. They conclude that by increasing the problem size, one can obtain speedups arbitrarily close to the number of processors. <p> isoefficiency analysis not only suggests that a higher rate of 5 Very often, apart from p, t o is also a function of W . 11 growth of W w.r.t p will be required, but also provides an expression for this rate of growth, thus answering the question posed in <ref> [60] </ref> as to how fast the problem should grow to attain speedups close to p. The analysis in [60] and [53] does not take the communication overheads into account. <p> is also a function of W . 11 growth of W w.r.t p will be required, but also provides an expression for this rate of growth, thus answering the question posed in <ref> [60] </ref> as to how fast the problem should grow to attain speedups close to p. The analysis in [60] and [53] does not take the communication overheads into account. The serial component W serial of an algorithm contributes W serial (p 1) pW serial to the total overhead cost of parallel execution using p processors. <p> For such systems, it is natural to use isoefficiency function or related metrics [31, 28, 8]. The analyses in <ref> [60, 61, 11, 38, 42, 52, 9] </ref> also attempt to study the behavior of a parallel system with some concern for overall efficiency. Although different scalability measures are appropriate for rather different situations, many of them are related to each other.
Reference: [61] <author> J. R. Zorbas, D. J. Reble, and R. E. VanKooten. </author> <title> Measuring the scalability of parallel computer systems. </title> <booktitle> In Supercomputing '89 Proceedings, </booktitle> <pages> pages 832-841, </pages> <year> 1989. </year> <month> 21 </month>
Reference-contexts: But as shown in [19], this algorithm-architecture combination has an isoefficiency function of fi (p log p) and can be considered quite scalable. Zorbas et al. <ref> [61] </ref> developed the following framework to characterize the scalability of parallel systems. A parallel algorithm with a serial component W serial and a parallelizable component W parallel , when executed on one processor, takes t c (W serial +W parallel ) time. Here t c is a positive constant. <p> For such systems, it is natural to use isoefficiency function or related metrics [31, 28, 8]. The analyses in <ref> [60, 61, 11, 38, 42, 52, 9] </ref> also attempt to study the behavior of a parallel system with some concern for overall efficiency. Although different scalability measures are appropriate for rather different situations, many of them are related to each other.
References-found: 61

