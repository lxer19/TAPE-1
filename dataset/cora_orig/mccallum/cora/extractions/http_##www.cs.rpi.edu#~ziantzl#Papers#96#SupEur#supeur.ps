URL: http://www.cs.rpi.edu/~ziantzl/Papers/96/SupEur/supeur.ps
Refering-URL: http://www.cs.rpi.edu/~ziantzl/Papers/papers.html
Root-URL: http://www.cs.rpi.edu
Email: email: fszymansk,deelmane,flaherje,nortonc,terescoj,ziantzlg@cs.rpi.edu  
Author: B. K. Szymanski, E. Deelman, J. E. Flaherty, C. D. Norton, J. D. Teresco and L. H. Ziantz 
Address: Troy, NY 12180  
Affiliation: Department of Computer Science Rensselaer Polytechnic Institute  
Note: Parallel Scientific Computing on the IBM SP2 at Rensselaer's Scientific Computation Research Center  
Abstract: Various research applications using the adaptive solution of partial differential equations and parallel discrete event simulation as well as the investigation of object oriented programming paradigms are described. This work utilizes the IBM SP2 at Rensselaer's Scientific Computation Research Center (SCOREC). The software is designed using C, C++ and Fortran 90, together with MPI. Tools described include a parallel mesh database, load balancers and run-time code optimizers. Additionally, using plasma codes as an example, we compare compilers and architecture performance of the SP2 to competing distributed memory parallel machines. Applications studying finite element analysis of problems in biomechanics and fluid dynamics, the simulation of fusion plasma processes and the spread of Lyme disease are presented.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. Clark, J. E. Flaherty, and M. S. Shephard. </author> <title> Applied Numerical Mathematics, special ed. on Adaptive Methods for Partial Differential Equations, </title> <booktitle> 14 </booktitle> <pages> 255-283, </pages> <year> 1994. </year>
Reference-contexts: During the solution process, portions of the discrete domain are spatially refined or coarsened (h-refinement), the method order is varied (p-refinement), and/or the mesh is moved to follow evolving phenomena (r-refinement), each of which concentrates the computational effort in regions where the solution resolution is inadequate <ref> [1] </ref>. In order to solve large problems within a reasonable period of time, these methods have been implemented on parallel computers. While parallelism can yield faster solution times or greater accuracy in the same computing time, there are complications such as coordinating interprocessor communication and managing distributed data structures.
Reference: [2] <author> M. W. Beall and M. S. Shephard. </author> <title> Mesh Data Structures for Advanced Finite Element Applications. </title> <type> SCOREC Report 23-1995. </type> <institution> Scientific Computation Research Center, Rens-selaer Polytechnic Institute, Troy, </institution> <year> 1995. </year>
Reference-contexts: Prototype models for tissue-engineering applications such as the control of wound contraction and for the engineering design of a bioartificial artery are under investigation. SCOREC Mesh Tools The system we are using is built upon the SCOREC Mesh Database (MDB) <ref> [2] </ref> which provides a hierarchical representation of a finite element mesh. It also includes a set of operators to query and update the mesh data structure. The basic mesh entity hierarchy consists of three-dimensional regions, and their bounding faces, edges, and vertices.
Reference: [3] <author> M. S. Shephard , J. E. Flaherty, H. L. de Cougny, C. Ozturan, C. L. Bottasso, and M. W. Beall. </author> <title> Parallel automated adaptive procedures for unstructured Meshes. </title> <booktitle> Parallel Computing in CFD, </booktitle> <address> R-807, AGARD, Neuilly-Sur-Seine, 6.1-6.49, </address> <year> 1995. </year>
Reference-contexts: In our case, each region is a tetrahedral finite element. Mesh entities are explicitly classified relative to a geometric model of the problem domain. This classification assures that the mesh remains valid after h-refinement. Meshes are created using the SCOREC Finite Octree Automatic Mesh Generator <ref> [3] </ref>. Beginning with a geometric model of the domain obtained from CAD software, the mesh generator creates a variable level octree that is consistent with the triangulation on the boundary of the domain. Octants are classified as interior, exterior, or boundary. <p> Octants are classified as interior, exterior, or boundary. Exterior octants receive no further consideration while interior octants are discretized using templates. Face removal procedures are used to connect the boundary triangulation to the interior octants. Mesh enrichment <ref> [3] </ref> performs spatial (h-) refinement and coarsening in parallel using error indicator information and enrichment threshold values. From this information, edges are marked to be coarsened, refined, or to remain untouched. <p> Since the mesh is distributed, the rebalancing algorithm must be parallel. Several dynamic load balancers are available and make use of PMDB data structures and mesh migration operators. Iterative Tree Balancing (ITB) [4] migrates entities to neighboring partitions to achieve balance. Parallel Sort Inertial Recursive Bisection (PSIRB) <ref> [3] </ref> is a parallel repartitioner based on the IRB static partitioner. The Octree partitioner [9] makes use of the octree structure underlying the mesh to achieve load balance. The first step in a dynamic rebalancing is to determine the measure of imbalance.
Reference: [4] <author> C. L. Bottasso, H. L. de Cougny, M. Dindar, J. E. Flaherty, C. Ozturan, Z. Rusak, and M. S. Shephard. </author> <title> Compressible aerodynamics using a parallel adaptive time-discontinuous Galerkin least-squares finite element method. </title> <type> Paper 94-1888, </type> <institution> Twelfeth AIAA App. Aero-dyn. Conf., Colorado Springs, </institution> <year> 1994. </year>
Reference-contexts: In stage 5 of the enrichment process, vertices created by the refinement process on a curved model boundary must be "snapped" to the appropriate model entity to ensure mesh validity. The Parallel Mesh Database A Parallel Mesh Database (PMDB) <ref> [4] </ref>, which provides operators to create and manipulate distributed meshes, is built on top of the MDB. Once an adaptive FEM calculation begins, it consists of alternating phases of computation, mesh enrichment, and load balancing. PMDB data structures and operators are used in all stages of this process. <p> Partitioning can become imbalanced at each adaptive step of the computation. Since the mesh is distributed, the rebalancing algorithm must be parallel. Several dynamic load balancers are available and make use of PMDB data structures and mesh migration operators. Iterative Tree Balancing (ITB) <ref> [4] </ref> migrates entities to neighboring partitions to achieve balance. Parallel Sort Inertial Recursive Bisection (PSIRB) [3] is a parallel repartitioner based on the IRB static partitioner. The Octree partitioner [9] makes use of the octree structure underlying the mesh to achieve load balance. <p> This process is repeated until the load becomes balanced to within a tolerance, or until the maximum number of iterations has been performed. The iterative migration can also be run periodically with only a few iterations to help maintain balance between substages of an operation like mesh enrichment (cf. <ref> [4] </ref>) without a large run-time penalty. PSIRB will produce the same partitions as the static IRB partitioner, but the coordinates in the inertial frame are sorted in parallel, allowing the algorithm to be used as a dynamic repartitioner. Thus, the mesh can be distributed before PSIRB is performed.
Reference: [5] <author> W. Gropp, E. Lusk, and A. Skjellum. </author> <title> Using MPI. </title> <publisher> The MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: Other information available include processor adjacencies and lists of off-processor duplicates of an entity. Fast traversal of entities on interprocessor boundaries is also provided. All interprocessor communication is done using the IBM implementation of the Message Passing Interface (MPI) <ref> [5] </ref>. pointers between boundary entities. Heavy lines and points indicate the unique owner of each shared boundary entity (left). Example of arbitrary multiple migration (right). Since meshes change during an analysis, portions will need to be moved from one processor to another to maintain load balance.
Reference: [6] <author> C. L. Bottasso, J. E. Flaherty, C. Ozturan, M. S. Shephard, B. K. Szymanski, J. D. Teresco, and L. H. Ziantz. </author> <title> The quality of partitions produced by an iterative load balancer. </title> <editor> In B. K. Szymanski and B. Sinharoy, Eds., </editor> <booktitle> Proceedings Third Workshop on Languages, Compilers, and Runtime Systems, </booktitle> <publisher> Kluwer, Boston, </publisher> <pages> pp. 265-277, </pages> <year> 1995. </year>
Reference-contexts: Routines for partition quality analysis are also in PMDB to produce qualitative (visual) and quantitative measurements <ref> [6] </ref>. Quality metrics include a ratio of the number of mesh faces on interprocessor boundaries to the total number of faces in the mesh, a measure of the average number of processors that need to exchange data during the solution stage, and the number of connected components within a partition.
Reference: [7] <author> A. Pothen, H. Simon, and K.-P. Liou. </author> <title> Partitioning sparse matrices with eigenvectors of graphs. </title> <journal> SIAM J. on Matrix Anal. and Applics., </journal> <volume> 11(3) </volume> <pages> 430-452, </pages> <year> 1990. </year>
Reference-contexts: Inertial Recursive Bisection (IRB) repeatedly bisects the mesh orthogonal to a principal axis of inertia of the domain. Recursive Spectral Bisection (RSB) <ref> [7] </ref> is generally considered as one of the best static mesh partitioners. However, RSB can be computationally costly, and is usually prohibitively expensive when used with adaptive computation. Static versions of ORB and IRB are available in PMDB while RSB is available in the Chaco [8] package.
Reference: [8] <author> B. Hendrickson and R. Leland. </author> <title> The Chaco user's guide, version 1.0. </title> <type> Technical Report SAND93-2339, </type> <institution> Sandia National Laboratories, </institution> <address> Albuquerque, New Mexico, </address> <year> 1993. </year>
Reference-contexts: Recursive Spectral Bisection (RSB) [7] is generally considered as one of the best static mesh partitioners. However, RSB can be computationally costly, and is usually prohibitively expensive when used with adaptive computation. Static versions of ORB and IRB are available in PMDB while RSB is available in the Chaco <ref> [8] </ref> package. Partitioning can become imbalanced at each adaptive step of the computation. Since the mesh is distributed, the rebalancing algorithm must be parallel. Several dynamic load balancers are available and make use of PMDB data structures and mesh migration operators.
Reference: [9] <author> K. D. Devine, J. E. Flaherty, R. M. Loy, and S. R. Wheat. </author> <title> Parallel partitioning strategies for the adaptive solution of conservation laws. </title> <editor> In I. Babuska, J.E. Flaherty, W.D. Hen-shaw, J.E. Hopcroft, J.E. Oliger, and T. Tezduyar, Eds., </editor> <title> Modeling, Mesh Generation, and Adaptive Numerical Methods for Partial Differential Equations, </title> <publisher> Springer-Verlag, Berlin-Heidelberg, </publisher> <pages> pp. 215-242, </pages> <year> 1995. </year>
Reference-contexts: Iterative Tree Balancing (ITB) [4] migrates entities to neighboring partitions to achieve balance. Parallel Sort Inertial Recursive Bisection (PSIRB) [3] is a parallel repartitioner based on the IRB static partitioner. The Octree partitioner <ref> [9] </ref> makes use of the octree structure underlying the mesh to achieve load balance. The first step in a dynamic rebalancing is to determine the measure of imbalance. This requires a "cost function" that reflects the computational load on each processor. <p> The aim is to examine the transient behavior of the shock as it traverses the cone. The flow is governed by the Euler equations <ref> [9] </ref>. Using symmetry, we solve for the flow about one half of the cone with an initial mesh containing 28,437 elements. The domain is quiescent initially, and a Mach 1.25 flow behind the Mach 2 shock enters the top of the flow domain (see Figure 3). <p> Outflow conditions apply at the bottom face. The transient analysis begins with a time step being attempted by the solver using the initial mesh with the initial conditions and boundary conditions. Each element has a maximum allowable time step size based on the Courant condition <ref> [9] </ref>. The minimal acceptable time step is selected as a global increment. The time step is either accepted or rejected based on an error indicator, which is currently the density gradient across element faces scaled by the element volume. Scaling forces smaller elements to have a lower error indicator.
Reference: [10] <author> E. Leiss and H. Reddy. </author> <title> Distributed load balancing: design and performance analysis. </title> <editor> W. M. </editor> <booktitle> Kuck Research Computation Laboratory, </booktitle> <volume> 5 </volume> <pages> 205-270, </pages> <year> 1989. </year>
Reference-contexts: Predictive load balancing methods, currently an area of research, require elements to be weighted by the expected cost at some future time in the computation. PMDB provides an element weighting scheme which can be used to address each of these needs. Like Leiss and Reddy <ref> [10] </ref>, lightly loaded processors request load from their most heavily loaded neighbors in ITB. However, instead of considering an immediate neighborhood of processors, the algorithm views the requests as forming a forest of trees.
Reference: [11] <author> C. D. Norton, B. K. Szymanski, and V. K. Decyk. </author> <title> Object Oriented Parallel Computation for Plasma Simulation. </title> <journal> Communications of the ACM, </journal> <volume> 38(10) </volume> <pages> 88-100, </pages> <month> October </month> <year> 1995. </year>
Reference-contexts: We have been investigating traditional and modern object oriented paradigms in modeling plasma particle-in-cell (PIC) simulations <ref> [11] </ref>. This work is in loose collaboration with the Numerical Tokamak Project (NTP) [12], which is an HPCC research effort to model and understand the transport of particles and energy in a tokamak fusion energy device.
Reference: [12] <author> B. I. Cohen, D. C. Barnes, J. M. Dawson, G. W. Hammett, W. W. Lee, G. D. Kerbel, J.- N. Leboeuf, P. C. Liewer, T. Tajima, and R. E. Waltz. </author> <title> The numerical tokamak project: simulation of turbulent transport. </title> <journal> Computer Physics Communications, </journal> 87(1&2):1-15, May II 1995. 
Reference-contexts: We have been investigating traditional and modern object oriented paradigms in modeling plasma particle-in-cell (PIC) simulations [11]. This work is in loose collaboration with the Numerical Tokamak Project (NTP) <ref> [12] </ref>, which is an HPCC research effort to model and understand the transport of particles and energy in a tokamak fusion energy device. The PIC model [13] follows the trajectories of millions of particles in their self-consistent electromagnetic fields (both external and self-generated).
Reference: [13] <author> C. K. Birdsall and A. B. Langdon. </author> <title> Plasma Physics via Computer Simulation. </title> <booktitle> The Adam Hilger Series on Plasma Physics. </booktitle> <address> Adam Hilger, New York, </address> <year> 1991. </year>
Reference-contexts: This work is in loose collaboration with the Numerical Tokamak Project (NTP) [12], which is an HPCC research effort to model and understand the transport of particles and energy in a tokamak fusion energy device. The PIC model <ref> [13] </ref> follows the trajectories of millions of particles in their self-consistent electromagnetic fields (both external and self-generated). Since the computation required is extremely large, both in terms of the field sizes and number of particles, the only alternative is to use massively parallel computers.
Reference: [14] <author> P. C. Liewer and V. K. Decyk. </author> <title> A General Concurrent Algorithm for Plasma Particle-in-Cell Simulation Codes. </title> <journal> J. of Computational Physics, </journal> <volume> 85 </volume> <pages> 302-322, </pages> <year> 1989. </year>
Reference-contexts: Object Oriented Plasma Modeling Computational modeling of plasma using the General Concurrent Particle-in-Cell model <ref> [14] </ref> involves operations on particles and fields that are partitioned across a distributed memory architecture. Finding appropriate abstractions to represent both the physical and computational/numerical aspects of this problem is challenging due to complex interactions among simulation components.
Reference: [15] <author> J. Rumbaugh, M. Blaha, W. Premerlani, F. Eddy, and W. Lorensen. </author> <title> Object-Oriented Modeling and Design. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1991. </year>
Reference-contexts: Finding appropriate abstractions to represent both the physical and computational/numerical aspects of this problem is challenging due to complex interactions among simulation components. In Figure 4 the class design for a three-dimensional parallel code is presented using the Object Modeling Technique (OMT) notation <ref> [15] </ref>. The Plasma class performs operations on the collection of particles to update their positions and velocities during a simulation time step, monitor energy diagnostics and to load balance particles across processors based on the domain decomposition of the particle partitioning.
Reference: [16] <author> V. K. Decyk, C. D. Norton, and B. K. Szymanski. </author> <title> Experiences with Object Oriented Parallel Plasma PIC Simulation. </title> <booktitle> In Proc. Computing in High Energy Physics (CHEP '95), </booktitle> <address> Singapore, Sept. 18-25 1995. Lafex/Fermilab, </address> <publisher> World Scientific. </publisher>
Reference: [17] <author> V. K. Decyk, C. D. Norton, and B. K. Szymanski. </author> <title> Introduction to Object-Oriented Concepts Using Fortran 90. </title> <type> Technical Report 10-1996, </type> <institution> Scientific Computation Research Center (SCOREC), Rensselaer Polytechnic Institute, </institution> <address> Troy, NY 12180-3590, </address> <month> June </month> <year> 1996. </year> <note> Submitted to Computers in Physics. </note>
Reference-contexts: Fortran 77 call pdost2 (part,q,npp,noff,qme,nx,idimp,npmax,nblok,nxv,nypmx) C++ plasma.ChargeDeposition (electrons, cdensity); Fortran 90 call plasma dpost2 (electrons,cdensity,edges) Table 1: Object/function calls in two-dimensional charge deposition operation. Fortran 90 provides a number of important features beyond Fortran 77 that allow for object oriented programming <ref> [17] </ref>. Encapsulation, overloading, generics, inheritance, dynamic memory management, and a rich set of array operations, have opened new design alternatives for scientists looking to modernize their existing applications. Since Fortran 90 is backward compatible with Fortran 77, the new features can be incrementally introduced.
Reference: [18] <author> A. Barbour and D. Fish. </author> <title> The biological and social phenomenon of Lyme disease. </title> <journal> Science, </journal> <volume> 260 </volume> <pages> 1610-1616, </pages> <year> 1993. </year>
Reference-contexts: We plan to investigate the design effects of abstraction, how abstraction effects efficiency and the integration of multidisciplinary codes in scientific computing at Rensselaer. 3 Simulating Lyme Disease Lyme Disease Lyme disease is heavily studied in the United States, particularly in the Northeast, where the disease is prevalent <ref> [18, 19] </ref>. It is transmitted to humans by infected deer ticks (Ixodes scapularis). The disease is sustained by the cycle of infection between ticks and white-footed mice (Peromyscus leucopus) (Figure 5) [20]. The tick life cycle spans two years.
Reference: [19] <author> G.L. Miller, R.B. Craven, R.E. Bailey, and T.F. Tsai. </author> <title> The epidemiology of Lyme disease in the United States 1987-1998. </title> <journal> Laboratory Medicine, </journal> <volume> 21 </volume> <pages> 285-289, </pages> <year> 1990. </year>
Reference-contexts: We plan to investigate the design effects of abstraction, how abstraction effects efficiency and the integration of multidisciplinary codes in scientific computing at Rensselaer. 3 Simulating Lyme Disease Lyme Disease Lyme disease is heavily studied in the United States, particularly in the Northeast, where the disease is prevalent <ref> [18, 19] </ref>. It is transmitted to humans by infected deer ticks (Ixodes scapularis). The disease is sustained by the cycle of infection between ticks and white-footed mice (Peromyscus leucopus) (Figure 5) [20]. The tick life cycle spans two years.
Reference: [20] <editor> F.S. Kantor. Disarming Lyme Disease. </editor> <publisher> Scientific American, </publisher> <pages> pages 34-39, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: It is transmitted to humans by infected deer ticks (Ixodes scapularis). The disease is sustained by the cycle of infection between ticks and white-footed mice (Peromyscus leucopus) (Figure 5) <ref> [20] </ref>. The tick life cycle spans two years. Larvae hatch in the middle of the summer, then start looking for a blood meal. When a larva finds a suitable host, such as a mouse, it bites it, takes a blood meal, and drops off.
Reference: [21] <author> R.S. Ostfeld, O.M. Cepeda, K.R. Hazler, and M.C. Miller. </author> <title> Ecology of Lyme disease: habitat associations of ticks (Ixodes scapularis) in a rural landscape. </title> <journal> Ecol. Applications, </journal> <volume> 5 </volume> <pages> 353-361, </pages> <year> 1995. </year>
Reference-contexts: Mice, during that time, are looking for nesting sites and may carry ticks a considerable distance <ref> [21] </ref>. On the first day of the simulation, mice and nymphs are present. Infected nymphs can infect uninfected mice, and infected mice can infect uninfected nymphs. At about the 90th day, uninfected larvae hatch. If they feed on infected mice, they acquire the disease.
Reference: [22] <author> C.G. Cassandras. </author> <title> Discrete Event Systems: Modeling and Performance Analysis. </title> <year> 1993. </year>
Reference-contexts: We want determine how many mice are needed to sustain the disease, or, conversely, under what conditions the disease will die off. The level of desired detail led us to discrete event simulation <ref> [22] </ref> and individual-based modeling [23]. Mice are treated as individuals and are the objects of the simulation. The following events are associated with mice: dispersal (locating a new nesting site), movement (from one location to another), being bitten by a tick (larval or nymphal), dropping a tick, and death.
Reference: [23] <author> O.P. Judson. </author> <title> The rise of the individual-based model in ecology. </title> <booktitle> Trends in Ecology and Evolution, </booktitle> <volume> 9 </volume> <pages> 9-14, </pages> <year> 1994. </year>
Reference-contexts: We want determine how many mice are needed to sustain the disease, or, conversely, under what conditions the disease will die off. The level of desired detail led us to discrete event simulation [22] and individual-based modeling <ref> [23] </ref>. Mice are treated as individuals and are the objects of the simulation. The following events are associated with mice: dispersal (locating a new nesting site), movement (from one location to another), being bitten by a tick (larval or nymphal), dropping a tick, and death.
Reference: [24] <author> R. M. Fujimoto. </author> <title> Parallel Discrete Event Simulation. </title> <journal> Communications of the ACM, </journal> <volume> 33(10) </volume> <pages> 31-53, </pages> <year> 1990. </year>
Reference-contexts: Events are removed from the event queue and processed. The processing of an event causes a state change. LPs communicate with each other via event messages. To address the causality constraints between events, two major protocols have been developed <ref> [24] </ref>. The first protocol, known as conservative, preserves the causality between events by frequently synchronizing the simulation processes [25]. In distributed memory machines, such as the IBM SP2, the overhead of frequent synchronization can cause significant performance degradation.
Reference: [25] <author> K. M. Chandy and J. Misra. </author> <title> Distributed Simulation: A Case Study in Design and Verification of Distributed Programs. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 5 </volume> <pages> 440-452, </pages> <year> 1979. </year>
Reference-contexts: The processing of an event causes a state change. LPs communicate with each other via event messages. To address the causality constraints between events, two major protocols have been developed [24]. The first protocol, known as conservative, preserves the causality between events by frequently synchronizing the simulation processes <ref> [25] </ref>. In distributed memory machines, such as the IBM SP2, the overhead of frequent synchronization can cause significant performance degradation.
Reference: [26] <author> D.R. Jefferson. </author> <title> Virtual Time. </title> <journal> Trans. Prog. Lang. and Syst., </journal> <volume> 7 </volume> <pages> 404-425, </pages> <year> 1985. </year>
Reference-contexts: This degradation and the fact that, in our application, many events can happen concurrently within an LP, led us to the implementation and use of the second protocol, known as optimistic <ref> [26] </ref>. In this approach, causality errors are allowed to occur. When an LP receives an event with a time lower than its logical time, it has to roll back the computation to the time of event, process the event, and then continue the regular processing.
Reference: [27] <author> E. Deelman, T. Caraco, and B. K. Szymanski. </author> <title> Parallel Discrete Event Simulation of Lyme Disease. </title> <booktitle> Pacific Biocomputing Conference, </booktitle> <year> 1996. </year>
Reference-contexts: Figure 6 presents an overview of an LP. The Processed Event List and Message List are also needed to support rollback. Results The aim of this project is not only to obtain biologically significant results <ref> [27] </ref>, but also to achieve good performance and to contribute to the field of PDES. As mentioned earlier, rollback is a vital part of optimistic PDES. In order to support the LP's ability to go backward as well as forward in time, state information has to be saved.
Reference: [28] <author> J. S. Steinman, C. A. Lee, L. F. Wilson, and D. M. Nicol. </author> <title> Global Virtual Time and Distributed Synchronization. </title> <address> pages 139-148. </address> <year> 1995. </year>
Reference-contexts: The main difference between our algorithm and others is the fact that there is no explicit synchronization between the processes. In other algorithms, rounds of synchronization messages have to be sent <ref> [28] </ref>, during which time no new simulation messages can enter the system. By contrast, our algorithm runs continuously on every process in the system. When messages are sent between processes, some information about the state of the sending LP is appended to the message.
Reference: [29] <author> E. Deelman, B. K. Szymanski and T. Caraco. </author> <title> Simulating Lyme Disease Using Parallel Discrete Event Simulation. </title> <booktitle> to appear in Winter Simulation Conference, </booktitle> <year> 1996. </year>
Reference-contexts: In order to prevent processes from running far ahead of others, restrictions on the amount of virtual time that an LP can be ahead of others are made. Imposing these restrictions, which we call, curbing the optimism, is easy when using CMGVT <ref> [29] </ref>. Since each LP knows about the local time of all the other LPs, it can decide not to process events beyond a certain amount of time ahead of the other LPs. Performance results with an optimism interval of 20 days are presented in Figure 7.
References-found: 29

