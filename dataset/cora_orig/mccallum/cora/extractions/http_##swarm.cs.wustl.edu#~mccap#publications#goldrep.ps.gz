URL: http://swarm.cs.wustl.edu/~mccap/publications/goldrep.ps.gz
Refering-URL: http://swarm.cs.wustl.edu/~mccap/publications/
Root-URL: 
Email: pjm3@cs.wustl.edu  barry@cs.wustl.edu  
Title: A Neural Network Model for the Gold Market  
Author: Peter J. McCann Barry L. Kalman 
Keyword: Neural networks, financial analysis.  
Address: Campus Box 1045 St. Louis, Missouri 63130-4899  Campus Box 1045 St. Louis, Missouri 63130-4899  
Affiliation: Department of Computer Science Washington University  Department of Computer Science Washington University  
Abstract: A neural network trend predictor for the gold bullion market is presented. A simple recurrent neural network was trained to recognize turning points in the gold market based on a to-date history of ten market indices. The network was tested on data that was held back from training, and a significant amount of predictive power was observed. The turning point predictions can be used to time transactions in the gold bullion and gold mining company stock index markets to obtain a significant paper profit during the test period. The training data consisted of daily closing prices for the ten input markets for a period of about five years. The turning point targets were labeled for the training phase without the help of a financial expert. Thus, this experiment shows that useful predictions can be made without the use of more extensive market data or knowledge. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Bosarge, W.E. Jr. </author> <year> (1991). </year> <title> Adaptive Processes to Exploit the Nonlinear Structure of Financial Markets. Neural Networks and Pattern Recognition in Forecasting Financial Markets (February 1991), </title> <booktitle> Santa Fe Institute of Complexity Conference. </booktitle>
Reference-contexts: The author simply looked at the performance of the gold market and chose trading days on which it would have been profitable to buy or sell, given perfect 20/20 hindsight. This implicitly defined a trading frequency and therefore a risk factor <ref> [1] </ref>. Figure 1 is marked with the trading day targets that Table 1: The target output encodings. ENCODING MEANING (+1,-1) Gold is in a trough. BUY. (-1,+1) Gold is at a peak. SELL. (0,0) No decision. were chosen.
Reference: [2] <author> Collard, J.E. </author> <year> (1992). </year> <title> Commodity Trading with a Three Year Old. Neural Networks in Finance and Investment, </title> <editor> ed. R. Trippi and E. </editor> <booktitle> Turban, </booktitle> <pages> 411-420. </pages> <address> Chicago: </address> <publisher> Probus Publishing Co. </publisher>
Reference-contexts: The alternative to this approach would be a system that makes guesses about a large scale commodities market or an aggregate stock index <ref> [2] </ref>. The second distinction is the one between pattern classification approaches and those that attempt to predict a numeric value such as a stock price or dividend return. White [14] takes the time series prediction approach for IBM daily stock dividends.
Reference: [3] <author> Elman, J.L. </author> <year> (1990). </year> <title> Finding Structure in Time. </title> <booktitle> Cognitive Science 14, </booktitle> <pages> 179-211. </pages>
Reference: [4] <author> Hoptroff, R.G., Bramson, M.J., and Hall, T.J. </author> <year> (1991). </year> <title> Forecasting Economic Turning Points with Neural Nets. </title> <booktitle> Proceedings of the IEEE International Joint Conference on Neural Networks (Seattle 1991), </booktitle> <volume> vol. I, </volume> <pages> 347-352. </pages>
Reference: [5] <author> Kalman, B.L., and Kwasny, Stan C. </author> <year> (1993). </year> <title> TRAINREC: A System for Training Feedforward and Simple Recurrent Networks Efficiently and Correctly. </title> <type> Technical Report WUCS-93-26, </type> <institution> St. Louis: Department of Computer Science, Washington University. </institution>
Reference-contexts: As a result, the networks used tend to be of the simple feed-forward variety, and are usually trained with first order back-propagation algorithms. Except for Kamijo [11], very little work in this area has made use of recurrent networks. We believe our second order conjugate gradient training method <ref> [5] </ref> and our data pretreatment procedures using singular value decomposition [6] represent significant steps forward in this area. They allow us to train networks in fewer epochs and with poorer data, and to obtain better generalization than previous methods. 2 METHODS January 24, 1994.
Reference: [6] <author> Kalman, B.L., Kwasny, Stan C., and Abella, A. </author> <year> (1993). </year> <title> Decomposing input patterns to facilitate training. </title> <booktitle> Proceedings of the World Congress on Neural Networks, </booktitle> <volume> vol. III, </volume> <pages> 503-506. </pages> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference-contexts: Except for Kamijo [11], very little work in this area has made use of recurrent networks. We believe our second order conjugate gradient training method [5] and our data pretreatment procedures using singular value decomposition <ref> [6] </ref> represent significant steps forward in this area. They allow us to train networks in fewer epochs and with poorer data, and to obtain better generalization than previous methods. 2 METHODS January 24, 1994. Daily closing indices during the same period for nine other major markets were also included. <p> In addition, weights can be back-transformed into the original input space after training so that new data can be tested without re-applying SVD. The decomposed vectors in the new orthogonal basis set often allow for faster training. See <ref> [6] </ref> for a more complete discussion of the benefits of this pretreatment method. The first training attempt was performed with a slightly larger network than the one shown in figure 2. The first attempt had one more context unit than the final, most successful network.
Reference: [7] <author> Kalman, B.L., and Kwasny, Stan C. </author> <year> (1992). </year> <title> Why Tanh: Choosing a Sigmoidal Function. </title> <booktitle> Proceedings of the IEEE International Joint Conference on Neural Networks (Baltimore 1992), </booktitle> <volume> vol. IV, </volume> <pages> 578-581. </pages> <address> New York: </address> <publisher> IEEE. </publisher>
Reference-contexts: Note that our in-house training software uses a squashing function with domain (-1,+1). We have found that a hyperbolic tangent squashing function coupled with a scaled square error function to be much more effective in training. See <ref> [7] </ref> for a complete derivation of these embellishments on the standard neural network model. The 1369 labeled input patterns were then preprocessed by singular value decomposition. This is a linear data transformation that finds a new orthogonal basis set for the 1369 input vectors.
Reference: [8] <author> Kalman, B.L., and Kwasny, Stan C. </author> <year> (1991). </year> <title> A Superior Error Function for Training Neural Nets. </title> <booktitle> Proceedings of the IEEE International Joint Conference on Neural Networks (Seattle 1991), </booktitle> <volume> vol. II, </volume> <pages> 49-52. </pages> <address> New York: </address> <publisher> IEEE. </publisher>
Reference: [9] <author> Marquez, L., et. al. </author> <year> (1991). </year> <title> Neural Network Models as an Alternative to Regression. </title> <booktitle> Proceedings of the IEEE 24th Annual Hawaii International Conference on Systems Sciences (Hawaii, 1991), </booktitle> <volume> vol. VI, </volume> <pages> 129-135. </pages> <address> Hawaii: </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: On the other hand, a neural network is a complicated nonlinear function, sometimes with two or more nested levels of nonlinearity. There have been several direct comparisons made in the literature between neural network and linear regression techniques applied to data fitting <ref> [9] </ref>. This might be a mistake. The most successful financial neural network applications seem to be the ones that do not attempt exact numeric prediction of a price, but that rather attempt to recognize patterns in the input data that can give clues to where the market might be headed.
Reference: [10] <author> Odom, M.D., and Sharda, R. </author> <year> (1990). </year> <title> A Neural Network Model for Bankruptcy Prediction. </title> <booktitle> Proceedings of the IEEE International Joint Conference on Neural Networks (San Diego 1990), </booktitle> <volume> vol. II, </volume> <pages> 163-168. </pages> <address> New York: </address> <publisher> IEEE. </publisher>
Reference-contexts: The applications seem to fall naturally into several broad categories. First, there are individual or single corporation analyses, such as credit assessment and bankruptcy prediction, that consider input data for only one entity and attempt to make a guess about future performance <ref> [10] </ref>. The alternative to this approach would be a system that makes guesses about a large scale commodities market or an aggregate stock index [2].
Reference: [11] <author> Kamijo, K., and Tanigawa, T. </author> <year> (1992). </year> <title> Stock Price Pattern Recognition: A Recurrent Neural Network Approach. </title> <booktitle> Proceedings of the IEEE International Joint Conference on Neural Networks (San Diego 1990), </booktitle> <volume> vol. I, </volume> <pages> 215-221. </pages> <address> New York: </address> <publisher> IEEE. </publisher>
Reference-contexts: Also, most of the work to date has been directed at introducing the concept of a neural network to the financial analysis community. As a result, the networks used tend to be of the simple feed-forward variety, and are usually trained with first order back-propagation algorithms. Except for Kamijo <ref> [11] </ref>, very little work in this area has made use of recurrent networks. We believe our second order conjugate gradient training method [5] and our data pretreatment procedures using singular value decomposition [6] represent significant steps forward in this area. <p> This work also indicates that there is enough information to train networks successfully in merely the daily closing prices of the ten markets under consideration. The previous successes of Kamijo <ref> [11] </ref> and Trippi [12] were based heavily on additional factors like daily high and low prices. Because of this data, Trippi was able to make trades (and profits) on a daily basis, instead of the approximate four month trading cycle seen here.
Reference: [12] <author> Trippi, R., and DeSieno, D. </author> <year> (1992). </year> <title> Trading Equity Index Futures with a Neural Network. </title> <booktitle> Journal of Portfolio Management 19 (Fall 1992), </booktitle> <pages> 27-33. </pages>
Reference-contexts: The second distinction is the one between pattern classification approaches and those that attempt to predict a numeric value such as a stock price or dividend return. White [14] takes the time series prediction approach for IBM daily stock dividends. On the other hand, Trippi <ref> [12] </ref> developed a trading system based on pattern classification. He relied on his neural networks only for a long/short recommendation. Note that the two distinctions are more or less orthogonal in that there are applications representing each of the four combinations of features. <p> This work also indicates that there is enough information to train networks successfully in merely the daily closing prices of the ten markets under consideration. The previous successes of Kamijo [11] and Trippi <ref> [12] </ref> were based heavily on additional factors like daily high and low prices. Because of this data, Trippi was able to make trades (and profits) on a daily basis, instead of the approximate four month trading cycle seen here.
Reference: [13] <author> Trippi, R., and E. Turban (eds.) </author> <year> (1992). </year> <title> Neural Networks in Finance and Investment: Applying Artificial Intelligence to Improve Real-World Performance. </title> <publisher> Chicago: Probus Publishing Co. </publisher>
Reference: [14] <author> White, H. </author> <year> (1988). </year> <title> Economic Prediction Using Neural Networks: The Case of IBM Daily Stock Returns. </title> <booktitle> Proceedings of the IEEE International Conference on Neural Networks. (July 1988), </booktitle> <volume> vol. II, </volume> <pages> 451-458. </pages>
Reference-contexts: The second distinction is the one between pattern classification approaches and those that attempt to predict a numeric value such as a stock price or dividend return. White <ref> [14] </ref> takes the time series prediction approach for IBM daily stock dividends. On the other hand, Trippi [12] developed a trading system based on pattern classification. He relied on his neural networks only for a long/short recommendation.
References-found: 14

