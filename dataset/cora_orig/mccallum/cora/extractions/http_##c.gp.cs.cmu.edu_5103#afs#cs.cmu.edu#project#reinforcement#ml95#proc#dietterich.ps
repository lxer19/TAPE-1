URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/project/reinforcement/ml95/proc/dietterich.ps
Refering-URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/project/reinforcement/ml95/proceedings.html
Root-URL: http://www.cs.cmu.edu
Title: Value Function Approximations and Job-Shop Scheduling  
Author: Wei Zhang and Thomas G. Dietterich 
Date: May 30, 1995  
Address: Corvallis, Oregon 97331-3202  
Affiliation: Department of Computer Science Oregon State University  
Abstract: We report a successful application of TD() with value function approximation to the task of job-shop scheduling. Our scheduling problems are based on the problem of scheduling payload processing steps for the NASA space shuttle program. The value function is approximated by a 2-layer feedforward network of sigmoid units. A one-step lookahead greedy algorithm using the learned evaluation function outperforms the best existing algorithm for this task, which is an iterative repair method incorporating simulated annealing. To understand the reasons for this performance improvement, this paper introduces several measurements of the learning process and discusses several hypotheses suggested by these measurements. We conclude that the use of value function approximation is not a source of difficulty for our method, and in fact, it may explain the success of the method independent of the use of value iteration. Additional experiments are required to discriminate among our hypotheses. 
Abstract-found: 1
Intro-found: 1
Reference: [ Boyan and Moore, 1995 ] <author> J. A. Boyan and A. W. Moore. </author> <title> Generalization in reinforcement learning: safely approximating the value function. </title> <booktitle> In Advances in Neural Information Processing Systems 6, </booktitle> <address> San Mateo, CA, 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: We see that TD maintains a constant factor advantage over IR. 10 4.4 Discussion The results clearly show that reinforcement learning is succeeding on this problem. Evidentally, this is another example of "lucky convergence" of value iteration under function approximation <ref> [ Boyan and Moore, 1995 ] </ref> . However, the results do not show completely successful learning of the value function. For example, while the smoothness error (Figure 5) over the cross-validation set decreases during learning, it is still much larger than the "correct" value of 0.001.
Reference: [ Lin, 1992 ] <author> Longji Lin. </author> <title> Self-improving reactive agents based on reinforcement learning, planning and teaching. </title> <journal> machine learning, </journal> <volume> 8 </volume> <pages> 293-321, </pages> <year> 1992. </year>
Reference-contexts: Then we update the network starting with the final action and working backward to the start of the action sequence. Experimentally, this works better than simple online training, because the values being backed up are more up-to-date. Third, we employ Lin's experience replay method <ref> [ Lin, 1992 ] </ref> . During learning, the best sequence of moves from start to goal is remembered, and after every four training sequences, we update the network using this best training sequence. This improved learning and performance significantly.
Reference: [ Pomerleau, 1991 ] <author> D. A. Pomerleau. </author> <title> Efficient training of artificial neural networks for autonomous navigation. </title> <journal> Neural Computation, </journal> <volume> 3(1) </volume> <pages> 88-97, </pages> <year> 1991. </year>
Reference-contexts: This also improved learning and performance significantly. 5 Next, to represent the value function, we trained feed-forward networks having 40 sigmoidal hidden units and 8 sigmoidal output units. The 8 output units encode the predicted RDF using the technique of overlapping gaussian ranges <ref> [ Pomerleau, 1991 ] </ref> as follows. Each output unit represents one assigned RDF value, v j (j = 1; : : : ; 8). For the artificial problems, these RDF values are v 1 = 0:8; v 2 = 1:0; : : : ; v 8 = 2:2.
Reference: [ Sutton, 1988 ] <author> R. S. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3(1) </volume> <pages> 9-44, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Fortunately, T D () will still converge under these conditions <ref> [ Sutton, 1988 ] </ref> . 3 Methods There are several modifications that we made to this algorithm based on preliminary experiments.
Reference: [ Zhang and Dietterich, 1995 ] <author> W. Zhang and T. Dietterich. </author> <title> A reinforcement learning approach to job-shop scheduling. </title> <booktitle> In Proceedings of the 14th International Joint Conference on Artificial Intelligence, </booktitle> <year> 1995. </year> <note> (to appear). </note>
Reference-contexts: 1 Introduction In <ref> [ Zhang and Dietterich, 1995 ] </ref> , we present a reinforcement learning approach to job-shop scheduling. Our research focuses on the application domain of space shuttle payload processing for NASA.
Reference: [ Zweben et al., 1994 ] <author> M. Zweben, B. Daun, and M. Deale. </author> <title> Scheduling and rescheduling with iterative repair. </title> <editor> In M. Zweben and M. S. Fox, editors, </editor> <title> Intelligent Scheduling, </title> <booktitle> chapter 8, </booktitle> <pages> pages 241-255. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA, </address> <year> 1994. </year> <month> 12 </month>
Reference-contexts: The goal is to schedule a set of tasks to satisfy a set of temporal and resource constraints while also seeking to minimize the total duration (makespan) of the schedule. In previous work on this task, Zweben and colleagues <ref> [ Zweben et al., 1994 ] </ref> developed an iterative repair-based scheduling procedure that combines a set of heuristics with a simulated annealing search procedure. A repair-based scheduler starts with a critical-path schedule and incrementally repairs constraint violations with the goal of finding a short conflict-free schedule.
References-found: 6

