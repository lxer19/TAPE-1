URL: http://www.research.microsoft.com/users/jplatt/smoTR.ps.gz
Refering-URL: http://www.research.microsoft.com/users/jplatt/smo.html
Root-URL: http://www.research.microsoft.com
Title: Sequential Minimal Optimization: A Fast Algorithm for Training Support Vector Machines  
Abstract: John C. Platt Microsoft Research jplatt@microsoft.com Technical Report MSR-TR-98-14 April 21, 1998 1998 John Platt ABSTRACT This paper proposes a new algorithm for training support vector machines: Sequential Minimal Optimization, or SMO. Training a support vector machine requires the solution of a very large quadratic programming (QP) optimization problem. SMO breaks this large QP problem into a series of smallest possible QP problems. These small QP problems are solved analytically, which avoids using a time-consuming numerical QP optimization as an inner loop. The amount of memory required for SMO is linear in the training set size, which allows SMO to handle very large training sets. Because matrix computation is avoided, SMO scales somewhere between linear and quadratic in the training set size for various test problems, while the standard chunking SVM algorithm scales somewhere between linear and cubic in the training set size. SMO's computation time is dominated by SVM evaluation, hence SMO is fastest for linear SVMs and sparse data sets. On real-world sparse data sets, SMO can be more than 1000 times faster than the chunking algorithm. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Bengio, Y., LeCun, Y., Henderson, D., </author> <title> "Globally Trained Handwritten Word Recognizer using Spatial Representation, Convolutional Neural Networks and Hidden Markov Models," </title> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> 5, </volume> <editor> J. Cowan, G. Tesauro, J. Alspector, eds., </editor> <month> 937-944, </month> <year> (1994). </year>
Reference-contexts: Fortunately, many real-world problems have sparse input. In addition to the real-word data sets described in section 3.1 and section 3.2, any quantized or fuzzy-membership-encoded problems will be sparse. Also, optical character recognition [12], handwritten character recognition <ref> [1] </ref>, and wavelet transform coefficients of natural images [13] [14] tend to be naturally expressed as sparse data. 17 The second artificial data set stands in stark contrast to the first easy data set.
Reference: 2. <author> Boser, B. E., Guyon, I. M., Vapnik, V., </author> <title> "A Training Algorithm for Optimal Margin Classifiers", </title> <booktitle> Fifth Annual Workshop on Computational Learning Theory, ACM, </booktitle> <year> (1992). </year>
Reference-contexts: SVMs can be even further generalized to nonlinear classifiers <ref> [2] </ref>.
Reference: 3. <author> Bregman, L. M., </author> <title> "The Relaxation Method of Finding the Common Point of Convex Sets and Its Application to the Solution of Problems in Convex Programming," </title> <journal> USSR Computational Mathematics and Mathematical Physics, </journal> <volume> 7 </volume> <pages> 200-217, </pages> <year> (1967). </year>
Reference-contexts: The SMO algorithm is closely related to a family of optimization algorithms called Bregman methods <ref> [3] </ref> or row-action methods [5]. These methods solve convex programming problems with linear constraints. They are iterative methods where each step projects the current primal point onto each constraint.
Reference: 4. <author> Burges, C. J. C., </author> <title> "A Tutorial on Support Vector Machines for Pattern Recognition," </title> <note> submitted to Data Mining and Knowledge Discovery, http://svm.research.bell-labs.com/SVMdoc.html, (1998). </note>
Reference-contexts: 1. INTRODUCTION In the last few years, there has been a surge of interest in Support Vector Machines (SVMs) [19] [20] <ref> [4] </ref>. SVMs have empirically been shown to give good generalization performance on a wide variety of problems such as handwritten character recognition [12], face detection [15], pedestrian detection [14], and text categorization [9]. However, the use of SVMs is still limited to a small group of researchers. <p> The separating hyperplane is the plane u=0. The nearest points lie on the planes u = -1. The margin m is thus m = 2 || || Maximizing margin can be expressed via the following optimization problem <ref> [4] </ref>: min || || ( ) , , r r r 1 2 1subject to - " (3) Positive Examples Negative Examples Maximize distances to nearest points Space of possible inputs 3 where x i is the ith training example, and y i is the correct output of the SVM for <p> Examples of K include Gaussians, polynomials, and neural network nonlinearities <ref> [4] </ref>. If K is linear, then the equation for the linear SVM (1) is recovered. The Lagrange multipliers a i are still computed via a quadratic program. <p> In order to make the QP problem above be positive definite, the kernel function K must obey Mercer's conditions <ref> [4] </ref>. The Karush-Kuhn-Tucker (KKT) conditions are necessary and sufficient conditions for an optimal point of a positive definite QP problem. The KKT conditions for the QP problem (11) are particularly simple. <p> At every step, chunking solves a QP problem that consists of the following examples: every nonzero Lagrange multiplier from the last step, and the M worst examples that violate the KKT conditions (12) <ref> [4] </ref>, for some value of M (see figure 2). If there are fewer than M examples that violate the KKT conditions at a step, all of the violating examples are added in. Each QP sub-problem is initialized with the results of the previous sub-problem. <p> The chunking algorithm uses the projected conjugate gradient algorithm [11] as its QP solver, as suggested by Burges <ref> [4] </ref>. In order to ensure that the chunking algorithm is a fair benchmark, Burges compared the speed of his chunking code on a 200 MHz Pentium II running Solaris with the speed of the benchmark chunking code (with the sparse dot product code turned off). <p> The threshold of 10 was chosen to be an insignificant error in classification tasks. The projected conjugate gradient code has a stopping threshold, which describes the minimum relative improvement in the objective function at every step <ref> [4] </ref>. If the projected conjugate gradient takes a step where the relative improvement is smaller than this minimum, the conjugate gradient code terminates and another chunking step is taken. Burges [4] recommends using a constant 10 -10 for this minimum. <p> conjugate gradient code has a stopping threshold, which describes the minimum relative improvement in the objective function at every step <ref> [4] </ref>. If the projected conjugate gradient takes a step where the relative improvement is smaller than this minimum, the conjugate gradient code terminates and another chunking step is taken. Burges [4] recommends using a constant 10 -10 for this minimum. In the experiments below, stopping the projected conjugate gradient at an accuracy of 10 -10 often left KKT violations larger than 10 -3 , especially for the very large scale problems. <p> The threshold starts at 3x10 -10 . After every chunking step, the output is computed for all examples whose Lagrange multipliers are not at bound. These outputs are computed in order to compute the value for the threshold (see <ref> [4] </ref>). Every example suggests a proposed threshold. If the largest proposed threshold is more than 2x10 -3 above the smallest proposed threshold, then the KKT conditions cannot possibly be fulfilled within 10 -3 .
Reference: 5. <author> Censor, Y., </author> <title> "Row-Action Methods for Huge and Sparse Systems and Their Applications", </title> <journal> SIAM Review, </journal> <volume> 23(4) </volume> <pages> 444-467, </pages> <year> (1981). </year>
Reference-contexts: The SMO algorithm is closely related to a family of optimization algorithms called Bregman methods [3] or row-action methods <ref> [5] </ref>. These methods solve convex programming problems with linear constraints. They are iterative methods where each step projects the current primal point onto each constraint.
Reference: 6. <author> Censor, Y., Lent, A., </author> <title> "An Iterative Row-Action Method for Interval Convex Programming," </title> <journal> J. Optimization Theory and Applications, </journal> <volume> 34(3) </volume> <pages> 321-353, </pages> <year> (1981). </year>
Reference: 7. <author> Cortes, C., Vapnik, V., </author> <title> "Support Vector Networks," </title> <journal> Machine Learning, </journal> <volume> 20 </volume> <pages> 273-297, </pages> <year> (1995). </year>
Reference-contexts: Of course, not all data sets are linearly separable. There may be no hyperplane that splits the positive examples from the negative examples. In the formulation above, the non-separable case would correspond to an infinite solution. However, in 1995, Cortes & Vapnik <ref> [7] </ref> suggested a modification to the original optimization statement (3) which allows, but penalizes, the failure of an example to reach the correct margin.
Reference: 8. <author> Duda, R. O., Hart, P. E., </author> <title> Pattern Classification and Scene Analysis, </title> <publisher> John Wiley & Sons, </publisher> <year> (1973). </year>
Reference-contexts: The choice of which Lagrange multiplier to optimize is the same as the first choice heuristic described in section 2.2. Fixed-threshold SMO for a linear SVM is similar in concept to the perceptron relaxation rule <ref> [8] </ref>, where the output of a perceptron is adjusted whenever there is an error, so that the output exactly lies on the margin. However, the fixed-threshold SMO algorithm will sometimes reduce the proportion of a training input in the weight vector in order to maximize margin.
Reference: 9. <author> Joachims, T., </author> <title> "Text Categorization with Support Vector Machines", </title> <type> LS VIII Technical Report, No. 23, </type> <institution> University of Dortmund, ftp://ftp-ai.informatik.uni-dortmund.de/pub/Reports/report23.ps.Z, </institution> <year> (1997). </year>
Reference-contexts: SVMs have empirically been shown to give good generalization performance on a wide variety of problems such as handwritten character recognition [12], face detection [15], pedestrian detection [14], and text categorization <ref> [9] </ref>. However, the use of SVMs is still limited to a small group of researchers. One possible reason is that training algorithms for SVMs are slow, especially for large problems. Another explanation is that SVM training algorithms are complex, subtle, and difficult for an average engineer to implement.
Reference: 10. <author> Hildreth, C., </author> <title> "A Quadratic Programming Procedure," </title> <journal> Naval Research Logistics Quarterly, </journal> <volume> 4 </volume> <pages> 79-85, </pages> <year> (1957). </year>
Reference-contexts: The update rule is a a 1 1 1 1 = + K x x ( , ) This update equation forces the output of the SVM to be y 1 (similar to Bregman methods or Hildreth's QP method <ref> [10] </ref>). After the new a is computed, it is clipped to the [0,C] interval (unlike previous methods). The choice of which Lagrange multiplier to optimize is the same as the first choice heuristic described in section 2.2.
Reference: 11. <author> Gill, P. E., Murray, W., Wright, M. H., </author> <title> Practical Optimization, </title> <publisher> Academic Press, </publisher> <year> (1981). </year>
Reference-contexts: If the input is a sparse binary vector, then the position of the "1"s in the input can be stored, and the dot product will sum the weights corresponding to the position of the "1"s in the input. The chunking algorithm uses the projected conjugate gradient algorithm <ref> [11] </ref> as its QP solver, as suggested by Burges [4].
Reference: 12. <author> LeCun, Y., Jackel, L. D., Bottou, L., Cortes, C., Denker, J. S., Drucker, H., Guyon, I., Muller, U. A., Sackinger, E., Simard, P. and Vapnik, V., </author> <title> "Learning Algorithms for Classification: A Comparison on Handwritten Digit Recognition," Neural Networks: The Statistical Mechanics Perspective, Oh, </title> <editor> J. H., Kwon, C. and Cho, S. (Ed.), </editor> <publisher> World Scientific, </publisher> <pages> 261-276, </pages> <year> (1995). </year>
Reference-contexts: 1. INTRODUCTION In the last few years, there has been a surge of interest in Support Vector Machines (SVMs) [19] [20] [4]. SVMs have empirically been shown to give good generalization performance on a wide variety of problems such as handwritten character recognition <ref> [12] </ref>, face detection [15], pedestrian detection [14], and text categorization [9]. However, the use of SVMs is still limited to a small group of researchers. One possible reason is that training algorithms for SVMs are slow, especially for large problems. <p> Fortunately, many real-world problems have sparse input. In addition to the real-word data sets described in section 3.1 and section 3.2, any quantized or fuzzy-membership-encoded problems will be sparse. Also, optical character recognition <ref> [12] </ref>, handwritten character recognition [1], and wavelet transform coefficients of natural images [13] [14] tend to be naturally expressed as sparse data. 17 The second artificial data set stands in stark contrast to the first easy data set.
Reference: 13. <author> Mallat, S., </author> <title> A Wavelet Tour of Signal Processing, </title> <publisher> Academic Press, </publisher> <year> (1998). </year>
Reference-contexts: Fortunately, many real-world problems have sparse input. In addition to the real-word data sets described in section 3.1 and section 3.2, any quantized or fuzzy-membership-encoded problems will be sparse. Also, optical character recognition [12], handwritten character recognition [1], and wavelet transform coefficients of natural images <ref> [13] </ref> [14] tend to be naturally expressed as sparse data. 17 The second artificial data set stands in stark contrast to the first easy data set. The second set is generated with random 300-dimensional binary input points (10% 1) and random output labels. The SVMs are thus fitting pure noise.
Reference: 14. <author> Oren, M., Papageorgious, C., Sinha, P., Osuna, E., Poggio, T., </author> <title> "Pedestrian Detection Using Wavelet Templates," </title> <booktitle> Proc. Computer Vision and Pattern Recognition '97, </booktitle> <pages> 193-199, </pages> <year> (1997). </year>
Reference-contexts: 1. INTRODUCTION In the last few years, there has been a surge of interest in Support Vector Machines (SVMs) [19] [20] [4]. SVMs have empirically been shown to give good generalization performance on a wide variety of problems such as handwritten character recognition [12], face detection [15], pedestrian detection <ref> [14] </ref>, and text categorization [9]. However, the use of SVMs is still limited to a small group of researchers. One possible reason is that training algorithms for SVMs are slow, especially for large problems. <p> Fortunately, many real-world problems have sparse input. In addition to the real-word data sets described in section 3.1 and section 3.2, any quantized or fuzzy-membership-encoded problems will be sparse. Also, optical character recognition [12], handwritten character recognition [1], and wavelet transform coefficients of natural images [13] <ref> [14] </ref> tend to be naturally expressed as sparse data. 17 The second artificial data set stands in stark contrast to the first easy data set. The second set is generated with random 300-dimensional binary input points (10% 1) and random output labels. The SVMs are thus fitting pure noise.
Reference: 15. <author> Osuna, E., Freund, R., Girosi, F., </author> <title> "Training Support Vector Machines: An Application to Face Detection," </title> <booktitle> Proc. Computer Vision and Pattern Recognition '97, </booktitle> <pages> 130-136, </pages> <year> (1997). </year> <month> 20 </month>
Reference-contexts: 1. INTRODUCTION In the last few years, there has been a surge of interest in Support Vector Machines (SVMs) [19] [20] [4]. SVMs have empirically been shown to give good generalization performance on a wide variety of problems such as handwritten character recognition [12], face detection <ref> [15] </ref>, pedestrian detection [14], and text categorization [9]. However, the use of SVMs is still limited to a small group of researchers. One possible reason is that training algorithms for SVMs are slow, especially for large problems.
Reference: 16. <author> Osuna, E., Freund, R., Girosi, F., </author> <title> "Improved Training Algorithm for Support Vector Machines," </title> <booktitle> Proc. IEEE NNSP '97, </booktitle> <year> (1997). </year>
Reference-contexts: Chunking seriously reduces the size of the matrix from the number of training examples squared to approximately the number of nonzero Lagrange multipliers squared. However, chunking still cannot handle large-scale training problems, since even this reduced matrix cannot fit into memory. In 1997, Osuna, et al. <ref> [16] </ref> proved a theorem which suggests a whole new set of QP algorithms for SVMs. The theorem proves that the large QP problem can be broken down into a series of smaller QP sub-problems. <p> Notice that the chunking algorithm obeys the conditions of the theorem, and hence will converge. Osuna, et al. suggests keeping a constant size matrix for every QP sub-problem, which implies adding and deleting the same number of examples at every step <ref> [16] </ref> (see figure 2). Using a constantsize matrix will allow the training on arbitrarily sized data sets. The algorithm given in Osunas paper [16] suggests adding one example and subtracting one example every step. <p> Osuna, et al. suggests keeping a constant size matrix for every QP sub-problem, which implies adding and deleting the same number of examples at every step <ref> [16] </ref> (see figure 2). Using a constantsize matrix will allow the training on arbitrarily sized data sets. The algorithm given in Osunas paper [16] suggests adding one example and subtracting one example every step. Clearly this would be inefficient, because it would use an entire numerical QP optimization step to cause one training example to obey the KKT conditions. In practice, researchers add and subtract multiple examples according to unpublished heuristics [17]. <p> below. 2.2 Heuristics for Choosing Which Multipliers To Optimize As long as SMO always optimizes and alters two Lagrange multipliers at every step and at least one of the Lagrange multipliers violated the KKT conditions before the step, then each step will decrease the objective function according to Osuna's theorem <ref> [16] </ref>. Convergence is thus guaranteed. In order to speed convergence, SMO uses heuristics to choose which two Lagrange multipliers to jointly optimize. There are two separate choice heuristics: one for the first Lagrange multiplier and one for the second.
Reference: 17. <author> Osuna, E., </author> <type> Personal Communication. </type>
Reference-contexts: Clearly this would be inefficient, because it would use an entire numerical QP optimization step to cause one training example to obey the KKT conditions. In practice, researchers add and subtract multiple examples according to unpublished heuristics <ref> [17] </ref>. In any event, a numerical QP solver is required for all of these methods. Numerical QP is notoriously tricky to get right; there are many numerical precision issues that need to be addressed. Chunking Osuna SMO each method, three steps are illustrated.
Reference: 18. <author> Platt, J. C., </author> <title> "A Resource-Allocating Network for Function Interpolation," </title> <journal> Neural Computation, </journal> <volume> 3(2) </volume> <pages> 213-225, </pages> <year> (1991). </year>
Reference-contexts: The relaxation rule constantly increases the amount of a training input in the weight vector and, hence, is not maximum margin. Fixed-threshold SMO for Gaussian kernels is also related to the resource allocating network (RAN) algorithm <ref> [18] </ref>. When RAN detects certain kinds of errors, it will allocate a kernel to exactly fix the error. SMO will perform similarly.
Reference: 19. <author> Vapnik, V., </author> <title> Estimation of Dependences Based on Empirical Data, </title> <publisher> Springer-Verlag, </publisher> <year> (1982). </year>
Reference-contexts: 1. INTRODUCTION In the last few years, there has been a surge of interest in Support Vector Machines (SVMs) <ref> [19] </ref> [20] [4]. SVMs have empirically been shown to give good generalization performance on a wide variety of problems such as handwritten character recognition [12], face detection [15], pedestrian detection [14], and text categorization [9]. However, the use of SVMs is still limited to a small group of researchers. <p> Finally, there is an appendix that describes the derivation of the analytic optimization. 1.1 Overview of Support Vector Machines Vladimir Vapnik invented Support Vector Machines in 1979 <ref> [19] </ref>. In its simplest, linear form, an SVM is a hyperplane that separates a set of positive examples from a set of negative examples with maximum margin (see figure 1). <p> The quadratic form in (11) involves a matrix that has a number of elements equal to the square of the number of training examples. This matrix cannot be fit into 128 Megabytes if there are more than 4000 training examples. Vapnik <ref> [19] </ref> describes a method to solve the SVM QP, which has since been known as chunking. The chunking algorithm uses the fact that the value of the quadratic form is the same if you remove the rows and columns of the matrix that corresponds to zero Lagrange multipliers.

References-found: 19

