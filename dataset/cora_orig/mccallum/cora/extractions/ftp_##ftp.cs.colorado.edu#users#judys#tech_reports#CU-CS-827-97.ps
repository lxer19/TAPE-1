URL: ftp://ftp.cs.colorado.edu/users/judys/tech_reports/CU-CS-827-97.ps
Refering-URL: ftp://ftp.cs.colorado.edu/users/judys/papers/INDEX.html
Root-URL: http://www.cs.colorado.edu
Title: Choosing Program Collections for Performance Evaluation Experiments  
Author: Judith A. Stafford and Benjamin G. Zorn 
Date: January 1997  
Address: Campus Box 430  Boulder, CO 80309-0430 USA  Boulder  
Affiliation: Department of Computer Science  University of Colorado  ffi University of Colorado at  
Pubnum: CU-CS-827-97  
Abstract: Technical Report CU-CS-827-97 Department of Computer Science Campus Box 430 University of Colorado Boulder, Colorado 80309 
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> ACM SIGPLAN. </editor> <booktitle> Proceedings of the ACM SIGPLAN '95 Conference on Programming Language Design and Implementation, </booktitle> <address> La Jolla, California, </address> <month> June 18-21, </month> <year> 1995. </year> <journal> SIGPLAN Notices, </journal> <volume> 30(6), </volume> <month> June </month> <year> 1995. </year>
Reference: [2] <author> J. Arnold, D. Buell, and E. Davis. </author> <title> SPLASH II. </title> <booktitle> In Proceedings of the 4th Annual Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 316-322, </pages> <address> San Diego, CA, USA, June 1992. </address> <publisher> ACM Press. </publisher>
Reference-contexts: This is common practice, especially in the case where the application domain does not sufficiently intersect that of the SPEC benchmark programs. Examples of this approach include some of our own previous research, in which we collected a set of allocation intensive programs [6], the Olden [13] and SPLASH <ref> [2] </ref> benchmark suites (intended for investigating software and hardware to support parallel programming), and the Safe-C suite [3]. Recent trends have shown an increase in the number and size of programs measured.
Reference: [3] <author> Todd M. Austin, Scott E. Breach, and Gurindar S. Sohi. </author> <title> Efficient detection of all pointer and array access errors. </title> <booktitle> In Proceedings of the ACM SIGPLAN '94 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 290-301, </pages> <address> Orlando, Florida, </address> <month> June 20-24, </month> <year> 1994. </year> <journal> SIGPLAN Notices, </journal> <volume> 29(6), </volume> <month> June </month> <year> 1994. </year>
Reference-contexts: Examples of this approach include some of our own previous research, in which we collected a set of allocation intensive programs [6], the Olden [13] and SPLASH [2] benchmark suites (intended for investigating software and hardware to support parallel programming), and the Safe-C suite <ref> [3] </ref>. Recent trends have shown an increase in the number and size of programs measured. Recent architecture research papers investigating branch prediction implementations provide performance evaluation based on two different benchmark suites [8, 17]. Experimental work by Bart Miller involves testing over 80 Unix utility programs [11]. <p> bmtoa, editres, fsinfo, fslsfonts, fstobdf, iceauth, lndir, mkfontdir, oclock, resize, sessreg, smproxy, twm, x11perf, xauth, xclipboard, xclock, xcmsdb, xconsole, xcutsel, xdm, xdpyinfo, xfd, xfs, xhost, xieperf, xinit, xkill, xlogo, xlsatoms, xlsclients, xlsfonts, xmag, xmh, xmodmap, xprop, xrdb, xrefresh, xset, xsetroot, xsm, xsmclient, xstdcmap, xterm, xwd, xwininfo, xwud, Xdec Safe C <ref> [3] </ref> anagram, backprop*, bc*, ft, ks, yacr2 GNU bash, bcy, bison, flex, gawk, gccy, gnuchess, gnugo, gnuplot, gzip, indent, od, sed, sort, tcsh, wdiff AF Vers. 3, Release 1 Aaxp, Aj300, Ajv, Alofi, Amsb, aecho, aevents, ahost, alsatoms, apass, aphone, aplay, aprop, arecord, aset, awgn MISC burg, cfrac, chameleon, gzip, hello,
Reference: [4] <author> Henry G. Baker, Jr. </author> <title> List processing in real time on a serial computer. </title> <journal> Communications of the ACM, </journal> <volume> 21(4) </volume> <pages> 280-294, </pages> <month> April </month> <year> 1978. </year>
Reference-contexts: Another optimization significantly affected by the indegree distribution is inlining, where high indegrees can lead to code explosion if selective inlining is not performed. * The implementation of such diverse techniques as software fault isolation [20], data breakpointing [21], incremental garbage collection <ref> [4] </ref>, and software distributed shared memory [15, 16] all require instrumenting a program's loads and stores.
Reference: [5] <author> Brad Calder, Dirk Grunwald, Donald Lindsay, James Martin, Michael Mozer, and Benjamin Zorn. </author> <title> Corpus-based static branch prediction. </title> <booktitle> In Proceedings of the ACM SIGPLAN '95 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 79-92, </pages> <address> La Jolla, CA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: Recent architecture research papers investigating branch prediction implementations provide performance evaluation based on two different benchmark suites [8, 17]. Experimental work by Bart Miller involves testing over 80 Unix utility programs [11]. Recent experimental work of our own involved measuring performance in 43 programs, including several benchmark suites <ref> [5] </ref>. Given that the use of program test suites for performance evaluation is widespread, and the methods of choosing such suites is ad hoc, our goal is to better understand the characteristics of large program suites.
Reference: [6] <author> David Detlefs, Al Dosser, and Benjamin Zorn. </author> <title> Memory allocation costs in large C and C++ programs. </title> <journal> Software| Practice and Experience, </journal> <volume> 24(6) </volume> <pages> 527-542, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: This is common practice, especially in the case where the application domain does not sufficiently intersect that of the SPEC benchmark programs. Examples of this approach include some of our own previous research, in which we collected a set of allocation intensive programs <ref> [6] </ref>, the Olden [13] and SPLASH [2] benchmark suites (intended for investigating software and hardware to support parallel programming), and the Safe-C suite [3]. Recent trends have shown an increase in the number and size of programs measured.
Reference: [7] <author> Ran Giladi and Niv Ahituv. </author> <title> SPEC as a performance evaluation measure. </title> <journal> IEEE Computer, </journal> <volume> 28(8) </volume> <pages> 33-42, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: Because it is so widely used, the various instantiations of the SPEC benchmark suite have been carefully scrutinized. Recently, several researchers have specifically questioned the actual value of the benchmarks themselves <ref> [7] </ref>, as well as the representativeness of SPEC performance results with respect to standard compiler optimization settings [12].
Reference: [8] <author> Nicolas Gloy, Cliff Young, J. Bradley Chen, and Michael D. Smith. </author> <title> An analysis of dynamic branch prediction schemes on system workloads. </title> <booktitle> In Proceedings of the 23rd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 12-21, </pages> <address> Philadelphia, PA, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: Recent trends have shown an increase in the number and size of programs measured. Recent architecture research papers investigating branch prediction implementations provide performance evaluation based on two different benchmark suites <ref> [8, 17] </ref>. Experimental work by Bart Miller involves testing over 80 Unix utility programs [11]. Recent experimental work of our own involved measuring performance in 43 programs, including several benchmark suites [5].
Reference: [9] <author> Stephan H. Kan. </author> <title> Metrics and Models in Software Quality Engineering. </title> <publisher> Addison-Wesley, </publisher> <year> 1995. </year>
Reference-contexts: For a survey of benchmarking efforts related to the Dhrystone, see [22]. Measures of program source code have been used widely in the field of software engineering for some time (e.g., see <ref> [9] </ref>). These measures are used to estimate project costs, evaluate productivity, predict staffing requirements, and to evaluate software quality. In these cases the metrics are used as tools for prediction or evaluation to somehow improve the production of software.
Reference: [10] <author> Jack L. Lo and Susan J. Eggers. </author> <title> Improving balanced scheduling with compiler optimizations that increase instruction-level parallelism. </title> <booktitle> In Proceedings of the ACM SIGPLAN '95 Conference on Programming Language Design and Implementation [1], </booktitle> <pages> pages 151-162. </pages> <booktitle> SIGPLAN Notices, </booktitle> <volume> 30(6), </volume> <month> June </month> <year> 1995. </year>
Reference-contexts: The measured overhead of all of these techniques depends heavily on the dynamic percentage of load and store operations executed by the programs in the test suite. * Implementation techniques such as instruction scheduling <ref> [10] </ref> and optimizations to increase instruction level parallelism rely heavily on the size and content of program basic blocks, which may vary significantly depending on the test programs used.
Reference: [11] <author> Barton P. Miller, David Koski, Cjin Pheow Lee, Vivekananda Meganty, Ravi Murthy, Ajitkumar Natarajan, and Jeff Steidl. </author> <title> Fuzz revisited: A re-examination of the reliability of UNIX utilities and services. </title> <type> Technical report, </type> <institution> Computer Sciences Dept., Univ. of Wisconsin|Madison, </institution> <year> 1996. </year>
Reference-contexts: Recent trends have shown an increase in the number and size of programs measured. Recent architecture research papers investigating branch prediction implementations provide performance evaluation based on two different benchmark suites [8, 17]. Experimental work by Bart Miller involves testing over 80 Unix utility programs <ref> [11] </ref>. Recent experimental work of our own involved measuring performance in 43 programs, including several benchmark suites [5].
Reference: [12] <author> Nikki Mirghafori, Margret Jacoby, and David Patterson. </author> <title> Truth in SPEC benchmarks. </title> <journal> Computer Architecture News, </journal> <volume> 23(5) </volume> <pages> 34-42, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: Because it is so widely used, the various instantiations of the SPEC benchmark suite have been carefully scrutinized. Recently, several researchers have specifically questioned the actual value of the benchmarks themselves [7], as well as the representativeness of SPEC performance results with respect to standard compiler optimization settings <ref> [12] </ref>. The SPEC benchmarks are selected to cover a diverse collection of application areas with the intention of allowing users of the performance results to look specifically at the result of the program in the application area in which they are interested.
Reference: [13] <author> Anne Rogers, Martin C. Carlisle, John H. Reppy, and L. J. Hendren. </author> <title> Supporting dynamic data structures on distributed-memory machines. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 17(2) </volume> <pages> 233-263, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: This is common practice, especially in the case where the application domain does not sufficiently intersect that of the SPEC benchmark programs. Examples of this approach include some of our own previous research, in which we collected a set of allocation intensive programs [6], the Olden <ref> [13] </ref> and SPLASH [2] benchmark suites (intended for investigating software and hardware to support parallel programming), and the Safe-C suite [3]. Recent trends have shown an increase in the number and size of programs measured.
Reference: [14] <author> Erik Ruf. </author> <title> Context-insensitive alias analysis reconsidered. </title> <booktitle> In Proceedings of the ACM SIGPLAN '95 Conference on Programming Language Design and Implementation [1], </booktitle> <pages> pages 13-22. </pages> <booktitle> SIGPLAN Notices, </booktitle> <volume> 30(6), </volume> <month> June </month> <year> 1995. </year>
Reference-contexts: As we show later, the value of this metric varies widely with the suite of test programs chosen. Recently, Ruf pointed out that context-insensitive analysis may perform very well, especially given the characteristics of the benchmark programs used in his analysis <ref> [14] </ref>. <p> Previous work has suggested that the program collection chosen can significantly affect the outcome of the evaluation <ref> [14] </ref>. We were interested in knowing how intrinsic program characteristics such as median procedure in and outdegree vary over the programs in a collection. We were further interested in knowing how the average of these characteristics varies with the size and content of the program collection chosen.
Reference: [15] <author> Daniel J. Scales, Kourosh Garachorloo, and Chandramohan A. Thekkath. </author> <title> Shasta: A low overhead, software-only approach for supporting fine-grain shared memory. </title> <booktitle> In Proceedings of the Seventh International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 174-185, </pages> <address> Cambridge, MA, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: Another optimization significantly affected by the indegree distribution is inlining, where high indegrees can lead to code explosion if selective inlining is not performed. * The implementation of such diverse techniques as software fault isolation [20], data breakpointing [21], incremental garbage collection [4], and software distributed shared memory <ref> [15, 16] </ref> all require instrumenting a program's loads and stores.
Reference: [16] <author> Ioannis Schoinas, Babak Falsafi, Alvin R. Lebeck, Steven K. Reinhardt, James R. Larus, and David A. Wood. </author> <title> Fine-grain access control for distributed shared memory. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 297-306, </pages> <address> San Jose, California, </address> <month> October 4-7, </month> <year> 1994. </year> <journal> ACM SIGARCH, SIGOPS, SIGPLAN, and the IEEE Computer Society. Computer Architecture News, 22, October 1994; Operating Systems Review, 28(5), December 1994; SIGPLAN Notices, </journal> <volume> 29(11), </volume> <month> November </month> <year> 1994. </year> <month> 17 </month>
Reference-contexts: Another optimization significantly affected by the indegree distribution is inlining, where high indegrees can lead to code explosion if selective inlining is not performed. * The implementation of such diverse techniques as software fault isolation [20], data breakpointing [21], incremental garbage collection [4], and software distributed shared memory <ref> [15, 16] </ref> all require instrumenting a program's loads and stores.
Reference: [17] <author> Stuart Sechrest, Chih-Chieh Lee, and Trevor Mudge. </author> <title> Correlation and aliasing in dynamic branch predictors. </title> <booktitle> In Proceedings of the 23rd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 22-32, </pages> <address> Philadelphia, PA, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: Recent trends have shown an increase in the number and size of programs measured. Recent architecture research papers investigating branch prediction implementations provide performance evaluation based on two different benchmark suites <ref> [8, 17] </ref>. Experimental work by Bart Miller involves testing over 80 Unix utility programs [11]. Recent experimental work of our own involved measuring performance in 43 programs, including several benchmark suites [5].
Reference: [18] <author> Amitabh Srivastava and Alan Eustace. </author> <title> ATOM: A system for building customized program analysis tools. </title> <booktitle> In Proceedings of the ACM SIGPLAN '94 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 196-205, </pages> <address> Orlando, FL, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: The SPEC92 versions of backprop, compress, gcc and li were instrumented and used to calculate the means for this subgroup but not included in the All group. 6 3.2 Data Collection We instrumented the programs using ATOM, a tool developed by Digital Equipment Corporation <ref> [18] </ref>. ATOM provides a flexible means by which to create program analysis tools based on instrumentation of executable code. ATOM allows a program executable to be analyzed and instrumented by providing an API that allows procedures, basic blocks, and instructions in the executable to be navigated.
Reference: [19] <institution> Standard Performance Evaluation Corportation, Manassas, VA. </institution> <note> SPEC95 Technical Manual, </note> <month> August </month> <year> 1995. </year>
Reference-contexts: Current experimental approaches include the following: * Use industry standard benchmark suites. In the field of programming language implementation research, this translates specifically to using the SPEC benchmark suites <ref> [19] </ref>. These suites include integer (i.e., C) programs, as well as Fortran programs. The number of programs of each type varies, but for the C programs has been 8 in the SPEC92 and SPEC95 suites. These suites have been used extensively, and measured carefully as well. <p> Section 4 presents our results and Section 5 concludes and suggests directions for future work. 2 Background Much of work related to this research has focused on the appropriateness of the various SPEC benchmark suites (e.g., see <ref> [19] </ref> for the latest release). The SPEC benchmark suites have proven very effective in standardizing the set of measurements used to describe CPU performance for integer and floating point benchmarks on new systems.
Reference: [20] <author> Robert Wahbe, Steven Lucco, Thomas Anderson, and Susan Graham. </author> <title> Efficient software-based fault isolation. </title> <booktitle> In Proceedings of the Fourteenth ACM Symposium on Operating System Principles, </booktitle> <pages> pages 203-216, </pages> <address> Asheville, NC, </address> <month> December </month> <year> 1993. </year>
Reference-contexts: Another optimization significantly affected by the indegree distribution is inlining, where high indegrees can lead to code explosion if selective inlining is not performed. * The implementation of such diverse techniques as software fault isolation <ref> [20] </ref>, data breakpointing [21], incremental garbage collection [4], and software distributed shared memory [15, 16] all require instrumenting a program's loads and stores.
Reference: [21] <author> Robert Wahbe, Steven Lucco, and Susan L. Graham. </author> <title> Practical data breakpoints: </title> <booktitle> Design and implementation. In Proceedings of the ACM SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 1-12, </pages> <address> Albuquerque, New Mexico, </address> <month> June 23-25, </month> <year> 1993. </year> <journal> SIGPLAN Notices, </journal> <volume> 28(6), </volume> <month> June </month> <year> 1993. </year>
Reference-contexts: Another optimization significantly affected by the indegree distribution is inlining, where high indegrees can lead to code explosion if selective inlining is not performed. * The implementation of such diverse techniques as software fault isolation [20], data breakpointing <ref> [21] </ref>, incremental garbage collection [4], and software distributed shared memory [15, 16] all require instrumenting a program's loads and stores.
Reference: [22] <author> Reihnold P. Weicker. </author> <title> An overview of common benchmarks. </title> <journal> IEEE Computer, </journal> <volume> 23(12) </volume> <pages> 65-75, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: Furthermore, the size of the collection we consider is significantly larger, both in terms of numbers of programs and in terms of program size, than the collections considered in the Dhrystone effort. For a survey of benchmarking efforts related to the Dhrystone, see <ref> [22] </ref>. Measures of program source code have been used widely in the field of software engineering for some time (e.g., see [9]). These measures are used to estimate project costs, evaluate productivity, predict staffing requirements, and to evaluate software quality.
Reference: [23] <author> Reinhold P. Weicker. Dhrystone: </author> <title> A synthetic systems programming benchmark. </title> <journal> Communications of the ACM, </journal> <volume> 27(10) </volume> <pages> 1013-1030, </pages> <month> October </month> <year> 1984. </year>
Reference-contexts: Other benchmark efforts have attempted to characterize a body of programs with statistical measures, and to construct a synthetic benchmark based on such characterization. In particular the Dhrystone benchmark was constructed after the author conducted a survey of the published literature on source language feature usage in systems programs <ref> [23] </ref>. While our effort does focus on surveying source-language level features in a large group of programs, our intention is not to construct a synthetic benchmark.
Reference: [24] <author> Robert P. Wilson and Monica S. Lam. </author> <title> Efficient context-sensitive pointer analysis for C programs. </title> <booktitle> In Proceedings of the ACM SIGPLAN '95 Conference on Programming Language Design and Implementation [1], </booktitle> <pages> pages 1-12. </pages> <booktitle> SIGPLAN Notices, </booktitle> <volume> 30(6), </volume> <month> June </month> <year> 1995. </year> <month> 18 </month>
Reference-contexts: support this point, we describe three current and important areas of programming language implementation research in which the results of experiments rely heavily on the intrinsic characteristics of the test programs measured. * Implementations of interprocedural pointer alias analysis can be classified as either context-sensitive or context-insensitive algorithms (e.g., see <ref> [24] </ref>). The context insensitive algorithms account for the fact that procedures can be called from multiple points while the context insensitive algorithms do not. The effectiveness of context insensitive algorithms relative to context sensitive depends heavily on the indegree of procedures in the program's static call-graph.
References-found: 24

