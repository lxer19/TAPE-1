URL: ftp://ftp.cs.rochester.edu/pub/papers/ai/94.traum-allen.ACL.ps.Z
Refering-URL: http://www.cs.rochester.edu/u/traum/papers.html
Root-URL: 
Email: traum@cs.rochester.edu and james@cs.rochester.edu  
Title: Discourse Obligations in Dialogue Processing  
Author: David R. Traum and James F. Allen 
Address: Rochester, NY 14627-0226  
Affiliation: Department of Computer Science University of Rochester  
Note: Appears in Proceedings ACL '94  
Abstract: We show that in modeling social interaction, particularly dialogue, the attitude of obligation can be a useful adjunct to the popularly considered attitudes of belief, goal, and intention and their mutual and shared counterparts. In particular, we show how discourse obligations can be used to account in a natural manner for the connection between a question and its answer in dialogue and how obligations can be used along with other parts of the discourse context to extend the coverage of a dialogue system. 
Abstract-found: 1
Intro-found: 1
Reference: [Airenti et al., 1993] <author> Gabriella Airenti, Bruno G. Bara, and Marco Colombetti, </author> <title> Conversation and Behavior Games in the Pragmatics of Dialogue, </title> <journal> Cognitive Science, </journal> <volume> 17 </volume> <pages> 197-256, </pages> <year> 1993. </year>
Reference-contexts: Games provide a better explanation of coherence, but still require the agents to recognize each other's intentions to perform the dialogue game. As a result, this work can be viewed as a special case of the intentional view. An interesting model is described by <ref> [Airenti et al., 1993] </ref>, which separates out the conversational games from the task-related games in a way similar way to [Litman and Allen, 1987].
Reference: [Allen and Perrault, 1980] <author> James Allen and C. Perrault, </author> <title> Analyzing Intention in Utterances, </title> <journal> Artificial Intelligence, </journal> <volume> 15(3) </volume> <pages> 143-178, </pages> <year> 1980. </year>
Reference: [Allen and Schubert, 1991] <author> James F. Allen and Lenhart K. Schubert, </author> <title> The TRAINS Project, </title> <type> TRAINS Technical Note 91-1, </type> <institution> Computer Science Dept. University of Rochester, </institution> <year> 1991. </year>
Reference-contexts: The TRAINS System <ref> [Allen and Schubert, 1991] </ref> is a large integrated natural language conversation and plan reasoning 2 This is a slightly simplified version of a spoken dialogue between two people. The original is dialogue 91-6.1 from [Gross et al., 1993].
Reference: [Bratman et al., 1988] <author> Michael E. Bratman, David J. Israel, and Martha E. Pollack, </author> <title> Plans and Resource-Bounded 7 Practical Reasoning, </title> <type> Technical Report TR425R, </type> <institution> SRI International, </institution> <month> September </month> <year> 1988, </year> <booktitle> Appears in Computational Intelligence, </booktitle> <volume> Vol. 4, No. 4, </volume> <year> 1988. </year>
Reference-contexts: (x) S 2 Inform-ref x utterance not understood repair utterance or incorrect Table 1: Sample Obligation Rules 3.1 Obligations and Behavior Obligations (or at least beliefs that the agent has obligations) will thus form an important part of the reasoning process of a deliberative agent, e.g., the architecture proposed by <ref> [Bratman et al., 1988] </ref>.
Reference: [Carberry, 1990] <author> S. Carberry, </author> <title> Plan Recognition in Natural Language Dialogue, </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: Obligations also cannot be reduced to simple expectations, although obligations may act as a source of expectations. Expectations can be used to guide the action interpretation and plan-recognition processes (as proposed by <ref> [Carberry, 1990] </ref>), but expectations do not in and of themselves provide a sufficient motivation for an agent to perform the expected action in many cases there is nothing wrong with doing the unexpected or not performing an expected action.
Reference: [Clark and Schaefer, 1989] <author> Herbert H. Clark and Edward F. Schaefer, </author> <title> Contributing to Discourse, </title> <journal> Cognitive Science, </journal> <volume> 13:259 - 94, </volume> <year> 1989. </year>
Reference-contexts: Conversational action is represented using the theory of Conversation Acts [Traum and Hinkelman, 1992] which augments traditional Core Speech Acts with levels of acts for turn-taking, grounding <ref> [Clark and Schaefer, 1989] </ref>, and argumentation.
Reference: [Cohen and Levesque, 1991] <author> Phillip R. Cohen and Hector J. Levesque, </author> <booktitle> Confirmations and Joint Action, In Proceedings IJCAI-91, </booktitle> <pages> pages 951-957, </pages> <year> 1991. </year>
Reference-contexts: Others have tried to account for this kind of behavior using social intentional constructs such as Joint intentions <ref> [Cohen and Levesque, 1991] </ref> or Shared Plans [Grosz and Sidner, 1990].
Reference: [Cohen and Perrault, 1979] <author> Phillip R. Cohen and C. R. Per rault, </author> <title> Elements of a Plan-Based Theory of Speech Acts, </title> <journal> Cognitive Science, </journal> <volume> 3(3) </volume> <pages> 177-212, </pages> <year> 1979. </year>
Reference: [Conte and Castelfranchi, 1993] <author> Rosaria Conte and Cris tiano Castelfranchi, </author> <title> Norms as mental objects. From normative beliefs to normative goals, </title> <booktitle> In Working Notes AAAI Spring Symposium on Reasoning about Mental States: Formal Theories and Applications., </booktitle> <pages> pages 40-47, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: There are a large number of strategies which may be used to incorporate obligations into the deliberative process, based on how much weight they are given compared to the agents goals. <ref> [Conte and Castelfranchi, 1993] </ref> present several strategies of moving from obligations to actions, including: automatically performing an obligated action, adopting all obligations as goals, or adopting an obligated action as a goal only when performing the action results in a state desired by the agent.
Reference: [Ferguson, 1994] <author> George Ferguson, </author> <title> Domain Plan Reason ing in TRAINS-93, </title> <type> Trains technical note, </type> <institution> Computer Science Dept. University of Rochester, forthcoming, </institution> <year> 1994. </year>
Reference-contexts: If the proposal is not accepted or rejected, the system can request an acceptance. If a proposal is rejected, the system can negotiate and offer a counterproposal or accept a counter proposal from the user. Since the domain plan reasoner <ref> [Ferguson, 1994] </ref> performs both plan recognition and plan elaboration in an incremental fashion, proposals from system and user can be integrated naturally in a mixed-initiative fashion.
Reference: [Gross et al., 1993] <author> Derek Gross, James Allen, and David Traum, </author> <title> The TRAINS 91 Dialogues, </title> <type> TRAINS Technical Note 92-1, </type> <institution> Computer Science Dept. University of Rochester, </institution> <month> July </month> <year> 1993. </year>
Reference-contexts: The TRAINS System [Allen and Schubert, 1991] is a large integrated natural language conversation and plan reasoning 2 This is a slightly simplified version of a spoken dialogue between two people. The original is dialogue 91-6.1 from <ref> [Gross et al., 1993] </ref>. The utterance numbering system used here reflects the relation to the turn and utterance numbering used there. `3-7' represents utterance 7 within turn 3. `=' is used to indicate merged utterances.
Reference: [Grosz and Sidner, 1986] <author> Barbara Grosz and Candice Sid ner, </author> <title> Attention, Intention, and the Structure of Discourse, </title> <journal> CL, </journal> <volume> 12(3) </volume> <pages> 175-204, </pages> <year> 1986. </year>
Reference: [Grosz and Sidner, 1990] <author> Barbara J. Grosz and Candace L. Sidner, </author> <title> Plans for Discourse, </title> <editor> In P. R. Cohen, J. Morgan, and M. E. Pollack, editors, </editor> <title> Intentions in Communication. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Others have tried to account for this kind of behavior using social intentional constructs such as Joint intentions [Cohen and Levesque, 1991] or Shared Plans <ref> [Grosz and Sidner, 1990] </ref>. While these accounts do help explain some discourse phenomena more satisfactorily, they still require a strong degree of co-operativity to account for dialogue coherence, and do not provide easy explanations of why an agent might act in cases that do not support high-level mutual goals.
Reference: [Kowtko et al., 1991] <author> J. Kowtko, S. Isard, and G. Doherty, </author> <title> Conversational games within dialogue., </title> <booktitle> In Proceedings of the ESPRIT Workshop on Discourse Coherence, </booktitle> <year> 1991. </year>
Reference-contexts: Questions do more than just provide evidence of a speaker's goals, and something more than adoption of the goals of an interlocutor is involved in formulating a response to a question. Some researchers, e.g., <ref> [Mann, 1988; Kowtko et al., 1991] </ref>, assume a library of discourse level actions, sometimes called dialogue games, which encode common communicative interactions. To be co-operative, an agent must always be participating in one of these games.
Reference: [Litman and Allen, 1987] <author> D. J. Litman and J. F. Allen, </author> <title> A Plan Recognition model for subdialogues in conversation, </title> <journal> Cognitive Science, </journal> <volume> 11 </volume> <pages> 163-200, </pages> <year> 1987. </year>
Reference-contexts: Agent A must adopt agent B's goals as her own. As a result, it does not explain why A says anything when she does not know the answer or when she is not predisposed to adopting B's goals. Several approaches have been suggested to account for this behavior. <ref> [Litman and Allen, 1987] </ref> introduced an intentional analysis at the discourse level in addition to the domain level, and assumed a set of conventional multi-agent actions at the discourse level. <p> As a result, this work can be viewed as a special case of the intentional view. An interesting model is described by [Airenti et al., 1993], which separates out the conversational games from the task-related games in a way similar way to <ref> [Litman and Allen, 1987] </ref>. Because of this separation, they do not have to assume co-operation on the tasks each agent is performing, but still require recognition of intention and cooperation at the conversational level. It is left unexplained what goals motivate conversational co-operation.
Reference: [Mann, 1988] <author> William C. Mann, </author> <title> Dialogue Games: Con ventions of Human Interaction, </title> <journal> Argumentation, </journal> <volume> 2 </volume> <pages> 511-532, </pages> <year> 1988. </year>
Reference-contexts: Questions do more than just provide evidence of a speaker's goals, and something more than adoption of the goals of an interlocutor is involved in formulating a response to a question. Some researchers, e.g., <ref> [Mann, 1988; Kowtko et al., 1991] </ref>, assume a library of discourse level actions, sometimes called dialogue games, which encode common communicative interactions. To be co-operative, an agent must always be participating in one of these games.
Reference: [McRoy, 1993] <author> Susan McRoy, </author> <title> Abductive Interpretation and Reinterpretation of Natural Language Utterances, </title> <type> PhD thesis, </type> <institution> University of Toronto, 1993, Reproduced as TR CSRI-288 Department of Computer Science, University of Toronto. </institution>
Reference-contexts: If the obligation is to say something then we call this a discourse obligation. Our model of obligation is very simple. We use a set of rules that encode discourse conventions. Whenever a new conversation act is determined 1 <ref> [McRoy, 1993] </ref> uses expectations derived from Adjacency Pair structure [Schegloff and Sacks, 1973], as are many of the discourse obligations considered in this paper. These expectations correspond to social norms and do impose the same notion of accountability.
Reference: [Schegloff and Sacks, 1973] <author> E. A. Schegloff and H. Sacks, </author> <title> Opening Up Closings, </title> <journal> Semiotica, </journal> <volume> 7 </volume> <pages> 289-327, </pages> <year> 1973. </year>
Reference-contexts: If the obligation is to say something then we call this a discourse obligation. Our model of obligation is very simple. We use a set of rules that encode discourse conventions. Whenever a new conversation act is determined 1 [McRoy, 1993] uses expectations derived from Adjacency Pair structure <ref> [Schegloff and Sacks, 1973] </ref>, as are many of the discourse obligations considered in this paper. These expectations correspond to social norms and do impose the same notion of accountability.
Reference: [Shoham and Tennenholtz, 1992] <author> Yoav Shoham and Moshe Tennenholtz, </author> <title> On the synthesis of useful social laws for artificial agent societies, </title> <booktitle> In Proceedings AAAI-92, </booktitle> <pages> pages 276-281, </pages> <year> 1992. </year>
Reference-contexts: An action that might occur or not-occur according to R is neither obligatory nor forbidden. Just because an action is obligatory with respect to a set of rules R does not mean that the agent will perform the action. So we do not adopt the model suggested by <ref> [Shoham and Tennenholtz, 1992] </ref> in which agents' behavior cannot violate the defined social laws. If an obligation is not satisfied, then this means that one of the rules must have been broken.
Reference: [Traum, 1994] <author> David R. Traum, </author> <title> The TRAINS-93 Dialogue Manager, </title> <type> Trains technical note, </type> <institution> Computer Science Dept. University of Rochester, forthcoming, </institution> <year> 1994. </year>
Reference-contexts: While the discussion here is informal and skips some details, the dialogue is actually processed in this manner by the implemented system. More detail both on the dialogue manager and its operation on this example can be found in <ref> [Traum, 1994] </ref>. Utterance 1 is interpreted as performing two Core Speech Acts. It is interpreted (literally) as the initiation 4 of an inform about an obligation to perform a domain action (shipping the oranges).
Reference: [Traum and Hinkelman, 1992] <author> David R. Traum and Eliz abeth A. Hinkelman, </author> <title> Conversation Acts in Task-oriented Spoken Dialogue, </title> <journal> Computational Intelligence, </journal> <volume> 8(3) </volume> <pages> 575-599, </pages> <year> 1992, </year> <note> Special Issue on Non-literal language. 8 </note>
Reference-contexts: The dialogue manager must keep track of the current state of the dialogue, determine the effects of observed conversation acts, generate utterances back, and send commands to the domain plan reasoner and domain plan executor when appropriate. Conversational action is represented using the theory of Conversation Acts <ref> [Traum and Hinkelman, 1992] </ref> which augments traditional Core Speech Acts with levels of acts for turn-taking, grounding [Clark and Schaefer, 1989], and argumentation. <p> Utterances 3-3=6 and 3-7 are interpreted, but not responded to yet since the user keeps the turn (in this case 4 According to the theory of Conversation Acts <ref> [Traum and Hinkelman, 1992] </ref>, Core Speech Acts such as inform are multi-agent actions which have as their effect a mutual belief, and are not completed unless/until they are grounded. by following up with subsequent utterances before the system has a chance to act).
References-found: 21

