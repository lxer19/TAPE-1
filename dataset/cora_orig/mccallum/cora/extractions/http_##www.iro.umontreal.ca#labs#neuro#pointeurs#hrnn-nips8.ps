URL: http://www.iro.umontreal.ca/labs/neuro/pointeurs/hrnn-nips8.ps
Refering-URL: http://www.cs.cmu.edu/Web/Groups/NIPS/NIPS95/Papers.html
Root-URL: 
Email: elhihi@iro.umontreal.ca  bengioy@iro.umontreal.ca  
Title: Hierarchical Recurrent Neural Networks for Long-Term Dependencies  
Author: Salah El Hihi Yoshua Bengio 
Address: Montreal, Qc H3C-3J7  Montreal, Qc H3C-3J7  
Affiliation: Dept. Informatique et Recherche Operationnelle Universite de Montreal  Dept. Informatique et Recherche Operationnelle Universite de Montreal  
Abstract: We have already shown that extracting long-term dependencies from sequential data is difficult, both for deterministic dynamical systems such as recurrent networks, and probabilistic models such as hidden Markov models (HMMs) or input/output hidden Markov models (IOHMMs). In practice, to avoid this problem, researchers have used domain specific a-priori knowledge to give meaning to the hidden or state variables representing past context. In this paper, we propose to use a more general type of a-priori knowledge, namely that the temporal dependencies are structured hierarchically. This implies that long-term dependencies are represented by variables with a long time scale. This principle is applied to a recurrent network which includes delays and multiple time scales. Experiments confirm the advantages of such structures. A similar approach is proposed for HMMs and IOHMMs.
Abstract-found: 1
Intro-found: 1
Reference: [BDGO92] <author> F. Brugnara, R. DeMori, D. Giuliani, and M. Omologo. </author> <title> A family of parallel hidden markov models. </title> <booktitle> In Proc. International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 377-370, </pages> <address> New York, NY, USA, 1992. </address> <publisher> IEEE. </publisher>
Reference-contexts: However, with these algorithms, one generally assumes that the state of the system is observed, whereas, in this paper we concentrate on the difficulty of learning what the state variable should represent. On the HMM side, several researchers <ref> [BDGO92, Sua94] </ref> have attempted to improve HMMs for speech recognition to better model the different types of variables, intrinsically varying at different time scales, observed in speech recognition. Again, however, the focus is on setting an a-priori representation, not on learning how to represent context. <p> In <ref> [BDGO92] </ref> two Markov chains are coupled in a "master/slave" configuration. For the "master" HMM, the observations are slowly varying features (such as the signal energy), whereas for the "slave" HMM the observations are the speech spectra themselves.
Reference: [BF94] <author> Y. Bengio and P. Frasconi. </author> <title> Credit assignment through time: Alternatives to back propagation. </title> <editor> In J.D. Cowan, G. Tesauro, and J. Alspector, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: In both cases, we compare a hierarchical recurrent network with a single-scale fully-connected recurrent network. In the first set of experiments, we want to evaluate the performance of a hierarchical recurrent network on a problem already used for studying the difficulty in learning long-term dependencies <ref> [BSF94, BF94] </ref>. In this 2-class problem, the network has to detect a pattern at the beginning of the sequence, keeping a bit of information in "memory" (while the inputs are noisy) until the end of the sequence (supervision is only a the end of the sequence). As in [BSF94, BF94] only <p> long-term dependencies <ref> [BSF94, BF94] </ref>. In this 2-class problem, the network has to detect a pattern at the beginning of the sequence, keeping a bit of information in "memory" (while the inputs are noisy) until the end of the sequence (supervision is only a the end of the sequence). As in [BSF94, BF94] only the first 3 time steps contain information about the class (a 3-number pattern was randomly chosen for each class within [1; 1] 3 ). The length of the sequences is varied to evaluate the effect of the span of input/output dependencies.
Reference: [BF95a] <author> Y. Bengio and P. Frasconi. </author> <title> Diffusion of credit in markovian models. </title> <editor> In G. Tesauro D.S. Touretzky and T.K. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: This general a-priori assumption considerably simplifies the learning problem. fl also, AT&T Bell Labs, Holmdel, NJ 07733 In previous papers <ref> [BSF94, BF95a] </ref>, we have shown for the above algorithms that, unfortunately, even with this assumption, dependencies that span longer intervals are significantly harder to learn. <p> In the case of Markovian models <ref> [BF95a] </ref> (such as HMMs or IOHMMs [BF95b]), the relation between the state at time t and the state at time t &gt; t , is linear and depends on the product of the matrices of transition probabilities from time t to t. <p> Experiments performed with this model are described in section 4. Finally, we discuss a similar scheme for HMMs and IOHMMs in section 5. 2 Too Many Products In this section, we take another look at the analyses of [BSF94] and <ref> [BF95a] </ref>, for recurrent networks and HMMs respectively. The objective is to draw a parallel between the problems encountered with the two approaches, in order to guide us towards some form of solution, and justify the proposals made in this paper. <p> Our earlier analysis <ref> [BF95a] </ref> shows again that the difficulty in representing and learning to represent context (i.e., learning what x t should represent) revolves around equation 4. The matrices in the above equations have one eigenvalue equal to 1 (because of the normalization constraint) and the others 1. <p> Note on the other hand that in most applications of HMMs to speech recognition the meaning of states is fixed a-priori rather than learned from the data (see <ref> [BF95a] </ref> for a discussion). In a more recent contribution, Nelly Suaudeau [Sua94] proposes a "two-level HMM" in which the higher level HMM represents "segmental" variables (such as phoneme duration).
Reference: [BF95b] <author> Y. Bengio and P. Frasconi. </author> <title> An em approach to learning sequential behavior. </title> <editor> In G. Tesauro D.S. Touretzky and T.K. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: In the case of Markovian models [BF95a] (such as HMMs or IOHMMs <ref> [BF95b] </ref>), the relation between the state at time t and the state at time t &gt; t , is linear and depends on the product of the matrices of transition probabilities from time t to t. <p> However, the above products then rapidly converge to 0 when t t increases. Consequently, the sum in 2 is dominated by terms corresponding to short-term dependencies (t t is small). Let us now consider the case of Markovian models (including HMMs and IOHMMs <ref> [BF95b] </ref>). These are probabilistic models, either of an "output" sequence P (y 1 : : : y T ) (HMMs) or of an output sequence given an input sequence P (y 1 : : : y T ju 1 : : : u T ) (IOHMMs).
Reference: [BLH94] <author> Y. Bengio, Y. LeCun, and D. Henderson. </author> <title> Globally trained handwritten word rec ognizer using spatial representation, space displacement neural networks and hidden Markov models. </title> <booktitle> In Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 937-944, </pages> <year> 1994. </year>
Reference-contexts: This yields a multi-resolution representation of state information. This general idea is not new and can be found in various approaches to learning and artificial intelligence. For example, in convolutional neural networks, both for sequential data with TDNNs [LH88, WHH + 89], and for 2-dimensional data with MLCNNs <ref> [LBD + 89, BLH94] </ref>, the network is organized in layers representing features of increasing temporal or spatial coarseness. Similarly, mostly as a tool for analyzing and preprocessing sequential or spatial data, wavelet transforms [Dau90] also represent such information at multiple resolutions.
Reference: [BPSW70] <author> L. E. Baum, T. Petrie, G. Soules, and N. Weiss. </author> <title> A maximization technique occuring in the statistical analysis of probabilistic functions of markov chains. </title> <journal> Ann. Math. Statistic., </journal> <volume> 41 </volume> <pages> 164-171, </pages> <year> 1970. </year>
Reference-contexts: Many learning algorithms take advantage of this sequential structure by assuming some kind of homogeneity or continuity of the model over time, e.g., by sharing parameters for different times, as in Time-Delay Neural Networks (TDNNs) [LH88, WHH + 89], recurrent neural networks [RHW86, WZ89], or hidden Markov models <ref> [BPSW70, LRS83, RJ86] </ref>. This general a-priori assumption considerably simplifies the learning problem. fl also, AT&T Bell Labs, Holmdel, NJ 07733 In previous papers [BSF94, BF95a], we have shown for the above algorithms that, unfortunately, even with this assumption, dependencies that span longer intervals are significantly harder to learn.
Reference: [BSF94] <author> Y. Bengio, P. Simard, and P. Frasconi. </author> <title> Learning long-term dependencies with gradient descent is difficult. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 5(2) </volume> <pages> 157-166, </pages> <year> 1994. </year>
Reference-contexts: This general a-priori assumption considerably simplifies the learning problem. fl also, AT&T Bell Labs, Holmdel, NJ 07733 In previous papers <ref> [BSF94, BF95a] </ref>, we have shown for the above algorithms that, unfortunately, even with this assumption, dependencies that span longer intervals are significantly harder to learn. <p> The hidden variable is not any more completely hidden. Learning becomes much easier. Unfortunately, this requires a very precise knowledge of the appropriate state variables, which is not available in many applications. In the case of recurrent networks <ref> [BSF94] </ref>, we found that when the dynamics are stable enough to store long-term context, learning is very difficult because gradients vanish as they are propagated backwards in time: changing infinitesimally the hidden state at time t has practically no effect on the hidden state at a later time t t . <p> Experiments performed with this model are described in section 4. Finally, we discuss a similar scheme for HMMs and IOHMMs in section 5. 2 Too Many Products In this section, we take another look at the analyses of <ref> [BSF94] </ref> and [BF95a], for recurrent networks and HMMs respectively. The objective is to draw a parallel between the problems encountered with the two approaches, in order to guide us towards some form of solution, and justify the proposals made in this paper. First, let us consider the deterministic dynamical systems [BSF94] <p> <ref> [BSF94] </ref> and [BF95a], for recurrent networks and HMMs respectively. The objective is to draw a parallel between the problems encountered with the two approaches, in order to guide us towards some form of solution, and justify the proposals made in this paper. First, let us consider the deterministic dynamical systems [BSF94] (such as recurrent networks), which map an input sequence u 1 ; : : :; u T to an output sequence ^y 1 ; : : : ; ^y T . <p> all time steps before t: @C t = t @x t @x t @W The Jacobian matrix of derivatives @x t @x t can further be factored as follows: @x t = @x t1 @x t2 @x t+1 = f 0 t1 : : : f 0 Our earlier analysis <ref> [BSF94] </ref> shows that the difficulty revolves around the matrix product in equation 3. <p> In both cases, we compare a hierarchical recurrent network with a single-scale fully-connected recurrent network. In the first set of experiments, we want to evaluate the performance of a hierarchical recurrent network on a problem already used for studying the difficulty in learning long-term dependencies <ref> [BSF94, BF94] </ref>. In this 2-class problem, the network has to detect a pattern at the beginning of the sequence, keeping a bit of information in "memory" (while the inputs are noisy) until the end of the sequence (supervision is only a the end of the sequence). As in [BSF94, BF94] only <p> long-term dependencies <ref> [BSF94, BF94] </ref>. In this 2-class problem, the network has to detect a pattern at the beginning of the sequence, keeping a bit of information in "memory" (while the inputs are noisy) until the end of the sequence (supervision is only a the end of the sequence). As in [BSF94, BF94] only the first 3 time steps contain information about the class (a 3-number pattern was randomly chosen for each class within [1; 1] 3 ). The length of the sequences is varied to evaluate the effect of the span of input/output dependencies.
Reference: [Dau90] <author> Ingrid Daubechies. </author> <title> The wavelet transform, time-frequency localization and signal analysis. </title> <journal> IEEE Transaction on Information Theory, </journal> <volume> 36(5) </volume> <pages> 961-1005, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: Similarly, mostly as a tool for analyzing and preprocessing sequential or spatial data, wavelet transforms <ref> [Dau90] </ref> also represent such information at multiple resolutions. Multi-scale representations have also been proposed to improve reinforcement learning systems [Sin92, DH93, Sut95] and path planning systems.
Reference: [DH93] <author> P. Dayan and G.E. Hinton. </author> <title> Feudal reinforcement learning. </title> <editor> In S. J. Hanson, J. D. Cowan, and C. L. Giles, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <address> San Mateo, CA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Similarly, mostly as a tool for analyzing and preprocessing sequential or spatial data, wavelet transforms [Dau90] also represent such information at multiple resolutions. Multi-scale representations have also been proposed to improve reinforcement learning systems <ref> [Sin92, DH93, Sut95] </ref> and path planning systems. However, with these algorithms, one generally assumes that the state of the system is observed, whereas, in this paper we concentrate on the difficulty of learning what the state variable should represent.
Reference: [FGMS93] <author> P. Frasconi, M. Gori, M. Maggini, and G. </author> <title> Soda. Unified integration of explicit rules and learning by example in recurrent networks. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <note> 1993. (in press). </note>
Reference-contexts: The "hard learning" problem is to learn to represent context (or state information), which involves performing the proper credit assignment through time. Indeed, in practice, recurrent networks (e.g., injecting prior knowledge for grammar inference <ref> [GO92, FGMS93] </ref>) and HMMs (e.g., for speech recognition [LRS83, RJ86]) work quite well when the representation of context (the meaning of the state variable) is decided a-priori. The hidden variable is not any more completely hidden. Learning becomes much easier.
Reference: [GO92] <author> C. L. Giles and C. W Omlin. </author> <title> Inserting rules into recurrent neural networks. </title> <editor> In Kung, Fallside, Sorenson, and Kamm, editors, </editor> <booktitle> Neural Networks for Signal Processing II, Proceedings of the 1992 IEEE workshop, </booktitle> <pages> pages 13-22. </pages> <publisher> IEEE Press, </publisher> <year> 1992. </year>
Reference-contexts: The "hard learning" problem is to learn to represent context (or state information), which involves performing the proper credit assignment through time. Indeed, in practice, recurrent networks (e.g., injecting prior knowledge for grammar inference <ref> [GO92, FGMS93] </ref>) and HMMs (e.g., for speech recognition [LRS83, RJ86]) work quite well when the representation of context (the meaning of the state variable) is decided a-priori. The hidden variable is not any more completely hidden. Learning becomes much easier.
Reference: [LBD + 89] <author> Y. LeCun, B. Boser, J.S. Denker, D. Henderson, R.E. Howard, W. Hubbard, and L.D. Jackel. </author> <title> Backpropagation applied to handwritten zip code recognition. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 541-551, </pages> <year> 1989. </year>
Reference-contexts: This yields a multi-resolution representation of state information. This general idea is not new and can be found in various approaches to learning and artificial intelligence. For example, in convolutional neural networks, both for sequential data with TDNNs [LH88, WHH + 89], and for 2-dimensional data with MLCNNs <ref> [LBD + 89, BLH94] </ref>, the network is organized in layers representing features of increasing temporal or spatial coarseness. Similarly, mostly as a tool for analyzing and preprocessing sequential or spatial data, wavelet transforms [Dau90] also represent such information at multiple resolutions.
Reference: [LH88] <author> K. J. Lang and G. E. Hinton. </author> <title> The development of the time-delay neural network architecture for speech recognition. </title> <type> Technical Report CMU-CS-88-152, </type> <institution> Carnegie-Mellon University, </institution> <year> 1988. </year>
Reference-contexts: Many learning algorithms take advantage of this sequential structure by assuming some kind of homogeneity or continuity of the model over time, e.g., by sharing parameters for different times, as in Time-Delay Neural Networks (TDNNs) <ref> [LH88, WHH + 89] </ref>, recurrent neural networks [RHW86, WZ89], or hidden Markov models [BPSW70, LRS83, RJ86]. <p> This yields a multi-resolution representation of state information. This general idea is not new and can be found in various approaches to learning and artificial intelligence. For example, in convolutional neural networks, both for sequential data with TDNNs <ref> [LH88, WHH + 89] </ref>, and for 2-dimensional data with MLCNNs [LBD + 89, BLH94], the network is organized in layers representing features of increasing temporal or spatial coarseness. <p> Another example is in the application of learning algorithms to financial and economic forecasting and decision taking. Some of the variables of interest are given daily, others weekly, monthly, etc... 4 Hierarchical Recurrent Neural Network: Experiments As in TDNNs <ref> [LH88, WHH + 89] </ref> and reverse-TDNNs [Sm92], we will use discrete time delays and subsampling (or oversampling) in order to implement the multiple time scales.
Reference: [LRS83] <author> S. E. Levinson, L. R. Rabiner, and M. M. Sondhi. </author> <title> An introduction to the application of the theory of probabilistic functions of a markov process to automatic speech recognition. </title> <journal> Bell System Technical Journal, </journal> <volume> 64(4) </volume> <pages> 1035-1074, </pages> <year> 1983. </year>
Reference-contexts: Many learning algorithms take advantage of this sequential structure by assuming some kind of homogeneity or continuity of the model over time, e.g., by sharing parameters for different times, as in Time-Delay Neural Networks (TDNNs) [LH88, WHH + 89], recurrent neural networks [RHW86, WZ89], or hidden Markov models <ref> [BPSW70, LRS83, RJ86] </ref>. This general a-priori assumption considerably simplifies the learning problem. fl also, AT&T Bell Labs, Holmdel, NJ 07733 In previous papers [BSF94, BF95a], we have shown for the above algorithms that, unfortunately, even with this assumption, dependencies that span longer intervals are significantly harder to learn. <p> The "hard learning" problem is to learn to represent context (or state information), which involves performing the proper credit assignment through time. Indeed, in practice, recurrent networks (e.g., injecting prior knowledge for grammar inference [GO92, FGMS93]) and HMMs (e.g., for speech recognition <ref> [LRS83, RJ86] </ref>) work quite well when the representation of context (the meaning of the state variable) is decided a-priori. The hidden variable is not any more completely hidden. Learning becomes much easier.
Reference: [RHW86] <author> D.E. Rumelhart, G.E. Hinton, and R.J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D.E. Rumelhart and J.L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> volume 1, chapter 8, </volume> <pages> pages 318-362. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1986. </year>
Reference-contexts: Many learning algorithms take advantage of this sequential structure by assuming some kind of homogeneity or continuity of the model over time, e.g., by sharing parameters for different times, as in Time-Delay Neural Networks (TDNNs) [LH88, WHH + 89], recurrent neural networks <ref> [RHW86, WZ89] </ref>, or hidden Markov models [BPSW70, LRS83, RJ86].
Reference: [RJ86] <author> L.R. Rabiner and B.H. Juang. </author> <title> An introduction to hidden Markov models. </title> <journal> IEEE ASSP Magazine, </journal> <pages> pages 257-285, </pages> <month> January </month> <year> 1986. </year>
Reference-contexts: Many learning algorithms take advantage of this sequential structure by assuming some kind of homogeneity or continuity of the model over time, e.g., by sharing parameters for different times, as in Time-Delay Neural Networks (TDNNs) [LH88, WHH + 89], recurrent neural networks [RHW86, WZ89], or hidden Markov models <ref> [BPSW70, LRS83, RJ86] </ref>. This general a-priori assumption considerably simplifies the learning problem. fl also, AT&T Bell Labs, Holmdel, NJ 07733 In previous papers [BSF94, BF95a], we have shown for the above algorithms that, unfortunately, even with this assumption, dependencies that span longer intervals are significantly harder to learn. <p> The "hard learning" problem is to learn to represent context (or state information), which involves performing the proper credit assignment through time. Indeed, in practice, recurrent networks (e.g., injecting prior knowledge for grammar inference [GO92, FGMS93]) and HMMs (e.g., for speech recognition <ref> [LRS83, RJ86] </ref>) work quite well when the representation of context (the meaning of the state variable) is decided a-priori. The hidden variable is not any more completely hidden. Learning becomes much easier.
Reference: [Sin92] <author> S.P. Singh. </author> <title> Reinforcement learning with a hierarchy of abstract models. </title> <booktitle> In Proceedings of the 10th National Conference on Artificial Intelligence, </booktitle> <pages> pages 202-207. </pages> <publisher> MIT/AAAI Press, </publisher> <year> 1992. </year>
Reference-contexts: Similarly, mostly as a tool for analyzing and preprocessing sequential or spatial data, wavelet transforms [Dau90] also represent such information at multiple resolutions. Multi-scale representations have also been proposed to improve reinforcement learning systems <ref> [Sin92, DH93, Sut95] </ref> and path planning systems. However, with these algorithms, one generally assumes that the state of the system is observed, whereas, in this paper we concentrate on the difficulty of learning what the state variable should represent.
Reference: [SJ95] <author> L.K. Saul and M.I. Jordan. </author> <title> Boltzmann chains and hidden markov models. </title> <editor> In G. Tesauro D.S. Touretzky and T.K. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: Another promising approach was recently proposed in <ref> [SJ95] </ref>. Using decimation techniques from statistical mechanics, a polynomial-time algorithm is derived for parallel Boltzmann chains (which are similar to parallel HMMs), which can operate at different time scales.
Reference: [Sm92] <author> P. Simard and Y. Le Cun. </author> <title> Reverse TDNN: An architecture for trajectory generation. </title> <editor> In J.E. Moody, S.J. Hanson, and R.P. Lipmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 579-588, </pages> <address> Denver 1991, 1992. </address> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo. </address>
Reference-contexts: Another example is in the application of learning algorithms to financial and economic forecasting and decision taking. Some of the variables of interest are given daily, others weekly, monthly, etc... 4 Hierarchical Recurrent Neural Network: Experiments As in TDNNs [LH88, WHH + 89] and reverse-TDNNs <ref> [Sm92] </ref>, we will use discrete time delays and subsampling (or oversampling) in order to implement the multiple time scales.
Reference: [Sua94] <author> N. Suaudeau. </author> <title> Un modele probabiliste pour integrer la dimension temporelle dans un systeme de reconnaissance automatique de la parole. </title> <type> PhD thesis, </type> <institution> Universite de Rennes I, France, </institution> <year> 1994. </year>
Reference-contexts: However, with these algorithms, one generally assumes that the state of the system is observed, whereas, in this paper we concentrate on the difficulty of learning what the state variable should represent. On the HMM side, several researchers <ref> [BDGO92, Sua94] </ref> have attempted to improve HMMs for speech recognition to better model the different types of variables, intrinsically varying at different time scales, observed in speech recognition. Again, however, the focus is on setting an a-priori representation, not on learning how to represent context. <p> Note on the other hand that in most applications of HMMs to speech recognition the meaning of states is fixed a-priori rather than learned from the data (see [BF95a] for a discussion). In a more recent contribution, Nelly Suaudeau <ref> [Sua94] </ref> proposes a "two-level HMM" in which the higher level HMM represents "segmental" variables (such as phoneme duration). The two levels operate at different scales: the higher level state variable represents the phonetic identity and models the distributions of the average energy and the duration within each phoneme.
Reference: [Sut95] <author> R.S. Sutton. </author> <title> TD models: modeling the world at a mixture of time scales. </title> <booktitle> In Proceed ings of the 12th International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: Similarly, mostly as a tool for analyzing and preprocessing sequential or spatial data, wavelet transforms [Dau90] also represent such information at multiple resolutions. Multi-scale representations have also been proposed to improve reinforcement learning systems <ref> [Sin92, DH93, Sut95] </ref> and path planning systems. However, with these algorithms, one generally assumes that the state of the system is observed, whereas, in this paper we concentrate on the difficulty of learning what the state variable should represent.
Reference: [WHH + 89] <author> A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K. Lang. </author> <title> Phoneme recognition using time-delay neural networks. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> 37 </volume> <pages> 328-339, </pages> <year> 1989. </year>
Reference-contexts: Many learning algorithms take advantage of this sequential structure by assuming some kind of homogeneity or continuity of the model over time, e.g., by sharing parameters for different times, as in Time-Delay Neural Networks (TDNNs) <ref> [LH88, WHH + 89] </ref>, recurrent neural networks [RHW86, WZ89], or hidden Markov models [BPSW70, LRS83, RJ86]. <p> This yields a multi-resolution representation of state information. This general idea is not new and can be found in various approaches to learning and artificial intelligence. For example, in convolutional neural networks, both for sequential data with TDNNs <ref> [LH88, WHH + 89] </ref>, and for 2-dimensional data with MLCNNs [LBD + 89, BLH94], the network is organized in layers representing features of increasing temporal or spatial coarseness. <p> Another example is in the application of learning algorithms to financial and economic forecasting and decision taking. Some of the variables of interest are given daily, others weekly, monthly, etc... 4 Hierarchical Recurrent Neural Network: Experiments As in TDNNs <ref> [LH88, WHH + 89] </ref> and reverse-TDNNs [Sm92], we will use discrete time delays and subsampling (or oversampling) in order to implement the multiple time scales.
Reference: [WZ89] <author> R.J. Williams and D. Zipser. </author> <title> A learning algorithm for continually running fully recurrent neural networks. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 270-280, </pages> <year> 1989. </year>
Reference-contexts: Many learning algorithms take advantage of this sequential structure by assuming some kind of homogeneity or continuity of the model over time, e.g., by sharing parameters for different times, as in Time-Delay Neural Networks (TDNNs) [LH88, WHH + 89], recurrent neural networks <ref> [RHW86, WZ89] </ref>, or hidden Markov models [BPSW70, LRS83, RJ86].
References-found: 23

