URL: ftp://ftp.ai.univie.ac.at/papers/oefai-tr-95-16.ps.Z
Refering-URL: http://wwwipd.ira.uka.de/~prechelt/NIPS_bench.html
Root-URL: 
Title: Statistical Evaluation of Neural Network Experiments: Minimum Requirements and Current Practice  
Author: Arthur Flexer 
Keyword: lack of statistical evaluation they contain.  
Address: Schottengasse 3, A-1010 Vienna, Austria  
Affiliation: The Austrian Research Institute for Artificial Intelligence  
Pubnum: Technical Report  
Email: arthur@ai.univie.ac.at  
Date: oefai-tr-95-16  
Abstract:  
Abstract-found: 1
Intro-found: 1
Reference: [ Cohen 95 ] <author> Cohen P.R.: </author> <title> Empirical Methods for Artificial Intelligence, A Bradford Book, </title> <publisher> MIT Press, </publisher> <address> Cam-bridge, MA, </address> <year> 1995. </year>
Reference-contexts: 1 Introduction There are only few papers that discuss the foundations of the role of experimentation in neural network research, although for the general field of artificial intelligence, recently a whole textbook has been devoted to this problem <ref> [ Cohen 95 ] </ref> . <p> It is possible to try to come to significant results by computing more and more runs of an experiment, since higher values for N A and N B imply more degrees of freedom and a decrease of the variance ^ x A x B . But as <ref> [ Cohen 95 ] </ref> (p.116) points out, this decrease in vari ance gets rather small when more than 20 runs are being computed. <p> The simplest approach to deal with this multiplicity effect is to divide ff through the number of tests that are being performed, which makes it rather hard to come to significant results). Some pointers to more sophisticated solutions can be found in [ Feelders & Verkooijen 95 ] or <ref> [ Cohen 95 ] </ref> (pp.189). 4 Current Practice To sum up the previous section, the following can be seen as minimum requirements for proper neural net work experimentation: * the use of different training and test sets * the computation of multiple runs using an appro priate resampling technique * the
Reference: [ Efron 82 ] <author> Efron B.: </author> <title> The Jackknife, the Bootstrap, and Other Resampling Plans, </title> <institution> Society for Industrial and Applied Mathematics, </institution> <year> 1982. </year>
Reference-contexts: The observed performance measures for the K different runs are averaged. This procedure is usually known as K-fold cross-validation. This technique is still biased in its estimation of performance and there are other techniques like bootstrap <ref> [ Efron 82 ] </ref> that are able to reduce this bias further at even greater computational cost by using resampling with replacement. Another important issue, often neglected within neural network research, is the fact that sometimes another third independent data set is needed for fair performance estimation.
Reference: [ Egmont-Petersen et al. 94 ] <author> Egmont-Petersen M., Talmon J.L., Brender J., McNair P.: </author> <title> On the quality of neural net classifiers, </title> <journal> Artificial Intelligence in Medicine, </journal> <volume> 6, </volume> <pages> 359-381, </pages> <year> 1994. </year>
Reference-contexts: Therefore it is advised to use a t-test, which should be computed to test the significance of the difference between means (see [ Feelders & Verkooijen 95 ] or <ref> [ Egmont-Petersen et al. 94 ] </ref> for a discussion related to neural nets and classifiers in general). The formulas for the computation of the t-test are given in (1), (2) and (3).
Reference: [ Feelders & Verkooijen 95 ] <author> Feelders A., Verkooijen W.: </author> <title> Which method learns most from the data?, </title> <booktitle> Proceedings of the fifth international workshop on AI and Statistics, </booktitle> <month> January </month> <year> 1995, </year> <title> Fort Lauderdale, </title> <booktitle> Florida, </booktitle> <pages> pp. 219-225, </pages> <year> 1995. </year>
Reference-contexts: But if only each of the sample means falls outside the confidence intervall around the other mean, a statistically significant difference is not guaranteed. Therefore it is advised to use a t-test, which should be computed to test the significance of the difference between means (see <ref> [ Feelders & Verkooijen 95 ] </ref> or [ Egmont-Petersen et al. 94 ] for a discussion related to neural nets and classifiers in general). The formulas for the computation of the t-test are given in (1), (2) and (3). <p> Since in the standard comparative experiment the performance measures are all estimated from the same test sample, which makes them highly correlated, a paired sample t-test should be used which gives a more powerful test statistic <ref> [ Feelders & Verkooijen 95 ] </ref> . This makes it necessary to actually parallelize the samples that are being drawn for neural networks A and B, i.e. to use the same data for training and testing for the networks during the multiple runs. <p> The simplest approach to deal with this multiplicity effect is to divide ff through the number of tests that are being performed, which makes it rather hard to come to significant results). Some pointers to more sophisticated solutions can be found in <ref> [ Feelders & Verkooijen 95 ] </ref> or [ Cohen 95 ] (pp.189). 4 Current Practice To sum up the previous section, the following can be seen as minimum requirements for proper neural net work experimentation: * the use of different training and test sets * the computation of multiple runs using
Reference: [ Flexer 95 ] <editor> Flexer A.: Connectionists and Statisticians, Friends or Foes?, in Mira J. & Sandoval F.(eds.), </editor> <booktitle> From Natural to Artificial Neural Computation, Proc.International Workshop on Artificial Neural Networks, </booktitle> <address> Malaga-Torremolinos, Spain, June. </address> <publisher> Springer, LNCS 930, </publisher> <pages> pp. 454-461, </pages> <year> 1995. </year>
Reference-contexts: However, it has already been recognized that the quality of the neural network research practice definitely needs improvement. <ref> [ Flexer 95 ] </ref> emphasizes the fact that statistical evaluation is necessary for neural network experiments as for any other empirical science and that problems connected with empirical research and experiment design are wellknown to statisticians, but that there seems to be little awareness of such issues within the neural network
Reference: [ Kibler & Langley 88 ] <author> Kibler D., Langley P.: </author> <title> Machine Learning as an Experimental Science, </title> <journal> Machine Learning, </journal> <volume> 3(1), </volume> <pages> 5-8, </pages> <year> 1988. </year>
Reference-contexts: But the last word in the decision is always spoken by an empirical check, an experiment, as in any other science that needs empirical evaluation of its theories (see <ref> [ Kibler & Langley 88 ] </ref> for a comparison of physics and machine learning).
Reference: [ Michie et al. 94 ] <author> Michie D., Spiegelhalter D.J., Taylor C.C.(eds.): </author> <title> Machine Learning, Neural and Statistical Classification, </title> <publisher> Ellis Horwood, </publisher> <address> England, </address> <year> 1994. </year>
Reference-contexts: It is not sufficient to use the so-called resubstitution method where the performance of a trained classifier is measured on the data set used for training. It is widely known (even within the neural network or machine learning community, see e.g. [ Ripley 92 ] , <ref> [ Michie et al. 94 ] </ref> ) that the performance measure estimated with this resubstitution method is usually over-optimistic, i.e. that the same performance measure computed on new, previously unknown, data is very likely to yield worse results. <p> Since it is usually necessary to tune some parameters (e.g. learning rate, number of layers, numbers of units, etc.) to get the best network performance, a division of the available data into three different sets is recommended. <ref> [ Michie et al. 94 ] </ref> recommend to hold back approximately 20% of the data and divide the remaining data in a set for training and a set for testing, and then tune the parameter using those two sets and an appropriate resampling technique.
Reference: [ Mosteller & Tukey 77 ] <author> Mosteller F., Tukey J.W.: </author> <title> Data Analysis and Regression a second course in statistics, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1977. </year>
Reference-contexts: If the use of such a third independent data set is omitted, the obtained error rates will again be biased and overoptimistic because the test set used for repeated tuning in fact becomes a training set. <ref> [ Mosteller & Tukey 77 ] </ref> (p.37) distinguish between the "form" of a method (i.e architecture of a network including learning parameters) and the "numerical values" of its coefficients (i.e. weight values) and calls the threefold division of data described above "double cross-validation". 3.2 Statistical testing All statistical tests are only <p> If parametric tests are being used and the assumption of normality does not hold, it can only happen that instances are being judged as "not significant" that otherwise would have been judged as "significant" but not vice versa (see e.g. <ref> [ Mosteller & Tukey 77 ] </ref> , p.16). If this actually happens, one can still try appropriate nonparametric tests. Therefore, and because of the ease of their computation, we recommend the use of parametric methods like those discussed below as a first approach.
Reference: [ Prechelt 96 ] <author> Prechelt L.: </author> <title> A Quantative Study of Experimental Evaluations of Neural Network Learning Algorithms: </title> <booktitle> Current Research Practice, Neural Networks, </booktitle> <volume> Vol. 9, </volume> <year> 1996. </year>
Reference-contexts: 95 ] emphasizes the fact that statistical evaluation is necessary for neural network experiments as for any other empirical science and that problems connected with empirical research and experiment design are wellknown to statisticians, but that there seems to be little awareness of such issues within the neural network community. <ref> [ Prechelt 96 ] </ref> in his study of 119 articles about neural network learning published in 1993 and 1994 in wellknown journals observes a fl This work has been published as: Flexer A.: Statistical Evaluation of Neural Network Experiments: Minimum Requirements and Current Practice, in Trappl R., Cybernetics and Systems '96, <p> One third of them do not present any quantitative comparison with previously known algortithms at all. Whereas <ref> [ Prechelt 96 ] </ref> is concerned with the quantitative amount of evaluation in neural network studies, this paper is concerned with the quality of such evaluations. <p> an appro priate resampling technique * the use of a third independent data set in the case of parameter tuning * to report mean, variance and confidence intervals * to compute a statistical test (e.g. a t-test) for the comparison of performances Following the approach in the related study by <ref> [ Prechelt 96 ] </ref> , articles from two leading journals, Neural Networks (numbers 1-5 of 1994, Elsevier) and Neural Computation (numbers 1-6 of 1994, numbers 1 and 2 of 1995, MIT Press), have been examined as to whether they meet those requirements.
Reference: [ Ripley 92 ] <author> Ripley B.D.: </author> <title> Statistical Aspects of Neural Networks, </title> <institution> Department of Statistics, University of Oxford, </institution> <year> 1992. </year>
Reference-contexts: It is not sufficient to use the so-called resubstitution method where the performance of a trained classifier is measured on the data set used for training. It is widely known (even within the neural network or machine learning community, see e.g. <ref> [ Ripley 92 ] </ref> , [ Michie et al. 94 ] ) that the performance measure estimated with this resubstitution method is usually over-optimistic, i.e. that the same performance measure computed on new, previously unknown, data is very likely to yield worse results.
Reference: [ Siegel 56 ] <author> Siegel S.: </author> <title> Nonparametric Statistics for the Behavioral Sciences, </title> <publisher> McGraw-Hill, </publisher> <address> Tokyo, </address> <year> 1956. </year>
Reference-contexts: But how can we be sure that the portion that we are able to observe is representative of the whole number of events in question? "The procedures of statistical inference" allow us "to draw conclusions from the evidence provided by samples" <ref> [ Siegel 56 ] </ref> . <p> of a network including learning parameters) and the "numerical values" of its coefficients (i.e. weight values) and calls the threefold division of data described above "double cross-validation". 3.2 Statistical testing All statistical tests are only valid under certain conditions and can be divided into parametric and non-parametric methods (see e.g. <ref> [ Siegel 56 ] </ref> ). Parametric methods have a variety of strong assumptions (e.g. that of normal distribution of the data) and are therefore more powerful (i.e. it is easier to come to significant results) than nonparametric methods. <p> This makes it necessary to actually parallelize the samples that are being drawn for neural networks A and B, i.e. to use the same data for training and testing for the networks during the multiple runs. See <ref> [ Siegel 56 ] </ref> or any standard statistical text book for more details on hypotheses testing and related issues.
References-found: 11

