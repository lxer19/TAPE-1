URL: http://robotics.stanford.edu/users/sahami/papers-dir/kdd96-learn-bn.ps
Refering-URL: http://robotics.stanford.edu/users/sahami/papers.html
Root-URL: 
Email: sahami@cs.stanford.edu  
Title: Learning Limited Dependence Bayesian Classifiers  
Author: Mehran 
Address: Gates Building 1A, Room 126  Stanford, CA 94305-9010  
Affiliation: Sahami  Computer Science Department Stanford University  
Abstract: We present a framework for characterizing Bayesian classification methods. This framework can be thought of as a spectrum of allowable dependence in a given probabilistic model with the Naive Bayes algorithm at the most restrictive end and the learning of full Bayesian networks at the most general extreme. While much work has been carried out along the two ends of this spectrum, there has been surprising little done along the middle. We analyze the assumptions made as one moves along this spectrum and show the tradeoffs between model accuracy and learning speed which become critical to consider in a variety of data mining domains. We then present a general induction algorithm that allows for traversal of this spectrum depending on the available computational power for carrying out induction and show its application in a number of domains with different properties. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Buntine, W. </author> <year> 1994. </year> <title> Operations for learning with graphical models. </title> <type> JAIR 2 </type> <pages> 159-225. </pages>
Reference-contexts: Introduction Recently, work in Bayesian methods for classification has grown enormously (Cooper & Herskovits 1992) <ref> (Buntine 1994) </ref>. Bayesian networks (Pearl 1988) have long been a popular medium for graphically representing the probabilistic dependencies which exist in a domain.
Reference: <author> Cooper, G. F., and Herskovits, E. </author> <year> 1992. </year> <title> A bayesian method for the induction of probabilistic networks from data. </title> <booktitle> Machine Learning 9 </booktitle> <pages> 309-347. </pages>
Reference-contexts: Introduction Recently, work in Bayesian methods for classification has grown enormously <ref> (Cooper & Herskovits 1992) </ref> (Buntine 1994). Bayesian networks (Pearl 1988) have long been a popular medium for graphically representing the probabilistic dependencies which exist in a domain. <p> It has only been in the past few years, however, that this framework has been employed with the goal of automatically learning the graphical structure of such a network from a store of data <ref> (Cooper & Herskovits 1992) </ref> (Heckerman, Geiger, & Chickering 1995). In this latter incarnation, such models lend themselves to better understanding of the domain in which they are employed by helping identify dependencies that exist between features in a database as well as being useful for classification tasks.
Reference: <author> Cooper, G. F. </author> <year> 1987. </year> <title> Probabilistic inference using belief networks is NP-Hard. </title> <type> Technical Report KSL-87-27, </type> <institution> Stanford Knowledge Systems Laboratory. </institution>
Reference-contexts: Learning in the domain of unrestricted Bayesian networks is often very time consuming and quickly becomes intractable as the number of features in a domain grows. Moreover, inference in such unrestricted models has been shown to be NP-hard <ref> (Cooper 1987) </ref>. Alternatively, the Naive Bayesian classifer, while very efficient for inference, makes very strong independence assumptions that are often violated in practice and can lead to poor predictive generalization.
Reference: <author> Ezawa, K. J., and Schuermann, T. </author> <year> 1995. </year> <title> Fraud/uncollectible debt detection using a bayesian network learning system. </title> <booktitle> In UAI-95, </booktitle> <pages> 157-166. </pages>
Reference: <author> Friedman, N., and Goldszmidt, M. </author> <year> 1996. </year> <title> Building classifiers using bayesian networks. </title> <booktitle> In AAAI-96. </booktitle>
Reference: <author> Geiger, D. </author> <year> 1992. </year> <title> An entropy-based learning algorithm of bayesian conditional trees. </title> <booktitle> In UAI-92, </booktitle> <pages> 92-97. </pages>
Reference: <author> Good, I. J. </author> <year> 1965. </year> <title> The Estimation of Probabilities: An Essay on Modern Bayesian Methods. </title> <publisher> M.I.T. Press. </publisher>
Reference-contexts: In this latter incarnation, such models lend themselves to better understanding of the domain in which they are employed by helping identify dependencies that exist between features in a database as well as being useful for classification tasks. A particularly restrictive model, the Naive Bayesian classifier <ref> (Good 1965) </ref>, has had a longer history as a simple, yet powerful classification technique. The computational efficiency of this classifier has made it the benefactor of a number of research efforts (Kononenko 1991).
Reference: <author> Heckerman, D.; Geiger, D.; and Chickering, D. </author> <year> 1995. </year> <title> Learning bayesian networks: The combination of knowledge and statistical data. </title> <booktitle> Machine Learning 20 </booktitle> <pages> 197-243. </pages>
Reference-contexts: It has only been in the past few years, however, that this framework has been employed with the goal of automatically learning the graphical structure of such a network from a store of data (Cooper & Herskovits 1992) <ref> (Heckerman, Geiger, & Chickering 1995) </ref>. In this latter incarnation, such models lend themselves to better understanding of the domain in which they are employed by helping identify dependencies that exist between features in a database as well as being useful for classification tasks. <p> Thus we present an alternative to the general trend in Bayesian network learning algorithms which do an expensive search through the space of network structures <ref> (Heckerman, Geiger, & Chicker ing 1995) </ref> or feature dependencies (Pazzani 1995). 1 The question becomes one of determining if the model has allowed for enough dependencies to represent the Markov Blanket (Pearl 1988) of each feature.
Reference: <author> Koller, D., and Sahami, M. </author> <year> 1996. </year> <title> Toward optimal feature selection. </title> <booktitle> In Proceedings of the Thirteenth Int. Conference on Machine Learning. </booktitle>
Reference: <author> Kononenko, I. </author> <year> 1991. </year> <title> Semi-naive bayesian classifier. </title> <booktitle> In Proceedings of the Sixth European Working Session on Learning, </booktitle> <pages> 206-219. </pages> <publisher> Pitman. </publisher>
Reference-contexts: A particularly restrictive model, the Naive Bayesian classifier (Good 1965), has had a longer history as a simple, yet powerful classification technique. The computational efficiency of this classifier has made it the benefactor of a number of research efforts <ref> (Kononenko 1991) </ref>. Although general Bayesian network learning as well as the Naive Bayesian classifier have both shown success in different domains, each has it shortcomings.
Reference: <author> Murphy, P. M., and Aha, D. W. </author> <year> 1995. </year> <note> UCI repository of machine learning databases. http://www.ics.uci.edu/~mlearn/MLRepository.html. </note>
Reference-contexts: Results We tested KDB on five datasets from the UCI repository <ref> (Murphy & Aha 1995) </ref> as well as a text classification domain with many features (a small subset of the Reuters Text database (Reuters 1995)). These datasets are described in Table 1.
Reference: <author> Pazzani, M. J. </author> <year> 1995. </year> <title> Searching for dependencies in bayesian classifiers. </title> <booktitle> In Proceedings of the Fifth Int. Workshop on AI and Statistics. </booktitle>
Reference-contexts: Thus we present an alternative to the general trend in Bayesian network learning algorithms which do an expensive search through the space of network structures (Heckerman, Geiger, & Chicker ing 1995) or feature dependencies <ref> (Pazzani 1995) </ref>. 1 The question becomes one of determining if the model has allowed for enough dependencies to represent the Markov Blanket (Pearl 1988) of each feature.
Reference: <author> Pearl, J. </author> <year> 1988. </year> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan-Kaufmann. </publisher> <address> Reuters. </address> <year> 1995. </year> <title> Reuters document collection. </title> <publisher> ftp://ciir-ftp.cs.umass.edu/pub/reuters1. </publisher>
Reference-contexts: Introduction Recently, work in Bayesian methods for classification has grown enormously (Cooper & Herskovits 1992) (Buntine 1994). Bayesian networks <ref> (Pearl 1988) </ref> have long been a popular medium for graphically representing the probabilistic dependencies which exist in a domain. <p> the general trend in Bayesian network learning algorithms which do an expensive search through the space of network structures (Heckerman, Geiger, & Chicker ing 1995) or feature dependencies (Pazzani 1995). 1 The question becomes one of determining if the model has allowed for enough dependencies to represent the Markov Blanket <ref> (Pearl 1988) </ref> of each feature. We refer the interested reader to Friedman & Goldszmidt (1996) and Koller & Sahami (1996) for more details Our algorithm, called KDB, is supplied with both a database of pre-classified instances, DB, and the k value for the maximum allowable degree of feature dependence.
Reference: <author> Shachter, R. D. </author> <year> 1986. </year> <title> Evaluating influence diagrams. </title> <journal> Operations Research 34(6) </journal> <pages> 871-882. </pages>
Reference-contexts: This is known as Bayes Optimal classification. This general distribution can be captured in a Bayesian network as shown in Figure 1 (a). It is insightful to apply arc reversal <ref> (Shachter 1986) </ref> to the network in Figure 1 (a) to produce the equivalent dependence structure in Figure 1 (b). Here, (X i ) = fC; X 1 ; :::; X i1 g.
References-found: 14

