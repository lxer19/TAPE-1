URL: http://www.cs.rutgers.edu/~uli/PPL98.ps
Refering-URL: http://www.cs.rutgers.edu/~uli/pubs.html
Root-URL: http://www.cs.rutgers.edu
Title: OPTIMAL AND NEAR-OPTIMAL SOLUTIONS FOR HARD COMPILATION PROBLEMS  
Author: ULRICH KREMER 
Keyword: compilers, programming environments, hard optimization prob lems, 0-1 integer programming  
Address: Piscataway, New Jersey 08855, U.S.A.  
Affiliation: Department of Computer Science, Rutgers University  
Note: Parallel Processing Letters c World Scientific Publishing Company  
Abstract: An optimizing compiler typically uses multiple program representations at different levels of program and performance abstractions in order to be able to perform transformations that at least in the majority of cases will lead to an overall improvement in program performance. The complexities of the program and performance abstractions used to formulate compiler optimization problems have to match the complexities of the high-level programming model and of the underlying target system. Scalable parallel systems typically have multi-level memory hierarchies and are able to exploit coarse-grain and fine-grain parallelism. Most likely, future systems will have even deeper memory hierarchies and more granular-ities of parallelism. As a result, future compiler optimizations will have to use more and more complex, multi-level computation and performance models in order to keep up with the complexities of their future target systems. Most of the optimization problems encountered in highly optimizing compilers are already NP-hard, and there is little hope that most newly encountered optimization formulations will not be at least NP-hard as well. To face this "complexity crisis", new methods are needed to evaluate the benefits of a compiler optimization formulation. A crucial step in this evaluation process is to compute the optimal solution of the formulation. Using ad-hoc methods to compute optimal solutions to NP-complete problems may be prohibitively expensive. Recent improvements in mixed integer and 0-1 integer programming suggest that this technology may provide the key to efficient, optimal and near-optimal solutions to NP-complete compiler optimization problems. In fact, early results indicate that integer programming formulations may be efficient enough to be included in not only evaluation prototypes, but in production programming environments or even production compilers. This paper discusses the potential benefits of integer programming as a tool to deal with NP-complete compiler optimization formulations in compilers and programming environments. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> G. Chaitin. </author> <title> Register allocation and spilling via graph coloring. </title> <booktitle> In Proceedings of the SIGPLAN '82 Symposium on Compiler Construction, </booktitle> <month> June </month> <year> 1982. </year>
Reference: 2. <author> P. Briggs. </author> <title> Register Allocation via Graph Coloring. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> April </month> <year> 1992. </year>
Reference: 3. <author> G. L. Nemhauser and L. A. Wolsey. </author> <title> Integer and Combinatorial Optimization. </title> <publisher> John Wiley & Sons, </publisher> <year> 1988. </year>
Reference-contexts: Solving a 0-1 integer programming problem has been shown to be NP-complete. An in-depth discussion of integer programming can be found in <ref> [3] </ref>. For decades, the integer and combinatorical optimization community has been working on methods to solve integer programming problems fast in practice. The ability to solve integer programming problems has been remarkably improved over the last five to ten years.
Reference: 4. <author> R. Bixby. </author> <title> Progress in linear programming. </title> <journal> ORSA Journal on Computing, </journal> <volume> 6(1), </volume> <year> 1994. </year>
Reference-contexts: Important improvements have occurred in three areas. First, linear programming codes are on average approximately two orders of magnitude faster than they were five years ago, particularly for larger 3 problems <ref> [4] </ref>. Combined with the improvements in computing speed over that same period these codes represent an approximate four to five orders of magnitude improvement in our ability to solve linear programming problems. Further algorithmic improvements are expected in linear programming, in preprocessing, and in branch-and-bound heuristics [5].
Reference: 5. <author> G. L. Nemhauser. </author> <title> The age of optimization: Solving large-scale real-world problems. </title> <journal> Operations Research, </journal> <volume> 42(1) </volume> <pages> 5-13, </pages> <month> January-February </month> <year> 1994. </year>
Reference-contexts: Combined with the improvements in computing speed over that same period these codes represent an approximate four to five orders of magnitude improvement in our ability to solve linear programming problems. Further algorithmic improvements are expected in linear programming, in preprocessing, and in branch-and-bound heuristics <ref> [5] </ref>. The second major development is in so-called cutting-plane technology. Motivated by work of Dantzig, Johnson and Fulkerson in the 50's [6], Padberg, Groetschel and others have shown how cutting-plane techniques could be used to strengthen the linear programming relaxations of many 0-1 integer programming problems [7].
Reference: 6. <author> G. B. Dantzig, D. R. Fulkerson, and S. M. Johnson. </author> <title> Solution of a large scale traveling salesman problem. </title> <journal> Operations Research, </journal> <volume> 7 </volume> <pages> 58-66, </pages> <year> 1954. </year>
Reference-contexts: Further algorithmic improvements are expected in linear programming, in preprocessing, and in branch-and-bound heuristics [5]. The second major development is in so-called cutting-plane technology. Motivated by work of Dantzig, Johnson and Fulkerson in the 50's <ref> [6] </ref>, Padberg, Groetschel and others have shown how cutting-plane techniques could be used to strengthen the linear programming relaxations of many 0-1 integer programming problems [7].
Reference: 7. <author> M. Padberg and G. Rinaldi. </author> <title> A branch-and-cut algorithm for the resolution of large-scale symmetric traveling salesman problems. </title> <journal> SIAM Review, </journal> <volume> 33 </volume> <pages> 60-100, </pages> <year> 1991. </year>
Reference-contexts: The second major development is in so-called cutting-plane technology. Motivated by work of Dantzig, Johnson and Fulkerson in the 50's [6], Padberg, Groetschel and others have shown how cutting-plane techniques could be used to strengthen the linear programming relaxations of many 0-1 integer programming problems <ref> [7] </ref>. The third major area of improvement has come in the application of parallel processing to handle the branching when cutting planes do not succeed in sufficiently strengthening the linear programming formulation.
Reference: 8. <author> R. Bixby, W. Cook, A. Cox, and E. K. Lee. </author> <title> Computational experience with parallel mixed integer programming in a distributed environment. </title> <type> Technical Report CRPC-TR95-554, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <month> June </month> <year> 1995. </year>
Reference-contexts: The extra computation at the nodes has the effect of making the computations sufficiently coarse grained that communication costs need not be significant. Parallel versions of mixed-integer programming tools have been developed in academia <ref> [8] </ref> and in industry by Silicon Graphics Incorporation (SGI) a . 3.1 Prototyping and Model Evaluation Advanced, experimental compilation systems and programming environments including Stanford's SUIF compiler and the D95 system currently under development at Rice University use aggressive techniques such as integer programming (Omega test [9]) to solve dependence analysis
Reference: 9. <author> W. Pugh. </author> <title> The Omega test: A fast and practical integer programming algorithm for dependence analysis. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: been developed in academia [8] and in industry by Silicon Graphics Incorporation (SGI) a . 3.1 Prototyping and Model Evaluation Advanced, experimental compilation systems and programming environments including Stanford's SUIF compiler and the D95 system currently under development at Rice University use aggressive techniques such as integer programming (Omega test <ref> [9] </ref>) to solve dependence analysis or code generation problems. These aggressive techniques allow the evaluation of the entire system in a best case scenario where no information is lost due to ad-hoc heuristics. <p> Pugh developed a dependence analysis test, called the Omega Test based on an integer programming algorithm <ref> [9] </ref>. Using integer programming for instruction scheduling under resource constraints for super-scalar machines has been discussed by Feautrier [15], Ning, Govindarajan, Altman and Gao [16,17,18], and Ruttenberg, Gao, Stoutchinin, and Lichtenstein [10]. Goodwin and Wilken used 0-1 integer programming for optimal and near-optimal register allocation [11].
Reference: 10. <author> J. Ruttenberg, G. R. Gao, A. Stoutchinin, and W. Lichtenstein. </author> <title> Software pipelining showdown: Optimal vs. heuristic methods in a production compiler. </title> <booktitle> In Proceedings of the SIGPLAN '96 Conference on Programming Language Design and Implementation, </booktitle> <address> Philadelphia, PA, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: In addition to evaluating models for compiler optimizations, optimal integer programming solutions can be used to assess the effectiveness of approximate solutions generated by heuristics. Work along this line has been done by Rutten a See http://www.sgi.com/Products/hardware/power/operations/perf.html 4 berg, Gao, Stoutchinin, and Lichtenstein in the context of software pipelining <ref> [10] </ref>, and by Goodwin and Wilken for global register allocation [11]. Ruttenberg, Gao, Stoutchinin, and Lichtenstein showed that the heuristics used in a SGI production compiler are effective and close to optimal. <p> Pugh developed a dependence analysis test, called the Omega Test based on an integer programming algorithm [9]. Using integer programming for instruction scheduling under resource constraints for super-scalar machines has been discussed by Feautrier [15], Ning, Govindarajan, Altman and Gao [16,17,18], and Ruttenberg, Gao, Stoutchinin, and Lichtenstein <ref> [10] </ref>. Goodwin and Wilken used 0-1 integer programming for optimal and near-optimal register allocation [11]. Integer programming techniques in the context of a distributed-memory compiler have been discussed by Phillipsen [19] and Garcia, Ayguade and Labarta [20,21]. <p> Possible heuristics include: * Return the best feasible solution within a given amount of time. Goodwin and Wilken discuss first results based on this approach [11] and to some extent Ruttenberg et al. as well <ref> [10] </ref>. * Return the first feasible solution within a specific percentage of optimal. * Run 0-1 formulation and a "conventional" heuristic in parallel. When the conventional heuristic terminates, compare result with the best feasible solution found for the 0-1 formulation. Return the best of the two solutions.
Reference: 11. <author> D. W. Goodwin and K. D. Wilken. </author> <title> Optimal and near-optimal global register allocation using 0-1 integer programming. </title> <journal> Software|Practice and Experience, </journal> <volume> 26(8) </volume> <pages> 929-965, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: Work along this line has been done by Rutten a See http://www.sgi.com/Products/hardware/power/operations/perf.html 4 berg, Gao, Stoutchinin, and Lichtenstein in the context of software pipelining [10], and by Goodwin and Wilken for global register allocation <ref> [11] </ref>. Ruttenberg, Gao, Stoutchinin, and Lichtenstein showed that the heuristics used in a SGI production compiler are effective and close to optimal. <p> Using integer programming for instruction scheduling under resource constraints for super-scalar machines has been discussed by Feautrier [15], Ning, Govindarajan, Altman and Gao [16,17,18], and Ruttenberg, Gao, Stoutchinin, and Lichtenstein [10]. Goodwin and Wilken used 0-1 integer programming for optimal and near-optimal register allocation <ref> [11] </ref>. Integer programming techniques in the context of a distributed-memory compiler have been discussed by Phillipsen [19] and Garcia, Ayguade and Labarta [20,21]. <p> A comparison of different 0-1 formulations for a NP-complete problem in the context of automatic layout can be found in [12]. Goodwin and Wilken discuss the impact of the choice of formulation in the context of global register allocation <ref> [11] </ref>. Another example from outside the compiler domain is described in [22]. 5 In the cases where computing the optimal solution is considered too expensive, integer programming formulations can be used to create families of heuristics that compute near-optimal solutions. <p> Possible heuristics include: * Return the best feasible solution within a given amount of time. Goodwin and Wilken discuss first results based on this approach <ref> [11] </ref> and to some extent Ruttenberg et al. as well [10]. * Return the first feasible solution within a specific percentage of optimal. * Run 0-1 formulation and a "conventional" heuristic in parallel. When the conventional heuristic terminates, compare result with the best feasible solution found for the 0-1 formulation.
Reference: 12. <author> R. Bixby, K. Kennedy, and U. Kremer. </author> <title> Automatic data layout using 0-1 integer programming. </title> <booktitle> In Proceedings of the International Conference on Parallel Architectures and Compilation Techniques (PACT94), </booktitle> <pages> pages 111-122, </pages> <address> Montreal, Canada, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: It is not true that a smaller problem formulation will necessarily lead to faster solution times. A comparison of different 0-1 formulations for a NP-complete problem in the context of automatic layout can be found in <ref> [12] </ref>. Goodwin and Wilken discuss the impact of the choice of formulation in the context of global register allocation [11].
Reference: 13. <author> K. Kennedy and U. Kremer. </author> <title> Automatic data layout for High Performance Fortran. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <address> San Diego, CA, </address> <month> December </month> <year> 1995. </year>
Reference: 14. <author> U. Kremer. </author> <title> Automatic Data Layout for Distributed Memory Machines. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> October </month> <year> 1995. </year> <note> Available as CRPC-TR95-559-S. </note>
Reference-contexts: For example, solving a 0-1 integer programming instance with 14950 variables and 4663 constraints for an automatic data layout problem took 357 seconds on a Sparc-10 using CPLEXv3.0 b <ref> [14] </ref>. CPLEX is a state-of-the-art linear integer programming tool and library, partly developed by Robert Bixby at Rice University [23]. CPLEX includes an implementation of a general-purpose branch-and-bound code for mixed integer programming.
Reference: 15. <author> P. Feautrier. </author> <title> Fine-grain scheduling under resource constraints. </title> <booktitle> In Proceedings of the Seventh Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Ithaca, New York, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: Pugh developed a dependence analysis test, called the Omega Test based on an integer programming algorithm [9]. Using integer programming for instruction scheduling under resource constraints for super-scalar machines has been discussed by Feautrier <ref> [15] </ref>, Ning, Govindarajan, Altman and Gao [16,17,18], and Ruttenberg, Gao, Stoutchinin, and Lichtenstein [10]. Goodwin and Wilken used 0-1 integer programming for optimal and near-optimal register allocation [11]. Integer programming techniques in the context of a distributed-memory compiler have been discussed by Phillipsen [19] and Garcia, Ayguade and Labarta [20,21].
Reference: 16. <author> Q. Ning and G. R. Gao. </author> <title> A novel framework of register allocation for software pipelin-ing. </title> <booktitle> In Proceedings of the Twentieth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Albuquerque, NM, </address> <month> January </month> <year> 1993. </year>
Reference: 17. <author> R. Govindarajan E. R. Altman and G. R. Gao. </author> <title> Minimizing register requirements under resource-constrained rate-optimal software pipelining. </title> <booktitle> In Proceedings of the 27th Annual International Symposium on Microarchitecture, </booktitle> <address> San Jose, CA, </address> <month> Decem-ber </month> <year> 1994. </year> <month> 7 </month>
Reference: 18. <author> E. R. Altman, R. Govindarajan, and G. R. Gao. </author> <title> Scheduling and mapping: Software pipelining in the presence of structural hazards. </title> <booktitle> In Proceedings of the SIGPLAN '95 Conference on Programming Language Design and Implementation, </booktitle> <address> La Jolla, CA, </address> <month> June </month> <year> 1995. </year>
Reference: 19. <author> M. Philippsen. </author> <title> Automatic alignment of array data and processes to reduce communication time on DMPPs. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Goodwin and Wilken used 0-1 integer programming for optimal and near-optimal register allocation [11]. Integer programming techniques in the context of a distributed-memory compiler have been discussed by Phillipsen <ref> [19] </ref> and Garcia, Ayguade and Labarta [20,21]. The latter two works have been based on the experience with 0-1 integer programming for efficient solutions of NP-complete problems in an automatic data layout tool as described by Bixby, Kennedy, and Kremer [12,13,14].
Reference: 20. <author> J. Garcia, E. Ayguade, and J. Labarta. </author> <title> A novel approach towards automatic data distribution. In Proceedings of the Workshop on Automatic Data Layout and Performance Prediction (AP'95), </title> <address> Houston, TX, </address> <month> April </month> <year> 1995. </year>
Reference: 21. <author> J. Garcia. </author> <title> Automatic Data Distribution for Massively Parallel Processors. </title> <type> PhD thesis, </type> <institution> Universitat Politecnica de Catalunya, Barcelona, </institution> <month> April </month> <year> 1997. </year>
Reference: 22. <author> C. Barnhart, E. L. Johnson, G. L. Nemhauser, B. Sigismondi, and P. Vance. </author> <title> Formulating a mixed integer programming problem to improve solvability. </title> <journal> Operations Research, </journal> <volume> 41(6) </volume> <pages> 1013-1019, </pages> <month> November-December </month> <year> 1993. </year>
Reference-contexts: Goodwin and Wilken discuss the impact of the choice of formulation in the context of global register allocation [11]. Another example from outside the compiler domain is described in <ref> [22] </ref>. 5 In the cases where computing the optimal solution is considered too expensive, integer programming formulations can be used to create families of heuristics that compute near-optimal solutions. Possible heuristics include: * Return the best feasible solution within a given amount of time.
Reference: 23. <author> R. Bixby. </author> <title> Implementing the Simplex method: The initial basis. </title> <journal> ORSA Journal on Computing, </journal> <volume> 4(3), </volume> <year> 1992. </year>
Reference-contexts: For example, solving a 0-1 integer programming instance with 14950 variables and 4663 constraints for an automatic data layout problem took 357 seconds on a Sparc-10 using CPLEXv3.0 b [14]. CPLEX is a state-of-the-art linear integer programming tool and library, partly developed by Robert Bixby at Rice University <ref> [23] </ref>. CPLEX includes an implementation of a general-purpose branch-and-bound code for mixed integer programming. Being general purpose, this code does not a priori exploit the structural properties of a particular 0-1 problem.
Reference: 24. <author> Irv Lustig. </author> <title> Director of numerical optimization, CPLEX Optimization, Inc., </title> <type> private communication, </type> <month> August </month> <year> 1996. </year> <month> 8 </month>
Reference-contexts: The same problem instance can now be solved on an UltraSparc-1 in 29 seconds using the new version CPLEXv4.0. Approximately 50% of this improvement can be attributed to algorithmic improvements in the presolver step of the tool <ref> [24] </ref>. Choosing the CPLEX flag that selects a dual instead of the default primal simplex for the initial LP relaxation results in a further reduction of the solution time to 3.4 seconds.
References-found: 24

