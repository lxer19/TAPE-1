URL: http://www.cs.rpi.edu/~ziantzl/Papers/96/LCR_EPL/lcr.ps
Refering-URL: http://www.cs.rpi.edu/~ziantzl/Papers/papers.html
Root-URL: http://www.cs.rpi.edu
Note: clxviii  
Abstract-found: 0
Intro-found: 1
Reference: [1] <editor> J. L. Gaudiot and L. Bic. </editor> <booktitle> Advanced topics in dataflow computing. </booktitle> <publisher> Prentice Hall, </publisher> <address> New Jersey, </address> <year> 1991. </year>
Reference-contexts: Every instance of each equation could be a separate thread enabled for execution when the data it requires becomes available <ref> [14, 1, 4] </ref>. Such synchronization would enforce a valid order of EPL program execution. However, this form of parallelization is not efficient on current dataflow machines, such as the Monsoon [5], because all synchronization is done at run-time, increasing the overhead incurred by token communication and matching.
Reference: [2] <author> A. Gerasoulis and T. Yang. </author> <title> Comparison of clustering heuristics for scheduling directed acyclic graphs on multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> (16):276-291, 1992. 
Reference-contexts: For the final step, we are investigating how known clustering techniques, e.g., reducing the makespan <ref> [2] </ref>, or increasing throughput [11], apply to this environment. Determining the Pipeline Parameters The following is a simple analytical model that is used to illustrate how optimal parameters of a pipeline can be determined.
Reference: [3] <author> A. Gerasoulis and T. Yang. </author> <title> On the granularity and clustering of directed acyclic task graphs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(6) </volume> <pages> 686-701, </pages> <year> 1993. </year>
Reference-contexts: It should be noted that the object code created by pipelining invokes different parallel tasks executing together, and therefore such parallelization reaches beyond the SPMD model. The goal of pipeline loop optimization is to enable the compiler to adjust the granularity <ref> [3] </ref> of the resulting computation to match the architectural parameters of a target architecture. This can be done by first adjusting the size of the pipeline stages to balance the computational load of each resulting cluster. <p> For parallelization and pipeline optimization, only the boundaries of the pipeline loops are significant. Thus, each schedule graph node in a pipeline loop can potentially be scheduled on a separate processor as a pipeline stage. However, Integrating Data and Task Parallelism in Scientific Programs 183 as shown in <ref> [8, 3] </ref> this may lead to a parallelization with too fine a granularity. Moreover, a pipeline's throughput is limited by its worst stage.
Reference: [4] <author> B. Lee and A. R. Hurston. </author> <title> Dataflow architectures and multithreading. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 27-39, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: Every instance of each equation could be a separate thread enabled for execution when the data it requires becomes available <ref> [14, 1, 4] </ref>. Such synchronization would enforce a valid order of EPL program execution. However, this form of parallelization is not efficient on current dataflow machines, such as the Monsoon [5], because all synchronization is done at run-time, increasing the overhead incurred by token communication and matching.
Reference: [5] <author> G. M. Papadopoulos. </author> <title> Implementation of a general-purpose dataflow multiprocessor. </title> <booktitle> Research Monographs in Parallel and Distributed Computing. </booktitle> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: Such synchronization would enforce a valid order of EPL program execution. However, this form of parallelization is not efficient on current dataflow machines, such as the Monsoon <ref> [5] </ref>, because all synchronization is done at run-time, increasing the overhead incurred by token communication and matching. A more efficient approach is to recognize at compile time which threads must execute sequentially relative to each other and then to merge them.
Reference: [6] <author> K. Psarris, X. Kong, and D. Klappholz. </author> <title> The direction vector I test. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 11(4), </volume> <month> November </month> <year> 1993. </year>
Reference-contexts: Note that assignments to two distinct array elements are valid in SAF; repeated assignments to the same array element are not valid. To determine whether assignments are made to the same array elements, we use various dependence tests (cf. <ref> [6, 7] </ref>). If the results of the dependence tests are negative within a loop, then no two elements are reassigned in the loop, and no temporal dimension is needed. The temporal dimension is so named because it represents a time-stamp for each iteration.
Reference: [7] <author> W. Pugh and D. Wonnacott. </author> <title> Nonlinear array dependence analysis. </title> <editor> In B. K. Szymanski and B. Sinharoy, editors, </editor> <booktitle> Languages, Compilers and Run-Time Systems for Scalable Computers, </booktitle> <pages> pages 1-14. </pages> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1995. </year> <title> Integrating Data and Task Parallelism in Scientific Programs 185 </title>
Reference-contexts: Note that assignments to two distinct array elements are valid in SAF; repeated assignments to the same array element are not valid. To determine whether assignments are made to the same array elements, we use various dependence tests (cf. <ref> [6, 7] </ref>). If the results of the dependence tests are negative within a loop, then no two elements are reassigned in the loop, and no temporal dimension is needed. The temporal dimension is so named because it represents a time-stamp for each iteration.
Reference: [8] <author> V. Sarkar. </author> <title> Partitioning and scheduling parallel programs for multiprocessors. </title> <booktitle> Research monographs in parallel and distributed computing. </booktitle> <publisher> MIT Press, </publisher> <address> Cam-bridge, MA., </address> <year> 1989. </year>
Reference-contexts: For parallelization and pipeline optimization, only the boundaries of the pipeline loops are significant. Thus, each schedule graph node in a pipeline loop can potentially be scheduled on a separate processor as a pipeline stage. However, Integrating Data and Task Parallelism in Scientific Programs 183 as shown in <ref> [8, 3] </ref> this may lead to a parallelization with too fine a granularity. Moreover, a pipeline's throughput is limited by its worst stage.
Reference: [9] <author> B. Sinharoy and B. K. Szymanski. </author> <title> Memory optimization for parallel functional programs. </title> <booktitle> Computer Systems in Engineering. To appear; abstract published in "Abstracts: International Meeting on Vector and Parallel Processing," </booktitle> <address> CICA, Porto, Portugal, </address> <year> 1993, </year> <note> p. 36.). </note>
Reference-contexts: In this way, each domination reduces the search space by one. Once the loop graph is fully constructed, the final step is to choose the most memory efficient, feasible (i.e., compatible) set of well-formed loops. Memory efficiency is based on the window sizes for each loop as calculated in <ref> [9] </ref>. The results of the memory optimization are either read by the code generator for serial output, or parallelized as described in the next section.
Reference: [10] <author> K. L. Spier and B. K. Szymanski. </author> <title> Interprocess analysis and optimization in the equational language compiler. </title> <editor> In Burkhart, editor, CONPAR 90-VAPP, </editor> <booktitle> Joint International Conference on Vector and Parallel Processing, </booktitle> <address> Zurich, Switzerland, </address> <booktitle> Lecture Notes In Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <month> September </month> <year> 1990. </year>
Reference-contexts: To determine the compatibility for each node in the CSG we will use an algorithm described in <ref> [10] </ref>, for internal data dependency propagation. 5.2 Optimizing Pipelines for Target Architectures Pipeline Optimization Process The following steps are used to optimize the execution of pipeline loops on a given target architecture: 1. Apply algorithm in Section 4.2, replacing Definition 4.8 with Definition 5.1.
Reference: [11] <author> J. Subhlok, D. R. O'Hallaron, T. Gross, P. A. Dinda, and J. Webb. </author> <title> Communication and memory requirements as the basis for mapping task and data parallel programs. </title> <booktitle> In Proceedings of SuperComputing 1994. ACM, </booktitle> <year> 1994. </year>
Reference-contexts: For the final step, we are investigating how known clustering techniques, e.g., reducing the makespan [2], or increasing throughput <ref> [11] </ref>, apply to this environment. Determining the Pipeline Parameters The following is a simple analytical model that is used to illustrate how optimal parameters of a pipeline can be determined.
Reference: [12] <author> B. K. Szymanski. </author> <title> EPL-parallel programming with recurrent equations. </title> <editor> In B.K. Szymanski, editor, </editor> <booktitle> Parallel Functional Languages and Compilers. </booktitle> <publisher> ACM Press/Addison Wesley, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: We are developing a system that automatically transforms serial FORTRAN into parallel C, performing memory optimization and introducing data and task parallelism (see Figure 1). The core of the system is a new version of the EPL compiler (the original version of EPL is described in <ref> [12] </ref>) with a y This work was partially supported by ONR Grant N00014-93-1-0076 and NSF Grant CCR-9216053. 170 Chapter 13 FORTRAN front-end. EPL is a functional language and therefore obeys the single-assignment rule. As such, data dependencies in EPL are readily visible. <p> For a special class of subscripts, called sublinear subscripts in EPL <ref> [12] </ref>, this is a straightforward extension and it will be included in the next version of the EPL compiler.) Variable definitions are characterized by their range-sets. Definition 4.1 A range-set defines the iteration space for a single dimension of a variable's defining equation. <p> Definition 5.1 A non-empty subset L N of nodes in a variable graph is called a pipeline loop iff: 1. The set of iterators over L is non-empty. (I (L) 6= ;) 9 Equivalence is defined over the relation in terms of subscript sublinearity (see <ref> [12] </ref>). Integrating Data and Task Parallelism in Scientific Programs 181 2. The closure of L is equal to L. (Cl (L) = L) Any clustering of the schedule graph nodes defines an equivalence relation between those nodes.
Reference: [13] <author> B. K. Szymanski and N. S. Prywes. </author> <title> Efficient handling of data structures in definitional languages. </title> <booktitle> Science of Computer Programming, </booktitle> <volume> 10(3) </volume> <pages> 221-245, </pages> <year> 1988. </year>
Reference-contexts: This allows the object code to allocate a "window" large enough for only this fixed number of elements, not the entire dimension of the Integrating Data and Task Parallelism in Scientific Programs 171 structure <ref> [13] </ref>. As with the original FORTRAN, the elements in the window are overwritten in each iteration. 1 Programmers use variable windows intuitively, often with the intension of maintaining code readability. This approach may discount several more efficient (but counter-intuitive) windowing opportunities.
Reference: [14] <author> A. H. Veen. </author> <title> Dataflow machine architecture. </title> <journal> ACM Computing Surveys, </journal> <volume> 18(4), </volume> <year> 1986. </year>
Reference-contexts: Every instance of each equation could be a separate thread enabled for execution when the data it requires becomes available <ref> [14, 1, 4] </ref>. Such synchronization would enforce a valid order of EPL program execution. However, this form of parallelization is not efficient on current dataflow machines, such as the Monsoon [5], because all synchronization is done at run-time, increasing the overhead incurred by token communication and matching.
References-found: 14

