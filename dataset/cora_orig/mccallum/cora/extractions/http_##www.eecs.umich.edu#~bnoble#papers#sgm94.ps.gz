URL: http://www.eecs.umich.edu/~bnoble/papers/sgm94.ps.gz
Refering-URL: http://www.eecs.umich.edu/~bnoble/papers/papers.html
Root-URL: http://www.eecs.umich.edu
Email: fbnoble,satyag@cs.cmu.edu  
Title: An Empirical Study of a Highly Available File System  
Author: Brian D. Noble and M. Satyanarayanan 
Affiliation: School of Computer Science Carnegie Mellon University  
Abstract: In this paper we present results from a six-month empirical study of the high availability aspectsof the Coda File System. We report on the servicefailures experienced by Coda clients, and show that such failures are masked successfully. We also explore the effectiveness and resource costs of key aspects of server replication and disconnected operation, the two high availability mechanisms of Coda. Wherever possible, we compare our measurements to simulation-based predictions from earlier papers and to anecdotal evidence from users. Finally, we explore how users take advantage of the support provided by Coda for mobile computing. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> BAKER, M. G., HARTMAN, J. H., KUPFER, M. D., SHIRRIFF, K. W., AND OUSTER-HOUT, J. K. </author> <title> Measurements of a Distributed File System. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating System Principles (Pacific Grove, </booktitle> <address> CA, </address> <month> October </month> <year> 1991). </year>
Reference: [2] <author> BHIDE, A., ELNOZAHY, E. N., AND MORGAN, S. P. </author> <title> A Highly Available Network File Server. </title> <booktitle> In Winter Usenix Conference Proceedings (Dallas, </booktitle> <address> TX, </address> <month> January </month> <year> 1991). </year>
Reference-contexts: 1 Introduction Providing high availability is a dominant theme of current file system research. Examples of systems with this goal include Coda [18], Echo [6], Ficus [14], HA-NFS <ref> [2] </ref>, Deceit [22], and FACE [3]. Now that serious use of such systems is feasible, it is appropriate to ask how well their high availability mechanisms function in practice. This paper is our attempt to answer this question for the Coda File System.
Reference: [3] <author> COVA, L. </author> <title> Resource Management in Federated Computing Environments. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Princeton University, </institution> <month> October </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Providing high availability is a dominant theme of current file system research. Examples of systems with this goal include Coda [18], Echo [6], Ficus [14], HA-NFS [2], Deceit [22], and FACE <ref> [3] </ref>. Now that serious use of such systems is feasible, it is appropriate to ask how well their high availability mechanisms function in practice. This paper is our attempt to answer this question for the Coda File System.
Reference: [4] <author> FLOYD, R. </author> <title> Directory Reference Patterns in a Unix Environment. </title> <type> Tech. Rep. </type> <institution> TR-179, Department of Computer Science, University of Rochester, </institution> <year> 1986. </year>
Reference-contexts: A700), the National Science Foundation, the IBM Corporation, Digital Equipment Corporation, and Intel Corporation. The views and conclusionexpressed in this paper are those of the authors, and should not be interpreted as those of the funding organizations or Carnegie Mellon University. Ousterhout [13], and Floyd <ref> [4, 5] </ref> formed the basis for our initial understanding of file system usage. This understanding was crucial to the design of distributed file systems such as AFS [7] and Sprite [12].
Reference: [5] <author> FLOYD, R. </author> <title> Short-Term File Reference Patterns in a Unix Environment. </title> <type> Tech. Rep. </type> <institution> TR-177, Department of Computer Science, University of Rochester, </institution> <year> 1986. </year>
Reference-contexts: A700), the National Science Foundation, the IBM Corporation, Digital Equipment Corporation, and Intel Corporation. The views and conclusionexpressed in this paper are those of the authors, and should not be interpreted as those of the funding organizations or Carnegie Mellon University. Ousterhout [13], and Floyd <ref> [4, 5] </ref> formed the basis for our initial understanding of file system usage. This understanding was crucial to the design of distributed file systems such as AFS [7] and Sprite [12].
Reference: [6] <author> HISGEN, A., BIRRELL, A., MANN, T., SCHROEDER, M., AND SWART, G. </author> <title> Availability and Consistency Tradeoffs in the EchoDistributed File System. </title> <booktitle> In Proceedings of the Second Workshop on Workstation Operating Systems (Pacific Grove, </booktitle> <address> CA, </address> <month> September </month> <year> 1989). </year>
Reference-contexts: 1 Introduction Providing high availability is a dominant theme of current file system research. Examples of systems with this goal include Coda [18], Echo <ref> [6] </ref>, Ficus [14], HA-NFS [2], Deceit [22], and FACE [3]. Now that serious use of such systems is feasible, it is appropriate to ask how well their high availability mechanisms function in practice. This paper is our attempt to answer this question for the Coda File System.
Reference: [7] <author> HOWARD, J. H., KAZAR, M. L., MENEES, S. G., NICHOLS, D. A., SATYA-NARAYANAN, M., SIDEBOTHAM, R. N., AND WEST, M. J. </author> <title> Scale and Performance in a Distributed File System. </title> <journal> ACM Transactions on Computer Systems 6, </journal> <month> 1 (February </month> <year> 1988). </year>
Reference-contexts: Ousterhout [13], and Floyd [4, 5] formed the basis for our initial understanding of file system usage. This understanding was crucial to the design of distributed file systems such as AFS <ref> [7] </ref> and Sprite [12]. In 1991, Baker et al.[1] examined Sprite with a view to establishing how closely its real usage matched predicted usage. More recently, Spasojevic and Satyanarayanan [24] reported on the use of wide-area AFS.
Reference: [8] <author> KISTLER, J. J. </author> <title> Disconnected Operation in a Distributed File System. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <year> 1993. </year>
Reference-contexts: Note that the y axis is log 10 scaled. that over 99% of all operation logs grow less than 240KB per day. 4.3 Disconnected Operation Mutations made during disconnected operation at a Coda client are recorded in a per-volume replay log. Coda employs many cancellation optimizations <ref> [8] </ref> to reduce the amount of log space used. Upon reconnection, the client transparently invokes reintegration of each modified volume. If a volume's log is successfully replayed by the servers, they proceed to backfetch the contents of modified files.
Reference: [9] <author> KISTLER, J. J., AND SATYANARAYANAN, M. </author> <title> Disconnected Operation in the Coda File System. </title> <journal> ACM Transactions on Computer Systems 10, </journal> <month> 1 (February </month> <year> 1992). </year>
Reference-contexts: In this section, we provide a short overview of Coda; further details can be found in earlier papers <ref> [9, 11, 17, 18, 19, 20, 25] </ref>. Clients view Coda as a single, location-transparent shared Unix file system. The Coda name space is mapped to individual file servers at the granularity of subtrees called volumes [21]. <p> These attempts are neither successful nor result in a conflict. Table 3 indicates a conflict rate of about 1.3%, only slightly larger than that predicted by an earlier study based on AFS <ref> [9] </ref>. <p> There is one outlier not pictured here, as detailed in Section 4.3.2. rate of reintegration conflicts would not prima facie contradict our earlier predictions of much lower likelihood of conflicts between different users <ref> [9] </ref>. One anomalous event is not included in the above analysis. A user who was unfamiliar with the write-sharing semantics of Coda ran simulations on five machines which logged information to a single Coda file.
Reference: [10] <author> KLEIMAN, S. R. Vnodes: </author> <title> An Architecture for Multiple File System Types in Sun UNIX. </title> <booktitle> In Summer Usenix Conference Proceedings (Atlanta, </booktitle> <address> GA, </address> <year> 1986). </year>
Reference-contexts: The Coda name space is mapped to individual file servers at the granularity of subtrees called volumes [21]. At each client, a user level process, Venus, caches data on demand on the client's local disk. An in-kernel VFS driver <ref> [10] </ref>, called the Mini-Cache, intercepts and forwards file references to Venus. Coda uses two distinct, but complementary, mechanisms to achieve high availability. Both mechanisms rely on an optimistic replica control strategy. This offers a high degree of availability, since data can be updated in any network partition.
Reference: [11] <author> KUMAR, P., AND SATYANARAYANAN, M. </author> <title> Log-Based Directory Resolution in the Coda File System. </title> <booktitle> In Proceedings of the Second International Conference on Parallel and Distributed Information Systems (San Diego, </booktitle> <address> CA, </address> <month> January </month> <year> 1993). </year>
Reference-contexts: In this section, we provide a short overview of Coda; further details can be found in earlier papers <ref> [9, 11, 17, 18, 19, 20, 25] </ref>. Clients view Coda as a single, location-transparent shared Unix file system. The Coda name space is mapped to individual file servers at the granularity of subtrees called volumes [21]. <p> Table 4 details reasons for directory conflict as observed at individual replica sites participating in a resolution attempt. Since these observations must be made at the replica sites themselves, as opposed to the resolution coordinator <ref> [11] </ref>, they give an upper bound on the total number of conflicts. <p> The figure indicates that log growth is quite modest, with a mean high-water mark of 19KB. Although a few instances of high-water marks over 250KB were observed, the vast majority were under 200KB. A previous study, based on trace-driven simulation of the resolution subsystem <ref> [11] </ref>, predicted a maximum log-growth of 3.3MB per volume per day. Our observations indicate that this grossly overestimates true log growth the largest value we have observed is 385KB per volume per day. That study also predicted that 99.5% of all resolution logs would grow less than 240KB per day.
Reference: [12] <author> NELSON, M. N., WELCH, B. B., AND OUSTERHOUT, J. K. </author> <title> Caching in the Sprite Network File System. </title> <journal> ACM Transactions on Computer Systems 6, </journal> <month> 1 (February </month> <year> 1988). </year>
Reference-contexts: Ousterhout [13], and Floyd [4, 5] formed the basis for our initial understanding of file system usage. This understanding was crucial to the design of distributed file systems such as AFS [7] and Sprite <ref> [12] </ref>. In 1991, Baker et al.[1] examined Sprite with a view to establishing how closely its real usage matched predicted usage. More recently, Spasojevic and Satyanarayanan [24] reported on the use of wide-area AFS. In this paper, we report on data collected from Coda over a 6-month period.
Reference: [13] <author> OUSTERHOUT, J. K., DACOSTA, H., HARRISON, D., KUNZE, J. A., KUPFER, M., AND THOMPSON, J. G. </author> <title> A Trace-Driven Analysis of the UNIX 4.2 BSD File System. </title> <booktitle> In Proceedings of the 10th ACM Symposium on Operating System Principles (Orcas Island, </booktitle> <address> WA, </address> <month> December </month> <year> 1985). </year>
Reference-contexts: A700), the National Science Foundation, the IBM Corporation, Digital Equipment Corporation, and Intel Corporation. The views and conclusionexpressed in this paper are those of the authors, and should not be interpreted as those of the funding organizations or Carnegie Mellon University. Ousterhout <ref> [13] </ref>, and Floyd [4, 5] formed the basis for our initial understanding of file system usage. This understanding was crucial to the design of distributed file systems such as AFS [7] and Sprite [12].
Reference: [14] <author> POPEK, G. J., GUY, R. G., PAGE, T. W., AND HEIDEMANN, J. S. </author> <title> Replication in Ficus Distributed File Systems. </title> <booktitle> In Proceedings of the Workshop on Management of Replicated Data (Houston, </booktitle> <address> TX, </address> <month> November </month> <year> 1990). </year>
Reference-contexts: 1 Introduction Providing high availability is a dominant theme of current file system research. Examples of systems with this goal include Coda [18], Echo [6], Ficus <ref> [14] </ref>, HA-NFS [2], Deceit [22], and FACE [3]. Now that serious use of such systems is feasible, it is appropriate to ask how well their high availability mechanisms function in practice. This paper is our attempt to answer this question for the Coda File System.
Reference: [15] <author> SATYANARAYANAN, M. </author> <title> A Study of File Sizes and Functional Lifetimes. </title> <booktitle> In Proceedings of the 8th ACM Symposium on Operating System Principles (Pacific Grove, </booktitle> <address> CA, </address> <month> December </month> <year> 1981). </year>
Reference-contexts: To the best of our knowledge, this is the first empirical study of a highly available distributed file system. Empirical studies of file systems have a long history, stretching back to the 1970s. Early studies of timesharing file systems such as those by Stritter [26], Smith [23], Satyanarayanan <ref> [15, 16] </ref>, This work has been supported by the Advanced Research Projects Agency (Avionics Lab, Wright Research and Development Center, Aeronautical Systems Division (AFSC), U.S. Air Force, Wright-Patterson AFB, Ohio, 45433-6543 under Contract F33615-90-C-1465, ARPA Order No. 7597 and Hanscom AFB, Contract F19628-93-C-0193, ARPA Order No.
Reference: [16] <author> SATYANARAYANAN, M. </author> <title> A Synthetic Driver for File System Simulations. </title> <booktitle> In Proceedings of the International Conference on Modelling Techniques and Tools for Performance Analysis (Paris, </booktitle> <month> May </month> <year> 1984). </year>
Reference-contexts: To the best of our knowledge, this is the first empirical study of a highly available distributed file system. Empirical studies of file systems have a long history, stretching back to the 1970s. Early studies of timesharing file systems such as those by Stritter [26], Smith [23], Satyanarayanan <ref> [15, 16] </ref>, This work has been supported by the Advanced Research Projects Agency (Avionics Lab, Wright Research and Development Center, Aeronautical Systems Division (AFSC), U.S. Air Force, Wright-Patterson AFB, Ohio, 45433-6543 under Contract F33615-90-C-1465, ARPA Order No. 7597 and Hanscom AFB, Contract F19628-93-C-0193, ARPA Order No.
Reference: [17] <author> SATYANARAYANAN, M. </author> <title> Scalable, Secure, and Highly Available Distributed File Access. </title> <booktitle> IEEE Computer 23, </booktitle> <month> 5 (May </month> <year> 1990). </year>
Reference-contexts: In this section, we provide a short overview of Coda; further details can be found in earlier papers <ref> [9, 11, 17, 18, 19, 20, 25] </ref>. Clients view Coda as a single, location-transparent shared Unix file system. The Coda name space is mapped to individual file servers at the granularity of subtrees called volumes [21].
Reference: [18] <author> SATYANARAYANAN, M., KISTLER, J. J., KUMAR, P., OKASAKI, M. E., SIEGEL, E. H., AND STEERE, D. C. Coda: </author> <title> A Highly Available File System for a Distributed Workstation Environment. </title> <journal> IEEE Transactions on Computers 39, </journal> <month> 4 (April </month> <year> 1990). </year>
Reference-contexts: 1 Introduction Providing high availability is a dominant theme of current file system research. Examples of systems with this goal include Coda <ref> [18] </ref>, Echo [6], Ficus [14], HA-NFS [2], Deceit [22], and FACE [3]. Now that serious use of such systems is feasible, it is appropriate to ask how well their high availability mechanisms function in practice. This paper is our attempt to answer this question for the Coda File System. <p> In this section, we provide a short overview of Coda; further details can be found in earlier papers <ref> [9, 11, 17, 18, 19, 20, 25] </ref>. Clients view Coda as a single, location-transparent shared Unix file system. The Coda name space is mapped to individual file servers at the granularity of subtrees called volumes [21]. <p> These situations corresponds to weak equality, where the replicas are actually equal, but their version information does not reflect this fact. The circumstances under which this can happen have been explained elsewhere <ref> [18] </ref>. Another common event is runt forcing. This corresponds to situations where an empty file replica was created via a previous resolution of the parent directory. As shown in the table, there were 21 directory resolution attempts that had to be aborted due to our deadlock avoidance policy.
Reference: [19] <author> SATYANARAYANAN, M., KISTLER, J. J., MUMMERT, L. B., EBLING, M. R., KUMAR, P., AND LU, Q. </author> <title> Experience with Disconnected Operation in a Mobile Computing Environment. </title> <booktitle> In Proceedings of the 1993 USENIX Symposium on Mobile and Location-IndependentComputing (Cambridge, </booktitle> <address> MA, </address> <month> August </month> <year> 1993). </year>
Reference-contexts: In this section, we provide a short overview of Coda; further details can be found in earlier papers <ref> [9, 11, 17, 18, 19, 20, 25] </ref>. Clients view Coda as a single, location-transparent shared Unix file system. The Coda name space is mapped to individual file servers at the granularity of subtrees called volumes [21]. <p> On average, replay logs without optimizations would have been over 2.5 times longer than the logs actually encountered in Coda. This corroborates earlier estimates, based on trace-driven simulation, that indicated that unoptimized logs would be between 2 and 3 times the length of optimized logs <ref> [19] </ref>.
Reference: [20] <author> SATYANARAYANAN, M., MASHBURN, H. H., KUMAR, P., STEERE, D. C., AND KISTLER, J. J. </author> <title> Lightweight Recoverable Virtual Memory. </title> <booktitle> In Proceedings of the Fourteenth ACM Symposium on Operating Systems Principles (Asheville, </booktitle> <address> NC, </address> <month> December </month> <year> 1993). </year>
Reference-contexts: In this section, we provide a short overview of Coda; further details can be found in earlier papers <ref> [9, 11, 17, 18, 19, 20, 25] </ref>. Clients view Coda as a single, location-transparent shared Unix file system. The Coda name space is mapped to individual file servers at the granularity of subtrees called volumes [21]. <p> On disconnected clients, which may be turned on and off many times before reconnection, we buffer collected data in non-volatile storage until reconnection. We use the recoverable virtual memory (RVM) transactional mechanism for this purpose because of its clean failure semantics <ref> [20] </ref>. Since RVM resources are precious on a resource-poor portable computer, we strictly and conservatively cap its usage; this favors availability of the system over completeness of the data collected.
Reference: [21] <author> SIDEBOTHAM, R. </author> <title> Volumes: The Andrew File System Data Structuring Primitive. In European Unix User Group Conference Proceedings (August 1986). </title> <note> Also available as Tech. Rep. </note> <institution> CMU-ITC-053, Carnegie Mellon University, Information Technology Center. </institution>
Reference-contexts: Clients view Coda as a single, location-transparent shared Unix file system. The Coda name space is mapped to individual file servers at the granularity of subtrees called volumes <ref> [21] </ref>. At each client, a user level process, Venus, caches data on demand on the client's local disk. An in-kernel VFS driver [10], called the Mini-Cache, intercepts and forwards file references to Venus. Coda uses two distinct, but complementary, mechanisms to achieve high availability.
Reference: [22] <author> SIEGEL, A., BIRMAN, K., AND MARZULLO, K. Deceit: </author> <title> A Flexible Distributed File System. </title> <type> Tech. Rep. TR 89-1042, </type> <institution> Department of Computer Science, Cornell University, </institution> <year> 1989. </year>
Reference-contexts: 1 Introduction Providing high availability is a dominant theme of current file system research. Examples of systems with this goal include Coda [18], Echo [6], Ficus [14], HA-NFS [2], Deceit <ref> [22] </ref>, and FACE [3]. Now that serious use of such systems is feasible, it is appropriate to ask how well their high availability mechanisms function in practice. This paper is our attempt to answer this question for the Coda File System.
Reference: [23] <author> SMITH, A. J. </author> <title> Analysis of Long Term File Reference Patterns for Application to File Migration Algorithms. </title> <journal> IEEE Transactions on Software Engineering 7, </journal> <month> 4 (July </month> <year> 1981). </year>
Reference-contexts: To the best of our knowledge, this is the first empirical study of a highly available distributed file system. Empirical studies of file systems have a long history, stretching back to the 1970s. Early studies of timesharing file systems such as those by Stritter [26], Smith <ref> [23] </ref>, Satyanarayanan [15, 16], This work has been supported by the Advanced Research Projects Agency (Avionics Lab, Wright Research and Development Center, Aeronautical Systems Division (AFSC), U.S. Air Force, Wright-Patterson AFB, Ohio, 45433-6543 under Contract F33615-90-C-1465, ARPA Order No. 7597 and Hanscom AFB, Contract F19628-93-C-0193, ARPA Order No.
Reference: [24] <author> SPASOJEVIC, M., AND SATYANARAYANAN, M. </author> <title> A Usage Profile and Evaluation of a Wide-Area Distributed File System. </title> <booktitle> In Winter Usenix Conference Proceedings (San Francisco, </booktitle> <address> CA, </address> <month> January </month> <year> 1994). </year>
Reference-contexts: This understanding was crucial to the design of distributed file systems such as AFS [7] and Sprite [12]. In 1991, Baker et al.[1] examined Sprite with a view to establishing how closely its real usage matched predicted usage. More recently, Spasojevic and Satyanarayanan <ref> [24] </ref> reported on the use of wide-area AFS. In this paper, we report on data collected from Coda over a 6-month period. During this time, Coda was relied on daily by a community of almost 40 users.
Reference: [25] <author> STEERE, D. C., KISTLER, J. J., AND SATYANARAYANAN, M. </author> <title> Efficient User-Level File Cache Management on the Sun Vnode Interface. </title> <booktitle> In Summer Usenix Conference Proceedings (Anaheim, </booktitle> <address> CA, </address> <month> June </month> <year> 1990). </year>
Reference-contexts: In this section, we provide a short overview of Coda; further details can be found in earlier papers <ref> [9, 11, 17, 18, 19, 20, 25] </ref>. Clients view Coda as a single, location-transparent shared Unix file system. The Coda name space is mapped to individual file servers at the granularity of subtrees called volumes [21]. <p> This is partly because a lookup typically precedes other operations on an object; failure of the lookup suppresses the later operations. Compounding this is the fact that the data for Table 2 is collected after the MiniCache has filtered out many successful lookups <ref> [25 ] </ref>. Combined, these two factors account for the high observed failure rate of lookup. The message of Table 2 is that as connectivity degrades, the success rates of operations barely decline. In other words, the user does not experience a corresponding increase in failures.
Reference: [26] <author> STRITTER, E. P. </author> <title> File Migration. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1977. </year>
Reference-contexts: To the best of our knowledge, this is the first empirical study of a highly available distributed file system. Empirical studies of file systems have a long history, stretching back to the 1970s. Early studies of timesharing file systems such as those by Stritter <ref> [26] </ref>, Smith [23], Satyanarayanan [15, 16], This work has been supported by the Advanced Research Projects Agency (Avionics Lab, Wright Research and Development Center, Aeronautical Systems Division (AFSC), U.S. Air Force, Wright-Patterson AFB, Ohio, 45433-6543 under Contract F33615-90-C-1465, ARPA Order No. 7597 and Hanscom AFB, Contract F19628-93-C-0193, ARPA Order No.
References-found: 26

