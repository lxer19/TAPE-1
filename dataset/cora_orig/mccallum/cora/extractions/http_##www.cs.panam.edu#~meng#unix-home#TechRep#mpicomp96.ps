URL: http://www.cs.panam.edu/~meng/unix-home/TechRep/mpicomp96.ps
Refering-URL: http://www.cs.panam.edu/~meng/unix-home/TechRep/
Root-URL: http://www.cs.panam.edu
Email: worleyph@ornl.gov  
Title: MPI Performance Evaluation and Characterization using a Compact Application Benchmark Code  
Author: Patrick H. Worley 
Address: P.O. Box 2008 Oak Ridge, TN 37831-6367  
Affiliation: Oak Ridge National Laboratory  
Date: 1996 1  
Note: IN PROCEEDINGS OF SECOND MPI DEVELOPERS CONFERENCE, IEEE COMPUTER SOCIETY,  
Abstract: In this paper the parallel benchmark code PSTSWM is used to evaluate the performance of the vendor-supplied implementations of the MPI message-passing standard on the Intel Paragon, IBM SP2, and Cray Research T3D. This study is meant to complement the performance evaluation of individual MPI commands by providing information on the practical significance of MPI performance on the execution of a communication-intensive application code. In particular, three performance questions are addressed: how important is the communication protocol in determining performance when using MPI, how does MPI performance compare with that of the native communication library, and how efficient are the collective communication routines. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Barnett, R. van de Geijn, S. Gupta, D. G. Payne, L. Shuler, and J. Watts. </author> <title> Interprocessor collective communication library (InterCom). </title> <booktitle> In Proc. Scalable High Performance Computing Conf., </booktitle> <pages> pages 357-364. </pages> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1994. </year>
Reference-contexts: While we consider our codes to contain good implementations of these collective functions, we would hope that the vendor-supplied collective routines could be optimized far more than our generic Fortran routines. The results on the Paragon may simply indicate that the underlying iCC library <ref> [1] </ref> needs to be retuned for the ORNL Paragon. 5. Acknowledgements This research was supported by the U.S. Department of Energy under Contract DE-AC05-96OR22464 with Lock-heed Martin Energy Research Inc. We thank NASA Ames for access to their SP2 system, and Cray Research for access to a T3D system.
Reference: [2] <author> J. D. Dongarra and T. H. Dunigan. </author> <title> Message-passing performance of various computers. </title> <type> Technical Report ORNL/TM-13006, </type> <institution> Oak Ridge National Laboratory, Oak Ridge, TN, </institution> <month> February </month> <year> 1996. </year>
Reference-contexts: A typical approach to evaluating interprocessor communication and communication libraries is to measure the performance of individual commands in isolation or in small kernels representing common communication functions <ref> [2] </ref>, [6].
Reference: [3] <author> I. T. Foster, B. Toonen, and P. H. Worley. </author> <title> Performance of parallel computers for spectral atmospheric models. </title> <type> Technical Report ORNL/TM-12986, </type> <institution> Oak Ridge National Laboratory, Oak Ridge, TN, </institution> <month> April </month> <year> 1995. </year>
Reference-contexts: PSTSWM is a parallel algorithm testbed and benchmark code that solves the nonlinear shallow water equations on a rotating sphere using the spectral transform method. PSTSWM was developed to evaluate strategies for parallelizing spectral global atmospheric circulation models <ref> [3] </ref>, [4], and has imbedded a large number of parallel algorithm options. Among these options are numerous choices for the communication protocols used to implement the different parallel algorithms and numerous choices of message-passing layer, including MPI. <p> Neither of these are necessarily optimal for the underlying problem or for the given platforms, although they are reasonable, nor are they necessarily appropriate for comparing performance between platforms. (In other studies we have investigated choosing appropriate algorithms for each platform and problem size to allow for fair intermachine benchmarking <ref> [3] </ref>.) Instead, these parallel algorithm were chosen because they have significantly different message-passing characteristics and exercise different aspects of message-passing performance, and because the second parallel algorithm can be implemented using MPI collective communication routines.
Reference: [4] <author> I. T. Foster and P. H. Worley. </author> <title> Parallel algorithms for the spectral transform method. </title> <type> Technical Report ORNL/TM-12507, </type> <institution> Oak Ridge National Laboratory, Oak Ridge, TN, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: PSTSWM is a parallel algorithm testbed and benchmark code that solves the nonlinear shallow water equations on a rotating sphere using the spectral transform method. PSTSWM was developed to evaluate strategies for parallelizing spectral global atmospheric circulation models [3], <ref> [4] </ref>, and has imbedded a large number of parallel algorithm options. Among these options are numerous choices for the communication protocols used to implement the different parallel algorithms and numerous choices of message-passing layer, including MPI.
Reference: [5] <author> W. Gropp, E. Lusk, N. Doss, and T. Skjellum. </author> <title> A high-performance, portable implementation of the MPI message-passing interface standard. </title> <type> Technical Report ANL/MCS-P567-0296, </type> <institution> Argonne National Laboratory, </institution> <month> February </month> <year> 1996. </year>
Reference-contexts: Overall, MPI performance was reasonable compared to the alternative proprietary message-passing layers, and performance should continue to improve over time. MPI on the T3D should probably be implemented on top of SHMEM PUT for performance reasons. MPICH <ref> [5] </ref> is implemented in this fashion, but it does not work currently for this code on the T3D (due to a known problem in the implementation of MPICH) so it is unclear how difficult a SHMEM PUT implementation will be.
Reference: [6] <author> R. Hockney and M. B. </author> <title> (Eds.). Public international benchmarks for parallel computers, parkbench committee report-1. </title> <journal> Scientific Programming, </journal> <volume> 3(2) </volume> <pages> 101-146, </pages> <year> 1994. </year>
Reference-contexts: A typical approach to evaluating interprocessor communication and communication libraries is to measure the performance of individual commands in isolation or in small kernels representing common communication functions [2], <ref> [6] </ref>. While these types of experiments are a necessary step in an evaluation, the communication protocols and the controlled measurement environment used in the experiments may not be typical of how the commands are used in practice, and it can be difficult for an application developer to interpret the results. <p> The study is qualitative in that the results reflect the peculiarities of the application code, but the overall conclusions as to MPI performance should be more generally applicable. In this study we use the ParkBench <ref> [6] </ref> compact application code PSTSWM [10], [11]. PSTSWM is a parallel algorithm testbed and benchmark code that solves the nonlinear shallow water equations on a rotating sphere using the spectral transform method.
Reference: [7] <author> MPI Committee. </author> <title> MPI: a message-passing interface standard. </title> <journal> Internat. J. Supercomputer Applications, </journal> 8(3/4):165-416, Fall/Winter 1994. 
Reference-contexts: 1. Introduction The MPI message-passing interface standard <ref> [7] </ref> holds great promise in providing both portability and, through a very rich model for interprocess communication, performance efficiency for application, library, and compiler developers.
Reference: [8] <author> R. A. van de Geijn. </author> <title> Efficient global combine operations. </title> <editor> In Q. F. Stout and M. Wolfe, editors, </editor> <booktitle> The Sixth Distributed Memory Computing Conference Proceedings, </booktitle> <pages> pages 291-294. </pages> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1991. </year>
Reference-contexts: Each transpose is functionally equivalent to MPI ALLTOALLV. The distributed LT used in algorithm TN completes all local computation before invoking a parallel vector sum routine. The vector sum is implemented using a variant of the algorithm proposed in <ref> [8] </ref>. Communication for the vector sum consists of log 2 PY swaps of decreasing size, each half the size of the previous swap, done twice for the forward transform but with no communication during the inverse. The vector sum is equivalent to MPI ALLREDUCE using the predefined operator MPI SUM.
Reference: [9] <author> D. L. Williamson, J. B. Drake, J. J. Hack, R. Jakob, and P. N. Swarztrauber. </author> <title> A standard test set for numerical approximations to the shallow water equations on the sphere. </title> <journal> J. Computational Physics, </journal> <volume> 102 </volume> <pages> 211-224, </pages> <year> 1992. </year>
Reference-contexts: The vector sum is equivalent to MPI ALLREDUCE using the predefined operator MPI SUM. With the exception of the tuning runs mentioned in the next section, all experiments involved five day simulations of the standard benchmarking problem specified in <ref> [9] </ref>, representing the calculation of solid body rotation steady state flow, using 64 bit precision. Two different problem sizes were considered: T42, using a physical grid of size 64 fi 128 fi 16, and T85, using a physical grid of size 128 fi 256 fi 16.
Reference: [10] <author> P. H. Worley and I. T. Foster. </author> <title> Parallel Spectral Transform Shallow Water Model: a runtime-tunable parallel benchmark code. </title> <editor> In J. J. Dongarra and D. W. Walker, editors, </editor> <booktitle> Proc. Scalable High Performance Computing Conf., </booktitle> <pages> pages 207-214. </pages> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1994. </year>
Reference-contexts: The study is qualitative in that the results reflect the peculiarities of the application code, but the overall conclusions as to MPI performance should be more generally applicable. In this study we use the ParkBench [6] compact application code PSTSWM <ref> [10] </ref>, [11]. PSTSWM is a parallel algorithm testbed and benchmark code that solves the nonlinear shallow water equations on a rotating sphere using the spectral transform method.
Reference: [11] <author> P. H. Worley and B. Toonen. </author> <title> A users' guide to PSTSWM. </title> <type> Technical Report ORNL/TM-12779, </type> <institution> Oak Ridge National Laboratory, Oak Ridge, TN, </institution> <month> July </month> <year> 1995. </year> <month> 8 </month>
Reference-contexts: The study is qualitative in that the results reflect the peculiarities of the application code, but the overall conclusions as to MPI performance should be more generally applicable. In this study we use the ParkBench [6] compact application code PSTSWM [10], <ref> [11] </ref>. PSTSWM is a parallel algorithm testbed and benchmark code that solves the nonlinear shallow water equations on a rotating sphere using the spectral transform method. <p> Moreover, the different stages of the hand-implemented SWAP and SENDRECV functions can be separated, allowing, for example, receive-ahead algorithms in which nonblocking receive requests for multiple SWAPs are posted together, or overlap algorithms in which computation is interleaved with the send and receive requests <ref> [11] </ref>. In order to evaluate the performance sensitivity of MPI to these types of optimizations, we first conducted thousands of short tuning experiments to identify a subset of good communication protocols.
References-found: 11

