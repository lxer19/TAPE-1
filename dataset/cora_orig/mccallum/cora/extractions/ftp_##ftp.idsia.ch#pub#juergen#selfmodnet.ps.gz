URL: ftp://ftp.idsia.ch/pub/juergen/selfmodnet.ps.gz
Refering-URL: http://www.idsia.ch/~juergen/topics.html
Root-URL: 
Title: A `SELF-REFERENTIAL' WEIGHT MATRIX  
Author: J. Schmidhuber 
Address: Arcisstr. 21, 8000 Munchen 40, Germany  
Affiliation: Institut fur Informatik Technische Universitat Munchen  
Abstract: Weight modifications in traditional neural nets are computed by hard-wired algorithms. Without exception, all previous weight change algorithms have many specific limitations. Is it (in principle) possible to overcome limitations of hard-wired algorithms by allowing neural nets to run and improve their own weight change algorithms? This paper constructively demonstrates that the answer (in principle) is `yes'. I derive an initial gradient-based sequence learning algorithm for a `self-referential' recurrent network that can `speak' about its own weight matrix in terms of activations. It uses some of its input and output units for observing its own errors and for explicitly analyzing and modifying its own weight matrix, including those parts of the weight matrix responsible for analyzing and modifying the weight matrix. The result is the first `introspective' neural net with explicit potential control over all of its own adaptive parameters. A disadvantage of the algorithm is its high computational complexity per time step which is independent of the sequence length and equals O(n conn logn conn ), where n conn is the number of connections. Another disadvantage is the high number of local minima of the unusually complex error surface. The purpose of this paper, however, is not to come up with the most efficient `introspective' or `self-referential' weight change algorithm, but to show that such algorithms are possible at all. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. Moller and S. Thrun. </author> <title> Task modularization by network modulation. </title> <editor> In J. Rault, editor, </editor> <booktitle> Proceedings of Neuro-Nimes '90, </booktitle> <pages> pages 419-432, </pages> <month> November </month> <year> 1990. </year>
Reference-contexts: The computational complexity per time step (of sequences with arbitrary length) is O (n 2 conn logn conn ). 1 It should be noted that in quite different contexts, previous papers have shown how one net may learn to perform appropriate lasting weight changes for a second net [4] <ref> [1] </ref>. However, these previous approaches could not be called `self-referential' | they all involve at least some weights that can not be manipulated other than by conventional gradient descent. 4.
Reference: [2] <author> A. J. Robinson and F. Fallside. </author> <title> The utility driven dynamic error propagation network. </title> <type> Technical Report CUED/F-INFENG/TR.1, </type> <institution> Cambridge University Engineering Department, </institution> <year> 1987. </year>
Reference-contexts: Structure of the paper. Section 2 starts with a general finite, `self-referential' architecture involving a sequence-processing recurrent neural-net (see e.g. Robinson and Fallside <ref> [2] </ref>, Williams and Zipser [8], and Schmidhuber [3]) that can potentially implement any computable function that maps input sequences to output sequences | the only limitations being unavoidable time and storage constraints imposed by the architecture's finiteness. <p> INITIAL LEARNING ALGORITHM The following algorithm 1 for minimizing E total is partly inspired by (but more complex than) conventional recurrent network algorithms (e.g. Robinson and Fallside <ref> [2] </ref>). Derivation of the algorithm.
Reference: [3] <author> J. H. Schmidhuber. </author> <title> A fixed size storage O(n 3 ) time complexity learning algorithm for fully recurrent continually running networks. </title> <journal> Neural Computation, </journal> <volume> 4(2) </volume> <pages> 243-248, </pages> <year> 1992. </year>
Reference-contexts: Structure of the paper. Section 2 starts with a general finite, `self-referential' architecture involving a sequence-processing recurrent neural-net (see e.g. Robinson and Fallside [2], Williams and Zipser [8], and Schmidhuber <ref> [3] </ref>) that can potentially implement any computable function that maps input sequences to output sequences | the only limitations being unavoidable time and storage constraints imposed by the architecture's finiteness. These constraints can be extended by simply adding storage and/or allowing for more processing time.
Reference: [4] <author> J. H. Schmidhuber. </author> <title> Learning to control fast-weight memories: An alternative to recurrent nets. </title> <journal> Neural Computation, </journal> <volume> 4(1) </volume> <pages> 131-139, </pages> <year> 1992. </year>
Reference-contexts: The computational complexity per time step (of sequences with arbitrary length) is O (n 2 conn logn conn ). 1 It should be noted that in quite different contexts, previous papers have shown how one net may learn to perform appropriate lasting weight changes for a second net <ref> [4] </ref> [1]. However, these previous approaches could not be called `self-referential' | they all involve at least some weights that can not be manipulated other than by conventional gradient descent. 4.
Reference: [5] <author> J. H. Schmidhuber. </author> <title> An introspective network that can learn to run its own weight change algorithm. </title> <booktitle> In Proc. of the Third International Conference on Artificial Neural Networks, </booktitle> <address> Brighton. </address> <publisher> IEE, </publisher> <year> 1993. </year> <note> Accepted for publication. </note>
Reference: [6] <author> J. H. Schmidhuber. </author> <title> A neural network that embeds its own meta-levels. </title> <booktitle> In Proc. of the International Conference on Neural Networks '93, </booktitle> <address> San Francisco. </address> <publisher> IEEE, </publisher> <year> 1993. </year> <note> Accepted for publication. </note>
Reference: [7] <author> R. J. Williams. </author> <title> Complexity of exact gradient computation algorithms for recurrent neural networks. </title> <type> Technical Report Technical Report NU-CCS-89-27, </type> <institution> Boston: Northeastern University, College of Computer Science, </institution> <year> 1989. </year>
Reference-contexts: Thus we obtain an exact gradient-based algorithm for minimizing E total under the `self-referential' dynamics given by (1)-(4). To reduce writing effort, I introduce some short-hand notation partly inspired by Williams <ref> [7] </ref>.
Reference: [8] <author> R. J. Williams and D. Zipser. </author> <title> A learning algorithm for continually running fully recurrent networks. </title> <journal> Neural Computation, </journal> <volume> 1(2) </volume> <pages> 270-280, </pages> <year> 1989. </year>
Reference-contexts: Structure of the paper. Section 2 starts with a general finite, `self-referential' architecture involving a sequence-processing recurrent neural-net (see e.g. Robinson and Fallside [2], Williams and Zipser <ref> [8] </ref>, and Schmidhuber [3]) that can potentially implement any computable function that maps input sequences to output sequences | the only limitations being unavoidable time and storage constraints imposed by the architecture's finiteness. These constraints can be extended by simply adding storage and/or allowing for more processing time.
References-found: 8

