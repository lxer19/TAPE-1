URL: ftp://ftp.cs.wisc.edu/tech-reports/reports/93/tr1175.ps.Z
Refering-URL: 
Root-URL: 
Title: PARALLEL VARIABLE DISTRIBUTION  
Author: M. C. FERRIS AND O. L. MANGASARIAN 
Keyword: Key words. parallel optimization, nonlinear programming  
Note: AMS subject classifications. 90C30,49D27  
Abstract: We present an approach for solving optimization problems in which the variables are distributed among p processors. Each processor has primary responsibility for updating its own block of variables in parallel while allowing the remaining variables to change in a restricted fashion (e. g. along a steepest descent, quasi-Newton, or any arbitrary direction). This "forget-me-not" approach is a distinctive feature of our algorithm which has not been analyzed before. The paral-lelization step is followed by a fast synchronization step wherein the affine hull of the points computed by the parallel processors and the current point is searched for an optimal point. Convergence to a stationary point under continuous differentiability is established for the unconstrained case, as well as a linear convergence rate under the additional assumption of a Lipschitzian gradient and strong convexity. For problems constrained to lie in the Cartesian product of closed convex sets, convergence is established to a point satisfying a necessary optimality condition under Lipschitz continuous differentiability of the objective function. For problems with more general constraints, convergence is established under stronger conditions. Encouraging computational results on the Thinking Machines CM-5 Multiprocessor on a subset of the publicly available CUTE set of nonlinear programming test problems are given. 1. Introduction. We present an approach for solving constrained optimization 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Armijo, </author> <title> Minimization of functions having Lipschitz-continuous first partial derivatives, </title> <journal> Pacific Journal on Mathematics, </journal> <volume> 16 (1966), </volume> <pages> pp. 1-3. </pages>
Reference-contexts: Lemma 3.2. If x is a stationary point for the optimality function given in (3.2), then x satisfies the minimum principle (3.3). We show now that using an Armijo stepsize rule <ref> [1] </ref> along a bounded Frank-Wolfe algorithm direction [9] produces a function decrease that dominates the minimum principle optimality function (3.2). This relationship will be needed in establishing the convergence of our PVD algorithm.
Reference: [2] <author> K. P. Bennett and O. L. Mangasarian, </author> <title> Serial and parallel multicategory separation, </title> <note> to appear SIAM Journal on Optimization, 4 (1994). </note>
Reference-contexts: These strategies include a variety of different choices for d i in the parallel subproblems. In addition, asynchronous implementation might overcome some of the low efficiencies obtained by our synchronous algorithm. Computational results for multicategory discrimination problems using a closely related algorithm to PVD0 can also be found in <ref> [2] </ref>. 6. Conclusion. Blocks of variables of optimization problems were distributed among parallel processors with each processor taking primary responsibility for updating its assigned block while not forgetting about the other variables by allowing them to vary in a restricted but plausible fashion.
Reference: [3] <author> D. P. Bertsekas, </author> <title> Constrained Optimization and Lagrange Multiplier Methods, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1982. </year>
Reference-contexts: Therefore, it seems that the only sensible way to distribute variables for problems with inseparable constraints is to convert them to unconstrained problems or to problems with separable constraints. Obvious methods for doing so are exterior penalty [8] and augmented Lagrangian methods <ref> [17, 3] </ref> for handling inseparable constraints while leaving separable constraints as explicit ones. However, a disadvantage of exterior penalty is the unboundedness of the penalty parameter, while the augmented Lagrangian formulation essentially changes the minimization problem into a saddle point problem.
Reference: [4] <author> D. P. Bertsekas and J. N. Tsitsiklis, </author> <title> Parallel and Distributed Computation, </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1989. </year>
Reference-contexts: We note immediately that a fundamental difference between our method and that of block Jacobi for solving nonlinear equations <ref> [4] </ref> and coordinate descent [19] is the presence of the "forget-me-not" term x i l + D i l l in problem (1.2). <p> Hence ff i g is bounded below and it converges. It must converge to f because for a convergent sequence all accumulations points are identical to the limit of that sequence. Lemma A.2. Quadratic Bound Lemma: [15, p. 144], <ref> [4, p. 639] </ref> Let f 2 LC 1 K (IR n ) then f (y)f (x)rf (x)(yx) jf (y) f (x) rf (x)(y x)j 2 2 Theorem A.3.
Reference: [5] <author> I. Bongartz, A. R. Conn, N. Gould, and P. Toint, CUTE: </author> <title> Constrained and unconstrained testing environment, </title> <institution> Publications du Department de Mathematique Report 93/10, Facultes Universitaires De Namur, </institution> <year> 1993. </year>
Reference-contexts: This will enable the unconstrained code used to report results in this section to be generalized for the algorithm given in Section 3. The test problems used below are a subset of the problems from the constrained and unconstrained testing environment (CUTE) <ref> [5] </ref>. Both the problems and tools for linking algorithms with the problems are available via anonymous ftp. The problems are written in SIF (standard input format), and include many practical and large-scale examples. We now give 8 tables of results.
Reference: [6] <author> D. Culler, </author> <title> The Split-C Programming Language, </title> <institution> Computer Science Department, University of California, Berkeley. </institution>
Reference-contexts: Preliminary computational results for the parallel variable distribution algorithm are given in the next section. 5. Computational Results. In this section we report on some preliminary computational results with the PVD algorithm for unconstrained optimization. Our implementation is written in Split-C <ref> [6] </ref>, a parallel extension of the C programming language primarily intended for distributed memory multiprocessors. Split-C is designed around two objectives.
Reference: [7] <author> M. C. Ferris and O. L. Mangasarian, </author> <title> Minimum principle sufficiency, </title> <journal> Mathematical Programming, </journal> <volume> 57 (1992), </volume> <pages> pp. 1-14. </pages>
Reference-contexts: If X is a closed 6 M.C. FERRIS AND O.L. MANGASARIAN convex set in IR n , then the following function determined from the minimum principle <ref> [12, 7] </ref> serves as the minimum principle optimality function: (x) := min frf (x)d j x + d 2 X; kdk 1 ff g ; for some ff &gt; 0:(3.2) This is closely related to the gap function for convex programs [11].
Reference: [8] <author> A. V. Fiacco and G. P. McCormick, </author> <title> Nonlinear Programming: Sequential Unconstrained Minimization Techniques, </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1968. </year>
Reference-contexts: Therefore, it seems that the only sensible way to distribute variables for problems with inseparable constraints is to convert them to unconstrained problems or to problems with separable constraints. Obvious methods for doing so are exterior penalty <ref> [8] </ref> and augmented Lagrangian methods [17, 3] for handling inseparable constraints while leaving separable constraints as explicit ones. However, a disadvantage of exterior penalty is the unboundedness of the penalty parameter, while the augmented Lagrangian formulation essentially changes the minimization problem into a saddle point problem.
Reference: [9] <author> M. Frank and P. Wolfe, </author> <title> An algorithm for quadratic programming, </title> <journal> Naval Research Logistics Quarterly, </journal> <volume> 3 (1956), </volume> <pages> pp. 95-110. </pages>
Reference-contexts: Lemma 3.2. If x is a stationary point for the optimality function given in (3.2), then x satisfies the minimum principle (3.3). We show now that using an Armijo stepsize rule [1] along a bounded Frank-Wolfe algorithm direction <ref> [9] </ref> produces a function decrease that dominates the minimum principle optimality function (3.2). This relationship will be needed in establishing the convergence of our PVD algorithm. However, we emphasize that the PVD algorithm does not employ either the Frank-Wolfe algorithm or the Armijo stepsize. Lemma 3.3.
Reference: [10] <author> S.-P. Han and O. L. Mangasarian, </author> <title> A dual differentiable exact penalty function, </title> <journal> Mathematical Programming, </journal> <volume> 25 (1983), </volume> <pages> pp. 293-301. </pages>
Reference-contexts: However, a disadvantage of exterior penalty is the unboundedness of the penalty parameter, while the augmented Lagrangian formulation essentially changes the minimization problem into a saddle point problem. An approach that avoids both of these difficulties is the dual differ entiable exact penalty function <ref> [10] </ref> formulation max (x; u; fl) := L (x; u) 2 2 where L (x; u) is the standard Lagrangian for (4.1) L (x; u) := f (x) + u T g (x):(4.4) Various theorems [10] relate (4.3) to (4.1), but the key point is that the penalty parameter fl remains <p> approach that avoids both of these difficulties is the dual differ entiable exact penalty function <ref> [10] </ref> formulation max (x; u; fl) := L (x; u) 2 2 where L (x; u) is the standard Lagrangian for (4.1) L (x; u) := f (x) + u T g (x):(4.4) Various theorems [10] relate (4.3) to (4.1), but the key point is that the penalty parameter fl remains finite and the objective function is differentiable.
Reference: [11] <author> D. W. Hearn, </author> <title> The gap function of a convex program, </title> <journal> Operations Research Letters, </journal> <volume> 1 (1982), </volume> <pages> pp. 67-71. </pages> <note> PARALLEL VARIABLE DISTIBUTION 15 </note>
Reference-contexts: then the following function determined from the minimum principle [12, 7] serves as the minimum principle optimality function: (x) := min frf (x)d j x + d 2 X; kdk 1 ff g ; for some ff &gt; 0:(3.2) This is closely related to the gap function for convex programs <ref> [11] </ref>. A simple argument shows that is lower semicontinuous on X when f 2 C 1 (IR n ).
Reference: [12] <author> O. L. Mangasarian, </author> <title> Nonlinear Programming, </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1969. </year> <title> [13] , Parallel gradient distribution in unconstrained optimization, </title> <type> Tech. Report 1145, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, Wisconsin 53706, </institution> <month> Apr. </month> <year> 1993. </year>
Reference-contexts: If X is a closed 6 M.C. FERRIS AND O.L. MANGASARIAN convex set in IR n , then the following function determined from the minimum principle <ref> [12, 7] </ref> serves as the minimum principle optimality function: (x) := min frf (x)d j x + d 2 X; kdk 1 ff g ; for some ff &gt; 0:(3.2) This is closely related to the gap function for convex programs [11].
Reference: [14] <author> B. A. Murtagh and M. A. Saunders, </author> <title> MINOS 5.0 user's guide, </title> <type> Technical Report SOL 83.20, </type> <institution> Stanford University, </institution> <year> 1983. </year>
Reference-contexts: Much of this can also be carried out using CMMD [18], the message passing library of the CM-5. However, Split-C enables the code to be written in a more readily portable manner. The current implementation uses MINOS 5.4, a newer version of <ref> [14] </ref>, to solve both the parallel subproblems and the synchronization problem. MINOS uses a Quasi-Newton approach for each of these problems. MINOS was chosen as the optimization tool since it is very reliable, efficient and can be called easily as a subroutine.
Reference: [15] <author> J. M. Ortega, </author> <title> Numerical Analysis, a Second Course, </title> <publisher> Academic Press, </publisher> <year> 1972. </year>
Reference-contexts: Hence ff i g is bounded below and it converges. It must converge to f because for a convergent sequence all accumulations points are identical to the limit of that sequence. Lemma A.2. Quadratic Bound Lemma: <ref> [15, p. 144] </ref>, [4, p. 639] Let f 2 LC 1 K (IR n ) then f (y)f (x)rf (x)(yx) jf (y) f (x) rf (x)(y x)j 2 2 Theorem A.3.
Reference: [16] <author> J. M. Ortega and W. C. Rheinboldt, </author> <title> Iterative Solution of Nonlinear Equations in Several Variables, </title> <publisher> Academic Press, </publisher> <year> 1970. </year>
Reference-contexts: IR n ! IR, is strongly convex on IR n with constant k &gt; 0 if f (y) f (x) rf (x)(y x) 2 2 or equivalently (rf (y) rf (x))(y x) k ky xk ; 8x; y 2 IR n : We adapt the definition of a forcing function <ref> [16, p. 479] </ref> for our purposes. Definition 1.1.
Reference: [17] <author> R. T. Rockafellar, </author> <title> Augmented Lagrange multiplier functions and duality in nonconvex programming, </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 12 (1974), </volume> <pages> pp. 268-285. </pages>
Reference-contexts: Therefore, it seems that the only sensible way to distribute variables for problems with inseparable constraints is to convert them to unconstrained problems or to problems with separable constraints. Obvious methods for doing so are exterior penalty [8] and augmented Lagrangian methods <ref> [17, 3] </ref> for handling inseparable constraints while leaving separable constraints as explicit ones. However, a disadvantage of exterior penalty is the unboundedness of the penalty parameter, while the augmented Lagrangian formulation essentially changes the minimization problem into a saddle point problem.
Reference: [18] <institution> Thinking Machines Corporation, </institution> <note> CMMD Reference Manual, Version 3.0, </note> <institution> Cambridge, </institution> <address> MA, </address> <year> 1993. </year>
Reference-contexts: Split-C facilitates easy coding of the synchronization problem which obtains its data via message passing, while allowing the data for the subproblems to be physically distributed across the processors. Much of this can also be carried out using CMMD <ref> [18] </ref>, the message passing library of the CM-5. However, Split-C enables the code to be written in a more readily portable manner. The current implementation uses MINOS 5.4, a newer version of [14], to solve both the parallel subproblems and the synchronization problem.

References-found: 17

