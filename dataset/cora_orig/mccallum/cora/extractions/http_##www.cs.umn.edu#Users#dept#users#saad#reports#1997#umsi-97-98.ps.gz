URL: http://www.cs.umn.edu/Users/dept/users/saad/reports/1997/umsi-97-98.ps.gz
Refering-URL: http://www.cs.umn.edu/Users/dept/users/saad/reports/1997/
Root-URL: http://www.cs.umn.edu
Title: Parallel Solution of General Sparse Linear Systems  
Author: Sergey Kuznetsov Gen-Ching Lo Yousef Saad 
Abstract: This paper discusses a few algorithms and their implementations for solving distributed general sparse linear systems. The preconditioners used are all variations of techniques originating from domain decomposition ideas. In particular we compare a number of variants of Schwarz procedures with Schur complement techniques. Numerical experiments on a few parallel platforms are reported. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. E. Bjtrstad. </author> <title> Multiplicative and Additive Schwarz Methods: Convergence in the 2 domain case. </title> <editor> In Tony Chan, Roland Glowinski, Jacques Periaux, and O. Widlund, editors, </editor> <title> Domain Decomposition Methods, </title> <address> Philadelphia, PA, </address> <year> 1989. </year> <note> SIAM. </note>
Reference-contexts: Of particular interest in this context are the overlapping additive Schwarz methods. In the domain decomposition literature <ref> [1, 3, 37, 19] </ref> it is known that overlapping is a good strategy to reduce the number of steps.
Reference: [2] <author> P. E. Bjtrstad and O. B. Widlund. </author> <title> Iterative methods for the solution of elliptic problems on regions partitioned into substructures. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 23(6) </volume> <pages> 1093-1120, </pages> <year> 1986. </year>
Reference-contexts: The simplest form of the multiplicative Schwarz is the block Gauss-Seidel algorithm used in domain decomposition techniques <ref> [2, 19, 37, 5] </ref>. The global ordering can be based on an arbitrary labeling of the processors provided two neighboring domains have a different label. The most common global ordering is a multi-coloring of the domains, which maximizes parallelism [4, 30, 36, 37].
Reference: [3] <author> P. E. Bjtrstad and O. B. Widlund. </author> <title> To overlap or not to overlap: A note on a domain decomposition method for elliptic problems. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 10(5) </volume> <pages> 1053-1061, </pages> <year> 1989. </year>
Reference-contexts: Of particular interest in this context are the overlapping additive Schwarz methods. In the domain decomposition literature <ref> [1, 3, 37, 19] </ref> it is known that overlapping is a good strategy to reduce the number of steps.
Reference: [4] <author> X. C. Cai and Y. Saad. </author> <title> Overlapping domain decomposition algorithms for general sparse matrices. </title> <type> Technical Report 93-027, </type> <institution> Army High Performance Computing Research Center, Minneapolis, MN, </institution> <year> 1993. </year>
Reference-contexts: The global ordering can be based on an arbitrary labeling of the processors provided two neighboring domains have a different label. The most common global ordering is a multi-coloring of the domains, which maximizes parallelism <ref> [4, 30, 36, 37] </ref>. Thus, if the domains are colored, the multiplicative Schwarz as executed in each processor would be as follows. Algorithm 2.3. Multicolor Multiplicative Schwarz procedure 1. Do col = 1 ; : : : ; numcols 2. If (col:eq:mycol ) then 6 3.
Reference: [5] <author> T. F. Chan and T. P. Mathew. </author> <title> Domain decomposition algorithms. </title> <journal> Acta Numerica, </journal> <pages> pages 61-143, </pages> <year> 1994. </year>
Reference-contexts: The simplest form of the multiplicative Schwarz is the block Gauss-Seidel algorithm used in domain decomposition techniques <ref> [2, 19, 37, 5] </ref>. The global ordering can be based on an arbitrary labeling of the processors provided two neighboring domains have a different label. The most common global ordering is a multi-coloring of the domains, which maximizes parallelism [4, 30, 36, 37].
Reference: [6] <author> A. Chapman and Y. Saad, </author> <title> Deflated and augmented Krylov subspace techniques, </title> <type> Tech. Report UMSI 95/181, </type> <institution> Minnesota Supercomputer Institute, </institution> <year> 1995. </year> <month> 18 </month>
Reference-contexts: The matrix is of order 189,370 with 6,260,236 nonzero entries. The condition number of this matrix is very high (see [7]). The system with BARTH1S is solved with the deflated GMRES <ref> [6] </ref>. The local systems are solved using one step an ILU solve, where the LU factors are obtained from a block ILU (k) preconditioner (see [7]) where k denotes a level of fill. Table 2 gives the timing results, and the number of matrix-vector multiplications.
Reference: [7] <author> A. Chapman, Y. Saad, and L. </author> <title> Wigton High-order ILU preconditioners for CFD problems, </title> <type> Tech. Report UMSI 96/14, </type> <institution> Minnesota Supercomputer Institute, </institution> <year> 1996. </year>
Reference-contexts: The next matrix, referred to as BARTH1S, was supplied by T. Barth of NASA Ames <ref> [7] </ref>. It is for a 2D high Reynolds number airfoli problem, with a turbulence model and this matrix has a 5x5 block structure. Two rows in each block are for the momentum equations, and one row each is for mass balance, energy balance, and turbulence balance. <p> Two rows in each block are for the momentum equations, and one row each is for mass balance, energy balance, and turbulence balance. The matrix is of order 189,370 with 6,260,236 nonzero entries. The condition number of this matrix is very high (see <ref> [7] </ref>). The system with BARTH1S is solved with the deflated GMRES [6]. The local systems are solved using one step an ILU solve, where the LU factors are obtained from a block ILU (k) preconditioner (see [7]) where k denotes a level of fill. <p> The condition number of this matrix is very high (see <ref> [7] </ref>). The system with BARTH1S is solved with the deflated GMRES [6]. The local systems are solved using one step an ILU solve, where the LU factors are obtained from a block ILU (k) preconditioner (see [7]) where k denotes a level of fill. Table 2 gives the timing results, and the number of matrix-vector multiplications. The number in the first column indicates the number of processors involved.
Reference: [8] <institution> Documentation for Harwell library subroutines, NS02, EA 15, </institution> <month> March </month> <year> 1984. </year>
Reference: [9] <author> M. Dryja and O. B. Widlund. </author> <title> Towards a unified theory of domain decomposition algorithms for elliptic problems. </title> <editor> In Tony Chan, Roland Glowinski, Jacques Periaux, and Olof Widlund, editors, </editor> <title> Third International Symposium on Domain Decomposition Methods for Partial Differential Equations, held in Houston, </title> <address> Texas, March 20-22, 1989. </address> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1990. </year>
Reference: [10] <author> J. Dongarra, J. Bunch, C. Moler, and G. W. Stewart, </author> <title> LINPACK Users' Guide, </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1979. </year>
Reference: [11] <author> J. Dongarra, V. Eijkhout, and V. Kahlan, </author> <title> Reverse communication interface for linear algebra templates for iterative methods, </title> <type> Tech. report CS-95-291, </type> <institution> University of Tennessee, </institution> <year> 1995. </year>
Reference-contexts: The main Krylov accelerator used in the examples is the flexible variant of GMRES known as FGMRES. This is a right-preconditioned variant which allows the preconditioning to vary at each step. For further details on the algorithm, see [29]. Reverse communication (see e.g. [8],[27], <ref> [11] </ref>) allows to have exactly the same codes work on parallel and sequential platforms. 3.1 Experiments with block Jacobi preconditioning The execution time depends on the time spent in the local solver.
Reference: [12] <author> J. A. George and J. W. Liu, </author> <title> Computer solution of large sparse positive definite systems, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1981. </year>
Reference: [13] <author> T. Goehring and Y. Saad, </author> <title> Heuristic algorithms for automatic graph partitioning, </title> <type> Technical Report UMSI 94-29, </type> <institution> University of Minnesota Supercomputer Institute, Minneapolis, MN, </institution> <year> 1994. </year>
Reference: [14] <author> W. Gropp, L. C. McInnes, and B. Smith, </author> <title> Scalable libraries for solving systems of nonlinear equations and unconstrained minimization problems, </title> <booktitle> Proceeding of the Scalable Parallel Libraries Conference, </booktitle> <publisher> IEEE Computer Society Press, </publisher> <pages> pp. 60-67, </pages> <year> 1995. </year>
Reference-contexts: In either case, each processor will wind up holding a set of equations (rows of the linear system) and a vector of the variables associated with these rows. This natural way of distributing a sparse linear system is fairly general (see [18], <ref> [14] </ref>) and is closely related to the physical viewpoint. This paper addresses mainly the issue of defining preconditioners for distributed sparse linear systems. Such systems are regarded a distributed objects and methods are developed for solving the global system by using the distributed data structure.
Reference: [15] <author> W. Gropp, E. Lusk, and A. Skjellum, </author> <title> Using MPI: Portable Parallel Programming with the Message-Passing Interface, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference: [16] <author> B. Hendrickson and R. </author> <title> Leland An improved spectral graph partitioning algorithm for mapping parallel computations, </title> <type> Tech. Report SAND92-1460, </type> <institution> Sandia National Laboratories, </institution> <address> Albu-querque, New Mexico, </address> <year> 1992. </year>
Reference: [17] <author> B. Hendrickson and R. </author> <title> Leland The chaco user's guide, </title> <type> version 1. 0, Tech. Report SAND93-2339, </type> <institution> Sandia National Laboratories, </institution> <address> Albuquerque, New Mexico, </address> <year> 1993. </year>
Reference: [18] <author> S. A. Hutchinson, J. N. Shadid, and R. Tuminaro, </author> <title> Aztec user's guide, </title> <type> Tech. Report SAND95-1559, </type> <institution> Sandia National Laboratories, </institution> <address> Albuquerque, New Mexico, </address> <year> 1995. </year>
Reference-contexts: In either case, each processor will wind up holding a set of equations (rows of the linear system) and a vector of the variables associated with these rows. This natural way of distributing a sparse linear system is fairly general (see <ref> [18] </ref>, [14]) and is closely related to the physical viewpoint. This paper addresses mainly the issue of defining preconditioners for distributed sparse linear systems. Such systems are regarded a distributed objects and methods are developed for solving the global system by using the distributed data structure.
Reference: [19] <author> M. T. Jones and P. E. Plassmann BlockSolve v1. </author> <title> 1: Scalable library software for the parallel solution of sparse linear systems, </title> <type> Tech. Report ANL-92/46, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <month> December, </month> <year> 1992. </year>
Reference-contexts: Of particular interest in this context are the overlapping additive Schwarz methods. In the domain decomposition literature <ref> [1, 3, 37, 19] </ref> it is known that overlapping is a good strategy to reduce the number of steps. <p> The simplest form of the multiplicative Schwarz is the block Gauss-Seidel algorithm used in domain decomposition techniques <ref> [2, 19, 37, 5] </ref>. The global ordering can be based on an arbitrary labeling of the processors provided two neighboring domains have a different label. The most common global ordering is a multi-coloring of the domains, which maximizes parallelism [4, 30, 36, 37].
Reference: [20] <author> G. Karypis, </author> <title> Graph partitioning and its application to scientific computing, </title> <type> Ph. D. thesis, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN, </institution> <year> 1996. </year>
Reference: [21] <author> G. Karypis and V. Kumar, </author> <title> A fast and high quality multilevel scheme for partitioning irregular graphs, </title> <type> Tech. </type> <institution> Report TR95-037 , Department of Computer Science, University of Minnesota, Minneapolis, MN, </institution> <year> 1995. </year>
Reference: [22] <author> D. E. Keyes and W. D. Gropp, </author> <title> A comparison of domain decomposition techniques for elliptic partial differential equation and their parallel implementation, </title> <journal> SIAM J. Sci. Statist. Comput. </journal> , <volume> 8 (1987), </volume> <pages> pp. 856-869. </pages>
Reference-contexts: As is known, with a consistent choice of the initial guess, a block-Jacobi (or SOR) iteration with the reduced system is equivalent with a block Jacobi iteration on the global system, see, e.g., <ref> [22] </ref>, [32].
Reference: [23] <author> G. -C. Lo and Y. Saad, </author> <title> Iterative solution of general sparse linear systems on clusters of workstations, </title> <type> Tech. Report UMSI 96/117, </type> <institution> Minnesota Supercomputer Institute, University of Minnesota, Minneapolis, MN, </institution> <year> 1995. </year>
Reference: [24] <author> A. Pothen, H. D. Simon, and P. K. Liou, </author> <title> Partitioning sparse matrices with eigenvectors of graphs, </title> <journal> SIAM J. Matrix. Anal. Appl. </journal> , <volume> 11 (1990), </volume> <pages> pp. 430-452. </pages>
Reference: [25] <author> A. Quarteroni, Yu. Kuznetsov, J. Periaux, and O. Widlund, </author> <title> editors. Domain decomposition methods in science and engineering: </title> <booktitle> The Sixth Iternational Conference on Domain Decomposition, </booktitle> <address> Como, Italy, June 15-19, </address> <booktitle> 1992, </booktitle> <volume> Vol. 15. </volume> <publisher> AMS, </publisher> <address> Providence, RI, </address> <year> 1994. </year>
Reference: [26] <author> Y. Saad, </author> <title> Preconditioned Krylov subspace methods for CFD applications, </title> <type> Tech. Report UMSI-94-171, </type> <institution> Minnesota Supercomputer Institute, </institution> <address> Minneapolis, MN 55415, </address> <month> August </month> <year> 1994. </year>
Reference: [27] <author> Y. Saad, </author> <title> Data structures and algorithms for domain decomposition and distributed sparse matrix computations, </title> <type> Tech. report 95-014, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN, </institution> <year> 1995. </year> <title> [28] , Y. Saad, Krylov Subspace Methods in Distributed Computing Environments, </title> <booktitle> In State of the art in CFD, M. Hafez, editor, </booktitle> <pages> pp. 741-755, </pages> <year> (1995). </year> <month> 19 </month>
Reference: [29] <author> Y. Saad. </author> <title> A flexible inner-outer preconditioned GMRES algorithm. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 14 </volume> <pages> 461-469, </pages> <year> 1993. </year>
Reference-contexts: D 6 D 7 Fig. 3. An example of splitting eight domains on the plane into two groups 2 Distributed Krylov Subspace Solvers The primary accelerator which we use is a flexible variant of GMRES called FGMRES <ref> [29] </ref>. This is a right-preconditioned variant of GMRES which allows variations in the preconditioner at each step. Details on the implementations of parallel Krylov accelerators are given in [33, 35, 28]. <p> The main Krylov accelerator used in the examples is the flexible variant of GMRES known as FGMRES. This is a right-preconditioned variant which allows the preconditioning to vary at each step. For further details on the algorithm, see <ref> [29] </ref>. Reverse communication (see e.g. [8],[27], [11]) allows to have exactly the same codes work on parallel and sequential platforms. 3.1 Experiments with block Jacobi preconditioning The execution time depends on the time spent in the local solver.
Reference: [30] <author> Y. Saad. </author> <title> Highly parallel preconditioners for general sparse matrices. </title> <editor> In G. Golub, M. Luskin, and A. Greenbaum, editors, </editor> <title> Recent Advances in Iterative Methods, </title> <journal> IMA Volumes in Mathematics and Its Applications, </journal> <volume> volume 60, </volume> <pages> pages 165-199, </pages> <address> New York, 1994. </address> <publisher> Springer Verlag. </publisher>
Reference-contexts: The global ordering can be based on an arbitrary labeling of the processors provided two neighboring domains have a different label. The most common global ordering is a multi-coloring of the domains, which maximizes parallelism <ref> [4, 30, 36, 37] </ref>. Thus, if the domains are colored, the multiplicative Schwarz as executed in each processor would be as follows. Algorithm 2.3. Multicolor Multiplicative Schwarz procedure 1. Do col = 1 ; : : : ; numcols 2. If (col:eq:mycol ) then 6 3.
Reference: [31] <author> Y. Saad. </author> <title> ILUT a dual threshold incomplete ILU factorization. Numerical Linear Algebra with Applications, </title> <booktitle> 1 </booktitle> <pages> 387-402, </pages> <year> 1994. </year>
Reference-contexts: Solve A i ffi i = r i 4. Update solution x i = x i + ffi i To solve the systems which arise in line 3 of the above algorithm, a standard (sequential) ILUT preconditioner <ref> [31] </ref> combined with GMRES or one step of an ILU preconditioner is used. Of particular interest in this context are the overlapping additive Schwarz methods. In the domain decomposition literature [1, 3, 37, 19] it is known that overlapping is a good strategy to reduce the number of steps.
Reference: [32] <author> Y. Saad. </author> <title> Iterative Methods for Sparse Linear Systems. </title> <publisher> PWS publishing, </publisher> <address> New York, </address> <year> 1996. </year>
Reference-contexts: As is known, with a consistent choice of the initial guess, a block-Jacobi (or SOR) iteration with the reduced system is equivalent with a block Jacobi iteration on the global system, see, e.g., [22], <ref> [32] </ref>.
Reference: [33] <author> Y. Saad and A. Malevsky. PSPARSLIB: </author> <title> A portable library of distributed memory sparse iterative solvers. </title> <editor> In V. E. Malyshkin et al. , editor, </editor> <booktitle> Proceedings of Parallel Computing Technologies (PaCT-95), 3-rd international conference, </booktitle> <address> St. Petersburg, </address> <month> Sept. </month> <year> 1995, 1995. </year>
Reference-contexts: The complete description of the data structure associated with this boundary information is given in <ref> [33] </ref> along with additional implementation details. 4 . . . D 2 D 3 : : : S 1 : : : : : : S 2 : : : . . . D 6 D 7 Fig. 3. <p> This is a right-preconditioned variant of GMRES which allows variations in the preconditioner at each step. Details on the implementations of parallel Krylov accelerators are given in <ref> [33, 35, 28] </ref>. In particular, a reverse communication protocol is used to avoid passing data structures related to the coefficient matrix or the preconditioner. With this implementation, the FGMRES code itself contains no communication calls.
Reference: [34] <author> Y. Saad and M. H. Schultz. </author> <title> GMRES: a generalized minimal residual algorithm for solving nonsymmetric linear systems. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 7 </volume> <pages> 856-869, </pages> <year> 1986. </year>
Reference: [35] <author> Y. Saad and K. Wu. </author> <title> Design of an iterative solution module for a parallel sparse matrix library (P SPARSLIB), </title> <editor> In W. Shonauer, editor, </editor> <booktitle> Proceedings of the IMACS conference, </booktitle> <address> Georgia, </address> <year> 1994. </year>
Reference-contexts: This is a right-preconditioned variant of GMRES which allows variations in the preconditioner at each step. Details on the implementations of parallel Krylov accelerators are given in <ref> [33, 35, 28] </ref>. In particular, a reverse communication protocol is used to avoid passing data structures related to the coefficient matrix or the preconditioner. With this implementation, the FGMRES code itself contains no communication calls.
Reference: [36] <author> J. N. Shadid and R. S. Tuminaro. </author> <title> A comparison of preconditioned nonsymmetric krylov methods on a large-scale mimd machine. </title> <journal> SIAM J. Sci Comput. </journal> , <volume> 15(2) </volume> <pages> 440-449, </pages> <year> 1994. </year>
Reference-contexts: The global ordering can be based on an arbitrary labeling of the processors provided two neighboring domains have a different label. The most common global ordering is a multi-coloring of the domains, which maximizes parallelism <ref> [4, 30, 36, 37] </ref>. Thus, if the domains are colored, the multiplicative Schwarz as executed in each processor would be as follows. Algorithm 2.3. Multicolor Multiplicative Schwarz procedure 1. Do col = 1 ; : : : ; numcols 2. If (col:eq:mycol ) then 6 3.
Reference: [37] <author> B. Smith, P. Bjtrstad, W. </author> <title> Gropp,Domain decomposition: Parallel multilevel methods for elliptic partial differential equations, </title> <publisher> Cabridge University Press, </publisher> <address> New-York, NY, </address> <year> 1996. </year>
Reference-contexts: Of particular interest in this context are the overlapping additive Schwarz methods. In the domain decomposition literature <ref> [1, 3, 37, 19] </ref> it is known that overlapping is a good strategy to reduce the number of steps. <p> The simplest form of the multiplicative Schwarz is the block Gauss-Seidel algorithm used in domain decomposition techniques <ref> [2, 19, 37, 5] </ref>. The global ordering can be based on an arbitrary labeling of the processors provided two neighboring domains have a different label. The most common global ordering is a multi-coloring of the domains, which maximizes parallelism [4, 30, 36, 37]. <p> The global ordering can be based on an arbitrary labeling of the processors provided two neighboring domains have a different label. The most common global ordering is a multi-coloring of the domains, which maximizes parallelism <ref> [4, 30, 36, 37] </ref>. Thus, if the domains are colored, the multiplicative Schwarz as executed in each processor would be as follows. Algorithm 2.3. Multicolor Multiplicative Schwarz procedure 1. Do col = 1 ; : : : ; numcols 2. If (col:eq:mycol ) then 6 3. <p> We can compute the dense matrix S i explicitly or solve the above system by using a computation of the matrix-vector product S i x which can be carried out with three sparse matrix vector multiplies and one linear system solve. As is known (see <ref> [37] </ref>), because of the large computational expense of these accurate solves, the resulting decrease in iteration counts does not, in itself, make the Schur complement attractive. Therefore, we need to use efficient preconditioners for the Schur complement.
References-found: 36

