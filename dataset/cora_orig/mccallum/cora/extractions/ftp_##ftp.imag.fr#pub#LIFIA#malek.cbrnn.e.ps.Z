URL: ftp://ftp.imag.fr/pub/LIFIA/malek.cbrnn.e.ps.Z
Refering-URL: http://cosmos.imag.fr/RESEAUX/malek/Publications.html
Root-URL: 
Email: Email: (Maria.Malek, Abderrahim.Labbi)@imag.fr  
Title: Integration of Case-Based Reasoning and Neural Networks Approaches for Classification  
Author: Maria Malek, Abderrahim Labbi 
Date: May 10, 1995  
Address: Bat. lifia, 46 ave Felix Viallet, 38031 Grenoble, France  
Affiliation: TIMC-IMAG, LIFIA-IMAG  
Abstract: Several different approaches have been used to describe concepts for supervised learning tasks. In this paper we describe two approaches which are: prototype-based incremental neural networks and case-based reasoning approaches. We show then how we can improve a prototype-based neural network model by storing some specific instances in a CBR memory system. This leads us to propose a co-processing hybrid model for classification. 1 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Aamodt and E. </author> <title> Plaza. Case-based reasoning : Foundational issues,methodological variations, and system approaches. </title> <journal> AICOM, </journal> <volume> 7(1), </volume> <month> March </month> <year> 1994. </year>
Reference-contexts: Case-based reasoning is a cyclic and integrated process of solving a problem, learning from this experience, solving a new problem, etc. 3.1 The CBR Cycle A general CBR cycle may be described by the following four processes <ref> [1] </ref> [21] [11](see figure 3): 1. RETRIEVE the most similar case or cases; 2. REUSE the information and knowledge in that case to solve the problem; 3. REVISE the proposed solution; 4. RETAIN the parts of this experience likely to be useful for future problem solving. <p> If successful, learn from the success, otherwise repair the case solution using domain-specific or user knowledge <ref> [1] </ref>. The evaluation task takes the result from applying the solution in a real environment. Case repair involves detecting the errors of the current solution and retrieving or generating explanations for them.
Reference: [2] <author> D.W. Aha, D. Kibler, and M.K. Albert. </author> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <year> 1991. </year>
Reference-contexts: In this paper we show how we can improve a model of incremental neural network by storing some specific instances. We propose a co-processing hybrid model which integrates a prototype-based incremental neural network model with a case-based classification algorithm (the instance based learning algorithm described in <ref> [2] </ref>). The rest of the paper is organized as follows: in the next section we present incremental prototype based neural networks, we then present in section 3 a state of the art of CBR in general and we particularly present case-based classification approaches. <p> case may not be retained or the two cases may be merged by following taxonomic links in the semantic network. 3.3.2 Instance-Based Learning (IBL) This is a case-based learning framework and methodology that generates classification prediction using only specific instances without maintaining a set of abstractions derived from these instances <ref> [2] </ref>. The representation of the instance is usually simple (feature vectors). IBL algorithms are derived from the nearest 15 neighbour algorithms, but they are incremental and their goals include maximiz-ing classification accuracy on subsequently presented instances. <p> rainingSet do for each y 2 CD do Sim [y] Similarity (x; y) y max y 2 CD with maximal Sim [y] if class (x) = class (y max ) then classification correct else classification incorrect CD CD [ x Another variant of the IBL algorithm (IB2) is described in <ref> [2] </ref>. It is identical to IBL except that it saves only misclassed instances. Therefore, a great reduction in storage requirements is gained by saving only informative instances. <p> Learning from a failure is performed by differentiation of certain units in the network. We have to mention here that the IB2 version of the instance-based learning algorithm <ref> [2] </ref> is based on storing only misclassified instances. The intuition in this version is that the most of misclassified instances are near boundary instances. In our system we store also boundary instances but we keep always prototypical ones in the network.
Reference: [3] <author> E. Alpaydin. </author> <title> Gal : Networks that grow when they learn and shrink when they forget. </title> <type> Technical report, </type> <institution> International Computer Science Institute, </institution> <month> May </month> <year> 1991. </year> <month> 23 </month>
Reference-contexts: In the next we present a general survey of the incremental learning in neural networks, then we describe briefly competitive learning which is used generally in prototype-based networks. Finally, we describe three prototype-based incremental networks [20], <ref> [3] </ref> and [4]. 2.1 Incremental Learning in Neural Networks The idea of incremental learning implies starting from a simplest network and adding units and/or connections whenever necessary to decrease error. It implies also starting from a big network and decreasing a network size without degrading significantly its performance [3]: 2.1.1 Start <p> networks [20], <ref> [3] </ref> and [4]. 2.1 Incremental Learning in Neural Networks The idea of incremental learning implies starting from a simplest network and adding units and/or connections whenever necessary to decrease error. It implies also starting from a big network and decreasing a network size without degrading significantly its performance [3]: 2.1.1 Start big and remove When we start with a large network and if the problem requires a simpler network, we like to have the weights of all unnecessary connections and the outputs of all unnecessary units equal to zero. <p> A number of the 3 least important units or connections may then be deleted. For example Grow and Learn (GAL <ref> [3] </ref>) algorithm has a "sleep" mode during which the network is closed to the environment, units that are no longer necessary are removed. * Instead of approximating how much the error will change if the unit or the connection is eliminated, the learning algorithm is modified so that after training, the <p> In fact there are some analogies between these learning methods and the biological brain building <ref> [3] </ref>; the brain is built according to some genetic program with an abundant number of cells. Neurons are generated before birth and followed by a migration process where young neurons migrate from one part of the brain to another. Finally, they settle down, maturate, specialize, and form synapses. <p> In the case of prototypes committed to inputs near a class border, the initially large influence regions can result in many incorrect or confused responses until the magnitude of the prototype is appropriately scaled. 5 2.3.2 GAL Network GAL network <ref> [3] </ref> has three layers: The first layer consists of input units. The second layer is the exemplars (prototypes) and the third is the layer of class units. <p> Nestor Learning System (NLS). The difference between these two models is that in the NLS model, the reference vectors are not updated and no winner-take-all competition is employed to search for the most active prototype. This approach is also similar to the GAL network <ref> [3] </ref>. The difference is that in GAL networks there is no differentiation process, and a sleep phase is introduced to get rid of units which are useless. 8 2.4 Remarks The advantage of prototype-based incremental networks is in terms of minimizing memory and computational requirements. <p> Prototype-based incremental networks can learn a given mapping when the range is discretized and each discrete part is taken as a different class. Learning incrementally is a very fast method comparing to classical models like back-propagation <ref> [3] </ref>. In prototype-based networks, generalization is linked to the influence regions of the different prototype units. All pattern vectors which lie within the influence region of a single prototype unit are classified under the category of this unit.
Reference: [4] <author> A. Azcarraza and A. Giacometti. </author> <title> A prototype-based incremental network model for classification task. </title> <booktitle> In Neuro-Nimes, </booktitle> <year> 1991. </year>
Reference-contexts: In the next we present a general survey of the incremental learning in neural networks, then we describe briefly competitive learning which is used generally in prototype-based networks. Finally, we describe three prototype-based incremental networks [20], [3] and <ref> [4] </ref>. 2.1 Incremental Learning in Neural Networks The idea of incremental learning implies starting from a simplest network and adding units and/or connections whenever necessary to decrease error. <p> Successive awake and sleep phases allow the system to choose a good subset. 2.3.3 ARN2 In this section, we describe the incremental prototype-based neural network model proposed in <ref> [4] </ref>. Figure 1 shows the architecture of the basic model. This model operates either in supervised learning mode or in recognition mode. <p> In learn ing mode two conditions are checked each time an input pattern is presented <ref> [4] </ref>: 1. The input vector must be within the influence region of at least one ref erence vector. 2. The output category must be equal to the supplied input category. 7 Many variants of the learning and the recognition procedures have been proposed.
Reference: [5] <author> S.K. Bamberger and k. Goos. </author> <title> Integration of case-based reasoning and inductive learning methods. </title> <booktitle> In First European Workshop on CBR, </booktitle> <volume> number 1, </volume> <month> November </month> <year> 1993. </year>
Reference-contexts: The study of some inductive approaches like ID3/C4 [17] [18] or BUBE have shown that the costs of altering large case bases are very high and lead sometimes to a recompilation of the complete case base after each alteration <ref> [5] </ref>. In [15] Authors compare the two components of their system (INRECA): the case-based component and the inductive component. the case-based component is always up-to-date because CBR can work incrementally, and it interprets cases dynamically. <p> Induction compiles past experiences into general knowledge used to solve problem. Case-based reasoning directly interprets past experiences. Some case-based reasoning systems integrate induction systems <ref> [5] </ref>. The induction system can compute a tree to index cases on a predefined number of levels in order to improve the efficiency of case-based reasoning. As a consequence, the case-based reasoner works on a much smaller set of candidates. In [15], an integration architecture is proposed.
Reference: [6] <author> E.R. Bareiss, B.W. Porter, </author> <title> and C.C. </title> <journal> Wier. Apprentissage Symbolique, </journal> <volume> volume 2, chapter 4, </volume> <pages> pages 105- 130. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: Several researchers have defended the exemplar-based models as psychologically plausible. We describe in this section an exemplar-based reasoning system: PROTOS <ref> [6] </ref>. PROTOS implements both case-based classification and case-based knowledge acquisition. Given a description of a situation or an object, it classifies the situation or object by type. PROTOS's domain of expertise is audiological (hearing) disorders.
Reference: [7] <author> R. Barletta. </author> <title> An introduction to case-based reasoning. </title> <journal> AI Expert, </journal> <month> August </month> <year> 1991. </year>
Reference-contexts: The developer could be faced with re-doing the entire system from scratch. Now, all the user has to do to add more knowledge to a case-based system, is to add more cases and let the indexing mechanism restructure the case-base <ref> [7] </ref>. This means that CBR systems are incremental and learn continuously new cases. Inductive clustering methods generally look for similarity over series of instances and form categories based on those similarities. Shared-feature networks provide a means of clustering cases so that cases that share many features are grouped together. <p> Thus may be done by using the system's own model of general domain knowledge, or by asking the user for confirmation and additional information. Case indexing processes usually fall into one of three kinds: nearest neigh-bour, inductive, and knowledge-guided, or a combination of these three ap proaches <ref> [7] </ref>. * Nearest-neighbour approaches let the user retrieve cases based on a weighted sum of features in the input case that match cases in memory. This approach is most effective if the number of cases in the case base is relatively small.
Reference: [8] <author> S.E. Fahlman and C. Lebiere. </author> <title> The cascade-correlation learning architecture. </title> <type> Technical report, </type> <institution> School of Computer Science Carnegie Mellon University, </institution> <address> Pittsburgh, PA 15213, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: One approach is to have sub-nets that have competition between them, another is to have each subnet as another hidden layer. For example, in the "cascade correlation" algorithm <ref> [8] </ref>, if the required mapping cannot be learned by one layer, a hidden unit is added and trained while the previously trained weights are "frozen".
Reference: [9] <author> A. Giacometti. Modeles Hybrides de l'Expertise. </author> <type> PhD thesis, </type> <institution> Telecom-Paris, </institution> <year> 1992. </year>
Reference-contexts: However, experience shows that the learning procedure may cause the creation of a considerable number of prototype units near the boundaries of classes in the input space <ref> [9] </ref>. This situation can be avoided by adding a new criterion to the differentiation process. This has been achieved by introducing an uncertainty region between two classes in which differentiation is not allowed [9]. This region presents vectors that cause nearly the same activation to the two prototypes. <p> creation of a considerable number of prototype units near the boundaries of classes in the input space <ref> [9] </ref>. This situation can be avoided by adding a new criterion to the differentiation process. This has been achieved by introducing an uncertainty region between two classes in which differentiation is not allowed [9]. This region presents vectors that cause nearly the same activation to the two prototypes. As a result, the network is unable to learn new cases that falls into this region (we call these cases: boundary cases) (Figure 2) (see section 4). <p> Being a prototype-based network, ARN2 has two essential drawbacks: * the creation of a considerable number of prototype units near the class boundaries (boundary instances), this situation can be avoided by introducing an uncertainty region in which the network is unable to learn new instances <ref> [9] </ref>. * the problem of particular cases: it might happen that some cases share some features with some classes without having the same classification. Such cases are called particular instances.
Reference: [10] <author> S. Grossberg. </author> <title> Competitive learning: From interactive activation to adaptive resonance. </title> <journal> Cognitive Science, </journal> <volume> (11):23- 63, </volume> <year> 1987. </year>
Reference-contexts: If this does not work either, another hidden unit is added as another hidden layer and so on. 2.2 Competitive Learning Models During the late of 1960 and throughout the 1970 Grossberg <ref> [10] </ref> introduced a variety of competitive learning schemes. Rumelhart and Zipser [23] summarize competitive learning models as follows: * Units in a given layer are broken into a set of non overlapping clusters. Each unit within a cluster inhibits every other unit within this cluster.
Reference: [11] <author> J. Kolodner. </author> <title> Case-Based Reasoning. </title> <publisher> Morgan Kaufmann Publishers, Inc, </publisher> <year> 1993. </year>
Reference-contexts: There are three major parts for any case: the problem description, 10 the solution and the resulting state of the world when the solution was carried out <ref> [11] </ref>. Building a structure that will turn the most appropriate case is the goal of the case memory and retrieval process. <p> Remembering and using both general knowledge structures and specific instances is crucial to understanding. The theory of Dynamic Memory presents indexing as the key of using experience in understanding [24] <ref> [11] </ref>. The premise was that remembering, understanding, experiencing, and learning can not be separated from each other. 3.2.1 The Dynamic Memory Model The case memory in this model [24] is a hierarchical structure of what is called "episodic memory organization packets".
Reference: [12] <author> P. Koton. </author> <title> Using Experience in Learning and Problem Solving. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, MIT, </institution> <year> 1989. </year>
Reference-contexts: Rules represent the general domain knowledge while cases represent the acquired knowledge. Rules or deep models may be 16 used to solve problem on their own if the case-based method fails. To illustrate this, we describe the CASEY system. CASEY <ref> [12] </ref> is a case-based diagnostician. As input it takes a description of its new patient, including normal signs and presenting signs and symptoms. Its output is a causal explanation of the disorders the patient has. CASEY diagnoses patients by applying model-based matching and adaptation heuristics to the available cases.
Reference: [13] <author> S. Krovvidy and W.G. Wee. </author> <title> Hybrid Architecture for Intelligent System, </title> <booktitle> chapter 17, </booktitle> <pages> pages 357-377. </pages> <publisher> CRC Press, Inc., </publisher> <year> 1992. </year>
Reference-contexts: Each node A ik updates its activation value. Units from layer 1 update their states assuming that the matching cases have prediction value for non identical, but similar cases. This architecture provides an efficient solution to the case indexing problem based on a parallel architecture. In <ref> [13] </ref>, an intelligent hybrid system for waste water treatment (WATTS) using inductive learning, case-based reasoning and Hopfield neural network approaches is described. WATTS generates production rules from a set of examples and integrates them into a single representation.
Reference: [14] <author> W.J. Long, S. Naimi, M.G. Criscitiello, and R. Jayes. </author> <title> The development and use of a causal model of reasoning for reasoning about heart failure. </title> <booktitle> In Symposium on Computer Applications in Medical Care, IEEE, </booktitle> <year> 1987. </year>
Reference-contexts: Its output is a causal explanation of the disorders the patient has. CASEY diagnoses patients by applying model-based matching and adaptation heuristics to the available cases. It has a case library of approximately twenty-five cases, all of which were diagnosed by the rule-based Heart Failure Program <ref> [14] </ref>. When a new patient case is presented, CASEY searches its case library to see if it knows a similar case and, if so, uses that to diagnose the new patient.
Reference: [15] <author> M. Manago, K. Althoff, E. Auriol, R. Traphoner, S. Wess, and N. Conruyt anf F. Maurer. </author> <title> Induction and reasoning from cases. </title> <booktitle> In First European Workshop on CBR, </booktitle> <volume> number 1, </volume> <month> November </month> <year> 1993. </year>
Reference-contexts: The study of some inductive approaches like ID3/C4 [17] [18] or BUBE have shown that the costs of altering large case bases are very high and lead sometimes to a recompilation of the complete case base after each alteration [5]. In <ref> [15] </ref> Authors compare the two components of their system (INRECA): the case-based component and the inductive component. the case-based component is always up-to-date because CBR can work incrementally, and it interprets cases dynamically. <p> Some case-based reasoning systems integrate induction systems [5]. The induction system can compute a tree to index cases on a predefined number of levels in order to improve the efficiency of case-based reasoning. As a consequence, the case-based reasoner works on a much smaller set of candidates. In <ref> [15] </ref>, an integration architecture is proposed.
Reference: [16] <author> P. Myllymaki and H.Tirri. </author> <title> Massively parallel case-based reasoning with probabilistic similarity metrics. </title> <booktitle> In First European Workshop on CBR, </booktitle> <volume> number 1, </volume> <month> November </month> <year> 1993. </year>
Reference-contexts: In the neural network model, there are additional units called "adapters" between the action and factor units. This part of the network may also be trained by an extension of the backpropagation algorithm. In <ref> [16] </ref> Authors propose another approach where case-based reasoning can be implemented on a connectionist network architecture. The method is based on implementing Pearl's probability propagation as a 3-layer hierarchical network. The first layer contains one node for each of the possible attribute values a ij .
Reference: [17] <author> J.R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> (1):81- 106, </volume> <year> 1986. </year>
Reference-contexts: Each internal node of a shared-feature network holds features shared by the cases below it. Leafs hold cases themselves. Building such a hierarchy needs a lot of time to generate the problem solving knowledge. The study of some inductive approaches like ID3/C4 <ref> [17] </ref> [18] or BUBE have shown that the costs of altering large case bases are very high and lead sometimes to a recompilation of the complete case base after each alteration [5].
Reference: [18] <author> J.R. Quinlan. </author> <title> C4.5. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1992. </year> <month> 24 </month>
Reference-contexts: Each internal node of a shared-feature network holds features shared by the cases below it. Leafs hold cases themselves. Building such a hierarchy needs a lot of time to generate the problem solving knowledge. The study of some inductive approaches like ID3/C4 [17] <ref> [18] </ref> or BUBE have shown that the costs of altering large case bases are very high and lead sometimes to a recompilation of the complete case base after each alteration [5]. <p> In general, the use of these approaches leads to retrieval times that increase linearly with the number of cases in the case base. * Inductive approaches are used in situations where the retrieval goal or case outcome is well-defined (like ID3 <ref> [18] </ref>). The task is to determine inductively which features do the best job of discriminating between the various case outcomes and to organize case memory with respect to those induced features.
Reference: [19] <author> E.B. Reategui and J. Campbell. </author> <title> A classification system for credit card transaction. </title> <booktitle> In Second European Workshop on CBR, </booktitle> <year> 1994. </year>
Reference-contexts: A CBR system is used to retrieve a relevant solution from the case base to be used as an initial solution. This is found to be useful in accelerating the convergence of the network towards optimal solutions. In <ref> [19] </ref>, a classification system for credit card transaction is proposed. It is based on using neural networks as general knowledge and a CBR system as a way of using specific instances of cases in problem solving.
Reference: [20] <author> D.L. Reilly, L.N. Cooper, and C. Elbaum. </author> <title> A neural model for category learning. </title> <booktitle> Biological Cybermetics, </booktitle> <year> 1982. </year>
Reference-contexts: In the next we present a general survey of the incremental learning in neural networks, then we describe briefly competitive learning which is used generally in prototype-based networks. Finally, we describe three prototype-based incremental networks <ref> [20] </ref>, [3] and [4]. 2.1 Incremental Learning in Neural Networks The idea of incremental learning implies starting from a simplest network and adding units and/or connections whenever necessary to decrease error. <p> Class regions corresponding to different pattern categories are assumed to be strictly disjoint. Figure 4 shows the general architecture of a prototype-based network. 2.3.1 A Neural Model for Category Learning- NLS In this model commercially known as the Nestor Learning System (NLS) - <ref> [20] </ref> a prototype is associated with each unit. A prototype gets activated only if the input falls into its domination region determined by a distance computation followed by a thresholding. <p> As a result, the network is unable to learn new cases that falls into this region (we call these cases: boundary cases) (Figure 2) (see section 4). This model is similar to the model described in <ref> [20] </ref>, commercially known as the different classes. S 1 is a situation vector which activates P 1 . S 2 is a situation vector which falls into an uncertainty region and the network is unable to give a decision because two different prototypes are activated. Nestor Learning System (NLS).
Reference: [21] <editor> C.K. Riesbeck and R.C. Schank. </editor> <title> Inside Case-Based Reasoning. </title> <publisher> Lawrence Erlbaum Associates, publishers, </publisher> <year> 1989. </year>
Reference-contexts: We have to mention that prototype-based incremental networks are suited for classification where little information on the input space is available. 3 Case-Based Reasoning A case-based reasoner solves new problems by adapting solutions that were used to solve old problems <ref> [21] </ref>. Case-based reasoning means reasoning from prior examples that are stored in what we call a case memory. <p> Case-based reasoning is a cyclic and integrated process of solving a problem, learning from this experience, solving a new problem, etc. 3.1 The CBR Cycle A general CBR cycle may be described by the following four processes [1] <ref> [21] </ref> [11](see figure 3): 1. RETRIEVE the most similar case or cases; 2. REUSE the information and knowledge in that case to solve the problem; 3. REVISE the proposed solution; 4. RETAIN the parts of this experience likely to be useful for future problem solving. <p> This is a trivial type of reuse <ref> [21] </ref>. * Adapt: there are two main ways to adapt past cases: reuse the past case solution (transformational reuse) and reuse the past method that constructed the solution (derivational reuse).
Reference: [22] <author> B.H. Ross. </author> <title> Some psychological results on case-based reasoning. </title> <editor> In K. Ham-mond, editor, </editor> <booktitle> Workshop on Case-Based Reasoning (DARPA), </booktitle> <address> Pensacola Beach, Florida, San mateo, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In <ref> [22] </ref> it has been shown that people learning a new skill often refer back to previous problems to refresh their memories on how to do the task.
Reference: [23] <author> D.E. Rumelhart and D. Zipser. </author> <title> Feature discovery by competitive learning. </title> <journal> Cognitive Science, </journal> <volume> (9):75- 112, </volume> <year> 1985. </year>
Reference-contexts: If this does not work either, another hidden unit is added as another hidden layer and so on. 2.2 Competitive Learning Models During the late of 1960 and throughout the 1970 Grossberg [10] introduced a variety of competitive learning schemes. Rumelhart and Zipser <ref> [23] </ref> summarize competitive learning models as follows: * Units in a given layer are broken into a set of non overlapping clusters. Each unit within a cluster inhibits every other unit within this cluster.
Reference: [24] <author> R. Schank. </author> <title> Dynamic Memory: A Theory of Learning in Computers and people. </title> <publisher> Cambridge Univ. Press. </publisher> <address> New York, </address> <year> 1982. </year>
Reference-contexts: Remembering and using both general knowledge structures and specific instances is crucial to understanding. The theory of Dynamic Memory presents indexing as the key of using experience in understanding <ref> [24] </ref> [11]. The premise was that remembering, understanding, experiencing, and learning can not be separated from each other. 3.2.1 The Dynamic Memory Model The case memory in this model [24] is a hierarchical structure of what is called "episodic memory organization packets". <p> The theory of Dynamic Memory presents indexing as the key of using experience in understanding <ref> [24] </ref> [11]. The premise was that remembering, understanding, experiencing, and learning can not be separated from each other. 3.2.1 The Dynamic Memory Model The case memory in this model [24] is a hierarchical structure of what is called "episodic memory organization packets". The basic idea is to organize specific cases which share similar properties under a more general structure (a generalized episode). A Generalized Episode (GE) contains three different types of objects: norms, cases and indices.
Reference: [25] <author> R. Schank and R. A. </author> <title> Scripts, Plans, Goals and Understanding. </title> <publisher> Erlbaum, </publisher> <address> Northvale, </address> <year> 1977. </year>
Reference-contexts: The conclusion drawn from several studies is that reasoning using analogy is a natural process for people especially when there is much uncertainty or many unknowns and during early learning. Schank and Abelson <ref> [25] </ref> proposed that our general knowledge about situations is recorded in scripts. This is a specialized version of the standard "schema" view of reasoning that says that general knowledge is resident in memory in form of chunks, and that reasoning is a process of applying those chunks to new situations.
Reference: [26] <author> J.W. Shavlik, R.I. Mooney, and G.G. Towell. </author> <title> Symbolic and neural learning algorithm: An experimental comparison. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 111-143, </pages> <year> 1991. </year>
Reference-contexts: In [28] author showed how the updating costs of incremental decision tree algorithms can be significantly decreased by saving specific instances. Neural Networks (NNs) have demonstrated interesting capabilities. They produce accurate classification results that are comparable to those produced by symbolic inductive approaches <ref> [26] </ref>. They are also very good at handling noisy data and once they are trained, they can classify new examples quickly, making this approach suitable for real-time application. Some neural models 2 display a form of learning manifested in human behaviour: supervised learning of pattern categories.
Reference: [27] <author> P. Thrift. </author> <title> A neural network model for case-based reasoning. </title> <booktitle> In Proceddings of the DARPA Case-Based Reasoning Workshop, </booktitle> <month> May </month> <year> 1989. </year>
Reference-contexts: Some neural network architectures for CBR systems have been proposed. In <ref> [27] </ref>, a neural network model for case-filtering is proposed, a feed forward network is designed such that the inputs are factors from instances in a library of cases and factors from a new observed situation. The filter is designed to select 17 relevant cases from the library.
Reference: [28] <author> P.E. Utgoff. </author> <title> Incremental induction of decision trees. </title> <journal> Machine Learning, </journal> (4):161-186, 1989. 
Reference-contexts: In the inductive component the consultation is consistent: what is true today will be true tomorrow, but it is more preferment than the CBR consultation. One method of improving algorithms using these representations is to storing and using specific instances. In <ref> [28] </ref> author showed how the updating costs of incremental decision tree algorithms can be significantly decreased by saving specific instances. Neural Networks (NNs) have demonstrated interesting capabilities. They produce accurate classification results that are comparable to those produced by symbolic inductive approaches [26].
References-found: 28

