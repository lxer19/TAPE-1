URL: http://www.aic.nrl.navy.mil/~gordon/papers/cogsci97.ps
Refering-URL: http://www.aic.nrl.navy.mil/~gordon/pubs.html
Root-URL: 
Email: gordon@aic.nrl.navy.mil  devika@cs.rice.edu  
Title: A Cognitive Model of Learning to Navigate  
Author: Diana Gordon Devika Subramanian 
Address: Code 5510 4555 Overlook Avenue, S.W. Washington, D.C. 20375  Houston, TX 77005  
Affiliation: Naval Research Laboratory,  Department of Computer Science Rice University  
Abstract: Our goal is to develop a cognitive model of how humans acquire skills on complex cognitive tasks. We are pursuing this goal by designing computational architectures for the NRL Navigation task, which requires competent sensorimotor coordination. In this paper, we analyze the NRL Navigation task in depth. We then use data from experiments with human subjects learning this task to guide us in constructing a cognitive model of skill acquisition for the task. Verbal protocol data augments the black box view provided by execution traces of inputs and outputs. Computational experiments allow us to explore a space of alternative architectures for the task, guided by the quality of fit to human performance data. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Arbib, M. A., & Liaw, J.-S. </author> <year> (1995). </year> <title> Sensorimotor transformations in the worlds of frogs and robots. </title> <journal> Artificial Intelligence, </journal> <volume> 72, </volume> <pages> 53-79. </pages>
Reference: <author> Cassandra, A. R., Littman, M. L., & Kaelbling, L. </author> <year> (1994). </year> <title> Acting optimally in partially observable stochastic domains. </title> <booktitle> Proceedings of the 12th National Conference on Artificial Intelligence (pp. </booktitle> <pages> 1023-1028). </pages> <address> Seattle, WA. </address>
Reference-contexts: The state space defined by the sensors for the NRL Navigation task is about 10 24 ; optimal controllers for POMDPs have been constructed tabula rasa only for state spaces on the order of 100 states <ref> (Cassandra, Littman, & Kaelbling, 1994) </ref> because of the time (and therefore, sample) complexity. 1 Our motivation for building an optimal controller for the task is two fold: first, it gives us an upper baseline for comparison with human performance; second, it allows us to independently analyze the complexity of learning the
Reference: <author> Gallistel, C. R. </author> <year> (1990). </year> <title> The organization of learning. </title> <address> Cam-bridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: The fairly coarse discretization in both the sensor and the action space forces the optimal controller 1 Our navigation problem has different dynamics than the ones faced by animals like rats and ants <ref> (Gallistel, 1990) </ref>, which have a richer sensor base and can use higher level features like landmarks. 2 We conjecture that it accelerates the rate of learning. to be stochastic.
Reference: <author> Gordon, D., & Subramanian, D. </author> <year> (1996a). </year> <title> Comparison of action selection learning methods. </title> <booktitle> Proceedings of the Third International Workshop on Multistrategy Learning, </booktitle> <pages> Harpers Ferry (pp. 95-102). </pages> <institution> Fairfax, VA: George Mason University. </institution>
Reference-contexts: An optimal controller for this task achieves a performance score of 100% for the task configuration of 25 mines, small mine drift and no sensor noise. This is also the task configuration for our human experiments on this task. The optimal controller was created by reinforcement learning <ref> (Gordon & Subramanian, 1996a) </ref>. The learner was initialized with a controller with a specific task decomposition, a specific abstraction of the state space that significantly reduced the complexity of learning, and with a correct but incomplete action choice policy.
Reference: <author> Gordon, D., & Subramanian, D. </author> <year> (1996b). </year> <title> Cognitive modeling of action selection learning. </title> <booktitle> Proceedings of the Eighteenth Annual Conference of the Cognitive Science Society (pp. </booktitle> <pages> 546-551). </pages> <address> Mahwah, NJ: </address> <publisher> Lawrence Elrbaum Associates. </publisher>
Reference: <author> Gordon, D., Schultz, A., Grefenstette, J., Ballas, J., & Perez, M. </author> <year> (1994). </year> <title> User's guide to the navigation and collision avoidance task (Tech. </title> <type> Rep. </type> <institution> AIC-94-013). Washington, D.C.: Naval Research Laboratory. </institution>
Reference-contexts: Data from Human Subjects In the experiments with humans, seven subjects were used, and each ran for two or three 45-minute sessions with the simulations. We instrumented 4 the simulation <ref> (Gordon et. al., 1994) </ref> to gather execution traces for subsequent analysis. We also obtained verbal protocols by recording subject utterances during play and by collecting answers to questions posed at the end of the individual sessions.
Reference: <author> Itoh, Y., Hayashi, Y., Tsukui, I., & Saito, S. </author> <year> (1990). </year> <title> The ergonomic evaluation of eye movement and mental workload in aircraft pilots, </title> <journal> Ergonomics, </journal> <volume> 33(6), </volume> <pages> 719-733. </pages>
Reference-contexts: Related work along these lines evaluates the scanning behavior and mental workload of aircraft pilots, who also make decisions based on gauges <ref> (e.g., see Itoh et al., 1990) </ref>. With more detailed human data, we plan to model individual subjects at a level that will enable us to predict the forms of their trajectories. Acknowledgements This research was sponsored by the Office of Naval Research N00014-95-WX30360, N00014-95-1-0846 and N00014-96-1-0538.
Reference: <author> Jaakkola, T., Singh, S. P., & Jordan, M. I. </author> <year> (1995). </year> <title> Reinforcement learning with soft state aggregation. </title> <booktitle> Advances in Neural Information Processing Systems 7 (pp. </booktitle> <pages> 361-368), </pages> <publisher> MIT Press. </publisher>
Reference: <author> Jordan, M., & Rumelhart, D. </author> <year> (1992). </year> <title> Forward models: Supervised learning with a distal teacher, </title> <journal> Cognitive Science, </journal> <volume> 16, </volume> <pages> 307-354. </pages>
Reference: <author> Kent, E. </author> <year> (1981). </year> <title> The brains of men and machines. </title> <address> Peterbor-ough, N.H.: Byte/McGraw Hill. </address>
Reference-contexts: When using these action models, our cognitive model chooses the turn that would yield the best next prediction, as evaluated by P sonars or P bearing . Evidence in the cognitive literature <ref> (Kent, 1981) </ref> suggests people learn specific values, but over time these specifics are chunked into relevant categories. For example, although people might memorize every size, color, and shape of birds they have seen, over time they generalize to a prototypical bird.
Reference: <author> Moore, A. W. & Atkeson, C. G. </author> <year> (1995). </year> <title> The Parti-game algorithm for variable resolution reinforcement learning in multidimensional state-spaces, </title> <journal> Machine Learning, </journal> <volume> 21. </volume>
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 81-107. </pages>
Reference-contexts: All paired differences between learning curves of variants of the model described in this section are statistically significant. 6 We used C4.5 <ref> (Quinlan, 1986) </ref>, which learns the action models in batch and has high noise tolerance an advantage for a POMDP. 7 We experimented with the number of episodes and chose a setting where performance improvement leveled off for all algorithms.
Reference: <author> Subramanian, D., & Gordon, D. </author> <year> (1997). </year> <title> Experiments with an optimal controller for the NRL Navigation Task (Tech. </title> <type> Rep.) </type> <institution> Houston, TX: Computer Science Department, Rice University. </institution>
Reference-contexts: Our own current work <ref> (Subramanian & Gordon, 1997) </ref> explores this connection as well as algorithms for state aggregation for very high dimensional discrete state spaces.
Reference: <author> Watkins, C.J.C.H. </author> <year> (1989). </year> <title> Learning from delayed rewards. </title> <type> Doctoral Thesis, </type> <institution> Cambridge University, </institution> <address> England. </address>
Reference-contexts: Because there is indication that this task decomposition (focus heuristic) is widely employed and can yield large benefits in performance, we further test its value on an alternative (reinforcement learning) architecture. We use a standard q-learner <ref> (Watkins, 1989) </ref>, that we modified for this task to allow for fair comparisons with variants of M . The details of the q-learning architectures, called Q focus and Q nofocus for with and without the focus heuristic, are irrelevant here. Details are in Gordon and Subramanian (1996a).
References-found: 14

