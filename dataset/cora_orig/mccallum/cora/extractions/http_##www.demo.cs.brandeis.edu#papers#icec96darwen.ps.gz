URL: http://www.demo.cs.brandeis.edu/papers/icec96darwen.ps.gz
Refering-URL: http://www.demo.cs.brandeis.edu/papers/long.html
Root-URL: http://www.cs.brandeis.edu
Email: darwen@cs.adfa.oz.au  xin@cs.adfa.oz.au  
Title: Automatic Modularization by Speciation  
Author: Paul Darwen Xin Yao 
Address: Canberra ACT 2600 AUSTRALIA  Canberra ACT 2600 AUSTRALIA  
Affiliation: Computational Intelligence Group School of Computer Science University College, UNSW Australian Defence Force Academy  Computational Intelligence Group School of Computer Science University College, UNSW Australian Defence Force Academy  
Abstract: Real-world problems are often too difficult to be solved by a single monolithic system. There are many examples of natural and artificial systems which show that a modular approach can reduce the total complexity of the system while solving a difficult problem satisfactorily. The success of modular artificial neural networks in speech and image processing is a typical example. However, designing a modular system is a difficult task. It relies heavily on human experts and prior knowledge about the problem. There is no systematic and automatic way to form a modular system for a problem. This paper proposes a novel evolutionary learning approach to designing a modular system automatically, without human intervention. Our starting point is speciation, using a technique based on fitness sharing. While speciation in genetic algorithms is not new, no effort has been made towards using a speciated population as a complete modular system. We harness the specialized expertise in the species of an entire population, rather than a single individual, by introducing a gating algorithm. We demonstrate our approach to automatic modularization by improving co-evolutionary game learning. Following earlier researchers, we learn to play iterated prisoner's dilemma. We review some problems of earlier co-evolutionary learning, and explain their poor generalization ability and sudden mass extinctions. The generalization ability of our approach is significantly better than past efforts. Using the specialized expertise of the entire speciated population though a gating algorithm, instead of the best individual, is the main contributor to this improvement. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. M. Axelrod, </author> <title> The Evolution of Cooperation. </title> <publisher> Basic Books, </publisher> <year> 1984. </year>
Reference-contexts: Similarly, the other player's previous actions are represented by 111. As strategies are too simple to learn that the game is finite, we may ignore the "shadow of the future" of IPD <ref> [1, page 13] </ref>. All games of IPD last for 100 iterations. There are 2 2l possible histories, so each individual's genotype must be that long to represent an action for each possibility. At the start of a game, there are no previous l rounds.
Reference: [2] <author> R. M. Axelrod, </author> <title> "The evolution of strategies in the iterated prisoner's dilemma," in Genetic Algorithms and Simulated Annealing, </title> <editor> (L. Davis, ed.), ch. </editor> <volume> 3, </volume> <pages> pp. 32-41, </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1987. </year>
Reference-contexts: C. Genetic Algorithm Details Some attempts at co-evolution use two GA populations: an individual's fitness is determined by how well it performs against the other population, with no intra-population competition [18]. We would like intra-population competition. Like Axelrod <ref> [2] </ref>, we use only a single GA: an individual's fitness is found by its performance against the members of its own population. Our representation scheme is similar to Axelrod's [2]. <p> We would like intra-population competition. Like Axelrod <ref> [2] </ref>, we use only a single GA: an individual's fitness is found by its performance against the members of its own population. Our representation scheme is similar to Axelrod's [2]. Each individual is a set of rules stored in a look-up table that covers every possible history, and is represented as a binary string. A game that runs for 100 iterations has too many possible histories, and only recent steps will have significance for the next move. <p> Each game of 2IPD, both during the GA learning and against the test set, was 100 iterations. The results are for 30 of each of the following strategies: * Best single individual from the final generation of a co-evolutionary GA without speciation, like Axelrod <ref> [2] </ref> (denoted by best.ns). * A co-evolutionary GA using implicit fitness sharing with random sample selection, using both the best individual from the final generation (best.sr) and the gating algorithm on the entire final generation (gate.sr). * A co-evolutionary GA using implicit fitness sharing with reverse assortative sample selection | the <p> Results against the best 25 strategies from the partial enumerative search, for 2IPD with remembered history l = 3. This is exactly the same as used by Axelrod <ref> [2] </ref>. Note that the modular approach works better than the best individual.
Reference: [3] <author> P. Darwen and X. Yao, </author> <title> "A dilemma for fitness sharing with a scaling function," </title> <booktitle> in Proceedings of the 1995 IEEE Conference on Evolutionary Computation, </booktitle> <pages> pp. 166-171, </pages> <publisher> IEEE Press, </publisher> <month> 29 November - 1 December </month> <year> 1995. </year>
Reference: [4] <author> P. Darwen and X. Yao, </author> <title> "On evolving robust strategies for iterated prisoner's dilemma," </title> <booktitle> in Progress in Evolutionary Computation, </booktitle> <pages> pp. 276-292, </pages> <address> Berlin: </address> <publisher> Springer, </publisher> <year> 1995. </year>
Reference-contexts: This can cause mass extinctions as in Figure 3 when mutation produces such a novel strategy. Such features have been observed before [14] [16], and only recently explained by us <ref> [4] </ref>. Unaware of this reason, some workers have nonetheless used both speciation and co-evolution [18] [20]. This prevents over-specialization and improves generalization ability. However, although these approaches learned diverse and specialized strategies, the authors only used one individual at a time.
Reference: [5] <author> R. Dawkins, </author> <title> The Blind Watchmaker. Longman, first ed., </title> <year> 1986. </year>
Reference: [6] <author> K. Deb and D. E. Goldberg, </author> <title> "An investigation of niche and species formation in genetic function optimization," </title> <booktitle> in Proceedings of the Third International Conference on Genetic Algorithms, </booktitle> <editor> (J. D. Schaffer, </editor> <publisher> ed.), </publisher> <pages> pp. 42-50, </pages> <publisher> Morgan Kaufmann, </publisher> <month> June </month> <year> 1989. </year>
Reference: [7] <author> D. B. Fogel, </author> <title> "Evolving behaviours in the iterated prisoner's dilemma," </title> <journal> Evolutionary Computation, </journal> <volume> vol. 1, no. 1, </volume> <pages> pp. 77-97, </pages> <year> 1993. </year>
Reference: [8] <author> T. C. Folsom, </author> <title> "A modular hierarchical neural network for machine vision," </title> <booktitle> in International Joint Conference on Neural Networks, </booktitle> <pages> pp. 897-902, </pages> <publisher> IEEE Press, </publisher> <year> 1990. </year>
Reference-contexts: The system described here is highly specific to learning game strategies, to demonstrate the modular approach. But speciation as automatic modularization can solve more realistic and complicated problems. Modular neural networks have been effective in many problems <ref> [8] </ref> [21], and genetic algorithms have evolved non-modular neural networks for various games [10] [15]. This indicates that a modular approach to evolving neural networks should give improved results.
Reference: [9] <author> S. Forrest, B. Javornik, R. E. Smith, and A. S. Perelson, </author> <title> "Using genetic algorithms to explore pattern recognition in the immune system," </title> <journal> Evolutionary Computation, </journal> <volume> vol. 1, no. 3, </volume> <pages> pp. 191-211, </pages> <year> 1993. </year>
Reference-contexts: In the case of a tie, payoff is shared equally among the tie-breakers. a Or the largest winning margin, if you prefer. Table 1. Payoff function for implicit fitness sharing [19]. 1. The best-in-sample receives a fixed credit, instead of their score as in the original implementation [19] <ref> [9] </ref>. Justification of this choice will be discussed in the longer version of this paper. In short, rewarding the best-in-sample with a fixed credit maintains all strategies that are the best reply to a particular opponent, no matter how high or low that best score actually is.
Reference: [10] <author> B. Freisleben, </author> <title> "Teaching a neural network to play go-moku," </title> <booktitle> in Proceedings of the 1992 International Conference on Artificial Neural Networks, </booktitle> <editor> (I. Aleksander, </editor> <publisher> ed.), </publisher> <pages> pp. 1659-1662, </pages> <month> Sep. </month> <year> 1992. </year>
Reference-contexts: But speciation as automatic modularization can solve more realistic and complicated problems. Modular neural networks have been effective in many problems [8] [21], and genetic algorithms have evolved non-modular neural networks for various games <ref> [10] </ref> [15]. This indicates that a modular approach to evolving neural networks should give improved results. Some previous studies have used co-evolving "species" (actually separate GA populations), each dealing with a different aspect of the problem [13] [17].
Reference: [11] <author> D. E. Goldberg, K. Deb, and J. Horn, </author> <title> "Massive multi-modality, deception, and genetic algorithms," in Parallel Problem Solving from Nature 2, </title> <editor> (R. Manner and B. Manderick, </editor> <booktitle> eds.), </booktitle> <pages> pp. 37-46, </pages> <publisher> North-Holland, </publisher> <month> Sep. </month> <year> 1992. </year>
Reference: [12] <author> D. Grady, </author> <title> "The vision thing: Mainly in the brain," </title> <journal> Discover, </journal> <volume> vol. 14, </volume> <pages> pp. 57-66, </pages> <month> June </month> <year> 1993. </year>
Reference: [13] <author> P. Husbands and F. Mill, </author> <title> "Simulated co-evolution as the mechanism for emergent planning and scheduling," </title> <booktitle> in Proceedings of the Fourth International Conference on Genetic Algorithms, </booktitle> <editor> (R. Belew and L. Booker, </editor> <booktitle> eds.), </booktitle> <pages> pp. 264-270, </pages> <publisher> Morgan Kaufmann, </publisher> <month> July </month> <year> 1991. </year>
Reference-contexts: This indicates that a modular approach to evolving neural networks should give improved results. Some previous studies have used co-evolving "species" (actually separate GA populations), each dealing with a different aspect of the problem <ref> [13] </ref> [17]. Unfortunately, these do not use a speciation method to form the separate sub-populations | the number of sub-populations is fixed in advance. In these studies, the various sub-solutions are recombined, but modularization is still done manually.
Reference: [14] <author> K. Lindgren, </author> <title> "Evolutionary phenomena in simple dynamics," </title> <booktitle> in Artificial Life 2, </booktitle> <pages> pp. 295-312, </pages> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference-contexts: As a canonical GA converges to only one solution, it over-specializes to one particular strategy, becoming vulnerable to novel opponents. This can cause mass extinctions as in Figure 3 when mutation produces such a novel strategy. Such features have been observed before <ref> [14] </ref> [16], and only recently explained by us [4]. Unaware of this reason, some workers have nonetheless used both speciation and co-evolution [18] [20]. This prevents over-specialization and improves generalization ability. However, although these approaches learned diverse and specialized strategies, the authors only used one individual at a time.
Reference: [15] <author> D. Moriarty and R. Miikkulainen, </author> <title> "Evolving complex Othello strategies using marker-based genetic encoding of neural networks," </title> <type> Tech. Rep. </type> <institution> AI93-206, The University of Texas at Austin, </institution> <month> Sep. </month> <year> 1993. </year>
Reference-contexts: But speciation as automatic modularization can solve more realistic and complicated problems. Modular neural networks have been effective in many problems [8] [21], and genetic algorithms have evolved non-modular neural networks for various games [10] <ref> [15] </ref>. This indicates that a modular approach to evolving neural networks should give improved results. Some previous studies have used co-evolving "species" (actually separate GA populations), each dealing with a different aspect of the problem [13] [17].
Reference: [16] <author> R. G. Palmer, W. B. Arthur, J. H. Holland, B. LeBaron, and P. Tayler, </author> <title> "Artificial economic life: a simple model of a stockmarket," </title> <journal> Physica D, </journal> <volume> vol. 75, </volume> <pages> pp. 264-274, </pages> <year> 1994. </year>
Reference-contexts: As a canonical GA converges to only one solution, it over-specializes to one particular strategy, becoming vulnerable to novel opponents. This can cause mass extinctions as in Figure 3 when mutation produces such a novel strategy. Such features have been observed before [14] <ref> [16] </ref>, and only recently explained by us [4]. Unaware of this reason, some workers have nonetheless used both speciation and co-evolution [18] [20]. This prevents over-specialization and improves generalization ability. However, although these approaches learned diverse and specialized strategies, the authors only used one individual at a time.
Reference: [17] <author> M. A. Potter, K. A. De Jong, and J. J. Grefenstette, </author> <title> "A coevolutionary approach to learning sequential decision rules," </title> <booktitle> in Proceedings of the Sixth International Conference on Genetic Algorithms, </booktitle> <pages> pp. 366-372, </pages> <publisher> Mor-gan Kaufmann, </publisher> <month> July </month> <year> 1995. </year>
Reference-contexts: This indicates that a modular approach to evolving neural networks should give improved results. Some previous studies have used co-evolving "species" (actually separate GA populations), each dealing with a different aspect of the problem [13] <ref> [17] </ref>. Unfortunately, these do not use a speciation method to form the separate sub-populations | the number of sub-populations is fixed in advance. In these studies, the various sub-solutions are recombined, but modularization is still done manually.
Reference: [18] <author> C. D. Rosin and R. K. Belew, </author> <title> "Methods for competitive co-evolution: Finding opponents worth beating," </title> <booktitle> in Proceedings of the Sixth International Conference on Genetic Algorithms, </booktitle> <editor> (L. Eshelman, </editor> <publisher> ed.), </publisher> <pages> pp. 373-380, </pages> <publisher> Morgan Kaufmann, </publisher> <month> July </month> <year> 1995. </year>
Reference-contexts: Justification of this choice will be discussed in the longer version of this paper. In short, rewarding the best-in-sample with a fixed credit maintains all strategies that are the best reply to a particular opponent, no matter how high or low that best score actually is. Rosin and Belew <ref> [18] </ref> also use a fixed credit, but without explaining why. 2. <p> But it is surprising that so far this has been overlooked in GA learning of game strategies. C. Genetic Algorithm Details Some attempts at co-evolution use two GA populations: an individual's fitness is determined by how well it performs against the other population, with no intra-population competition <ref> [18] </ref>. We would like intra-population competition. Like Axelrod [2], we use only a single GA: an individual's fitness is found by its performance against the members of its own population. Our representation scheme is similar to Axelrod's [2]. <p> This can cause mass extinctions as in Figure 3 when mutation produces such a novel strategy. Such features have been observed before [14] [16], and only recently explained by us [4]. Unaware of this reason, some workers have nonetheless used both speciation and co-evolution <ref> [18] </ref> [20]. This prevents over-specialization and improves generalization ability. However, although these approaches learned diverse and specialized strategies, the authors only used one individual at a time. They did not address the question of when to use which specialized strategy. This wastes much of the diverse expertise found by speciation.
Reference: [19] <author> R. E. Smith, S. Forrest, and A. S. Perelson, </author> <title> "Searching for diverse, cooperative populations with genetic algorithms," </title> <journal> Evolutionary Computation, </journal> <volume> vol. 1, no. 2, </volume> <pages> pp. 127-149, </pages> <year> 1992. </year>
Reference-contexts: The best in the sample receives payoff. In the case of a tie, payoff is shared equally among the tie-breakers. a Or the largest winning margin, if you prefer. Table 1. Payoff function for implicit fitness sharing <ref> [19] </ref>. 1. The best-in-sample receives a fixed credit, instead of their score as in the original implementation [19] [9]. Justification of this choice will be discussed in the longer version of this paper. <p> In the case of a tie, payoff is shared equally among the tie-breakers. a Or the largest winning margin, if you prefer. Table 1. Payoff function for implicit fitness sharing <ref> [19] </ref>. 1. The best-in-sample receives a fixed credit, instead of their score as in the original implementation [19] [9]. Justification of this choice will be discussed in the longer version of this paper. In short, rewarding the best-in-sample with a fixed credit maintains all strategies that are the best reply to a particular opponent, no matter how high or low that best score actually is.
Reference: [20] <author> R. E. Smith and B. Gray, </author> <title> "Co-adaptive genetic algorithms: An example in Othello strategy," </title> <booktitle> in Proceedings of the 1994 Florida Artificial Intelligence Research Symposium, </booktitle> <editor> (D. D. Dankel, ed.), </editor> <address> (Saint Peters-burg, Florida), </address> <pages> pp. 259-264, </pages> <institution> Florida AI Research Society, </institution> <year> 1994. </year>
Reference-contexts: This indicates the need for improved speciation methods in evolutionary computation. B. Future Work The method presented here is a very simple implementation of a modular approach. Further refinement should give improved results. For example, an earlier speciated coevolutionary system <ref> [20] </ref> displayed a never-ending turnover of species. This suggests that sampling the GA population during a run may produce more general responses than only using the final generation. A future project will be to apply this approach to a more complex task. <p> This can cause mass extinctions as in Figure 3 when mutation produces such a novel strategy. Such features have been observed before [14] [16], and only recently explained by us [4]. Unaware of this reason, some workers have nonetheless used both speciation and co-evolution [18] <ref> [20] </ref>. This prevents over-specialization and improves generalization ability. However, although these approaches learned diverse and specialized strategies, the authors only used one individual at a time. They did not address the question of when to use which specialized strategy. This wastes much of the diverse expertise found by speciation.
Reference: [21] <author> M. Van Hulle and G. Orban, </author> <title> "The EDANN concept: a modular artificial neural network model for biological vision and image processing," </title> <booktitle> in World Congress on Neural Networks-San Diego, </booktitle> <address> p. 320, </address> <publisher> Lawrence Erl-baum Associates, </publisher> <year> 1994. </year>
Reference-contexts: The system described here is highly specific to learning game strategies, to demonstrate the modular approach. But speciation as automatic modularization can solve more realistic and complicated problems. Modular neural networks have been effective in many problems [8] <ref> [21] </ref>, and genetic algorithms have evolved non-modular neural networks for various games [10] [15]. This indicates that a modular approach to evolving neural networks should give improved results. Some previous studies have used co-evolving "species" (actually separate GA populations), each dealing with a different aspect of the problem [13] [17].
Reference: [22] <author> D. H. Wolpert, </author> <title> "A mathematical theory of generalization," </title> <journal> Complex Systems, </journal> <volume> vol. 4, </volume> <pages> pp. 151-249, </pages> <year> 1990. </year>
Reference-contexts: IV. Results and Discussion We compare strategies produced by different evolutionary methods. We do this by seeing how well they perform against a set of test opponents. As Wolpert <ref> [22, page 246] </ref> observes, "It may turn out that ... there is no way to measure generalization more rigorously than via test problems." To obtain this test set, we performed a partial enumerative search of all strategy genotypes, for each value of remembered history l.
Reference: [23] <author> X. Yao and P. Darwen, </author> <title> "An experimental study of N-person iterated prisoner's dilemma games," </title> <booktitle> in Progress in Evolutionary Computation, </booktitle> <pages> pp. 90-108, </pages> <address> Berlin: </address> <publisher> Springer, </publisher> <year> 1995. </year>
References-found: 23

