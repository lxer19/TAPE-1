URL: http://s2k-ftp.cs.berkeley.edu:8000/sequoia/tech-reports/s2k-92-19/s2k-92-19.ps.Z
Refering-URL: http://s2k-ftp.cs.berkeley.edu:8000/sequoia/tech-reports/s2k-92-19/
Root-URL: http://www.cs.berkeley.edu
Title: d d A Static Analysis of I/O Characteristics of a Broad Class of Scientific Applications  
Author: Barbara K. Pasquale George C. Polyzos 
Address: La Jolla, CA 92093-0114, USA  
Affiliation: Computer Systems Laboratory Department of Computer Science and Engineering University of California, San Diego  
Date: 1992  
Note: November  
Abstract: SEQUOIA 2000 Technical Report #92/19 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Bell, G., </author> <title> ``The Future of High Performance Computers in Science and Engineering,'' </title> <journal> Communications of the ACM, </journal> <volume> Vol. 32, No. 9, </volume> <pages> pp. 1091-1101, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: Resulting progress in the areas of raw processor speed and parallelism, both in hardware and software, has produced GFLOPS machines, but has done little to close the ever widening gap between CPU performance and that of the attached I/O subsystem <ref> [1, 8, 9] </ref>. It is now apparent, however, that the successful future of high performance computing relies on our ability to design well-balanced systems which can easily provide the functionality and performance required by current and future scientific applications. <p> The sheer volume of this data intensifies I/O demands within the local system and communication requirements across networks [16]. Continuing to increase CPU speeds and to further exploit parallelism without improving the I/O system will create more I/O bound jobs which can become a bottleneck to system performance <ref> [1, 8, 9, 11, 12, 18] </ref>. In a recent study of the San Diego Supercomputer Center (SDSC), a monthly CPU idle time increase of 66% was directly attributed to I/O blocking [13].
Reference: [2] <author> Calzarossa, M., and Ferrari, D., </author> <title> ``A Sensitivity Study of the Clustering Approach to Workload Modeling,'' </title> <journal> Performance Evaluation, </journal> <volume> Vol. 6, </volume> <pages> pp. 25-33, </pages> <publisher> North-Holland, </publisher> <year> 1986. </year>
Reference-contexts: An average logical I/O request rate was also calculated for each resulting cluster. Although clustering algorithms are non-trivial because they must recognize ``nearness'' among the characteristic parameters selected, Calzarossa and Ferrari <ref> [2] </ref> found that the non-hierarchical K-means algorithm produces reliable results for workload characterization and modeling.
Reference: [3] <author> Calzarossa, M., and Serazzi, G., </author> <title> ``Workload Characterization for Supercomputers,'' Performance Evaluation of Supercomputers, </title> <editor> Ed. J. L. </editor> <publisher> Martin, </publisher> <pages> pp. 283-315, </pages> <publisher> North-Holland, </publisher> <year> 1988. </year>
Reference-contexts: 1. Introduction Past efforts to develop high performance computer systems have been primarily focused on the computational speeds of processors, often ignoring other important system components, such as the I/O subsystem and the operating system <ref> [3, 8, 17, 19] </ref>. Resulting progress in the areas of raw processor speed and parallelism, both in hardware and software, has produced GFLOPS machines, but has done little to close the ever widening gap between CPU performance and that of the attached I/O subsystem [1, 8, 9]. <p> Although the CSA data only provides aggregate resource usage, it enabled us to analyze the workload at two important levels, the functional level and the physical resource level <ref> [3, 5, 7] </ref>. Serazzi [15] showed that reliable workload characterizations can capture the functionality of the real system workload while still preserving the underlying physical resource usage.
Reference: [4] <author> Denning, P. J., and Adams III, G. B., </author> <title> ``Research Questions for Performance Analysis of Supercomputers,'' Performance Evaluation of Supercomputers, </title> <editor> J. L. Martin, </editor> <publisher> ed., </publisher> <pages> pp. 403-419, </pages> <publisher> North-Holland, </publisher> <year> 1988. </year>
Reference-contexts: Applications involving simulation based modeling require massive amounts of diverse data to be stored, organized, accessed, distributed, analyzed, and visualized. Relying on these large-scale computations and data analysis techniques, progress in many scientific disciplines is limited only by the available capacity of high performance computing <ref> [4] </ref>. In particular, computational Global Change research in fields such as oceanography, atmospheric science, and geology, relies on and produces atmospheric and oceanographic data sets ranging from hundreds of megabytes up to tens of gigabytes per execution.
Reference: [5] <author> Ferrari, D., </author> <title> ``Workload Characterization and Selection in Computer Performance Measurement,'' </title> <booktitle> Computer, </booktitle> <pages> pp. 18-24, </pages> <month> July/August </month> <year> 1972. </year>
Reference-contexts: Although the CSA data only provides aggregate resource usage, it enabled us to analyze the workload at two important levels, the functional level and the physical resource level <ref> [3, 5, 7] </ref>. Serazzi [15] showed that reliable workload characterizations can capture the functionality of the real system workload while still preserving the underlying physical resource usage.
Reference: [6] <author> Hartigan, J. A., </author> <title> Clustering Algorithms, </title> <editor> J. </editor> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1975. </year>
Reference-contexts: Cluster Analysis To find natural partitions or patterns within the resource usage records for a given application, the multidimensional analysis technique of K-means clustering was used <ref> [6, 7] </ref>. A notable feature of the clustering algorithm was that it used weighted Euclidean distance as a dissimilarity measure, which allowed the size of each cluster to vary in inverse proportion to its variance.
Reference: [7] <author> Heidelberger, P., and Lavenberg, S. S., </author> <title> ``Computer Performance Evaluation Methodology,'' </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. 33, No. 12, </volume> <pages> pp. 1195-1220, </pages> <month> December </month> <year> 1984. </year>
Reference-contexts: Although the CSA data only provides aggregate resource usage, it enabled us to analyze the workload at two important levels, the functional level and the physical resource level <ref> [3, 5, 7] </ref>. Serazzi [15] showed that reliable workload characterizations can capture the functionality of the real system workload while still preserving the underlying physical resource usage. <p> Cluster Analysis To find natural partitions or patterns within the resource usage records for a given application, the multidimensional analysis technique of K-means clustering was used <ref> [6, 7] </ref>. A notable feature of the clustering algorithm was that it used weighted Euclidean distance as a dissimilarity measure, which allowed the size of each cluster to vary in inverse proportion to its variance.
Reference: [8] <author> Hennessy, J. L., and Patterson, D. A., </author> <title> ``Computer Architecture: A Quantitative Approach,'' </title> <publisher> Morgan Kauf-mann Publishers, Inc., </publisher> <year> 1990. </year>
Reference-contexts: 1. Introduction Past efforts to develop high performance computer systems have been primarily focused on the computational speeds of processors, often ignoring other important system components, such as the I/O subsystem and the operating system <ref> [3, 8, 17, 19] </ref>. Resulting progress in the areas of raw processor speed and parallelism, both in hardware and software, has produced GFLOPS machines, but has done little to close the ever widening gap between CPU performance and that of the attached I/O subsystem [1, 8, 9]. <p> Resulting progress in the areas of raw processor speed and parallelism, both in hardware and software, has produced GFLOPS machines, but has done little to close the ever widening gap between CPU performance and that of the attached I/O subsystem <ref> [1, 8, 9] </ref>. It is now apparent, however, that the successful future of high performance computing relies on our ability to design well-balanced systems which can easily provide the functionality and performance required by current and future scientific applications. <p> The sheer volume of this data intensifies I/O demands within the local system and communication requirements across networks [16]. Continuing to increase CPU speeds and to further exploit parallelism without improving the I/O system will create more I/O bound jobs which can become a bottleneck to system performance <ref> [1, 8, 9, 11, 12, 18] </ref>. In a recent study of the San Diego Supercomputer Center (SDSC), a monthly CPU idle time increase of 66% was directly attributed to I/O blocking [13].
Reference: [9] <author> Katz, R. H., Gibson, G. A., and Patterson, D. A., </author> <title> ``Disk System Architectures for High Performance Computing,'' </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> Vol. 77, No. 12, </volume> <pages> pp. 1842-1858, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Resulting progress in the areas of raw processor speed and parallelism, both in hardware and software, has produced GFLOPS machines, but has done little to close the ever widening gap between CPU performance and that of the attached I/O subsystem <ref> [1, 8, 9] </ref>. It is now apparent, however, that the successful future of high performance computing relies on our ability to design well-balanced systems which can easily provide the functionality and performance required by current and future scientific applications. <p> The sheer volume of this data intensifies I/O demands within the local system and communication requirements across networks [16]. Continuing to increase CPU speeds and to further exploit parallelism without improving the I/O system will create more I/O bound jobs which can become a bottleneck to system performance <ref> [1, 8, 9, 11, 12, 18] </ref>. In a recent study of the San Diego Supercomputer Center (SDSC), a monthly CPU idle time increase of 66% was directly attributed to I/O blocking [13].
Reference: [10] <author> Lim, S. B., and Condry, M. W., </author> <title> ``Supercomputing Application Access Characteristics,'' </title> <type> Technical Report No. </type> <institution> UTUCDCS-R-91-1708, University of Illinois at Urbana-Champaign, </institution> <month> October </month> <year> 1991. </year>
Reference-contexts: Related Research The analysis of application I/O has been seriously addressed in two recent studies by Miller and Katz [12] and Lim and Condry <ref> [10] </ref>. By recording detailed information about each I/O request (e.g., type of I/O, request size, number of bytes transferred, file accessed, and file position), they were able to construct run-time profiles depicting an application's I/O behavior over time. <p> This strong correlation between CPU time and these I/O measures is compatible with the hypothesis that griz.exe contains a regular, repetitive I/O processing phase, like those observed in <ref> [10, 12] </ref>. Regression results for all selected applications are provided in Appendix C. Table C.1 shows results for the standard linear model Number_of_Bytes_Transferred = c + a CPU_Time We have grouped the 24 applications into three categories based on the regression results.
Reference: [11] <author> Martin, J. L., </author> <title> ``Supercomputer Performance Evaluation: The Comparative Analysis of High-Speed Architectures Against Their Applications,'' Performance Evaluation of Supercomputers, </title> <editor> J. L. Martin, </editor> <publisher> ed., </publisher> <pages> pp. 3-19, </pages> <publisher> North-Holland, </publisher> <year> 1988. </year>
Reference-contexts: The sheer volume of this data intensifies I/O demands within the local system and communication requirements across networks [16]. Continuing to increase CPU speeds and to further exploit parallelism without improving the I/O system will create more I/O bound jobs which can become a bottleneck to system performance <ref> [1, 8, 9, 11, 12, 18] </ref>. In a recent study of the San Diego Supercomputer Center (SDSC), a monthly CPU idle time increase of 66% was directly attributed to I/O blocking [13].
Reference: [12] <author> Miller, E. L. and Katz, R. H., </author> <booktitle> ``Input/Output Behavior of Supercomputing Applications,'' Proceedings of Supercomputing '91, </booktitle> <month> November </month> <year> 1991. </year>
Reference-contexts: The sheer volume of this data intensifies I/O demands within the local system and communication requirements across networks [16]. Continuing to increase CPU speeds and to further exploit parallelism without improving the I/O system will create more I/O bound jobs which can become a bottleneck to system performance <ref> [1, 8, 9, 11, 12, 18] </ref>. In a recent study of the San Diego Supercomputer Center (SDSC), a monthly CPU idle time increase of 66% was directly attributed to I/O blocking [13]. <p> We believe that it is necessary to undertake a thorough study of production workloads in order to extract the characteristic behavior of I/O intensive applications. 2.1. Related Research The analysis of application I/O has been seriously addressed in two recent studies by Miller and Katz <ref> [12] </ref> and Lim and Condry [10]. By recording detailed information about each I/O request (e.g., type of I/O, request size, number of bytes transferred, file accessed, and file position), they were able to construct run-time profiles depicting an application's I/O behavior over time. <p> On a real memory machine like the Cray Y-MP, the entire application and its data set must be resident in memory during execution. Since data sets often eclipse the maximum available memory partition, the technique of data overlaying or data staging is used <ref> [12] </ref>. At regular intervals in the computation or at each iteration of a looping construct, processing is temporarily suspended while the entire in-core data set is replaced with a new data set. <p> As a result of this data swapping, a cyclic pattern of same size I/O requests is generated by the application. The same is also true when checkpointing is performed <ref> [12] </ref>. Checkpointing involves writing a portion of the in-core data set to disk to save the state of the computation should the system fail. If failure does occur, the application can then be restarted from the last checkpoint, with minimal recomputation. <p> This strong correlation between CPU time and these I/O measures is compatible with the hypothesis that griz.exe contains a regular, repetitive I/O processing phase, like those observed in <ref> [10, 12] </ref>. Regression results for all selected applications are provided in Appendix C. Table C.1 shows results for the standard linear model Number_of_Bytes_Transferred = c + a CPU_Time We have grouped the 24 applications into three categories based on the regression results. <p> The stability in logical I/O rates across different execution times may represent the dominanting effects of a highly regular main processing phase within the application. Through dynamic profiling, Miller and Katz <ref> [12] </ref> observed a class of scientific applications whose main processing phase consisted of a period of CPU processing followed by a burst of intense I/O activity, which repeated at regular intervals.
Reference: [13] <author> Moore, R., </author> <title> ``File Servers, Supercomputers, </title> <booktitle> and Networking,'' Proceedings of the NSSDC Conference on Mass Storage Systems and Technologies for Space and Earth Science Applications, </booktitle> <year> 1991. </year>
Reference-contexts: In a recent study of the San Diego Supercomputer Center (SDSC), a monthly CPU idle time increase of 66% was directly attributed to I/O blocking <ref> [13] </ref>. <p> To support the I/O requirements of a combined interactive and production workload, the storage system for SDSC's Cray Y-MP is a 5-level buffer and cache hierarchy. Table 1 details the levels in this hierarchy and their characteristics <ref> [13] </ref>. Table 1: SDSC Storage Hierarchy Caching Capacity Max.
Reference: [14] <author> Pasquale, J. C., Bittel, B. K., and Kraiman, D. J., </author> <title> ``A Static and Dynamic Workload Characterization Study of the San Diego Supercomputer Center CRAY X-MP,'' </title> <booktitle> Proceedings of the 1991 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pp. 218-219, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Even though it represents an extremely small portion of all executed jobs (5%), it accounts for 92% of total CPU time and 88% of total bytes transferred. The results of this workload characterization study are consistent with an earlier study of the SDSC workload <ref> [14] </ref> where it was found that user applications represented 7% of the workload and consumed 90% of total CPU time and 75% of I/O channel time. 4.3.
Reference: [15] <author> Serazzi, G., </author> <title> ``A Functional and Resource-Oriented Procedure for Workload Modeling,'' </title> <booktitle> Performance '81, </booktitle> <pages> pp. 345-361, </pages> <publisher> North-Holland, </publisher> <year> 1981. </year>
Reference-contexts: Although the CSA data only provides aggregate resource usage, it enabled us to analyze the workload at two important levels, the functional level and the physical resource level [3, 5, 7]. Serazzi <ref> [15] </ref> showed that reliable workload characterizations can capture the functionality of the real system workload while still preserving the underlying physical resource usage.
Reference: [16] <author> Stonebraker, M., and Dozier, J., </author> <title> ``Large Capacity Object Servers to Support Global Change Research,'' Project Sequoia 2000 Report #91/1, </title> <institution> University of California, Berkeley, </institution> <month> July 23, </month> <year> 1991. </year>
Reference-contexts: The sheer volume of this data intensifies I/O demands within the local system and communication requirements across networks <ref> [16] </ref>. Continuing to increase CPU speeds and to further exploit parallelism without improving the I/O system will create more I/O bound jobs which can become a bottleneck to system performance [1, 8, 9, 11, 12, 18].
Reference: [17] <author> Williams, E., </author> <title> ``The Effects of Operating Systems on Supercomputer Performance,'' Performance Evaluation of Supercomputers, </title> <editor> J. L. Martin, </editor> <publisher> ed., </publisher> <pages> pp. 69-81, </pages> <publisher> North-Holland, </publisher> <year> 1988. </year>
Reference-contexts: 1. Introduction Past efforts to develop high performance computer systems have been primarily focused on the computational speeds of processors, often ignoring other important system components, such as the I/O subsystem and the operating system <ref> [3, 8, 17, 19] </ref>. Resulting progress in the areas of raw processor speed and parallelism, both in hardware and software, has produced GFLOPS machines, but has done little to close the ever widening gap between CPU performance and that of the attached I/O subsystem [1, 8, 9].
Reference: [18] <author> Lazowska, E. D., and Sevcik, K. C., co-chairs, </author> <title> ``Report of the Workshop on Scientific Computing Performance Analysis,'' </title> <institution> Division of Advanced Scientific Computing, NSF, Boulder, Colorado, </institution> <month> August 29-31, 1898. </month>
Reference-contexts: The sheer volume of this data intensifies I/O demands within the local system and communication requirements across networks [16]. Continuing to increase CPU speeds and to further exploit parallelism without improving the I/O system will create more I/O bound jobs which can become a bottleneck to system performance <ref> [1, 8, 9, 11, 12, 18] </ref>. In a recent study of the San Diego Supercomputer Center (SDSC), a monthly CPU idle time increase of 66% was directly attributed to I/O blocking [13].
Reference: [19] <author> Committee on Supercomputer Performance and Development, </author> <title> ``An Agenda for Improved Evaluation of Supercomputer Performance,'' </title> <institution> National Research Council, </institution> <address> Washington, D.C., 1986. 11 d d </address>
Reference-contexts: 1. Introduction Past efforts to develop high performance computer systems have been primarily focused on the computational speeds of processors, often ignoring other important system components, such as the I/O subsystem and the operating system <ref> [3, 8, 17, 19] </ref>. Resulting progress in the areas of raw processor speed and parallelism, both in hardware and software, has produced GFLOPS machines, but has done little to close the ever widening gap between CPU performance and that of the attached I/O subsystem [1, 8, 9].
References-found: 19

