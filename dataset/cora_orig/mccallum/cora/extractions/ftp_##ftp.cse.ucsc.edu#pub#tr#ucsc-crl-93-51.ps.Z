URL: ftp://ftp.cse.ucsc.edu/pub/tr/ucsc-crl-93-51.ps.Z
Refering-URL: ftp://ftp.cse.ucsc.edu/pub/tr/README.html
Root-URL: http://www.cse.ucsc.edu
Title: Learning Binary Relations Using Weighted Majority Voting  
Author: Sally A. Goldman Manfred K. Warmuth 
Note: Net address: manfred@cs.ucsc.edu. Mailing address:  95064. This author was supported by ONR grant NO0014-91-J-1162 and NSF grant IRI-9123692.  
Address: Santa Cruz, CA 95064 USA  
Affiliation: Baskin Center for Computer Engineering Information Sciences University of California, Santa Cruz  Department of Computer Science, University of California, Santa Cruz, California  
Date: December 29, 1993  
Pubnum: UCSC-CRL-93-51  
Abstract: In this paper we apply a weighted majority voting algorithm to the problem of learning a binary relation between two sets of objects. When using exponentially many weights, the mistake bound of the algorithm is essentially optimal. We present a construction where a number of copies of our algorithm divide the problem amongst themselves and learn the relation cooperatively. In this construction the total number of weights is polynomial. The mistake bounds are non-optimal (at least when compared to the best bound obtainable when computational resources are ignored) but significantly improve previous mistake bound bounds achieved by polynomial algorithms. Moreover our method can handle noise, which widens the applicability of the results. fl Net address: sg@cs.wustl.edu. Mailing address: Department of Computer Science, Washington University, St. Louis, Missouri 63130. This author was supported in part by NSF grant CCR-91110108.
Abstract-found: 1
Intro-found: 1
Reference: [Ang88] <author> Dana Angluin. </author> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 319-342, </pages> <year> 1988. </year>
Reference-contexts: The learning session continues until the learner has predicted each matrix entry. The goal of the learner is to make as few mistakes as possible. Since the number of binary relations is at most 2 km k n the standard halving algorithm <ref> [BF72, Lit88, Ang88] </ref> makes at most km + n lg k mistakes 2 . Observe that the halving algorithm can be viewed as keeping 2 km k n weights, one weight per possible binary relation.
Reference: [BF72] <author> J. Barzdin and R. Freivald. </author> <title> On the prediction of general recursive functions. </title> <journal> Soviet Mathematics Doklady, </journal> <volume> 13 </volume> <pages> 1224-1228, </pages> <year> 1972. </year>
Reference-contexts: The learning session continues until the learner has predicted each matrix entry. The goal of the learner is to make as few mistakes as possible. Since the number of binary relations is at most 2 km k n the standard halving algorithm <ref> [BF72, Lit88, Ang88] </ref> makes at most km + n lg k mistakes 2 . Observe that the halving algorithm can be viewed as keeping 2 km k n weights, one weight per possible binary relation.
Reference: [CBFH + 93] <author> Nicolo Cesa-Bianchi, Yoav Freund, David P. Helmbold, David Haussler, Robert E. Schapire, and Manfred K. Warmuth. </author> <title> How to use expert advice. </title> <booktitle> In Proceedings of the Twenty Fifth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 382-391, </pages> <month> May </month> <year> 1993. </year> <note> References 17 </note>
Reference-contexts: Such unknown ("sleeping") entries are naturally set to 1=2, leading to split votes. There are many relatives of the basic weighted majority algorithm that we could use within our two constructions such as a probabilistic variant due to Vovk [Vov90] (see also Cesa-Bianchi et al. <ref> [CBFH + 93] </ref>). <p> Finally, by applying the results of Cesa-Bianchi et al. <ref> [CBFH + 93] </ref>) we can tune fi as a function of an upper bound ff on the noise. Lemma 1: [CBFH + 93] For any real value z 0 1 + z ln 1 2 ln 2 z + z + 2 ln 2 where g (z) = 1 2 p <p> Finally, by applying the results of Cesa-Bianchi et al. <ref> [CBFH + 93] </ref>) we can tune fi as a function of an upper bound ff on the noise. Lemma 1: [CBFH + 93] For any real value z 0 1 + z ln 1 2 ln 2 z + z + 2 ln 2 where g (z) = 1 2 p z and g (0) = 0. <p> most ff and k p denotes the size and ff p the noise of partition p. 4 We also can get rid of the factor 2 in the first formulation of the bound by either letting the algorithm predict probabilistically in f0; 1g or deterministically in the interval [0; 1] <ref> [CBFH + 93, KW93] </ref>. 8 4. Construction Two: A Polynomial-time Algorithm For the above tuning we needed an upper bound for both the size and the noise of the partition. <p> If an upper bound for only one the two is known, then a standard doubling trick can be used to guess the other. This causes only a slight increase in the mistake bound (See Cesa-Bianchi et al. <ref> [CBFH + 93] </ref>.) Note that in the above mistake bound there is a subtle tradeoff between the noise ff p and size k p of a partition p. We recall that when this first construction is applied in the noise-free case that it essentially matches the information-theoretic lower bound. <p> Construction Two: A Polynomial-time Algorithm 15 We now apply the results of Cesa-Bianchi et al. <ref> [CBFH + 93] </ref> to tune fi for the more general case in which the relation is not pure. <p> Nevertheless, a simple modification of the projective geometry lower bound of Goldman et al. [GRS93] can be 5 Again, we can get rid of the factor 2 in this mistake bound by either letting the algorithm predict probabilistically in f0; 1g or deterministically in the interval [0; 1] <ref> [CBFH + 93, KW93] </ref>. 16 5. Concluding Remarks used to show an (n p m) lower bound for m n on the number of prediction mistakes by our algorithm. <p> Our first construction uses exponentially many weights. In the noise-free case this construction is essentially optimal. We believe that by proving lower bounds for the noisy case (possibly using the techniques developed in <ref> [CBFH + 93] </ref>) one can show that the tuned version of the first contruction (Theorem 1) is close to optimal in the more general case as well.
Reference: [Che92] <author> William Chen, </author> <year> 1992. </year> <type> Personal communication. </type>
Reference-contexts: For this class of algorithms they show a lower bound to (n p m) for m n on the number of mistakes that any algorithm must make. Recently, William Chen <ref> [Che92] </ref> has extended their proof to obtain a lower bound of (n p m lg k) for m n lg k.
Reference: [GRS93] <author> Sally A. Goldman, Ronald L. Rivest, and Robert E. Schapire. </author> <title> Learning binary relations and total orders. </title> <journal> SIAM Journal of Computing, </journal> <volume> 22 </volume> <pages> 1006-1034, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: 1 Introduction In this paper we demonstrate how weighted majority voting can be applied to obtain robust algorithms for learning binary relations. Following Goldman, Rivest and Schapire <ref> [GRS93] </ref>, a binary relation is defined between two sets of objects, one of cardi-nality n and the other of cardinality m. For all possible pairings of objects, there is a predicate relating the two sets of variables that is either true (1) or false (0). <p> Note that the mistake bound of this algorithm is essentially optimal since Goldman et al. <ref> [GRS93] </ref> prove an information-theoretic lower bound of km + (n k)blg kc mistakes. While this construction has assumed that an upper bound on k is known, if no such bound is provided the standard doubling trick can be applied. <p> Here k is the size of the smallest partition consistent with the whole matrix. The best previous bound for a polynomial algorithm was km + n p (k 1)m <ref> [GRS93] </ref>. An interesting aspect of our problem besides its parallel nature is that when node r is to predict for entry M rj then not all other n 1 entries in the j-th column may have been uncovered. <p> To motivate this problem we briefly review the allergist example given by Goldman et al. <ref> [GRS93] </ref>. Consider an allergist with a set of patients to be tested for a given set of allergens. Each patient is either highly allergic, mildly allergic, or not allergic to any given allergen. <p> The key to our approach is to view minor discrepancies between the row templates and the actual rows as noise. This greatly reduces the mistake bounds that one can obtain when using the original formulation of Goldman et al. <ref> [GRS93] </ref> by reducing the number of row types. The robust nature of the weighted majority algorithm enables us to handle noise. <p> that ff p = n, then the number of mistakes is at most k p m + 3mn 2 lg k + 4mn 2 + mn 2 24 ln k: In addition to presenting their algorithm to make at most km + n p (k 1)m mistakes, Goldman et al. <ref> [GRS93] </ref> present an information-theoretic lower bound for a class of algorithms that they call row-filter algorithms. <p> Nevertheless, a simple modification of the projective geometry lower bound of Goldman et al. <ref> [GRS93] </ref> can be 5 Again, we can get rid of the factor 2 in this mistake bound by either letting the algorithm predict probabilistically in f0; 1g or deterministically in the interval [0; 1] [CBFH + 93, KW93]. 16 5.
Reference: [KW93] <author> Jyrki Kivinen and Manfred K. Warmuth. </author> <title> Using Experts for Predicting Continuous Outcomes. </title> <month> Eurocolt </month> <year> 1993. </year>
Reference-contexts: There are many relatives of the basic weighted majority algorithm that we could use within our two constructions such as a probabilistic variant due to Vovk [Vov90] (see also Cesa-Bianchi et al. [CBFH + 93]). Also the Vee algorithm of Kivinen and Warmuth <ref> [KW93] </ref> handles inputs and predictions in [0,1] and the algorithm of Littlestone, Long and War-muth [LLW91] has small square loss against the best convex combination of the inputs. (Incidentally all these on-line prediction algorithms have multiplicative weight updates in common.) We chose the simplest setting for showing the usefulness of our <p> most ff and k p denotes the size and ff p the noise of partition p. 4 We also can get rid of the factor 2 in the first formulation of the bound by either letting the algorithm predict probabilistically in f0; 1g or deterministically in the interval [0; 1] <ref> [CBFH + 93, KW93] </ref>. 8 4. Construction Two: A Polynomial-time Algorithm For the above tuning we needed an upper bound for both the size and the noise of the partition. <p> Nevertheless, a simple modification of the projective geometry lower bound of Goldman et al. [GRS93] can be 5 Again, we can get rid of the factor 2 in this mistake bound by either letting the algorithm predict probabilistically in f0; 1g or deterministically in the interval [0; 1] <ref> [CBFH + 93, KW93] </ref>. 16 5. Concluding Remarks used to show an (n p m) lower bound for m n on the number of prediction mistakes by our algorithm.
Reference: [Lit88] <author> Nick Littlestone. </author> <title> Learning when irrelevant attributes abound: A new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 285-318, </pages> <year> 1988. </year>
Reference-contexts: We shall study the problem of learning binary relations under the standard on-line (or incremental) learning model <ref> [Lit89, Lit88] </ref>. The learning session consists of a sequence of trials. In each trial, the learner must predict the value of some unknown matrix entry that has been selected by the adversary 1 . After predicting the learner receives the value of the matrix entry in question as feedback. <p> The learning session continues until the learner has predicted each matrix entry. The goal of the learner is to make as few mistakes as possible. Since the number of binary relations is at most 2 km k n the standard halving algorithm <ref> [BF72, Lit88, Ang88] </ref> makes at most km + n lg k mistakes 2 . Observe that the halving algorithm can be viewed as keeping 2 km k n weights, one weight per possible binary relation.
Reference: [Lit89] <author> Nicholas Littlestone. </author> <title> Mistake Bounds and Logarithmic Linear-threshold Learning algorithms. </title> <type> PhD thesis, </type> <address> U. C. Santa Cruz, </address> <month> March </month> <year> 1989. </year>
Reference-contexts: We shall study the problem of learning binary relations under the standard on-line (or incremental) learning model <ref> [Lit89, Lit88] </ref>. The learning session consists of a sequence of trials. In each trial, the learner must predict the value of some unknown matrix entry that has been selected by the adversary 1 . After predicting the learner receives the value of the matrix entry in question as feedback.
Reference: [LLW91] <author> Nicholas Littlestone, Philip M. Long, and Manfred K. Warmuth. </author> <title> On-line learning of linear functions. </title> <booktitle> In Proceedings of the Twenty Third Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 465-475, </pages> <month> May </month> <year> 1991. </year> <note> To appear in Journal of Computational Complexity. </note>
Reference-contexts: Also the Vee algorithm of Kivinen and Warmuth [KW93] handles inputs and predictions in [0,1] and the algorithm of Littlestone, Long and War-muth <ref> [LLW91] </ref> has small square loss against the best convex combination of the inputs. (Incidentally all these on-line prediction algorithms have multiplicative weight updates in common.) We chose the simplest setting for showing the usefulness of our method: the entries of the matrix are binary and the predictions must be deterministic. 3.
Reference: [LW89] <author> Nick Littlestone and Manfred K. Warmuth. </author> <title> The weighted majority algorithm. </title> <booktitle> In 30th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 256-261, </pages> <month> October </month> <year> 1989. </year> <note> To appear in Information and Computation. </note>
Reference-contexts: Thus we are interested in algorithms that use a small number of weights in representing their hypotheses. The algorithms we present update the weights according to a variant of the weighted majority algorithm of Littlestone and Warmuth <ref> [LW89] </ref> called WMG. We view WMG as a node that is connected to each of its inputs by a weighted edge. The inputs are in the interval [0; 1]. An input x of weight w votes with xw for 1 and (1 x)w for 0. <p> The mistake bound of Theorem 2 obtained for this algorithm is larger than the mistake bound of the first construction. However this algorithm uses only n weights as opposed to exponentially many. We begin by giving an update that is equivalent to the one used in WMG <ref> [LW89] </ref>. Recall that in WMG if x is the prediction of an input with weight w, then if the feedback is the bit then w is multiplied by 1 (1 fi)jx j for fi 2 [0; 1).
Reference: [Vov90] <author> Volodimir G. Vovk. </author> <title> Aggregating strategies. </title> <booktitle> In Proceedings of the Third Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 371-383. </pages> <publisher> Morgan Kauf-mann, </publisher> <month> August </month> <year> 1990. </year> <note> 18 References </note>
Reference-contexts: Such unknown ("sleeping") entries are naturally set to 1=2, leading to split votes. There are many relatives of the basic weighted majority algorithm that we could use within our two constructions such as a probabilistic variant due to Vovk <ref> [Vov90] </ref> (see also Cesa-Bianchi et al. [CBFH + 93]).
References-found: 11

