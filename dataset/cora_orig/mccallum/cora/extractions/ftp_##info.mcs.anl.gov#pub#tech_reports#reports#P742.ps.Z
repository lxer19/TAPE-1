URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P742.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts99.htm
Root-URL: http://www.mcs.anl.gov
Email: luskg@mcs.anl.gov  
Title: Achieving High Performance with MPI-IO  
Author: Rajeev Thakur William Gropp Ewing Lusk fthakur, gropp, 
Note: Preliminary versions of portions of this work were published in two conference papers [36, 37].  
Date: February 1999  
Address: Argonne, IL 60439, USA  
Affiliation: Mathematics and Computer Science Division Argonne National Laboratory  
Pubnum: Preprint ANL/MCS-P742-0299  
Abstract: The I/O access patterns of many parallel applications consist of accesses to a large number of small, noncontiguous pieces of data. If an application's I/O needs are met by making many small, distinct I/O requests, however, the I/O performance degrades drastically. To avoid this problem, MPI-IO allows users to access noncontiguous data with a single I/O function call, unlike in Unix I/O. In this paper, we explain how critical this feature of MPI-IO is for high performance and how it enables implementations to perform optimizations. An application can be written in many different ways with MPI-IO. We classify the different ways of expressing an application's I/O access pattern in MPI-IO into four levels, called level 0 through level 3. We demonstrate that, for applications with noncontiguous access patterns, the I/O performance improves significantly if users write the application such that it makes level-3 MPI-IO requests (noncontiguous, collective) rather than level-0 requests (Unix style). We describe how our MPI-IO implementation, ROMIO, delivers high performance for noncontiguous requests. We explain in detail the two key optimizations ROMIO performs: data sieving for noncontiguous requests from one process and collective I/O for noncontiguous requests from multiple processes. We describe how one can implement these optimizations portably on multiple machines and file systems, control their memory requirements, and also achieve high performance. We demonstrate the performance and portability with performance results for three applications|an astrophysics-application template (DIST3D), the NAS BTIO benchmark, and an unstructured code (UNSTRUC)|on five different parallel machines: HP Exemplar, IBM SP, Intel Paragon, NEC SX-4, and SGI Origin2000. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Baylor and C. Wu. </author> <title> Parallel I/O Workload Characteristics Using Vesta. </title> <editor> In R. Jain, J. Werth, and J. Browne, editors, </editor> <booktitle> Input/Output in Parallel and Distributed Computer Systems, chapter 7, </booktitle> <pages> pages 167-185. </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1996. </year>
Reference-contexts: The small I/O requests made by parallel programs are a result of the combination of two factors: 1. In many parallel applications, each process needs to access a large number of relatively small pieces of data that are not contiguously located in the file <ref> [1, 6, 20, 29, 30, 35] </ref>. 2. <p> MPI_File_open (MPI_COMM_WORLD, "filename", ..., &fh) MPI_File_open (MPI_COMM_WORLD, "filename", ..., &fh) File Space collective contiguous requests (level 1) 0 1 2 3 Processes request (level 0) independent contiguous independent, noncontiguous request using a derived datatype (level 2) collective, noncontiguous requests using derived datatypes (level 3) 9 array_of_gsizes [0] = num_global_rows; array_of_gsizes <ref> [1] </ref> = num_global_cols; array_of_distribs [0] = array_of_distribs [1] = MPI_DISTRIBUTE_BLOCK; array_of_dargs [0] = array_of_dargs [1] = MPI_DISTRIBUTE_DFLT_DARG; array_of_psizes [0] = array_of_psizes [1] = 4; MPI_Comm_rank (MPI_COMM_WORLD, &mynode); MPI_Type_create_darray (16, mynode, 2, array_of_gsizes, array_of_distribs, array_of_dargs, array_of_psizes, MPI_ORDER_C, MPI_FLOAT, &filetype); MPI_Type_commit (&filetype); local_array_size = num_local_rows * num_local_cols; MPI_File_open (MPI_COMM_WORLD, "/pfs/test", MPI_MODE_CREATE | MPI_MODE_RDWR, <p> "filename", ..., &fh) File Space collective contiguous requests (level 1) 0 1 2 3 Processes request (level 0) independent contiguous independent, noncontiguous request using a derived datatype (level 2) collective, noncontiguous requests using derived datatypes (level 3) 9 array_of_gsizes [0] = num_global_rows; array_of_gsizes <ref> [1] </ref> = num_global_cols; array_of_distribs [0] = array_of_distribs [1] = MPI_DISTRIBUTE_BLOCK; array_of_dargs [0] = array_of_dargs [1] = MPI_DISTRIBUTE_DFLT_DARG; array_of_psizes [0] = array_of_psizes [1] = 4; MPI_Comm_rank (MPI_COMM_WORLD, &mynode); MPI_Type_create_darray (16, mynode, 2, array_of_gsizes, array_of_distribs, array_of_dargs, array_of_psizes, MPI_ORDER_C, MPI_FLOAT, &filetype); MPI_Type_commit (&filetype); local_array_size = num_local_rows * num_local_cols; MPI_File_open (MPI_COMM_WORLD, "/pfs/test", MPI_MODE_CREATE | MPI_MODE_RDWR, MPI_INFO_NULL, &fh); MPI_File_set_view (fh, 0, MPI_FLOAT, filetype, <p> requests (level 1) 0 1 2 3 Processes request (level 0) independent contiguous independent, noncontiguous request using a derived datatype (level 2) collective, noncontiguous requests using derived datatypes (level 3) 9 array_of_gsizes [0] = num_global_rows; array_of_gsizes <ref> [1] </ref> = num_global_cols; array_of_distribs [0] = array_of_distribs [1] = MPI_DISTRIBUTE_BLOCK; array_of_dargs [0] = array_of_dargs [1] = MPI_DISTRIBUTE_DFLT_DARG; array_of_psizes [0] = array_of_psizes [1] = 4; MPI_Comm_rank (MPI_COMM_WORLD, &mynode); MPI_Type_create_darray (16, mynode, 2, array_of_gsizes, array_of_distribs, array_of_dargs, array_of_psizes, MPI_ORDER_C, MPI_FLOAT, &filetype); MPI_Type_commit (&filetype); local_array_size = num_local_rows * num_local_cols; MPI_File_open (MPI_COMM_WORLD, "/pfs/test", MPI_MODE_CREATE | MPI_MODE_RDWR, MPI_INFO_NULL, &fh); MPI_File_set_view (fh, 0, MPI_FLOAT, filetype, "native", MPI_INFO_NULL); MPI_File_read_all (fh, local_array, local_array_size, MPI_FLOAT, <p> Processes request (level 0) independent contiguous independent, noncontiguous request using a derived datatype (level 2) collective, noncontiguous requests using derived datatypes (level 3) 9 array_of_gsizes [0] = num_global_rows; array_of_gsizes <ref> [1] </ref> = num_global_cols; array_of_distribs [0] = array_of_distribs [1] = MPI_DISTRIBUTE_BLOCK; array_of_dargs [0] = array_of_dargs [1] = MPI_DISTRIBUTE_DFLT_DARG; array_of_psizes [0] = array_of_psizes [1] = 4; MPI_Comm_rank (MPI_COMM_WORLD, &mynode); MPI_Type_create_darray (16, mynode, 2, array_of_gsizes, array_of_distribs, array_of_dargs, array_of_psizes, MPI_ORDER_C, MPI_FLOAT, &filetype); MPI_Type_commit (&filetype); local_array_size = num_local_rows * num_local_cols; MPI_File_open (MPI_COMM_WORLD, "/pfs/test", MPI_MODE_CREATE | MPI_MODE_RDWR, MPI_INFO_NULL, &fh); MPI_File_set_view (fh, 0, MPI_FLOAT, filetype, "native", MPI_INFO_NULL); MPI_File_read_all (fh, local_array, local_array_size, MPI_FLOAT, &status); MPI_File_close (&fh); cases, as level-0 requests <p> Most real parallel applications, however, do not fall into this category. Several studies of I/O access patterns in parallel applications <ref> [1, 6, 20, 29, 30, 35] </ref> have shown that each process in a parallel program may need to access a number of relatively small, noncontiguous portions of a file.
Reference: [2] <author> J. Bruno and P. Cappello. </author> <title> Implementing the Beam and Warming Method on the Hypercube. </title> <booktitle> In Proceedings of the Third Conference on Hypercube Concurrent Computers and Applications, </booktitle> <month> January </month> <year> 1988. </year>
Reference-contexts: The benchmark performs only writes, but we modified it to perform reads also, in order to measure the read bandwidths. In BTIO, a three-dimensional array (actually four-dimensional, but the first dimension has only five elements and is not distributed) is distributed among processes by using a multipartition distribution <ref> [2] </ref>. In this distribution, each process is responsible for several disjoint subblocks of points (cells) of the grid.
Reference: [3] <author> P. Cao, E. Felten, A. Karlin, and K. Li. </author> <title> Implementation and Performance of Integrated Application-Controlled File Caching, Prefetching, and Disk Scheduling. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 14(4) </volume> <pages> 311-343, </pages> <month> November </month> <year> 1996. </year>
Reference-contexts: Examples of hints include parameters for file striping, prefetching/caching information, and access-pattern information. Hints do not affect the semantics of a program, but they may enable the MPI-IO implementation or underlying file system to improve performance or minimize the use of system resources <ref> [3, 22] </ref>. The implementation, however, is free to ignore all hints. 4 MPI-IO also has a set of rigorously defined consistency and atomicity semantics that specify the results of concurrent file accesses. For details of all these features, we refer readers to [11, 18].
Reference: [4] <author> P. Corbett, D. Feitelson, Y. Hsu, J. Prost, M. Snir, S. Fineberg, B. Nitzberg, B. Traver-sat, and P. Wong. </author> <title> MPI-IO: A Parallel I/O Interface for MPI, </title> <note> Version 0.2. Technical Report IBM Research Report RC 19841(87784), </note> <institution> IBM T. J. Watson Research Center, </institution> <month> November </month> <year> 1994. </year>
Reference-contexts: The idea of using message-passing concepts in an I/O library appeared successful, and the effort was expanded into a collaboration with parallel-I/O researchers from NASA Ames Research Center. The resulting specification appeared in <ref> [4] </ref>. At this point a large email discussion group was formed, with participation from a wide variety of institutions. This group, calling itself the MPI-IO Committee, pushed the idea further in a series of proposals, culminating in [40]. During this time, the MPI Forum had resumed meeting.
Reference: [5] <author> P. Corbett, J. Prost, C. Demetriou, G. Gibson, E. Reidel, J. Zelenka, Y. Chen, E. Felten, K. Li, J. Hartman, L. Peterson, B. Bershad, A. Wolman, and R. Aydt. </author> <title> Proposal for a Common Parallel File System Programming Interface, </title> <note> Version 1.0. On the World-Wide Web at http://www.cs.arizona.edu/sio/api1.0.ps.gz, September 1996. </note>
Reference-contexts: By following such an approach, we achieved portability with very low overhead [34]. Now that MPI-IO has emerged as the standard, we use ADIO as a mechanism for implementing MPI-IO, as illustrated in Figure 1. We can easily implement MPI-IO on other file systems (such as <ref> [5, 10, 12, 16, 19] </ref>) simply by implementing ADIO on those file systems. The MPI-2 chapter on external interfaces defines a set of functions that provide access to some of the internal data structures of the MPI implementation.
Reference: [6] <author> P. Crandall, R. Aydt, A. Chien, and D. Reed. </author> <title> Input-Output Characteristics of Scalable Parallel Applications. </title> <booktitle> In Proceedings of Supercomputing '95. </booktitle> <publisher> ACM Press, </publisher> <month> December </month> <year> 1995. </year>
Reference-contexts: The small I/O requests made by parallel programs are a result of the combination of two factors: 1. In many parallel applications, each process needs to access a large number of relatively small pieces of data that are not contiguously located in the file <ref> [1, 6, 20, 29, 30, 35] </ref>. 2. <p> Most real parallel applications, however, do not fall into this category. Several studies of I/O access patterns in parallel applications <ref> [1, 6, 20, 29, 30, 35] </ref> have shown that each process in a parallel program may need to access a number of relatively small, noncontiguous portions of a file.
Reference: [7] <author> J. del Rosario, R. Bordawekar, and A. Choudhary. </author> <title> Improved Parallel I/O via a Two-Phase Run-time Access Strategy. </title> <booktitle> In Proceedings of the Workshop on I/O in Parallel Computer Systems at IPPS '93, </booktitle> <pages> pages 56-70, </pages> <month> April </month> <year> 1993. </year> <note> Also published in Computer Architecture News, </note> <month> 21(5) </month> <pages> 31-38, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: The paper also explains why performance improves when users use MPI-IO the right way: the MPI-IO implementation can then perform optimizations, such as data sieving and collective I/O. Although these optimizations have been proposed earlier <ref> [7, 15, 28, 33] </ref>, this is the only paper that discusses in detail the practical issues involved in implementing these optimizations, in the context of a standard, portable API, on real state-of-the-art parallel machines and file systems. <p> Collective I/O can be performed in different ways and has been studied by many researchers in recent years. It can be done at the disk level (disk-directed I/O [15]), at the server level (server-directed I/O [27, 28]), or at the client level (two-phase I/O <ref> [7] </ref> or collective buffering [21]). Each method has its merits and disadvantages. Since ROMIO is a portable, user-level library with no separate I/O servers, it performs collective I/O at the client level. For this purpose, it uses a generalized version of the extended two-phase method described in [32]. <p> Most level-1 requests do not contain enough information for ROMIO to perform optimizations, and ROMIO therefore implements them internally as level-0 requests. Some level-1 requests, such as those that represent a read-broadcast type of access pattern, are optimized collectively, however. 6.1 Two-Phase I/O Two-phase I/O was first proposed in <ref> [7] </ref> in the context of accessing distributed arrays from files. Consider the example of reading a two-dimensional array from a file into a (block,block) distribution in memory, as shown in Figure 7. Assume that the array is stored in the file in row-major order. <p> The basic two-phase method was extended in [32] to access sections of out-of-core arrays. Since MPI-IO is a general parallel-I/O interface, I/O requests in MPI-IO can represent any access pattern, not just sections of arrays. The two-phase method in <ref> [7] </ref> must therefore be generalized to handle any noncontiguous I/O request. Such a generalized implementation of two-phase I/O, described below, is used in ROMIO. Two-phase I/O does increase the memory requirements of a program.
Reference: [8] <author> P. Dickens and R. Thakur. </author> <title> Improving Collective I/O Performance Using Threads. </title> <booktitle> In Proceedings of the 13th International Parallel Processing Symposium and 10th Symposium on Parallel and Distributed Processing, </booktitle> <month> April </month> <year> 1999. </year> <note> To appear. </note>
Reference-contexts: The most natural way to implement split collective I/O in a nonblocking fashion is to spawn a thread that performs the entire collective-I/O operation in the background. The results in <ref> [8] </ref>, however, indicate that, on most machines, this approach performs much worse than if collective I/O is done entirely in the main thread during the begin function.
Reference: [9] <author> S. Fineberg, P. Wong, B. Nitzberg, and C. Kuszmaul. </author> <title> PMPIO|A Portable Implementation of MPI-IO. </title> <booktitle> In Proceedings of the Sixth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 188-195. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1996. </year>
Reference-contexts: This interface is commonly referred to as MPI-IO. MPI-IO is a rich interface with many features designed specifically for performance and portability. Multiple implementations of MPI-IO, both portable and machine specific, are available <ref> [9, 14, 24, 26, 39] </ref>. To avoid the abovementioned problem of many distinct, small I/O requests, MPI-IO allows users to specify the entire noncontiguous access pattern and read or write all the data with a single I/O function call. <p> The three applications we used are the following: 1. DIST3D, a template representing the I/O access pattern in an astrophysics application, ASTRO3D [35], from the University of Chicago; 2. the NAS BTIO benchmark <ref> [9] </ref>; and 3. an unstructured code (which we call UNSTRUC) written by Larry Schoof and Wilbur Johnson of Sandia National Laboratories. 1.1 Contributions of this Paper This paper demonstrates how users can achieve high parallel-I/O performance by using a good API and by using that API the right way. <p> It measures the performance of reading/writing a three-dimensional array distributed in a (block,block,block) fashion among processes from/to a file containing the global array in row-major order. The second application is the BTIO benchmark <ref> [9] </ref> from NASA Ames Research Center, which simulates the I/O required by a time-stepping flow solver that periodically writes its solution matrix. The benchmark performs only writes, but we modified it to perform reads also, in order to measure the read bandwidths.
Reference: [10] <author> G. Gibson, D. Stodolsky, P. Chang, W. Courtwright II, C. Demetriou, E. Ginting, M. Holland, Q. Ma, L. Neal, R. Patterson, J. Su, R. Youssef, and J. Zelenka. </author> <title> The Scotch Parallel Storage Systems. </title> <booktitle> In Proceedings of 40th IEEE Computer Society International Conference (COMPCON 95), </booktitle> <pages> pages 403-410. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> Spring </month> <year> 1995. </year>
Reference-contexts: By following such an approach, we achieved portability with very low overhead [34]. Now that MPI-IO has emerged as the standard, we use ADIO as a mechanism for implementing MPI-IO, as illustrated in Figure 1. We can easily implement MPI-IO on other file systems (such as <ref> [5, 10, 12, 16, 19] </ref>) simply by implementing ADIO on those file systems. The MPI-2 chapter on external interfaces defines a set of functions that provide access to some of the internal data structures of the MPI implementation.
Reference: [11] <author> W. Gropp, S. Huss-Lederman, A. Lumsdaine, E. Lusk, B. Nitzberg, W. Saphir, and M. Snir. </author> <title> MPI|The Complete Reference. Volume 2, The MPI Extensions. </title> <publisher> The MIT Press, </publisher> <year> 1998. </year>
Reference-contexts: The implementation, however, is free to ignore all hints. 4 MPI-IO also has a set of rigorously defined consistency and atomicity semantics that specify the results of concurrent file accesses. For details of all these features, we refer readers to <ref> [11, 18] </ref>.
Reference: [12] <author> J. Huber, C. Elford, D. Reed, A. Chien, and D. Blumenthal. </author> <title> PPFS: A High Performance Portable Parallel File System. </title> <booktitle> In Proceedings of the 9th ACM International Conference on Supercomputing, </booktitle> <pages> pages 385-394. </pages> <publisher> ACM Press, </publisher> <month> July </month> <year> 1995. </year>
Reference-contexts: By following such an approach, we achieved portability with very low overhead [34]. Now that MPI-IO has emerged as the standard, we use ADIO as a mechanism for implementing MPI-IO, as illustrated in Figure 1. We can easily implement MPI-IO on other file systems (such as <ref> [5, 10, 12, 16, 19] </ref>) simply by implementing ADIO on those file systems. The MPI-2 chapter on external interfaces defines a set of functions that provide access to some of the internal data structures of the MPI implementation.
Reference: [13] <author> IEEE/ANSI Std. 1003.1. </author> <title> Portable Operating System Interface (POSIX)-Part 1: System Application Program Interface (API) [C Language], </title> <note> 1996 edition. </note>
Reference-contexts: However, the requests in the list can be a mixture of reads and writes, and the POSIX standard says that each of the requests will be submitted as a separate nonblocking request <ref> [13] </ref>. Therefore, POSIX implementations cannot optimize I/O for the entire list of requests, for example, by performing data sieving as described in Section 5. Furthermore, since the lio listio interface is not collective, implementations cannot perform collective I/O. 1 An application can be written in many different ways with MPI-IO.
Reference: [14] <author> T. Jones, R. Mark, J. Martin, J. May, E. Pierce, and L. Stanberry. </author> <title> An MPI-IO Interface to HPSS. </title> <booktitle> In Proceedings of the Fifth NASA Goddard Conference on Mass Storage Systems, </booktitle> <pages> pages I:37-50, </pages> <month> September </month> <year> 1996. </year>
Reference-contexts: This interface is commonly referred to as MPI-IO. MPI-IO is a rich interface with many features designed specifically for performance and portability. Multiple implementations of MPI-IO, both portable and machine specific, are available <ref> [9, 14, 24, 26, 39] </ref>. To avoid the abovementioned problem of many distinct, small I/O requests, MPI-IO allows users to specify the entire noncontiguous access pattern and read or write all the data with a single I/O function call.
Reference: [15] <author> D. Kotz. </author> <title> Disk-directed I/O for MIMD Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 15(1) </volume> <pages> 41-74, </pages> <month> February </month> <year> 1997. </year>
Reference-contexts: The paper also explains why performance improves when users use MPI-IO the right way: the MPI-IO implementation can then perform optimizations, such as data sieving and collective I/O. Although these optimizations have been proposed earlier <ref> [7, 15, 28, 33] </ref>, this is the only paper that discusses in detail the practical issues involved in implementing these optimizations, in the context of a standard, portable API, on real state-of-the-art parallel machines and file systems. <p> Collective I/O can be performed in different ways and has been studied by many researchers in recent years. It can be done at the disk level (disk-directed I/O <ref> [15] </ref>), at the server level (server-directed I/O [27, 28]), or at the client level (two-phase I/O [7] or collective buffering [21]). Each method has its merits and disadvantages. Since ROMIO is a portable, user-level library with no separate I/O servers, it performs collective I/O at the client level.
Reference: [16] <author> O. Krieger and M. Stumm. </author> <title> HFS: A Performance-Oriented Flexible File System Based on Building-Block Compositions. </title> <booktitle> In Proceedings of Fourth Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 95-108. </pages> <publisher> ACM Press, </publisher> <month> May </month> <year> 1996. </year>
Reference-contexts: By following such an approach, we achieved portability with very low overhead [34]. Now that MPI-IO has emerged as the standard, we use ADIO as a mechanism for implementing MPI-IO, as illustrated in Figure 1. We can easily implement MPI-IO on other file systems (such as <ref> [5, 10, 12, 16, 19] </ref>) simply by implementing ADIO on those file systems. The MPI-2 chapter on external interfaces defines a set of functions that provide access to some of the internal data structures of the MPI implementation.
Reference: [17] <author> Message Passing Interface Forum. </author> <title> MPI: A Message-Passing Interface Standard. </title> <note> Version 1.1, June 1995. On the World-Wide Web at http://www.mpi-forum.org/docs/ docs.html. </note>
Reference-contexts: feature|the ability to access noncontiguous data with a single I/O function by using MPI's derived datatypes|because it is critical for high performance in parallel applications. 2.3 Noncontiguous Accesses in MPI-IO In MPI-1, the amount of data a function sends or receives is specified in terms of instances of a datatype <ref> [17] </ref>. Datatypes in MPI are of two kinds: basic and derived. Basic datatypes are those that correspond to the basic datatypes in the host programming language|integers, floating-point numbers, and so forth.
Reference: [18] <author> Message Passing Interface Forum. </author> <title> MPI-2: Extensions to the Message-Passing Interface. </title> <month> July </month> <year> 1997. </year> <note> On the World-Wide Web at http://www.mpi-forum.org/docs/docs.html. </note>
Reference-contexts: To overcome the performance and portability limitations of existing parallel-I/O interfaces, the MPI Forum (made up of parallel-computer vendors, researchers, and applications scientists) defined a new interface for parallel I/O as part of the MPI-2 standard <ref> [18] </ref>. This interface is commonly referred to as MPI-IO. MPI-IO is a rich interface with many features designed specifically for performance and portability. Multiple implementations of MPI-IO, both portable and machine specific, are available [9, 14, 24, 26, 39]. <p> The MPI Forum used the latest version of the existing MPI-IO specification [40] as a starting point for the I/O chapter in MPI-2. The I/O chapter evolved over many meetings of the Forum and was released in its final form along with the rest of MPI-2 in July 1997 <ref> [18] </ref>. MPI-IO now refers to this I/O chapter in MPI-2. 2.2 Main Features of MPI-IO MPI-IO is a rich interface with many features specifically intended for portable, high-performance parallel I/O. The basic I/O functions in MPI-IO provide functionality equivalent to the Unix functions open, close, read, write, and lseek. <p> The implementation, however, is free to ignore all hints. 4 MPI-IO also has a set of rigorously defined consistency and atomicity semantics that specify the results of concurrent file accesses. For details of all these features, we refer readers to <ref> [11, 18] </ref>. <p> The process is also assured that concurrent writes from processes other than those involved in this collective-I/O operation will not occur, because MPI-IO's consistency semantics <ref> [18] </ref> do not automatically guarantee consistency for such writes. (In such cases, users must use MPI File sync and ensure that the operations are not concurrent.) 6.2.3 Performance Issues Even if I/O is performed in large contiguous chunks, the performance of the collective-I/O implementation can be significantly affected by the amount
Reference: [19] <author> N. Nieuwejaar and D. Kotz. </author> <title> The Galley Parallel File System. </title> <journal> Parallel Computing, </journal> <volume> 23(4) </volume> <pages> 447-476, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: By following such an approach, we achieved portability with very low overhead [34]. Now that MPI-IO has emerged as the standard, we use ADIO as a mechanism for implementing MPI-IO, as illustrated in Figure 1. We can easily implement MPI-IO on other file systems (such as <ref> [5, 10, 12, 16, 19] </ref>) simply by implementing ADIO on those file systems. The MPI-2 chapter on external interfaces defines a set of functions that provide access to some of the internal data structures of the MPI implementation.
Reference: [20] <author> N. Nieuwejaar, D. Kotz, A. Purakayastha, C. Ellis, and M. </author> <title> Best. File-Access Characteristics of Parallel Scientific Workloads. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 7(10) </volume> <pages> 1075-1089, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: The small I/O requests made by parallel programs are a result of the combination of two factors: 1. In many parallel applications, each process needs to access a large number of relatively small pieces of data that are not contiguously located in the file <ref> [1, 6, 20, 29, 30, 35] </ref>. 2. <p> Most real parallel applications, however, do not fall into this category. Several studies of I/O access patterns in parallel applications <ref> [1, 6, 20, 29, 30, 35] </ref> have shown that each process in a parallel program may need to access a number of relatively small, noncontiguous portions of a file.
Reference: [21] <author> B. Nitzberg and V. Lo. </author> <title> Collective Buffering: Improving Parallel I/O Performance. </title> <booktitle> In Proceedings of the Sixth IEEE International Symposium on High Performance Distributed Computing, </booktitle> <pages> pages 148-157. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> August </month> <year> 1997. </year>
Reference-contexts: Collective I/O can be performed in different ways and has been studied by many researchers in recent years. It can be done at the disk level (disk-directed I/O [15]), at the server level (server-directed I/O [27, 28]), or at the client level (two-phase I/O [7] or collective buffering <ref> [21] </ref>). Each method has its merits and disadvantages. Since ROMIO is a portable, user-level library with no separate I/O servers, it performs collective I/O at the client level. For this purpose, it uses a generalized version of the extended two-phase method described in [32].
Reference: [22] <author> R. Patterson, G. Gibson, E. Ginting, D. Stodolsky, and J. Zelenka. </author> <title> Informed Prefetching and Caching. </title> <booktitle> In Proceedings of the 15th Symposium on Operating System Principles, </booktitle> <pages> pages 79-95. </pages> <publisher> ACM Press, </publisher> <month> December </month> <year> 1995. </year>
Reference-contexts: Examples of hints include parameters for file striping, prefetching/caching information, and access-pattern information. Hints do not affect the semantics of a program, but they may enable the MPI-IO implementation or underlying file system to improve performance or minimize the use of system resources <ref> [3, 22] </ref>. The implementation, however, is free to ignore all hints. 4 MPI-IO also has a set of rigorously defined consistency and atomicity semantics that specify the results of concurrent file accesses. For details of all these features, we refer readers to [11, 18].
Reference: [23] <author> J. </author> <title> Pool. Scalable I/O Initiative. </title> <note> World-Wide Web page at http://www.cacr.caltech.edu/SIO, September 1995. </note>
Reference-contexts: During this time, the MPI Forum had resumed meeting. Its purpose was to address a number of topics that had been deliberately left out of the original MPI Standard, including parallel I/O. The MPI Forum initially recognized that both the MPI-IO Committee and the Scalable I/O Initiative <ref> [23] </ref> represented efforts to develop a standard parallel-I/O interface and therefore decided not to address I/O in its deliberations.
Reference: [24] <author> J. Prost. MPI-IO/PIOFS. </author> <note> World-Wide Web page at http://www.research.ibm.com/people/p/prost/sections/mpiio.html, 1996. 27 </note>
Reference-contexts: This interface is commonly referred to as MPI-IO. MPI-IO is a rich interface with many features designed specifically for performance and portability. Multiple implementations of MPI-IO, both portable and machine specific, are available <ref> [9, 14, 24, 26, 39] </ref>. To avoid the abovementioned problem of many distinct, small I/O requests, MPI-IO allows users to specify the entire noncontiguous access pattern and read or write all the data with a single I/O function call.
Reference: [25] <author> J. Prost, M. Snir, P. Corbett, and D. Feitelson. </author> <title> MPI-IO, A Message-Passing Interface for Concurrent I/O. </title> <type> Technical Report RC 19712 (87394), </type> <institution> IBM T.J. Watson Research Center, </institution> <month> August </month> <year> 1994. </year>
Reference-contexts: A group at IBM wrote an important paper <ref> [25] </ref> that explores the analogy between MPI message passing and I/O. Roughly speaking, one can consider reads and writes to a file system as receives and sends.
Reference: [26] <author> D. Sanders, Y. Park, and M. Brodowicz. </author> <title> Implementation and Performance of MPI-IO File Access Using MPI Datatypes. </title> <type> Technical Report UH-CS-96-12, </type> <institution> University of Houston, </institution> <month> November </month> <year> 1996. </year>
Reference-contexts: This interface is commonly referred to as MPI-IO. MPI-IO is a rich interface with many features designed specifically for performance and portability. Multiple implementations of MPI-IO, both portable and machine specific, are available <ref> [9, 14, 24, 26, 39] </ref>. To avoid the abovementioned problem of many distinct, small I/O requests, MPI-IO allows users to specify the entire noncontiguous access pattern and read or write all the data with a single I/O function call.
Reference: [27] <author> K. Seamons. Panda: </author> <title> Fast Access to Persistent Arrays Using High Level Interfaces and Server Directed Input/Output. </title> <type> Ph.D. thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> May </month> <year> 1996. </year>
Reference-contexts: Collective I/O can be performed in different ways and has been studied by many researchers in recent years. It can be done at the disk level (disk-directed I/O [15]), at the server level (server-directed I/O <ref> [27, 28] </ref>), or at the client level (two-phase I/O [7] or collective buffering [21]). Each method has its merits and disadvantages. Since ROMIO is a portable, user-level library with no separate I/O servers, it performs collective I/O at the client level.
Reference: [28] <author> K. Seamons, Y. Chen, P. Jones, J. Jozwiak, and M. Winslett. </author> <title> Server-Directed Collective I/O in Panda. </title> <booktitle> In Proceedings of Supercomputing '95. </booktitle> <publisher> ACM Press, </publisher> <month> December </month> <year> 1995. </year>
Reference-contexts: The paper also explains why performance improves when users use MPI-IO the right way: the MPI-IO implementation can then perform optimizations, such as data sieving and collective I/O. Although these optimizations have been proposed earlier <ref> [7, 15, 28, 33] </ref>, this is the only paper that discusses in detail the practical issues involved in implementing these optimizations, in the context of a standard, portable API, on real state-of-the-art parallel machines and file systems. <p> Collective I/O can be performed in different ways and has been studied by many researchers in recent years. It can be done at the disk level (disk-directed I/O [15]), at the server level (server-directed I/O <ref> [27, 28] </ref>), or at the client level (two-phase I/O [7] or collective buffering [21]). Each method has its merits and disadvantages. Since ROMIO is a portable, user-level library with no separate I/O servers, it performs collective I/O at the client level.
Reference: [29] <author> E. Smirni, R. Aydt, A. Chien, and D. Reed. </author> <title> I/O Requirements of Scientific Applications: An Evolutionary View. </title> <booktitle> In Proceedings of the Fifth IEEE International Symposium on High Performance Distributed Computing, </booktitle> <pages> pages 49-59. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1996. </year>
Reference-contexts: The small I/O requests made by parallel programs are a result of the combination of two factors: 1. In many parallel applications, each process needs to access a large number of relatively small pieces of data that are not contiguously located in the file <ref> [1, 6, 20, 29, 30, 35] </ref>. 2. <p> Most real parallel applications, however, do not fall into this category. Several studies of I/O access patterns in parallel applications <ref> [1, 6, 20, 29, 30, 35] </ref> have shown that each process in a parallel program may need to access a number of relatively small, noncontiguous portions of a file.
Reference: [30] <author> E. Smirni and D. Reed. </author> <title> Lessons from Characterizing the Input/Output Behavior of Parallel Scientific Applications. Performance Evaluation: </title> <journal> An International Journal, </journal> <volume> 33(1) </volume> <pages> 27-44, </pages> <month> June </month> <year> 1998. </year>
Reference-contexts: The small I/O requests made by parallel programs are a result of the combination of two factors: 1. In many parallel applications, each process needs to access a large number of relatively small pieces of data that are not contiguously located in the file <ref> [1, 6, 20, 29, 30, 35] </ref>. 2. <p> Most real parallel applications, however, do not fall into this category. Several studies of I/O access patterns in parallel applications <ref> [1, 6, 20, 29, 30, 35] </ref> have shown that each process in a parallel program may need to access a number of relatively small, noncontiguous portions of a file.
Reference: [31] <author> R. Thakur, R. Bordawekar, A. Choudhary, R. Ponnusamy, and T. Singh. </author> <title> PASSION Runtime Library for Parallel I/O. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <pages> pages 119-128, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Data sieving is a technique that enables an implementation to make a few large, contiguous requests to the file system even if the user's request consists of several small, noncontiguous accesses. Data sieving was first used in PASSION <ref> [31, 33] </ref> in the context of accessing sections of out-of-core arrays.
Reference: [32] <author> R. Thakur and A. Choudhary. </author> <title> An Extended Two-Phase Method for Accessing Sections of Out-of-Core Arrays. </title> <journal> Scientific Programming, </journal> <volume> 5(4) </volume> <pages> 301-317, </pages> <month> Winter </month> <year> 1996. </year>
Reference-contexts: Each method has its merits and disadvantages. Since ROMIO is a portable, user-level library with no separate I/O servers, it performs collective I/O at the client level. For this purpose, it uses a generalized version of the extended two-phase method described in <ref> [32] </ref>. ROMIO performs collective I/O when the user makes level-3 MPI-IO requests. Most level-1 requests do not contain enough information for ROMIO to perform optimizations, and ROMIO therefore implements them internally as level-0 requests. <p> The advantage of this method is that by making all file accesses large and contiguous, the I/O time is reduced significantly. The added cost of interprocess communication for redistribution is (almost always) small compared with the savings in I/O time. The basic two-phase method was extended in <ref> [32] </ref> to access sections of out-of-core arrays. Since MPI-IO is a general parallel-I/O interface, I/O requests in MPI-IO can represent any access pattern, not just sections of arrays. The two-phase method in [7] must therefore be generalized to handle any noncontiguous I/O request.
Reference: [33] <author> R. Thakur, A. Choudhary, R. Bordawekar, S. More, and S. Kuditipudi. </author> <title> Passion: Optimized I/O for Parallel Applications. </title> <journal> Computer, </journal> <volume> 29(6) </volume> <pages> 70-78, </pages> <month> June </month> <year> 1996. </year>
Reference-contexts: The paper also explains why performance improves when users use MPI-IO the right way: the MPI-IO implementation can then perform optimizations, such as data sieving and collective I/O. Although these optimizations have been proposed earlier <ref> [7, 15, 28, 33] </ref>, this is the only paper that discusses in detail the practical issues involved in implementing these optimizations, in the context of a standard, portable API, on real state-of-the-art parallel machines and file systems. <p> Data sieving is a technique that enables an implementation to make a few large, contiguous requests to the file system even if the user's request consists of several small, noncontiguous accesses. Data sieving was first used in PASSION <ref> [31, 33] </ref> in the context of accessing sections of out-of-core arrays.
Reference: [34] <author> R. Thakur, W. Gropp, and E. Lusk. </author> <title> An Abstract-Device Interface for Implementing Portable Parallel-I/O Interfaces. </title> <booktitle> In Proceedings of the 6th Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 180-187. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1996. </year>
Reference-contexts: in the MPI-2 I/O chapter except support for file interoperability, I/O error handling, and I/O error classes have been implemented in ROMIO. (The missing functions will be implemented in a future release.) A key component of ROMIO that enables such a portable MPI-IO implementation is an internal layer called ADIO <ref> [34] </ref>. ADIO, an abstract-device interface for I/O, is a mechanism for implementing multiple parallel-I/O APIs (application programming interfaces) portably on multiple file systems. We developed ADIO before MPI-IO became a standard, as a means to implement and experiment with various parallel-I/O APIs that existed at the time. <p> ADIO thus separates the machine-dependent and machine-independent aspects involved in implementing an API. We used ADIO to implement Intel's PFS API and subsets of IBM's PIOFS and the original MPI-IO proposal [40] on multiple file systems. By following such an approach, we achieved portability with very low overhead <ref> [34] </ref>. Now that MPI-IO has emerged as the standard, we use ADIO as a mechanism for implementing MPI-IO, as illustrated in Figure 1. We can easily implement MPI-IO on other file systems (such as [5, 10, 12, 16, 19]) simply by implementing ADIO on those file systems. <p> Data sieving and collective I/O are implemented as ADIO functions <ref> [34] </ref>; data sieving is used in the ADIO functions that read/write noncontiguous data, and collective I/O is used in ADIO's collective-I/O functions. Both these optimizations ultimately make contiguous I/O requests to the underlying file system, which are implemented by using ADIO's contiguous-I/O functions.
Reference: [35] <author> R. Thakur, W. Gropp, and E. Lusk. </author> <title> An Experimental Evaluation of the Parallel I/O Systems of the IBM SP and Intel Paragon Using a Production Application. </title> <booktitle> In Proceedings of the 3rd International Conference of the Austrian Center for Parallel Computation (ACPC) with Special Emphasis on Parallel Databases and Parallel I/O, </booktitle> <pages> pages 24-35. </pages> <booktitle> Lecture Notes in Computer Science 1127. </booktitle> <publisher> Springer-Verlag, </publisher> <month> September </month> <year> 1996. </year>
Reference-contexts: The small I/O requests made by parallel programs are a result of the combination of two factors: 1. In many parallel applications, each process needs to access a large number of relatively small pieces of data that are not contiguously located in the file <ref> [1, 6, 20, 29, 30, 35] </ref>. 2. <p> The three applications we used are the following: 1. DIST3D, a template representing the I/O access pattern in an astrophysics application, ASTRO3D <ref> [35] </ref>, from the University of Chicago; 2. the NAS BTIO benchmark [9]; and 3. an unstructured code (which we call UNSTRUC) written by Larry Schoof and Wilbur Johnson of Sandia National Laboratories. 1.1 Contributions of this Paper This paper demonstrates how users can achieve high parallel-I/O performance by using a good <p> Most real parallel applications, however, do not fall into this category. Several studies of I/O access patterns in parallel applications <ref> [1, 6, 20, 29, 30, 35] </ref> have shown that each process in a parallel program may need to access a number of relatively small, noncontiguous portions of a file. <p> We then present performance results. 7.1 Applications The first application we used is DIST3D, a template representing the I/O access pattern in an astrophysics application, ASTRO3D <ref> [35] </ref>, from the University of Chicago. It measures the performance of reading/writing a three-dimensional array distributed in a (block,block,block) fashion among processes from/to a file containing the global array in row-major order.
Reference: [36] <author> R. Thakur, W. Gropp, and E. Lusk. </author> <title> A Case for Using MPI's Derived Datatypes to Improve I/O Performance. </title> <booktitle> In Proceedings of SC98: High Performance Networking and Computing, </booktitle> <month> November </month> <year> 1998. </year> <month> 28 </month>
Reference: [37] <author> R. Thakur, W. Gropp, and E. Lusk. </author> <title> Data Sieving and Collective I/O in ROMIO. </title> <booktitle> In Proceedings of the 7th Symposium on the Frontiers of Massively Parallel Computation. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <month> February </month> <year> 1999. </year> <note> To appear. </note>
Reference: [38] <author> R. Thakur, W. Gropp, and E. Lusk. </author> <title> On Implementing MPI-IO Portably and with High Performance. </title> <booktitle> In Proceedings of the 6th Workshop on I/O in Parallel and Distributed Systems. </booktitle> <publisher> ACM Press, </publisher> <month> May </month> <year> 1999. </year> <note> To appear. </note>
Reference-contexts: PFS HFS Portable Implementation network remote site Implementations File-system-specific implementation that also has these two functions. 2 At present, those implementations are MPICH, HP MPI, and SGI MPI. (In fact, ROMIO is now included as part of these MPI implementations.) Details on how ROMIO is implemented can be found in <ref> [38] </ref>. 4 A Classification of I/O Request Structures In this section we examine the different ways of writing an application with MPI-IO and how that choice impacts performance. Any application has a particular "I/O access pattern" based on its I/O needs.
Reference: [39] <author> R. Thakur, E. Lusk, and W. Gropp. </author> <title> Users Guide for ROMIO: A High-Performance, Portable MPI-IO Implementation. </title> <type> Technical Report ANL/MCS-TM-234, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <note> Revised July 1998. </note>
Reference-contexts: This interface is commonly referred to as MPI-IO. MPI-IO is a rich interface with many features designed specifically for performance and portability. Multiple implementations of MPI-IO, both portable and machine specific, are available <ref> [9, 14, 24, 26, 39] </ref>. To avoid the abovementioned problem of many distinct, small I/O requests, MPI-IO allows users to specify the entire noncontiguous access pattern and read or write all the data with a single I/O function call. <p> The file view and the data layout in memory can be defined by using any MPI basic or derived datatype; therefore, any general, noncontiguous access pattern can be compactly represented. 3 ROMIO: A Portable Implementation of MPI-IO We have developed a high-performance, portable implementation of MPI-IO, called ROMIO <ref> [39] </ref>. It is freely available from the Web site http://www.mcs.anl.gov/romio. ROMIO is designed to run on multiple machines and file systems.
Reference: [40] <author> The MPI-IO Committee. </author> <title> MPI-IO: A Parallel File I/O Interface for MPI, </title> <note> Version 0.5. On the World-Wide Web at http://parallel.nas.nasa.gov/MPI-IO, April 1996. 29 </note>
Reference-contexts: The resulting specification appeared in [4]. At this point a large email discussion group was formed, with participation from a wide variety of institutions. This group, calling itself the MPI-IO Committee, pushed the idea further in a series of proposals, culminating in <ref> [40] </ref>. During this time, the MPI Forum had resumed meeting. Its purpose was to address a number of topics that had been deliberately left out of the original MPI Standard, including parallel I/O. <p> The result was that, from the summer of 1996, the MPI-IO design activities took place in the context of the MPI Forum meetings. The MPI Forum used the latest version of the existing MPI-IO specification <ref> [40] </ref> as a starting point for the I/O chapter in MPI-2. The I/O chapter evolved over many meetings of the Forum and was released in its final form along with the rest of MPI-2 in July 1997 [18]. <p> ADIO thus separates the machine-dependent and machine-independent aspects involved in implementing an API. We used ADIO to implement Intel's PFS API and subsets of IBM's PIOFS and the original MPI-IO proposal <ref> [40] </ref> on multiple file systems. By following such an approach, we achieved portability with very low overhead [34]. Now that MPI-IO has emerged as the standard, we use ADIO as a mechanism for implementing MPI-IO, as illustrated in Figure 1.
References-found: 40

