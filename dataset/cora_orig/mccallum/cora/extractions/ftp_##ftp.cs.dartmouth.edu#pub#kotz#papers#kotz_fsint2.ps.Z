URL: ftp://ftp.cs.dartmouth.edu/pub/kotz/papers/kotz:fsint2.ps.Z
Refering-URL: http://www.cs.dartmouth.edu/~dfk/papers/fsint2.html
Root-URL: http://www.cs.dartmouth.edu
Email: David.Kotz@Dartmouth.edu  
Title: Multiprocessor File System Interfaces  
Author: David Kotz 
Address: Hanover, NH 03755-3551  
Affiliation: Department of Math and Computer Science Dartmouth College  
Web: URL ftp://ftp.cs.dartmouth.edu/pub/CS-papers/Kotz/kotz:fsint2.ps.Z  
Note: Copyright 1993 by IEEE. Appeared in Conf. on Parallel and Distributed Information Systems, pages 194-201. Available at  
Abstract: Increasingly, file systems for multiprocessors are designed with parallel access to multiple disks, to keep I/O from becoming a serious bottleneck for parallel applications. Although file system software can transparently provide high-performance access to parallel disks, a new file system interface is needed to facilitate parallel access to a file from a parallel application. We describe the difficulties faced when using the conventional (Unix-like) interface in parallel applications, and then outline ways to extend the conventional interface to provide convenient access to the file for parallel programs, while retaining the traditional interface for programs that have no need for explicitly parallel file access. Our interface includes a single naming scheme, a multiopen operation, local and global file pointers, mapped file pointers, logical records, multi-files, and logical coercion for backward compatibility. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Katherine Jean Armstrong. </author> <title> Improving file access performance: Cache management for mapped files. </title> <type> Master's thesis, </type> <institution> Univ. of Washington, </institution> <year> 1990. </year>
Reference-contexts: While memory-mapped files have many advantages, they have many disadvantages as a general solution. Unless the address space is segmented, writing segmented files may be difficult. Files typically have different access patterns than virtual memory, possibly requiring different memory management techniques <ref> [1] </ref>. If files are mapped into a distributed shared memory (DSM) system, consistency protocols may need adjustment (since they are normally designed for virtual memory access patterns). Indeed, many operating systems for distributed memory machines do not support DSM, and thus could not easily support memory-mapped files.
Reference: [2] <author> Raymond K. Asbury and David S. Scott. </author> <title> FORTRAN I/O on the iPSC/2: </title> <booktitle> Is there read after write? In Fourth Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 129-132, </pages> <year> 1989. </year>
Reference-contexts: The standard interface is there for compatibility, the tools for performance, and the parallel-open interface for a compromise. Intel's file system for their iPSC/2 and iPSC/860 multiprocessors, CFS [25], also provides three interfaces <ref> [2] </ref>: standard (conventional); random-sequential access, which uses a self-scheduled shared file pointer (allowing atomic append); and coordinated, which is for interleaved access with either a fixed or variable record size. CFS forces each process to open the file independently.
Reference: [3] <author> Thomas W. Crockett. </author> <title> Specification of the operating system interface for parallel file organizations. </title> <type> Publication status unknown (ICASE technical report), </type> <year> 1988. </year>
Reference-contexts: If the file system does not maintain the declustering information for each file, forcing the programmer to specify the set of disks, disk files, or disk blocks, then transparency is lost and the interface is much harder to use. An example of this situation is in <ref> [3] </ref>. Another example is the nCUBE file system prior to 1992, which does not distribute a single file across disks [27]. <p> The file system for the newer Intel Paragon appears to be a Unix file system, based on the OSF/1 operating system [15], although CFS access modes are still available. Another parallel file system is based on ways to lay out a file on parallel disks <ref> [4, 3] </ref>. One interface provides self-scheduled access with a shared file pointer. Another provides individual file pointers. A unified access mode provides the standard interface for compatibility. One deficiency in this interface is that the user must supply a list of disks to the open operation. <p> Each concept directly addresses one or more of the problems outlined in the previous sections. 6.1 Concepts Directory Structure. There should be a single file-naming directory structure for the entire parallel file system. The user should not have to specify the list of disks involved <ref> [3] </ref> or the list of local disk files [27] when opening a file. The name structure should be the same for parallel applications as for sequential applications (such as file-maintenance and directory-listing tools). For maximum portability and interoperability, it should appear to be a Unix file system. Multiopen.
Reference: [4] <author> Thomas W. Crockett. </author> <title> File concepts for parallel I/O. </title> <booktitle> In Proceedings of Supercomputing '89, </booktitle> <pages> pages 574-579, </pages> <year> 1989. </year>
Reference-contexts: Alternatively, the pattern of accesses may only look sequential from a global perspective, in which many processes share access to the portion, reading disjoint records of the portion. Typically, these arise from self-scheduled access to the file <ref> [4] </ref>. We call these globally sequential access patterns, or just global access patterns. <p> The file system for the newer Intel Paragon appears to be a Unix file system, based on the OSF/1 operating system [15], although CFS access modes are still available. Another parallel file system is based on ways to lay out a file on parallel disks <ref> [4, 3] </ref>. One interface provides self-scheduled access with a shared file pointer. Another provides individual file pointers. A unified access mode provides the standard interface for compatibility. One deficiency in this interface is that the user must supply a list of disks to the open operation.
Reference: [5] <author> Erik DeBenedictus and Juan Miguel del Rosario. </author> <title> nCUBE parallel I/O software. </title> <booktitle> In Eleventh Annual IEEE International Phoenix Conference on Computers and Communications (IPCCC), </booktitle> <pages> pages 0117-0124, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: In either case files are declustered over many disks. We call the latter structure Parallel Independent Disks (PID). Examples of multiprocessors using a PID architecture include the Intel [14, 15], nCUBE <ref> [22, 5, 27] </ref>, and Kendall Square Research [20] multiprocessors. Network Memory Processor Memory Processor Memory Processor Disk Disk Disk an MIMD multiprocessor. 3 The workload Parallel file systems and the applications that use them are not sufficiently mature for us to know what access patterns might be typical. <p> The original file system for the nCUBE hypercube multiprocessor [27] is primitive, in the sense that each disk has a local file system independent of the others, and no global file system is provided. In a new nCUBE file system <ref> [6, 5, 7] </ref>, designed around the Unix model, each process specifies a mapping from the bytes of the file to the bytes in its own access stream. The file system specifies a similar mapping, from the bytes in the file to positions on the disks. <p> We define the readp and writep operations, which are the same as read and write, respectively, except that they also return the original file pointer position. Mapped File Pointers. One of the advantages of the nCUBE's mapping functions <ref> [5] </ref> is their ability to remap the address space of the whole file into smaller, contiguous address spaces for each process. Their mapping function maps from (process, pointer) to (position). Each process then sees a single byte stream, indexed by its file pointer, whereas the file is indexed by position.
Reference: [6] <author> Erik DeBenedictus and Peter Madams. </author> <title> nCUBE's parallel I/O with Unix capability. </title> <booktitle> In Sixth Annual Distributed-Memory Computer Conference, </booktitle> <pages> pages 270-277, </pages> <year> 1991. </year>
Reference-contexts: The original file system for the nCUBE hypercube multiprocessor [27] is primitive, in the sense that each disk has a local file system independent of the others, and no global file system is provided. In a new nCUBE file system <ref> [6, 5, 7] </ref>, designed around the Unix model, each process specifies a mapping from the bytes of the file to the bytes in its own access stream. The file system specifies a similar mapping, from the bytes in the file to positions on the disks.
Reference: [7] <author> Juan Miguel del Rosario. </author> <title> High performance parallel I/O on the nCUBE 2. </title> <booktitle> Institute of Electronics, In formation and Communications Engineers (Transac--tions), </booktitle> <month> August </month> <year> 1992. </year> <note> To appear. </note>
Reference-contexts: The original file system for the nCUBE hypercube multiprocessor [27] is primitive, in the sense that each disk has a local file system independent of the others, and no global file system is provided. In a new nCUBE file system <ref> [6, 5, 7] </ref>, designed around the Unix model, each process specifies a mapping from the bytes of the file to the bytes in its own access stream. The file system specifies a similar mapping, from the bytes in the file to positions on the disks.
Reference: [8] <author> Peter C. Dibble. </author> <title> A Parallel Interleaved File System. </title> <type> PhD thesis, </type> <institution> University of Rochester, </institution> <month> March </month> <year> 1990. </year>
Reference-contexts: Programmers need a higher-level interface to easily take advantage of parallel I/O. 5 Existing multiprocessor file system interfaces Several researchers have discussed parallel I/O interfaces for MIMD multiprocessors. Dibble, in his design of the Bridge file system <ref> [8] </ref>, defines three interfaces: standard, which is essentially our conventional interface; parallel open, in which a control process issues all the read and write requests, automatically transferring one record in or out of every process; and tools. <p> A given file position may be mapped by any number of processes (including zero). Also note that self-scheduled access, through a global file pointer, is still possible. Logical Records. Dibble <ref> [8] </ref> argues for direct sup-port for logical records in the file system. The Unix file system does not have any built-in support for logical records, in contrast to some traditional systems (typified by commercial mainframes).
Reference: [9] <author> Jan Edler, Jim Lipkis, and Edith Schonberg. </author> <title> Memory management in Symunix II: A design for large-scale shared memory multiprocessors. </title> <booktitle> In Proceedings of the 1988 Usenix Supercomputer Workshop, </booktitle> <pages> pages 151-168, </pages> <year> 1988. </year>
Reference-contexts: Multiopen opens the file only once, avoiding repeated directory searches and other overhead, and gives each process in the application its own file descriptor (through some implementation-dependent mechanism, e.g., shared memory; Symunix II supports a pdup system call <ref> [9] </ref>). If processes may join the process group, then they must be able to access previously-opened files, and participate in future multiopens. Multiopen can optionally create a file if it does not exist. File pointer.
Reference: [10] <author> Rick Floyd. </author> <title> Short-term file reference patterns in a UNIX environment. </title> <type> Technical Report 177, </type> <institution> Dept. of Computer Science, Univ. of Rochester, </institution> <month> March </month> <year> 1986. </year>
Reference-contexts: Since we concentrate on the programmer's interface to the file system, we work with file access patterns, rather than disk access patterns. In our research we do not investigate read/write file access patterns, because most files are opened for either reading or writing, with few files updated <ref> [10, 23] </ref>. We expect this to be especially true for the large files used in scientific applications. Thus, we consider primarily sequential, read-only and write-only patterns of access to the records of a file. All sequential patterns consist of a sequence of accesses to sequential portions.
Reference: [11] <author> G. Fox, M. Johnson, G. Lyzenga, S. Otto, J. Salmon, and D. Walker. </author> <title> Solving Problems on Concurrent Processors, </title> <booktitle> volume 1, chapter 6 and 15. </booktitle> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year>
Reference-contexts: This mechanism is extended to pipes between parallel programs and to graphics output. Self-scheduled global access is not possible. The CUBIX file system for the CrOS system on hypercubes <ref> [11] </ref> connects a sequential file server on a host processor to a parallel application program on the hypercube. It has two interfaces: singular, in which all processes simultaneously write the same data, and multiple, in which variable-length records are interleaved by process.
Reference: [12] <author> Allan Gottlieb, B. D. Lubachevsky, and Larry Rudolph. </author> <title> Basic techniques for the efficient coordination of very large numbers of cooperating sequential processors. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 5(2) </volume> <pages> 164-189, </pages> <month> April </month> <year> 1983. </year>
Reference-contexts: File locking is supported by some Unix versions, and could be used to enforce atomic access. 3 Fetch-and-add is described in <ref> [12] </ref>. Note that it can, if necessary, be implemented on top of an existing lock primitive. original value of the counter, obtained from the fetch-and-add, is used in a seek operation, which is followed by the read or write. There are three problems with this implementation.
Reference: [13] <author> Andrew S. Grimshaw and Jeff Prem. </author> <title> High performance parallel file objects. </title> <booktitle> In Sixth Annual Distributed-Memory Computer Conference, </booktitle> <pages> pages 720-723, </pages> <year> 1991. </year> <title> [14] iPSC/2 I/O facilities. </title> <publisher> Intel Corporation, </publisher> <year> 1988. </year> <title> Order number 280120-001. </title>
Reference-contexts: Indeed, many operating systems for distributed memory machines do not support DSM, and thus could not easily support memory-mapped files. Grimshaw, Loyot, and Prem <ref> [13] </ref> outline an extensible object-oriented interface based on a simple low-level, Unix-like file system interface. The object-oriented front-end encapsulates access methods, caching, prefetching, and file layout in application-specific ways. They focus on providing the mechanism without specifying particular access methods.
Reference: [15] <institution> Paragon XP/S product overview. Intel Corporation, </institution> <year> 1991. </year>
Reference-contexts: In either case files are declustered over many disks. We call the latter structure Parallel Independent Disks (PID). Examples of multiprocessors using a PID architecture include the Intel <ref> [14, 15] </ref>, nCUBE [22, 5, 27], and Kendall Square Research [20] multiprocessors. <p> This is particularly difficult when creating a file: one process creates the file, all processes synchronize at a barrier, and then the others open the file. The file system for the newer Intel Paragon appears to be a Unix file system, based on the OSF/1 operating system <ref> [15] </ref>, although CFS access modes are still available. Another parallel file system is based on ways to lay out a file on parallel disks [4, 3]. One interface provides self-scheduled access with a shared file pointer. Another provides individual file pointers.
Reference: [16] <author> Michelle Y. Kim. </author> <title> Synchronized disk interleaving. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-35(11):978-988, </volume> <month> November </month> <year> 1986. </year>
Reference-contexts: First, we give some background. 2 Background Much of the previous work in I/O hardware parallelism involves disk striping. In this technique, a file is interleaved across numerous disks and accessed in parallel to simultaneously obtain many blocks of the file with the positioning overhead of one block <ref> [30, 16, 24] </ref>. All of these schemes rely on a single controller to manage all of the disks, and are intended for uniprocessors. There are two ways to attach multiple disks to a multiprocessor.
Reference: [17] <author> David Kotz. </author> <title> Prefetching and Caching Techniques in File Systems for MIMD Multiprocessors. </title> <type> PhD thesis, </type> <institution> Duke University, </institution> <month> April </month> <year> 1991. </year> <note> Available as technical report CS-1991-016. </note>
Reference-contexts: This is particularly useful if the records have variable length. * By understanding logical records, the file system can avoid splitting a record over two blocks. This increases concurrency in some parallel access patterns <ref> [17] </ref>. It can also increase performance in random access patterns (at the cost of wasted space). In our interface, then, we divide the files into byte files and record files. The file type is an attribute of the file.
Reference: [18] <author> David Kotz and Carla Schlatter Ellis. </author> <title> Caching and writeback policies in parallel file systems. </title> <booktitle> In 1991 IEEE Symposium on Parallel and Distributed Processing, </booktitle> <pages> pages 60-67, </pages> <month> December </month> <year> 1991. </year> <note> To appear in the Journal of Parallel and Distributed Computing. </note>
Reference-contexts: These hide the underlying parallel nature of the file, providing portability. Although sequential applications can access parallel file systems with high performance, parallel applications with all processes participating in reading or writing the file are more successful <ref> [19, 18] </ref>. To scale without the limitations of Amdahl's Law, parallel programs must parallelize file access. For concreteness, we use the Unix file system interface [29] as an example of a conventional interface.
Reference: [19] <author> David Kotz and Carla Schlatter Ellis. </author> <title> Practical prefetching techniques for parallel file systems. </title> <booktitle> In Proceedings of the First International Conference on Parallel and Distributed Information Systems, </booktitle> <pages> pages 182-189, </pages> <month> December </month> <year> 1991. </year> <note> To appear in Distributed and Parallel Databases. </note>
Reference-contexts: These hide the underlying parallel nature of the file, providing portability. Although sequential applications can access parallel file systems with high performance, parallel applications with all processes participating in reading or writing the file are more successful <ref> [19, 18] </ref>. To scale without the limitations of Amdahl's Law, parallel programs must parallelize file access. For concreteness, we use the Unix file system interface [29] as an example of a conventional interface.
Reference: [20] <institution> KSR1 technology background. Kendall Square Research, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: In either case files are declustered over many disks. We call the latter structure Parallel Independent Disks (PID). Examples of multiprocessors using a PID architecture include the Intel [14, 15], nCUBE [22, 5, 27], and Kendall Square Research <ref> [20] </ref> multiprocessors. Network Memory Processor Memory Processor Memory Processor Disk Disk Disk an MIMD multiprocessor. 3 The workload Parallel file systems and the applications that use them are not sufficiently mature for us to know what access patterns might be typical. <p> Variable-length records are buffered until complete, then atomically written to the file. To the best of our knowledge, the interface on the BBN, Sequent, and Encore multiprocessors is simply the conventional interface. The Kendall Square Research KSR1 multiprocessor <ref> [20] </ref> uses a PID structure with a RAID attached to individual processors. Files are mapped into the shared memory address space and accessed with normal memory operations. While memory-mapped files have many advantages, they have many disadvantages as a general solution.
Reference: [21] <author> Samuel J. Le*er, Marshall Kirk McKusick, Michael J. Karels, and John S. Quarterman. </author> <title> The Design and Implementation of the 4.3BSD UNIX Operating System. </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year> <title> [22] nCUBE Corporation. nCUBE 2 supercomputers: </title> <type> Technical overview. </type> <institution> Brochure, </institution> <year> 1990. </year>
Reference: [23] <author> John Ousterhout, Herve Da Costa, David Harrison, John Kunze, Mike Kupfer, and James Thompson. </author> <title> A trace driven analysis of the UNIX 4.2 BSD file system. </title> <booktitle> In Proceedings of the Tenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 15-24, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: Since we concentrate on the programmer's interface to the file system, we work with file access patterns, rather than disk access patterns. In our research we do not investigate read/write file access patterns, because most files are opened for either reading or writing, with few files updated <ref> [10, 23] </ref>. We expect this to be especially true for the large files used in scientific applications. Thus, we consider primarily sequential, read-only and write-only patterns of access to the records of a file. All sequential patterns consist of a sequence of accesses to sequential portions.
Reference: [24] <author> David Patterson, Garth Gibson, and Randy Katz. </author> <title> A case for redundant arrays of inexpensive disks (RAID). </title> <booktitle> In ACM SIGMOD Conference, </booktitle> <pages> pages 109-116, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: First, we give some background. 2 Background Much of the previous work in I/O hardware parallelism involves disk striping. In this technique, a file is interleaved across numerous disks and accessed in parallel to simultaneously obtain many blocks of the file with the positioning overhead of one block <ref> [30, 16, 24] </ref>. All of these schemes rely on a single controller to manage all of the disks, and are intended for uniprocessors. There are two ways to attach multiple disks to a multiprocessor. <p> There are two ways to attach multiple disks to a multiprocessor. The first is to use a striped array of disks (e.g., a Redundant Array of Inexpensive Disks, or RAID <ref> [24] </ref>), and attach the array's controller to a processor or to the interconnection network. The second is to attach independent controllers and disks to separate processors or ports on the interconnection network, as shown in Figure 1. In either case files are declustered over many disks.
Reference: [25] <author> Paul Pierce. </author> <title> A concurrent file system for a highly parallel mass storage system. </title> <booktitle> In Fourth Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 155-160, </pages> <year> 1989. </year>
Reference-contexts: This requires all processes to have access to the file name and read/write intention. It also generates many open requests that must be processed by the file system. Thus, it is both inconvenient and inefficient to depend on a single-process open operation. An example is CFS <ref> [25] </ref>. Note that with Unix process semantics, not necessarily included in a system supporting Unix-like file semantics, a file open at the time of a fork is also open in the new process created by the fork ([21], page 175). They also share the same file pointer. <p> The standard interface is there for compatibility, the tools for performance, and the parallel-open interface for a compromise. Intel's file system for their iPSC/2 and iPSC/860 multiprocessors, CFS <ref> [25] </ref>, also provides three interfaces [2]: standard (conventional); random-sequential access, which uses a self-scheduled shared file pointer (allowing atomic append); and coordinated, which is for interleaved access with either a fixed or variable record size. CFS forces each process to open the file independently.
Reference: [26] <author> Rob Pike, Dave Presotto, Ken Thompson, and Howard Trickey. </author> <title> Plan 9 from Bell Labs. </title> <booktitle> In Proceedings of the Summer 1990 UKUUG Conference, </booktitle> <pages> pages 1-9, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: Our interface ideas could be combined with their framework to provide a powerful, extensible interface. It is not possible in any of these interfaces to write segmented files without foreknowledge of the segment size. Some of these issues may be addressed with the capabilities of the Plan 9 system <ref> [26, 28] </ref>, particularly the support for per-process name spaces. Thus, to solve the segmented file problem in Plan 9, described in Section 4.4, each process would bind the name foo to the file foo/#, where # is the process's unique id number within the parallel application.
Reference: [27] <author> Terrence W. Pratt, James C. French, Phillip M. Dickens, and Stanley A. Janet, Jr. </author> <title> A comparison of the architecture and performance of two parallel file systems. </title> <booktitle> In Fourth Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 161-166, </pages> <year> 1989. </year>
Reference-contexts: In either case files are declustered over many disks. We call the latter structure Parallel Independent Disks (PID). Examples of multiprocessors using a PID architecture include the Intel [14, 15], nCUBE <ref> [22, 5, 27] </ref>, and Kendall Square Research [20] multiprocessors. Network Memory Processor Memory Processor Memory Processor Disk Disk Disk an MIMD multiprocessor. 3 The workload Parallel file systems and the applications that use them are not sufficiently mature for us to know what access patterns might be typical. <p> An example of this situation is in [3]. Another example is the nCUBE file system prior to 1992, which does not distribute a single file across disks <ref> [27] </ref>. We believe that it is important to have a single name (e.g., Unix pathname) that defines the parallel file, and to leave the rest to the file system. 4.4 Segmented files Consider programming the read-only segmented access pattern. <p> Another provides individual file pointers. A unified access mode provides the standard interface for compatibility. One deficiency in this interface is that the user must supply a list of disks to the open operation. The original file system for the nCUBE hypercube multiprocessor <ref> [27] </ref> is primitive, in the sense that each disk has a local file system independent of the others, and no global file system is provided. <p> There should be a single file-naming directory structure for the entire parallel file system. The user should not have to specify the list of disks involved [3] or the list of local disk files <ref> [27] </ref> when opening a file. The name structure should be the same for parallel applications as for sequential applications (such as file-maintenance and directory-listing tools). For maximum portability and interoperability, it should appear to be a Unix file system. Multiopen.
Reference: [28] <author> Dave Presotto, Rob Pike, Ken Thompson, and Howard Trickey. </author> <title> Plan 9, a distributed system. </title> <booktitle> In Proceedings of the Spring 1991 EurOpen Conference, </booktitle> <pages> pages 43-50, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Our interface ideas could be combined with their framework to provide a powerful, extensible interface. It is not possible in any of these interfaces to write segmented files without foreknowledge of the segment size. Some of these issues may be addressed with the capabilities of the Plan 9 system <ref> [26, 28] </ref>, particularly the support for per-process name spaces. Thus, to solve the segmented file problem in Plan 9, described in Section 4.4, each process would bind the name foo to the file foo/#, where # is the process's unique id number within the parallel application.
Reference: [29] <author> D. M. Ritchie and K. Thompson. </author> <title> The UNIX timesharing system. </title> <journal> The Bell System Technical Journal, </journal> <volume> 6(2) </volume> <pages> 1905-1930, </pages> <month> July-August </month> <year> 1978. </year>
Reference-contexts: To scale without the limitations of Amdahl's Law, parallel programs must parallelize file access. For concreteness, we use the Unix file system interface <ref> [29] </ref> as an example of a conventional interface. Advantages to using the Unix (or similar) interface for a multiprocessor include application portability, programmer familiarity, and simplicity. This interface does not, however, directly support parallel file access. <p> In the Unix file system a file is modeled as an addressable sequence of bytes (sometimes referred to as a "seekable stream"). The interface is defined by the kernel file system calls <ref> [29] </ref>. The operations provided are open, create (called creat in Unix), close, read, write, and seek (called lseek in Unix). The open and close operations mark the start and end of activity on a given file. Create creates a file if necessary.
Reference: [30] <author> Kenneth Salem and Hector Garcia-Molina. </author> <title> Disk striping. </title> <booktitle> In IEEE 1986 Conference on Data Engineering, </booktitle> <pages> pages 336-342, </pages> <year> 1986. </year>
Reference-contexts: First, we give some background. 2 Background Much of the previous work in I/O hardware parallelism involves disk striping. In this technique, a file is interleaved across numerous disks and accessed in parallel to simultaneously obtain many blocks of the file with the positioning overhead of one block <ref> [30, 16, 24] </ref>. All of these schemes rely on a single controller to manage all of the disks, and are intended for uniprocessors. There are two ways to attach multiple disks to a multiprocessor.
Reference: [31] <institution> Thinking Machines Corporation. </institution> <note> CMMD User's Guide, </note> <month> January </month> <year> 1992. </year>
Reference-contexts: Note that a multifile cannot be easily simulated on top of a conventional file system. Storing it as multiple files clutters up the directories (for example, on the CM-5 <ref> [31] </ref>), and storing it as a single file limits the extensibility of each subfile, due to the linear address space provided by the conventional file. When opening an existing multifile, an optional mapping (unrelated to file pointer mapping) may be specified that indicates the assignment of subfiles to processes.
References-found: 29

