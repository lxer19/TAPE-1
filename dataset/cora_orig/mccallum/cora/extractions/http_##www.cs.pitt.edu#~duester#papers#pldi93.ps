URL: http://www.cs.pitt.edu/~duester/papers/pldi93.ps
Refering-URL: http://www.cs.pitt.edu/~duester/papers.html
Root-URL: http://www.cs.pitt.edu
Email: duester@cs.pitt.edu  
Title: A Practical Data Flow Framework for Array Reference Analysis and its Use in Optimizations  
Author: Evelyn Duesterwald Rajiv Gupta Mary Lou Soffa 
Address: Pittsburgh, Pittsburgh, PA 15260  
Affiliation: Department of Computer Science University of  
Abstract: Data flow analysis techniques have traditionally been restricted to the analysis of scalar variables. This restriction, however, imposes a limitation on the kinds of optimizations that can be performed in loops containing array references. We present a data flow framework for array reference analysis that provides the information needed in various optimizations targeted at sequential or fine-grained parallel architectures. The framework extends the traditional scalar framework by incorporating iteration distance values into the analysis to qualify the computed data flow solution during the fixed point iteration. Analyses phrased in this framework are capable of discovering recurrent access patterns among array references that evolve during the execution of a loop. The framework is practical in that the fixed point solution requires at most three passes over the body of structured loops. Applications of our framework are discussed for register allocation, load/store optimizations, and controlled loop unrolling. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> A. V. Aho, R. Sethi, and J. D. Ullman, </author> <booktitle> in Compilers, principles, techniques, and tools, </booktitle> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Massachusetts, </address> <year> 1986. </year>
Reference-contexts: A loop is controlled by a basic induction variable, i , and we assume that no statement in the loop contains an assignment to i . Furthermore, we assume that prior to the analysis, non-basic induction variables have been identified and removed <ref> [1] </ref> and that all loops are normalized, i.e., the induction variable ranges from 1 to an upper bound UB with increment one. <p> To uniformly model these two cases we define a predicate pr : pr (d ,n ) = K 1 otherwise if reference d occurs in a predecessor node of n Let I denote the iteration range <ref> [1, . . . , UB ] </ref> of induction vari able i , then p n f 2 (i ) f 1 (i -d) - Let the subscripts f 1 and f 2 be f 1 (i ) = a 1 i + b 1 and f 2 (i ) = <p> x 3 , 0), x 4 ) f 5 (x 1 , x 2 , x 3 , x 4 ) = ( x 1 ++ , x 2 ++ , x 3 ++ , x 4 ++ ) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii initialization pass tuples (C [i+2],B [2i],C [i],B [i]) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii IN <ref> [1] </ref> ( -| , -| , -| , -| ) - | , -| , -| , -| ) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii IN [2] ( | , -| , -| , -| ) - - | , -| , -| ) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii IN [3] ( | , | , -| , -| ) <p> - - | ) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiic c c c c c c c c c c c c c c c c c c c c c c c c c (i) Initialization pass. iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii 1. pass 2.pass tuples (C [i+2],B [2i],C [i],B [i]) (C [i+2],B [2i],C [i],B [i]) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii IN <ref> [1] </ref> ( | , | , -| , | ) ( 2, 1, -| , | ) - - - - | ) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii IN [2] ( | , | , -| , | ) ( 2, 1, -| , | ) - - - - | ) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii IN [3] <p> Live Range Analysis The live range of a variable value v starts at a point where v is generated (either be a definition or a use site) and extends up to the last use of v . Live ranges of scalar variables are determined using conventional methods <ref> [1] </ref>. To construct live ranges for array elements, the individual reuse points of a generated value must be identified. However, a generated value can only be preserved in a register for reuse if all instances of the generation site are available at the reuse point. <p> This information is captured by an instance of our framework that computes a must problem: the d-available values inside a loop. This problem is an extension of the classical scalar problem of computing available subexpression <ref> [1] </ref>. A value v of an array element e generated at point p is d-available at point p if there is no redefinition of e along all paths leading to p that start at p and extend up to d iterations. <p> By collecting the individual reuse points in a live range in this fashion, the complete ranges inside the loop are determined. 4.1.2. Construction of the IRIG Traditionally, the problem of assigning registers to live ranges is formulated as the problem of k-coloring the register interference graph <ref> [1] </ref>, where k is the number of available registers. The nodes in this graph represent the live ranges of variables in the loop. Two nodes are connected by an edge, i.e., interfere, if the corresponding live ranges overlap and cannot be assigned the same register. <p> Fig. 6 shows an example of a redundant store that arises across one iteration (i) and the transformed loop with the redundancy eliminated (ii). The data flow problem of detecting redundant stores is a generalization of the scalar problem of determining very busy expressions <ref> [1] </ref> applied to definitions of subscripted variables. We refer to this problem as computing the d-busy stores. <p> Thus, the same framework instance for computing the d-available values is used. Using this information, redundant loads can be eliminated in a similar way as available subexpressions are eliminated for scalars <ref> [1] </ref>. Thus, for each redundancy a scalar temporary is created that can be held in a register to avoid memory accesses. Fig. 7 shows an example of a 1-redundant load and its removal. do i=1,1000 t:=A [1]; -1-redund. load- do I=1,1000 if cond then ...:=A [i]; if cond then ...:=t; A <p> can be eliminated in a similar way as available subexpressions are eliminated for scalars <ref> [1] </ref>. Thus, for each redundancy a scalar temporary is created that can be held in a register to avoid memory accesses. Fig. 7 shows an example of a 1-redundant load and its removal. do i=1,1000 t:=A [1]; -1-redund. load- do I=1,1000 if cond then ...:=A [i]; if cond then ...:=t; A [i+1]:=...; -store- A [i+1]:=...; enddo t:=A [i+1]; enddo (i) (ii) Fig. 7.
Reference: 2. <author> T. Brandes, </author> <title> ``The importance of direct dependences for automatic parallelism,'' </title> <booktitle> Proc. of the '88 Int. Conf. on Supercomputing, </booktitle> <pages> pp. 407-424, </pages> <year> 1988. </year>
Reference-contexts: = ( x 1 ++ , x 2 ++ , x 3 ++ , x 4 ++ ) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii initialization pass tuples (C [i+2],B [2i],C [i],B [i]) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii IN [1] ( -| , -| , -| , -| ) - | , -| , -| , -| ) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii IN <ref> [2] </ref> ( | , -| , -| , -| ) - - | , -| , -| ) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii IN [3] ( | , | , -| , -| ) - - - | , -| ) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii IN [4] ( | , | , -| , -| ) - - <p> c c c c c (i) Initialization pass. iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii 1. pass 2.pass tuples (C [i+2],B [2i],C [i],B [i]) (C [i+2],B [2i],C [i],B [i]) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii IN [1] ( | , | , -| , | ) ( 2, 1, -| , | ) - - - - | ) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii IN <ref> [2] </ref> ( | , | , -| , | ) ( 2, 1, -| , | ) - - - - | ) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii IN [3] ( | , | , -| , | ) ( 2, 1, -| , | ) - - - | ) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii IN [4] ( <p> A general framework for obtaining flow-sensitive data dependence information by incorporating sequencing information was presented by Feautrier [10]. Other work to obtain flow-sensitive data dependence information includes the techniques by Maydan et al [19], Pugh and Wonnacott [21], Brandes <ref> [2] </ref>, Ribas [23], and by Kallis and Klappholz [15]. Some research specifically addresses the exploitation of reuse opportunities for individual array elements.
Reference: 3. <author> D. Callahan, S. Carr, and K. Kennedy, </author> <title> ``Improving register allocation for subscripted variables,'' </title> <booktitle> Proc. of the ACM SIGPLAN '90 Conf. Programming Language Design and Implementation, </booktitle> <pages> pp. 53-65, </pages> <month> Jun. </month> <year> 1990. </year>
Reference-contexts: pass tuples (C [i+2],B [2i],C [i],B [i]) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii IN [1] ( -| , -| , -| , -| ) - | , -| , -| , -| ) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii IN [2] ( | , -| , -| , -| ) - - | , -| , -| ) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii IN <ref> [3] </ref> ( | , | , -| , -| ) - - - | , -| ) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii IN [4] ( | , | , -| , -| ) - - - | ) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii IN [5] ( | , | , -| , | ) - - - | ) <p> [1] ( | , | , -| , | ) ( 2, 1, -| , | ) - - - - | ) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii IN [2] ( | , | , -| , | ) ( 2, 1, -| , | ) - - - - | ) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii IN <ref> [3] </ref> ( | , | , -| , | ) ( 2, 1, -| , | ) - - - | ) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii IN [4] ( 1 , | , -| , | ) ( 1, 1, -| , | ) - - | ) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii IN [5] ( 1 , <p> Other work to obtain flow-sensitive data dependence information includes the techniques by Maydan et al [19], Pugh and Wonnacott [21], Brandes [2], Ribas [23], and by Kallis and Klappholz [15]. Some research specifically addresses the exploitation of reuse opportunities for individual array elements. Scalar replacement <ref> [3] </ref> improves register allocation for array elements by introducing new scalar temporaries for each dependence in a similar way as we have described for eliminating redundant loads. This method is based on conventional data dependence information and thus may miss reuse opportunities in the presence of conditional control flow.
Reference: 4. <author> G. J. Chaitin, </author> <title> ``Register allocation and spilling via graph coloring,'' </title> <booktitle> Proc. of the ACM SIGPLAN `82 Symp. on Compiler Construction, </booktitle> <pages> pp. 201-207, </pages> <month> Jun. </month> <year> 1982. </year>
Reference-contexts: - | , -| , -| , -| ) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii IN [2] ( | , -| , -| , -| ) - - | , -| , -| ) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii IN [3] ( | , | , -| , -| ) - - - | , -| ) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii IN <ref> [4] </ref> ( | , | , -| , -| ) - - - | ) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii IN [5] ( | , | , -| , | ) - - - | ) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiic c c c c c c c c c c c c c c c c c c <p> IN [2] ( | , | , -| , | ) ( 2, 1, -| , | ) - - - - | ) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii IN [3] ( | , | , -| , | ) ( 2, 1, -| , | ) - - - | ) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii IN <ref> [4] </ref> ( 1 , | , -| , | ) ( 1, 1, -| , | ) - - | ) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii IN [5] ( 1 , 0 , -| , | ) ( 1, 0, -| , | ) - - | ) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiic c c c c c c <p> The major challenge in allocating register pipelines is the construction of live ranges for subscripted variables, as the entities of the register assignment. Once live ranges for both the scalar and subscripted variables have been determined, it is possible to generalize traditional register assignment strategies <ref> [4, 5] </ref> to effectively include subscripted variables. We have developed such an integrated register assignment strategy based on a register allocation technique using priority-based coloring to enable a fair and uniform competition of both classes of variables for the available registers [9].
Reference: 5. <author> F. Chow and J. Hennessy, </author> <title> ``Register allocation by priority-based coloring,'' </title> <journal> ACM SIGPLAN Notices, </journal> <volume> vol. 19, no. 6, </volume> <pages> pp. 222-232, </pages> <year> 1984. </year>
Reference-contexts: -| , -| ) - - | , -| , -| ) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii IN [3] ( | , | , -| , -| ) - - - | , -| ) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii IN [4] ( | , | , -| , -| ) - - - | ) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii IN <ref> [5] </ref> ( | , | , -| , | ) - - - | ) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiic c c c c c c c c c c c c c c c c c c c c c c c c c (i) Initialization pass. iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii 1. pass 2.pass tuples (C [i+2],B <p> ) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii IN [3] ( | , | , -| , | ) ( 2, 1, -| , | ) - - - | ) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii IN [4] ( 1 , | , -| , | ) ( 1, 1, -| , | ) - - | ) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii IN <ref> [5] </ref> ( 1 , 0 , -| , | ) ( 1, 0, -| , | ) - - | ) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiic c c c c c c c c c c c c c c c c c c c c c c c c c c c c (ii) <p> The major challenge in allocating register pipelines is the construction of live ranges for subscripted variables, as the entities of the register assignment. Once live ranges for both the scalar and subscripted variables have been determined, it is possible to generalize traditional register assignment strategies <ref> [4, 5] </ref> to effectively include subscripted variables. We have developed such an integrated register assignment strategy based on a register allocation technique using priority-based coloring to enable a fair and uniform competition of both classes of variables for the available registers [9]. <p> Multi-Coloring Register pipelines are assigned to live ranges by multi-coloring the IRIG based on the calculated priorities. In the standard priority-based coloring strategy for scalars <ref> [5] </ref>, the coloring of nodes that have fewer interferences (i.e., neighbors in the graph) than available registers is postponed knowing that these nodes can always be colored. These nodes are called the unconstrained nodes. A node that has more interferences than available registers is called a constrained node.
Reference: 6. <author> P. Cousot and R. Cousot, </author> <title> ``Abstract interpretation: a unified lattice model for static analysis of programs by construction or approximation of fixpoints,'' </title> <booktitle> Conf. Record of the 4th Annual ACM Symp. on Principles of Programming Languages, </booktitle> <pages> pp. 238-252, </pages> <month> Jan. </month> <year> 1977. </year>
Reference-contexts: The solution IN [n ,d ] = x denotes that definition d must reach node n with iteration distance d for pr (d ,n ) d x . ________________ The increment effect of function f exit can be formally modeled using a widening operation <ref> [6] </ref>. if C [i ] 1 3 i :=i +1; 5 B [i ]:=C [i +1]; Fig. 3. Loop flow graph for the loop in Fig. 1. The data flow analysis for must-reaching definitions is illustrated for the loop in Fig. 1 whose flow graph is depicted in Fig. 3.
Reference: 7. <author> J.C. Dehnert, P.Y.-T. Hsu, and J.P. Bratt, </author> <title> ``Overlapped loop support in the Cydra 5,'' </title> <booktitle> Proc. of the 3rd Int. Conf. on Architectural Support for Programming Languages and Operating Systems., </booktitle> <pages> pp. 26-39, </pages> <month> Apr. </month> <year> 1989. </year>
Reference-contexts: This problem of potential overallocation can be avoided by incorporating the costs of the pipeline progression as a negative factor into the priority calculation. Alternatively, hardware support may be exploited to perform the pipeline progression at constant costs. The Cydra 5 architecture <ref> [7] </ref> provides iteration control pointer (ICP) that is implicitly addressed by each register reference. By appropriately updating the ICP at the end of each iteration, this architecture can be used to implement the pipeline progression as a register windowing scheme.
Reference: 8. <author> J.J. Dongarra and A.R. Hinds, </author> <title> ``Unrolling loops in FORTRAN,'' </title> <journal> Software-Practice and Experience, </journal> <volume> vol. 9, no. 3, </volume> <month> Mar. </month> <year> 1979. </year>
Reference-contexts: Controlled Loop Unrolling When compiling a loop body for execution on a fine-grained parallel architecture, it may be beneficial to unroll the loop <ref> [8] </ref>. The original loop body may not have enough parallelism to exploit the available architecture. Sufficient parallelism could then be provided by the larger loop body that is obtained by unrolling. However, loop unrolling may also impact the code in a negative way.
Reference: 9. <author> E. Duesterwald, R. Gupta, </author> <title> and M.L. Soffa, ``Register pipeling: An integrated approach to register allocation for scalar and subscripted variables,'' </title> <booktitle> Proc. of the 4th Int. Workshop on Compiler Construction, </booktitle> <publisher> LNCS 641 Springer Verlag, </publisher> <pages> pp. 192-206, </pages> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: The optimizations supported by the framework extend traditional scalar optimizations to individual array elements. We demonstrate three classes of applications. First, we show how the computed information can be used to extend register alloca-tion techniques to subscripted variables. We outline our technique for register pipelining <ref> [9] </ref> that presents an integrated approach to register allocation for both scalar and subscripted variables. The second class of applications is simple load/store related optimizations of array references. <p> We have developed such an integrated register assignment strategy based on a register allocation technique using priority-based coloring to enable a fair and uniform competition of both classes of variables for the available registers <ref> [9] </ref>. The overall allocation task for a loop is achieved in four phases: (i) Live range analysis (ii) Construction of the integrated register interference graph (iii) Multi-coloring (iv) Code generation First, the loop is analyzed to determine the live ranges of the scalar and subscripted variables in the loop. <p> Based on this graph we generalize the conventional priority based coloring heuristic to a multi-coloring strategy. Finally, the code to implement register pipelines is generated. We show how the developed framework is used for live range analysis, which presents an improvement over the analysis used in <ref> [9] </ref>. The remainder of this section overviews the remaining three phases. Details of the integrated allocation technique are found in [9]. 4.1.1. <p> We show how the developed framework is used for live range analysis, which presents an improvement over the analysis used in <ref> [9] </ref>. The remainder of this section overviews the remaining three phases. Details of the integrated allocation technique are found in [9]. 4.1.1. Live Range Analysis The live range of a variable value v starts at a point where v is generated (either be a definition or a use site) and extends up to the last use of v . Live ranges of scalar variables are determined using conventional methods [1]. <p> If the above inequality holds, there will always be depth (n ) colors to multi-color node n . The remainder of the coloring pro cedure is essentially unchanged and details are found in <ref> [9] </ref>. 4.1.4. Code Generation The use of a register pipeline involves three phases. First, the pipeline is initialized prior to entering the loop. Let l be a live range and let the generating reference of l be of the form X [ f (i )].
Reference: 10. <author> P. Feautrier, </author> <title> ``Data flow analysis of array and scalar references,'' </title> <journal> Int. Journal of Parallel Programming, </journal> <volume> vol. 20, no. 1, </volume> <year> 1991. </year>
Reference-contexts: These techniques provide more accurate data dependence information at the costs of additional complexity, which is acceptable in relation to the performance improvements of loop parallelization. A general framework for obtaining flow-sensitive data dependence information by incorporating sequencing information was presented by Feautrier <ref> [10] </ref>. Other work to obtain flow-sensitive data dependence information includes the techniques by Maydan et al [19], Pugh and Wonnacott [21], Brandes [2], Ribas [23], and by Kallis and Klappholz [15]. Some research specifically addresses the exploitation of reuse opportunities for individual array elements.
Reference: 11. <author> E. Granston and A. Veidenbaum, </author> <title> ``Detecting redundant accesses to array data,'' </title> <booktitle> Proc. of Supercomputing '91, </booktitle> <pages> pp. 854-865, </pages> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: Therefore, the framework operates on individual reference instances. Both approaches share the advantage that data flow frameworks can handle conditional control flow inside loops. Techniques that follow the first approach include the frameworks described by Gross and Steenkiste [12], Granston and Veidenbaum <ref> [11] </ref>, Rosene [24], and Hanxleden et al [14]. In these techniques, the array regions accessed by an individual array reference are locally approximated by some form of area descriptor. The locally determined summary information is globally propagated during an interval-based or iterative data flow computation.
Reference: 12. <author> T. Gross and P. Steenkiste, </author> <title> ``Structured dataflow analysis for arrays and its use in an optimizing compiler,'' </title> <journal> Software Practice and Experience, </journal> <volume> vol. 20, no. 2, </volume> <pages> pp. 133-155, </pages> <month> Feb. </month> <year> 1990. </year>
Reference-contexts: Therefore, the framework operates on individual reference instances. Both approaches share the advantage that data flow frameworks can handle conditional control flow inside loops. Techniques that follow the first approach include the frameworks described by Gross and Steenkiste <ref> [12] </ref>, Granston and Veidenbaum [11], Rosene [24], and Hanxleden et al [14]. In these techniques, the array regions accessed by an individual array reference are locally approximated by some form of area descriptor. The locally determined summary information is globally propagated during an interval-based or iterative data flow computation.
Reference: 13. <author> R. Gupta and M.L. Soffa, </author> <title> ``Region scheduling: an approach for detecting and redistributing parallelism,'' </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> vol. 16, no. 4, </volume> <pages> pp. 421-431, </pages> <month> Apr. </month> <year> 1990. </year>
Reference-contexts: A strategy to control loop unrolling based on the advance calculation of the critical path length l unroll was described for region scheduling <ref> [13] </ref>. In this strategy, unrolling is performed incrementally under the assumption that all relevant dependence information is available. During each step the length l unroll is calculated from information about loop-carried dependencies with distance 1.
Reference: 14. <author> R. Hanxleden, K. Kennedy, C. Koelbel, R. Das, and J. Saltz, </author> <title> ``Complier analysis for irregular problems in Fortran D,'' </title> <booktitle> 5th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pp. 65-77, </pages> <month> Aug. </month> <year> 1992. </year>
Reference-contexts: Therefore, the framework operates on individual reference instances. Both approaches share the advantage that data flow frameworks can handle conditional control flow inside loops. Techniques that follow the first approach include the frameworks described by Gross and Steenkiste [12], Granston and Veidenbaum [11], Rosene [24], and Hanxleden et al <ref> [14] </ref>. In these techniques, the array regions accessed by an individual array reference are locally approximated by some form of area descriptor. The locally determined summary information is globally propagated during an interval-based or iterative data flow computation.
Reference: 15. <author> A. D. Kallis and D. Klappholz, </author> <title> ``Reaching definitions analysis on code containing array references,'' </title> <booktitle> Conf. Rec. of the 4th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: A general framework for obtaining flow-sensitive data dependence information by incorporating sequencing information was presented by Feautrier [10]. Other work to obtain flow-sensitive data dependence information includes the techniques by Maydan et al [19], Pugh and Wonnacott [21], Brandes [2], Ribas [23], and by Kallis and Klappholz <ref> [15] </ref>. Some research specifically addresses the exploitation of reuse opportunities for individual array elements. Scalar replacement [3] improves register allocation for array elements by introducing new scalar temporaries for each dependence in a similar way as we have described for eliminating redundant loads.
Reference: 16. <author> J.B. Kam and J.D. Ullman, </author> <title> ``Monotone data flow analysis frameworks,'' </title> <journal> Acta Informatica, </journal> <volume> vol. 7, no. 3, </volume> <pages> pp. 305-317, </pages> <month> Jul. </month> <year> 1977. </year>
Reference-contexts: This paper presents a data flow framework that addresses these two requirements to systematically exploit fine-grained optimization opportunities for array references. The framework extends the traditional scalar framework <ref> [16] </ref> to the analysis of subscripted variables to meet the requirement of flow-sensitivity which includes the proper handling handling conditional control flow. The detection of recurrent access patterns among subscripted references that evolve during the execution of a loop is modeled in the framework as a fixed point computation. <p> Which subscripted references act as generating or killing references (e.g., uses and/or definitions of subscripted references) depends on the specific data flow problem, and is thus a parameter of the framework. We describe the analysis in a meet data flow framework <ref> [16] </ref>, which most naturally models data flow problems that provide must-information, i.e., an underestimate of the actual information that holds. However, as in the scalar case, problems to provide may-information, i.e., an overestimate of the actual information, can also be formulated in the framework.
Reference: 17. <author> D.J. Kuck, R.H. Kuhn, D. Padua, B.R, Leisure, and M. Wolfe, </author> <title> ``Dependence graphs and compiler optimization,'' </title> <booktitle> Proc. of the 8th ACM Symp. on Principles of Programming Languages, </booktitle> <pages> pp. 207-218, </pages> <month> Jan. </month> <year> 1981. </year>
Reference-contexts: Interest in the analysis of array references is growing with the recognition of its importance in detecting loop-level parallelism. The development of parallel architectures motivated the study of data dependence tests for array references to fully exploit the available parallelism <ref> [17, 25, 26] </ref>. These tests are designed to disambiguate array references, that is, to determine whether two array references may refer to the same memory location. <p> We briefly show how our framework can be used to compute this dependence information. There are three types of loop-carried dependencies: flow dependencies, anti-dependencies, and output dependencies <ref> [17] </ref>. Flow and anti-dependencies can be detected based on reaching definition information. To include the detection of anti-dependencies the uses of subscripted variables are propagated in addition to the definitions. The data flow analysis for dependence detection computes the d-reaching references.
Reference: 18. <author> T.J. Marlowe and B.G. Ryder, </author> <title> ``Properties of data flow frameworks, a unified model,'' </title> <journal> Acta Informatica, </journal> <volume> vol. 28, no. 2, </volume> <pages> pp. 121-163, </pages> <month> Dec. </month> <year> 1990. </year>
Reference-contexts: Weak idempotence implies that one traversal around a cycle is sufficient to compute the contribution of the cycle <ref> [18] </ref>. Thus, in addition to the initialization pass, two passes over the loop body are sufficient for the fixed point convergence. Let N denote the number of statements in the loop.
Reference: 19. <author> D.E. Maydan, S.P. Amarasinghe, and M.S. Lam, </author> <title> ``Array data-flow analysis and its use in array privatization,'' </title> <booktitle> Proc. of the 20th ACM Symp.on Principles of Programming Languages, </booktitle> <pages> pp. 2-15, </pages> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: A general framework for obtaining flow-sensitive data dependence information by incorporating sequencing information was presented by Feautrier [10]. Other work to obtain flow-sensitive data dependence information includes the techniques by Maydan et al <ref> [19] </ref>, Pugh and Wonnacott [21], Brandes [2], Ribas [23], and by Kallis and Klappholz [15]. Some research specifically addresses the exploitation of reuse opportunities for individual array elements.
Reference: 20. <author> Q. Ning and G.R. Gao, </author> <title> ``A novel framework of register allocation for software pipelining,'' </title> <booktitle> Proc. of the 20th ACM Symp. on Principles of Programming Languages, </booktitle> <pages> pp. 29-42, </pages> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: This method is based on conventional data dependence information and thus may miss reuse opportunities in the presence of conditional control flow. Register allocation for array elements across loop iterations in the context of software pipelined loops is described in <ref> [20] </ref>. 6. Conclusions In this paper we addressed the problem of efficiently and effectively analyzing array references to provide the information needed for various optimization problems targeted at sequential or fine-grained parallel architectures.
Reference: 21. <author> W. Pugh and D. Wonnacott, </author> <title> ``Eliminating false data dependences using the omega test,'' </title> <booktitle> Proc. of the SIG-PLAN '92 Conf. on Programming Language Design and Implementation, </booktitle> <pages> pp. 140-151, </pages> <month> Jun. </month> <year> 1992. </year>
Reference-contexts: A general framework for obtaining flow-sensitive data dependence information by incorporating sequencing information was presented by Feautrier [10]. Other work to obtain flow-sensitive data dependence information includes the techniques by Maydan et al [19], Pugh and Wonnacott <ref> [21] </ref>, Brandes [2], Ribas [23], and by Kallis and Klappholz [15]. Some research specifically addresses the exploitation of reuse opportunities for individual array elements.
Reference: 22. <author> B. R. Rau, </author> <title> ``Data flow and dependence analysis for instruction-level parallelism,'' </title> <booktitle> Conf. Rec. of the 4th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: An array analysis framework that follows the second approach and operates on reference instances as opposed to access summaries was proposed by Rau <ref> [22] </ref>. This analysis propagates textual names of referenced array elements throughout the program. Unlike the analysis presented in this paper, the number of iterations over the program is in general unbounded and is thus, in practice, limited by a chosen upper bound resulting in a limited amount of information.
Reference: 23. <author> H. Ribas, </author> <title> ``Obtaining depedence vectors for nested-loop computations,'' </title> <booktitle> Proc. of the 1990 Int. Conf. on Parallel Processing, </booktitle> <pages> pp. 212-219 (II), </pages> <year> 1990. </year>
Reference-contexts: A general framework for obtaining flow-sensitive data dependence information by incorporating sequencing information was presented by Feautrier [10]. Other work to obtain flow-sensitive data dependence information includes the techniques by Maydan et al [19], Pugh and Wonnacott [21], Brandes [2], Ribas <ref> [23] </ref>, and by Kallis and Klappholz [15]. Some research specifically addresses the exploitation of reuse opportunities for individual array elements. Scalar replacement [3] improves register allocation for array elements by introducing new scalar temporaries for each dependence in a similar way as we have described for eliminating redundant loads.
Reference: 24. <author> C. Rosene, </author> <title> ``Incremental dependence analysis,'' </title> <type> Ph.D. thesis, </type> <institution> Rice University, </institution> <month> March </month> <year> 1990. </year>
Reference-contexts: Therefore, the framework operates on individual reference instances. Both approaches share the advantage that data flow frameworks can handle conditional control flow inside loops. Techniques that follow the first approach include the frameworks described by Gross and Steenkiste [12], Granston and Veidenbaum [11], Rosene <ref> [24] </ref>, and Hanxleden et al [14]. In these techniques, the array regions accessed by an individual array reference are locally approximated by some form of area descriptor. The locally determined summary information is globally propagated during an interval-based or iterative data flow computation.
Reference: 25. <author> M. Wolfe and U. Banerjee, </author> <title> ``Data dependence and its application to parallel processing,'' </title> <journal> Int. Journal of Parallel Programming, </journal> <volume> vol. 16, no. 2, </volume> <pages> pp. 137-178, </pages> <year> 1987. </year>
Reference-contexts: Interest in the analysis of array references is growing with the recognition of its importance in detecting loop-level parallelism. The development of parallel architectures motivated the study of data dependence tests for array references to fully exploit the available parallelism <ref> [17, 25, 26] </ref>. These tests are designed to disambiguate array references, that is, to determine whether two array references may refer to the same memory location.
Reference: 26. <author> M. Wolfe, </author> <title> ``Optimizing supercompilers for supercomputers,'' </title> <publisher> Pitman Publishing Company, London, MIT Press, </publisher> <address> Cambridge, Massachusets, </address> <year> 1989. </year>
Reference-contexts: Interest in the analysis of array references is growing with the recognition of its importance in detecting loop-level parallelism. The development of parallel architectures motivated the study of data dependence tests for array references to fully exploit the available parallelism <ref> [17, 25, 26] </ref>. These tests are designed to disambiguate array references, that is, to determine whether two array references may refer to the same memory location.
References-found: 26

