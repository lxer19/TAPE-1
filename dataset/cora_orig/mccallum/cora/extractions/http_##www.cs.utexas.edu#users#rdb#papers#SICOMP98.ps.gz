URL: http://www.cs.utexas.edu/users/rdb/papers/SICOMP98.ps.gz
Refering-URL: http://www.cs.utexas.edu/users/cilk/papers.html
Root-URL: 
Title: SPACE-EFFICIENT SCHEDULING OF MULTITHREADED COMPUTATIONS  
Author: ROBERT D. BLUMOFE AND CHARLES E. LEISERSON 
Keyword: Key words. parallel computing, multithreaded computing, parallel algorithms, scheduling algorithms, randomized algorithms, strict execution, stack memory  
Note: AMS subject classifications. 68Q22, 68Q25, 68M20  
Abstract: This paper considers the problem of scheduling dynamic parallel computations to achieve linear speedup without using significantly more space per processor than that required for a single-processor execution. Utilizing a new graph-theoretic model of multithreaded computation, execution efficiency is quantified by three important measures: T 1 is the time required for executing the computation on 1 processor, T 1 is the time required by an infinite number of processors, and S 1 is the space required to execute the computation on 1 processor. A computation executed on P processors is time-efficient if the time is O(T 1 =P + T 1 ), that is, it achieves linear speedup when P = O(T 1 =T 1 ), and it is space-efficient if it uses O(S 1 P ) total space, that is, the space per processor is within a constant factor of that required for a 1-processor execution. The first result derived from this model shows that there exist multithreaded computations such that no execution schedule can simultaneously achieve efficient time and efficient space. But by restricting attention to "strict" computations|those in which all arguments to a procedure must be available before the procedure can be invoked|much more positive results are obtainable. Specifically, for any strict multithreaded computation, a simple online algorithm can compute a schedule that is both time-efficient and space-efficient. Unfortunately, because the algorithm uses a global queue, the overhead of computing the schedule can be substantial. This problem is overcome by a decentralized algorithm that can compute and execute a P -processor schedule online in expected time O(T 1 =P + T 1 lg P ) and worst-case space O(S 1 P lg P ), including overhead costs. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal, B.-H. Lim, D. Kranz, and J. Kubiatowicz, </author> <month> APRIL: </month> <title> A processor architecture for multiprocessing, </title> <booktitle> in Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <address> Seattle, Washington, </address> <month> May </month> <year> 1990, </year> <pages> pp. 104-114. </pages> <note> Also: </note> <institution> MIT Laboratory for Computer Science Technical Report MIT/LCS/TM-450. </institution>
Reference-contexts: At run time, a scheduler dynamically orders execution of the threads. Other systems employ schedulers that dynamically order threads based on the avail ability of data in shared-memory multiprocessors <ref> [1, 10, 23] </ref> or message arrivals in message-passing multicomputers [2, 17, 29, 44]. Rapid execution of a multithreaded computation on a parallel computer requires exposing and exploiting parallelism in the computation by keeping enough threads concurrently alive to keep the processors of the computer busy.
Reference: [2] <author> W. C. Athas and C. L. Seitz, </author> <title> Multicomputers: Message-passing concurrent computers, </title> <booktitle> Computer, 21 (1988), </booktitle> <pages> pp. 9-24. </pages>
Reference-contexts: At run time, a scheduler dynamically orders execution of the threads. Other systems employ schedulers that dynamically order threads based on the avail ability of data in shared-memory multiprocessors [1, 10, 23] or message arrivals in message-passing multicomputers <ref> [2, 17, 29, 44] </ref>. Rapid execution of a multithreaded computation on a parallel computer requires exposing and exploiting parallelism in the computation by keeping enough threads concurrently alive to keep the processors of the computer busy.
Reference: [3] <author> S. Bhatt, D. Greenberg, T. Leighton, and P. Liu, </author> <title> Tight bounds for on-line tree embeddings, </title> <booktitle> in Proceedings of the Second Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <address> San Francisco, California, </address> <month> Jan. </month> <year> 1991, </year> <pages> pp. 344-350. </pages>
Reference-contexts: Thus, the relevant issue in this work is how to balance the load across processors. Important work in this area includes a randomized work-stealing algorithm for load balancing [38]; dynamic tree-embedding algorithms <ref> [3, 31] </ref>; and algorithms for backtrack search [27, 37, 45], which can be viewed as a multithreaded computation with no data-dependency edges.
Reference: [4] <author> R. D. Blumofe, </author> <title> Managing storage for multithreaded computations, </title> <type> master's thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <month> Sept. </month> <year> 1992. </year> <title> Also available as MIT Laboratory for Computer Science Technical Report MIT/LCS/TR-552. [5] , Executing Multithreaded Programs Efficiently, </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <month> Sept. </month> <year> 1995. </year>
Reference-contexts: Alternatively, by "sequestering" the nonstrictly spawned threads, the scheduler itself can budget the nonstrict spawns and achieve these same time and space bounds; details can be found in <ref> [4] </ref>. 6. Distributed scheduling algorithms. In a distributed thread-scheduling algorithm, each processor works depth-first out of its own local priority queue. Specifically, to get a thread to work on, a processor removes the deepest ready thread from its local queue. <p> In particular, threads must migrate occasionally and some degree of synchronization is needed to avoid the large deviations that result if this random process is run over a long period of time. Further discourse on these problems can be found in <ref> [4] </ref>. In order to achieve the desired result, we modify the Karp and Zhang technique by incorporating a new mechanism to enforce a modest degree of synchrony among the processors. <p> In particular, if we choose r = fi (lg P + lg S 1 ), then the total time spent rerandomizing is O (T 1 =P ) and the per-processor storage bound is O (S 1 (lg P + lg S 1 )). Details can be found in <ref> [4] </ref>. 7. Related and future work. Although the work we have presented here provides some theoretical underpinnings for understanding the resource requirements of multithreaded computations, much remains to be done. In this section, we review some of the related work, both theoretical and empirical, on scheduling dynamic computations.
Reference: [6] <author> R. D. Blumofe, C. F. Joerg, B. C. Kuszmaul, C. E. Leiserson, K. H. Randall, and Y. Zhou, Cilk: </author> <title> An efficient multithreaded runtime system, </title> <booktitle> in Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP), </booktitle> <address> Santa Barbara, California, </address> <month> July </month> <year> 1995, </year> <pages> pp. 207-216. </pages>
Reference-contexts: Additionally, in contrast to Algorithm LDF, this thread-stealing algorithm is efficient with respect to communication. We have implemented this thread-stealing algorithm in the runtime system for Cilk <ref> [5, 6] </ref>, a parallel multithreaded extension of the C language. By employing a provably efficient scheduler, Cilk is able to deliver efficient and predictable performance, guaranteed.
Reference: [7] <author> R. D. Blumofe and C. E. Leiserson, </author> <title> Space-efficient scheduling of multithreaded computations, </title> <booktitle> in Proceedings of the Twenty Fifth Annual ACM Symposium on Theory of Computing (STOC), </booktitle> <address> San Diego, California, </address> <month> May </month> <year> 1993, </year> <pages> pp. </pages> <month> 362-371. </month> <title> [8] , Scheduling multithreaded computations by work stealing, </title> <booktitle> in Proceedings of the 35th Annual Symposium on Foundations of Computer Science (FOCS), </booktitle> <address> Santa Fe, New Mexico, </address> <month> Nov. </month> <year> 1994, </year> <pages> pp. 356-368. </pages>
Reference-contexts: An extended abstract of this paper appeared as <ref> [7] </ref>. y Department of Computer Sciences, the University of Texas at Austin, Austin, Texas, 78712-1188 (rdb@cs.utexas.edu). <p> By making some generous modeling assumptions, we have been able to analyze this algorithm and to obtain similar bounds to those for Algorithm LDF. We are currently working on improving these results. Appendix. During the time between our results becoming publicly known <ref> [7] </ref> 26 R. D. BLUMOFE AND C. E. LEISERSON and this journal publication, we have explored multithreaded computing more fully. We have been able to characterize the performance of a distributed thread-stealing algorithm [5, 8].
Reference: [9] <author> R. D. Blumofe and D. S. Park, </author> <title> Scheduling large-scale parallel computations on networks of workstations, </title> <booktitle> in Proceedings of the Third International Symposium on High Performance Distributed Computing (HPDC), </booktitle> <address> San Francisco, California, </address> <month> Aug. </month> <year> 1994, </year> <pages> pp. 96-105. </pages>
Reference-contexts: By employing a provably efficient scheduler, Cilk is able to deliver efficient and predictable performance, guaranteed. Moreover, structure in the Cilk programming model facilitates the implementation of "adaptive parallelism" and transparent fault tolerance in a runtime system for Cilk on networks of workstations <ref> [5, 9] </ref>. More information about Cilk is available at http://theory.lcs.mit.edu/~cilk. Acknowledgments. The authors thank Bonnie Berger, Tom Cormen, Esther Jesurum, Mike Klugerman, Bradley Kuszmaul, Tom Leighton, Arthur Lent, Greg Pa-padopoulos, Atul Shrivastava, and Ethan Wolf of the MIT Laboratory for Computer Science for insightful discussions.
Reference: [10] <author> B. Boothe and A. Ranade, </author> <title> Improved multithreading techniques for hiding communication latency in multiprocessors, </title> <booktitle> in Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992, </year> <pages> pp. 214-223. </pages>
Reference-contexts: At run time, a scheduler dynamically orders execution of the threads. Other systems employ schedulers that dynamically order threads based on the avail ability of data in shared-memory multiprocessors <ref> [1, 10, 23] </ref> or message arrivals in message-passing multicomputers [2, 17, 29, 44]. Rapid execution of a multithreaded computation on a parallel computer requires exposing and exploiting parallelism in the computation by keeping enough threads concurrently alive to keep the processors of the computer busy.
Reference: [11] <author> R. P. Brent, </author> <title> The parallel evaluation of general arithmetic expressions, </title> <journal> J. Assoc. Comput. Mach., </journal> <volume> 21 (1974), </volume> <pages> pp. 201-206. </pages>
Reference-contexts: A computer with P processors can execute at most P tasks per step, and since the computation has T 1 tasks, we have T P T 1 =P . Of course, we also have T P T 1 . Early work by Graham [20, 21] and independently by Brent <ref> [11, Lemma 2] </ref> yields the bound T P T 1 =P + T 1 .
Reference: [12] <author> F. W. Burton and M. R. Sleep, </author> <title> Executing functional programs on a virtual tree of processors, </title> <booktitle> in Proceedings of the 1981 Conference on Functional Programming Languages and Computer Architecture, </booktitle> <address> Portsmouth, New Hampshire, </address> <month> Oct. </month> <year> 1981, </year> <pages> pp. </pages> <month> 187-194. </month> <title> SCHEDULING MULTITHREADED COMPUTATIONS 27 </title>
Reference-contexts: Other researchers have also addressed the storage issue by attempting to relate parallel storage requirements to serial storage requirements. Burton and Sleep <ref> [12] </ref> and Halstead [22], for example, considered unfair scheduling policies based on thread stealing. In these thread-stealing strategies, each processor works depth-first|just like a serial execution|but when a processor runs out of ready threads, it steals threads from other processors.
Reference: [13] <author> D. E. Culler, </author> <title> Resource management for the tagged token dataflow architecture, </title> <type> master's thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <month> Jan. </month> <year> 1980. </year> <title> Also: MIT Laboratory for Computer Science Technical Report MIT/LCS/TR-332. [14] , Managing Parallelism and Resources in Scientific Dataflow Programs, </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <month> Mar. </month> <year> 1990. </year> <note> Also: </note> <institution> MIT Laboratory for Computer Science Technical Report MIT/LCS/TR-446. </institution>
Reference-contexts: The problem of storage management for multithreaded computations has been a growing concern among practitioners <ref> [13, 22] </ref>. To date, most existing techniques for controlling storage requirements have consisted of heuristics to either bound storage SCHEDULING MULTITHREADED COMPUTATIONS 25 use by explicitly controlling storage as a resource or reduce storage use by modifying the scheduler's behavior.
Reference: [15] <author> D. E. Culler and Arvind, </author> <title> Resource requirements of dataflow programs, </title> <booktitle> in Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <address> Honolulu, Hawaii, </address> <month> May </month> <year> 1988, </year> <pages> pp. 141-150. </pages> <note> Also: </note> <institution> MIT Laboratory for Computer Science, </institution> <note> Computation Structures Group Memo 280. </note>
Reference-contexts: This research was conducted at the MIT Laboratory for Computer Science with additional support from a National Science Foundation Graduate Fellowship. z MIT Laboratory for Computer Science, 545 Technology Square, Cambridge, Massachusetts, 02139 (cel@mit.edu). 2 R. D. BLUMOFE AND C. E. LEISERSON memory capacity of the machine <ref> [15, 22, 24, 39, 43] </ref>. To date, the space requirements of multithreaded computations have been managed with heuristics or not at all [14, 15, 22, 24, 26, 32, 39, 43]. In this paper, we use algorithmic techniques to address the problem of managing storage for multithreaded computations. <p> D. BLUMOFE AND C. E. LEISERSON memory capacity of the machine [15, 22, 24, 39, 43]. To date, the space requirements of multithreaded computations have been managed with heuristics or not at all <ref> [14, 15, 22, 24, 26, 32, 39, 43] </ref>. In this paper, we use algorithmic techniques to address the problem of managing storage for multithreaded computations. <p> A fair scheduler aggressively exposes parallelism, often resulting in excessive space requirements. In order to curb the excessive use of space exhibited by fair scheduling, researchers from the dataflow community have developed heuristics to explicitly manage storage <ref> [15, 39] </ref>. The effectiveness of these heuristics is documented with encouraging empirical evidence but no provable time bounds. In contrast with these heuristic techniques, we have chosen to develop an algorithmic foundation that manages storage by allowing programmers to leverage their knowledge of storage requirements for serially executed programs.
Reference: [16] <author> D. E. Culler, A. Sah, K. E. Schauser, T. von Eicken, and J. Wawrzynek, </author> <title> Fine-grain parallelism with minimal hardware support: A compiler-controlled threaded abstract machine, </title> <booktitle> in Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Santa Clara, California, </address> <month> Apr. </month> <year> 1991, </year> <pages> pp. 164-175. </pages>
Reference-contexts: 1. Introduction. In the course of investigating schemes for general-purpose MIMD-style parallel computation, many diverse research groups have converged on multithreading as a dominant paradigm. As an example, modern dataflow systems <ref> [16, 19, 25, 33, 34, 35, 40, 41] </ref> partition the dataflow instructions into fixed groups called threads and arrange the instructions of each thread into a fixed sequential order at compile time. At run time, a scheduler dynamically orders execution of the threads.
Reference: [17] <author> W. J. Dally, L. Chao, A. Chien, S. Hassoun, W. Horwat, J. Kaplan, P. Song, B. Totty, and S. Wills, </author> <title> Architecture of a message-driven processor, </title> <booktitle> in Proceedings of the 14th Annual International Symposium on Computer Architecture, </booktitle> <address> Pittsburgh, Pennsylvania, </address> <month> June </month> <year> 1987, </year> <pages> pp. 189-196. </pages>
Reference-contexts: At run time, a scheduler dynamically orders execution of the threads. Other systems employ schedulers that dynamically order threads based on the avail ability of data in shared-memory multiprocessors [1, 10, 23] or message arrivals in message-passing multicomputers <ref> [2, 17, 29, 44] </ref>. Rapid execution of a multithreaded computation on a parallel computer requires exposing and exploiting parallelism in the computation by keeping enough threads concurrently alive to keep the processors of the computer busy.
Reference: [18] <author> R. Feldmann, P. Mysliwietz, and B. Monien, </author> <title> Game tree search on a massively parallel system, </title> <booktitle> Advances in Computer Chess 7, </booktitle> <year> (1993), </year> <pages> pp. 203-219. </pages>
Reference-contexts: If the processor goes idle, however, linear speedup is not guaranteed. For these unfair scheduling policies, characterizing the performance analytically is difficult. Thread stealing has also been employed in two parallel chess-playing programs. Zugzwang <ref> [18] </ref> is a program in which processors steal subcomputations of a chess tree using a parallel alpha-beta search algorithm. StarTech [30] is another parallel program organized along similar lines, but with a parallel scout-search algorithm.
Reference: [19] <author> V. G. Grafe and J. E. Hoch, </author> <title> The Epsilon-2 hybrid dataflow architecture, </title> <booktitle> in COMPCON 90, </booktitle> <address> San Francisco, California, </address> <month> Feb. </month> <year> 1990, </year> <pages> pp. 88-93. </pages>
Reference-contexts: 1. Introduction. In the course of investigating schemes for general-purpose MIMD-style parallel computation, many diverse research groups have converged on multithreading as a dominant paradigm. As an example, modern dataflow systems <ref> [16, 19, 25, 33, 34, 35, 40, 41] </ref> partition the dataflow instructions into fixed groups called threads and arrange the instructions of each thread into a fixed sequential order at compile time. At run time, a scheduler dynamically orders execution of the threads.
Reference: [20] <author> R. L. Graham, </author> <title> Bounds for certain multiprocessing anomalies, </title> <journal> The Bell System Technical Journal, </journal> <volume> 45 (1966), </volume> <pages> pp. </pages> <month> 1563-1581. </month> <title> [21] , Bounds on multiprocessing timing anomalies, </title> <journal> SIAM Journal on Applied Mathematics, </journal> <volume> 17 (1969), </volume> <pages> pp. 416-429. </pages>
Reference-contexts: A computer with P processors can execute at most P tasks per step, and since the computation has T 1 tasks, we have T P T 1 =P . Of course, we also have T P T 1 . Early work by Graham <ref> [20, 21] </ref> and independently by Brent [11, Lemma 2] yields the bound T P T 1 =P + T 1 .
Reference: [22] <author> R. H. Halstead, Jr., </author> <title> Multilisp: A language for concurrent symbolic computation, </title> <journal> ACM Trans. Prog. Lang. Syst., </journal> <volume> 7 (1985), </volume> <pages> pp. 501-538. </pages>
Reference-contexts: This research was conducted at the MIT Laboratory for Computer Science with additional support from a National Science Foundation Graduate Fellowship. z MIT Laboratory for Computer Science, 545 Technology Square, Cambridge, Massachusetts, 02139 (cel@mit.edu). 2 R. D. BLUMOFE AND C. E. LEISERSON memory capacity of the machine <ref> [15, 22, 24, 39, 43] </ref>. To date, the space requirements of multithreaded computations have been managed with heuristics or not at all [14, 15, 22, 24, 26, 32, 39, 43]. In this paper, we use algorithmic techniques to address the problem of managing storage for multithreaded computations. <p> D. BLUMOFE AND C. E. LEISERSON memory capacity of the machine [15, 22, 24, 39, 43]. To date, the space requirements of multithreaded computations have been managed with heuristics or not at all <ref> [14, 15, 22, 24, 26, 32, 39, 43] </ref>. In this paper, we use algorithmic techniques to address the problem of managing storage for multithreaded computations. <p> The problem of storage management for multithreaded computations has been a growing concern among practitioners <ref> [13, 22] </ref>. To date, most existing techniques for controlling storage requirements have consisted of heuristics to either bound storage SCHEDULING MULTITHREADED COMPUTATIONS 25 use by explicitly controlling storage as a resource or reduce storage use by modifying the scheduler's behavior. <p> Other researchers have also addressed the storage issue by attempting to relate parallel storage requirements to serial storage requirements. Burton and Sleep [12] and Halstead <ref> [22] </ref>, for example, considered unfair scheduling policies based on thread stealing. In these thread-stealing strategies, each processor works depth-first|just like a serial execution|but when a processor runs out of ready threads, it steals threads from other processors.
Reference: [23] <author> R. H. Halstead, Jr. and T. Fujita, MASA: </author> <title> A multithreaded processor architecture for parallel symbolic computing, </title> <booktitle> in Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <address> Honolulu, Hawaii, </address> <month> May </month> <year> 1988, </year> <pages> pp. 443-451. </pages>
Reference-contexts: At run time, a scheduler dynamically orders execution of the threads. Other systems employ schedulers that dynamically order threads based on the avail ability of data in shared-memory multiprocessors <ref> [1, 10, 23] </ref> or message arrivals in message-passing multicomputers [2, 17, 29, 44]. Rapid execution of a multithreaded computation on a parallel computer requires exposing and exploiting parallelism in the computation by keeping enough threads concurrently alive to keep the processors of the computer busy.
Reference: [24] <author> W. Horwat, </author> <title> Concurrent Smalltalk on the message-driven processor, </title> <type> Tech. </type> <institution> Report MIT/AI/ TR-1321, MIT Artificial Intelligence Laboratory, </institution> <month> Sept. </month> <year> 1991. </year>
Reference-contexts: This research was conducted at the MIT Laboratory for Computer Science with additional support from a National Science Foundation Graduate Fellowship. z MIT Laboratory for Computer Science, 545 Technology Square, Cambridge, Massachusetts, 02139 (cel@mit.edu). 2 R. D. BLUMOFE AND C. E. LEISERSON memory capacity of the machine <ref> [15, 22, 24, 39, 43] </ref>. To date, the space requirements of multithreaded computations have been managed with heuristics or not at all [14, 15, 22, 24, 26, 32, 39, 43]. In this paper, we use algorithmic techniques to address the problem of managing storage for multithreaded computations. <p> D. BLUMOFE AND C. E. LEISERSON memory capacity of the machine [15, 22, 24, 39, 43]. To date, the space requirements of multithreaded computations have been managed with heuristics or not at all <ref> [14, 15, 22, 24, 26, 32, 39, 43] </ref>. In this paper, we use algorithmic techniques to address the problem of managing storage for multithreaded computations.
Reference: [25] <author> R. A. </author> <title> Iannucci, Toward a dataflow / von Neumann hybrid architecture, </title> <booktitle> in Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <address> Honolulu, Hawaii, </address> <month> May </month> <year> 1988, </year> <pages> pp. 131-140. </pages> <note> Also: </note> <institution> MIT Laboratory for Computer Science, </institution> <note> Computation Structures Group Memo 275. </note>
Reference-contexts: 1. Introduction. In the course of investigating schemes for general-purpose MIMD-style parallel computation, many diverse research groups have converged on multithreading as a dominant paradigm. As an example, modern dataflow systems <ref> [16, 19, 25, 33, 34, 35, 40, 41] </ref> partition the dataflow instructions into fixed groups called threads and arrange the instructions of each thread into a fixed sequential order at compile time. At run time, a scheduler dynamically orders execution of the threads.
Reference: [26] <author> S. Jagannathan and J. Philbin, </author> <title> A customizable substrate for concurrent languages, </title> <booktitle> in Proceedings of the ACM SIGPLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <address> San Francisco, California, </address> <month> June </month> <year> 1992, </year> <pages> pp. 55-67. </pages>
Reference-contexts: D. BLUMOFE AND C. E. LEISERSON memory capacity of the machine [15, 22, 24, 39, 43]. To date, the space requirements of multithreaded computations have been managed with heuristics or not at all <ref> [14, 15, 22, 24, 26, 32, 39, 43] </ref>. In this paper, we use algorithmic techniques to address the problem of managing storage for multithreaded computations.
Reference: [27] <author> C. Kaklamanis and G. Persiano, </author> <title> Branch-and-bound and backtrack search on mesh-connected arrays of processors, </title> <booktitle> in Proceedings of the Fourth Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <address> San Diego, California, </address> <month> June </month> <year> 1992, </year> <pages> pp. 118-126. </pages>
Reference-contexts: Thus, the relevant issue in this work is how to balance the load across processors. Important work in this area includes a randomized work-stealing algorithm for load balancing [38]; dynamic tree-embedding algorithms [3, 31]; and algorithms for backtrack search <ref> [27, 37, 45] </ref>, which can be viewed as a multithreaded computation with no data-dependency edges. Although this work ignores aggregate space requirements, it is interesting to note that Zhang's work-stealing algorithm for backtrack search [45] actually gives at most linear expansion of space, but he does not mention this fact.
Reference: [28] <author> R. M. Karp and Y. Zhang, </author> <title> A randomized parallel branch-and-bound procedure, </title> <booktitle> in Proceedings of the Twentieth Annual ACM Symposium on Theory of Computing, </booktitle> <address> Chicago, Illinois, </address> <month> May </month> <year> 1988, </year> <pages> pp. 290-300. </pages>
Reference-contexts: Of course, this approach could result in processors with empty queues sitting idle while other processors have large queues. Thus, we require each processor to have some access to nonlocal queues in order to facilitate some type of load balancing. The technique of Karp and Zhang <ref> [28] </ref> suggests a randomized algorithm in which threads are located in random queues in order to achieve some balance. We can show, however, that the naive adoption of this technique does not work.
Reference: [29] <author> S. W. Keckler and W. J. Dally, </author> <title> Processor coupling: Integrating compile time and runtime scheduling for parallelism, </title> <booktitle> in Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992, </year> <pages> pp. 202-213. </pages>
Reference-contexts: At run time, a scheduler dynamically orders execution of the threads. Other systems employ schedulers that dynamically order threads based on the avail ability of data in shared-memory multiprocessors [1, 10, 23] or message arrivals in message-passing multicomputers <ref> [2, 17, 29, 44] </ref>. Rapid execution of a multithreaded computation on a parallel computer requires exposing and exploiting parallelism in the computation by keeping enough threads concurrently alive to keep the processors of the computer busy.
Reference: [30] <author> B. C. Kuszmaul, </author> <title> Synchronized MIMD Computing, </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <month> May </month> <year> 1994. </year> <note> Available as MIT Laboratory for Computer Science Technical Report MIT/LCS/TR-645 or ftp://theory.lcs.mit.edu/pub/bradley/phd.ps.Z. </note>
Reference-contexts: For these unfair scheduling policies, characterizing the performance analytically is difficult. Thread stealing has also been employed in two parallel chess-playing programs. Zugzwang [18] is a program in which processors steal subcomputations of a chess tree using a parallel alpha-beta search algorithm. StarTech <ref> [30] </ref> is another parallel program organized along similar lines, but with a parallel scout-search algorithm. Although the authors make no guarantees of performance for their algorithms, the empirical results of these programs are good: both have won prizes in international chess competitions.
Reference: [31] <author> T. Leighton, M. Newman, A. G. Ranade, and E. Schwabe, </author> <title> Dynamic tree embeddings in butterflies and hypercubes, </title> <booktitle> in Proceedings of the 1989 ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <address> Santa Fe, New Mexico, </address> <month> June </month> <year> 1989, </year> <pages> pp. 224-234. </pages>
Reference-contexts: Thus, the relevant issue in this work is how to balance the load across processors. Important work in this area includes a randomized work-stealing algorithm for load balancing [38]; dynamic tree-embedding algorithms <ref> [3, 31] </ref>; and algorithms for backtrack search [27, 37, 45], which can be viewed as a multithreaded computation with no data-dependency edges.
Reference: [32] <author> E. Mohr, D. A. Kranz, and R. H. Halstead, Jr., </author> <title> Lazy task creation: A technique for increasing the granularity of parallel programs, </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2 (1991), </volume> <pages> pp. 264-280. </pages> <note> 28 R. </note> <editor> D. BLUMOFE AND C. E. </editor> <publisher> LEISERSON </publisher>
Reference-contexts: D. BLUMOFE AND C. E. LEISERSON memory capacity of the machine [15, 22, 24, 39, 43]. To date, the space requirements of multithreaded computations have been managed with heuristics or not at all <ref> [14, 15, 22, 24, 26, 32, 39, 43] </ref>. In this paper, we use algorithmic techniques to address the problem of managing storage for multithreaded computations.
Reference: [33] <author> R. S. Nikhil and Arvind, </author> <title> Can dataflow subsume von Neumann computing?, </title> <booktitle> in Proceedings of the 16th Annual International Symposium on Computer Architecture, </booktitle> <address> Jerusalem, Is-rael, </address> <month> May </month> <year> 1989, </year> <pages> pp. 262-272. </pages> <note> Also: </note> <institution> MIT Laboratory for Computer Science, </institution> <note> Computation Structures Group Memo 292. </note>
Reference-contexts: 1. Introduction. In the course of investigating schemes for general-purpose MIMD-style parallel computation, many diverse research groups have converged on multithreading as a dominant paradigm. As an example, modern dataflow systems <ref> [16, 19, 25, 33, 34, 35, 40, 41] </ref> partition the dataflow instructions into fixed groups called threads and arrange the instructions of each thread into a fixed sequential order at compile time. At run time, a scheduler dynamically orders execution of the threads.
Reference: [34] <author> R. S. Nikhil, G. M. Papadopoulos, and Arvind, </author> <title> fl T: A multithreaded massively parallel architecture, </title> <booktitle> in Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992, </year> <pages> pp. 156-167. </pages> <note> Also: </note> <institution> MIT Laboratory for Computer Science, </institution> <note> Computation Structures Group Memo 325-1. </note>
Reference-contexts: 1. Introduction. In the course of investigating schemes for general-purpose MIMD-style parallel computation, many diverse research groups have converged on multithreading as a dominant paradigm. As an example, modern dataflow systems <ref> [16, 19, 25, 33, 34, 35, 40, 41] </ref> partition the dataflow instructions into fixed groups called threads and arrange the instructions of each thread into a fixed sequential order at compile time. At run time, a scheduler dynamically orders execution of the threads.
Reference: [35] <author> G. M. Papadopoulos and K. R. Traub, </author> <title> Multithreading: A revisionist view of dataflow architectures, </title> <booktitle> in Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <address> Toronto, Canada, </address> <month> May </month> <year> 1991, </year> <pages> pp. 342-351. </pages> <note> Also: </note> <institution> MIT Laboratory for Computer Science, </institution> <note> Computation Structures Group Memo 330. </note>
Reference-contexts: 1. Introduction. In the course of investigating schemes for general-purpose MIMD-style parallel computation, many diverse research groups have converged on multithreading as a dominant paradigm. As an example, modern dataflow systems <ref> [16, 19, 25, 33, 34, 35, 40, 41] </ref> partition the dataflow instructions into fixed groups called threads and arrange the instructions of each thread into a fixed sequential order at compile time. At run time, a scheduler dynamically orders execution of the threads.
Reference: [36] <author> P. Raghavan, </author> <title> Probabilistic construction of deterministic algorithms: Approximating packing integer programs, </title> <journal> J. Comput. System Sci., </journal> <volume> 37 (1988), </volume> <pages> pp. 130-143. </pages>
Reference-contexts: We can decompose W as a weighted sum of independent indicator random variables and show that E [W ] 2rS 1 . Then, using a theorem due to Raghavan <ref> [36, Theorem 1] </ref>, we can show that with probability at least 1 e 2r , we have W 2erS 1 .
Reference: [37] <author> A. Ranade, </author> <title> Optimal speedup for backtrack search on a butterfly network, </title> <booktitle> in Proceedings of the Third Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <address> Hilton Head, South Carolina, </address> <month> July </month> <year> 1991, </year> <pages> pp. 40-48. </pages>
Reference-contexts: Thus, the relevant issue in this work is how to balance the load across processors. Important work in this area includes a randomized work-stealing algorithm for load balancing [38]; dynamic tree-embedding algorithms [3, 31]; and algorithms for backtrack search <ref> [27, 37, 45] </ref>, which can be viewed as a multithreaded computation with no data-dependency edges. Although this work ignores aggregate space requirements, it is interesting to note that Zhang's work-stealing algorithm for backtrack search [45] actually gives at most linear expansion of space, but he does not mention this fact.
Reference: [38] <author> L. Rudolph, M. Slivkin-Allalouf, and E. Upfal, </author> <title> A simple load balancing scheme for task allocation in parallel machines, </title> <booktitle> in Proceedings of the Third Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <address> Hilton Head, South Carolina, </address> <month> July </month> <year> 1991, </year> <pages> pp. 237-245. </pages>
Reference-contexts: Thus, the relevant issue in this work is how to balance the load across processors. Important work in this area includes a randomized work-stealing algorithm for load balancing <ref> [38] </ref>; dynamic tree-embedding algorithms [3, 31]; and algorithms for backtrack search [27, 37, 45], which can be viewed as a multithreaded computation with no data-dependency edges.
Reference: [39] <author> C. A. Ruggiero and J. Sargeant, </author> <title> Control of parallelism in the Manchester dataflow machine, </title> <booktitle> in Functional Programming Languages and Computer Architecture, no. 274 in Lecture Notes in Computer Science, </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1987, </year> <pages> pp. 1-15. </pages>
Reference-contexts: This research was conducted at the MIT Laboratory for Computer Science with additional support from a National Science Foundation Graduate Fellowship. z MIT Laboratory for Computer Science, 545 Technology Square, Cambridge, Massachusetts, 02139 (cel@mit.edu). 2 R. D. BLUMOFE AND C. E. LEISERSON memory capacity of the machine <ref> [15, 22, 24, 39, 43] </ref>. To date, the space requirements of multithreaded computations have been managed with heuristics or not at all [14, 15, 22, 24, 26, 32, 39, 43]. In this paper, we use algorithmic techniques to address the problem of managing storage for multithreaded computations. <p> D. BLUMOFE AND C. E. LEISERSON memory capacity of the machine [15, 22, 24, 39, 43]. To date, the space requirements of multithreaded computations have been managed with heuristics or not at all <ref> [14, 15, 22, 24, 26, 32, 39, 43] </ref>. In this paper, we use algorithmic techniques to address the problem of managing storage for multithreaded computations. <p> A fair scheduler aggressively exposes parallelism, often resulting in excessive space requirements. In order to curb the excessive use of space exhibited by fair scheduling, researchers from the dataflow community have developed heuristics to explicitly manage storage <ref> [15, 39] </ref>. The effectiveness of these heuristics is documented with encouraging empirical evidence but no provable time bounds. In contrast with these heuristic techniques, we have chosen to develop an algorithmic foundation that manages storage by allowing programmers to leverage their knowledge of storage requirements for serially executed programs.
Reference: [40] <author> M. Sato, Y. Kodama, S. Sakai, Y. Yamaguchi, and Y. Koumura, </author> <title> Thread-based programming for the EM-4 hybrid dataflow machine, </title> <booktitle> in Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992, </year> <pages> pp. 146-155. </pages>
Reference-contexts: 1. Introduction. In the course of investigating schemes for general-purpose MIMD-style parallel computation, many diverse research groups have converged on multithreading as a dominant paradigm. As an example, modern dataflow systems <ref> [16, 19, 25, 33, 34, 35, 40, 41] </ref> partition the dataflow instructions into fixed groups called threads and arrange the instructions of each thread into a fixed sequential order at compile time. At run time, a scheduler dynamically orders execution of the threads.
Reference: [41] <author> K. R. Traub, </author> <title> Sequential Implementation of Lenient Programming Languages, </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <year> 1988. </year> <note> Also: </note> <institution> MIT Laboratory for Computer Science Technical Report MIT/LCS/TR-417. </institution>
Reference-contexts: 1. Introduction. In the course of investigating schemes for general-purpose MIMD-style parallel computation, many diverse research groups have converged on multithreading as a dominant paradigm. As an example, modern dataflow systems <ref> [16, 19, 25, 33, 34, 35, 40, 41] </ref> partition the dataflow instructions into fixed groups called threads and arrange the instructions of each thread into a fixed sequential order at compile time. At run time, a scheduler dynamically orders execution of the threads.
Reference: [42] <author> L. G. Valiant, </author> <title> A bridging model for parallel computation, </title> <journal> Comm. ACM, </journal> <volume> 33 (1990), </volume> <pages> pp. 103-111. </pages>
Reference-contexts: D. BLUMOFE AND C. E. LEISERSON We can view the lg P factors in the space bound and the average available parallelism required to achieve linear speedup as the computational slack required by Valiant's bulk-synchronous model <ref> [42] </ref>. The space bound S (X ) = O (S 1 P lg P ) indicates that Algorithm LDF (6 lg P ) requires memory to scale sufficiently to allow each physical processor enough space to simulate fi (lg P ) virtual processors.
Reference: [43] <author> M. T. Vandevoorde and E. S. Roberts, WorkCrews: </author> <title> An abstraction for controlling parallelism, </title> <journal> International Journal of Parallel Programming, </journal> <volume> 17 (1988), </volume> <pages> pp. 347-366. </pages>
Reference-contexts: This research was conducted at the MIT Laboratory for Computer Science with additional support from a National Science Foundation Graduate Fellowship. z MIT Laboratory for Computer Science, 545 Technology Square, Cambridge, Massachusetts, 02139 (cel@mit.edu). 2 R. D. BLUMOFE AND C. E. LEISERSON memory capacity of the machine <ref> [15, 22, 24, 39, 43] </ref>. To date, the space requirements of multithreaded computations have been managed with heuristics or not at all [14, 15, 22, 24, 26, 32, 39, 43]. In this paper, we use algorithmic techniques to address the problem of managing storage for multithreaded computations. <p> D. BLUMOFE AND C. E. LEISERSON memory capacity of the machine [15, 22, 24, 39, 43]. To date, the space requirements of multithreaded computations have been managed with heuristics or not at all <ref> [14, 15, 22, 24, 26, 32, 39, 43] </ref>. In this paper, we use algorithmic techniques to address the problem of managing storage for multithreaded computations.
Reference: [44] <author> T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser, </author> <title> Active messages: a mechanism for integrated communication and computation, </title> <booktitle> in Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992, </year> <pages> pp. 256-266. </pages>
Reference-contexts: At run time, a scheduler dynamically orders execution of the threads. Other systems employ schedulers that dynamically order threads based on the avail ability of data in shared-memory multiprocessors [1, 10, 23] or message arrivals in message-passing multicomputers <ref> [2, 17, 29, 44] </ref>. Rapid execution of a multithreaded computation on a parallel computer requires exposing and exploiting parallelism in the computation by keeping enough threads concurrently alive to keep the processors of the computer busy.
Reference: [45] <author> Y. Zhang, </author> <title> Parallel Algorithms for Combinatorial Search Problems, </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering and Computer Science, University of California at Berkeley, </institution> <month> Nov. </month> <year> 1989. </year> <note> Also: </note> <institution> University of California at Berkeley, Computer Science Division, </institution> <type> Technical Report UCB/CSD 89/543. </type>
Reference-contexts: Thus, the relevant issue in this work is how to balance the load across processors. Important work in this area includes a randomized work-stealing algorithm for load balancing [38]; dynamic tree-embedding algorithms [3, 31]; and algorithms for backtrack search <ref> [27, 37, 45] </ref>, which can be viewed as a multithreaded computation with no data-dependency edges. Although this work ignores aggregate space requirements, it is interesting to note that Zhang's work-stealing algorithm for backtrack search [45] actually gives at most linear expansion of space, but he does not mention this fact. <p> Although this work ignores aggregate space requirements, it is interesting to note that Zhang's work-stealing algorithm for backtrack search <ref> [45] </ref> actually gives at most linear expansion of space, but he does not mention this fact. The problem of storage management for multithreaded computations has been a growing concern among practitioners [13, 22].
References-found: 41

