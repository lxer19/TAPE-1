URL: ftp://ftp.cs.wisc.edu/wwt/tocs93_csm.ps.gz
Refering-URL: http://www.cs.wisc.edu/~stever/pubs.html
Root-URL: 
Title: Cooperative Shared Memory: Software and Hardware for Scalable Multiprocessors  
Author: Mark D. Hill, James R. Larus, Steven K. Reinhardt, David A. Wood 
Date: November 1, 1993  
Affiliation: University of Wisconsin-Madison  
Note: To appear in: "ACM Transactions on Computer Systems," November 1993. Reprinted by permission of ACM.  
Abstract: We believe the paucity of massively-parallel, shared-memory machines follows from the lack of a shared-memory programming performance model that can inform programmers of the cost of operations (so they can avoid expensive ones) and can tell hardware designers which cases are common (so they can build simple hardware to optimize them). Cooperative shared memory, our approach to shared-memory design, addresses this problem. Our initial implementation of cooperative shared memory uses a simple programming model, called Check-In/Check-Out (CICO), in conjunction with even simpler hardware, called Dir 1 SW. In CICO, programs bracket uses of shared data with a check out directive marking the expected first use and a check in directive terminating the expected use of the data. A cooperative prefetch directive helps hide communication latency. Dir 1 SW is a minimal directory protocol that adds little complexity to message-passing hardware, but efficiently supports programs written within the CICO model.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Sarita V. Adve, Vikram S. Adve, Mark D. Hill, and Mary K. Vernon. </author> <title> Comparison of Hardware and Software Cache Coherence Schemes. </title> <booktitle> In Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 298-308, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: The bars labeled Dir 32 NB show the programs' performance without CICO directives under Dir 32 NB . A height of less than one indicates that a program ran faster than it did under Dir 1 SW . dynamically necessary for two reasons not shared by CICO <ref> [1] </ref>. First, correctness requires inval-idates along all possible execution paths|even those that will not occur dynamically. Second, correctness requires conservative static analysis, which makes worst-case assumptions. Dir 1 SW leaves the burden of correctness with hardware, while providing software with the ability to optimize performance.
Reference: [2] <author> Anant Agarwal, Richard Simoni, Mark Horowitz, and John Hennessy. </author> <title> An Evaluation of Directory Schemes for Cache Coherence. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 280-289, </pages> <year> 1988. </year>
Reference-contexts: To compound matters, existing shared-memory hardware either does not extend to highly parallel systems or does so only at a large cost in complexity. Multis [5] are a successful small-scale shared-memory architecture that cannot scale because of limits on bus capacity and bandwidth. An alternative is directory-based cache-coherence protocols <ref> [2, 17, 23] </ref>. These protocols are complex because hardware must correctly handle many transient states and race conditions. <p> Processors send a Msg Get X (Msg Get S) message when a local program references a block that is not in the local cache or performs an explicit 1 We derived the name Dir 1 SW by extending the directory protocol taxonomy of Agarwal, et al. <ref> [2] </ref>. They use Dir i B and Dir i NB to stand for directories with i pointers that do or do not use broadcast. <p> CICO's hierarchy of performance models has similar goals to Hill and Larus's models for programmers of multis [19]. CICO's models, however, provide richer options for reasoning about relinquishing data and initiating prefetches. Many researchers have investigated using directory protocols for hardware cache coherence in large-scale shared-memory systems <ref> [2] </ref>. Stanford DASH [23] connects n clusters (n 64) with a mesh and the processors within a cluster with a bus. It maintains coherence with a Dir n NB protocol between clusters and snooping within a cluster.
Reference: [3] <author> James Archibald and Jean-Loup Baer. </author> <title> An Economical Solution to the Cache Coherence Problem. </title> <booktitle> In Proceedings of the 11th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 355-362, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: Unlike Dir 1 SW, Baylor et al. do not discuss returning a block to the idle state when all copies are returned, 16 probably because this is unlikely to occur without CICO. We are aware of two other efforts to reduce directory complexity. Archibald and Baer <ref> [3] </ref> propose a directory scheme that uses four states and no pointers. As mentioned above Alewife [8] uses hardware with four pointers and traps to handle additional readers. Both are more complex than Dir 1 SW, because both must process multiple messages in hardware.
Reference: [4] <author> Sandra Johnson Baylor, Kevin P. McAuliffe, and Bharat Deep Rathi. </author> <title> An Evaluation of Cache Coherence Protocols for MIN-Based Multiprocessors. </title> <booktitle> In International Symposium on Shared Memory Multiprocessing, </booktitle> <pages> pages 230-241, </pages> <address> Tokyo, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: All four other protocols, for example, use hardware to send multiple messages to handle the transition from four readers to one writer. Dir 1 SW expects the readers to check-in the block and traps to software if this does not happen. Both Baylor et al. <ref> [4] </ref> and Dir 1 SW use a counter to track extant shared copies. On a write, invalidations are sent to all processors, but only acknowledged by processors that had copies of the block (Chaiken [9] calls this approach a notifying implementation of Dir i B).
Reference: [5] <author> C. Gordon Bell. Multis: </author> <title> A New Class of Multiprocessor Computers. </title> <journal> Science, </journal> <volume> 228 </volume> <pages> 462-466, </pages> <year> 1985. </year>
Reference-contexts: For example, a remote memory reference on Stanford DASH costs at least 100 times more than a local reference [23, 24]. To compound matters, existing shared-memory hardware either does not extend to highly parallel systems or does so only at a large cost in complexity. Multis <ref> [5] </ref> are a successful small-scale shared-memory architecture that cannot scale because of limits on bus capacity and bandwidth. An alternative is directory-based cache-coherence protocols [2, 17, 23]. These protocols are complex because hardware must correctly handle many transient states and race conditions.
Reference: [6] <author> David Callahan, Ken Kennedy, and Allan Poterfield. </author> <title> Software Prefetching. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <pages> pages 40-52, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Archibald and Baer must send messages to all processors to find two or more readers, while Alewife hardware uses multiple messages with 1-4 readers. Dir 1 SW's trapping mechanism was inspired by Alewife's. Dir 1 SW supports software-initiated prefetches <ref> [6, 15] </ref> that leave prefetched data in cache, rather than registers, so data prefetched early do not become incoherent. Dir 1 SW's cooperative prefetch support also reduces the chance that data are prefetched too early since a prefetch remains pending until a block is checked-in.
Reference: [7] <author> M. D. Canon, D. H. Fritz, J. H. Howard, T. D. Howell, M. F. Mitoma, and J. Rodriguez-Rosell. </author> <title> A Virtual Machine Emulator for Performance Evaluation. </title> <journal> Communications of the ACM, </journal> <volume> 23(2) </volume> <pages> 71-80, </pages> <month> February </month> <year> 1980. </year>
Reference: [8] <author> David Chaiken, John Kubiatowics, and Anant Agarwal. </author> <title> LimitLESS Directories: A Scalable Cache Coherence Scheme. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <pages> pages 224-234, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: The CICO annotations can be passed to Dir 1 SW hardware as memory system directives to improve performance. Programs not conforming to the CICO model or using CICO directives run correctly, but trap to system software that performs more complex operations (in a manner similar to MIT Alewife <ref> [8] </ref>). Measurements of eight programs running on a 32-processor system show that Dir 1 SW's performance is comparable to a more complex protocol (Dir 32 NB ) and that the CICO directives improve performance (Section 4). <p> Several state transitions in Table 2 set a trap bit and trap to a software trap handler running on the directory processor (not the requesting processor), as in MIT Alewife <ref> [8] </ref>. The trap bit serializes traps on the same block. The software trap handlers will read directory entries from the hardware and send explicit messages to other processors to complete the request that trapped and to continue the program running on their processor. <p> These interactions make verification extremely difficult, even for this simple bus-based protocol. Furthermore, most directory protocols, such 11 as Stanford DASH's [23] and MIT Alewife's <ref> [8] </ref>, are far more complex than any bus-based pro-tocol. They require hardware to support transitions involving n nodes and 2n messages, where n ranges from 4 to the number of nodes or clusters. By contrast, the base Dir 1 SW protocol (without prefetching) is simpler than most bus-based cache-coherence protocols. <p> Programs that cannot substantially eliminate these traps will not scale on Dir 1 SW hardware. One solution is to extend the hardware to Dir i SW, which maintains up to i pointers to shared copies. Dir i SW traps on requests for more than i shared copies (like Alewife <ref> [8] </ref>) and when a check out X request encounters any shared copies (unlike Alewife, which sometimes handles this transition in hardware). Like Dir 1 SW (and unlike Alewife), Dir i SW never sends more than a single request/response pair, since software handles all cases requiring multiple messages. <p> IEEE Scalable Coherent Interface (SCI) [16] allows an arbitrary interconnection network between n nodes (n &lt; 64K). It implements a Dir n NB protocol with a linked-list whose head is stored in the directory and other list elements are associated with blocks in processor caches. MIT Alewife <ref> [8] </ref> connects multithreaded nodes with a mesh and maintains coherence with a LimitLESS directory that has four pointers in hardware and supports additional pointers by trapping to software. Dir 1 SW shares many goals with these coherence protocols. <p> We are aware of two other efforts to reduce directory complexity. Archibald and Baer [3] propose a directory scheme that uses four states and no pointers. As mentioned above Alewife <ref> [8] </ref> uses hardware with four pointers and traps to handle additional readers. Both are more complex than Dir 1 SW, because both must process multiple messages in hardware.
Reference: [9] <author> David Lars Chaiken. </author> <title> Cache Coherence Protocols for Large-Scale Multiprocessors. </title> <type> Technical Report MIT/LCS/TR-489, </type> <institution> MIT Laboratory for Computer Science, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: Both Baylor et al. [4] and Dir 1 SW use a counter to track extant shared copies. On a write, invalidations are sent to all processors, but only acknowledged by processors that had copies of the block (Chaiken <ref> [9] </ref> calls this approach a notifying implementation of Dir i B). Unlike Dir 1 SW, Baylor et al. do not discuss returning a block to the idle state when all copies are returned, 16 probably because this is unlikely to occur without CICO.
Reference: [10] <author> J. Cheong and A.V. Veidenbaum. </author> <title> A Cache Coherence Scheme With Fast Selective Invalidation. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 299-307, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: for a moderate-sized computer, Dir 1 SW performs comparably to a significantly more complex directory protocol and that the gap can be further narrowed by using the CICO annotations as memory system directive. 5 Related Work Inserting CICO directives is superficially similar to inserting coherence primitives in software cache-coherent systems <ref> [13, 10, 27] </ref>. Software coherence schemes invalidate far more data than 14 and running under the Dir n NB protocol. No prefetching is done to facilitate comparison of the basic protocols.
Reference: [11] <author> David R. Cheriton, Hendrick A. Goosen, and Patrick D. Boyle. </author> <title> Paradigm: A Highly Scalable Shared-Memory Multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 24(2) </volume> <pages> 33-46, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: Stanford DASH [23] connects n clusters (n 64) with a mesh and the processors within a cluster with a bus. It maintains coherence with a Dir n NB protocol between clusters and snooping within a cluster. Each multiprocessor in Stanford Paradigm <ref> [11] </ref> connects n clusters (n 13) with a bus and uses a two-level bus hierarchy within a cluster. It uses a Dir n NB protocol between clusters and a similar protocol within each cluster.
Reference: [12] <author> David R. Cheriton, Hendrik A. Goosen, and Philip Machanick. </author> <title> Restructuring a Parallel Simulation to Improve Cache Behavior in a Shared-Memory Multiprocessor: A First Experience. </title> <booktitle> In International Symposium on Shared Memory Multiprocessing, </booktitle> <pages> pages 109-118, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: To examine this phenomena, we modified mp3d to eliminate some unsynchronized sharing and increase cache reuse. Unlike previous attempts, which rewrote the program <ref> [12] </ref>, our changes were minor. The major problem is that unsynchronized races for the space cell data structure increased communication. We reduced conflicts by having processors lock the cells they operate on.
Reference: [13] <author> Ron Cytron, Steve Karlovsky, and Kevin P. McAuliffe. </author> <title> Automatic Management of Programmable Caches. </title> <booktitle> In Proceedings of the 1988 International Conference on Parallel Processing (Vol. II Software), </booktitle> <pages> pages 229-238, </pages> <month> Aug 188. </month>
Reference-contexts: for a moderate-sized computer, Dir 1 SW performs comparably to a significantly more complex directory protocol and that the gap can be further narrowed by using the CICO annotations as memory system directive. 5 Related Work Inserting CICO directives is superficially similar to inserting coherence primitives in software cache-coherent systems <ref> [13, 10, 27] </ref>. Software coherence schemes invalidate far more data than 14 and running under the Dir n NB protocol. No prefetching is done to facilitate comparison of the basic protocols.
Reference: [14] <author> James R. Goodman, Mary K. Vernon, and Philip J. Woest. </author> <title> Efficient Synchronization Primitives for Large-Scale Cache-Coherent Multiprocessors. </title> <booktitle> In Proceedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS III), </booktitle> <pages> pages 64-77, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: Dir 1 SW's cooperative prefetch support also reduces the chance that data are prefetched too early since a prefetch remains pending until a block is checked-in. This avoids having the block ping-pong from the prefetcher to the writer and back. Similar, but richer support is provided by QOSB <ref> [14] </ref>, now called QOLB. QOLB allows many prefetchers to join a list, spin locally, and obtain the data when it is released.
Reference: [15] <author> Anoop Gupta, John Hennessy, Kourosh Gharachorloo, Todd Mowry, and Wolf-Dietrich Weber. </author> <title> Comparative Evaluation of Latency Reducing and Tolerating Techniques. </title> <booktitle> In Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 254-263, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Archibald and Baer must send messages to all processors to find two or more readers, while Alewife hardware uses multiple messages with 1-4 readers. Dir 1 SW's trapping mechanism was inspired by Alewife's. Dir 1 SW supports software-initiated prefetches <ref> [6, 15] </ref> that leave prefetched data in cache, rather than registers, so data prefetched early do not become incoherent. Dir 1 SW's cooperative prefetch support also reduces the chance that data are prefetched too early since a prefetch remains pending until a block is checked-in.
Reference: [16] <author> David B. Gustavson. </author> <title> The Scalable Coherent Interface and Related Standards Projects. </title> <journal> IEEE Micro, </journal> <volume> 12(2) </volume> <pages> 10-22, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: Each multiprocessor in Stanford Paradigm [11] connects n clusters (n 13) with a bus and uses a two-level bus hierarchy within a cluster. It uses a Dir n NB protocol between clusters and a similar protocol within each cluster. IEEE Scalable Coherent Interface (SCI) <ref> [16] </ref> allows an arbitrary interconnection network between n nodes (n &lt; 64K). It implements a Dir n NB protocol with a linked-list whose head is stored in the directory and other list elements are associated with blocks in processor caches.
Reference: [17] <author> David B. Gustavson and David V. James, </author> <title> editors. SCI: Scalable Coherent Interface: Logical, Physical and Cache Coherence Specifications, </title> <journal> volume P1596/D2.00 18Nov91. IEEE, </journal> <month> November </month> <year> 1991. </year> <note> Draft 2.00 for Recirculation to the Balloting Body. </note>
Reference-contexts: To compound matters, existing shared-memory hardware either does not extend to highly parallel systems or does so only at a large cost in complexity. Multis [5] are a successful small-scale shared-memory architecture that cannot scale because of limits on bus capacity and bandwidth. An alternative is directory-based cache-coherence protocols <ref> [2, 17, 23] </ref>. These protocols are complex because hardware must correctly handle many transient states and race conditions.
Reference: [18] <author> John L. Hennessy and David A. Patterson. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: RISCs have shown that fast, cost-effective hardware requires hardware designers to identify common cases and cooperate with programmers to find mutually-agreeable models that can be implemented with simple hardware <ref> [18] </ref>. This combination permits hardware designers to devote their attention to making common cases run fast. Message-passing computers, which are based on a simple model, are built from simple, scalable hardware.
Reference: [19] <author> Mark D. Hill and James R. Larus. </author> <title> Cache Considerations for Programmers of Multiprocessors. </title> <journal> Communications of the ACM, </journal> <volume> 33(8) </volume> <pages> 97-102, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: Indeed, many existing shared-memory programs would perform poorly on massively-parallel systems because the programs were written under a naive model that assumes all memory references have equal cost. This assumption is wrong because remote references require communication and run slower than local references <ref> [19] </ref>. For example, a remote memory reference on Stanford DASH costs at least 100 times more than a local reference [23, 24]. To compound matters, existing shared-memory hardware either does not extend to highly parallel systems or does so only at a large cost in complexity. <p> Second, correctness requires conservative static analysis, which makes worst-case assumptions. Dir 1 SW leaves the burden of correctness with hardware, while providing software with the ability to optimize performance. CICO's hierarchy of performance models has similar goals to Hill and Larus's models for programmers of multis <ref> [19] </ref>. CICO's models, however, provide richer options for reasoning about relinquishing data and initiating prefetches. Many researchers have investigated using directory protocols for hardware cache coherence in large-scale shared-memory systems [2].
Reference: [20] <author> Mark D. Hill, James R. Larus, Steven K. Reinhardt, and David A. Wood. </author> <title> Cooperative Shared Memory: Software and Hardware for Scalable Multiprocessors. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS V), </booktitle> <pages> pages 262-273, </pages> <month> October </month> <year> 1992. </year> <month> 19 </month>
Reference-contexts: The evaluation compares the programs' performance, both with and without check out and check in directives, against their performance under Dir n NB. This protocol maintains a record of all outstanding copies of a cache block and never broadcasts an invalidate. Unlike our earlier paper <ref> [20] </ref>, we compare the programs' simulated execution times, not the number of directory events. The measurements were collected by executing applications programs|hand-annotated with CICO directives|on a Thinking Machines CM-5 augmented with an additional layer of software to simulate Dir 1 SW and other protocols such as Dir n NB.
Reference: [21] <author> Douglas Johnson. </author> <title> Trap Architectures for Lisp Systems. </title> <booktitle> In Proceedings of the 1990 ACM Conference on LISP and Functional Programming, </booktitle> <pages> pages 79-86, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Shifting the burden of atomically handling multiple-message requests to software dramatically reduces the number of transient hardware states and greatly simplifies the coherence hardware. For programs that trap occasionally, the incurred costs should be small. These costs can be further reduced by microprocessors that efficiently support traps <ref> [21] </ref> or by adopting the approach used in Intel's Paragon computer of handling traps in a companion processor. 3.2 Prefetch Support This section illustrates how Dir 1 SW supports cooperative prefetch, which allows communication to be overlapped with computation.
Reference: [22] <author> Randy H. Katz, Susan J. Eggers, David A. Wood, C.L. Perkins, and R.G. Sheldon. </author> <title> Implementing a Cache Consistency Protocol. </title> <booktitle> In Proceedings of the 12th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 276-283, </pages> <month> June </month> <year> 1985. </year>
Reference-contexts: Race conditions, and the myriad of transient states they produce, make most hardware cache-coherence protocols difficult to implement correctly. For example, although Berkeley SPUR's bus-based Berkeley Ownership coherence protocol <ref> [22] </ref> has only six states, interactions between caches and state machines within a single cache controller produce thousands of transient states [32]. These interactions make verification extremely difficult, even for this simple bus-based protocol.
Reference: [23] <author> Daniel Lenoski, James Laudon, Kourosh Gharachorloo, Wolf-Dietrich Weber, Anoop Gupta, John Hennessy, Mark Horowitz, and Monica Lam. </author> <title> The Stanford DASH Multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: This assumption is wrong because remote references require communication and run slower than local references [19]. For example, a remote memory reference on Stanford DASH costs at least 100 times more than a local reference <ref> [23, 24] </ref>. To compound matters, existing shared-memory hardware either does not extend to highly parallel systems or does so only at a large cost in complexity. Multis [5] are a successful small-scale shared-memory architecture that cannot scale because of limits on bus capacity and bandwidth. <p> To compound matters, existing shared-memory hardware either does not extend to highly parallel systems or does so only at a large cost in complexity. Multis [5] are a successful small-scale shared-memory architecture that cannot scale because of limits on bus capacity and bandwidth. An alternative is directory-based cache-coherence protocols <ref> [2, 17, 23] </ref>. These protocols are complex because hardware must correctly handle many transient states and race conditions. <p> An alternative is directory-based cache-coherence protocols [2, 17, 23]. These protocols are complex because hardware must correctly handle many transient states and race conditions. Although this complexity can be managed (as, for example, in the Stanford DASH multiprocessor <ref> [23] </ref>), architects must expend considerable effort designing, building, and testing complex hardware rather than improving the performance of simpler hardware. Nevertheless, shared memory offers many advantages, including a uniform address space and referential transparency. <p> These interactions make verification extremely difficult, even for this simple bus-based protocol. Furthermore, most directory protocols, such 11 as Stanford DASH's <ref> [23] </ref> and MIT Alewife's [8], are far more complex than any bus-based pro-tocol. They require hardware to support transitions involving n nodes and 2n messages, where n ranges from 4 to the number of nodes or clusters. <p> CICO's models, however, provide richer options for reasoning about relinquishing data and initiating prefetches. Many researchers have investigated using directory protocols for hardware cache coherence in large-scale shared-memory systems [2]. Stanford DASH <ref> [23] </ref> connects n clusters (n 64) with a mesh and the processors within a cluster with a bus. It maintains coherence with a Dir n NB protocol between clusters and snooping within a cluster.
Reference: [24] <author> Daniel Lenoski, James Laudon, Truman Joe, David Nakahira, Luis Stevens, Anoop Gupta, and John Hennessy. </author> <title> The DASH Prototype: Logic Overhead and Performance. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(1) </volume> <pages> 41-61, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: This assumption is wrong because remote references require communication and run slower than local references [19]. For example, a remote memory reference on Stanford DASH costs at least 100 times more than a local reference <ref> [23, 24] </ref>. To compound matters, existing shared-memory hardware either does not extend to highly parallel systems or does so only at a large cost in complexity. Multis [5] are a successful small-scale shared-memory architecture that cannot scale because of limits on bus capacity and bandwidth.
Reference: [25] <author> Calvin Lin and Lawrence Snyder. </author> <title> A Comparison of Programming Models for Shared Memory Multiprocessors. </title> <booktitle> In Proceedings of the 1990 International Conference on Parallel Processing (Vol. II Software), </booktitle> <pages> pages II-163-170, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: To copy otherwise, or to republish, requires a fee and/or specific permission. 1 Shared-memory computers are rare and currently lag in both number and speed of processors. Their absence is due, in part, to a widespread belief that neither shared-memory software nor shared-memory hardware is scalable <ref> [25] </ref>. Indeed, many existing shared-memory programs would perform poorly on massively-parallel systems because the programs were written under a naive model that assumes all memory references have equal cost. This assumption is wrong because remote references require communication and run slower than local references [19].
Reference: [26] <author> John M. Mellor-Crummey and Michael L. Scott. </author> <title> Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(1) </volume> <pages> 21-65, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: We are investigating whether this support is justified. 3.3 Synchronization Support Mellor-Crummey and Scott's locks and barriers are efficient if a processor can spin on a shared-memory block that is physically local <ref> [26] </ref>. A block is local either because it is allocated in a processor's physically local, but logically shared, memory module or because a cache-coherence protocol copies it into the local cache.
Reference: [27] <author> Sang Lyul Min and Jean-Loup Baer. </author> <title> A Timestamp-based Cache Coherence Scheme. </title> <booktitle> In Proceedings of the 1989 International Conference on Parallel Processing (Vol. I Architecture), </booktitle> <pages> pages I-23-32, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: for a moderate-sized computer, Dir 1 SW performs comparably to a significantly more complex directory protocol and that the gap can be further narrowed by using the CICO annotations as memory system directive. 5 Related Work Inserting CICO directives is superficially similar to inserting coherence primitives in software cache-coherent systems <ref> [13, 10, 27] </ref>. Software coherence schemes invalidate far more data than 14 and running under the Dir n NB protocol. No prefetching is done to facilitate comparison of the basic protocols.
Reference: [28] <author> Steven K. Reinhardt, Mark D. Hill, James R. Larus, Alvin R. Lebeck, James C. Lewis, and David A. Wood. </author> <title> The Wisconsin Wind Tunnel: Virtual Prototyping of Parallel Computers. </title> <booktitle> In Proceedings of the 1993 ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 48-60, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The combination of CM-5 hardware and a software layer is called the Wisconsin Wind Tunnel (WWT) <ref> [28] </ref>. WWT runs a parallel shared-memory program on a parallel message-passing computer (a CM-5) and concurrently calculates the program's execution time on the proposed system. We call WWT a virtual prototype because it exploits similarities between the system under design (the target) and an existing evaluation platform (the host)[7]. <p> Furthermore, the results provide strong evidence for the virtual prototyping method, since with less than a person-year of effort we can run Dir 1 SW programs and collect statistics at speeds comparable to real machines. We are seeking to refine cooperative shared memory and enhance WWT <ref> [28, 31] </ref>. A promising approach, for example, is to sequence directory operations in software to enable higher-level programmer or compiler directives (e.g., vector check out). We are studying cooperative prefetch, non-binding prefetch, and other variants.
Reference: [29] <author> Jaswinder Pal Singh, Wolf-Dietrich Weber, and Anoop Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared Memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Sparse is a locally-written program that solves AX = B for a sparse matrix A. Tomcatv is a parallel version of the SPEC benchmark. All other benchmarks are from the SPLASH benchmark suite <ref> [29] </ref>. 4 Evaluation This section evaluates the performance of CICO and Dir 1 SW with a collection of eight benchmarks running on a 32-processor system. The evaluation compares the programs' performance, both with and without check out and check in directives, against their performance under Dir n NB. <p> Dave Douglas, Danny Hillis, Roger Lee, and Steve Swartz of TMC provided invaluable advice and assistance in building the Wind Tunnel. Sarita Adve, Jim Goodman, Guri Sohi, and Mary Vernon provided helpful comments and discussions. Singh et al. <ref> [29] </ref> performed an invaluable service by writing and distributing the SPLASH benchmarks. 18
Reference: [30] <author> Wolf-Dietrich Weber and Anoop Gupta. </author> <title> Analysis of Cache Invalidation Patterns in Multiprocessors. </title> <booktitle> In Proceedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS III), </booktitle> <pages> pages 243-256, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: Get messages (Msg Get X and Msg Get S) conflict with a pending prefetch and trap. Msg Prefetch X works well for blocks used by one processor at a time, called migratory data by Weber and Gupta <ref> [30] </ref>. It is also straightforward to augment Dir 1 SW to support a single cooperative prefetch of a shared copy|providing the block is idle or checked-out exclusive. It is, however, a much larger change to Dir 1 SW to support in hardware multiple concurrent prefetches of a block.
Reference: [31] <author> David A. Wood, Satish Chandra, Babak Falsafi, Mark D. Hill, James R. Larus, Alvin R. Lebeck, James C. Lewis, Shubhendu S. Mukherjee, Subbarao Palacharla, and Steven K. Reinhardt. </author> <title> Mechanisms for Cooperative Shared Memory. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 156-168, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Until now, message-passing computers have dominated this arena. fl A preliminary version of this paper appeared in the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-V). The performance measurements in Section 4 were previously reported in <ref> [31] </ref>. This work is supported in part by NSF PYI Awards CCR-9157366 and MIPS-8957278, NSF Grants CCR-9101035 and MIP-9225097, Univ. of Wisconsin Graduate School Grant, Wisconsin Alumni Research Foundation Fellowship and donations from A.T.&T. Bell Laboratories, Digital Equipment Corporation, Thinking Machine Corporation, and Xerox Corporation. <p> Furthermore, the results provide strong evidence for the virtual prototyping method, since with less than a person-year of effort we can run Dir 1 SW programs and collect statistics at speeds comparable to real machines. We are seeking to refine cooperative shared memory and enhance WWT <ref> [28, 31] </ref>. A promising approach, for example, is to sequence directory operations in software to enable higher-level programmer or compiler directives (e.g., vector check out). We are studying cooperative prefetch, non-binding prefetch, and other variants.
Reference: [32] <author> David A. Wood, Garth G. Gibson, and Randy H. Katz. </author> <title> Verifying a Multiprocessor Cache Controller Using Random Case Generation. </title> <journal> IEEE Design and Test of Computers, </journal> <volume> 7(4) </volume> <pages> 13-25, </pages> <month> August </month> <year> 1990. </year> <month> 20 </month>
Reference-contexts: For example, although Berkeley SPUR's bus-based Berkeley Ownership coherence protocol [22] has only six states, interactions between caches and state machines within a single cache controller produce thousands of transient states <ref> [32] </ref>. These interactions make verification extremely difficult, even for this simple bus-based protocol. Furthermore, most directory protocols, such 11 as Stanford DASH's [23] and MIT Alewife's [8], are far more complex than any bus-based pro-tocol.
References-found: 32

