URL: ftp://ftp.cs.toronto.edu/pub/yiming/IccvWorkshop.ps.gz
Refering-URL: http://www.cs.toronto.edu/~yiming/Research.html
Root-URL: http://www.cs.toronto.edu
Email: yiming@watson.ibm.com  tsotsos@vis.toronto.edu  bennet@vnet.ibm.com  eharley@db.toronto.edu  
Title: Tracking a Person with Pre-recorded Image Database and a Pan, Tilt, and Zoom Camera  
Author: Yiming Ye John K. Tsotsos Karen Bennet Eric Harley 
Address: P.O. Box 704,Yorktown Heights, N.Y. 10598  Toronto, Canada, M5S 1A4  North York, Ontario  Toronto  
Affiliation: IBM T.J Watson Research Center  Department of Computer Science University of Toronto  IBM Canada Center for Advanced Studies  Department of Computer Science University of  
Abstract: This paper proposes a novel tracking strategy that can robustly track a person or other object within a fixed environment using a pan, tilt, and zoom camera with the help of a pre-recorded image database. We define a set called the Minimum Camera Parameter Settings (MCPS) which contains just enough camera states as required to survey the environment for the target. This set of states is used to facilitate tracking and segmentation. The idea is to store a background image of the environment for every camera state in MCPS, thus creating an image database. During tracking camera movements are restricted to states in MCPS (or a version of this set that is augmented to improve smoothness of tracking). Scanning for the target and segmentation of the target from the background are simplified as each current image can be compared with the corresponding pre-recorded background image. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Aloimonos, I. Weiss, and A. Bandyopadhyay. </author> <title> Active vision. </title> <journal> International Journal of Computer Vision, </journal> <volume> 1(4) </volume> <pages> 333-356, </pages> <year> 1988. </year>
Reference-contexts: The method is based on active control of a pan, tilt, and zoom camera and the use of a pre-recorded image database of the environment. Active control of the camera is a form of sensor planning advocated in [2] and <ref> [1] </ref> and analyzed in [21]. The task of sensor planning, while receiving little attention in the past, is very important during tracking because the camera's state parameters determine the quality of the resulting image and indeed whether the target will be within the image.
Reference: [2] <author> R. </author> <title> Bajcsy. Active perception vs. passive perception. </title> <booktitle> In Third IEEE Workshop on Vision, </booktitle> <pages> pages 55-59, </pages> <address> Bel-laire, </address> <year> 1985. </year>
Reference-contexts: The method is based on active control of a pan, tilt, and zoom camera and the use of a pre-recorded image database of the environment. Active control of the camera is a form of sensor planning advocated in <ref> [2] </ref> and [1] and analyzed in [21]. The task of sensor planning, while receiving little attention in the past, is very important during tracking because the camera's state parameters determine the quality of the resulting image and indeed whether the target will be within the image. <p> Let t t e ff 3. Let pan 2arctanf sin ( ff sin ((t ff 2 ) g 4. Use pan to divide the range <ref> [0; 2] </ref> for the given slice into a series of intervals [p b ; p e ], as follows: [0; pan ], [ pan ; 2 pan ], : : :, [k pan ; 2]. Note: the length of the last in terval may not be pan . 5. <p> Use pan to divide the range [0; 2] for the given slice into a series of intervals [p b ; p e ], as follows: [0; pan ], [ pan ; 2 pan ], : : :, <ref> [k pan ; 2] </ref>. Note: the length of the last in terval may not be pan . 5.
Reference: [3] <author> J.L. Crowley and F. Berard. </author> <title> Multi-modal tracking of faces for video communications. </title> <booktitle> In CVPR, </booktitle> <year> 1997. </year>
Reference: [4] <author> T. Darrell, B. Moghaddam, </author> <title> and A.P. Pentland. Active face tracking and pose estimation in an interactive room. </title> <booktitle> In CVPR, </booktitle> <pages> pages 67-71, </pages> <year> 1996. </year>
Reference-contexts: Approaches to this problem include the use of multiple cameras [6, 10], two- and three-dimensional models of the target [6, 8] and attempts to follow specific features of the moving target, such as head or hands through the use of an active camera <ref> [4] </ref>. The stability of these tracking methods is adversely affected by the complexity of the environment. In this paper we present a novel tracking strategy which can be used effectively in tracking tasks where the identity of the moving target is not an issue.
Reference: [5] <author> S.J. Dickinson, H.I. Christensen, J.K. Tsotsos, and G. Olofsson. </author> <title> Active object recognition integrating attention and viewpoint control. </title> <booktitle> Computer Vision and Image Understanding, </booktitle> <volume> 67(3) </volume> <pages> 239-260, </pages> <year> 1997. </year>
Reference-contexts: Demonstrations of the efficacy of planned camera motion in object recognition and tracking can be found in <ref> [5] </ref> and [15, 17, 18, 20], respectively. Whereas segmentation is generally difficult and unstable, we show that some problems can be alleviated through the use of a pre-recorded image database and intelligent control of the camera.
Reference: [6] <author> D.M. Gavrila and L.S. Davis. </author> <title> 3-d model based tracking of humans in action: a multi-view approach. </title> <booktitle> In CVPR, </booktitle> <pages> pages 73-79, </pages> <year> 1996. </year>
Reference-contexts: The task is a challenging one because it not only involves the difficulties of segmenting the target from various backgrounds, but also analysis and prediction of the target's motion. Approaches to this problem include the use of multiple cameras <ref> [6, 10] </ref>, two- and three-dimensional models of the target [6, 8] and attempts to follow specific features of the moving target, such as head or hands through the use of an active camera [4]. The stability of these tracking methods is adversely affected by the complexity of the environment. <p> The task is a challenging one because it not only involves the difficulties of segmenting the target from various backgrounds, but also analysis and prediction of the target's motion. Approaches to this problem include the use of multiple cameras [6, 10], two- and three-dimensional models of the target <ref> [6, 8] </ref> and attempts to follow specific features of the moving target, such as head or hands through the use of an active camera [4]. The stability of these tracking methods is adversely affected by the complexity of the environment.
Reference: [7] <author> H.P. Graf, E. Cosatto, D. Gibbon, M. Kocheisen, and E. Petajan. </author> <title> Multi-modal system for locating heads and faces. </title> <booktitle> In International Conference on Automatic Face and Gesture Recognition, </booktitle> <pages> pages 88-93, </pages> <address> Killing-ton, Vermont, </address> <month> October </month> <year> 1996. </year>
Reference: [8] <author> D. Huttenlocher, J. Noh, and W. Rucklidge. </author> <title> Tracking non-rigid objects in complex scenes. </title> <booktitle> In ICCV93, </booktitle> <pages> pages 93-101, </pages> <year> 1993. </year>
Reference-contexts: The task is a challenging one because it not only involves the difficulties of segmenting the target from various backgrounds, but also analysis and prediction of the target's motion. Approaches to this problem include the use of multiple cameras [6, 10], two- and three-dimensional models of the target <ref> [6, 8] </ref> and attempts to follow specific features of the moving target, such as head or hands through the use of an active camera [4]. The stability of these tracking methods is adversely affected by the complexity of the environment.
Reference: [9] <author> S.X. Ju, M.J. Black, and Y. Yacoob. </author> <title> Cardboard people: A parameterized model of articulated image motion. </title> <booktitle> In International Conference on Automatic Face and Gesture Recognition, </booktitle> <pages> pages 38-44, </pages> <address> Killington, Vermont, </address> <month> October </month> <year> 1996. </year>
Reference: [10] <author> I. Kakadiaris and D. Metaxas. </author> <title> Model-based estimation of 3d human motion with occlusion based on active multi-viewpoint selection. </title> <booktitle> In CVPR, </booktitle> <pages> pages 81-87, </pages> <year> 1996. </year>
Reference-contexts: The task is a challenging one because it not only involves the difficulties of segmenting the target from various backgrounds, but also analysis and prediction of the target's motion. Approaches to this problem include the use of multiple cameras <ref> [6, 10] </ref>, two- and three-dimensional models of the target [6, 8] and attempts to follow specific features of the moving target, such as head or hands through the use of an active camera [4]. The stability of these tracking methods is adversely affected by the complexity of the environment.
Reference: [11] <author> C. Kervrann and F. Heitz. </author> <title> A hierarchical statistical framework for the segmentation of deformable objects in image sequences. </title> <booktitle> In CVPR, </booktitle> <pages> pages 724-728, </pages> <year> 1994. </year>
Reference: [12] <author> J. Koenderink and A. van Doorn. </author> <title> The internal representation of solid shape with respect to vision. </title> <journal> Biological Cybernetics, </journal> <volume> 32 </volume> <pages> 211-216, </pages> <year> 1979. </year>
Reference-contexts: Similarly, if the target aspect changes, then the next aspect should be one adjacent to the current aspect in a graph relating the various aspects (cf. <ref> [12] </ref>). In this case the target's new position should be in a cell defined for the new aspect and which intersects a cell in the surrounding region. Tables VCC1 Camera A tracking environment. (c) Global view of the tracking environment.
Reference: [13] <author> J.J. Kuch and T.S. Huang. </author> <title> Vision based hand modeling and tracking. </title> <booktitle> In Proceedings of International Conference on Computer VIsion, </booktitle> <pages> pages 81-87, </pages> <year> 1996. </year>
Reference: [14] <author> B. Moghaddam and A. Pentland. </author> <title> Probabilistic visual learning for object detection. </title> <booktitle> In ICCV95, </booktitle> <pages> pages 786-793, </pages> <year> 1995. </year>
Reference: [15] <author> D.W. Murray, K.J. Bradshaw, P.F. McLauchlan, </author> <title> I.D. Reid, and P.M. Sharkey. Driving saccade to pursuit using image motion. </title> <journal> International Journal of Computer Vision, </journal> <volume> 16 </volume> <pages> 205-228, </pages> <year> 1995. </year>
Reference-contexts: Demonstrations of the efficacy of planned camera motion in object recognition and tracking can be found in [5] and <ref> [15, 17, 18, 20] </ref>, respectively. Whereas segmentation is generally difficult and unstable, we show that some problems can be alleviated through the use of a pre-recorded image database and intelligent control of the camera.
Reference: [16] <author> N. Olivier, A. Pentland, and F. Berard. Lafter: </author> <title> Lips and face real time tracker. </title> <booktitle> In CVPR, </booktitle> <year> 1997. </year>
Reference: [17] <author> T.J. Olson and D.J. Coombs. </author> <title> Real-time vergence control for binocular robots. </title> <journal> International Journal of Computer Vision, </journal> <volume> 7(1) </volume> <pages> 67-89, </pages> <year> 1991. </year>
Reference-contexts: Demonstrations of the efficacy of planned camera motion in object recognition and tracking can be found in [5] and <ref> [15, 17, 18, 20] </ref>, respectively. Whereas segmentation is generally difficult and unstable, we show that some problems can be alleviated through the use of a pre-recorded image database and intelligent control of the camera.
Reference: [18] <author> K. Pahlavan and J.-O. Eklundh. </author> <title> A head-eye system analysis and design. CVGIP: </title> <booktitle> Image Understanding, </booktitle> <volume> 56(1) </volume> <pages> 41-56, </pages> <year> 1992. </year>
Reference-contexts: Demonstrations of the efficacy of planned camera motion in object recognition and tracking can be found in [5] and <ref> [15, 17, 18, 20] </ref>, respectively. Whereas segmentation is generally difficult and unstable, we show that some problems can be alleviated through the use of a pre-recorded image database and intelligent control of the camera.
Reference: [19] <author> K. Rohr. </author> <title> Towards model-based recognition of human movements in image sequences. CVGIP: </title> <booktitle> Image Understanding, </booktitle> <volume> 59(1) </volume> <pages> 94-115, </pages> <year> 1994. </year>
Reference: [20] <author> P.M. Sharkey, I.D. Reid, P.F. McLauchlan, and D.W. Murray. </author> <title> Real-time control of an active stereo head/eye platform. </title> <booktitle> In Proceedings of the 2nd International Conference on Automation, Robotics and Computer Vision, </booktitle> <year> 1992. </year>
Reference-contexts: Demonstrations of the efficacy of planned camera motion in object recognition and tracking can be found in [5] and <ref> [15, 17, 18, 20] </ref>, respectively. Whereas segmentation is generally difficult and unstable, we show that some problems can be alleviated through the use of a pre-recorded image database and intelligent control of the camera.
Reference: [21] <author> K. Tarabanis, R.Y. Tsai, and P.K. Allen. </author> <title> Analytical characterization of the feature detectability constraints of resolution, focus, and field of view for vision sensor planning. CVGIP: </title> <booktitle> Image Understanding, </booktitle> <volume> 59 </volume> <pages> 340-358, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: The method is based on active control of a pan, tilt, and zoom camera and the use of a pre-recorded image database of the environment. Active control of the camera is a form of sensor planning advocated in [2] and [1] and analyzed in <ref> [21] </ref>. The task of sensor planning, while receiving little attention in the past, is very important during tracking because the camera's state parameters determine the quality of the resulting image and indeed whether the target will be within the image.
Reference: [22] <author> J. Weng, N. Ahuja, and T.S. Huang. </author> <title> Learning recognition and segmentation using the cresceptron. </title> <booktitle> In ICCV93, </booktitle> <pages> pages 121-128, </pages> <year> 1993. </year>
Reference: [23] <author> C. Wren, A. Azarbayejani, T. Darrell, and A. Pentland. Pfinder: </author> <title> Real-time tracking of the human body. </title> <booktitle> In International Conference on Automatic Face and Gesture Recognition, </booktitle> <pages> pages 51-60, </pages> <address> Killington, Vermont, </address> <month> October </month> <year> 1996. </year>
Reference: [24] <author> J. Yang and A. Waibel. </author> <title> A real-time face tracker. </title> <booktitle> In WACV, </booktitle> <year> 1996. </year>
Reference: [25] <author> Y. Ye. </author> <title> Sensor planning for object search. </title> <type> PhD Thesis, </type> <institution> Comp. Sci., University of Toronto, </institution> <year> 1997. </year>
Reference-contexts: The number of such layers or angles is n 0 = b ln ( D ln ( F 0 1c. These equations are derived using geometric constraints and the requirement that the area of the target patch in the image remain constant from one layer to the next (see <ref> [25] </ref> for details). Each layer of the layered sphere can be successfully scanned for the target using the corresponding angle size hw; hi by sweeping the pan and tilt parameters hp; ti of the camera. <p> To examine the entire layer for the target we need a set of camera directions, hp; ti, such that the union of their effective volumes cover the whole layer with little overlap. The following algorithm generates a set S of viewing directions required for covering the whole sphere. See <ref> [25] </ref> for details of derivation. 1.
References-found: 25

