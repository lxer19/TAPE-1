URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-93-1136/CS-TR-93-1136.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-93-1136/
Root-URL: http://www.cs.wisc.edu
Title: EXTENDING THE SCALABLE COHERENT INTERFACE FOR LARGE-SCALE SHARED-MEMORY MULTIPROCESSORS  
Author: by ROSS EVAN JOHNSON James R. Goodman 
Degree: A thesis submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy (Computer Sciences) at the  
Date: 1993  
Affiliation: UNIVERSITY OF WISCONSIN-MADISON  
Note: Under the supervision of Professor  
Abstract-found: 0
Intro-found: 1
Reference: [AdLa62] <author> G. M. Adel'son-Vel'skii and E. M. Landis, </author> <title> ``An Algorithm for the Organization of Information,'' </title> <journal> Soviet Mathematics Doklady 3, </journal> <month> 5 (September </month> <year> 1962), </year> <month> 1259-1262. </month> <journal> Russian original in Doklady Akademii NAUK SSSR, </journal> <volume> TOM 146, </volume> <pages> Nos. 1-6, 263-266. </pages>
Reference-contexts: Ch. 1 17 1.3.5. Data Structures Balanced trees have been widely researched as a structure for storing information, beginning with AVL trees <ref> [AdLa62] </ref>. However, the standard uses of trees lead to tree protocols that are inappropriate for cache coherence. The parallelizing of tree operations has been spear-headed by the database community [BaSc77, Elli80a, Elli80b, KuLe80, LeYa81, FoCa84, Sagi85]. For database applications, a search tree must support efficient insertion, deletion, and searching. <p> Instead, beginning an insertion at a leaf makes sense because this is where the tag will most likely come to rest. Furthermore, if appending a single tag to a leaf creates an unbalanced tree, then the familiar AVL rotations <ref> [AdLa62, Knut73] </ref> can be used to balance the tree in h rotations for tree-height h. Simultaneous appends to the same leaf are also possible, using a combining mechanism [JLGS90] to avoid the directory hot spot, and then the familiar AVL rotations can be used in parallel, with some care. <p> Purging is accomplished by forwarding a message throughout the tree from the insertion point to the leaves and then collecting the responses from the leaves to the insertion point. The height of the tree is balanced during insertions by the simplest AVL rotation of Adel'son-Vel'skii and Landis <ref> [AdLa62, Knut73] </ref>. The AVL balancing property is that a node's Ch. 5 169 subtree heights differ by at most one. In contrast, our tree maintains a new balancing property, called strictly growing, which allows parallelization of the AVL rotations. <p> This implies that there does not exist one structure that is (near) optimal for each multicast. Ch. 7 237 STEM is a new cache-coherence protocol for multiprocessors with thousands of processors. Tree merging constructs a binary tree from a linked-list of cache lines by using single AVL rotations <ref> [AdLa62] </ref>. Trees are probabilistically balanced by the strictly growing property, which is applied to the left side of the tree (originally the list). The original list may be created by using the combining mechanism of James et al. [JLGS90].
Reference: [AdHi90] <author> Sarita V. Adve and Mark D. Hill, </author> <title> ``Weak Ordering ANew Definition,'' </title> <booktitle> Proceedings of the Seventeenth Annual International Symposium on Computer Architecture 18, </booktitle> <month> 2 (May </month> <year> 1990), </year> <pages> 2-14. </pages>
Reference-contexts: This is another argument against write update: it is difficult to implement updating writes that obey sequentially consistency over general networks. There exist weaker memory models <ref> [DuSB86, DuSB88, AdHi90, GLLG90, DuSc90, Good91, GAGH92, Mosb93] </ref> that may provide better performance than sequential consistency. A complete discussion of these is beyond the scope of this dissertation. It suffices to say that many models make use of an mechanism called a weak write. <p> When the purge initiator (the head) becomes the tail too, the purge is completed. To maintain a view of sequential consistency [Lamp79], the writing processor stalls and the cache does not release the new data to other caches until the purge is completed. However, some weaker memory models <ref> [DuSB86, AdHi90, GLLG90] </ref> allow processors to continue and data to be released before the purge is completed, a topic for future work. Next, we discuss each of the six overlapping phases in detail, four phases for reading data and two phases for writing data. <p> The SCI working group (P1596.2) is considering ways to extend STEM to do write update. This work should be continued and completed. Furthermore, when writes are not atomic, it is important to know how different memory models can help the programmer reason with (or without) sequential consistency <ref> [Lamp79, DuSB86, AdHi90, GLLG90, Netz91, GAGH92] </ref>. What information does the hardware and/or compiler require from the programmer and what debugging tools can help the programmer find data races [AHMN91]? The same working group should also look for ways to simplify STEM.
Reference: [AHMN91] <author> Sarita V. Adve, Mark D. Hill, Barton P. Miller, and Robert H. B. Netzer, </author> <title> ``Detecting Data Races on Weak Memory Systems,'' </title> <booktitle> Proceedings of the Eighteenth Annual International Symposium on Computer Architecture 19, </booktitle> <month> 3 (May </month> <year> 1991), </year> <pages> 234-243. </pages>
Reference-contexts: What information does the hardware and/or compiler require from the programmer and what debugging tools can help the programmer find data races <ref> [AHMN91] </ref>? The same working group should also look for ways to simplify STEM. A detailed comparison to RDE and SCI shows that reducing SCI's latency from linear to logarithmic in the Ch. 7 239 number of participating cache is extremely important.
Reference: [ASHH88] <author> Anant Agarwal, Richard Simoni, John Hennessy, and Mark Horowitz, </author> <title> ``An Evaluation of Directory Schemes for Cache Coherence,'' </title> <booktitle> Proceedings of the Fifteenth Annual International Symposium on Computer Architecture 16, </booktitle> <month> 2 (May </month> <year> 1988), </year> <pages> 280-289. </pages>
Reference-contexts: A version of this is implemented in the Stanford DASH [LLGG90]. Directory protocols have a performance problem in that each directory serializes insertions and deletions and the network connection serializes the messages for purges. This problem has Ch. 1 9 not been highlighted in current performance studies <ref> [ASHH88, EgKa88, OKNe90, CFKA90, GHGM91, GuWe92] </ref> because they have focused on the performance of small machines with at least one study [GHGM91] admitting the assumption that shared instructions hit in the cache. With thousands of processors, anything sequential is unacceptable, unless the case is extremely infrequent. <p> If the overflow bit is set, then future purges are broadcast to all caches in the system. Agarwal et al. <ref> [ASHH88] </ref> classify directory protocols with Dir i X, where i is the number of pointers and X is either B or NB for broadcast or no broadcast respectively. Full-map protocols [Tang76, CeFe78, LLGG90] function as Dir n NB, where n is the maximum number of data locations. <p> The bit is set whenever any cache in the group has a copy of the data. Gupta et al. recommend the use of limited pointers until the directory overflows, after which coarse vectors are used. The superset scheme, proposed by Agarwal et al. <ref> [ASHH88] </ref>, stores one pointer and one bit mask of the same size. The first set insertion determines the pointer and sets all bits in the mask. Each subsequent insertion unsets the corresponding mask bit for every bit of the insertion pointer that is different from the directory pointer. <p> Therefore, a DP directory can be simpler than the directories of all previously proposed directory and hierarchical protocols, including the Dir 0 B and Dir 1 X varieties <ref> [ArBa84, ASHH88, HLRW92] </ref>. Nearly all of the complexity of a DP protocol is in the cache controllers. By using a programmable entity for a cache controller, DP protocols can be debugged and enhanced without affecting the fast and simple directories that are implemented in hardware. <p> Therefore, STEM's directory is simpler than previously proposed distributed-directory protocols, including ones of the Dir 0 B and Dir 1 X varieties <ref> [ArBa84, ASHH88, HLRW92] </ref>. We expect a first implementation of STEM to use programmable cache controllers to substantially reduce the design time for producing a working prototype and allow minor hhhhhhhhhhhhhhhhhhhhhhhhhhh 21 This published mechanism is compatible with the memory controller defined in SCI [Comm91]. <p> Although this hot spot is of short duration for small machines, a sequential-access latency will have a significant impact on the performance of massively parallel machines. Current performance studies of coherence protocols <ref> [ArBa86, ASHH88, EgKa88, OKNe90, CFKA90, GHGM91, GuWe92] </ref> have not addressed this problem because they have focused on the performance of small machines with at least one study [GHGM91] assuming that shared instructions hit in the cache. <p> An optimization for pairwise sharing is also described. 6.2.1.1. Stanford DASH We use the DASH coherence protocol [LLGG90, LLGW92] as a representative for all distributed-directory protocols that do not distribute the storage of each sharing set <ref> [Tang76, CeFe78, ArBa84, ASHH88, OKNe90, ChKA91, SiHo91, HLRW92] </ref>. DASH maintains the sharing set as a bit map of 64 clusters, one bit per shared-bus cluster of four processors. To simplify the discussion and since all directory protocols could cluster several processors, we reduce DASH's cluster size to one.
Reference: [AlGo89] <author> George S. Almasi and Allan Gottlieb, </author> <title> Highly Parallel Computing, </title> <publisher> Benjamin/Cummings Publishing Company, Inc., </publisher> <address> Redwood City, California, </address> <year> 1989. </year>
Reference-contexts: The recursive-doubling extensions to SCI, outlined in Chapter 3, are not currently in vogue. In addition to efficient reads, writes, and cache-line replacements, it is conceivable that massively parallel machines will need nonserial synchronization mechanisms, such as combining fetch-and-add <ref> [Ston84, AlGo89, SoSG89] </ref>. On one hand, it is unclear whether software techniques [YeTL87, GoVW89, YeTa90] can compete with hardware speeds. On the other hand, current hardware mechanisms [GGKM83, PBGH85] are expensive 20 , save state in the network, and constrain the return path of responses.
Reference: [Amda67] <author> G. M. </author> <title> Amdahl, ``Validity of the Single-Processor Approach to Achieving Large Scale Computing Capabilities,'' </title> <booktitle> AFIPS Proceedings of the 1967 Spring Joint Computer Conference, </booktitle> <month> April </month> <year> 1967, </year> <pages> 483-485. </pages>
Reference-contexts: However, we would entertain a cost of O (P 1.5 ), although we know of no such networks. Ch. 1 5 of notions. On one hand, Amdahl <ref> [Amda67] </ref> observes that every program has a serial part. No matter how many processors are used, speedup of a program is limited by sequential execution of its serial part. This implies that scalability is useless.
Reference: [AGKP90] <author> S. B. Anderson, P. W. Gorham, S. R. Kulkarni, T. A. Prince, and A. Wolszczan, </author> <title> ``Discovery of Two Radio Pulsars in the Globular Cluster M15,'' </title> <booktitle> Nature 346, </booktitle> <month> 6279 (July </month> <year> 1990), </year> <pages> 42-44. </pages>
Reference-contexts: Message-passing multiprocessors are commercially hhhhhhhhhhhhhhhhhhhhhhhhhhh 44 In contrast, a Harvard Architecture has separate memories for data and instructions. Ch. 7 234 successful and they are the right choice for some applications, such as nearest-neighbor algorithms with course-grain sharing <ref> [AGKP90] </ref>. However, we believe that some successful machines of the future will support the higher-level mechanism of shared memory, where an efficient view of a single memory is presented to the programmer through automatic replication and coherence-management of data stored in physically separate memories.
Reference: [ArBa84] <author> James Archibald and Jean-Loup Baer, </author> <title> ``An Economical Solution to the Cache Coherence Problem,'' </title> <booktitle> Proceedings of the Eleventh Annual International Symposium on Computer Architecture 12, </booktitle> <month> 3 (June </month> <year> 1984), </year> <pages> 355-362. </pages> <note> This paper is not in the 12th ISCA, as is commonly cited. </note>
Reference-contexts: Data coherence between caches is known as cache coherence. We say that a multiprocessor system is cache coherent if a read access to any line always returns the most recently written value of that line <ref> [ArBa84] </ref>. A cache-coherence protocol uses network messages to replicate data and manage coherence in a cache coherent multiprocessor. An efficient coherence protocol minimizes the number of network messages on the critical path, called latency, and minimizes the total number of network messages, called traffic. <p> Full-map protocols [Tang76, CeFe78, LLGG90] function as Dir n NB, where n is the maximum number of data locations. The two-bit solution of Archibald and Baer <ref> [ArBa84] </ref> is Dir 0 B. One disadvantage of limited directories is that they do not allow all caches to share the same data, such as shared instructions. 1.3.3.2. Directory Groupings Three directory schemes reduce the storage requirements by grouping the sharing sets for different lines. <p> Therefore, a DP directory can be simpler than the directories of all previously proposed directory and hierarchical protocols, including the Dir 0 B and Dir 1 X varieties <ref> [ArBa84, ASHH88, HLRW92] </ref>. Nearly all of the complexity of a DP protocol is in the cache controllers. By using a programmable entity for a cache controller, DP protocols can be debugged and enhanced without affecting the fast and simple directories that are implemented in hardware. <p> Therefore, STEM's directory is simpler than previously proposed distributed-directory protocols, including ones of the Dir 0 B and Dir 1 X varieties <ref> [ArBa84, ASHH88, HLRW92] </ref>. We expect a first implementation of STEM to use programmable cache controllers to substantially reduce the design time for producing a working prototype and allow minor hhhhhhhhhhhhhhhhhhhhhhhhhhh 21 This published mechanism is compatible with the memory controller defined in SCI [Comm91]. <p> An optimization for pairwise sharing is also described. 6.2.1.1. Stanford DASH We use the DASH coherence protocol [LLGG90, LLGW92] as a representative for all distributed-directory protocols that do not distribute the storage of each sharing set <ref> [Tang76, CeFe78, ArBa84, ASHH88, OKNe90, ChKA91, SiHo91, HLRW92] </ref>. DASH maintains the sharing set as a bit map of 64 clusters, one bit per shared-bus cluster of four processors. To simplify the discussion and since all directory protocols could cluster several processors, we reduce DASH's cluster size to one.
Reference: [ArBa86] <author> James Archibald and Jean-Loup Baer, </author> <title> ``Cache Coherence Protocols: Evaluation Using a Multiprocessor Simulation Model,'' </title> <journal> ACM Transactions on Computer Systems 4, </journal> <month> 4 (November </month> <year> 1986), </year> <pages> 273-298. </pages>
Reference-contexts: We next describe four classes of hardware cache-coherence protocols: snooping, hierarchical, directory, and distributed-pointer protocols. After that, we discuss related work in data structures. 1.3.1. Snooping Protocols Snooping protocols <ref> [Good83, RuSe84, PaPa84, KEWP85, SwSm86, ArBa86, ThSt87] </ref> rely on the broadcast capability of a shared bus. Each cache remembers to which sharing sets it belongs (implicit by the lines that are stored), as well as the notions of data validity, exclusiveness, and ownership for each cache line. <p> Although this hot spot is of short duration for small machines, a sequential-access latency will have a significant impact on the performance of massively parallel machines. Current performance studies of coherence protocols <ref> [ArBa86, ASHH88, EgKa88, OKNe90, CFKA90, GHGM91, GuWe92] </ref> have not addressed this problem because they have focused on the performance of small machines with at least one study [GHGM91] assuming that shared instructions hit in the cache.
Reference: [AKLM89] <author> M. J. Atallah, S. R. Kosaraju, L. L. Larmore, G. L. Miller, and S-H. Teng, </author> <title> ``Constructing Trees in Parallel,'' </title> <booktitle> Proceedings of the 1989 ACM Symposium on Parallel Algorithms and Architectures (SPAA '89), </booktitle> <month> June </month> <year> 1989, </year> <pages> 421-431. </pages>
Reference-contexts: Due to the sorted nature of a search tree, all insertions and searches invariably start at the root. The hot spot at the root makes all of these algorithms inappropriate for efficient cache-coherence protocols. The theory community <ref> [MoIy85, DePI86, AKLM89, KiPr90] </ref> constructs various optimal and near optimal search trees for various applications, such as Huffman encoding. However, these theoretical algorithms make unrealistic assumptions about communication through shared memory, with at least two [MoIy85, DePI86] assuming that the data structure is stored in an array.
Reference: [BaWa88] <author> Jean-Loup Baer and Wen-Hann Wang, </author> <title> ``On the Inclusion Properties for Multilevel Cache Hierarchies,'' </title> <booktitle> Proceedings of the Fifteenth Annual International Symposium on Computer Architecture 16, </booktitle> <month> 2 (May </month> <year> 1988), </year> <pages> 73-80. </pages>
Reference-contexts: Hierarchical Protocols Hierarchical protocols make use of the cache hierarchy implied by a tree of buses <ref> [Wils87, BaWa88, VeJS89, ChGB91, YaTB92] </ref>. A cache hierarchy on a k-ary n-cube, implemented as either a grid of buses [GoWo88] or a set of rings [Scot91, Scot92], has a different tree per Ch. 1 7 memory module so that the tree root is not a network bottleneck. <p> Multilevel Inclusion One way to avoid the storage overhead of the naive solution is to maintain multilevel inclusion (MLI) <ref> [LaMu79, Wils87, BaWa88] </ref>, where a cache stores a superset of all the lines stored in Ch. 1 8 descendant caches. When a cache line is purged, the purge is propagated to only those children that have copies. <p> To save space, a parent need only store a vector if children have copies. To avoid storing data for every inclusion, these vectors can be stored in a separate inclusion cache. If it is infeasible to guarantee that there will always be space in the inclusion cache <ref> [BaWa88] </ref>, as argued by Scott [Scot92] for grids of buses, then older children can be purged to make room for newer children [Wils87]. 1.3.2.2. <p> We also limit our scope to the coherence of caches and memories that are connected to the network, assuming that the intracluster coherence for TLB and cache hierarchies can be managed by another mechanism, such as multilevel inclusion <ref> [Wils87, BaWa88] </ref> or pruning caches [GoWo88, Scot91]. The description of a write-invalidate protocol must specify how the set of sharing caches is stored, how caches are inserted, deleted, and purged with respect to the set, and how the notions of data validity and ownership are managed.
Reference: [BaSc77] <author> R. Bayer and M. Schkolnick, </author> <title> ``Concurrency of Operations on B-Trees,'' </title> <journal> Acta Informatica 9, </journal> <volume> 1 (1977), </volume> <pages> 1-21. 246 </pages>
Reference-contexts: Data Structures Balanced trees have been widely researched as a structure for storing information, beginning with AVL trees [AdLa62]. However, the standard uses of trees lead to tree protocols that are inappropriate for cache coherence. The parallelizing of tree operations has been spear-headed by the database community <ref> [BaSc77, Elli80a, Elli80b, KuLe80, LeYa81, FoCa84, Sagi85] </ref>. For database applications, a search tree must support efficient insertion, deletion, and searching. Due to the sorted nature of a search tree, all insertions and searches invariably start at the root.
Reference: [Bian89] <author> Ronald Bianchini, </author> <title> ``Ultracomputer Packaging and Prototypes,'' Ultracomputer Note #152, </title> <institution> Ultracomputer Research Laboratory, Courant Institute, NYU, </institution> <month> January </month> <year> 1989. </year>
Reference-contexts: The topologies are always ordered as given for fanout 5, except that Multicube is lower than Livefly for fanout 3 and up to 128 nodes. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh As the topology size increases, so does the physical size of the multiprocessor. Bianchini <ref> [Bian89] </ref> and Dally [Dall90] have considered the effects of three-space on the performance of topologies, but this is beyond our scope. Essentially, the physical layout of the topology becomes increasingly important for increasing system size, affecting the accuracy of our results on delay under light load.
Reference: [BrHo90] <author> Eugene D. Brooks III and Joseph E. Hoag, </author> <title> ``A Scalable Coherent Cache System with Incomplete Directory State,'' </title> <booktitle> Proceedings of the 1990 International Conference on Parallel Processing (ICPP '90), </booktitle> <month> August </month> <year> 1990, </year> <month> I-553-554. </month>
Reference-contexts: However, Censier and Feautrier do not mention the directory's need for an owner pointer per superline, limiting their proposal to networks that allow snooping. Coarse vectors, independently proposed by Gupta et al. [GuWM90] and Brooks and Hoag <ref> [BrHo90] </ref>, group storage by caches. Instead of maintaining a bit map of size proportional to the number of caches, each bit represents a group of caches. The bit is set whenever any cache in the group has a copy of the data.
Reference: [Burk92] <author> Henry Burkhardt, III, </author> <title> ``Kendall Square Research Technical Summary,'' </title> <type> Technical Report, </type> <institution> Kendall Square Research, </institution> <year> 1992. </year> <title> Contact Susan Parrish (info@ksr.com) for current information. </title>
Reference-contexts: Given reasonable cycle times, Scott et al. [ScGV92] show that a single ring does indeed have better performance than a single bus, both for throughput and message delay. The KSR1 uses what its designers <ref> [Burk92] </ref> call an insertion-modification token ring. Ch. 1 19 the time such a standard is complete, the technology it standardized is usually obsolete and irrelevant. Therefore it has become necessary to be more pro-active, creating and inventing in the standardization process, in order to have a chance at relevance.
Reference: [BuGv46] <author> Arthur W. Burks, Herman H. Goldstine, and John von Neumann, </author> <title> ``Preliminary Discussion of the Logical Design of an Electronic Computing Instrument,'' in Computer Structures: Readings and Examples, </title> <editor> C. G. Bell and A. Newell (eds.), </editor> <publisher> McGraw-Hill, </publisher> <address> New York, NY, </address> <year> 1971, </year> <pages> 92-119. </pages> <note> Taken from Report to U.S. </note> <institution> Army Ordinance Department, </institution> <year> 1946. </year>
Reference-contexts: Motivation People covet more and more computing power and it is the job of computer architects and engineers to create this power. In 1641, Blaise Pascal (age 18) designed a computing machine capable of performing simple arithmetic calculations. In the 1940s, John von Neumann <ref> [BuGv46] </ref> proposed his well-known computer model with one processor and memory, where the memory stores both instructions and data 44 and the processor is composed of an arithmetic unit, registers, and an instruction counter. The foundations of computer architecture rest on this model.
Reference: [CaKP91] <author> David Callahan, Ken Kennedy, and Allan Porterfield, </author> <title> ``Software Prefetching,'' </title> <booktitle> Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-IV), </booktitle> <month> April </month> <year> 1991, </year> <pages> 40-52. </pages>
Reference-contexts: Ch. 6 213 reasonable framework for comparing various cache-coherence protocols, paying special attention to results that affect programs with high levels of sharing. Since this dissertation is the first study of some new coherence protocols, we do not consider the effects of other latency-reducing techniques <ref> [CaKP91, GHGM91] </ref>. For cache-line replacements, we compare the average latency to prepare a cache line to hold new data, starting with valid data in a stable state with a different memory address.
Reference: [CeFe78] <author> Lucien M. Censier and Paul Feautrier, </author> <title> ``A New Solution to Coherence Problems in Multicache Systems,'' </title> <journal> IEEE Transactions on Computers 27, </journal> <month> 12 (December </month> <year> 1978), </year> <pages> 1112-1118. </pages>
Reference-contexts: A directory protocol can be implemented on top of a point-to-point network with any topology, thereby avoiding the bottleneck of a centralized resource (except for the central-directory protocol of Tang [Tang76]). Censier and Feautrier <ref> [CeFe78] </ref> store the sharing set as a bit map per directory, where a bit for each cache indicates whether the cache has a copy of the line. A version of this is implemented in the Stanford DASH [LLGG90]. <p> Other studies [CFKA90, GuWe92] also suggest rewriting software that does not scale. However, the point of the shared-memory paradigm is to make it easier to generate and reuse correct and efficient programs. The storage for a bit-map implementation of each directory <ref> [CeFe78, LLGG90] </ref> grows in proportion to the number of caches in the system, making this implementation too costly for kiloprocessor systems. A number of alternatives have been proposed. 1.3.3.1. Limited Directories Limited directories reduce the directory storage by maintaining only i cache pointers for the sharing set. <p> Agarwal et al. [ASHH88] classify directory protocols with Dir i X, where i is the number of pointers and X is either B or NB for broadcast or no broadcast respectively. Full-map protocols <ref> [Tang76, CeFe78, LLGG90] </ref> function as Dir n NB, where n is the maximum number of data locations. The two-bit solution of Archibald and Baer [ArBa84] is Dir 0 B. <p> This reduces the storage overhead by a constant factor of x for large systems, where x is the number of lines per superline. An earlier Ch. 1 10 and more complex version of sectored directories is proposed by Censier and Feautrier <ref> [CeFe78] </ref>, where each cache stores one counter per superline so that a cache's last cache-line replacement of the given superline can reset the corresponding bit in the directory. <p> An optimization for pairwise sharing is also described. 6.2.1.1. Stanford DASH We use the DASH coherence protocol [LLGG90, LLGW92] as a representative for all distributed-directory protocols that do not distribute the storage of each sharing set <ref> [Tang76, CeFe78, ArBa84, ASHH88, OKNe90, ChKA91, SiHo91, HLRW92] </ref>. DASH maintains the sharing set as a bit map of 64 clusters, one bit per shared-bus cluster of four processors. To simplify the discussion and since all directory protocols could cluster several processors, we reduce DASH's cluster size to one.
Reference: [CFKA90] <author> David Chaiken, Craig Fields, Kiyoshi Kurihara, and Anant Agarwal, </author> <title> ``Directory-Based Cache Coherence in Large-Scale Multiprocessors,'' </title> <booktitle> IEEE Computer 23, </booktitle> <month> 6 (June </month> <year> 1990), </year> <pages> 49-58. </pages>
Reference-contexts: A version of this is implemented in the Stanford DASH [LLGG90]. Directory protocols have a performance problem in that each directory serializes insertions and deletions and the network connection serializes the messages for purges. This problem has Ch. 1 9 not been highlighted in current performance studies <ref> [ASHH88, EgKa88, OKNe90, CFKA90, GHGM91, GuWe92] </ref> because they have focused on the performance of small machines with at least one study [GHGM91] admitting the assumption that shared instructions hit in the cache. With thousands of processors, anything sequential is unacceptable, unless the case is extremely infrequent. <p> With thousands of processors, anything sequential is unacceptable, unless the case is extremely infrequent. Matloff [Matl91] claims that scalability is not important, arguing that most software can be rewritten to group the sharing. Other studies <ref> [CFKA90, GuWe92] </ref> also suggest rewriting software that does not scale. However, the point of the shared-memory paradigm is to make it easier to generate and reuse correct and efficient programs. <p> Although this hot spot is of short duration for small machines, a sequential-access latency will have a significant impact on the performance of massively parallel machines. Current performance studies of coherence protocols <ref> [ArBa86, ASHH88, EgKa88, OKNe90, CFKA90, GHGM91, GuWe92] </ref> have not addressed this problem because they have focused on the performance of small machines with at least one study [GHGM91] assuming that shared instructions hit in the cache. <p> Matloff [Matl91] claims that scalability is not important because most software can be rewritten to group the sharing. Other studies <ref> [CFKA90, GuWe92] </ref> also suggest rewriting software that does not scale. However, the point of the shared-memory paradigm is to make it easier to generate and reuse correct and efficient parallel programs. In this chapter, we compare a representative set of cache-coherence protocols. <p> Therefore, when considering the effects of hot spots and tree saturation, the examination of poorly written applications is more important than the examination of well-behaved applications. In particular, some applications exhibit large degrees of sharing with frequently changing data. Ch. 6 209 Some studies <ref> [CFKA90, Matl91, GuWe92] </ref> suggest rewriting applications that have a lot of system-wide sharing and we agree that it is always possible to rewrite software to avoid large sharing levels, using software combining [YeTL87, GoVW89, YeTa90] and/or other techniques.
Reference: [ChKA91] <author> David Chaiken, John Kubiatowicz, and Anant Agarwal, </author> <note> ``LimitLESS Directories: </note>
Reference-contexts: Gupta et al. [GuWM90] claim that the superset scheme performs poorly compared to a full bit map. 1.3.3.3. Software Support Two directory schemes reduce directory storage by invoking a trap to software for problem cases. The LimitLESS Directories of Chaiken et al. <ref> [ChKA91] </ref>, implemented in the MIT Alewife, use limited directories of size i and trap to software when a pointer overflow occurs. LimitLESS stands for limited directories that are Locally Extended through Software Support. <p> An optimization for pairwise sharing is also described. 6.2.1.1. Stanford DASH We use the DASH coherence protocol [LLGG90, LLGW92] as a representative for all distributed-directory protocols that do not distribute the storage of each sharing set <ref> [Tang76, CeFe78, ArBa84, ASHH88, OKNe90, ChKA91, SiHo91, HLRW92] </ref>. DASH maintains the sharing set as a bit map of 64 clusters, one bit per shared-bus cluster of four processors. To simplify the discussion and since all directory protocols could cluster several processors, we reduce DASH's cluster size to one. <p> We do not consider the scaling problems associated with DASH's directory storage because reasonable solutions exist, such as pointer caches of Section 1.3.3.4 and LimitLESS Directories <ref> [ChKA91] </ref>. Ch. 6 208 6.2.2.1. Hot Spots and Combining Pfister and Norton [PfNo85] propose a hot-spot model that incorporates a background of uniform traffic and a percentage of messages destined for a single location (the hot spot).
References-found: 20

