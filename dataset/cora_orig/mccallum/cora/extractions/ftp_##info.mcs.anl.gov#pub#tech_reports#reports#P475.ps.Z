URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P475.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts94.htm
Root-URL: http://www.mcs.anl.gov
Title: Computational Electromagnetics and Parallel Dense Matrix Computations  
Author: Kimmo Forsman William Gropp Lauri Kettunen David Levine 
Abstract: We present computational results using CORAL, a parallel, three-dimensional, nonlinear magnetostatic code based on a volume integral equation formulation. A key feature of CORAL is the ability to solve, in parallel, the large, dense systems of linear equations that are inherent in the use of integral equation methods. Using the Chameleon and PSLES libraries ensures portability and access to the latest linear algebra solution technology. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Bossavit. </author> <title> Whitney forms: A class of finite elements for three-dimensional computations in electromagnetism. </title> <booktitle> In IEE Proceedings, </booktitle> <volume> volume 135, Pt. A, </volume> <pages> pages 493-499, </pages> <year> 1988. </year>
Reference-contexts: In order to establish a discrete problem, the magnetic field H is approximated in the space of "edge elements," that is, Whitney elements of degree p = 1 <ref> [1] </ref>. The system of equations is developed from Eq. (6) by choosing h 0 the same as the basis functions of the space spanned by the edge elements.
Reference: [2] <author> W. D. Gropp and B. Smith. </author> <title> Users manual for the Chameleon parallel programming tools. </title> <type> Technical Report ANL-93/23, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <month> March </month> <year> 1993. </year>
Reference-contexts: One of these, the Chameleon <ref> [2] </ref> library, is a second-generation message-passing system that provides a uniform way to access third-party message-passing libraries such as p4, PICL, and PVM, as well as vendor-specific message-passing libraries such as Intel's NX and IBM's EUI-H.
Reference: [3] <author> W. D. Gropp and B. F. Smith. </author> <title> Scalable, extensible, and portable numerical libraries. </title> <booktitle> In Proceedings of Scalable Parallel Libraries Conference, </booktitle> <pages> pages 87-93. </pages> <publisher> IEEE, </publisher> <year> 1994. </year>
Reference-contexts: After a few cycles the Newton-Raphson method is applied to accelerate the convergence. 3 Parallelization Aspects The parallel version of CORAL code is built on top of two libraries from PETSc (Portable and Extensible Tools for Scientific computing) <ref> [3] </ref>, a large toolkit of software for portable, parallel (and serial) scientific computation. <p> The other PETSc library used was the Parallel Simplified Linear Equation Solvers (PSLES) package <ref> [3] </ref>. This library provides access to a wide variety of iterative solvers, preconditioners, and accelerators in an easy-to-use, efficient, and portable manner. PSLES supports direct methods using sparse or dense LU, and a wide variety of iterative methods and preconditioners.
Reference: [4] <author> L. Kettunen, K. Forsman, D. Levine, and W. Gropp. </author> <title> Solutions of team problem #13 using integral equations in a sequential and parallel computing environment. </title> <booktitle> In Proceedings of the Fourth International TEAM Workshop, </booktitle> <pages> pages 41-43, </pages> <institution> Florida International University, Miami, </institution> <year> 1994. </year>
Reference-contexts: Note the similarity of the two code fragments. 4 Computational Experiments In this paper we focus on linear algebra and parallel computing experiments. Details of the accuracy of the calculated electromagnetic field have been given in other papers <ref> [4, 5, 6] </ref>. 4 Table 1 TEAM 13 Timings, 2041 Equations Preconditioner LU GMRES Bi-CGStab None 99.1 BDD (4) 25.8/(25) 40.8/(23) Band (25%) 26.4/(21) 38.7/(18) SILUF 229.5/(177) Not converged Table 2 TEAM 20 Timings, 2867 Equations Preconditioner LU GMRES Bi-CGStab None 166.4 BDD (4) 30.5/(16) 41.7/(15) Band (25%) 28.3/(12) 39.4/(11) SILUF
Reference: [5] <author> L. Kettunen, K. Forsman, D. Levine, and W. Gropp. </author> <title> Solutions of team problems 13 and 20 using a volume integral formulation. </title> <booktitle> In Proceedings of Aix-les-Bains TEAM Workshop, </booktitle> <address> Aix-les-Bains, France, </address> <year> 1994. </year>
Reference-contexts: Note the similarity of the two code fragments. 4 Computational Experiments In this paper we focus on linear algebra and parallel computing experiments. Details of the accuracy of the calculated electromagnetic field have been given in other papers <ref> [4, 5, 6] </ref>. 4 Table 1 TEAM 13 Timings, 2041 Equations Preconditioner LU GMRES Bi-CGStab None 99.1 BDD (4) 25.8/(25) 40.8/(23) Band (25%) 26.4/(21) 38.7/(18) SILUF 229.5/(177) Not converged Table 2 TEAM 20 Timings, 2867 Equations Preconditioner LU GMRES Bi-CGStab None 166.4 BDD (4) 30.5/(16) 41.7/(15) Band (25%) 28.3/(12) 39.4/(11) SILUF
Reference: [6] <author> L. Kettunen, K. Forsman, D. Levine, and W. Gropp. </author> <title> Volume integral equations in nonlinear 3-d magnetostatics. </title> <type> Technical Report MCS-P460-0894, </type> <institution> Argonne National Laboratory, </institution> <year> 1994. </year>
Reference-contexts: More complete details may be found in <ref> [6] </ref>. We denote magnetic flux density with B, magnetic field strength with H, magnetization with M , permeability with , and susceptibility with O. The formulation behind CORAL is based on the idea of superposition of fields from current and magnetization sources. <p> Note the similarity of the two code fragments. 4 Computational Experiments In this paper we focus on linear algebra and parallel computing experiments. Details of the accuracy of the calculated electromagnetic field have been given in other papers <ref> [4, 5, 6] </ref>. 4 Table 1 TEAM 13 Timings, 2041 Equations Preconditioner LU GMRES Bi-CGStab None 99.1 BDD (4) 25.8/(25) 40.8/(23) Band (25%) 26.4/(21) 38.7/(18) SILUF 229.5/(177) Not converged Table 2 TEAM 20 Timings, 2867 Equations Preconditioner LU GMRES Bi-CGStab None 166.4 BDD (4) 30.5/(16) 41.7/(15) Band (25%) 28.3/(12) 39.4/(11) SILUF
Reference: [7] <author> T. Nakata, N. Takahashi, K. Fujiwara, K. Muramatsu, T. Imai, and Y. Shiraki. </author> <title> Numerical analysis and experiments of 3-D non-linear magnetostatic model. </title> <booktitle> In Proceedings of TEAM Workshop on Computation of Applied Electromagnetics in Materials, </booktitle> <pages> pages 308-310, </pages> <address> Okayama, Japan, </address> <year> 1990. </year>
Reference-contexts: All the results in this section were computed on a DEC Alpha 3000-600 AXP workstation. The test problems used are the international TEAM (Testing Electromagnetic Analysis Methods) benchmark problems, numbers 13 <ref> [7] </ref> and 20 [8]. TEAM 13 was run using a mesh that resulted in a system of 2041 equations. TEAM 20 was run using a mesh that resulted in a system of 2867 equations.
Reference: [8] <author> T. Nakata, N. Takahashi, and H. Morishige. </author> <title> Analysis of 3-d static force problem. </title> <booktitle> In Proceedings of TEAM Workshop on Computation of Applied Electromagnetics in Materials, </booktitle> <address> Sapporo, Japan, </address> <year> 1993. </year>
Reference-contexts: All the results in this section were computed on a DEC Alpha 3000-600 AXP workstation. The test problems used are the international TEAM (Testing Electromagnetic Analysis Methods) benchmark problems, numbers 13 [7] and 20 <ref> [8] </ref>. TEAM 13 was run using a mesh that resulted in a system of 2041 equations. TEAM 20 was run using a mesh that resulted in a system of 2867 equations.
References-found: 8

