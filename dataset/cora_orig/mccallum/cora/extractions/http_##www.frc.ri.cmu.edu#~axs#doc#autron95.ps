URL: http://www.frc.ri.cmu.edu/~axs/doc/autron95.ps
Refering-URL: http://www.frc.ri.cmu.edu/~axs/
Root-URL: 
Title: In Autonomous Robots,  A Complete Navigation System for Goal Acquisition in Unknown Environments  
Author: Anthony Stentz and Martial Hebert 
Address: Pittsburgh, Pennsylvania 15213  
Affiliation: The Robotics Institute Carnegie Mellon University  
Date: 2, August 1995  
Note: Volume 2, Number  1.0 Introduction  
Abstract: Most autonomous outdoor navigation systems tested on actual robots have centered on local navigation tasks such as avoiding obstacles or following roads. Global navigation has been limited to simple wandering, path tracking, straight-line goal seeking behaviors, or executing a sequence of scripted local behaviors. These capabilities are insufficient for unstructured and unknown environments, where replanning may be needed to account for new information discovered in every sensor image. To address these problems, we have developed a complete system that integrates local and global navigation. The local system uses a scanning laser rangef inder to detect obstacles and recommend steering commands to ensure robot safety . These obstacles are passed to the global system which stores them in a map of the environment. W ith each addition to the map, the global system uses an incremental path planning algorithm to optimally replan the global path and recommend steering commands to reach the goal. An arbiter combines the steering recommendations to achieve the proper balance between safety and goal acquisition. This system was tested on a real robot and successfully drove it 1.4 kilometers to find a goal given no a priori map of the environment. Autonomous outdoor navigators have a number of uses, including planetary exploration, construction site work, min-ing, military reconnaissance, and hazardous waste remediation. These tasks require a mobile robot to move between points in its environment in order to transport cargo, deploy sensors, or rendezvous with other robots or equipment. The problem is complicated in environments that are unstructured and unknown. In such cases, the robot can be equipped with one or more sensors to measure the location of obstacles, locate itself within the environment, and check for hazards. By sweeping the terrain for obstacles, recording its progress through the environment, and building a map of sensed areas, the navigator can find the goal position if a path exists, even if it has no prior knowledge of the environment. A number of systems have been developed to address this scenario in part. W e limit our discussion to those systems tested on a real robot in outdoor terrain. Outdoor robots operating in expansive, unstructured environments pose new problems, since the optimal global routes can be complicated enough that simple replanning approaches are not feasible for real-time operation. The bulk of the work in outdoor navigation has centered on local navigation, that is, avoiding obstacles [1][2][3][5][7][14][17][20][27][28][31] or following roads [4][8][15][25]. The global navigation capabilities of these systems have been limited to tracking a preplanned trajectory , wandering, maintaining a constant heading, or following a specif ic type of terrain feature. Other systems with global navigation components typically operate by planning a global route based on initial map information and then replanning as needed when the sensors discover an obstruction [9][10][21]. These approaches are insufficient if there is no initial map of the environment or if the environment does not exhibit coarse, global structure (e.g., a small network of routes or channels). It is possible to replan a new global trajectory from scratch for each new piece of sensor data, but in cluttered environments the sensors can detect new information almost continuously, thus precluding real-time operation. Furthermore, sensor noise and robot position error can lead to the detection of phantom obstacles and the incorrect location of obstacles, ooding the global navigator with more, and sometimes erroneous, data. 
Abstract-found: 1
Intro-found: 0
Reference: [1] <author> Bhatt, R., Venetsky, L., Gaw, D., Lowing, D., Meystel, A., </author> <title> A Real-Time Pilot for an Autonomous Robot, </title> <booktitle> Proceedings of IEEE Conference on Intelligent Control, </booktitle> <year> 1987. </year>
Reference: [2] <author> Chatila, R., Devy, M. Herrb, M., </author> <title> Perception System and Functions for Autonomous Navigation in a Natural Environment, </title> <booktitle> Proceedings of CIRFFSS, </booktitle> <year> 1994. </year>
Reference: [3] <author> Daily, M., Harris, J., Keirsey, D., Olin, K., Payton, D., Reiser, K., Rosenblatt, J., Tseng, D., Wong, V., </author> <title> Autonomous Cross Country Navigation with the ALV, </title> <booktitle> Proceedings of the IEEE International Conference on Robotics and Automation, </booktitle> <year> 1988. </year>
Reference: [4] <author> Dickmanns, E.D., Zapp, A. </author> <title> A Curvature-Based Scheme for Improving Road Vehicle Guidance by Computer Vision, </title> <booktitle> Pro-ceedings of the SPIE Conference on Mobile Robots, </booktitle> <year> 1986. </year>
Reference: [5] <author> Dunlay, </author> <title> R.T., Morgenthaler, D.G., Obstacle Avoidance on Roadways Using Range Data, </title> <booktitle> Proceedings of SPIE Conference on Mobile Robots, </booktitle> <year> 1986. </year>
Reference: [6] <author> Fedor, C., TCX, </author> <title> Task Communications Users Manual. </title> <type> Internal Report, </type> <institution> The Robotics Institute, Carnegie Mellon, </institution> <year> 1993. </year>
Reference-contexts: The system diagram is shown in SMARTY local navigator processes this sensor data to find obstacles and sends the (x,y) locations of detected obsta-cles (untraversable cells) to D* at regular intervals, using the TCX <ref> [6] </ref> message passing system. Additionally, SMARTY sends (x,y) locations of cells known to be devoid of obstacles (traversable cells). Since the D* map is used to represent a global area, its grid cells are of coarser resolution than SMARTYs (i.e., 1 meter versus 0.4 meter).
Reference: [7] <author> Feng, D., Singh, S., Krogh, B., </author> <title> Implementation of Dynamic Obstacle Avoidance on the CMU Navlab, </title> <booktitle> Proceedings of IEEE Conference on Systems Engineering, </booktitle> <year> 1990. </year>
Reference: [8] <author> Franke, U., Fritz, H., Mehring, S., </author> <title> Long Distance Driving with the Daimler-Benz Autonomous Vehicle VITA, </title> <booktitle> PROMETHEUS Workshop, </booktitle> <address> Grenoble, </address> <month> December, </month> <year> 1991. </year>
Reference: [9] <author> Gat, E., Slack, M. G., Miller, D. P., Firby, R. J., </author> <title> Path Planning and Execution Monitoring for a Planetary Rover, </title> <booktitle> Proceedings of the IEEE International Conference on Robotics and Automation, </booktitle> <year> 1990. </year>
Reference: [10] <author> Goto, Y., Stentz, A., </author> <title> Mobile Robot Navigation: The CMU System, </title> <journal> IEEE Expert, </journal> <volume> Vol. 2, No. 4, </volume> <month> Winter, </month> <year> 1987. </year>
Reference-contexts: One approach to path planning in this scenario is to generate a global path using the known information and then circumvent obstacles on the main route detected by the sensors <ref> [10] </ref>, generating a new global plan if the route is totally obstructed. Another approach is to move directly toward the goal, skirting the perim-eter of any obstructions until the point on the obstacle nearest the goal is found, and then to proceed directly toward the goal again [19].
Reference: [11] <author> Hebert, M., </author> <title> Pixel-Based Range Processing for Autonomous Driving, </title> <booktitle> Proceedings of the IEEE International Conference on Robotics and Automation, </booktitle> <year> 1994. </year>
Reference-contexts: In practice, a minimum of 5 points per cell is used. The processing is performed scanline by scanline instead of processing the entire range image and sending the appropriate map cells at completion, as described in Hebert <ref> [11] </ref>. In scanline- or pixel-based processing, each pixel is converted to Cartesian coordinates and the state of the corresponding cell in the map is updated.
Reference: [12] <author> Hebert, M.,Krotkov, E., </author> <title> 3D Measurements from Imaging Laser Radars, </title> <journal> Image and Vision Computing, </journal> <volume> Vol. 10, No. 3, </volume> <month> April, </month> <year> 1992. </year>
Reference-contexts: The initial stage of image filtering resolves the ambiguity due to the maximum range of the scanner, and removes outliers due to effects such as mixed pixels and reections from specular surfaces. (See Hebert <ref> [12] </ref> for a complete description of these effects.) After image filtering, the (x,y,z) location of every pixel in the range image is computed in a coordinate system relative to the current robot position.
Reference: [13] <author> Keirsey, D.M., Payton, D.W. and Rosenblatt, J.K., </author> <title> Autonomous Navigation in Cross-Country Terrain, </title> <booktitle> in Image Under-standing Workshop, </booktitle> <address> Cambridge, MA, </address> <month> April, </month> <year> 1988. </year>
Reference-contexts: We give here only a brief description of the approach and refer the reader to Keirsey <ref> [13] </ref> and Langer [17] for a detailed description of the planning archi-tecture. Each steering arc is evaluated by computing the distance between every untraversable cell in the local map and the arc. <p> After the vote for each individual arc is computed, the entire array of votes is sent to an arbiter module <ref> [13] </ref> which generates an actual steering command that is sent to the robot. minimum turning radius of -8 meters to a maximum of +8 meters.
Reference: [14] <author> Kelly, A. J., </author> <title> RANGER - An Intelligent Predictive Controller for Unmanned Ground Vehicles, </title> <journal> The Robotics Institute, </journal> <note> Carn-egie Mellon, </note> <year> 1994. </year>
Reference: [15] <author> Kluge, K., YARF: </author> <title> An Open-Ended Framework for Robot Road Following, </title> <type> Ph.D. Dissertation, </type> <institution> CMU-CS-93-104, School of Computer Science, Carnegie Mellon University, </institution> <year> 1993. </year>
Reference: [16] <author> Korf, R. E., </author> <title> Real-Time Heuristic Search: First Results, </title> <booktitle> Proceedings of the Sixth National Conference on Artificial Intelli-gence, </booktitle> <month> July, </month> <year> 1987. </year>
Reference-contexts: A fourth approach is to use map information to estimate the cost to the goal for each location in the environment and efficiently update these costs with backtracking costs as the robot moves through the environment <ref> [16] </ref>. These approaches are complete (i.e., the robot will find the goal if a path exists), but they are suboptimal in the sense that they do not make optimal use of sensor information as it is acquired.
Reference: [17] <author> Langer, D., Rosenblatt, J.K., Hebert, M., </author> <title> An Integrated System for Autonomous Off-Road Navigation, </title> <booktitle> Proceedings of the IEEE International Conference on Robotics and Automation, </booktitle> <year> 1994. </year>
Reference-contexts: We give here only a brief description of the approach and refer the reader to Keirsey [13] and Langer <ref> [17] </ref> for a detailed description of the planning archi-tecture. Each steering arc is evaluated by computing the distance between every untraversable cell in the local map and the arc. <p> The curve shows the maximum votes are for moderate right turns of the robot and are close to -1 for left and right turns. 2.3.5 Cell Transmission to D* The system described so far is one of our conventional autonomous driving systems <ref> [17] </ref>. In order to use SMARTY in conjunction with the D* module, we added a direct link between D* and SMARTY (see Figure 3), because D* needs to update its internal map based on the information extracted from the range images.
Reference: [18] <author> Latombe, J.-C., </author> <title> Robot Motion Planning, </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year>
Reference-contexts: A vast amount of literature has addressed the path-finding problem in known environments (see Latombe <ref> [18] </ref> for a good survey). In many cases, however, this scenario is unrealistic. Often the robot has only a par tial map or no map at all. In these cases, the robot uses its sensors to discover the environment as it moves and modi-fies its plans accordingly.
Reference: [19] <author> Lumelsky, V. J., Stepanov, A. A., </author> <title> Dynamic Path Planning for a Mobile Automaton with Limited Information on the Environ-ment, </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> Vol. AC-31, No. 11, </volume> <month> November, </month> <year> 1986. </year>
Reference-contexts: Another approach is to move directly toward the goal, skirting the perim-eter of any obstructions until the point on the obstacle nearest the goal is found, and then to proceed directly toward the goal again <ref> [19] </ref>. A third approach is to direct the robot to wander around the environment until it finds the goal, penalizing forays onto terrain previously traversed, so that new areas are explored [24].
Reference: [20] <author> Matthies, L., </author> <title> Stereo Vision for Planetary Rovers: Stochastic Modeling to Near Real-Time Implementation, </title> <journal> International Journal of Computer Vision, </journal> <volume> Vol. 8, No. 1, </volume> <year> 1992. </year>
Reference: [21] <author> McTamaney, </author> <title> L.S., Mobile Robots: Real-Time Intelligent Control, </title> <journal> IEEE Expert, </journal> <volume> Vol. 2, No. 4, </volume> <month> Winter, </month> <year> 1987. </year>
Reference: [22] <author> Nilsson, N. J., </author> <booktitle> Principles of Artificial Intelligence, </booktitle> <publisher> Tioga Publishing Company, </publisher> <year> 1980. </year>
Reference-contexts: It is possible to produce an optimal traverse by using A* <ref> [22] </ref> to compute an optimal path from the known map information, moving the robot along the path until either it reaches the goal or its sensors detect a discrepancy between the map and the environment, updating the map, and then replanning a new optimal path from the robot s current location
Reference: [23] <author> Payton, D.W., Rosenblatt, J.K., Keirsey, </author> <title> D.M., Plan Guided Reaction, </title> <journal> IEEE Transactions on Systems Man and Cybernetics, </journal> <volume> Vol. 20, No. 6, </volume> <pages> pp. 1370-1382, </pages> <year> 1990. </year>
Reference: [24] <author> Pirzadeh, A., Snyder, W., </author> <title> A Unified Solution to Coverage and Search in Explored and Unexplored Terrains Using Indirect Control, </title> <booktitle> Proceedings of the IEEE International Conference on Robotics and Automation, </booktitle> <year> 1990. </year>
Reference-contexts: A third approach is to direct the robot to wander around the environment until it finds the goal, penalizing forays onto terrain previously traversed, so that new areas are explored <ref> [24] </ref>. A fourth approach is to use map information to estimate the cost to the goal for each location in the environment and efficiently update these costs with backtracking costs as the robot moves through the environment [16].
Reference: [25] <author> Pomerleau, D.A., </author> <title> Efficient Training of Artificial Neural Networks for Autonomous Navigation, </title> <journal> Neural Computation, </journal> <volume> Vol. 3, No. 1, </volume> <year> 1991. </year>
Reference: [26] <author> Rosenblatt, J.K., Payton, D.W., </author> <title> A Fine-Grained Alternative to the Subsumption Architecture for Mobile Robot Control, </title> <booktitle> in Proceedings of the IEEE/INNS International Joint Conference on Neural Networks, Washington DC, </booktitle> <volume> Vol. 2, </volume> <pages> pp. 317-324, </pages> <month> June, </month> <year> 1989. </year>
Reference: [27] <author> Singh, S., Feng, D., Keller, P., Shaffer, G., Shi, W.F., Shin, D.H., West, J., Wu, </author> <title> B.X., A System for Fast Navigation of Auton-omous Vehicles, </title> <type> Robotics Institute Technical Report CMU-RI-TR-91-20, </type> <institution> Carnegie Mellon University, </institution> <year> 1991. </year>
Reference: [28] <author> Stentz, A., Brumitt, B.L., Coulter, R.C., Kelly, A., </author> <title> An Autonomous System for Cross-Country Navigation, </title> <booktitle> Proceedings of the SPIE Conference on Mobile Robots, </booktitle> <year> 1992. </year> <month> 28 </month>
Reference: [29] <author> Stentz, A., </author> <title> Optimal and Efficient Path Planning for Partially-Known Environments, </title> <booktitle> Proceedings of the IEEE International Conference on Robotics and Automation, </booktitle> <year> 1994. </year>
Reference-contexts: For large environments requiring a million map cells to represent, experimental results indicate that it is over 200 times faster than A* in replanning, thus enabling real-time operation. (See Stentz <ref> [29] </ref> for a detailed description of the algorithm and the experimental results.) D* uses a Cartesian grid of eight-connected cells to represent the map. The connections, or arcs, are labelled with positive scalar values indicating the cost of moving between the two cells.
Reference: [30] <author> Thorpe, C., Amidi, O., Gowdy, J., Hebert, M., Pomerleau, D., </author> <title> Integrating Position Measurement and Image Understanding for Autonomous Vehicle Navigation, </title> <booktitle> Proceedings of the Workshop on High Precision Navigation, </booktitle> <publisher> Springer-Verlag Publisher, </publisher> <year> 1991. </year>
Reference-contexts: Because the DAMN arbiter does not need to know the semantics of the modules from which it combines votes, it is very general and has been used in a number of systems with different configurations of modules <ref> [30] </ref>. We concentrate here on the configuration of our navigation system as illustrated in Figure 3. The arbiter receives votes from two modules, D* and SMARTY. The global navigator, D*, sends votes based on its global map and the goal location.
Reference: [31] <author> Wilcox, B., Matthies, L., Gennery, D., Cooper, B., Nguyen, T., Litwin, T., Mishkin, A., Stone, H., </author> <title> Robotic Vehicles for Plan-etary Exploration, </title> <booktitle> Proceedings of the IEEE International Conference on Robotics and Automation,1992. </booktitle>
References-found: 31

