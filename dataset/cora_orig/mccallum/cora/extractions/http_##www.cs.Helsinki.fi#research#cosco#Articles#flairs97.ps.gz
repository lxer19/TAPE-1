URL: http://www.cs.Helsinki.fi/research/cosco/Articles/flairs97.ps.gz
Refering-URL: 
Root-URL: 
Email: cosco@cs.Helsinki.FI,  
Phone: 26,  
Title: EXPERIMENTING WITH THE CHEESEMAN-STUTZ EVIDENCE APPROXIMATION FOR PREDICTIVE MODELING AND DATA MINING  
Author: Petri Kontkanen Petri Myllymaki Henry Tirri 
Note: Proceedings of the 10th Florida Artificial Intelligence Research Symposium, 1997, by the Florida AI Research Society 0-9620-1739-6/97/204 1997 FLAIRS  
Web: http://www.cs.Helsinki.FI/research/cosco/  
Address: P.O.Box  FIN-00014 University of Helsinki, Finland  
Affiliation: Complex Systems Computation Group (CoSCo)  Department of Computer Science  
Abstract: The work discussed in this paper is motivated by the need of building decision support systems for real-world problem domains. Our goal is to use these systems as a tool for supporting Bayes optimal decision making, where the action maximizing the expected utility, with respect to predicted probabilities of the possible outcomes, should be selected. For this reason, the models used need to be probabilistic in nature | the output of a model has to be a probability distribution, not just a set of numbers. For the model family, we have chosen the set of simple discrete finite mixture models which have the advantage of being computationally very efficient. In this work, we describe a Bayesian approach for constructing finite mixture models from sample data. Our approach is based on a two-phase unsupervised learning process which can be used both for exploratory analysis and model construction. In the first phase, the selection of a model class, i.e., the number of parameters, is performed by calculating the Cheeseman-Stutz approximation for the model class evidence. In the second phase, the MAP parameters in the selected class are estimated by the EM algorithm. In this framework, the overfitting problem common to many traditional learning approaches can be avoided, as the learning process automatically regulates the complexity of the model. This paper focuses on the model class selection phase and the approach is validated by presenting empirical results with both natural and synthetic data. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J.O. Berger. </author> <title> Statistical Decision Theory and Bayesian Analysis. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: In this paper we do not address details of the Risk Analysis part of the system, but only point out that in order to be able to perform theoretically solid risk analysis as described in, e.g., <ref> [1] </ref>, the Inference module of the system has to be able to calculate predictive distributions for possible outcomes of interest.
Reference: [2] <author> P. Cheeseman. </author> <title> On Bayesian model selection. </title> <editor> In D. Wolpert, editor, </editor> <booktitle> The Mathematics of Generalization, volume XX of SFI Studies in the Sciences of Complexity, </booktitle> <pages> pages 315-330. </pages> <publisher> Addison-Wesley, </publisher> <year> 1995. </year>
Reference-contexts: In this paper we approximate the Bayes optimal predictive inference by using a single maximum a posterior (MAP) probability model for computing the predictive distribution. For general discussion on the feasibility of this approximation approach, see <ref> [2] </ref>. Following the work in [3], we divide the learning process into two separate phases. The goal of the first phase is to determine the optimal model class, i.e., the number of mixing components used.
Reference: [3] <author> P. Cheeseman and J. Stutz. </author> <title> Bayesian classification (AutoClass): Theory and results. </title> <editor> In U. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, chapter 6. </booktitle> <publisher> AAAI Press, </publisher> <address> Menlo Park, </address> <year> 1996. </year>
Reference-contexts: In this paper we approximate the Bayes optimal predictive inference by using a single maximum a posterior (MAP) probability model for computing the predictive distribution. For general discussion on the feasibility of this approximation approach, see [2]. Following the work in <ref> [3] </ref>, we divide the learning process into two separate phases. The goal of the first phase is to determine the optimal model class, i.e., the number of mixing components used. <p> (D) P (D; Z) P (D; Zj ^ fi) Taking the logarithm of the right-side of (2) and replacing the unobserved data Z with its conditional expectation given the observed data D and parameters ^ fi, gives the Cheeseman-Stutz (C-S) approximation (or measure), which is used in the Autoclass system <ref> [3] </ref>. This measure has been found to be compu-tationally feasible, yet quite accurate when compared to alternative approximations [4]. After selecting the model class there remains the task of finding the MAP estimate of the parameters inside the chosen model class.
Reference: [4] <author> D.M. Chickering and D. Heckerman. </author> <title> Efficient approximations for the marginal likelihood of 209 and 4 (down) with the DG10 dataset. incomplete data given a Bayesian network. </title> <editor> In E. Horvitz and F. Jensen, editors, </editor> <booktitle> Proceedings of the 12th conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 158-168, </pages> <address> Portland, Oregon, August 1996. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: Fortunately, efficient algorithms for estimating this type of missing data exist. In this paper we use a variant of the Expectation-Maximization (EM) algorithm [7] for this purpose. In Section 4 we demonstrate the feasibility of the Bayesian model class selection approach empirically. As opposed to the work in <ref> [4] </ref>, we are interested in validating the approach for both data mining and predictive modeling purposes. In the data mining setting we wish to examine whether the Bayesian model class selection scheme can be used for determining the "correct" model class for the problem domain in question. As in [4], we <p> in <ref> [4] </ref>, we are interested in validating the approach for both data mining and predictive modeling purposes. In the data mining setting we wish to examine whether the Bayesian model class selection scheme can be used for determining the "correct" model class for the problem domain in question. As in [4], we study this case by using synthetic data generated from a K-component mixture model, in which case the correct model class is obviously known. <p> This measure has been found to be compu-tationally feasible, yet quite accurate when compared to alternative approximations <ref> [4] </ref>. After selecting the model class there remains the task of finding the MAP estimate of the parameters inside the chosen model class. Finding the exact MAP estimate of fi is a computationally infeasible task, thus we are again forced to use numerical approximation methods.
Reference: [5] <author> G. Cooper and E. Herskovits. </author> <title> A Bayesian method for the induction of probabilistic networks from data. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 309-347, </pages> <year> 1992. </year>
Reference-contexts: In the case of complete data|in other words, if the unobserved data Z is known|the evidence (1) can be solved analytically as shown in <ref> [5, 9, 12] </ref>. In this paper, however, we consider the incomplete data situation and assume Z to be unknown, as is the case with any realistic application. In this case computing the evidence analytically requires summing over all the possible instantiations of Z, which is naturally not computationally feasible.
Reference: [6] <author> G.F. Cooper. </author> <title> The computational complexity of probabilistic inference using Bayesian belief networks. </title> <journal> Artificial Intelligence, </journal> <volume> 42(2-3):393-405, </volume> <month> March </month> <year> 1990. </year>
Reference-contexts: This is in contrast to the case with general Bayesian network structures, where computing the predictive distribution is known to be NP-hard <ref> [6] </ref>. What is more, finite mixture models can be realized by neural architectures [14], thus being ideal for massively parallel hardware [15].
Reference: [7] <author> A.P. Dempster, N.M. Laird, and D.B. Rubin. </author> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, Series B, </journal> <volume> 39(1) </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference-contexts: Fortunately, efficient algorithms for estimating this type of missing data exist. In this paper we use a variant of the Expectation-Maximization (EM) algorithm <ref> [7] </ref> for this purpose. In Section 4 we demonstrate the feasibility of the Bayesian model class selection approach empirically. As opposed to the work in [4], we are interested in validating the approach for both data mining and predictive modeling purposes. <p> Finding the exact MAP estimate of fi is a computationally infeasible task, thus we are again forced to use numerical approximation methods. We use here a variant of the Expectation-Maximization (EM) algorithm <ref> [7] </ref> for this purpose, since the method is easily applicable in this domain and produces good solutions quite rapidly.
Reference: [8] <author> B.S. </author> <title> Everitt and D.J. Hand. Finite Mixture Distributions. </title> <publisher> Chapman and Hall, </publisher> <address> London, </address> <year> 1981. </year>
Reference-contexts: Furthermore, we assume that each data vector originates from exactly one of these mechanisms. Thus the instantiation space is divided into K local regions called clusters, each consisting of the data vectors generated by the corresponding mechanism. An appropriate statistical model for such a case is a finite mixture <ref> [8, 21] </ref>.
Reference: [9] <author> D. Heckerman, D. Geiger, </author> <title> and D.M. Chicker-ing. Learning Bayesian networks: The combina-Figure 6: The C-S measure for cluster counts 6 (up) and 8 (down) with the DG10 dataset. </title> <journal> tion of knowledge and statistical data. Machine Learning, </journal> <volume> 20(3) </volume> <pages> 197-243, </pages> <month> September </month> <year> 1995. </year>
Reference-contexts: In the case of complete data|in other words, if the unobserved data Z is known|the evidence (1) can be solved analytically as shown in <ref> [5, 9, 12] </ref>. In this paper, however, we consider the incomplete data situation and assume Z to be unknown, as is the case with any realistic application. In this case computing the evidence analytically requires summing over all the possible instantiations of Z, which is naturally not computationally feasible.
Reference: [10] <author> H. Kitano. </author> <title> Challenges of massive parallelism. </title> <booktitle> In Proc. of IJCAI-93, the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 813-834, </pages> <address> Chambery, France, August 1993. </address> <publisher> Mor-gan Kaufmann Publishers. </publisher>
Reference-contexts: What is more, finite mixture models can be realized by neural architectures [14], thus being ideal for massively parallel hardware [15]. The finite mixture model also conforms interestingly to the intuitively appealing memory-based reasoning paradigm (see e.g., <ref> [10] </ref>), and it can be seen to offer a Bayesian solution to the important case matching and adaptation problems in such domains [19].
Reference: [11] <author> P. Kontkanen, P. Myllymaki, T. Silander, H. Tirri, and P. Grunwald. </author> <title> Comparing predictive inference methods for discrete domains. </title> <booktitle> In Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics, </booktitle> <pages> pages 311-318, </pages> <address> Ft. Lauderdale, Florida, </address> <month> January </month> <year> 1997. </year>
Reference-contexts: The task of the Learning module in Figure 1 is to construct a single model from the sample data (and possibly a priori information). Strictly speaking, this approach is not optimal, since in principle the Bayesian theory would require us to average over all the possible models. In <ref> [11] </ref> we showed how this type of "integrated" predictive inference can be performed in the Naive Bayes classifier case, which can be seen as a simple special case of the finite mixture model.
Reference: [12] <author> P. Kontkanen, P. Myllymaki, and H. Tirri. </author> <title> Comparing Bayesian model class selection criteria by discrete finite mixtures. </title> <editor> In D. Dowe, K. Korb, and J. Oliver, editors, </editor> <booktitle> Information, Statistics and Induction in Science, </booktitle> <pages> pages 364-374, </pages> <booktitle> Proceedings of the ISIS'96 Conference, </booktitle> <address> Melbourne, Australia, </address> <month> August </month> <year> 1996. </year> <title> World Scientific, Singa-pore. 210 and 4 (down) with the DG20 dataset. </title>
Reference-contexts: In the case of complete data|in other words, if the unobserved data Z is known|the evidence (1) can be solved analytically as shown in <ref> [5, 9, 12] </ref>. In this paper, however, we consider the incomplete data situation and assume Z to be unknown, as is the case with any realistic application. In this case computing the evidence analytically requires summing over all the possible instantiations of Z, which is naturally not computationally feasible.
Reference: [13] <author> P. Kontkanen, P. Myllymaki, and H. Tirri. </author> <title> Constructing Bayesian finite mixture models by the EM algorithm. </title> <type> Technical Report C-1996-9, </type> <institution> University of Helsinki, Department of Computer Science, </institution> <month> February </month> <year> 1996. </year>
Reference-contexts: We use here a variant of the Expectation-Maximization (EM) algorithm [7] for this purpose, since the method is easily applicable in this domain and produces good solutions quite rapidly. The detailed derivation of the EM update formulas in our finite mixture case can be found in <ref> [13] </ref>. 4 EMPIRICAL RESULTS In order to validate the Bayesian approach, two sets of experiments were performed, one with synthetic data and one with natural data from public domain databases.
Reference: [14] <author> P. Myllymaki and H. Tirri. </author> <title> Bayesian case-based reasoning with neural networks. </title> <booktitle> In Proceedings of the IEEE International Conference on Neural Networks, </booktitle> <volume> volume 1, </volume> <pages> pages 422-427, </pages> <address> San Fran-cisco, March 1993. </address> <publisher> IEEE, </publisher> <address> Piscataway, NJ. </address>
Reference-contexts: This is in contrast to the case with general Bayesian network structures, where computing the predictive distribution is known to be NP-hard [6]. What is more, finite mixture models can be realized by neural architectures <ref> [14] </ref>, thus being ideal for massively parallel hardware [15]. The finite mixture model also conforms interestingly to the intuitively appealing memory-based reasoning paradigm (see e.g., [10]), and it can be seen to offer a Bayesian solution to the important case matching and adaptation problems in such domains [19].
Reference: [15] <author> P. Myllymaki and H. Tirri. </author> <title> Massively parallel case-based reasoning with probabilistic similarity metrics. </title> <editor> In S. Wess, K.-D. Althoff, and M Richter, editors, </editor> <booktitle> Topics in Case-Based Reasoning, volume 837 of Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 144-154. </pages> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: This is in contrast to the case with general Bayesian network structures, where computing the predictive distribution is known to be NP-hard [6]. What is more, finite mixture models can be realized by neural architectures [14], thus being ideal for massively parallel hardware <ref> [15] </ref>. The finite mixture model also conforms interestingly to the intuitively appealing memory-based reasoning paradigm (see e.g., [10]), and it can be seen to offer a Bayesian solution to the important case matching and adaptation problems in such domains [19].
Reference: [16] <author> J. Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1988. </year> <title> and 8 (down) with the DG20 dataset. </title>
Reference-contexts: The discrete finite mixture models discussed here can be seen as a special case of Bayesian network 204 Problem domain Sampling Model family Learning Training data Domain model Value assignment Inference Predictive distribution Risk Analysis Determined actions models (see e.g., <ref> [16] </ref>). Namely, a finite mixture model can be regarded as a tree-structured Bayesian network, where the root of the tree corresponds to a latent (hidden) variable.
Reference: [17] <author> J. Rissanen. </author> <title> Stochastic Complexity in Statistical Inquiry. </title> <publisher> World Scientific Publishing Company, </publisher> <address> New Jersey, </address> <year> 1989. </year>
Reference-contexts: In this case computing the evidence analytically requires summing over all the possible instantiations of Z, which is naturally not computationally feasible. An approximation presented in <ref> [18, 17] </ref> suggests a feasible approach: it can be shown that the evidence P (D) is approximatively (with increasing N ) P (Dj ^ fi) C, where ^ fi denotes the MAP estimate of fi, and C is a constant depending only on N , and on the dimensionality of fi.
Reference: [18] <author> G. Schwarz. </author> <title> Estimating the dimension of a model. </title> <journal> Annals of Statistics, </journal> <volume> 6 </volume> <pages> 461-464, </pages> <year> 1978. </year>
Reference-contexts: In this case computing the evidence analytically requires summing over all the possible instantiations of Z, which is naturally not computationally feasible. An approximation presented in <ref> [18, 17] </ref> suggests a feasible approach: it can be shown that the evidence P (D) is approximatively (with increasing N ) P (Dj ^ fi) C, where ^ fi denotes the MAP estimate of fi, and C is a constant depending only on N , and on the dimensionality of fi.
Reference: [19] <author> H. Tirri, P. Kontkanen, and P. Myllymaki. </author> <title> A Bayesian framework for case-based reasoning. </title> <editor> In I. Smith and B. Faltings, editors, </editor> <booktitle> Advances in Case-Based Reasoning, volume 1168 of Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 413-427, </pages> <booktitle> Proceedings of the 3rd European Workshop, </booktitle> <address> Lausanne, Switzerland, November 1996. </address> <publisher> Springer-Verlag, </publisher> <address> Berlin Heidelberg. </address>
Reference-contexts: The finite mixture model also conforms interestingly to the intuitively appealing memory-based reasoning paradigm (see e.g., [10]), and it can be seen to offer a Bayesian solution to the important case matching and adaptation problems in such domains <ref> [19] </ref>. As shown in Section 2, the computational complexity of predictive inference with finite mixture models is linear with respect to the number of mixture components used (number of values of the latent variable introduced).
Reference: [20] <author> H. Tirri, P. Kontkanen, and P. Myllymaki. </author> <title> Probabilistic instance-based learning. </title> <editor> In L. Saitta, editor, </editor> <booktitle> Machine Learning: Proceedings of the Thirteenth International Conference, </booktitle> <pages> pages 507-515. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1996. </year>
Reference-contexts: On the other hand, if only a feasible (polynomial) number of mixing components is used, finite mixture models can represent only a subset of the probability distributions representable by Bayesian networks in general. Nevertheless, a recent empirical study <ref> [20] </ref> shows that simple finite mixture models perform very well in a wide range of real-world classification problems, which suggests that computationally efficient finite mixture models with a small number of mixture components can in many cases approximate the problem domain probability distribution well enough for practical applications. <p> The actual predictive accuracy of the constructed models is discussed in more detail in <ref> [20] </ref>, where the approach was shown to perform favorably when compared to various neural network and machine learning algorithms with several public domain datasets. 2 PREDICTIVE INFERENCE BY FINITE MIXTURES In the following we model the problem domain by m discrete random variables X 1 ; : : : ; X
Reference: [21] <author> D.M. Titterington, A.F.M. Smith, and U.E. Makov. </author> <title> Statistical Analysis of Finite Mixture Distributions. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: The probabilistic model family chosen here is the family of simple discrete finite mixtures, a subset of the more general family of all the finite mixture models <ref> [21] </ref>. Due to our decision support context, instead of the supervised learning framework, we explore the unsupervised case where we do not determine in advance which predictive distributions are to be estimated, but aim at building a full probability model of the problem domain. <p> Furthermore, we assume that each data vector originates from exactly one of these mechanisms. Thus the instantiation space is divided into K local regions called clusters, each consisting of the data vectors generated by the corresponding mechanism. An appropriate statistical model for such a case is a finite mixture <ref> [8, 21] </ref>.
References-found: 21

