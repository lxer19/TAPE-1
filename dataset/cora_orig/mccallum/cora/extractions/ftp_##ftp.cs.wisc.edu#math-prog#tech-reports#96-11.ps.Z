URL: ftp://ftp.cs.wisc.edu/math-prog/tech-reports/96-11.ps.Z
Refering-URL: 
Root-URL: 
Title: A direct search algorithm for optimization with noisy function evaluations  
Author: Edward J. Anderson Michael C. Ferris 
Address: Sydney 2052 Australia  Madison, WI 53706, USA  
Affiliation: Australian Graduate School of Management, University of New South Wales  University of Wisconsin-Madison, Computer Sciences Department,  
Date: November 27, 1996  
Abstract: We consider the unconstrained optimization of a function when each function evaluation is subject to some random noise. Our model of computation assumes that averaging repeated observations at the same point can lead to a better estimate of the underlying function value. In practice problems of this form may occur when choosing the best settings for the controls in a processing plant, or in choosing the parameters in an experiment of some kind. We consider direct search methods with the possibility of repeated function evaluations at the same point. We describe an algorithm of this type which has reasonable computational performance and for which convergence can be established. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Barton, R. & Ivey, J. S. </author> <year> (1996), </year> <title> `Nelder Mead simplex modifications for simulation optimization', </title> <booktitle> Management Science 42(7), </booktitle> <pages> 954-973. </pages>
Reference: <author> Brent, R. P. </author> <year> (1973), </year> <title> Algorithms for Minimization without Derivatives, </title> <publisher> Prentice-Hall, Inc, </publisher> <address> Englewood Cliffs, New Jersey. </address>
Reference-contexts: Mead method was invented more than 30 years ago, some version of this approach is probably still the most common way to carry out optimization when each function evaluation requires a separate experiment; this is despite the apparent superiority of other direct search methods, such as that due to Powell <ref> (Brent 1973, Powell 1964, del Valle, Poch, Alonso & Bartroli 1990) </ref>. Methods based on a simplicial approach have the disadvantage that, on poorly behaved problems, they can fail to converge.
Reference: <author> Conn, A. R. & Toint, P. L. </author> <year> (1996), </year> <title> An algorithm using quadratic interpolation for unconstrained derivative free optimization, </title> <editor> in G. D. Pillo & F. Giannessi, eds, </editor> <booktitle> `Proceedings of Nonlinear Optimization and Applications Workshop, </booktitle> <address> Erice June 1995', </address> <publisher> Plenum Press, </publisher> <address> New York. </address> <note> del Valle, </note> <author> M., Poch, M., Alonso, J. & Bartroli, J. </author> <year> (1990), </year> <title> `Comparison of the Powell and simplex methods in the optimization of flow-injection systems', </title> <journal> Analytica Chimica Acta 241, </journal> <pages> 31-42. </pages>
Reference-contexts: A major factor in the continuing popularity of simplicial methods amongst users of optimization software is their ability to deal effectively with situations in which function evaluations are inaccurate. In this case more complex methods which approximate the function with some polynomial based on recent function evaluations <ref> (Conn & Toint 1996, Powell 1994) </ref> may be led seriously astray: even the estimation of gradient information by finite differencing must be carried out carefully (Gill, Murray, Saunders & Wright 1983).
Reference: <author> Dennis, J. E. & Torczon, V. </author> <year> (1991), </year> <title> `Direct search methods on parallel machines', </title> <note> SIAM Journal on Optimization 1, 448-474. 24 Elster, </note> <author> C. & Neumaier, A. </author> <year> (1995), </year> <title> `A grid algorithm for bound constrained optimization of noisy functions', </title> <journal> IMA Journal of Numerical Analysis 15, </journal> <pages> 585-608. </pages>
Reference-contexts: However this disadvantage is partly offset by the fact that they may be capable of easy parallelisation <ref> (Dennis & Torczon 1991) </ref>. A major factor in the continuing popularity of simplicial methods amongst users of optimization software is their ability to deal effectively with situations in which function evaluations are inaccurate.
Reference: <author> Feller, W. </author> <year> (1968), </year> <title> An Introduction to Probability Theory and its Applications, </title> <publisher> John Wiley & Sons, </publisher> <address> New York. </address>
Reference: <author> Glad, T. & Goldstein, A. </author> <year> (1977), </year> <title> `Optimization of functions whose values are subject to small errors', </title> <journal> BIT 17, </journal> <pages> 160-169. </pages>
Reference: <author> Gill, P. E., Murray, W., Saunders, M. A. & Wright, M. H. </author> <year> (1983), </year> <title> `Computing forward-difference intervals for numerical optimization', </title> <journal> SIAM Journal on Scientific and Statistical Computing 4, </journal> <pages> 310-321. </pages>
Reference-contexts: In this case more complex methods which approximate the function with some polynomial based on recent function evaluations (Conn & Toint 1996, Powell 1994) may be led seriously astray: even the estimation of gradient information by finite differencing must be carried out carefully <ref> (Gill, Murray, Saunders & Wright 1983) </ref>. In this paper we deal explicitly with the optimization of functions where 2 the accuracy of the function evaluation depends on the time devoted to it. An example occurs when attempting to optimize settings to achieve the maximum yield from a chemical reaction.
Reference: <author> Hedlund, P. & Gustavsson, A. </author> <year> (1992), </year> <title> `Design and evaluation of modified simplex methods having enhanced convergence ability', </title> <journal> Analytica Chimica Acta 259, </journal> <pages> 243-256. </pages>
Reference: <author> Hooke, R. & Jeeves, T. A. </author> <year> (1961), </year> <title> `Direct search solution of numerical and statistical problems', </title> <journal> Journal of Associated Computing Machinery 8, </journal> <pages> 212-229. </pages>
Reference: <author> Kushner, H. J. & Clark, D. S. </author> <year> (1978), </year> <title> Stochastic Approximation Methods for Constrained and Unconstrained Systems, </title> <publisher> Springer-Verlag, </publisher> <address> New York. </address>
Reference-contexts: Then a line search is carried out in the negative gradient direction before the whole process is repeated. The method is closely related to the Keifer Wolkowitz method (see for example <ref> (Kushner & Clark 1978) </ref> and (Polyak 1987)).
Reference: <author> Nelder, J. A. & Mead, R. </author> <year> (1965), </year> <title> `A simplex method for function minimization', </title> <journal> Computer Journal 7, </journal> <pages> 308-313. </pages>
Reference: <author> Parker, L. R., Cave, M. R. & Barnes, R. M. </author> <year> (1985), </year> <title> `Comparison of simplex algorithms', </title> <journal> Analytica Chimica Acta 175, </journal> <pages> 231-237. </pages>
Reference: <author> Polyak, B. T. </author> <year> (1987), </year> <title> Introduction to Optimization, Optimization Software, </title> <publisher> Inc., Publications Division, </publisher> <address> New York. </address>
Reference-contexts: Then a line search is carried out in the negative gradient direction before the whole process is repeated. The method is closely related to the Keifer Wolkowitz method (see for example (Kushner & Clark 1978) and <ref> (Polyak 1987) </ref>).
Reference: <author> Powell, M. J. D. </author> <year> (1964), </year> <title> `An efficient method for finding the minimum of a function of several variables without calculating derivatives', </title> <journal> Computer Journal 17, </journal> <pages> 155-162. </pages>
Reference: <author> Powell, M. J. D. </author> <year> (1994), </year> <title> A direct search optimization method that models the objective and constraint functions by linear interpolation, in `Advances in Optimization and Numerical Analysis, </title> <booktitle> Proceedings of the Sixth Workshop on Optimization and Numerical Analysis, Oax-aca, Mexico', </booktitle> <volume> Vol. 275, </volume> <publisher> Kluwer Academic Publishers, Dordrect, NL, </publisher> <pages> pp. 51-67. </pages>
Reference: <author> Rykov, A. S. </author> <year> (1980), </year> <title> `Simplex direct search algorithms', </title> <booktitle> Automation and Remote Control 41, </booktitle> <pages> 784-793. </pages>
Reference: <author> Spendley, W., Hext, G. R. & Himsworth, F. R. </author> <year> (1962), </year> <title> `Sequential application of simplex designs in optimization and evolutionary operation', </title> <type> Technometrics 4, </type> <pages> 441-461. </pages>
Reference: <author> Torczon, V. </author> <year> (1991), </year> <title> `On the convergence of the multidirectional search algorithm', </title> <journal> SIAM Journal on Optimization 1, </journal> <pages> 123-145. </pages>
Reference-contexts: However this disadvantage is partly offset by the fact that they may be capable of easy parallelisation <ref> (Dennis & Torczon 1991) </ref>. A major factor in the continuing popularity of simplicial methods amongst users of optimization software is their ability to deal effectively with situations in which function evaluations are inaccurate. <p> A2 The noise distribution is Normal. A3 The sequence i is bounded away from zero. The first assumption is stronger than the assumptions made by (Tseng 1996) and <ref> (Torczon 1991) </ref> in their proofs of convergence for direct search methods where there is no noise component. Tseng, for example requires that the function is continuously differentiable and that the lower level set L 0 = fx 2 &lt; n jf (x) F (S (0) g is compact.
Reference: <author> Tseng, P. </author> <year> (1996), </year> <title> Fortified-descent simplicial search method: a general approach, </title> <type> Technical report, </type> <institution> Mathematics Department, University of Washington. </institution>
Reference-contexts: A2 The noise distribution is Normal. A3 The sequence i is bounded away from zero. The first assumption is stronger than the assumptions made by <ref> (Tseng 1996) </ref> and (Torczon 1991) in their proofs of convergence for direct search methods where there is no noise component.
Reference: <author> Yu, W. C. </author> <year> (1979), </year> <title> `The convergence property of the simplex evolutionary techniques', </title> <journal> Scientia Sinica, Special issue of Mathematics 1, </journal> <pages> 68-77. 26 </pages>
References-found: 20

