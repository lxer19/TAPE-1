URL: http://www.cs.umd.edu/~kalvis/PAPERS/game.ps
Refering-URL: http://www.cs.umd.edu/~kalvis/PAPERS/
Root-URL: 
Phone: 2  
Title: Team Learning as a Game  
Author: Andris Ambainis Kalvis Apstis Rusi~ns Freivalds William Gasarch Carl H. Smith 
Address: Latvia, Rai~na bulvaris 29 Rga, LV-1459, Latvia  College Park, MD 20742, USA  
Affiliation: 1 Institute of Mathematics and Computer Science University of  Department of Computer Science, University of Maryland  
Abstract: A FIN -learning machine M receives successive values of the function f it is learning; at some point M outputs conjecture which should be a correct index of f . When n machines simultaneously learn the same function f and at least k of these machines output correct indices of f , we have team learning [k; n]FIN . Papers [DKV92, DK96] show that sometimes a team or a probabilistic learner can simulate another one, if its probability p (or team success ratio k=n) is close enough. On the other hand, there are critical ratios which make simulation of FINhp 2 i by FIN hp 1 i impossible whenever p 2 r &lt; p 1 for some critical ratio r. Accordingly to [DKV92] the critical ratio closest to 1=2 from the left is 24=49; [DK96] provides other unusual constants. These results are complicated and provide a full picture of comparisons only for FIN - learners with success ratio above 12=25. We generalize [k; n]FIN teams to asymmetric teams [AFS97]. We introduce a two player game on two 0-1 matrices defining two asymmetric teams. The result of the game reflects the comparative power of these asymmetric teams. Hereby we show that the problem to determine whether [k 1 ; n 1 ]FIN [k 2 ; n 2 ]FIN is algorithmically solvable. We also show that the set of all critical ratios is well-ordered. Simulating asymmetric teams with probabilistic machines from [AFS97] provides some insight about the unusual constants like 24=49.
Abstract-found: 1
Intro-found: 1
Reference: [AFS97] <author> K. Apstis, R. Freivalds, and C. Smith. </author> <title> Asymmetric team learning. </title> <note> Available from http://www.cs.umd.edu/~kalvis/, Accepted to COLT'97. </note>
Reference-contexts: To achieve this, we use generalized team learning | asymmetric teams <ref> [AFS97] </ref>; and a combinatorial game on two matrices. Game-theoretic approach to inductive learning has been used before [KS94]. <p> X 6 Conclusions The exact complexity for the problem to determine whether [A]FIN [B]FIN remains open. It follows from <ref> [AFS97] </ref> that it is at least NP-hard. By Theorem 25 the hierarchy H FIN is well ordered; we do not know which ordinal is its order type.
Reference: [Amb96] <author> A. Ambainis. </author> <title> Probabilistic pfin-type learning: general properties. </title> <booktitle> In Proceedings of the 9th Conference on Computational Learning Theory. ACM, </booktitle> <year> 1996. </year>
Reference-contexts: Having been formalized, capability trees become a powerful research tool by themselves as shown in the subsequent sections. In Section 5 we explain why the probabilistic hierarchy H FIN is well ordered (Theorem 25). For the PFIN -learning <ref> [Amb96] </ref>, the hierarchy H PFIN is well ordered with the order type " 0 . For H FIN the order type is yet unknown. 3 FIN -Game Player A and Player B are given 0-1 matrices A and B respectively. They alternately make moves on their matrices.
Reference: [CS83] <author> J. Case and C. Smith. </author> <title> Comparison of identification criteria for machine inductive inference. </title> <journal> Theoretical Computer Science, </journal> <volume> 25(2) </volume> <pages> 193-220, </pages> <year> 1983. </year>
Reference-contexts: Often the only relations which hold between different kinds of machine learning are those which trivially follow from their definitions. (E.g. EX a b EX c d iff a c and b d as shown in <ref> [CS83] </ref>.) It was hard to develop a comprehensive mathematical theory in a situation where almost every time two different definitions lead to two different concepts. The initial stage in the research of inductive inference was mostly descriptive: there were more and more new learning paradigms and demonstrations of their differences.
Reference: [DK95] <author> R. Daley and B. Kalyanasundaram. </author> <title> Towards reduction arguments for finite learning. </title> <booktitle> Lecture Notes in Artificial Intelligence, </booktitle> <volume> 961 </volume> <pages> 63-75, </pages> <year> 1995. </year>
Reference-contexts: The recent papers <ref> [DK95, DK96] </ref> use capability trees to study FIN -learning abilities of different teams. However these papers deal with capability trees informally, using them only to illustrate proofs. <p> In this section we formally describe these diagrams and the rules of their labeling. Due to historical reasons <ref> [DK95] </ref>, we call them "capability trees", although, in general, these diagrams are collections of several trees. Definition 15.
Reference: [DK96] <author> R. Daley and B Kalyanasundaram. </author> <title> Finite learning capabilities and their limits. </title> <note> Available from http://www.cs.pitt.edu/~daley/fin/fin.html, 1996. </note>
Reference-contexts: Then FIN hpi [a; b]FIN . Definition 9. Probabilistic hierarchy for the learning paradigm FIN (denoted by H FIN ) is the set of all probabilities p such that 8" &gt; 0, hp + "iFIN hpiFIN . By convention, probability 1 also belongs to H FIN . Accordingly to <ref> [DPVW91, DK96] </ref> we have H FIN = 1; 3 3 ; 7 1 ; 49 2.1 Technical Summary In Section 3 we address the problem of the FIN team inclusions from the perspective of a newly introduced FIN -game. <p> The recent papers <ref> [DK95, DK96] </ref> use capability trees to study FIN -learning abilities of different teams. However these papers deal with capability trees informally, using them only to illustrate proofs.
Reference: [DKV92] <author> R. Daley, B. Kalyanasundaram, and M. Velauthapillai. </author> <title> Breaking the probability 1/2 barrier in fin-type learning. </title> <editor> In L. Valiant and M. Warmuth, editors, </editor> <booktitle> Proceedings of the Fifth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 203-217. </pages> <publisher> ACM Press, </publisher> <year> 1992. </year>
Reference-contexts: The authors of <ref> [DKV92] </ref> used "trial and error" to come up with the ratio 24=49, therefore one can ask an interesting (but informal) question: Where does the 24/49 come from? In this paper we explain where the constants like 24=49 come from, give an algorithm which for any a; b; c; d determines whether <p> Accordingly to Theorems 11 and 12 one team can learn more than another iff its matrix beats the other one's matrix in the FIN -game. Comparisons between particular teams of FIN -learners given in <ref> [Vel89, JS90, DKV92] </ref> provided typical diagonalization and simulation methods which we incorporated in our FIN -game. (See Examples 2 and 3 for the game analysis of these early results.) In Section 4 we describe a static version of the FIN -game by capability trees, where the possible strategies correspond to tree <p> Searching over all possible move sequences, we either find a winning strategy for Player A, or notice that B can always respond so that the same game position is repeated twice. In the latter case A has no winning strategy. For either case apply Corollary 13. X Example 3 <ref> [DKV92] </ref>. [24; 49]FIN 6 [2; 4]FIN . Matrix A in the figure below defines an asymmetric type [A] such that [A]FIN [24; 49]FIN. Indeed, after replicating its columns 9,4,4,7,1,8,4,2,10 times respectively, we get nine rows of the matrix [24; 49]. We show [A]FIN 6 [2; 4]FIN .
Reference: [DPVW91] <author> R. Daley, L. Pitt, M. Velauthapillai, and T. </author> <title> Will. Relations between probabilistic and team one-shot learners. </title> <editor> In M. Warmuth and L. Valiant, editors, </editor> <booktitle> Proceedings of the 1991 Workshop on Computational Learning Theory, </booktitle> <pages> pages 228-239, </pages> <address> Palo Alto, CA., 1991. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: Initial results about the type FIN (a.k.a. EX 0 0 , i.e. learning with 0 anomalies and 0 mindchanges) were just as simple. All probabilistic types FIN hpi, p &gt; 1=2, are equivalent to team types (see <ref> [DPVW91] </ref>). The first surprize was the proper inclusion [1; 2]FIN [2; 4]FIN in [Vel89]. This contrasts the behavior of type EX. <p> Written: f 2 FIN hpi (M ). The following result states that probabilistic FIN -learning can be simulated by team learning with arbitrarily small success ratio overhead. Theorem 8 <ref> [DPVW91] </ref>. Let a; b 2 IN and a+1 b+2 &lt; p 1. Then FIN hpi [a; b]FIN . Definition 9. Probabilistic hierarchy for the learning paradigm FIN (denoted by H FIN ) is the set of all probabilities p such that 8" &gt; 0, hp + "iFIN hpiFIN . <p> Then FIN hpi [a; b]FIN . Definition 9. Probabilistic hierarchy for the learning paradigm FIN (denoted by H FIN ) is the set of all probabilities p such that 8" &gt; 0, hp + "iFIN hpiFIN . By convention, probability 1 also belongs to H FIN . Accordingly to <ref> [DPVW91, DK96] </ref> we have H FIN = 1; 3 3 ; 7 1 ; 49 2.1 Technical Summary In Section 3 we address the problem of the FIN team inclusions from the perspective of a newly introduced FIN -game.
Reference: [JS90] <author> S. Jain and A. Sharma. </author> <title> Finite learning by a team. </title> <editor> In M. Fulk and J. Case, editors, </editor> <booktitle> Proceedings of the Third Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 163-177, </pages> <address> Palo Alto, CA., 1990. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: Accordingly to Theorems 11 and 12 one team can learn more than another iff its matrix beats the other one's matrix in the FIN -game. Comparisons between particular teams of FIN -learners given in <ref> [Vel89, JS90, DKV92] </ref> provided typical diagonalization and simulation methods which we incorporated in our FIN -game. (See Examples 2 and 3 for the game analysis of these early results.) In Section 4 we describe a static version of the FIN -game by capability trees, where the possible strategies correspond to tree
Reference: [KS94] <author> M. Kummer and F. Stephan. </author> <title> Inclusion problems in parallel learning and games. </title> <booktitle> In Proc. 7th Annu. ACM Workshop on Comput. Learning Theory, </booktitle> <pages> pages 287-298. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: To achieve this, we use generalized team learning | asymmetric teams [AFS97]; and a combinatorial game on two matrices. Game-theoretic approach to inductive learning has been used before <ref> [KS94] </ref>. Although their problem is vey different from ours, it also comes from comparing learners with different sets of parameters. 2 Preliminaries IN and F denote the set of natural numbers and the set of all recursive functions respectively. fhg denotes the partial recursive function with index h.
Reference: [NW63] <author> C. Nash-Williams. </author> <title> On well-quasi-ordering finite trees. </title> <journal> Proc. Cambridge Phil. Soc., </journal> <volume> 59 </volume> <pages> 833-853, </pages> <year> 1963. </year>
Reference-contexts: On the set of all trees we define contraction order so that T 1 T 2 iff T 1 can be obtained from T 2 by some sequence of contractions. Lemma 24. The set of all capability trees is quasi-well ordered by . Proof: <ref> [NW63] </ref> shows that the collection of all rooted trees ordered by contraction is well-quasi ordered. We can modify this proof for our variety of capability trees, also ordered by contraction.
Reference: [PS88] <author> L. Pitt and C. Smith. </author> <title> Probability and plurality for aggregations of learning machines. </title> <journal> Information and Computation, </journal> <volume> 77 </volume> <pages> 77-92, </pages> <year> 1988. </year>
Reference-contexts: The initial stage in the research of inductive inference was mostly descriptive: there were more and more new learning paradigms and demonstrations of their differences. Probabilistic and team learning changed this situation. In <ref> [PS88] </ref> it was shown that not all probabilistic limit learning types EXhpi are different (e.g. if p 2 ( 1 n ), then EXhpi can be improved to EXh1=ni). For the case of EX, probabilistic learner is always equivalent to some team [1; n]EX.
Reference: [Smu61] <author> R. </author> <title> Smullyan. </title> <journal> Theory of Formal Systems, Annals of Mathematical Studies, </journal> <volume> volume 47. </volume> <publisher> Princeton University Press, </publisher> <address> Princeton, New Jersey, </address> <year> 1961. </year>
Reference: [Vel89] <author> M. Velauthapillai. </author> <title> Inductive inference with a bounded number of mind changes. </title> <editor> In R. Rivest, D. Haussler, and M. Warmuth, editors, </editor> <booktitle> Proceedings of the 1989 Workshop on Computational Learning Theory, </booktitle> <pages> pages 200-213, </pages> <address> Palo Alto, CA., 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: EX 0 0 , i.e. learning with 0 anomalies and 0 mindchanges) were just as simple. All probabilistic types FIN hpi, p &gt; 1=2, are equivalent to team types (see [DPVW91]). The first surprize was the proper inclusion [1; 2]FIN [2; 4]FIN in <ref> [Vel89] </ref>. This contrasts the behavior of type EX. <p> Accordingly to Theorems 11 and 12 one team can learn more than another iff its matrix beats the other one's matrix in the FIN -game. Comparisons between particular teams of FIN -learners given in <ref> [Vel89, JS90, DKV92] </ref> provided typical diagonalization and simulation methods which we incorporated in our FIN -game. (See Examples 2 and 3 for the game analysis of these early results.) In Section 4 we describe a static version of the FIN -game by capability trees, where the possible strategies correspond to tree <p> X For Example 2 the above construction yields the same proof of [2; 4]FIN 6 [1; 2]FIN as given in <ref> [Vel89] </ref>. 3.2 Simulation Theorem 12. If Player A has no winning strategy for the pair of matrices (A; B) in the FIN -game, then [A]FIN [B]FIN . Proof: Let [A] and [B] be matrices having m and n rows respectively.
References-found: 13

