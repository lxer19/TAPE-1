URL: ftp://ftp.cs.washington.edu/tr/1995/09/UW-CSE-95-09-01.PS.Z
Refering-URL: http://www.cs.washington.edu/homes/zahorjan/homepage/listof.htm
Root-URL: 
Title: Optimizing Data Locality by Array Restructuring  
Abstract: Shun-Tak Leung and John Zahorjan Department of Computer Science and Engineering University of Washington Technical Report 95-09-01 September 1995 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Jennifer M. Anderson, Saman P. Amarasinghe, and Monica S. Lam. </author> <title> Data and computation transformations for multiprocessors. </title> <booktitle> In Proceedings of Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: By contrast, we consider more general data transformations, select one algorithmically, and focus on array restructuring. Anderson et al. restructure arrays to improve spatial locality and reduce false sharing on shared-memory multiprocessors <ref> [1] </ref>. Given a data distribution, they place array elements assigned to the same processor together in memory.
Reference: [2] <author> Steve Carr, Kathryn S. McKinley, and Chau-Wen Tseng. </author> <title> Compiler optimizations for improving data locality. </title> <booktitle> In Proceedings of Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 252-262, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: A number of techniques that follow this approach can be found in the literature <ref> [11, 10, 2, 18, 15, 14] </ref>. This paper focuses on a less widely studied approach: restructuring arrays. Data locality is improved by selecting the layouts of the array elements to better match the order of the accesses. <p> For CFFT2D, which implements a two-dimensional FFT, there is no obvious way to restructure the loop nest for better locality, whereas restructuring the arrays speeds up execution significantly. Previous work on loop restructuring did not perform any optimizations on this loop nest [13, 19] or reported no performance improvement <ref> [2] </ref>. Loop restructuring is frustrated by imperfect loop nesting and non-affine array indexing expressions that involve indirection arrays. Neither of these, however, prevents us from restructuring the 2 Loop tiling is not included here. <p> Although these are merely standard transformations, it seems questionable whether this combination can be automatically selected with existing techniques. (Carr et al., who work on improving locality with such loop transformations, report no performance gain for this loop nest <ref> [2] </ref>.) In earlier experiments not reported here, we observed significant performance degradation when only some of the loop transformations were applied. <p> In the cases of BTRIX and VPENTA, this overhead would make array restructuring unattractive. For EMIT, the original code is already highly optimized for data locality. We have not performed any optimization on this routine. Neither have several others who have experimented with it <ref> [13, 2, 19] </ref>. No performance results are reported here. To sum up, we have compared the performance of array restructuring with that of some common loop restructuring techniques. <p> Kennedy and McKinley developed a cost model to guide the selection of loop permutation transformations for enhancing data locality [11]. They also studied the use of loop fusion and distribution for this purpose [10]. Carr et al. reported an extensive performance study of these techniques <ref> [2] </ref>. Wolf and Lam use loop tiling, which combines stripmining and loop permutation, to increase the likelihood of reusing cached data in the innermost loops [18]. Li and Pingali proposed a linear algebraic framework for loop transformation [15].
Reference: [3] <author> Michal Cierniak and Wei Li. </author> <title> Unifying data and control transformations for distributed shared memory machines. </title> <booktitle> In Proceedings of ACM SIGPLAN '95 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: Our general transformation subsumes previous techniques that are, in principle or in practice, restricted to the permutation of array dimensions <ref> [9, 3] </ref>. Such generality is useful in the optimization of banded matrix computations, for example [3]. We have implemented our technique in the SUIF compiler [17], an experimental parallelizing compiler form Stanford. In this paper, we present our array restructuring techniques and report experiments to evaluate their effect on performance. <p> Our general transformation subsumes previous techniques that are, in principle or in practice, restricted to the permutation of array dimensions [9, 3]. Such generality is useful in the optimization of banded matrix computations, for example <ref> [3] </ref>. We have implemented our technique in the SUIF compiler [17], an experimental parallelizing compiler form Stanford. In this paper, we present our array restructuring techniques and report experiments to evaluate their effect on performance. <p> Our results show that array restructuring can substantially speed up loop execution and compares favorably to alternative approaches. In addition to restructuring loops and restructuring arrays, a compiler might also restructure both simultaneously <ref> [9, 3] </ref>. Although this hybrid approach is potentially more powerful than the other two, how to efficiently and fully exploit its potential is still unclear. <p> So far, this has proven to be too difficult. Cierniak and Li discussed a framework that can represent and evaluate a rich set of array and loop transformations <ref> [3] </ref>. However, finding the resultant general optimization problem too hard, they select specific transformations to apply to individual loop nests by exhaustively searching an a priori subset of the possibilities allowed by their framework [3]. <p> discussed a framework that can represent and evaluate a rich set of array and loop transformations <ref> [3] </ref>. However, finding the resultant general optimization problem too hard, they select specific transformations to apply to individual loop nests by exhaustively searching an a priori subset of the possibilities allowed by their framework [3]. Exhaustive searching can become prohibitively expensive because its cost grows exponentially with the number of arrays and loops analyzed. This limits how many options can be explored within a reasonable amount of time, even with heuristic pruning of unpromising options. <p> On the other hand, they consider both array and loop restructuring while we focus on array restructuring. Cierniak and Li propose a linear algebraic framework integrating control and data transformations to enhance locality and reduce false sharing <ref> [3] </ref>. Their framework allows index-to-address mappings to be any linear mapping, but in practice they restrict attention to those that, in effect, represent different permutations of array dimensions and exhaustively search the design space for an optimal solution.
Reference: [4] <institution> Digital Equipment Corporation, Maynard, </institution> <address> MA. </address> <note> DEC 3000 Model 400/400S AXP Technical Summary, </note> <month> November </month> <year> 1992. </year>
Reference-contexts: This allows procedures that have been transformed by our compiler to be linked with separately compiled procedures that have not. The compiler outputs C code, which is then compiled by the native C compiler. The experiments were done on a DEC3000 Model 400 workstation running DEC OSF/1 <ref> [4, 6] </ref>. The workstation is based on the DEC Alpha architecture. It has two levels of cache for data: an on-chip, write-through, 8 KB data cache, and an off-chip, write-back, 512 KB unified cache shared by instructions and data. Both levels of cache are direct-mapped.
Reference: [5] <author> J. Dongarra, J. Du Croz, S. Hammarling, and I. Duff. </author> <title> A set of level 3 basic linear algebra subprograms. </title> <journal> ACM Transactions on Mathematical Software, </journal> <pages> pages 1-17, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: Different parts of this figure will be explained as they become relevant to our discussion. This code, adapted from [14], implements a symmetric rank 2k update (SYR2K) for banded matrices. (SYR2K is one of the Level 3 Basic Linear Algebra Subprograms <ref> [5] </ref>.) The original and transformed codes are shown at the top, the access and transformation matrices in the middle, and graphical depictions of array layouts at various stages of transformation at the bottom.
Reference: [6] <author> Todd A. Dutton, Daniel Eiref, Hugh R. Kurth, James J. Reisert, and Robin L. Stewart. </author> <title> The design of the DEC 3000 AXP systems, two high-performance workstations. </title> <journal> Digital Technical Journal, </journal> <volume> 4(4), </volume> <year> 1992. </year>
Reference-contexts: This allows procedures that have been transformed by our compiler to be linked with separately compiled procedures that have not. The compiler outputs C code, which is then compiled by the native C compiler. The experiments were done on a DEC3000 Model 400 workstation running DEC OSF/1 <ref> [4, 6] </ref>. The workstation is based on the DEC Alpha architecture. It has two levels of cache for data: an on-chip, write-through, 8 KB data cache, and an off-chip, write-back, 512 KB unified cache shared by instructions and data. Both levels of cache are direct-mapped.
Reference: [7] <author> Susan J. Eggers and Tor E. Jeremiassen. </author> <title> Eliminating false sharing. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1991. </year>
Reference-contexts: The array layout on each individual processor is not further optimized for locality. We, on the other hand, optimize array layouts for single-processor execution, whether on a uniprocessor or on one processor of a shared-memory multiprocessor. Jeremiassen and Eggers transform array data structures to reduce false sharing <ref> [7, 8] </ref>. Their work differs form ours in several ways. First, they focus on coarse-grain, explicitly parallel programs while we are concerned with both sequential and parallelized loops. Second, their goal is to reduce false sharing while we primarily aim to improve spatial locality.
Reference: [8] <author> Tor E. Jeremiassen and Susan J. Eggers. </author> <title> Reducing false sharing on shared memory multiprocessors through compile time data transformations. </title> <booktitle> In Proceedings of Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: A different form of data structures reorganization has been used to reduce false sharing on shared-memory multiprocessors <ref> [8, 12] </ref>. Array restructuring is attractive in several ways. First, it is not constrained by loop-carried dependences or imperfect loop nesting, which complicate and sometimes frustrate loop restructuring. <p> The array layout on each individual processor is not further optimized for locality. We, on the other hand, optimize array layouts for single-processor execution, whether on a uniprocessor or on one processor of a shared-memory multiprocessor. Jeremiassen and Eggers transform array data structures to reduce false sharing <ref> [7, 8] </ref>. Their work differs form ours in several ways. First, they focus on coarse-grain, explicitly parallel programs while we are concerned with both sequential and parallelized loops. Second, their goal is to reduce false sharing while we primarily aim to improve spatial locality.
Reference: [9] <author> Y.-J. Ju and H. Dietz. </author> <title> Reduction of cache coherence overhead by compiler data layout and loop transformation. </title> <booktitle> In Proceedings of the Fourth International Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 344-358, </pages> <month> August </month> <year> 1991. </year> <month> 24 </month>
Reference-contexts: Our general transformation subsumes previous techniques that are, in principle or in practice, restricted to the permutation of array dimensions <ref> [9, 3] </ref>. Such generality is useful in the optimization of banded matrix computations, for example [3]. We have implemented our technique in the SUIF compiler [17], an experimental parallelizing compiler form Stanford. In this paper, we present our array restructuring techniques and report experiments to evaluate their effect on performance. <p> Our results show that array restructuring can substantially speed up loop execution and compares favorably to alternative approaches. In addition to restructuring loops and restructuring arrays, a compiler might also restructure both simultaneously <ref> [9, 3] </ref>. Although this hybrid approach is potentially more powerful than the other two, how to efficiently and fully exploit its potential is still unclear. <p> Data restructuring has also been used to improve spatial locality and reduce false sharing on shared memory multiprocessors. Ju and Dietz reduce cache coherence overhead by permuting array dimensions and by loop permutation <ref> [9] </ref>. They restrict their array restructuring options to permutations of array dimensions, which are subsumed by the linear mappings in our approach. On the other hand, they consider both array and loop restructuring while we focus on array restructuring.
Reference: [10] <author> K. Kennedy and K. S. McKinley. </author> <title> Maximizing loop parallelism and improving data locality via loop fusion and distribution. </title> <booktitle> In Proceedings of the Sixth International Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: A number of techniques that follow this approach can be found in the literature <ref> [11, 10, 2, 18, 15, 14] </ref>. This paper focuses on a less widely studied approach: restructuring arrays. Data locality is improved by selecting the layouts of the array elements to better match the order of the accesses. <p> Kennedy and McKinley developed a cost model to guide the selection of loop permutation transformations for enhancing data locality [11]. They also studied the use of loop fusion and distribution for this purpose <ref> [10] </ref>. Carr et al. reported an extensive performance study of these techniques [2]. Wolf and Lam use loop tiling, which combines stripmining and loop permutation, to increase the likelihood of reusing cached data in the innermost loops [18]. Li and Pingali proposed a linear algebraic framework for loop transformation [15].
Reference: [11] <author> Ken Kennedy and Kathryn S. McKinley. </author> <title> Optimizing for parallelism and data locality. </title> <booktitle> In Proceedings of 1992 International Conference on Supercomputing, </booktitle> <year> 1992. </year>
Reference-contexts: A number of techniques that follow this approach can be found in the literature <ref> [11, 10, 2, 18, 15, 14] </ref>. This paper focuses on a less widely studied approach: restructuring arrays. Data locality is improved by selecting the layouts of the array elements to better match the order of the accesses. <p> Previous work found this loop interchange to be optimal for overall locality <ref> [14, 11] </ref>, and our experiments with all six possible loop permutations also confirm this. However, it sacrifices some temporal locality in the accesses to C, the product array, for much better spatial locality in the accesses to B, one of the two operand arrays. <p> Kennedy and McKinley developed a cost model to guide the selection of loop permutation transformations for enhancing data locality <ref> [11] </ref>. They also studied the use of loop fusion and distribution for this purpose [10]. Carr et al. reported an extensive performance study of these techniques [2].
Reference: [12] <author> Shun-Tak Leung and John Zahorjan. </author> <title> Restructuring arrays for efficient parallel loop execution. </title> <type> Technical Report 94-02-01, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <year> 1994. </year>
Reference-contexts: A different form of data structures reorganization has been used to reduce false sharing on shared-memory multiprocessors <ref> [8, 12] </ref>. Array restructuring is attractive in several ways. First, it is not constrained by loop-carried dependences or imperfect loop nesting, which complicate and sometimes frustrate loop restructuring. <p> Leung and Zahorjan dynamically restructure arrays to reduce false sharing and improve spatial locality in runtime parallelized loops <ref> [12] </ref>. Since this work focuses on runtime parallelized loops for which the access patterns are irregular, the index-to-address mapping is represented by a runtime generated indirection table rather than a parametric form like a linear mapping.
Reference: [13] <author> Wei Li. </author> <title> Compiling for NUMA Parallel Machines. </title> <type> PhD thesis, </type> <institution> Cornell University, </institution> <address> Ithaca, NY, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: For CFFT2D, which implements a two-dimensional FFT, there is no obvious way to restructure the loop nest for better locality, whereas restructuring the arrays speeds up execution significantly. Previous work on loop restructuring did not perform any optimizations on this loop nest <ref> [13, 19] </ref> or reported no performance improvement [2]. Loop restructuring is frustrated by imperfect loop nesting and non-affine array indexing expressions that involve indirection arrays. Neither of these, however, prevents us from restructuring the 2 Loop tiling is not included here. <p> In the cases of BTRIX and VPENTA, this overhead would make array restructuring unattractive. For EMIT, the original code is already highly optimized for data locality. We have not performed any optimization on this routine. Neither have several others who have experimented with it <ref> [13, 2, 19] </ref>. No performance results are reported here. To sum up, we have compared the performance of array restructuring with that of some common loop restructuring techniques.
Reference: [14] <author> Wei Li. </author> <title> Compiler cache optimizations for banded matrix problems. </title> <booktitle> In Proceedings of 1995 International Conference on Supercomputing, </booktitle> <year> 1995. </year>
Reference-contexts: A number of techniques that follow this approach can be found in the literature <ref> [11, 10, 2, 18, 15, 14] </ref>. This paper focuses on a less widely studied approach: restructuring arrays. Data locality is improved by selecting the layouts of the array elements to better match the order of the accesses. <p> Specifically, the focus is on the nonzero structures of these matrices. We use the example in Figure 2 to aid the following discussion, as well as discussion in following sections. Different parts of this figure will be explained as they become relevant to our discussion. This code, adapted from <ref> [14] </ref>, implements a symmetric rank 2k update (SYR2K) for banded matrices. (SYR2K is one of the Level 3 Basic Linear Algebra Subprograms [5].) The original and transformed codes are shown at the top, the access and transformation matrices in the middle, and graphical depictions of array layouts at various stages of <p> Our compiler in effect decides to store C and A in row-major order and B in column-major order. DO i = 1, N DO k = 1, N The second loop nest, adapted from <ref> [14] </ref>, implements a symmetric rank 2k update (SYR2K) for banded matrices. We have already discussed this loop nest extensively as our running example in Section 3 and Section 4. <p> Previous work found this loop interchange to be optimal for overall locality <ref> [14, 11] </ref>, and our experiments with all six possible loop permutations also confirm this. However, it sacrifices some temporal locality in the accesses to C, the product array, for much better spatial locality in the accesses to B, one of the two operand arrays. <p> It does improve performance, but array restructuring achieves even better performance because we gain spatial locality without losing temporal locality. Runtime restructuring overhead, even if needed, is trivial, as we saw earlier. * As for SYR2K, adapted from <ref> [14] </ref>, array restructuring reduces execution time to 15% of the original, compared with about 25% for loop restructuring. This loop nest is particularly difficult to optimize because of its complex access pattern. The restructured loop nest that we used is obtained using the technique in [14] 3 . <p> As for SYR2K, adapted from <ref> [14] </ref>, array restructuring reduces execution time to 15% of the original, compared with about 25% for loop restructuring. This loop nest is particularly difficult to optimize because of its complex access pattern. The restructured loop nest that we used is obtained using the technique in [14] 3 . As in the case of MATMUL, array restructuring achieves better performance because it needs not trade temporal locality for gains in spatial locality. * The performance of MXM (a highly optimized, hand-tuned implementation of multiplying two matrices) is not changed by array restructuring. <p> No performance results are reported here. To sum up, we have compared the performance of array restructuring with that of some common loop restructuring techniques. One loop nest that defies loop restructuring is optimized by restructuring the 3 The restructured loop nest given in <ref> [14] </ref> contains loop bounds and array indices with integer divisions. We manually transformed the loop nest further, without reordering the iterations, to eliminate these divisions. 18 array instead. In other cases, array restructuring improves performance as much as or sometimes more than loop restructuring. <p> Li and Pingali proposed a linear algebraic framework for loop transformation [15]. Based on that 20 framework, Li also developed algorithms for selecting loop transformations to enhance locality and reduce false sharing, especially in banded matrix applications <ref> [14] </ref>. Data restructuring has also been used to improve spatial locality and reduce false sharing on shared memory multiprocessors. Ju and Dietz reduce cache coherence overhead by permuting array dimensions and by loop permutation [9].
Reference: [15] <author> Wei Li and Keshav Pingali. </author> <title> A singular loop transformation framework based on non-singular matrices. </title> <booktitle> In Proceedings of the Fifth International Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1992. </year>
Reference-contexts: A number of techniques that follow this approach can be found in the literature <ref> [11, 10, 2, 18, 15, 14] </ref>. This paper focuses on a less widely studied approach: restructuring arrays. Data locality is improved by selecting the layouts of the array elements to better match the order of the accesses. <p> Carr et al. reported an extensive performance study of these techniques [2]. Wolf and Lam use loop tiling, which combines stripmining and loop permutation, to increase the likelihood of reusing cached data in the innermost loops [18]. Li and Pingali proposed a linear algebraic framework for loop transformation <ref> [15] </ref>. Based on that 20 framework, Li also developed algorithms for selecting loop transformations to enhance locality and reduce false sharing, especially in banded matrix applications [14]. Data restructuring has also been used to improve spatial locality and reduce false sharing on shared memory multiprocessors.
Reference: [16] <author> Alexander Schrijver. </author> <title> Theory of Linear and Integer Programming. Series in Discrete Mathematics. </title> <publisher> Wiley, </publisher> <year> 1986. </year>
Reference-contexts: The detailed algorithm to construct an appropriate series of row operations and compute the corresponding Q efficiently can be found in Appendix B. Computing U from R in the second step hinges on finding the Hermite normal form <ref> [16] </ref> of R. Let H be this Hermite normal form. We choose U = H 1 R. The proof of Lemma 2 in Appendix A shows that this U is unimodular and corresponding columns of U A and RA have the same heights. <p> We choose U = H 1 R. The proof of Lemma 2 in Appendix A shows that this U is unimodular and corresponding columns of U A and RA have the same heights. An algorithm to find the Hermite normal form of a nonsingular matrix is in the literature <ref> [16] </ref>. 3.3 Multiple Accesses in Imperfectly Nested Loops Finally, we briefly discuss how to handle cases that do not satisfy the two simplifying restrictions imposed at the beginning of this section: single access and perfect loop nesting. 8 To handle multiple accesses to the same array, we merge columns from the <p> The first transformation T makes the k-direction horizontal; the second transformation S keeps it that way. To find a tight enclosing parallelogram with horizontal sides, we apply the Fourier-Motzkin algorithm <ref> [16] </ref> to the range parallelogram and select for each dimension the closest pair of parallel lines from those generated by the algorithm. The two chosen pairs define the enclosing parallelogram. In general, the enclosing parallelogram is defined by pairs of parallel hyperplanes that tightly bound the range parallelogram between them. <p> A Mathematical Proofs Lemma 1 Let T be a nonsingular matrix. Both T and T 1 are integral if and only if T is unimodular. Proof: First consider the "if" part. If T is unimodular, it is integral by definition. Its inverse T 1 is also unimodular <ref> [16] </ref> and thus integral by definition. Now, we turn to the "only if" part. Since T is integral, jT j is an integer. Since T is nonsingular, jT j 6= 0. Thus, the absolute value of jT j must be at least 1. <p> Proof: Since R is nonsingular, it is of full row rank. There exists a unimodular matrix V such that RV (denoted H from now on) is in Hermite normal form <ref> [16] </ref>. The inverses of V and of H both exist. For V , this is because V is unimodular and therefore, by definition, nonsingular [16]. For H, the reason is that H = RV and both R and V are nonsingular. Let U = H 1 R. <p> There exists a unimodular matrix V such that RV (denoted H from now on) is in Hermite normal form <ref> [16] </ref>. The inverses of V and of H both exist. For V , this is because V is unimodular and therefore, by definition, nonsingular [16]. For H, the reason is that H = RV and both R and V are nonsingular. Let U = H 1 R. We now show that U is unimodular and columns of U A have the same heights as corresponding columns of RA. <p> First, note that H = RV ) I = (H 1 R)V = U V Thus, U is the inverse of a unimodular matrix V and for that reason also unimodular <ref> [16] </ref>. Second, for any matrix A, U A = (H 1 R)A = H 1 (RA) Since H is in Hermite normal form, it is by definition lower-triangular and therefore so is its inverse H 1 . The corresponding columns U A and RA have the same heights.
Reference: [17] <author> R. P. Wilson, R. S. French, C. S. Wilson, S. P. Amarasinghe, J. M. Anderson, S. W. K. Tjiang, S. W. Liao, C. W. Tseng, M. W. Hall, M. S. Lam, and J. L. Hennessy. </author> <title> SUIF: An infrastructure for research on parallelizing and optimizing compilers. </title> <journal> SIGPLAN Notices, </journal> <volume> 29(12) </volume> <pages> 31-37, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: Our general transformation subsumes previous techniques that are, in principle or in practice, restricted to the permutation of array dimensions [9, 3]. Such generality is useful in the optimization of banded matrix computations, for example [3]. We have implemented our technique in the SUIF compiler <ref> [17] </ref>, an experimental parallelizing compiler form Stanford. In this paper, we present our array restructuring techniques and report experiments to evaluate their effect on performance. Our results show that array restructuring can substantially speed up loop execution and compares favorably to alternative approaches. <p> Measurements of the performance impact of this transformation are reported in Section 6. 6 Performance We have implemented our array restructuring technique in the SUIF compiler <ref> [17] </ref> and performed a number of experiments to evaluate their effectiveness. The results are reported in this section. We first study the performance impact of array restructuring by itself. Then, array restructuring is compared with common loop restructuring techniques. <p> We first study the performance impact of array restructuring by itself. Then, array restructuring is compared with common loop restructuring techniques. Finally, we discuss how it interacts with loop tiling. 6.1 Experiments We have implemented our array restructuring technique in the SUIF compiler <ref> [17] </ref>. For each loop nest, the compiler analyzes how arrays are accessed, chooses the index transformations, and modifies the array accesses. The compiler also generates calls to our runtime system, which dynamically restructures array data between loop nests when needed.
Reference: [18] <author> Michael E. Wolf and Monica S. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In Proceedings of ACM SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 30-44, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: A number of techniques that follow this approach can be found in the literature <ref> [11, 10, 2, 18, 15, 14] </ref>. This paper focuses on a less widely studied approach: restructuring arrays. Data locality is improved by selecting the layouts of the array elements to better match the order of the accesses. <p> In some cases, however, they are too high for array restructuring to benefit performance if arrays have to be restructured dynamically. 6.4 Array Restructuring and Tiling In this section, we study how array restructuring interacts with tiling, a powerful locality optimization technique with wide applicability <ref> [18] </ref>. We manually tiled the innermost loop (s) of three versions of each loop nest wherever appropriate: the original version, the one after array restructuring, and the loop transformed version used in earlier experiments. Each tiled loop nest's execution time was measured for a range of tile sizes. <p> Carr et al. reported an extensive performance study of these techniques [2]. Wolf and Lam use loop tiling, which combines stripmining and loop permutation, to increase the likelihood of reusing cached data in the innermost loops <ref> [18] </ref>. Li and Pingali proposed a linear algebraic framework for loop transformation [15]. Based on that 20 framework, Li also developed algorithms for selecting loop transformations to enhance locality and reduce false sharing, especially in banded matrix applications [14].

References-found: 18

