URL: http://arch.cs.ucdavis.edu/~chong/250C/now/hpdc94.ps
Refering-URL: http://arch.cs.ucdavis.edu/~chong/250C/now/
Root-URL: http://www.cs.ucdavis.edu
Title: Scheduling Large-Scale Parallel Computations on Networks of Workstations  
Author: Robert D. Blumofe and David S. Park 
Address: Cambridge, Massachusetts  
Affiliation: MIT Laboratory for Computer Science  
Abstract: Workstation networks are an underutilized yet valuable resource for solving large-scale parallel problems. In this paper, we present "idle-initiated" techniques for efficiently scheduling large-scale parallel computations on workstation networks. By "idle-initiated," we mean that idle computers actively search out work to do rather than wait for work to be assigned. The idle-initiated scheduler operates at both the macro and the micro levels. On the macro level, a computer without work joins an ongoing parallel computation as a participant. On the micro level, a participant without work "steals" work from some other participant of the same computation. We have implemented these scheduling techniques in Phish, a portable system for running dynamic parallel applications on a network of workstations. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Andrew Birrell, Greg Nelson, Susan Owicki, and Edward Wobber. </author> <title> Network objects. </title> <booktitle> In Proceedings of the Fourteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 217-230, </pages> <address> Asheville, North Carolina, </address> <month> December </month> <year> 1993. </year>
Reference-contexts: Pages migrate between processors on demand while a protocol ensures the consistency of multiple copies of a page. As an alternate approach, the shared global state in systems such as Emerald [16], Amber [8], Amoeba/Orca [24] and Network Objects <ref> [1] </ref> is a collection of abstract data types or objects. Objects can be placed on and mi grated between the network nodes. Operations can be invoked on an object no matter where the object is located, and protocols ensure the consistency of duplicate objects.
Reference: [2] <author> Robert D. Blumofe and Charles E. Leiserson. </author> <title> Scheduling multithreaded computations by work stealing. </title> <booktitle> In Proceedings of the 35th Annual Symposium on Foundations of Computer Science, </booktitle> <address> Santa Fe, New Mexico, </address> <month> November </month> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: Analytic results of Blumofe and Leiserson <ref> [2] </ref> show that for a large class of dynamic computations, the randomized work stealing strategy combined with LIFO execution order and FIFO steal order achieves linear speedup (with high probability) as well as tightly bounded communication and memory requirements. <p> Thus, our LIFO scheduler keeps the working set small, which is vital to achieving good performance because of the hierarchical memory organization of modern RISC workstations. Furthermore, increasing the number of participants does not increase the size of the working set. This property provably holds <ref> [2] </ref> for an algorithm that is only slightly different from the one we use. The remaining data in Table 2 show the extent to which the idle-initiated scheduler is able to avoid expensive network communication. Very few tasks need to be stolen. <p> DIB is of particular relevance to us, since backtrack search exhibits dynamic parallelism. In fact, DIB's scheduler inspired our idle-initiated scheduler. This type of scheduling technique actually goes back before DIB to MultiLisp [14] and has become known as work stealing <ref> [2] </ref>. Other systems address parallel computing on a network of workstations by maintaining shared global state. In the Ivy system [18], the global state is a paged virtual address space. Pages migrate between processors on demand while a protocol ensures the consistency of multiple copies of a page.
Reference: [3] <author> Matthias A. Blumrich, Kai Li, Richard Alpert, Cezary Dubnicki, Edward W. Felten, and Jonathan Sandberg. </author> <title> Virtual memory mapped network interface for the SHRIMP multicom-puter. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 142-153, </pages> <address> Chicago, Illinois, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: Furthermore, and more importantly, improving workstation networks with low-latency and high-bandwidth interconnects (such as ATM) is a very active field of research <ref> [3, 15, 17] </ref>, which is closing the gap between workstation networks and supercomputer interconnects. Parallel computing on a network of workstations presents a vast array of scheduling choices.
Reference: [4] <author> Eric A. Brewer and Robert Blumofe. Strata: </author> <title> A multi-layer communications library. </title> <note> Technical Report to appear, </note> <institution> MIT Laboratory for Computer Science, </institution> <month> January </month> <year> 1994. </year> <note> [Anonymous ftp: ftp.lcs.mit.edu:/pub/supertech/strata]. </note>
Reference-contexts: Phish applications are coded using a simple extension to the C programming language and a simple preprocessor that outputs native C embellished with calls to the Phish scheduling library. We support this programming model on both the CM-5 with the Strata <ref> [4, 13] </ref> scheduling library and on a network of workstations with Phish. More details of this programming model can be found in [13].
Reference: [5] <author> Eric A. Brewer and Bradley C. Kuszmaul. </author> <title> How to get good performance from the CM-5 data network. </title> <booktitle> In Proceedings of the 8th International Parallel Processing Symposium, </booktitle> <pages> pages 858-867, </pages> <address> Can-cun, Mexico, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: In the context of shared-memory multiprocessors, Tucker and Gupta [26] found that utilization and throughput are improved by space-sharing instead of time-sharing. They cite context-switch overhead as a significant factor. In the realm of message-passing parallel computing, Brewer and Kuszmaul <ref> [5] </ref> found another reason to avoid time-sharing. They found that achieving performance in message passing is critically tied to the rate at which messages can be received. When a process is swapped-out, it cannot receive messages | messages fill up available buffers and potentially clog the network.
Reference: [6] <author> Clemens H. Cap and Volker Strumpen. </author> <title> Efficient parallel computing in distributed workstation environments. </title> <journal> Parallel Computing, </journal> <volume> 19 </volume> <pages> 1221-1234, </pages> <year> 1993. </year>
Reference-contexts: PVM does not, however, provide much support for scheduling beyond a basic, static scheduler. In contrast, Phish provides a relatively high-level programming model that relieves the programmer of the need to schedule at the message-passing level. The Parform <ref> [6] </ref> is a message-passing system with an emphasis on dynamic load balancing. The Parform employs load sensors to determine dynamically the relative load of the various machines that make up the Parform. This information is then used to divide and distribute the various parallel tasks.
Reference: [7] <author> Nicholas Carriero and David Gelernter. </author> <title> Linda in context. </title> <journal> Communications of the ACM, </journal> <volume> 32(4) </volume> <pages> 444-458, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: On the other hand, there are important applications that don't need a shared global state for which Phish delivers tremendous performance. In the future, we plan to add shared global state to Phish. Linda <ref> [7] </ref> combines shared global state and scheduling issues into one simple paradigm: generative communication. The basic idea is that objects called "tuples" can be placed in, removed from, or simply read from a common "tuple-space." This simple notion turns out to be surprisingly expressive.
Reference: [8] <author> Jeffrey S. Chase, Franz G. Amador, Edward D. Lazowska, Henry M. Levy, and Richard J. Little-field. </author> <title> The Amber system: Parallel programming on a network of multiprocessors. </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 147-158, </pages> <address> Litchfield Park, Arizona, </address> <month> December </month> <year> 1989. </year>
Reference-contexts: In the Ivy system [18], the global state is a paged virtual address space. Pages migrate between processors on demand while a protocol ensures the consistency of multiple copies of a page. As an alternate approach, the shared global state in systems such as Emerald [16], Amber <ref> [8] </ref>, Amoeba/Orca [24] and Network Objects [1] is a collection of abstract data types or objects. Objects can be placed on and mi grated between the network nodes. Operations can be invoked on an object no matter where the object is located, and protocols ensure the consistency of duplicate objects.
Reference: [9] <author> David R. Cheriton. </author> <title> The V distributed system. </title> <journal> Communications of the ACM, </journal> <volume> 31(3) </volume> <pages> 314-333, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: Also, an overwhelming majority of synchronizations are local and therefore do not require any network communication. Ultimately, very few messages are sent. 5 Related work Much recent work in operating systems has gone to improving the utilization of workstation networks. To varying degrees, distributed operating systems such as V <ref> [9] </ref>, Sprite [21], and Amoeba [25] view the workstation network as a pool of processors on which processes are transparently placed and migrated. This transparent process placement improves throughput and utilization by more evenly spreading the load across processors.
Reference: [10] <author> Robert E. Felderman, Eve M. Schooler, and Leonard Kleinrock. </author> <title> The Benevolent Bandit Laboratory: A testbed for distributed algorithms. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 7(2) </volume> <pages> 303-311, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: In particular, as workstations become idle, they may join an ongoing computation, and when reclaimed by their owners, workstations may leave a computation. Piranha's creators call this capability "adaptive parallelism." This capability is also present in the Benevolent Bandit Laboratory <ref> [10] </ref>, a PC-based system. Phish also possesses this capability, and Phish's macro-level scheduler is very similar to these other systems. Phish's micro-level scheduler, however, is very different and works with the macro-level to give Phish the added capability of adapting parallelism to internal, as well as external, forces.
Reference: [11] <author> Raphael Finkel and Udi Manber. </author> <title> DIB | a distributed implementation of backtracking. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(2) </volume> <pages> 235-256, </pages> <month> April </month> <year> 1987. </year>
Reference-contexts: This information is then used to divide and distribute the various parallel tasks. In contrast, the idle-initiated scheduler does not move a task unless an idle machine requests work. The EcliPSe system [23] and the DIB system <ref> [11] </ref> employ workstation networks to run parallel applications from specific domains. EcliPSe performs stochastic simulation, and DIB performs backtrack search. DIB is of particular relevance to us, since backtrack search exhibits dynamic parallelism. In fact, DIB's scheduler inspired our idle-initiated scheduler.
Reference: [12] <author> David Gelernter and David Kaminsky. </author> <title> Supercomputing out of recycled garbage: Preliminary experience with Piranha. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <pages> pages 417-427, </pages> <address> Washington, D.C., </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Although no particular scheduling is actually built into Linda, our scheduling techniques | or any scheduling technique for that matter | could be implemented with Linda. In fact, Piranha <ref> [12] </ref> is a system built on top of Linda with design goals very similar to those of Phish. (The fact that these systems share a piscene name is purely coincidental.) Like Phish, Piranha allows a parallel application to run on a set of workstations that may grow and shrink during the
Reference: [13] <author> Michael Halbherr, Yuli Zhou, and Chris F. Jo-erg. </author> <title> MIMD-style parallel programming with continuation-passing threads. </title> <booktitle> In Proceedings of the 2nd International Workshop on Massive Parallelism: Hardware, Software, and Applications, </booktitle> <address> Capri, Italy, </address> <month> September </month> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: Phish applications are coded using a simple extension to the C programming language and a simple preprocessor that outputs native C embellished with calls to the Phish scheduling library. We support this programming model on both the CM-5 with the Strata <ref> [4, 13] </ref> scheduling library and on a network of workstations with Phish. More details of this programming model can be found in [13]. <p> We support this programming model on both the CM-5 with the Strata [4, 13] scheduling library and on a network of workstations with Phish. More details of this programming model can be found in <ref> [13] </ref>. Once a program has been compiled and bound with the Phish library, a user can set off a flurry of parallel computation on workstations throughout the network by simply invoking the program on his or her workstation. <p> The ray-tracing application renders images by tracing light rays around a mathematical model of a scene. More details of both the ray tracer and the protein folder can be found in <ref> [13] </ref>. We begin with data measuring the serial slowdown incurred by parallel scheduling overhead. The serial slowdown of an application is measured as the ratio of the single-processor execution time of the parallel code to the execution time of the best serial implementation of the same algorithm.
Reference: [14] <author> Robert H. Halstead, Jr. </author> <title> Implementation of Mul-tilisp: Lisp on a multiprocessor. </title> <booktitle> In Conference Record of the 1984 ACM Symposium on Lisp and Functional Programming, </booktitle> <pages> pages 9-17, </pages> <address> Austin, Texas, </address> <month> August </month> <year> 1984. </year>
Reference-contexts: EcliPSe performs stochastic simulation, and DIB performs backtrack search. DIB is of particular relevance to us, since backtrack search exhibits dynamic parallelism. In fact, DIB's scheduler inspired our idle-initiated scheduler. This type of scheduling technique actually goes back before DIB to MultiLisp <ref> [14] </ref> and has become known as work stealing [2]. Other systems address parallel computing on a network of workstations by maintaining shared global state. In the Ivy system [18], the global state is a paged virtual address space.
Reference: [15] <author> James C. Hoe. </author> <title> Effective parallel computation on workstation cluster with user-level communication network. </title> <type> Master's thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <month> February </month> <year> 1994. </year>
Reference-contexts: Furthermore, and more importantly, improving workstation networks with low-latency and high-bandwidth interconnects (such as ATM) is a very active field of research <ref> [3, 15, 17] </ref>, which is closing the gap between workstation networks and supercomputer interconnects. Parallel computing on a network of workstations presents a vast array of scheduling choices.
Reference: [16] <author> Eric Jul, Henry Levy, Norman Hutchinson, and Andrew Black. </author> <title> Fine-grained mobility in the Emerald system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6 </volume> <pages> 109-133, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: In the Ivy system [18], the global state is a paged virtual address space. Pages migrate between processors on demand while a protocol ensures the consistency of multiple copies of a page. As an alternate approach, the shared global state in systems such as Emerald <ref> [16] </ref>, Amber [8], Amoeba/Orca [24] and Network Objects [1] is a collection of abstract data types or objects. Objects can be placed on and mi grated between the network nodes.
Reference: [17] <author> H. T. Kung, Robert Sansom, Steven Schlick, Peter Steenkiste, Matthieu Arnould, Francois J. Bitz, Fred Christianson, Eric C. Cooper, Onat Menzilcioglu, Denise Ombres, and Brian Zill. </author> <title> Network-based multicomputers: An emerging parallel architecture. </title> <booktitle> In Supercomputing '91, </booktitle> <pages> pages 664-673, </pages> <address> Albuquerque, New Mexico, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: Furthermore, and more importantly, improving workstation networks with low-latency and high-bandwidth interconnects (such as ATM) is a very active field of research <ref> [3, 15, 17] </ref>, which is closing the gap between workstation networks and supercomputer interconnects. Parallel computing on a network of workstations presents a vast array of scheduling choices.
Reference: [18] <author> Kai Li and Paul Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: In fact, DIB's scheduler inspired our idle-initiated scheduler. This type of scheduling technique actually goes back before DIB to MultiLisp [14] and has become known as work stealing [2]. Other systems address parallel computing on a network of workstations by maintaining shared global state. In the Ivy system <ref> [18] </ref>, the global state is a paged virtual address space. Pages migrate between processors on demand while a protocol ensures the consistency of multiple copies of a page.
Reference: [19] <author> Kai Li, Richard Lipton, David DeWitt, and Jeffrey Naughton. </author> <title> SHRIMP (scalable high-performance really inexpensive multicomputer project). </title> <booktitle> In ARPA High Performance Computing Software PI Meeting, </booktitle> <address> San Diego, California, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: Furthermore, a typical workstation is far less expensive than a compute node of most massively parallel supercomputers <ref> [19] </ref>. Because of these characteristics, networks of workstations are worthy of study as a way of performing large-scale parallel computations. When compared to a massively parallel supercomputer, such as the CM-5 from Thinking Machines, a network of workstations suffers an obvious weakness: network performance.
Reference: [20] <author> Michael J. Litzkow, Miron Livny, and Matt W. </author> <title> Mutka. Condor | a hunter of idle workstations. </title> <booktitle> In Proceedings of the 8th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 104-111, </pages> <address> San Jose, California, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. government. lent, and since much of a typical workstation's computing capacity goes unused <ref> [20] </ref>, a workstation network presents a large source of compute power on which to run large-scale parallel applications. Furthermore, a typical workstation is far less expensive than a compute node of most massively parallel supercomputers [19].
Reference: [21] <author> John K. Ousterhout, Andrew R. Cherenson, Frederick Douglis, Michael N. Nelson, and Brent B. Welch. </author> <title> The Sprite network operating system. </title> <journal> IEEE Computer, </journal> <volume> 21(2) </volume> <pages> 23-36, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: Ultimately, very few messages are sent. 5 Related work Much recent work in operating systems has gone to improving the utilization of workstation networks. To varying degrees, distributed operating systems such as V [9], Sprite <ref> [21] </ref>, and Amoeba [25] view the workstation network as a pool of processors on which processes are transparently placed and migrated. This transparent process placement improves throughput and utilization by more evenly spreading the load across processors.
Reference: [22] <author> V. S. Sunderam. </author> <title> PVM: A framework for parallel distributed computing. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 2(4) </volume> <pages> 315-339, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: Unlike Phish, which focuses on dynamic parallel applications, these systems are concerned primarily with static distributed applications. Among systems that do directly address parallel computing on workstation networks, some focus primarily on message-passing. Of particular note in this category is PVM <ref> [22] </ref>. PVM provides a collection of message passing and coordination primitives that an application can use to orchestrate the operation of its various parallel components. PVM does not, however, provide much support for scheduling beyond a basic, static scheduler.
Reference: [23] <author> V. S. Sunderam and Vernon J. Rego. </author> <title> EcliPSe: A system for high performance concurrent simulation. </title> <journal> Software|Practice and Experience, </journal> <volume> 21(11) </volume> <pages> 1189-1219, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: This information is then used to divide and distribute the various parallel tasks. In contrast, the idle-initiated scheduler does not move a task unless an idle machine requests work. The EcliPSe system <ref> [23] </ref> and the DIB system [11] employ workstation networks to run parallel applications from specific domains. EcliPSe performs stochastic simulation, and DIB performs backtrack search. DIB is of particular relevance to us, since backtrack search exhibits dynamic parallelism. In fact, DIB's scheduler inspired our idle-initiated scheduler.
Reference: [24] <author> Andrew S. Tanenbaum, Henri E. Bal, and M. Frans Kaashoek. </author> <title> Programming a distributed system using shared objects. </title> <booktitle> In Proceedings of the Second International Symposium on High Performance Distributed Computing, </booktitle> <pages> pages 5-12, </pages> <address> Spokane, Washington, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: In the Ivy system [18], the global state is a paged virtual address space. Pages migrate between processors on demand while a protocol ensures the consistency of multiple copies of a page. As an alternate approach, the shared global state in systems such as Emerald [16], Amber [8], Amoeba/Orca <ref> [24] </ref> and Network Objects [1] is a collection of abstract data types or objects. Objects can be placed on and mi grated between the network nodes. Operations can be invoked on an object no matter where the object is located, and protocols ensure the consistency of duplicate objects.
Reference: [25] <author> Andrew S. Tanenbaum, Robbert van Renesse, Hans van Staveren, Gregory J. Sharp, Sape J. Mullender, Jack Jansen, and Guido van Rossum. </author> <title> Experiences with the Amoeba distributed operating system. </title> <journal> Communications of the ACM, </journal> <volume> 33(12) </volume> <pages> 46-63, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: Ultimately, very few messages are sent. 5 Related work Much recent work in operating systems has gone to improving the utilization of workstation networks. To varying degrees, distributed operating systems such as V [9], Sprite [21], and Amoeba <ref> [25] </ref> view the workstation network as a pool of processors on which processes are transparently placed and migrated. This transparent process placement improves throughput and utilization by more evenly spreading the load across processors.
Reference: [26] <author> Andrew Tucker and Anoop Gupta. </author> <title> Process control and scheduling issues for multiprogrammed shared-memory multiprocessors. </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 159-166, </pages> <address> Litchfield Park, Arizona, </address> <month> December </month> <year> 1989. </year>
Reference-contexts: Clearly, this technique of allocating the 32 processors to the 4 jobs may not be the most efficient choice. First, empirical evidence (though in a somewhat different context) <ref> [26] </ref> indicates that better throughput may be achieved by space-sharing rather than time-sharing | in other words, assign each of the 4 jobs to 8 of the processors. In this manner, each job gets a dedicated set of processors, and all context-switching overheads are avoided. <p> Whenever possible, however, the macro-level scheduler shares compute resources among parallel jobs by space-sharing. The preference for space-sharing over time-sharing is supported both by intuition and to some extent by empirical data. In the context of shared-memory multiprocessors, Tucker and Gupta <ref> [26] </ref> found that utilization and throughput are improved by space-sharing instead of time-sharing. They cite context-switch overhead as a significant factor. In the realm of message-passing parallel computing, Brewer and Kuszmaul [5] found another reason to avoid time-sharing.
References-found: 26

