URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1992/tr-92-004.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1992.html
Root-URL: http://www.icsi.berkeley.edu
Title: Best-First Model Merging for Dynamic Learning and Recognition  
Author: Stephen M. Omohundro 
Address: 1947 Center Street, Suite 600 Berkeley, California 94704  
Affiliation: International Computer Science Institute  
Abstract: Best-first model merging is a general technique for dynamically choosing the structure of a neural or related architecture while avoiding overfitting. It is applicable to both learning and recognition tasks and often generalizes significantly better than fixed structures. We demonstrate the approach applied to the tasks of choosing radial basis functions for function learning, choosing local affine models for curve and constraint surface modelling, and choosing the structure of a balltree or bumptree to maximize efficiency of access.
Abstract-found: 1
Intro-found: 1
Reference: <author> D. H. Ballard and C. M. Brown. </author> <booktitle> (1982) Computer Vision. </booktitle> <address> Englewood Cliffs, N. J: </address> <publisher> Prentice-Hall. </publisher>
Reference: <author> W. L. Buntine and A. S. Weigend. </author> <title> (1992) Bayesian Back-Propagation. </title> <note> To appear in: Complex Systems. </note>
Reference-contexts: Many of the Gestalt phenomena can be considered in the same terms. Many of the processes used in recognition (eg. segmentation, grouping) have direct analogs in learning and vice versa. There has been much recent interest in the network community in Bayesian methods for model selection while avoiding overfitting <ref> (eg. Buntine and Weigend, 1992 and MacKay 1992) </ref>. Learning and recognition fit naturally together in a Bayesian framework. The Bayesian approach makes explicit the need for a prior distribution. The posterior distribution generated by learning becomes the prior distribution for recognition.
Reference: <author> L. Devroye and L. Gyorfi. </author> <title> (1985) Nonparametric Density Estimation: The L1 View, </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference-contexts: A powerful standard technique is adaptvie kernel estimation in which a normalized Gaussian (or other kernel) is placed at each sample point with a width determined by the local sample density <ref> (Devroye and Gyorfi, 1985) </ref>. Model merging can be applied to improve the generalization performance of this approach by choosing successively more complex component densities once enough data has accumulated by merging. For example, consider a density supported on a curve in a high dimensional space.
Reference: <author> D. J. MacKay. </author> <title> (1992) A Practical Bayesian Framework for Backprop Networks. </title> <type> Caltech preprint. </type>
Reference-contexts: Many of the Gestalt phenomena can be considered in the same terms. Many of the processes used in recognition (eg. segmentation, grouping) have direct analogs in learning and vice versa. There has been much recent interest in the network community in Bayesian methods for model selection while avoiding overfitting <ref> (eg. Buntine and Weigend, 1992 and MacKay 1992) </ref>. Learning and recognition fit naturally together in a Bayesian framework. The Bayesian approach makes explicit the need for a prior distribution. The posterior distribution generated by learning becomes the prior distribution for recognition.
Reference: <author> J. Moody and C. Darken. </author> <title> (1989) Fast learning in networks of locally-tuned processing units. </title> <journal> Neural Computation, </journal> <volume> 1, </volume> <pages> 281-294. </pages>
Reference-contexts: Better models may be obtained by using fewer basis functions than data points. Most work on choosing the centers of these functions uses a clustering technique such as k-means <ref> (eg. Moody and Darken, 1989) </ref>. This is reasonable because it puts the representational power of the model in the regions of highest density where errors are more critical. It ignores the structure of the modelled function, however.
Reference: <author> S. M. Omohundro. </author> <title> (1987) Efficient algorithms with neural network behavior. </title> <booktitle> Complex Systems 1 </booktitle> <pages> 273-347. </pages>
Reference-contexts: Complex time-varying curves may easily be processed in real time on typical workstations. In higher dimensions, hierarchical geometric data structures <ref> (as in Omohundro, 1987, 1990) </ref> allow a similar reduction in computation based on locality. Error=1 Error=20Error=5 Error=10Error=2 5 BALLTREE CONSTRUCTION The model merging approach is applicable to a wide variety of adaptive structures. The balltree structure described in (Omohundro, 1989) provides efficient access to regions in geometric spaces.
Reference: <author> S. M. Omohundro. </author> <title> (1989) Five balltree construction algorithms. </title> <institution> International Computer Science Institute Technical Report TR-89-063. </institution>
Reference-contexts: In higher dimensions, hierarchical geometric data structures (as in Omohundro, 1987, 1990) allow a similar reduction in computation based on locality. Error=1 Error=20Error=5 Error=10Error=2 5 BALLTREE CONSTRUCTION The model merging approach is applicable to a wide variety of adaptive structures. The balltree structure described in <ref> (Omohundro, 1989) </ref> provides efficient access to regions in geometric spaces. It consists of a nested hierarchy of hyper-balls surrounding given leaf balls and efficiently supports querries which test for intersection, inclusion, or nearness to a leaf ball.
Reference: <author> S. M. Omohundro. </author> <title> (1990) Geometric learning algorithms. </title> <journal> Physica D 42 </journal> <pages> 307-321. </pages>
Reference-contexts: The continuity prior is that the world is geometric and unless there is contrary data a system should prefer continuous models over discontinuous ones. This prior leads to a wide variety of what may be called geometric learning algorithms <ref> (Omohundro, 1990) </ref>. The sparseness prior is that the world is sparsely interacting. This says that probable models naturally decompose into components which only directly affect one another in a sparse manner.
Reference: <author> S. M. Omohundro. </author> <title> (1991) Bumptrees for Efficient Function, Constraint, and Classification Learning. </title> <editor> In Lippmann, Moody, and Touretzky, (eds.) </editor> <booktitle> Advances in Neural Information Processing Systems 3. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: ap -1.0 -0.6 -0.2 0.2 -0.5 1.5 Dots are training points Triangles are mm centers xs are k-means centers 21 samples, 6 centers RBF width .4 Gaussian width .4 Gaussian center -.5 Sigmoid width .4 Sigmoid Gaussian x proach naturally complements the efficient mapping and constraint surface representations described in <ref> (Omohundro, 1991) </ref> based on bumptrees. shows the endpoints chosen by the algorithm at various levels of allowed error. The bottom row shows the corresponding approximation to the curve. <p> We presented three different examples which only begin to touch on the possibilities. To hint at the broad applicability, we will briefly describe several other applications we are currently examining. In <ref> (Omohundro, 1991) </ref> we presented an efficient structure for modelling mappings based on a collection of local mapping models which were combined according to a partition of unity formed by influence functions associated with each model. This representation is very flexible and can be made computationally efficient.
Reference: <author> R. N. Shepard. </author> <title> (1987) Toward a universal law of generalization for psychological science. </title> <publisher> Science. </publisher>
Reference-contexts: Early in learning, an organism doesnt know which features of an experience are important unless it has a strong prior knowledge of the domain. Without such prior knowledge,its best strategy is to gener-alize on the basis of a similarity measure to individual stored experiences. <ref> (Shepard, 1987) </ref> shows that there is a universal exponentially decaying form for this kind of similarity based generalization over a wide variety of sensory domains in several studied species. As experiences accumulate, the organism eventually gets enough data to reliably validate models from complex classes.
Reference: <author> V. Vapnik. </author> <title> (1982) Estimation of Dependences Based on Empirical Data, </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: We would like a learning system to be able to induce extremely complex models of the world but we dont want to have to present it with the enormous amount of data needed to validate such a model unless it is really needed. <ref> (Vapnik, 1982) </ref> proposes a technique for avoiding overfitting while allowing models of arbitrary complexity. The idea is to start with a nested familty of model spaces, whose members contain ever more complex models.
References-found: 11

