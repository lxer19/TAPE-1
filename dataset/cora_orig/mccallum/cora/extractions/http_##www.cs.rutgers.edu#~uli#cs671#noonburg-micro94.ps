URL: http://www.cs.rutgers.edu/~uli/cs671/noonburg-micro94.ps
Refering-URL: http://www.cs.rutgers.edu/~uli/cs671/index.html
Root-URL: http://www.cs.rutgers.edu
Email: fderekn,sheng@ece.cmu.edu  
Title: Theoretical Modeling of Superscalar Processor Performance  
Author: Derek B. Noonburg John P. Shen 
Address: Pittsburgh, PA 15213  
Affiliation: Department of Electrical and Computer Engineering Carnegie Mellon University  
Date: November 1994.  
Note: To appear in Micro-27,  
Abstract: The current trace-driven simulation approach to determine superscalar processor performance is widely used but has some shortcomings. Modern benchmarks generate extremely long traces, resulting in problems with data storage, as well as very long simulation run times. More fundamentally, simulation generally does not provide significant insight into the factors that determine performance or a characterization of their interactions. This paper proposes a theoretical model of superscalar processor performance that addresses these shortcomings. Performance is viewed as an interaction of program parallelism and machine parallelism. Both program and machine parallelisms are decomposed into multiple component functions. Methods for measuring or computing these functions are described. The functions are combined to provide a model of the interaction between program and machine parallelisms and an accurate estimate of the performance. The computed performance, based on this model, is compared to simulated performance for six benchmarks from the SPEC 92 suite on several configurations of the IBM RS/6000 instruction set architecture. 
Abstract-found: 1
Intro-found: 1
Reference: [AS92] <author> Todd M. Austin and Gurindar S. Sohi. </author> <title> Dynamic Dependency Analysis of Ordinary Programs. </title> <booktitle> In Proc. ISCA 19, </booktitle> <pages> pages 342-351, </pages> <year> 1992. </year> <title> for various implementations of the RS/6000. The machine parallelism is varied by adjusting the numbers of integer and floating point functional units. </title>
Reference-contexts: Section 6 presents the theoretical model proposed in this work. Finally, Section 7 concludes with remarks about future research. 2 Background Numerous researchers have proposed methods to determine the maximum program parallelism present in code, using various ideal machine models <ref> [Jou89, Wal91, AS92, LW92, TGH92, DAF94] </ref>. Most of these are limit studies. Few have attempted to characterize the parallelism which can actually be obtained by realistic processors. Furthermore, most of these are experimental studies that attempt to obtain performance data empirically, usually by simulation.
Reference: [DAF94] <author> Pradeep K. Dubey, George B. Adams, III, and Michael J. Flynn. </author> <title> Instruction Window Size Trade-Offs and Characterization of Program Parallelism. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 43(4) </volume> <pages> 431-442, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Section 6 presents the theoretical model proposed in this work. Finally, Section 7 concludes with remarks about future research. 2 Background Numerous researchers have proposed methods to determine the maximum program parallelism present in code, using various ideal machine models <ref> [Jou89, Wal91, AS92, LW92, TGH92, DAF94] </ref>. Most of these are limit studies. Few have attempted to characterize the parallelism which can actually be obtained by realistic processors. Furthermore, most of these are experimental studies that attempt to obtain performance data empirically, usually by simulation. <p> Few have attempted to characterize the parallelism which can actually be obtained by realistic processors. Furthermore, most of these are experimental studies that attempt to obtain performance data empirically, usually by simulation. Dubey et al. have recently proposed an analytical model similar to ours <ref> [DAF94] </ref>. 2.1 The piecewise linear model Jouppi describes a simple theoretical performance model which uses the concepts of machine parallelism and benchmark (i.e., program) parallelism [Jou89]. Machine parallelism is defined as the product of the average degree of superpipelining [Jou89] and the degree of parallel issue.
Reference: [DSP93] <author> Trung A. Diep, John P. Shen, and Mike Phillip. </author> <title> EXPLORER: A Retargetable and Visualization-Based Trace-Driven Simulator for Superscalar Processors. </title> <booktitle> In Proc. MICRO-26, </booktitle> <pages> pages 225-235, </pages> <year> 1993. </year>
Reference-contexts: The benchmarks are simulated with the Explorer trace-driven RS/6000 simulator, which is part of the Visualization-based Microarchitecture Workbench <ref> [DSP93] </ref>. This simulator is easily retargetable and is cycle-by-cycle accurate; the baseline RS/6000 simulator has been validated with the real system. The error in the modeled performance ranges from 0:6% to +22%.
Reference: [HP90] <author> John L. Hennessy and David A. Patterson. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1990. </year>
Reference-contexts: This includes effects due to the source language, compiler, and ISA, and properties inherent to the program. The constraints are due to program dependences which can take two forms: control dependences and data dependences <ref> [HP90] </ref>. Hence, program parallelism is correspondingly decomposed into control parallelism and data parallelism in our model (see Figure 3). 4.1 Components Control parallelism is a measure of the number of useful instructions which are visible to the processor's instruction-fetch mechanism. <p> For superscalar processors, there are constraints that limit the capacity at various intermediate points of instruction processing. In our model, these constraints are divided into three categories | branch constraints, fetch/decode constraints, and issue constraints | to account for potential structural dependences <ref> [HP90] </ref> in various portions of the machine. Hence, machine parallelism is decomposed into branch parallelism, fetch parallelism, and issue parallelism (see Figure 3). 5.1 Components Instruction flow through the control path of a superscalar processor is limited by branch instructions.
Reference: [Jou89] <author> Norman P. Jouppi. </author> <title> The Nonuniform Distribution of Instruction-Level and Machine Parallelism and Its Effect on Performance. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(12) </volume> <pages> 1645-1658, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Section 6 presents the theoretical model proposed in this work. Finally, Section 7 concludes with remarks about future research. 2 Background Numerous researchers have proposed methods to determine the maximum program parallelism present in code, using various ideal machine models <ref> [Jou89, Wal91, AS92, LW92, TGH92, DAF94] </ref>. Most of these are limit studies. Few have attempted to characterize the parallelism which can actually be obtained by realistic processors. Furthermore, most of these are experimental studies that attempt to obtain performance data empirically, usually by simulation. <p> Dubey et al. have recently proposed an analytical model similar to ours [DAF94]. 2.1 The piecewise linear model Jouppi describes a simple theoretical performance model which uses the concepts of machine parallelism and benchmark (i.e., program) parallelism <ref> [Jou89] </ref>. Machine parallelism is defined as the product of the average degree of superpipelining [Jou89] and the degree of parallel issue. This is effectively the maximum number of instructions "in flight" (in the execution stages) in the processor. <p> et al. have recently proposed an analytical model similar to ours [DAF94]. 2.1 The piecewise linear model Jouppi describes a simple theoretical performance model which uses the concepts of machine parallelism and benchmark (i.e., program) parallelism <ref> [Jou89] </ref>. Machine parallelism is defined as the product of the average degree of superpipelining [Jou89] and the degree of parallel issue. This is effectively the maximum number of instructions "in flight" (in the execution stages) in the processor. Program parallelism is defined as the average speedup when the program is executed on an infinitely parallel superscalar processor, compared to execution on a single-issue processor. <p> See Figure 1. Jouppi describes several factors which account for the differences between the modeled and actual behaviors. These include variations in instruction latency, variations in per-cycle instruction parallelism, and variations in parallelism by instruction class. The suggested second-order model <ref> [Jou89] </ref> takes these nonuniformities into account to explain the lowering of the two performance asymptotes, i.e., the tilting of the machine parallelism-limited asymptote and the dropping of the program parallelism-limited asymptote (see Figure 1). <p> Our model focuses on the region where the two asymptotic regions meet. This region is important because it contains the optimal operating point. This model is an extension of the work described above. The idea of separately measuring program and machine parallelism is adapted from Jouppi's model <ref> [Jou89] </ref>. In addition, we provide a method (represented by the "INTERACTION" box in Figure 3) for combining program and machine parallelism to model ILP in the transition region. Theobald et al.'s smoothability experiments [TGH92] indicate that a single average parallelism number is not sufficient to model performance.
Reference: [LW92] <author> Monica S. Lam and Robert P. Wilson. </author> <title> Limits of Control Flow on Parallelism. </title> <booktitle> In Proc. ISCA 19, </booktitle> <pages> pages 46-57, </pages> <year> 1992. </year>
Reference-contexts: Section 6 presents the theoretical model proposed in this work. Finally, Section 7 concludes with remarks about future research. 2 Background Numerous researchers have proposed methods to determine the maximum program parallelism present in code, using various ideal machine models <ref> [Jou89, Wal91, AS92, LW92, TGH92, DAF94] </ref>. Most of these are limit studies. Few have attempted to characterize the parallelism which can actually be obtained by realistic processors. Furthermore, most of these are experimental studies that attempt to obtain performance data empirically, usually by simulation.
Reference: [RF93] <author> B. Ramakrishna Rau and Joseph A. Fisher. </author> <title> Instruction-Level Parallel Processing: History, Overview, and Perspective. </title> <journal> The Journal of Supercomputing, </journal> <volume> 7 </volume> <pages> 9-50, </pages> <year> 1993. </year>
Reference-contexts: instructions after that branch must wait for it to be resolved. (These instructions may be fetched, but they are not considered useful because it is not yet known whether they will be executed.) A processor can, however, extract control parallelism from beyond branches by using speculation, such as branch prediction <ref> [RF93] </ref>. With branch prediction, potentially useful instructions can be fetched from beyond branches.
Reference: [TGH92] <author> Kevin B. Theobald, Guang R. Gao, and Lau-rie J. Hendren. </author> <title> On the Limits of Program Parallelism and its Smoothability. </title> <booktitle> In Proc. MICRO-25, </booktitle> <pages> pages 10-19, </pages> <year> 1992. </year>
Reference-contexts: Section 6 presents the theoretical model proposed in this work. Finally, Section 7 concludes with remarks about future research. 2 Background Numerous researchers have proposed methods to determine the maximum program parallelism present in code, using various ideal machine models <ref> [Jou89, Wal91, AS92, LW92, TGH92, DAF94] </ref>. Most of these are limit studies. Few have attempted to characterize the parallelism which can actually be obtained by realistic processors. Furthermore, most of these are experimental studies that attempt to obtain performance data empirically, usually by simulation. <p> Effectively, Jouppi's piecewise linear model focuses on the asymptotic behavior of processor performance and not on the interaction of program and machine parallelism in the "transition" region. 2.2 Smoothability Theobald et al. introduce the concept of smoothability of program parallelism <ref> [TGH92] </ref> to explain this rounded knee in the transition region. Let P be the average program parallelism of a benchmark. If the program parallelism is perfectly smooth, i.e., evenly distributed over time, then a machine parallelism of dP e is sufficient to achieve maximum performance. <p> The idea of separately measuring program and machine parallelism is adapted from Jouppi's model [Jou89]. In addition, we provide a method (represented by the "INTERACTION" box in Figure 3) for combining program and machine parallelism to model ILP in the transition region. Theobald et al.'s smoothability experiments <ref> [TGH92] </ref> indicate that a single average parallelism number is not sufficient to model performance. Our model uses parallelism distributions to account for uneven, or nonuniform, distribution of parallelism.
Reference: [Wal91] <author> David W. Wall. </author> <title> Limits of Instruction-Level Parallelism. </title> <booktitle> In Proc. ASPLOS-IV, </booktitle> <pages> pages 176-188, </pages> <year> 1991. </year>
Reference-contexts: Section 6 presents the theoretical model proposed in this work. Finally, Section 7 concludes with remarks about future research. 2 Background Numerous researchers have proposed methods to determine the maximum program parallelism present in code, using various ideal machine models <ref> [Jou89, Wal91, AS92, LW92, TGH92, DAF94] </ref>. Most of these are limit studies. Few have attempted to characterize the parallelism which can actually be obtained by realistic processors. Furthermore, most of these are experimental studies that attempt to obtain performance data empirically, usually by simulation.
References-found: 9

