URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-444.ps.Z
Refering-URL: http://www-white.media.mit.edu/cgi-bin/tr_pagemaker/
Root-URL: http://www.media.mit.edu
Title: of Affective Signals  
Author: Jennifer Healey and Rosalind Picard Rm 
Address: 20 Ames St., Cambridge, MA 02139  
Affiliation: E15-389, The Media Laboratory Massachusetts Institute of Technology  
Note: Digital Processing  
Abstract: M.I.T Media Laboratory Perceptual Computing Section Technical Report No. 444 Submitted to: ICASSP97 Abstract Affective signal processing algorithms were developed to allow a digital computer to recognize the affective state of a user who is intentionally expressing that state. This paper describes the method used for collecting the training data, the feature extraction algorithms used and the results of pattern recognition using a Fisher linear discriminant and the leave one out test method. Four physiological signals, skin conductivity, blood volume pressure, respiration and an electromyo-gram (EMG) on the masseter muscle were analyzed. It was found that anger was well differentiated from peaceful emotions (90%- 100%), that high and low arousal states were distinguished (80%-88%), but positive and negative valence states were difficult to distinguish (50%-82%). Subsets of three emotion states could be well separated (75%- 87%) and characteristic patterns for single emotions were found.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. W. </author> <title> Picard. Affective Computing. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA. </address> <month> Sep. </month> <year> 1997. </year>
Reference-contexts: Traditionally, the fields of psychology and psychophysiology have sought features which indicate common affective states across large populations, despite the fact that there are wide variations in individual response patterns due to gender, personality type and ethnic background <ref> [1] </ref>. In this paper, we propose a tailored model that is based on recognizing the physiological signature of a single user's affective patterns over time. This analysis also differs from that of traditional emotion research in the time window considered.
Reference: [2] <author> P. Ekman. </author> <title> An argument for basic emotions. </title> <journal> Cognition and Emotion, </journal> 6(3/4):169-200, 1992. 
Reference-contexts: This analysis also differs from that of traditional emotion research in the time window considered. This experiment analyses how the user's affective state changes over an average period of three minutes, far longer than the one-to-ten second reaction usually analyzed in emotion research <ref> [2] </ref>.
Reference: [3] <author> R. W. Picard and J. Healey. </author> <title> Affective wearables. </title> <booktitle> In Proceedings of the First International Symposium on Wearable Computers, </booktitle> <address> Cambridge, MA, </address> <month> Oct. </month> <year> 1997. </year> <note> To appear. </note>
Reference-contexts: In other analysis, we have found that affective measurements for a single individual are easily overwhelmed by physiological factors within such a short window <ref> [3] </ref>; therefore in this work, we look for larger changes that could be detected in an individual over a three minute window. 2 Experimental Protocol In this experiment, a single subject intentionally expressed eight affective states toward a computer over a period of 32 days.
Reference: [4] <author> Dr. M. Clynes. Sentics: </author> <title> The Touch of the Emotions. </title> <address> Anchor Press/Doubleday, </address> <year> 1977. </year>
Reference-contexts: This experiment was chosen for ease of use and because Clynes reported finding a unique finger pressure signature for each of the eight emotion states used <ref> [4] </ref>. These states: no emotion, anger, hate, grief, love, romantic love, joy and reverence, are not the most commonly used in human computer design research, but they provide a set of affective states which span the ranges of high and low arousal and positive and negative valence.
Reference: [5] <author> J. Cacioppo and L. Tassinary. </author> <title> Principles of Psychophysiology: Physical, social, and inferential elements. </title> <publisher> Cambridge Press, </publisher> <address> Cambridge, </address> <year> 1990. </year>
Reference-contexts: The feature HR2 is multiplied by 100 here for formatting purposes. 2 hanning) was used to smooth the raw data and char-acterize the general trend of each affective state. The signal was normalized using the following suggested criteria to account for baseline fluctuations between days <ref> [5] </ref>: SCRmean (SCR) max (SCR)min (SCR) The mean of this signal for each emotion state was then calculated (SCR1) A second feature, the average change in the slope was calculated by taking the mean of the first difference of the smoothed signal (SCR2).
Reference: [6] <author> W. M. Winton, L. Putnam, and R. Krauss. </author> <title> Facial and autonomic manifestations of the dimensional structure of emotion. </title> <journal> Journal of Experimental Social Psychology, </journal> <volume> 20 </volume> <pages> 195-216, </pages> <year> 1984. </year>
Reference-contexts: For this time window it may be that the more general measure of overall rate is more descriptive, whereas for a shorter time window, heart rate acceleration has been a more discriminating feature <ref> [6] </ref>. For the respiration signal, the mean and variance of the time signal together differentiated between shallow regular breathing, shallow breathing punctuated by deep gasps and regular deep breathing.
Reference: [7] <author> R. O. Duda and P. E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley-Interscience, </publisher> <year> 1973. </year>
Reference-contexts: The PSD energy band features give more general characterization of the rhythmic patterns of respiration in each emotion state. 4 Pattern Recognition Results The techniques to discriminate states based on the eleven features used a Fisher linear discriminant projection <ref> [7] </ref> and the leave one out test method. For each trial, a single point, x, was excluded from the data set and a Fisher projection matrix, W was calculated for remaining members of the set. <p> The excluded point was then projected using W and classified using quadratic and linear classifiers, in the standard method described by Therrien [8]. The Fisher projection matrix which in some sense maximizes the ratio of the between-class scatter, S B , to the within class scatter, S W <ref> [7] </ref>, where these matrices are defined as: S W = i=1 x2 i S B = i=1 given c is the number of classes, n i is the number of sample vectors in a class, m i is the sample mean for class i, m is the total mean, and x
Reference: [8] <author> C. W. Therrien. </author> <title> Discrete Random Signals and Statistical Signal Processing. </title> <publisher> Prentice-Hall, </publisher> <address> En-glewood Cliffs, NJ, </address> <year> 1992. </year>
Reference-contexts: For each trial, a single point, x, was excluded from the data set and a Fisher projection matrix, W was calculated for remaining members of the set. The excluded point was then projected using W and classified using quadratic and linear classifiers, in the standard method described by Therrien <ref> [8] </ref>.
Reference: [9] <author> K. R. Scherer. Ch. </author> <title> 10: Speech and emotional states. </title> <editor> In J. K. Darby, editor, </editor> <booktitle> Speech Evaluation in Psychiatry, </booktitle> <pages> pages 189-220. </pages> <publisher> Grune and Strat-ton, Inc., </publisher> <year> 1981. </year>
Reference-contexts: In studies of affect in speech, in absence of context clues, people were able to identify affective states with only about a 60% recognition rate <ref> [9] </ref> and approval or disapproval with a rate of 65-85% [10].
Reference: [10] <author> D. Roy and A. Pentland. </author> <title> Automatic spoken affect analysis and classification. </title> <booktitle> In Proceedings of the Second International Conf. on Automatic Face and Gesture Recognition, </booktitle> <pages> pages 363-367, </pages> <address> Killing-ton, VT, </address> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: In studies of affect in speech, in absence of context clues, people were able to identify affective states with only about a 60% recognition rate [9] and approval or disapproval with a rate of 65-85% <ref> [10] </ref>.
Reference: [11] <author> Y. Yacoob and L. S. Davis. </author> <title> Recognizing human facial expressions from log image sequences using optical flow. </title> <journal> IEEE T. Patt. Analy. and Mach. Intell., </journal> <volume> 18(6) </volume> <pages> 636-642, </pages> <month> June </month> <year> 1996. </year>
Reference-contexts: Anger was most easily distinguished, and good discrimination was achieved for the high vs. low arousal states. Positive and negative valence were not well separated. of 80% - 98% on small sets of emotions <ref> [11] </ref>, [12], with the best performance on deliberately expressed (mildly exaggerated) emotions. Processing physiological signals offers yet another avenue to communicating affective state to a computer.
Reference: [12] <author> I. A. Essa and A. Pentland. </author> <title> Facial expression recognition using a dynamic model and motion energy. </title> <booktitle> In IEEE Int. Conf. on Comp. Vis., </booktitle> <pages> pages 360-367, </pages> <address> Cambridge, MA, </address> <year> 1995. </year> <journal> IEEE Computer Society. </journal> <volume> 5 </volume>
Reference-contexts: Anger was most easily distinguished, and good discrimination was achieved for the high vs. low arousal states. Positive and negative valence were not well separated. of 80% - 98% on small sets of emotions [11], <ref> [12] </ref>, with the best performance on deliberately expressed (mildly exaggerated) emotions. Processing physiological signals offers yet another avenue to communicating affective state to a computer.
References-found: 12

