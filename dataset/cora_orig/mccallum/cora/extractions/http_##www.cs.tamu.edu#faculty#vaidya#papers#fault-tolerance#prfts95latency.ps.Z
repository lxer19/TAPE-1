URL: http://www.cs.tamu.edu/faculty/vaidya/papers/fault-tolerance/prfts95latency.ps.Z
Refering-URL: http://www.cs.tamu.edu/faculty/vaidya/Vaidya-ftc.html
Root-URL: http://www.cs.tamu.edu
Email: E-mail: vaidya@cs.tamu.edu  
Title: On Checkpoint Latency  
Author: Nitin H. Vaidya 
Web: Web: http://www.cs.tamu.edu/faculty/vaidya/  
Address: College Station, TX 77843-3112  
Affiliation: Department of Computer Science Texas A&M University  
Abstract: Checkpointing and rollback is a technique for minimizing loss of computation in presence of failures. Two metrics can be used to characterize a checkpoint-ing scheme: (i) checkpoint overhead (increase in the execution time of the application because of a checkpoint), and (ii) checkpoint latency (duration of time required to save the checkpoint). For many checkpoint-ing methods, checkpoint latency is larger than checkpoint overhead. This paper evaluates the expression for "average overhead" of the checkpointing scheme as a function of checkpoint latency and overhead. It is shown that the "average overhead" is much more sensitive to the changes in checkpoint overhead, as compared to checkpoint latency. Also, for equi-distant checkpoints, the optimal checkpoint interval is shown to be independent of the checkpoint latency. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. M. Chandy, J. C. Browne, C. W. Dissly, and W. R. Uhrig, </author> <title> "Analytic models for rollback and recovery strategies in data base systems," </title> <journal> IEEE Trans. Softw. Eng., </journal> <volume> vol. 1, </volume> <month> March </month> <year> 1975. </year>
Reference-contexts: Such applications can encounter loss of a significant amount of computation if a failure occurs during the execution. Check-pointing and rollback recovery is a technique used to minimize the loss of computation in an environment subject to failures <ref> [1] </ref>. A checkpoint is a copy of the application's state stored on a stable storage a stable storage is not subject to failures. The application periodically saves checkpoints; the application recovers from a failure by rolling back to a recent checkpoint. <p> In many imple fl This work is supported in part by the National Science Foundation under CAREER Grant MIP-9502563. mentations, checkpoint latency is larger than the checkpoint overhead. (Illustrated in Section 2.) In the past, a large number of researchers have analyzed the checkpointing and rollback recovery scheme (e.g. <ref> [1, 2, 3, 11] </ref>). However, to our knowledge, the past work has not taken checkpoint latency into account. This paper evaluates the impact of checkpoint latency on the performance of a checkpointing scheme. <p> The above discussion is also applicable if the failure occurs during the checkpoint latency period of checkpoint CP2. Such a failure will also require a rollback to checkpoint CP1, as checkpoint CP2 is not established when the failure occurred. 1 The above discussion can 1 Chandy et al. <ref> [1] </ref> present an analysis of checkpointing schemes that does not take checkpoint latency into account. However, for sequential checkpointing (with L = C), our analysis is similar to theirs with one exception. An assumption made by Chandy et al. [1] implies that a failure that occurs while checkpoint CP2 (in Figure <p> failure occurred. 1 The above discussion can 1 Chandy et al. <ref> [1] </ref> present an analysis of checkpointing schemes that does not take checkpoint latency into account. However, for sequential checkpointing (with L = C), our analysis is similar to theirs with one exception. An assumption made by Chandy et al. [1] implies that a failure that occurs while checkpoint CP2 (in Figure 5) is being saved, only requires re-initiation of the checkpointing operation.
Reference: [2] <author> E. Gelenbe, </author> <title> "On the optimum checkpointing interval," </title> <journal> J. ACM, </journal> <volume> vol. 2, </volume> <pages> pp. 259-270, </pages> <month> April </month> <year> 1979. </year>
Reference-contexts: In many imple fl This work is supported in part by the National Science Foundation under CAREER Grant MIP-9502563. mentations, checkpoint latency is larger than the checkpoint overhead. (Illustrated in Section 2.) In the past, a large number of researchers have analyzed the checkpointing and rollback recovery scheme (e.g. <ref> [1, 2, 3, 11] </ref>). However, to our knowledge, the past work has not taken checkpoint latency into account. This paper evaluates the impact of checkpoint latency on the performance of a checkpointing scheme.
Reference: [3] <author> P. L'Ecuyer and J. Malenfant, </author> <title> "Computing optimal checkpointing strategies for rollback and recovery systems," </title> <journal> IEEE Trans. Computers, </journal> <volume> vol. 37, </volume> <pages> pp. 491-496, </pages> <month> April </month> <year> 1988. </year>
Reference-contexts: In many imple fl This work is supported in part by the National Science Foundation under CAREER Grant MIP-9502563. mentations, checkpoint latency is larger than the checkpoint overhead. (Illustrated in Section 2.) In the past, a large number of researchers have analyzed the checkpointing and rollback recovery scheme (e.g. <ref> [1, 2, 3, 11] </ref>). However, to our knowledge, the past work has not taken checkpoint latency into account. This paper evaluates the impact of checkpoint latency on the performance of a checkpointing scheme.
Reference: [4] <author> K. Li, J. F. Naughton, and J. S. Plank, </author> <title> "Low-latency, concurrent checkpointing for parallel programs," </title> <journal> IEEE Trans. Par. Distr. Syst., </journal> <volume> vol. 5, </volume> <pages> pp. 874-879, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: This paper evaluates the impact of checkpoint latency on the performance of a checkpointing scheme. This work is motivated by the schemes that attempt to reduce checkpoint overhead while causing an increase in the checkpoint latency (e.g., <ref> [5, 4, 8] </ref>). Related work: Plank et al. [5, 4] present measurements of checkpoint latency and overhead for a few applications, however, they do not present any performance analysis. <p> This paper evaluates the impact of checkpoint latency on the performance of a checkpointing scheme. This work is motivated by the schemes that attempt to reduce checkpoint overhead while causing an increase in the checkpoint latency (e.g., [5, 4, 8]). Related work: Plank et al. <ref> [5, 4] </ref> present measurements of checkpoint latency and overhead for a few applications, however, they do not present any performance analysis. <p> The "measured L" curve in Figure 9 plots checkpoint overhead and latency measured for a merge sort program using four different checkpointing schemes - the data is borrowed from Li et al. <ref> [4] </ref>. (Although the data in [4] corresponds to a parallel implementation on a shared memory machine, our analysis is applicable to this implementation.) One of the four schemes in the "measured L" curve is sequential checkpointing with overhead C max = 31 seconds. For comparison, . <p> The "measured L" curve in Figure 9 plots checkpoint overhead and latency measured for a merge sort program using four different checkpointing schemes - the data is borrowed from Li et al. <ref> [4] </ref>. (Although the data in [4] corresponds to a parallel implementation on a shared memory machine, our analysis is applicable to this implementation.) One of the four schemes in the "measured L" curve is sequential checkpointing with overhead C max = 31 seconds. For comparison, .
Reference: [5] <author> J. S. Plank, M. Beck, G. Kingsley, and K. Li, "Libckpt: </author> <title> Transparent checkpointing under Unix," </title> <booktitle> in Usenix Winter 1995 Technical Conference, </booktitle> <address> New Orleans, </address> <month> January </month> <year> 1995. </year>
Reference-contexts: This paper evaluates the impact of checkpoint latency on the performance of a checkpointing scheme. This work is motivated by the schemes that attempt to reduce checkpoint overhead while causing an increase in the checkpoint latency (e.g., <ref> [5, 4, 8] </ref>). Related work: Plank et al. [5, 4] present measurements of checkpoint latency and overhead for a few applications, however, they do not present any performance analysis. <p> This paper evaluates the impact of checkpoint latency on the performance of a checkpointing scheme. This work is motivated by the schemes that attempt to reduce checkpoint overhead while causing an increase in the checkpoint latency (e.g., [5, 4, 8]). Related work: Plank et al. <ref> [5, 4] </ref> present measurements of checkpoint latency and overhead for a few applications, however, they do not present any performance analysis. <p> Sequential checkpointing is an approach for which checkpoint overhead is identical to checkpoint latency. In this approach, when an application process wants to take a checkpoint, it pauses and saves its state on the stable storage <ref> [5] </ref>. Therefore, the time required to save the checkpoint (i.e., checkpoint latency) is practically identical to the increase in the execution time of the process (i.e., checkpoint overhead). Figure 1 illustrates this approach. The horizontal line represents processor execution, time increasing from left to right. <p> However, it results in a larger checkpoint overhead as compared to other approaches. Forked checkpointing is an approach for which checkpoint overhead is usually much smaller than the checkpoint latency. In this approach, when a process wants to take a checkpoint, it forks a child process <ref> [5] </ref>. The state of the child process is identical to that of the parent process when fork is performed. After the fork, the parent process continues computation, while the child process saves its state on the stable storage. Figure 2 (a) illustrates this approach.
Reference: [6] <author> K. S. Trivedi, </author> <title> Probability and Statistics with Reliability, Queueing and Computer Science Applications. </title> <publisher> Prentice-Hall, </publisher> <year> 1988. </year>
Reference-contexts: Let denote the expected (average) execution time of an interval. Then, it is easy to see that, overhead ratio r = lim t!1 t 1 Expected execution time of a single checkpoint interval can be evaluated using the Markov chain <ref> [6, 12] </ref> in Figure 7. State 0 is the initial state, when an interval starts execution. A transition from state 0 to state 1 occurs when the interval is completed without a failure.
Reference: [7] <author> N. H. Vaidya, </author> <title> "Another two-level failure recovery scheme: Performance impact of checkpoint placement and checkpoint latency," </title> <type> Tech. Rep. 94-068, </type> <institution> Computer Science Dept., Texas A&M University, College Station, </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: We measured checkpoint latency and overhead for a few uni-process applications, and briefly analyzed the impact of checkpoint latency on performance of "two-level" recovery schemes <ref> [7, 9] </ref>. 2 Checkpoint Latency We limit the discussion to uni-process applications. Due to lack of space, multi-process applications are not discussed here [10]. In this section, we illustrate the distinction between checkpoint latency and checkpoint overhead with two examples.
Reference: [8] <author> N. H. Vaidya, </author> <title> "Consistent logical checkpointing," </title> <type> Tech. Rep. 94-051, </type> <institution> Computer Science Department, Texas A&M University, College Station, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: This paper evaluates the impact of checkpoint latency on the performance of a checkpointing scheme. This work is motivated by the schemes that attempt to reduce checkpoint overhead while causing an increase in the checkpoint latency (e.g., <ref> [5, 4, 8] </ref>). Related work: Plank et al. [5, 4] present measurements of checkpoint latency and overhead for a few applications, however, they do not present any performance analysis.
Reference: [9] <author> N. H. Vaidya, </author> <title> "A case for two-level distributed recovery schemes," </title> <booktitle> in ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <month> May </month> <year> 1995. </year>
Reference-contexts: We measured checkpoint latency and overhead for a few uni-process applications, and briefly analyzed the impact of checkpoint latency on performance of "two-level" recovery schemes <ref> [7, 9] </ref>. 2 Checkpoint Latency We limit the discussion to uni-process applications. Due to lack of space, multi-process applications are not discussed here [10]. In this section, we illustrate the distinction between checkpoint latency and checkpoint overhead with two examples.
Reference: [10] <author> N. H. Vaidya, </author> <title> "On checkpoint latency," </title> <type> Tech. Rep. 95-015, </type> <institution> Computer Science Department, Texas A&M University, College Station, </institution> <month> March </month> <year> 1995. </year>
Reference-contexts: We measured checkpoint latency and overhead for a few uni-process applications, and briefly analyzed the impact of checkpoint latency on performance of "two-level" recovery schemes [7, 9]. 2 Checkpoint Latency We limit the discussion to uni-process applications. Due to lack of space, multi-process applications are not discussed here <ref> [10] </ref>. In this section, we illustrate the distinction between checkpoint latency and checkpoint overhead with two examples. Sequential checkpointing is an approach for which checkpoint overhead is identical to checkpoint latency. <p> It follows that, = P 01 K 01 + P 02 K 02 + 1 P 22 Substituting the expressions for various costs and transition probabilities, and simplifying, the following expression is obtained <ref> [10] </ref>. = 1 e (LC+R) (e (T+C) 1) (1) It follows that, the overhead ratio r is given by r = T 1 e (LC+R) (e (T +C) 1) 1 (2) 4.1 Minimizing the Overhead Ratio Consider a checkpointing scheme that achieves a certain overhead C and checkpoint latency L. <p> More precisely, the objective is to determine a function g of C such that, for any C &lt; C max , the overhead ratio r is smaller than the sequential check pointing scheme if L &lt; g (C). Derivation of function g (C) is omitted here <ref> [10] </ref>. It can be shown that, g (C) = C + 1 ln 1 T m where T c is the value of T that satisfies Equation 3, and T m is the solution of Equation 3 with C = C max . 2 . <p> The paper considers only uni-process applications; the results can potentially be extended to multi-process applications as well <ref> [10] </ref>.
Reference: [11] <author> J. W. Young, </author> <title> "A first order approximation to the optimum checkpoint interval," </title> <journal> Comm. ACM, </journal> <volume> vol. 17, </volume> <pages> pp. 530-531, </pages> <month> September </month> <year> 1974. </year>
Reference-contexts: In many imple fl This work is supported in part by the National Science Foundation under CAREER Grant MIP-9502563. mentations, checkpoint latency is larger than the checkpoint overhead. (Illustrated in Section 2.) In the past, a large number of researchers have analyzed the checkpointing and rollback recovery scheme (e.g. <ref> [1, 2, 3, 11] </ref>). However, to our knowledge, the past work has not taken checkpoint latency into account. This paper evaluates the impact of checkpoint latency on the performance of a checkpointing scheme. <p> This indicates that, the checkpointing 2 T c is approximately p 2 fl C= when C &lt;< 1= (similarly, T m 2 fl C max =). Young <ref> [11] </ref> previously obtained this ex pression by a somewhat different analysis and g (C) for = 10 4 ; 10 6 ; 10 8 /sec techniques used in practice can achieve a significantly smaller overhead ratio as compared to the sequential checkpointing scheme. 6 Conclusions This paper evaluates an expression for
Reference: [12] <author> A. Ziv and J. Bruck, </author> <title> "Analysis of checkpointing schemes for multiprocessor systems," </title> <type> Tech. Rep. RJ 9593, </type> <institution> IBM Almaden Res. Center, </institution> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: Let denote the expected (average) execution time of an interval. Then, it is easy to see that, overhead ratio r = lim t!1 t 1 Expected execution time of a single checkpoint interval can be evaluated using the Markov chain <ref> [6, 12] </ref> in Figure 7. State 0 is the initial state, when an interval starts execution. A transition from state 0 to state 1 occurs when the interval is completed without a failure.
References-found: 12

