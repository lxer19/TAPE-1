URL: http://www.cs.wisc.edu/~zhang/paper.ps
Refering-URL: http://www.cs.wisc.edu/~zhang/
Root-URL: 
Email: fmiron, raghu, zhangg@cs.wisc.edu  
Phone: Fax: 608-262-9777 Ph: 608-262-1204  
Title: Fast Density and Probability Estimation Using CF-Kernel Method for Very Large Databases  
Author: Miron Livny, Raghu Ramakrishnan, Tian Zhang 
Date: July 25, 1996  
Address: Madison, WI 53706, U.S.A.  
Affiliation: Computer Sciences Department, University of Wisconsin-Madison,  
Abstract: Density and probability estimations, as discussed in this paper, are the constructions of estimates of the density function and the probability function from the observed data. 1 It is the natural and valuable way of exploring and presenting the data either for determining specific patterns, or for requiring further analysis. In database area, for a specific example, density and probability estimations help make accurate estimation of query result sizes possible. Prior work in density and probability estimations [Sil86, Dev87, WJ95], has been in Statistics, and does not adequately address the problems of very large datasets wrt. limited amount of resources (i.e., memory and running time), as well as how to minimize I/O costs. This paper presents the CF-kernel method that estimates density and probability functions for any large datasets in the "best-efforts under the given resources" manner. By integrating the advantages of both the kernel method [Sil86, Dev87, WJ95] and the dynamical and incremental CF tree structure [ZRL96a, ZRL96b], the CF-kernel method improves time/space efficiency dramatically, while maintaining the estimation accuracy as close to the kernel method as the available memory allows. It only scans the dataset once, and requires no prior knowledge about the dataset. Anytime during the scanning, it can be interrupted with the current estimations available for use, and then it can be resumed incrementally from where it has stopped. Our theoretical proof, as well as experimental results of applying the CF-kernel method to various datasets show that it is much more time/space efficient compared with the kernel method whereas its estimation accuracy is comparable with, and can be arbitrarily close to that of the kernel method as the memory increases. 
Abstract-found: 1
Intro-found: 1
Reference: [AC94] <author> Arie Segev, and Abhirup Chatterjee, </author> <title> Supporting Statistics In Extensible Databases: A Case Study, Scientific and Statistical Database Management, </title> <year> 1994. </year>
Reference: [Dev87] <author> Luc Devroye, </author> <title> A Course in Density Estimation, </title> <publisher> Birkhauser Boston, </publisher> <year> 1987. </year>
Reference-contexts: So it is generally more useful in reality. The oldest and most widely-used nonparametric method for density and probability estimations is the histogram method <ref> [Sil86, Dev87, WJ95] </ref>. <p> Apart from the histogram method, the kernel method <ref> [Sil86, Dev87, WJ95] </ref> is the most mathematically studied and commonly-used nonparametric density and probability estimation method.
Reference: [ST87] <author> David W. Scott, and George R. Terrell, </author> <title> Biased and Unbiased Cross-Validation in Density Estimation, </title> <journal> Journal of the American Statistical Association, </journal> <volume> Vol. 82, No. 400, p1131-1146, </volume> <month> Dec., </month> <year> 1987. </year>
Reference: [Sil86] <author> B. W. Silverman, </author> <title> Density Estimation for Statistics and Data Analysis, </title> <publisher> Chapman and Hall, </publisher> <year> 1986. </year>
Reference-contexts: So it is generally more useful in reality. The oldest and most widely-used nonparametric method for density and probability estimations is the histogram method <ref> [Sil86, Dev87, WJ95] </ref>. <p> Apart from the histogram method, the kernel method <ref> [Sil86, Dev87, WJ95] </ref> is the most mathematically studied and commonly-used nonparametric density and probability estimation method. <p> Finally the conclusions are presented in Section 6. 2 Kernel Method The kernel function K (x) is usually a unimodal, symmetric and bounded density function. That is R R R x 2 K (x)dx = K 2 &lt; 1. In this paper, as generally agreed in Statistics <ref> [Sil86] </ref>, we choose the standard normal density function as K (x), that is: K (x) = p e x 2 3 Let K h (x X i ) be the notation representing a new density function transformed from K (x) by moving the mean to X i , and by multipling <p> ( ^ f ; f ) = E ( ^ f (x) f (x)) 2 dx: (6) With the following two assumptions, the above M ISE ( ^ f ; f ) can be derived into a more com prehensible formula in terms of h and n via Taylor's expansions <ref> [Sil86] </ref>. * If f 00 (x) exists, and is continuous, square integrable 2 and ultimately monotone 3 , and * If the smoothing parameter h is a variable depending only on n, and lim n!1 h = 0, lim n!1 nh = 1 4 .
Reference: [WJ95] <author> M.P. Wand, and M.C. Jones, </author> <title> Kernel Smoothing, </title> <publisher> Chapman and Hall, </publisher> <year> 1995. </year>
Reference-contexts: So it is generally more useful in reality. The oldest and most widely-used nonparametric method for density and probability estimations is the histogram method <ref> [Sil86, Dev87, WJ95] </ref>. <p> Apart from the histogram method, the kernel method <ref> [Sil86, Dev87, WJ95] </ref> is the most mathematically studied and commonly-used nonparametric density and probability estimation method. <p> However it has been shown <ref> [WJ95] </ref> that the choice of kernel function K (x) is not particularly important, and the accuracy is primarily affected by the smoothing parameter h. <p> But the real underlying density function f (x) is unknown, let alone f 00 (x), and R (f 00 ). The plug in validation method of using R ( ^ f 00 ) as R (f 00 ) is suggested <ref> [WJ95] </ref>. <p> First for any f (x) with standard deviation , the following upper bound on h fl AMISE has been proved <ref> [WJ95] </ref>: h fl AMISE [ 243R (K) ] 5 (11) The above upper bound motivations the oversmoothed h, or h OS , which is defined as below: ^ h OS = [ 35K 2 n 1 where ^ is the sample standard deviation. <p> But it differs from the existing binned kernel methods <ref> [WJ95] </ref> in that: * it does not require the data range in advance for allocating bins; * given the amount of memory, its "bins" are not allocated statically, but formed dynamically and incrementally according to the data distribution and the available memory; * theoretically and empirically, we can show that its
Reference: [ZRL96a] <author> Tian Zhang, Raghu Ramakrishnan, and Miron Livny, </author> <title> BIRCH: An Efficient Data Clustering Method for Very Large Databases, </title> <booktitle> Proc. of ACM SIGMOD Int'l Conf. on Management of Data, </booktitle> <address> p103-114, June 1996, Montreal, Canada. </address>
Reference-contexts: In this paper, first we summarize the dataset into an in-memory CF tree <ref> [ZRL96a, ZRL96b] </ref>, second instead of placing a kernel function on each single data point, we place a CK-kernel function on each subcluster (or leaf entry of the CF tree), third we use the sum of all the CF-kernel functions to estimate the overall data distribution. <p> to fit in memory, which is very likely in large databases, there will be large amount of I/O costs involved in the above computations. 6 3 CF Tree The concepts of Clustering Feature (CF) and CF tree , and relevant insertion, threshold increasing and rebuilding algorithms are first introduced in <ref> [ZRL96a] </ref>. Here we provide a brief review of them. CF is a triple summarizing the information that we maintain about a cluster. <p> When memory is run out before data is consumed, by increasing the threshold value, a more compact tree can be rebuilt from the existing tree. The new tree will continue to consume more data. Please refer to <ref> [ZRL96a] </ref> for details of the insertion, threshold increasing and rebuilding algorithms. 4 CF-kernel Method The CF-kernel method improves the time/space efficiency of the kernel method, and reduces I/O costs to a minimum by: * dynamically and incrementally binning data with an in-memory CF tree; * instead of placing a kernel function <p> x i 2 2 )]: (35) 5 Performance Studies Since the time/space complexity reduction from O (n) in the kernel method to O (m) in the CF-kernel method has been shown very clearly in previous sections, and the scalability and stability of inserting data into CF-tree has been studied in <ref> [ZRL96a] </ref>, in this section, we will concentrate on studying: * the effects of different distribution assumptions within subcluster, * accuracy comparison of the kernel method and the CF-kernel method, * the effects of varying memory sizes, or more directly, the effects of varying m's. 5.1 Synthetic Data Generation To compare the <p> the second derivative of f 4 (x) for DS4 does not exist, we can not calculate its real h fl AMISE . 5.2 Default Settings The parameter settings for scanning the data points and inserting them into the CF tree is similar to those used in Phase 1 of BIRCH <ref> [ZRL96a] </ref> except that the outlier handling option is turned off for fair comparisons with the kernel method. Among them, the memory size is set to 40 kbytes, which is about 5of the dataset size.
Reference: [ZRL96b] <author> Tian Zhang, Raghu Ramakrishnan, and Miron Livny, </author> <title> Interactive Classification of Very Large Datasets with BIRCH, </title> <booktitle> Proc. of Workshop on Research Issues on Data Mining and Knowledge Discovery (in cooperation with ACM-SIGMOD'96), </booktitle> <address> June 196, Montreal, Canada. </address> <month> 22 </month>
Reference-contexts: In this paper, first we summarize the dataset into an in-memory CF tree <ref> [ZRL96a, ZRL96b] </ref>, second instead of placing a kernel function on each single data point, we place a CK-kernel function on each subcluster (or leaf entry of the CF tree), third we use the sum of all the CF-kernel functions to estimate the overall data distribution.
References-found: 7

