URL: http://www-db.stanford.edu/pub/tomasic/1993/sigmod.93.ps
Refering-URL: http://karna.cs.umd.edu:3264/people/godfrey/cites.html
Root-URL: 
Email: e-mail: tomasic@cs.stanford.edu hector@cs.stanford.edu  
Title: Caching and Database Scaling in Distributed Shared-Nothing Information Retrieval Systems  
Author: Anthony Tomasic and Hector Garcia-Molina 
Address: Margaret Jacks Hall, Stanford, CA 94305-2140  
Affiliation: Stanford University Department of Computer Science  
Note: Published in Proceedings of SIGMOD '93  
Abstract: A common class of existing information retrieval system provides access to abstracts. For example Stanford University, through its FOLIO system, provides access to the INSPEC database of abstracts of the literature on physics, computer science, electrical engineering, etc. In this paper this database is studied by using a trace-driven simulation. We focus on physical index design, inverted index caching, and database scaling in a distributed shared-nothing system. All three issues are shown to have a strong effect on response time and throughput. Database scaling is explored in two ways. One way assumes an "optimal" configuration for a single host and then linearly scales the database by duplicating the host architecture as needed. The second way determines the optimal number of hosts given a fixed database size. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> F. J. Burkowski. </author> <title> Retrieval performance of a distributed text database utilizing a parallel processor document server. </title> <booktitle> In Proceedings of the Second International Symposium on Databases in Parallel and Distributed Systems, </booktitle> <pages> pages 71-79, </pages> <address> Dublin, Ireland, </address> <year> 1990. </year>
Reference-contexts: For an overview of implementation techniques see [7]. Not that much has been done on distributing inverted lists and searching them in parallel. Reference [12] discusses some of the basic issues. Burkowski simulates a shared-nothing information retrieval system <ref> [1] </ref> to study the performance impact of the placement of documents and inverted indexes. Jeong and Omiecinski [10] independently study for a shared-everything architecture similar issues of the physical index design as in this paper.
Reference: [2] <author> A. L. Chervenak. </author> <title> Performance measurements of the first raid prototype. </title> <type> Technical Report UCB/UCD 90/574, </type> <institution> University of California, Berkley, </institution> <month> May </month> <year> 1990. </year>
Reference-contexts: A configuration is the total collection of variable-value pairs used in an experiment. The base configuration is the collection of variable-value pairs given in the tables in this section. Table 4 shows the base configuration variables for the hardware. The values for this table were taken from <ref> [2] </ref>.
Reference: [3] <author> J. K. Cringean, R. England, G. A. Manson, and P. Willett. </author> <title> Parallel text searching in serial files using a processor farm. </title> <booktitle> In SIGIR 1990, </booktitle> <pages> pages 429-453, </pages> <year> 1990. </year>
Reference-contexts: Jeong and Omiecinski [10] independently study for a shared-everything architecture similar issues of the physical index design as in this paper. Work has also been done on searching with a variety of other architectures, e.g., processor farms <ref> [3] </ref>, fine-grained parallelism [11], and shared-memory multiprocessors [4]. As mentioned earlier, our work is a continuation of an earlier paper [14]. The four index organization we have described are from that earlier paper.
Reference: [4] <author> S. DeFazio and J. Hull. </author> <title> Toward servicing textual database transactions on symmetric shared memory multiprocessors. </title> <booktitle> In Proceedings of the International Workshop on High Performance Transaction Systems, Asilomar, </booktitle> <year> 1991. </year>
Reference-contexts: Jeong and Omiecinski [10] independently study for a shared-everything architecture similar issues of the physical index design as in this paper. Work has also been done on searching with a variety of other architectures, e.g., processor farms [3], fine-grained parallelism [11], and shared-memory multiprocessors <ref> [4] </ref>. As mentioned earlier, our work is a continuation of an earlier paper [14]. The four index organization we have described are from that earlier paper.
Reference: [5] <author> P. A. Emrath. </author> <title> Page Indexing for Textual Information Retrieval Systems. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> October </month> <year> 1983. </year>
Reference-contexts: As we will see, the fact that inverted lists are shorter for abstracts dramatically changes the relative performance of the various organizations. Emrath <ref> [5] </ref> focuses on the performance trade-offs involved in partial or complete indexing. * Here we drive our study with real traces from the Stanford library system. Furthermore, the traces also give the result sizes, so we can use that in our simulation. <p> For the system index organizations, we believe that this model is reasonably accurate for a small number of query terms (it is exact for single keyword subqueries). (Emrath <ref> [5] </ref> reports some measurements which support this model.) However, as the number of keywords in the query increases, the expected number of answers approaches zero.
Reference: [6] <author> C. Faloutsos. </author> <title> Access methods for text. </title> <journal> ACM Computing Surveys, </journal> <volume> 17 </volume> <pages> 50-74, </pages> <year> 1985. </year>
Reference-contexts: The study reported here also represents the first time (to our knowledge) that an actual user query trace drives the evaluation of a distributed architecture. An abstracts database typically uses an inverted index to speed up query processing (see <ref> [6] </ref> for a survey of access methods for text). For each word, an inverted list is constructed that gives all the abstracts in which the word appears. In a multiprocessor environment, the inverted lists can be distributed in various ways: Disk Organization.
Reference: [7] <author> W. B. Frakes and R. Baeza-Yates. </author> <title> Information Retrieval: Data Structures and Algorithms. </title> <publisher> Prentice-Hall, </publisher> <year> 1992. </year>
Reference-contexts: The hardware model is presented in Section 4. Our results are given in Section 5 and conclusions in Section 6. 1.1 Previous Work A substantial amount of work has been done in the general area of Information Retrieval. For an overview of implementation techniques see <ref> [7] </ref>. Not that much has been done on distributing inverted lists and searching them in parallel. Reference [12] discusses some of the basic issues. Burkowski simulates a shared-nothing information retrieval system [1] to study the performance impact of the placement of documents and inverted indexes.
Reference: [8] <author> J. Gray and A. Reuter. </author> <title> Transaction Processing: Concepts and Techniques. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: In other words, one has to take the throughput rates we report here, and divide them by the dollar cost of each configuration, to obtain a query/sec/dollar measure, as is done in transaction processing systems <ref> [8] </ref>. Our caching results indicate that a relatively small cache can improve performance significantly.
Reference: [9] <author> R. Jain. </author> <title> The Art of Computer Systems Performance Analysis. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: Value a is 512 B and b is 16 KB. function along a single dimension. In some cases it is necessary to change the value of multiple variables in a systematic fashion in a 2 k factor experiment <ref> [9] </ref>. For three variables, this can intuitively be viewed as examining the values of a function at the corners of a three-dimensional cube (each axis of the cube corresponds to a variable).
Reference: [10] <author> B.-S. Jeong and E. Omiecinski. </author> <title> Inverted file partitioning schemes for a shared-everything multiprocessor. </title> <type> Technical Report GIT-CC-92/39, </type> <institution> Georgia Institute of Technology, College of Computing, </institution> <year> 1992. </year>
Reference-contexts: Not that much has been done on distributing inverted lists and searching them in parallel. Reference [12] discusses some of the basic issues. Burkowski simulates a shared-nothing information retrieval system [1] to study the performance impact of the placement of documents and inverted indexes. Jeong and Omiecinski <ref> [10] </ref> independently study for a shared-everything architecture similar issues of the physical index design as in this paper. Work has also been done on searching with a variety of other architectures, e.g., processor farms [3], fine-grained parallelism [11], and shared-memory multiprocessors [4].
Reference: [11] <author> C. Stanfill. </author> <title> Partitioned posting files: A parallel inverted file structure for information retrieval. </title> <booktitle> In ACM Special Interest Group on Information Retrieval (SIGIR), </booktitle> <year> 1990. </year>
Reference-contexts: Jeong and Omiecinski [10] independently study for a shared-everything architecture similar issues of the physical index design as in this paper. Work has also been done on searching with a variety of other architectures, e.g., processor farms [3], fine-grained parallelism <ref> [11] </ref>, and shared-memory multiprocessors [4]. As mentioned earlier, our work is a continuation of an earlier paper [14]. The four index organization we have described are from that earlier paper.
Reference: [12] <author> H. S. Stone. </author> <title> Parallel querying of large databases: A case study. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 11-21, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: For an overview of implementation techniques see [7]. Not that much has been done on distributing inverted lists and searching them in parallel. Reference <ref> [12] </ref> discusses some of the basic issues. Burkowski simulates a shared-nothing information retrieval system [1] to study the performance impact of the placement of documents and inverted indexes. Jeong and Omiecinski [10] independently study for a shared-everything architecture similar issues of the physical index design as in this paper.
Reference: [13] <author> A. Tomasic and H. Garcia-Molina. </author> <title> Caching and database scaling in distributed shared-nothing information retrieval systems. </title> <type> Technical Report STAN-CS-92-1456, </type> <institution> Stanford University, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: The number of bytes per document is approximately 1,800. The total database size can be (very roughly) estimated at 2 Gigabytes. For more detail on this database, see <ref> [13] </ref>. To drive our simulation, we combine the information from the trace and postings files into a single trace file that is easy to use. Figure 3 shows a sample of this final trace file. <p> For our INSPEC database that has an index size of 308MB (129 million postings compressed) a cache of about 3.8 MB (800,000 postings uncompressed), on the order of 1.2% of the index, can improve throughput by about 136% for the prefetch strategy <ref> [13] </ref> . For the other strategies, improvements are smaller. Although not reported here, we also experimented with various cache policies.
Reference: [14] <author> A. Tomasic and H. Garcia-Molina. </author> <title> Performance of inverted indices in shared-nothing distributed text document information retrieval systems. </title> <booktitle> In Proceedings of the Second International Conference On Parallel and Distributed Information Systems, </booktitle> <address> San Diego, </address> <year> 1993. </year> <month> 10 </month>
Reference-contexts: To handle the increased load, a distributed architecture can be used, dispersing the data and index structures across several computers and performing searches in parallel. This paper studies the performance trade-offs in such a shared-nothing distributed system. Our work complements an earlier paper <ref> [14] </ref> where a full-text information retrieval system was studied (entire document is indexed, as opposed to just its fl This research was partially supported by the Defense Advanced Research Projects Agency of the Department of Defense under Contract No. DABT63-91-C-0025. id: 0 author: A. <p> Work has also been done on searching with a variety of other architectures, e.g., processor farms [3], fine-grained parallelism [11], and shared-memory multiprocessors [4]. As mentioned earlier, our work is a continuation of an earlier paper <ref> [14] </ref>. The four index organization we have described are from that earlier paper. There are, however, two important differences between the earlier study and this one. * Here we study an abstracts database as opposed to a full-text system. In a full-text system, every single word occurrence is indexed. <p> Emrath [5] focuses on the performance trade-offs involved in partial or complete indexing. * Here we drive our study with real traces from the Stanford library system. Furthermore, the traces also give the result sizes, so we can use that in our simulation. In <ref> [14] </ref> we modeled queries probabilistically, assuming query terms were picked at random from a vocabulary. Clearly, using traces (which incidentally are hard to get from commercial information vendors) yields more realistic results. <p> For queries with multiple search terms (e.g., the one by user 26), each term is listed, together with the number of postings as described above. 3 Query Processing As discussed in the introduction, four physical index organizations are considered. We found in previous work <ref> [14] </ref> that the LAN may be the bottleneck for the system index organization. To ameliorate this problem we adopt one query processing optimization named "prefetch I" that operates as follows: we divide the processing for a query into two phases. <p> This significantly reduces the data volume on the LAN by reducing the mean subquery answer size. (In <ref> [14] </ref> two other prefetch variations are studied. For our current study, we evaluated all three variations; Prefetch I was the variation with the best performance, so to economize on space, we only describe the winning variation and its performance.) To simulate the processing of a query, we consider five stages. <p> However, the system organization does utilize the 9 cache grows in size. LAN or processor interconnect more heavily, so it would not be appropriate for systems with slow networks. Our conclusion is different from that of our earlier work <ref> [14] </ref> where a full-text information retrieval system was analyzed. In that case, inverted lists are much longer, and striping them does pay off. In particular, the host organization was superior in that scenario. In our experiments, we explored the "mainframes vs. workstations" issue.
References-found: 14

