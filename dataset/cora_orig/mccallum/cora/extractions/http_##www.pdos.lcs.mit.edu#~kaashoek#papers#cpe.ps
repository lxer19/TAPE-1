URL: http://www.pdos.lcs.mit.edu/~kaashoek/papers/cpe.ps
Refering-URL: http://www.pdos.lcs.mit.edu/~kaashoek/papers.html
Root-URL: 
Email: Email: bal@cs.vu.nl  
Title: REPLICATION TECHNIQUES FOR SPEEDING UP PARALLEL APPLICATIONS ON DISTRIBUTED SYSTEMS  
Author: Henri E. Bal M. Frans Kaashoek Andrew S. Tanenbaum Jack Jansen 
Note: *This research was supported in part by the Netherlands organization for scientific research (N.W.O.) under grant 125-30-10.  
Date: 333333333333333333  
Address: 1081 HV Amsterdam The Netherlands  Kruislaan 413 1098 SJ Amsterdam The Netherlands  
Affiliation: Dept. of Mathematics and Computer Science Vrije Universiteit De Boelelaan 1081a  Centrum voor Wiskunde en Informatica  
Abstract-found: 0
Intro-found: 1
Reference: 1. <author> H.E. Bal, J.G. </author> <title> Steiner, and A.S. Tanenbaum, </title> <booktitle> ``Programming Languages for Distributed Computing Systems,'' ACM Computing Surveys 21(3), </booktitle> <pages> pp. </pages> <month> 261-322 (Sept. </month> <year> 1989). </year>
Reference-contexts: These systems are easy to build and extend, and offer a good price/performance ratio. The issue of how to program parallel applications that use many loosely-coupled machines is still open. Traditional programming methods are based on some form of message-passing <ref> [1] </ref>. More recently, methods have emerged based on sharing data. Since distributed systems lack shared memory, this sharing of data is logical, not physical. For many applications, support for shared data makes programming easier, since it allows processes on different machines to share state information.
Reference: 2. <author> K. Li, ``IVY: </author> <title> A Shared Virtual Memory System for Parallel Computing,'' </title> <booktitle> Proceedings 1988 International Conference Parallel Processing (Vol. II), </booktitle> <address> St. Charles, </address> <publisher> Ill., </publisher> <pages> pp. </pages> <month> 94-101 (Aug. </month> <year> 1988). </year>
Reference-contexts: In this paper we introduce a new model providing shared data and we discuss efficient implementation techniques for this model, based on data replication. Several systems exist that use replication for implementing shared data. Probably the best known example is Kai Li's Shared Virtual Memory (SVM) <ref> [2] </ref>. This system gives the user the illusion of a shared memory. It stores multiple read-only copies of the same page on different processors. Each processor having a copy can read the page as if it were in normal local memory.
Reference: 3. <author> M. Stumm and S. Zhou, </author> <title> ``Algorithms Implementing Distributed Shared Memory,'' </title> <booktitle> IEEE Computer 23(5), </booktitle> <pages> pp. </pages> <month> 54-64 (May </month> <year> 1990). </year>
Reference-contexts: This system gives the user the illusion of a shared memory. It stores multiple read-only copies of the same page on different processors. Each processor having a copy can read the page as if it were in normal local memory. Other systems providing replicated shared data are surveyed in <ref> [3, 4] </ref>. The model studied in this paper is called the shared data-object model. It is intended for implementing parallel applications on distributed systems. The unit of replication in our model is not dictated by the system (as in the SVM), but is determined by the programmer.
Reference: 4. <author> B. Nitzberg and V. Lo, </author> <title> ``Distributed Shared Memory: a Survey of Issues and Algorithms,'' </title> <booktitle> IEEE Computer 24(8), </booktitle> <pages> pp. </pages> <month> 52-60 (Aug. </month> <year> 1991). </year>
Reference-contexts: This system gives the user the illusion of a shared memory. It stores multiple read-only copies of the same page on different processors. Each processor having a copy can read the page as if it were in normal local memory. Other systems providing replicated shared data are surveyed in <ref> [3, 4] </ref>. The model studied in this paper is called the shared data-object model. It is intended for implementing parallel applications on distributed systems. The unit of replication in our model is not dictated by the system (as in the SVM), but is determined by the programmer.
Reference: 5. <author> K.P. Eswaran, J.N. Gray, R.A. Lorie, </author> <title> and I.L. Traiger, ``The Notions of Consistency and Predicate Locks in a Database System,'' </title> <booktitle> Communications of the ACM 19(11), </booktitle> <pages> pp. </pages> <month> 624-633 (Nov. </month> <year> 1976). </year>
Reference-contexts: In this way, the object becomes a communication channel between the processes that share it. The shared data-object model uses two important principles related to operations on objects: 1. All operations on a given object are executed atomically (i.e., indivisibly). To be more precise, the model guarantees serializability <ref> [5] </ref> of operation invocations: if two operations are executed simultaneously, then the result is as if one of them is executed before the other; the order of invocation, how ever, is nondeterministic. 2. All operations apply to single objects, so an operation invocation can modify at most one object.
Reference: 6. <author> R. Bisiani and A. Forin, </author> <title> ``Multilanguage Parallel Programming of Heterogeneous Machines,'' </title> <journal> IEEE Transactions on Computers 37(8), </journal> <pages> pp. </pages> <month> 930-945 (Aug. </month> <year> 1988). </year>
Reference-contexts: Making sequences of operations on different objects indivisible is the responsibility of the programmer. These two principles make the model easy to understand and efficient. The first principle makes our model fundamentally different from Agora <ref> [6] </ref> and the problem-oriented shared memory [7], which do not have this consistency constraint. The second principle makes the model efficient to implement, since it avoids expensive atomic transactions on multiple objects stored on different processors. In our experience thus far, the model provides sufficient support for many parallel applications. <p> It provides a higher level of abstraction and, in many cases, is more efficient. Below, we will compare our model with several related ones. Some systems provide the programmer a shared address space without guaranteeing coherency or consistency. In Agora <ref> [6] </ref> and the problem-oriented shared memory [7], for example, read operations can return stale data. Therefore, these systems do not make replication transparent to the user.
Reference: 7. <author> D.R. Cheriton, </author> <title> ``Preliminary Thoughts on Problem-oriented Shared Memory: A Decentralized Approach to Distributed Systems,'' </title> <booktitle> ACM Operating Systems Review 19(4), </booktitle> <pages> pp. </pages> <month> 26-33 (Oct. </month> <year> 1985). </year>
Reference-contexts: Making sequences of operations on different objects indivisible is the responsibility of the programmer. These two principles make the model easy to understand and efficient. The first principle makes our model fundamentally different from Agora [6] and the problem-oriented shared memory <ref> [7] </ref>, which do not have this consistency constraint. The second principle makes the model efficient to implement, since it avoids expensive atomic transactions on multiple objects stored on different processors. In our experience thus far, the model provides sufficient support for many parallel applications. <p> It provides a higher level of abstraction and, in many cases, is more efficient. Below, we will compare our model with several related ones. Some systems provide the programmer a shared address space without guaranteeing coherency or consistency. In Agora [6] and the problem-oriented shared memory <ref> [7] </ref>, for example, read operations can return stale data. Therefore, these systems do not make replication transparent to the user.
Reference: 8. <author> E. Jul, H. Levy, N. Hutchinson, and A. Black, </author> <title> ``Fine-Grained Mobility in the Emerald System,'' </title> <journal> ACM Transactions on Computer Systems 6(1), </journal> <pages> pp. </pages> <month> 109-133 (Feb. </month> <year> 1988). </year>
Reference-contexts: These issues are addressed by other models, such as atomic transactions and concurrent object-oriented programming and are not the topic of this paper. Our model also differs from the object-based models supported by Emerald <ref> [8] </ref> and Amber [9]. Objects in these languages are migrated between processors, but are not replicated. An Emerald object, for example, can be activeit may contain a processand should not be replicated. <p> In a loosely-coupled system, the model is implemented by replicating objects in the local memories of the processors. This ability to replicate objects is a significant difference with other object-based models, such as Emerald <ref> [8] </ref> and Amber [9]. We have studied several protocols for keeping all these copies consistent and we have looked at replication strategies. We have described two implementations of the model. One implementation replicates objects everywhere and updates copies through a fast multicast protocol. The other implementation uses only point-to-point messages.
Reference: 9. <author> J.S. Chase, F.G. Amador, E.D. Lazowska, H.M. Levy, and R.J. Littlefield, </author> <title> ``The Amber System: Parallel Programming on a Network of Multiprocessors,'' </title> <booktitle> Proceedings of the 12th ACM Symposium on Operating System Principles, </booktitle> <address> Litchfield Park, AZ, </address> <pages> pp. </pages> <month> 147-158 (Dec. </month> <year> 1989). </year>
Reference-contexts: These issues are addressed by other models, such as atomic transactions and concurrent object-oriented programming and are not the topic of this paper. Our model also differs from the object-based models supported by Emerald [8] and Amber <ref> [9] </ref>. Objects in these languages are migrated between processors, but are not replicated. An Emerald object, for example, can be activeit may contain a processand should not be replicated. <p> In a loosely-coupled system, the model is implemented by replicating objects in the local memories of the processors. This ability to replicate objects is a significant difference with other object-based models, such as Emerald [8] and Amber <ref> [9] </ref>. We have studied several protocols for keeping all these copies consistent and we have looked at replication strategies. We have described two implementations of the model. One implementation replicates objects everywhere and updates copies through a fast multicast protocol. The other implementation uses only point-to-point messages.
Reference: 10. <author> H.E. Bal, M.F. Kaashoek, </author> <title> and A.S. Tanenbaum, ``Orca: A Language for Parallel Programming of Distributed Systems,'' </title> <journal> IEEE Transactions on Software Engineering (1992, </journal> <note> to appear). </note>
Reference-contexts: Rather, it is a simple, procedural, type-secure language. It supports abstract data types, processes, a variety of data structures, modules, and generics. Various implementations of Orca on different hardware configurations have been in use for three years. The language, its implementation, and use are described elsewhere <ref> [10, 11, 12] </ref>. In the rest of this paper we will study replication techniques for the shared data-object model. In Section 2, we will describe the space of possible design choices.
Reference: 11. <author> A.S. Tanenbaum, M.F. Kaashoek, and H.E. Bal, </author> <title> ``Parallel Programming using Shared Objects and Broadcasting,'' </title> <journal> IEEE Computer (1992, </journal> <note> to appear). </note>
Reference-contexts: Rather, it is a simple, procedural, type-secure language. It supports abstract data types, processes, a variety of data structures, modules, and generics. Various implementations of Orca on different hardware configurations have been in use for three years. The language, its implementation, and use are described elsewhere <ref> [10, 11, 12] </ref>. In the rest of this paper we will study replication techniques for the shared data-object model. In Section 2, we will describe the space of possible design choices.
Reference: 12. <author> H.E. Bal, </author> <title> Programming Distributed Systems, </title> <publisher> Prentice Hall International, </publisher> <address> Hemel Hemp-stead, England (1991). </address>
Reference-contexts: Rather, it is a simple, procedural, type-secure language. It supports abstract data types, processes, a variety of data structures, modules, and generics. Various implementations of Orca on different hardware configurations have been in use for three years. The language, its implementation, and use are described elsewhere <ref> [10, 11, 12] </ref>. In the rest of this paper we will study replication techniques for the shared data-object model. In Section 2, we will describe the space of possible design choices. <p> Under no circumstances should these two events be mixed, since that would violate serializability. Because of this restriction, it does not suffice to implement a write operation by sending it to the primary-copy site and having this site forward it to the secondary-copy sites <ref> [12] </ref>. The problem can be solved using a more complicated and expensive update protocol [12]. Therefore, updating copies will be expensive, so it is not clear whether updating will be more efficient than invalidation. <p> Because of this restriction, it does not suffice to implement a write operation by sending it to the primary-copy site and having this site forward it to the secondary-copy sites <ref> [12] </ref>. The problem can be solved using a more complicated and expensive update protocol [12]. Therefore, updating copies will be expensive, so it is not clear whether updating will be more efficient than invalidation. We have decided to implement both options and to determine experimentally which of the two is best. 3.2. <p> The main disadvantage of full replication then is the fact that each processor will be interrupted once for each write operation. With partial replication, this CPU overhead would be less. 5. PERFORMANCE There are several ways to measure the performances of the replication techniques. The approach taken in <ref> [12] </ref> is to implement several user applications in Orca, execute them on the different run time systems, and measure the speedups. Applications we have looked at are matrix multiplication, the all-pairs shortest paths problem, branch-and-bound, alpha-beta search, and successive overrelaxation. <p> In the second experiment we measured the cost for updating an entire 1 Kb array object. These two types of objects occur frequently in application programs <ref> [12] </ref>. 5.1. Performance of the RPC Run Time System RPC. The figure shows the costs for invalidating N copies of an object and for updating 4-byte and 1 Kb objects. <p> TSP uses another object (the bound) for keeping track of the current best solution. This object is shared among all slave processes. Measurements of the program for a 12-city problem show that this object may be read a million times and updated only a few times <ref> [12] </ref>. After the object has been changed (i.e., a slave has found a better route for the salesman), this new value is read many times by all the slaves. Thus the best strategy is to replicate the variable everywhere and update all copies whenever the variable changes. <p> In conclusion, the invalidation protocol is not appropriate for ASP. With the update protocol, it is possible to obtain reasonable speedups <ref> [12] </ref>, although far from linear. To obtain good (close to linear) speedups, the multicast protocol is required. 6. CONCLUSIONS The model discussed in this paper allows programmers to define operations of arbitrary complexity on shared data-objects.
Reference: 13. <author> D.K. Gifford, </author> <title> ``Weighted Voting for Replicated Data,'' </title> <booktitle> Proceedings 7th Symposium - 18 - Operating Systems Principles, </booktitle> <address> Pacific Grove, CA, </address> <pages> pp. 150-162, </pages> <note> ACM SIGOPS (Dec. </note> <year> 1979). </year>
Reference-contexts: In Section 6, we will present our conclusions and compare our work with that of others. 2. DESIGN SPACE The technique of data replication in distributed systems is typically used to increase the availability and reliability of the data in the presence of processor failures and network partitions <ref> [13, 14, 15, 16, 17] </ref>. For example, if multiple copies of the same logical data are stored on different processors, the data can still be accessed if some of the processors are down.
Reference: 14. <author> P.A. Bernstein and N. Goodman, </author> <title> ``Concurrency Control in Distributed Database systems,'' </title> <journal> ACM Comping Surveys 13(2), </journal> <pages> pp. </pages> <month> 185-221 (June </month> <year> 1981). </year>
Reference-contexts: In Section 6, we will present our conclusions and compare our work with that of others. 2. DESIGN SPACE The technique of data replication in distributed systems is typically used to increase the availability and reliability of the data in the presence of processor failures and network partitions <ref> [13, 14, 15, 16, 17] </ref>. For example, if multiple copies of the same logical data are stored on different processors, the data can still be accessed if some of the processors are down. <p> Also, we will look at the differences and resemblances between protocols for replication and coherence protocols for CPU caches [18, 27], nonuniform memory access (NUMA) architectures [28, 29], file caches [30, 31, 32], and distributed database systems <ref> [14] </ref>. Based on this analysis, we will try to improve our implementations. Our model has several advantages over other models based on logically shared data. It provides a higher level of abstraction and, in many cases, is more efficient. Below, we will compare our model with several related ones.
Reference: 15. <author> T.A. Joseph and K.P. Birman, </author> <title> ``Low Cost Management of Replicated Data in Fault-Tolerant Distributed Systems,'' </title> <journal> ACM Transactions on Computer Systems 4(1) (Feb. </journal> <year> 1987). </year>
Reference-contexts: In Section 6, we will present our conclusions and compare our work with that of others. 2. DESIGN SPACE The technique of data replication in distributed systems is typically used to increase the availability and reliability of the data in the presence of processor failures and network partitions <ref> [13, 14, 15, 16, 17] </ref>. For example, if multiple copies of the same logical data are stored on different processors, the data can still be accessed if some of the processors are down. <p> These processes belong to a single job and run in a single address space, so they can share copies of objects. It is useful to distinguish between read operations and write operations on replicated data: a read operation does not modify the data, while a write operation (potentially) does <ref> [15] </ref>. For our model, we define a read operation as an operation that does not change the internal data of the object it is applied to.
Reference: 16. <author> R. van Renesse and A.S. Tanenbaum, </author> <title> ``Voting with Ghosts,'' </title> <booktitle> Proceedings of the 8th International Conference on Distributed Computing Systems, </booktitle> <address> San Jose, CA, </address> <pages> pp. </pages> <month> 456-462 (June </month> <year> 1988). </year>
Reference-contexts: In Section 6, we will present our conclusions and compare our work with that of others. 2. DESIGN SPACE The technique of data replication in distributed systems is typically used to increase the availability and reliability of the data in the presence of processor failures and network partitions <ref> [13, 14, 15, 16, 17] </ref>. For example, if multiple copies of the same logical data are stored on different processors, the data can still be accessed if some of the processors are down.
Reference: 17. <author> S.B. Davidson, H. Garcia-Molina, and D. Skeen, </author> <title> ``Consistency in Partitioned Networks,'' </title> <journal> ACM Comping Surveys 17(3), </journal> <pages> pp. </pages> <month> 341-370 (Sept. </month> <year> 1985). </year>
Reference-contexts: In Section 6, we will present our conclusions and compare our work with that of others. 2. DESIGN SPACE The technique of data replication in distributed systems is typically used to increase the availability and reliability of the data in the presence of processor failures and network partitions <ref> [13, 14, 15, 16, 17] </ref>. For example, if multiple copies of the same logical data are stored on different processors, the data can still be accessed if some of the processors are down.
Reference: 18. <author> S.J. Eggers and R.H. Katz, </author> <title> ``A Characterization of Sharing in Parallel Programs and Its Application to Coherency Protocol Evaluation,'' </title> <booktitle> 15th International Symposium on Computer Architecture, Jerusalem, Israel, </booktitle> <pages> pp. </pages> <month> 373-382 (May </month> <year> 1989). </year>
Reference-contexts: This scheme is a departure from techniques that replicate for availability. These techniques in general need interprocess communication for every read and write operation. With our approach, read operations are executed locally. Since, for many parallel applications, read operations far outnumber write operations <ref> [18] </ref>, this is a significant advantage. The second goal of replication is to increase parallelism. If an object is stored on only one processor, each operation must be executed by that processor. This processor may easily become a sequential bottleneck. <p> In the future we intend to do a more detailed analysis of our protocols and strategies, using a large set of user applications. Also, we will look at the differences and resemblances between protocols for replication and coherence protocols for CPU caches <ref> [18, 27] </ref>, nonuniform memory access (NUMA) architectures [28, 29], file caches [30, 31, 32], and distributed database systems [14]. Based on this analysis, we will try to improve our implementations. Our model has several advantages over other models based on logically shared data.
Reference: 19. <author> K. Li and P. Hudak, </author> <title> ``Memory Coherence in Shared Virtual Memory Systems,'' </title> <journal> ACM Transactions on Computer Systems 7(4) (Nov. </journal> <year> 1989). </year>
Reference-contexts: The size of the object. 3. The size of the parameters of the write operation. - 6 - 4. Whether the write operation is followed by a read operation or by another write operation. Kai Li argues that, for the Shared Virtual Memory system, an update scheme is inappropriate <ref> [19] </ref>. In addition to being almost impossible to implement, it will cause a page fault on every write instruction. In our model, however, this disadvantage is far less severe. Users can define write operations of any complexity on shared objects. <p> When a processor executes a read operation, it locates the primary copy and asks for the value of the object. A more sophisticated scheme allows the primary copy to move from one processor to another. Kai Li compares several of these schemes and analyzes their performance <ref> [19] </ref>. In an update scheme, mutual exclusion can be achieved in at least two ways. One way is to appoint one copy of each object as primary copy and direct all write operations to the processor containing the primary copy.
Reference: 20. <author> S.E. Lucco, </author> <title> ``A Heuristic Linda Kernel for Hypercube Multiprocessors,'' </title> <booktitle> Conf. on Hypercube Multiprocessors, </booktitle> <pages> pp. </pages> <month> 32-38 </month> <year> (1987). </year>
Reference-contexts: Its novelty is its dynamic replication strategy based on run-time statistics. Although statistics are used frequently in distributed data bases, they are uncommon in distributed programming languages (the only exception we know of is <ref> [20] </ref>). The second RTS is based on a novel multicast protocol [21]. This protocol provides the necessary semantics for keeping all copies of each object consistent. Also, it is optimized for parallel applications, in which processes communicate fairly often.
Reference: 21. <author> M.F. Kaashoek and A.S. Tanenbaum, </author> <title> ``Group Communication in the Amoeba Distributed Operating System,'' </title> <booktitle> 11th Int'l Conf. on Distributed Computing Systems, Arling-ton, Texas, </booktitle> <pages> pp. </pages> <month> 222-230 (20-24 May </month> <year> 1991). </year>
Reference-contexts: Its novelty is its dynamic replication strategy based on run-time statistics. Although statistics are used frequently in distributed data bases, they are uncommon in distributed programming languages (the only exception we know of is [20]). The second RTS is based on a novel multicast protocol <ref> [21] </ref>. This protocol provides the necessary semantics for keeping all copies of each object consistent. Also, it is optimized for parallel applications, in which processes communicate fairly often. <p> Again, statistics are used to determine to best location for an object. If an object is migrated, precautions are taken for dealing with machines that are unaware of the object's new location. 4. AN IMPLEMENTATION USING MULTICAST COMMUNICATION The second RTS uses Amoeba's indivisible reliable multicast protocol described in <ref> [21] </ref>. This protocol is highly efficient and usually only requires two packets (one point-to-point and one multicast) per reliable multicast. Sending a short message reliably to 10 processors, for example, takes 2.7 msec on the hardware described above. <p> For all examples discussed in this paper, the first method is used. Of course, there are many more issues involved in the protocol, such as buffer management of messages, group management, and crashes of the sequencer or regular nodes. These issues are described in <ref> [21] </ref>. With the protocol outlined above, programs need not worry about lost messages. Recovery of communication failures is handled automatically and transparently by the protocol. Efficiency is obtained by optimizing the protocol for no communication failures, as these rarely happen with current state of microprocessor and network technology. 4.1.
Reference: 22. <author> A.S. Tanenbaum, R. van Renesse, H. van Staveren, G.J. Sharp, S.J. Mullender, A.J. Jan-sen, and G. van Rossum, </author> <title> ``Experiences with the Amoeba Distributed Operating System,'' </title> <journal> Comm. ACM 33(2), </journal> <pages> pp. </pages> <month> 46-63 (Dec. </month> <year> 1990). </year>
Reference-contexts: This protocol provides the necessary semantics for keeping all copies of each object consistent. Also, it is optimized for parallel applications, in which processes communicate fairly often. Both run time systems are implemented on top of the Amoeba distributed operating system <ref> [22] </ref> and use the FLIP routing protocol [23], which supports point-to-point communication as well as multicast. 3. AN IMPLEMENTATION USING POINT-TO-POINT COMMUNICATION The first run time system we describe uses only point-to-point messages (Amoeba Remote Procedure Call) for interprocess communication.
Reference: 23. <author> M.F. Kaashoek, R. van Renesse, H. van Staveren, </author> <title> and A.S. Tanenbaum, ``FLIP: an Internet Protocol for Supporting Distributed Systems,'' </title> <type> Report IR-251, </type> <institution> Vrije Universi-teit, </institution> <address> Amsterdam, The Netherlands (June 1991). </address>
Reference-contexts: This protocol provides the necessary semantics for keeping all copies of each object consistent. Also, it is optimized for parallel applications, in which processes communicate fairly often. Both run time systems are implemented on top of the Amoeba distributed operating system [22] and use the FLIP routing protocol <ref> [23] </ref>, which supports point-to-point communication as well as multicast. 3. AN IMPLEMENTATION USING POINT-TO-POINT COMMUNICATION The first run time system we describe uses only point-to-point messages (Amoeba Remote Procedure Call) for interprocess communication.
Reference: 24. <author> B. Liskov, </author> <title> ``Distributed Programming in Argus,'' </title> <journal> Communications of the ACM 31(3), </journal> <pages> pp. </pages> <month> 300-312 (March </month> <year> 1988). </year>
Reference-contexts: The implementation is optimized, however, to overlap regular computations with sending reply messages, so the latter overhead is small. The usage of a 2-phase update protocol in a language RTS is certainly not new. Languages based on atomic transaction (e.g., Argus <ref> [24] </ref>) also use 2-phase protocols. In our model, however, a 2-phase protocol is used for updating copies of the same object, rather than for updating many different objects.
Reference: 25. <author> L. Lamport, </author> <title> ``Time, Clocks, and the Ordering of Events in a Distributed System,'' </title> <journal> Communications of the ACM 21(7), </journal> <pages> pp. </pages> <month> 558-565 (July </month> <year> 1978). </year>
Reference-contexts: The protocol described above correctly implements the serializability requirement. The protocol guarantees that all processors observe changes to shared objects in the same order. Note that it does not provide a total (temporal) ordering <ref> [25] </ref> among operations. Suppose Processor P1 initiates a write operation on object X and, a few microseconds later, Processor P2 reads the value of X. The update message for X sent by P1 need not have even reached P2 yet, so P2 may still use the old value of X.
Reference: 26. <author> J.-F. Jenq and S. Sahni, </author> <title> ``All Pairs Shortest Paths on a Hypercube Multiprocessor,'' </title> <booktitle> Proceedings of the 1987 International Conference on Parallel Processing, </booktitle> <address> St. Charles, </address> <publisher> Ill., </publisher> <pages> pp. </pages> <month> 713-716 (Aug. </month> <year> 1987). </year>
Reference-contexts: The parallel algorithm we use is similar to the one given in <ref> [26] </ref>, which is a parallel version of Floyd's algorithm. - 16 - The distances between the nodes are represented in a matrix. Each processor contains a worker process that computes part of the result matrix. The parallel algorithm performs N iterations.
Reference: 27. <author> S. Owicki and A. Agarwal, </author> <title> ``Evaluating the Performance of Software Cache Coherence,'' </title> <booktitle> Proceedings 3nd International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Boston, MA, </address> <pages> pp. </pages> <month> 230-242 (April </month> <year> 1989). </year>
Reference-contexts: In the future we intend to do a more detailed analysis of our protocols and strategies, using a large set of user applications. Also, we will look at the differences and resemblances between protocols for replication and coherence protocols for CPU caches <ref> [18, 27] </ref>, nonuniform memory access (NUMA) architectures [28, 29], file caches [30, 31, 32], and distributed database systems [14]. Based on this analysis, we will try to improve our implementations. Our model has several advantages over other models based on logically shared data.
Reference: 28. <author> A.L. Cox and R.J. Fowler, </author> <title> ``The Implementation of a Coherent Memory Abstraction on a NUMA Multiprocessor: Experience with PLATINUM,'' </title> <booktitle> Proceedings 12th Symposium Operating System Principles, </booktitle> <address> Litchfield Park, AZ, </address> <pages> pp. 32-44, </pages> <address> Rochester (Dec. </address> <year> 1989). </year>
Reference-contexts: In the future we intend to do a more detailed analysis of our protocols and strategies, using a large set of user applications. Also, we will look at the differences and resemblances between protocols for replication and coherence protocols for CPU caches [18, 27], nonuniform memory access (NUMA) architectures <ref> [28, 29] </ref>, file caches [30, 31, 32], and distributed database systems [14]. Based on this analysis, we will try to improve our implementations. Our model has several advantages over other models based on logically shared data. It provides a higher level of abstraction and, in many cases, is more efficient.
Reference: 29. <author> M.L. Scott, T.J. Leblanc, and B.D. Marsh, </author> <title> ``Design Rationale for Psyche, a General-Purpose Multiprocessor Operating System,'' </title> <booktitle> International Conference on Parallel Processing, </booktitle> <address> St. Charles, </address> <publisher> Ill., </publisher> <pages> pp. </pages> <month> 255-261 (Aug. </month> <year> 1988). </year>
Reference-contexts: In the future we intend to do a more detailed analysis of our protocols and strategies, using a large set of user applications. Also, we will look at the differences and resemblances between protocols for replication and coherence protocols for CPU caches [18, 27], nonuniform memory access (NUMA) architectures <ref> [28, 29] </ref>, file caches [30, 31, 32], and distributed database systems [14]. Based on this analysis, we will try to improve our implementations. Our model has several advantages over other models based on logically shared data. It provides a higher level of abstraction and, in many cases, is more efficient.
Reference: 30. <author> J.D. Noe, A.B. Proudfoot, and C. Pu, </author> <title> ``Replication in Distributed Systems: The Eden - 19 - Experience,'' </title> <institution> TR-85-08-06, Dept. of Computer Science, University of Washington, </institution> <address> Seattle (Sept. </address> <year> 1985). </year>
Reference-contexts: Also, we will look at the differences and resemblances between protocols for replication and coherence protocols for CPU caches [18, 27], nonuniform memory access (NUMA) architectures [28, 29], file caches <ref> [30, 31, 32] </ref>, and distributed database systems [14]. Based on this analysis, we will try to improve our implementations. Our model has several advantages over other models based on logically shared data. It provides a higher level of abstraction and, in many cases, is more efficient.
Reference: 31. <author> J.H. Morris, M. Satyanarayan, M.H. Conner, J.H. Howard, D.S.H. Rosenthal, and F.D. Smith, </author> <title> ``Andrew a Distributed Personal Computing Environment,'' </title> <journal> Communications of the ACM 29(3), </journal> <pages> pp. </pages> <month> 184-201 (March </month> <year> 1986). </year>
Reference-contexts: Also, we will look at the differences and resemblances between protocols for replication and coherence protocols for CPU caches [18, 27], nonuniform memory access (NUMA) architectures [28, 29], file caches <ref> [30, 31, 32] </ref>, and distributed database systems [14]. Based on this analysis, we will try to improve our implementations. Our model has several advantages over other models based on logically shared data. It provides a higher level of abstraction and, in many cases, is more efficient.
Reference: 32. <author> J.K. Ousterhout, A.R. Cherenson, F. Douglis, M.N. Nelson, and B.B. Welch, </author> <title> ``The Sprite Network Operating System,'' </title> <booktitle> IEEE Computer 21(2), </booktitle> <pages> pp. </pages> <month> 23-37 (Feb. </month> <year> 1988). </year>
Reference-contexts: Also, we will look at the differences and resemblances between protocols for replication and coherence protocols for CPU caches [18, 27], nonuniform memory access (NUMA) architectures [28, 29], file caches <ref> [30, 31, 32] </ref>, and distributed database systems [14]. Based on this analysis, we will try to improve our implementations. Our model has several advantages over other models based on logically shared data. It provides a higher level of abstraction and, in many cases, is more efficient.
Reference: 33. <author> W.G. Levelt, M.F. Kaashoek, H.E. Bal, </author> <title> and A.S. Tanenbaum, ``A Comparison of Two Paradigms for Distributed Shared Memory,'' </title> <note> SoftwarePractice and Experience (1992, to appear). </note>
Reference-contexts: We prefer to have simple and easy-to-use semantics and therefore support consistency of replicated shared data. Kai Li's Shared Virtual Memory supports memory coherency, but it has other disadvantages <ref> [33] </ref>. For example, it can only invalidate but not update copies of data. Also, the SVM will perform very poorly if processes on many different processors repeatedly write on the same page.
Reference: 34. <author> S. Ahuja, N. Carriero, and D. Gelernter, ``Linda and Friends,'' </author> <booktitle> IEEE Computer 19(8), </booktitle> <pages> pp. </pages> <month> 26-34 (Aug. </month> <year> 1986). </year>
Reference-contexts: Also, the SVM will perform very poorly if processes on many different processors repeatedly write on the same page. This situation arises if multiple processors write the same variable, or if they write different variables placed on the same page. Linda's Tuple Space <ref> [34] </ref> is another model that hides replication from the programmer. It provides a fixed number of low-level operations on shared data (tuples) [35]. Logical operations on shared data structures frequently consist of several low-level operations, each of which can require physical communication.
Reference: 35. <author> M.F. Kaashoek, H.E. Bal, </author> <title> and A.S. Tanenbaum, ``Experience with the Distributed Data Structure Paradigm in Linda,'' First USENIX/SERC Workshop on Experiences with Building Distributed and Multiprocessor Systems, </title> <address> Ft. Lauderdale, FL, </address> <pages> pp. </pages> <month> 175-191 (Oct. </month> <year> 1989). </year>
Reference-contexts: This situation arises if multiple processors write the same variable, or if they write different variables placed on the same page. Linda's Tuple Space [34] is another model that hides replication from the programmer. It provides a fixed number of low-level operations on shared data (tuples) <ref> [35] </ref>. Logical operations on shared data structures frequently consist of several low-level operations, each of which can require physical communication. In our model, the programmer can define a single high-level operation that does the job with lower communication costs.
References-found: 35

