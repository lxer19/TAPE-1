URL: ftp://ftp.cs.toronto.edu/pub/tap/plate.ijcai91.ps.Z
Refering-URL: http://www.cs.utoronto.ca/~tap/
Root-URL: 
Email: tap@ai.utoronto.ca  
Title: Holographic Reduced Representations: Convolution Algebra for Compositional Distributed Representations  
Phone: 1991.  
Author: Tony Plate 
Affiliation: Department of Computer Science University of Toronto  
Address: San Mateo, CA,  Toronto, Ontario, Canada, M5S 1A4  
Note: In Proceedings of the 12th International Joint Conference on Artificial Intelligence, edited by John Mylopoulos and Ray Reiter, Morgan Kaufmann,  
Abstract: A solution to the problem of representing compositional structure using distributed representations is described. The method uses circular convolution to associate items, which are represented by vectors. Arbitrary variable bindings, short sequences of various lengths, frames, and reduced representations can be compressed into a fixed width vector. These representations are items in their own right, and can be used in constructing compositional structures. The noisy reconstructions given by convolution memories can be cleaned up by using a separate associative memory that has good reconstructive properties.
Abstract-found: 1
Intro-found: 1
Reference: [ AIJ, 1990 ] <editor> Special issue on connectionist symbol processing. </editor> <booktitle> Artifi cial Intelligence, </booktitle> <pages> 46(1-2), </pages> <year> 1990. </year>
Reference-contexts: However, Hinton does not suggest any concrete way of performing this reduction mapping. Some researchers have built models or designed frame works in which some some compositional structure is present in distributed representations. For some examples see the papers of Touretzky, Pollack, or Smolensky in <ref> [ AIJ, 1990 ] </ref> . In this paper I propose a new method for representing compositional structure in distributed representations. Circular convolution is used to construct associations of vectors. The representation of an association is a vector of the same dimensionality as the vectors which are associated.
Reference: [ Fodor and Pylyshyn, 1988 ] <author> J. A. Fodor and Z. W. Pylyshyn. </author> <title> Connec tionism and cognitive architecture: A critical analysis. </title> <journal> Cognition, </journal> <volume> 28 </volume> <pages> 3-71, </pages> <year> 1988. </year>
Reference-contexts: However, the problem of representing compositional structure in distributed representations has been for some time a prominent concern of both followers and critics of the connectionist faith <ref> [ Fodor and Pylyshyn, 1988; Hinton, 1990 ] </ref> . Using connectionist networks, e.g., back propagation nets, Hopfield nets, Boltzmann machines, or Willshaw nets, it is easy to represent associations of a fixed number of items.
Reference: [ Hinton, 1981 ] <author> G. E. Hinton. </author> <title> Implementing semantic networks in par allel hardware. </title> <editor> In G. E. Hinton and J. A. Anderson, editors, </editor> <booktitle> Parallel Models of Associative Memory. </booktitle> <address> Hillsdale, NJ: </address> <publisher> Erlbaum, </publisher> <year> 1981. </year>
Reference: [ Hinton, 1984 ] <author> G. E. Hinton. </author> <title> Distributed representations. </title> <type> Technical Report CMU-CS-84-157, </type> <institution> Carnegie-Mellon University, </institution> <address> Pittsburgh PA, </address> <year> 1984. </year>
Reference-contexts: 1 Introduction Distributed representations <ref> [ Hinton, 1984 ] </ref> are attractive for a number of reasons. They offer the possibility of representing concepts in a continuous space, they degrade gracefully with noise, and they can be processed in a parallel network of simple processing elements.
Reference: [ Hinton, 1990 ] <author> G. E. Hinton. </author> <title> Mapping part-whole heirarchies into con nectionist networks. </title> <journal> Artificial Intelligence, </journal> <volume> 46(1-2):47-76, </volume> <year> 1990. </year>
Reference-contexts: However, the problem of representing compositional structure in distributed representations has been for some time a prominent concern of both followers and critics of the connectionist faith <ref> [ Fodor and Pylyshyn, 1988; Hinton, 1990 ] </ref> . Using connectionist networks, e.g., back propagation nets, Hopfield nets, Boltzmann machines, or Willshaw nets, it is easy to represent associations of a fixed number of items.
Reference: [ Hopfield, 1982 ] <author> J. J. </author> <title> Hopfield. Neural networks and physical systems with emergent collective computational abilities. </title> <booktitle> Proceedings of the National Academy of Sciences U.S.A., </booktitle> <volume> 79 </volume> <pages> 2554-2558, </pages> <year> 1982. </year>
Reference-contexts: The properties of matrix memories are well understood. Two of the best known matrix memories are "Willshaw" networks [ Willshaw, 1981 ] and Hopfield networks <ref> [ Hopfield, 1982 ] </ref> . Matrix memories can be used to construct auto-associative (or "content addressable") memories for pattern correction and completion. They can also be used to represent associations between two vectors. After two vectors are associated one can be used as a cue to retrieve the other.
Reference: [ Kanerva, 1988 ] <author> P. Kanerva. </author> <title> Sparse Distributed Memory. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1988. </year>
Reference-contexts: The item memory acts as an auto-associator to clean up the noisy items retrieved from the convolution trace. The exact method of implementation of the item memory is unimportant. Hopfield networks are probably not a good candidate because of their low capacity. Kanerva networks <ref> [ Kanerva, 1988 ] </ref> have sufficient capacity, but can only store binary vectors. 6 For experiments I have been using a nearest neighbor matching memory. 5 Representing more complex structure Pairs of items are easy to represent in any type of associative memory, but convolution memory is also suited to the
Reference: [ Metcalfe Eich, 1982 ] <author> Janet Metcalfe Eich. </author> <title> A composite holographic associative recall model. </title> <journal> Psychological Review, </journal> <volume> 89 </volume> <pages> 627-661, </pages> <year> 1982. </year>
Reference: [ Murdock, 1982 ] <author> Bennet B. Murdock. </author> <title> A theory for the storage and retrieval of item and associative information. </title> <journal> Psychological Review, </journal> <volume> 89(6) </volume> <pages> 316-338, </pages> <year> 1982. </year>
Reference: [ Murdock, 1983 ] <author> B. B. Murdock. </author> <title> A distributed memory model for serial-order information. </title> <journal> Psychological Review, </journal> <volume> 90(4) </volume> <pages> 316-338, </pages> <year> 1983. </year>
Reference-contexts: Nearly all work on associative memory has been concerned with storing items or pairs of items. Convolution-correlation memories (sometimes referred to as holographic-like) and matrix memories have been regarded as alternate methods for implementing associative memory <ref> [ Willshaw, 1981; Murdock, 1983; Pike, 1984; Schonemann, 1987 ] </ref> . Matrix memories have received more interest, probably due to their relative simplicity and their higher capacity in terms of the dimensionality of the vectors being associated. The properties of matrix memories are well understood.
Reference: [ Murdock, 1987 ] <author> Bennet B. Murdock. </author> <title> Serial-order effects in a distributed-memory model. </title> <editor> In David S. Gorfein and Robert R. Hoff-man, editors, </editor> <booktitle> MEMORY AND LEARNING: The Ebbinghaus Centennial Conference, </booktitle> <pages> pages 277-310. </pages> <publisher> Lawrence Erlbaum Associates, </publisher> <year> 1987. </year>
Reference-contexts: The end of the sequence is detected when the correlation of the trace with the current item is not similar to any item in the item memory. Another way to represent sequences is to use the entire previous sequence as context rather than just the previous item <ref> [ Murdock, 1987 ] </ref> . This makes it possible to store sequences with repeated of items. <p> As the sequences get longer the noise in the retrieved items increases until the items are impossible to identify. This limit can be overcome by chunking | creating new "non terminal" items representing subsequences <ref> [ Murdock, 1987 ] </ref> . The second sequence representation method is the most suitable one to do chunking with. Suppose we want to represent the sequence abcdefgh.
Reference: [ Pike, 1984 ] <author> Ray Pike. </author> <title> Comparison of convolution and matrix dis tributed memory systems for associative recall and recognition. </title> <journal> Psychological Review, </journal> <volume> 91(3) </volume> <pages> 281-294, </pages> <year> 1984. </year>
Reference-contexts: Nearly all work on associative memory has been concerned with storing items or pairs of items. Convolution-correlation memories (sometimes referred to as holographic-like) and matrix memories have been regarded as alternate methods for implementing associative memory <ref> [ Willshaw, 1981; Murdock, 1983; Pike, 1984; Schonemann, 1987 ] </ref> . Matrix memories have received more interest, probably due to their relative simplicity and their higher capacity in terms of the dimensionality of the vectors being associated. The properties of matrix memories are well understood.
Reference: [ Plate, 1991 ] <author> Tony A. </author> <title> Plate. Holographic Reduced Representations. </title> <type> Technical Report CRG-TR-91-1, </type> <institution> Department of Computer Science, University of Toronto, </institution> <year> 1991. </year>
Reference-contexts: At each iteration the machine executes one of three action sequences depending on the output of the classifier. The stack could be implemented in any of a number of ways; including the way suggested in <ref> [ Plate, 1991 ] </ref> , or in a network with fast weights. The machine is shown in Figure 5. The control loop for the chunked sequence readout machine is: Loop: (until stack gives END signal) Clean up the trace to recover most prominent item: x = Clean (t).
Reference: [ Schonemann, 1987 ] <author> P. H. Schonemann. </author> <title> Some algebraic relations be tween involutions, convolutions, and correlations, with applications to holographic memories. </title> <journal> Biological Cybernetics, </journal> <volume> 56 </volume> <pages> 367-374, </pages> <year> 1987. </year>
Reference-contexts: Nearly all work on associative memory has been concerned with storing items or pairs of items. Convolution-correlation memories (sometimes referred to as holographic-like) and matrix memories have been regarded as alternate methods for implementing associative memory <ref> [ Willshaw, 1981; Murdock, 1983; Pike, 1984; Schonemann, 1987 ] </ref> . Matrix memories have received more interest, probably due to their relative simplicity and their higher capacity in terms of the dimensionality of the vectors being associated. The properties of matrix memories are well understood.
Reference: [ Slack, 1984a ] <author> J. N. Slack. </author> <title> A parsing architecture based on distributed memory machines. </title> <booktitle> In Proceedings of COLING-84, </booktitle> <pages> pages 92-95, </pages> <institution> Stanford, Calif., 1984. Association for Computational Linguistics. </institution>
Reference: [ Slack, 1984b ] <author> J. N. Slack. </author> <title> The role of distributed memory in natural language processing. </title> <editor> In T. O'Shea, editor, </editor> <booktitle> Advances in Artificial Intelligence: Proceedings of the Sixth European Conference on Artificial Intelligence, ECAI-84. </booktitle> <publisher> Elsevier Science Publishers, </publisher> <year> 1984. </year>
Reference: [ Slack, 1986 ] <author> J. N. Slack. </author> <title> A parsing architecture based on distributed memory machines. </title> <booktitle> In Proceedings of COLING-86, </booktitle> <pages> pages 476-481. </pages> <institution> Association for Computational Linguistics, </institution> <year> 1986. </year>
Reference: [ Smolensky, 1990 ] <author> P. Smolensky. </author> <title> Tensor product variable binding and the representation of symbolic structures in connectionist systems. </title> <journal> Artificial Intelligence, </journal> <volume> 46(1-2):159-216, </volume> <year> 1990. </year>
Reference: [ Touretzky and Geva, 1987 ] <author> D. S. Touretzky and S. Geva. </author> <title> A dis tributed connectionist representation for concept structures. </title> <booktitle> In Proceedings of the Ninth Annual Cognitive Science Society Conference, </booktitle> <address> Hillsdale, NJ, 1987. </address> <publisher> Erlbaum. </publisher>
Reference: [ Touretzky and Hinton, 1985 ] <author> D. S. Touretzky and G. E. Hinton. </author> <title> Symbols among the neurons: Details of a connectionist inference architecture. </title> <booktitle> In IJCAI 9, </booktitle> <pages> pages 238-243, </pages> <year> 1985. </year>
Reference-contexts: This gives ffx~a + fix + fla as the representation of a variable binding. This type of variable binding can also be implemented in other types of associative memory, e.g., the triple-space of BoltzCONS <ref> [ Touretzky and Hinton, 1985 ] </ref> , or the outer product of roles and fillers in DUCS [ Touret-zky and Geva, 1987 ] . However, in those systems the variable and value objects were of a different dimension than the binding object.
Reference: [ Willshaw, 1981 ] <author> D. Willshaw. </author> <title> Holography, associative memory, and inductive generalization. </title> <editor> In G. E. Hinton and J. A. Anderson, editors, </editor> <title> Parallel models of associative memory. </title> <publisher> Erlbaum, </publisher> <address> Hillsdale, NJ, </address> <year> 1981. </year>
Reference-contexts: Nearly all work on associative memory has been concerned with storing items or pairs of items. Convolution-correlation memories (sometimes referred to as holographic-like) and matrix memories have been regarded as alternate methods for implementing associative memory <ref> [ Willshaw, 1981; Murdock, 1983; Pike, 1984; Schonemann, 1987 ] </ref> . Matrix memories have received more interest, probably due to their relative simplicity and their higher capacity in terms of the dimensionality of the vectors being associated. The properties of matrix memories are well understood. <p> Matrix memories have received more interest, probably due to their relative simplicity and their higher capacity in terms of the dimensionality of the vectors being associated. The properties of matrix memories are well understood. Two of the best known matrix memories are "Willshaw" networks <ref> [ Willshaw, 1981 ] </ref> and Hopfield networks [ Hopfield, 1982 ] . Matrix memories can be used to construct auto-associative (or "content addressable") memories for pattern correction and completion. They can also be used to represent associations between two vectors. <p> An entire sequence can be repre 6 Although most of this paper assumes items are represented as real vectors, convolution memories also work with binary vectors <ref> [ Willshaw, 1981 ] </ref> . 32 sented in one memory trace (providing the soft capacity limits are not exceeded), or chunking can be used to represent a sequence of any length in a number of memory traces.
References-found: 21

