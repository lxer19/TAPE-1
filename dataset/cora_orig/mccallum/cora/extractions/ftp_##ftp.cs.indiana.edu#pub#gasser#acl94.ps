URL: ftp://ftp.cs.indiana.edu/pub/gasser/acl94.ps
Refering-URL: http://www.cs.indiana.edu/ai/Gasser/Morphophon/home.html
Root-URL: http://www.cs.indiana.edu
Title: Acquiring Receptive Morphology: A Connectionist Model  
Author: Michael Gasser 
Affiliation: Computer Science and Linguistics Departments Indiana University  
Abstract: This paper describes a modular connectionist model of the acquisition of receptive inflectional morphology. The model takes inputs in the form of phones one at a time and outputs the associated roots and inflections. Simulations using artificial language stimuli demonstrate the capacity of the model to learn suffixation, prefixation, infixation, circumfixation, mutation, template, and deletion rules. Separate network modules responsible for syllables enable to the network to learn simple reduplication rules as well. The model also embodies constraints against association-line crossing. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Garrison W. Cottrell and Kim Plunkett. </author> <title> Learning the Past Tense in a Recurrent Network: Acquiring the Mapping from Meaning to Sounds. </title> <booktitle> Annual Conference of the Cognitive Science Society, </booktitle> <pages> 13 , 328-333, </pages> <year> 1991. </year>
Reference-contexts: Most work which has viewed the acquisition of morphology in this way, e.g., <ref> [1] </ref>, has taken the perspective of production. But a human language learner almost certainly learns to understand polymorphemic words before learning to produce them, and production may need to build on perception [6]. Thus it seems reasonable to begin with a model of the acquisition of receptive morphology.
Reference: [2] <author> Jeffrey Elman. </author> <title> Finding structure in time. </title> <journal> Cognitive Science, </journal> <volume> 14 . 179-211. </volume> <pages> 8 </pages>
Reference-contexts: The model to be described here is one of the simpler possible networks of this type, a version of the simple recurrent network due to <ref> [2] </ref>. The Version 1 network is shown in Figure 1 Each box represents a layer of connectionist processing units and each arrow a complete set of weighted connections between two layers. The network operates as follows. A sequence of phones is presented to the input layer one at a time.
Reference: [3] <author> Jeffrey L. Elman. </author> <title> Distributed representations, sim-ple recurrent networks, and grammatical structure. </title> <booktitle> Machine Learning, </booktitle> <pages> 7 , 195-225, </pages> <year> 1991. </year>
Reference-contexts: This 15-dimensional space is impossible to observe directly, but we can get an idea of the most significant movements through this space through the use of principal component analysis, a technique which is by now a familiar way of analyzing the behavior of recurrent networks <ref> [3, 9] </ref>. Given a set of data vectors, principal component analysis yields a set of orthogonal vectors, or components, which are ranked in terms of how much of the variance in the data they account for.
Reference: [4] <author> Michael Gasser. </author> <title> Learning distributed syllable representations. </title> <booktitle> Annual Conference of the Cognitive Science Society, </booktitle> <pages> 14 , 396-401, </pages> <year> 1992. </year>
Reference-contexts: This paper describes a psychologically motivated connectionist model (Modular Connectionist Network for the Acquisition of Morphology, MCNAM) which shows evidence of acquiring all of the basic rule types and which also experiences relative difficulty learning rules which seem not to be possible. In another paper <ref> [4] </ref>, I show how the representations that develop during the learning of root and inflection identification can support word production. Although still tentative in several respects, MCNAM appears to be the first computational model of the acquisition of receptive morphology to apply to this diversity of morphological rules. <p> When presented to the network, each syllable sequence was followed by a boundary segment. The hidden-layer pattern appearing at the end of each syllable-plus-boundary sequence was then treated as a static representation of the syllable sequence for a second task. Previous work <ref> [4] </ref> has shown that these representations embody the structure of the input sequences in ways which permit generalizations. In this case, the sort of generalization which interests us concerns the recognition of similarities between syllables with the same onsets or rimes.
Reference: [5] <author> Michael Gasser. </author> <title> Modularity in a connectionist model of morphology acquisition, </title> <booktitle> Proceedings of the International Conference on Computational Linguistics, </booktitle> <month> 15 , </month> <year> 1994. </year>
Reference-contexts: In this paper, I first describe a version of the model which is modular with respect to the identification of root and inflections. The advantages of this version over the simpler model in which these tasks are shared by the same hidden layer is described in a separate paper <ref> [5] </ref>. Later I discuss a version of the model which incorporates modularity at the level of the syllable and metrical foot; this is required to learn reduplication. The model described here is connectionist. There are several reasons why one might want to investigate language acquisition from the perspective of connectionism.
Reference: [6] <author> Michael Gasser. </author> <title> Learning words in time: Towards a modular connectionist account of the acquisition of receptive morphology. </title> <type> Technical Report 384, </type> <institution> Indiana University, Computer Science Department, Bloom-ington, </institution> <year> 1993. </year>
Reference-contexts: Most work which has viewed the acquisition of morphology in this way, e.g., [1], has taken the perspective of production. But a human language learner almost certainly learns to understand polymorphemic words before learning to produce them, and production may need to build on perception <ref> [6] </ref>. Thus it seems reasonable to begin with a model of the acquisition of receptive morphology. In this paper, I will deal with that component of receptive morphology which takes sequences of phones, each expressed as a vector of phonetic features, and identifies them as particular morphemes.
Reference: [7] <author> William D. Marslen-Wilson and Lorraine K. Tyler. </author> <title> The temporal structure of spoken language understanding. </title> <journal> Cognition, </journal> <volume> 8 , 1-71, </volume> <year> 1980. </year>
Reference-contexts: Words takes place in time, and a psychologically plausible account of word recognition must take this fact into account. Words are often recognized long before they finish; hearers seem to be continuously comparing the contents of a linguistic short-term memory with the phonological representations in their mental lexicons <ref> [7] </ref>. Thus the task at hand requires a short-term memory of some sort. Of the various ways of representing short-term memory in connectionist networks [9], the most flexible approach makes use of recurrent connections on hidden units.
Reference: [8] <author> Kim Plunkett and Virginia Marchman. </author> <title> U-shaped learning and frequency effects in . multi-layered perceptron: Implications for child language acquisition, </title> <journal> Cognition, </journal> <volume> 38 , 1-60, </volume> <year> 1991. </year>
Reference-contexts: While the acquisition of morphology has sometimes been seen as the problem of learning how to transform one linguistic form into another form, e.g., by <ref> [8] </ref> and [10], from the learner's perspective, the problem is one of learning how forms map onto meanings. Most work which has viewed the acquisition of morphology in this way, e.g., [1], has taken the perspective of production.
Reference: [9] <author> Robert Port. </author> <title> Representation and recognition of temporal patterns. </title> <booktitle> Connection Science, </booktitle> <pages> 2 , 151-176, </pages> <year> 1990. </year>
Reference-contexts: Thus the task at hand requires a short-term memory of some sort. Of the various ways of representing short-term memory in connectionist networks <ref> [9] </ref>, the most flexible approach makes use of recurrent connections on hidden units. This has the effect of turning the hidden layer into a short-term memory which is not bounded by a fixed limit on the length of the period it can store. <p> This 15-dimensional space is impossible to observe directly, but we can get an idea of the most significant movements through this space through the use of principal component analysis, a technique which is by now a familiar way of analyzing the behavior of recurrent networks <ref> [3, 9] </ref>. Given a set of data vectors, principal component analysis yields a set of orthogonal vectors, or components, which are ranked in terms of how much of the variance in the data they account for.
Reference: [10] <author> David E. Rumelhart and James L. McClelland. </author> <title> On learning the past tense of English verbs. </title> <editor> In James L. McClelland and David E. Rumelhart (Eds.), </editor> <booktitle> Parallel Distributed Processing, </booktitle> <pages> Volume 2 , pages 216-271. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: While the acquisition of morphology has sometimes been seen as the problem of learning how to transform one linguistic form into another form, e.g., by [8] and <ref> [10] </ref>, from the learner's perspective, the problem is one of learning how forms map onto meanings. Most work which has viewed the acquisition of morphology in this way, e.g., [1], has taken the perspective of production.
Reference: [11] <author> David E. Rumelhart and Geoffrey Hinton and Ronald Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In James L. McClelland and David E. Rumelhart (Eds.), </editor> <booktitle> Parallel Distributed Processing, </booktitle> <pages> Volume 1 , pages 318-364. </pages> <publisher> MIT Press, </publisher> <address> Cam-bridge, MA, </address> <year> 1986. </year> <month> 9 </month>
Reference-contexts: The phone target is identical to the input phone. The root and inflection targets, which are constant throughout the presentation of a word, are the patterns associated with the root and inflection for the input word. The network is trained using the backpropagation learning algorithm <ref> [11] </ref>, which adjusts the weights on the network's connections in such a way as to minimize the error, that is, the difference between the network's outputs and the targets. For each morphological rule, a separate network is trained on a subset of the possible combinations of root and inflection.
References-found: 11

