URL: http://robotics.stanford.edu/users/jek/Papers/aaai94_readable.ps.gz
Refering-URL: http://robotics.stanford.edu/users/jek/bio.html
Root-URL: http://www.cs.stanford.edu
Email: jek@cs.stanford.edu  
Title: The Impact of Locality and Authority on Emergent Conventions: Initial Observations  
Author: James E. Kittock 
Note: Appears in: Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI '94), pages 420-425, 1994. Available as: http://robotics.stanford.edu/people/jek/Papers/aaai94.ps This research was supported in part by the Air Force Office of Scientific Research under grant number F49620-92-J-0547-P00001 and by the National Science Foundation under grant number IRI-9220645.  
Address: Stanford, CA 94305  
Affiliation: Robotics Laboratory Computer Science Department Stanford University  
Abstract: In the design of systems of multiple agents, we must deal with the potential for conflict that is inherent in the interactions among agents; to ensure efficient operation, these interactions must be coordinated. We extend, in two related ways, an existing framework that allows behavioral conventions to emerge in agent societies. We first consider localizing agents, thus limiting their interactions. We then consider giving some agents authority over others by implementing asymmetric interactions. Our primary interest is to explore how locality and authority affect the emergence of conventions. Through computer simulations of agent societies of various configurations, we begin to develop an intuition about what features of a society promote or inhibit the spontaneous generation of coordinating conventions.
Abstract-found: 1
Intro-found: 1
Reference: [ Glance and Huberman, 1993 ] <author> Natalie S. Glance and Bernardo Huberman. </author> <title> 13 Organizational fluidity and sustainable cooperation. </title> <booktitle> In Decentralized A.I. 4: Proceedings of the Fourth European Workshop on Modelling Autonomous Agents in a Multi-Agent World. </booktitle> <publisher> In press, </publisher> <year> 1993. </year>
Reference-contexts: While our model incorporates adaptive agents that learn purely from experience, many researchers have taken the view that agents should be treated as "rational" in the game-theoretic sense, choosing their actions based on some expected outcome ( [ Stary, 1993 ] is an overview of these approaches). In <ref> [ Glance and Huberman, 1993 ] </ref> , Glance and Huber-man approach closest to our work, exploring the effects of a dynamic social structure on cooperation between agents.
Reference: [ Kandori et al., 1993 ] <author> M. Kandori, G. Mailath, and R. Rob. </author> <title> Learning, mutation and long equilibria in games. </title> <journal> Econometrica, </journal> <volume> 61 </volume> <pages> 29-56, </pages> <year> 1993. </year>
Reference-contexts: Our notion of locality differs because it is not based on an expectation of interaction: it is based on the actual occurrence or non-occurrence of interactions. This research bears a close resemblence to work in economics and game theory <ref> [ Kandori et al., 1993 ] </ref> . One of our current goals is to gain a better understanding of this relationship.
Reference: [ Lindgren, 1992 ] <author> Kristian Lindgren. </author> <title> Evolutionary phenomena in simple dynamics. </title> <booktitle> In Artificial Life II: Proceedings of the 1990 Artificial Life Workshop. </booktitle> <address> Santa Fe Institute, </address> <publisher> Addison-Wesley Publishing Co., </publisher> <year> 1992. </year>
Reference-contexts: One of our current goals is to gain a better understanding of this relationship. More generally, research on adaptive multi-agent systems appears to have ties to work in artificial life <ref> [ Lindgren, 1992 ] </ref> , population genetics [ Mettler et al., 1988 ] , and quantitative sociology [ Weidlich and Haag, 1983 ] . However, while systems from these diverse fields share characteristics such as distributed components and complex dynamics, their particulars remain unreconciled.
Reference: [ Mettler et al., 1988 ] <author> Lawrence E. Mettler, Thomas G. Gregg, and Henry E. Schaffer. </author> <title> Population Genetics and Evolution. </title> <publisher> Prentice Hall, </publisher> <address> second edition, </address> <year> 1988. </year>
Reference-contexts: One of our current goals is to gain a better understanding of this relationship. More generally, research on adaptive multi-agent systems appears to have ties to work in artificial life [ Lindgren, 1992 ] , population genetics <ref> [ Mettler et al., 1988 ] </ref> , and quantitative sociology [ Weidlich and Haag, 1983 ] . However, while systems from these diverse fields share characteristics such as distributed components and complex dynamics, their particulars remain unreconciled.
Reference: [ Shoham and Tennenholtz, 1992a ] <author> Yoav Shoham and Moshe Tennenholtz. </author> <title> Emergent conventions in multi-agent systems: initial experimental results and observations. </title> <booktitle> In KR-92, </booktitle> <year> 1992. </year>
Reference-contexts: It has been shown that it is possible for an agent society to reach a convention without any centralized control if agents interact and learn from their experiences <ref> [ Shoham and Tennenholtz, 1992a ] </ref> . <p> achieved have been called "emergent conventions", and the process for reaching them has been dubbed "co-learning." [ Shoham and Tennenholtz, 1993 ] In previous work on the emergence of conventions through co-learning, it was assumed that each agent in a society is equally likely to interact with any other agent <ref> [ Shoham and Tennenholtz, 1992a ] </ref> . This seems an unreasonable assumption in the general case, and we consider ways to extend the framework by allowing for non-uniform interaction probabilities.
Reference: [ Shoham and Tennenholtz, 1992b ] <author> Yoav Shoham and Moshe Tennenholtz. </author> <title> On the synthesis of useful social laws for artificial agent societies. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence (AAAI '92). </booktitle> <publisher> AAAI, MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: In general, designing all necessary conventions into a system or developing a centralized control mechanism to legislate new conventions is a difficult and perhaps intractable task <ref> [ Shoham and Tennenholtz, 1992b ] </ref> . It has been shown that it is possible for an agent society to reach a convention without any centralized control if agents interact and learn from their experiences [ Shoham and Tennenholtz, 1992a ] .
Reference: [ Shoham and Tennenholtz, 1993 ] <author> Yoav Shoham and Moshe Tennenholtz. </author> <title> Co-learning and the evolution of social activity. </title> <note> Submitted for publication, </note> <year> 1993. </year>
Reference-contexts: Conventions thus achieved have been called "emergent conventions", and the process for reaching them has been dubbed "co-learning." <ref> [ Shoham and Tennenholtz, 1993 ] </ref> In previous work on the emergence of conventions through co-learning, it was assumed that each agent in a society is equally likely to interact with any other agent [ Shoham and Tennenholtz, 1992a ] . <p> For our preliminary investigations, we have chosen to use a learning rule similar to the Highest Cumulative Reward rule used by Shoham and Tennenholtz <ref> [ Shoham and Tennenholtz, 1993 ] </ref> . To decide which strategy it will use, an agent first computes the cumulative reward for each strategy by summing the feedback from all interactions in which it used that strategy and then chooses the strategy with the highest cumulative reward (HCR).
Reference: [ Stary, 1993 ] <author> Chris Stary. </author> <title> Dynamic modelling of collaboration among rational agents: redefining the research agenda. </title> <booktitle> In IFIP Transactions A (Computer Science and Technology), volume A-24. Human, Organizational and Social Dimensions of Information Systems Development. IFIP WG8.2 Working Group, </booktitle> <year> 1993. </year>
Reference-contexts: While our model incorporates adaptive agents that learn purely from experience, many researchers have taken the view that agents should be treated as "rational" in the game-theoretic sense, choosing their actions based on some expected outcome ( <ref> [ Stary, 1993 ] </ref> is an overview of these approaches). In [ Glance and Huberman, 1993 ] , Glance and Huber-man approach closest to our work, exploring the effects of a dynamic social structure on cooperation between agents.
Reference: [ Watkins, 1989 ] <author> Christopher Watkins. </author> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> King's College, </institution> <year> 1989. </year>
Reference-contexts: assume neither that agents are "rational" nor that they can access the contents of the matrix directly. 2 We limit our discussion here to the two-strategy case, but the results are qualitatively similar when more strategies are available to agents. 4 including more sophisticated reinforcement learning techniques such as Q--learning <ref> [ Watkins, 1989 ] </ref> ; however, using the simpler HCR rule fits with our program of starting with a simpler system.
Reference: [ Weidlich and Haag, 1983 ] <author> W. Weidlich and G. Haag. </author> <title> Concepts and Models of a Quantitative Sociology; The Dynamics of Interacting Populations. </title> <publisher> Springer-Verlag, </publisher> <year> 1983. </year> <month> 14 </month>
Reference-contexts: One of our current goals is to gain a better understanding of this relationship. More generally, research on adaptive multi-agent systems appears to have ties to work in artificial life [ Lindgren, 1992 ] , population genetics [ Mettler et al., 1988 ] , and quantitative sociology <ref> [ Weidlich and Haag, 1983 ] </ref> . However, while systems from these diverse fields share characteristics such as distributed components and complex dynamics, their particulars remain unreconciled.
References-found: 10

