URL: ftp://ftp.engr.orst.edu/pub/dambrosi/uai-96.ps.Z
Refering-URL: http://www.cs.orst.edu/~dambrosi/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: dambrosi@research.cs.orst.edu  burgess@research.cs.orst.edu  
Title: Some Experiments with Real-time Decision Algorithms  
Author: Bruce DAmbrosio Scott Burgess 
Address: Corvallis, Oregon 97331  Corvallis, Oregon 97331  
Affiliation: Computer Science Oregon State University  Computer Science Oregon State University  
Abstract: Real-time Decision algorithms are a class of incremental resource-bounded [Horvitz, 89] or anytime [Dean, 93] algorithms for evaluating influence diagrams. We present a test domain for real-time decision algorithms, and the results of experiments with several Real-time Decision Algorithms in this domain. The results demonstrate high performance for two algorithms, a decision-evaluation variant of Incremental Probabilisitic Inference [DAmbrosio, 93] and a variant of an algorithm suggested by Goldszmidt, [Goldszmidt, 95], PK-reduced. We discuss the implications of these experimental results and explore the broader applicability of these algorithms.
Abstract-found: 1
Intro-found: 1
Reference: [Cassandra, Kaelbling, & Littman, 94] <institution> Acting Optimally in Partially Observable Stochastic Domains. Brown Univ. Tech. </institution> <note> Report CS-94-20 [DAmbrosio, 92]. </note> <author> B. DAmbrosio. </author> <title> Valuedriven real-time diagnosis. </title> <booktitle> In Proceedings, Third International Workshop on the Principles of Diagnosis, </booktitle> <month> October </month> <year> 1992. </year> <note> [DAmbrosio, 93]. </note> <author> B. DAmbrosio. </author> <title> Incremental Probabilistic Inference. </title> <booktitle> In Proceedings of the Ninth Annual Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pp 301-308, </pages> <address> July 1993. </address> <publisher> Morgan Kaufmann, Publishers. </publisher> <address> [DAmbrosio, </address> <note> 95], </note> <author> B. D'Ambrosio, </author> <title> Local Expression Languages for Probabilistic Dependence. </title> <journal> International Journal of Approximate Reasoning, </journal> <volume> 13, #1, </volume> <pages> pp. 61-81, </pages> <month> July, </month> <year> 1995. </year>
Reference: [Dean, 93] <author> T. Dean and M. Boddy. </author> <title> An Analysis of Time-Dependent Planning. </title> <booktitle> In AAAI88, </booktitle> <pages> pp 49-54. </pages>
Reference-contexts: Introduction The problem A variety of algorithms have been proposed as candidates for anytime <ref> [Dean, 93] </ref> or resource-bounded [Horvitz et al, 89] inference, including [DAmbrosio, 93], [Horvitz et al, 89b], and a variety of simulation-based algorithms such as [Fung, 89]. The need for such algorithms arises because implementable agents have finite computing resources [Russell, 91].
Reference: [Draper & Hanks, 94] <editor> Localized Partial Evaluation of Belief Networks. </editor> <booktitle> In Proceedings of the Tenth Conference on Uncertainty in AI, </booktitle> <pages> pp 170-177. </pages>
Reference-contexts: Further study is needed to better understand the conditions that enable this. Remember that IPI is a search-based algorithm that proceeds by instantiating variables in the network - we call each such instantiation a scenario. Related work <ref> [Draper & Hanks, 94] </ref> investigate localized partial evaluation of Bayesian belief networks to perform anytime inference. [Poole, 93] has done work on the use of conflicts for reducing necessary computation, and [Wellman & Liu, 94] have applied state space abstraction to address resource-bounded computation.
Reference: [Fung & Chang, 89] <author> R. Fung, and K. Chang. </author> <title> Weighing and Integrating Evidence for Stochastic Simulation in Bayesian Networks. </title> <booktitle> In Proceedings of the Fifth Conference on Uncertainty in AI. </booktitle> <pages> pp 112-117, </pages> <month> August, </month> <year> 1989. </year>
Reference-contexts: We likewise will seek competitive forms of stochastic simulation <ref> [Fung & Chang, 89] </ref>, and continue our explorations with the kappa calculus [Goldszmidt, 95]. Conclusions We are interested in developing and characterizing decision algorithms with robust real-time performance. We presented the OnLine Maintenance domain, a domain we think is uniquely suited for effective evaluation of real-time decision methods.
Reference: [Goldszmidt, 95] <author> M. Goldszmidt. </author> <title> Fast Belief Updating Using Order of Magnitude Probabilities. </title> <booktitle> In Uncertainty in Artificial Intelligence, Proceedings of the Eleventh Conference. </booktitle> <pages> pp 208-216. </pages> <publisher> Morgan Kaufmann, Publishers, </publisher> <month> July, </month> <year> 1995. </year> <note> [Horvitz, et al, 89]. </note> <author> E. Horvitz, G. Cooper, and D. Heckerman. </author> <title> Reflection and action under scarce resources: Theoretical principles and empirical study. </title> <booktitle> In Proceedings of IJCAI89. IJCAI, </booktitle> <month> August </month> <year> 1989. </year> <editor> [Horvitz et al, 89b]. E. Horvitz, H. J. Suermondt, and G. Cooper. </editor> <title> Bounded conditioning: Flexible inference for decisions under scarce resources. </title> <booktitle> In Proceedings of the Fifth Conference on Uncertainty in AI , August 1989. </booktitle>
Reference-contexts: In this paper we present experimental results characterizing several promising candidate real-time decision algorithms. We begin with a short review of the set of algorithms we chose to characterize: a search-based algorithm (Incremental Probabilistic Inference, [DAmbrosio, 93]) and two variants of a decision algorithm suggested by Goldszmidt <ref> [Goldszmidt, 95] </ref>. Characterizing such algorithms is nontrivial. We describe the OnLine Maintenance Agent (OLMA) [DAmbrosio, 92], [DAmbrosio, 96], an idealized task domain that has the necessary properties to permit informative experimental estimation of the performance properties of the various algorithms. <p> The Candidate Algorithms Our evaluation focused on two promising approximate decision algorithms we term DIPI and K-reduced. DIPI is an extension of the IPI search algorithm [DAmbrosio, 93] to include decision and value nodes. K-reduced is a use of Goldszmidts fast method of computing prior Kappa values <ref> [Goldszmidt, 95] </ref>. In this section we briefly describe each of these algorithms, as well as several reference algorithms we used to establish benchmark solution values. D-IPI DIPI is a simple extension of the IPI incremental inference algorithm [DAmbrosio, UAI-93]. <p> We likewise will seek competitive forms of stochastic simulation [Fung & Chang, 89], and continue our explorations with the kappa calculus <ref> [Goldszmidt, 95] </ref>. Conclusions We are interested in developing and characterizing decision algorithms with robust real-time performance. We presented the OnLine Maintenance domain, a domain we think is uniquely suited for effective evaluation of real-time decision methods.
Reference: [Li & DAmbrosio, 94] <author> Z. Li and B. DAmbrosio. </author> <title> Efficient Inference in Bayes Nets as a Combinatorial Optimization Problem, </title> <journal> International Journal of Approximate Reasoning, </journal> <volume> 11, 1: </volume> <pages> 55-81, </pages> <year> 1994. </year>
Reference-contexts: In this section we briefly describe each of these algorithms, as well as several reference algorithms we used to establish benchmark solution values. D-IPI DIPI is a simple extension of the IPI incremental inference algorithm [DAmbrosio, UAI-93]. IPI is an incremental search-based variant of the SPI <ref> [Li & DAmbrosio, 94] </ref> algorithm. It first forms a symbolic expression (marginalization over the joint pdf) corresponding to a query. It then constructs an evaluation tree for the query by applying simple algebraic transforms to convert the expression into efficiently evaluable form.
Reference: [Littman, Cassandra, & Kaelbling, 95] <editor> Learning Policies for Partially Observable Environments: </editor> <title> Scaling Up. </title> <booktitle> Machine Learning 12: </booktitle> <pages> 362-370. </pages>
Reference: [Parr & Russell, 95] <editor> Approximating Optimal Policies for Partially Observable Stochastic Domains. </editor> <booktitle> Proceedings IJCAI95, </booktitle> <pages> pp 1088-1094. </pages>
Reference-contexts: Many researchers in machine learning seek optimal or near-optimal policies for POMDPs using variations on value iteration and Q-Learning [Parr & Russell, 95; Jaakkola, Jordan and Singh; Littman, Cassandra, & Kaelbling, 95]. Finding optimal policies for models requiring even tens of states currently stretches the limits of feasible computation <ref> [Parr & Russell, 95] </ref>. Still, these papers demonstrate a marked improvement in the ability to calculate optimal policies. Closer to home, we have begun to investigate POMDP methods for the OLMA domain [DAmbrosio, NIPS96-submitted].
Reference: [Poole, 93] <editor> The Use of Conflicts in Searching Bayesian Networks. </editor> <booktitle> In Proceedings CUAI93, </booktitle> <pages> pp 359-367. </pages>
Reference-contexts: Remember that IPI is a search-based algorithm that proceeds by instantiating variables in the network - we call each such instantiation a scenario. Related work [Draper & Hanks, 94] investigate localized partial evaluation of Bayesian belief networks to perform anytime inference. <ref> [Poole, 93] </ref> has done work on the use of conflicts for reducing necessary computation, and [Wellman & Liu, 94] have applied state space abstraction to address resource-bounded computation.
Reference: [Russell & Wefald, 91] <author> S. Russell and E. Wefald. </author> <title> Do the Right Thing. </title> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference: [Wellman & Liu, 94] <institution> State-Space Abstraction for Anytime Evaluation of Probabilistic Networks, </institution> <note> In Proceedings CUAI 94, pp 567-574 </note>
Reference-contexts: Related work [Draper & Hanks, 94] investigate localized partial evaluation of Bayesian belief networks to perform anytime inference. [Poole, 93] has done work on the use of conflicts for reducing necessary computation, and <ref> [Wellman & Liu, 94] </ref> have applied state space abstraction to address resource-bounded computation. While the ideas are promising, further empirical validation is necessary to demonstrate that these techniques are scalable to and competitive on large, general problems. The OLMA may be viewed as a POMDP.
References-found: 11

