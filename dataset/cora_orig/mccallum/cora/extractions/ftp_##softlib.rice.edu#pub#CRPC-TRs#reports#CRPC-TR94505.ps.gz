URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR94505.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: A Comparison of Different Message-Passing Paradigms for the Parallelization of Two Irregular Applications  
Author: Seungjo Bae and Sanjay Ranka 
Note: Submitted to the Journal of Supercomputing  
Date: July 28, 1994  
Address: Syracuse, NY 13244  
Affiliation: 4-116 Center for Science and Technology School of Computer and Information Science Syracuse University  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> C. Aykanat and T. Kurc. </author> <title> Efficient parallel maze routing algorithm on a hypercube multi computer. </title> <booktitle> In Proc. 1991 Intl. Conf. on Parallel Processing, </booktitle> <volume> Vol 3, </volume> <pages> pages 224-227, </pages> <year> 1991. </year>
Reference-contexts: One is the classical message-passing paradigm using send/receive functions, and the other uses the active messages developed by von Eicken et al. [11]. This paper describes the parallelization of Wolff cluster algorithm [8, 19] and Lee's maze routing algorithm <ref> [15, 12, 20, 1] </ref> using the above message-passing paradigms available on the CM-5. These applications require fine-grained communication (small messages) for efficient parallelization. We have studied the tradeoffs of two optimizations to reduce the overhead of the communication cost for the above applications: 1. <p> Unlike the Wolff cluster algorithm, the grid has four boundaries. The goal is to find a shortest path from a source cell to a destination cell. Some cells are blocked and therefore cannot be on the shortest path [20]. Lee's maze-routing algorithm consists of three phases (Figure 7) <ref> [20, 1] </ref>. The first phase, wavefront expansion, is similar to the ants-in-the-labyrinth algorithm and uses a breadth-first-search for expanding the wavefront. However, in Lee's maze-routing algorithm any two neighboring cells in a grid are connected unless one of the sites is blocked.
Reference: [2] <author> Seungjo Bae, Sung-Hoon Ko, and Paul Coddington. </author> <title> Parallel Wolff Cluster Algorithm. </title> <type> Tech nical Report SCCS-619, </type> <institution> Northeast Parallel Architectures Center, Syracuse University, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: Effective parallelization requires maintaining good load balance while minimizing communication. We used the cyclic column-wise striped partitioning (Figure 10) scheme for vertically partitioning the lattice in both algorithms <ref> [2, 14, 12] </ref>. Let the width of each partition be w.
Reference: [3] <author> Clive F. Baillie and Paul D. Coddington. </author> <title> Cluster Identification Algorithms for Spin Models| Sequential and Parallel. </title> <type> Technical Report C 3 P -855, </type> <institution> Caltech, </institution> <month> June </month> <year> 1990. </year>
Reference-contexts: One method used for growing a cluster in the Wolff cluster algorithm is the ants-in-the-labyrinth algorithm. This algorithm is similar to breadth-first search on an undirected graph <ref> [3, 8] </ref>. An initial site which acts as the first element of a cluster is selected at random from a lattice and an ant is placed on it. The ant propagates by placing a child on each of its four neighboring sites with bond activation probability p. <p> Both algorithms use the same bond activation probabilities [13, 17, 18]. The sequential algorithm for the Wolff cluster algorithm is more efficient than the Swendsen-Wang algorithm <ref> [3] </ref>, but it is more difficult to parallelize the Wolff cluster algorithm because it involves only a single cluster, while the Swendsen-Wang algorithm involves the entire lattice [3, 8]. <p> The sequential algorithm for the Wolff cluster algorithm is more efficient than the Swendsen-Wang algorithm [3], but it is more difficult to parallelize the Wolff cluster algorithm because it involves only a single cluster, while the Swendsen-Wang algorithm involves the entire lattice <ref> [3, 8] </ref>. Let the cluster size be M (the number of sites in a cluster). 2 The sequential ants algorithm takes time fi (M ). The size of a cluster depends on the bond activation probability p. The value of p depends on the value of fi (Figure 4).
Reference: [4] <author> G. Brassard and P. Bratley. </author> <title> Algorithmics: Theory and Practice. </title> <publisher> Prentice Hall, </publisher> <year> 1987. </year>
Reference-contexts: The cluster continues to expand until no more ants are produced [8]. For the sequential ants algorithm we need queues that allow two operations, enqueue and dequeue. The pseudo code 1 is shown in Figure 6, <ref> [4] </ref>. The difference between this algorithm and breadth-first-search is that there is no global adjacency matrix (or list). When a site is removed from a queue, the bond is generated dynamically between the site and its neighbors. The Wolff cluster algorithm is a variation on the Swendsen-Wang cluster algorithm [16]. <p> Each cell on the current wavefront should check its four neighbors to determine if they are already visited. Since the depth is D after the expansion is finished, it takes time fi (D (D + 1)) = fi (D 2 ) <ref> [4] </ref>. In the path-recovery phase every cell on the path is visited, so this phase takes time fi (D).
Reference: [5] <author> E. A. Brewer and R. D. Blumofe. Strata: </author> <title> A Multi-Layer Communication Library. </title> <note> Technical Report (to appear), </note> <institution> MIT Laboratory for Computer Science, </institution> <month> February </month> <year> 1994. </year>
Reference-contexts: Strata is compatible with CMMD as well as with CMAML and has been shown to have slightly lower communication overhead than the CMAML implementation <ref> [5, 6] </ref>. The active messages implementation on the CM-5 (CMAML and Strata) has been shown to have a significantly lower startup cost than send/receive message-passing functions, a reduction achieved largely by removing the handshaking required in previous send/receive-based protocol. <p> CM Active Message Layer (CMAML), a protocol-less transport layer on which the high-level CMMD functions are built [10], and 2. Strata, which is the multi-layer communications library developed at MIT and is compatible with CMMD as well as with CMAML <ref> [5, 6] </ref>. CMAML and Strata provide two kinds of message-transfer functions, Active Messages and Block Data Transfer (Active Messages-PUT), which is similar to PUT in Split-C developed in [11]. <p> Strata provides two more transfer functions in addition to the three CMAML active messages mentioned above. Each of these five functions specifies which interface of the data network is used when sending or polling active messages. Strata active messages have been explained in detail in <ref> [5] </ref>. Receiving Active Messages On the receiving node any incoming active messages can be received using either automatic interrupt or explicit polling. In either case, active messages are fetched from the data network and corresponding handler functions are invoked immediately [10]. <p> CMAML and Strata provide various polling functions, each of which uses the left, right, or both interfaces to the data network for receiving incoming active messages. Strata provides a wider range of polling functions <ref> [10, 5] </ref>. Most of CMAML and Strata active message functions alternate trying to send an active message with polling any incoming active message. Therefore, if a processor is sending active messages as well as receiving active messages (as in circular shift), 16 explicit polling is not always necessary. <p> Strata provides two active messages-PUT functions. These functions use both interfaces when trying to send data. When trying to receive data, one polls both interfaces, but the other polls only the right interface <ref> [5] </ref>. Block data is transferred in a series of single-packet active messages. Since packet ordering is not guaranteed on the CM-5 network [11], each message header contains the offset of data and an rport number. <p> This handler function stores data (four words) in the corresponding rport according to the offset of data. After all messages are received, user-defined handler function is invoked, and this handler function transfers data from the rport to the local communication buffer <ref> [5, 10] </ref>. Active messages and active messages-PUT provide the application programmer a great deal of flexibility.
Reference: [6] <author> E. A. Brewer and B. C. Kuszmaul. </author> <title> How to get good performance from the CM-5 data network. </title> <booktitle> In International Parallel Processing Symposium, </booktitle> <year> 1994. </year>
Reference-contexts: Strata is compatible with CMMD as well as with CMAML and has been shown to have slightly lower communication overhead than the CMAML implementation <ref> [5, 6] </ref>. The active messages implementation on the CM-5 (CMAML and Strata) has been shown to have a significantly lower startup cost than send/receive message-passing functions, a reduction achieved largely by removing the handshaking required in previous send/receive-based protocol. <p> The network interface has no DMA and accepts only five-words-per-packet messages (one word for the message head and four words for data). If a message size is larger than four words, a processor should first packetize the message and then transfer data to the network interface packet by packet <ref> [6, 9] </ref>. Overlapping communication with computation is limited on the CM-5 due to the low network capacity (average ten packets) and network latency [6]. 4.2 Send/Receive-Based Message Passing CMMD supports two kinds of message-passing functions, synchronous message passing and asynchronous message passing. <p> Overlapping communication with computation is limited on the CM-5 due to the low network capacity (average ten packets) and network latency <ref> [6] </ref>. 4.2 Send/Receive-Based Message Passing CMMD supports two kinds of message-passing functions, synchronous message passing and asynchronous message passing. In the synchronous send/receive case, a blocking-send function and a blocking-receive function are used; both a sender and a receiver are synchronized using three-phase 12 protocol. <p> CM Active Message Layer (CMAML), a protocol-less transport layer on which the high-level CMMD functions are built [10], and 2. Strata, which is the multi-layer communications library developed at MIT and is compatible with CMMD as well as with CMAML <ref> [5, 6] </ref>. CMAML and Strata provide two kinds of message-transfer functions, Active Messages and Block Data Transfer (Active Messages-PUT), which is similar to PUT in Split-C developed in [11]. <p> If the receiving node can predict when the message will arrive explicit polling is more suitable and efficient. Polling has much lower overhead than automatic interrupt has. When any incoming active messages are received via explicit polling, one should poll often to prevent network congestion <ref> [10, 6] </ref>. CMAML and Strata provide various polling functions, each of which uses the left, right, or both interfaces to the data network for receiving incoming active messages. Strata provides a wider range of polling functions [10, 5]. <p> Overlapping communication (actual transmission without the idle time resulting from handshaking) with computation is limited on the CM-5 due to the low network capacity and network latency <ref> [6] </ref>. 32 Scheme Size of code Ease of programming Performance SSR-C-NO 2 1 3 ASR-C-O 4 2 4 Smaller values imply a smaller sized code, relative ease of programming, and better performance, respectively.
Reference: [7] <author> F. T. Chong, S. D. Sharma, E. A. Brewer, and J. Saltz. </author> <title> Multiprocessor Runtime Support for Fine-Grained, Irregular DAGs. </title> <note> Submitted for publication, </note> <year> 1994. </year>
Reference-contexts: Therefore, if a processor is sending active messages as well as receiving active messages (as in circular shift), 16 explicit polling is not always necessary. One of the exceptions is CMAML RPC, which does not poll at all unless it fails to send an active message <ref> [10, 7] </ref>. Block Data Transfer Data can be transferred in blocks on CMAML using Active Messages-PUT. There is neither internal two-way handshaking nor an explicit receive function. The following steps are needed between a sender and a receiver before actual transfer of data [10]: 1.
Reference: [8] <author> Paul D. Coddington. </author> <note> Lecture note. Lecture notes for the course CPS713 at Syracuse Uni versity, Spring 1993. </note>
Reference-contexts: 1 Introduction Two different message-passing paradigms are available on the CM-5 for point-to-point communication. One is the classical message-passing paradigm using send/receive functions, and the other uses the active messages developed by von Eicken et al. [11]. This paper describes the parallelization of Wolff cluster algorithm <ref> [8, 19] </ref> and Lee's maze routing algorithm [15, 12, 20, 1] using the above message-passing paradigms available on the CM-5. These applications require fine-grained communication (small messages) for efficient parallelization. <p> Section 6 presents the experimental results. 2 Applications 2.1 Wolff Cluster Algorithm The Wolff cluster algorithm is a single-cluster Monte Carlo algorithm for the Ising model <ref> [8, 19] </ref>. The algorithm [8, 16, 19, 13] is given in Figure 4 and a simple example is presented in Figure 5 which assumes that the bond activation probability p is one (a site is always connected with its neighboring site if both sites have the same spin values). <p> Section 6 presents the experimental results. 2 Applications 2.1 Wolff Cluster Algorithm The Wolff cluster algorithm is a single-cluster Monte Carlo algorithm for the Ising model [8, 19]. The algorithm <ref> [8, 16, 19, 13] </ref> is given in Figure 4 and a simple example is presented in Figure 5 which assumes that the bond activation probability p is one (a site is always connected with its neighboring site if both sites have the same spin values). <p> One method used for growing a cluster in the Wolff cluster algorithm is the ants-in-the-labyrinth algorithm. This algorithm is similar to breadth-first search on an undirected graph <ref> [3, 8] </ref>. An initial site which acts as the first element of a cluster is selected at random from a lattice and an ant is placed on it. The ant propagates by placing a child on each of its four neighboring sites with bond activation probability p. <p> Each ant in the first generation of ants checks each of four neighboring sites in turn and places a child of its own with probability p on any unoccupied site. The cluster continues to expand until no more ants are produced <ref> [8] </ref>. For the sequential ants algorithm we need queues that allow two operations, enqueue and dequeue. The pseudo code 1 is shown in Figure 6, [4]. The difference between this algorithm and breadth-first-search is that there is no global adjacency matrix (or list). <p> The sequential algorithm for the Wolff cluster algorithm is more efficient than the Swendsen-Wang algorithm [3], but it is more difficult to parallelize the Wolff cluster algorithm because it involves only a single cluster, while the Swendsen-Wang algorithm involves the entire lattice <ref> [3, 8] </ref>. Let the cluster size be M (the number of sites in a cluster). 2 The sequential ants algorithm takes time fi (M ). The size of a cluster depends on the bond activation probability p. The value of p depends on the value of fi (Figure 4).
Reference: [9] <institution> Thinking Machines Corporation. </institution> <type> CM-5 Technical Summary, </type> <month> November </month> <year> 1993. </year>
Reference-contexts: For receiving data, a receiving processor either polls data by checking the incoming FIFO or is notified by interrupt on arrival of data. <ref> [9] </ref>. <p> The network interface has no DMA and accepts only five-words-per-packet messages (one word for the message head and four words for data). If a message size is larger than four words, a processor should first packetize the message and then transfer data to the network interface packet by packet <ref> [6, 9] </ref>. Overlapping communication with computation is limited on the CM-5 due to the low network capacity (average ten packets) and network latency [6]. 4.2 Send/Receive-Based Message Passing CMMD supports two kinds of message-passing functions, synchronous message passing and asynchronous message passing.
Reference: [10] <institution> Thinking Machines Corporation. </institution> <note> CMMD version 3.0 Reference manual, </note> <month> May </month> <year> 1993. </year>
Reference-contexts: There are two implementations of the active messages paradigm available on the CM-5: CMMD Active Message Layer (CMAML), which is the protocol-less transport layer on which the CMMD functions are built <ref> [10] </ref>, and Strata, which is the multi-layer communications library developed at MIT. Strata is compatible with CMMD as well as with CMAML and has been shown to have slightly lower communication overhead than the CMAML implementation [5, 6]. <p> For receiving data, a receiving processor either polls data by checking the incoming FIFO or is notified by interrupt on arrival of data. [9]. Data transfer between a sending processor and a network interface can be performed in two different ways <ref> [10] </ref>: described in Section 5. 11 1 procedure Parallel-Wolff-Cluster 2 fStep 1: Selection of an initial random site followed by a broadcastingg 3 if pnum = 0 then 4 select an initial random site S, and broadcast it 5 else 6 receive information about the selected random site S from processor <p> In the synchronous send/receive case, a blocking-send function and a blocking-receive function are used; both a sender and a receiver are synchronized using three-phase 12 protocol. In the asynchronous send/receive case, a nonblocking send function and a nonblocking receive function are used <ref> [10] </ref>. 7 Synchronous Send/Receive To establish a communication link between a sender and a receiver when using the synchronous message-passing function, a two-way handshake is required as a preliminary step. First, a sender sends a Request signal to the receiver and waits for its reply. <p> When the receiver receives the sender's request, it sends its Reply signal back to the sender. After the two-way handshake is finished, the sender and the receiver are synchronized implicitly. The sender then prepares its message and transmits it to the receiver <ref> [10] </ref>. The time taken by the two-way handshake is overhead on both a sender and a receiver, and this time should be added to the software overhead. <p> On the receiver the arrival of a message causes an interrupt. Once all messages arrive and are stored in local memory, the control is returned into the main thread <ref> [10] </ref>. Unlike synchronous message passing, a node is not blocked while it waits for its partner to be ready. Therefore, it is possible to do useful work during that period. <p> On the other hand, here they are used to define different ways for cooperating a sender and a receiver. 13 14 so that the sender can modify its original buffer. However, efficiency is decreased because of the extra time required to copy the original buffer into a temporary buffer <ref> [10] </ref>. 4.3 Active Messages There are at least two different implementations of Active Messages available on the CM-5: 1. CM Active Message Layer (CMAML), a protocol-less transport layer on which the high-level CMMD functions are built [10], and 2. <p> of the extra time required to copy the original buffer into a temporary buffer <ref> [10] </ref>. 4.3 Active Messages There are at least two different implementations of Active Messages available on the CM-5: 1. CM Active Message Layer (CMAML), a protocol-less transport layer on which the high-level CMMD functions are built [10], and 2. Strata, which is the multi-layer communications library developed at MIT and is compatible with CMMD as well as with CMAML [5, 6]. <p> By using a handler function associated with every active message we can integrate a communication message into computation [11]. Since there is no buffering on a receiving processor the handler function is invoked immediately when an active message is received by interrupt or polling <ref> [10] </ref>. The block data transfer (Active Messages-PUT) function transfers a continuous data block from one source processor to a single destination processor. Unlike CMMD send and receive functions, a source node and a destination node should agree as to the size of the data block. <p> Unlike CMMD send and receive functions, a source node and a destination node should agree as to the size of the data block. A handler function may be used in both the source processor and in the destination processor. The startup latency is much smaller than the send/receive functions <ref> [10] </ref>. Sending Active Messages Unlike synchronous and asynchronous message-passing functions, two-way handshaking is not required when using active messages, since there is no explicit receive function on a receiver. For this reason the receiver does not identify the sender when receiving active messages. <p> An active message consists of the address of the handler function executed on a receiver and arguments to be passed to the remote function. On a receiver, receipt of an active message causes a corresponding handler function to be invoked immediately <ref> [10] </ref>. CMAML provides three types of active messages on either network interface [10]: 8 A packet consists of five words, and one word is four bytes on the CM-5. 15 * Request message: A request message can be sent from the main thread of control using left interface to the data <p> On a receiver, receipt of an active message causes a corresponding handler function to be invoked immediately <ref> [10] </ref>. CMAML provides three types of active messages on either network interface [10]: 8 A packet consists of five words, and one word is four bytes on the CM-5. 15 * Request message: A request message can be sent from the main thread of control using left interface to the data network. <p> In cases when a request-reply communication pattern is not required, RPC messages can be used instead to give better performance since they use both interfaces <ref> [10] </ref>. Strata provides two more transfer functions in addition to the three CMAML active messages mentioned above. Each of these five functions specifies which interface of the data network is used when sending or polling active messages. Strata active messages have been explained in detail in [5]. <p> Strata active messages have been explained in detail in [5]. Receiving Active Messages On the receiving node any incoming active messages can be received using either automatic interrupt or explicit polling. In either case, active messages are fetched from the data network and corresponding handler functions are invoked immediately <ref> [10] </ref>. If the receiving node can predict when the message will arrive explicit polling is more suitable and efficient. Polling has much lower overhead than automatic interrupt has. When any incoming active messages are received via explicit polling, one should poll often to prevent network congestion [10, 6]. <p> If the receiving node can predict when the message will arrive explicit polling is more suitable and efficient. Polling has much lower overhead than automatic interrupt has. When any incoming active messages are received via explicit polling, one should poll often to prevent network congestion <ref> [10, 6] </ref>. CMAML and Strata provide various polling functions, each of which uses the left, right, or both interfaces to the data network for receiving incoming active messages. Strata provides a wider range of polling functions [10, 5]. <p> CMAML and Strata provide various polling functions, each of which uses the left, right, or both interfaces to the data network for receiving incoming active messages. Strata provides a wider range of polling functions <ref> [10, 5] </ref>. Most of CMAML and Strata active message functions alternate trying to send an active message with polling any incoming active message. Therefore, if a processor is sending active messages as well as receiving active messages (as in circular shift), 16 explicit polling is not always necessary. <p> Therefore, if a processor is sending active messages as well as receiving active messages (as in circular shift), 16 explicit polling is not always necessary. One of the exceptions is CMAML RPC, which does not poll at all unless it fails to send an active message <ref> [10, 7] </ref>. Block Data Transfer Data can be transferred in blocks on CMAML using Active Messages-PUT. There is neither internal two-way handshaking nor an explicit receive function. The following steps are needed between a sender and a receiver before actual transfer of data [10]: 1. <p> Block Data Transfer Data can be transferred in blocks on CMAML using Active Messages-PUT. There is neither internal two-way handshaking nor an explicit receive function. The following steps are needed between a sender and a receiver before actual transfer of data <ref> [10] </ref>: 1. Two nodes should agree which receive port (say rport) will be used. The receive port is a data structure containing all information. Since there is no explicit receive function, the sender should send data to the rport agreed upon by both nodes. 2. <p> This handler function stores data (four words) in the corresponding rport according to the offset of data. After all messages are received, user-defined handler function is invoked, and this handler function transfers data from the rport to the local communication buffer <ref> [5, 10] </ref>. Active messages and active messages-PUT provide the application programmer a great deal of flexibility.
Reference: [11] <author> T. Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser. </author> <title> Active Messages: a mechanism for integrated communication and computation. </title> <booktitle> In Proc. of ISCA 1992, </booktitle> <pages> pages 256-266, </pages> <year> 1992. </year>
Reference-contexts: 1 Introduction Two different message-passing paradigms are available on the CM-5 for point-to-point communication. One is the classical message-passing paradigm using send/receive functions, and the other uses the active messages developed by von Eicken et al. <ref> [11] </ref>. This paper describes the parallelization of Wolff cluster algorithm [8, 19] and Lee's maze routing algorithm [15, 12, 20, 1] using the above message-passing paradigms available on the CM-5. These applications require fine-grained communication (small messages) for efficient parallelization. <p> One is high-level CMMD message passing using send and receive functions which requires three-phase protocol, and the other is Active Messages developed by von Eicken et al. <ref> [11] </ref>. In the following subsections we describe the CM-5 data network and different message-passing paradigms in detail. 4.1 Data Network on the CM-5 On the CM-5 the Data Network consists of two independent but identical network interfaces that are called left and right interfaces. <p> Strata, which is the multi-layer communications library developed at MIT and is compatible with CMMD as well as with CMAML [5, 6]. CMAML and Strata provide two kinds of message-transfer functions, Active Messages and Block Data Transfer (Active Messages-PUT), which is similar to PUT in Split-C developed in <ref> [11] </ref>. An active message on the CM-5 is a single communication packet (five words) that consists of the address of a handler function (first word) invoked on the destination processor and arguments (remaining four words) to the handler function. <p> This requires programming using a SPMD programming model so that a sender can know the address of any code run in the receiver. By using a handler function associated with every active message we can integrate a communication message into computation <ref> [11] </ref>. Since there is no buffering on a receiving processor the handler function is invoked immediately when an active message is received by interrupt or polling [10]. The block data transfer (Active Messages-PUT) function transfers a continuous data block from one source processor to a single destination processor. <p> However, active messages are blocking functions for a sender when used in the main thread of control. Each active message is associated with its handler function whose role is to integrate communication messages into the local computation by invoking itself on a receiver <ref> [11] </ref>. For a sender, a handler function can be regarded as a remote procedure call (with limited capabilities) executed on a receiver. An active message consists of the address of the handler function executed on a receiver and arguments to be passed to the remote function. <p> These functions use both interfaces when trying to send data. When trying to receive data, one polls both interfaces, but the other polls only the right interface [5]. Block data is transferred in a series of single-packet active messages. Since packet ordering is not guaranteed on the CM-5 network <ref> [11] </ref>, each message header contains the offset of data and an rport number. On arrival of each active message, a special system handler function is invoked on a receiving processor. This handler function stores data (four words) in the corresponding rport according to the offset of data. <p> This ensures that all messages have arrived before proceeding to the next cycle (by comparing the value of sizeRR with countR or sizeRL with countL). Since packet-ordering is not guaranteed on the CM-5 <ref> [11] </ref>, each node should record the number of active messages to be received to ensure that there is no pending message before it proceeds to the next cycle. <p> In the overlapping scheme without message coalescing, the bond activation probability p for a pair of sites can be different even for the same partition width, since the packet-ordering is not guaranteed on the CM-5 <ref> [11] </ref>. Hence the scale time T s for one sweep was used for performance comparisons: T s = S where T =average execution time for one sweep and S = average cluster size.
Reference: [12] <author> Y. Fang, I. Yen, and R. Dubash. </author> <title> Improving the performance of Lee's maze routing algorithm on parallel computers via semi-dynamic mapping strategies. </title> <type> Technical Report CPS-93-35, </type> <institution> Michigan State University, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: One is the classical message-passing paradigm using send/receive functions, and the other uses the active messages developed by von Eicken et al. [11]. This paper describes the parallelization of Wolff cluster algorithm [8, 19] and Lee's maze routing algorithm <ref> [15, 12, 20, 1] </ref> using the above message-passing paradigms available on the CM-5. These applications require fine-grained communication (small messages) for efficient parallelization. We have studied the tradeoffs of two optimizations to reduce the overhead of the communication cost for the above applications: 1. <p> Flip the spins in the cluster expanded in Step 2. 4. Go to Step 2. execution time of the parallel ants algorithm depends on the size as well as the shape of a cluster. 2.2 Lee's Maze-Routing Algorithm Lee's maze-routing algorithm is a well-known routing algorithm in VLSI circuits <ref> [15, 12, 20] </ref>. In this paper we consider only the single layer case in which a surface can be represented by cells in a two-dimensional grid. The grid and the cell correspond to the lattice and the site, respectively, in the Wolff cluster algorithm. <p> Effective parallelization requires maintaining good load balance while minimizing communication. We used the cyclic column-wise striped partitioning (Figure 10) scheme for vertically partitioning the lattice in both algorithms <ref> [2, 14, 12] </ref>. Let the width of each partition be w.
Reference: [13] <author> D. W. Heermann and A. N. Burki. </author> <booktitle> Parallel Algorithms in Computational Science. </booktitle> <publisher> Springer Verlag, </publisher> <year> 1990. </year>
Reference-contexts: Section 6 presents the experimental results. 2 Applications 2.1 Wolff Cluster Algorithm The Wolff cluster algorithm is a single-cluster Monte Carlo algorithm for the Ising model [8, 19]. The algorithm <ref> [8, 16, 19, 13] </ref> is given in Figure 4 and a simple example is presented in Figure 5 which assumes that the bond activation probability p is one (a site is always connected with its neighboring site if both sites have the same spin values). <p> Both algorithms use the same bond activation probabilities <ref> [13, 17, 18] </ref>. The sequential algorithm for the Wolff cluster algorithm is more efficient than the Swendsen-Wang algorithm [3], but it is more difficult to parallelize the Wolff cluster algorithm because it involves only a single cluster, while the Swendsen-Wang algorithm involves the entire lattice [3, 8].
Reference: [14] <author> V. Kurmar, A. Grama, G. Karypis, and A. Gupta. </author> <title> Parallel Computing: Design and Analysis of Algorithms. </title> <publisher> Benjamin/Cummings Publishing Company, Inc., </publisher> <year> 1994. </year>
Reference-contexts: Effective parallelization requires maintaining good load balance while minimizing communication. We used the cyclic column-wise striped partitioning (Figure 10) scheme for vertically partitioning the lattice in both algorithms <ref> [2, 14, 12] </ref>. Let the width of each partition be w.
Reference: [15] <author> C. Y. Lee. </author> <title> An algorithm for path connections and its applications. </title> <journal> IRE Trans, Electronic Computers, </journal> <volume> EC-10:346-365, </volume> <month> September </month> <year> 1961. </year>
Reference-contexts: One is the classical message-passing paradigm using send/receive functions, and the other uses the active messages developed by von Eicken et al. [11]. This paper describes the parallelization of Wolff cluster algorithm [8, 19] and Lee's maze routing algorithm <ref> [15, 12, 20, 1] </ref> using the above message-passing paradigms available on the CM-5. These applications require fine-grained communication (small messages) for efficient parallelization. We have studied the tradeoffs of two optimizations to reduce the overhead of the communication cost for the above applications: 1. <p> Flip the spins in the cluster expanded in Step 2. 4. Go to Step 2. execution time of the parallel ants algorithm depends on the size as well as the shape of a cluster. 2.2 Lee's Maze-Routing Algorithm Lee's maze-routing algorithm is a well-known routing algorithm in VLSI circuits <ref> [15, 12, 20] </ref>. In this paper we consider only the single layer case in which a surface can be represented by cells in a two-dimensional grid. The grid and the cell correspond to the lattice and the site, respectively, in the Wolff cluster algorithm.
Reference: [16] <author> R. H. Swendsen and J. S. Wang. </author> <title> Nonuniversal critical dynamics in Monte Carlo simulation. </title> <journal> Phys. Rev. Lett., </journal> <volume> 58(2):86, </volume> <year> 1987. </year> <month> 42 </month>
Reference-contexts: Section 6 presents the experimental results. 2 Applications 2.1 Wolff Cluster Algorithm The Wolff cluster algorithm is a single-cluster Monte Carlo algorithm for the Ising model [8, 19]. The algorithm <ref> [8, 16, 19, 13] </ref> is given in Figure 4 and a simple example is presented in Figure 5 which assumes that the bond activation probability p is one (a site is always connected with its neighboring site if both sites have the same spin values). <p> The difference between this algorithm and breadth-first-search is that there is no global adjacency matrix (or list). When a site is removed from a queue, the bond is generated dynamically between the site and its neighbors. The Wolff cluster algorithm is a variation on the Swendsen-Wang cluster algorithm <ref> [16] </ref>. The main difference is that in the former a single cluster is grown from a site selected at random and flipped with probability 1, while in the latter multiple clusters are grown and all clusters are formed and flipped with probability 1/2.
Reference: [17] <author> P. Tamayo, R. C. Brower, and W. Klein. </author> <title> Single-Cluster Monte Carlo dynamics for the Ising model. </title> <journal> J. Statis. Phys., </journal> <volume> 58(5):1083, </volume> <year> 1990. </year>
Reference-contexts: Both algorithms use the same bond activation probabilities <ref> [13, 17, 18] </ref>. The sequential algorithm for the Wolff cluster algorithm is more efficient than the Swendsen-Wang algorithm [3], but it is more difficult to parallelize the Wolff cluster algorithm because it involves only a single cluster, while the Swendsen-Wang algorithm involves the entire lattice [3, 8].
Reference: [18] <author> J. S. Wang and R. H. Swendsen. </author> <title> Cluster Monte Carlo algorithm. </title> <journal> Physica A, </journal> <volume> 167:565, </volume> <year> 1990. </year>
Reference-contexts: Both algorithms use the same bond activation probabilities <ref> [13, 17, 18] </ref>. The sequential algorithm for the Wolff cluster algorithm is more efficient than the Swendsen-Wang algorithm [3], but it is more difficult to parallelize the Wolff cluster algorithm because it involves only a single cluster, while the Swendsen-Wang algorithm involves the entire lattice [3, 8].
Reference: [19] <author> U. Wolff. </author> <title> Lattice field theory as a percolation process. </title> <journal> Phys. Rev. Lett., </journal> <volume> 62(15):361, </volume> <year> 1988. </year>
Reference-contexts: 1 Introduction Two different message-passing paradigms are available on the CM-5 for point-to-point communication. One is the classical message-passing paradigm using send/receive functions, and the other uses the active messages developed by von Eicken et al. [11]. This paper describes the parallelization of Wolff cluster algorithm <ref> [8, 19] </ref> and Lee's maze routing algorithm [15, 12, 20, 1] using the above message-passing paradigms available on the CM-5. These applications require fine-grained communication (small messages) for efficient parallelization. <p> Section 6 presents the experimental results. 2 Applications 2.1 Wolff Cluster Algorithm The Wolff cluster algorithm is a single-cluster Monte Carlo algorithm for the Ising model <ref> [8, 19] </ref>. The algorithm [8, 16, 19, 13] is given in Figure 4 and a simple example is presented in Figure 5 which assumes that the bond activation probability p is one (a site is always connected with its neighboring site if both sites have the same spin values). <p> Section 6 presents the experimental results. 2 Applications 2.1 Wolff Cluster Algorithm The Wolff cluster algorithm is a single-cluster Monte Carlo algorithm for the Ising model [8, 19]. The algorithm <ref> [8, 16, 19, 13] </ref> is given in Figure 4 and a simple example is presented in Figure 5 which assumes that the bond activation probability p is one (a site is always connected with its neighboring site if both sites have the same spin values).
Reference: [20] <author> Y. Won and S. Sahni. </author> <title> Maze routing on a hypercube multiprocessors computers. </title> <booktitle> In Proceedings of Intrl. Conf. on Parallel Processing, </booktitle> <address> St. Charles, </address> <pages> pages 630-637, </pages> <month> August </month> <year> 1987. </year> <month> 43 </month>
Reference-contexts: One is the classical message-passing paradigm using send/receive functions, and the other uses the active messages developed by von Eicken et al. [11]. This paper describes the parallelization of Wolff cluster algorithm [8, 19] and Lee's maze routing algorithm <ref> [15, 12, 20, 1] </ref> using the above message-passing paradigms available on the CM-5. These applications require fine-grained communication (small messages) for efficient parallelization. We have studied the tradeoffs of two optimizations to reduce the overhead of the communication cost for the above applications: 1. <p> Flip the spins in the cluster expanded in Step 2. 4. Go to Step 2. execution time of the parallel ants algorithm depends on the size as well as the shape of a cluster. 2.2 Lee's Maze-Routing Algorithm Lee's maze-routing algorithm is a well-known routing algorithm in VLSI circuits <ref> [15, 12, 20] </ref>. In this paper we consider only the single layer case in which a surface can be represented by cells in a two-dimensional grid. The grid and the cell correspond to the lattice and the site, respectively, in the Wolff cluster algorithm. <p> Unlike the Wolff cluster algorithm, the grid has four boundaries. The goal is to find a shortest path from a source cell to a destination cell. Some cells are blocked and therefore cannot be on the shortest path <ref> [20] </ref>. Lee's maze-routing algorithm consists of three phases (Figure 7) [20, 1]. The first phase, wavefront expansion, is similar to the ants-in-the-labyrinth algorithm and uses a breadth-first-search for expanding the wavefront. <p> Unlike the Wolff cluster algorithm, the grid has four boundaries. The goal is to find a shortest path from a source cell to a destination cell. Some cells are blocked and therefore cannot be on the shortest path [20]. Lee's maze-routing algorithm consists of three phases (Figure 7) <ref> [20, 1] </ref>. The first phase, wavefront expansion, is similar to the ants-in-the-labyrinth algorithm and uses a breadth-first-search for expanding the wavefront. However, in Lee's maze-routing algorithm any two neighboring cells in a grid are connected unless one of the sites is blocked. <p> A synchronization step is required after completing Steps 2, 3, and 4 to ensure all the wavefronts have been completed (i.e., that the cluster stops growing) <ref> [20] </ref>. This can easily be done by using a global reduction function such as logical AND. The naive way is to synchronize all processors every time we finish Steps 2, 3, and 4, but this method increases the idle time of processors [20]. <p> been completed (i.e., that the cluster stops growing) <ref> [20] </ref>. This can easily be done by using a global reduction function such as logical AND. The naive way is to synchronize all processors every time we finish Steps 2, 3, and 4, but this method increases the idle time of processors [20]. The cost of synchronization can be reduced by performing it every ffi-steps (called the ffi-synchronization scheme by Won and Sahni [20]). The value of ffi is determined experimentally. <p> The naive way is to synchronize all processors every time we finish Steps 2, 3, and 4, but this method increases the idle time of processors <ref> [20] </ref>. The cost of synchronization can be reduced by performing it every ffi-steps (called the ffi-synchronization scheme by Won and Sahni [20]). The value of ffi is determined experimentally. <p> In the IC step each processor cooperates with its two neighboring processors to exchange coalesced messages. At the end of cycle k, a new wavefront of distance k is completely formed. IC and RE steps can be delayed by one cycle in order to overlap local computation with communication <ref> [20] </ref>. At the beginning of the cycle k, each processor has two inputs: partial local wavefront of distance k 1 expanded at cycle k 1 (which is the input for the LE step), and candidates for remote wavefront of distance k 1 (which is the input for the IC step). <p> Each processor begins to process the communication buffer as soon as it finishes its receiving operation <ref> [20] </ref>. The pseudo code of Expansion is given in Figure 20. The variable old represents the number of sites on the current wavefront, and the variable new records the number of sites expanded from the current wavefront. <p> When using ffi synchronization the value of ffi was chosen to be 0:1 fi (N=2) for the Wolff cluster algorithm and 0.2M 12 for Lee's maze-routing algorithm <ref> [20] </ref>. <p> we fixed the inverse temperature fi to the critical inverse 12 If the coordinates of the source cell and the destination cell are (x s ; y s ) and (x d ; y d ), respectively, M = jx s x d j + jy s y d j <ref> [20] </ref>. temperature fi c 13 because it generates clusters that are highly irregular in size and shape. The average execution time for one sweep was measured over 100 sweeps. A thermalization step involving 100 steps was used before taking these measurements.
References-found: 20

