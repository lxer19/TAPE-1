URL: http://www.umiacs.umd.edu/users/sarit/Articles/20.ps
Refering-URL: http://www.umiacs.umd.edu/users/sarit/articles.html
Root-URL: 
Email: grosz@eecs.harvard.edu  sarit@cs.biu.ac.il and  
Title: Collaborative Plans for Complex Group Action  
Author: Barbara J. Grosz Sarit Kraus 
Note: Note: This article appears in Artificial Intelligence 86(2):269-357, 1996.  
Address: Cambridge MA 02138 USA  Ramat Gan 52900 Israel  College Park MD 20742  
Affiliation: Division of Applied Sciences Harvard University  Department of Mathematics and Computer Science Bar Ilan University  Institute for Advanced Computer Studies University of Maryland,  
Abstract: fl This paper is an extension of our paper in IJCAI-93 (Grosz and Kraus, 1993). We thank Joyce Friedman for many thought-provoking questions, and Karen Lochbaum for the same and for helpful comments on many drafts. We also thank Michael Bratman, David Israel, and Martha Pollack for comments on earlier versions, and the anonymous reviewers for their helpful suggestions and illuminating comments. This research was initiated when the first author was a Harold Perlman Visiting Professor, Hebrew University, Jerusalem. Partial support for the first author was provided by U S WEST Advanced Technologies. The second author was supported in part by NSF Grant Number IRI-9423967 and the Israeli Science Ministry grant No. 6288. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Allen, James and C. Raymond Perrault. </author> <year> 1980. </year> <title> Analyzing intention in utterances. </title> <journal> Artificial Intelligence, </journal> <volume> 15(3) </volume> <pages> 143-178. </pages>
Reference: <author> Appelt, Douglas and A. Kronfeld. </author> <year> 1987. </year> <title> A computational model of referring. </title> <booktitle> In: Proceedings of the 10th International Joint Conference on Artificial Intelligence (IJCAI-87), </booktitle> <pages> pp. 640-647. </pages>
Reference: <author> Arrow, K. J. </author> <year> 1985. </year> <title> The economics of agency. </title> <editor> In: J. Pratt and R. Zeckhauser, eds., </editor> <title> Principals and Agents: The Structure of Business. </title> <publisher> Harvard Business School Press, </publisher> <pages> pp. 37-51. </pages>
Reference-contexts: inter-alia) has used it to refer either to the kinds of coordination we accomplish with SharedPlans or to "helpful behavior" like that achieved by the intention-that axioms described later in the paper. "Contracting" as we use it is closer to the concept of "incentive contracting" used in the economics literature <ref> (Arrow, 1985, inter alia) </ref>. 24 FIP (P; G; ff; T p ; T ff ; R ff ; C ff ) 3. Contracting-out case: G intends to get another agent G c to do the subact fi i . 3a.
Reference: <author> Balkanski, Cecile T. </author> <year> 1990. </year> <title> Modelling act-type relations in collaborative activity. </title> <type> Technical Report TR-23-90, </type> <institution> Harvard University. </institution>
Reference: <author> Balkanski, Cecile T. </author> <year> 1993. </year> <title> Actions, Beliefs and Intentions in Rationale Clauses and Means Clauses. </title> <type> Ph.D. thesis, </type> <institution> Harvard University. </institution>
Reference: <author> Bond, A.H. and L. Gasser. </author> <year> 1988. </year> <title> An analysis of problems and research in DAI. </title> <editor> In: A.H. Bond and L. Gasser, eds., </editor> <booktitle> Readings in Distributed Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, CA, </address> <pages> pp. 4-46. </pages>
Reference: <author> Bratman, Michael E. </author> <year> 1992. </year> <title> Shared cooperative activity. </title> <journal> The Philosophical Review, </journal> <volume> 101 </volume> <pages> 327-341. </pages>
Reference-contexts: Potential intentions typically arise in the course of means-ends reasoning. Attitudes of Pot.Int.To stem from an agent's deliberations about how to do some action it is committed to performing. Pot.Int.Th's derive from the need to ensure that collaborating agents' plans mesh correctly <ref> (Bratman, 1992) </ref>. Int.To and Pot.Int.To are used to represent an agent's intentions to do some action; Int.Th and Pot.Int.Th are used to represent an agent's intention that some proposition hold. <p> Group members may have different reasons for engaging in the collaborative activity of doing ff, so C ff may vary across group members. For example, hunger might underlie Kate's making dinner with Dan, whereas a desire for social interaction underlies Dan's making dinner with Kate (cf. <ref> (Bratman, 1992) </ref>). In addition, because the beliefs and intentions about the plan are distributed, each of the agents in GR will have its own internal name for the plan; P refers to an agent-internal name. Thus, the distributed property of SharedPlans yields an additional constraint on agent design. <p> In addition, to ensure that subplans are compatible or "mesh" in Bratman's terminology <ref> (Bratman, 1992) </ref> and provide sufficiently for helpful behavior, the definition requires that the full group form a commitment to the ability of the agent or subgroup to carry out fi i .
Reference: <author> Bratman, Michael E. </author> <year> 1987. </year> <title> Intention, Plans, and Practical Reason. </title> <publisher> Harvard University Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: C prop is the analogue for propositions of C ff for actions. The commonality between intentions-to and intentions-that is that both commit an agent not to adopt conflicting intentions (Werner, 1987) and constrain replanning in case of failure <ref> (Bratman, 1987) </ref>. The significant distinction between them is not in the types of objects each relates, but in their connection to means-ends reasoning and in their different presumptions about an agent's ability to act in service of the intention. An Int.To commits an agent to means-ends reasoning (Bratman, 1987) and, at <p> case of failure <ref> (Bratman, 1987) </ref>. The significant distinction between them is not in the types of objects each relates, but in their connection to means-ends reasoning and in their different presumptions about an agent's ability to act in service of the intention. An Int.To commits an agent to means-ends reasoning (Bratman, 1987) and, at some point, to acting. In contrast, an Int.Th does not directly engender such behavior. <p> As others have noted (e.g., Pollack (1986a), p. 38, and others cited there), this stance is too strong. Although it is clear that the agent cannot believe it is incapable of succeeding, it may have doubts about the success of the intended action <ref> (Bratman, 1987) </ref>. Thus, our formalization would be better served by a probabilistic approach to the modeling of ability, but we have not identified a suitable computational model. Such an approach would enable us to replace "flat-out" belief (Bratman, 1987)[pp. 36ff ] with the more realistic requirement that an agent's belief in <p> of succeeding, it may have doubts about the success of the intended action <ref> (Bratman, 1987) </ref>. Thus, our formalization would be better served by a probabilistic approach to the modeling of ability, but we have not identified a suitable computational model. Such an approach would enable us to replace "flat-out" belief (Bratman, 1987)[pp. 36ff ] with the more realistic requirement that an agent's belief in the likelihood of success of its actions be above a certain threshold for the agent to be able to intend to perform the act. 6 Vermazen (1993) describes the need to consider more than a single attitude <p> The first part of this definition [Clause (1)] deals with the case of an agent intending to do an action that is basic level. Two standard constraints <ref> (Bratman, 1987) </ref> are represented in this part of the definition: that the agent be committed to doing the action and that the agent believe it can execute the action. The second part of the definition addresses the case of an agent intending to do an action that is complex. <p> In addition, the agent must believe that the recipe it will select is one it will be able to execute. These additional constraints follow from intentions-to engendering means-ends reasoning <ref> (Bratman, 1987) </ref>. If an agent does not have a recipe for ff and furthermore has no idea at all about how to find or construct a recipe, then it cannot do any means-ends reasoning about ff. <p> Commitment to mutual support (Bratman's third criterion) is realized in a more complex way. It requires a combination of the intentions-that the agents form and the axioms for helpful behavior that originate from intentions-that. Each of the three basic roles of intention that Bratman describes in earlier work <ref> (Bratman, 1987) </ref> also play a significant role in the formalization. That an intention-to engenders means-ends reasoning is built into the definition of Int.To; the FIP to Elaborate Individual represents this commitment. Conflict-avoidance is also explicitly represented, in Axiom (A1) of Figure 4.
Reference: <author> Bratman, Michael E. </author> <year> 1990. </year> <title> What is intention? In: </title> <editor> P.R. Cohen, J. Morgan, and M.E. Pollack, eds., </editor> <title> Intentions in Communication. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, chapter 2, </address> <pages> pp. 15-31. </pages>
Reference: <author> Bratman, Michael E., David J. Israel, and Martha E. Pollack. </author> <year> 1988. </year> <title> Plans and resource-bounded practical reasoning. </title> <journal> Computational Intelligence, </journal> <volume> 4(4) </volume> <pages> 349-355. </pages>
Reference-contexts: In particular, we assume that the agent design incorporates capabilities for managing pending and adopted intentions, including capabilities for deciding when to consider adopting an intention; choosing among competing options; scheduling and executing the intended actions; and monitoring their effects and the state of the world <ref> (Bratman, Israel, and Pollack, 1988) </ref>. The definitions given in this paper entail certain constraints on each of these processes, but leave other options open. We discuss ramifications of those choices that affect collaboration as we develop the model. <p> Potential intentions are used to represent an agent's mental state when it is considering adopting an intention but has not yet deliberated about the interaction of that intention with the others it currently holds. Potential intentions motivate an agent to weigh different possible courses of actions or options <ref> (Bratman, Israel, and Pollack, 1988) </ref>. They thus represent intentions that an agent would like to adopt, but to which it is not yet committed. Potential intentions typically arise in the course of means-ends reasoning. <p> The identification of the recipe entails means-ends reasoning. As she does this means-ends reasoning, she will determine actions she needs to perform to make the mushroom puffs and will adopt potential intentions to [Pot.Int.To] perform these actions. The potential intentions will become part of a deliberation process <ref> (Bratman, Israel, and Pollack, 1988) </ref> and through that process may become Int.To's. 4.3.2 Modal Operators for Attitudes of Intention The definition of Int.To is given in Figure 3.
Reference: <author> Cohen, P. and H. Leveque. </author> <year> 1990. </year> <title> Intention is choice with commitment. </title> <journal> Artificial Intelligence, </journal> <volume> 42 </volume> <pages> 263-310. </pages>
Reference-contexts: The most significant difference between SP and Int.To, however, is that SP is a meta-predicate not a modal operator. There is no attitude of "we-intending" (Searle, 1990) or joint-intention <ref> (Levesque, Cohen, and Nunes, 1990) </ref>. As a consequence, the definition of SP has one less clause than that for Int.To.
Reference: <author> Cohen, P. and H. Levesque. </author> <year> 1991. </year> <title> Teamwork. </title> <journal> Nous, </journal> <volume> 25 </volume> <pages> 487-512. </pages>
Reference-contexts: They do not discuss or represent in detail partial plans for individual or joint action. In two papers (Levesque, Cohen, and Nunes, 1990; Cohen and Leveque, 1990), only actions with all constituent subactions specified are considered. A subsequent paper <ref> (Cohen and Levesque, 1991) </ref> allows some partiality in the definition of individual and joint intention by allowing the operators INTEND* and IJ* to take open action expressions as arguments. In contrast, we examine in detail various types of partiality. As described in Section 7, doing so complicates our definitions.
Reference: <author> Conry, S. E., K. Kuwabara, V. R. Lesser, and R. A. Meyer. </author> <year> 1991. </year> <title> Multistage negotiation for distributed satisfaction. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 21(6) </volume> <pages> 1462-1477. </pages> <note> Special Issue on Distributed Artificial Intelligence. 62 Davis, </note> <author> R. and R. G. Smith. </author> <year> 1983. </year> <title> Negotiation as a metaphor for distributed problem solving. </title> <journal> Artificial Intelligence, </journal> <volume> 20 </volume> <pages> 63-109. </pages>
Reference: <author> Decker, Keith S., Edmund H. Durfee, and Victor R. Lesser. </author> <year> 1989. </year> <title> Evaluating research in cooperative distributed problem solving. </title> <editor> In: Michael N. Huhns, ed., </editor> <booktitle> Distributed Artificial Intelligence, </booktitle> <volume> volume 2. </volume> <publisher> Pitman/Morgan Kaufmann, London, </publisher> <pages> pp. 487-519. </pages>
Reference: <author> Durfee, E. H. </author> <year> 1988. </year> <title> Coordination of Distributed Problem Solvers. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, MA. </address>
Reference: <author> Ephrati, E. and J. S. Rosenschein. </author> <year> 1992. </year> <title> Reaching agreement through partial revelation of preferences. </title> <booktitle> In: Proceedings of ECAI-92, </booktitle> <pages> pp. 229-233. </pages>
Reference: <author> Ephrati, E. and J. S. Rosenschein. </author> <year> 1993. </year> <title> Planning to please: Following another agent's intended plan. </title> <journal> Journal of Group Decision and Negotiation, </journal> <volume> 2(3) </volume> <pages> 219-235. </pages>
Reference: <author> Gasser, L. </author> <year> 1991. </year> <title> Social concepts of knowledge and action: DAI foundations and open systems semantics. </title> <journal> Artificial Intelligence, 47(1-3):107-138. </journal>
Reference: <author> Goldman, A.I. </author> <year> 1970. </year> <title> A Theory of Human Action. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ. </address>
Reference: <author> Grosz, Barbara and Sarit Kraus. </author> <year> 1993. </year> <title> Collaborative plans for group activities. </title> <editor> In: Ruzena Bajcsy, ed., </editor> <booktitle> Proceedings of the 1993 International Joint Conference on Artificial Intelligence (IJCAI-93), </booktitle> <pages> pp. 367-373. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, CA. </address>
Reference: <author> Grosz, Barbara and Candace Sidner. </author> <year> 1990a. </year> <title> Plans for discourse. </title> <editor> In: P. Cohen, J. Morgan, and M. Pollack, eds., </editor> <title> Intentions in Communication. </title> <publisher> Bradford Books, MIT Press. </publisher>
Reference-contexts: Thus, the FIP definition for full individual plans extends previous work by treating more complex recipes (the formalization of recipes is more general than that in the original formulation <ref> (Grosz and Sidner, 1990a) </ref>), providing an expanded notion of what it means to be able to carry out a complex action, and allowing for contracting to another agent. <p> designer needing to determine in advance all of the problems that might arise during execution and the information that needs to be communicated once one of them does. 9 Conclusions and Future Work To provide an account of collaborative activity, Searle (1990) introduced the notion of `we-intention.' Grosz and Sidner <ref> (Grosz and Sidner, 1990a) </ref> argued that such a notion should not be necessary and their initial formulation of SharedPlans avoids use of one. However, the definitions provided in that formulation could only accommodate group activity that directly decomposed into actions of individual agents.
Reference: <author> Grosz, Barbara and Candace Sidner. </author> <year> 1990b. </year> <title> A reply to Hobbs. </title> <editor> In: P.R. Cohen, J.L. Morgan, and M.E. Pollack, eds., </editor> <title> Intentions in Communication. </title> <publisher> Bradford Books at MIT Press, </publisher> <pages> pp. 461-462. </pages>
Reference: <author> Grosz, Barbara and Candace Sidner. </author> <year> 1986. </year> <title> Attention, intentions, and the structure of discourse. </title> <journal> Computational Linguistics, </journal> <volume> 12(3) </volume> <pages> 175-204. </pages>
Reference: <editor> Grosz, Barbara J., Karen Sparck Jones, and Bonnie Lynn Webber. </editor> <booktitle> 1986. Readings in Natural Language Processing. </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> Los Altos, CA. </address>
Reference: <author> Hobbs, Jerry. </author> <year> 1985. </year> <title> Ontological promiscuity. </title> <booktitle> In: Proceedings of the ACL. </booktitle>
Reference: <author> Huhns, Michael N., Munindar P. Singh, Tomasz Ksiezyk, and Nigel Jacobs. </author> <year> 1994. </year> <title> Global information management via local autonomous agents. </title> <booktitle> In: Proceedings of ICOT International Symposium on Fifth Generation Computer Systems: Workshop on Heterogeneous Cooperative Knowledge Bases, </booktitle> <pages> pp. 1-15. </pages> <address> Tokyo, Japan. </address>
Reference-contexts: Thus, neither of these alternatives seems practical. Several mechanisms have been developed for conflict detection and resolution in the context of cooperation of autonomous agents (Klein, 1991; Polat, Shekhar, and Guvenir, 1993; Lesser, 1991, inter alia) and for global information management using local autonomous agents <ref> (Huhns et al., 1994, inter alia) </ref>. Other research has addressed this problem in the context of task allocation among autonomous agents under incomplete information (Moehlman, Lesser, and Buteau, 1992). Each of these approaches requires that different specific information be communicated when less than the full information can be.
Reference: <author> Israel, David, John Perry, and Syun Tutiya. </author> <year> 1991. </year> <title> Actions and movements. </title> <editor> In: John Mylopoulos and Ray Reiter, eds., </editor> <booktitle> 12th International Joint Conference on Artificial Intelligence (IJCAI-91), </booktitle> <volume> volume 2, </volume> <pages> pp. 1060-1065. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, CA. </address>
Reference: <author> Jennings, N. </author> <year> 1992. </year> <title> On being responsible. In: </title> <journal> Decentralized Artificial Intelligence, </journal> <volume> Volume 3. </volume> <publisher> Elsevier Science Publishers, </publisher> <pages> pp. 93-102. </pages>
Reference: <author> Jennings, Nick R. </author> <year> 1995. </year> <title> Controlling cooperative problem solving in industrial multi-agent systems using joint intentions. </title> <journal> Artificial Intelligence Journal, </journal> <volume> 75(2) </volume> <pages> 1-46. </pages> <note> 63 Kautz, </note> <author> Henry. </author> <year> 1989. </year> <title> A circumscriptive theory of plan recognition. In: P.N. </title> <editor> Cohen, J.L. Morgan, and M.E. Pollack, eds., </editor> <title> Intentions in Communication. </title> <publisher> Bradford Books, MIT Press. </publisher>
Reference: <author> Klein, M. </author> <year> 1991. </year> <title> Supporting conflict resolution in cooperative design systems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, Special Issue of Distributed Artificial Intelligence, </journal> <volume> 21(6) </volume> <pages> 1379-1390. </pages>
Reference: <author> Konolige, Kurt and Martha Pollack. </author> <year> 1993. </year> <title> A representationalist theory of intention. </title> <editor> In: Ruzena Bajcsy, ed., </editor> <booktitle> Proceedings of the 13th International Joint Conference on Artificial Intelligence, </booktitle> <volume> volume 1, </volume> <pages> pp. 390-395. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, CA. </address>
Reference: <author> Kraus, S. and D. Lehmann. </author> <year> 1988. </year> <title> Knowledge, belief and time. </title> <journal> Theoretical Computer Science, </journal> <volume> 58 </volume> <pages> 155-174. </pages>
Reference-contexts: by which ff is being done is also part of this constituent; formally, the definitions require that each plan be identified by a name. 4.2 Basic Modal Operators We use two standard modal operators for belief, Bel and MB for belief and mutual belief respectively; they have their usual definitions <ref> (Kraus and Lehmann, 1988, inter alia) </ref>. In addition, we specify several modal operators that relate agents and actions: Exec, Commit, and Do, and the intention operators presented in the next section.
Reference: <author> Kraus, S. and D. Lehmann. </author> <year> 1995. </year> <title> Designing and building a negotiating automated agent. </title> <journal> Computational Intelligence, </journal> <volume> 11(1) </volume> <pages> 132-171. </pages>
Reference: <author> Kraus, S., M. Nirkhe, and K. Sycara. </author> <year> 1993. </year> <title> Reaching agreements through argumentation: A logical model. </title> <booktitle> In: Proceedings of the Workshop on Distributed Artificial Intelligence, </booktitle> <pages> pp. 233-247. </pages> <booktitle> Also presented in AAAI-93 Workshop on AI Theories of Groups and Organizations: Conceptual and Empirical Research. </booktitle>
Reference-contexts: Clause (3c) represents this commitment using the Int.Th modal operator and the CBA meta-predicate. Contracting, unlike collaborative plans, does not require reciprocity in this commitment; contracting is not in and of itself collaborative <ref> (Kraus, 1993) </ref>. Thus, there is no correlate of Clause (3c) for the contractor G c . 23 A different situation holds among the agents of a SharedPlan, as will be discussed in Section 6. <p> For Dan's plan to be complete, Dan must believe that this action fl is either a basic-level action that he is able to 23 Some mechanism, typically involving communication, is needed for G to believe that G c will actually perform the contracted action. Legal contracts serve this purpose <ref> (Kraus, 1993) </ref>. 25 do or is an action for which he knows a recipe and for which he has a full individual plan.
Reference: <author> Kraus, S. and J. Wilkenfeld. </author> <year> 1991. </year> <title> Negotiations over time in a multi-agent environment: Preliminary report. </title> <booktitle> In: Proceedings of IJCAI-91, </booktitle> <pages> pp. 56-61. </pages> <booktitle> International Joint Conference on Artificial Intelligence, </booktitle> <address> Australia. </address>
Reference: <author> Kraus, S. and J. Wilkenfeld. </author> <year> 1993. </year> <title> A strategic negotiations model with applications to an international crisis. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 23(1) </volume> <pages> 313-323. </pages>
Reference-contexts: Clause (3c) represents this commitment using the Int.Th modal operator and the CBA meta-predicate. Contracting, unlike collaborative plans, does not require reciprocity in this commitment; contracting is not in and of itself collaborative <ref> (Kraus, 1993) </ref>. Thus, there is no correlate of Clause (3c) for the contractor G c . 23 A different situation holds among the agents of a SharedPlan, as will be discussed in Section 6. <p> For Dan's plan to be complete, Dan must believe that this action fl is either a basic-level action that he is able to 23 Some mechanism, typically involving communication, is needed for G to believe that G c will actually perform the contracted action. Legal contracts serve this purpose <ref> (Kraus, 1993) </ref>. 25 do or is an action for which he knows a recipe and for which he has a full individual plan.
Reference: <author> Kraus, Sarit. </author> <year> 1993. </year> <title> Agents contracting tasks in non-collaborative environments. </title> <booktitle> In: Proceedings of AAAI-93, </booktitle> <pages> pp. 243-248. </pages> <booktitle> American Association of Artificial Intelligence, </booktitle> <address> Washington, DC. </address>
Reference-contexts: Clause (3c) represents this commitment using the Int.Th modal operator and the CBA meta-predicate. Contracting, unlike collaborative plans, does not require reciprocity in this commitment; contracting is not in and of itself collaborative <ref> (Kraus, 1993) </ref>. Thus, there is no correlate of Clause (3c) for the contractor G c . 23 A different situation holds among the agents of a SharedPlan, as will be discussed in Section 6. <p> For Dan's plan to be complete, Dan must believe that this action fl is either a basic-level action that he is able to 23 Some mechanism, typically involving communication, is needed for G to believe that G c will actually perform the contracted action. Legal contracts serve this purpose <ref> (Kraus, 1993) </ref>. 25 do or is an action for which he knows a recipe and for which he has a full individual plan.
Reference: <author> Lesser, V.R. </author> <year> 1991. </year> <title> A retrospective view of FA/C distributed problem solving. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, Special Issue on Distributed Artificial Intelligence, </journal> <volume> 21(6) </volume> <pages> 1347-1362. </pages>
Reference: <author> Levesque, H., P. Cohen, and J. Nunes. </author> <year> 1990. </year> <title> On acting together. </title> <booktitle> In: Proceedings of the Annual Conference of the American Association for Artificial Intelligence (AAAI-90), </booktitle> <pages> pp. 94-99. </pages>
Reference-contexts: The most significant difference between SP and Int.To, however, is that SP is a meta-predicate not a modal operator. There is no attitude of "we-intending" (Searle, 1990) or joint-intention <ref> (Levesque, Cohen, and Nunes, 1990) </ref>. As a consequence, the definition of SP has one less clause than that for Int.To.
Reference: <author> Litman, Diane J. and James F. Allen. </author> <year> 1990. </year> <title> Discourse processing and commonsense plans. </title> <editor> In: Philip R. Cohen, Jerry L. Morgan, and Martha E. Pollack, eds., </editor> <title> Intentions in Communication. </title> <publisher> MIT Press, </publisher> <address> Cambridge MA. </address>
Reference: <author> Lochbaum, Karen. </author> <year> 1991. </year> <title> An algorithm for plan recognition in collaborative discourse. </title> <booktitle> In: Proceedings of the 29th Annual Meeting of the ACL, </booktitle> <pages> pp. 33-38. </pages> <address> Berkeley, CA. </address>
Reference: <author> Lochbaum, Karen. </author> <year> 1994. </year> <title> Using Collaborative Plans to Model the Intentional Structure of Discourse. </title> <type> Ph.D. thesis, </type> <institution> Harvard University. </institution> <note> Available as Tech Report TR-25-94. </note>
Reference-contexts: She may become involved if Dan needs help determining an appropriate recipe <ref> (Lochbaum, 1994) </ref>. 11 The definition of CONF encompasses only conflicts among propositions and the performance of single-agent actions. <p> using the terminology developed in the plan definitions. 15 We recognize in so doing we are somewhat abusing the formal vocabulary; however, the alternative is more complex and less easily understood language. 16 In addition to being more realistic for planning systems, incremental algorithms are crucial for dialogue mod els <ref> (Lochbaum, 1994) </ref>. 20 Select Rec (G; ff; R p ff ; T ) refers to the activity of an individual agent G extending its partial recipe R p ff for ff.
Reference: <author> Lochbaum, Karen, Barbara Grosz, and Candace Sidner. </author> <year> 1990. </year> <title> Models of plans to support communication: An initial report. </title> <booktitle> In: Proceedings of AAAI-90, </booktitle> <pages> pp. 485-490. </pages> <address> Boston, MA. </address> <note> 64 Lochbaum, </note> <author> Karen E. </author> <year> 1995. </year> <title> The use of knowledge preconditions in language processing. In: Chris S. </title>
Reference-contexts: For example, if ff is being done as part of doing some higher-level action A, i.e., ff is part of the recipe adopted in the plan to do A, then C ff encodes this fact (e.g., using the Contributes relation <ref> (Lochbaum, Grosz, and Sidner, 1990) </ref>); alternatively the agent might have chosen to do ff to satisfy some independent desire. This constituent of C ff is constructed recursively as an agent chooses recipes and constructs plans for the actions in them.
Reference: <editor> Mellish, ed., </editor> <booktitle> Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI-95), </booktitle> <volume> volume 2, </volume> <pages> pp. 1260-1266. </pages> <publisher> Morgan Kaufmann Publishers, Inc, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Malone, Thomas W., Richard E. Fikes, and M. T. Howard. </author> <year> 1988. </year> <title> Enterprise: A market-like task scheduler for distributed computing environments. </title> <editor> In: B. A. Huberman, ed., </editor> <booktitle> The Ecology of Computation. </booktitle> <publisher> North-Holland Publishing Company, Amsterdam, </publisher> <pages> pp. 177-205. </pages>
Reference: <author> Moehlman, T., V. Lesser, and B. Buteau. </author> <year> 1992. </year> <title> Decentralized negotiation: An approach to the distributed planning problem. Group Decision and Negotiation, </title> <booktitle> 2 </booktitle> <pages> 161-191. </pages>
Reference-contexts: Other research has addressed this problem in the context of task allocation among autonomous agents under incomplete information <ref> (Moehlman, Lesser, and Buteau, 1992) </ref>. Each of these approaches requires that different specific information be communicated when less than the full information can be. Thus, a range of options are possible all of which provide reasonable, though different, support for collaborative activity.
Reference: <author> Morgenstern, L. </author> <year> 1988. </year> <title> Foundations of a Logic of Knowledge, Action, and Communication. </title> <type> Ph.D. thesis, </type> <address> New York University. </address>
Reference: <author> Osawa, E. and M. Tokoro. </author> <year> 1992. </year> <title> Collaborative plan construction for multiagent mutual planning. In: </title> <journal> Decentralized Artificial Intelligence, </journal> <volume> Volume 3. </volume> <publisher> Elsevier Science Publishers, </publisher> <pages> pp. 169-185. </pages>
Reference: <author> Polat, F., S. Shekhar, and A. Guvenir. </author> <year> 1993. </year> <title> Distributed conflict resolution among cooperating expert systems. </title> <journal> Expert Systems, </journal> <volume> 10(4) </volume> <pages> 227-236. </pages>
Reference: <author> Pollack, M. </author> <year> 1986a. </year> <title> Inferring Domain Plans in Question-Answering. </title> <type> Ph.D. thesis, </type> <institution> University of Pennsylvania. </institution>
Reference: <author> Pollack, Martha E. </author> <year> 1986b. </year> <title> Inferring domain plans in question-answering. </title> <type> Technical Report MS-CIS-86-40, </type> <institution> Department of Computer Science and Information Science,Moore School,University of Pennsylvania, </institution> <address> Philadephia, PA. </address>
Reference: <author> Pollack, Martha E. </author> <year> 1986c. </year> <title> A model of plan inference that distinguishes between the beliefs of actors and observers. </title> <booktitle> In: Proceedings of the 24th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. 207-214. </pages> <address> New York. </address>
Reference-contexts: The predicate basic.level (ff) holds if ff is a basic-level action. We assume basic-level actions are executable at will if appropriate situational conditions hold, and do not define this further (see Pollack's argument that this is a reasonable assumption in a computational setting <ref> (Pollack, 1986c) </ref>). Furthermore, we assume that agents' beliefs are correct with respect to whether actions are basic level or complex. If an action is basic level, agents believe it is so; if an agent believes an action is basic level, it is.
Reference: <author> Pollack, Martha E. </author> <year> 1990. </year> <title> Plans as complex mental attitudes. In: P.N. </title> <editor> Cohen, J.L. Morgan, and M.E. Pollack, eds., </editor> <title> Intentions in Communication. </title> <publisher> Bradford Books, MIT Press. </publisher>
Reference: <author> Rao, A. and M. Georgeff. </author> <year> 1991. </year> <title> Modelling rational agents within bdi architecture. </title> <booktitle> In: Proceedings of the Second International Conference of Knowledge Representation, </booktitle> <pages> pp. 473-484. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, CA. </address>
Reference: <author> Russell, S. J. and E. H. Wefald. </author> <year> 1989. </year> <booktitle> Principles of meta-reasoning. In: Proceedings of the First International Conference of Knowledge Representation and Reasoning, </booktitle> <pages> pp. 400-411. </pages>
Reference: <author> Russell, Stuart and Peter Norvig. </author> <year> 1994. </year> <title> Artificial Intelligence: A Modern Approach. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ. </address>
Reference: <author> Searle, John R. </author> <year> 1990. </year> <title> Collective intentions and actions. In: Intentions in Communication. </title> <publisher> The MIT Press, </publisher> <address> chapter 19. </address>
Reference-contexts: In addition, the account we provide retains the "broadly individualistic" tenor of Bratman's characterization. The formalization developed in this paper does not require any unreduced notion of joint intentions or "we-intentions" <ref> (Searle, 1990, p. 404) </ref>. A notion of collective intentionality presents two possible difficulties. Either one must presuppose some kind of group mental state or one must explain how "we-intentions" can be realized in terms of the mental state of individuals. <p> A notion of collective intentionality presents two possible difficulties. Either one must presuppose some kind of group mental state or one must explain how "we-intentions" can be realized in terms of the mental state of individuals. The notion of group mental state not only presents philosophical problems <ref> (Searle, 1990) </ref>, but also appears to necessitate that any agents that might work together in a group be designed together. Searle (1990) explains "we-intentions" as attitudes held by all members of a group toward a group action. <p> The most significant difference between SP and Int.To, however, is that SP is a meta-predicate not a modal operator. There is no attitude of "we-intending" <ref> (Searle, 1990) </ref> or joint-intention (Levesque, Cohen, and Nunes, 1990). As a consequence, the definition of SP has one less clause than that for Int.To.
Reference: <author> Shehory, Onn and Sarit Kraus. </author> <year> 1995. </year> <title> Task allocation via coalition formation among autonomous agents. </title> <editor> In: Chris S. Mellish, ed., </editor> <booktitle> Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI-95), </booktitle> <volume> volume 1, </volume> <pages> pp. 655-661. </pages> <publisher> Morgan Kaufmann Publishers Inc., </publisher> <address> San Mateo, CA. </address>
Reference: <author> Shoham, Y. </author> <year> 1993. </year> <title> Agent oriented programing. </title> <journal> Artificial Intelligence, </journal> <volume> 1(60) </volume> <pages> 51-92. </pages> <note> 65 Sonenberg, </note> <author> E., G. Tidhar, E. Werner, D. Kinny, M. Ljungberg, and A. Rao. </author> <year> 1992. </year> <title> Planned team activity. </title> <type> Technical Report 26, </type> <institution> Australian Artificial Intelligence Institute, Australia. </institution>
Reference: <author> Sycara, K. P. </author> <year> 1988. </year> <title> Resolving goal conflicts via negotiation. </title> <booktitle> In: Proceedings of AAAI-88, </booktitle> <pages> pp. 245-250. </pages> <booktitle> American Association of Artificial Intelligence, </booktitle> <address> St. Paul, Minnesota. </address>
Reference: <author> Vermazen, Bruce. </author> <year> 1993. </year> <title> Objects of intention. </title> <journal> Philosophical Studies, </journal> <volume> 71 </volume> <pages> 223-265. </pages>
Reference: <author> Werner, E. </author> <year> 1987. </year> <title> Cooperating agents: A unified theory of communication and social structure. </title>
Reference-contexts: C prop is the analogue for propositions of C ff for actions. The commonality between intentions-to and intentions-that is that both commit an agent not to adopt conflicting intentions <ref> (Werner, 1987) </ref> and constrain replanning in case of failure (Bratman, 1987). The significant distinction between them is not in the types of objects each relates, but in their connection to means-ends reasoning and in their different presumptions about an agent's ability to act in service of the intention. <p> First, an agent only has intentions-to toward acts for which it is the agent; intentions-that represent its responsibilities with respect to the actions of other agents. Second, agents do not need to know complete recipes for those actions that they are not personally committed to doing <ref> (Werner, 1987) </ref>. In the meals example, Kate and Dan need to establish mutual belief of a recipe for making dinner, namely that it will comprise Kate's making the appetizer, Dan the main course, and the two of them together making the dessert.
Reference: <editor> In: L. Gasser and M. N. Huhns, eds., </editor> <booktitle> Distributed Artificial Intelligence, volume 2 of Research Notes on Artificial Intelligence. </booktitle> <publisher> Pitman/Morgan Kaufmann, </publisher> <address> London/Los Altos, CA, </address> <pages> pp. 3-36. </pages>
Reference: <author> Werner, Eric. </author> <year> 1988. </year> <title> Toward a theory of communication and cooperation for multiagent planning. </title> <booktitle> In: Proceedings of the Second Conference on Theoretical Aspects of Reasoning about Knowledge, </booktitle> <pages> pp. 129-143. </pages> <address> Pacific Grove, California. </address>
Reference: <author> Wooldridge, M. and N. R. Jennings. </author> <year> 1994. </year> <title> Formalizing the cooperative problem solving process. </title> <booktitle> In: Proceedings of the 13th International Workshop on Distributed Intelligence, </booktitle> <pages> pp. 403-417. </pages> <address> Lake Quinalt, WA. </address>
Reference: <author> Zlotkin, G. and J. S. Rosenschein. </author> <year> 1991. </year> <title> Cooperation and conflict resolution via negotiation among autonomous agents in noncooperative domains. </title> <journal> IEEE Transactions on System, Man, and Cybernetics, </journal> <volume> 21(6) </volume> <pages> 1317-1324. </pages> <note> Special Issue on Distributed Artificial Intelligence. 66 </note>
References-found: 66

