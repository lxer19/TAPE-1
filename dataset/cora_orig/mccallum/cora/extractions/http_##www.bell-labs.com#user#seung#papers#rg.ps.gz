URL: http://www.bell-labs.com/user/seung/papers/rg.ps.gz
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00428.html
Root-URL: 
Email: fnds|ddlee|seungg@bell-labs.com  
Title: The Rectified Gaussian Distribution  
Author: N. D. Socci, D. D. Lee and H. S. Seung 
Address: Murray Hill, NJ 07974  
Affiliation: Bell Laboratories, Lucent Technologies  
Abstract: A simple but powerful modification of the standard Gaussian distribution is studied. The variables of the rectified Gaussian are constrained to be nonnegative, enabling the use of nonconvex energy functions. Two multimodal examples, the competitive and cooperative distributions, illustrate the representational power of the rectified Gaussian. Since the cooperative distribution can represent the translations of a pattern, it demonstrates the potential of the rectified Gaussian for modeling pattern manifolds.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. H. Ackley, G. E. Hinton, and T. J. Sejnowski. </author> <title> A learning algorithm for Boltzmann machines. </title> <journal> Cognitive Science, </journal> <volume> 9 </volume> <pages> 147-169, </pages> <year> 1985. </year>
Reference-contexts: Sampling from a standard Gaussian can be done by generating one dimensional normal deviates, followed by a linear transformation. Sampling from a rectified Gaussian requires Monte Carlo methods. Mode-finding and sampling algorithms are basic tools that are important in probabilistic modeling. Like the Boltzmann machine <ref> [1] </ref>, the rectified Gaussian is an undirected graphical model. The rectified Gaussian is a better representation for probabilistic modeling (a) (b) (c) of continuous-valued data. It is unclear whether learning will be more tractable for the rectified Gaussian than it is for the Boltzmann machine.
Reference: [2] <author> G. E. Hinton and Z. Ghahramani. </author> <title> Generative models for discovering sparse distributed representations. </title> <journal> Phil. Trans. Roy. Soc., </journal> <volume> B352:1177-90, </volume> <year> 1997. </year>
Reference-contexts: It is unclear whether learning will be more tractable for the rectified Gaussian than it is for the Boltzmann machine. A different version of the rectified Gaussian was recently introduced by Hinton and Ghahramani <ref> [2, 3] </ref>. Their version is for a single variable, and has a singularity at the origin designed to produce sparse activity in directed graphical models.
Reference: [3] <author> Z. Ghahramani and G. E. Hinton. </author> <title> Hierarchical non-linear factor analysis and topographic maps. </title> <journal> Adv. Neural Info. Proc. Syst., </journal> <volume> 11, </volume> <year> 1998. </year>
Reference-contexts: It is unclear whether learning will be more tractable for the rectified Gaussian than it is for the Boltzmann machine. A different version of the rectified Gaussian was recently introduced by Hinton and Ghahramani <ref> [2, 3] </ref>. Their version is for a single variable, and has a singularity at the origin designed to produce sparse activity in directed graphical models.
Reference: [4] <author> H. S. Seung. </author> <title> How the brain keeps the eyes still. </title> <booktitle> Proc. </booktitle> <institution> Natl. Acad. Sci. USA, </institution> <month> 93 </month> <pages> 13339-13344, </pages> <year> 1996. </year>
Reference-contexts: The present work is inspired by biological neural network models that use continuous dynamical attractors <ref> [4] </ref>.
Reference: [5] <author> R. Ben-Yishai, R. L. Bar-Or, and H. Sompolinsky. </author> <title> Theory of orientation tuning in visual cortex. </title> <booktitle> Proc. </booktitle> <institution> Nat. Acad. Sci. USA, </institution> <month> 92 </month> <pages> 3844-3848, </pages> <year> 1995. </year>
Reference-contexts: The present work is inspired by biological neural network models that use continuous dynamical attractors [4]. In particular, the energy function of the cooperative distribution was previously studied in models of the visual cortex <ref> [5] </ref>, motor cortex [6], and head direction system [7]. 2 ENERGY FUNCTIONS: BOWL, TROUGH, AND SADDLE The standard Gaussian distribution P (x) is defined as P (x) = Z 1 e fiE (x) ; (1) 1 x T Ax b T x : (2) The symmetric matrix A and vector b <p> The minima, or ground states, of the energy function can be found numerically by the methods described earlier. An analytic calculation of the ground states in the large N limit is also possible <ref> [5] </ref>. As shown in Figure 4 (a), each ground state is a lump of activity centered at some angle on the ring. This delocalized pattern of activity is different from the singleton modes of the competitive distribution, and arises from the cooperative interactions between neurons on the ring. <p> Here the energy function has N discrete minima arranged along a ring. In the limit of large N the barriers between these minima become quite small. A reasonable approximation is to regard the energy function as having a continuous line of minima with a ring geometry <ref> [5] </ref>. In other words, the energy surface looks like a curved trough, similar to the bottom of a wine bottle. The mean is the centroid of the ring and is not close to any minimum.
Reference: [6] <author> A. P. Georgopoulos, M. Taira, and A. Lukashin. </author> <title> Cognitive neurophysiology of the motor cortex. </title> <journal> Science, </journal> <volume> 260 </volume> <pages> 47-52, </pages> <year> 1993. </year>
Reference-contexts: The present work is inspired by biological neural network models that use continuous dynamical attractors [4]. In particular, the energy function of the cooperative distribution was previously studied in models of the visual cortex [5], motor cortex <ref> [6] </ref>, and head direction system [7]. 2 ENERGY FUNCTIONS: BOWL, TROUGH, AND SADDLE The standard Gaussian distribution P (x) is defined as P (x) = Z 1 e fiE (x) ; (1) 1 x T Ax b T x : (2) The symmetric matrix A and vector b define the quadratic
Reference: [7] <author> K. Zhang. </author> <title> Representation of spatial orientation by the intrinsic dynamics of the head-direction cell ensemble: a theory. </title> <journal> J. Neurosci., </journal> <volume> 16 </volume> <pages> 2112-2126, </pages> <year> 1996. </year>
Reference-contexts: The present work is inspired by biological neural network models that use continuous dynamical attractors [4]. In particular, the energy function of the cooperative distribution was previously studied in models of the visual cortex [5], motor cortex [6], and head direction system <ref> [7] </ref>. 2 ENERGY FUNCTIONS: BOWL, TROUGH, AND SADDLE The standard Gaussian distribution P (x) is defined as P (x) = Z 1 e fiE (x) ; (1) 1 x T Ax b T x : (2) The symmetric matrix A and vector b define the quadratic energy function E (x).
Reference: [8] <author> D. P. Bertsekas. </author> <title> Nonlinear programming. </title> <publisher> Athena Scientific, </publisher> <address> Belmont, MA, </address> <year> 1995. </year>
Reference-contexts: If the step size is chosen correctly, this algorithm can provably be shown to converge to a stationary point of the energy function <ref> [8] </ref>. In practice, this stationary point is generally a local minimum. Neural networks can also solve quadratic programming problems.
Reference: [9] <author> S. Amari and M. A. Arbib. </author> <title> Competition and cooperation in neural nets. </title> <editor> In J. Metzler, editor, </editor> <booktitle> Systems Neuroscience, </booktitle> <pages> pages 119-165. </pages> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1977. </year>
Reference-contexts: In particular, the reduced probability density between the two peaks would not be representable at all with a single Gaussian. The competitive distribution gets its name because its energy function is similar to the ones that govern winner-take-all networks <ref> [9] </ref>. When N becomes large, the N global minima of the energy function are singleton vectors (fig 3), with one component equal to unity, and the rest zero. This is due to a competitive interaction between the components.
Reference: [10] <author> G. E. Hinton, P. Dayan, and M. Revow. </author> <title> Modeling the manifolds of images of handwritten digits. </title> <journal> IEEE Trans. Neural Networks, </journal> <volume> 8 </volume> <pages> 65-74, </pages> <year> 1997. </year>
Reference-contexts: The cooperative distribution can also be approximated by a mixture of N Gaussians, one for each location of the lump on the ring. A more economical approximation would reduce the number of Gaussians in the mixture, but make each one anisotropic <ref> [10] </ref>. Whether the rectified Gaussian is superior to these mixture models is an empirical question that should be investigated empirically with specific real-world probabilistic modeling tasks.
References-found: 10

