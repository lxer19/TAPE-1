URL: http://www.cse.ogi.edu/~denni/Publications/langevin.ps.Z
Refering-URL: http://www.cse.ogi.edu/~denni/publications.html
Root-URL: http://www.cse.ogi.edu
Title: LU TP 93-13 On Langevin Updating in Multilayer Perceptrons  
Author: Thorsteinn Rognvaldsson 
Note: Solvegatan 14 A, S-223 62 Lund, Sweden Submitted to Neural Computation 1 denni@thep.lu.se  
Affiliation: Department of Theoretical Physics, University of Lund,  
Abstract: The Langevin updating rule, in which noise is added to the weights during learning, is presented and shown to improve learning on problems with initially ill-conditioned Hessians. This is particularly important for multilayer perceptrons with many hidden layers, that often have ill-conditioned Hessians. In addition, Manhattan updating is shown to have a similar effect. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Yau, H., and Wallace, D. </author> <year> 1991. </year> <title> Enlarging the Attractor Basins of Neural Networks with Noisy External Fields. </title> <journal> J. Phys. </journal> <volume> A 24, </volume> <pages> 5639-5650. </pages>
Reference-contexts: Introduction There are numerous examples in the literature of cases where performances of artificial neural networks (ANN) have been improved by clever use of noise in the training. For instance, their generalization ability can be improved by the use of noise-corrupted training patterns <ref> [1, 2, 3, 4, 5] </ref> and they can escape local minima with the help of synaptic noise [6, 7, 8]. This paper presents and analyzes such performance improvements achieved with a very simple method, the so-called Langevin updating (LV), where gaussian noise is added to the weights during training. <p> The scaling constant c comes from the transfer function and equals 0:5 for a <ref> [0; 1] </ref> sigmoid. With two hidden layers in the MLP we get additional terms of O (! 3 ), and similarly for more hidden layers. The rightmost column is N i fi N h elements wide and the two middle columns are N h elements wide.
Reference: [2] <author> Sietsma, J., and Dow, R. </author> <year> 1991. </year> <title> Creating Artificial Neural Networks That Generalize. </title> <journal> Neur. Netw. </journal> <volume> 4, </volume> <pages> 67-79 </pages>
Reference-contexts: Introduction There are numerous examples in the literature of cases where performances of artificial neural networks (ANN) have been improved by clever use of noise in the training. For instance, their generalization ability can be improved by the use of noise-corrupted training patterns <ref> [1, 2, 3, 4, 5] </ref> and they can escape local minima with the help of synaptic noise [6, 7, 8]. This paper presents and analyzes such performance improvements achieved with a very simple method, the so-called Langevin updating (LV), where gaussian noise is added to the weights during training.
Reference: [3] <author> Krogh, A., and Hertz, J. </author> <year> 1992. </year> <title> Generalization in a Linear Perceptron in the Presence of Noise. </title> <journal> J. Phys. </journal> <volume> A 25, </volume> <pages> 1135-1147. </pages>
Reference-contexts: Introduction There are numerous examples in the literature of cases where performances of artificial neural networks (ANN) have been improved by clever use of noise in the training. For instance, their generalization ability can be improved by the use of noise-corrupted training patterns <ref> [1, 2, 3, 4, 5] </ref> and they can escape local minima with the help of synaptic noise [6, 7, 8]. This paper presents and analyzes such performance improvements achieved with a very simple method, the so-called Langevin updating (LV), where gaussian noise is added to the weights during training.
Reference: [4] <author> Holmstrom, L., and Koistinen, P. </author> <year> 1992. </year> <title> Using Additive Noise in BackPropagation Training. </title> <journal> IEEE Trans. Neur. Netw. </journal> <volume> 3, </volume> <pages> 24-38 </pages>
Reference-contexts: Introduction There are numerous examples in the literature of cases where performances of artificial neural networks (ANN) have been improved by clever use of noise in the training. For instance, their generalization ability can be improved by the use of noise-corrupted training patterns <ref> [1, 2, 3, 4, 5] </ref> and they can escape local minima with the help of synaptic noise [6, 7, 8]. This paper presents and analyzes such performance improvements achieved with a very simple method, the so-called Langevin updating (LV), where gaussian noise is added to the weights during training.
Reference: [5] <author> Bishop, C. </author> <year> 1993. </year> <title> Training with Noise is Equivalent to Tikhonov Regularization. </title> <journal> Subm. </journal> <note> to Neur. Comp.. </note>
Reference-contexts: Introduction There are numerous examples in the literature of cases where performances of artificial neural networks (ANN) have been improved by clever use of noise in the training. For instance, their generalization ability can be improved by the use of noise-corrupted training patterns <ref> [1, 2, 3, 4, 5] </ref> and they can escape local minima with the help of synaptic noise [6, 7, 8]. This paper presents and analyzes such performance improvements achieved with a very simple method, the so-called Langevin updating (LV), where gaussian noise is added to the weights during training.
Reference: [6] <author> Hanson, S. </author> <year> 1990. </year> <title> A Stochastic Version of the Delta Rule. </title> <journal> Physica D 42, </journal> <pages> 265-272. </pages>
Reference-contexts: For instance, their generalization ability can be improved by the use of noise-corrupted training patterns [1, 2, 3, 4, 5] and they can escape local minima with the help of synaptic noise <ref> [6, 7, 8] </ref>. This paper presents and analyzes such performance improvements achieved with a very simple method, the so-called Langevin updating (LV), where gaussian noise is added to the weights during training. <p> The initial Hessian for this problem has a condition number of O (10 15 ) and LV updating outperforms all other attempts on the problem. Previous Results At a first glance, these results seem to contradict previous studies where learning of the parity problem is improved with synaptic noise <ref> [6, 7] </ref>. The parity problem is not ill-conditioned and should consequently not be easier to learn for LV updating. There is, however, no disagreement. <p> There is, however, no disagreement. In both these studies the effect comes from escaping local minima that are encountered late in the training; in [7] by keeping a constantly high noise level (20%), and in <ref> [6] </ref> by increasing the noise level if the weight makes mistakes. This effect is not present in the results reported here, the noise is decreased too fast to aid escape from local minima encountered late in the learning process.
Reference: [7] <author> Murray, A. </author> <year> 1992. </year> <title> Multilayer Perceptron Learning Optimized for On-Chip Implementation: A Noise-Robust System. </title> <journal> Neur. Comp. </journal> <volume> 4, </volume> <pages> 366-381. </pages>
Reference-contexts: For instance, their generalization ability can be improved by the use of noise-corrupted training patterns [1, 2, 3, 4, 5] and they can escape local minima with the help of synaptic noise <ref> [6, 7, 8] </ref>. This paper presents and analyzes such performance improvements achieved with a very simple method, the so-called Langevin updating (LV), where gaussian noise is added to the weights during training. <p> The initial Hessian for this problem has a condition number of O (10 15 ) and LV updating outperforms all other attempts on the problem. Previous Results At a first glance, these results seem to contradict previous studies where learning of the parity problem is improved with synaptic noise <ref> [6, 7] </ref>. The parity problem is not ill-conditioned and should consequently not be easier to learn for LV updating. There is, however, no disagreement. <p> The parity problem is not ill-conditioned and should consequently not be easier to learn for LV updating. There is, however, no disagreement. In both these studies the effect comes from escaping local minima that are encountered late in the training; in <ref> [7] </ref> by keeping a constantly high noise level (20%), and in [6] by increasing the noise level if the weight makes mistakes. This effect is not present in the results reported here, the noise is decreased too fast to aid escape from local minima encountered late in the learning process.
Reference: [8] <author> Murray, A., and Edwards, P. </author> <year> 1993. </year> <title> Synaptic Weight Noise During MLP Learning Enhances Fault-Tolerance, Generalisation and Learning Trajectory. </title> <journal> Adv. in Neur. Inf. Proc. Syst. </journal> <volume> 5, </volume> <editor> Hanson, Cowan, and Giles (eds.), </editor> <address> 491-498. </address> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: For instance, their generalization ability can be improved by the use of noise-corrupted training patterns [1, 2, 3, 4, 5] and they can escape local minima with the help of synaptic noise <ref> [6, 7, 8] </ref>. This paper presents and analyzes such performance improvements achieved with a very simple method, the so-called Langevin updating (LV), where gaussian noise is added to the weights during training.
Reference: [9] <author> Guillerm, T., and Cotter, N. </author> <year> 1991. </year> <title> A Diffusion Process for Global Optimization in Neural Networks. </title> <booktitle> Proc. Int. Joint Conf. on Neur. Netw. 1991, </booktitle> <publisher> IEEE Press. </publisher>
Reference-contexts: Asymptotic properties of LV updating have previously been studied in the field of simulated annealing and results can be transferred directly to ANN learning (see <ref> [9] </ref> and references therein). Consequently, our focus will instead be on its positive effects on the initial phase of learning.
Reference: [10] <author> Kushner, H. </author> <year> 1987. </year> <title> Asymptotic Global Behavior for Stochastic Approximation and Diffusion with Slowly Decreasing Noise Effects: Global Minimization via Monte Carlo. </title> <journal> SIAM J. Appl. Math. </journal> <volume> 47, </volume> <pages> 169-185. </pages>
Reference-contexts: (2) where the variance of the gaussian noise ~ is varied during learning. 1 To make sure the system escapes all local minima and ends up in the global minimum when t ! 1, the temperature in eq. (1) should decrease as T (t) / log t for large t <ref> [10] </ref>. The optimal constant of proportionality, the "critical depth", can be calculated from knowledge about the deepest local minima. This annealing schedule is unfortunately prohibitively slow for practical purposes and we instead use a geometric annealing schedule T (t) = kT (t 1) (4) where k &lt; 1.
Reference: [11] <author> Soderberg, B. </author> <year> 1988. </year> <title> On the Complex Langevin Equation. Nucl. </title> <journal> Phys. </journal> <volume> B295, </volume> <pages> 396-408. </pages>
Reference-contexts: This equation can also be modified to allow for other noise correlations, for instance if it is desired to vary the noise with the position in weight space <ref> [11] </ref>. The effect of the Langevin equation is to place the network in a "heat bath" with temperature T and a Gibbs equilibrium distribution.
Reference: [12] <author> Heskes, T., Slijpen, E., and Kappen, B. </author> <year> 1993. </year> <title> Cooling Schedules for Learning in Neural Networks. </title> <journal> Phys. Rev. </journal> <volume> E 47, </volume> <pages> 4457-4464. 10 </pages>
Reference-contexts: The noise level depends on the variation of the training patterns, the procedure by which these are chosen, and the location in weight space <ref> [12, 13] </ref>. In this case, the learning rate takes the role of the temperature and an annealing procedure similar to eq. (3) is needed to guarantee convergence to the global minima [12]. <p> In this case, the learning rate takes the role of the temperature and an annealing procedure similar to eq. (3) is needed to guarantee convergence to the global minima <ref> [12] </ref>. Furthermore, to achieve optimal convergence, the learning rate for the weights should be tuned with the curvature of the energy surface, which is computationally expensive and practically impossible in most applications.
Reference: [13] <author> Leen, T., and Moody, J. </author> <year> 1993. </year> <title> Weight Space Probability Densities in Stochastic Learning: I. Dynamics and Equilibria. </title> <journal> Adv. in Neur. Inf. Proc. Syst. </journal> <volume> 5, </volume> <editor> Hanson, Cowan, and Giles (eds.), </editor> <address> 451-458. </address> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: The noise level depends on the variation of the training patterns, the procedure by which these are chosen, and the location in weight space <ref> [12, 13] </ref>. In this case, the learning rate takes the role of the temperature and an annealing procedure similar to eq. (3) is needed to guarantee convergence to the global minima [12].
Reference: [14] <author> Battiti, R. </author> <year> 1992. </year> <title> First- and Second-Order Methods for Learning: Between Steepest Descent and Newton's Method. </title> <journal> Neur. Comp. </journal> <volume> 4, </volume> <pages> 141-166. </pages>
Reference-contexts: Hessian Analysis ANN learning can be extremely slow if the problem is "stiff" or if the error surface is very flat. The first difficulty, stiffness, is usually handled by using second order training algorithms (see <ref> [14] </ref> for a review), dynamic learning rates (see [15] and references therein), or simply by proper rescaling of the inputs. The second difficulty, flatness, is on the other hand a serious and common problem in ANN applications [16], that cannot be handled by resorting to second order minimization algorithms.
Reference: [15] <author> Schiffmann, W., Joost, M., and Werner, R. </author> <year> 1993. </year> <title> Comparison of Optimized Backpropagation Algorithms. </title> <booktitle> Proc. </booktitle> <address> ESANN'93, Brussels. </address>
Reference-contexts: Hessian Analysis ANN learning can be extremely slow if the problem is "stiff" or if the error surface is very flat. The first difficulty, stiffness, is usually handled by using second order training algorithms (see [14] for a review), dynamic learning rates (see <ref> [15] </ref> and references therein), or simply by proper rescaling of the inputs. The second difficulty, flatness, is on the other hand a serious and common problem in ANN applications [16], that cannot be handled by resorting to second order minimization algorithms. However, LV updating takes care of this problem. <p> Angle brackets denote averages. The rightmost column shows the best classification performance achieved. The latter should be compared with 98.5%, which is the best classification performance reported in ref. <ref> [15] </ref>. The fourth problem is to determine whether a patient is hypothyroid or not [22]. It is a real world data set that has recently been used as "a very hard practical classification task" in a benchmark test of different ANN training algorithms [15], with the conclusion that a variation of <p> the best classification performance reported in ref. <ref> [15] </ref>. The fourth problem is to determine whether a patient is hypothyroid or not [22]. It is a real world data set that has recently been used as "a very hard practical classification task" in a benchmark test of different ANN training algorithms [15], with the conclusion that a variation of MH updating with dynamic individual learning rates is the quickest learning algorithm. The inputs are normalized to zero mean and a root mean square deviation of unity. <p> The inputs are normalized to zero mean and a root mean square deviation of unity. This normalization shortens the convergence time with an order of magnitude compared to the results in <ref> [15] </ref>. The same network architecture and weight initialization procedure is used as in [15], and the momentum term is set to zero. Table 2 shows the average training and generalization errors for networks trained 1000 epochs each. <p> The inputs are normalized to zero mean and a root mean square deviation of unity. This normalization shortens the convergence time with an order of magnitude compared to the results in <ref> [15] </ref>. The same network architecture and weight initialization procedure is used as in [15], and the momentum term is set to zero. Table 2 shows the average training and generalization errors for networks trained 1000 epochs each. No improvement is made with LV updating (MH was not tested on this problem). <p> Conjugate Gradients We have also tested the Conjugate Gradient algorithms on the overlapping gaussians and the Mackey-Glass time series, but they never reach final performances compatible with BP or LV. Similar results on the thyroid classification problem are reported in <ref> [15] </ref>. Acknowledgements I want to thank Tom Heskes and two anonymous reviewers for useful pointers to references. This research was sponsored in part by the Goran Gustafsson Foundation for Research in Natural Sciences and Medicine.
Reference: [16] <author> Saarinen, S., Bramley, R., and Cybenko, G. </author> <year> 1993. </year> <title> Ill-conditioning in Neural Network Training Problems. </title> <journal> SIAM J. Sci. Comp. </journal> <volume> 14, </volume> <pages> 693-714. </pages>
Reference-contexts: The second difficulty, flatness, is on the other hand a serious and common problem in ANN applications <ref> [16] </ref>, that cannot be handled by resorting to second order minimization algorithms. However, LV updating takes care of this problem. The curvature of the error surface is given by the eigenvalues i of the Hessian matrix H (!) = r 2 E (!).
Reference: [17] <author> Peterson, C. and Hartman, E. </author> <year> 1989. </year> <title> Explorations of the Mean Field Theory Learning Algorithm. </title> <journal> Neur. Netw. </journal> <volume> 2, </volume> <pages> 475-494. </pages>
Reference-contexts: more input units than output units, the subspace corresponding to small eigenvalues (the "nullity") will be larger than the subspace associated with large eigenvalues. 3 Note on the Manhattan algorithm As a digression, we note that random search of the "nullity" is also achieved with the Manhattan (MH) updating rule <ref> [17] </ref>: !(t) = M sign [rE (t)] (7) which could explain why the MH algorithm has been more successful than BP on some problems [17, 23]. <p> 3 Note on the Manhattan algorithm As a digression, we note that random search of the "nullity" is also achieved with the Manhattan (MH) updating rule [17]: !(t) = M sign [rE (t)] (7) which could explain why the MH algorithm has been more successful than BP on some problems <ref> [17, 23] </ref>. To see this we rewrite eq. (7) as !(t) = M sign [rE (t)] = rE (t) + ffi!(t) (8) and study the distribution of the "corrections" ffi!. The gradient rE (t) can be considered a random variable for weights inside the "nullity". <p> Results The first problem is to separate two overlapping 10-dimensional gaussian distributions with the same mean but different standard deviations. Earlier studies indicate that LV or MH updating is essential to achieve high quality results on this problem <ref> [17, 19] </ref>. Networks with 10 inputs, 20 hidden and one output unit are trained for 100 epochs, with 1000 pattern presentations per epoch. Initial weight values are ! 2 [0:01; 0:01] and momentum is set to ff = 0:5.
Reference: [18] <author> Lonnblad, L., Peterson, C., and Rognvaldsson, T., </author> <title> work in progress. </title>
Reference-contexts: These heuristic parameter adjustment schemes are all chosen for their simplicity, not for optimality. All simulations are done with the feed-forward network simulation package JETNET 2.2 <ref> [18] </ref>. Results The first problem is to separate two overlapping 10-dimensional gaussian distributions with the same mean but different standard deviations. Earlier studies indicate that LV or MH updating is essential to achieve high quality results on this problem [17, 19].
Reference: [19] <author> Rognvaldsson, T. </author> <year> 1993. </year> <title> Pattern Discrimination using Feed-forward Networks. </title> <journal> Neur. Comp. </journal> <volume> 5, </volume> <pages> 483-491. </pages>
Reference-contexts: Results The first problem is to separate two overlapping 10-dimensional gaussian distributions with the same mean but different standard deviations. Earlier studies indicate that LV or MH updating is essential to achieve high quality results on this problem <ref> [17, 19] </ref>. Networks with 10 inputs, 20 hidden and one output unit are trained for 100 epochs, with 1000 pattern presentations per epoch. Initial weight values are ! 2 [0:01; 0:01] and momentum is set to ff = 0:5.
Reference: [20] <author> Hartman, E., and Keeler, J. </author> <year> 1991. </year> <title> Predicting the Future: Advantages of Semilo-cal Units. </title> <journal> Neur. Comp. </journal> <volume> 3, </volume> <pages> 566-578. </pages>
Reference-contexts: The second problem is to predict the Mackey-Glass time series at time x (t + 85) given x (t); x (t 6); x (t 12) and x (t 18). This problem has been used in several benchmark tests on learning the dynamics of time series (see <ref> [20] </ref> and references therein). The data is translated to its mean value, which speeds up learning. <p> The solid line in (a) and (b) marks the lowest error reported in ref. <ref> [20] </ref>. networks have 4 inputs, two hidden layers with 10 and 5 sigmoidal units, and one linear output unit. The training and test set contain 500 patterns each and the networks are trained for 20000 epochs.
Reference: [21] <author> Johansson, E., Dowla, F., and Goodman, D. </author> <year> 1992. </year> <title> Backpropagation Learning for Multilayer Feed-forward Neural Networks using the Conjugate Gradient Algorithm. </title> <journal> Int. J. Neur. Syst. </journal> <volume> 2, </volume> <pages> 291-301. </pages>
Reference-contexts: By this time the noise level in LV is zero and there is no difference between LV and BP. The third problem, n-parity, has previously been used in benchmark studies where Conjugate Gradient algorithms greatly outperformed BP <ref> [21] </ref>. We study 4, 5 and 6 dimensional parity problems, with the architecture n inputs, 8 hidden units and one output. Using more hidden units than inputs decreases the risk of getting stuck in a local minima. Each network is trained for 10000 epochs or until 100% correct classification.
Reference: [22] <author> Quinlan, J. </author> <year> 1987. </year> <title> Simplifying Decision Trees. </title> <journal> Int. J. Man-Machine Stud., </journal> <volume> 221. </volume>
Reference-contexts: Angle brackets denote averages. The rightmost column shows the best classification performance achieved. The latter should be compared with 98.5%, which is the best classification performance reported in ref. [15]. The fourth problem is to determine whether a patient is hypothyroid or not <ref> [22] </ref>. It is a real world data set that has recently been used as "a very hard practical classification task" in a benchmark test of different ANN training algorithms [15], with the conclusion that a variation of MH updating with dynamic individual learning rates is the quickest learning algorithm.
Reference: [23] <author> Lonnblad, L., Peterson, C., and Rognvaldsson, T. </author> <year> 1992. </year> <title> Mass Reconstruction with a Neural Network. </title> <journal> Phys. Lett. </journal> <volume> B 278, </volume> <pages> 181-186. </pages>
Reference-contexts: 3 Note on the Manhattan algorithm As a digression, we note that random search of the "nullity" is also achieved with the Manhattan (MH) updating rule [17]: !(t) = M sign [rE (t)] (7) which could explain why the MH algorithm has been more successful than BP on some problems <ref> [17, 23] </ref>. To see this we rewrite eq. (7) as !(t) = M sign [rE (t)] = rE (t) + ffi!(t) (8) and study the distribution of the "corrections" ffi!. The gradient rE (t) can be considered a random variable for weights inside the "nullity".
Reference: [24] <author> Ohlsson, M., Peterson, C., Pi, H., Rognvaldsson, T., and Soderberg, B. </author> <year> 1994. </year> <title> "Predicting Utility Loads with Artificial Neural Networks Methods and Results from the Great Energy Predictor Shootout". </title> <note> To appear in 1994 ASHRAE Trans. 100, part 2. 11 </note>
Reference-contexts: Different data subsets may produce different "subnullities", but if these "subnullities" have no overlap, the effect on the training will be negligible (see fig. 3d). We have verified these results on a fifth problem; predicting the solar insolation onto a building using a two hidden layer MLP <ref> [24] </ref>. The initial Hessian for this problem has a condition number of O (10 15 ) and LV updating outperforms all other attempts on the problem.
References-found: 24

