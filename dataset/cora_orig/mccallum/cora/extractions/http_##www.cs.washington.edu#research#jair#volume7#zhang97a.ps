URL: http://www.cs.washington.edu/research/jair/volume7/zhang97a.ps
Refering-URL: http://www.cs.washington.edu/research/jair/abstracts/zhang97a.html
Root-URL: 
Email: lzhang@cs.ust.hk  wliu@cs.ust.hk  
Title: A Model Approximation Scheme for Planning in Partially Observable Stochastic Domains  
Author: Nevin L. Zhang Wenju Liu 
Address: Hong Kong, China  
Affiliation: Department of Computer Science Hong Kong University of Science and Technology  
Note: Journal of Artificial Intelligence Research 7 (1997) 199-230 Submitted 5/97; published 11/97  
Abstract: Partially observable Markov decision processes (POMDPs) are a natural model for planning problems where effects of actions are nondeterministic and the state of the world is not completely observable. It is difficult to solve POMDPs exactly. This paper proposes a new approximation scheme. The basic idea is to transform a POMDP into another one where additional information is provided by an oracle. The oracle informs the planning agent that the current state of the world is in a certain region. The transformed POMDP is consequently said to be region observable. It is easier to solve than the original POMDP. We propose to solve the transformed POMDP and use its optimal policy to construct an approximate policy for the original POMDP. By controlling the amount of additional information that the oracle provides, it is possible to find a proper tradeoff between computational time and approximation quality. In terms of algorithmic contributions, we study in details how to exploit region observability in solving the transformed POMDP. To facilitate the study, we also propose a new exact algorithm for general POMDPs. The algorithm is conceptually simple and yet is significantly more efficient than all previous exact algorithms.
Abstract-found: 1
Intro-found: 1
Reference: <author> Bellman, R. </author> <year> (1957). </year> <title> Dynamic Programming. </title> <publisher> Princeton University Press. </publisher>
Reference-contexts: Such a policy is called an optimal policy. The value function of an optimal policy is called the optimal value function and is denoted by V fl . 204 Model Approximation for Planning under Uncertainty 3.4 Value Iteration Value iteration is a standard way for solving infinite horizon MDPs <ref> (Bellman, 1957) </ref>. It begins with an arbitrary initial function V fl 0 (b) and improves it iteratively by using the following equation V fl X P (o + jb; a)V fl where b + is the belief state given by equation (1).
Reference: <author> Bertsekas, D. P. </author> <year> (1987). </year> <title> Dynamic Programming: Deterministic and Stochastic Models. </title> <publisher> Prentice-Hall. </publisher>
Reference: <author> Bertsekas, D. P., & Castanon, D. C. </author> <year> (1989). </year> <title> Adaptive Aggregation for Infinite Horizon Dynamic Programming. </title> <journal> IEEE trans. on auto. control, </journal> <volume> vol 34, No 6. </volume>
Reference: <author> Boutilier, C., Dearden, R., & Goldszmidt, M. </author> <year> (1995). </year> <title> Exploiting structures in policy construction. </title> <booktitle> In Proceedings of IJCAI-95, </booktitle> <pages> 1104-1111. </pages>
Reference: <author> Boutilier, C., & Poole, D. </author> <year> (1996). </year> <title> Computing optimal policies for partially observable decision processes using compact representations. </title> <booktitle> In Proceedings of AAAI-96, </booktitle> <pages> 1168-1175. </pages>
Reference: <author> Brafman, R. I. </author> <year> (1997). </year> <title> A heuristic variable grid solution method for POMDPs. </title> <booktitle> In Proceedings of AAAI-97, </booktitle> <pages> 727-733. </pages>
Reference: <author> Cassandra, A. R. </author> <year> (1994). </year> <title> Optimal polices for partially observable Markov decision processes. </title> <type> TR CS-94-14, </type> <institution> Department of Computer Science, Brown University, </institution> <address> Providence, Rhode Island 02912, USA. </address>
Reference-contexts: Due to this theorem, we say that the collection V t of state value functions is representation of V fl 4.5 Parsimonious Representations The size of V t increases exponentially with t. As a matter of fact, the total number of t-step policy trees <ref> (Cassandra, 1994) </ref> is: jP t j = jAj jOj1 : There are potentially the same number of t-step state value functions. Fortunately, many of the state value functions can be pruned without affecting the induced belief space function.
Reference: <author> Cassandra, A. R., Kaelbling, L. P., & Littman, M. L. </author> <year> (1994). </year> <title> Acting optimally in partially observable stochastic domains. </title> <booktitle> In Proceedings of AAAI-94, </booktitle> <pages> 1023-1028. </pages>
Reference-contexts: Due to this theorem, we say that the collection V t of state value functions is representation of V fl 4.5 Parsimonious Representations The size of V t increases exponentially with t. As a matter of fact, the total number of t-step policy trees <ref> (Cassandra, 1994) </ref> is: jP t j = jAj jOj1 : There are potentially the same number of t-step state value functions. Fortunately, many of the state value functions can be pruned without affecting the induced belief space function.
Reference: <author> Cassandra, A. R., Kaelbling, L. P., & Kurien, J. </author> <year> (1996). </year> <title> Acting under uncertainty: Discrete Bayesian models for mobile-robot navigation. </title> <booktitle> In Proceedings of IEEE/Robotics Society of Japan Conference on Intelligent Robotics and Systems (IROS-96). </booktitle>
Reference: <author> Cassandra, A. R., Littman, M. L., & Zhang, N. L. </author> <year> (1997). </year> <title> Incremental pruning: A simple, fast, exact method for partially observable Markov decision processes. </title> <booktitle> In Proceedings of Thirteenth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> 54-61. </pages>
Reference-contexts: Region-Based Model Approximation We have so far been concerned with exact algorithms. Experiments with incremental pruning, presently the most efficient exact algorithm, have revealed that it can solve only small POMDPs <ref> (Cassandra et al., 1997) </ref>. One needs to resort to approximation in order to solve large real-world problems. Most previous approximation methods solve a POMDP directly; they approximate the t-step optimal value function of the POMDP.
Reference: <author> Cheng, H. T. </author> <year> (1988). </year> <title> Algorithms for partially observable Markov decision processes. </title> <type> PhD thesis, </type> <institution> University of British Columbia, Vancouver, BC, Canada. </institution>
Reference: <author> Dean, T. L., Givan, R., & Leach, S. </author> <year> (1997). </year> <title> Model reduction techniques for computing approximately optimal solution for Markov decision processes. </title> <booktitle> In Proceedings of the Thirteenth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> 124-131. </pages>
Reference: <author> Dean, T. L., Kaelbling, L. P., Kirman, J., & Nicholson A. </author> <year> (1993). </year> <title> Planning with deadlines in stochastic domains. </title> <booktitle> In Proceedings of AAAI-93, </booktitle> <pages> 574-579. </pages>
Reference: <author> Dean T. L., & Lin, S. H. </author> <year> (1995). </year> <title> Decomposition techniques for planning in stochastic domains. </title> <type> TR CS-95-10, </type> <institution> Department of Computer Science, Brown University, </institution> <address> Providence, Rhode Island 02912, USA. </address>
Reference: <author> Dean, T. L., & Wellman, M. P. </author> <year> (1991). </year> <note> Planning and Control. Morgan Kaufmann. </note> <author> 229 Zhang & Liu Eagle, J. N. </author> <year> (1984). </year> <title> The optimal search for a moving target when the search path is constrained. </title> <journal> Operations Research, </journal> <volume> 32(5), </volume> <pages> 1107-1115. </pages>
Reference: <author> Hauskrecht, M. </author> <year> (1997). </year> <title> Incremental methods for computing bounds in partially observable Markov decision processes. </title> <booktitle> In Proceedings of AAAI-97, </booktitle> <pages> 734-739. </pages>
Reference: <author> Littman, M. L. </author> <year> (1994). </year> <title> The witness algorithm: Solving partially observable Markov decision processes. </title> <type> TR CS-94-40, </type> <institution> Department of Computer Science, Brown University, </institution> <address> Providence, Rhode Island 02912, USA. </address>
Reference-contexts: Storing these values exactly could require unbounded precision. Approximations are implicitly being made due to the fact that machine precision is bounded. 203 Zhang & Liu where k=1= P s;s + P (s + ; o + js; a)b (s) is the normalization constant <ref> (e.g., Littman, 1994) </ref>. 3.2 POMDPs as MDPs For any belief state b and any action a, define r (b; a) = s It is the expected immediate reward for taking action a in belief state b. <p> Fortunately, it can be carried out implicitly due to the piecewise linearity of the t-step optimal value functions. To explain piecewise linearity, we need the concept of policy trees. 4.1 Policy Trees A t-step policy tree p t <ref> (Littman, 1994) </ref> prescribes an action for the current time point and an action for each possible information scenario (o 1 ; : : : ; o i ; a 0 ; : : : ; a i1 ) at each of the next t1 time points i. <p> Theorem 3 All parsimonious coverings of a set of state space functions consist of the same number of state space functions. 2 The theorem has been known for sometime <ref> (e.g., Littman, 1994) </ref>. Due to this theorem, one can also define a parsimonious covering as a covering that contains the minimum number of state space functions. <p> A correct way to break ties is as follows: Fix an ordering among states in R. This induces a lexicographic ordering among all state space functions. Among the tied state space functions, chose the one that is the largest under the lexicographic ordering <ref> (Littman, 1994) </ref>. The following implementation of purge is based on Lark's algorithm (White, 1991). Procedure purge (W; R) * Inputs: W | A set of state space functions, R | A region. * Output: A set of state space functions that parsimoniously covers W in region R. 1.
Reference: <author> Littman, M. L., Cassandra, A. R., & Kaelbling, L. P. </author> <year> (1995). </year> <title> Efficient dynamic-programming updates in partially observable Markov decision processes. </title> <type> TR CS-95-19, </type> <institution> Department of Computer Science, Brown University, </institution> <address> Providence, Rhode Island 02912, USA. </address>
Reference-contexts: The process of computing a parsimonious covering of V t from a parsimonious covering of V t1 is called dynamic-programming updates <ref> (Littman et al., 1995) </ref>. It is a key step in algorithms that solve POMDPs via value iteration. <p> The witness algorithm has been empirically proved to be the most efficient among all those algorithms <ref> (Littman et al., 1995) </ref>. 4.7 Implicit Value Iteration The procedure solvePOMDP shown in Figure 2 carries out value iteration implicitly: instead inductively computing the t-step optimal value function V fl t itself, it computes a parsimonious covering of V t | a set of state space functions that represents V fl
Reference: <author> Lovejoy, W. S. </author> <year> (1991a). </year> <title> A survey of algorithmic methods for solving partially observable Markov decision processes. </title> <journal> Annals of Operations Research, </journal> <volume> 28 (1), </volume> <pages> 47-65. </pages>
Reference-contexts: The problem is that the number of ways the agent can behave is exponential in the number of future steps considered. 1.2 Previous Work Previous methods for solving POMDPs are usually classified into exact methods and approximate methods <ref> (Lovejoy, 1991a) </ref>. They can also can be classified according to which of the aforementioned two difficulties they directly address. Most previous methods address the difficulty of exponential number of future behaviors (Sondik, 1971; Sondik & Mendelssohn, 1979; Monahan, 1982; Cheng, 1988; Lovejoy, 1991b; and Cassandra et al., 1994). <p> Algorithms for POMDPs are classified into exact or approximation algorithms depending on whether they compute the t-step optimal value function V fl t exactly <ref> (Lovejoy, 1991a) </ref>. In the next two sections, we discuss the theoretical foundations of exact algorithms and develop a new exact algorithm. Thereafter, we propose a new approximation algorithm. 4. Piecewise Linearity and Implicit Value Iteration Since the belief space is continuous, exact value iteration cannot be carried out explicitly.
Reference: <author> Lovejoy, W. S. </author> <year> (1991b). </year> <title> Computationally feasible bounds for partially observed Markov decision processes. </title> <journal> Operations Research, </journal> <volume> 39 (1), </volume> <pages> 162-175. </pages>
Reference: <author> Monahan, G. E. </author> <year> (1982). </year> <title> A survey of partially observable Markov decision processes: theory, models, and algorithms. </title> <journal> Management Science, </journal> <volume> 28 (1), </volume> <pages> 1-16. </pages>
Reference: <author> Parr, R., & Russell, S. </author> <year> (1995). </year> <title> Approximating optimal polices for partially observable stochastic domains. </title> <booktitle> In Proceedings of IJCAI-95, </booktitle> <pages> 1088-1094. </pages>
Reference: <author> Platzman, L. K. </author> <year> (1977). </year> <title> Finite-memory estimation and control of finite probabilistic systems. </title> <type> Ph.D. Thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology. </institution>
Reference: <author> Puterman, M. L. </author> <year> (1990). </year> <title> Markov decision processes. </title> <editor> In D. P. Heyman and M. J. Sobel (eds.), </editor> <title> Handbooks in OR & MS., </title> <journal> Elsevier Science Publishers, </journal> <volume> Vol. 2, </volume> <pages> 331-434. </pages>
Reference-contexts: A policy 1 dominates another policy 2 if for each belief state b2B V 1 (b) V 2 (b): (5) Domination is a partial ordering among policies. It is well known that there exists a policy that dominates all other policies <ref> (e.g., Puterman, 1990) </ref>. Such a policy is called an optimal policy. <p> If V fl 0 =0, then V fl t is called the t-step optimal value function. For any belief state b, V fl t (b) is the optimal expected reward the agent can get in t steps starting from b. The following theorem <ref> (Puterman, 1990, page 361) </ref> tells one when to terminate value iteration and how to construct a "good enough" policy.
Reference: <author> Sondik, E. J. </author> <year> (1971). </year> <title> The optimal control of partially observable Markov processes. </title> <type> PhD thesis, </type> <institution> Stanford University, Stanford, California, USA. </institution>
Reference: <author> Sondik, E. J., & Mendelssohn, R. </author> <year> (1979). </year> <title> Information seeking in Markov decision processes, </title> <institution> Southwest Fisheries Center Administrative Report H-79-13, National Marine Fisheries Service, </institution> <address> Honolulu, Hawaii. </address>
Reference: <author> White III, C. C. </author> <year> (1991). </year> <title> Partially observed Markov decision processes: A survey. </title> <journal> Annals of Operations Research, </journal> <volume> 32. </volume>
Reference-contexts: It is a key step in algorithms that solve POMDPs via value iteration. Previous algorithms for dynamic-programming updates include the enumeration and pruning algorithms by Monahan (1992), Eagle (1984), and Lark <ref> (White, 1991) </ref>, the one-pass algorithm by Sondik (1971), the linear support and relaxed region algorithms by Cheng (1988), and the witness algorithm by Cassandra et al. (1994) and Littman (1994). <p> This induces a lexicographic ordering among all state space functions. Among the tied state space functions, chose the one that is the largest under the lexicographic ordering (Littman, 1994). The following implementation of purge is based on Lark's algorithm <ref> (White, 1991) </ref>. Procedure purge (W; R) * Inputs: W | A set of state space functions, R | A region. * Output: A set of state space functions that parsimoniously covers W in region R. 1. W pointwisePurge (W; R). 2. X ;. 3.
Reference: <author> White, D. J. </author> <year> (1993). </year> <title> Markov Decision Processes. </title> <publisher> John Wiley & Sons. </publisher>
Reference: <author> White III, C. C., & Scherer, W. T., </author> <year> (1994). </year> <title> Finite-memory suboptimal design for partially observed Markov decision processes. </title> <journal> Operations Research, </journal> <volume> 42(3), </volume> <pages> 440-455. 230 </pages>
References-found: 29

