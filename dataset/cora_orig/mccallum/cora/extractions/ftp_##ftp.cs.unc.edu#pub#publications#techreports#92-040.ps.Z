URL: ftp://ftp.cs.unc.edu/pub/publications/techreports/92-040.ps.Z
Refering-URL: ftp://ftp.cs.unc.edu/pub/publications/techreports/FILE.html
Root-URL: http://www.cs.unc.edu
Email: -prins,palmerd-@cs.unc.edu  
Title: Transforming High-Level Data-Parallel Programs into Vector Operations  
Author: Jan F. Prins and Daniel W. Palmer 
Address: Chapel Hill NC 27599-3175 (919)-962-1913  
Affiliation: Department of Computer Science, University of North Carolina  
Abstract: Fully-parallel execution of a high-level data-parallel language based on nested sequences, higher order functions and generalized iterators can be realized in the vector model using a suitable representation of nested sequences and a small set of transformational rules to distribute iterators through the constructs of the language. 
Abstract-found: 1
Intro-found: 1
Reference: [Back78] <author> Backus, J., </author> <title> "Can Programming be Liberated from the VonNeumann Style? A Functional Style and its Algebra of Programs", </title> <journal> Communications of the ACM, </journal> <year> 1978. </year>
Reference-contexts: A nested array This work supported in part by DARPA/ISTO Contract N00014-91-C-0114. 1 The Proteus language is a component of the DARPA CPL (Common Prototyping Language) effort. 2 foundation for APL was described by [More79], and can be found in NIAL, APL2, J, SETL and FP <ref> [Back78] </ref>. Although data-parallel programs are conveniently specified in these languages, they can only be executed sequentially due to the complex and fine-grain synchronization requirements in a parallel implementation of general data-parallelism. Thus these languages are not parallel programming languages.
Reference: [BCS+90] <author> Blelloch, G., Chatterjee, S., Sipelstein, J., Zahga, M., "CVL: </author> <title> A C Vector-Library", </title> <type> Draft Technical Note, </type> <institution> Carnegie Mellon University, </institution> <year> 1990. </year>
Reference-contexts: Vector-Model Representation and Operations Expressions formed using the data-parallel notation P of the previous section will be transformed and then translated to an implementation of vector-model parallelism [Blel90] such as C with the C Vector Library of <ref> [BCS+90] </ref>. Here we characterize such an implementation V as a flat, low-level data-parallel notation with the following types and operations.
Reference: [Blel90] <author> Blelloch, G., </author> <title> Vector Models for Data-Parallel Computing, </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Yet this is the key step in any parallel divide-and-conquer sorting algorithm. Indeed, there is extensive evidence that nested data-parallelism is an important component in the compact expression of efficient parallel computations <ref> [Blel90, Skil90, MNP+91] </ref>. The difficulty is not in the languages, since general data-parallel languages can easily express nested parallel computations. Rather the problem lies in the difficulty of translating nested parallelism to achieve fully-parallel execution. <p> The difficulty is not in the languages, since general data-parallel languages can easily express nested parallel computations. Rather the problem lies in the difficulty of translating nested parallelism to achieve fully-parallel execution. A major step in this direction was developed in <ref> [Blel90] </ref> where it was shown that for nested sequence aggregates subject to a restricted set of operations, an equivalent vector model program operating on partitioned (segmented) flat sequences can be derived. The vector model is efficiently executed on a wide class of parallel machines. <p> Vector-Model Representation and Operations Expressions formed using the data-parallel notation P of the previous section will be transformed and then translated to an implementation of vector-model parallelism <ref> [Blel90] </ref> such as C with the C Vector Library of [BCS+90]. Here we characterize such an implementation V as a flat, low-level data-parallel notation with the following types and operations.
Reference: [Blel92] <author> Blelloch, G., NESL: </author> <title> A Nested Data-Parallel Language, </title> <type> Technical Report CMU-CS-92-103, </type> <institution> Carnegie Mellon University, </institution> <month> January </month> <year> 1990. </year>
Reference-contexts: This requires extensive bookkeeping and use of low-level facilities; thus, we believe that the expressive utility of nested parallelism in this setting is limited. In [BS90] it is shown how to compile a subset of Paralation LISP into vector model code. More recently, the nested vector model language NESL <ref> [Blel92] </ref> has used similar techniques to yield vector model code.
Reference: [BS90] <author> Blelloch, G., Sabot, G., </author> <title> "Compiling Collection-Oriented Languages onto Massively Parallel Computers", </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8(2), </volume> <month> February </month> <year> 1990. </year>
Reference-contexts: This requires extensive bookkeeping and use of low-level facilities; thus, we believe that the expressive utility of nested parallelism in this setting is limited. In <ref> [BS90] </ref> it is shown how to compile a subset of Paralation LISP into vector model code. More recently, the nested vector model language NESL [Blel92] has used similar techniques to yield vector model code.
Reference: [CBZ90] <author> Chatterjee, S., Blelloch, G., Zagha, M., </author> <title> "Scan Primitives for Vector Computers", </title> <booktitle> Proceedings Supercomputing '90, IEEE , 1990. </booktitle>
Reference: [HQ91] <author> Hatcher, P., Quinn, M., </author> <title> Data-Parallel Programming on MIMD Computers, </title> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference: [Iver62] <author> Iverson, K., </author> <title> A Programming Language. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1962. </year>
Reference-contexts: The data-parallel constructs of Proteus permit the construction and manipulation of aggregate values (sets or sequences) and, in particular, include the ability to apply a function in parallel to all elements of an aggregate value to yield an aggregate result. High-level programming languages like APL <ref> [Iver62] </ref> and SETL [Schw70] pioneered the inclusion of data-parallel constructs to gain expressive power by bringing the languages closer to familiar and powerful mathematical notations. The aggregate in the original APL language was the flat array, an array whose elements are all scalar values.
Reference: [KLS+90] <author> Knobe, K., Lukas, J., Steele, G., </author> <title> "Data Optimization: Allocation of Arrays to Reduce Communication on SIMD Machines", </title> <journal> Journal of Parallel and Distributed Computing 8, </journal> <year> 1990. </year>
Reference-contexts: More recent languages like CMFortran and C* are portable across various SIMD and MIMD machines. The aggregates in these languages are restricted to flat arrays distributed in a regular manner over processors in an effort to predict and minimize communication requirements in execution <ref> [KLS+90, Prin90] </ref>. Because aggregates are flat, only a limited class of arithmetic and logical operations may be applied in a data-parallel fashion. Consequently, using these languages, it is not possible to directly express nested parallelism the data-parallel application of a function which is itself data-parallel.
Reference: [Mag79] <author> Mag, G., </author> <title> "A Network of Computers to Execute Reduction Languages." </title> <journal> International Journal of Computer and Information Sciences, </journal> <year> 1979. </year>
Reference: [McCr87] <author> McCrosky, C., </author> <title> "Realizing the Parallelism of Array-based Computation", </title> <booktitle> Parallel Computing 10 1989. </booktitle>
Reference-contexts: CM Lisp [SH86] and Paralation Lisp [Sabo88] are fully-general data-parallel languages implemented as high-level programming languages for the Connection Machine. However, implementations of these languages apply nested data-parallel operations in a serial fashion. McCrosky <ref> [McCr87] </ref> describes a way to represent the nested arrays of APL and gives implementations for APL primitives on a SIMD execution model, but nested parallel execution is also not addressed.
Reference: [MNP+91] <author> Mills, P., Nyland, L., Prins, J., Reif, J., Wagner, </author> <title> R.,"Prototyping Parallel and Distributed Programs in Proteus", </title> <booktitle> Proceedings Symposium on Parallel and Distributed Processing 92. </booktitle> <year> 1992. </year>
Reference-contexts: To make things worse, little or none of this effort may be portable to other settings. Proteus 1 is a high-level language designed for the prototyping of parallel computations <ref> [MNP+91, NP92] </ref> . A Proteus program specifies parallelism in a high-level and machine-independent fashion. The parallel semantics of such a program can be simulated sequentially, to observe and assess its behavior. <p> Yet this is the key step in any parallel divide-and-conquer sorting algorithm. Indeed, there is extensive evidence that nested data-parallelism is an important component in the compact expression of efficient parallel computations <ref> [Blel90, Skil90, MNP+91] </ref>. The difficulty is not in the languages, since general data-parallel languages can easily express nested parallel computations. Rather the problem lies in the difficulty of translating nested parallelism to achieve fully-parallel execution.
Reference: [MNP+92] <author> Mills, P., Nyland, L., Prins, J., Reif, J., </author> <booktitle> "Prototyping N-body Simulations in Proteus" , Proceedings IPPS 92, IEEE, </booktitle> <year> 1992. </year>
Reference-contexts: Discussion Status An early, problemspecific form of the transformations was implemented in KIDS and was used to transform a simple data-parallel computation <ref> [MNP+92] </ref>, although the final translation step was performed manually. We are currently implementing the general transformation rules and constructing CVL versions of the basic data-parallel operations. Optimizations Because they are so frequently applied, it is critically important that the insert/extract operations have minimal overhead.
Reference: [More79] <author> More, T. </author> <title> "The Nested Rectangular Array as a Model of Data" APL79 Conference Proceedings. </title> <booktitle> ACM 1979. </booktitle>
Reference-contexts: A nested array This work supported in part by DARPA/ISTO Contract N00014-91-C-0114. 1 The Proteus language is a component of the DARPA CPL (Common Prototyping Language) effort. 2 foundation for APL was described by <ref> [More79] </ref>, and can be found in NIAL, APL2, J, SETL and FP [Back78]. Although data-parallel programs are conveniently specified in these languages, they can only be executed sequentially due to the complex and fine-grain synchronization requirements in a parallel implementation of general data-parallelism.
Reference: [NP92] <author> Nyland, L., Prins, J., </author> <title> "Prototyping Parallel Programs", </title> <booktitle> Proceedings 1992 Dartmouth Institute for Advanced Graduate Studies in Parallel Computing Symposium, </booktitle> <year> 1992. </year>
Reference-contexts: To make things worse, little or none of this effort may be portable to other settings. Proteus 1 is a high-level language designed for the prototyping of parallel computations <ref> [MNP+91, NP92] </ref> . A Proteus program specifies parallelism in a high-level and machine-independent fashion. The parallel semantics of such a program can be simulated sequentially, to observe and assess its behavior.
Reference: [Prin90] <author> Prins, J., </author> <title> "A Framework for Efficient Execution of Array-Based Languages on SIMD Computers", </title> <booktitle> Proceedings Frontiers 90, IEEE 1990. </booktitle>
Reference-contexts: More recent languages like CMFortran and C* are portable across various SIMD and MIMD machines. The aggregates in these languages are restricted to flat arrays distributed in a regular manner over processors in an effort to predict and minimize communication requirements in execution <ref> [KLS+90, Prin90] </ref>. Because aggregates are flat, only a limited class of arithmetic and logical operations may be applied in a data-parallel fashion. Consequently, using these languages, it is not possible to directly express nested parallelism the data-parallel application of a function which is itself data-parallel.
Reference: [PTH91] <author> Philippsen, M., Tichy, W., Herter, C., </author> <title> "Modula-2* and its Compilation", </title> <booktitle> Proceedings Austrian Conference on Parallel Computing, </booktitle> <year> 1991. </year>
Reference-contexts: However, implementations of these languages apply nested data-parallel operations in a serial fashion. McCrosky [McCr87] describes a way to represent the nested arrays of APL and gives implementations for APL primitives on a SIMD execution model, but nested parallel execution is also not addressed. Philippsen <ref> [PTH91] </ref> describes an implementation of nested parallelism in Modula-2* for a SIMD computer, but Modula-2* has no data-parallel nested aggregates. So while nested parallel operations may be applied, the programmer must 3 orchestrate their parallel access to the appropriate portions of a shared global variable.
Reference: [Sabo88] <author> Sabot, G., </author> <title> The Paralation Model : Architecture-Independent Parallel Programing. </title> <publisher> MIT Press, </publisher> <year> 1988. </year>
Reference-contexts: Related work Many researchers have addressed the problem of deriving parallel programs by transformation. In this paper, we are concerned specifically with the translation of data-parallelism, so we restrict our review of related work to that concerned with the implementation of nested parallelism. CM Lisp [SH86] and Paralation Lisp <ref> [Sabo88] </ref> are fully-general data-parallel languages implemented as high-level programming languages for the Connection Machine. However, implementations of these languages apply nested data-parallel operations in a serial fashion.
Reference: [Schw70] <author> Schwartz, J, </author> <title> "Set Theory as a Language for Program Specification and Programming" Technical Report Computer Science Department, </title> <institution> Courant Institute of Mathematical Sciences, </institution> <address> New York University, </address> <year> 1970. </year>
Reference-contexts: The data-parallel constructs of Proteus permit the construction and manipulation of aggregate values (sets or sequences) and, in particular, include the ability to apply a function in parallel to all elements of an aggregate value to yield an aggregate result. High-level programming languages like APL [Iver62] and SETL <ref> [Schw70] </ref> pioneered the inclusion of data-parallel constructs to gain expressive power by bringing the languages closer to familiar and powerful mathematical notations. The aggregate in the original APL language was the flat array, an array whose elements are all scalar values.
Reference: [Skil90] <author> Skillicorn, D., </author> <note> "Architecture-Independent Parallel Computation" IEEE Computer 11, Vol.23 No. 12 (Dec. 1990) pp.38-50. </note>
Reference-contexts: Yet this is the key step in any parallel divide-and-conquer sorting algorithm. Indeed, there is extensive evidence that nested data-parallelism is an important component in the compact expression of efficient parallel computations <ref> [Blel90, Skil90, MNP+91] </ref>. The difficulty is not in the languages, since general data-parallel languages can easily express nested parallel computations. Rather the problem lies in the difficulty of translating nested parallelism to achieve fully-parallel execution.
Reference: [Smit90] <author> Smith, D., </author> <title> "KIDS - A Semiautomatic Program Development System", </title> <journal> IEEE Transactions on Software Engineering Special Issue on Formal Methods in Software Engineering Vol 16, </journal> <volume> No.9, </volume> <year> 1990. </year>
Reference: [SH86] <author> Steele, G. L., Hillis, W., </author> <title> "Connection Machine LISP: </title> <booktitle> Fine-grained Parallel Symbolic Processing " Proceedings 1986 ACM Conference on Lisp and Function Programming ACM SIGPLAN/SIGACT/SIGART, </booktitle> <year> 1986. </year> <note> A </note>
Reference-contexts: Related work Many researchers have addressed the problem of deriving parallel programs by transformation. In this paper, we are concerned specifically with the translation of data-parallelism, so we restrict our review of related work to that concerned with the implementation of nested parallelism. CM Lisp <ref> [SH86] </ref> and Paralation Lisp [Sabo88] are fully-general data-parallel languages implemented as high-level programming languages for the Connection Machine. However, implementations of these languages apply nested data-parallel operations in a serial fashion.
References-found: 22

