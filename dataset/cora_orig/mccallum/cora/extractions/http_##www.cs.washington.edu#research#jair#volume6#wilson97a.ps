URL: http://www.cs.washington.edu/research/jair/volume6/wilson97a.ps
Refering-URL: http://www.cs.washington.edu/research/jair/abstracts/wilson97a.html
Root-URL: 
Email: RANDY@AXON.CS.BYU.EDU  MARTINEZ@CS.BYU.EDU  
Title: Improved Heterogeneous Distance Functions  
Author: D. Randall Wilson Tony R. Martinez 
Address: Provo, UT 84602, USA  
Affiliation: Computer Science Department Brigham Young University  
Note: Journal of Artificial Intelligence Research 6 (1997) 1-34 Submitted 5/96; published 1/97 1997 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.  
Abstract: Instance-based learning techniques typically handle continuous and linear input values well, but often do not handle nominal input attributes appropriately. The Value Difference Metric (VDM) was designed to find reasonable distance values between nominal attribute values, but it largely ignores continuous attributes, requiring discretization to map continuous values into nominal values. This paper proposes three new heterogeneous distance functions, called the Heterogeneous Value Difference Metric (HVDM), the Interpolated Value Difference Metric (IVDM), and the Windowed Value Difference Metric (WVDM). These new distance functions are designed to handle applications with nominal attributes, continuous attributes, or both. In experiments on 48 applications the new distance metrics achieve higher classification accuracy on average than three previous distance functions on those datasets that have both nominal and continuous attributes.
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, David W., </author> <year> (1992). </year> <title> Tolerating noisy, irrelevant and novel attributes in instance-based learning algorithms. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> Vol. 36, </volume> <pages> pp. 267-287. </pages>
Reference: <author> Aha, David W., Dennis Kibler, and Marc K. Albert, </author> <year> (1991). </year> <title> Instance-Based Learning Algorithms. </title> <journal> Machine Learning, </journal> <volume> Vol. 6, </volume> <pages> pp. 37-66. </pages>
Reference-contexts: He found that using discretization to preprocess data often degraded accuracy, and recommended that machine learning algorithms be designed to handle continuous attributes directly. Ting (1994, 1996) used several different discretization techniques in conjunction with MVDM and IB1 <ref> (Aha, Kibler & Albert, 1991) </ref>. His results showed improved generalization accuracy when using discretization. Discretization allowed his algorithm to use MVDM on all attributes instead of using a linear distance on continuous attributes, and thus avoided some of the normalization problems discussed above in Sections 3.1 and 3.2.
Reference: <author> WILSON & MARTINEZ 30 Atkeson, Chris, </author> <year> (1989). </year> <title> Using local models to control movement. </title> <editor> In D. S. Touretzky (Ed.), </editor> <booktitle> Advances in Neural Information Processing Systems 2. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Atkeson, Chris, Andrew Moore, and Stefan Schaal, </author> <year> (1996). </year> <title> Locally weighted learning. </title> <note> To appear in Artificial Intelligence Review. </note>
Reference-contexts: Distance functions are also used in many fields besides machine learning and neural networks, including statistics <ref> (Atkeson, Moore & Schaal, 1996) </ref>, pattern recognition (Diday, 1974; Michalski, Stepp & Diday, 1981), and cognitive psychology (Tversky, 1977; Nosofsky, 1986).
Reference: <author> Batchelor, Bruce G., </author> <year> (1978). </year> <title> Pattern Recognition: </title> <booktitle> Ideas in Practice. </booktitle> <address> New York: </address> <publisher> Plenum Press, </publisher> <pages> pp. 71-72. </pages>
Reference-contexts: Previous Distance Functions As mentioned in the introduction, there are many learning systems that depend upon a good distance function to be successful. A variety of distance functions are available for such uses, including the Minkowsky <ref> (Batchelor, 1978) </ref>, Mahalanobis (Nadler & Smith, 1993), Camberra, Chebychev, Quadratic, Correlation, and Chisquare distance metrics (Michalski, Stepp & Diday, 1981; Diday, 1974); the ContextSimilarity measure (Biberman, 1994); the Contrast Model (Tversky, 1977); hyperrectangle distance functions (Salzberg, 1991; Domingos, 1995) and others. Several of these functions are defined in Figure 1. <p> An alternative function, the city-block or Manhattan distance function, requires less computation and is defined as: M (x, y) = x a - y a m The Euclidean and Manhattan distance functions are equivalent to the Minkowskian r distance function <ref> (Batchelor, 1978) </ref> with r=2 and 1, respectively. WILSON & MARTINEZ 4 (x and y are vectors of m attribute values).
Reference: <author> Biberman, Yoram, </author> <year> (1994). </year> <title> A Context Similarity Measure. </title> <booktitle> In Proceedings of the European Conference on Machine Learning (ECML-94). </booktitle> <address> Catalina, Italy: </address> <publisher> Springer Verlag, </publisher> <pages> pp. 49-63. </pages>
Reference-contexts: A variety of distance functions are available for such uses, including the Minkowsky (Batchelor, 1978), Mahalanobis (Nadler & Smith, 1993), Camberra, Chebychev, Quadratic, Correlation, and Chisquare distance metrics (Michalski, Stepp & Diday, 1981; Diday, 1974); the ContextSimilarity measure <ref> (Biberman, 1994) </ref>; the Contrast Model (Tversky, 1977); hyperrectangle distance functions (Salzberg, 1991; Domingos, 1995) and others. Several of these functions are defined in Figure 1.
Reference: <author> Broomhead, D. S., and D. </author> <title> Lowe (1988). Multi-variable functional interpolation and adaptive networks. </title> <journal> Complex Systems, </journal> <volume> Vol. 2, </volume> <pages> pp. 321-355. </pages>
Reference: <author> Cameron-Jones, R. M., </author> <year> (1995). </year> <title> Instance Selection by Encoding Length Heuristic with Random Mutation Hill Climbing. </title> <booktitle> In Proceedings of the Eighth Australian Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 99-106. </pages>
Reference: <author> Carpenter, Gail A., and Stephen Grossberg, </author> <year> (1987). </year> <title> A Massively Parallel Architecture for a Self-Organizing Neural Pattern Recognition Machine. Computer Vision, </title> <journal> Graphics, and Image Processing, </journal> <volume> Vol. 37, </volume> <pages> pp. 54-115. </pages>
Reference-contexts: Such algorithms have had much success on a wide variety of applications (real-world classification tasks). Many neural network models also make use of distance functions, including radial basis function networks (Broomhead & Lowe, 1988; Renals & Rohwer, 1989; Wasserman, 1993), counterpropagation networks (Hecht-Nielsen, 1987), ART <ref> (Carpenter & Grossberg, 1987) </ref>, self-organizing maps (Kohonen, 1990) and competitive learning (Rumelhart & McClelland, 1986).
Reference: <author> Chang, Chin-Liang, </author> <year> (1974). </year> <title> Finding Prototypes for Nearest Neighbor Classifiers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. 23, No. 11, </volume> <pages> pp. 1179-1184. </pages>
Reference-contexts: Several techniques have been introduced, including IB3 (Aha, Kibler & Albert, 1991; Aha, 1992), the condensed nearest neighbor rule (Hart, 1968), the reduced nearest neighbor rule (Gates, 1972), the selective nearest neighbor rule (Rittler et al., 1975), typical instance based learning algorithm (Zhang, 1992), prototype methods <ref> (Chang, 1974) </ref>, hyperrectangle techniques (Salzberg, 1991; Wettschereck & Dietterich, 1995), rule-based techniques (Domingos, 1995), random mutation hill climbing (Skalak, 1994; Cameron-Jones, 1995) and others (Kibler & Aha, 1987; Tomek, 1976; Wilson, 1972). 8.
Reference: <author> Cleveland, W. S., and C. Loader, </author> <year> (1994). </year> <title> Computational Methods for Local Regression. </title> <type> Technical Report 11, </type> <institution> Murray Hill, NJ: AT&T Bell Laboratories, Statistics Department. </institution>
Reference: <author> Cost, Scott, and Steven Salzberg, </author> <year> (1993). </year> <title> A Weighted Nearest Neighbor Algorithm for Learning with Symbolic Features. </title> <journal> Machine Learning, </journal> <volume> Vol. 10, </volume> <pages> pp. 57-78. </pages>
Reference: <author> Cover, T. M., and P. E. Hart, </author> <year> (1967). </year> <title> Nearest Neighbor Pattern Classification. </title> <journal> Institute of Electrical and Electronics Engineers Transactions on Information Theory , Vol. </journal> <volume> 13, No. 1, </volume> <pages> pp. 21-27. </pages>
Reference: <author> Dasarathy, Belur V., </author> <year> (1991). </year> <title> Nearest Neighbor (NN) Norms: NN Pattern Classification Techniques. </title> <publisher> Los Alamitos, </publisher> <address> CA: </address> <publisher> IEEE Computer Society Press. </publisher>
Reference: <author> Deng, Kan, and Andrew W. Moore, </author> <year> (1995). </year> <title> Multiresolution Instance-Based Learning. </title> <booktitle> To appear in The Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI95). </booktitle>
Reference-contexts: These distance functions could also be used in systems that perform regression (Atkeson, Moore & Schaal, 1996; Atkeson, 1989; Cleveland & Loader, 1994), in which the output is a real value, often interpolated from nearby points, as in kernel regression <ref> (Deng & Moore, 1995) </ref>. As mentioned in Section 6.2 and elsewhere, pruning techniques can be used to reduce the storage requirements of instance-based systems and improve classification speed.
Reference: <author> Diday, Edwin, </author> <year> (1974). </year> <title> Recent Progress in Distance and Similarity Measures in Pattern Recognition. </title> <booktitle> Second International Joint Conference on Pattern Recognition, </booktitle> <pages> pp. 534-539. </pages>
Reference: <author> Domingos, Pedro, </author> <year> (1995). </year> <title> Rule Induction and Instance-Based Learning: A Unified Approach. </title> <booktitle> to appear in The 1995 International Joint Conference on Artificial Intelligence (IJCAI-95). </booktitle>
Reference-contexts: The function HVDM is similar to the function HOEM given in Section 2.3, except that it IMPROVED HETEROGENEOUS DISTANCE FUNCTIONS 9 uses VDM instead of an overlap metric for nominal values and it also normalizes differently. It is also similar to the distance function used by RISE 2.0 <ref> (Domingos, 1995) </ref>, but has some important differences noted below in Section 3.2. Section 3.1 presents three alternatives for normalizing the nominal and linear attributes. Section 3.2 presents experimental results which show that one of these schemes provides better normalization than the other two on a set of several datasets. <p> This is similar to the formula used in PEBLS (Rachlin et al., 1994) and RISE <ref> (Domingos, 1995) </ref> for nominal attributes. N2 uses q=2, thus squaring the individual differences. This is analogous to using Euclidean distance instead of Manhattan distance. <p> We repeat it here for convenience: vdm a (x, y) = P a,x,c Pa,y,c c=1 (20) Unknown input values (Quinlan, 1989) are treated as simply another discrete value, as was done in <ref> (Domingos, 1995) </ref>. Table 4. Example from the Iris database. <p> Kibler & Albert, 1991; Aha, 1992), the condensed nearest neighbor rule (Hart, 1968), the reduced nearest neighbor rule (Gates, 1972), the selective nearest neighbor rule (Rittler et al., 1975), typical instance based learning algorithm (Zhang, 1992), prototype methods (Chang, 1974), hyperrectangle techniques (Salzberg, 1991; Wettschereck & Dietterich, 1995), rule-based techniques <ref> (Domingos, 1995) </ref>, random mutation hill climbing (Skalak, 1994; Cameron-Jones, 1995) and others (Kibler & Aha, 1987; Tomek, 1976; Wilson, 1972). 8. Conclusions & Future Research Areas There are many learning systems that depend on a reliable distance function to achieve accurate generalization.
Reference: <author> Dudani, Sahibsingh A., </author> <year> (1976). </year> <title> The Distance-Weighted k-Nearest-Neighbor Rule. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> Vol. 6, No. 4, </volume> <month> April </month> <year> 1976, </year> <pages> pp. </pages> <month> 325-327. </month> <title> IMPROVED HETEROGENEOUS DISTANCE FUNCTIONS 31 Gates, </title> <editor> G. W., </editor> <year> (1972). </year> <title> The Reduced Nearest Neighbor Rule. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. IT-18, No. 3, </volume> <pages> pp. 431-433. </pages>
Reference-contexts: Note that in practice the square root in (11) is not typically performed because the distance is always positive, and the nearest neighbor (s) will still be nearest whether or not the distance is squared. However, there are some models <ref> (e.g., distance-weighted k-nearest neighbor, Dudani, 1976) </ref> that require the square root to be evaluated. Many applications contain unknown input values which must be handled appropriately in a practical system (Quinlan, 1989).
Reference: <author> Giraud-Carrier, Christophe, and Tony Martinez, </author> <year> (1995). </year> <title> An Efficient Metric for Heterogeneous Inductive Learning Applications in the Attribute-Value Language. </title> <booktitle> Intelligent Systems, </booktitle> <pages> pp. 341-350. </pages>
Reference: <author> Hart, P. E., </author> <year> (1968). </year> <title> The Condensed Nearest Neighbor Rule. </title> <journal> Institute of Electrical and Electronics Engineers Transactions on Information Theory, </journal> <volume> Vol. 14, </volume> <pages> pp. 515-516. </pages>
Reference-contexts: As mentioned in Section 6.2 and elsewhere, pruning techniques can be used to reduce the storage requirements of instance-based systems and improve classification speed. Several techniques have been introduced, including IB3 (Aha, Kibler & Albert, 1991; Aha, 1992), the condensed nearest neighbor rule <ref> (Hart, 1968) </ref>, the reduced nearest neighbor rule (Gates, 1972), the selective nearest neighbor rule (Rittler et al., 1975), typical instance based learning algorithm (Zhang, 1992), prototype methods (Chang, 1974), hyperrectangle techniques (Salzberg, 1991; Wettschereck & Dietterich, 1995), rule-based techniques (Domingos, 1995), random mutation hill climbing (Skalak, 1994; Cameron-Jones, 1995) and others
Reference: <author> Hecht-Nielsen, R., </author> <year> (1987). </year> <title> Counterpropagation Networks. </title> <journal> Applied Optics, </journal> <volume> Vol. 26, No. 23, </volume> <pages> pp. 4979-4984. </pages>
Reference: <author> Kibler, D., and David W. Aha, </author> <year> (1987). </year> <title> Learning representative exemplars of concepts: An initial case study. </title> <booktitle> Proceedings of the Fourth International Workshop on Machine Learning. </booktitle> <address> Irvine, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 24-30. </pages> <address> Kohonen, Teuvo, </address> <year> (1990). </year> <title> The Self-Organizing Map. </title> <booktitle> In Proceedings of the IEEE, </booktitle> <volume> Vol. 78, No. 9, </volume> <pages> pp. 1464-1480. </pages>
Reference: <author> Lebowitz, Michael, </author> <year> (1985). </year> <title> Categorizing Numeric Information for Generalization. </title> <journal> Cognitive Science, </journal> <volume> Vol. 9, </volume> <pages> pp. 285-308. </pages>
Reference: <author> Merz, C. J., and P. M. Murphy, </author> <year> (1996). </year> <title> UCI Repository of Machine Learning Databases. </title> <address> Irvine, CA: </address> <institution> University of California Irvine, Department of Information and Computer Science. Internet: </institution> <note> http://www.ics.uci.edu/~mlearn/MLRepository.html. </note>
Reference-contexts: Instead, they rely upon discretization (Lebowitz, 1985; Schlimmer, 1987), which can degrade generalization accuracy (Ventura & Martinez, 1995). Many real-world applications have both nominal and linear attributes, including, for example, over half of the datasets in the UCI Machine Learning Database Repository <ref> (Merz & Murphy, 1996) </ref>. This paper introduces three new distance functions that are more appropriate than previous functions for applications with both nominal and continuous attributes. <p> Normalization Experiments In order to determine whether each normalization scheme N1, N2 and N3 gave unfair weight to either nominal or linear attributes, experiments were run on 15 databases from the machine learning database repository at the University of California, Irvine <ref> (Merz & Murphy, 1996) </ref>. All of the datasets for this experiment have at least some nominal and some linear attributes, and thus require a heterogeneous distance function. In each experiment, fivefold cross validation was used.
Reference: <author> Michalski, Ryszard S., Robert E. Stepp, and Edwin Diday, </author> <year> (1981). </year> <title> A Recent Advance in Data Analysis: Clustering Objects into Classes Characterized by Conjunctive Concepts. </title> <journal> Progress in Pattern Recognition, </journal> <volume> Vol. 1, </volume> <editor> Laveen N. Kanal and Azriel Rosenfeld (Eds.). </editor> <address> New York: </address> <publisher> North-Holland, </publisher> <pages> pp. 33-56. </pages>
Reference: <author> Mitchell, Tom M., </author> <year> (1980). </year> <title> The Need for Biases in Learning Generalizations. </title> <editor> in J. W. Shavlik & T. G. Dietterich (Eds.), </editor> <booktitle> Readings in Machine Learning . San Mateo, </booktitle> <address> CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1990, </year> <pages> pp. 184-191. </pages>
Reference-contexts: The choice of distance function influences the bias of a learning algorithm. A bias is a rule or method that causes an algorithm to choose one generalized output over another <ref> (Mitchell, 1980) </ref>. A learning algorithm must have a bias in order to generalize, and it has been shown that no learning algorithm can generalize more accurately than any other when summed over all possible problems (Schaffer, 1994) (unless information about the problem other than the training data is available). <p> Other biases, such as decisions made on the basis of additional domain knowledge for a particular problem <ref> (Mitchell, 1980) </ref>, can also improve generalization. In this light, the distance functions presented in this paper are more appropriate than those used for comparison in that they on average yield improved generalization accuracy on a collection of 48 applications.
Reference: <author> Mohri, Takao, and Hidehiko Tanaka, </author> <year> (1994). </year> <title> An Optimal Weighting Criterion of Case Indexing for both Numeric and Symbolic Attributes. </title> <editor> In D. W. Aha (Ed.), </editor> <booktitle> Case-Based Reasoning: Papers from the 1994 Workshop , Technical Report WS-94-01. </booktitle> <address> Menlo Park, CA: </address> <publisher> AIII Press, </publisher> <pages> pp. 123-127. </pages>
Reference: <author> Nadler, Morton, and Eric P. Smith, </author> <year> (1993). </year> <title> Pattern Recognition Engineering . New York: </title> <publisher> Wiley, </publisher> <pages> pp. 293-294. </pages>
Reference-contexts: Previous Distance Functions As mentioned in the introduction, there are many learning systems that depend upon a good distance function to be successful. A variety of distance functions are available for such uses, including the Minkowsky (Batchelor, 1978), Mahalanobis <ref> (Nadler & Smith, 1993) </ref>, Camberra, Chebychev, Quadratic, Correlation, and Chisquare distance metrics (Michalski, Stepp & Diday, 1981; Diday, 1974); the ContextSimilarity measure (Biberman, 1994); the Contrast Model (Tversky, 1977); hyperrectangle distance functions (Salzberg, 1991; Domingos, 1995) and others. Several of these functions are defined in Figure 1.
Reference: <author> Nosofsky, Robert M., </author> <year> (1986). </year> <title> Attention, Similarity, and the Identification-Categorization Relationship. </title> <journal> Journal of Experimental Psychology: General, </journal> <volume> Vol. 115, No. 1, </volume> <pages> pp. 39-57. </pages>
Reference: <author> Papadimitriou, Christos H., and Jon Louis Bentley, </author> <year> (1980). </year> <title> A Worst-Case Analysis of Nearest Neighbor Searching by Projection. </title> <booktitle> Lecture Notes in Computer Science , Vol. 85, Automata Languages and Programming, </booktitle> <pages> pp. 470-482. </pages>
Reference-contexts: Though m and C are typically fairly small, the generalization process can require a significant amount of time and/or computational resources as n grows large. Techniques such as k -d trees (Deng & Moore, 1995; Wess, Althoff & Derwand, 1993; Sproull, 1991) and projection <ref> (Papadimitriou & Bentley, 1980) </ref> can reduce the time required to locate nearest neighbors from the training set, though such algorithms may require modification to handle both continuous and nominal attributes.
Reference: <author> WILSON & MARTINEZ 32 Parzen, Emanuel, </author> <year> (1962). </year> <title> On estimation of a probability density function and mode. </title> <journal> Annals of Mathematical Statistics. </journal> <volume> Vol. 33, </volume> <pages> pp. 1065-1076. </pages>
Reference: <author> Quinlan, J. R., </author> <year> (1989). </year> <title> Unknown Attribute Values in Induction. </title> <booktitle> In Proceedings of the 6th International Workshop on Machine Learning . San Mateo, </booktitle> <address> CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 164-168. </pages>
Reference-contexts: However, there are some models (e.g., distance-weighted k-nearest neighbor, Dudani, 1976) that require the square root to be evaluated. Many applications contain unknown input values which must be handled appropriately in a practical system <ref> (Quinlan, 1989) </ref>. The function d a (x,y) therefore returns a distance of 1 if either x or y is unknown, as is done by Aha, Kibler & Albert (1991) and Giraud-Carrier & Martinez (1995). <p> We repeat it here for convenience: vdm a (x, y) = P a,x,c Pa,y,c c=1 (20) Unknown input values <ref> (Quinlan, 1989) </ref> are treated as simply another discrete value, as was done in (Domingos, 1995). Table 4. Example from the Iris database.
Reference: <author> Rachlin, John, Simon Kasif, Steven Salzberg, David W. Aha, </author> <year> (1994). </year> <title> Towards a Better Understanding of Memory-Based and Bayesian Classifiers. </title> <booktitle> In Proceedings of the Eleventh International Machine Learning Conference. </booktitle> <address> New Brunswick, NJ: </address> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 242-250. </pages>
Reference-contexts: This is similar to the formula used in PEBLS <ref> (Rachlin et al., 1994) </ref> and RISE (Domingos, 1995) for nominal attributes. N2 uses q=2, thus squaring the individual differences. This is analogous to using Euclidean distance instead of Manhattan distance.
Reference: <author> Renals, Steve, and Richard Rohwer, </author> <year> (1989). </year> <title> Phoneme Classification Experiments Using Radial Basis Functions. </title> <booktitle> In Proceedings of the IEEE International Joint Conference on Neural Networks (IJCNN89), </booktitle> <volume> Vol. 1, </volume> <pages> pp. 461-467. </pages>
Reference: <author> Rittler, G. L., H. B. Woodruff, S. R. Lowry, and T. L. Isenhour, </author> <year> (1975). </year> <title> An Algorithm for a Selective Nearest Neighbor Decision Rule. </title> <journal> IEEE Transactions on Information Theory , Vol. </journal> <volume> 21, No. 6, </volume> <pages> pp. 665-669. </pages>
Reference-contexts: Several techniques have been introduced, including IB3 (Aha, Kibler & Albert, 1991; Aha, 1992), the condensed nearest neighbor rule (Hart, 1968), the reduced nearest neighbor rule (Gates, 1972), the selective nearest neighbor rule <ref> (Rittler et al., 1975) </ref>, typical instance based learning algorithm (Zhang, 1992), prototype methods (Chang, 1974), hyperrectangle techniques (Salzberg, 1991; Wettschereck & Dietterich, 1995), rule-based techniques (Domingos, 1995), random mutation hill climbing (Skalak, 1994; Cameron-Jones, 1995) and others (Kibler & Aha, 1987; Tomek, 1976; Wilson, 1972). 8.
Reference: <author> Rosenblatt, Murray, </author> <year> (1956). </year> <title> Remarks on Some Nonparametric Estimates of a Density Function. </title> <journal> Annals of Mathematical Statistics. </journal> <volume> Vol. 27, </volume> <pages> pp. 832-835. </pages>
Reference-contexts: Thus, instead of having a fixed number s of sampling points, a window of instances, centered on each training instance, is used for determining the probability at a given point. This technique is similar in concept to shifted histogram estimators <ref> (Rosenblatt, 1956) </ref> and to Parzen window techniques (Parzen, 1962). For each attribute the values are sorted (using an O (nlogn) sorting algorithm) so as to allow a sliding window to be used and thus collect the needed statistics in O (n) time for each attribute. <p> IVDM and WVDM use nonparametric density estimation techniques (Tapia & Thompson, 1978) in determining values of P for use in computing distances. Parzen windows (Parzen, 1962) and shifting histograms <ref> (Rosenblatt, 1956) </ref> are similar in concept to these techniques, especially to WVDM. These techniques often use gaussian kernels or other more advanced techniques instead of a fixed-sized sliding window.
Reference: <author> Rumelhart, D. E., and J. L. McClelland, </author> <year> (1986). </year> <title> Parallel Distributed Processing , MIT Press, </title> <journal> Ch. </journal> <volume> 8, </volume> <pages> pp. 318-362. </pages>
Reference-contexts: Many neural network models also make use of distance functions, including radial basis function networks (Broomhead & Lowe, 1988; Renals & Rohwer, 1989; Wasserman, 1993), counterpropagation networks (Hecht-Nielsen, 1987), ART (Carpenter & Grossberg, 1987), self-organizing maps (Kohonen, 1990) and competitive learning <ref> (Rumelhart & McClelland, 1986) </ref>. Distance functions are also used in many fields besides machine learning and neural networks, including statistics (Atkeson, Moore & Schaal, 1996), pattern recognition (Diday, 1974; Michalski, Stepp & Diday, 1981), and cognitive psychology (Tversky, 1977; Nosofsky, 1986).

Reference: <author> Schaffer, Cullen, </author> <year> (1994). </year> <title> A Conservation Law for Generalization Performance. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning (ML94), </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: A learning algorithm must have a bias in order to generalize, and it has been shown that no learning algorithm can generalize more accurately than any other when summed over all possible problems <ref> (Schaffer, 1994) </ref> (unless information about the problem other than the training data is available). It follows then that no distance function can be strictly better than any other in terms of generalization ability, when considering all possible problems with equal probability.
Reference: <author> Schlimmer, Jeffrey C., </author> <year> (1987). </year> <title> Learning and Representation Change. </title> <booktitle> In Proceedings of the Sixth National Conference on Artificial Intelligence (AAAI87), </booktitle> <volume> Vol. 2, </volume> <pages> pp. 511-535. </pages>
Reference: <author> Skalak, D. B., </author> <year> (1994). </year> <title> Prototype and Feature Selection by Sampling and Random Mutation Hill Climbing Algorithsm. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning (ML94). </booktitle> <publisher> Morgan Kaufman, </publisher> <pages> pp. 293-301. </pages>
Reference: <author> Sproull, Robert F., </author> <year> (1991). </year> <title> Refinements to Nearest-Neighbor Searching in k-Dimensional Trees. </title> <journal> Algorithmica, </journal> <volume> Vol.6, </volume> <pages> pp. 579-589. </pages>
Reference: <author> Stanfill, C., and D. Waltz, </author> <year> (1986). </year> <title> Toward memory-based reasoning. </title> <journal> Communications of the ACM, </journal> <volume> Vol. 29, </volume> <month> December </month> <year> 1986, </year> <pages> pp. </pages> <month> 1213-1228. </month> <title> IMPROVED HETEROGENEOUS DISTANCE FUNCTIONS 33 Tapia, </title> <editor> Richard A., and James R. Thompson, </editor> <year> (1978). </year> <title> Nonparametric Probability Density Estimation. </title> <address> Baltimore, MD: </address> <publisher> The Johns Hopkins University Press. </publisher>
Reference-contexts: Many of these metrics work well for numerical attributes but do not appropriately handle nominal (i.e., discrete, and perhaps unordered) attributes. The Value Difference Metric (VDM) <ref> (Stanfill & Waltz, 1986) </ref> was introduced to define an appropriate distance function for nominal (also called symbolic) attributes. The Modified Value Difference Metric (MVDM) uses a different weighting scheme than VDM and is used in the PEBLS system (Cost & Salzberg, 1993; Rachlin et al., 1994). <p> The original VDM algorithm <ref> (Stanfill & Waltz, 1986) </ref> makes use of feature weights that are not included in the above equations, and some variants of VDM (Cost & Salzberg, 1993; Rachlin et al., 1994; Domingos, 1995) have used alternate weighting schemes.
Reference: <author> Ting, Kai Ming, </author> <year> (1994). </year> <title> Discretization of Continuous-Valued Attributes and Instance-Based Learning. </title> <type> Technical Report No. 491, </type> <institution> Basser Department of Computer Science, University of Sydney, Australia. </institution>
Reference: <author> Ting, Kai Ming, </author> <year> (1996). </year> <note> Discretisation in Lazy Learning. To appear in the special issue on Lazy Learning in Artificial Intelligence Review. </note>
Reference: <author> Tomek, Ivan, </author> <year> (1976). </year> <title> An Experiment with the Edited Nearest-Neighbor Rule. </title> <journal> I E E E Transactions on Systems, Man, and Cybernetics, </journal> <volume> Vol. 6, No. 6, </volume> <month> June </month> <year> 1976, </year> <pages> pp. 448-452. </pages>
Reference: <author> Turney, Peter, </author> <year> (1994). </year> <title> Theoretical Analyses of Cross-Validation Error and Voting in Instance - Based Learning. </title> <journal> Journal of Experimental and Theoretical Artificial Intelligence (JETAI) , pp. </journal> <pages> 331-360. </pages>
Reference: <author> Turney, Peter, </author> <year> (1993). </year> <title> Exploiting context when learning to classify. </title> <booktitle> In Proceedings of the European Conference on Machine Learning . Vienna, </booktitle> <address> Austria: </address> <publisher> Springer-Verlag, </publisher> <pages> pp. 402-407. </pages>
Reference: <author> Turney, Peter, and Michael Halasz, </author> <year> (1993). </year> <title> Contextual Normalization Applied to Aircraft Gas Turbine Engine Diagnosis. </title> <journal> Journal of Applied Intelligence, </journal> <volume> Vol. 3, </volume> <pages> pp. 109-129. </pages>
Reference: <author> Tversky, Amos, </author> <year> (1977). </year> <title> Features of Similarity. </title> <journal> Psychological Review, </journal> <volume> Vol. 84, No. 4, </volume> <pages> pp. 327-352. </pages>
Reference-contexts: A variety of distance functions are available for such uses, including the Minkowsky (Batchelor, 1978), Mahalanobis (Nadler & Smith, 1993), Camberra, Chebychev, Quadratic, Correlation, and Chisquare distance metrics (Michalski, Stepp & Diday, 1981; Diday, 1974); the ContextSimilarity measure (Biberman, 1994); the Contrast Model <ref> (Tversky, 1977) </ref>; hyperrectangle distance functions (Salzberg, 1991; Domingos, 1995) and others. Several of these functions are defined in Figure 1.
Reference: <author> Ventura, Dan, </author> <year> (1995). </year> <title> On Discretization as a Preprocessing Step for Supervised Learning Models, </title> <type> Masters Thesis, </type> <institution> Department of Computer Science, Brigham Young University. </institution>
Reference-contexts: These distance metrics work well in many nominal domains, but they do not handle continuous attributes directly. Instead, they rely upon discretization (Lebowitz, 1985; Schlimmer, 1987), which can degrade generalization accuracy <ref> (Ventura & Martinez, 1995) </ref>. Many real-world applications have both nominal and linear attributes, including, for example, over half of the datasets in the UCI Machine Learning Database Repository (Merz & Murphy, 1996). <p> However, discretization can lose much of the important information available in the continuous values. For example, two values in the same discretized range are considered equal even if they are on opposite ends of the range. Such effects can reduce generalization accuracy <ref> (Ventura & Martinez, 1995) </ref>. In this paper we propose three new alternatives, which are presented in the following three sections. Section 3 presents a heterogeneous distance function that uses Euclidean distance for linear attributes and VDM for nominal attributes.
Reference: <author> Ventura, Dan, and Tony R. </author> <title> Martinez (1995). An Empirical Comparison of Discretization Methods. </title> <booktitle> In Proceedings of the Tenth International Symposium on Computer and Information Sciences, </booktitle> <pages> pp. 443-450. </pages>
Reference-contexts: These distance metrics work well in many nominal domains, but they do not handle continuous attributes directly. Instead, they rely upon discretization (Lebowitz, 1985; Schlimmer, 1987), which can degrade generalization accuracy <ref> (Ventura & Martinez, 1995) </ref>. Many real-world applications have both nominal and linear attributes, including, for example, over half of the datasets in the UCI Machine Learning Database Repository (Merz & Murphy, 1996). <p> However, discretization can lose much of the important information available in the continuous values. For example, two values in the same discretized range are considered equal even if they are on opposite ends of the range. Such effects can reduce generalization accuracy <ref> (Ventura & Martinez, 1995) </ref>. In this paper we propose three new alternatives, which are presented in the following three sections. Section 3 presents a heterogeneous distance function that uses Euclidean distance for linear attributes and VDM for nominal attributes.
Reference: <author> Wasserman, Philip D., </author> <year> (1993). </year> <booktitle> Advanced Methods in Neural Computing. </booktitle> <address> New York, NY: </address> <publisher> Van Nostrand Reinhold, </publisher> <pages> pp. 147-176. </pages>
Reference: <author> Wess, Stefan, Klaus-Dieter Althoff and Guido Derwand, </author> <year> (1994). </year> <title> Using k-d Trees to Improve the Retrieval Step in Case-Based Reasoning. </title> <editor> Stefan Wess, Klaus-Dieter Althoff, & M. </editor> <publisher> M. </publisher>
Reference: <editor> Richter (Eds.), </editor> <booktitle> Topics in Case-Based Reasoning. </booktitle> <address> Berlin: </address> <publisher> Springer-Verlag, </publisher> <pages> pp. 167-181. </pages>
Reference: <author> Wettschereck, Dietrich, and Thomas G. Dietterich, </author> <year> (1995). </year> <title> An Experimental Comparison of Nearest-Neighbor and Nearest-Hyperrectangle Algorithms. </title> <journal> Machine Learning , Vol. </journal> <volume> 19, No. 1, </volume> <pages> pp. 5-28. </pages>
Reference: <author> Wettschereck, Dietrich, David W. Aha, and Takao Mohri, </author> <year> (1995). </year> <title> A Review and Comparative Evaluation of Feature Weighting Methods for Lazy Learning Algorithms. </title> <type> Technical Report AIC-95-012. </type> <institution> Washington, D.C.: Naval Research Laboratory, </institution> <note> Navy Center for Applied Research in Artificial Intelligence. </note>
Reference: <author> WILSON & MARTINEZ 34 Wilson, D. Randall, and Tony R. Martinez, </author> <year> (1993). </year> <title> The Potential of Prototype Styles of Generalization. </title> <booktitle> In Proceedings of the Sixth Australian Joint Conference on Artifical Intelligence (AI93), </booktitle> <pages> pp. 356-361. </pages>
Reference-contexts: The function d a (x,y) therefore returns a distance of 1 if either x or y is unknown, as is done by Aha, Kibler & Albert (1991) and Giraud-Carrier & Martinez (1995). Other more complicated methods have been tried <ref> (Wilson & Martinez, 1993) </ref>, but with little effect on accuracy. The function HVDM is similar to the function HOEM given in Section 2.3, except that it IMPROVED HETEROGENEOUS DISTANCE FUNCTIONS 9 uses VDM instead of an overlap metric for nominal values and it also normalizes differently.
Reference: <author> Wilson, D. Randall, and Tony R. Martinez, </author> <year> (1996). </year> <title> Heterogeneous Radial Basis Functions. </title> <booktitle> In Proceedings of the International Conference on Neural Networks (ICNN96) , Vol. </booktitle> <volume> 2, </volume> <pages> pp. 1263-1267. </pages>
Reference-contexts: N1 would not be able to distinguish between these two. In practice the square root is not taken, because the individual attribute distances are themselves squared by the HVDM function. N3 is the function used in Heterogeneous Radial Basis Function Networks <ref> (Wilson & Martinez, 1996) </ref>, where HVDM was first introduced. 3.2.
Reference: <author> Wilson, Dennis L., </author> <year> (1972). </year> <title> Asymptotic Properties of Nearest Neighbor Rules Using Edited Data. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> Vol. 2, No. 3, </volume> <pages> pp. 408-421. </pages>
Reference: <author> Wolpert, David H., </author> <title> (1993 ). On Overfitting Avoidance as Bias. </title> <type> Technical Report SFI TR 92 - 03-5001. </type> <institution> Santa Fe, NM: The Santa Fe Institute. </institution>
Reference-contexts: However, when there is a higher probability of one class of problems occurring than another, some learning algorithms can generalize more accurately than others <ref> (Wolpert, 1993) </ref>. This is not because they are better when summed over all problems, but because the problems on which they perform well are more likely to occur.
Reference: <author> Zhang, Jianping, </author> <year> (1992). </year> <title> Selecting Typical Instances in Instance-Based Learning. </title> <booktitle> Proceedings of the Ninth International Conference on Machine Learning. </booktitle>
Reference-contexts: Several techniques have been introduced, including IB3 (Aha, Kibler & Albert, 1991; Aha, 1992), the condensed nearest neighbor rule (Hart, 1968), the reduced nearest neighbor rule (Gates, 1972), the selective nearest neighbor rule (Rittler et al., 1975), typical instance based learning algorithm <ref> (Zhang, 1992) </ref>, prototype methods (Chang, 1974), hyperrectangle techniques (Salzberg, 1991; Wettschereck & Dietterich, 1995), rule-based techniques (Domingos, 1995), random mutation hill climbing (Skalak, 1994; Cameron-Jones, 1995) and others (Kibler & Aha, 1987; Tomek, 1976; Wilson, 1972). 8.
References-found: 61

