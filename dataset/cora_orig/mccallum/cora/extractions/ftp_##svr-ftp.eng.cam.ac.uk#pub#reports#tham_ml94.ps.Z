URL: ftp://svr-ftp.eng.cam.ac.uk/pub/reports/tham_ml94.ps.Z
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/mitchell/ftp/robot-learning.html
Root-URL: 
Email: ckt@eng.cam.ac.uk  rwp@eng.cam.ac.uk  
Title: A Modular Q-Learning Architecture for Manipulator Task Decomposition  
Author: Chen K. Tham Richard W. Prager 
Note: 1975) structures.  
Address: Cambridge CB2 1PZ United Kingdom  Cambridge CB2 1PZ United Kingdom  
Affiliation: Department of Engineering University of Cambridge  Department of Engineering University of Cambridge  
Abstract: Compositional Q-Learning (CQ-L) (Singh 1992) is a modular approach to learning to perform composite tasks made up of several elemental tasks by reinforcement learning. Skills acquired while performing elemental tasks are also applied to solve composite tasks. Individual skills compete for the right to act and only winning skills are included in the decomposition of the composite task. We extend the original CQ-L concept in two ways: (1) a more general reward function, and (2) the agent can have more than one actuator. We use the CQ-L architecture to acquire skills for performing composite tasks with a simulated two-linked manipulator having large state and action spaces. The manipulator is a non-linear dynamical system and we require its end-effector to be at specific positions in the workspace. Fast function approximation in each of the Q-modules is achieved through the use of an array of Cerebellar Model Articulation Controller (CMAC) (Albus 
Abstract-found: 1
Intro-found: 1
Reference: <author> Albus, J. </author> <year> (1975), </year> <title> `Data storage in the cerebellar model articulation controller (CMAC)', Journal of Dynamic Systems, </title> <booktitle> Measurement and Control 97(3), </booktitle> <pages> 228-233. </pages>
Reference-contexts: In order to store Q-values in large state and action spaces as well as obtain fast function approximation, we use an array of Cerebellar Model Articulation Controller (CMAC) <ref> (Albus 1975) </ref> structures. This paper is organized as follows. In Section 2, we describe reinforcement learning and Q-Learning. In Section 3, we describe elemental and composite tasks, the CQ-L architecture and our extensions. <p> This produces the torque commands for driving the manipulator. Unlike a purely kinematic system, an instantaneous response to position change commands is not possible. 5 CEREBELLAR MODEL ARTICULATION CONTROLLER (CMAC) The CMAC <ref> (Albus 1975) </ref> is a coarse-coding structure where each region in the input space has a set of overlapping but offset hypercubes associated with it. Each hypercube is defined by quantizing functions operating on every input and it corresponds to a component of the desired output value.
Reference: <author> Barto, A., Sutton, R. & Anderson, C. </author> <year> (1983), </year> <title> `Neuron-like elements that can solve difficult learning control problems', </title> <journal> IEEE Transactions on Systems, Man and Cybernetics SMC-13(5), </journal> <pages> 835-846. </pages>
Reference-contexts: Results are presented in Section 7 and a comparison with related work is given in Section 8. Finally, we draw some conclusions in Section 9. 2 REINFORCEMENT LEARNING Reinforcement learning <ref> (Barto, Sutton & Anderson 1983) </ref> is a method useful for solving Markovian sequential decision tasks where the agent operates in a stochastic dynamical environment.
Reference: <author> Jacobs, R. & Jordan, M. </author> <year> (1993), </year> <title> `Learning piecewise control strategies in a modular neural network architecture', </title> <journal> IEEE Transactions on Systems, Man and Cybernetics 23(2), </journal> <pages> 337-345. </pages>
Reference-contexts: As mentioned above, the CQ-L architecture is very similar to modular networks (Jacobs et al. 1991) for supervised learning. Recent developments such as the use the Expectation-Maximization algorithm for parameter updates in the Hierarchical Mixtures of Experts architecture <ref> (Jordan & Jacobs 1993) </ref> have reduced significantly the training time required to reach a given level of performance in supervised learning tasks. It will be interesting to see how well these methods work in reinforcement learning situations.
Reference: <author> Jacobs, R., Jordan, M., Nowlan, S. & Hinton, G. </author> <year> (1991), </year> <title> `Adaptive mixtures of local experts', </title> <booktitle> Neural Computation 3, </booktitle> <pages> 79-87. </pages>
Reference-contexts: There are eleven CMAC struc tures within a Q-network and 66 CMAC structures in the entire architecture. 3.2 CQ-L ARCHITECTURE The CQ-L architecture evolved out of a modular network for supervised learning consisting of expert networks and a gating network that arbitrates between them <ref> (Jacobs, Jordan, Nowlan & Hinton 1991) </ref>. The basic idea here is to allow a separate expert network, which is a Q-module here, to learn each elemental task and let the gating module perform task decomposition by selecting the appropriate Q-module. <p> In addition, we made several modifications to the strategy for parameter updates in order to achieve reliable task decomposition. Fast function approximation by CMAC structures enabled local generalization to be obtained without requiring experience replay. As mentioned above, the CQ-L architecture is very similar to modular networks <ref> (Jacobs et al. 1991) </ref> for supervised learning. Recent developments such as the use the Expectation-Maximization algorithm for parameter updates in the Hierarchical Mixtures of Experts architecture (Jordan & Jacobs 1993) have reduced significantly the training time required to reach a given level of performance in supervised learning tasks.
Reference: <author> Jordan, M. & Jacobs, R. </author> <year> (1993), </year> <title> Hierarchical mixtures of experts and the EM algorithm, </title> <type> Technical Report 9301, </type> <institution> MIT Computational Cognitive Science. </institution>
Reference-contexts: As mentioned above, the CQ-L architecture is very similar to modular networks (Jacobs et al. 1991) for supervised learning. Recent developments such as the use the Expectation-Maximization algorithm for parameter updates in the Hierarchical Mixtures of Experts architecture <ref> (Jordan & Jacobs 1993) </ref> have reduced significantly the training time required to reach a given level of performance in supervised learning tasks. It will be interesting to see how well these methods work in reinforcement learning situations.
Reference: <author> Lin, L. </author> <year> (1992), </year> <title> `Self-improving reactive agents based on reinforcement learning, planning and teaching', </title> <booktitle> Machine Learning 8(3/4), </booktitle> <pages> 293-321. </pages>
Reference: <author> Lin, L. </author> <year> (1993a), </year> <title> Hierarchical learning of robot skills by reinforcement, </title> <booktitle> in `Proceedings of the International Conference on Neural Netwroks (ICNN'93)', </booktitle> <volume> Vol. 1, </volume> <pages> pp. 181-186. </pages>
Reference: <author> Lin, L. </author> <year> (1993b), </year> <title> Scaling up reinforcement learning for robot control, </title> <booktitle> in `Machine Learning: Proceedings of the Tenth International Conference (ML93)', </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Maes, P. & Brooks, R. </author> <year> (1990), </year> <title> Learning to coordinate be-haviours, </title> <booktitle> in `Proceedings of the 8th AAAI Conference', </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 796-802. </pages>
Reference: <author> Mahadevan, S. & Connell, J. </author> <year> (1990), </year> <title> Automatic programming of behaviour-based robots using reinforcement learning, </title> <type> Research Report RC 16359 # 72625, </type> <institution> IBM T.J. Watson Research Center, </institution> <address> Yorktown Heights, NY 10598. </address>
Reference: <author> Nowlan, S. </author> <year> (1991), </year> <title> Soft Competitive Adaptation: Neural Network Learning Algorithms based on Fitting Statistical Mixtures, </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA 15213. </address>
Reference-contexts: There are eleven CMAC struc tures within a Q-network and 66 CMAC structures in the entire architecture. 3.2 CQ-L ARCHITECTURE The CQ-L architecture evolved out of a modular network for supervised learning consisting of expert networks and a gating network that arbitrates between them <ref> (Jacobs, Jordan, Nowlan & Hinton 1991) </ref>. The basic idea here is to allow a separate expert network, which is a Q-module here, to learn each elemental task and let the gating module perform task decomposition by selecting the appropriate Q-module.
Reference: <author> Singh, S. </author> <year> (1991), </year> <title> The efficient learning of multiple task sequences, </title> <editor> in J. Moody, S. Hanson & R. Lippman, eds, </editor> <booktitle> `Advances in Neural Information Processing Systems 4', </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <pages> pp. 251-258. </pages>
Reference: <author> Singh, S. </author> <year> (1992), </year> <title> `Transfer of learning by composing solutions of elemental sequential tasks', </title> <booktitle> Machine Learning 8(3/4), </booktitle> <pages> 323-339. </pages>
Reference: <author> Thrun, S. </author> <year> (1993), </year> <title> Exploration and model building in mobile robot domains, </title> <booktitle> in `Proceedings of the IEEE International Conference on Neural Networks', </booktitle> <address> San Fran-cisco, CA. </address>
Reference: <author> Watkins, C. </author> <year> (1989), </year> <title> Learning from Delayed Rewards, </title> <type> PhD thesis, </type> <institution> University of Cambridge, </institution> <address> Cambridge, UK. </address>
Reference-contexts: An example is the expected return, which is the expected long term discounted sum of payoff. The agent performs dif ferent actions on the environment in order to discover their utilities and increases the probabilities of selecting promising actions. Q-LEARNING In Q-Learning <ref> (Watkins 1989) </ref>, the state-action value Q (x; a) is estimated. Q (x; a) is the return in state x when action a is performed and the optimal policy is followed thereafter. The action space is discrete and a separate Q (x; a) exists for each action a.
References-found: 15

