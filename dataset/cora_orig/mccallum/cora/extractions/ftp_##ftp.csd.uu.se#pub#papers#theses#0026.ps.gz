URL: ftp://ftp.csd.uu.se/pub/papers/theses/0026.ps.gz
Refering-URL: http://www.csd.uu.se/papers/long-theses.html
Root-URL: 
Title: Compilation Techniques for Prolog  
Author: Thomas Lindgren 
Degree: Thesis for the Degree of Doctor of Philosophy  
Date: UPPSALA 1996  
Address: Uppsala University  
Affiliation: Computing Science Department  
Note: UPPSALA THESES IN COMPUTING SCIENCE 26  
Abstract-found: 0
Intro-found: 0
Reference: 1. <author> H. At-Kaci, </author> <title> The WAM: A (Real) Tutorial, </title> <publisher> MIT Press, </publisher> <year> 1991. </year> <month> f5g </month>
Reference-contexts: The traditional solution to this problem is to translate the Prolog code into abstract machine instructions that manage the control structures of Prolog, typically using a variant of WAM. The disadvantage with this approach is that the WAM is quite complex <ref> [159, 1] </ref> and thus difficult to implement, optimize and reason about. While most Prolog implementations use a Prolog-to-WAM compiler, an alternative approach, binarization, has been proposed by Tarau [139, 140, 141] and Demoen and Marien [51, 50].
Reference: 2. <author> K.A.M. Ali, R. Karlsson, </author> <title> The Muse or-parallel Prolog model and its performance, </title> <booktitle> in Proceedings of the North American Conference on Logic Programming, </booktitle> <publisher> MIT Press, </publisher> <year> 1990. </year> <month> f3g </month>
Reference-contexts: If several goals are resolved simultaneously, the system extracts and-parallelism. When the resolved goals share free variables, the parallelism is classified as dependent and-parallelism. When goals do not share free variables, the parallelism is said to be independent and-parallelism. Or-parallel systems have been successfully built for some years <ref> [2, 90] </ref> and have been applied to search problems. In recent years, or-parallel constraint solving has emerged as a potential application area [31, 147]. For and-parallel systems, the main problem is the management of parallel backtracking.
Reference: 3. <author> A.W. Appel, </author> <title> A runtime system, </title> <booktitle> Lisp and Symbolic Computation 3(4), </booktitle> <year> 1990. </year> <month> f74g </month>
Reference-contexts: In contrast, our algorithm only supports partial reclamation 74 Copying Garbage Collector for Prolog of memory by backtracking. Our measurements indicate that this is sufficient: the copying algorithms we describe do not reclaim appreciably less memory on backtracking than the standard mark-sweep algorithm on the measured benchmarks. Appel <ref> [3, 4] </ref> describes a simple generational garbage collector for Standard ML. The collector uses Cheney's garbage collection algorithm, which is the basis of our algorithm as well. However, his collector relies on assignments being infrequent. In Prolog, variable binding is assignment in this sense. Our algorithm handles frequent assignments efficiently.
Reference: 4. <author> A.W. Appel, </author> <title> Simple generational garbage collection and fast allocation, </title> <journal> Software|Practice and Experience 19(2) </journal> <pages> 171-183, </pages> <year> 1989. </year> <note> f10, 74, 78g </note>
Reference-contexts: That stack is then collected using a mark-sweep algorithm to retain the ordering between variables, while copying is used for the rest of the heap. Finally, the paper shows how to extend the copying algorithm to generational collection. Generational garbage collection <ref> [84, 4] </ref> relies on the observation that newly created objects tend to be short-lived. Thus, garbage collection should concentrate on recently created data. The heap is split into two or more generations, and the most recent generation is collected most frequently. <p> In contrast, our algorithm only supports partial reclamation 74 Copying Garbage Collector for Prolog of memory by backtracking. Our measurements indicate that this is sufficient: the copying algorithms we describe do not reclaim appreciably less memory on backtracking than the standard mark-sweep algorithm on the measured benchmarks. Appel <ref> [3, 4] </ref> describes a simple generational garbage collector for Standard ML. The collector uses Cheney's garbage collection algorithm, which is the basis of our algorithm as well. However, his collector relies on assignments being infrequent. In Prolog, variable binding is assignment in this sense. Our algorithm handles frequent assignments efficiently. <p> Naturally, if most of the live unbound variables have been compared in this way, the collector will have to spend more time in compacting the cv-stack. We believe this situation to be rare. 5.4 INTRODUCING GENERATIONAL GARBAGE COLLEC TION Generational garbage collection <ref> [84, 4] </ref> relies on the observation that newly created objects tend to be short-lived. Thus, garbage collection should concentrate on recently created data. The heap is split into two or more generations, and the most recent generation is collected most frequently.
Reference: 5. <author> A.W. Appel, </author> <title> Compiling With Continuations, </title> <publisher> Cambridge University Press, </publisher> <year> 1992. </year> <journal> f16, </journal> <volume> 25, 27, </volume> <month> 37g </month>
Reference-contexts: We have found that explicit operations that save and restore state can be useful, since they then can be moved and removed by program transformations. Since they generate higher-order terms in their translation, a subsequent pass of closure conversion <ref> [5] </ref> converts the term to a first-order representation. Our algorithm, in contrast, directly generates first-order terms as continuations. Neumerkel [105, 104] has proposed Continuation Prolog, which allows compilers to manipulate continuations and remove some auxilliary output variables. The new program is then translated to binary or standard Prolog. <p> This is promising for advanced compilation, and has been successfully exploited in a number of compilers <ref> [136, 79, 5] </ref>. In this paper, we propose a compilation of first-order Prolog programs into continuation-passing Horn clauses. The resulting predicates are first order and have a completely deterministic control: all control decisions have been moved to the source level. <p> An example is given below, where a predicate is recognized as deterministic and a pair of state save/restore operations can be removed altogether. Since they generate higher-order terms in their translation, a subsequent pass of closure conversion <ref> [5] </ref> converts the term to a first-order representation. Our algorithm directly generates first-order terms as continuations. Brisset and Ridoux also show how to translate exceptions by continuation capturing primitives. <p> For instance, free variables can be retrieved from linked environments instead of flat ones; several continuations may share a single environment (in particular, some success and failure continuations might share environments, e.g., in the style of Appel, where closures can share share bindings <ref> [5] </ref>). Other possibilities are discussed below. 2.5 CONCLUSION We have shown how to compile Prolog procedures into BCS, a continuation-based intermediate form (which happens to be directly executable).
Reference: 6. <author> A.W. Appel, Z. Shao, </author> <title> Callee-saves registers in continuation-passing style, </title> <booktitle> Lisp and Symbolic Computation, </booktitle> <pages> pp. 191-221, </pages> <year> 1992. </year> <month> f37g </month>
Reference-contexts: Bevemyr and Lindgren have previously shown how to adapt generational copying garbage collection to a standard WAM [21]; we expect to reuse that method. Shao and Appel have shown how to optimize continuation representations for a heap based implementation of SML <ref> [125, 6] </ref>. If their results can be applied to our representation, we can pass arguments in memory when registers are scarce, or pass continuations in registers when registers are plentiful.
Reference: 7. <author> A.W. Appel, Z. Shao, </author> <title> An Empirical and Analytic Study of Stack vs. Heap Cost for Languages with Closures, </title> <institution> Princeton University CS-TR-450-94, Princeton University, </institution> <year> 1994. </year> <month> f37g </month>
Reference-contexts: Implemented straightforwardly, a BCS program allocates all data on the heap. Research in functional languages has shown that this is not necessarily worse than stack allocation given generational copying garbage collection and fast handling of write-misses in the cache <ref> [53, 7] </ref>. Bevemyr and Lindgren have previously shown how to adapt generational copying garbage collection to a standard WAM [21]; we expect to reuse that method. Shao and Appel have shown how to optimize continuation representations for a heap based implementation of SML [125, 6].
Reference: 8. <author> K. Appleby, M. Carlsson, S. Haridi, and D. Sahlin, </author> <title> Garbage Collection for Prolog Based on WAM, </title> <journal> Communications of the ACM, </journal> <volume> 31(6) </volume> <pages> 719-741, </pages> <month> June </month> <year> 1988. </year> <journal> f19, </journal> <volume> 20, 72, 74, </volume> <month> 80g </month>
Reference-contexts: Paper D Prolog implementations such as SICStus Prolog use a mark-sweep algorithm that first marks the live data, then compacts the heap. We take the 1.3. Related work 19 implementation of Appleby et al. <ref> [8] </ref> as typical. This algorithm works in four steps and is based on the Deutsch-Schorr-Waite [121, 36] algorithm for marking and on Morris' algorithm [101, 36] for compaction (a more extensive summary is given in Paper D). <p> Demoen et al do not directly compare their algorithm's efficiency with ours. However, their mark-copy algorithm appears to be approximately as efficient as ours. They do not present measurements for generational mark-copy. Sahlin [115] has developed a method that makes the execution time of the Appleby et al. <ref> [8] </ref> algorithm proportional to the size of the live data. The main drawback of Sahlin's algorithm is that implementing the mark-sweep algorithm becomes more difficult. To our knowledge it has never been implemented. <p> Memory allocation can then be resumed. 5.2 RELATED WORK Prolog implementations such as SICStus Prolog use a mark-sweep algorithm that first marks the live data, then compacts the heap. We take the implementation of Appleby et al. <ref> [8] </ref> as typical. This algorithm works in four steps and is based on the Deutsch-Schorr-Waite [121, 36] algorithm for marking and on Morris' algorithm [101, 36] for compacting. 1. <p> However, his collector relies on assignments being infrequent. In Prolog, variable binding is assignment in this sense. Our algorithm handles frequent assignments efficiently. Sahlin [115] has developed a method that makes the execution time of the Appleby et al. <ref> [8] </ref> algorithm proportional to the size of the live data. The main drawback of Sahlin's algorithm is that implementing the mark-sweep algorithm becomes more difficult, not to mention guaranteeing that there are no programming errors in its implementation. To our knowledge it has never been implemented. <p> this 80 Copying Garbage Collector for Prolog choice point in the new generation the trail limit is set as usual. overhead is already present in the form of trail tests and there is no extra runtime penalty for using generational collection. 5.5 EVALUATION We have implemented a standard mark-sweep algorithm <ref> [8] </ref> and compared it to our copying algorithms. All garbage collection algorithms have been implemented in the same system, a sequential version of Reform Prolog. All algorithms implement early reset. The TSP program implements an approximation algorithm for the Travelling Salesman Problem. A tour of 60 cities was computed.
Reference: 9. <author> U. Banerjee, R. Eigenmann, A. Nicolau, D.A. Padua, </author> <title> Automatic program parallelization, </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> vol. 81, no. 2, </volume> <month> February </month> <year> 1993 </year> <month> f22g 136 </month>
Reference-contexts: Synchronization requirements are derived from dependence analysis, which determines what loads and stores can interfere. Recently, array dataflow analysis [54] has been developed to deal more precisely with this problem. Automatic loop-level parallelization of imperative programs has met some difficulties. For instance, Banerjee et al <ref> [9] </ref> note: "Dependence analysis in the presence of pointers has been found to be a particularly difficult problem. . . .
Reference: 10. <author> J. Barklund, J. Bevemyr, </author> <title> Executing bounded quantifications on shared memory multiprocessors, </title> <booktitle> in Programming Language Implementation and Logic Programming 1993, </booktitle> <publisher> LNCS 714, Springer Verlag, </publisher> <year> 1993. </year> <month> f22g </month>
Reference-contexts: Barklund and Millroth [11] constructed Nova Prolog, a data-parallel logic language, which later was generalized into bounded quantifications [12]. A bounded quantification expresses some action to be taken over a finite set of elements, which often allows data-parallel execution <ref> [10] </ref>. Voronkov [157] independently laid theoretical foundations for bounded quantifications. Using bounded quantifications allowed a concise data parallel approach to logic programming. Finally, Sehr [123, 124] has independently proposed extraction of data-parallelism in the context of Prolog; see also Section 1.3.
Reference: 11. <author> J. Barklund, H. Millroth, </author> <title> Nova Prolog, </title> <type> UPMAIL Technical Report 52, </type> <institution> Computing Science Department, Uppsala University, </institution> <year> 1988. </year> <title> f4, </title> <publisher> 22g </publisher>
Reference-contexts: The single-program, multiple-data (SPMD) paradigm, where each process is a parallel loop iteration, exploits data-parallelism and is arguably the most popular programming model for conventional multiprocessors. In logic programming, data-parallelism has been used in the contexts of or-parallelism [132, 147] and and-parallelism <ref> [11, 100, 12, 157] </ref>. The sixth and final approach combines the previous methods. A program may lack parallelism of a given form, which will yield poor performance on systems that extract only that form of parallelism. Some researchers 1.2. <p> The data-parallel paradigm, where the user specifies a single thread of control that operates simultaneously on many data objects, is suitable for this purpose. Barklund and Millroth <ref> [11] </ref> constructed Nova Prolog, a data-parallel logic language, which later was generalized into bounded quantifications [12]. A bounded quantification expresses some action to be taken over a finite set of elements, which often allows data-parallel execution [10]. Voronkov [157] independently laid theoretical foundations for bounded quantifications.
Reference: 12. <author> J. Barklund, H. Millroth, </author> <title> Providing iteration and concurrency in logic programs through bounded quantifications, </title> <booktitle> in Proc. International Conference on Fifth Generation Systems, </booktitle> <publisher> Ohmsha, </publisher> <year> 1992. </year> <title> f4, </title> <publisher> 22g </publisher>
Reference-contexts: The single-program, multiple-data (SPMD) paradigm, where each process is a parallel loop iteration, exploits data-parallelism and is arguably the most popular programming model for conventional multiprocessors. In logic programming, data-parallelism has been used in the contexts of or-parallelism [132, 147] and and-parallelism <ref> [11, 100, 12, 157] </ref>. The sixth and final approach combines the previous methods. A program may lack parallelism of a given form, which will yield poor performance on systems that extract only that form of parallelism. Some researchers 1.2. <p> The data-parallel paradigm, where the user specifies a single thread of control that operates simultaneously on many data objects, is suitable for this purpose. Barklund and Millroth [11] constructed Nova Prolog, a data-parallel logic language, which later was generalized into bounded quantifications <ref> [12] </ref>. A bounded quantification expresses some action to be taken over a finite set of elements, which often allows data-parallel execution [10]. Voronkov [157] independently laid theoretical foundations for bounded quantifications. Using bounded quantifications allowed a concise data parallel approach to logic programming.
Reference: 13. <author> J. Barklund, H. Millroth, </author> <title> Garbage cut for garbage collection of iterative Prolog programs, </title> <booktitle> 3rd Symposium on Logic Programming, </booktitle> <address> Salt Lake City, </address> <month> September </month> <year> 1986, </year> <note> IEEE. f19, 73g </note>
Reference-contexts: For the older generation they use a mark-sweep algorithm. The technique is similar to that described by Barklund and Millroth <ref> [13] </ref> and later by Older and Rummell [107]. We show in Paper D how a simpler copying collector can be implemented, how the troublesome primitives can be accomodated better and how generational collection can be done in a simple and intuitive way. <p> For the older generation they use a mark-sweep algorithm. The technique is similar to that described by Barklund and Millroth <ref> [13] </ref> and later by Older and Rummell [107]. We show how a simpler copying collector can be implemented, how the troublesome primitives can be accomodated better and how generational collection can be done in a simple and intuitive way. However, our view is also more radical than theirs.
Reference: 14. <author> J. Beer, </author> <title> The occur-check problem revisited, </title> <journal> Journal of Logic Programming Vol. </journal> <volume> 5, </volume> <pages> pp. 243-261, </pages> <publisher> North-Holland, </publisher> <year> 1988. </year> <note> f17, 56, 68g </note>
Reference-contexts: We have studied control flow analysis for a language with less general and somewhat different control structures, and have shown that the solution can be found quickly and represented compactly. Paper C Beer <ref> [14] </ref> was the first to exploit uninitialized variables. This was done by a runtime approach, where registers and heap cells were tagged as uninitial-ized when created and modified during execution when unifications and similar operations occurred. He found that a large portion of the dynamically occurring variables actually were uninitialized. <p> An important special case is the uninitialized variable, first studied by Beer <ref> [14] </ref> and further explored by Van Roy [154], Getzinger [58, 57] and Bigot, Gudeman and Debray [22]. Roughly speaking, a variable is unini-tialized if it is unaliased and appears the first time in the current goal. <p> Unfortunately, we have at the time of writing no access to a compiler (such as Aquarius Prolog) that enables such a comparison. 4.6 RELATED WORK Beer <ref> [14] </ref> was the first to exploit uninitialized variables. This was done by a runtime approach, where registers and heap cells were tagged as uninitial 4.7. Related work 69 ized when created and modified during execution when unifications and similar operations occurred.
Reference: 15. <author> Y. Bekkers, O. Ridoux and L. Ungaro, </author> <title> Dynamic Memory Management for Sequential Logic Programming Languages, </title> <booktitle> Proceedings of the International Workshop on Memory Management 92, </booktitle> <publisher> LNCS 637, Springer-Verlag, </publisher> <address> Berlin, 1992. f19, 73g </address>
Reference-contexts: However, where Touati and Hama still wish to retain properties such as memory recovery on backtracking, we take a more radical approach: ease of garbage collection is more important than recovering memory on backtracking. Bekkers, Ridoux and Ungaro <ref> [15] </ref> observe that it is possible to reclaim garbage collected data on backtracking if copying collection starts at the oldest choice point (bottom-to-top). Their method has several differences to ours. * Their algorithm does not preserve the heap order, which means primitives such as @&lt;/2 will work incorrectly. <p> We show below that memory recovery by backtracking is still possible, and that the new approach in practice recovers approximately as much garbage by backtracking as the conventional approach. Bekkers, Ridoux and Ungaro <ref> [15] </ref> describe an algorithm for copying garbage collector for Prolog. They observe that it is possible to reclaim garbage collected data on backtracking if copying starts at the oldest choice point (bottom-to-top).
Reference: 16. <author> G. Bell, </author> <title> Ultracomputers: a Teraflop before its time, </title> <journal> in Communications of the ACM, </journal> <volume> Vol. 35, No. 8, </volume> <year> 1992. </year> <month> f85g </month>
Reference-contexts: Promising performance figures, showing high parallel efficiency and low overhead for parallelization, have been obtained on a 24 processor shared-memory mul tiprocessor. 6.1 INTRODUCTION The Single Program Multiple Data (SPMD) model of parallel computation has recently received a lot of attention (see e.g. the article by Bell <ref> [16] </ref>).
Reference: 17. <author> J. Bevemyr, </author> <title> A Recursion Parallel Prolog Engine, </title> <booktitle> Licentiate of Philosophy Thesis, Uppsala Theses in Computer Science 16/93, 1993. f13, </booktitle> <volume> 92, 122, 152, 163, 166, </volume> <month> 167g </month>
Reference-contexts: Bindings to variables shared between loop iterations may never be conditional [103]. 3. When a loop iteration is finished, it is always deterministic. 4. A (parallel) loop iteration can not start a parallel loop. Further details, e.g., on handling predicates with side-effects, can be found in Refs. <ref> [17, 85] </ref>. By making the first restriction, we ensure that parallel and sequential execution yield the same answer. <p> Due to space limitations, we can only describe these by means of a simple example and refer to other sources for a full discussion <ref> [20, 17, 85] </ref>. Consider the program: map ([],[]). The program is compiled into the following extended WAM code. map/2: switch_on_term Lv La L1 fail Lv: try La trust L1 6.7. <p> The design of the Reform Prolog system is an attempt to meet these challenges without making programming much harder than in the sequential case. Reform Prolog has been implemented on a varity of shared address space multiprocessors. The parallel implementation is designed as extension of a sequential Prolog machine <ref> [17] </ref>. The implementation imposes very little overhead for process management, such as scheduling and load balancing. We have previously described the compilation scheme [98, 100], execution model [19, 20], and parallel abstract machine [19, 20] of Reform Prolog.
Reference: 18. <author> J. Bevemyr, </author> <title> A scheme for executing nested recursion parallelism, </title> <booktitle> in JICSLP'96 Post-Conference Workshop on Implementation Techniques, </booktitle> <month> September </month> <year> 1996. </year> <month> f15g </month>
Reference-contexts: Examples of such systems are &-Prolog [68] and ACE [63]. Results have generally been encouraging. Recently, Bevemyr <ref> [18] </ref> and the author [88] have proposed methods to extend the `flat' Reform Prolog system into efficiently executing arbitrarily nested recursion-parallel loops. 1.3 RELATED WORK Paper A There has been considerable interest in transformations of Prolog into Scheme and partial continuation passing styles and the relation of Prolog to intermediate representations,
Reference: 19. <author> J. Bevemyr, T. Lindgren, H. Millroth, </author> <title> Exploiting recursion-parallelism in Prolog, </title> <editor> in PARLE-93, eds. A. Bode, M. Reeve, G. Wolf, </editor> <publisher> LNCS 694, Springer Verlag, </publisher> <year> 1993. </year> <note> f110, 122, 123g </note>
Reference-contexts: Local operations do not require suspension or locking unification; shared terms require locking unification but no suspension, while fragile terms may not be instantiated out of the sequential order. If the compiler were not to respect fragility, the system might stray from simulating sequential behavior <ref> [19] </ref>. The goal of locality analysis is to generate precisely WAM code for parallel operations on unshared data. When this is possible, there is no paralleliza-tion overhead once the parallel execution has started. <p> The parallel implementation is designed as extension of a sequential Prolog machine [17]. The implementation imposes very little overhead for process management, such as scheduling and load balancing. We have previously described the compilation scheme [98, 100], execution model <ref> [19, 20] </ref>, and parallel abstract machine [19, 20] of Reform Prolog. In this paper we describe a set of compiler analyses and optimizations for increasing available parallelism and reducing parallelization overheads. We discuss the effectiveness of the optimizations and the runtime space/time efficiency of the compiler. <p> The parallel implementation is designed as extension of a sequential Prolog machine [17]. The implementation imposes very little overhead for process management, such as scheduling and load balancing. We have previously described the compilation scheme [98, 100], execution model <ref> [19, 20] </ref>, and parallel abstract machine [19, 20] of Reform Prolog. In this paper we describe a set of compiler analyses and optimizations for increasing available parallelism and reducing parallelization overheads. We discuss the effectiveness of the optimizations and the runtime space/time efficiency of the compiler. <p> However, the processes descend through the tree in parallel, temporally suspending when encountering not-yet-created subtrees. 2 The parallel execution model of Reform Prolog restricts the nondeterministic behaviour of parallel programs so that the following properties hold <ref> [19, 20] </ref>: * Parallel programs obey the sequential semantics of Prolog. This implies that time-dependent operations (type tests, etc.) on shared, unbound variables are carried out only when leftmost is the sequential computation order. * Parallel programs do not conditionally bind shared variables.
Reference: 20. <author> J. Bevemyr, T. Lindgren, H. Millroth, </author> <title> Reform Prolog: The language and its implementation, </title> <booktitle> in Logic Programming: Proceedings of the Tenth International Conference, </booktitle> <publisher> MIT Press, </publisher> <year> 1993. </year> <note> f92, 122, 123g </note>
Reference-contexts: Due to space limitations, we can only describe these by means of a simple example and refer to other sources for a full discussion <ref> [20, 17, 85] </ref>. Consider the program: map ([],[]). The program is compiled into the following extended WAM code. map/2: switch_on_term Lv La L1 fail Lv: try La trust L1 6.7. <p> The parallel implementation is designed as extension of a sequential Prolog machine [17]. The implementation imposes very little overhead for process management, such as scheduling and load balancing. We have previously described the compilation scheme [98, 100], execution model <ref> [19, 20] </ref>, and parallel abstract machine [19, 20] of Reform Prolog. In this paper we describe a set of compiler analyses and optimizations for increasing available parallelism and reducing parallelization overheads. We discuss the effectiveness of the optimizations and the runtime space/time efficiency of the compiler. <p> The parallel implementation is designed as extension of a sequential Prolog machine [17]. The implementation imposes very little overhead for process management, such as scheduling and load balancing. We have previously described the compilation scheme [98, 100], execution model <ref> [19, 20] </ref>, and parallel abstract machine [19, 20] of Reform Prolog. In this paper we describe a set of compiler analyses and optimizations for increasing available parallelism and reducing parallelization overheads. We discuss the effectiveness of the optimizations and the runtime space/time efficiency of the compiler. <p> However, the processes descend through the tree in parallel, temporally suspending when encountering not-yet-created subtrees. 2 The parallel execution model of Reform Prolog restricts the nondeterministic behaviour of parallel programs so that the following properties hold <ref> [19, 20] </ref>: * Parallel programs obey the sequential semantics of Prolog. This implies that time-dependent operations (type tests, etc.) on shared, unbound variables are carried out only when leftmost is the sequential computation order. * Parallel programs do not conditionally bind shared variables.
Reference: 21. <author> J. Bevemyr, T. Lindgren, </author> <title> Simple and efficient copying garbage collection for Prolog, </title> <booktitle> in Programming Language Implementation and Logic Programming 1994, </booktitle> <publisher> LNCS 844, Springer Verlag, </publisher> <year> 1994. </year> <month> f37g </month>
Reference-contexts: Research in functional languages has shown that this is not necessarily worse than stack allocation given generational copying garbage collection and fast handling of write-misses in the cache [53, 7]. Bevemyr and Lindgren have previously shown how to adapt generational copying garbage collection to a standard WAM <ref> [21] </ref>; we expect to reuse that method. Shao and Appel have shown how to optimize continuation representations for a heap based implementation of SML [125, 6].
Reference: 22. <author> P. Bigot, D. Gudeman, S.K. Debray, </author> <title> Output value placement in moded logic programs, </title> <booktitle> in Logic Programming: Proceedings of the Eleventh International Conference, </booktitle> <publisher> MIT Press, </publisher> <year> 1994. </year> <note> f18, 56, 69g 137 </note>
Reference-contexts: No indication of the number of uninitialized arguments derived was given. The recently developed strongly-typed, strongly-moded logic programming language Mercury [133] restricts programs so that outputs are always unini-tialized. A recent release performs a mode analysis similar to the one proposed in Paper C. Bigot, Gudeman and Debray <ref> [22] </ref> have developed an alternative to Van Roy's algorithm, which they use to decide which output arguments should be returned in registers, and which should be returned in memory. <p> An important special case is the uninitialized variable, first studied by Beer [14] and further explored by Van Roy [154], Getzinger [58, 57] and Bigot, Gudeman and Debray <ref> [22] </ref>. Roughly speaking, a variable is unini-tialized if it is unaliased and appears the first time in the current goal. <p> No dereferencing is needed, since the variable is unaliased. No trailing is needed, since the variable is an output argument of the predicate. In some cases, the binding can even be returned in a register <ref> [154, 22] </ref>, even though Prolog normally returns results in memory. Beer's approach was to tag uninitialized variables as such and dynamically test for uninitializedness; Van Roy instead detected them through global analysis. Debray et al relied on declared outputs in the framework of a concurrent logic language. <p> However, no indication of the number of uninitialized arguments derived was given. The recently developed strongly-typed, strongly-moded logic programming language Mercury [133] restricts programs so that outputs are always unini-tialized; however, the compiler requires declarations to this effect. Bigot, Gudeman and Debray <ref> [22] </ref> have developed an algorithm to decide which output arguments should be returned in registers, and which should be returned in memory. It may be interesting to consider this for our benchmark set, and to possibly use multiple versions of a predicate for different call sites.
Reference: 23. <author> K. De Bosschere, S.K. Debray, D. Gudeman, S. Kannan, </author> <title> Call forwarding: A simple interprocedural optimization technique for dynamically typed languages, </title> <booktitle> in Proc. Principles of Programming Languages, </booktitle> <publisher> ACM Press, </publisher> <year> 1994. </year> <journal> f18, </journal> <volume> 40, 63, </volume> <month> 69g </month>
Reference-contexts: It may be interesting to consider this for our benchmark set, and to possibly use multiple versions of a predicate for different call sites. The Bigot-Gudeman-Debray algorithm uses a single version per predicate. Gudeman, De Bosschere, Debray and Kannan <ref> [23] </ref> have defined call forwarding as a way to hoist type tests out of loops or in general to elide type tests when the call site can statically decide tests in the callee. <p> Control flow analysis of Prolog Compilers for Prolog and related languages have so far mostly taken a predicate-level view of compilation, even if global analysis has been used to improve the compilation of each predicate, and concentrated their efforts on reducing the cost of primitive operations. (Exceptions include call forwarding <ref> [23] </ref>.) Perhaps unsurprisingly, the greatest improvements have been in smaller programs, where primitive operations are relatively common and the compiler has some scope to improve sequences of primitives. <p> If p=n is a simple loop predicate, the above implies one iteration is peeled off the loop, and the rest is spent in the optimised version, as shown above in the nreverse example. (This can be viewed as a simplistic form of call forwarding <ref> [23] </ref>.) 4.5 EMPIRICAL EVALUATION We have performed the proposed transformation on a set of Prolog benchmark programs. In order to compare the sizes of programs, we listed them on a canonical format using listing/0 (i.e., no blank lines or comments). <p> It may be interesting to consider this for our benchmark set, and to possibly use multiple versions of a predicate for different call sites. The Bigot-Gudeman-Debray algorithm uses a single version per predicate. Gudeman, de Bosschere, Debray and Kannan <ref> [23] </ref> have defined call forwarding, as a way to hoist type tests out of loops, or in general when the call site can statically decide tests in the callee. As shown in the nreverse example, our polyvariant transformation occasionally generates crude `call forwarding' by breaking out calls with uninitialized arguments.
Reference: 24. <author> P. Brisset, O. Ridoux, </author> <title> Continuations in Prolog, </title> <booktitle> in Proceedings of the Tenth International Conference on Logic Programming, </booktitle> <editor> ed. D.S. Warren, </editor> <publisher> MIT Press, </publisher> <year> 1993. </year> <note> f16, 27g </note>
Reference-contexts: Nilsson [106] shows how to derive the WAM choicepoint instructions by partial evaluation of a meta interpreter. Our work is distinct from the control aspects of the WAM and adds translations of other control constructs such as cut. Brisset and Ridoux <ref> [24] </ref> propose a CPS for Prolog. As that language has -abstractions, their translation is similar to Scheme or Lisp translations discussed below, but does not explicitly manage substitutions. <p> Nilsson [106] shows how to derive the WAM choicepoint instructions by partial evaluation of a meta interpreter. Our work is distinct from the control aspects of the WAM and adds translations of other control constructs such as cut. Brisset and Ridoux <ref> [24] </ref> propose a CPS for Prolog. Since this language has -abstractions, their translation is similar to Scheme or Lisp translations discussed below, but does not explicitly manage substitutions.
Reference: 25. <author> M. Carlsson, </author> <title> On Implementing Prolog in Functional Programming, </title> <booktitle> NGC 2 (1984), </booktitle> <pages> pp. 347-359. </pages> <address> f17, 28g </address>
Reference-contexts: Translations into Scheme and Lisp Several translations of Prolog into Lisp have been proposed. We take the work of Kahn and Carlsson as representa 1.3. Related work 17 tive of this approach. Kahn and Carlsson <ref> [75, 25] </ref> propose the use of upward failure or downward success continuations. The former relies on using lazy streams to produce solutions, while the latter performs a procedure return on failure. <p> We believe that, mutatis mutandis, these optimizations could be carried out on our compiled programs as well. Translations into Scheme and Lisp Several translations from Prolog into Lisp have been proposed. We take the work of Kahn and Carlsson as representative in this approach. Kahn and Carlsson <ref> [75, 25] </ref> propose the use of upward failure or downward success continuations. The former relies on using lazy streams to produce solutions, while the latter performs a procedure return on failure.
Reference: 26. <author> M. Carlsson, </author> <title> Design and Implementation of an Or-Parallel Prolog Engine, </title> <type> Ph.D. Thesis, </type> <institution> SICS-RITA/02, </institution> <year> 1990. </year> <title> f2, </title> <publisher> 151g </publisher>
Reference-contexts: Warren's design has been the starting point of two major directions of research. The first of these is to improve on Warren's engine by local optimizations which target each clause or predicate in isolation <ref> [26] </ref>. An example of this is when clauses fail early. In this case, backtracking can be optimized [27, 96]. A second example is that unifications can be compiled into very efficient code [150, 91, 95].
Reference: 27. <author> M. Carlsson, </author> <title> On the efficiency of optimizing shallow backtracking in compiled Prolog, </title> <booktitle> in Proc. Sixth International Conference on Logic Programming, </booktitle> <publisher> MIT Press, </publisher> <year> 1989. </year> <month> f2g </month>
Reference-contexts: The first of these is to improve on Warren's engine by local optimizations which target each clause or predicate in isolation [26]. An example of this is when clauses fail early. In this case, backtracking can be optimized <ref> [27, 96] </ref>. A second example is that unifications can be compiled into very efficient code [150, 91, 95]. As a third example, clause indexing can be improved by sophisticated source-to-source transformation or code generation [42, 29].
Reference: 28. <author> J.-H. Chang, </author> <title> High performance execution of Prolog programs based on a static dependency analysis, </title> <type> Ph.D. Thesis, </type> <institution> UCB/CSD 86/263, Univ. Calif. Berkeley, </institution> <year> 1986. </year> <note> f21, 24, 124g </note>
Reference-contexts: In general, the performance is quite good when independent and-parallelism can be exploited. The overhead for parallel execution is mainly 1.3. Related work 21 related to process management and runtime tests for independence. An optimizing compiler can reduce the overhead of independence tests <ref> [102, 28] </ref>. Subsequent work combined independent and-parallelism with or-parallel [63] and dependent and-parallel [60] execution to extend the scope for parallel execution. <p> Get-zinger [58] found that a domain similar to Taylor's provided the best cost-benefit ratio for sequential compilation, out of a large collection of domains. 24 Summary The treatment of aliases in Reform Prolog is similar to that of Chang's SDDA <ref> [28] </ref>, in that aliases are treated as equivalence classes of possibly or certainly aliased variables. Subsequently, more powerful tracking of aliases has been proposed, e.g., by Muthukumar and Hermenegildo [102], Sun-dararajan [134], Jacobs and Langen [71] and the PROP domain [39, 153]. <p> The analysis precision is similar to that of Aquarius Prolog [155]. Aliasing and linearity The analyzer derives possible and certain aliases by maintaining equivalence classes of possibly or certainly aliased variables. This is similar to the techniques used by Chang <ref> [28] </ref>. A term is linear if no variable occurs more than once in it. To improve aliasing information, the analyzer tracks whether terms are linear [73]. Three classes of linearity are distinguished: linear, nonlinear, and indlist. The latter denotes lists where elements do not share variables.
Reference: 29. <author> T. Chen, I.V. Ramakrishnan, R. Ramesh, </author> <title> Multistage indexing algorithms, </title> <booktitle> in Proc. Joint International Conference & Symposium on Logic Programming'92, </booktitle> <publisher> MIT Press, </publisher> <year> 1992. </year> <title> f2, </title> <publisher> 52g </publisher>
Reference-contexts: In this case, backtracking can be optimized [27, 96]. A second example is that unifications can be compiled into very efficient code [150, 91, 95]. As a third example, clause indexing can be improved by sophisticated source-to-source transformation or code generation <ref> [42, 29] </ref>. The drawback of local techniques is that they cannot take advantage of the calling context. For example, it may be the case that a predicate is always 1.1. Introduction 3 called in a single mode. <p> We close with noting that the choice of indexing algorithm influenced these numbers heavily at times. More sophisticated indexing algorithms, e.g., Refs. <ref> [29, 154] </ref>, could be used to further improve the precision of the control flow analysis. Future work.
Reference: 30. <author> C.J. </author> <title> Cheney, A nonrecursive list compacting algorithm, </title> <journal> Communications of the ACM, </journal> <volume> 13(11) </volume> <pages> 677-678, </pages> <month> November </month> <year> 1970. </year> <journal> f9, </journal> <volume> 10, 72, </volume> <month> 75g </month>
Reference-contexts: In particular, the paper shows that copying and generational copying garbage collection can be implemented straightforwardly and efficiently on standard hardware. The basic idea of the paper is to adapt an efficient collector, such as Cheney's algorithm <ref> [30] </ref>. The paper solves a number of problems that appear when adapting generational copying collection to Prolog. Interior pointers are common in Prolog. An interior pointer is one that points directly to a field in a structure, without referring to the head of the structure. <p> On backtracking, a Prolog implementation can reclaim the topmost segment, since all data therein belong to a failed branch of the proof tree. This has led to the general use of mark-sweep algorithms that carefully preserve the segments. However, Cheney-style 10 Summary copying <ref> [30] </ref> does not preserve the segment ordering of data. Instead, it traverses the live data in breadth-first order. There are three problems with not preserving the ordering of data on the heap. First, memory can not always be reclaimed on backtracking. <p> Garbage collection is done by starting at a set of root pointers, such as registers and the local stack, and discovering what data are reachable from these pointers, or live. Memory is reclaimed by compacting the live data [101], copying them to a new area <ref> [30] </ref> or putting the dead data on a free list. Memory allocation can then be resumed. 5.2 RELATED WORK Prolog implementations such as SICStus Prolog use a mark-sweep algorithm that first marks the live data, then compacts the heap. We take the implementation of Appleby et al. [8] as typical. <p> The unnecessary trail entries are deleted by the next collection. 5.3. Algorithm 75 Mark-and-copy The copying collector is a straightforward adaption of Cheney's algorithm <ref> [30] </ref> and works in three phases. The algorithm allows the standard optimizations of early reset. The old data reside in fromspace and are evacuated into tospace. 1. Mark the live data. When a structure is encountered, mark the functor cell and all internal cells.
Reference: 31. <author> D.A. Clark, C.J. Rawlings, J. Shirazi, L-L. Li, K. Schuerman, M. Reeve, A. Veron, </author> <title> Solving large combinatorial problems in molecular biology using the ElipSys parallel constraint logic programming system, </title> <note> in Computer Journal 36(4). f3g </note>
Reference-contexts: When goals do not share free variables, the parallelism is said to be independent and-parallelism. Or-parallel systems have been successfully built for some years [2, 90] and have been applied to search problems. In recent years, or-parallel constraint solving has emerged as a potential application area <ref> [31, 147] </ref>. For and-parallel systems, the main problem is the management of parallel backtracking.
Reference: 32. <author> K.L. Clark, F. McCabe, </author> <title> The control facilities of IC-Prolog, in Expert Systems in the Micro-Electronic World (ed. </title> <editor> D. Michie), </editor> <publisher> Edinburgh University Press, </publisher> <year> 1979. </year> <month> f21g </month>
Reference-contexts: By requiring programs to be binding determinate, i.e., not undo any bindings during parallel execution, the binding conflict problem was once again reduced to global failure. Binding determinism was a major influence on the Reform Prolog execution model. Clark and McCabe introduced explicitly concurrent language constructs in IC-Prolog <ref> [32] </ref> and further developed by Clark and Gregory in their work on the Relational Language [33]. From this school of thought sprang three concurrent logic languages, Concurrent Prolog [126], Parlog [34] and Guarded Horn Clauses (GHC) [152].
Reference: 33. <author> K.L. Clark, S. Gregory, </author> <title> A relational language for parallel programming, </title> <booktitle> in Proceedings ACM Symposium on Functional Programming and Computer Architecture, </booktitle> <year> 1981. </year> <month> f21g </month>
Reference-contexts: Binding determinism was a major influence on the Reform Prolog execution model. Clark and McCabe introduced explicitly concurrent language constructs in IC-Prolog [32] and further developed by Clark and Gregory in their work on the Relational Language <ref> [33] </ref>. From this school of thought sprang three concurrent logic languages, Concurrent Prolog [126], Parlog [34] and Guarded Horn Clauses (GHC) [152]. A restriction of the latter was chosen as the base language of the Japanese Fifth Generation Project.
Reference: 34. <author> K.L. Clark, S. Gregory, </author> <title> PARLOG: A parallel logic programming language, </title> <type> report DOC 83/5, </type> <institution> Department of Computing, Imperial College, </institution> <address> London, </address> <year> 1983. </year> <note> f4, 21g 138 </note>
Reference-contexts: There are several solutions to these problems. The first, and perhaps simplest, is to define a new language, possibly better suited to concurrent execution. This approach was taken for languages such as Concurrent Prolog [126], Parlog <ref> [34] </ref> and GHC [152], and was termed stream and-parallelism: processes are goals, and communicate by means of shared variables that implement, e.g., streams of messages. Tick [146] gives a good overview of the state of the art in this field. <p> Clark and McCabe introduced explicitly concurrent language constructs in IC-Prolog [32] and further developed by Clark and Gregory in their work on the Relational Language [33]. From this school of thought sprang three concurrent logic languages, Concurrent Prolog [126], Parlog <ref> [34] </ref> and Guarded Horn Clauses (GHC) [152]. A restriction of the latter was chosen as the base language of the Japanese Fifth Generation Project.
Reference: 35. <author> M. Codish, A. Mulkers, M. Bruynooghe, M. Garcia de la Banda, M. Hermenegildo, </author> <title> Improving abstract interpretations by combining domains, </title> <booktitle> in Proceedings of the 1993 Symposium on Partial Evaluation and Program Manipulation, </booktitle> <publisher> ACM Press, </publisher> <year> 1993. </year> <month> f24g </month>
Reference-contexts: The linearity domain of Reform Prolog keeps track of whether a term contains repeated occurrences of variables [73]. The approach taken by Reform Prolog is less sophisticated than other proposed domains <ref> [73, 135, 35, 72] </ref>, but appears to work well in practice.
Reference: 36. <author> J. Cohen, </author> <title> Garbage Collection of Linked Data Structure, </title> <journal> Computing Surveys, </journal> <volume> 13(3) </volume> <pages> 341-367, </pages> <month> September, </month> <year> 1981. </year> <note> f19, 72g </note>
Reference-contexts: Paper D Prolog implementations such as SICStus Prolog use a mark-sweep algorithm that first marks the live data, then compacts the heap. We take the 1.3. Related work 19 implementation of Appleby et al. [8] as typical. This algorithm works in four steps and is based on the Deutsch-Schorr-Waite <ref> [121, 36] </ref> algorithm for marking and on Morris' algorithm [101, 36] for compaction (a more extensive summary is given in Paper D). Our copying collector uses a similar mark-phase, but copies data rather than compacting them. Touati and Hama [149] developed a generational copying garbage collector. <p> We take the 1.3. Related work 19 implementation of Appleby et al. [8] as typical. This algorithm works in four steps and is based on the Deutsch-Schorr-Waite [121, 36] algorithm for marking and on Morris' algorithm <ref> [101, 36] </ref> for compaction (a more extensive summary is given in Paper D). Our copying collector uses a similar mark-phase, but copies data rather than compacting them. Touati and Hama [149] developed a generational copying garbage collector. The heap is split into an old and a new generation. <p> We take the implementation of Appleby et al. [8] as typical. This algorithm works in four steps and is based on the Deutsch-Schorr-Waite <ref> [121, 36] </ref> algorithm for marking and on Morris' algorithm [101, 36] for compacting. 1. All live data are marked through roots found in registers, choice points, environments, and value trail entries (entries in the trail where the old value have been recorded, e.g., as a result of using setarg/3). <p> We take the implementation of Appleby et al. [8] as typical. This algorithm works in four steps and is based on the Deutsch-Schorr-Waite [121, 36] algorithm for marking and on Morris' algorithm <ref> [101, 36] </ref> for compacting. 1. All live data are marked through roots found in registers, choice points, environments, and value trail entries (entries in the trail where the old value have been recorded, e.g., as a result of using setarg/3).
Reference: 37. <author> A. Colmerauer, H. Kanoui, R. Pasero, P. Roussel, </author> <title> Un Systeme de Communication Homme-Machine en Fran~cais, </title> <institution> Groupe de Recherche en Intelligence Artificielle, Univ. de Aix-Marseille, </institution> <address> Luminy, </address> <year> 1972. </year> <month> f1g </month>
Reference-contexts: Robinson [114] showed that resolution could be used to efficiently prove theorems for such theories; Colmerauer <ref> [37] </ref> and Kowalski [78] subsequently noted that such clauses could be viewed as programs in addition to logical theories. For example, consider a predicate specifying list concatenation such that the third argument is the concatenation of the first two arguments.
Reference: 38. <author> J.S. Conery, D.F. Kibler, </author> <title> Parallel interpretation of logic programs, </title> <booktitle> in Proceedings ACM Symposium on Functional Programming and Computer Architecture, </booktitle> <year> 1981. </year> <title> f4, </title> <publisher> 20g </publisher>
Reference-contexts: This is also called determinism-driven and-parallelism, and has been the basis of a number of research systems. A fourth approach is to record the dependences between processes dynamically, so that backtracking can be done properly. This approach is taken by Conery and Kibler <ref> [38] </ref>, and subsequently by Shen [127, 128, 129]. Dependence recording and dependence checking delegated entirely to runtime is too inefficient. <p> Papers E,F and G And-parallelism A parallel logic programming system exploiting dependent and-parallelism as well as or-parallelism was designed by Conery and Kibler <ref> [38] </ref>, though with impractical overheads. De Groot [49] and Hermenegildo [67] restricted parallel execution to independent and-parallelism, by performing runtime tests to determine whether a given conjunction is to be run in parallel or sequentially. When goals do not share variables, they can be executed in parallel.
Reference: 39. <author> A. Cortesi, G. File, W. Winsborough, </author> <title> Prop revisited: propositional formulas as abstract domain for groundness analysis, </title> <booktitle> in Proc. Sixth Annual IEEE Symposium on Logic in Computer Science, </booktitle> <publisher> IEEE Press, </publisher> <year> 1991. </year> <month> f24g </month>
Reference-contexts: Subsequently, more powerful tracking of aliases has been proposed, e.g., by Muthukumar and Hermenegildo [102], Sun-dararajan [134], Jacobs and Langen [71] and the PROP domain <ref> [39, 153] </ref>. These proposals have been used for accurate groundness information, however, while Reform Prolog uses aliases to keep track of freeness. The linearity domain of Reform Prolog keeps track of whether a term contains repeated occurrences of variables [73].
Reference: 40. <author> P. Cousot, R. Cousot, </author> <title> Abstract interpretation: a unified lattice model for static analysis of programs by construction or approximation of fixpoints, </title> <booktitle> in Conference Record of the Fourth ACM Symposium on Principles of Programming Languages, </booktitle> <year> 1977. </year> <month> f124g </month>
Reference-contexts: In particular, the compiler can generate precisely the code for a sequential Prolog machine when data are local. 8.3 COMPILER ANALYSES The compiler analyses in the Reform Prolog compiler are based on abstract interpretation <ref> [40] </ref>. The abstract interpreter for Reform Prolog shares most of the characteristics of an abstract interpreter for sequential Prolog. This is natural, since each parallel process executes almost as a sequential Prolog machine. We have modified Debray's dataflow algorithms [43, 45] for analysis of parallel recursive predicates.
Reference: 41. <author> P. Cousot, R. Cousot, </author> <title> Abstract interpretation and application to logic programs, </title> <journal> Journal of Logic Programming, 1992:13:103-179. f18, 69g </journal>
Reference-contexts: We finally note that the proposed transformation can be seen as an abstract interpretation <ref> [41] </ref> followed by a program transformation based on the derived results. Paper D Prolog implementations such as SICStus Prolog use a mark-sweep algorithm that first marks the live data, then compacts the heap. We take the 1.3. Related work 19 implementation of Appleby et al. [8] as typical. <p> We finally note that the proposed transformation can be seen as an abstract interpretation <ref> [41] </ref> followed by a program transformation based on the derived results. 70 Detection of Uninitialized Arguments 4.7 CONCLUSION We have proposed a simple polyvariant transformation that detects not only `always uninitialized' predicate arguments, but also `sometimes uninitial-ized' predicate arguments, and exploits such opportunities by specializing calls and predicates.
Reference: 42. <author> S. Dawson, C.R. Ramakrishnan, I.V. Ramakrishnan, K. Sagonas, S. Skiena, T. Swift, D.S. Warren, </author> <title> Unification factoring for efficient execution of logic programs, </title> <booktitle> in Proc. Principles of Programming Languages, </booktitle> <publisher> MIT Press, </publisher> <year> 1996. </year> <month> f2g </month>
Reference-contexts: In this case, backtracking can be optimized [27, 96]. A second example is that unifications can be compiled into very efficient code [150, 91, 95]. As a third example, clause indexing can be improved by sophisticated source-to-source transformation or code generation <ref> [42, 29] </ref>. The drawback of local techniques is that they cannot take advantage of the calling context. For example, it may be the case that a predicate is always 1.1. Introduction 3 called in a single mode.
Reference: 43. <author> S.K. Debray, </author> <title> Static inference of modes and data dependencies in logic programs, </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> Vol. 11, No. 3, </volume> <month> July </month> <year> 1989, </year> <pages> pp. 418-450. </pages> <address> f23, 124g </address>
Reference-contexts: Sehr's system was, as far as we know, never implemented. Static analysis The dataflow analyzer of Reform Prolog was built by extending a framework proposed by Debray [45]. The abstract domain used is an extension of that of Debray and Warren <ref> [46, 43] </ref>. While their domain tracked may-aliases and modes, Reform Prolog tracks modes and list types (including some difference lists), may- and must-aliases, linearity, locality and determinism. Aquarius Prolog [154] used a simple mode analysis to improve sequential code. <p> The abstract interpreter for Reform Prolog shares most of the characteristics of an abstract interpreter for sequential Prolog. This is natural, since each parallel process executes almost as a sequential Prolog machine. We have modified Debray's dataflow algorithms <ref> [43, 45] </ref> for analysis of parallel recursive predicates. These algorithms compute call and success patterns for each procedure in the program. Call and success patterns describe the abstract values of the variables in a procedure call at procedure entry and exit, respectively.
Reference: 44. <author> S.K.Debray, </author> <title> A simple code improvement scheme for Prolog, </title> <journal> Journal of Logic Programming, 1992:13:57-88. f5, </journal> <volume> 16, 17, 28, 51, 91, </volume> <month> 110g </month>
Reference-contexts: In Prolog programs, the clause and goal selection rules are implicit. Paper A shows how to translate Prolog into a form where control is explicit, which simplifies the underlying implementation. Paper B shows how to derive a control flow graph from a Prolog program. Debray <ref> [44] </ref> has shown that the control flow graph is a useful tool for an optimizing compiler. Paper C proposes a robust static analysis to detect uninitialized arguments to predicates; this information can be used for important compiler optimizations. <p> Neumerkel [105, 104] has proposed Continuation Prolog, which allows compilers to manipulate continuations and remove some auxilliary output variables. The new program is then translated to binary or standard Prolog. The transformation is manual in nature, but could possibly be automated. Optimization of Prolog using control flow graphs Debray <ref> [44] </ref> proposes several optimizations based on Prolog predicates translated into control flow graphs with success and failure edges. <p> In particular, a generalized trail was required to suspend and resume a proof tree branch. Such operations are beyond the scope of this paper; all our failure continuations obey a stack-like discipline. Paper B Debray <ref> [44] </ref> and Sehr [123, 124] use control flow graphs to optimize sequential programs and extract parallelism, respectively. Both authors considered only intraprocedural control flow. In our formulation, procedure calls disappear in an interprocedural sea of assignments, continuation creation and primitive operations. <p> Our algorithm directly generates first-order terms as continuations. Brisset and Ridoux also show how to translate exceptions by continuation capturing primitives. Such operations are beyond the scope of this paper. 28 A Continuation-Passing Style for Prolog Optimization of Prolog using control flow graphs Debray <ref> [44] </ref> proposes several optimizations based on Prolog predicates translated into control flow graphs with success and failure edges. <p> This defect can be remedied. Finally, we note that changing the indexing algorithm turned out to reduce control flow information substantially, by (a) improving indexing and (b) reducing the number of new predicates generated to avoid code duplication. 3.7 RELATED WORK Debray <ref> [44] </ref> and Sehr [123, 124] use control flow graphs to optimize sequential programs and extract parallelism, respectively. However, both authors considered only intraprocedural control flow. In our formulation, procedure calls disappear in an interprocedural sea of assignments, continuation creation and primitive operations. <p> The compiler then emits instructions based on this information, possibly falling back to more conservative code generation schemes when high-precision analysis results cannot be obtained. Type analysis The type inference phase employs an abstract domain based on the standard mode-analysis domain <ref> [44] </ref>, augmented with support for lists and difference lists as well as handling of certain aliases. The compiler distinguishes the parallel and sequential types of a predicate. <p> This is managed by combining three analyses: type inference, safeness analysis and locality analysis. The type inference domain is an extension of the Debray-Warren domain <ref> [44] </ref>, with the addition of support for lists and difference lists. The compiler uses both parallel and sequential types in code generation; parallel types hold at all times in the program, while sequential types hold when leftmost. Safeness analysis investigates when the computation is in a nondeterminate, parallel state.
Reference: 45. <author> S.K. Debray, </author> <title> Efficient dataflow analysis of logic programs, </title> <journal> Journal of the ACM, </journal> <volume> Vol 39, No. 4, </volume> <month> October </month> <year> 1992. </year> <note> f23, 124g </note>
Reference-contexts: Sehr's system was, as far as we know, never implemented. Static analysis The dataflow analyzer of Reform Prolog was built by extending a framework proposed by Debray <ref> [45] </ref>. The abstract domain used is an extension of that of Debray and Warren [46, 43]. While their domain tracked may-aliases and modes, Reform Prolog tracks modes and list types (including some difference lists), may- and must-aliases, linearity, locality and determinism. <p> The abstract interpreter for Reform Prolog shares most of the characteristics of an abstract interpreter for sequential Prolog. This is natural, since each parallel process executes almost as a sequential Prolog machine. We have modified Debray's dataflow algorithms <ref> [43, 45] </ref> for analysis of parallel recursive predicates. These algorithms compute call and success patterns for each procedure in the program. Call and success patterns describe the abstract values of the variables in a procedure call at procedure entry and exit, respectively.
Reference: 46. <author> S.K. Debray, D.S. Warren, </author> <title> Automatic mode inference for logic programs, </title> <journal> Journal of Logic Programming, </journal> <volume> Vol. 5, No. 3, </volume> <year> 1988. </year> <note> f23, 124g </note>
Reference-contexts: Sehr's system was, as far as we know, never implemented. Static analysis The dataflow analyzer of Reform Prolog was built by extending a framework proposed by Debray [45]. The abstract domain used is an extension of that of Debray and Warren <ref> [46, 43] </ref>. While their domain tracked may-aliases and modes, Reform Prolog tracks modes and list types (including some difference lists), may- and must-aliases, linearity, locality and determinism. Aquarius Prolog [154] used a simple mode analysis to improve sequential code. <p> The compiler carries out four different analyses using the same basic algorithm. The abstract domains of these analyses are described below. Types The type domain is similar to that of Debray and Warren <ref> [46] </ref>, extended to handle difference lists [85]. For our present concerns it suffices to note that the type analysis can discover ground and nonvariable terms. The analysis precision is similar to that of Aquarius Prolog [155].
Reference: 47. <author> S.K. Debray, </author> <type> personal communication, </type> <year> 1995. </year> <note> f51g 139 </note>
Reference-contexts: However, both authors considered only intraprocedural control flow. In our formulation, procedure calls disappear in an interprocedural sea of assignments, continuation creation and primitive operations. Debray et al have recently implemented a control flow analysis based on context free grammars <ref> [47] </ref> in the jc system. The net result is equivalent to the success control flow analysis described in this paper, though their analysis also handles concurrency. Shivers [130] proposed control flow analysis for the purpose of recovering the control flow graph of higher-order functional programs.
Reference: 48. <author> S.K. Debray, T. Proebsting, </author> <title> Inter-procedural control flow analysis of first order programs with tail call optimization, </title> <type> Draft, </type> <institution> University of Arizona, </institution> <month> May </month> <year> 1996. </year> <month> f17g </month>
Reference-contexts: Paper B Debray [44] and Sehr [123, 124] use control flow graphs to optimize sequential programs and extract parallelism, respectively. Both authors considered only intraprocedural control flow. In our formulation, procedure calls disappear in an interprocedural sea of assignments, continuation creation and primitive operations. Debray and Proebsting <ref> [48] </ref> have recently shown that control flow analyses can be transformed into parsing problems, and use LR (0) and LR (1)-items to perform the analysis. They consider languages with tail recursion, but lacking backtracking.
Reference: 49. <author> D. De Groot, </author> <title> Restricted AND-parallelism, </title> <booktitle> in Proceedings of the International Conference on Fifth Generation Systems, </booktitle> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1984. </year> <title> f4, </title> <publisher> 20g </publisher>
Reference-contexts: Tick [146] gives a good overview of the state of the art in this field. An example of a recent implementation of such a language is the Monaco compiler [145]. A second approach, compatible with Prolog, is to extract independent and-parallelism <ref> [49, 67] </ref>. A failed process needs in this case only notify its siblings of the failure, which is substantially simpler than full dependence checking. A number of systems have been built to exploit independent and-parallelism [68, 63]. <p> Papers E,F and G And-parallelism A parallel logic programming system exploiting dependent and-parallelism as well as or-parallelism was designed by Conery and Kibler [38], though with impractical overheads. De Groot <ref> [49] </ref> and Hermenegildo [67] restricted parallel execution to independent and-parallelism, by performing runtime tests to determine whether a given conjunction is to be run in parallel or sequentially. When goals do not share variables, they can be executed in parallel.
Reference: 50. <author> B. Demoen, A. Marien, </author> <title> Implementation of Prolog as Binary Definite Programs, </title> <booktitle> Proceedings of the Second Russian Conference on Logic Programming, </booktitle> <publisher> Springer Verlag. f5, </publisher> <address> 16, 27g </address>
Reference-contexts: The disadvantage with this approach is that the WAM is quite complex [159, 1] and thus difficult to implement, optimize and reason about. While most Prolog implementations use a Prolog-to-WAM compiler, an alternative approach, binarization, has been proposed by Tarau [139, 140, 141] and Demoen and Marien <ref> [51, 50] </ref>. The idea is to manage control by transforming general Prolog programs into a subset of Prolog that can be implemented more easily. When binarization is applied globally (i.e., for the entire program), the result is that every clause has a single goal. <p> This simplifies compilation but excludes some optimizations. Our representation directly admits a translation into control flow graphs, since control flow is explicit. Translations in Prolog Binarization, as proposed by Tarau [139] and De-moen and Marien <ref> [51, 50] </ref> ignores the failure continuation that relates the different clauses of a predicate. <p> This simplifies compilation but excludes some optimizations. Our representation directly admits a translation into control flow graphs, since control flow is explicit. 2.2. Related work 27 Translations in Prolog Our technique can be viewed as an extension of binarization as described by Tarau [139] and Demoen <ref> [51, 50] </ref>. Binarization translates clauses H into their (success-) continuation passing forms: H (S) call (S) H (S) B 1 (B 2 (: : : (B n (S)) : : :)) Thus, the control rule of SLD-resolution is made explicit.
Reference: 51. <author> B. Demoen, </author> <title> On the Transformation of a Prolog Program to a More Efficient Binary Program, </title> <booktitle> Proceedings of the LOPSTR'92 workshop, </booktitle> <address> Manchester, </address> <month> July </month> <year> 1992. </year> <note> f5, 16, 27g </note>
Reference-contexts: The disadvantage with this approach is that the WAM is quite complex [159, 1] and thus difficult to implement, optimize and reason about. While most Prolog implementations use a Prolog-to-WAM compiler, an alternative approach, binarization, has been proposed by Tarau [139, 140, 141] and Demoen and Marien <ref> [51, 50] </ref>. The idea is to manage control by transforming general Prolog programs into a subset of Prolog that can be implemented more easily. When binarization is applied globally (i.e., for the entire program), the result is that every clause has a single goal. <p> This simplifies compilation but excludes some optimizations. Our representation directly admits a translation into control flow graphs, since control flow is explicit. Translations in Prolog Binarization, as proposed by Tarau [139] and De-moen and Marien <ref> [51, 50] </ref> ignores the failure continuation that relates the different clauses of a predicate. <p> This simplifies compilation but excludes some optimizations. Our representation directly admits a translation into control flow graphs, since control flow is explicit. 2.2. Related work 27 Translations in Prolog Our technique can be viewed as an extension of binarization as described by Tarau [139] and Demoen <ref> [51, 50] </ref>. Binarization translates clauses H into their (success-) continuation passing forms: H (S) call (S) H (S) B 1 (B 2 (: : : (B n (S)) : : :)) Thus, the control rule of SLD-resolution is made explicit.
Reference: 52. <author> B. Demoen, G. Engels, P. Tarau, </author> <title> Segment order preserving copying garbage collection for WAM based Prolog, </title> <booktitle> in ACM Symposium on Applied Computing, </booktitle> <publisher> ACM Press, </publisher> <year> 1995. </year> <month> f20g </month>
Reference-contexts: In contrast, our algorithm only supports partial reclamation of memory by backtracking. Our measurements indicate that this is sufficient: the copying algorithms we describe do not reclaim appreciably less 20 Summary memory on backtracking than the standard mark-sweep algorithm on the measured benchmarks. Subsequently, Demoen, Engels and Tarau <ref> [52] </ref> proposed and implemented an extension of the Bekkers-Ridoux-Ungaro bottom-up copying algorithm, combined with our mark-copy algorithm. They note that instant reclaiming of segments can be improved by moving data between segments.
Reference: 53. <author> A. Diwan, D. Tarditi, E. Moss, </author> <title> Memory Subsystem Performance of Programs with Intensive Heap Allocation, </title> <type> Technical report CMU-CS-93-227, </type> <institution> Carnegie-Mellon University, </institution> <year> 1993. </year> <month> f37g </month>
Reference-contexts: Implemented straightforwardly, a BCS program allocates all data on the heap. Research in functional languages has shown that this is not necessarily worse than stack allocation given generational copying garbage collection and fast handling of write-misses in the cache <ref> [53, 7] </ref>. Bevemyr and Lindgren have previously shown how to adapt generational copying garbage collection to a standard WAM [21]; we expect to reuse that method. Shao and Appel have shown how to optimize continuation representations for a heap based implementation of SML [125, 6].
Reference: 54. <author> P. Feautrier, </author> <title> Dataflow analysis of array and scalar references, </title> <journal> International Journal of Parallel Programming, </journal> <volume> Vol. 20, No. 1, </volume> <month> February </month> <year> 1991, </year> <note> Plenum Press f22g </note>
Reference-contexts: Vectorization translates program statements into vector instructions for supercomputers, and we will not consider it further. Concurrentization entails adding synchronization primitives to the loop body to ensure correct execution. Synchronization requirements are derived from dependence analysis, which determines what loads and stores can interfere. Recently, array dataflow analysis <ref> [54] </ref> has been developed to deal more precisely with this problem. Automatic loop-level parallelization of imperative programs has met some difficulties. For instance, Banerjee et al [9] note: "Dependence analysis in the presence of pointers has been found to be a particularly difficult problem. . . .
Reference: 55. <author> M. Felleisen, </author> <title> Transliterating Prolog into Scheme, </title> <institution> Computer Science Department Technical Report 182, Indiana University, </institution> <year> 1985. </year> <note> f17, 28g </note>
Reference-contexts: Kahn and Carlsson [76] then employ partial evaluation to produce quite good Lisp code for naive reverse with modes. Research at Indiana University showed that Horn clauses could be translated into Scheme by fairly straightforward means <ref> [55, 77] </ref> and that some care was needed to extend Prolog with continuations [66]. In particular, a generalized trail was required to suspend and resume a proof tree branch. Such operations are beyond the scope of this paper; all our failure continuations obey a stack-like discipline. <p> Kahn and Carlsson [76] then employ partial evaluation to produce quite good Lisp code for naive reverse with modes. Research at Indiana University showed that Horn clauses could be translated into Scheme by fairly straightforward means <ref> [55, 77] </ref> and that some care was needed to extend Prolog with continuations [66]. In particular, a generalized trail was required to suspend and resume a proof tree branch.
Reference: 56. <author> M.A. Friedman, </author> <title> A characterization of Prolog execution, </title> <type> Ph.D. Thesis, </type> <institution> University of Wisconsin at Madison, </institution> <year> 1992. </year> <note> f39, 131g </note>
Reference-contexts: Failure control is more complex, though there are typically 10 or less jump targets per predicate for the benchmark set. 3.1 INTRODUCTION Prolog programs spend much of their time in control management <ref> [144, 148, 56] </ref>. Since procedure calls are very common, much effort is spent in saving and restoring parameters from the stack; furthermore, a procedure call typically requires some shu*ing of parameters, as well as building new arguments. <p> Two factors contribute to this phenomenon: First, locking instructions are infrequent even in unoptimized code. Locking is spatially infrequent, since assignments to heap variables is a small fraction of the total amount of data written in Prolog implementations <ref> [144, 56] </ref>. 132 Compiler Optimizations in Reform Prolog Locking is temporally infrequent, since our Prolog implementation is based on byte-code emulation of WAM [159] instructions. In unoptimized code, 2100-3700 machine instructions were executed for each locking operation on the three larger benchmarks.
Reference: 57. <author> T. Getzinger, </author> <title> Abstract Interpretation for the Compile-Time Analysis of Logic Programs, </title> <type> Ph.D. Thesis, Technical Report ACAL-TR-93-09, </type> <institution> University of South California, </institution> <month> September </month> <year> 1993. </year> <journal> f8, </journal> <volume> 18, 56, 57, 69, </volume> <month> 128g </month>
Reference-contexts: While some systems require the user to declare output arguments, Prolog compilers traditionally derive such information by global analysis. A simple and effective approach is taken by Van Roy [155] and Getzinger <ref> [57] </ref>. However, their analysis has the drawback of being monovariant : all calls to a predicate in the entire program are used to summarize which arguments are uninitialized. <p> Van Roy also developed an algorithm that could return approximately 1/3 of the outputs in registers. Getzinger improved on Van Roy's analysis and explored some alternatives, but remained within the monovariant framework <ref> [57, 58] </ref>. Taylor [142] subsequently incorporated uninitialized variables into his Parma compiler, and reported substantial performance gains. No indication of the number of uninitialized arguments derived was given. The recently developed strongly-typed, strongly-moded logic programming language Mercury [133] restricts programs so that outputs are always unini-tialized. <p> An important special case is the uninitialized variable, first studied by Beer [14] and further explored by Van Roy [154], Getzinger <ref> [58, 57] </ref> and Bigot, Gudeman and Debray [22]. Roughly speaking, a variable is unini-tialized if it is unaliased and appears the first time in the current goal. <p> When the predicate returns, we assume that all uninitialized variables have been initialized. This is similar to Van Roy's or Getzinger's treatments <ref> [154, 57] </ref>. Note that last-use information is vital. Consider the following example. p :- X = [A|As], q (X), r (A,As). r (a,[b]). Since X, A and As appear the first time in X=[A|As], it is tempting to consider A and As uninitialized. <p> Getzinger improved on Van Roy's analysis and explored some alternatives, but remained within the monovariant framework <ref> [57, 58] </ref>. Taylor [142] subsequently incorporated uninitialized variables into his Parma compiler, and reported substantial performance gains. However, no indication of the number of uninitialized arguments derived was given. <p> Furthermore, the absolute compilation times (0.6 to 14 seconds) are quite reasonable, in particular when considering that the SUN 630/MP is not a particulary fast machine by today's standards. Aquarius Prolog seems to have similar absolute analysis times on similar hardware <ref> [57] </ref>. 8.7 ANALYSIS RESULTS We measured analysis results for arguments in procedures called from parallel predicates. The following table shows the percentages of ground arguments and the locality information of non-ground arguments. The `total' percentage is weighted with respect to the total number of predicate arguments in all benchmarks. 8.8.
Reference: 58. <author> T. Getzinger, </author> <title> The Costs and Benefits of Abstract Interpretation-Driven Prolog Optimization, </title> <booktitle> in Proc. First International Static Analysis Symposium, </booktitle> <publisher> LNCS 864, Springer Verlag, </publisher> <year> 1994. </year> <journal> f18, </journal> <volume> 23, 56, </volume> <month> 69g </month>
Reference-contexts: Van Roy also developed an algorithm that could return approximately 1/3 of the outputs in registers. Getzinger improved on Van Roy's analysis and explored some alternatives, but remained within the monovariant framework <ref> [57, 58] </ref>. Taylor [142] subsequently incorporated uninitialized variables into his Parma compiler, and reported substantial performance gains. No indication of the number of uninitialized arguments derived was given. The recently developed strongly-typed, strongly-moded logic programming language Mercury [133] restricts programs so that outputs are always unini-tialized. <p> Taylor [142, 143] proposed a somewhat stronger domain, based on depth-k tracking of term structure, modes, constant types and recursive list types, but did not publish any precision results. Get-zinger <ref> [58] </ref> found that a domain similar to Taylor's provided the best cost-benefit ratio for sequential compilation, out of a large collection of domains. 24 Summary The treatment of aliases in Reform Prolog is similar to that of Chang's SDDA [28], in that aliases are treated as equivalence classes of possibly or <p> An important special case is the uninitialized variable, first studied by Beer [14] and further explored by Van Roy [154], Getzinger <ref> [58, 57] </ref> and Bigot, Gudeman and Debray [22]. Roughly speaking, a variable is unini-tialized if it is unaliased and appears the first time in the current goal. <p> Getzinger improved on Van Roy's analysis and explored some alternatives, but remained within the monovariant framework <ref> [57, 58] </ref>. Taylor [142] subsequently incorporated uninitialized variables into his Parma compiler, and reported substantial performance gains. However, no indication of the number of uninitialized arguments derived was given.
Reference: 59. <author> D. Gudeman, K. De Bosschere, S.K. Debray, </author> <title> jc: an efficient and portable sequential implementation of Janus, </title> <booktitle> in Logic Programming: Proceedings of the Joint International Conference and Symposium on Logic Programming, </booktitle> <publisher> MIT Press, </publisher> <year> 1992. </year> <note> f8g 140 </note>
Reference-contexts: An uninitialized variable need not be trailed or dereferenced when it is bound, nor need we initialize its memory location beforehand. Optimizing uninitialized variables can lead to considerable execution time savings, and is used by all high-performance logic programming implementations today <ref> [155, 142, 133, 59] </ref>. While some systems require the user to declare output arguments, Prolog compilers traditionally derive such information by global analysis. A simple and effective approach is taken by Van Roy [155] and Getzinger [57].
Reference: 60. <author> G. Gupta, V. Santos Costa, R. Yang, M.V. Hermenegildo, IDIOM: </author> <title> Intergrating Dependent and-, Independent and- and Or-parallelism, </title> <booktitle> in Logic Programming: Proceedings of the 1991 International Symposium, </booktitle> <publisher> MIT Press, </publisher> <year> 1991. </year> <note> f5, 21g </note>
Reference-contexts: A program may lack parallelism of a given form, which will yield poor performance on systems that extract only that form of parallelism. Some researchers 1.2. Summary 5 have set the goal to extract maximal parallelism from Prolog programs, by simultaneously exploiting multiple forms of parallelism <ref> [60, 63, 62, 61, 118] </ref>. 1.2 SUMMARY This section summarizes the papers in this thesis and discusses the scientific contributions, and my personal contributions. In Prolog programs, the clause and goal selection rules are implicit. <p> The overhead for parallel execution is mainly 1.3. Related work 21 related to process management and runtime tests for independence. An optimizing compiler can reduce the overhead of independence tests [102, 28]. Subsequent work combined independent and-parallelism with or-parallel [63] and dependent and-parallel <ref> [60] </ref> execution to extend the scope for parallel execution. The execution engines for the combination proposals are far more complex than a sequential Prolog engine, which is unfortunate if the goal is high absolute speedups with respect to a sequential system.
Reference: 61. <author> G. Gupta, B. Jayaraman, </author> <title> Compiled and-or parallel execution of logic programs, </title> <booktitle> in Proc. North American Conference on Logic Programming '89, </booktitle> <publisher> MIT Press, </publisher> <year> 1989. </year> <month> f5g </month>
Reference-contexts: A program may lack parallelism of a given form, which will yield poor performance on systems that extract only that form of parallelism. Some researchers 1.2. Summary 5 have set the goal to extract maximal parallelism from Prolog programs, by simultaneously exploiting multiple forms of parallelism <ref> [60, 63, 62, 61, 118] </ref>. 1.2 SUMMARY This section summarizes the papers in this thesis and discusses the scientific contributions, and my personal contributions. In Prolog programs, the clause and goal selection rules are implicit.
Reference: 62. <author> G. Gupta, B. Jayaraman, </author> <title> Optimizing and-or parallel implementations, </title> <booktitle> in Proc. North American Conference on Logic Programming '90, </booktitle> <publisher> MIT Press, </publisher> <year> 1990. </year> <month> f5g </month>
Reference-contexts: A program may lack parallelism of a given form, which will yield poor performance on systems that extract only that form of parallelism. Some researchers 1.2. Summary 5 have set the goal to extract maximal parallelism from Prolog programs, by simultaneously exploiting multiple forms of parallelism <ref> [60, 63, 62, 61, 118] </ref>. 1.2 SUMMARY This section summarizes the papers in this thesis and discusses the scientific contributions, and my personal contributions. In Prolog programs, the clause and goal selection rules are implicit.
Reference: 63. <author> G. Gupta, </author> <title> M.V. Hermenegildo, ACE: And/Or-parallel copying-based execution of logic programs, in Parallel Execution of Logic Programs, </title> <publisher> LNCS 569, Springer Verlag, </publisher> <year> 1991. </year> <title> f4, </title> <type> 5, 15, </type> <institution> 21g </institution>
Reference-contexts: A second approach, compatible with Prolog, is to extract independent and-parallelism [49, 67]. A failed process needs in this case only notify its siblings of the failure, which is substantially simpler than full dependence checking. A number of systems have been built to exploit independent and-parallelism <ref> [68, 63] </ref>. Subsequently, the notion of independence has been refined so that processes may share variables but still are independent as long as they do not affect each other's control [69]. The third approach (again used with Prolog) is to only bind variables when the binding process is deterministic. <p> A program may lack parallelism of a given form, which will yield poor performance on systems that extract only that form of parallelism. Some researchers 1.2. Summary 5 have set the goal to extract maximal parallelism from Prolog programs, by simultaneously exploiting multiple forms of parallelism <ref> [60, 63, 62, 61, 118] </ref>. 1.2 SUMMARY This section summarizes the papers in this thesis and discusses the scientific contributions, and my personal contributions. In Prolog programs, the clause and goal selection rules are implicit. <p> Related work 15 Reflections and later work Our results inspired a number of subsequent papers [70, 112, 111] which incorporate some of the Reform Prolog techniques into control-parallel systems, in particular the `fast unfolding' of Reform compilation. Examples of such systems are &-Prolog [68] and ACE <ref> [63] </ref>. Results have generally been encouraging. <p> The overhead for parallel execution is mainly 1.3. Related work 21 related to process management and runtime tests for independence. An optimizing compiler can reduce the overhead of independence tests [102, 28]. Subsequent work combined independent and-parallelism with or-parallel <ref> [63] </ref> and dependent and-parallel [60] execution to extend the scope for parallel execution. The execution engines for the combination proposals are far more complex than a sequential Prolog engine, which is unfortunate if the goal is high absolute speedups with respect to a sequential system.
Reference: 64. <author> S. Haridi, </author> <title> A logic programming language based on the Andorra model, </title> <booktitle> New Generation Computing 7(1990), </booktitle> <pages> pp. 109-125, </pages> <publisher> Springer Verlag, </publisher> <year> 1990. </year> <month> f21g </month>
Reference-contexts: A consequence was that binding conflicts caused global failure rather than backtracking. (Certain proposed metaprimitives allow programmers to encapsulate a computation to recover from failure.) The Andorra Kernel Language <ref> [64] </ref> was proposed to join concurrent logic programming with Prolog's don't-know nondeterminism by splitting nondeterministic states into several deterministic computations. 22 Summary A different approach is to provide the programmer with powerful linguistic constructs to specify large, regular parallel computations.
Reference: 65. <author> W.L. Harrison III, </author> <title> The interprocedural analysis and parallelization of Scheme programs, </title> <journal> Lisp and Symbolic Computation, </journal> <volume> Vol. 2, no. 3/4, </volume> <year> 1989 </year> <month> f22g </month>
Reference-contexts: We believe this is in part due to the inability to parallelize loops with procedure calls. Reform Prolog allows procedure calls in parallel procedures and checks statically that they conform to the execution model. Harrison <ref> [65] </ref> developed a system that parallelizes recursive calls in Scheme. In particular, the implementation of recursion-parallelism is similar to ours, though presented in a more general context. Harrison performs side-effect and dependence analysis to restructure and parallelize Scheme programs. 1.3.
Reference: 66. <author> C. Haynes, </author> <title> Logic continuations, </title> <journal> Journal of Logic Programming, </journal> <volume> 4(2) </volume> <pages> 157-176, </pages> <month> June </month> <year> 1987. </year> <note> f17, 28g </note>
Reference-contexts: Kahn and Carlsson [76] then employ partial evaluation to produce quite good Lisp code for naive reverse with modes. Research at Indiana University showed that Horn clauses could be translated into Scheme by fairly straightforward means [55, 77] and that some care was needed to extend Prolog with continuations <ref> [66] </ref>. In particular, a generalized trail was required to suspend and resume a proof tree branch. Such operations are beyond the scope of this paper; all our failure continuations obey a stack-like discipline. <p> Kahn and Carlsson [76] then employ partial evaluation to produce quite good Lisp code for naive reverse with modes. Research at Indiana University showed that Horn clauses could be translated into Scheme by fairly straightforward means [55, 77] and that some care was needed to extend Prolog with continuations <ref> [66] </ref>. In particular, a generalized trail was required to suspend and resume a proof tree branch.
Reference: 67. <author> M.V. Hermenegildo, </author> <title> An abstract machine for restricted AND-parallel execution of logic programs, </title> <booktitle> in Third International Conference on Logic Programming, </booktitle> <publisher> LNCS 225, Springer Verlag, </publisher> <year> 1986. </year> <title> f4, </title> <publisher> 20g </publisher>
Reference-contexts: Tick [146] gives a good overview of the state of the art in this field. An example of a recent implementation of such a language is the Monaco compiler [145]. A second approach, compatible with Prolog, is to extract independent and-parallelism <ref> [49, 67] </ref>. A failed process needs in this case only notify its siblings of the failure, which is substantially simpler than full dependence checking. A number of systems have been built to exploit independent and-parallelism [68, 63]. <p> Papers E,F and G And-parallelism A parallel logic programming system exploiting dependent and-parallelism as well as or-parallelism was designed by Conery and Kibler [38], though with impractical overheads. De Groot [49] and Hermenegildo <ref> [67] </ref> restricted parallel execution to independent and-parallelism, by performing runtime tests to determine whether a given conjunction is to be run in parallel or sequentially. When goals do not share variables, they can be executed in parallel. <p> When goals do not share variables, they can be executed in parallel. Backtracking can then be managed locally, possibly followed by killing all sibling goals when one goal in a parallel conjunction failed. Systems that rely solely on independent and-parallelism have been investigated by Hermenegildo <ref> [67, 68, 69] </ref> and others. In general, the performance is quite good when independent and-parallelism can be exploited. The overhead for parallel execution is mainly 1.3. Related work 21 related to process management and runtime tests for independence. An optimizing compiler can reduce the overhead of independence tests [102, 28].
Reference: 68. <author> M.V. Hermenegildo, K.J. Greene, </author> <title> &-Prolog and its performance: exploiting independent and-parallelism, </title> <booktitle> in Proceedings of the Seventh International Conference on Logic Programming, </booktitle> <publisher> MIT Press, </publisher> <year> 1990. </year> <note> f4, 15, 20g </note>
Reference-contexts: A second approach, compatible with Prolog, is to extract independent and-parallelism [49, 67]. A failed process needs in this case only notify its siblings of the failure, which is substantially simpler than full dependence checking. A number of systems have been built to exploit independent and-parallelism <ref> [68, 63] </ref>. Subsequently, the notion of independence has been refined so that processes may share variables but still are independent as long as they do not affect each other's control [69]. The third approach (again used with Prolog) is to only bind variables when the binding process is deterministic. <p> Related work 15 Reflections and later work Our results inspired a number of subsequent papers [70, 112, 111] which incorporate some of the Reform Prolog techniques into control-parallel systems, in particular the `fast unfolding' of Reform compilation. Examples of such systems are &-Prolog <ref> [68] </ref> and ACE [63]. Results have generally been encouraging. <p> When goals do not share variables, they can be executed in parallel. Backtracking can then be managed locally, possibly followed by killing all sibling goals when one goal in a parallel conjunction failed. Systems that rely solely on independent and-parallelism have been investigated by Hermenegildo <ref> [67, 68, 69] </ref> and others. In general, the performance is quite good when independent and-parallelism can be exploited. The overhead for parallel execution is mainly 1.3. Related work 21 related to process management and runtime tests for independence. An optimizing compiler can reduce the overhead of independence tests [102, 28].
Reference: 69. <author> M.V. Hermenegildo, F. Rossi, </author> <title> Non-strict independent and-parallelism, </title> <booktitle> in Proceedings of the Seventh International Conference on Logic Programming, </booktitle> <publisher> MIT Press, </publisher> <year> 1990. </year> <note> f4, 20, 126g </note>
Reference-contexts: A number of systems have been built to exploit independent and-parallelism [68, 63]. Subsequently, the notion of independence has been refined so that processes may share variables but still are independent as long as they do not affect each other's control <ref> [69] </ref>. The third approach (again used with Prolog) is to only bind variables when the binding process is deterministic. No dependence checking is then needed, because the producer of the binding cannot backtrack to produce another binding [162, 160, 103]. <p> When goals do not share variables, they can be executed in parallel. Backtracking can then be managed locally, possibly followed by killing all sibling goals when one goal in a parallel conjunction failed. Systems that rely solely on independent and-parallelism have been investigated by Hermenegildo <ref> [67, 68, 69] </ref> and others. In general, the performance is quite good when independent and-parallelism can be exploited. The overhead for parallel execution is mainly 1.3. Related work 21 related to process management and runtime tests for independence. An optimizing compiler can reduce the overhead of independence tests [102, 28]. <p> (will-be-fragile) when subjected to time-dependent operations|to subsequent processes, wbf variables will be seen as fragile; * unshared data are local. 126 Compiler Optimizations in Reform Prolog The locality domain is thus: local v robust v wbf v fragile This locality domain can furthermore be used to detect non-strict independent and-parallelism <ref> [69] </ref>. As long as a process does not contain fragile data, it is independent of the results of other processes.
Reference: 70. <author> M. Hermenegildo, M. Carro, </author> <title> Relating data-parallelism and (And )parallelism in logic programs, </title> <journal> Computer Languages Journal, </journal> <note> to appear. f15g </note>
Reference-contexts: The Reform Prolog compiler was written by me, while the execution engine was written by Bevemyr. The compiler optimizations were designed in collaboration with with Johan Bevemyr and implemented by me. 1.3. Related work 15 Reflections and later work Our results inspired a number of subsequent papers <ref> [70, 112, 111] </ref> which incorporate some of the Reform Prolog techniques into control-parallel systems, in particular the `fast unfolding' of Reform compilation. Examples of such systems are &-Prolog [68] and ACE [63]. Results have generally been encouraging.
Reference: 71. <author> D. Jacobs, A. Langen, </author> <title> Accurate and efficient approximation of variable aliasing in logic programs, </title> <booktitle> in Proc. North American Conference on Logic Programming 1989, </booktitle> <publisher> MIT Press, </publisher> <year> 1989. </year> <note> f24g 141 </note>
Reference-contexts: Subsequently, more powerful tracking of aliases has been proposed, e.g., by Muthukumar and Hermenegildo [102], Sun-dararajan [134], Jacobs and Langen <ref> [71] </ref> and the PROP domain [39, 153]. These proposals have been used for accurate groundness information, however, while Reform Prolog uses aliases to keep track of freeness. The linearity domain of Reform Prolog keeps track of whether a term contains repeated occurrences of variables [73].
Reference: 72. <author> G. Janssens, M. Bruynooghe, </author> <title> Deriving descriptions of possible values of program variables by means of abstract interpretation, </title> <journal> Journal of Logic Programming 1992:13:205-258. f24g </journal>
Reference-contexts: The linearity domain of Reform Prolog keeps track of whether a term contains repeated occurrences of variables [73]. The approach taken by Reform Prolog is less sophisticated than other proposed domains <ref> [73, 135, 35, 72] </ref>, but appears to work well in practice.
Reference: 73. <author> N. Jones & H. Stndergaard, </author> <title> A semantics-based framework for the abstract interpretation of Prolog, </title> <type> report 86/14, </type> <institution> University of Copen-hagen, </institution> <year> 1986. </year> <note> f24, 124g </note>
Reference-contexts: These proposals have been used for accurate groundness information, however, while Reform Prolog uses aliases to keep track of freeness. The linearity domain of Reform Prolog keeps track of whether a term contains repeated occurrences of variables <ref> [73] </ref>. The approach taken by Reform Prolog is less sophisticated than other proposed domains [73, 135, 35, 72], but appears to work well in practice. <p> The linearity domain of Reform Prolog keeps track of whether a term contains repeated occurrences of variables [73]. The approach taken by Reform Prolog is less sophisticated than other proposed domains <ref> [73, 135, 35, 72] </ref>, but appears to work well in practice. <p> This is similar to the techniques used by Chang [28]. A term is linear if no variable occurs more than once in it. To improve aliasing information, the analyzer tracks whether terms are linear <ref> [73] </ref>. Three classes of linearity are distinguished: linear, nonlinear, and indlist. The latter denotes lists where elements do not share variables.
Reference: 74. <author> N. Jones, C. Gomard, P. Sestoft, </author> <title> Partial Evaluation and Automatic Program Generation, </title> <publisher> Prentice-Hall, </publisher> <year> 1993. </year> <month> f17g </month>
Reference-contexts: Debray and Proebsting [48] have recently shown that control flow analyses can be transformed into parsing problems, and use LR (0) and LR (1)-items to perform the analysis. They consider languages with tail recursion, but lacking backtracking. Shivers [130] and Jones, Gomard and Sestoft <ref> [74] </ref> proposed control flow analysis for the purpose of recovering the control flow graph of higher-order functional programs. We have studied control flow analysis for a language with less general and somewhat different control structures, and have shown that the solution can be found quickly and represented compactly.
Reference: 75. <author> K. Kahn, M. Carlsson, </author> <title> How To Implement Prolog on a Lisp Machine, in Implementations of Prolog, </title> <editor> ed. J. Campbell, </editor> <publisher> Ellis Horwood, </publisher> <year> 1984. </year> <note> f17, 28g </note>
Reference-contexts: Translations into Scheme and Lisp Several translations of Prolog into Lisp have been proposed. We take the work of Kahn and Carlsson as representa 1.3. Related work 17 tive of this approach. Kahn and Carlsson <ref> [75, 25] </ref> propose the use of upward failure or downward success continuations. The former relies on using lazy streams to produce solutions, while the latter performs a procedure return on failure. <p> We believe that, mutatis mutandis, these optimizations could be carried out on our compiled programs as well. Translations into Scheme and Lisp Several translations from Prolog into Lisp have been proposed. We take the work of Kahn and Carlsson as representative in this approach. Kahn and Carlsson <ref> [75, 25] </ref> propose the use of upward failure or downward success continuations. The former relies on using lazy streams to produce solutions, while the latter performs a procedure return on failure.
Reference: 76. <author> K. Kahn, M. Carlsson, </author> <title> The Compilation of Prolog Programs without the Use of a Prolog Compiler, </title> <type> UPMAIL Technical Report 27, </type> <institution> Uppsala University, </institution> <year> 1984. </year> <note> f17, 28g </note>
Reference-contexts: The former relies on using lazy streams to produce solutions, while the latter performs a procedure return on failure. We note that both methods to some extent bury the symmetry of success and failure continuations, and that neither method eliminates all control stacks. Kahn and Carlsson <ref> [76] </ref> then employ partial evaluation to produce quite good Lisp code for naive reverse with modes. Research at Indiana University showed that Horn clauses could be translated into Scheme by fairly straightforward means [55, 77] and that some care was needed to extend Prolog with continuations [66]. <p> The former relies on using lazy streams to produce solutions, while the latter performs a procedure return on failure. We note that both methods to some extent bury the symmetry of success and failure continuations, and that neither method eliminates all control stacks. Kahn and Carlsson <ref> [76] </ref> then employ partial evaluation to produce quite good Lisp code for naive reverse with modes. Research at Indiana University showed that Horn clauses could be translated into Scheme by fairly straightforward means [55, 77] and that some care was needed to extend Prolog with continuations [66].
Reference: 77. <author> E. Kohlbecker, eu-Prolog, </author> <type> Technical Report 155, </type> <institution> Computer Science Department, Indiana University, </institution> <year> 1984. </year> <note> f17, 28g </note>
Reference-contexts: Kahn and Carlsson [76] then employ partial evaluation to produce quite good Lisp code for naive reverse with modes. Research at Indiana University showed that Horn clauses could be translated into Scheme by fairly straightforward means <ref> [55, 77] </ref> and that some care was needed to extend Prolog with continuations [66]. In particular, a generalized trail was required to suspend and resume a proof tree branch. Such operations are beyond the scope of this paper; all our failure continuations obey a stack-like discipline. <p> Kahn and Carlsson [76] then employ partial evaluation to produce quite good Lisp code for naive reverse with modes. Research at Indiana University showed that Horn clauses could be translated into Scheme by fairly straightforward means <ref> [55, 77] </ref> and that some care was needed to extend Prolog with continuations [66]. In particular, a generalized trail was required to suspend and resume a proof tree branch.
Reference: 78. <author> R.A. Kowalski, </author> <title> Predicate logic as a computer language, </title> <booktitle> in Information Processing 74, </booktitle> <pages> pp. 569-574, </pages> <publisher> North-Holland, </publisher> <year> 1974. </year> <month> f1g </month>
Reference-contexts: Robinson [114] showed that resolution could be used to efficiently prove theorems for such theories; Colmerauer [37] and Kowalski <ref> [78] </ref> subsequently noted that such clauses could be viewed as programs in addition to logical theories. For example, consider a predicate specifying list concatenation such that the third argument is the concatenation of the first two arguments.
Reference: 79. <author> D. Kranz, </author> <title> Orbit: An Optimizing Compiler for Scheme, </title> <type> Doctoral Thesis, </type> <institution> Yale University, </institution> <year> 1988. </year> <note> f25, 38g </note>
Reference-contexts: This is promising for advanced compilation, and has been successfully exploited in a number of compilers <ref> [136, 79, 5] </ref>. In this paper, we propose a compilation of first-order Prolog programs into continuation-passing Horn clauses. The resulting predicates are first order and have a completely deterministic control: all control decisions have been moved to the source level. <p> Tarau has shown that allocating WAM environments on the heap simplifies the execution machinery while being competitive in execution speed to a standard WAM implementation [140, 141]. Finally, some continuation-passing com 38 A Continuation-Passing Style for Prolog pilers reintroduce stack allocation by escape analysis <ref> [136, 79] </ref>. In summary, we do not find default heap-only allocation a fatal flaw. Future work. There are several issues to be explored. First, Prolog includes a great number of features, such as higher order predicates, dynamic predicates, input/output, coroutining and so on.
Reference: 80. <author> C.P. Kruskal, A. Weiss, </author> <title> Allocating Independent Subtasks on Parallel Processors, </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Vol. 11, No. 10, </volume> <year> 1985. </year> <month> f109g </month>
Reference-contexts: However, this is less a problem in a recursion-parallel Prolog system than in loop-parallel Fortran systems, since the granularity of a single recursion level typically is greater than the granularity of a single iteration of a parallel loop. More sophisticated dynamic algorithms has been proposed <ref> [80, 110, 92] </ref>. In these algorithms each processor is allocated a chunk of iterations at a time, instead of a single iteration. The chunk size may be fixed or variable. These algorithms have not yet been tested in Reform Prolog. Task switching.
Reference: 81. <author> J.R. Larus, </author> <title> P.N. Hilfinger, Detecting conflicts between structure accesses, </title> <booktitle> in Proceedings of the SIGPLAN'88 Conference on Programming Language Design and Implementation, </booktitle> <publisher> ACM Press, </publisher> <year> 1988 </year> <month> f23g </month>
Reference-contexts: The Reform compiler handles general doacross loops. Harrison performs a thorough job of restructuring the computation, which could be a useful future extension to our system. Larus and Hilfinger describe an advanced parallelizing compiler for Lisp, Curare [82], that performs alias analysis prior to restructuring <ref> [81] </ref>. By computing the program dependence graph of the program, they can extract parallelism from the program. The alias analysis then serves as a data dependence analysis. Reform Prolog also computes an alias analysis, but extracts parallelism less freely.
Reference: 82. <author> J.R. Larus, </author> <title> P.N. Hilfinger, Restructuring Lisp programs for concurrent execution, </title> <booktitle> in Proceedings of the ACM/SIGPLAN PPEALS 1988 Parallel Programming: Experience with Applications, Languages and Systems, </booktitle> <publisher> ACM Press, </publisher> <year> 1988 </year> <month> f23g </month>
Reference-contexts: The Reform compiler handles general doacross loops. Harrison performs a thorough job of restructuring the computation, which could be a useful future extension to our system. Larus and Hilfinger describe an advanced parallelizing compiler for Lisp, Curare <ref> [82] </ref>, that performs alias analysis prior to restructuring [81]. By computing the program dependence graph of the program, they can extract parallelism from the program. The alias analysis then serves as a data dependence analysis. Reform Prolog also computes an alias analysis, but extracts parallelism less freely.
Reference: 83. <author> S. Le Huitouze, </author> <title> A new datastructure for implementing extensions to Prolog, </title> <booktitle> Proc. Programming Language Implementation and Logic Programming 1990, </booktitle> <publisher> LNCS 456, Springer Verlag, </publisher> <year> 1990. </year> <month> f19g </month>
Reference-contexts: We think our approach leads to better locality of reference. However, we have not found any published measurements of the effi ciency of the Bekkers-Ridoux-Ungaro algorithm. * Variable shunting <ref> [83, 116] </ref> is used to avoid duplication of variables inside structures. This may introduce new variable chains, as shown in Paper D. We want to avoid this situation. Their algorithm does preserve the segment structure of the heap (but not the ordering within a segment).
Reference: 84. <author> H. Lieberman, C. Hewitt, </author> <title> A real-time garbage collector based on the lifetimes of objects, </title> <journal> Communications of the ACM, </journal> <volume> 26(6) </volume> <pages> 419-429, </pages> <month> June </month> <year> 1983. </year> <note> f10, 78g 142 </note>
Reference-contexts: That stack is then collected using a mark-sweep algorithm to retain the ordering between variables, while copying is used for the rest of the heap. Finally, the paper shows how to extend the copying algorithm to generational collection. Generational garbage collection <ref> [84, 4] </ref> relies on the observation that newly created objects tend to be short-lived. Thus, garbage collection should concentrate on recently created data. The heap is split into two or more generations, and the most recent generation is collected most frequently. <p> Naturally, if most of the live unbound variables have been compared in this way, the collector will have to spend more time in compacting the cv-stack. We believe this situation to be rare. 5.4 INTRODUCING GENERATIONAL GARBAGE COLLEC TION Generational garbage collection <ref> [84, 4] </ref> relies on the observation that newly created objects tend to be short-lived. Thus, garbage collection should concentrate on recently created data. The heap is split into two or more generations, and the most recent generation is collected most frequently.
Reference: 85. <author> T. Lindgren, </author> <title> The compilation and execution of recursion-parallel Pro-log on shared-memory multiprocessors, </title> <booktitle> Licentiate of Philosophy Thesis, Uppsala Theses in Computer Science 18/93, </booktitle> <month> November </month> <year> 1993. </year> <journal> f13, </journal> <volume> 92, 124, 150, 151, </volume> <month> 152g </month>
Reference-contexts: Bindings to variables shared between loop iterations may never be conditional [103]. 3. When a loop iteration is finished, it is always deterministic. 4. A (parallel) loop iteration can not start a parallel loop. Further details, e.g., on handling predicates with side-effects, can be found in Refs. <ref> [17, 85] </ref>. By making the first restriction, we ensure that parallel and sequential execution yield the same answer. <p> Finally, the analyzer also keeps track of whether loop iterations are deterministic or not, at every goal and at the end of an iteration. Code generation using this information is described in greater detail in the author's licentiate thesis <ref> [85] </ref>. Briefly, the compiler uses knowledge of locality, modes and determinism to optimize operations. <p> Due to space limitations, we can only describe these by means of a simple example and refer to other sources for a full discussion <ref> [20, 17, 85] </ref>. Consider the program: map ([],[]). The program is compiled into the following extended WAM code. map/2: switch_on_term Lv La L1 fail Lv: try La trust L1 6.7. <p> However, in contrast to Naish's binding determinism, nondeterminis tic bindings to local variables are allowed. In order to ensure these properties, the Reform Prolog compiler performs a global dataflow analysis and generates code that suspends processes and 124 Compiler Optimizations in Reform Prolog perform atomic updates only when necessary <ref> [85] </ref>. In particular, the compiler can generate precisely the code for a sequential Prolog machine when data are local. 8.3 COMPILER ANALYSES The compiler analyses in the Reform Prolog compiler are based on abstract interpretation [40]. <p> The compiler carries out four different analyses using the same basic algorithm. The abstract domains of these analyses are described below. Types The type domain is similar to that of Debray and Warren [46], extended to handle difference lists <ref> [85] </ref>. For our present concerns it suffices to note that the type analysis can discover ground and nonvariable terms. The analysis precision is similar to that of Aquarius Prolog [155]. Aliasing and linearity The analyzer derives possible and certain aliases by maintaining equivalence classes of possibly or certainly aliased variables.
Reference: 86. <author> T. Lindgren, </author> <title> A continuation-passing style for Prolog, </title> <booktitle> in Proc. International Logic Programming Symposium 1994, </booktitle> <publisher> MIT Press, </publisher> <year> 1994. </year> <note> f40, 41g </note>
Reference-contexts: and discusses future work. 3.2 PRELIMINARIES We assume that all predicates are in single-clausal form, i.e., each predicate consist of a single clause where the head is a linear sequence of variables, and the clause body consists of disjunction, conjunction, if-then-else, cut and calls to primitive operations and user-defined predicates <ref> [86] </ref>. We furthermore assume that predicates have been converted into first-order programs (call/1 is assumed not to be present, or transformed away) and 3.3. Making control explicit 41 do not use dynamic scheduling, calls to dynamic code or similar operations. <p> At each procedure call, we compute the live and defined variables (i.e., the set of variables that occur in the predicate both prior to and after the call) as in Ref <ref> [86] </ref>. The first oc currences of variables local to the clause body are explicitly marked in order to generate reentrant code later on.
Reference: 87. <author> T. Lindgren, </author> <title> Control flow analysis of Prolog (extended remix), </title> <type> Technical Report 112, </type> <institution> Uppsala University, </institution> <year> 1995. </year> <month> f42g </month>
Reference-contexts: where B has live and defined variables Xs is rewritten into: (fail cont (L [Xs]); A 0 ; label (f,L,Xs); B 0 ) Explicit state save/restore and frame creation operations have the benefit that the compiler can remove them when redundant as well perform limited code motion on frame creations <ref> [87] </ref>. 3.4. Control flow analysis 43 Consider the following annotated predicate.
Reference: 88. <author> T. Lindgren, </author> <title> Compiling for nested recursion-parallelism, </title> <booktitle> in JICSLP'96 post-conference workshop on implementation, </booktitle> <month> September </month> <year> 1996. </year> <month> f15g </month>
Reference-contexts: Examples of such systems are &-Prolog [68] and ACE [63]. Results have generally been encouraging. Recently, Bevemyr [18] and the author <ref> [88] </ref> have proposed methods to extend the `flat' Reform Prolog system into efficiently executing arbitrarily nested recursion-parallel loops. 1.3 RELATED WORK Paper A There has been considerable interest in transformations of Prolog into Scheme and partial continuation passing styles and the relation of Prolog to intermediate representations, such as Warren's abstract
Reference: 89. <author> J. Lloyd, </author> <booktitle> Foundations of Logic Programming (2nd ed.), </booktitle> <publisher> Springer Ver-lag, f2g </publisher>
Reference-contexts: The use of such a strategy separates Prolog from pure theorem proving. Many good books on the basics of Prolog and logic programming have been published, such as Sterling and Shapiro's textbook [137] on Prolog programming, or Lloyd's text on logic programming theory <ref> [89] </ref>. The novice reader is directed to those publications; we will in the rest of this thesis assume a working knowledge of the concepts and programming methods of Prolog and logic programming, such as resolution, unification, backtracking, cut, clause indexing, higher order predicates, and so on.
Reference: 90. <author> E. Lusk, D.H.D. Warren, S. Haridi, P. Brand, R. Butler, A. Calder-wood, M. Carlsson, A. Ciepielewski, T. Disz, B. Hausman, R. Olson, R. Overbeek, R. Stevens, P. </author> <title> Szeredi, The Aurora or-parallel system, </title> <journal> New Generation Computing, </journal> <volume> vol 7(2-3), </volume> <year> 1990. </year> <month> f3g </month>
Reference-contexts: If several goals are resolved simultaneously, the system extracts and-parallelism. When the resolved goals share free variables, the parallelism is classified as dependent and-parallelism. When goals do not share free variables, the parallelism is said to be independent and-parallelism. Or-parallel systems have been successfully built for some years <ref> [2, 90] </ref> and have been applied to search problems. In recent years, or-parallel constraint solving has emerged as a potential application area [31, 147]. For and-parallel systems, the main problem is the management of parallel backtracking.
Reference: 91. <author> A. Marien, B. Demoen, </author> <title> A new scheme for unification in WAM, </title> <booktitle> in International Logic Programming Symposium 1991, </booktitle> <publisher> MIT Press, </publisher> <year> 1991. </year> <month> f2g </month>
Reference-contexts: An example of this is when clauses fail early. In this case, backtracking can be optimized [27, 96]. A second example is that unifications can be compiled into very efficient code <ref> [150, 91, 95] </ref>. As a third example, clause indexing can be improved by sophisticated source-to-source transformation or code generation [42, 29]. The drawback of local techniques is that they cannot take advantage of the calling context. For example, it may be the case that a predicate is always 1.1.
Reference: 92. <author> E.P. Markatos & T.J. LeBlanc, </author> <title> Using Processor Affinity in Loop Scheduling on Shared-Memory Multiprocessors, </title> <type> Technical Report 410, </type> <institution> University of Rochester, </institution> <month> March </month> <year> 1992. </year> <month> f109g </month>
Reference-contexts: However, this is less a problem in a recursion-parallel Prolog system than in loop-parallel Fortran systems, since the granularity of a single recursion level typically is greater than the granularity of a single iteration of a parallel loop. More sophisticated dynamic algorithms has been proposed <ref> [80, 110, 92] </ref>. In these algorithms each processor is allocated a chunk of iterations at a time, instead of a single iteration. The chunk size may be fixed or variable. These algorithms have not yet been tested in Reform Prolog. Task switching.
Reference: 93. <author> J.M. </author> <title> Mellor-Crummey & M.L. Scott, Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors, </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol 9, Febr., </volume> <year> 1991. </year> <month> f106g </month>
Reference-contexts: The other workers binding must then be unified with this workers, to ensure consistency. A similar method is used in the implementation of Parallel NU- Prolog [103]. We have found that in our system this method is significantly faster than spin locking <ref> [93] </ref>. Creating shared structures When building a structure on the heap other workers should not have access to the structure until it has been fully initialized. In WAM a structure is built using either put instructions or get instructions.
Reference: 94. <author> M. Meier, </author> <title> Recursion vs. iteration in Prolog, </title> <booktitle> in Proc. Eighth Intl. Conf. on Logic Programming, </booktitle> <publisher> MIT Press, </publisher> <year> 1991. </year> <month> f52g </month>
Reference-contexts: Future work. Future work involves extending the analysis to handle exceptions and dynamic scheduling, improving the precision of the CFA, developing optimizations based on the ideas developed in this paper <ref> [94] </ref>, and using the CFG for native code compilation. We expect previously developed techniques [154, 142] to be useful in the latter regard. Acknowledgements. I would like to thank Per Mildner and H-akan Mill-roth for valuable discussions and their comments on this paper, and Saumya Debray for valuable comments.
Reference: 95. <author> M. Meier, </author> <title> Compilation of compound terms in Prolog, </title> <type> technical report ECRC-95-12, </type> <institution> ECRC, </institution> <month> July </month> <year> 1990. </year> <month> f2g </month>
Reference-contexts: An example of this is when clauses fail early. In this case, backtracking can be optimized [27, 96]. A second example is that unifications can be compiled into very efficient code <ref> [150, 91, 95] </ref>. As a third example, clause indexing can be improved by sophisticated source-to-source transformation or code generation [42, 29]. The drawback of local techniques is that they cannot take advantage of the calling context. For example, it may be the case that a predicate is always 1.1.
Reference: 96. <author> M. Meier, </author> <title> Shallow backtracking in Prolog programs, </title> <type> technical report ECRC-95-11, </type> <institution> ECRC, </institution> <month> February </month> <year> 1987. </year> <month> f2g </month>
Reference-contexts: The first of these is to improve on Warren's engine by local optimizations which target each clause or predicate in isolation [26]. An example of this is when clauses fail early. In this case, backtracking can be optimized <ref> [27, 96] </ref>. A second example is that unifications can be compiled into very efficient code [150, 91, 95]. As a third example, clause indexing can be improved by sophisticated source-to-source transformation or code generation [42, 29].
Reference: 97. <author> H. Millroth, </author> <title> Reforming the compilation of logic programs, </title> <type> Ph.D. Thesis, </type> <institution> Uppsala Theses in Computer Science 10, </institution> <year> 1991. </year> <month> f11g </month>
Reference-contexts: Bevemyr wrote the actual collector code, collected the benchmarking data and integrated the three collectors with Reform Prolog. Reform Prolog Papers E and F introduced Reform Prolog, a language built on Millroth's principle of recursion parallelism <ref> [97] </ref>. Paper G evaluates compiler optimizations that reduce the number of synchronizations and locking operations needed during parallel execution. <p> Paper G evaluates compiler optimizations that reduce the number of synchronizations and locking operations needed during parallel execution. Consider the list recursive predicate p. p ([]; : : :) p ([XjXs]; : : :) ^ p (Xs; : : :) ^ Millroth <ref> [97, 100] </ref> showed that such predicates could be compiled into effi cient loop code, using Reform compilation. 1. Compute the length of the input list as n and build the n instances of and . 2. For i ranging from 1 to n, execute i . 3.
Reference: 98. <author> H. Millroth, </author> <title> Reforming compilation of logic programs, </title> <booktitle> in Logic Programming: Proceedings of the Eighth International Conference, </booktitle> <publisher> MIT Press, </publisher> <year> 1991. </year> <note> f122g 143 </note>
Reference-contexts: Reform Prolog has been implemented on a varity of shared address space multiprocessors. The parallel implementation is designed as extension of a sequential Prolog machine [17]. The implementation imposes very little overhead for process management, such as scheduling and load balancing. We have previously described the compilation scheme <ref> [98, 100] </ref>, execution model [19, 20], and parallel abstract machine [19, 20] of Reform Prolog. In this paper we describe a set of compiler analyses and optimizations for increasing available parallelism and reducing parallelization overheads. We discuss the effectiveness of the optimizations and the runtime space/time efficiency of the compiler. <p> Each thread runs an instance of the same recursive program in an asynchronous parallel computation. This model is often called SPMD (Single Program Multiple Data). The programming model is realized by a compilation technique that translates a regular form of recursion to a parallelizable form of iteration <ref> [98, 100] </ref>. Example. The following program compares a sequence B with a list of sequences. Each comparison, carried out by match/3, computes a similarity value V that is stored in a sorted tree T for later access.

References-found: 98

