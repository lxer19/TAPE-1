URL: ftp://ftp.cs.brown.edu/pub/techreports/92/cs92-36.ps.Z
Refering-URL: http://www.cs.brown.edu/publications/techreports/reports/CS-92-36.html
Root-URL: http://www.cs.brown.edu/
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> J. L. Carter and M. N. Wegman. </author> <title> Universal Classes of Hash Functions, </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 18: </volume> <pages> 143-154, </pages> <month> April </month> <year> 1979. </year>
Reference: [2] <author> T. H. Cormen, C. E. Leiserson, and R. L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> McGraw-Hill, </publisher> <address> New York, NY, </address> <year> 1990. </year>
Reference: [3] <author> M. Dietzfelbinger, A. Karlin, K. Mehlhorn, F. Meyer auf der Heide, H. Rohnert, and R. E. Tarjan. </author> <title> Dynamic Perfect Hashing: Upper and Lower Bounds, </title> <booktitle> Proceedings of the 29th IEEE Symposium on Foundations of Computer Science, </booktitle> <publisher> IEEE Computer Society Press, </publisher> <pages> 524-531, </pages> <month> October </month> <year> 1988. </year>
Reference: [4] <editor> M. Dietzfelbinger and F. Meyer auf der Heide. </editor> <title> A New Universal Class of Hash Functions and Dynamic Hashing in Real Time, </title> <booktitle> Proceedings of the 17th International Colloquium on Automata, Languages, and Programming, Springer-Verlag, Lecture Notes in Computer Science, </booktitle> <volume> 443: </volume> <pages> 6-19, </pages> <month> July </month> <year> 1990. </year>
Reference: [5] <author> A. Greenberg and J. S. Vitter. </author> <title> Constant-Time Generation of Dynamic Random Variates, </title> <booktitle> Notes, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: Each generation requires one call to a random number generator that provides a uniform random number in the range [0; 1iN w i ). Recently, Rajeskaran and Ross [8] and Greenberg and Vitter <ref> [5] </ref> developed different algorithms for the dynamic case that do generation and update in constant expected time for various restricted classes of updates.
Reference: [6] <author> D. E. Knuth. </author> <title> Seminumerical Algorithms, </title> <booktitle> Volume 2: The Art of Computer Programming. </booktitle> <publisher> Addison Wesley, </publisher> <address> Reading, MA, </address> <year> 1981. </year>
Reference-contexts: 1 Introduction The generation of random variates based on arbitrary finite, discrete distributions has long been a key component of many computer simulations <ref> [6] </ref>, [10]. <p> In the static case, when the N weights are fixed, we can utilize the clever optimal algorithm by Walker, commonly called the alias method; the time to generate a random variate is constant and the preprocessing cost is O (N) <ref> [6] </ref>, [10]. In this paper we consider the problem in the important and more challenging dynamic case, in which the weights of the elements can vary dynamically. The relevant measures of efficiency are the generation time and the update time.
Reference: [7] <author> Y. Matias. </author> <title> Rolling a Dice with Varying Biases, </title> <type> Manuscript, </type> <month> July </month> <year> 1992. </year>
Reference-contexts: Recently, Rajeskaran and Ross [8] and Greenberg and Vitter [5] developed different algorithms for the dynamic case that do generation and update in constant expected time for various restricted classes of updates. Independently to our work, Matias <ref> [7] </ref> developed an algorithm that does generation in O (log fl N) expected time and with O (log fl N) expected calls to a uniform random number generator, although general update requires O (2 log fl N ) expected time. 1 In this paper, we introduce a very practical and more <p> The same result that we have described so far, namely, O (log fl N ) expected time and expected calls to a uniform random number generator per generation, but an expected update time of O (2 log fl N ), was obtained independently by Matias <ref> [7] </ref> using another interesting technique. Matias inserts elements into ranges (as we do), but "elevates" to the next level all the "significant" ranges, defined as those nonempty ranges whose range numbers are within 4 lg N of the largest range number. <p> However, it may be better to use degree bound d &gt; 2 because of its effect on lessening the height of the data structure. Experimentation is needed. The interesting algorithm developed independently by Matias <ref> [7] </ref> (see Section 4) is roughly comparable to our first algorithm: the maximum height of its data structure is about log fl N , each generation uses on the average O (log fl N) time and O (log fl N) calls to a uniform [0; 1) random number generator, and the
Reference: [8] <author> S. Rajasekaran and K. W. Ross. </author> <title> Fast Algorithms for Generating Discrete Random Variates with Changing Distributions, </title> <type> Manuscript, </type> <month> February </month> <year> 1992. </year>
Reference-contexts: Each generation requires one call to a random number generator that provides a uniform random number in the range [0; 1iN w i ). Recently, Rajeskaran and Ross <ref> [8] </ref> and Greenberg and Vitter [5] developed different algorithms for the dynamic case that do generation and update in constant expected time for various restricted classes of updates.
Reference: [9] <author> R. E. Tarjan. </author> <title> Amortized Computational Complexity, </title> <journal> SIAM Journal on Algebraic and Discrete Methods, </journal> <volume> 6(2): </volume> <pages> 306-318, </pages> <year> 1985. </year>
Reference-contexts: All the nodes on the two paths should revise their weights to reflect the changes. To facilitate the amortized analysis, we use an accounting method <ref> [9] </ref>, where we charge C ` units of cost to a level` node w that changes its parent.
Reference: [10] <author> A. J. Walker. </author> <title> New Fast Method for Generating Discrete Random Numbers with Arbitrary Distributions, </title> <journal> Electronic Letters, </journal> <volume> 10(8) </volume> <pages> 127-128, </pages> <year> 1974. </year>
Reference-contexts: 1 Introduction The generation of random variates based on arbitrary finite, discrete distributions has long been a key component of many computer simulations [6], <ref> [10] </ref>. <p> In the static case, when the N weights are fixed, we can utilize the clever optimal algorithm by Walker, commonly called the alias method; the time to generate a random variate is constant and the preprocessing cost is O (N) [6], <ref> [10] </ref>. In this paper we consider the problem in the important and more challenging dynamic case, in which the weights of the elements can vary dynamically. The relevant measures of efficiency are the generation time and the update time.
Reference: [11] <author> C. K. Wong and M. C. Easton. </author> <title> An Efficient Method for Weighted Sampling without Replacement, </title> <journal> SIAM Journal on Computing, </journal> <volume> 9(1) </volume> <pages> 111-114, </pages> <year> 1980. </year> <title> 18 A PREPROCESSING </title>
Reference-contexts: We can rerun Walker's algorithm each time a weight is updated, but the update cost O (N ) is too high. Up until recently, the best known algorithm for the dynamic problem was the binary tree-based scheme developed by Wong and Easton <ref> [11] </ref>, whose generation and update times are both O (log N). Each generation requires one call to a random number generator that provides a uniform random number in the range [0; 1iN w i ). <p> Theorem 1 The expected cost for generating a random variate according to the current weights is O (log fl N ), where N is the number of elements. The dynamic scheme of Wong and Easton <ref> [11] </ref> uses O (log N ) time per generation, but it requires only one call to a random number generator that outputs a uniform number in the range [0; P 1iN w i ).
References-found: 11

