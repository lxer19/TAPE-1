URL: http://www.cs.berkeley.edu/~culler/hoti97/ibel.ps
Refering-URL: http://www.cs.berkeley.edu/~culler/hoti97/
Root-URL: 
Abstract: DRAFT High-Performance Cluster Computing Using SCI fl Maximilian Ibel, Klaus E. Schauser, Chris J. Scheiman, and Manfred Weis Department of Computer Science University of California, Santa Barbara Santa Barbara, CA 93106 fibel,schauser,chriss,weisg@cs.ucsb.edu http://www.cs.ucsb.edu/research/sci Abstract The Scalable Coherent Interface (SCI) is a recent communication standard for cluster interconnects. We study the use of SCI in a high-performance parallel computing setting, using a cluster of UltraSparcs connected via Dolphin SCI SBus-2 adapters. We chose SCI as network fabric since it offers very low latencies and high bandwidth. In this paper, we study how to map a variety of programming models efficiently onto the SCI hardware. We focus on message passing and global address space support, implementing Active Messages and Split-C. We present implementation trade-offs and present performance measurements. We found that the user-level load/store programming interface of SCI is very convenient to use, achieves low latencies, and is fully virtualized, simultaneously supporting multiple parallel programs and communication channels. On the other hand, neither of the programming models studied maps directly to SCI. Issues such as notification, atomic operations, and virtual address space limitations represent major implementation challenges, which we address with a combination of compiler and run-time support. Overall, we found the SCI network to form a good substrate for high-performance cluster computing. 1 Introduction Networks of workstations (NOWs) have been tremendously successful in supporting high-performance parallel computing, offering a cost-effective alternative to custom MPP systems [ACP95]. The recent explosive growth of the Internet, as well as advances and interest in multime-fl This work was supported by the National Science Foundation NSF CAREER Award CCR-9502661, NSF Postdoctoral Award ASC-9504291, Sun Microsystems, and the California Micro Program. Computational resources were provided by the NSF Instrumentation Grant CDA-9529418 and NSF Infrastructure Grant CDA-9216202. dia, data mining and data warehousing have created additional communication-intensive parallel applications that require cluster networks with low latency and high bandwidth. Several such networks have recently emerged into the commodity market, making it feasible to create a high-end NOW by connecting standard workstations and PCs. In this paper, we focus on one of the new networking technologies, the Scalable Coherent Interface (SCI), and analyze its use for high-performance parallel cluster computing. We examine two important programming models: message passing and global memory. Our testbed consists of a cluster of 8 UltraSparcs connected by Dolphin's SCI SBus-2 adapters [Dol95b], as shown in Figure 1. The SCI adapter cards provide a simple user-level load/store programming interface for accessing remote memory. Processes can export memory segments which can then be imported by other processes on remote nodes. Accesses to the imported memory via plain load/store instructions are handled by the SCI adapter cards, which send messages to access the remote data. Although the SCI standard contains a coherency protocol to allow for cache-coherent shared memory, the Dolphin adapters reside on the I/O bus and therefore do not implement cache coherency. This user-level load/store interface is a result of more than a decade of development of network interfaces in parallel machines and workstation clusters. Traditional network interfaces rely on the operating system for protection among multiple users and for virtualizing the network. As a result, each communication event involves a system call which incurs a high overhead. However, the demand for low-latency, high-performance clusters has led to the development of more efficient approaches in the commodity market, as witnessed by the popularity of Tandem Server-Net [Hor95], Myrinet [BCF + 95], and the recently defined VIA interface [DR97]. A common approach to achieving low latency is to bypass the operating system for normal communication, but still ensure protection by relying on the virtual memory mechanism and using the operating system 
Abstract-found: 1
Intro-found: 1
Reference: [ACP95] <author> T. E. Anderson, D. E. Culler, and D. Patterson. </author> <title> A case for NOW (Networks of Workstations). </title> <journal> IEEE Micro, </journal> <volume> 15(1), </volume> <month> February </month> <year> 1995. </year>
Reference: [AHKL96] <author> G. Acher, H. Hellwagner, W. Karl, and M. </author> <month> Leberecht. </month>
Reference-contexts: Besides Dolphin Interconnects [Dol95a], Interconnect Systems Solution [Kib97] and Vitesse are also developing commodity SCI products to connect clusters of workstations. Several other groups even build their own SCI network cards or bridges, for example the SMILE group at the University of Munich <ref> [AHKL96] </ref>, or the RD24 project at CERN. SCI has been used in several parallel machines (e.g. HP Convex) and forms the basis for existing and future large scale enterprise servers (e.g. Sun Starfire [Mic96], Data General [CA96], and Sequent NUMA-Q [LCS96]).
References-found: 2

