URL: http://csg-www.lcs.mit.edu:8001/Users/vivek/ps/ChLS96.ps
Refering-URL: http://csg-www.lcs.mit.edu:8001/Users/vivek/sark_pub.html
Root-URL: 
Email: Email: chowjh@vnet.ibm.com, lenlyon@aol.com, vivek@lcs.mit.edu  
Title: Automatic Parallelization for Symmetric Shared-Memory Multiprocessors  
Author: Jyh-Herng Chow Leonard E. Lyon Vivek Sarkar 
Affiliation: Application Development Technology Institute IBM Software Solutions Division  
Abstract: In this paper, we present our solution to automatic SMP parallelization. Our solution is unique in its robust support for unbalanced processor loads and nesting of parallel loops and parallel sections, in conjunction with its tight integration with high-order transformations for improved uniprocessor performance, so that the speedup due to parallelism is truly a multiplicative speedup over highly optimized uniprocessor execution times. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> AIX Version 4.1 General Programming Concepts: </editor> <title> Writing and Debugging Programs, </title> <year> 1994. </year>
Reference-contexts: The solution described in this paper has been implemented for the following environments: * Machines: PowerPC-based symmetric multi processors (SMPs) * Operating System: AIX 4.1 with kernel threads implementation of POSIX threads <ref> [1] </ref> * Languages: FORTRAN 90 and High Perfor mance FORTRAN * Compilers: IBM XL FORTRAN and High Performance FORTRAN Compilers. We believe that this solution can be adapted for SMP parallelization of engineering and scientific applications written in C/C++ for Pow-erPC/SMPs running AIX and Windows/NT.
Reference: [2] <author> Jeanne Ferrante, Vivek Sarkar, and Wendy Thrash. </author> <title> On Estimating and Enhancing Cache Effectiveness. </title> <booktitle> Lecture Notes in Computer Science, </booktitle> (589):328-343, 1991. Proceedings of the Fourth International Workshop on Languages and Compilers for Parallel Computing, Santa Clara, California, USA, August 1991. Edited by U. Banerjee, D. Gelernter, A. Nicolau, D. Padua. 
Reference-contexts: SMP parallelization is that the Transformer functionality [11] in ASTI has a robust framework for uniform treatment of SMP parallelization in conjunction with high-order transformations for improved uniprocessor performance such as loop interchange, loop tiling, loop unrolling, and scalar replacement, as well as general techniques for estimating cache/TLB memory costs <ref> [2] </ref> and representing various target hardware characteristics in parameterized forms. ASTI operates on HIL, a high-level internal program representation, and associated data structures designed to represent the constructs of the FORTRAN and C programming languages in a form that is suitable for high-level program analysis and transformation. <p> After the locality group is identified, SMP parallelization is only attempted on loops outside the locality group, including outer sectioning loops that are obtained after tiling. ASTI currently has a sophisticated parameterized memory hierarchy model <ref> [2] </ref>, that includes hardware configuration parameters such as num INPUT PROGRAM: integer*4 sum (3,10000) do 10 j = 1, 20000 10 sum (2,i) = sum (2,i) + i + j AFTER TRANSFORMATION AND SMP PARALLELIZATION: pardo i = 1, 10000 screp = sum (2,i) do j = 1, 20000 screp =
Reference: [3] <author> Susan Flynn Hummel and Edith Schonberg. </author> <title> Low-Overhead Scheduling of Nested Parallelism. </title> <journal> IBM Journal of Research and Development, </journal> <year> 1991. </year> <month> 13 </month>
Reference-contexts: To provide efficient work queue management, the idea presented by Hummel and Schonberg <ref> [3] </ref> to avoid locks on the queue has been used. They observed that work items may be enqueued without a lock by using an atomic operation, such as compare-and-swap, for pushing a new item onto the head of a linked list.
Reference: [4] <author> Susan Flynn Hummel, Edith Schonberg, and Lawrence Flynn. </author> <title> Factoring: A Practical and Robust Method for Scheduling Parallel Loops. </title> <booktitle> Supercomputing 91, </booktitle> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: Also, the algorithm switches to Fixed chunking when the chunk size drops to the economic minimum size. Note that Fixed and N-Each both result in the Fixed method [5], and that 7 an N-Each request for one chunk per thread gives Simple or Block chunking. Hummel, Schonberg and Flynn <ref> [4] </ref> have proposed another method, which they refer to as Factoring; this provides a Batched (staged Adaptive) approach, and could be added to the prototype in the future. numbers shown are the sequences of chunk sizes under various chunking algorithms when there are four threads of execution, a thousand iterations in <p> When Fixed chunking is requested below the minimum economic size, that size is used rather than the requested size. N-Each is simply another (probably more convenient) way to specify Fixed chunking. (The GSS and Batched examples are from <ref> [4] </ref>). For each parallel DO loop, the compiler tells the library which chunking method to use.
Reference: [5] <author> Clyde Kruskal and Alan Weiss. </author> <title> Allocating Independent Subtasks on Parallel Processors. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-11(10), </volume> <month> October </month> <year> 1985. </year>
Reference-contexts: Also, the algorithm switches to Fixed chunking when the chunk size drops to the economic minimum size. Note that Fixed and N-Each both result in the Fixed method <ref> [5] </ref>, and that 7 an N-Each request for one chunk per thread gives Simple or Block chunking.
Reference: [6] <author> Jim Q. Ning, Andre Engberts, and W. Koza-czynski. </author> <title> Automated Support for Legacy Code Understanding. </title> <journal> Communications of ACM, </journal> <volume> 5(37) </volume> <pages> 50-57, </pages> <year> 1994. </year>
Reference-contexts: We have implemented a reverse technique, called out 4 lining, which transforms a selected program region into a subroutine. The outlining transformation is not new. For example, it has been used in restructuring old legacy code <ref> [6] </ref> to improve readability, reusability, and maintainability. Outlining has also been used in compilation for parallel execution. One such example is the FORTRAN parallelizing preprocessor developed by APR for the IBM Power/4 multiprocessor [8].
Reference: [7] <author> Constantine D. Polychronopoulos and David J. Kuck. </author> <title> Guided Self-Scheduling: </title>
Reference-contexts: Default The library decides what chunking method to use. This will either be a built-in default method or a default method specified by the user via runtime options (see "Parallell DO Chunking Options"). Adaptive chunking is similar to the traditional Guided Self Scheduling (GSS) method <ref> [7] </ref>, but in a number of cases the division is approximated by a shift. Also, the algorithm switches to Fixed chunking when the chunk size drops to the economic minimum size.
References-found: 7

