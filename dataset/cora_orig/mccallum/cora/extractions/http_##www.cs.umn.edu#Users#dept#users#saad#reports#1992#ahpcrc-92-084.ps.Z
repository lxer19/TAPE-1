URL: http://www.cs.umn.edu/Users/dept/users/saad/reports/1992/ahpcrc-92-084.ps.Z
Refering-URL: http://www.cs.umn.edu/Users/dept/users/saad/reports/1992/
Root-URL: http://www.cs.umn.edu
Title: BASIC SPARSE MATRIX COMPUTATIONS ON MASSIVELY PARALLEL COMPUTERS  
Author: W. Ferng K. Wu S. Petiton Y. Saad 
Date: February 1, 1993  
Abstract: This paper presents a preliminary experimental study of the performance of basic sparse matrix computations on the CM-200 and the CM-5. We concentrate on examining various ways of performing general sparse matrix-vector operations and the basic primitives on which these are based. We compare various data structures for storing sparse matrices and their corresponding matrix vector operations. Both SPMD and Data parallel modes are examined and a comparison of the two modes is made.
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Connection Machine Model CM-2 Technical Summary, Thinking Machine Corporation, Cambridge, Massachusetts. </institution>
Reference-contexts: The Connection Machine model CM-2 is an SIMD computer sometimes refered to as data parallel computing system. A Connection Machine system consists of a front-end computer, a large number of data processors, a few sequencers that control the data processors, an interprocessor communication network, and I/O controllers <ref> [1] </ref>. The system may be configured with up to 64K physical processors. Here, and throughout this paper, "K" stands for 1024. Parallel processing instructions issued by the front-end computer are received by one of the sequencers, which interprets them to produce a series of single-cycle nanoinstructions. <p> The floating-point accelerator consists of a floating-point interface chip and a floating-point execution chip, and is shared by every 32 data processors. Therefore, a fully configured CM-2 may contain 2K floating-point processors and 2 gigabytes of memory <ref> [1] </ref>. Communication Model. Each data processor of the CM-2 system has its own memory and the memory is bit-addressable. Each data processor can access its local memory at a rate of 5 Megabits per second [1]. <p> Therefore, a fully configured CM-2 may contain 2K floating-point processors and 2 gigabytes of memory <ref> [1] </ref>. Communication Model. Each data processor of the CM-2 system has its own memory and the memory is bit-addressable. Each data processor can access its local memory at a rate of 5 Megabits per second [1]. Thus, a fully configured CM-2 has 2 gigabytes of memory that can in theory be accessed at about 300 gigabits per second. The CM-2 system provides three forms of communication within the parallel processing unit: routing, NEWS, and scanning [1]. <p> local memory at a rate of 5 Megabits per second <ref> [1] </ref>. Thus, a fully configured CM-2 has 2 gigabytes of memory that can in theory be accessed at about 300 gigabits per second. The CM-2 system provides three forms of communication within the parallel processing unit: routing, NEWS, and scanning [1]. The router which allows any processor to communicate with any other processor is the most general communication mechanism. The NEWS grid is a faster but more structured communication mechanism. It allows processors to pass data in a multi-dimensional rectangular topology. <p> Thus, we have a (i; j) 6= 0 only when j = i + k fl nc with k = b i1 nc c; N nc c 1, 8 i 2 <ref> [1; N ] </ref>. Therefore, for the same nc, we have the same number of non zero elements, namely, nc, along each row or column. The distance between the physical processor holding a given diagonal and that holding the main diagonal can reach values close to N 2 for standard mappings. <p> C-diagonal matrices. A matrix with a C-diagonal pattern has nc 1 diagonals to the right of, and in addition to, the main diagonal. Thus, a (i; j) 6= 0 for only j = i + k with k = 0; : : : ; nc 1 , 8i 2 <ref> [1; N ] </ref> with j N .
Reference: [2] <institution> The Connection Machine CM-5 Technical Summary, Thinking Machine Corporation, Cambridge, Massachusetts. </institution>
Reference-contexts: In this section, we briefly summarize some of the most important aspects of this new massively parallel architecture from the Connection Machine CM-5 Technical Summary by Thinking Machine Corporation <ref> [2] </ref>. Processors. A CM-5 computer may consist of hundreds or thousands of processors. A system administrator may divide these processors into groups, known as partitions. There is a seperate processor, called Control Processor (CP), for each partition. A control processor is essentially like a standard high-performance workstation computer.
Reference: [3] <institution> CM-Fortran User's Guide. Thinking Machine Corporation, </institution> <address> Cambridge, Mas-sachusetts. </address>
Reference-contexts: Next, we describe and compare the different execution models available on each machine. Slicewise Execution Model on the CM-2 Beginning with version 1.0, the CM-FORTRAN compiler offers an alternative execution model called slicewise model, as opposed to the so-called Paris or fieldwise model <ref> [3] </ref>. The slicewise model takes full advantage of the registers and vector-processing capabilities of the 64-bit floating-point accelerator unit (FPU). All CM processors are organized into Processing Elements (PEs), each containing 32 bit-serial processors, some memory, one optional FPU chip, and other associated hardware. <p> In slicewise computations, double-precision floating-point arithmetic does not cost any more than single-precision arithmetic. However, loads and stores to/from memory of double-precision numbers will still cost twice as much as single-precision loads and stores <ref> [3] </ref>. Message-Passing Model on the CM-5. In the SPMD model on the CM-5, each PN holds an identical copy of the same program, called the node program, and executes its own copy concurrently. The host (control) processor can execute a seperate program independently.
Reference: [4] <author> E. Denning Dahl, </author> <title> Mapping and Compiled Communication on the Connection Machine System, </title> <booktitle> Proc. Fifth Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <year> 1990. </year>
Reference-contexts: Each CM chip has (log 2 p 4) data paths attached to it. And there are p=16 such chips. In the Sprint chip model, a CM-2 with p processors is regarded as a (log 2 p 5) dimensional hypercube and has p=32 such chips <ref> [4] </ref>. The CM-200 at AHPCRC. The Connection Machine Model CM-200 available to us for the experiments at the Army High Performance Computing Research Center (AH-PCRC) is an upgraded version of the CM-2 which has a clock rate that is higher than that of the CM-2. <p> An alternative way of doing the gathering operation is to use the communication compiler routines in the new CMSSL version 3.0 Beta. The communication Compiler, which was developed by Dahl <ref> [4] </ref>, is a software facility for scheduling completely general communications on the Connection Machines. It produces output data structures which are used by a message delivery system to perform synchronous processor to processor message passing.
Reference: [5] <author> E. Denning Dahl, </author> <type> Personal communication, </type> <year> 1992. </year>
Reference-contexts: Some of these 2 As of the time of this writing the vector units are planned to be installed by the end of August 1992. 48 optimisations are possible on the CM-5 <ref> [5] </ref>. Another possible improvement [5], may come from exploiting the fact that even though communication in the higher levels of the data network can be viewed as indeterministic, they can be deterministic in the first level. <p> Some of these 2 As of the time of this writing the vector units are planned to be installed by the end of August 1992. 48 optimisations are possible on the CM-5 <ref> [5] </ref>. Another possible improvement [5], may come from exploiting the fact that even though communication in the higher levels of the data network can be viewed as indeterministic, they can be deterministic in the first level. As a result optimization tools similar to the communication compiler may be developed for this limited but important level.
Reference: [6] <author> D. S. Dodson, R. G. Grimes, and J. G. Lewis. </author> <title> Sparse extensions to the Fortran basic linear algebra subprograms. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 17(2) </volume> <pages> 253-263, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: The indirect-addressing versions, are listed below. Note that there is a standard sparse BLAS defined by Grimes and Lewis <ref> [6] </ref> and that DOTI and sparse blas kernels DOTI and AXPYI given below follow the description of the standard. 1. DOTI (a = P a = 0 a = a + x (i) * y (ind (i)) enddo 2.
Reference: [7] <author> K. Gallivan, W. Jalby, A. Malony, and H. Wijshoff. </author> <title> Performance prediction for parallel numerical algorithms. </title> <journal> International Journal of High Speed Computing, </journal> <volume> 3 </volume> <pages> 31-62, </pages> <year> 1991. </year>
Reference-contexts: It is a rule of thumb that for sparse computations performance can often virtually be stated in terms of communication speeds alone <ref> [7] </ref>. With communication latencies generally high and difficult to reduce to acceptable levels, this problem is a challange to algorithm designers and manufacturers alike. We would like to discuss the possible improvements that can be made to bring the communication times down to much lower levels.
Reference: [8] <author> S. Hammond and R. Schreiber, </author> <title> Mapping Unstructured Grid problems to the Connection Machine, </title> <type> Tech. Report 90.22, RIACS, 90. </type>
Reference: [9] <author> M. Misra and P. Kumar, </author> <title> Efficient VLSI implementation of iterative solutions to sparse linear systems, </title> <type> Tech. Report 246, </type> <institution> Institute for Robotics and Intelligent Systems, University of Southerm California, </institution> <year> 1988. </year>
Reference: [10] <author> S. Petiton, </author> <title> Massively Parallel Sparse Matrix Computation for Iterative Methods, </title> <type> Tech. Report YALE/DCS/878, </type> <institution> Department of Computer Science, Yale University, </institution> <year> 1991. </year>
Reference-contexts: This is clearly a more general scheme than the two prevous ones. However, it may be inefficient if many rows have fewer than ncol elements. A column-based scheme can also be defined similarly. Sparse General Pattern format (SGP) This is a variation of the Ellpack-Itpack format introduced in <ref> [10] </ref> for distributed memory massively parallel machines using a data parallel programming model. <p> Note that the first version uses a get (gather) operation and the second uses a one-to-one send operation for the general communications generated. In this section we will use two types of matrices to test the performance on the CM-5. The terminology used here is borrowed from <ref> [11, 10] </ref>. 27 Multiplication Gather SUM Total nx fi ny fi nz ms % ms % ms % ms 32 fi 32 fi 32 0.234 3.33% 6.140 87.63% 0.014 0.20% 7.007 Table 8: Timing (ms) Results for Finite Difference Data in ELL format using Sparse util gather routine on CM-200 with <p> With classical 2D geometries as described above, we can expect communication between processors that are farther apart for C-distributed matrices, with ki jk larger than N 2 , than for C-diagonal matrices, with ki jk nc; 8 (i; j). These results were already observed in the previous work <ref> [11, 10] </ref> for the CM-2.
Reference: [11] <author> S. Petiton and C. Weill-Duflos, </author> <title> Very sparse preconditioned conjugate gradient on mas-sively parallel architectures, </title> <booktitle> in Proceeding of 13th World Congress on Computation and Applied Mathematics, </booktitle> <year> 1991. </year>
Reference-contexts: Note that the first version uses a get (gather) operation and the second uses a one-to-one send operation for the general communications generated. In this section we will use two types of matrices to test the performance on the CM-5. The terminology used here is borrowed from <ref> [11, 10] </ref>. 27 Multiplication Gather SUM Total nx fi ny fi nz ms % ms % ms % ms 32 fi 32 fi 32 0.234 3.33% 6.140 87.63% 0.014 0.20% 7.007 Table 8: Timing (ms) Results for Finite Difference Data in ELL format using Sparse util gather routine on CM-200 with <p> With classical 2D geometries as described above, we can expect communication between processors that are farther apart for C-distributed matrices, with ki jk larger than N 2 , than for C-diagonal matrices, with ki jk nc; 8 (i; j). These results were already observed in the previous work <ref> [11, 10] </ref> for the CM-2.
Reference: [12] <author> D.A. Reed and R.M. Fujimoto, </author> <title> Multicomputer Networks, Message-Based Parallel Processing section 7.2, </title> <publisher> The MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: When the start-up time is very high one-way dissection is the most economical method. However, when the start-up time is small, then minimizing the total number of bytes passed would also roughly minimize communication time <ref> [12] </ref>. In the 2-D case, dividing the domain into hexagonal subdomains would normally be best in that it provides the tessalation which requires the least amount of data to be passed [12]. Since this optimal subdivision is not easy to implement or generalize, we simply use squares. <p> the start-up time is small, then minimizing the total number of bytes passed would also roughly minimize communication time <ref> [12] </ref>. In the 2-D case, dividing the domain into hexagonal subdomains would normally be best in that it provides the tessalation which requires the least amount of data to be passed [12]. Since this optimal subdivision is not easy to implement or generalize, we simply use squares. We try to select two parameters q and r such that p = q fl r and q : r = m : n for a m fi n grid.
Reference: [13] <author> Y. Saad, SPARSKIT: </author> <title> A basic tool kit for sparse matrix computations. </title> <type> Technical Report 90-20, </type> <institution> Research Institute for Advanced Computer Science, NASA Ames Research Center, Moffet Field, </institution> <address> CA, </address> <year> 1990. </year>
Reference-contexts: However, we have elected to restrict our attention to just a few of them that have the best potential for delivering good performance on massively parallel computers. We consider six basic different storage formats and some variants of them. Using some of the terminology of SPARSKIT <ref> [13] </ref> these are the banded format (BND), the Compressed Sparse Row/Column (CSR/CSC) formats, the diagonal format (DIA), the Ellpack-Itpack generalized diagonal format (ELL), the Sparse General Pattern format (SGP), and the Coordinate format (COO) used by the CMSSL library.
Reference: [14] <author> Y. Saad and H. Wijshoff. </author> <title> A benchmark package for sparse matrix computations. </title> <editor> In J. Lenfant and D. De groot, editors, </editor> <booktitle> Proceedings of ICS conference 1988, </booktitle> <address> St Malo, France, </address> <pages> pages 500-509. </pages> <publisher> ACM, </publisher> <year> 1988. </year>
Reference-contexts: Thus, the early CRAY machines which did not provide hardware instructions for scatter and gather operations, were quickly found lacking by users of sparse matrix techniques and the missing instructions were added within a short time by the manufacturer. In <ref> [14] </ref> a benchmark code specifically designed for testing the performance of computers on sparse matrix computations, was proposed and used to compare a number of shared memory computers.
Reference: [15] <author> Joel Saltz, Serge Petiton, Harry Berrryman, and Adam Rifkin, </author> <title> Performance Effects of Irregular Communication Patterns on Massively Parallel Multiprocessors, </title> <journal> 1 Journal of Parallel and Distributed Computing, </journal> <volume> 13 (1991), </volume> <pages> pp. 202-212. </pages>
Reference-contexts: Similar observation has been made in <ref> [15] </ref>. 4.6 Analysis of the data parallel ELL/SGP algorithms In this section we analyze the performance on the CM-5 of the algorithms that utilize the Ellpack and the variant SGP in a data parallel programming model. We evaluate the performance of each part of these two versions.
Reference: [16] <author> K. Wu and Y. Saad, </author> <title> Experiments with the CM-5 message passing primitives, </title> <type> AH-PCRC internal report, </type> <year> 1992. </year> <month> 50 </month>
Reference-contexts: Table 22 show the speeds of the subroutine using this communication scheme. We know that the communication rate for this operation is about 0.87 MegaBytes/sec <ref> [16] </ref>. Assuming that arithmetic operations take basically no time, the execution time for the procedure in double-precision (8-Byte words), would be at least 8n=0:87sec, where n is the total number of rows.
References-found: 16

