URL: http://www.cs.ucsb.edu/~martin/techreport/TRCS96-08.ps
Refering-URL: http://www.cs.ucsb.edu/~martin/techreport/index.html
Root-URL: http://www.cs.ucsb.edu
Email: (martin@cs.ucsb.edu)  (pedro@cs.ucsb.edu)  
Title: Commutativity Analysis: A New Analysis Framework for Parallelizing Compilers  
Author: Martin C. Rinard Pedro C. Diniz 
Address: Santa Barbara, CA 93106  
Affiliation: Department of Computer Science University of California, Santa Barbara  
Abstract: This paper presents a new analysis technique, commutativity analysis, for automatically parallelizing computations that manipulate dynamic, pointer-based data structures. Com-mutativity analysis views the computation as composed of operations on objects. It then analyzes the program at this granularity to discover when operations commute (i.e. generate the same final result regardless of the order in which they execute). If all of the operations required to perform a given computation commute, the compiler can automatically generate parallel code. We have implemented a prototype compilation system that uses commutativity analysis as its primary analysis framework. We have used this system to automatically parallelize two complete scientific computations: the Barnes-Hut N-body solver and the Water code. This paper presents performance results for the generated parallel code running on the Stanford DASH machine. These results provide encouraging evidence that commutativity analysis can serve as the basis for a successful parallelizing compiler. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> U. Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, MA, </address> <year> 1988. </year>
Reference-contexts: In the right context this approach works well researchers have successfully used data dependence analysis to parallelize computations that manipulate dense arrays using affine access functions <ref> [1, 32, 13, 17] </ref>. But data dependence analysis is, by itself, inadequate for computations that manipulate dynamic, pointer-based data structures. Its limitations include a need to perform complicated analysis to extract global properties of the data structure topology and an fl Supported in part by an Alfred P.
Reference: [2] <author> U. Banerjee, R. Eigenmann, A. Nicolau, and D. Padua. </author> <title> Automatic program parallelization. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 81(2) </volume> <pages> 211-243, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Sloan Research Fellowship. y Sponsored by the PRAXIS XXI program administrated by Portugal's JNICT Junta Nacional de Investigac ao Cient ifica e Tecnol ogica, and holds a Fulbright travel grant. inherent inability to parallelize computations that manipulate graphs <ref> [2] </ref>. We believe the key to automatically parallelizing dynamic, pointer-based computations is to recognize and exploit commuting operations, or operations that generate the same final result regardless of the order in which they execute.
Reference: [3] <author> J. Barnes and P. Hut. </author> <title> A hierarchical O(NlogN) force-calculation algorithm. </title> <booktitle> Nature, </booktitle> <pages> pages 446-449, </pages> <month> December </month> <year> 1976. </year>
Reference-contexts: We have implemented a run-time system that provides this functionality. It currently runs on the Stanford DASH multiprocessor [27] and on multiprocessors from Silicon Graphics. We have used the compilation system to automatically par 1 allelize two complete scientific applications: the Barnes-Hut N-body solver <ref> [3] </ref> and the Water code [44]. The Barnes-Hut is representative of our target class of dynamic computations: it performs well because it uses a pointer-based data structure (a space subdivision tree) to organize the computation. <p> It uses an enhanced version of the code generation algorithms in Section 5. We have used this compiler to automatically parallelize two applications: the Barnes-Hut hierarchical N-body solver <ref> [3] </ref> and the Water [39] code. 8 Explicitly parallel versions of the applications are available in the SPLASH [39] and SPLASH-2 [44] benchmark suites.
Reference: [4] <author> A. J. Bernstein. </author> <title> Analysis of programs for parallel processing. </title> <journal> IEEE Transactions on Electronic Computers, </journal> <volume> 15(5) </volume> <pages> 757-763, </pages> <month> October </month> <year> 1966. </year>
Reference-contexts: data structure, it would have to analyze the code that built the data structure. 8 Related Work One of the very first papers in the field of parallelizing compilers identifies the concept of commuting computations as distinct from and more general than the concept of computations that can execute concurrently <ref> [4] </ref>. The commutativity conditions, however, were formulated in terms of the memory locations that computations read and write and were therefore very restrictive.
Reference: [5] <author> M. Berry, D. Chen, P. Koss, D. Kuck, S. Lo, Y. Pang, L. Pointer, R. Roloff, A. Sameh, E. Clementi, S. Chin, D. Schneider, G. Fox, P. Messina, D. Walker, C. Hsiung, J. Schwarzmeier, K. Lue, S. Orszag, F. Seidl, O. Johnson, R. Goodrum, and J. Martin. </author> <title> The Perfect Club benchmarks: Effective performance evaluation of supercomputers. </title> <type> ICASE Report 827, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, Urbana, IL, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: We believe that this approach accurately reflects how parallelizing compilers in general will be used in practice. We expect that programmers may have to tune their programs to the capabilities of the compiler to get good performance. The experience of other researchers supports this hypothesis <ref> [5, 6] </ref>. For our two applications it was relatively straightforward to produce code that the compiler could successfully analyze. Almost all of the translation effort was devoted to expressing the computation in a clean object-based style with classes, objects and methods instead of structures and procedures.
Reference: [6] <author> W. Blume and R. Eigenmann. </author> <title> Performance analysis of paral-lelizing compilers on the Perfect Benchmarks programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(6) </volume> <pages> 643-656, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: We believe that this approach accurately reflects how parallelizing compilers in general will be used in practice. We expect that programmers may have to tune their programs to the capabilities of the compiler to get good performance. The experience of other researchers supports this hypothesis <ref> [5, 6] </ref>. For our two applications it was relatively straightforward to produce code that the compiler could successfully analyze. Almost all of the translation effort was devoted to expressing the computation in a clean object-based style with classes, objects and methods instead of structures and procedures.
Reference: [7] <author> W. Blume and R. Eigenmann. </author> <title> Symbolic range propagation. </title> <booktitle> In Proceedings of the 9th International Parallel Processing Symposium, </booktitle> <pages> pages 357-363, </pages> <address> Santa Barbara, CA, </address> <month> April </month> <year> 1995. </year>
Reference-contexts: We have also developed rules for conditional and array expressions [35]. In the worst case the expression manipulation algorithms may take exponential running time. Like other researchers applying similar expression manipulation techniques in other analysis contexts <ref> [7] </ref>, we have not observed this behavior in practice. Finally, it is undecidable in general to determine if two expressions always denote the same value [20].
Reference: [8] <author> F. Bodin, P. Beckman, D. Gannon, J. Gotwals, S. Narayana, S. Srinivas, and Beata Winnicka. Sage++: </author> <title> An object-oriented toolkit and class library for building Fortran and C++ structuring tools. </title> <booktitle> In Proceedings of the Object-Oriented Numerics Conference, </booktitle> <year> 1984. </year>
Reference-contexts: We use Sage++ <ref> [8] </ref> as a front end. The analysis phase consists of approximately 14,000 lines of C++ code, with approximately 1,800 devoted to interfacing with Sage++. The generated code contains calls to a run-time library that provides the basic concurrency management and synchronization functionality.
Reference: [9] <author> D. Callahan. </author> <title> Recognizing and parallelizing bounded recurrences. </title> <booktitle> In Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Santa Clara, CA, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: In the long run we believe parallelizing compilers will incorporate both commutativity analysis and data dependence analysis for pointer-based data structures, using each when it is appropriate. 8.2 Reductions Several existing compilers can recognize when a loop performs a reduction of many values into a single value <ref> [16, 15, 29, 9] </ref>. These compilers recognize when the reduction primitive (typically addition) is associative. They then exploit this algebraic property to eliminate the data dependence associated with the serial accumulation of values into the result. The generated program computes the reduction in parallel.
Reference: [10] <author> M. Carlisle and A. Rogers. </author> <title> Software caching and computation migration in Olden. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Given the dynamic nature of our target application set, the compiler would have to rely on dynamic techniques such as replication and task migration to optimize the locality of the generated computation <ref> [10, 34] </ref>. 7.4 Pointer Analysis The algorithms in Section 4 perform a data usage analysis at the granularity of the type system. An obvious alternative is to use pointer analysis [14, 43, 24] to identify the regions of memory that each operation may access.
Reference: [11] <author> R. Chandra, A. Gupta, and J. Hennessy. </author> <title> Data locality and load balancing in COOL. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year> <month> 22 </month>
Reference-contexts: One of these relations is that the parts of the program commute. The goal is to help the implementation exploit concurrency while ensuring that the parallelization does not change the semantics of the program. Many concurrent object-oriented languages support the notion of mutually exclusive operations on objects <ref> [11, 45] </ref>. Although the concept of commuting operations is never explicitly identified, the expectation is that all mutually exclusive operations that may attempt to concurrently access the same object commute.
Reference: [12] <author> D. Chase, M. Wegman, and F. Zadek. </author> <title> Analysis of pointers and structures. </title> <booktitle> In Proceedings of the SIGPLAN '90 Conference on Program Language Design and Implementation, </booktitle> <address> White Plains, NY, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: We also discuss reduction analysis and commuting operations in the context of parallel programming languages. 8.1 Data Dependence Analysis Research on automatically parallelizing serial computations that manipulate pointer-based data structures has focused on techniques that precisely represent the run-time topology of the heap <ref> [18, 25, 12, 30] </ref>. The idea is that the analysis can use this precise representation to discover independent pieces of code. To recognize independent pieces of code, the compiler must understand the global topology of the manipulated data structures [18, 25].
Reference: [13] <author> R. Eigenmann, J. Hoeflinger, Z. Li, and D. Padua. </author> <title> Experience in the automatic parallelization of four Perfect benchmark programs. </title> <editor> In U. Banerjee, D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, Fourth International Workshop, </booktitle> <address> Santa Clara, CA, </address> <month> August </month> <year> 1991. </year> <note> Springer-Verlag. </note>
Reference-contexts: In the right context this approach works well researchers have successfully used data dependence analysis to parallelize computations that manipulate dense arrays using affine access functions <ref> [1, 32, 13, 17] </ref>. But data dependence analysis is, by itself, inadequate for computations that manipulate dynamic, pointer-based data structures. Its limitations include a need to perform complicated analysis to extract global properties of the data structure topology and an fl Supported in part by an Alfred P.
Reference: [14] <author> M. Emami, R. Ghiya, and L. Hendren. </author> <title> Context-sensitive in-terprocedural points-to analysis in the presence of function pointers. </title> <booktitle> In Proceedings of the SIGPLAN '94 Conference on Program Language Design and Implementation, </booktitle> <address> Orlando, FL, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: An obvious alternative is to use pointer analysis <ref> [14, 43, 24] </ref> to identify the regions of memory that each operation may access. A major advantage of this approach for non type-safe languages like C and C++ is that it would allow the compiler to analyze programs that may violate their type declarations.
Reference: [15] <author> A. Fisher and A. Ghuloum. </author> <title> Parallelizing complex scans and reductions. </title> <booktitle> In Proceedings of the SIGPLAN '94 Conference on Program Language Design and Implementation, </booktitle> <address> Orlando, FL, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: In the long run we believe parallelizing compilers will incorporate both commutativity analysis and data dependence analysis for pointer-based data structures, using each when it is appropriate. 8.2 Reductions Several existing compilers can recognize when a loop performs a reduction of many values into a single value <ref> [16, 15, 29, 9] </ref>. These compilers recognize when the reduction primitive (typically addition) is associative. They then exploit this algebraic property to eliminate the data dependence associated with the serial accumulation of values into the result. The generated program computes the reduction in parallel.
Reference: [16] <author> A. Ghuloum and A. Fisher. </author> <title> Flattening and parallelizing irregular, recurrent loop nests. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: In the long run we believe parallelizing compilers will incorporate both commutativity analysis and data dependence analysis for pointer-based data structures, using each when it is appropriate. 8.2 Reductions Several existing compilers can recognize when a loop performs a reduction of many values into a single value <ref> [16, 15, 29, 9] </ref>. These compilers recognize when the reduction primitive (typically addition) is associative. They then exploit this algebraic property to eliminate the data dependence associated with the serial accumulation of values into the result. The generated program computes the reduction in parallel.
Reference: [17] <author> M.W. Hall, S.P. Amarasinghe, B.R. Murphy, S. Liao, and M.S. Lam. </author> <title> Detecting coarse-grain parallelism using an interproce-dural parallelizing compiler. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: In the right context this approach works well researchers have successfully used data dependence analysis to parallelize computations that manipulate dense arrays using affine access functions <ref> [1, 32, 13, 17] </ref>. But data dependence analysis is, by itself, inadequate for computations that manipulate dynamic, pointer-based data structures. Its limitations include a need to perform complicated analysis to extract global properties of the data structure topology and an fl Supported in part by an Alfred P. <p> Number Processors of Bodies Serial 1 2 4 8 16 32 8192 65.0 63.4 31.9 15.8 8.8 5.3 3.6 Table 3: Execution Times for Barnes-Hut (seconds) We start our analysis of the performance with the parallelism coverage <ref> [17] </ref>, which measures the amount of time that the serial computation spends in parallelized sections. Barnes-Hut (8192 bodies) Barnes-Hut (16384 bodies) To obtain good parallel performance, the compiler must par-allelize a substantial part of the computation. <p> The generated program computes the reduction in parallel. Researchers have recently generalized the basic reduction recognition algorithms to recognize reductions of arrays instead of scalars. The reported results indicate that this optimization is crucial for obtaining good performance for the measured set of applications <ref> [17] </ref>. There are interesting connections between reduction analysis and commutativity analysis. Many (but not all) of the computations that commutativity analysis is designed to handle can be viewed as performing multiple reductions concurrently across a large data structure.
Reference: [18] <author> L. Hendren, J. Hummel, and A. Nicolau. </author> <title> Abstractions for recursive pointer data structures: Improving the analysis and transformation of imperative programs. </title> <booktitle> In Proceedings of the SIGPLAN '92 Conference on Program Language Design and Implementation, </booktitle> <address> San Francisco, CA, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: We also discuss reduction analysis and commuting operations in the context of parallel programming languages. 8.1 Data Dependence Analysis Research on automatically parallelizing serial computations that manipulate pointer-based data structures has focused on techniques that precisely represent the run-time topology of the heap <ref> [18, 25, 12, 30] </ref>. The idea is that the analysis can use this precise representation to discover independent pieces of code. To recognize independent pieces of code, the compiler must understand the global topology of the manipulated data structures [18, 25]. <p> The idea is that the analysis can use this precise representation to discover independent pieces of code. To recognize independent pieces of code, the compiler must understand the global topology of the manipulated data structures <ref> [18, 25] </ref>. It must therefore analyze the code that builds the data structures and propagate the results of this analysis through the program to the section that uses the data. A limitation of these techniques is an inherent inability to parallelize computations that manipulate graphs.
Reference: [19] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: The key question is how well the generated code would perform on such a platform. Message-passing machines have traditionally suffered from much higher communication costs than shared-memory machines. Compilation research for message-passing machines has therefore emphasized the development of data and computation placement algorithms that minimize communication <ref> [19] </ref>.
Reference: [20] <author> O. Ibarra, P. Diniz, and M. Rinard. </author> <title> On the complexity of commutativity analysis. </title> <booktitle> In Proceedings of the 2nd Annual International Computing and Combinatorics Conference, </booktitle> <address> Hong Kong, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: Like other researchers applying similar expression manipulation techniques in other analysis contexts [7], we have not observed this behavior in practice. Finally, it is undecidable in general to determine if two expressions always denote the same value <ref> [20] </ref>.
Reference: [21] <author> R. Kemmerer and S. Eckmann. UNISEX: </author> <title> a UNIx-based Symbolic EXecutor for pascal. </title> <journal> SoftwarePractice and Experience, </journal> <volume> 15(5) </volume> <pages> 439-458, </pages> <month> May </month> <year> 1985. </year>
Reference-contexts: Execution Order New Value of sum r-&gt;visit (p1);r-&gt;visit (p2) (sum+p1)+p2 r-&gt;visit (p2);r-&gt;visit (p1) (sum+p2)+p1 Table 1: New Values of sum Under Different Execution Orders 3.4 Symbolic Execution The compiler uses symbolic execution <ref> [21] </ref> to extract the expressions that denote the new values of instance variables and the multiset of invoked operations. Symbolic execution simply executes the methods, computing with expressions instead of values.
Reference: [22] <author> M. Lam and M. Rinard. </author> <title> Coarse-grain parallel programming in Jade. </title> <booktitle> In Proceedings of the Third ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 94-105, </pages> <address> Williamsburg, VA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: In this context it becomes clear that the compiler must reason about how operations are invoked as well as how they access memory. The implicitly parallel programming language Jade explicitly supports the concept of commuting operations on user-defined objects <ref> [22] </ref>. In this case the motivation is to extend the range of expressible computations while preserving deterministic execution. It is the programmer's responsibility to ensure that operations that are declared to commute do in fact commute.
Reference: [23] <author> Butler W. Lampson and David D. Redell. </author> <title> Experience with processes and monitors in Mesa. </title> <journal> Communications of the ACM, </journal> <volume> 23(2) </volume> <pages> 105-117, </pages> <month> February </month> <year> 1980. </year>
Reference-contexts: Even though traditional compilers have not exploited commuting operations, these operations play an important role in other areas of parallel computing. Explicitly parallel programs, for example, often use locks, monitors and critical regions to ensure that operations execute atomically <ref> [23] </ref>. For the program to execute correctly, the programmer must ensure that all of the atomic operations commute.
Reference: [24] <author> W. Landi and B. Ryder. </author> <title> A safe approximation algorithm for in-terprocedural pointer aliasing. </title> <booktitle> In Proceedings of the SIGPLAN '92 Conference on Program Language Design and Implementation, </booktitle> <address> San Francisco, CA, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: An obvious alternative is to use pointer analysis <ref> [14, 43, 24] </ref> to identify the regions of memory that each operation may access. A major advantage of this approach for non type-safe languages like C and C++ is that it would allow the compiler to analyze programs that may violate their type declarations.
Reference: [25] <author> J. Larus and P. Hilfinger. </author> <title> Detecting conflicts between structure accesses. </title> <booktitle> In Proceedings of the SIGPLAN '88 Conference on Program Language Design and Implementation, </booktitle> <address> Atlanta, GA, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: We also discuss reduction analysis and commuting operations in the context of parallel programming languages. 8.1 Data Dependence Analysis Research on automatically parallelizing serial computations that manipulate pointer-based data structures has focused on techniques that precisely represent the run-time topology of the heap <ref> [18, 25, 12, 30] </ref>. The idea is that the analysis can use this precise representation to discover independent pieces of code. To recognize independent pieces of code, the compiler must understand the global topology of the manipulated data structures [18, 25]. <p> The idea is that the analysis can use this precise representation to discover independent pieces of code. To recognize independent pieces of code, the compiler must understand the global topology of the manipulated data structures <ref> [18, 25] </ref>. It must therefore analyze the code that builds the data structures and propagate the results of this analysis through the program to the section that uses the data. A limitation of these techniques is an inherent inability to parallelize computations that manipulate graphs.
Reference: [26] <author> C. Lengauer and E. Hehner. </author> <title> A methodology for programming with concurrency: An informal presentation. </title> <booktitle> Science of Computer Programming, </booktitle> <volume> 2(1) </volume> <pages> 1-18, </pages> <month> October </month> <year> 1982. </year>
Reference-contexts: In this case the motivation is to extend the range of expressible computations while preserving deterministic execution. It is the programmer's responsibility to ensure that operations that are declared to commute do in fact commute. Lengauer and Hehner <ref> [26] </ref> propose a programming methodology in which the programmer exposes concurrency by declaring semantic relations between different parts of the program. One of these relations is that the parts of the program commute.
Reference: [27] <author> D. Lenoski. </author> <title> The Design and Analysis of DASH: A Scalable Directory-Based Multiprocessor. </title> <type> PhD thesis, </type> <institution> Stanford, </institution> <address> CA, </address> <month> February </month> <year> 1992. </year>
Reference-contexts: The dynamic nature of our target application set means that the compiler must rely on a run-time system to provide basic task management functionality such as synchronization and dynamic load balancing. We have implemented a run-time system that provides this functionality. It currently runs on the Stanford DASH multiprocessor <ref> [27] </ref> and on multiprocessors from Silicon Graphics. We have used the compilation system to automatically par 1 allelize two complete scientific applications: the Barnes-Hut N-body solver [3] and the Water code [44]. <p> This section presents performance results for both the automatically parallelized and explicitly parallel versions on a 32 processor Stanford DASH machine <ref> [27] </ref> running a modified version of the IRIX 5.2 operating system.
Reference: [28] <author> E. Mohr, D. Kranz, and R. Halstead. </author> <title> Lazy task creation: a technique for increasing the granularity of parallel programs. </title> <booktitle> In Proceedings of the 1990 ACM Conference on Lisp and Functional Programming, </booktitle> <pages> pages 185-197, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: The parallel visit operation executes the recursive calls concurrently using the spawn construct, which creates a new task for each operation. A straightforward application of lazy task creation techniques <ref> [28] </ref> can increase the granularity of the resulting parallel computation. The compiler also augments each graph node with a mutual exclusion lock mutex.
Reference: [29] <author> S. Pinter and R. Pinter. </author> <title> Program optimization and paral-lelization using idioms. </title> <booktitle> In Proceedings of the Eighteenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Orlando, FL, </address> <month> January </month> <year> 1991. </year>
Reference-contexts: In the long run we believe parallelizing compilers will incorporate both commutativity analysis and data dependence analysis for pointer-based data structures, using each when it is appropriate. 8.2 Reductions Several existing compilers can recognize when a loop performs a reduction of many values into a single value <ref> [16, 15, 29, 9] </ref>. These compilers recognize when the reduction primitive (typically addition) is associative. They then exploit this algebraic property to eliminate the data dependence associated with the serial accumulation of values into the result. The generated program computes the reduction in parallel.
Reference: [30] <author> J. Plevyak, V. Karamcheti, and A. Chien. </author> <title> Analysis of dynamic structures for efficient parallel execution. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: We also discuss reduction analysis and commuting operations in the context of parallel programming languages. 8.1 Data Dependence Analysis Research on automatically parallelizing serial computations that manipulate pointer-based data structures has focused on techniques that precisely represent the run-time topology of the heap <ref> [18, 25, 12, 30] </ref>. The idea is that the analysis can use this precise representation to discover independent pieces of code. To recognize independent pieces of code, the compiler must understand the global topology of the manipulated data structures [18, 25].
Reference: [31] <author> C. Polychronopoulos and D. Kuck. </author> <title> Guided self-scheduling: A practical scheduling scheme for parallel computers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <pages> pages 1425-1439, </pages> <month> December </month> <year> 1987. </year>
Reference-contexts: If a for loop contains nothing but invocations of parallel versions of methods, the compiler generates parallel loop code instead of code that serially spawns each invoked operation. The generated code can then apply standard parallel loop execution techniques; it currently uses guided self-scheduling <ref> [31] </ref>. 5.2 Suppressing Excess Concurrency In practice parallel execution inevitably generates overhead in the form of synchronization and task management overhead. If the compiler exploits too much concurrency, the resulting overhead may overwhelm the performance benefits of parallel execution.
Reference: [32] <author> W. Pugh and D. Wonnacott. </author> <title> Eliminating false data dependences using the Omega test. </title> <booktitle> In Proceedings of the SIGPLAN '92 Conference on Program Language Design and Implementation, </booktitle> <address> San Francisco, CA, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: In the right context this approach works well researchers have successfully used data dependence analysis to parallelize computations that manipulate dense arrays using affine access functions <ref> [1, 32, 13, 17] </ref>. But data dependence analysis is, by itself, inadequate for computations that manipulate dynamic, pointer-based data structures. Its limitations include a need to perform complicated analysis to extract global properties of the data structure topology and an fl Supported in part by an Alfred P.
Reference: [33] <author> M. Rinard. </author> <title> The Design, Implementation and Evaluation of Jade, a Portable, Implicitly Parallel Programming Language. </title> <type> PhD thesis, </type> <institution> Stanford, </institution> <address> CA, </address> <year> 1994. </year>
Reference-contexts: It is clearly feasible, however, to generate code for message-passing machines. The basic required functionality is a software layer that uses message-passing primitives to implement the abstraction of a single shared object store <ref> [33, 37] </ref>. The key question is how well the generated code would perform on such a platform. Message-passing machines have traditionally suffered from much higher communication costs than shared-memory machines. Compilation research for message-passing machines has therefore emphasized the development of data and computation placement algorithms that minimize communication [19]. <p> The goal of commutativity analysis is to preserve the sequential programming paradigm while 21 using parallel execution to deliver increased performance. While deterministic execution is one of the most important advantages of the serial programming paradigm, there are many others <ref> [33] </ref>. Commutativity analysis is also designed to recognize complex commuting operations that may recursively invoke other operations. Steele's framework focuses on atomic operations that only update memory.
Reference: [34] <author> M. Rinard. </author> <title> Communication optimizations for parallel computing using data access information. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <address> San Diego, CA, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: Given the dynamic nature of our target application set, the compiler would have to rely on dynamic techniques such as replication and task migration to optimize the locality of the generated computation <ref> [10, 34] </ref>. 7.4 Pointer Analysis The algorithms in Section 4 perform a data usage analysis at the granularity of the type system. An obvious alternative is to use pointer analysis [14, 43, 24] to identify the regions of memory that each operation may access.
Reference: [35] <author> M. Rinard and P. Diniz. </author> <title> Commutativity analysis: A technique for automatically parallelizing pointer-based computations. </title> <booktitle> In Proceedings of the 10th International Parallel Processing Symposium, </booktitle> <address> Honolulu, HI, </address> <month> April </month> <year> 1996. </year>
Reference-contexts: It then sorts the operands according to an arbitrary order on expressions. This sort facilitates the eventual expression comparison by making it easier to identify isomorphic subexpres-sions. We have also developed rules for conditional and array expressions <ref> [35] </ref>. In the worst case the expression manipulation algorithms may take exponential running time. Like other researchers applying similar expression manipulation techniques in other analysis contexts [7], we have not observed this behavior in practice.
Reference: [36] <author> J. K. Salmon. </author> <title> Parallel Hierarchical N-body Methods. </title> <type> PhD thesis, </type> <institution> California Institute of Technology, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: Although it is considered to be an important, widely studied computation, all previously existing parallel versions were parallelized by hand using low-level, explicitly parallel programming systems <ref> [36, 38] </ref>. We are aware of no other compiler that is capable of automatically parallelizing this computation. The space subdivision tree organizes the data as follows.
Reference: [37] <author> D. Scales and M. S. Lam. </author> <title> The design and evaluation of a shared object system for distributed memory machines. </title> <booktitle> In Proceedings of the First USENIX Symposium on Operating Systems Design and Implementation, </booktitle> <address> Monterey, CA, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: For the program to execute correctly, the programmer must ensure that all of the atomic operations commute. Four of the six parallel applications in the SPLASH benchmark suite [39] and three of the four parallel applications described in <ref> [37] </ref> rely on commuting operations to expose the concurrency and generate correct parallel execution. This experience suggests that compilers will be unable to parallelize a wide range of computations unless they recognize and exploit commuting operations. We have developed a new analysis framework called com-mutativity analysis. <p> It is clearly feasible, however, to generate code for message-passing machines. The basic required functionality is a software layer that uses message-passing primitives to implement the abstraction of a single shared object store <ref> [33, 37] </ref>. The key question is how well the generated code would perform on such a platform. Message-passing machines have traditionally suffered from much higher communication costs than shared-memory machines. Compilation research for message-passing machines has therefore emphasized the development of data and computation placement algorithms that minimize communication [19].
Reference: [38] <author> J. Singh. </author> <title> Parallel Hierarchical N-body Methods and their Implications for Multiprocessors. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: Although it is considered to be an important, widely studied computation, all previously existing parallel versions were parallelized by hand using low-level, explicitly parallel programming systems <ref> [36, 38] </ref>. We are aware of no other compiler that is capable of automatically parallelizing this computation. The space subdivision tree organizes the data as follows. <p> As part of the translation we eliminated several computations that dealt with parallel execution. For example, the parallel version used costzones partitioning to schedule the force computation phase <ref> [38] </ref>; the serial version eliminated the costzones code and the associated data structures. We also split a loop in the force computation phase into three loops. <p> The largest contribution to the performance difference is that the explicitly parallel version builds the space subdivision tree in parallel, while the automatically parallelized version builds the tree serially. The explicitly parallel version also uses an application-specific scheduling algorithm called costzones partitioning in the force computation phase <ref> [38] </ref>. This algorithm provides better locality than the guided self-scheduling algorithm in the automatically parallelized version.
Reference: [39] <author> J. Singh, W. Weber, and A. Gupta. </author> <title> SPLASH: Stanford parallel applications for shared memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Explicitly parallel programs, for example, often use locks, monitors and critical regions to ensure that operations execute atomically [23]. For the program to execute correctly, the programmer must ensure that all of the atomic operations commute. Four of the six parallel applications in the SPLASH benchmark suite <ref> [39] </ref> and three of the four parallel applications described in [37] rely on commuting operations to expose the concurrency and generate correct parallel execution. This experience suggests that compilers will be unable to parallelize a wide range of computations unless they recognize and exploit commuting operations. <p> It uses an enhanced version of the code generation algorithms in Section 5. We have used this compiler to automatically parallelize two applications: the Barnes-Hut hierarchical N-body solver [3] and the Water <ref> [39] </ref> code. 8 Explicitly parallel versions of the applications are available in the SPLASH [39] and SPLASH-2 [44] benchmark suites. <p> It uses an enhanced version of the code generation algorithms in Section 5. We have used this compiler to automatically parallelize two applications: the Barnes-Hut hierarchical N-body solver [3] and the Water <ref> [39] </ref> code. 8 Explicitly parallel versions of the applications are available in the SPLASH [39] and SPLASH-2 [44] benchmark suites. This section presents performance results for both the automatically parallelized and explicitly parallel versions on a 32 processor Stanford DASH machine [27] running a modified version of the IRIX 5.2 operating system.
Reference: [40] <author> G. Steele. </author> <title> Making asynchronous parallelism safe for the world. </title> <booktitle> In Proceedings of the Seventeenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 218-231, </pages> <address> San Francisco, CA, </address> <month> January </month> <year> 1990. </year>
Reference-contexts: The need to exploit reductions in traditional data parallel computations suggests that less structured computations will require generalized but similar techniques. 8.3 Commuting Operations in Parallel Lan guages Steele describes an explicitly parallel computing framework that includes primitive commuting operations such as the addition of a number into an accumulator <ref> [40] </ref>. The motivation is to deliver a flexible system for parallel computing that guarantees deterministic execution. The paper describes an enforcement mechanism that dynamically detects violations of the deterministic paradigm and mentions the possibility that a compiler could statically detect such violations.
Reference: [41] <author> C. Tseng. </author> <title> Compiler optimizations for eliminating barrier synchronization. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 144-155, </pages> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: A standard problem with traditional parallelizing compilers, for example, has been the difficulty of successfully amortizing the barrier synchronization overhead at each parallel loop <ref> [41] </ref>. Our prototype compiler introduces four sources of overhead when it generates parallel code: * Loop Overhead: The overhead generated by the execution of a parallel loop.
Reference: [42] <author> W. Weihl. </author> <title> Commutativity-based concurrency control for abstract data types. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(12) </volume> <pages> 1488-1505, </pages> <month> December </month> <year> 1988. </year>
Reference-contexts: Although we designed commutativity analysis to paral-lelize serial programs, it may also benefit other areas of computer science. For example, commuting operations allow computations on the persistent data in object-oriented databases to execute in parallel. Transaction processing systems can exploit commuting operations to use more efficient locking algorithms <ref> [42] </ref>. Commuting operations make protocols from distributed systems easier to implement efficiently; the corresponding reduction in the size of the associated state space may make it easier to verify the correctness of the protocol. In all of these cases the system relies on commuting operations for its correct operation.
Reference: [43] <author> R. Wilson and M. Lam. </author> <title> Efficient context-sensitive pointer analysis for C programs. </title> <booktitle> In Proceedings of the SIGPLAN '95 Conference on Program Language Design and Implementation, </booktitle> <address> La Jolla, CA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: An obvious alternative is to use pointer analysis <ref> [14, 43, 24] </ref> to identify the regions of memory that each operation may access. A major advantage of this approach for non type-safe languages like C and C++ is that it would allow the compiler to analyze programs that may violate their type declarations.
Reference: [44] <author> S. Woo, M. Ohara, E. Torrie, J.P. Singh, and A. Gupta. </author> <title> The SPLASH-2 programs: Characterization and methodological considerations. </title> <booktitle> In Proceedings of the 22th International Symposium on Computer Architecture, </booktitle> <address> Santa Margherita Ligure, Italy, </address> <month> June </month> <year> 1995. </year> <month> 23 </month>
Reference-contexts: We have implemented a run-time system that provides this functionality. It currently runs on the Stanford DASH multiprocessor [27] and on multiprocessors from Silicon Graphics. We have used the compilation system to automatically par 1 allelize two complete scientific applications: the Barnes-Hut N-body solver [3] and the Water code <ref> [44] </ref>. The Barnes-Hut is representative of our target class of dynamic computations: it performs well because it uses a pointer-based data structure (a space subdivision tree) to organize the computation. The Water code is a more traditional scientific computation that organizes its data as arrays of objects representing water molecules. <p> It uses an enhanced version of the code generation algorithms in Section 5. We have used this compiler to automatically parallelize two applications: the Barnes-Hut hierarchical N-body solver [3] and the Water [39] code. 8 Explicitly parallel versions of the applications are available in the SPLASH [39] and SPLASH-2 <ref> [44] </ref> benchmark suites. This section presents performance results for both the automatically parallelized and explicitly parallel versions on a 32 processor Stanford DASH machine [27] running a modified version of the IRIX 5.2 operating system.
Reference: [45] <author> Akinori Yonezawa, Jean-Pierre Briot, and Etsuya Shibayama. </author> <title> Object oriented concurrent programming in ABCL/1. </title> <booktitle> In Proceedings of the OOPSLA-86 Conference, </booktitle> <pages> pages 258-268, </pages> <address> Port-land OR, </address> <month> September </month> <year> 1986. </year> <month> 24 </month>
Reference-contexts: One of these relations is that the parts of the program commute. The goal is to help the implementation exploit concurrency while ensuring that the parallelization does not change the semantics of the program. Many concurrent object-oriented languages support the notion of mutually exclusive operations on objects <ref> [11, 45] </ref>. Although the concept of commuting operations is never explicitly identified, the expectation is that all mutually exclusive operations that may attempt to concurrently access the same object commute.
References-found: 45

