URL: ftp://ftp.cs.man.ac.uk/pub/TR/UMCS-97-8-1.ps.Z
Refering-URL: http://www.cs.man.ac.uk/cstechrep/Abstracts/UMCS-97-8-1.html
Root-URL: http://www.cs.man.ac.uk
Title: Aspects of Hardware-Controlled Data Prefetching  
Author: Ando Ki Alan E. Knowles 
Affiliation: Computer Science University of Manchester  
Pubnum: ISSN 1361 6161  
Abstract: Department of Computer Science University of Manchester Technical Report Series UMCS-97-8-1 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Jean-Loup Baer and Wen-Hann Wang. </author> <title> Architectural choices for multilevel cache hierarchies. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <pages> pages 258-261, </pages> <month> August </month> <year> 1987. </year>
Reference-contexts: The secondary cache is a write-back, direct-mapped data cache with write-allocate on write misses. The size of primary cache is smaller than that of secondary cache, and we use a mechanism to guarantee the content multi-level inclusion property <ref> [1] </ref>: the contents of the primary cache are a subset of that of the secondary cache. The global shared memory modules are organised as low-order interleaved memory, so that multiple memory accesses can be handled concurrently.
Reference: [2] <author> Richard R. Billig. </author> <title> A fast backplane cluster heralds a 1000-MIPS computer. </title> <booktitle> Electronic Design, </booktitle> <month> July </month> <year> 1987. </year>
Reference-contexts: We do not want to make this work too specific to any particular network system design details. However, the data transfer protocol in [16] or <ref> [2] </ref> can be an alternative choice. Table 1 shows simulation parameters and the latency times for a memory request without conflict.
Reference: [3] <author> David Callahan, Ken Kennedy, and Allan Porterfield. </author> <title> Software prefetching. </title> <booktitle> In ASPLOS-IV, </booktitle> <pages> pages 40-52, </pages> <year> 1991. </year> <month> 23 </month>
Reference-contexts: The former is called sequential prefetching and the latter stride prefetching. With software (directed) prefetching, the compiler or programmer directs prefetching by inserting prefetch instructions into the code <ref> [3, 9, 19] </ref>. In this work, we only consider the non-binding and explicit prefetching schemes, and concentrate on hardware-controlled prefetching. Especially, we study Smith's tagged pre-fetching [21] and Sklenar's stride prefetching [20]. We address the effects of prefetching in shared-memory multiprocessors.
Reference: [4] <author> Tien-Fu Chen and Jean-Loup Baer. </author> <title> Reducing memory latency via non-blocking and prefetching caches. </title> <booktitle> In ASPLOS-V, </booktitle> <pages> pages 51-61, </pages> <year> 1992. </year>
Reference-contexts: Further observation is that when memory bandwidth is not sufficient, pending stall time increases, as shown in the cases of cholesky, fft, lu, ocean, and radix in Figure 6. Therefore, to reduce this pending stall time, a lookahead <ref> [4] </ref> or an adaptive [5, 15] mecha nism is needed to issue prefetching requests early. In most cases except barnes, prefetching with multi-way interleaving experiences less stall time than no-prefetching. Prefetching can degrade the performance when the memory bandwidth is not sufficient 12 or memory contention is a primary hindrance. <p> In the water case, prefetching is less useful. Prefetching for long latency may not result in completely hiding the latency because it is likely to have significant pending stall time. Therefore, it is necessary to overcome pending stall by employing early prefetching mechanism <ref> [4] </ref> or adaptive [5, 15] mechanism. 4.3 Effects of cache size It is generally true that a larger cache has higher cache hit rate than a smaller one, because spatial and temporal localities can be better exploited.
Reference: [5] <author> Fredrik Dahlgren, Michel Dubois, and Per Stenstrom. </author> <title> Sequential hardware prefetching in shared-memory multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 6(7) </volume> <pages> 733-746, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: Therefore, prefetches are handled dynamically without intervention of the compiler or programmer, and there is no need to change code. Hardware-controlled prefetching schemes can be classified according to whether they are based on spatial locality <ref> [5, 21] </ref> or calculated stride [7, 20]. The former is called sequential prefetching and the latter stride prefetching. With software (directed) prefetching, the compiler or programmer directs prefetching by inserting prefetch instructions into the code [3, 9, 19]. <p> Further observation is that when memory bandwidth is not sufficient, pending stall time increases, as shown in the cases of cholesky, fft, lu, ocean, and radix in Figure 6. Therefore, to reduce this pending stall time, a lookahead [4] or an adaptive <ref> [5, 15] </ref> mecha nism is needed to issue prefetching requests early. In most cases except barnes, prefetching with multi-way interleaving experiences less stall time than no-prefetching. Prefetching can degrade the performance when the memory bandwidth is not sufficient 12 or memory contention is a primary hindrance. <p> In the water case, prefetching is less useful. Prefetching for long latency may not result in completely hiding the latency because it is likely to have significant pending stall time. Therefore, it is necessary to overcome pending stall by employing early prefetching mechanism [4] or adaptive <ref> [5, 15] </ref> mechanism. 4.3 Effects of cache size It is generally true that a larger cache has higher cache hit rate than a smaller one, because spatial and temporal localities can be better exploited.
Reference: [6] <author> Christopher W. Fraser and David R. Hanson. </author> <title> A Retargetable C Compiler: Design and Implementation. </title> <publisher> Benjamin Cummings, </publisher> <year> 1995. </year>
Reference-contexts: prefetch operation does not bring data into a register, ii) non-blocking property - prefetch operation does not cause processor stalls, iii) non-excepting property - prefetch operation does not make memory access related exceptions, such as bus error and page fault. 2.3 Compiler The compiler is implemented using Princeton University Lcc <ref> [6] </ref>. The compiler transforms a C-language source program to an assembly language output which is a simple non-pipelined RISC-like load/store order code [13]. The simulator interprets the assembly language directly. The word size is fixed at the host machine's integer type size (typically, it is 32-bit (4-Byte)).
Reference: [7] <author> John W. C. Fu, Janak H. Patel, and Bob L. Janssens. </author> <title> Stride directed prefetching in scalar processors. </title> <booktitle> In Proceedings of the 25th International Symposium on Microarchi-tecture, </booktitle> <pages> pages 102-110, </pages> <year> 1992. </year>
Reference-contexts: Therefore, prefetches are handled dynamically without intervention of the compiler or programmer, and there is no need to change code. Hardware-controlled prefetching schemes can be classified according to whether they are based on spatial locality [5, 21] or calculated stride <ref> [7, 20] </ref>. The former is called sequential prefetching and the latter stride prefetching. With software (directed) prefetching, the compiler or programmer directs prefetching by inserting prefetch instructions into the code [3, 9, 19]. In this work, we only consider the non-binding and explicit prefetching schemes, and concentrate on hardware-controlled prefetching. <p> In this work, we use 256 entries for the hardware lookup table. According to the work of Fu et al. <ref> [7] </ref>, over 90% of the memory references cause hits in the hardware lookup table with 256 entries. Results for the baseline architecture (see Table 1) are shown in Figure 1, 2, 3, 4 and 5.
Reference: [8] <author> Kourosh Gharachorloo, Anoop Gupta, and John Hennessy. </author> <title> Performance evaluation of memory consistency models for shared-memory multiprocessors. </title> <booktitle> In ASPLOS-IV, </booktitle> <pages> pages 245-257, </pages> <year> 1991. </year>
Reference-contexts: However, we are not interested in those portions. Prefetchings are prohibited during synchronisation, because prefetching for synchronisation variables could make the effects of sharing worse. The store stall times can be removed by employing a write-buffer and a relaxed consistency model <ref> [8, 14] </ref>, so we draw only load stall times in The cholesky benchmark uses a task-queue and each process waits on it until new task shows up.
Reference: [9] <author> Edward H. Gornish, Elana D. Granston, and Alexander V. Veidenbaum. </author> <title> Compiler-directed data prefetching in multiprocessors with memory hierarchies. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <pages> pages 354-368, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: The former is called sequential prefetching and the latter stride prefetching. With software (directed) prefetching, the compiler or programmer directs prefetching by inserting prefetch instructions into the code <ref> [3, 9, 19] </ref>. In this work, we only consider the non-binding and explicit prefetching schemes, and concentrate on hardware-controlled prefetching. Especially, we study Smith's tagged pre-fetching [21] and Sklenar's stride prefetching [20]. We address the effects of prefetching in shared-memory multiprocessors.
Reference: [10] <author> Anoop Gupta, John Hennessy, Kourosh Gharachorloo, Todd C. Mowry, and Wolf-Dietrich Weber. </author> <title> Comparative evaluation of latency reducing and tolerating techniques. </title> <booktitle> In 18th ISCA, </booktitle> <pages> pages 254-263, </pages> <year> 1991. </year>
Reference-contexts: Data prefetching can be classified based on whether it is binding or non-binding <ref> [10] </ref>. With a binding prefetch, the value obtained by a later reference is that obtained at the time of the prefetch.
Reference: [11] <author> John L. Hennessy and Norman P. Jouppi. </author> <title> Computer technology and architecture: An evolving interaction. </title> <journal> IEEE Computer, </journal> <volume> 24(9) </volume> <pages> 18-29, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: 1 Introduction The speed gap between processors and memory has widened in the last few years, and the technology trends indicate that this speed gap is likely to increase in the future <ref> [11] </ref>. As the speed gap becomes wider, high-performance processors become more sensitive to stalls for memory accesses. The latency of memory accesses is an important issue in the design of a computer system because processors usually stall for the duration of memory requests.
Reference: [12] <author> Brian W. Kernighan and Dennis M. Ritchie. </author> <title> The C Programming Language. </title> <booktitle> Prentice-Hall, second edition, </booktitle> <year> 1988. </year>
Reference-contexts: Furthermore, we subdivide the execution time into code cycles (instruction execution time), memory access stall time (waiting for data returning), and synch wait cycles (waiting for synchronisation). 5 2.5 Benchmark programs We use parallel benchmark programs taken from SPLASH-2 [23]. They are written in the C-language <ref> [12] </ref> with a shared-memory architecture in mind and parallelism is implemented by augmenting the program with the notation of the Argonne National Laboratory parallel macros (PARMACS) [18]. Table 2 summarises benchmark programs.
Reference: [13] <author> Ando Ki. </author> <title> MUSIC Processor: Manchester university simplified instruction code processor. </title> <type> internal report, </type> <institution> University of Manchester, 1995. Department of Computer Science. </institution>
Reference-contexts: Each processing node consists of a processor, a primary cache, and a secondary cache. The processor models a load-store RISC architecture called MUSIC (Manchester University Simplified Instruction Code) processor <ref> [13] </ref>, and it uses a simple stall-on-miss model. All instructions execute in one processor cycle except load and store instructions which require memory references. <p> The compiler transforms a C-language source program to an assembly language output which is a simple non-pipelined RISC-like load/store order code <ref> [13] </ref>. The simulator interprets the assembly language directly. The word size is fixed at the host machine's integer type size (typically, it is 32-bit (4-Byte)).
Reference: [14] <author> Ando Ki. </author> <title> The effectiveness of write-buffers in reducing the impact of memory latency. </title> <type> internal report, </type> <institution> University of Manchester, 1997. Department of Computer Science. </institution>
Reference-contexts: However, we are not interested in those portions. Prefetchings are prohibited during synchronisation, because prefetching for synchronisation variables could make the effects of sharing worse. The store stall times can be removed by employing a write-buffer and a relaxed consistency model <ref> [8, 14] </ref>, so we draw only load stall times in The cholesky benchmark uses a task-queue and each process waits on it until new task shows up.
Reference: [15] <author> Ando Ki and Alan E. Knowles. </author> <title> Adaptive data prefetching using cache information. </title> <booktitle> In Proceedings of the 11th ACM International Conference on Supercomputing, </booktitle> <pages> pages 204-212. </pages> <publisher> ACM, </publisher> <month> July </month> <year> 1997. </year> <institution> Vienna, Austria. </institution>
Reference-contexts: Further observation is that when memory bandwidth is not sufficient, pending stall time increases, as shown in the cases of cholesky, fft, lu, ocean, and radix in Figure 6. Therefore, to reduce this pending stall time, a lookahead [4] or an adaptive <ref> [5, 15] </ref> mecha nism is needed to issue prefetching requests early. In most cases except barnes, prefetching with multi-way interleaving experiences less stall time than no-prefetching. Prefetching can degrade the performance when the memory bandwidth is not sufficient 12 or memory contention is a primary hindrance. <p> In the water case, prefetching is less useful. Prefetching for long latency may not result in completely hiding the latency because it is likely to have significant pending stall time. Therefore, it is necessary to overcome pending stall by employing early prefetching mechanism [4] or adaptive <ref> [5, 15] </ref> mechanism. 4.3 Effects of cache size It is generally true that a larger cache has higher cache hit rate than a smaller one, because spatial and temporal localities can be better exploited.
Reference: [16] <author> Ando Ki, Byung Kwan Park, Won Sae Sim, Kyeng Yong Kang, and Yong Ho Yoon. </author> <title> Highly Pipelined Bus : HiPi-Bus. </title> <booktitle> In Proceedings of the Joint Technical Conference, </booktitle> <pages> pages 528-533, </pages> <address> 1991. Hirosima Japan. </address> <month> 24 </month>
Reference-contexts: We do not want to make this work too specific to any particular network system design details. However, the data transfer protocol in <ref> [16] </ref> or [2] can be an alternative choice. Table 1 shows simulation parameters and the latency times for a memory request without conflict.
Reference: [17] <author> David Kroft. </author> <title> Lockup-free instruction fetch/prefetch cache organization. </title> <booktitle> In 8th ISCA, </booktitle> <pages> pages 81-87, </pages> <year> 1981. </year>
Reference-contexts: We assume a write-buffer between the primary cache and the secondary cache, and write hits on the secondary cache do not stall the processor. Write misses on the secondary cache make the processor stall, and loads cannot bypass store in the write-buffer. The caches are lockup-free <ref> [17] </ref> : i.e. they can support multiple outstanding requests. When a request misses in the cache, a cache line is reserved and the request is sent to the memory. Prefetching is considered for load operation only. <p> Likewise, prefetching also benefits from set-associativity. 4.6 Effects of Cache Support The secondary caches used in the baseline architecture are lockup-free <ref> [17] </ref>, so that multiple requests can be outstanding. In other words, normal reads are allowed to be serviced while there are prefetching requests outstanding. This ensures that normal reads are not affected by previous prefetchings. Note that the lockup-free cache also allows multiple prefetching requests to be pipelined.
Reference: [18] <author> Ewing Lusk, James Boyle, Ralph Bntler, Terrence Disz, Barnett Glickfeld, Ross Over-beek, James Patterson, and Rick Stevens. </author> <title> Portable Programs for Parallel Processors. </title> <publisher> Holt, Rinehar and Winston, Inc., </publisher> <year> 1987. </year>
Reference-contexts: They are written in the C-language [12] with a shared-memory architecture in mind and parallelism is implemented by augmenting the program with the notation of the Argonne National Laboratory parallel macros (PARMACS) <ref> [18] </ref>. Table 2 summarises benchmark programs.
Reference: [19] <author> Todd C. Mowry and Anoop Gupta. </author> <title> Tolerating latency through software-controlled prefetching in shared-memory multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 12 </volume> <pages> 87-106, </pages> <year> 1991. </year>
Reference-contexts: The former is called sequential prefetching and the latter stride prefetching. With software (directed) prefetching, the compiler or programmer directs prefetching by inserting prefetch instructions into the code <ref> [3, 9, 19] </ref>. In this work, we only consider the non-binding and explicit prefetching schemes, and concentrate on hardware-controlled prefetching. Especially, we study Smith's tagged pre-fetching [21] and Sklenar's stride prefetching [20]. We address the effects of prefetching in shared-memory multiprocessors. <p> Prefetching is considered for load operation only. Prefetched data is brought into the secondary cache, and this data remains visible to the cache coherence protocol to guarantee the non-binding property <ref> [19] </ref>. We assume that cache lines for prefetches are allocated in the cache at issue time in order to find the interference produced by prefetching. Prefetches for non-allocated pages are not allowed in order to reduce not-used-prefetches (see subsection 2.4) and to prevent prefetching of out of scope data. <p> Prefetches for non-allocated pages are not allowed in order to reduce not-used-prefetches (see subsection 2.4) and to prevent prefetching of out of scope data. The prefetch operation used in this work has the following properties <ref> [19] </ref>: i) non-binding property - prefetch operation does not bring data into a register, ii) non-blocking property - prefetch operation does not cause processor stalls, iii) non-excepting property - prefetch operation does not make memory access related exceptions, such as bus error and page fault. 2.3 Compiler The compiler is implemented
Reference: [20] <author> Ivan Sklenar. </author> <title> Prefetch unit for vector operations on scalar computers. </title> <journal> Computer Architecture News, </journal> <volume> 20(4) </volume> <pages> 31-37, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: Therefore, prefetches are handled dynamically without intervention of the compiler or programmer, and there is no need to change code. Hardware-controlled prefetching schemes can be classified according to whether they are based on spatial locality [5, 21] or calculated stride <ref> [7, 20] </ref>. The former is called sequential prefetching and the latter stride prefetching. With software (directed) prefetching, the compiler or programmer directs prefetching by inserting prefetch instructions into the code [3, 9, 19]. In this work, we only consider the non-binding and explicit prefetching schemes, and concentrate on hardware-controlled prefetching. <p> With software (directed) prefetching, the compiler or programmer directs prefetching by inserting prefetch instructions into the code [3, 9, 19]. In this work, we only consider the non-binding and explicit prefetching schemes, and concentrate on hardware-controlled prefetching. Especially, we study Smith's tagged pre-fetching [21] and Sklenar's stride prefetching <ref> [20] </ref>. We address the effects of prefetching in shared-memory multiprocessors. Caches are an integral part of data prefetching, and, therefore, cache parameters, such as overall cache size, cache line size, and cache support, are likely to affect data prefetching performance. <p> Some prefetching of shared data can cause extra invalidation traffic on this data that might yet have to be used exclusively by another processor. We use representatives of two hardware-controlled prefetching methods for our experiment. One is Smith's tagged prefetching [21] and the other is Sklenar's stride prefetching <ref> [20] </ref>. The former method initiates a prefetching request for the next cache line when a demand for a cache line is the first access since a prefetch or miss on it. There is a tag bit in each cache line.
Reference: [21] <author> Alan Jay Smith. </author> <title> Sequential program prefetching in memory hierarchies. </title> <journal> IEEE Computer, </journal> <volume> 11(2) </volume> <pages> 7-21, </pages> <month> December </month> <year> 1978. </year>
Reference-contexts: Therefore, prefetches are handled dynamically without intervention of the compiler or programmer, and there is no need to change code. Hardware-controlled prefetching schemes can be classified according to whether they are based on spatial locality <ref> [5, 21] </ref> or calculated stride [7, 20]. The former is called sequential prefetching and the latter stride prefetching. With software (directed) prefetching, the compiler or programmer directs prefetching by inserting prefetch instructions into the code [3, 9, 19]. <p> With software (directed) prefetching, the compiler or programmer directs prefetching by inserting prefetch instructions into the code [3, 9, 19]. In this work, we only consider the non-binding and explicit prefetching schemes, and concentrate on hardware-controlled prefetching. Especially, we study Smith's tagged pre-fetching <ref> [21] </ref> and Sklenar's stride prefetching [20]. We address the effects of prefetching in shared-memory multiprocessors. Caches are an integral part of data prefetching, and, therefore, cache parameters, such as overall cache size, cache line size, and cache support, are likely to affect data prefetching performance. <p> Some prefetching of shared data can cause extra invalidation traffic on this data that might yet have to be used exclusively by another processor. We use representatives of two hardware-controlled prefetching methods for our experiment. One is Smith's tagged prefetching <ref> [21] </ref> and the other is Sklenar's stride prefetching [20]. The former method initiates a prefetching request for the next cache line when a demand for a cache line is the first access since a prefetch or miss on it. There is a tag bit in each cache line.
Reference: [22] <author> Dean M. Tullsen and Susan J. Eggers. </author> <title> Limitations of cache prefetching on a bus-based multiprocessor. </title> <booktitle> In 20th ISCA, </booktitle> <pages> pages 278-288, </pages> <year> 1993. </year>
Reference-contexts: In most cases except barnes, prefetching with multi-way interleaving experiences less stall time than no-prefetching. Prefetching can degrade the performance when the memory bandwidth is not sufficient 12 or memory contention is a primary hindrance. The results presented in this subsection confirm the work of Tullsen and Eggers <ref> [22] </ref> in showing that an adequate bandwidth is necessary to get the benefits of prefetching. 4.2 Effects of memory access latency Memory access latency affects directly the effectiveness of data prefetching, because it determines the time spent from the moment the prefetching request is issued, to the moment the prefetched data
Reference: [23] <author> Steven Cameron Woo, Moriyoshi Ohara, Evan Torrie, Jaswinder Pal Singh, and Anoop Gupta. </author> <title> The SPLASH-2 Programs: Characterization and methodological considerations. </title> <booktitle> In 22nd ISCA, </booktitle> <pages> pages 24-36, </pages> <year> 1995. </year> <month> 25 </month>
Reference-contexts: Other system parameters such as memory bandwidth and latency also deserve attention to study the effect on data prefetching performance. The goal of our work is to investigate the impact of cache and system parameters on the performance of prefetching. Our experiments simulate parallel benchmark programs from SPLASH-2 <ref> [23] </ref> on a shared-memory multiprocessor, with and without hardware-controlled prefetching. The metrics of interest include the reduction of memory stall time and cache misses, the effectiveness of prefetching, and the increase in network traffic. The remainder of this paper is organised as follows. <p> Furthermore, we subdivide the execution time into code cycles (instruction execution time), memory access stall time (waiting for data returning), and synch wait cycles (waiting for synchronisation). 5 2.5 Benchmark programs We use parallel benchmark programs taken from SPLASH-2 <ref> [23] </ref>. They are written in the C-language [12] with a shared-memory architecture in mind and parallelism is implemented by augmenting the program with the notation of the Argonne National Laboratory parallel macros (PARMACS) [18]. Table 2 summarises benchmark programs.
References-found: 23

