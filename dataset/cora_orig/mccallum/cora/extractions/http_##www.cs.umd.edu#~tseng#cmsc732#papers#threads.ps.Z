URL: http://www.cs.umd.edu/~tseng/cmsc732/papers/threads.ps.Z
Refering-URL: http://www.cs.umd.edu/~tseng/cmsc732/papers.html
Root-URL: 
Email: grunwald@cs.colorado.edu  rneves@watson.ibm.com  
Title: Whole-Program Optimization for Time and Space Efficient Threads  
Author: Dirk Grunwald Richard Neves P.O. Box 
Address: Campus Box 430, Boulder, CO 80309-0430  Yorktown Heights, NY 10598  
Affiliation: Dept. of Computer Science University of Colorado  IBM. T. J. Watson Research  
Abstract: In general, it would be desirable if threaded programs could be written to expose the largest degree of parallelism possible, or to simplify the program design. However, threads incur time and space overheads, and programmers often compromise simple designs for performance. In this paper, we show how to reduce time and space thread overhead using control flow and register liveness information inferred after compilation. Our techniques work on binaries, are not specific to a particular compiler or thread library and reduce the the overall execution time of fine-grain threaded programs by 1530%. We use execution-driven analysis and an instrumented operating system to show why the execution time is reduced and to indicate areas for future work. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. </author> <booktitle> Compilers Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley Publish--ing Company, </publisher> <address> Reading, Massachusetts, </address> <year> 1986. </year>
Reference-contexts: In general, iterative data-flow algorithms are characterized by a set of recurrence equations. The equations for this reaching definitions problem are shown in Figure 2. Instead of applying this data-flow algorithm to basic blocks, as is the case in <ref> [1] </ref>, the algorithm is instead applied to the nodes of the call site graph. As in all data-flow algorithms, information is generated and killed while iterating over the recurrence equations. <p> Therefore, iterating over these equations is guaranteed to converge since one of the out sets for a node in the graph will eventually not change (see <ref> [1] </ref>). Indirect procedures are handles as in [20], where "potential call edges" are placed between all indirect procedure calls and all possible call targets. However , we actually refine the call targets by only considering procedures whose addresses are taken in the program being optimized. cost.
Reference: [2] <author> R. Alverson, D. Callahan, D. Cummings, B. Koblenz, A. Porterfield, and B. Smith. </author> <title> The Tera computer system. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <pages> pages 1-6, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Link-time optimization can have drawbacks, because higher-level information such as variable type information or possible indirect call targets for languages such as C++ are not represented in the final program. The compiler for the Tera <ref> [2, 10] </ref> computer system also performs whole-program optimization using a program database.
Reference: [3] <author> Thomas E. Anderson, Henry M. Levy, Brian N. Bershad, and Edward D. Lazowska. </author> <title> The interaction of architecture and operating system design. </title> <booktitle> In Proceedings of the 18th International Symposium on Computing Architecture, </booktitle> <year> 1991. </year>
Reference-contexts: Later, we will show that the performance improvement due to smaller stack segments justifies the small increase in executed instructions. 3 Context Switch Optimization A whole program optimizer can also be used to reduce time overhead of threads. Anderson, et. al. <ref> [3] </ref> conclude that the performance of fine-grained user level threaded systems are sensitive to the number of registers that must be saved and restored at context switches. In this section, we are primarily interested in reducing the context switch time of non-preemptive context switches.
Reference: [4] <author> Andrew W. Appel and Zhong Shao. </author> <title> An empirical and analytic study of stack vs. heap cost for languages with closures. </title> <type> Technical Report CS-TR-450-94, </type> <institution> Princeton University, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: In either case, these explicit checks are necessary at each procedure call resulting in additional overhead. In some systems using this approach, activation records are allocated on the heap and recovered via garbage collection <ref> [4] </ref>. Other systems manage stacks using explicit memory management. <p> It is not clear from their paper how frequently these checks could be eliminated. Like the technique of Nilsen et al, the method proposed by Hieb requires additional work for each procedure call, but causes less fragmentation. Appel and Shao <ref> [4] </ref> concluded that heap allocation of activation records was more efficient than stack allocation for languages with closures if a garbage collection system is already being used. They measured the performance of stack and heap allocation for a variety of programs using Standard ML of New Jersey. <p> However, as Hieb pointed out when suggesting that garbage collection might be faster than stack allocation, such a method assumes a large physical memory. Also, the conclusions of <ref> [4] </ref> may not hold for the programs written for systems where garbage collection is rarely used, procedure activations come in a variety of sizes, and continuation passing style is not used. Shao and Appel [17] also consider call-graph analysis to reduce the cost of heap-allocated activation records in Standard ML.
Reference: [5] <author> M.E. Benitez and Jack W. Davidson. </author> <title> A portable global optimizer and linker. </title> <booktitle> In Proceedings of the ACM SIGPLAN '88 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 329-338, </pages> <address> Atlanta, GA, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: Both the storage management and context switch optimization techniques use whole-program or link-time optimizations. Whole program optimizers are common for functional languages, but less common for imperative languages. Wall and Powell describe the Mahler [23] system, which allowed link-time optimizations. Ben-itez and Davidson <ref> [5] </ref> describe VPO, the very portable optimizer, which was capable of some link-time optimizations. Santhanam and Odnert [16] described an interprocedural register allocation algorithm constructed using the compilers on the HP PA-RISC. More recently, Srivastiva and Wall [20] describe OM, a system for intermodule code optimization.
Reference: [6] <author> D.E. Culler, A. Sah, K.E. Schauser, T. von Eicken, and J. Wawrzynek. </author> <title> Fine-grain parallelism with minimal hardware support: a compiler-controlled threaded abstract machine. </title> <booktitle> In Proceedings of the 4th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 164-175, </pages> <address> Santa Clara, CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Shao and Appel [17] also consider call-graph analysis to reduce the cost of heap-allocated activation records in Standard ML. They reduce the number of frames that need to be allocated and the register save/restore around frame allocation points. The TAM system <ref> [6] </ref> uses heap-allocated frames to store activation records, but requires a specific compiler to utilize the TAM runtime system. The stacklets of Goldstein et al [7] are similar, but allow multiple frames to reside in a single stacklet.
Reference: [7] <author> Seth Goldstein, Klaus Schauser, and David Culler. </author> <title> Languages, Compilers and Run-Time Systems for Scalable Computers, </title> <booktitle> chapter 12, </booktitle> <pages> pages 153-168. </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1996. </year>
Reference-contexts: They reduce the number of frames that need to be allocated and the register save/restore around frame allocation points. The TAM system [6] uses heap-allocated frames to store activation records, but requires a specific compiler to utilize the TAM runtime system. The stacklets of Goldstein et al <ref> [7] </ref> are similar, but allow multiple frames to reside in a single stacklet. The stacklets work is similar to work by Hieb et al [9], but applied to a different problem domain.
Reference: [8] <author> Dirk Grunwald. </author> <title> Heaps o' stacks: Time and space efficient threads without operating system support. </title> <type> Technical Report CU-CS-750-94, </type> <institution> University of Colorado, </institution> <month> November </month> <year> 1994. </year>
Reference-contexts: The allocation stubs rely on special purpose memory alloca-tors which are very efficient since the size of each activation record is known at compile time. The allocation and deallocation stubs are very efficient inlined allocators that simply manipulate free lists. In related work <ref> [8] </ref>, we applied this technique to a number of C programs and found that the additional allocation overhead is very small (1% overhead). <p> We did not use profiles in our optimizations, and did not implement this alternative. 4 Performance of the Optimizations and time optimizations for four applications. 2 We used the MP3D 2 An earlier report <ref> [8] </ref> presents detailed results for a number of sequential programs that illustrate the generality of the space optimization; we omit those results for brevity. program from the SPLASH-I suite and Ocean from the SPLASH-II suite. <p> Similarly, the reduced cost of context switches provides a safe, consistent reduction in thread overhead while maintaining an investment in current compiler and runtime system infrastructure. A more detailed comparison of a number of C and FORTRAN programs <ref> [8] </ref> has shown that combined heap-based allocation occasionally consumes considerably more memory than the conventional allocation method. We have also shown that it is useful for activation records to use a few, distinct sizes to reduce the cost of memory allocation.
Reference: [9] <author> Robert Heib, R. Kent Dybvig, and Carl Bruggeman. </author> <title> Representing control in the presenceof first-class continuations. </title> <booktitle> In Proceedings of the ACM SIGPLAN '90 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 66-77, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Control over thread stack space is possible at the granularity of an activation record with the assistance of a runtime library. One common technique is to allocate contiguous memory regions and check that sufficient space remains in a given region for the current activation record in each procedure call <ref> [9] </ref>. If the check occurs prior to the procedure, the caller must know the amount of memory needed by the callee. <p> They then used an efficient linked-list mechanism to pre-allocate a number of different sized activation records (as in the Mesa architecture), and return unneeded activations to that linked list. There are a number of software-assisted frame allocation techniques. Hieb <ref> [9] </ref> discussed the problem of heap-allocation activation records for a language with continuations. They allocated a large block of storage and used explicit checks at each procedure call to insure there was sufficient space. <p> They measured the performance of stack and heap allocation for a variety of programs using Standard ML of New Jersey. In their system, programs are compiled using continuation passing style, and all procedures have a fixed-sized 32 byte activation record; the system used by Hieb <ref> [9] </ref> normally used 30-byte activation records. However, as Hieb pointed out when suggesting that garbage collection might be faster than stack allocation, such a method assumes a large physical memory. <p> The stacklets of Goldstein et al [7] are similar, but allow multiple frames to reside in a single stacklet. The stacklets work is similar to work by Hieb et al <ref> [9] </ref>, but applied to a different problem domain. The Concert system uses a hybrid execution model [15] that allows some procedure calls to use heap-based allocation while other calls use sequential allocation, but the decisions are made by a single compiler and are not applicable to the program binary.
Reference: [10] <author> Robert Henry, Allan Porterfield, and Burton Smith. </author> <title> Tera compiler overview. (discussion), </title> <month> January </month> <year> 1994. </year>
Reference-contexts: Link-time optimization can have drawbacks, because higher-level information such as variable type information or possible indirect call targets for languages such as C++ are not represented in the final program. The compiler for the Tera <ref> [2, 10] </ref> computer system also performs whole-program optimization using a program database.
Reference: [11] <author> Richard K. Johnsson and John D. Wick. </author> <title> An overview of the Mesa processor architecture. </title> <booktitle> In Proceedings of the Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 20-29, </pages> <address> Palo Alto, California, </address> <year> 1982. </year>
Reference-contexts: These proposals can be divided into hardware-assisted and software-assisted methods. One of the earliest hardware-assisted proposals was by Lamp-son [13], who proposed a mechanism for fast procedure calls where procedure, co-routine, and process frame allocation were treated uniformly. The mechanism was implemented in the Mesa architecture <ref> [11] </ref>.
Reference: [12] <author> R. Kessler and M. Hill. </author> <title> Page placement algorithms for large direct-mapped real-index caches. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10(4) </volume> <pages> 338-359, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: We were unable to determine where the remaining performance improvement arose; we conjecture that it may come from second level data cache misses or a better mapping of the large virtual space onto the physically indexed second level cache <ref> [12] </ref>. We were able to determine that the performance improvement was not a cold start by varying the parameters of Ocean to force the program to execute 10 times longer. The performance counters indicated that the 20.8% improvement dropped slightly to 17% for the longer running program.
Reference: [13] <author> Butler W. Lampson. </author> <title> Fast procedure calls. </title> <booktitle> In Proceedings of the Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 66-76, </pages> <address> Palo Alto, California, </address> <year> 1982. </year>
Reference-contexts: These proposals can be divided into hardware-assisted and software-assisted methods. One of the earliest hardware-assisted proposals was by Lamp-son <ref> [13] </ref>, who proposed a mechanism for fast procedure calls where procedure, co-routine, and process frame allocation were treated uniformly. The mechanism was implemented in the Mesa architecture [11].
Reference: [14] <author> Kelvin D. Nilsen and William J. Schmidt. </author> <title> A high-performance hardware assisted real-time garbage collection system. </title> <booktitle> In Proceedings of the 5th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> Oct </month> <year> 1994. </year>
Reference-contexts: In the opt'n/full-clone data, all context switch routines are uniquely cloned. In the opt'n/unique-clone, duplicate context switch routines are removed. We simulated the 8KByte direct-mapped cache used on the DEC 21064. common frame sizes that were maintained by free lists. Nilsen and Schmidt <ref> [14] </ref> describe a hardware system for real-time garbage collection. They mentioned that they initially allocated procedure activation records using this mechanism, but found it to be very expensive because function calls contributed up to 99% of the allocation activity.
Reference: [15] <author> John Plevyak, Vijay Karamcheti, Xingbin Zhang, and Andrew A. Chien. </author> <title> A hybrid execution model for fine-grained languages on distributed memory multicomputers. </title> <booktitle> In Proceedings SuperComputing '95, </booktitle> <address> San Diego, California, </address> <month> November </month> <year> 1995. </year>
Reference-contexts: The stacklets of Goldstein et al [7] are similar, but allow multiple frames to reside in a single stacklet. The stacklets work is similar to work by Hieb et al [9], but applied to a different problem domain. The Concert system uses a hybrid execution model <ref> [15] </ref> that allows some procedure calls to use heap-based allocation while other calls use sequential allocation, but the decisions are made by a single compiler and are not applicable to the program binary. The storage allocation technique we propose is similar to most of the preceding examples.
Reference: [16] <author> Vatsa Santhanam and Daryl Odnert. </author> <title> Register allocation across procedure and module boundaries. </title> <booktitle> In Proceedings of the ACM SIGPLAN '90 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 28-39, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Whole program optimizers are common for functional languages, but less common for imperative languages. Wall and Powell describe the Mahler [23] system, which allowed link-time optimizations. Ben-itez and Davidson [5] describe VPO, the very portable optimizer, which was capable of some link-time optimizations. Santhanam and Odnert <ref> [16] </ref> described an interprocedural register allocation algorithm constructed using the compilers on the HP PA-RISC. More recently, Srivastiva and Wall [20] describe OM, a system for intermodule code optimization.
Reference: [17] <author> Zhong Shao and Andrew W. Appel. </author> <title> Space-efficient closure representations. </title> <booktitle> In Proceedings of the 1994 ACM Conference on LISP and Functional Programming, </booktitle> <pages> pages 150-161. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1994. </year>
Reference-contexts: Also, the conclusions of [4] may not hold for the programs written for systems where garbage collection is rarely used, procedure activations come in a variety of sizes, and continuation passing style is not used. Shao and Appel <ref> [17] </ref> also consider call-graph analysis to reduce the cost of heap-allocated activation records in Standard ML. They reduce the number of frames that need to be allocated and the register save/restore around frame allocation points.
Reference: [18] <author> Jasweinder Pal Singh, Wolf-Dietrich Weber, and Anoop Gupta. </author> <title> Splash: Stanford parallel applications for shared-memory. </title> <type> Technical Report CSL-TR-91-469, </type> <institution> Computer Systems Laboratory, Stanford, </institution> <year> 1991. </year>
Reference-contexts: However , we actually refine the call targets by only considering procedures whose addresses are taken in the program being optimized. cost. The figure shows a call chain ending in a context switch for a particular thread library. The code is taken from MP3D in the SPLASH-I benchmark suite <ref> [18] </ref>. The only live general purpose registers at procedure calls in this particular call chain are registers 9 and 10. These registers are live at the call from reservoir move to stp barrier5.
Reference: [19] <author> Amitabh Srivastava and Alan Eustace. </author> <title> Atom: A system for building customized program analysis tools. </title> <booktitle> In Proceedings of the ACM SIGPLAN '94 Conference on Programming Language Design and Implementation. ACM, ACM, </booktitle> <year> 1994. </year>
Reference-contexts: The number of threads is application dependent since the number of threads that can be used profitably varies for each application. For example, it is not reasonable to use as many as 900 threads in the MP3D application since computation cannot be partitioned that finely. We used ATOM <ref> [19] </ref> and OM [20] to perform the dataflow and call graph analysis necessary to implement the optimizations, but we modified each program by hand because we cannot use OM to modify a program and still instrument it using ATOM. We used ATOM for many of our measurements. <p> Researchers have previously examined interprocedural register analysis. Some of these techniques activity modify the mapping between program variables and registers while others are passive and do not modify how registers are allocated to variables. The ATOM tool <ref> [19] </ref> is an example of a tool using passive global register optimization. In ATOM, the global view of register usage in the program is used to reduce spill code at procedure calls.
Reference: [20] <author> Amitabh Srivastava and David W. Wall. </author> <title> A practical system for intermodule code optimizations at link-time. </title> <journal> Journal of Programming Languages, </journal> <month> March </month> <year> 1992. </year> <note> (Also available as DEC-WRL TR-92-6). </note>
Reference-contexts: We identify recursive call sites by performing a depth-first traversal of each independent graph and determining the `back edges' in the call graph, as shown in Figure 1. Each back edge defines a `procedural loop,' akin to a conventional loop in a control flow graph. Srivastava and Wall <ref> [20] </ref> have shown that it is profitable to `hoist' invariant code from such procedural loops. We recognize that procedural loops indicate indirect or direct recursive subroutine calls and `hoist' stack allocation out of those loops. <p> We have built a link-time optimization pass to reduce the number of callee-saves registers using interprocedural dataflow analysis. We use the same whole program structure and implementation used by Srivastava and Wall <ref> [20] </ref>. The problem here is different than the live register analysis used to perform the interprocedural live-variable analysis described in their work. The context switch optimization requires solving an interprocedural reaching definition problem and an intra-procedural register liveness problem. <p> Therefore, iterating over these equations is guaranteed to converge since one of the out sets for a node in the graph will eventually not change (see [1]). Indirect procedures are handles as in <ref> [20] </ref>, where "potential call edges" are placed between all indirect procedure calls and all possible call targets. However , we actually refine the call targets by only considering procedures whose addresses are taken in the program being optimized. cost. <p> For example, it is not reasonable to use as many as 900 threads in the MP3D application since computation cannot be partitioned that finely. We used ATOM [19] and OM <ref> [20] </ref> to perform the dataflow and call graph analysis necessary to implement the optimizations, but we modified each program by hand because we cannot use OM to modify a program and still instrument it using ATOM. We used ATOM for many of our measurements. <p> We also addressed the implications of cloning on instruction cache behavior. We are not aware of any efforts to take advantage of interpro-cedural register analysis to optimize context switches. The register analysis by Srivastava and Wall <ref> [20] </ref> in OM is most similar and was used as a starting point in carrying out the context switch optimization, although the overall analysis is different since we are solving a bottom up instead of top down interprocedural dataflow problem over a call site graph. <p> Ben-itez and Davidson [5] describe VPO, the very portable optimizer, which was capable of some link-time optimizations. Santhanam and Odnert [16] described an interprocedural register allocation algorithm constructed using the compilers on the HP PA-RISC. More recently, Srivastiva and Wall <ref> [20] </ref> describe OM, a system for intermodule code optimization. The program analysis system we use ATOM, is based on this technique, and we used this system to implement the stack optimization and experiment with cloning in the context switch optimization.
Reference: [21] <author> P.A. Steenkiste and J.L. Hennessy. </author> <title> A simple interprocedural register allocation algorithm and its effectiveness for Lisp. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 11(1) </volume> <pages> 1-32, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: OM also provided an infrastructure to disassemble, modify, and reassemble a linked program binary that we took advantage of. This paper examines the effect of procedure cloning, quantifies the amount of unnecessary spill code, and applies interprocedural register analysis to the optimization of context switches. Wall [22] and Steenkiste <ref> [21] </ref> describe active global register optimizations that re-allocate registers using interprocedural information. This work showed that assigning registers at link time rather than at compile time can result in much better register utilization. Both the storage management and context switch optimization techniques use whole-program or link-time optimizations.
Reference: [22] <author> David W. Wall. </author> <title> Global register allocation at link time. </title> <booktitle> In Proceedings of the ACM SIGPLAN '86 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 264-275. </pages> <publisher> ACM, </publisher> <year> 1986. </year>
Reference-contexts: OM also provided an infrastructure to disassemble, modify, and reassemble a linked program binary that we took advantage of. This paper examines the effect of procedure cloning, quantifies the amount of unnecessary spill code, and applies interprocedural register analysis to the optimization of context switches. Wall <ref> [22] </ref> and Steenkiste [21] describe active global register optimizations that re-allocate registers using interprocedural information. This work showed that assigning registers at link time rather than at compile time can result in much better register utilization. Both the storage management and context switch optimization techniques use whole-program or link-time optimizations.
Reference: [23] <author> David W. Wall and Michael L. Powell. </author> <title> The mahler experience: Using an intermediate language as the machine description. </title> <booktitle> In Proceedings of the 2nd Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 100-104. </pages> <publisher> ACM, </publisher> <month> October </month> <year> 1987. </year>
Reference-contexts: Both the storage management and context switch optimization techniques use whole-program or link-time optimizations. Whole program optimizers are common for functional languages, but less common for imperative languages. Wall and Powell describe the Mahler <ref> [23] </ref> system, which allowed link-time optimizations. Ben-itez and Davidson [5] describe VPO, the very portable optimizer, which was capable of some link-time optimizations. Santhanam and Odnert [16] described an interprocedural register allocation algorithm constructed using the compilers on the HP PA-RISC.
References-found: 23

