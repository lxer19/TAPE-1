URL: http://www.cs.concordia.ca/~faculty/lata/PS/det-sorting.ps.gz
Refering-URL: http://www.cs.concordia.ca/~faculty/lata/papers.html
Root-URL: http://www.cs.concordia.ca
Email: email: dachrao@cs.concordia.ca  email: lata@cs.concordia.ca  
Title: Fast Deterministic Sorting on Large Parallel Machines  
Author: Taoufik Dachraoui Lata Narayanan 
Address: Montreal, Quebec, Canada, H3G 1M8  Montreal, Quebec, Canada, H3G 1M8  
Affiliation: Department of Computer Science Concordia University  Department of Computer Science Concordia University  
Abstract: Many sorting algorithms that perform well on uniformly distributed data suffer significant performance degradation on non-random data. Unfortunately many real-world applications require sorting on data that is not uniformly distributed. In this paper, we consider distributions of varying entropies. We describe A-Ranksort, a new sorting algorithm for parallel machines, whose behavior on input distributions of different entropies is relatively stable. Our algorithm is based on a deterministic strategy to find approximate ranks for all keys. We implemented A-Ranksort, B-Flashsort [10], Radixsort [5], and Bitonic sort [2] on a 2048 processor Maspar MP-1. Our experiments show that A-Ranksort outperforms all the other algorithms on a variety of input distributions, when the output is required to be balanced. We are also able to provide bounds on the average-case and worst-case complexities of our algorithm, in terms of the costs of some chosen primitive operations. The predicted performance is very close to the empirical results, thus justifying our model. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Angluin and L. Valiant. </author> <title> Fast probabilistic algorithms for hamiltonian circuits and matchings. </title> <journal> Journal of Computer and System Science, </journal> <volume> 18 </volume> <pages> 155-193, </pages> <year> 1979. </year>
Reference-contexts: We use the term with high probability to mean with probability greater than 1 1=N c for some constant c. In our analysis, we make use of the following Chernoff bounds on the tails of the binomial distribution <ref> [4, 1] </ref>. Fact 1 (Bernstein-Chernoff bounds) Let S N;p be a random variable having binomial distribution with parameters N and p (S N;p is the sum of N independent Bernoulli variables each with mean p).
Reference: [2] <author> K. Batcher. </author> <title> Sorting networks and their applications. </title> <booktitle> In Pro ceedings of the AFIPS Spring Joint Computer Conference, </booktitle> <pages> pages 307-314, </pages> <year> 1968. </year>
Reference-contexts: Thearling [19] proposes a test suite of inputs with which to evaluate the performance of sorting algorithms. We tested A-Ranksort on a very similar test suite: the results are presented in Section 5. We also implemented B-Flashsort [10], Bitonic sort <ref> [2] </ref>, and Radix sort [5]. In [10], B-Flashsort was shown to outperform all the other algorithms for randomly chosen data.
Reference: [3] <author> G. Blelloch, C. Leiserson, B. Maggs, G. Plaxton, S. Smith, and M. Zagha. </author> <title> A comparison of sorting algorithms for con nection machine CM-2. </title> <booktitle> In Symposium on Parallel Algo rithms and Architecture, </booktitle> <pages> pages 23-16. </pages> <publisher> ACM, </publisher> <year> 1991. </year>
Reference-contexts: Implementations of the algorithm can then confirm the predictive capability of the analytic model. There have been a few such efforts to find the best algorithm for sorting on parallel machines; Blelloch et al. <ref> [3] </ref> provided a quantitative analysis of the circumstances under which different algorithms for sorting on the CM-2 were advantageous, and a similar effort was made by Hightower et al. [10] on Maspar's MP-1, by Dickmann et al. for a 1 Parsytec GCel [7], and by Helman et al. on a number <p> When a key is sent to its destination processor, there is no need to append the array index of the destination to the key message packet, as required in radix sort <ref> [3, 21] </ref>. This is because the keys are not necessarily ranked within segments. This observation reduces the time required for routing keys. 3.3 Phases 3 and 4: Local and final sorting The third phase sorts the keys locally within each proces sor.
Reference: [4] <author> H. Chernoff. </author> <title> A measure of asymptotic efficiency for tests of a hypothesis based on the sum of observations. </title> <journal> Annals of Mathematics and Statistics, </journal> <volume> 23 </volume> <pages> 493-507, </pages> <year> 1952. </year>
Reference-contexts: We use the term with high probability to mean with probability greater than 1 1=N c for some constant c. In our analysis, we make use of the following Chernoff bounds on the tails of the binomial distribution <ref> [4, 1] </ref>. Fact 1 (Bernstein-Chernoff bounds) Let S N;p be a random variable having binomial distribution with parameters N and p (S N;p is the sum of N independent Bernoulli variables each with mean p).
Reference: [5] <author> T. H. Cormen, C. Leiserson, and R. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> MIT Press and McGraw Hill, </publisher> <year> 1990. </year>
Reference-contexts: Thearling [19] proposes a test suite of inputs with which to evaluate the performance of sorting algorithms. We tested A-Ranksort on a very similar test suite: the results are presented in Section 5. We also implemented B-Flashsort [10], Bitonic sort [2], and Radix sort <ref> [5] </ref>. In [10], B-Flashsort was shown to outperform all the other algorithms for randomly chosen data. <p> This observation reduces the time required for routing keys. 3.3 Phases 3 and 4: Local and final sorting The third phase sorts the keys locally within each proces sor. We use the standard serial radix sort in which each pass is implemented using a counting sort <ref> [5] </ref>. Radix sort was used in our implementation, since it is significantly faster than comparison sorts such as quicksort. In Phase 4 the algorithm performs at most dM=(N=p)e + 1 steps of odd-even transposition sort on the overall Key ar ray in row-major order.
Reference: [6] <author> T. Dachraoui and L. Narayanan. </author> <title> Fast deterministic sorting on large parallel machines. </title> <type> Technical report, </type> <institution> Concordia Uni versity, </institution> <year> 1996. </year>
Reference-contexts: RANKS procedure. For a more detailed explanation of this procedure, see <ref> [6] </ref>. For completeness, we include here the pseudocode for the procedure Compute-Ranks in Figure 3. The algorithm will eventually rank all the keys, so long as at least one new bit of the keys is processed in every iteration. <p> S 454 Time for scan operation Table 1. Time for different operations on Mas-par MP-1 such bounds, empirical observations show that regardless of the entropy of the distribution, the number of iterations is usually relatively small. The reader is referred to <ref> [6] </ref> for a discussion of this issue. 4.4 Space requirements The total space requirements of the algorithm can be summarized as follows (for an explanation, see [6]): Type of variable Array size (per processor) Key type N=p log N bits 2 r + 2 q r bits N=p 5 Empirical results <p> The reader is referred to <ref> [6] </ref> for a discussion of this issue. 4.4 Space requirements The total space requirements of the algorithm can be summarized as follows (for an explanation, see [6]): Type of variable Array size (per processor) Key type N=p log N bits 2 r + 2 q r bits N=p 5 Empirical results All our results and graphs are for 32-bit keys on uniformly distributed data where N=p = 1024, unless specified otherwise.
Reference: [7] <author> R. Diekmann, J. Gehring, R. Luling, B. Monien, M. Nubel, and R. Wanka. </author> <title> Sorting large data sets on a massively parallel system. </title> <booktitle> In Proceedings of the Symposium on Parallel and Distributed Processing, </booktitle> <pages> pages 2-9. </pages> <publisher> IEEE, </publisher> <year> 1994. </year>
Reference-contexts: for sorting on parallel machines; Blelloch et al. [3] provided a quantitative analysis of the circumstances under which different algorithms for sorting on the CM-2 were advantageous, and a similar effort was made by Hightower et al. [10] on Maspar's MP-1, by Dickmann et al. for a 1 Parsytec GCel <ref> [7] </ref>, and by Helman et al. on a number of platforms [9]. In this paper, we propose a new strategy for sorting. Our method essentially consists of computing approximate ranks for elements, and using these to route to destinations that are guaranteed to be close to the final destination.
Reference: [8] <author> A. Dusseau. </author> <title> Modeling parallel sorts with LogP on the CM-5. </title> <type> Technical Report UCB//CSD-94-829, </type> <institution> University of California-Berkeley, </institution> <year> 1994. </year>
Reference-contexts: We have explained above that our algorithm does not perform well on all initial input orders. It should be noted that there are initial orders for which Radix sort has very bad performance as well <ref> [8] </ref>, though it performs well on all the 6 distributions in our test suite.
Reference: [9] <author> D. R. Helman, D. A. Bader, and J. JaJa. </author> <title> A practical sorting algorithm with an experimental study. </title> <type> Technical Report 95 102, UMIACS, </type> <year> 1995. </year>
Reference-contexts: quantitative analysis of the circumstances under which different algorithms for sorting on the CM-2 were advantageous, and a similar effort was made by Hightower et al. [10] on Maspar's MP-1, by Dickmann et al. for a 1 Parsytec GCel [7], and by Helman et al. on a number of platforms <ref> [9] </ref>. In this paper, we propose a new strategy for sorting. Our method essentially consists of computing approximate ranks for elements, and using these to route to destinations that are guaranteed to be close to the final destination.
Reference: [10] <author> W. Hightower, J. Prins, and J. Reif. </author> <title> Implementations of ran domized sorting on large parallel machines. </title> <booktitle> In Symposium on Parallel Algorithms and Architecture, </booktitle> <pages> pages 158-167. </pages> <publisher> ACM, </publisher> <year> 1992. </year>
Reference-contexts: There have been a few such efforts to find the best algorithm for sorting on parallel machines; Blelloch et al. [3] provided a quantitative analysis of the circumstances under which different algorithms for sorting on the CM-2 were advantageous, and a similar effort was made by Hightower et al. <ref> [10] </ref> on Maspar's MP-1, by Dickmann et al. for a 1 Parsytec GCel [7], and by Helman et al. on a number of platforms [9]. In this paper, we propose a new strategy for sorting. <p> Our algorithm was implemented on a 2048-processor Maspar machine MP-1. Thearling [19] proposes a test suite of inputs with which to evaluate the performance of sorting algorithms. We tested A-Ranksort on a very similar test suite: the results are presented in Section 5. We also implemented B-Flashsort <ref> [10] </ref>, Bitonic sort [2], and Radix sort [5]. In [10], B-Flashsort was shown to outperform all the other algorithms for randomly chosen data. <p> Thearling [19] proposes a test suite of inputs with which to evaluate the performance of sorting algorithms. We tested A-Ranksort on a very similar test suite: the results are presented in Section 5. We also implemented B-Flashsort <ref> [10] </ref>, Bitonic sort [2], and Radix sort [5]. In [10], B-Flashsort was shown to outperform all the other algorithms for randomly chosen data.
Reference: [11] <author> C. Kakalamanis and D. Krizanc. </author> <title> Optimal sorting on mesh connected processor arrays. </title> <booktitle> In Symposium on Parallel Algo rithms and Architecture, </booktitle> <pages> pages 50-59, </pages> <year> 1992. </year>
Reference-contexts: The first Q ( p p) sorting algorithms on the mesh were due to Thomson and Kung [22] and Schnorr and Shamir [17]. Since then, there have been several new algorithms for sorting on the mesh <ref> [12, 15, 11, 13] </ref>, each reducing the multiplicative constant in the leading term. Some of these algorithms are optimal in the sense that they can be proved to require less than 2 p p p) communication steps, when each processor contains a single input element. <p> In this paper, we propose a new strategy for sorting. Our method essentially consists of computing approximate ranks for elements, and using these to route to destinations that are guaranteed to be close to the final destination. While this general approach has been used in previous algorithms <ref> [11] </ref>, our method is quite different. The main idea of our algorithm is to divide the keys into small segments that are ranked relative to each other. The actual ranks within the segments will be calculated at the end. The calculation of these approximate ranks is done deterministically.
Reference: [12] <author> C. Kaklamanis, D. Krizanc, L. Narayanan, and A. </author> <title> Tsanti las. Randomized sorting and selection on mesh-connected processor arrays. </title> <booktitle> In Symposium on Parallel Algorithms and Architecture, </booktitle> <pages> pages 17-28, </pages> <year> 1991. </year>
Reference-contexts: The first Q ( p p) sorting algorithms on the mesh were due to Thomson and Kung [22] and Schnorr and Shamir [17]. Since then, there have been several new algorithms for sorting on the mesh <ref> [12, 15, 11, 13] </ref>, each reducing the multiplicative constant in the leading term. Some of these algorithms are optimal in the sense that they can be proved to require less than 2 p p p) communication steps, when each processor contains a single input element.
Reference: [13] <author> M. Kaufmann, J. Sibeyn, and T. Suel. </author> <title> Derandomizing algo rithms for routing and sorting on meshes. </title> <booktitle> In Symposium on Discrete Algorithms. ACM-SIAM, </booktitle> <year> 1994. </year>
Reference-contexts: The first Q ( p p) sorting algorithms on the mesh were due to Thomson and Kung [22] and Schnorr and Shamir [17]. Since then, there have been several new algorithms for sorting on the mesh <ref> [12, 15, 11, 13] </ref>, each reducing the multiplicative constant in the leading term. Some of these algorithms are optimal in the sense that they can be proved to require less than 2 p p p) communication steps, when each processor contains a single input element.
Reference: [14] <author> D. Knuth. </author> <title> The art of Computer programming, Vol. 3 (Sorting and Searching. </title> <publisher> Addison-Wesley, </publisher> <year> 1973. </year>
Reference-contexts: 1 Introduction Sorting is one of the best-studied problems in computer science. An early but exhaustive compilation of sorting algorithms and their behavior can be found in <ref> [14] </ref>. The reason for the continuing interest in sorting is evident when we consider the wide variety of applications to this problem, ranging from database management to computational geometry. Sorting has been studied widely in the parallel computation setting as well.
Reference: [15] <author> M. Kunde. </author> <title> Concentrated regular data streams on grids: Sorting and routing near to the bisection bound. </title> <booktitle> In Symposium on the Foundations of Computer Science, </booktitle> <pages> pages 141-150. </pages> <publisher> IEEE, </publisher> <year> 1991. </year>
Reference-contexts: The first Q ( p p) sorting algorithms on the mesh were due to Thomson and Kung [22] and Schnorr and Shamir [17]. Since then, there have been several new algorithms for sorting on the mesh <ref> [12, 15, 11, 13] </ref>, each reducing the multiplicative constant in the leading term. Some of these algorithms are optimal in the sense that they can be proved to require less than 2 p p p) communication steps, when each processor contains a single input element.
Reference: [16] <author> F. T. Leighton. </author> <title> Introduction to Parallel Algorithms and Ar chitectures: Arrays, Trees and Hypercubes. </title> <publisher> Morgan Kauf mann, </publisher> <year> 1992. </year>
Reference-contexts: Therefore, M steps of odd-even transposition sort on the entire sequence suffice to sort the input. (The description of odd-even transposition sort and the above claim of its behavior are omitted here. Details can be found in <ref> [16] </ref>). 2 It remains to determine the number of iterations taken by the loop in the procedure Count-Ranks.
Reference: [17] <author> C. Schnorr and A. Shamir. </author> <title> An optimal sorting algorithm for mesh-connected computers. </title> <booktitle> In Symposium on the Theory of Computation, </booktitle> <pages> pages 255-263, </pages> <year> 1986. </year>
Reference-contexts: Clearly, sorting on a two-dimensional p processor square mesh requires W ( p p) communication steps. The first Q ( p p) sorting algorithms on the mesh were due to Thomson and Kung [22] and Schnorr and Shamir <ref> [17] </ref>. Since then, there have been several new algorithms for sorting on the mesh [12, 15, 11, 13], each reducing the multiplicative constant in the leading term.
Reference: [18] <author> C. Shannon and W. Weaver. </author> <title> The Mathematical Theory of Communication. </title> <publisher> University of Illinois Press, </publisher> <year> 1949. </year>
Reference-contexts: Then, for any h such that 0 h 1:8N p, 2 One technique to characterize the distribution of data is to measure the entropy of the data distribution. The Shan-non entropy of a distribution <ref> [18] </ref> is defined as Sp i j log p i j where p i is the probability of symbol i occurring in the distribution. The entropy of the key distribution thus specifies the number of unique bits in the key.
Reference: [19] <author> K. Thearling. </author> <title> An evaluation of sorting as a supercomputing benchmark. </title> <type> Technical report. </type>
Reference-contexts: When these ranks are computed, keys are sent to approximate destinations which are guaranteed to be close to the final destinations. No keys are moved until the approximate ranks of keys are calculated. Our algorithm was implemented on a 2048-processor Maspar machine MP-1. Thearling <ref> [19] </ref> proposes a test suite of inputs with which to evaluate the performance of sorting algorithms. We tested A-Ranksort on a very similar test suite: the results are presented in Section 5. We also implemented B-Flashsort [10], Bitonic sort [2], and Radix sort [5]. <p> For example, the graph suggests that for N=p 128, the value of r that would minimize the running time would be 7 and similarly for N=p &gt; 128, the value r = 9 could be the best choice of r. Thearling <ref> [19] </ref> proposes a number of benchmarks to study the performance of sorting algorithms on different types of input distributions, key sizes, numbers of keys, and different allocation distributions. Figure 7 shows the performance of our algorithm on input distributions with varying entropies.
Reference: [20] <author> K. Thearling and S. Smith. </author> <title> An imporved supercomputing benchmark. </title> <booktitle> In Supercomputing, </booktitle> <year> 1992. </year>
Reference: [21] <author> K. Thearling and S. Smith. </author> <title> An improved supercomputing sorting benchmark. </title> <booktitle> In Supercomputing. ACM, </booktitle> <year> 1992. </year>
Reference-contexts: Our algorithm improves slightly on the performance of B-Flashsort for uniformly distributed data, for the case when the sorting is required to be balanced, that is, when each processor is required to contain the same number of keys at the end. However, few real-world applications involve uniformly distributed data <ref> [21] </ref>, and algorithms that work well on random data often perform poorly on non-uniform data. Our experiments show that on many input distributions, the space requirements of B-Flashsort simply exceed system limits. Even if the space limits were higher, the running time of B-Flashsort degrades significantly on many input distributions. <p> The entropy of the key distribution thus specifies the number of unique bits in the key. We define the sparse data distribution to have keys chosen uniformly at random from among 16 different keys. Following <ref> [21] </ref>, we call the Sparse/random distribution to be the distribution that has 1% of keys chosen uniformly at random from the set of all 32-bit keys, and the remaining keys chosen from the sparse distribution defined above. We consider four main types of primitive operations provided by the Maspar. <p> When a key is sent to its destination processor, there is no need to append the array index of the destination to the key message packet, as required in radix sort <ref> [3, 21] </ref>. This is because the keys are not necessarily ranked within segments. This observation reduces the time required for routing keys. 3.3 Phases 3 and 4: Local and final sorting The third phase sorts the keys locally within each proces sor.
Reference: [22] <author> C. Thompson and H. Kung. </author> <title> Sorting on a mesh connected parallel computer. </title> <journal> Communications of the ACM, </journal> <volume> 20:263 270, </volume> <year> 1977. </year> <title> for A-Ranksort different algorithms on uniformly distributed data 8 </title>
Reference-contexts: Clearly, sorting on a two-dimensional p processor square mesh requires W ( p p) communication steps. The first Q ( p p) sorting algorithms on the mesh were due to Thomson and Kung <ref> [22] </ref> and Schnorr and Shamir [17]. Since then, there have been several new algorithms for sorting on the mesh [12, 15, 11, 13], each reducing the multiplicative constant in the leading term.
References-found: 22

