URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3457/3457.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Email: wak@cs.umd.edu pugh@cs.umd.edu  ejr@cs.umd.edu murka@cs.umd.edu  
Title: Transitive Closure of Infinite Graphs and its Applications  
Author: Wayne Kelly William Pugh Evan Rosser Tatiana Shpeisman 
Note: This work is supported by an NSF PYI grant CCR-9157384 and by a Packard Fellowship.  
Address: College Park, MD 20742  
Affiliation: Dept. of Computer Science Institute for Advanced Computer Studies Dept. of Computer Science  Dept. of Computer Science Dept. of Computer Science Univ. of Maryland,  
Date: April, 1994  
Pubnum: UMIACS-TR-95-48  CS-TR-3457  
Abstract: Integer tuple relations can concisely summarize many types of information gathered from analysis of scientific codes. For example they can be used to precisely describe which iterations of a statement are data dependent of which other iterations. It is generally not possible to represent these tuple relations by enumerating the related pairs of tuples. For example, it is impossible to enumerate the related pairs of tuples in the relation f[i] ! [i + 2] j 1 i n 2 g. Even when it is possible to enumerate the related pairs of tuples, such as for the relation f[i; j] ! [i 0 ; j 0 ] j 1 i; j; i 0 ; j 0 100 g, it is often not practical to do so. We instead use a closed form description by specifying a predicate consisting of affine constraints on the related pairs of tuples. As we just saw, these affine constraints can be parameterized, so what we are really describing are infinite families of relations (or graphs). Many of our applications of tuple relations rely heavily on an operation called transitive closure. Computing the transitive closure of these "infinite graphs" is very different from the traditional problem of computing the transitive closure of a graph whose edges can be enumerated. For example, the transitive closure of the first relation above is the relation f [i] ! [i 0 ] j 9fi s:t: i 0 i = 2fi ^ 1 i i 0 n g. As we will prove, this computation is not computable in the general case. We have developed algorithms that produce exact results in most commonly occurring cases and produce upper or lower bounds (as necessary) in the other cases. This paper will describe our algorithms for computing transitive closure and some of its applications such as determining which inter-processor synchronizations are redundant. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Ding-Kai Chen. </author> <title> Compiler Optimizations for Parallel Loops With Fine-Grained Synchronization. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, U. of Illinois at Urbana-Champaign, </institution> <year> 1994. </year> <note> Also available as CSRD Report 1374. </note>
Reference-contexts: So, the dependences that we do have to explicitly synchronize are d d 2+ . Note that this is equivalent to computing the transitive reduction of d. An example of the technique is presented in Figure 3, an example from <ref> [1] </ref>. In cases where more complex dependence relations cause the transitive closure calculation to be inexact, we can still produce useful results. We can safely subtract a lower bound on the 2+ closure from the dependences and still produce correct (but perhaps conservative) synchronization. <p> Example of non-constant dependence distances and partial redundancy distances (such as (1,-2)) can result in non-uniformly redundant synchronizations <ref> [1] </ref>. <p> So, any redundancy that we find based on this stronger requirement can be safely eliminated. Related work <ref> [6, 7, 1] </ref> considers the case of synchronization between statements with methods similar to the simple case. All of the methods build an explicit graph of a subset of the iteration space, with each node representing an iteration of a statement. <p> All of the methods build an explicit graph of a subset of the iteration space, with each node representing an iteration of a statement. Redundancy is found either by searching the graph <ref> [1] </ref> or using transitive closure of the graph [6, 7]; dependences are restricted to constant distances; and the problem regarding boundary cases still exists. These methods search a small graph which finds all redundancy when nesting level is 2, but may miss some redundancy when the nesting level is greater [1]. <p> <ref> [1] </ref> or using transitive closure of the graph [6, 7]; dependences are restricted to constant distances; and the problem regarding boundary cases still exists. These methods search a small graph which finds all redundancy when nesting level is 2, but may miss some redundancy when the nesting level is greater [1]. None of the above methods consider non-perfectly nested loops, and they do not use information regarding distribution. <p> ; i 0 1; 20i 0 + 2j 0 20] j 2 i 0 n ^ 1 j 0 10g [ 1 q = 0 3 for i = 1 to n 5 q = q + 2 7 p = i R2 = f [i; j; p; q] ! <ref> [i; 1; p; q] </ref> j i ng R4 = f [i; j; p; q] ! [i; j + 1; p; q]g R6 = f [i; j; p; q] ! [i; j; p; q] i &gt; ng Fig. 8.
Reference: 2. <author> Wayne Kelly, Vadim Maslov, William Pugh, Evan Rosser, Tatiana Shpeisman, and David Wonnacott. </author> <title> The omega library interface guide. </title> <type> Technical Report CS-TR-3445, </type> <institution> Dept. of Computer Science, University of Maryland, College Park, </institution> <month> March </month> <year> 1995. </year>
Reference-contexts: The implementation of these operations is described elsewhere <ref> [2] </ref> (see also http://www.cs.umd.ed u/projects/omega or ftp://ftp.cs.umd.edu/pub/omega) In addition to these operations we have also implemented and use in our applications the transitive closure operator: x ! z 2 F fl , x = z _ 9y s:t: x ! y 2 F ^ y ! z 2 F fl and
Reference: 3. <author> Wayne Kelly and William Pugh. </author> <title> A framework for unifying reordering transformations. </title> <type> Technical Report CS-TR-3193, </type> <institution> Dept. of Computer Science, University of Maryland, College Park, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: The above program has dependence distance (0), but that doesn't tell us that only the last iteration of j loop is involved in the dependence. This type of additional information is crucial for determining the legality of a number of advanced transformations <ref> [3] </ref>. Tuple relations can also be used to represent other forms of ordering constraints between iterations that don't necessarily correspond to data dependences. For example, we can construct relations that represent which iterations will be executed before which other iterations.
Reference: 4. <author> Wayne Kelly and William Pugh. </author> <title> Finding legal reordering transformations using mappings. </title> <booktitle> In Lecture Notes in Computer Science 892: Seventh International Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Ithaca, NY, </address> <month> August </month> <year> 1994. </year> <note> Springer-Verlag. </note>
Reference-contexts: _ 9y s:t: x ! y 2 F ^ y ! z 2 F fl and positive transitive closure operator: x ! z 2 F + , x ! z 2 F _ 9y s:t: x ! y 2 F ^ y ! z 2 F + In previous work <ref> [4] </ref>, we developed algorithms for a closely related operation called affine closure. Affine closure is well suited to testing the legality of reordering transformations and is generally easier to compute than transitive closure. But many of our applications require the full generality of transitive closure.
Reference: 5. <author> V.P. Krothapalli and P. Sadayappan. </author> <title> Removal of redundant dependences in DOACROSS loops with constant dependences. </title> <booktitle> In Proc. of the 3rd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 51-60, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: This is the class of problems considered by some related work <ref> [5] </ref> in this area. We will show how our approach improves on the related work in this limited domain, then in Section 3.3, we will show how to extend the approach to the more general problem. <p> Related work builds an explicit graph of a subset of the iteration space, with each node representing an iteration of the loop body, and each edge representing a dependence <ref> [5] </ref>. Redundancy is found either through taking the transitive closure or reduction of this graph, or using algorithms that search a subgraph starting at the first iteration. <p> Figure 5 shows an example of finding an alternate path to handle the boundary cases. Methods that search a small graph, but which may miss some redundancy when nesting is greater than 2 have been developed <ref> [5] </ref>. Fig. 5. Finding alternate paths at boundaries; (3,0) is redundant when n &gt; 1 Because we start with more precise dependence information, we do not have the same problem. No out-of-bounds iteration is in the range or domain of any dependence relation.
Reference: 6. <author> S.P. Midkiff and D.A. Padua. </author> <title> Compiler algorithm for synchronization. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-36(12):1485-1495, </volume> <year> 1987. </year>
Reference-contexts: So, any redundancy that we find based on this stronger requirement can be safely eliminated. Related work <ref> [6, 7, 1] </ref> considers the case of synchronization between statements with methods similar to the simple case. All of the methods build an explicit graph of a subset of the iteration space, with each node representing an iteration of a statement. <p> All of the methods build an explicit graph of a subset of the iteration space, with each node representing an iteration of a statement. Redundancy is found either by searching the graph [1] or using transitive closure of the graph <ref> [6, 7] </ref>; dependences are restricted to constant distances; and the problem regarding boundary cases still exists. These methods search a small graph which finds all redundancy when nesting level is 2, but may miss some redundancy when the nesting level is greater [1].
Reference: 7. <author> S.P. Midkiff and D.A. Padua. </author> <title> A comparison of four synchronization optimization techniques. </title> <booktitle> In Proc. 1991 IEEE International Conf. on Parallel Processing, </booktitle> <address> pages II-9 II-16, </address> <month> August </month> <year> 1991. </year> <title> This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: So, any redundancy that we find based on this stronger requirement can be safely eliminated. Related work <ref> [6, 7, 1] </ref> considers the case of synchronization between statements with methods similar to the simple case. All of the methods build an explicit graph of a subset of the iteration space, with each node representing an iteration of a statement. <p> All of the methods build an explicit graph of a subset of the iteration space, with each node representing an iteration of a statement. Redundancy is found either by searching the graph [1] or using transitive closure of the graph <ref> [6, 7] </ref>; dependences are restricted to constant distances; and the problem regarding boundary cases still exists. These methods search a small graph which finds all redundancy when nesting level is 2, but may miss some redundancy when the nesting level is greater [1]. <p> None of the above methods consider non-perfectly nested loops, and they do not use information regarding distribution. One previous technique has such the ability to generate the conditions under which a non-uniformly redundant dependence must be enforced <ref> [7] </ref>, but the authors indicate that their technique may require taking transitive closure of a large subset of the iteration space. 3.4 Induction variables Tuple relations and the transitive closure operation can also be used to compute closed form expressions for induction variables.
References-found: 7

