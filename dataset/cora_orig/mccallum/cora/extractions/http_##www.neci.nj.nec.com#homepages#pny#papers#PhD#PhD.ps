URL: http://www.neci.nj.nec.com/homepages/pny/papers/PhD/PhD.ps
Refering-URL: http://www.neci.nj.nec.com/homepages/pny/papers/PhD/main.html
Root-URL: 
Title: TOPICS IN COMPUTATIONAL HIDDEN STATE MODELING  
Author: Peter N. Yianilos 
Degree: A DISSERTATION PRESENTED TO THE FACULTY  IN CANDIDACY FOR THE DEGREE OF DOCTOR OF PHILOSOPHY RECOMMENDED FOR ACCEPTANCE BY THE DEPARTMENT OF COMPUTER SCIENCE  
Date: June 1997  
Affiliation: OF PRINCETON UNIVERSITY  
Abstract-found: 0
Intro-found: 1
Reference: [Act70] <author> Forman S. Acton. </author> <title> Numerical Methods that Work. </title> <publisher> Harper and Row, </publisher> <year> 1970. </year>
Reference-contexts: Its virtue is that the particular density may then be viewed as a data type referred to by a single optimization algorithm. A particularly simple numerical prescription for optimization is that of Powell (see <ref> [Act70, Bre73, PFTV88] </ref>). Speed of convergence is traded for simplicity in that no gradient computation is required. Emphasis reparameterization does however introduce one wrinkle. The number of parameters can easily exceed the degrees of freedom in the underlying problem.
Reference: [Bau72] <author> L. E. Baum. </author> <title> An inequality and associated maximization technique in statistical estimation of probabilistic functions of Markov processes. </title> <journal> Inequalities, </journal> <volume> 3 </volume> <pages> 1-8, </pages> <year> 1972. </year>
Reference: [BBS88] <author> H. A. P. Blom and Y. Bar-Shalom. </author> <title> The interacting multiple model algorithm for systems with markovian switching coefficients. </title> <journal> IEEE Trans. Automatic Control, </journal> <pages> pages 780-783, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: That is, they depend on the context c and the mixture stochastically selects a component based on the context. If each component is a normal density with fixed parameters, the IMM technique for adaptive estimation and tracking results <ref> [BSL93, BBS88] </ref>. From this perspective, our interest is in recovering the model's parameters from observations.
Reference: [BE67] <author> L. E. Baum and J. E. Eagon. </author> <title> An inequality with application to statistical estimation for probabalistic functions of a Markov process and to models for ecology. </title> <journal> Bull. AMS, </journal> <volume> 73 </volume> <pages> 360-363, </pages> <year> 1967. </year>
Reference-contexts: Section 5.3 shows that such search problems may almost always be carried out using reparameterizations we refer to as emphasis methods that are motivated by the CHAPTER 5. EMPHASIS REPARAMETERIZATION 91 simple and intuitive expectation maximization (EM) algorithm <ref> [BE67, DLR77, RW84] </ref> for unsupervised maximum likelihood (ML) parameter estimation. The reparameter-ized problem cannot express an arbitrary mixture but nevertheless, because of degeneracy, can express a mixture that induces optimal class functions with respect to the prediction of training labels.
Reference: [Ber79] <author> J. Berstel. </author> <title> Transductions and Context-Free Languages. </title> <publisher> Teubner, </publisher> <address> Stut-tgart, </address> <year> 1979. </year>
Reference-contexts: We remark, however, that useful models may exist that violate this assumption and therefore generate values that without normalization do not represent probabilities. CHAPTER 2. FINITE GROWTH MODELS 44 2.5 Stochastic Transducers The notion of transduction has its roots in classical formal language theory (e.g. <ref> [Ber79] </ref>) and represents the formalization of the idea of a machine that converts an input sequence into an output sequence. Later work within the syntactic pattern recognition outlook addressed the induction problem for a particular subclass of finite transducer [OGV93].
Reference: [BF96] <author> Y. Bengio and P. Frasconi. </author> <title> Input/output hmms for sequence processing. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 7(5), </volume> <year> 1996. </year>
Reference-contexts: Later work within the syntactic pattern recognition outlook addressed the induction problem for a particular subclass of finite transducer [OGV93]. More recently a stochastic approach was taken in <ref> [BF96] </ref> where hidden Markov models were used to capture an input-output relationship between synchronous time series. Finite growth models can be used to derive Baum-Welch/EM based learning algorithms for stochastic transducers. This interesting class of stochastic models capture in a somewhat general way the relationship between dependent symbol streams.
Reference: [BGT93] <author> C. Berrou, A. Glavieux, and P. Thitimajshima. </author> <title> Near shannon limit error-correcting coding and decoding: Turbo codes (1). </title> <booktitle> In Proc. ICC'93, </booktitle> <pages> pages 1064-1070, </pages> <address> Geneva, Switzerland, </address> <year> 1993. </year>
Reference-contexts: The relationship between these networks and hidden Markov models is elucidated in [SHJ96]. Exploring the relationship of FGMs to error correcting codes such as those based on trellises, and the recently developed class of turbo codes <ref> [BGT93] </ref>, is a subject for future work. We wonder broadly whether our framework's generality might lead to better codes, and in particular whether DAGs that encode nonlinear time orderings might have value. Such a possibility is suggested by [WLK95, Wib96].
Reference: [BPSW70] <author> L. E. Baum, T. Petrie, G. Soules, and N. Weiss. </author> <title> A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains. </title> <journal> Ann. Math Stat., </journal> <volume> 41 </volume> <pages> 164-171, </pages> <year> 1970. </year> <note> 127 BIBLIOGRAPHY 128 </note>
Reference-contexts: A trivial reduction to FGMs results in a simple iterative algorithm. This problem is considered in [Cov84] where the same algorithm, essentially a rediscovery of Baum-Welch/EM, is described. The multiplicative update perspective of [Cov84] is essentially the growth transform of <ref> [BPSW70] </ref>. We describe this problem using the notation of [Cov84] and reduce it to an FGM. A stock which appreciates by 25% in say one month, is said to have a return of 1:25. CHAPTER 2.
Reference: [Bre73] <author> Richard P. Brent. </author> <title> Algorithms for Minimization without Derivatives. </title> <publisher> Prentice-Hall, </publisher> <year> 1973. </year>
Reference-contexts: Its virtue is that the particular density may then be viewed as a data type referred to by a single optimization algorithm. A particularly simple numerical prescription for optimization is that of Powell (see <ref> [Act70, Bre73, PFTV88] </ref>). Speed of convergence is traded for simplicity in that no gradient computation is required. Emphasis reparameterization does however introduce one wrinkle. The number of parameters can easily exceed the degrees of freedom in the underlying problem.
Reference: [Bro87] <author> Peter F. Brown. </author> <title> Acoustic-phonetic modeling problem in automatic speech recognition. </title> <type> PhD thesis, </type> <institution> Carnegie-Mellon University, </institution> <year> 1987. </year>
Reference-contexts: Formally the observation models are then conditional probability functions where the conditioning is on earlier observation components notearlier edge sequences. Each source-sink path then corresponds to a causal chain of conditional probabilities so that the FGM's value is a probability. Brown observes in his thesis <ref> [Bro87] </ref> that the HMM output independence assumption may be relaxed to allow generation of the current speech sample window to depend on the previous one. He then uses Baum-Welch reestimation without reproof. <p> Each constituent normal density is referred to as a component of the mixture, and m 1 ; : : : ; m k are the mixing coefficients. Normal mixtures have proven useful in several areas including pattern recognition [DH73] and speech recognition <ref> [RJLS85, Bro87, HAJ90] </ref> along with vector quantization and many others. <p> That is, to find a model M which maximizes: n Y p (!(s i )js i ; M ) where !(s i ) is the class label associated with sample s i . This is sometimes called the maximum mutual information (MMI) criterion in the speech recognition literature <ref> [Bro87] </ref>. We remark that more general forms of this problem may be stated in which the labels are themselves probabilities. Sections 5.3 and 5.4 consider the question of whether emphasis-like reparameterizations may be used to attack the more general form of this problem.
Reference: [BSL93] <author> Yaakov Bar-Shalom and Xiao-Rong Li. </author> <title> Estimation and Tracking: </title> <booktitle> Principles, Techniques, and Software, chapter 11, </booktitle> <pages> pages 461-465. </pages> <address> Artech House, Boston, London, </address> <year> 1993. </year>
Reference-contexts: That is, they depend on the context c and the mixture stochastically selects a component based on the context. If each component is a normal density with fixed parameters, the IMM technique for adaptive estimation and tracking results <ref> [BSL93, BBS88] </ref>. From this perspective, our interest is in recovering the model's parameters from observations.
Reference: [CJ93] <author> Rama Chellappa and Anil Jain, </author> <title> editors. Markov Random Fields: Theory and Application. </title> <publisher> Academic Press, </publisher> <year> 1993. </year>
Reference-contexts: If the causality restriction is discarded each pixel's model is strengthened but their product is no longer a probability. This corresponds to relaxing the requirement that the projection functions in an SFGM correspond to a permutation of the observation dimensions. Noncausal neighborhood systems have proven useful in image processing <ref> [CJ93] </ref>. Since reestimation must nevertheless climb we have the result that even a noncausal unnormalized models like that sketched above may be improved within the FGM framework. 2.4.6 Dynamic Choice Function As remarked earlier we might have allowed choice functions to depend on the observation x.
Reference: [Cov84] <author> Thomas M. </author> <title> Cover. An algorithm for maximizing expected log investment return. </title> <journal> IEEE Transactions on Information Theory, </journal> <year> 1984. </year>
Reference-contexts: A trivial reduction to FGMs results in a simple iterative algorithm. This problem is considered in <ref> [Cov84] </ref> where the same algorithm, essentially a rediscovery of Baum-Welch/EM, is described. The multiplicative update perspective of [Cov84] is essentially the growth transform of [BPSW70]. We describe this problem using the notation of [Cov84] and reduce it to an FGM. <p> A trivial reduction to FGMs results in a simple iterative algorithm. This problem is considered in <ref> [Cov84] </ref> where the same algorithm, essentially a rediscovery of Baum-Welch/EM, is described. The multiplicative update perspective of [Cov84] is essentially the growth transform of [BPSW70]. We describe this problem using the notation of [Cov84] and reduce it to an FGM. A stock which appreciates by 25% in say one month, is said to have a return of 1:25. CHAPTER 2. <p> A trivial reduction to FGMs results in a simple iterative algorithm. This problem is considered in <ref> [Cov84] </ref> where the same algorithm, essentially a rediscovery of Baum-Welch/EM, is described. The multiplicative update perspective of [Cov84] is essentially the growth transform of [BPSW70]. We describe this problem using the notation of [Cov84] and reduce it to an FGM. A stock which appreciates by 25% in say one month, is said to have a return of 1:25. CHAPTER 2. FINITE GROWTH MODELS 64 The returns of each stock over time period i forms a vector denoted X i . <p> This matches the equation given in <ref> [Cov84] </ref>. <p> Instead we turn to a concrete example loosely motivated by the problem of portfolio optimization as framed in <ref> [Cov84] </ref> and discussed in chapter 2, in order to illustrate the mathematical point above.
Reference: [CT91] <author> Thomas M. Cover and Joy A. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> Wiley, </publisher> <year> 1991. </year>
Reference: [DH73] <author> Richard O. Duda and Peter E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> John Wiley & Sons, Inc., </publisher> <year> 1973. </year>
Reference-contexts: Each constituent normal density is referred to as a component of the mixture, and m 1 ; : : : ; m k are the mixing coefficients. Normal mixtures have proven useful in several areas including pattern recognition <ref> [DH73] </ref> and speech recognition [RJLS85, Bro87, HAJ90] along with vector quantization and many others. <p> A convenient visualization of these a posteriori class functions, focuses on decision boundaries, i.e. the surfaces along which classification is ambiguous. Imagery like ours in figure 11, and pages 28-31 of <ref> [DH73] </ref>, suggest an intuitive relationship between mixture component locations, and the resulting a posteriori class structure and decision surfaces. One imagines each mean to be asserting ownership over some volume of space surrounding it.
Reference: [DLR77] <author> A. P. Dempster, N. M. Laird, and D. B. Rubin. </author> <title> Maximum-likelihood from incomplete data via the EM algorithm. </title> <journal> J. Royal Statistical Society Ser. B (methodological), </journal> <volume> 39 </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference-contexts: The result is a conceptually clear and computationally tractable approach to implementing more sophisticated estimation schemes for both existing and new stochastic models. That EM could accomplish this in general was observed in <ref> [DLR77] </ref> but these penalized likelihood functions do not appear to have significantly impacted the use of EM in practice. We consider two related approaches: maximum a posteriori probability (MAP) estimation, and minimum description length (MDL). Our discussion begins with MAP. <p> This objective is recognized as an instance of the ML mixture density/function parameter estimation problem and we apply the expectation maximization (EM) <ref> [DLR77] </ref> as discussed in chapter 2. Our implementation actually models the product p (`jw; )p (wj) as a single joint probability p (`; wj). The initial parameters for this mixture and all edit distance costs are uniform. <p> Section 5.3 shows that such search problems may almost always be carried out using reparameterizations we refer to as emphasis methods that are motivated by the CHAPTER 5. EMPHASIS REPARAMETERIZATION 91 simple and intuitive expectation maximization (EM) algorithm <ref> [BE67, DLR77, RW84] </ref> for unsupervised maximum likelihood (ML) parameter estimation. The reparameter-ized problem cannot express an arbitrary mixture but nevertheless, because of degeneracy, can express a mixture that induces optimal class functions with respect to the prediction of training labels.
Reference: [FJM80] <author> F F. Jelinek and R. L. Mercer. </author> <title> Interpolated estimation of Markov source parameters from sparse data. </title> <editor> In E. S. Gelsema and L. N. Kanal, editors, </editor> <booktitle> Pattern Recognition in Practice, volume Amsterdam, </booktitle> <month> May 21-23, </month> <pages> pages 381-397. </pages> <publisher> North Holland, </publisher> <year> 1980. </year>
Reference-contexts: There are many other possibilities and the general approach represents an interesting area for future work. In our experiments the mixing coefficients are fixed and uniform, but they might have been optimized using withheld training data <ref> [FJM80] </ref>. 3.4.3 Learning-Recognition Strategies During training we do not know a priori which lexical phonetic form gave rise to the surface form observed.
Reference: [GHE96] <author> S. Greenberg, J. Hollenbach, and D. Ellis. </author> <title> Insights into spoken language gleaned from phonetic transcription of the Switchboard corpus. </title> <booktitle> In Proc. ICSLP, </booktitle> <address> Philadelphia, </address> <month> October </month> <year> 1996. </year> <note> BIBLIOGRAPHY 129 </note>
Reference-contexts: Switchboard also includes a lexicon with 71,200 entries using a modified Pronlex alphabet (long form) [LDC95]. Roughly 280,000 of Switchboard's over 3 million words have been phonetically transcribed manually by ICSI using a proprietary alphabet <ref> [GHE96] </ref>. Both the Switchboard lexicon and ICSI transcription were filtered to make the two compatible. We removed 148 entries from the lexicon that included unusual punctuation ([&lt;!.]).
Reference: [GHM95] <author> J. Godfrey, E. Holliman, and J. McDaniel. </author> <title> Switchboard: telephone speech corpus for research and development. </title> <booktitle> In Proc. IEEE ICASSP, </booktitle> <pages> pages 517-520, </pages> <year> 1995. </year>
Reference-contexts: CHAPTER 3. LEARNING STRING EDIT DISTANCE COSTS 70 3.3 The Switchboard Corpus The Switchboard corpus consists of over 3 million words of recorded telephone speech conversations <ref> [GHM95] </ref>. The variability resulting from its spontaneous nature makes it one of the most difficult corpora for speech recognition research. Current recognizers exhibit word error rates of above 45% on it compared to rates closer to 5% on easier tests based on read speech.
Reference: [HAJ90] <author> X. D. Huang, Y. Ariki, and M. A. Jack. </author> <title> Hidden Markov Models for Speech Recognition. </title> <publisher> Edinburgh University Press, </publisher> <year> 1990. </year>
Reference-contexts: See <ref> [Por88, HAJ90] </ref> for reviews. <p> This suggests that FGMs may be more expressive than HMMs. We now discuss an important variation: time duration HMMs [Lev86, RC85] and sketch their rederivation in FGM terms illustrating the utility of parameterized choice models. See <ref> [HAJ90] </ref> pages 218-221 for a brief review. If in a conventional HMM a state transitions to itself with probability p, then duration in that state is geometrically distributed, i.e. as p t1 (1 p). <p> Each constituent normal density is referred to as a component of the mixture, and m 1 ; : : : ; m k are the mixing coefficients. Normal mixtures have proven useful in several areas including pattern recognition [DH73] and speech recognition <ref> [RJLS85, Bro87, HAJ90] </ref> along with vector quantization and many others.
Reference: [HD80] <author> Patrick A. V. Hall and Geoff R. Dowling. </author> <title> Approximate string matching. </title> <journal> Computing Surveys, </journal> <volume> 12(4) </volume> <pages> 381-402, </pages> <month> December </month> <year> 1980. </year>
Reference-contexts: These costs are represented as a table c i;j of size jj + 1 fi jj + 1, where CHAPTER 2. FINITE GROWTH MODELS 48 row and column 0 correspond to an additional alphabet member * representing the null character. See [SK83] for review or <ref> [HD80] </ref> for a compact discussion. The entry in table position i; j gives the cost of substituting symbol j of s for symbol i of t. The first row gives insertion costs, the first column gives deletion costs, and the remaining entries specify substitution costs. <p> This outlook was first introduced in section 3.2.2 of <ref> [HD80] </ref>. Our contribution is its further development to include the learning of costs and the construction of more sophisticated distance functions as described later in this section. In [RY96b] the authors implement the learning of costs and give experimental results. <p> The logarithm E of the joint probability of s and t is returned rather than the probability itself, so that the caller need not deal with extended range floating point values. As observed by <ref> [HD80] </ref>, the algorithm's structure resembles that of the standard dynamic program for edit distance. CHAPTER 2.
Reference: [HJ89] <author> X. D. Huang and M. A. Jack. </author> <title> Unified modelling of vector quantization and hidden markov models using semi-continuous hidden markov models. </title> <booktitle> In Proc. ICASSP, </booktitle> <pages> pages 639-642, </pages> <year> 1989. </year>
Reference-contexts: Our earlier reduction of HMMs to FGMs then results in the mixture-density HMMs widely used in speech recognition. To illustrate the variations possible we observe that tying every component with the same index to a single density yields the semi-continuous variation <ref> [HJ89] </ref>. 2.6.4 Portfolio Optimization Our final example consists of a problem that is not strictly speaking a stochastic probability model but nevertheless fits easily into the FGM formalism.
Reference: [HU79] <author> John E. Hopcroft and Jeffrey D. Ullman. </author> <title> Introduction to Automata Theory, Languages, and Computation. </title> <publisher> Addison-Wesley, </publisher> <year> 1979. </year>
Reference-contexts: This modified machine will impose the same extrinsic distribution as the original but is free of nonemitting cycles and may be reduced to an FGM. 2.6.2 Stochastic Context Free Grammars A stochastic context free grammar (SCFG) is a conventional context free grammar <ref> [HU79] </ref> with a probability attached to each production such that for every nonterminal CHAPTER 2. FINITE GROWTH MODELS 61 A the probabilities attached to all productions "A ! : : :" sum to unity. Applied to a sentence of finite length these grammars have an interesting recursive FGM structure.
Reference: [Lau96] <author> Steffen L. Lauritzen. </author> <title> Graphical Models. </title> <publisher> Clarendon Press, Oxford, </publisher> <year> 1996. </year>
Reference-contexts: This matches the equation given in [Cov84]. Assuming the X j occur with equal probability then maximizing the training set's likelihood also maximizes our expected log return from the portfolio. 2.6.5 Other Applications Bayes nets [Pea88], also known as graphical models <ref> [Lau96] </ref>, can be represented in their simplest forms as FGMs in which vertices correspond to variables, edge weights to conditional probabilities, and vertex a posteriori probabilities (fl) to belief. More complex networks can still be represented as FGMs although the reduction is more involved.
Reference: [LDC95] <editor> COMLEX pronouncing lexicon, version 0.2. </editor> <booktitle> Linguistic Data Consortium LDC95L3, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: Current recognizers exhibit word error rates of above 45% on it compared to rates closer to 5% on easier tests based on read speech. Switchboard also includes a lexicon with 71,200 entries using a modified Pronlex alphabet (long form) <ref> [LDC95] </ref>. Roughly 280,000 of Switchboard's over 3 million words have been phonetically transcribed manually by ICSI using a proprietary alphabet [GHE96]. Both the Switchboard lexicon and ICSI transcription were filtered to make the two compatible. We removed 148 entries from the lexicon that included unusual punctuation ([&lt;!.]).
Reference: [Lev66] <author> V. Levenshtein. </author> <title> Binary codes capable of correcting deletions, </title> <journal> insertions, and reversals. Soviet Physics - Doklady 10, </journal> <volume> 10 </volume> <pages> 707-710, </pages> <year> 1966. </year>
Reference-contexts: The function we learn exhibits one third the error rate of Levenshtein distance <ref> [Lev66] </ref>; the canonical untrained edit distance case.
Reference: [Lev86] <author> S. E. Levinson. </author> <title> Continuously variable duration hidden markov models for automatic speech recognition. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 1 </volume> <pages> 29-45, </pages> <year> 1986. </year>
Reference-contexts: Finally, an FGM that generates joint continuous observations will correspond to an HMM only if these joint densities are trivial, i.e. a product of independent terms. This suggests that FGMs may be more expressive than HMMs. We now discuss an important variation: time duration HMMs <ref> [Lev86, RC85] </ref> and sketch their rederivation in FGM terms illustrating the utility of parameterized choice models. See [HAJ90] pages 218-221 for a brief review.
Reference: [MMC96] <author> Robert J. McEliece, D. J. C. MacKay, and J.F. Cheng. </author> <title> Turbo decoding as an instance of pearl's `belief propagation' algorithm. </title> <journal> Submitted to IEEE Journal on Selected Areas in Communication, </journal> <year> 1996. </year>
Reference-contexts: We wonder broadly whether our framework's generality might lead to better codes, and in particular whether DAGs that encode nonlinear time orderings might have value. Such a possibility is suggested by [WLK95, Wib96]. The relationship between Turbo codes and Bayes nets is examined in <ref> [MMC96] </ref> and understanding the relationship of FGMs to this specific outlook is another interesting area for future work. CHAPTER 2.
Reference: [OGV93] <author> Jose Oncina, Pedro Garc ia, and Enrique Vidal. </author> <title> Learning subsequential transducers for pattern recognition interpretation tasks. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 15(5), </volume> <month> May </month> <year> 1993. </year> <note> BIBLIOGRAPHY 130 </note>
Reference-contexts: Later work within the syntactic pattern recognition outlook addressed the induction problem for a particular subclass of finite transducer <ref> [OGV93] </ref>. More recently a stochastic approach was taken in [BF96] where hidden Markov models were used to capture an input-output relationship between synchronous time series. Finite growth models can be used to derive Baum-Welch/EM based learning algorithms for stochastic transducers.
Reference: [Pea88] <author> Judea Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: This matches the equation given in [Cov84]. Assuming the X j occur with equal probability then maximizing the training set's likelihood also maximizes our expected log return from the portfolio. 2.6.5 Other Applications Bayes nets <ref> [Pea88] </ref>, also known as graphical models [Lau96], can be represented in their simplest forms as FGMs in which vertices correspond to variables, edge weights to conditional probabilities, and vertex a posteriori probabilities (fl) to belief. More complex networks can still be represented as FGMs although the reduction is more involved.
Reference: [PFTV88] <author> William H. Press, Brian P. Flannery, Saul A. Teukolsky, and William T. Vetterling. </author> <title> Numerical Recipes in C The Art of Scientific Computing, chapter 10. </title> <publisher> Cambridge University Press, </publisher> <year> 1988. </year>
Reference-contexts: Its virtue is that the particular density may then be viewed as a data type referred to by a single optimization algorithm. A particularly simple numerical prescription for optimization is that of Powell (see <ref> [Act70, Bre73, PFTV88] </ref>). Speed of convergence is traded for simplicity in that no gradient computation is required. Emphasis reparameterization does however introduce one wrinkle. The number of parameters can easily exceed the degrees of freedom in the underlying problem.
Reference: [Por88] <author> A. B. Poritz. </author> <title> Hidden Markov models: a guided tour. </title> <booktitle> In Proc. ICASSP-88, </booktitle> <pages> pages 7-13, </pages> <year> 1988. </year>
Reference-contexts: See <ref> [Por88, HAJ90] </ref> for reviews. <p> Applied to a sentence of finite length these grammars have an interesting recursive FGM structure. Baum-Welch/EM may then be used to learn the attached probabilities from a corpus of training sentences. The result is essentially the same as the inside-outside algorithm described in <ref> [Por88] </ref>. We assume the grammar G is in Chomsky normal form and use t to denote the input sentence. Viewing G as a stochastic generator of strings each nonterminal A may expand into sentences of many lengths.
Reference: [RC85] <author> M. J. Russell and A. E. Cook. </author> <title> Experimental evaluation of duration mod-elling techniques for automatic speech recognition. </title> <booktitle> In Proc. ICASSP-85, </booktitle> <pages> pages 5-8, </pages> <year> 1985. </year>
Reference-contexts: Finally, an FGM that generates joint continuous observations will correspond to an HMM only if these joint densities are trivial, i.e. a product of independent terms. This suggests that FGMs may be more expressive than HMMs. We now discuss an important variation: time duration HMMs <ref> [Lev86, RC85] </ref> and sketch their rederivation in FGM terms illustrating the utility of parameterized choice models. See [HAJ90] pages 218-221 for a brief review.
Reference: [RJLS85] <author> L. R. Rabiner, B. H. Juang, S. E. Levinson, and M. M. Sondhi. </author> <title> Recognition of isolated digits using hidden markov models with continuous mixture densities. </title> <journal> AT&T Technical Journal, </journal> <year> 1985. </year>
Reference-contexts: Each constituent normal density is referred to as a component of the mixture, and m 1 ; : : : ; m k are the mixing coefficients. Normal mixtures have proven useful in several areas including pattern recognition [DH73] and speech recognition <ref> [RJLS85, Bro87, HAJ90] </ref> along with vector quantization and many others.
Reference: [RW84] <author> R. A. Redner and H. F. Walker. </author> <title> Mixture densities, maximum likelihood, and the EM algorithm. </title> <journal> SIAM Review, </journal> <volume> 26 </volume> <pages> 195-239, </pages> <year> 1984. </year>
Reference-contexts: Section 5.3 shows that such search problems may almost always be carried out using reparameterizations we refer to as emphasis methods that are motivated by the CHAPTER 5. EMPHASIS REPARAMETERIZATION 91 simple and intuitive expectation maximization (EM) algorithm <ref> [BE67, DLR77, RW84] </ref> for unsupervised maximum likelihood (ML) parameter estimation. The reparameter-ized problem cannot express an arbitrary mixture but nevertheless, because of degeneracy, can express a mixture that induces optimal class functions with respect to the prediction of training labels.
Reference: [RY94] <author> Eric Sven Ristad and Peter N. Yianilos. </author> <title> Probability value library. </title> <type> Technical report, </type> <institution> Princeton University, Department of Computer Science, </institution> <year> 1994. </year>
Reference: [RY96a] <author> Eric Sven Ristad and Peter N. Yianilos. </author> <title> Finite growth models. </title> <type> Technical Report 533-96, </type> <institution> Princeton University, Department of Computer Science, </institution> <year> 1996. </year>
Reference: [RY96b] <author> Eric Sven Ristad and Peter N. Yianilos. </author> <title> Learning string edit distance. </title> <type> Technical report, </type> <institution> Princeton University, Department of Computer Science, </institution> <year> 1996. </year>
Reference-contexts: This outlook was first introduced in section 3.2.2 of [HD80]. Our contribution is its further development to include the learning of costs and the construction of more sophisticated distance functions as described later in this section. In <ref> [RY96b] </ref> the authors implement the learning of costs and give experimental results. This report also The process is formally a 2-way stochastic transducer with a single state, no memory, and D = 2. The initial state of the process is (;; ;). <p> It is straightforward to compute the probability of event S jsj;jtj . Division then yields the conditional probability P ((s; t)j; S jsj;jtj ) which amounts to conditioning on string lengths. It is also possible to define the FGM differently in order to directly implement conditioning on string lengths <ref> [RY96b] </ref>. The stochastic edit distance log 2 P ((s; t)j) is highly related but not identical to conventional edit distance. <p> The text is typically regarded as a sequence of words from a fixed vocabulary recorded in a lexicon L. This transduction bridges a wide representational 1 The work described in this chapter was first described in <ref> [RY96b] </ref> and will appear in [RY97a] 66 CHAPTER 3. LEARNING STRING EDIT DISTANCE COSTS 67 gap and investigators commonly introduce an intermediate phonetic language P and the problem is then viewed as S ! P ! T . Our experiments focus on the second segment P ! T .
Reference: [RY96c] <author> Eric Sven Ristad and Peter N. Yianilos. </author> <title> On the strange a posteriori degeneracy of normal mixtures, and related reparameterization theorems. </title> <type> BIBLIOGRAPHY 131 Technical report, </type> <institution> Princeton University, Department of Computer Science, </institution> <year> 1996. </year>
Reference-contexts: We prove that EM-style reparameterization is not capable of expressing arbitrary a posteriori behavior, and is therefore incapable of expressing some solutions. 1 This chapter first appeared as a technical report <ref> [RY96c] </ref> 89 CHAPTER 5. EMPHASIS REPARAMETERIZATION 90 However a slightly different reparameterization is presented which is almost always fully expressive a fact proven by exploiting the degeneracy described above. 5.1 Introduction A normal mixture is a finite stochastic combination of multivariate normal (Gaussian) densities.
Reference: [RY97a] <author> Eric S. Ristad and Peter N. Yianilos. </author> <title> Learning string edit distance. </title> <booktitle> In (to appear) The Fourteenth International Conference on Machine Learning (ICML'97), </booktitle> <year> 1997. </year>
Reference-contexts: The text is typically regarded as a sequence of words from a fixed vocabulary recorded in a lexicon L. This transduction bridges a wide representational 1 The work described in this chapter was first described in [RY96b] and will appear in <ref> [RY97a] </ref> 66 CHAPTER 3. LEARNING STRING EDIT DISTANCE COSTS 67 gap and investigators commonly introduce an intermediate phonetic language P and the problem is then viewed as S ! P ! T . Our experiments focus on the second segment P ! T .
Reference: [RY97b] <author> Eric Sven Ristad and Peter N. Yianilos. </author> <title> Library of practical abstractions, release 1.2. </title> <address> ftp://ftp.cs.princeton.edu/pub/packages/libpa, </address> <month> March </month> <year> 1997. </year>
Reference-contexts: Several software development steps are taken to help ensure the quality and portability of the resulting system. First, it is based at the lowest level on memory allocation primitives and other modules of libpa the "library of practical abstractions" <ref> [RY97b] </ref>. 1 Mathematica is a product of Wolfram Research, and Matlab is a product of the Math Works Inc. CHAPTER 4. A MODELING ASSEMBLY LANGUAGE 84 Because of this, run-time analyzers such as Purify 2 are particular effective identifiers of common implementation problems.
Reference: [SHJ96] <author> Padhraic Smith, David Heckerman, and Michael I. Jordan. </author> <title> Probabilistic independence networks for hidden probability models. </title> <type> Technical Report MSR-TR-93-03, </type> <institution> Microsoft Research, </institution> <year> 1996. </year>
Reference-contexts: More complex networks can still be represented as FGMs although the reduction is more involved. The relationship between these networks and hidden Markov models is elucidated in <ref> [SHJ96] </ref>. Exploring the relationship of FGMs to error correcting codes such as those based on trellises, and the recently developed class of turbo codes [BGT93], is a subject for future work.
Reference: [SK83] <author> D. Sankoff and J. B. Kruskal. </author> <title> Macromolecules: The Theory and Practice of Sequence Comparison. </title> <publisher> Addison-Wesley, </publisher> <year> 1983, 1983. </year>
Reference-contexts: These costs are represented as a table c i;j of size jj + 1 fi jj + 1, where CHAPTER 2. FINITE GROWTH MODELS 48 row and column 0 correspond to an additional alphabet member * representing the null character. See <ref> [SK83] </ref> for review or [HD80] for a compact discussion. The entry in table position i; j gives the cost of substituting symbol j of s for symbol i of t. The first row gives insertion costs, the first column gives deletion costs, and the remaining entries specify substitution costs.
Reference: [Wib96] <author> Niclas Wiberg. </author> <title> Codes and Decoding on General Graphs. </title> <type> PhD thesis, </type> <institution> Linkoping Studies in Science and Technology, Sweden, </institution> <year> 1996. </year> <note> No. 440. </note>
Reference-contexts: We wonder broadly whether our framework's generality might lead to better codes, and in particular whether DAGs that encode nonlinear time orderings might have value. Such a possibility is suggested by <ref> [WLK95, Wib96] </ref>. The relationship between Turbo codes and Bayes nets is examined in [MMC96] and understanding the relationship of FGMs to this specific outlook is another interesting area for future work. CHAPTER 2.
Reference: [WLK95] <author> N. Wiberg, H. Loeliger, and R. Kotter. </author> <title> Codes and iterative decoding on general graphs. </title> <journal> European Transactions on Telecommunications, </journal> <volume> 6(5) </volume> <pages> 513-526, </pages> <month> October </month> <year> 1995. </year>
Reference-contexts: We wonder broadly whether our framework's generality might lead to better codes, and in particular whether DAGs that encode nonlinear time orderings might have value. Such a possibility is suggested by <ref> [WLK95, Wib96] </ref>. The relationship between Turbo codes and Bayes nets is examined in [MMC96] and understanding the relationship of FGMs to this specific outlook is another interesting area for future work. CHAPTER 2.
References-found: 45

