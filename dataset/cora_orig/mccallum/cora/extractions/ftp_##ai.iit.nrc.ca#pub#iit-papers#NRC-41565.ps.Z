URL: ftp://ai.iit.nrc.ca/pub/iit-papers/NRC-41565.ps.Z
Refering-URL: http://ai.iit.nrc.ca/cgi-bin/ftpsearch/?turney
Root-URL: 
Title: Preprocessing by a cost-sensitive literal reduction algorithm: REDUCE 1  
Author: Nada Lavrac J. Stefan Dragan Gamberger Peter Turney 
Address: Jamova 39, 1000 Ljubljana, Slovenia  Bijenicka 54, 10000 Zagreb, Croatia  M-50 Montreal Road, Ottawa, Ontario, Canada, K1A 0R6  
Affiliation: Institute  Rudjer Boskovic Institute  Institute for Information Technology National Research Council Canada  
Abstract: This study is concerned with whether it is possible to detect what information contained in the training data and background knowledge is relevant for solving the learning problem, and whether irrelevant information can be eliminated in preprocessing before starting the learning process. A case study of data preprocessing for a hybrid genetic algorithm shows that the elimination of irrelevant features can substantially improve the efficiency of learning. In addition, cost-sensitive feature elimination can be effective for reducing costs of induced hypotheses. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Caruana, R. and D. Freitag: </author> <title> Greedy Attribute Selection, </title> <booktitle> in: Proceedings of the 11th International Conference on Machine Learning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1994, </year> <pages> 28-36. </pages>
Reference: 2. <author> Fayyad, </author> <title> U.M. and K.B. Irani: On the handling of continuous-valued attributes in decision tree generation, </title> <booktitle> Machine Learning, 8 (1992), </booktitle> <pages> 87-102. </pages>
Reference-contexts: The motivation is similar to that suggested in <ref> [2] </ref>. * For integer valued attributes A i , literals are generated as if A i were both discrete and continuous, resulting in literals of four different forms: A i (v ix + w iy )=2, A i &gt; (v ix + w iy )=2, A i = v ix ,
Reference: 3. <author> Gamberger, D.: </author> <title> A Minimization Approach to Propositional Inductive Learning, </title> <booktitle> in: Proceedings of the 8th European Conference on Machine Learning, </booktitle> <publisher> Springer, </publisher> <year> 1995, </year> <pages> 151-160. </pages>
Reference-contexts: This theorem is the basis of the REDUCE algorithm for literal elimination. 3 Cost-sensitive literal elimination 3.1 Cost-sensitive literal elimination algorithm REDUCE Algorithm 1 implements the cost-sensitive literal elimination algorithm, initially developed within the ILLM learner <ref> [3] </ref>. This algorithm is the core of REDUCE [9]. The complexity of Algorithm 1 is O (jLj 2 fi jEj), where jLj is the number of literals and jEj is the number of examples.
Reference: 4. <author> Grefenstette, J.J.: </author> <title> Optimization of control parameters for genetic algorithms, </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 16 (1986), </volume> <pages> 122-128. </pages>
Reference-contexts: ICET is a hybrid of a genetic algorithm and a decision tree induction algorithm. The genetic algorithm is Grefenstette's GENESIS <ref> [4] </ref> and the decision tree induction algorithm is Quinlan's C4.5 [13]. ICET uses a two-tiered search strategy. On the bottom tier, C4.5 uses a TDIDT (Top Down Induction of Decision Trees) strategy to search through the space of decision trees.
Reference: 5. <author> John, G.H., R. Kohavi and K. Pfleger: </author> <title> Irrelevant Features and the Subset Selection Problem, </title> <booktitle> in: Proceedings of the 11th International Conference on Machine Learning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1994, </year> <pages> 190-198. </pages>
Reference: 6. <author> Lavrac, N., S. Dzeroski and M. Grobelnik:. </author> <title> Learning Nonrecursive Definitions of Relations with LINUS, </title> <booktitle> in: Proceedings of the 5th European Working Session on Learning, </booktitle> <publisher> Springer, </publisher> <year> 1991, </year> <pages> 265-281. </pages>
Reference: 7. <author> Lavrac, N. and S. Dzeroski: </author> <title> Inductive Logic Programming: Techniques and Applications, </title> <publisher> Ellis Horwood, </publisher> <year> 1994. </year>
Reference-contexts: (v ix + w iy )=2, A i &gt; (v ix + w iy )=2, A i = v ix , and A i 6= w iy . 2.1.2 Inductive logic programming Inductive logic programming (ILP) refers to first-order learning of relational descriptions in the representation formalism of logic programs <ref> [7] </ref>. In this setting, a LINUS transformation approach is assumed that is appropriate for a limited hypothesis language of constrained nonrecursive clauses [6,7].
Reference: 8. <author> Lavrac, N., D. Gamberger and S. Dzeroski: </author> <title> An Approach to Dimensionality Reduction in Learning from Deductive Databases, </title> <booktitle> in: Proceedings of the 5th International Workshop on Inductive Logic Programming, Scientific Report, </booktitle> <institution> Katholieke Universiteit Leuven, </institution> <year> 1995, </year> <pages> 337-354. </pages>
Reference-contexts: The complexity of Algorithm 1 is O (jLj 2 fi jEj), where jLj is the number of literals and jEj is the number of examples. This algorithm can be easily transformed into an iterative algorithm that can be used during the process of generation of literals <ref> [8] </ref>. 2 Hypothesis H is complete if it covers all the positive examples p 2 P . Hypothesis H is consistent if it does not cover any negative example n 2 N . 5 Algorithm 1.
Reference: 9. <author> Lavrac, N., D. Gamberger and P. Turney: </author> <title> Cost-Sensitive Feature Reduction Applied to a Hybrid Genetic Algorithm, </title> <booktitle> in: Proceedings of the 7th International Workshop on Algorithmic Learning Theory, </booktitle> <publisher> Springer, </publisher> <year> 1996, </year> <pages> 127-134. </pages>
Reference-contexts: This theorem is the basis of the REDUCE algorithm for literal elimination. 3 Cost-sensitive literal elimination 3.1 Cost-sensitive literal elimination algorithm REDUCE Algorithm 1 implements the cost-sensitive literal elimination algorithm, initially developed within the ILLM learner [3]. This algorithm is the core of REDUCE <ref> [9] </ref>. The complexity of Algorithm 1 is O (jLj 2 fi jEj), where jLj is the number of literals and jEj is the number of examples.
Reference: 10. <author> Michalski, R.S. and J.B. Larson: </author> <title> Inductive Inference of VL Decision Rules, </title> <journal> ACM SIGART Newsletter, </journal> <volume> 63 (1977), </volume> <pages> 38-44. </pages>
Reference-contexts: The challenge was inspired by a problem posed by Michalski and Larson <ref> [10] </ref>. The original challenge [12] included three separate tasks. Michie later issued a second challenge, involving a fourth task. Our experiments described here involve the first and fourth tasks.
Reference: 11. <author> Michalski, </author> <title> R.S.: A Theory and Methodology of Inductive Learning, in: Machine Learning: An Artificial Intelligence Approach (Eds. </title> <editor> R. Michalski, J. Carbonell and T. Mitchell), </editor> <publisher> Tioga, </publisher> <year> 1983, </year> <pages> 83-134. </pages>
Reference-contexts: 1 Introduction The problem of relevance was addressed in early research on inductive concept learning <ref> [11] </ref>. Recently, this problem has also attracted much attention in the context of feature selection in attribute-value learning [1,5,14]. Basically one can say that all learners are concerned with the selection of `good' literals or features which will be used to construct the hypothesis.
Reference: 12. <author> Michie, D., S. Muggleton, D. Page and A. Srinivasan: </author> <title> To the International Computing Community: A new East-West Challenge. </title> <institution> Oxford University Computing Laboratory, Oxford, </institution> <year> 1994. </year> <note> [Available at URL ftp://ftp.comlab.ox.ac.uk/pub/Packages/ILP/trains.tar.Z.] </note>
Reference-contexts: This means that although there does not exist a feature f that covers both literals of the feature g, feature g can be irrelevant. 4 4 Utility study: The East-West challenge Michie et al. <ref> [12] </ref> issued a "challenge to the international computing community" to discover low size-complexity Prolog programs for classifying trains as Eastbound or Westbound. The challenge was inspired by a problem posed by Michalski and Larson [10]. The original challenge [12] included three separate tasks. <p> be irrelevant. 4 4 Utility study: The East-West challenge Michie et al. <ref> [12] </ref> issued a "challenge to the international computing community" to discover low size-complexity Prolog programs for classifying trains as Eastbound or Westbound. The challenge was inspired by a problem posed by Michalski and Larson [10]. The original challenge [12] included three separate tasks. Michie later issued a second challenge, involving a fourth task. Our experiments described here involve the first and fourth tasks.
Reference: 13. <author> Quinlan, J.R.: C4.5: </author> <title> Programs for Machine Learning, </title> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: The results of RL-ICET are also compared to those of C4.5 <ref> [13] </ref>. 2 Relevance of literals and features Consider a two-class learning problem where training set E consists of positive and negative examples of a concept, and examples e 2 E are tuples of truth-values of terms in a hypothesis language. <p> ICET is a hybrid of a genetic algorithm and a decision tree induction algorithm. The genetic algorithm is Grefenstette's GENESIS [4] and the decision tree induction algorithm is Quinlan's C4.5 <ref> [13] </ref>. ICET uses a two-tiered search strategy. On the bottom tier, C4.5 uses a TDIDT (Top Down Induction of Decision Trees) strategy to search through the space of decision trees. On the top tier, GENESIS uses a genetic algorithm to search through the space of biases. <p> fully automate the transformation of the decision trees to Prolog programs and then modify RL-ICET to search for the least complex Prolog program, instead of the least costly decision tree. 4.6 Applying C4.5 in the East-West Challenge We have compared the results of RL-ICET with the ones achieved using C4.5 <ref> [13] </ref>. This experiment was made in order to check whether our claims of the usefulness of feature reduction can be made more general. To do so, C4.5 was first applied to the 24 trains problem, using the default settings. <p> In order to evaluate the effects of feature reduction, we have also compared the results of ICET (with and without feature reduction) with the results achieved using C4.5 <ref> [13] </ref>. In both experiments, feature reduction (reduction to 86 and 116 features, respectively) helped ICET to outperform C4.5 when comparing costs of decision trees, both in terms of minimal and average costs.
Reference: 14. <author> Skalak, D.: </author> <title> Prototype and Feature Selection by Sampling and Random Mutation Hill Climbing Algorithms, </title> <booktitle> in: Proceedings of the 11th International Conference on Machine Learning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1994, </year> <pages> 293-301. </pages>
Reference: 15. <author> Turney, P.: </author> <title> Cost-Sensitive Classification: Empirical Evaluation of a Hybrid Genetic Decision Tree Induction Algorithm, </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2 (1995), </volume> <pages> 369-409. </pages> <note> [Available at URL http://www.cs.washington.edu/research/ jair/home.html.] </note>
Reference-contexts: The size-complexity of the Prolog program was calculated as the sum of the number of clause occurrences, the number of term occurrences, and the number of atom occurrences. 4.1 RL-ICET A cost-sensitive algorithm ICET was developed for generating low-cost decision trees <ref> [15] </ref>. ICET is a hybrid of a genetic algorithm and a decision tree induction algorithm. The genetic algorithm is Grefenstette's GENESIS [4] and the decision tree induction algorithm is Quinlan's C4.5 [13]. ICET uses a two-tiered search strategy.
Reference: 16. <author> Turney, P.: </author> <title> Low Size-Complexity Inductive Logic Programming: The East-West Challenge as a Problem in Cost-Sensitive Classification, in: Advances in Inductive Logic Programming (Ed. </title> <editor> L. De Raedt), </editor> <publisher> IOS Press, </publisher> <year> 1996, </year> <pages> 308-321. 18 </pages>
Reference-contexts: Section 3 presents the cost-sensitive literal elimination algorithm REDUCE. Section 4 introduces the problem domain, the 20 and the 24 trains East-West Challenges, and presents the results of our experiments that show that the performance of a hybrid genetic algorithm RL-ICET <ref> [16] </ref> can be significantly improved by applying REDUCE in preprocessing of the dataset. <p> The East-West Challenge involves data in the form of relations, and theories in the form of Prolog programs. For the East-West Challenge, ICET was extended to handle Prolog input. This algorithm is called RL-ICET (Relational Learning with ICET) <ref> [16] </ref>. 4 This analysis helps us to see that the standard approach to rule construction which is based on feature selection is sub-optimal. <p> Therefore, when solving a problem, ICET needs to be run several times. The best (lowest cost) decision tree that was generated for the first competition <ref> [16] </ref> is shown below. <p> In this way, the complexity of the learning problem was reduced to about 7% (86/1199) of the initial learning problem. Results of 10 runs of ICET on the 1199 feature set are the results reported in <ref> [16] </ref>, whereas results of 10 runs of ICET on the training examples described with 86 features are new. The results show that the efficiency of learning significantly increased. <p> It is important to note that, using the reduced set of 86 features, in Trial 6, the same best tree as reported in <ref> [16] </ref> and obtained by Trial 15 from 1199 features, was induced (cost = 18, complexity = 19, see Section 4.3). The fact that the same optimal non-recursive Prolog program was induced and the substantial efficiency increase confirm the usefulness of our approach for learning with genetic algorithms.
References-found: 16

