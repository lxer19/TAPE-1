URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/dayne/www/ps/ms-ie.ps.Z
Refering-URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/dayne/www/cv.html
Root-URL: 
Email: dayne@cs.cmu.edu  
Title: Multistrategy Learning for Information Extraction  
Author: Dayne Freitag 
Address: Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Abstract: Information extraction (IE) is the problem of filling out pre-defined structured summaries from text documents. We are interested in performing IE in non-traditional domains, where much of the text is often ungrammatical, such as electronic bulletin board posts and Web pages. We suggest that the best approach is one that takes into account many different kinds of information, and argue for the suitability of a multistrat-egy approach. We describe learners for IE drawn from three separate machine learning paradigms: rote memorization, term-space text classification, and relational rule induction. By building regression models mapping from learner confidence to probability of correctness and combining probabilities appropriately, it is possible to improve extraction accuracy over that achieved by any individual learner. We describe three different mul-tistrategy approaches. Experiments on two IE domains, a collection of electronic seminar announcements from a university computer science department and a set of newswire articles describing corporate acquisitions from the Reuters collection, demonstrate the effectiveness of all three approaches.
Abstract-found: 1
Intro-found: 1
Reference: <author> Califf, M. E., and Mooney, R. J. </author> <year> 1997. </year> <title> Relational learning of pattern-match rules for information extraction. </title> <booktitle> In Working Papers of ACL-97 Workshop on Natural Language Learning. </booktitle>
Reference-contexts: To perform well, these methods must be prepared to exploit non-linguistic information, such as stock phrases, document formatting, meta-textual structure (e.g., in HTML), and term frequency statistics. Several learning IE systems have been proposed which are also targeted at such domains (Soderland, 1997) <ref> (Califf & Mooney, 1997) </ref> (Kushmerick, 1997). These previous investigations all take a single approach or attack a particular kind of domain. <p> Symbolic learning algorithms from the "covering" family form hypotheses that match such data spaces well. Previous research has shown the effectiveness of such methods for the IE problem (Soderland, 1996) <ref> (Califf & Mooney, 1997) </ref>. Our relational learner, called SRV, is a variant of FOIL (Quinlan, 1990). Its example space consists of all text fragments from the training document collection as long (in number of tokens) as the smallest field instance in the training corpus but no longer than the largest.
Reference: <author> Cardie, C. </author> <year> 1997. </year> <title> Empirical methods in information extraction. </title> <journal> AI Magazine 18(4) </journal> <pages> 65-79. </pages>
Reference-contexts: The three approaches we will discuss, all based on standard ideas from ML, each implement G. Note that this learning task is only a part of the functionality of a typical participating system at the Message Understanding Conference (MUC) <ref> (Cardie, 1997) </ref>. What we have called fields correspond to slots in the MUC setting. A slot is a component of a larger structure, called a template, which summarizes the relevant information contained in a document.
Reference: <author> Chan, P., and Stolfo, S. </author> <year> 1993. </year> <title> Experiments on multi-strategy learning by meta-learning. </title> <booktitle> In Proceedings of the Second International Conference on Information and Knowledge Management (CIKM 93), </booktitle> <pages> 314-323. </pages>
Reference-contexts: Elsewhere we have shown that heuristic combination of two learners from different paradigms can yield substantial performance improvements for the IE problem (Freitag, 1997). Here, we ask how we might profitably combine component learners by treating them as black boxes. This approach has been called "meta-learning" in the literature <ref> (Chan & Stolfo, 1993) </ref>. Although we might expect a heuristic combination to achieve better performance, there are clear advantages to the meta-learning approach. It is modular and flexible, making no assumptions about the design of component learners or the number of learners available.
Reference: <author> Cowie, J., and Lehnert, W. </author> <year> 1996. </year> <title> Information extraction. </title> <journal> Communications of the ACM 39(1). </journal>
Reference-contexts: In hypertext, it can support directed and efficient automatic navigation. It can serve as a source of high-quality features for document categorization. And the output of an IE system can be viewed as a kind of succinct and directed summarization. Although traditional IE <ref> (Cowie & Lehnert, 1996) </ref> concentrates on domains consisting of grammatical prose, we are interested in extracting information from "messy" text, such as Web pages, email, and finger plan files. Our goal is the development of machine learning methods for such domains.
Reference: <author> Domingos, P. </author> <year> 1996. </year> <title> Unifying instance-based and rule-based induction. </title> <booktitle> Machine Learning 24(2) </booktitle> <pages> 141-168. </pages>
Reference-contexts: The bulk of emphasis in past research in this area has been on systems which combine analytical and empirical techniques. Our work, however, is an example of what has been called "empirical multistrategy learning" <ref> (Domingos, 1996) </ref>. All constituent learners are inductive, each designed to solve the IE problem individually. Elsewhere we have shown that heuristic combination of two learners from different paradigms can yield substantial performance improvements for the IE problem (Freitag, 1997).
Reference: <author> Freitag, D. </author> <year> 1997. </year> <title> Using grammatical inference to improve precision in information extraction. In Notes of the ICML-97 Workshop on Automata Induction, Grammatical Inference, and Language Acquisition. </title>
Reference-contexts: All constituent learners are inductive, each designed to solve the IE problem individually. Elsewhere we have shown that heuristic combination of two learners from different paradigms can yield substantial performance improvements for the IE problem <ref> (Freitag, 1997) </ref>. Here, we ask how we might profitably combine component learners by treating them as black boxes. This approach has been called "meta-learning" in the literature (Chan & Stolfo, 1993). Although we might expect a heuristic combination to achieve better performance, there are clear advantages to the meta-learning approach. <p> In particular, a context window parameter w is set prior to training, and the w tokens on either side of a fragment are used to form the estimate, in addition to the in-field tokens. The algorithm is described in greater detail elsewhere <ref> (Freitag, 1997) </ref>. 2.3 RELATIONAL LEARNING Both Bayes and Rote are hobbled by their inability to take into account anything but simple term frequency statistics. It may be the case, however, that the information needed to perform information extraction comes in other forms.
Reference: <author> Freitag, D. </author> <year> 1998. </year> <title> Information extraction from html: Application of a general machine learning approach. </title> <booktitle> In Proceedings of the Fifteenth National Conference on Artificial Intelligence (AAAI-98). </booktitle>
Reference-contexts: With each rule learned by SRV we store its performance on the hold-out set. From this performance we estimate a rule's actual accuracy. The confidence of a prediction made by SRV is formed from the estimated accuracy of matching rules. For additional details on SRV, please refer to <ref> (Freitag, 1998) </ref>. 3 COMBINING LEARNERS Certain features of the IE problem make it particularly amenable to a multistrategy approach. Among these are the following: * Examples have multiple representations. Because documents and text fragments are "nat-ural" objects which must be mapped to appropriate representations for learning, multiple mappings are possible.
Reference: <author> Kushmerick, N. </author> <year> 1997. </year> <title> Wrapper Induction for Information Extraction. </title> <type> Ph.D. Dissertation, </type> <institution> University of Washington. </institution> <note> Tech Report UW-CSE-97-11-04. </note>
Reference-contexts: To perform well, these methods must be prepared to exploit non-linguistic information, such as stock phrases, document formatting, meta-textual structure (e.g., in HTML), and term frequency statistics. Several learning IE systems have been proposed which are also targeted at such domains (Soderland, 1997) (Califf & Mooney, 1997) <ref> (Kushmerick, 1997) </ref>. These previous investigations all take a single approach or attack a particular kind of domain.
Reference: <author> Lewis, D. </author> <year> 1992. </year> <title> Representation and Learning in Information Retrieval. </title> <type> Ph.D. Dissertation, </type> <institution> Univ. of Massachusetts. </institution> <type> CS Tech. Report 91-93. </type>
Reference-contexts: We manually tagged these announcements for four fields: speaker, location, stime (start time), and etime (end time). The other domain is a collection of 600 newswire articles on corporate acquisitions from the Reuters data set <ref> (Lewis, 1992) </ref>. We defined nine fields for this domain and manually annotated the collection to identify all instances of them.
Reference: <author> Maron, M. </author> <year> 1961. </year> <title> Automatic indexing: An experimental inquiry. </title> <journal> Journal of the Association for Computing Machinery 8 </journal> <pages> 404-417. </pages>
Reference-contexts: Although it is hard to exploit contextual regularities by memorizing, statistical approaches are well suited for this. We base our bag-of-words learner, which we call Bayes, on the Naive Bayes algorithm, as used in document classification and elsewhere (originally in <ref> (Maron, 1961) </ref>). Each fragment of text in a document (of appropriate size) is regarded as a competing hypothesis. Given a document, we want to find the most likely hypothesis (the fragment most likely to be a field instance).
Reference: <editor> Michalski, R., and Tecuci, G., eds. </editor> <year> 1994. </year> <title> Machine Learning: A Multistrategy Approach. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: These considerations suggest at multistrategy approach. Multistrategy learning is an attempt to devise systems which, by employing multiple constituent learners, which are typically drawn from diverse paradigms, achieve performance superior to any single learner <ref> (Michalski & Tecuci, 1994) </ref>. The bulk of emphasis in past research in this area has been on systems which combine analytical and empirical techniques. Our work, however, is an example of what has been called "empirical multistrategy learning" (Domingos, 1996).
Reference: <author> Quinlan, J. R. </author> <year> 1990. </year> <title> Learning logical definitions from relations. </title> <booktitle> Machine Learning 5(3) </booktitle> <pages> 239-266. </pages>
Reference-contexts: Symbolic learning algorithms from the "covering" family form hypotheses that match such data spaces well. Previous research has shown the effectiveness of such methods for the IE problem (Soderland, 1996) (Califf & Mooney, 1997). Our relational learner, called SRV, is a variant of FOIL <ref> (Quinlan, 1990) </ref>. Its example space consists of all text fragments from the training document collection as long (in number of tokens) as the smallest field instance in the training corpus but no longer than the largest. A negative example is any fragment that is not tagged as a field instance.
Reference: <author> Soderland, S. </author> <year> 1996. </year> <title> Learning Text Analysis Rules for Domain-specific Natural Language Processing. </title> <type> Ph.D. Dissertation, </type> <institution> University of Massachusetts. </institution> <type> CS Tech. Report 96-087. </type>
Reference-contexts: Symbolic learning algorithms from the "covering" family form hypotheses that match such data spaces well. Previous research has shown the effectiveness of such methods for the IE problem <ref> (Soderland, 1996) </ref> (Califf & Mooney, 1997). Our relational learner, called SRV, is a variant of FOIL (Quinlan, 1990).
Reference: <author> Soderland, S. </author> <year> 1997. </year> <title> Learning to extract text-based information from the world wide web. </title> <booktitle> In Proceedings of the 3rd International Conference on Knowledge Discovery and Data Mining. </booktitle>
Reference-contexts: To perform well, these methods must be prepared to exploit non-linguistic information, such as stock phrases, document formatting, meta-textual structure (e.g., in HTML), and term frequency statistics. Several learning IE systems have been proposed which are also targeted at such domains <ref> (Soderland, 1997) </ref> (Califf & Mooney, 1997) (Kushmerick, 1997). These previous investigations all take a single approach or attack a particular kind of domain.
References-found: 14

