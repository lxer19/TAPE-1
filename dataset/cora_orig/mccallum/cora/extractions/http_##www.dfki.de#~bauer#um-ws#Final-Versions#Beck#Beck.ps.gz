URL: http://www.dfki.de/~bauer/um-ws/Final-Versions/Beck/Beck.ps.gz
Refering-URL: http://www.dfki.de/~bauer/um-ws/
Root-URL: 
Email: beck@cs.umass.edu  
Title: Modeling the Student with Reinforcement Learning  
Author: Joseph Beck 
Address: Amherst, MA, U.S.A.  
Affiliation: Department of Computer Science University of Massachusetts  
Abstract: We describe a methodology for enabling an intelligent teaching system to make high level strategy decisions on the basis of low level student modeling information. This framework is less costly to construct, and superior to hand coding teaching strategies as it is more responsive to the learner's needs. In order to accomplish this, reinforcement learning is used to learn to associate superior teaching actions with certain states of the student's knowledge. Reinforcement learning (RL) has been shown to be flexible in handling noisy data, and does not need expert domain knowledge. A drawback of RL is that it often needs a significant number of trials for learning. We propose an off-line learning methodology using sample data, simulated students, and small amounts of expert knowledge to bypass this problem. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Beck, M. Stern, and B. Woolf. </author> <title> Using the student model to control problem difficulty. </title> <booktitle> In Proceedings of the Seventh International Conference on User Modeling, </booktitle> <year> 1997. </year>
Reference-contexts: This measurement commonly takes the form of the probability a student has mastered a topic. This suffices for basic reasoning such as whether to promote the student to a more complex topic, or to control the activity's level of difficulty <ref> [1] </ref>. However, frequently more complex types of reasoning are needed. <p> Fortunately, much progress has been made on the former via Bayesian Networks [2] and other formal reasoning techniques while less progress has been made on the latter. 2 Motivation This framework is being designed for use in the MFD (Mixed Number, Fractions, Decimals) system <ref> [1] </ref>. The goal of this system is to both to help grade school students learn fractional and decimal arithmetic, and to increase the confidence of students using the system.
Reference: [2] <author> J. Collins, J. Greer, and S. Huang. </author> <title> Adaptive assessment of using granularity hierarchies and Bayesien nets. </title> <booktitle> In Proceedings of Intelligent Tutoring Systems, </booktitle> <pages> pages 569577, </pages> <year> 1996. </year>
Reference-contexts: Therefore, having a model that tries to reason about what a student knows at a fine-grained level of detail is likely to be less effective than trying to capture more coarse-gained features. Fortunately, much progress has been made on the former via Bayesian Networks <ref> [2] </ref> and other formal reasoning techniques while less progress has been made on the latter. 2 Motivation This framework is being designed for use in the MFD (Mixed Number, Fractions, Decimals) system [1].
Reference: [3] <author> R. Crites and R. Sutton. </author> <title> Improving elevator performance using reinforcement learning. </title> <booktitle> Advances in Neural Information Processing, </booktitle> <year> 1996. </year>
Reference-contexts: That is, it guesses the value of the current state from its guess about the value of adjacent states. Systems built with RL have outperformed expert humans players in backgammon [10], and have demonstrated superior performance in elevator scheduling compared to commercial, hand crafted algorithms <ref> [3] </ref>. These results suggest the potential for impressive performance by these systems. An important point is that RL is not a learning algorithm per se, but rather a method of determining the value of being in a certain state.
Reference: [4] <author> L.-J. Lin. </author> <title> Self-improving reactive agents based on reinforcement learning, planning and teaching. </title> <booktitle> Machine Learning, </booktitle> <address> 8:293321, </address> <year> 1992. </year>
Reference-contexts: In order to gather the most from each item of student datum, replay <ref> [4] </ref> can be used. This technique presents several examples of useful trials. <p> This technique presents several examples of useful trials. If an action receives a particularly large or a particularly small reward then it is selected to be presented to the system several more times (i.e. the state, action, reward sequence is given to the function approximator several times). Lin <ref> [4] </ref> has shown that this results in faster learning. 5 Conclusions and Future Work Using a learning algorithm in the method we describe has clear advantages over hand coding teaching strategies. First, it has the potential to be less costly than spending time constructing and empirically validating a pedagogical module.
Reference: [5] <author> J. Mertz jr. </author> <title> Using a simulated student for instructional design. </title> <booktitle> In Proceedings of Seventh World Conference on Artificial Intelligence in Education, </booktitle> <year> 1995. </year>
Reference: [6] <author> M. Quafafou, A. Mekaouche, and N. H.S. </author> <title> Multiviews learning and intelligent tutoring systems. </title> <booktitle> In Proceedings of Seventh World Conference on Artificial Intelligence in Education, </booktitle> <year> 1995. </year>
Reference-contexts: By providing these weak initial methods, the cost of determining pedagogical behavior is decreased (as opposed to constructing a policy by hand). Given the high cost of hand-coding teaching action selection, Quafafou's NeTutor <ref> [6] </ref> proposes providing the system with examples, and using machine learning to learn teaching actions applicable to a specific student. A difference between our approach and the one outlined there is that they were concerned with learning symbolic rules while we are interested in numerical techniques.
Reference: [7] <author> R. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <booktitle> Machine Learning, </booktitle> <address> 3:944, </address> <year> 1988. </year>
Reference-contexts: For these reasons we have chosen to use reinforcement learning as a general framework. 3.1 Reinforcement Learning Reinforcement learning (RL) is a system for providing target values for a functional approximator <ref> [7] </ref> [9]. It best thought of as a combination of dynamic programming (DP) and monte carlo (MC) methods. While the RL system is solving a problem it is learning approximate values for each state that it encounters, and in many ways gains the best of both DP and MC. <p> First, since the system is learning to associate the value of a state-action pair with the value of the succeeding state, eventually values will propagate back to the current state. Unfortunately, this process can be slow. Therefore, a set of learning procedures known as TD () <ref> [7] </ref> have been developed. This adjusts the value of a state-action pair to be equal to a combination on the values of the next several states visited, giving decreasing weight to successive states. Drawbacks include having to keep track of what states have been visited.
Reference: [8] <author> R. Sutton. </author> <title> Generalization in reinforcement learning: Successful examples using sparse coarse coding. </title> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <address> 8:10381044, </address> <year> 1996. </year>
Reference-contexts: Using intermediate values of Lambda controls how far to propagate values, and has been shown to be somewhat better than TD (0) and much better than TD (1) <ref> [8] </ref>. By learning the value of performing teaching actions in a state, the system is learning to customize its behavior to the student. Once the systems learns how to compute the expected gain for using each teaching interaction, a teaching policy can be constructed.
Reference: [9] <author> R. Sutton and A. Barto. </author> <title> An Introduction to Reinforcement Learning. </title> <publisher> MIT Press, </publisher> <year> 1997. </year>
Reference-contexts: For these reasons we have chosen to use reinforcement learning as a general framework. 3.1 Reinforcement Learning Reinforcement learning (RL) is a system for providing target values for a functional approximator [7] <ref> [9] </ref>. It best thought of as a combination of dynamic programming (DP) and monte carlo (MC) methods. While the RL system is solving a problem it is learning approximate values for each state that it encounters, and in many ways gains the best of both DP and MC. <p> RL is agnostic on which ML algorithm should be used to compute the value of a state. Successful systems have been built using neural networks [10], linear function approximators (LFA), and lookup tables <ref> [9] </ref>. Given that the amount of training data is not likely to be large, using a simpler scheme such as an LFA seems prudent. If the problem is small enough, a lookup table that stores learned values may be applicable.
Reference: [10] <author> G. Tesauro. </author> <title> Temporal difference learning and TD-Gammon. </title> <journal> Communications of the ACM, </journal> <volume> 38(3), </volume> <year> 1995. </year>
Reference-contexts: That is, it guesses the value of the current state from its guess about the value of adjacent states. Systems built with RL have outperformed expert humans players in backgammon <ref> [10] </ref>, and have demonstrated superior performance in elevator scheduling compared to commercial, hand crafted algorithms [3]. These results suggest the potential for impressive performance by these systems. <p> The goal of the learner is to maximize the amount of rewards received over the long term. RL is agnostic on which ML algorithm should be used to compute the value of a state. Successful systems have been built using neural networks <ref> [10] </ref>, linear function approximators (LFA), and lookup tables [9]. Given that the amount of training data is not likely to be large, using a simpler scheme such as an LFA seems prudent. If the problem is small enough, a lookup table that stores learned values may be applicable.
Reference: [11] <author> K. VanLehn, S. Ohlsson, and R. Nason. </author> <title> Applications of simulated students: An exploration. </title> <journal> Journal of Artificial Intelligence in Education, </journal> <volume> 5(2):135175, </volume> <year> 1996. </year>
Reference: [12] <author> B. Widrow and M. Hoff. </author> <title> Adaptive Signal Processing. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1960. </year>
Reference-contexts: Once the reward is received some method of moving the estimate of the previous state towards the reward is needed. The simplest such method is simply to use the Widrow-Hoff rule <ref> [12] </ref>: w t = ff (z w T x t )x t , where ff is the step size (or learning rate) parameter.
References-found: 12

