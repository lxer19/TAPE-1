URL: http://www-anw.cs.umass.edu/~mharmon/rltutorial/docs/sim_tech.ps
Refering-URL: http://www-anw.cs.umass.edu/~mharmon/rltutorial/tut.html
Root-URL: 
Email: harmonme@aa.wpafb.mil  baird@cs.usafa.af.mil  
Title: MultiPlayer Residual Advantage Learning With General Function Approximation  
Author: Mance E. Harmon Leemon C. Baird III 
Note: 2241 Avionics Circle Wright-Patterson Air Force  
Address: WL/AACF  OH 45433-7308  2354 Fairchild Dr. Suite 6K41 USAFA, CO 80840-6234  
Affiliation: Wright Laboratory  Base,  U.S.A.F. Academy  
Abstract: A new algorithm, advantage learning, is presented that improves on advantage updating by requiring that a single function be learned rather than two. Furthermore, advantage learning requires only a single type of update, the learning update, while advantage updating requires two different types of updates, a learning update and a normilization update. The reinforcement learning system uses the residual form of advantage learning. An application of reinforcement learning to a Markov game is presented. The testbed has continuous states and nonlinear dynamics. The game consists of two players, a missile and a plane; the missile pursues the plane and the plane evades the missile. On each time step , each player chooses one of two possible actions; turn left or turn right, resulting in a 90 degree instantaneous change in the aircraft s heading. Reinforcement is given only when the missile hits the plane or the plane reaches an escape distance from the missile. The advantage function is stored in a single-hidden-layer sigmoidal network. Speed of learning is increased by a new algorithm , Incremental Delta-Delta (IDD), which extends Jacobs (1988) Delta-Delta for use in incremental training, and differs from Suttons Incremental Delta-Bar-Delta (1992) in that it does not require the use of a trace and is amenable for use with general function approximation systems. The advantage learning algorithm for optimal control is modified for games in order to find the minimax point, rather than the maximum. Empirical results gathered using the missile/aircraft testbed validate theory that suggests residual forms of reinforcement learning algorithms converge to a local minimum of the mean squared Bellman residual when using general function approximation systems. Also, to our knowledge, this is the first time an approximate second order method has been used with residual algorithms. Empirical results are presented comparing convergence rates with and without the use of IDD for the reinforcement learning testbed described above and for a supervised learning testbed. The results of these experiments demonstrate IDD increased the rate of convergence and resulted in an order of magnitude lower total asymptotic error than when using backpropagation alone. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Baird, L.C. </author> <year> (1993). </year> <title> Advantage updating Wright-Patterson Air Force Base, </title> <institution> OH. ( Wright Laboratory Technical Report WL-TR-93-1146, </institution> <note> available from the Defense Technical information Center, </note> <institution> Cameron Station, </institution> <address> Alexandria, VA 22304-6145). </address>
Reference-contexts: of learned behavior, a comparison of the system s ability to reduce the mean squared Bellman error for different values of f (including an adaptive f ), and a comparison of the systems performance with and without the use of IDD. 2 BACKGROUND 2.1 Advantage Updating The advantage updating algorithm <ref> (Baird, 1993) </ref> is a reinforcement learning algorithm in which two types of information are stored. For each state x , the value V (x) is stored, representing an estimate of the total discounted return expected when starting in state x and performing optimal 2 actions. <p> Advantage updating has been shown to learn faster than Q-learning ( Watkins, 1989), especially for continuous-time problems <ref> (Baird, 1993, Harmon, Baird, & Klopf, 1995) </ref>. 2.2 Advantage Learning Advantage learning improves on advantage updating by requiring only a single function to be stored, the advantage function A ( x , u ).
Reference: <author> Baird, L. C. </author> <year> (1995). </year> <title> Residual Algorithms: Reinforcement Learning with Function Approximation. </title> <editor> In Armand Prieditis & Stuart Russell, eds. </editor> <booktitle> Machine Learning: Proceedings of the Twelfth International Conference , 9-12 July, </booktitle> <publisher> Morgan Kaufman Publishers, </publisher> <address> San Francisco, CA. </address>
Reference-contexts: We propose a simpler algorithm, advantage learning , which retains the properties of advantage updating but requires only one function to be learned rather than two. A faster class of algorithms, residual algorithms, is proposed in <ref> (Baird, 95) </ref>. We present empirical results demonstrating the residual form of advantage learning solving a nonlinear game using a general neural network. The game is a Markov decision process (MDP) with continuous states and nonlinear dynamics. <p> Such an algorithm would cause the mean squared Bellman residual to decrease monotonically, but would not necessarily follow the negative gradient, which would be the path of steepest descent. Therefore, it would be reasonable to refer to such algorithms as residual algorithms <ref> (Baird, 1995) </ref>. There is the question of how to choose f appropriately. One approach is to treat it as a constant, like the learning rate constant.
Reference: <author> Barto, A. G., Sutton, R. S., & Watkins, C. J. C. H. </author> <year> (1989). </year> <title> Learning and sequential decision making. </title> <type> Technical Report 89-95, </type> <institution> Department of Computer and Information Science, Univeristy of Massachusetts, Amherst, Massachusetts. </institution> <note> Also published in Learning and Computational Neuroscience: </note> <editor> Foundations of Adaptive Networks , Michael Gabriel and John Moore, editors. </editor> <publisher> MIT Press, </publisher> <address> Cambridge MA (1991). </address>
Reference-contexts: Fortunately, it is easy to calculate the f that ensures a decreasing mean squared residual, while bringing the weight change vector as close to the direct algorithm as possible (described in Section 6). 4 MULTIPLAYER GAMES The theory of Markov decision processes <ref> (Barto et al. , 1989, Howard 1960) </ref> is the basis for most of the recent reinforcement learning theory. However, this body of theory assumes that the learning systems environment is stationary and, therefore, contains no other adaptive systems.
Reference: <author> Boyan, J.A., and Moore, A.W. </author> <year> (1995). </year> <title> Generalization in reinforcement learning: Safely approximating the value function. </title> <editor> In Tesauro, G., Touretzky, D.S., and Leen, T.K. (eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 7 . MIT Press, </booktitle> <address> Cambridge MA. </address>
Reference-contexts: could also be called the direct implementation of incremental value iteration, Q learning, and advantage learning. 3.2 Residual Gradient Algorithms Reinforcement learning algorithms can be guaranteed to converge for lookup tables, yet be unstable for function-approximation systems that have even a small amount of generalization when using the direct implementation <ref> (Boyan, 95) </ref>. To find an algorithm that is more stable than the direct algorithm, it is useful to specify the exact goal for the learning system.
Reference: <author> Isaacs, </author> <title> Rufus (1965). Differential games . New York: </title> <publisher> John Wiley and Sons, Inc. </publisher>
Reference-contexts: However, this body of theory assumes that the learning systems environment is stationary and, therefore, contains no other adaptive systems. Game theory (von Neumann and Morgenstern, 1947) is explicitly designed for reasoning about multiplayer environments. Differential games <ref> ( Isaacs, 1965) </ref> are games played in continuous time, or use sufficiently small time steps to approximate continuous time. Both players evaluate the given state and simultaneously execute an action, with no knowledge of the other player's selected action.
Reference: <author> Jacobs, R. A. </author> <year> (1988). </year> <title> Increased rates of convergence through learning rate adaptation. Neural Networks 1 , pp. </title> <type> 295-307. 13 Harmon, M.E., Baird, L.C, </type> & <institution> Klopf, A.H. </institution> <year> (1995). </year> <title> Advantage updating applied to a differential game. </title> <editor> In Tesauro, G., Touretzky, D.S., and Leen, T.K. (eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 7 . MIT Press, </booktitle> <address> Cambridge MA. </address>
Reference-contexts: If a saddlepoint does not exist, this assumption confers a slight advantage to player B. 5 INCREMENTAL DELTA-DELTA (IDD) FOR NONLINEAR FUNCTION APPROXIMATORS 5.1 IDD Derivation Incremental Delta-Bar-Delta (IDBD) was proposed by Sutton (1992) as an extension to the Delta-Bar-Delta algorithm <ref> ( Jacobs, 1988) </ref> that makes the algorithm amenable to incremental tasks (learning tasks in which examples are processed one by one and then discarded). The IDBD algorithm was described by Sutton as a metalearning algorithm in the sense that it learns the learning-rate parameters of an underlying base learning system.
Reference: <author> Howard, R. A. </author> <year> (1960). </year> <title> Dynamic Programming and Markov Processes . MIT Press, </title> <address> Cambridge MA. </address>
Reference: <author> Millington, P. J. </author> <year> (1991). </year> <title> Associative reinforcement learning for optimal control . Unpublished master's thesis, </title> <institution> Massachusetts Institute of Technology, </institution> <address> Cambridge, MA. </address>
Reference: <author> Rajan, N., Prasad, U. R., and Rao, N. J. </author> <year> (1980). </year> <title> Pursuit-evasion of two aircraft in a horizontal plane. </title> <journal> Journal of Guidance and Control . 3 (3), </journal> <volume> May-June, </volume> <pages> 261-267. </pages>
Reference: <author> Rumelhart, D., Hinton, G., & Williams, R. </author> <year> (1986). </year> <title> Learning representations by backpropagating errors. </title> <booktitle> Nature . 323 , 9 October, </booktitle> <pages> 533-536. </pages>
Reference-contexts: there is only one possible action), an obvious algorithm is an incremental form of value iteration, which is defined as: [ ] (3) If V (x) is represented by a function-approximation system other than a lookup table, update (3) can be implemented directly by combining it with the backpropagation algorithm <ref> ( Rumelhart, Hinton, & Williams, 86) </ref>.
Reference: <author> Sutton, R. S. </author> <year> (1992). </year> <title> Adapting bias by gradient descent: an incremental version of delta-bar-delta. </title> <booktitle> In Proceedings of the Tenth National Conference on Machine Learning . MIT Press, </booktitle> <pages> pp. 171-176, </pages> <address> Cambridge MA. </address>
Reference: <author> Tesauro, G. </author> <year> (1990). </year> <title> Neurogammon: A neural-network backgammon program. </title> <booktitle> Proceedings of the International Joint Conference on Neural Networks (pp. </booktitle> <pages> 33-40). </pages> <address> San Diego, CA. </address>
Reference-contexts: For this reason, our metric is reduced to simple observation of the system behavior, and is analogous to the metric used to evaluate Tesauros TD-Gammon <ref> ( Tesauro, 1990) </ref>. Also, it is possible this game might be made less difficult to solve if expressed in an appropriate coordinate system, such as plane and missile centered polar coordinates.
Reference: <author> Von Neumann, J., and Morgenstern, O. </author> <year> (1947). </year> <institution> Theory of Games and Economic Behavior . Princeton University Press, Princeton NJ. </institution>
Reference-contexts: However, this body of theory assumes that the learning systems environment is stationary and, therefore, contains no other adaptive systems. Game theory <ref> (von Neumann and Morgenstern, 1947) </ref> is explicitly designed for reasoning about multiplayer environments. Differential games ( Isaacs, 1965) are games played in continuous time, or use sufficiently small time steps to approximate continuous time.
Reference: <author> Watkins, C. J. C. H. </author> <year> (1989). </year> <title> Learning from delayed rewards . Doctoral thesis, </title> <address> Cambridge University, Cambridge, England. </address>
Reference-contexts: Advantage updating has been shown to learn faster than Q-learning <ref> ( Watkins, 1989) </ref>, especially for continuous-time problems (Baird, 1993, Harmon, Baird, & Klopf, 1995). 2.2 Advantage Learning Advantage learning improves on advantage updating by requiring only a single function to be stored, the advantage function A ( x , u ).
Reference: <author> Widrow, B., and Stearns, S. D. </author> <year> (1985). </year> <title> Adaptive Signal Processing . Englewood Cliffs, </title> <address> NJ: </address> <publisher> Prentice-Hall. </publisher>
Reference-contexts: The IDBD algorithm was described by Sutton as a metalearning algorithm in the sense that it learns the learning-rate parameters of an underlying base learning system. In Sutton (1992), the base 5 learning system was the Least-Mean-Square (LMS) rule, also known as the Widrow-Hoff rule <ref> (Widrow and Stearns, 1985) </ref>, and the IDBD algorithm was derived for linear function approximation systems. Here, we present an extension to Jacobs (1988) Delta-Delta algorithm that is appropriate for incremental training when using nonlinear function approximation systems.
References-found: 15

