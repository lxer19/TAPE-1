URL: ftp://ftp.cs.umd.edu/pub/realtime/sigplan-PLDI93.ps.gz
Refering-URL: http://www.cs.umd.edu/projects/TimeWare/TimeWare-index-no-abs.html
Root-URL: 
Title: Compiling Real-Time Programs into Schedulable Code  
Author: Seongsoo Hong and Richard Gerber 
Address: College Park, MD 20742  
Affiliation: Department of Computer Science University of Maryland  
Note: In Proc. of ACM SIGPLAN Programming Language Design and Implementation, pp.  
Email: sshong@cs.umd.edu rich@cs.umd.edu  
Phone: (301) 405-2710  
Date: 166-176, June, 1993  
Abstract: We present a programming language with first-class timing constructs, whose semantics is based on time-constrained relationships between observable events. Since a system specification postulates timing relationships between events, realizing the specification in a program becomes a more straightforward process. Using these constraints, as well as those imposed by data and control flow properties, our objective is to transform the code so that its worst-case execution time is consistent with its real-time requirements. To accomplish this goal we first translate an event-based source program into intermediate code, in which the timing constraints are imposed on the code itself, and then use a compilation technique which synthesizes feasible code from the original source program. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Ad-dison Wesley Publishing Company, </publisher> <year> 1986. </year>
Reference-contexts: After a timing constraint (or a loop) has been synthesized, it gets treated as a single node in the surrounding control flow graph. Of course, to claim that loops can be reduced into single representatives, the flow graph of the program must be reducible <ref> [1] </ref>. Since "structured" programs without unrestricted goto's lead to reducible flow graphs, without loss of generality we assume that our programs possess this property. While our algorithm is "bottom-up," there is also a "top-down" component to handle nested constraints. That is, the innermost constructs are synthesized first.
Reference: [2] <author> Alexander Aiken and Alexandru Nicolau. </author> <title> A development environment for horizontal microcode. </title> <journal> IEEE Transactions on Software Engineering, </journal> <pages> pages 584-594, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: Such a process is similar to that of code scheduling, which is a well-defined problem for automatic fine-grain (instruction level) parallelization for superscalar and VLIW processors <ref> [2, 6, 7, 8, 16, 20] </ref>. However, we cannot directly use the techniques such as Trace Scheduling [7] or Percolation Scheduling [2, 6], as our problem context has a different goal. <p> Such a process is similar to that of code scheduling, which is a well-defined problem for automatic fine-grain (instruction level) parallelization for superscalar and VLIW processors [2, 6, 7, 8, 16, 20]. However, we cannot directly use the techniques such as Trace Scheduling [7] or Percolation Scheduling <ref> [2, 6] </ref>, as our problem context has a different goal. In what follows, we present a code scheduling algorithm which achieves the advantages of both techniques by the means of SSA form translation and section-wise trace scheduling. <p> Percolation Scheduling <ref> [2] </ref> addresses this problem, and exploits the strategy of unification transformation as a solution. This transformation allows identical instructions from different nodes in a CFG to be merged, if possible.
Reference: [3] <author> R. Cytron, J. Ferrante, B. K. Rosen, M. N. Weg-man, and F. K. Zadeck. </author> <title> Efficiently computing static single assignment form and the control dependence graph. </title> <journal> ACM Transactions on Programming Languages and systems, </journal> <volume> 9 </volume> <pages> 319-345, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: This second part can be lifted out of the do statement altogether. The compilation stages are outlined in Figure 2. The machine-independent pass translates the source program into intermediate code in static single assignment form (for an overview of the SSA form, see <ref> [3] </ref>). Then, consistency analysis is performed on the timing constraints themselves, under the assumption that the code's execution time is 0. The constraints are consistent if and only if the "within" modifiers do not contradict the "after" modifiers. As a simple example, consider Program A.
Reference: [4] <author> Ron Cytron, Andy Lowry, and Kenneth Zadeck. </author> <title> Code motion of control structures in high-level languages. </title> <booktitle> In Conference Record 13th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 70-85. </pages> <publisher> ACM, </publisher> <month> January </month> <year> 1986. </year>
Reference-contexts: These times are generated by a timing analysis tool, such as those found in [15, 18, 19]. Our synthesis phase depends heavily on the technique of code motion, one of the most frequently used transformations in optimizing compilers <ref> [4, 7, 8] </ref>. Our algorithm pushes this transformation to the limit, in that it "greedily" performs inter-basic block optimizations. Such a technique proves quite fruitful when attempting to achieve real-time feasibility. The code generation pass completes the compilation of a real-time program.
Reference: [5] <author> B. Dasarathy. </author> <title> Timing constraints of real-time systems: Constructs for expressing them, method for validating them. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 11(1) </volume> <pages> 80-86, </pages> <month> January </month> <year> 1985. </year>
Reference-contexts: Thus, their timing constructs have not been adopted in any production-level programming languages. We believe the reason is straightforward: Language constructs such as "within 10ms do B" establish constraints on blocks of code. However, "true" real-time properties establish constraints between the occurrences of events <ref> [5, 10] </ref>. These constraints typically arise from a requirements specification, or from a detailed analysis of the application environment. While language-based constraints are very sensitive to a program's execution time, specification-based constraints must be maintained regardless of the platform's CPU characteristics, memory cycle times, bus arbitration delays, etc.
Reference: [6] <author> Kemal Ebcioglu and Alexandru Nicolau. </author> <title> A global resource-constrained parallelization technique. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <pages> pages 154-163. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1989. </year>
Reference-contexts: Such a process is similar to that of code scheduling, which is a well-defined problem for automatic fine-grain (instruction level) parallelization for superscalar and VLIW processors <ref> [2, 6, 7, 8, 16, 20] </ref>. However, we cannot directly use the techniques such as Trace Scheduling [7] or Percolation Scheduling [2, 6], as our problem context has a different goal. <p> Such a process is similar to that of code scheduling, which is a well-defined problem for automatic fine-grain (instruction level) parallelization for superscalar and VLIW processors [2, 6, 7, 8, 16, 20]. However, we cannot directly use the techniques such as Trace Scheduling [7] or Percolation Scheduling <ref> [2, 6] </ref>, as our problem context has a different goal. In what follows, we present a code scheduling algorithm which achieves the advantages of both techniques by the means of SSA form translation and section-wise trace scheduling.
Reference: [7] <author> Joseph A Fisher. </author> <title> Trace scheduling: A technique for global microcode compaction. </title> <journal> IEEE Transactions on Computer, </journal> <volume> 30 </volume> <pages> 478-490, </pages> <month> July </month> <year> 1981. </year>
Reference-contexts: These times are generated by a timing analysis tool, such as those found in [15, 18, 19]. Our synthesis phase depends heavily on the technique of code motion, one of the most frequently used transformations in optimizing compilers <ref> [4, 7, 8] </ref>. Our algorithm pushes this transformation to the limit, in that it "greedily" performs inter-basic block optimizations. Such a technique proves quite fruitful when attempting to achieve real-time feasibility. The code generation pass completes the compilation of a real-time program. <p> Such a process is similar to that of code scheduling, which is a well-defined problem for automatic fine-grain (instruction level) parallelization for superscalar and VLIW processors <ref> [2, 6, 7, 8, 16, 20] </ref>. However, we cannot directly use the techniques such as Trace Scheduling [7] or Percolation Scheduling [2, 6], as our problem context has a different goal. <p> Such a process is similar to that of code scheduling, which is a well-defined problem for automatic fine-grain (instruction level) parallelization for superscalar and VLIW processors [2, 6, 7, 8, 16, 20]. However, we cannot directly use the techniques such as Trace Scheduling <ref> [7] </ref> or Percolation Scheduling [2, 6], as our problem context has a different goal. In what follows, we present a code scheduling algorithm which achieves the advantages of both techniques by the means of SSA form translation and section-wise trace scheduling. <p> If either violates its duration constraint, the algorithm attempts to reduce the surplus execution time of the section. To accomplish this, we have adapted a technique from the approach to Trace Scheduling found in <ref> [7] </ref>. In our algorithm, instructions lying on paths that exceed their section's duration constraints are considered for code motion. We distinguish such paths as critical traces. <p> j path in S3g; while (wt (t) &gt; dur) f perform Code Scheduling into S1 on t; if (t is still critical) then exit ("Unable to synthesize."); recompute t such that wt (t) = maxfwt (path) j path in S3g; g behind instead of moving it (unlike the approach in <ref> [7] </ref>). In Figure 10 (D), no bookkeeping is needed, since nothing has been changed in code having data precedence from "x = a+1" as a result of copying "if c".
Reference: [8] <author> Franco Gasperoni. </author> <title> Compilation techniques for VLIW architectures. </title> <type> Technical Report RC 14915(#66741), </type> <institution> IBM T. J. Watson Research Center, </institution> <month> September </month> <year> 1989. </year>
Reference-contexts: These times are generated by a timing analysis tool, such as those found in [15, 18, 19]. Our synthesis phase depends heavily on the technique of code motion, one of the most frequently used transformations in optimizing compilers <ref> [4, 7, 8] </ref>. Our algorithm pushes this transformation to the limit, in that it "greedily" performs inter-basic block optimizations. Such a technique proves quite fruitful when attempting to achieve real-time feasibility. The code generation pass completes the compilation of a real-time program. <p> Such a process is similar to that of code scheduling, which is a well-defined problem for automatic fine-grain (instruction level) parallelization for superscalar and VLIW processors <ref> [2, 6, 7, 8, 16, 20] </ref>. However, we cannot directly use the techniques such as Trace Scheduling [7] or Percolation Scheduling [2, 6], as our problem context has a different goal.
Reference: [9] <author> Y. Ishikawa, H. Tokuda, and C. W. Mercer. </author> <title> Object-oriented real-time language design: Constructs for timing constraints. </title> <booktitle> In Proceedings of OOPSLA-90, </booktitle> <pages> pages 289-298, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: Recently, experimental languages have been proposed which provide first-class, real-time constructs <ref> [9, 12, 13, 14, 17] </ref>. An example of such a construct is "within 10ms do B," where the block of code "B" must be executed within 10 milliseconds. This constraint is, in turn, conveyed to the real-time scheduler as a directive.
Reference: [10] <author> F. Jahanian and Al Mok. </author> <title> Safety analysis of timing properties in real-time systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 12(9) </volume> <pages> 890-904, </pages> <month> Septem-ber </month> <year> 1986. </year>
Reference-contexts: Thus, their timing constructs have not been adopted in any production-level programming languages. We believe the reason is straightforward: Language constructs such as "within 10ms do B" establish constraints on blocks of code. However, "true" real-time properties establish constraints between the occurrences of events <ref> [5, 10] </ref>. These constraints typically arise from a requirements specification, or from a detailed analysis of the application environment. While language-based constraints are very sensitive to a program's execution time, specification-based constraints must be maintained regardless of the platform's CPU characteristics, memory cycle times, bus arbitration delays, etc.
Reference: [11] <author> Kevin B. Kenny and Kwei-Jay Lin. </author> <title> Building flexible real-time systems using the Flex language. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 70-78, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: In the next subsection we discuss our code-scheduling techniques to handle the cases in which one of these conditions fails to hold. Using the derived constraints, we annotate the sections with their code-based timing properties. We adopt mechanism similar to that found in Flex language <ref> [11] </ref>, in which blocks are designated with a label, and tagged with two sublabels, start and finish. Timing constraints are expressed as conjunctions of linear inequalities between the start and finish sublabels of different sections.
Reference: [12] <author> E. Kligerman and A. D. Stoyenko. </author> <title> Real-time Euclid: A language for reliable real-time systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 12 </volume> <pages> 941-949, </pages> <month> September </month> <year> 1986. </year>
Reference-contexts: Recently, experimental languages have been proposed which provide first-class, real-time constructs <ref> [9, 12, 13, 14, 17] </ref>. An example of such a construct is "within 10ms do B," where the block of code "B" must be executed within 10 milliseconds. This constraint is, in turn, conveyed to the real-time scheduler as a directive.
Reference: [13] <author> Insup Lee and Vijay Gehlot. </author> <title> Language constructs for real-time programming. </title> <booktitle> In Proceedings IEEE Real-Time Systems Symposium, </booktitle> <pages> pages 57-66. </pages> <publisher> IEEE, </publisher> <year> 1985. </year>
Reference-contexts: Recently, experimental languages have been proposed which provide first-class, real-time constructs <ref> [9, 12, 13, 14, 17] </ref>. An example of such a construct is "within 10ms do B," where the block of code "B" must be executed within 10 milliseconds. This constraint is, in turn, conveyed to the real-time scheduler as a directive. <p> Both constructs are syntactic descendents of the temporal scope, first introduced in <ref> [13] </ref>. However, as we have stated, our semantics is quite different, in that it relies on constrained relationships between observable events. We first elaborate the "do" construct used in Section 1, which establishes several types of relative timing constraints.
Reference: [14] <author> K. J. Lin and S. Natarajan. </author> <title> Expressing and maintaining timing constraints in FLEX. </title> <booktitle> In Proceedings IEEE Real-Time Systems Symposium, </booktitle> <month> Decem-ber </month> <year> 1988. </year>
Reference-contexts: Recently, experimental languages have been proposed which provide first-class, real-time constructs <ref> [9, 12, 13, 14, 17] </ref>. An example of such a construct is "within 10ms do B," where the block of code "B" must be executed within 10 milliseconds. This constraint is, in turn, conveyed to the real-time scheduler as a directive.
Reference: [15] <author> Al Mok and et al. </author> <title> Evaluating tight execution time bounds of programs by annotations. </title> <booktitle> In 6th Workshop on Real-Time Operating Systems and Software, </booktitle> <pages> pages 74-80. </pages> <publisher> IEEE, </publisher> <month> May </month> <year> 1989. </year>
Reference-contexts: That is, observable events occur instantaneously during the executions of all "send" and "receive" operations. The timing parameters are the minimum and maximum execution times for each instruction on the given CPU. These times are generated by a timing analysis tool, such as those found in <ref> [15, 18, 19] </ref>. Our synthesis phase depends heavily on the technique of code motion, one of the most frequently used transformations in optimizing compilers [4, 7, 8]. Our algorithm pushes this transformation to the limit, in that it "greedily" performs inter-basic block optimizations.
Reference: [16] <author> Alexandru Nicolau. </author> <title> Parallelism, Memory Antialiasing and Correctness Issues for a Trace Scheduling Compiler. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <month> June </month> <year> 1984. </year>
Reference-contexts: Such a process is similar to that of code scheduling, which is a well-defined problem for automatic fine-grain (instruction level) parallelization for superscalar and VLIW processors <ref> [2, 6, 7, 8, 16, 20] </ref>. However, we cannot directly use the techniques such as Trace Scheduling [7] or Percolation Scheduling [2, 6], as our problem context has a different goal. <p> However, the code explosion associated with trace scheduling mandated our technique to reduce bookkeeping. The proof that our algorithm terminates is similar to the termination proof for trace scheduling found in <ref> [16] </ref>. In our case termination will either occur when all movable code is depleted, or preferably, when all traces in a section become non-critical.
Reference: [17] <author> V. Nirkhe, S. Tripathi, and A. Agrawala. </author> <title> Language support for the Maruti real-time system. </title> <booktitle> In Proceedings IEEE Real-Time Systems Symposium, </booktitle> <month> De-cember </month> <year> 1990. </year>
Reference-contexts: Recently, experimental languages have been proposed which provide first-class, real-time constructs <ref> [9, 12, 13, 14, 17] </ref>. An example of such a construct is "within 10ms do B," where the block of code "B" must be executed within 10 milliseconds. This constraint is, in turn, conveyed to the real-time scheduler as a directive.
Reference: [18] <author> ChangYun Park and Alan C. Shaw. </author> <title> Experimenting with a program timing tool based on source-level timing schema. </title> <booktitle> In Proceedings IEEE Real-Time Systems Symposium, </booktitle> <pages> pages 72-81, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: That is, observable events occur instantaneously during the executions of all "send" and "receive" operations. The timing parameters are the minimum and maximum execution times for each instruction on the given CPU. These times are generated by a timing analysis tool, such as those found in <ref> [15, 18, 19] </ref>. Our synthesis phase depends heavily on the technique of code motion, one of the most frequently used transformations in optimizing compilers [4, 7, 8]. Our algorithm pushes this transformation to the limit, in that it "greedily" performs inter-basic block optimizations.
Reference: [19] <author> Alan C. Shaw. </author> <title> Reasoning about time in higher level language software. </title> <journal> IEEE Transactions on Software Engineering, </journal> <pages> pages 875-889, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: That is, observable events occur instantaneously during the executions of all "send" and "receive" operations. The timing parameters are the minimum and maximum execution times for each instruction on the given CPU. These times are generated by a timing analysis tool, such as those found in <ref> [15, 18, 19] </ref>. Our synthesis phase depends heavily on the technique of code motion, one of the most frequently used transformations in optimizing compilers [4, 7, 8]. Our algorithm pushes this transformation to the limit, in that it "greedily" performs inter-basic block optimizations. <p> We use the timing schema approach in <ref> [19] </ref> to determine the wt function.
Reference: [20] <author> Michael Smith, Mark Horowitz, and Monica Lam. </author> <title> Efficient superscalar performance through boosting. </title> <booktitle> In Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 248-259. </pages> <publisher> ACM, </publisher> <month> October </month> <year> 1992. </year>
Reference-contexts: Such a process is similar to that of code scheduling, which is a well-defined problem for automatic fine-grain (instruction level) parallelization for superscalar and VLIW processors <ref> [2, 6, 7, 8, 16, 20] </ref>. However, we cannot directly use the techniques such as Trace Scheduling [7] or Percolation Scheduling [2, 6], as our problem context has a different goal.
References-found: 20

