URL: http://www.neci.nj.nec.com/homepages/giles/papers/UMD-CS-TR-3599.fuzzy.automata.encoding.recurrent.nets.ps.Z
Refering-URL: http://www.neci.nj.nec.com/homepages/giles/papers/
Root-URL: http://www.neci.nj.nec.com
Title: Fuzzy Finite-State Automata Can Be Deterministically Encoded Into Recurrent Neural Networks  
Author: Christian W. Omlin a Karvel K. Thornber a C. Lee Giles a;b 
Address: Princeton, NJ 08540  College Park, MD 20742  
Affiliation: a NEC Research Institute,  b UMIACS, U. of Maryland,  U. of Maryland  
Pubnum: Technical Report CS-TR-3599 and UMIACS-96-12  
Abstract: There has been an increased interest in combining fuzzy systems with neural networks because fuzzy neural systems merge the advantages of both paradigms. On the one hand, parameters in fuzzy systems have clear physical meanings, and rule-based and linguistic information can be incorporated into adaptive fuzzy systems in a systematic way. On the other hand, there exist powerful algorithms for training various neural network models. However, most of the proposed combined architectures are only able to process static input-output relationships; they are not able to process temporal input sequences of arbitrary length. Fuzzy finite-state automata (FFAs) can model dynamical processes whose current state depends on the current input and previous states. Unlike in the case of deterministic finite-state automata (DFAs), FFAs are not in one particular state, rather each state is occupied to some degree defined by a membership function. Based on previous work on encoding DFAs in discrete-time, second-order recurrent neural networks, we propose an algorithm that constructs an augmented recurrent neural network that encodes a FFA and recognizes a given fuzzy regular language with arbitrary accuracy. We then empirically verify the encoding methodology by correct string recognition of randomly generated FFAs. In particular, we examined how the networks' performance varies as a function of synaptic weight strengths. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> N. Alon, A. Dewdney, and T. Ott, </author> <title> "Efficient simulation of finite automata by neural nets," </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> vol. 38, no. 2, </volume> <pages> pp. 495-514, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: State 1 is the DFA's start and accepting state. 2.2 Network Construction Various methods have been proposed for implementing DFAs in recurrent neural networks <ref> [1, 2, 15, 14, 25, 35, 39] </ref>. <p> Definition 3.3 A fuzzy finite-state automaton (FFA) f M is a 6-tuple f M =&lt; ; Q; Z; e R; ffi; ! &gt; where and Q are the same as in DFAs; Z is a finite output alphabet, e R is the fuzzy initial state, ffi : fi Q fi <ref> [0; 1] </ref> ! Q is the fuzzy transition map and ! : Q ! Z is the output map. <p> greatly simplifies the encoding of FFAs in recurrent networks with continuous discriminant functions: Theorem 3.2 Given a regular fuzzy automaton f M , there exists a deterministic finite-state automaton M with output alphabet Z f : is a production weightg [ f0g which computes the membership function : fl ! <ref> [0; 1] </ref> of the language L ( f M ). The algorithm, as given in [54], is shown in Fig. 6. An example of FFA-to-DFA transformation is shown in Fig. 7a 6 . <p> On the other hand, FFAs can be in several states at any given time with different degrees of vagueness; vagueness is specified by a real number from the interval <ref> [0; 1] </ref>. Theorem 3.2 enables us to transform any FFA into a deterministic automaton which computes the same membership function : fl ! [0; 1]. We just need to demonstrate how to implement the computation of with continuous discriminant functions. <p> other hand, FFAs can be in several states at any given time with different degrees of vagueness; vagueness is specified by a real number from the interval <ref> [0; 1] </ref>. Theorem 3.2 enables us to transform any FFA into a deterministic automaton which computes the same membership function : fl ! [0; 1]. We just need to demonstrate how to implement the computation of with continuous discriminant functions. For that purpose, we augment the network architecture used for encoding DFAs with additional weights which connect the recurrent state neurons to a linear output neuron (Fig. 8).
Reference: [2] <author> R. Alquezar and A. Sanfeliu, </author> <title> "An algebraic framework to represent finite state machines in single-layer recurrent neural networks," </title> <journal> Neural Computation, </journal> <volume> vol. 7, no. 5, </volume> <editor> p. </editor> <volume> 931, </volume> <year> 1995. </year>
Reference-contexts: State 1 is the DFA's start and accepting state. 2.2 Network Construction Various methods have been proposed for implementing DFAs in recurrent neural networks <ref> [1, 2, 15, 14, 25, 35, 39] </ref>.
Reference: [3] <author> H. Berenji and P. Khedkar, </author> <title> "Learning and fine tuning fuzzy logic controllers through reinforcement," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 3, no. 5, </volume> <pages> pp. 724-740, </pages> <year> 1992. </year>
Reference-contexts: Neural network representations of fuzzy logic interpolation have also been used within the context of reinforcement learning <ref> [3] </ref>. Consider the following set of linguistic fuzzy rules: IF (x 1 is A 1 ) AND (x 2 is A 3 ) THEN y 1 is C 1 .
Reference: [4] <author> J. Bezdek, </author> <title> ed., </title> <journal> IEEE Transactions on Neural Networks Special Issue on Fuzzy Logic and Neural Networks, </journal> <volume> vol. 3. </volume> <booktitle> IEEE Neural Networks Council, </booktitle> <year> 1992. </year>
Reference-contexts: 1 Introduction 1.1 Fuzzy Systems and Neural Networks There has been an increased interest in combining artificial neural networks and fuzzy systems (see <ref> [4] </ref> for a collection of papers). Fuzzy logic [67] provides a mathematical foundation for approximate reasoning; fuzzy fl Revised September, 1996. 1 logic has proven very successful in a variety of applications [6, 8, 13, 22, 27, 28, 42, 65].
Reference: [5] <author> M. Casey, </author> <title> Computation in discrete-time dynamical systems. </title> <type> PhD thesis, </type> <institution> Department of Mathematics, University of California at San Diego, La Jolla, </institution> <address> CA, </address> <year> 1995. </year>
Reference: [6] <author> S. Chiu, S. Chand, D. Moore, and A. Chaudhary, </author> <title> "Fuzzy logic for control of roll and moment for a flexible wing aircraft," </title> <journal> IEEE Control Systems Magazine, </journal> <volume> vol. 11, no. 4, </volume> <pages> pp. 42-48, </pages> <year> 1991. </year>
Reference-contexts: Fuzzy logic [67] provides a mathematical foundation for approximate reasoning; fuzzy fl Revised September, 1996. 1 logic has proven very successful in a variety of applications <ref> [6, 8, 13, 22, 27, 28, 42, 65] </ref>. The parameters of adaptive fuzzy systems have clear physical meanings which facilitates the choice of their initial values. Furthermore, rule-based information can be incorporated into fuzzy systems in a systematic way.
Reference: [7] <author> A. Cleeremans, D. Servan-Schreiber, and J. McClelland, </author> <title> "Finite state automata and simple recurrent recurrent networks," </title> <journal> Neural Computation, </journal> <volume> vol. 1, no. 3, </volume> <pages> pp. 372-381, </pages> <year> 1989. </year>
Reference: [8] <author> J. Corbin, </author> <title> "A fuzzy logic-based financial transaction system," </title> <journal> Embedded Systems Programming, </journal> <volume> vol. 7, no. 12, </volume> <editor> p. </editor> <volume> 24, </volume> <year> 1994. </year>
Reference-contexts: Fuzzy logic [67] provides a mathematical foundation for approximate reasoning; fuzzy fl Revised September, 1996. 1 logic has proven very successful in a variety of applications <ref> [6, 8, 13, 22, 27, 28, 42, 65] </ref>. The parameters of adaptive fuzzy systems have clear physical meanings which facilitates the choice of their initial values. Furthermore, rule-based information can be incorporated into fuzzy systems in a systematic way.
Reference: [9] <author> G. Cybenko, </author> <title> "Approximation by superpositions of a sigmoidal function," Mathematics of Control, Signals, </title> <journal> and Systems, </journal> <volume> vol. 2, </volume> <pages> pp. 303-314, </pages> <year> 1989. </year>
Reference-contexts: Artificial neural networks have become valuable computational tools in their own right for tasks such as pattern recognition, control, and forecasting. Fuzzy systems and multilayer perceptrons are computationally equivalent, i.e. they are both universal approximators <ref> [9, 60] </ref>. Recurrent neural networks have been shown to be computationally at least as powerful as Turing machines [53, 52]; whether or not these results also apply to recurrent fuzzy systems remains an open question.
Reference: [10] <author> S. Das and M. Mozer, </author> <title> "A unified gradient-descent/clustering architecture for finite state machine induction," </title> <booktitle> in Advances in Neural Information Processing Systems 6 (J. </booktitle> <editor> Cowan, G. Tesauro, and J. Alspector, eds.), </editor> <address> (San Francisco, CA), </address> <pages> pp. 19-26, </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference: [11] <author> D. Dubois and H. Prade, </author> <title> Fuzzy sets and systems: </title> <journal> theory and applications, </journal> <volume> vol. </volume> <booktitle> 144 of Mathematics in Science and Engineering, </booktitle> <pages> pp. 220-226. </pages> <publisher> Academic Press, </publisher> <year> 1980. </year> <month> 20 </month>
Reference-contexts: Any fuzzy automaton as described in definition 3.3 is equivalent to a restricted fuzzy automaton <ref> [11] </ref>. Notice that a FFA reduces to a conventional DFA by restricting the transition weights to 1. 5 5 Under the common definition of DFAs, we have Z = f0; 1g (i.e., a string is either rejected or accepted). <p> We extend that definition of DFAs by making the string membership function fuzzy: DFA M =&lt; ; Q; R; F; ffi; Z; ! &gt; where Z is the finite set 13 As in the case of DFAs and regular grammars, there exist a correspondence between FFAs and fuzzy regular grammars <ref> [11] </ref>: Theorem 3.1 For a given fuzzy grammar e G, there exists a fuzzy automaton f M such that L ( e G) = L ( f M ). Our goal is to use only continuous (sigmoidal and linear) discriminant functions for the neural network implementation of FFAs.
Reference: [12] <author> J. Elman, </author> <title> "Finding structure in time," </title> <journal> Cognitive Science, </journal> <volume> vol. 14, </volume> <pages> pp. 179-211, </pages> <year> 1990. </year>
Reference: [13] <author> L. Franquelo and J. Chavez, "Fasy: </author> <title> A fuzzy-logic based tool for analog synthesis," </title> <journal> IEEE Transactions on Computer-Aided Design of Integrated Circuits, </journal> <volume> vol. 15, no. 7, </volume> <editor> p. </editor> <volume> 705, </volume> <year> 1996. </year>
Reference-contexts: Fuzzy logic [67] provides a mathematical foundation for approximate reasoning; fuzzy fl Revised September, 1996. 1 logic has proven very successful in a variety of applications <ref> [6, 8, 13, 22, 27, 28, 42, 65] </ref>. The parameters of adaptive fuzzy systems have clear physical meanings which facilitates the choice of their initial values. Furthermore, rule-based information can be incorporated into fuzzy systems in a systematic way.
Reference: [14] <author> P. Frasconi, M. Gori, and G.Soda, </author> <title> "Recurrent neural networks and prior knowledge for sequence processing: A constrained nondeterminstic approach," </title> <journal> Knowledge-Based Systems, </journal> <volume> vol. 8, no. 6, </volume> <pages> pp. 313-332, </pages> <year> 1995. </year>
Reference-contexts: State 1 is the DFA's start and accepting state. 2.2 Network Construction Various methods have been proposed for implementing DFAs in recurrent neural networks <ref> [1, 2, 15, 14, 25, 35, 39] </ref>.
Reference: [15] <author> P. Frasconi, M. Gori, M. Maggini, and G. </author> <title> Soda, "Representation of finite state automata in recurrent radial basis function networks," </title> <journal> Machine Learning, </journal> <volume> vol. 23, </volume> <pages> pp. 5-32, </pages> <year> 1996. </year>
Reference-contexts: State 1 is the DFA's start and accepting state. 2.2 Network Construction Various methods have been proposed for implementing DFAs in recurrent neural networks <ref> [1, 2, 15, 14, 25, 35, 39] </ref>.
Reference: [16] <author> B. Gaines and L. Kohout, </author> <title> "The logic of automata," </title> <journal> International Journal of General Systems, </journal> <volume> vol. 2, </volume> <pages> pp. 191-208, </pages> <year> 1976. </year>
Reference-contexts: Fuzzy grammars have been found to be useful in a variety of applications such as in the analysis of Xrays [43], in digital circuit design [34], and in the design of intelligent human-computer interfaces [49]. The fundamentals of FFAs have been in discussed in <ref> [16, 48, 64] </ref> without presenting a systematic method for machine synthesis. Neural network implementations of fuzzy automata have been proposed in the literature [20, 21, 29, 59]. The synthesis method proposed in [20] uses digital design technology to implement fuzzy representations of states and outputs.
Reference: [17] <author> C. Giles, C. Miller, D. Chen, H. Chen, G. Sun, and Y. Lee, </author> <title> "Learning and extracting finite state automata with second-order recurrent neural networks," </title> <journal> Neural Computation, </journal> <volume> vol. 4, no. 3, </volume> <editor> p. </editor> <volume> 380, </volume> <year> 1992. </year>
Reference: [18] <author> P. Goode and M. Chow, </author> <title> "A hybrid fuzzy/neural systems used to extract heuristic knowledge from a fault detection problem," </title> <booktitle> in Proceedings of the Third IEEE Conference on Fuzzy Systems, </booktitle> <volume> vol. III, </volume> <pages> pp. 1731-1736, </pages> <year> 1994. </year>
Reference-contexts: fuzzy systems which adopted some learning algorithms; there exist backpropagation training algorithms for fuzzy logic systems which are similar to the training algorithms for neural networks [19, 61]. 1.2 Fuzzy Knowledge Representation in Neural Networks In some cases, neural networks can be structured based on the principles of fuzzy logic <ref> [18, 44] </ref>. Neural network representations of fuzzy logic interpolation have also been used within the context of reinforcement learning [3]. Consider the following set of linguistic fuzzy rules: IF (x 1 is A 1 ) AND (x 2 is A 3 ) THEN y 1 is C 1 . <p> The extraction of fuzzy if-then-rules from trained multilayer perceptrons has also been investigated <ref> [18, 23, 36, 37] </ref>.
Reference: [19] <author> V. Gorrini and H. Bersini, </author> <title> "Recurrent fuzzy systems," </title> <booktitle> in Proceedings of the Third IEEE Conference on Fuzzy Systems, </booktitle> <volume> vol. I, </volume> <pages> pp. 193-198, </pages> <year> 1994. </year>
Reference-contexts: The development of learning algorithms for neural networks has been beneficial to the field of fuzzy systems which adopted some learning algorithms; there exist backpropagation training algorithms for fuzzy logic systems which are similar to the training algorithms for neural networks <ref> [19, 61] </ref>. 1.2 Fuzzy Knowledge Representation in Neural Networks In some cases, neural networks can be structured based on the principles of fuzzy logic [18, 44]. Neural network representations of fuzzy logic interpolation have also been used within the context of reinforcement learning [3].
Reference: [20] <author> J. Grantner and M. Patyra, </author> <title> "Synthesis and analysis of fuzzy logic finite state machine models," </title> <booktitle> in Proceedings of the Third IEEE Conference on Fuzzy Systems, </booktitle> <volume> vol. I, </volume> <pages> pp. 205-210, </pages> <year> 1994. </year>
Reference-contexts: The fundamentals of FFAs have been in discussed in [16, 48, 64] without presenting a systematic method for machine synthesis. Neural network implementations of fuzzy automata have been proposed in the literature <ref> [20, 21, 29, 59] </ref>. The synthesis method proposed in [20] uses digital design technology to implement fuzzy representations of states and outputs. <p> The fundamentals of FFAs have been in discussed in [16, 48, 64] without presenting a systematic method for machine synthesis. Neural network implementations of fuzzy automata have been proposed in the literature [20, 21, 29, 59]. The synthesis method proposed in <ref> [20] </ref> uses digital design technology to implement fuzzy representations of states and outputs. In [59], the implementation of a Moore machine with fuzzy inputs and states is realized by training a feedforward network explicitly on the state transition table using a modified backpropagation algorithm.
Reference: [21] <author> J. Grantner and M. Patyra, </author> <title> "VLSI implementations of fuzzy logic finite state machines," </title> <booktitle> in Proceedings of the Fifth IFSA Congress, </booktitle> <pages> pp. 781-784, </pages> <year> 1993. </year>
Reference-contexts: The fundamentals of FFAs have been in discussed in [16, 48, 64] without presenting a systematic method for machine synthesis. Neural network implementations of fuzzy automata have been proposed in the literature <ref> [20, 21, 29, 59] </ref>. The synthesis method proposed in [20] uses digital design technology to implement fuzzy representations of states and outputs.
Reference: [22] <author> T. L. Hardy, </author> <title> "Multi-objective decision-making under uncertainty fuzzy logic methods," </title> <type> Tech. Rep. TM 106796, </type> <institution> NASA, </institution> <address> Washington, D.C., </address> <year> 1994. </year>
Reference-contexts: Fuzzy logic [67] provides a mathematical foundation for approximate reasoning; fuzzy fl Revised September, 1996. 1 logic has proven very successful in a variety of applications <ref> [6, 8, 13, 22, 27, 28, 42, 65] </ref>. The parameters of adaptive fuzzy systems have clear physical meanings which facilitates the choice of their initial values. Furthermore, rule-based information can be incorporated into fuzzy systems in a systematic way.
Reference: [23] <author> Y. Hayashi and A. Imura, </author> <title> "Fuzzy neural expert system with automated extraction of fuzzy if-then rules from a trained neural network," </title> <booktitle> in Proceedings of the First IEEE Conference on Fuzzy Systems, </booktitle> <pages> pp. 489-494, </pages> <year> 1990. </year>
Reference-contexts: The extraction of fuzzy if-then-rules from trained multilayer perceptrons has also been investigated <ref> [18, 23, 36, 37] </ref>.
Reference: [24] <author> J. Hopcroft and J. Ullman, </author> <title> Introduction to Automata Theory, </title> <booktitle> Languages, and Computation. </booktitle> <address> Reading, MA: </address> <publisher> Addison-Wesley Publishing Company, Inc., </publisher> <year> 1979. </year>
Reference-contexts: Details can be found in [38], and briefly with experimental verification in [41]. 2.1 Deterministic Finite-State Automata Regular languages represent the smallest class of formal languages in the Chomsky hierarchy <ref> [24] </ref>. Regular languages are generated by regular grammars. <p> Derive a DFA from the regular expressions S This union defines a non-deterministic finite-state automata (NDFA) which can be transformed into an equivalent DFA using standard algorithms <ref> [24] </ref>. 15 1 3 1 3 5 a,b/0.3 b/0.2 b/0.4 a a b 0.2 b a b b with weighted state transitions. State 1 is the automaton's start state; accepting states are drawn with double circles.
Reference: [25] <author> B. Horne and D. Hush, </author> <title> "Bounds on the complexity of recurrent neural network implementations of finite state machines," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 9, no. 2, </volume> <pages> pp. 243-252, </pages> <year> 1996. </year> <month> 21 </month>
Reference-contexts: State 1 is the DFA's start and accepting state. 2.2 Network Construction Various methods have been proposed for implementing DFAs in recurrent neural networks <ref> [1, 2, 15, 14, 25, 35, 39] </ref>.
Reference: [26] <author> E. Khan and F. Unal, </author> <title> "Recurrent fuzzy logic using neural networks," in Advances in fuzzy logic, neural networks, and genetic algorithms (T. Furuhashi, </title> <editor> ed.), </editor> <booktitle> Lecture Notes in Artificial Intelligence, </booktitle> <address> Berlin: </address> <publisher> Springer Verlag, </publisher> <year> 1995. </year>
Reference-contexts: The combination of neural networks with fuzzy logic so-called neuro-fuzzy systems is enjoying increased popularity. However, it is very rare that these systems contain feedback in their structure <ref> [26] </ref>; thus, they are often inadequate for dealing with time-varying systems. On the one hand, the above mentioned applications demonstrate that fuzzy automata are gaining significance as synthesis tools for a variety of problems. On the other hand, recurrent neural networks are very promising candidates for modeling time-varying systems.
Reference: [27] <author> W. J. M. Kickert and H. van Nauta Lemke, </author> <title> "Application of a fuzzy controller in a warm water plant," </title> <journal> Automatica, </journal> <volume> vol. 12, no. 4, </volume> <pages> pp. 301-308, </pages> <year> 1976. </year>
Reference-contexts: Fuzzy logic [67] provides a mathematical foundation for approximate reasoning; fuzzy fl Revised September, 1996. 1 logic has proven very successful in a variety of applications <ref> [6, 8, 13, 22, 27, 28, 42, 65] </ref>. The parameters of adaptive fuzzy systems have clear physical meanings which facilitates the choice of their initial values. Furthermore, rule-based information can be incorporated into fuzzy systems in a systematic way.
Reference: [28] <author> C. Lee, </author> <title> "Fuzzy logic in control systems: fuzzy logic controller," </title> <journal> IEEE Transactions on Man, Systems, and Cybernetics, </journal> <volume> vol. SMC-20, no. 2, </volume> <pages> pp. 404-435, </pages> <year> 1990. </year>
Reference-contexts: Fuzzy logic [67] provides a mathematical foundation for approximate reasoning; fuzzy fl Revised September, 1996. 1 logic has proven very successful in a variety of applications <ref> [6, 8, 13, 22, 27, 28, 42, 65] </ref>. The parameters of adaptive fuzzy systems have clear physical meanings which facilitates the choice of their initial values. Furthermore, rule-based information can be incorporated into fuzzy systems in a systematic way.
Reference: [29] <author> S. Lee and E. Lee, </author> <title> "Fuzzy neural networks," </title> <journal> Mathematical Biosciences, </journal> <volume> vol. 23, </volume> <pages> pp. 151-177, </pages> <year> 1975. </year>
Reference-contexts: The fundamentals of FFAs have been in discussed in [16, 48, 64] without presenting a systematic method for machine synthesis. Neural network implementations of fuzzy automata have been proposed in the literature <ref> [20, 21, 29, 59] </ref>. The synthesis method proposed in [20] uses digital design technology to implement fuzzy representations of states and outputs.
Reference: [30] <author> T. Ludermir, </author> <title> "Logical networks capable of computing weighted regular languages," </title> <booktitle> in Proceedings of the International Joint Conference on Neural Networks 1991, </booktitle> <volume> vol. II, </volume> <pages> pp. 1687-1692, </pages> <year> 1991. </year>
Reference-contexts: The fuzzification of inputs and states reduces the memory size that is required to implement the automaton with a microcontroller (e.g., antilock braking systems). In related work, an algorithm for implementing weighted regular languages in neural networks with probabilistic logic nodes was discussed <ref> [30] </ref>. A general synthesis method for synchronous fuzzy sequential circuits has been discussed in [62]. A synthesis method for a class of discrete-time neural networks with multilevel threshold neurons with applications to gray level image processing has been proposed in [51].
Reference: [31] <author> R. Maclin and J. Shavlik, </author> <title> "Refining algorithms with knowledge-based neural networks: Improving the Chou-Fasman algorithm for protein folding," in Computational Learning Theory and Natural Learning Systems (S. </title> <editor> Hanson, G. Drastal, and R. Rivest, eds.), </editor> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: This approach outperformed the best known `traditional' algorithm for protein folding <ref> [31, 32] </ref>. 4 to extend the role of recurrent networks as tools for knowledge revision and refinement to problem domains that can be modeled as FFAs and where prior (fuzzy) knowledge is available.
Reference: [32] <author> R. Maclin and J. Shavlik, </author> <title> "Refining domain theories expressed as finite-state automata," </title> <booktitle> in Proceedings of the Eighth International Workshop on Machine Learning (ML'91) (L. </booktitle> <editor> B. . G. Collins, ed.), </editor> <address> (San Mateo, CA), </address> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: This approach outperformed the best known `traditional' algorithm for protein folding <ref> [31, 32] </ref>. 4 to extend the role of recurrent networks as tools for knowledge revision and refinement to problem domains that can be modeled as FFAs and where prior (fuzzy) knowledge is available.
Reference: [33] <author> C. Mead, </author> <title> Analog VLSI and Neural Systems. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: Since DFAs are used in high-level VLSI design, and recurrent neural networks can be directly implemented in chips, this approach could be useful for applications where FFAs are used in conjunction with recurrent networks and VLSI <ref> [50, 33] </ref>. 1.3 Outline of Paper We briefly review deterministic, finite-state automata and their implementation in recurrent neural networks in Section 2. The extension of DFAs to FFAs is discussed in Section 3.
Reference: [34] <author> S. Mensch and H. Lipp, </author> <title> "Fuzzy specification of finite state machines," </title> <booktitle> in Proceedings of the European Design Automation Conference, </booktitle> <pages> pp. 622-626, </pages> <year> 1990. </year>
Reference-contexts: Fuzzy grammars have been found to be useful in a variety of applications such as in the analysis of Xrays [43], in digital circuit design <ref> [34] </ref>, and in the design of intelligent human-computer interfaces [49]. The fundamentals of FFAs have been in discussed in [16, 48, 64] without presenting a systematic method for machine synthesis. Neural network implementations of fuzzy automata have been proposed in the literature [20, 21, 29, 59].
Reference: [35] <author> M. Minsky, </author> <title> Computation: Finite and Infinite Machines, </title> <journal> ch. </journal> <volume> 3, </volume> <pages> pp. 32-66. </pages> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall, Inc., </publisher> <year> 1967. </year>
Reference-contexts: State 1 is the DFA's start and accepting state. 2.2 Network Construction Various methods have been proposed for implementing DFAs in recurrent neural networks <ref> [1, 2, 15, 14, 25, 35, 39] </ref>.
Reference: [36] <author> S. Mitra and S. Pal, </author> <title> "Fuzzy multilayer perceptron, inferencing and rule generation," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 6, no. 1, </volume> <pages> pp. 51-63, </pages> <year> 1995. </year>
Reference-contexts: The extraction of fuzzy if-then-rules from trained multilayer perceptrons has also been investigated <ref> [18, 23, 36, 37] </ref>.
Reference: [37] <author> T. Nishina, M. Hagiwara, and M. Nakagawa, </author> <title> "Fuzzy inference neural networks which automatically partition a pattern space and extract fuzzy if-then rules," </title> <booktitle> in Proceedings of the Third IEEE Conference on Fuzzy Systems, </booktitle> <volume> vol. II, </volume> <pages> pp. 1314-1319, </pages> <year> 1994. </year>
Reference-contexts: The extraction of fuzzy if-then-rules from trained multilayer perceptrons has also been investigated <ref> [18, 23, 36, 37] </ref>.
Reference: [38] <author> C. Omlin and C. Giles, </author> <title> "Constructing deterministic finite-state automata in recurrent neural networks," </title> <journal> Journal of the ACM, </journal> <note> 1996. In press. This is a revised version of University of Maryland Technical Report UMIACS-TR-95-50. </note>
Reference-contexts: The extension of DFAs to FFAs is discussed in Section 3. In Section 4, we show how FFAs can be implemented in recurrent networks based on previous work on the encoding of DFAs <ref> [38] </ref>. In particular, our results show that FFAs can be encoded into recurrent networks such that a constructed network assigns membership grades to strings of arbitrary length with arbitrary accuracy. Simulation results in Section 5 validate our theoretical analysis. <p> A summary and directions for future work in Section 6 conclude this paper. 2 Finite-State Automata and Recurrent Neural Networks Here, we discuss the relationship between finite-state automata and recurrent neural networks for mapping fuzzy automata into recurrent networks. Details can be found in <ref> [38] </ref>, and briefly with experimental verification in [41]. 2.1 Deterministic Finite-State Automata Regular languages represent the smallest class of formal languages in the Chomsky hierarchy [24]. Regular languages are generated by regular grammars. <p> all legal strings can be written in the form of 3 Since the representation of FFAs in recurrent networks is based on results derived for the representation of DFAs, this paper highlights without proofs the relevant theoretical results; the details of the analysis and relevant proofs can be found in <ref> [38] </ref>. 5 production (or rewriting) rules: S ! * j a j aS j bA The terminal and nonterminal symbols are `a' and `b', and S and A, respectively (S is also the start symbol of this grammar), and * denotes the empty string. <p> We have proven that DFAs can be encoded in discrete-time, second-order recurrent neural networks with sigmoidal discriminant functions such that the DFA and constructed network accept and recognize the same regular language. Here, we give a brief description of the encoding algorithm; for more details, please see <ref> [38] </ref>. As previously mentioned, there are many encoding methods. The one described here is part of our particular proof process. The desired finite-state dynamics are encoded into a network by programming a small subset of all available weights to values +H and H. <p> This represents a worst case which is used to prove network stability. The more neurons contribute to the input of any given neuron, the more likely the internal DFA state representation becomes unstable. Details on the worst case analysis can be found in <ref> [38, 41] </ref>. 2.3 Network Stability There exist only two kinds of signals in a constructed neural network that models a DFA: Recurrent state neurons have high output signals only when they correspond to the current DFA state; all other recurrent neurons have low output signals. <p> In the remainder of this section, we state results which establish that the stability of the internal repre sentation can be achieved. The proofs of these results can be found in <ref> [38] </ref>. The terms principal and residual inputs will be useful for the following discussion: Definition 2.3 Let S i be a neuron with low output signal S t i and S j be a neuron with high output signal S t j . <p> The graphs of the function f (x; r) are shown in Fig. 4 for different values of the parameter r. The function f (x; r) has some desirable properties <ref> [38] </ref>: Lemma 2.2 For any H &gt; 0, the function f (x; r) has at least one fixed point OE 0 f . <p> As such, they represent worst cases, i.e. the finite-state dynamics of a given neural network implementation may remain stable for smaller values of H even for very large networks <ref> [38] </ref>. For any given DFA, the value of H that satisfies the conditions of the above theorem must be determined numerically since neither the values of the fixed points nor the conditions for the existence of fixed points can be solved explicitly.
Reference: [39] <author> C. Omlin and C. Giles, </author> <title> "Constructing deterministic finite-state automata in sparse recurrent neural networks," </title> <booktitle> in IEEE International Conference on Neural Networks (ICNN'94), </booktitle> <pages> pp. 1732-1737, </pages> <year> 1994. </year> <month> 22 </month>
Reference-contexts: State 1 is the DFA's start and accepting state. 2.2 Network Construction Various methods have been proposed for implementing DFAs in recurrent neural networks <ref> [1, 2, 15, 14, 25, 35, 39] </ref>.
Reference: [40] <author> C. Omlin and C. Giles, </author> <title> "Rule revision with recurrent neural networks," </title> <journal> IEEE Transactions on Knowl--edge and Data Engineering, </journal> <volume> vol. 8, no. 1, </volume> <pages> pp. 183-188, </pages> <year> 1996. </year>
Reference-contexts: They are particularly well-suited for problem domains where incomplete or contradictory prior knowledge is available. We have shown that recurrent networks can be initialized with that prior knowledge; the goal of training networks then becomes that of knowledge revision or refinement <ref> [40] </ref>. 2 Similarly, it would be useful 2 Maclin & Shavlik have demonstrated the power of combining DFAs and neural networks in the area of computational molecular biology (protein folding): They represented prior knowledge about protein structures as DFAs, initialized a neural network with that prior knowledge, and trained it on
Reference: [41] <author> C. Omlin and C. Giles, </author> <title> "Stable encoding of large finite-state automata in recurrent neural networks with sigmoid discriminants," </title> <journal> Neural Computation, </journal> <volume> vol. 8, no. 7, </volume> <pages> pp. 675-696, </pages> <year> 1996. </year>
Reference-contexts: Details can be found in [38], and briefly with experimental verification in <ref> [41] </ref>. 2.1 Deterministic Finite-State Automata Regular languages represent the smallest class of formal languages in the Chomsky hierarchy [24]. Regular languages are generated by regular grammars. <p> This represents a worst case which is used to prove network stability. The more neurons contribute to the input of any given neuron, the more likely the internal DFA state representation becomes unstable. Details on the worst case analysis can be found in <ref> [38, 41] </ref>. 2.3 Network Stability There exist only two kinds of signals in a constructed neural network that models a DFA: Recurrent state neurons have high output signals only when they correspond to the current DFA state; all other recurrent neurons have low output signals.
Reference: [42] <author> C. Pappis and E. Mamdani, </author> <title> "A fuzzy logic controller for a traffic junction," </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> vol. SMC-7, no. 10, </volume> <pages> pp. 707-717, </pages> <year> 1977. </year>
Reference-contexts: Fuzzy logic [67] provides a mathematical foundation for approximate reasoning; fuzzy fl Revised September, 1996. 1 logic has proven very successful in a variety of applications <ref> [6, 8, 13, 22, 27, 28, 42, 65] </ref>. The parameters of adaptive fuzzy systems have clear physical meanings which facilitates the choice of their initial values. Furthermore, rule-based information can be incorporated into fuzzy systems in a systematic way.
Reference: [43] <author> A. Pathak and S. Pal, </author> <title> "Fuzzy grammars in syntactic recognition of skeletal maturity from x-rays," </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> vol. 16, no. 5, </volume> <pages> pp. 657-667, </pages> <year> 1986. </year>
Reference-contexts: Thus, it is only natural to ask whether recurrent neural networks can also represent fuzzy finite-state automata (FFAs), and thus be used to implement recognizers of fuzzy regular grammars. Fuzzy grammars have been found to be useful in a variety of applications such as in the analysis of Xrays <ref> [43] </ref>, in digital circuit design [34], and in the design of intelligent human-computer interfaces [49]. The fundamentals of FFAs have been in discussed in [16, 48, 64] without presenting a systematic method for machine synthesis.
Reference: [44] <author> C. Perneel, J.-M. Renders, J.-M. Themlin, and M. Acheroy, </author> <title> "Fuzzy reasoning and neural networks for decision making problems in uncertain environments," </title> <booktitle> in Proceedings of the Third IEEE Conference on Fuzzy Systems, </booktitle> <volume> vol. II, </volume> <pages> pp. 1111-1125, </pages> <year> 1994. </year>
Reference-contexts: fuzzy systems which adopted some learning algorithms; there exist backpropagation training algorithms for fuzzy logic systems which are similar to the training algorithms for neural networks [19, 61]. 1.2 Fuzzy Knowledge Representation in Neural Networks In some cases, neural networks can be structured based on the principles of fuzzy logic <ref> [18, 44] </ref>. Neural network representations of fuzzy logic interpolation have also been used within the context of reinforcement learning [3]. Consider the following set of linguistic fuzzy rules: IF (x 1 is A 1 ) AND (x 2 is A 3 ) THEN y 1 is C 1 .
Reference: [45] <author> J. Pollack, </author> <title> "The induction of dynamical recognizers," </title> <journal> Machine Learning, </journal> <volume> vol. 7, </volume> <pages> pp. 227-252, </pages> <year> 1991. </year>
Reference: [46] <author> M. Rabin, </author> <title> "Probabilistic automata," </title> <journal> Information and Control, </journal> <volume> vol. 6, </volume> <pages> pp. 230-245, </pages> <year> 1963. </year>
Reference-contexts: used: e G (s) = e G (S ) s) = max fl min [ e G (S ! ff 1 ); e G (ff 1 ! ff 2 ); : : : ; e G (ff m ! s)] This is akin to the definition of stochastic regular languages <ref> [46] </ref> where the min- and max-operators are replaced by the product- and sum-operators, respectively. Both fuzzy and stochastic regular languages are examples of weighted regular languages [47].
Reference: [47] <author> A. Salommaa, </author> <title> "Probabilistic and weighted grammars," </title> <journal> Information and Control, </journal> <volume> vol. 15, </volume> <pages> pp. 529-544, </pages> <year> 1969. </year>
Reference-contexts: Both fuzzy and stochastic regular languages are examples of weighted regular languages <ref> [47] </ref>. However, there are also distinct differences: In stochastic regular languages, the production rules are applied according to a probability distribution (i.e., the production weights are interpreted as probabilities).
Reference: [48] <author> E. Santos, </author> <title> "Maximin automata," </title> <journal> Information and Control, </journal> <volume> vol. 13, </volume> <pages> pp. 363-377, </pages> <year> 1968. </year>
Reference-contexts: Fuzzy grammars have been found to be useful in a variety of applications such as in the analysis of Xrays [43], in digital circuit design [34], and in the design of intelligent human-computer interfaces [49]. The fundamentals of FFAs have been in discussed in <ref> [16, 48, 64] </ref> without presenting a systematic method for machine synthesis. Neural network implementations of fuzzy automata have been proposed in the literature [20, 21, 29, 59]. The synthesis method proposed in [20] uses digital design technology to implement fuzzy representations of states and outputs.
Reference: [49] <author> H. Senay, </author> <title> "Fuzzy command grammars for intelligent interface design," </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> vol. 22, no. 5, </volume> <pages> pp. 1124-1131, </pages> <year> 1992. </year>
Reference-contexts: Fuzzy grammars have been found to be useful in a variety of applications such as in the analysis of Xrays [43], in digital circuit design [34], and in the design of intelligent human-computer interfaces <ref> [49] </ref>. The fundamentals of FFAs have been in discussed in [16, 48, 64] without presenting a systematic method for machine synthesis. Neural network implementations of fuzzy automata have been proposed in the literature [20, 21, 29, 59].
Reference: [50] <author> B. J. Sheu, </author> <booktitle> Neural Information Processing and VLSI. </booktitle> <address> Boston, MA: </address> <publisher> Kluwer Academic Publishers, </publisher> <year> 1995. </year>
Reference-contexts: Since DFAs are used in high-level VLSI design, and recurrent neural networks can be directly implemented in chips, this approach could be useful for applications where FFAs are used in conjunction with recurrent networks and VLSI <ref> [50, 33] </ref>. 1.3 Outline of Paper We briefly review deterministic, finite-state automata and their implementation in recurrent neural networks in Section 2. The extension of DFAs to FFAs is discussed in Section 3.
Reference: [51] <author> J. Si and A. Michel, </author> <title> "Analysis and synthesis of a class of discrete-time neural networks with multilevel threshold neurons," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 6, no. 1, </volume> <editor> p. </editor> <volume> 105, </volume> <year> 1995. </year>
Reference-contexts: A general synthesis method for synchronous fuzzy sequential circuits has been discussed in [62]. A synthesis method for a class of discrete-time neural networks with multilevel threshold neurons with applications to gray level image processing has been proposed in <ref> [51] </ref>. The combination of neural networks with fuzzy logic so-called neuro-fuzzy systems is enjoying increased popularity. However, it is very rare that these systems contain feedback in their structure [26]; thus, they are often inadequate for dealing with time-varying systems.
Reference: [52] <author> H. Siegelmann, </author> <title> "Computation beyond the turing limit," </title> <journal> Science, </journal> <volume> vol. 268, </volume> <pages> pp. 545-548, </pages> <year> 1995. </year>
Reference-contexts: Fuzzy systems and multilayer perceptrons are computationally equivalent, i.e. they are both universal approximators [9, 60]. Recurrent neural networks have been shown to be computationally at least as powerful as Turing machines <ref> [53, 52] </ref>; whether or not these results also apply to recurrent fuzzy systems remains an open question. While the methodologies underlying fuzzy systems and neural networks are quite different, their functional forms are often similar.
Reference: [53] <author> H. Siegelmann and E. Sontag, </author> <title> "On the computational power of neural nets," </title> <journal> Journal of Computer and System Sciences, </journal> <volume> vol. 50, no. 1, </volume> <pages> pp. 132-150, </pages> <year> 1995. </year>
Reference-contexts: Fuzzy systems and multilayer perceptrons are computationally equivalent, i.e. they are both universal approximators [9, 60]. Recurrent neural networks have been shown to be computationally at least as powerful as Turing machines <ref> [53, 52] </ref>; whether or not these results also apply to recurrent fuzzy systems remains an open question. While the methodologies underlying fuzzy systems and neural networks are quite different, their functional forms are often similar.
Reference: [54] <author> M. Thomason and P. Marinos, </author> <title> "Deterministic acceptors of regular fuzzy languages," </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> no. 3, </volume> <pages> pp. 228-230, </pages> <year> 1974. </year>
Reference-contexts: The algorithm, as given in <ref> [54] </ref>, is shown in Fig. 6. An example of FFA-to-DFA transformation is shown in Fig. 7a 6 .
Reference: [55] <author> K. Thornber, </author> <title> "The fidelity of fuzzy-logic inference," </title> <journal> IEEE Transactions on Fuzzy Systems, </journal> <volume> vol. 1, no. 4, </volume> <pages> pp. 288-297, </pages> <year> 1993. </year>
Reference-contexts: : : from properties of fuzzy sets A 1 ; A 2 ; : : : with the help of an inference scheme A 1 ; A 2 ; : : : ! B 1 ; B 2 ; : : : which is governed by a set of rules <ref> [55, 56] </ref>. 3 recursive linguistic rules with recursive depth 1 if all state variables x (t) are observable.
Reference: [56] <author> K. Thornber, </author> <title> "A key to fuzzy-logic inference," </title> <journal> International Journal of Approximate Reasoning, </journal> <volume> vol. 8, </volume> <pages> pp. 105-121, </pages> <year> 1993. </year>
Reference-contexts: : : from properties of fuzzy sets A 1 ; A 2 ; : : : with the help of an inference scheme A 1 ; A 2 ; : : : ! B 1 ; B 2 ; : : : which is governed by a set of rules <ref> [55, 56] </ref>. 3 recursive linguistic rules with recursive depth 1 if all state variables x (t) are observable.
Reference: [57] <author> P. Tino, B. Horne, and C.L.Giles, </author> <title> "Finite state machines and recurrent neural networks automata and dynamical systems approaches," in Progress in Neural Networks: Temporal Dynamics and Time-Varying Pattern Recognition (J. </title> <editor> Dayhoff and O. Omidvar, eds.), </editor> <address> Norwood, NJ: </address> <publisher> Birkhauser, In press. </publisher>
Reference: [58] <author> P. Tino and J. Sajda, </author> <title> "Learning and extracting initial mealy machines with a modular neural network model," </title> <journal> Neural Computation, </journal> <volume> vol. 7, no. 4, </volume> <pages> pp. 822-844, </pages> <year> 1995. </year>
Reference: [59] <author> F. Unal and E. Khan, </author> <title> "A fuzzy finite state machine implementation based on a neural fuzzy system," </title> <booktitle> in Proceedings of the Third International Conference on Fuzzy Systems, </booktitle> <volume> vol. 3, </volume> <pages> pp. 1749-1754, </pages> <year> 1994. </year>
Reference-contexts: The fundamentals of FFAs have been in discussed in [16, 48, 64] without presenting a systematic method for machine synthesis. Neural network implementations of fuzzy automata have been proposed in the literature <ref> [20, 21, 29, 59] </ref>. The synthesis method proposed in [20] uses digital design technology to implement fuzzy representations of states and outputs. <p> Neural network implementations of fuzzy automata have been proposed in the literature [20, 21, 29, 59]. The synthesis method proposed in [20] uses digital design technology to implement fuzzy representations of states and outputs. In <ref> [59] </ref>, the implementation of a Moore machine with fuzzy inputs and states is realized by training a feedforward network explicitly on the state transition table using a modified backpropagation algorithm.
Reference: [60] <author> L.-X. Wang, </author> <title> Adaptive Fuzzy Systems and Control: Design and Stability Analysis. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall, </publisher> <year> 1994. </year>
Reference-contexts: Artificial neural networks have become valuable computational tools in their own right for tasks such as pattern recognition, control, and forecasting. Fuzzy systems and multilayer perceptrons are computationally equivalent, i.e. they are both universal approximators <ref> [9, 60] </ref>. Recurrent neural networks have been shown to be computationally at least as powerful as Turing machines [53, 52]; whether or not these results also apply to recurrent fuzzy systems remains an open question.
Reference: [61] <author> L.-X. Wang, </author> <title> "Fuzzy systems are universal approximators," </title> <booktitle> in Proceedings of the First International Conference on Fuzzy Systems, </booktitle> <pages> pp. 1163-1170, </pages> <year> 1992. </year>
Reference-contexts: The development of learning algorithms for neural networks has been beneficial to the field of fuzzy systems which adopted some learning algorithms; there exist backpropagation training algorithms for fuzzy logic systems which are similar to the training algorithms for neural networks <ref> [19, 61] </ref>. 1.2 Fuzzy Knowledge Representation in Neural Networks In some cases, neural networks can be structured based on the principles of fuzzy logic [18, 44]. Neural network representations of fuzzy logic interpolation have also been used within the context of reinforcement learning [3].
Reference: [62] <author> T. Watanabe, M. Matsumoto, and M. Enokida, </author> <title> "Synthesis of synchronous fuzzy sequential circuits," </title> <booktitle> in Proceedings of the Third IFSA World Congress, </booktitle> <pages> pp. 288-291, </pages> <year> 1989. </year>
Reference-contexts: In related work, an algorithm for implementing weighted regular languages in neural networks with probabilistic logic nodes was discussed [30]. A general synthesis method for synchronous fuzzy sequential circuits has been discussed in <ref> [62] </ref>. A synthesis method for a class of discrete-time neural networks with multilevel threshold neurons with applications to gray level image processing has been proposed in [51]. The combination of neural networks with fuzzy logic so-called neuro-fuzzy systems is enjoying increased popularity.
Reference: [63] <author> R. Watrous and G. Kuhn, </author> <title> "Induction of finite-state languages using second-order recurrent networks," </title> <journal> Neural Computation, </journal> <volume> vol. 4, no. 3, </volume> <editor> p. </editor> <volume> 406, </volume> <year> 1992. </year>
Reference: [64] <author> W. Wee and K. Fu, </author> <title> "A formulation of fuzzy automata and its applications as a model of learning systems," </title> <journal> IEEE Transactions on System Science and Cybernetics, </journal> <volume> vol. 5, </volume> <pages> pp. 215-223, </pages> <year> 1969. </year>
Reference-contexts: Fuzzy grammars have been found to be useful in a variety of applications such as in the analysis of Xrays [43], in digital circuit design [34], and in the design of intelligent human-computer interfaces [49]. The fundamentals of FFAs have been in discussed in <ref> [16, 48, 64] </ref> without presenting a systematic method for machine synthesis. Neural network implementations of fuzzy automata have been proposed in the literature [20, 21, 29, 59]. The synthesis method proposed in [20] uses digital design technology to implement fuzzy representations of states and outputs.
Reference: [65] <author> X. Yang and G. Kalambur, </author> <title> "Design for machining using expert system and fuzzy logic approach," </title> <journal> Journal of Materials Engineering and Performance, </journal> <volume> vol. 4, no. 5, </volume> <editor> p. </editor> <volume> 599, </volume> <year> 1995. </year>
Reference-contexts: Fuzzy logic [67] provides a mathematical foundation for approximate reasoning; fuzzy fl Revised September, 1996. 1 logic has proven very successful in a variety of applications <ref> [6, 8, 13, 22, 27, 28, 42, 65] </ref>. The parameters of adaptive fuzzy systems have clear physical meanings which facilitates the choice of their initial values. Furthermore, rule-based information can be incorporated into fuzzy systems in a systematic way.
Reference: [66] <author> L. Zadeh, </author> <title> "Fuzzy languages and their relation to human and machine intelligence," </title> <type> Tech. Rep. </type> <institution> ERL-M302, Electronics Research Laboratory, University of California, Berkeley, </institution> <year> 1971. </year>
Reference-contexts: DFA states after the transformation of a FFA into of all possible string memberships and ! : Q ! Z is a labeling defined for all DFA states. 6 Another method of decomposing a fuzzy grammar into crisp grammars has been investigated by Zadeh using the concept of level set <ref> [66] </ref> 14 Input: Fuzzy regular grammar e G Output: DFA M such that 8s 2 fl : M (s) = eG (s). 1. Derive a FFA e M from e G using standard techniques.
Reference: [67] <author> L. Zadeh, </author> <title> "Fuzzy sets," </title> <journal> Information and Control, </journal> <volume> vol. 8, </volume> <pages> pp. 338-353, </pages> <year> 1965. </year>
Reference-contexts: 1 Introduction 1.1 Fuzzy Systems and Neural Networks There has been an increased interest in combining artificial neural networks and fuzzy systems (see [4] for a collection of papers). Fuzzy logic <ref> [67] </ref> provides a mathematical foundation for approximate reasoning; fuzzy fl Revised September, 1996. 1 logic has proven very successful in a variety of applications [6, 8, 13, 22, 27, 28, 42, 65]. The parameters of adaptive fuzzy systems have clear physical meanings which facilitates the choice of their initial values.
Reference: [68] <author> Z. Zeng, R. Goodman, and P. Smyth, </author> <title> "Learning finite state machines with self-clustering recurrent networks," </title> <journal> Neural Computation, </journal> <volume> vol. 5, no. 6, </volume> <pages> pp. 976-990, </pages> <year> 1993. </year> <month> 24 </month>
References-found: 68

