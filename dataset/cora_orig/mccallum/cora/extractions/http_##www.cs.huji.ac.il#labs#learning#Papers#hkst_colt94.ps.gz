URL: http://www.cs.huji.ac.il/labs/learning/Papers/hkst_colt94.ps.gz
Refering-URL: http://www.cs.huji.ac.il/labs/learning/Papers/MLT_list.html
Root-URL: http://www.cs.huji.ac.il
Title: Rigorous Learning Curve Bounds from Statistical Mechanics  
Author: David Haussler Michael Kearns H. Sebastian Seung Naftali Tishby 
Address: Santa Cruz, California  Murray Hill, New Jersey  Murray Hill, New Jersey  Jerusalem, Israel  
Affiliation: U.C. Santa Cruz  AT&T Bell Laboratories  AT&T Bell Laboratories  Hebrew University  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> S. Amari, N. Fujita, and S. Shinomoto. </author> <title> Four types of learning curves. </title> <journal> Neural Computation, </journal> <volume> 4(4) </volume> <pages> 605-618, </pages> <year> 1992. </year>
Reference-contexts: The theoretical revisions of the VC theory mentioned above cannot explain such behavior, because they conservatively modify only with the constant factors of the same power laws. In this paper, we show that ideas from statistical mechanics (namely, the annealed approximation <ref> [20, 1, 21] </ref> and the thermodynamic limit [21]) can be used as the basis of a mathematically precise and rigorous theory of learning curves. 3 Speaking coarsely, there are two main ideas be 2 By a power law, we mean the functional form (a=m) b , where a; b &gt; 0 <p> As mentioned already, these are functions that are assumed to be given in the thermodynamic limit method. Let t (N ) be any mapping from the natural numbers to the natural numbers such that t (N ) ! 1 as N ! 1, and let s : <ref> [0; 1] </ref> ! &lt; + be any continuous function. <p> Let us now let m; N ! 1 (and thus t (N ) ! 1) but let m=t (N ) = ff &gt; 0 remain constant. Define * fl 2 <ref> [0; 1] </ref> to be the largest * 2 [0; 1] such that s (*) ff log (1 *). Note that both s (*) and ff log (1 *) are non-negative functions, and 0 = ff log (1 *) s (*) for * = 0. <p> Let us now let m; N ! 1 (and thus t (N ) ! 1) but let m=t (N ) = ff &gt; 0 remain constant. Define * fl 2 <ref> [0; 1] </ref> to be the largest * 2 [0; 1] such that s (*) ff log (1 *). Note that both s (*) and ff log (1 *) are non-negative functions, and 0 = ff log (1 *) s (*) for * = 0. <p> Let us define by = minfff log (1 *) s (*) : * 2 [* fl Note that is well-defined since the quantify ff log (1 *) s (*) is strictly positive for all * 2 <ref> [* fl + t; 1] </ref>. <p> We can now write r (N) X e j )+ff log (1* N (12) r (N) X e (13) t (N) r (N )e (15) where the first inequality follows from the fact that for all i N;t j r (N ) we have * N j 2 <ref> [* fl + t; 1] </ref>. <p> Similarly, the lower bound shows that better learning curves for the Ising perceptron and boolean conjunction problems that depend only on the entropy bound cannot be obtained. Theorem 5 Let s : [0; 1=2] ! <ref> [0; 1] </ref> be any continuous function bounded away from 1 and such that s (0) = s (1) = 0. <p> A high-level sketch of the main ideas follows. For any N , the class F N will be constructed so that there are exactly N=2 error levels, namely * N j = j=N for 1 j N=2. Now let s : [0; 1=2] ! <ref> [0; 1] </ref> be any continuous function bounded away from 1 and satisfying s (0) = s (1=2) = 0. The idea is that for any N and any 1 j N=2, F N will contain exactly 2 s (j=N)N functions whose error with respect to f N is j=N . <p> Let the target function f N be the perceptron in which every weight is +1, and let the function class F N consist of all Ising perceptrons which have at least flN weights (fl 2 <ref> [0; 1] </ref>) that are 1. (Note that unlike the realizable Ising perceptron case, here the choice of target function matters.) Again let the distribution D N be any spherically symmetric distribution on &lt; N .
Reference: [2] <author> E. B. Baum and Y.-D. Lyuu. </author> <title> The transition to perfect generalization in perceptrons. </title> <journal> Neural Comput., </journal> <volume> 3 </volume> <pages> 386-401, </pages> <year> 1991. </year>
Reference-contexts: This bound on the critical value was known from the work of Gardner and Derrida [10], and extended to the case of boolean inputs by Baum, Lyuu and Rivin <ref> [2, 15] </ref>.
Reference: [3] <author> G. Benedek and A. Itai. </author> <title> Learnability with respect to fixed distributions. </title> <journal> Theoret. Comput. Sci., </journal> <volume> 86(2) </volume> <pages> 377-389, </pages> <year> 1991. </year>
Reference-contexts: some theorists have sought to preserve the functional form of the VC bounds, but to replace the VC dimension in this functional form by an appropriate distribution-specific quantity, such as the VC entropy (which is the expectation of the logarithm of the number of dichotomies realized by the function class) <ref> [24, 14, 3] </ref>. Work on the "empirical VC dimension" has tried to measure the dependence of learning curves on both the algorithm and the distribution via backpropagation experiments [23].
Reference: [4] <author> D. Cohn and G. Tesauro. </author> <title> How tight are the Vapnik-Chervonenkis bounds. </title> <journal> Neural Comput., </journal> <volume> 4 </volume> <pages> 249-269, </pages> <year> 1992. </year>
Reference-contexts: For instance, some researchers have attempted to fit learning curves from backpropagation experiments with a variety of functional forms, including expo-nentials <ref> [4] </ref>. Backpropagation experiments with handwritten digits and characters indicate that good generalization error is sometimes obtained for sample sizes considerably smaller than the number of weights (presumed to be roughly the same as the VC dimension) [16], though the VC bounds are vacuous for m smaller than d.
Reference: [5] <author> L. Devroye and G. Lugosi. </author> <title> Lower bounds in pattern recognition and learning. </title> <type> Preprint, </type> <year> 1994. </year>
Reference-contexts: It has been shown that these bounds are essentially the best distribution-independent bounds possible, in the sense that for any function class, there exists an input distribution for which matching lower bounds on the generalization error can be given <ref> [5] </ref>. The universal VC bounds can give the impression that the true behavior of learning curves is also universal, and es sentially described by the functional forms d=m and p However, it is becoming clear that learning curves exhibit a diversity of behaviors.
Reference: [6] <author> R. M. Dudley. </author> <title> Central limit theorems for empirical measures. </title> <journal> Annals of Probability, </journal> <volume> 6(6) </volume> <pages> 899-929, </pages> <year> 1978. </year>
Reference-contexts: In particular, we will exhibit examples of distribution-specific bounds that are much tighter than the distribution-free VC bounds. It is only for infinite function classes that the union bound fails spectacularly, for here the bound diverges and becomes useless. The VC dimension, VC entropy, and random covering number <ref> [24, 18, 6, 13] </ref> are the known tools for dealing with the correlations neglected by the union bound. These tools have previously been applied to the function class as a whole. In our current research efforts, we are attempting to refine these tools by applying them to error shells.
Reference: [7] <author> A. Engel and C. V. den Broeck. </author> <title> Systems that can learn from examples: replica calculation of uniform convergence bounds for the perceptron. </title> <journal> Phys. Rev. Lett., </journal> <volume> 71 </volume> <pages> 1772-1775, </pages> <year> 1993. </year>
Reference-contexts: Third, we aim to precisely relate the statistical mechanics approach to the VC theory. version space <ref> [7, 8] </ref>. Although the replica method produces exact results when used correctly, it rests upon an interchange of limits for which no rigorous justification has been found. 2 The Finite and Realizable Case We begin with the most basic model of learning an unknown boolean target function.
Reference: [8] <author> A. Engel and W. Fink. </author> <title> Statistical mechanics calculation of Vapnik Chervonenkis bounds for perceptrons. </title> <journal> J. Phys., </journal> <volume> 26 </volume> <pages> 6893-6914, </pages> <year> 1993. </year>
Reference-contexts: Third, we aim to precisely relate the statistical mechanics approach to the VC theory. version space <ref> [7, 8] </ref>. Although the replica method produces exact results when used correctly, it rests upon an interchange of limits for which no rigorous justification has been found. 2 The Finite and Realizable Case We begin with the most basic model of learning an unknown boolean target function. <p> With high probability, there are no hypotheses in the version space with error less than the leftmost intersection except for the target itself. So the version space minus the target is contained within an annulus <ref> [8] </ref> whose inner and outer limits are the leftmost and rightmost intersections. In all of these examples, we have concentrated on the qualitative behavior (including coarse phenomena such as phase transitions) of scaled learning curves at moderate values of ff. <p> learning curve, and the behavior of our bound is very similar in shape to learning curves obtained in 5 The designation "Ising" refers to the 1 constraint, which is present in the original Ising model of magnetism with N interacting spins. both simulations and non-rigorous replica calculations from statistical physics <ref> [12, 22, 21, 8] </ref>. 6 It is instructive to compare our bounds with the cardi-nality and VC bounds for this problem.
Reference: [9] <author> E. Gardner. </author> <title> The space of interactions in neural network models. </title> <journal> J. Phys., </journal> <volume> A21:257-270, </volume> <year> 1988. </year>
Reference-contexts: These bounds hold for all empirical error minimization algorithms, including the zero temperature limit of the Gibbs algorithm. Because of our desire for rigor, we have not used the replica method <ref> [9] </ref> in this paper. Engel, van den Broeck, and Fink have used the replica method to calculate the maximum deviation between empirical and generalization error in the function class, and the maximum generalization error in the hind our theory that are novel to someone familiar with the VC theory.
Reference: [10] <author> E. Gardner and B. Derrida. </author> <title> Three unfinished works on the optimal storage capacity of networks. </title> <journal> J. Phys., </journal> <volume> A22:1983-1994, </volume> <year> 1989. </year>
Reference-contexts: We first consider the class of Ising perceptrons <ref> [10, 12, 22] </ref>. <p> However, this bound does show that any consistent learning algorithm must have reached zero error with probability approaching 1 in the thermodynamic limit for scaled sample size greater than 1:448. This bound on the critical value was known from the work of Gardner and Derrida <ref> [10] </ref>, and extended to the case of boolean inputs by Baum, Lyuu and Rivin [2, 15].
Reference: [11] <author> S. A. Goldman, M. J. Kearns, and R. E. Schapire. </author> <title> On the sample complexity of weak learning. </title> <booktitle> In Proceedings of the 3rd Workshop on Computational Learning Theory, </booktitle> <pages> pages 217-231. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Ma-teo, CA, </address> <year> 1990. </year>
Reference-contexts: The remaining sample points fail to eliminate any of the functions of generalization error * since they all agree with the target function f N on the remaining points. Now it is known <ref> [11] </ref> that in order to eliminate 2 s (*)N parity functions over a uniform distribution, the sample size m must obey m s (*) N ; for smaller m, there is a constant probability that at least one parity function remains in the version space.
Reference: [12] <author> G. Gyorgyi. </author> <title> First-order transition to perfect generalization in a neural network with binary synapses. </title> <journal> Phys. Rev., </journal> <volume> A41:7097-7100, </volume> <year> 1990. </year>
Reference-contexts: We first consider the class of Ising perceptrons <ref> [10, 12, 22] </ref>. <p> learning curve, and the behavior of our bound is very similar in shape to learning curves obtained in 5 The designation "Ising" refers to the 1 constraint, which is present in the original Ising model of magnetism with N interacting spins. both simulations and non-rigorous replica calculations from statistical physics <ref> [12, 22, 21, 8] </ref>. 6 It is instructive to compare our bounds with the cardi-nality and VC bounds for this problem.
Reference: [13] <author> D. Haussler. </author> <title> Decision-theoretic generalizations of the PAC model for neural net and other learning applications. </title> <journal> Information and Computation, </journal> <volume> 100(1) </volume> <pages> 78-150, </pages> <year> 1992. </year>
Reference-contexts: In particular, we will exhibit examples of distribution-specific bounds that are much tighter than the distribution-free VC bounds. It is only for infinite function classes that the union bound fails spectacularly, for here the bound diverges and becomes useless. The VC dimension, VC entropy, and random covering number <ref> [24, 18, 6, 13] </ref> are the known tools for dealing with the correlations neglected by the union bound. These tools have previously been applied to the function class as a whole. In our current research efforts, we are attempting to refine these tools by applying them to error shells.
Reference: [14] <author> D. Haussler, M. Kearns, and R. E. Schapire. </author> <title> Bounds on the sample complexity of Bayesian learning using information theory and the VC dimension. </title> <booktitle> In Proceedings of the 4th Workshop on Computational Learning Theory, </booktitle> <pages> pages 61-74. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: some theorists have sought to preserve the functional form of the VC bounds, but to replace the VC dimension in this functional form by an appropriate distribution-specific quantity, such as the VC entropy (which is the expectation of the logarithm of the number of dichotomies realized by the function class) <ref> [24, 14, 3] </ref>. Work on the "empirical VC dimension" has tried to measure the dependence of learning curves on both the algorithm and the distribution via backpropagation experiments [23].
Reference: [15] <author> Y.-D. Lyuu and I. Rivin. </author> <title> Tight bounds on transition to perfect generalization in perceptrons. </title> <journal> Neural Comput., </journal> <volume> 4 </volume> <pages> 854-862, </pages> <year> 1992. </year>
Reference-contexts: This bound on the critical value was known from the work of Gardner and Derrida [10], and extended to the case of boolean inputs by Baum, Lyuu and Rivin <ref> [2, 15] </ref>.
Reference: [16] <author> G. L. Martin and J. A. Pittman. </author> <title> Recognizing hand-printed letters and digits using backpropagation learning. </title> <journal> Neural Comput., </journal> <volume> 3 </volume> <pages> 258-267, </pages> <year> 1991. </year>
Reference-contexts: Backpropagation experiments with handwritten digits and characters indicate that good generalization error is sometimes obtained for sample sizes considerably smaller than the number of weights (presumed to be roughly the same as the VC dimension) <ref> [16] </ref>, though the VC bounds are vacuous for m smaller than d. Discrepancies between the VC bounds and actual learning curve behavior have also been pointed out and analyzed in other machine learning work [19, 17].
Reference: [17] <author> E. Oblow. </author> <title> Implementing Valiant's learnability theory using random sets. </title> <journal> Machine Learning, </journal> <volume> 8(1) </volume> <pages> 45-74, </pages> <year> 1992. </year>
Reference-contexts: Discrepancies between the VC bounds and actual learning curve behavior have also been pointed out and analyzed in other machine learning work <ref> [19, 17] </ref>. Of course, the VC bounds might simply be inapplicable to these experiments, because backpropagation is not equivalent to empirical error minimization. <p> The in put distribution D N is uniform over f0; 1g N . A similar sce nario has also been analyzed in the machine learning litera ture <ref> [19, 17] </ref>. We will examine the thermodynamic limit for two different choices of target functions f N . We begin with the target function f = f0; 1g N , in which every input is a pos itive example.
Reference: [18] <author> D. Pollard. </author> <title> Convergence of Stochastic Processes. </title> <publisher> Springer-Verlag, </publisher> <year> 1984. </year>
Reference-contexts: In particular, we will exhibit examples of distribution-specific bounds that are much tighter than the distribution-free VC bounds. It is only for infinite function classes that the union bound fails spectacularly, for here the bound diverges and becomes useless. The VC dimension, VC entropy, and random covering number <ref> [24, 18, 6, 13] </ref> are the known tools for dealing with the correlations neglected by the union bound. These tools have previously been applied to the function class as a whole. In our current research efforts, we are attempting to refine these tools by applying them to error shells.
Reference: [19] <author> W. Sarrett and M. Pazzani. </author> <title> Average case analysis of empirical and explanation-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 9(4) </volume> <pages> 349-372, </pages> <year> 1992. </year>
Reference-contexts: Discrepancies between the VC bounds and actual learning curve behavior have also been pointed out and analyzed in other machine learning work <ref> [19, 17] </ref>. Of course, the VC bounds might simply be inapplicable to these experiments, because backpropagation is not equivalent to empirical error minimization. <p> The in put distribution D N is uniform over f0; 1g N . A similar sce nario has also been analyzed in the machine learning litera ture <ref> [19, 17] </ref>. We will examine the thermodynamic limit for two different choices of target functions f N . We begin with the target function f = f0; 1g N , in which every input is a pos itive example.
Reference: [20] <author> D. B. Schwartz, V. K. Samalam, J. S. Denker, and S. A. Solla. </author> <title> Exhaustive learning. </title> <journal> Neural Comput., </journal> <volume> 2 </volume> <pages> 374-385, </pages> <year> 1990. </year>
Reference-contexts: The theoretical revisions of the VC theory mentioned above cannot explain such behavior, because they conservatively modify only with the constant factors of the same power laws. In this paper, we show that ideas from statistical mechanics (namely, the annealed approximation <ref> [20, 1, 21] </ref> and the thermodynamic limit [21]) can be used as the basis of a mathematically precise and rigorous theory of learning curves. 3 Speaking coarsely, there are two main ideas be 2 By a power law, we mean the functional form (a=m) b , where a; b &gt; 0
Reference: [21] <author> H. S. Seung, H. Sompolinsky, and N. Tishby. </author> <title> Statistical mechanics of learning from examples. </title> <journal> Physical Review, </journal> <volume> A45:6056-6091, </volume> <year> 1992. </year>
Reference-contexts: The theoretical revisions of the VC theory mentioned above cannot explain such behavior, because they conservatively modify only with the constant factors of the same power laws. In this paper, we show that ideas from statistical mechanics (namely, the annealed approximation <ref> [20, 1, 21] </ref> and the thermodynamic limit [21]) can be used as the basis of a mathematically precise and rigorous theory of learning curves. 3 Speaking coarsely, there are two main ideas be 2 By a power law, we mean the functional form (a=m) b , where a; b &gt; 0 <p> The theoretical revisions of the VC theory mentioned above cannot explain such behavior, because they conservatively modify only with the constant factors of the same power laws. In this paper, we show that ideas from statistical mechanics (namely, the annealed approximation [20, 1, 21] and the thermodynamic limit <ref> [21] </ref>) can be used as the basis of a mathematically precise and rigorous theory of learning curves. 3 Speaking coarsely, there are two main ideas be 2 By a power law, we mean the functional form (a=m) b , where a; b &gt; 0 are constants. 3 Aside to the statistical <p> learning curve, and the behavior of our bound is very similar in shape to learning curves obtained in 5 The designation "Ising" refers to the 1 constraint, which is present in the original Ising model of magnetism with N interacting spins. both simulations and non-rigorous replica calculations from statistical physics <ref> [12, 22, 21, 8] </ref>. 6 It is instructive to compare our bounds with the cardi-nality and VC bounds for this problem.
Reference: [22] <author> H. Sompolinsky, N. Tishby, and H. S. Seung. </author> <title> Learning from examples in large neural networks. </title> <journal> Phys. Rev. Lett., </journal> <volume> 65(13) </volume> <pages> 1683-1686, </pages> <year> 1990. </year>
Reference-contexts: We first consider the class of Ising perceptrons <ref> [10, 12, 22] </ref>. <p> learning curve, and the behavior of our bound is very similar in shape to learning curves obtained in 5 The designation "Ising" refers to the 1 constraint, which is present in the original Ising model of magnetism with N interacting spins. both simulations and non-rigorous replica calculations from statistical physics <ref> [12, 22, 21, 8] </ref>. 6 It is instructive to compare our bounds with the cardi-nality and VC bounds for this problem.
Reference: [23] <author> V. Vapnik, E. Levin, and Y. LeCun. </author> <title> Measuring the VC dimension of a learning machine. </title> <journal> Neural Comput., </journal> <note> 1994. To appear. </note>
Reference-contexts: Work on the "empirical VC dimension" has tried to measure the dependence of learning curves on both the algorithm and the distribution via backpropagation experiments <ref> [23] </ref>. Perhaps the most striking evidence for the fact that the VC bounds can sometimes fail to model the true behavior of learning curves has come from statistical physics.
Reference: [24] <author> V. N. Vapnik. </author> <title> Estimation of Dependences Based on Empirical Data. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1982. </year>
Reference-contexts: 1 Introduction According to the Vapnik-Chervonenkis (VC) theory of learning curves <ref> [25, 24] </ref>, minimizing empirical error within a function class F on a random sample of m examples leads to generalization error bounded by ~ O (d=m) (in the case that the target function is contained in F ) or ~ O ( p d=m) plus the optimal generalization error achievable within <p> some theorists have sought to preserve the functional form of the VC bounds, but to replace the VC dimension in this functional form by an appropriate distribution-specific quantity, such as the VC entropy (which is the expectation of the logarithm of the number of dichotomies realized by the function class) <ref> [24, 14, 3] </ref>. Work on the "empirical VC dimension" has tried to measure the dependence of learning curves on both the algorithm and the distribution via backpropagation experiments [23]. <p> In particular, we will exhibit examples of distribution-specific bounds that are much tighter than the distribution-free VC bounds. It is only for infinite function classes that the union bound fails spectacularly, for here the bound diverges and becomes useless. The VC dimension, VC entropy, and random covering number <ref> [24, 18, 6, 13] </ref> are the known tools for dealing with the correlations neglected by the union bound. These tools have previously been applied to the function class as a whole. In our current research efforts, we are attempting to refine these tools by applying them to error shells.
Reference: [25] <author> V. N. Vapnik and A. Y. Chervonenkis. </author> <title> On the uniform convergence of relative frequencies of events to their probabilities. </title> <journal> Theory of Probability and its Applications, </journal> <volume> 16(2) </volume> <pages> 264-280, </pages> <year> 1971. </year>
Reference-contexts: 1 Introduction According to the Vapnik-Chervonenkis (VC) theory of learning curves <ref> [25, 24] </ref>, minimizing empirical error within a function class F on a random sample of m examples leads to generalization error bounded by ~ O (d=m) (in the case that the target function is contained in F ) or ~ O ( p d=m) plus the optimal generalization error achievable within

References-found: 25

