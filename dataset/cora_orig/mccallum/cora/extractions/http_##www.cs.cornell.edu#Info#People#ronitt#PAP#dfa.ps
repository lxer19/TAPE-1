URL: http://www.cs.cornell.edu/Info/People/ronitt/PAP/dfa.ps
Refering-URL: http://www.cs.cornell.edu/Info/People/ronitt/papers.html
Root-URL: 
Title: Efficient Learning of Typical Finite Automata from Random Walks  
Author: Yoav Freund Michael Kearns Dana Ron Ronitt Rubinfeld Robert E. Schapire Linda Sellie 
Date: September 20, 1997  
Address: Murray Hill, NJ 07974  Murray Hill, NJ 07974  Cambridge, MA 02138  Ithaca, NY 14853  Murray Hill, NJ 07974  Chicago, IL 60637  
Affiliation: AT&T Research  AT&T Research  MIT  Cornell University  AT&T Research  University of Chicago  
Abstract: This paper describes new and efficient algorithms for learning deterministic finite automata. Our approach is primarily distinguished by two features: (1) the adoption of an average-case setting to model the "typical" labeling of a finite automaton, while retaining a worst-case model for the underlying graph of the automaton, along with (2) a learning model in which the learner is not provided with the means to experiment with the machine, but rather must learn solely by observing the automaton's output behavior on a random input sequence. The main contribution of this paper is in presenting the first efficient algorithms for learning non-trivial classes of automata in an entirely passive learning model. We adopt an on-line learning model in which the learner is asked to predict the output of the next state, given the next symbol of the random input sequence; the goal of the learner is to make as few prediction mistakes as possible. Assuming the learner has a means of resetting the target machine to a fixed start state, we first present an efficient algorithm that makes an expected polynomial number of mistakes in this model. Next, we show how this first algorithm can be used as a subroutine by a second algorithm that also makes a polynomial number of mistakes even in the absence of a reset. Along the way, we prove a number of combinatorial results for randomly labeled automata. We also show that the labeling of the states and the bits of the input sequence need not be truly random, but merely semi-random. Finally, we discuss an extension of our results to a model in which automata are used to represent distributions over binary strings. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Dana Angluin. </author> <title> On the complexity of minimum inference of regular sets. </title> <journal> Information and Control, </journal> <volume> 39 </volume> <pages> 337-350, </pages> <year> 1978. </year>
Reference-contexts: The intractability results for various passive learning models begin with the work of Gold [11] and 3 Angluin <ref> [1] </ref>, who proved that the problem of finding the smallest automaton consistent with a set of accepted and rejected strings is NP-complete. <p> Based on this information, the adversary is then allowed to choose the bias of the next output bit to be any real number in the range <ref> [; 1 ] </ref> for a fixed constant 0 &lt; 1=2. The next output bit is then generated by flipping a coin of bias . Thus, a semi-random source guarantees only a rather weak form of independence among its output bits. <p> In this model no binary labels are associated with the states Q. Instead, there is a real-valued function ' : Q ! <ref> [0; 1] </ref>. The underlying state graph (defined by the transition function t ) together with ' defines a probabilistic generator of binary strings in the following manner: we now think of the bits on the edges of the machine as the output bits. <p> Their algorithm has been successfully applied to the problem of handwriting recognition and to construct multiple-pronunciation models for spoken words. We define two parameters 0 &lt; ; &lt; 1=2 and associate with each state q 2 Q some distribution P q over the interval <ref> [; 1 ] </ref>. We assume that the value of '(q) is chosen independently for each state q according to P q . <p> Thus the analysis of this case essentially reduces to the analysis of of learning typical DFA. Since '(q) 2 <ref> [; 1 ] </ref> also determines the bias for the choice of the next state, we can apply the result stated in the previous subsection and get that the expected number of default mistakes made by the algorithm is O m n (n 2 =ffi) 2 log (1=) log (n=ffi) ; where
Reference: [2] <author> Dana Angluin. </author> <title> Learning regular sets from queries and counterexamples. </title> <journal> Information and Computation, </journal> <volume> 75 </volume> <pages> 87-106, </pages> <month> November </month> <year> 1987. </year>
Reference-contexts: This again implies intractability for the same problem in the mistake-bound models. The situation becomes considerably brighter when we turn to the problem of actively learning finite automata. Angluin <ref> [2] </ref>, elaborating on an algorithm of Gold [10], proved that if a learning algorithm is provided with both passive counterexamples to its current hypothesis automaton (that is, arbitrary strings on which the hypothesis automaton disagrees with the target) and the ability to actively query the target machine on any string of
Reference: [3] <author> Y. Azar, A. Z. Broder, A. R. Karlin, N. Linial, and S. Phillips. </author> <title> Biased random walks. </title> <booktitle> In Proceedings of the Twenty-Fourth Annual ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 1-9, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: In addition to investigations of their properties as a computational resource [6, 24, 27, 28], semi-random sources have also been used as a model for studying the complexity of graph coloring that falls between worst-case and average-case (random) models [5], and as a model for biased random walks on graphs <ref> [3] </ref>. <p> However, we are not the first authors to use semi-randomness to investigate models between the worst case and the average (random) case. Blum [5] studied the complexity of coloring semi-random graphs, and Azar et al. have considered semi-random sources to model biased random walks on graphs <ref> [3] </ref>. Now assume that the adversary can choose the label of each state in G M by flipping a coin whose bias is chosen by the adversary from the range [ 1 ; 1 1 ].
Reference: [4] <author> Ya. M. Barzdin'. </author> <title> Deciphering of sequential networks in the absence of an upper limit on the number of states. </title> <journal> Soviet Physics Doklady, </journal> <volume> 15(2) </volume> <pages> 94-97, </pages> <month> August </month> <year> 1970. </year>
Reference-contexts: All of the results discussed above, whether in a passive or an active model, have considered the worst-case complexity of learning: to be considered efficient, algorithms must have small running time on any finite automaton. However, average-case models have been examined in the extensive work of Trakhtenbrot and Barzdin' <ref> [4, 25] </ref>.
Reference: [5] <author> Avrim Blum. </author> <title> Some tools for approximate 3-coloring. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 41(3) </volume> <pages> 470-516, </pages> <year> 1994. </year>
Reference-contexts: In addition to investigations of their properties as a computational resource [6, 24, 27, 28], semi-random sources have also been used as a model for studying the complexity of graph coloring that falls between worst-case and average-case (random) models <ref> [5] </ref>, and as a model for biased random walks on graphs [3]. <p> However, we are not the first authors to use semi-randomness to investigate models between the worst case and the average (random) case. Blum <ref> [5] </ref> studied the complexity of coloring semi-random graphs, and Azar et al. have considered semi-random sources to model biased random walks on graphs [3].
Reference: [6] <author> B. Chor and O. Goldreich. </author> <title> Unbiased bits from sources of weak randomness and probabilistic communication complexity. </title> <journal> SIAM Journal on Computing, </journal> <volume> 17 </volume> <pages> 230-261, </pages> <year> 1988. </year>
Reference-contexts: In addition to investigations of their properties as a computational resource <ref> [6, 24, 27, 28] </ref>, semi-random sources have also been used as a model for studying the complexity of graph coloring that falls between worst-case and average-case (random) models [5], and as a model for biased random walks on graphs [3]. <p> The next output bit is then generated by flipping a coin of bias . Thus, a semi-random source guarantees only a rather weak form of independence among its output bits. Semi-randomness was introduced by Santha and Vazirani and subsequently investigated by several researchers <ref> [6, 24, 27, 28] </ref> for its abstract properties as a computational resource and its relationship to true randomness. However, we are not the first authors to use semi-randomness to investigate models between the worst case and the average (random) case.
Reference: [7] <author> T. Dean, D. Angluin, K. Basye, S. Engelson, L. Kaelbling, E. Kokkevis, and O. Maron. </author> <title> Inferring finite automata with stochastic output functions and an application to map learning. </title> <journal> Machine Learning, </journal> <volume> 18(1) </volume> <pages> 81-108, </pages> <year> 1995. </year>
Reference-contexts: Nevertheless, Rivest and Schapire extend Angluin's algorithm and provide a polynomial time algorithm for inferring any finite automaton from a single continuous walk on the target automaton. Variants of this algorithm have recently been examined by Dean et al. <ref> [7] </ref>. All of the results discussed above, whether in a passive or an active model, have considered the worst-case complexity of learning: to be considered efficient, algorithms must have small running time on any finite automaton.
Reference: [8] <author> M. Feder, N. Merhav, and M. Gutman. </author> <title> Universal prediction of individual sequences. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 38 </volume> <pages> 1258-1270, </pages> <year> 1992. </year>
Reference-contexts: ` (`2 ` + 1)m R to n2 ` 0 (` 0 ` 0 R . 6.2 Learning Distributions on Strings An interesting extension of our results is to a model where automata are used to represent distributions over binary strings such as those discussed by Feder, Merhav and Gutman <ref> [8] </ref>, rather than as acceptors of languages. In this model no binary labels are associated with the states Q. Instead, there is a real-valued function ' : Q ! [0; 1].
Reference: [9] <author> William Feller. </author> <title> An Introduction to Probability and its Applications, volume 1. </title> <publisher> John Wiley and Sons, </publisher> <address> third edition, </address> <year> 1968. </year>
Reference-contexts: The expected number of default mistakes made is then the sum of the expectations of the random variables defined above. Computing the expectation of each of these variables in turn reduces to the so-called Coupon Collector's Problem <ref> [9] </ref>: there are N types of coupons, and at each step we are given a uniformly chosen coupon.
Reference: [10] <author> E. Mark Gold. </author> <title> System identification via state characterization. </title> <journal> Automatica, </journal> <volume> 8 </volume> <pages> 621-636, </pages> <year> 1972. </year>
Reference-contexts: This again implies intractability for the same problem in the mistake-bound models. The situation becomes considerably brighter when we turn to the problem of actively learning finite automata. Angluin [2], elaborating on an algorithm of Gold <ref> [10] </ref>, proved that if a learning algorithm is provided with both passive counterexamples to its current hypothesis automaton (that is, arbitrary strings on which the hypothesis automaton disagrees with the target) and the ability to actively query the target machine on any string of the algorithm's choosing (known as membership queries),
Reference: [11] <author> E. Mark Gold. </author> <title> Complexity of automaton identification from given data. </title> <journal> Information and Control, </journal> <volume> 37 </volume> <pages> 302-320, </pages> <year> 1978. </year>
Reference-contexts: The intractability results for various passive learning models begin with the work of Gold <ref> [11] </ref> and 3 Angluin [1], who proved that the problem of finding the smallest automaton consistent with a set of accepted and rejected strings is NP-complete.
Reference: [12] <author> David Haussler, Nick Littlestone, and Manfred K. Warmuth. </author> <title> Predicting f0; 1g-functions on randomly drawn points. </title> <journal> Information and Computation, </journal> <volume> 115 </volume> <pages> 248-292, </pages> <year> 1994. </year>
Reference-contexts: Such results imply the intractability of learning finite automata (when using finite automata as the hypothesis representation) in a variety of passive learning models, including the well-studied "probably approximately correct" (or PAC ) model introduced by Valiant [26] and the mistake-bound models of Littlestone [16] and Haussler, Littlestone and Warmuth <ref> [12] </ref>. These results demonstrated the intractability of passively learning finite automaton when we insist that the hypothesis constructed by the learner also be a finite automaton, but did not address the complexity of passively learning finite automata by more powerful representations.
Reference: [13] <author> M. J. Kearns, Y. Mansour, D. Ron, R. Rubinfeld, R. E. Schapire, and L. Sellie. </author> <title> On the learnability of discrete distributions. </title> <booktitle> In The 25th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 273-282, </pages> <year> 1994. </year>
Reference-contexts: A common and natural learning goal in this context is to learn to approximately predict the probability of a binary string after observing a sample of strings from the distribution. In general, this problem is shown to be as hard as the problem of learning parity with noise in <ref> [13] </ref>, which is related to a longstanding open problem in coding theory. However, there is a simple variant of Reset that can perform an online version of this task efficiently with high probability if the function ' is chosen according to some natural classes of distributions.
Reference: [14] <author> M. J. Kearns and L. G. Valiant. </author> <title> Cryptographic limitations on learning Boolean formulae and finite automata. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 41 </volume> <pages> 67-95, </pages> <year> 1994. </year>
Reference-contexts: Although such changes of hypothesis representation can in some instances provably reduce the complexity of certain learning problems from NP-hard to polynomial time [18], Kearns and Valiant <ref> [14] </ref> demonstrated that this is not the case for finite automata by proving that passive learning in the PAC model by any reasonable representation is as hard as breaking various cryptographic protocols that are based on factoring. This again implies intractability for the same problem in the mistake-bound models. <p> This result provides an efficient algorithm for learning finite automata in the PAC model augmented with membership queries. Together with the results of Kearns and Valiant <ref> [14] </ref>, this separates (under cryptographic assumptions) the PAC model and the PAC model with membership queries, so experimentation provably helps for learning finite automata in the PAC setting.
Reference: [15] <author> K. J. Lang. </author> <title> Random DFA's can be approximately learned from sparse uniform examples. </title> <booktitle> In Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 45-52, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: For an interesting empirical study of the performance of one of these algorithms, see Lang's paper <ref> [15] </ref> on experiments he conducted using automata that were chosen partially or completely at random. The primary lesson to be gleaned from the previous work on learning finite automata is that passive learning of automata tends to be computationally difficult.
Reference: [16] <author> Nick Littlestone. </author> <title> Learning when irrelevant attributes abound: A new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 285-318, </pages> <year> 1988. </year>
Reference-contexts: Such results imply the intractability of learning finite automata (when using finite automata as the hypothesis representation) in a variety of passive learning models, including the well-studied "probably approximately correct" (or PAC ) model introduced by Valiant [26] and the mistake-bound models of Littlestone <ref> [16] </ref> and Haussler, Littlestone and Warmuth [12]. These results demonstrated the intractability of passively learning finite automaton when we insist that the hypothesis constructed by the learner also be a finite automaton, but did not address the complexity of passively learning finite automata by more powerful representations.
Reference: [17] <author> L. Pitt and M. K. Warmuth. </author> <title> The minimum consistent DFA problem cannot be approximated within any polynomial. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 40(1) </volume> <pages> 95-142, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: This result left open the possibility of efficiently approximating the smallest machine, which was later dismissed in a very strong sense by the NP -hardness results of Pitt and Warmuth <ref> [17, 19] </ref>.
Reference: [18] <author> Leonard Pitt and Leslie G. Valiant. </author> <title> Computational limitations on learning from examples. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 35(4) </volume> <pages> 965-984, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: Although such changes of hypothesis representation can in some instances provably reduce the complexity of certain learning problems from NP-hard to polynomial time <ref> [18] </ref>, Kearns and Valiant [14] demonstrated that this is not the case for finite automata by proving that passive learning in the PAC model by any reasonable representation is as hard as breaking various cryptographic protocols that are based on factoring.
Reference: [19] <author> Leonard Pitt and Manfred K. Warmuth. </author> <title> Prediction-preserving reducibility. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 41(3) </volume> <pages> 430-467, </pages> <month> December </month> <year> 1990. </year> <month> 17 </month>
Reference-contexts: This result left open the possibility of efficiently approximating the smallest machine, which was later dismissed in a very strong sense by the NP -hardness results of Pitt and Warmuth <ref> [17, 19] </ref>.
Reference: [20] <author> R. Rivest and R. Schapire. </author> <title> Diversity-based inference of finite automata. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 43(3) </volume> <pages> 555-589, </pages> <year> 1994. </year>
Reference-contexts: Rivest and Schapire <ref> [20, 21] </ref> considered the natural extension in which we regard the target automaton as representing some aspect of the learner's physical environment, and in which experimentation is allowed, but without a reset.
Reference: [21] <author> R. L. Rivest and R. E. Schapire. </author> <title> Inference of finite automata using homing sequences. </title> <journal> Information and Computation, </journal> <volume> 103(2) </volume> <pages> 299-347, </pages> <year> 1993. </year>
Reference-contexts: For our second algorithm, which eliminates the reset mechanism, the important combinatorial object is the local homing sequence, which is related to but weaker than the homing sequences used by Rivest and Schapire <ref> [21] </ref>. Informally, a (local) homing sequence is an input sequence which, when executed, may allow the learner to determine "where it is" in the machine based on the observed output sequence. <p> Rivest and Schapire <ref> [20, 21] </ref> considered the natural extension in which we regard the target automaton as representing some aspect of the learner's physical environment, and in which experimentation is allowed, but without a reset. <p> In contrast, without a reset, the learner can easily "get lost" with no obvious means of reorienting itself. In a related setting, Rivest and Schapire <ref> [21] </ref> introduced the idea of using a homing sequence for learning finite automata in the absence of a reset.
Reference: [22] <author> Ronald L. Rivest and Robert Sloan. </author> <title> Learning complicated concepts reliably and usefully. </title> <booktitle> In Proceedings AAAI-88, </booktitle> <pages> pages 635-639, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Our first algorithm, for any state graph, and with high probability over the random labeling of the state graph, will make only an expected polynomial number of mistakes. In fact, we show that this algorithm has the stronger property of reliability <ref> [22] </ref>: if allowed to output either a f+; g-prediction or the special 2 symbol "?" (called a default mistake) the algorithm will make no prediction mistakes, and only an expected polynomial number of default mistakes. In other words, every f+; g-prediction made by the algorithm will be correct.
Reference: [23] <author> D. Ron, Y. Singer, and N. Tishby. </author> <title> On the learnability and usage of acyclic probabilistic finite automata. </title> <booktitle> In Proceedings of the Eighth Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 31-40, </pages> <year> 1995. </year>
Reference-contexts: Finally we give a modification of our algorithm which applies to the setting of learning probability distributions over binary strings. The algorithm of Ron, Singer and Tishby <ref> [23] </ref> which learns acyclic probabilistic finite automata builds on the algorithm given here. Their algorithm has been successfully applied to the problem of handwriting recognition and to construct multiple-pronunciation models for spoken words. <p> As usual, the structure of the automaton as defined by t is unrestricted (worst-case). The algorithm of Ron, Singer and Tishby <ref> [23] </ref> which learns acyclic probabilistic finite automata builds on the algorithm given here. Their algorithm has been successfully applied to the problem of handwriting recognition and to construct multiple-pronunciation models for spoken words.
Reference: [24] <author> M. Santha and U. V. Vazirani. </author> <title> Generating quasi-random sequences from semi-random sources. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 33(1) </volume> <pages> 75-87, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: Limited independence is formalized using the semi-random model of Santha and Vazirani <ref> [24] </ref>, in which each label is determined by the outcome of a coin flip of variable bias chosen by an omniscient adversary to be between and 1 . <p> In addition to investigations of their properties as a computational resource <ref> [6, 24, 27, 28] </ref>, semi-random sources have also been used as a model for studying the complexity of graph coloring that falls between worst-case and average-case (random) models [5], and as a model for biased random walks on graphs [3]. <p> Then our algorithms still work even in the case that G does not generate independent, unbiased bits but is instead a semi-random source as defined by Santha and Vazirani <ref> [24] </ref>. Briefly, a semi-random source in our context is an omniscient adversary with complete knowledge of the current state of our algorithm and complete memory of all bits previously generated. <p> The next output bit is then generated by flipping a coin of bias . Thus, a semi-random source guarantees only a rather weak form of independence among its output bits. Semi-randomness was introduced by Santha and Vazirani and subsequently investigated by several researchers <ref> [6, 24, 27, 28] </ref> for its abstract properties as a computational resource and its relationship to true randomness. However, we are not the first authors to use semi-randomness to investigate models between the worst case and the average (random) case.
Reference: [25] <author> B. A. Trakhtenbrot and Ya. M. Barzdin'. </author> <title> Finite Automata: Behavior and Synthesis. </title> <publisher> North-Holland, </publisher> <year> 1973. </year>
Reference-contexts: One of our main motivations in studying a model mixing worst-case and average-case analyses was the hope for efficient passive learning algorithms that remained in the gap between the pioneering work of Trakhtenbrot and Barzdin' <ref> [25] </ref>, and the intractability results discussed in the history section below for passive learning in models where both the state graph and the labeling are worst-case. <p> All of the results discussed above, whether in a passive or an active model, have considered the worst-case complexity of learning: to be considered efficient, algorithms must have small running time on any finite automaton. However, average-case models have been examined in the extensive work of Trakhtenbrot and Barzdin' <ref> [4, 25] </ref>. <p> The expression "uniformly almost all automata" is borrowed from Trakhtenbrot and Barzdin' <ref> [25] </ref>, and was used by them to refer to a property holding with high probability for any fixed underlying graph. (The term "uniformly" thus indicates that the graph is chosen in a worst-case manner.) Throughout the paper ffi quantifies confidence only over the random choice of labeling for the target automaton <p> Definition 4 A string x 2 f0; 1g fl is a distinguishing string for q 1 and q 2 if q 1 hxi 6= q 2 hxi. The statement of the key combinatorial theorem needed for our algorithm follows. This theorem is also presented by Trakhtenbrot and Barzdin' <ref> [25, Theorem 5.2] </ref> but we include our proof in Appendix A for completeness. Theorem 2 For uniformly almost all automata, every pair of inequivalent states have a distinguishing string of length at most 2 log (n 2 =ffi).
Reference: [26] <author> L. G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <month> November </month> <year> 1984. </year>
Reference-contexts: Such results imply the intractability of learning finite automata (when using finite automata as the hypothesis representation) in a variety of passive learning models, including the well-studied "probably approximately correct" (or PAC ) model introduced by Valiant <ref> [26] </ref> and the mistake-bound models of Littlestone [16] and Haussler, Littlestone and Warmuth [12].
Reference: [27] <author> U. V. Vazirani. </author> <title> Strong communication complexity or generating quasi-random sequences from two communicating semi-random sources. </title> <journal> Combinatorica, </journal> <volume> 7 </volume> <pages> 375-392, </pages> <year> 1987. </year>
Reference-contexts: In addition to investigations of their properties as a computational resource <ref> [6, 24, 27, 28] </ref>, semi-random sources have also been used as a model for studying the complexity of graph coloring that falls between worst-case and average-case (random) models [5], and as a model for biased random walks on graphs [3]. <p> The next output bit is then generated by flipping a coin of bias . Thus, a semi-random source guarantees only a rather weak form of independence among its output bits. Semi-randomness was introduced by Santha and Vazirani and subsequently investigated by several researchers <ref> [6, 24, 27, 28] </ref> for its abstract properties as a computational resource and its relationship to true randomness. However, we are not the first authors to use semi-randomness to investigate models between the worst case and the average (random) case.
Reference: [28] <author> U. V. Vazirani and V. V. Vazirani. </author> <title> Random polynomial time is equal to slightly-random polynomial time. </title> <booktitle> In 26th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 417-428, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: In addition to investigations of their properties as a computational resource <ref> [6, 24, 27, 28] </ref>, semi-random sources have also been used as a model for studying the complexity of graph coloring that falls between worst-case and average-case (random) models [5], and as a model for biased random walks on graphs [3]. <p> The next output bit is then generated by flipping a coin of bias . Thus, a semi-random source guarantees only a rather weak form of independence among its output bits. Semi-randomness was introduced by Santha and Vazirani and subsequently investigated by several researchers <ref> [6, 24, 27, 28] </ref> for its abstract properties as a computational resource and its relationship to true randomness. However, we are not the first authors to use semi-randomness to investigate models between the worst case and the average (random) case.
References-found: 28

