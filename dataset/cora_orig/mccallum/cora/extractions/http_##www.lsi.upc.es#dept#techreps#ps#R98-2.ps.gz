URL: http://www.lsi.upc.es/dept/techreps/ps/R98-2.ps.gz
Refering-URL: http://www.lsi.upc.es/dept/techreps/1998.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: Email:fcastro,davidg@lsi.upc.es  
Title: Query, PACS and simple-PAC Learning  
Author: Jorge Castro and David Guijarro 
Date: January 30, 1998  
Address: UPC, Campus Nord, Modul C5 Jordi Girona Salgado, 1-3, 08034 Barcelona, Spain  
Affiliation: Dept. Llenguatges i Sistemes Informatics,  
Abstract: We study a distribution dependent form of PAC learning that uses probability distributions related to Kolmogorov complexity. We relate the PACS model, defined by Denis, D'Halluin and Gilleron in [3], with the standard simple-PAC model and give a general technique that subsumes the results in [3] and [6]. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Angluin. </author> <title> "Learning regular sets from queries and counterexamples". </title> <journal> Information and Computation, </journal> <volume> 75 </volume> <pages> 87-106, </pages> <month> November </month> <year> 1987. </year>
Reference-contexts: We show here that any query learnable class is also learnable under this extended framework. So, for example, learnability of DFA's in the PACS model (shown in [6]) is a consequence of its query learnability <ref> [1] </ref>. We also study the relationships between simple-PAC and PACS. We show that they are correlated if some conditions hold. These conditions are not unreasonable since they hold for all known algorithms. <p> We do not formalize this issue to keep the understandability of the paper and because it is straightforward. Also, from Theorem 3.3 immediately follows that the class of DFAs, that Angluin showed learnable by membership and equivalence queries in <ref> [1] </ref>, is learnable in the PACS model. This result was shown in a recent paper ([6]) by an argumentation specific to DFA. 4 PACS vs. simple-PAC In this section we study the relationships between PACS and simple-PAC learning models.
Reference: [2] <author> D. Angluin. </author> <title> "Queries and concept learning". </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 319-342, </pages> <year> 1988. </year>
Reference-contexts: Section 2 is devoted to definitions and previous results, Section 3 presents the relation between query learning and PACS and Section 4 relates PACS and simple-PAC. 2 Preliminaires For definitions on learning-from-examples, PAC/query learning and representation classes we refer the reader to <ref> [2, 7, 8] </ref>. We use for the alphabet that codifies examples and l (r) to denote, in a fixed representation class, the length of the shortest representation for a concept r in that class. The simmetric difference of sets A and B is denoted by A4B.
Reference: [3] <author> C. Denis, F. D'Halluin and R. Gilleron. </author> <title> "Pac learning with simple examples". </title> <booktitle> In Proc. of the 13 th Annual Syposium on the Theoretical Aspects of Computer Science, </booktitle> <pages> pages 231-242. </pages> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> 1046. </volume> <publisher> Springer, </publisher> <year> 1996. </year>
Reference-contexts: With the new model, Li and Vitanyi [4] developed a theory of learning for simple concepts |concepts with low Kolmogorov complexity| that intuitively should be poly-nomially learnable. In fact, they showed several examples that strengthen this intuition. Recently, Denis et al. in <ref> [3] </ref> have restricted this model to the case where a benign teacher might choose examples based on the knowledge of the target concept. In this framework, called PACS model, examples with short descriptions with respect to the target concept have high probability to be drawn. <p> In [4] it is shown how to exploit this completeness theorem to obtain new learning algorithms for DNF with simple terms and simple reversible languages. Recently, Denis, D'Halluin and Gilleron <ref> [3] </ref> introduced the PACS model where the learning system might be aided by a benign teacher who knows the target concept and uses this knowledge in selecting the examples. Under this model, examples with low conditional Kolmogorov complexity have high probability. <p> By Kraft's inequality it holds that r 1 (see [5]). Concept classes such as poly-term DNF and k-reversible DFA were shown to be learnable under the PACS model <ref> [3] </ref>. In a later work [6] the learnability of DFA's was also shown. Note that m r and m , where denotes the empty word, are similar to the normalized counterparts of the enumerable distributions m (jr) and m. <p> We use in this paper as universal distributions m r and m instead of m (jr) and m (as Li and Vitanyi proposed) in order to follow the choices in <ref> [3] </ref> and [6]. All the results below can be shown also if m and m (jr) are the choices considered and some of them can be obtained in an easier way. We note that Theorem 2.2 above remains true changing m by m . <p> Observe that Li and Vitanyi showed in [4] that superclasses of simple (DNF) and simple (k-reversible Automata) are simple-PAC learnable. As a consequence of the previous discussion DNF and k-reversible Automata are PACS learnable classes, which subsumes <ref> [3] </ref>. 5 Conclusions and Further Work We have studied a new variation of distribution dependent PAC learning, the PACS model.
Reference: [4] <author> M. Li and P. Vitanyi. </author> <title> "Learning simple concepts under simple distributions". </title> <journal> SIAM Journal of Computing 20, </journal> <pages> pages 911-935, </pages> <year> 1991. </year>
Reference-contexts: Distribution-independent learning is a strong requirement. Many concept classes are not known to be polynomially learnable, or known not to be polynomially learnable if RP 6= NP, although some such concept classes are polynomially learnable under some fixed distributions. Li and Vitanyi proposed in <ref> [4] </ref> the simple-PAC learning model that, roughly speaking, replaces the condition of learning under all distributions of Valiant's original model by the request of learning under all simple distributions, provided the sample is given according to the "universal" distribution. fl Supported by the Esprit Long Term Research Project ALCOM IT (nr. <p> Specifically, they are those multiplicatively dominated by the universal enumerable distribution m. This distribution assigns high probabilities to low Kolmogorov complexity examples. As we will see later, these properties of m have nice consequences with regard to PAC learning under simple distributions. With the new model, Li and Vitanyi <ref> [4] </ref> developed a theory of learning for simple concepts |concepts with low Kolmogorov complexity| that intuitively should be poly-nomially learnable. In fact, they showed several examples that strengthen this intuition. <p> Theorem 2.2 (<ref> [4] </ref>) A concept class C is PAC learnable under the universal distribution m, iff it is PAC learnable under all simple distributions, provided that in the learning phase the set of examples is drawn according to m. In [4] it is shown how to exploit this completeness theorem to obtain new learning algorithms for DNF with simple terms and simple reversible languages. <p> The following standard lemma shows that, drawing a sample of polynomial size according to m r , we get, with high probability, all examples of low conditional Kolmogorov complexity (see <ref> [4] </ref>). Lemma 3.2 Drawing according to m r a sample S of size n d (d ln n+ln 1=ffi), with probability greater than 1 ffi, S includes all the examples in S r d (n). Finally, we show the main result. <p> For a representation class R we define as simple (R) the set fr 2 R : K (r) c log (l (r))g for an arbitrary c that we fix for the rest of the paper, as Li and Vitanyi do in <ref> [4] </ref>. Theorem 4.2 Let R be a PACS learnable representation class that has a learning algorithm with property P . Then, simple (R) is simple-PAC learnable. <p> This does not seem to be enough to guarantee that a simple-PAC learning algorithm that works with m U 1 has to work also with m U 2 . Observe that Li and Vitanyi showed in <ref> [4] </ref> that superclasses of simple (DNF) and simple (k-reversible Automata) are simple-PAC learnable. As a consequence of the previous discussion DNF and k-reversible Automata are PACS learnable classes, which subsumes [3]. 5 Conclusions and Further Work We have studied a new variation of distribution dependent PAC learning, the PACS model.
Reference: [5] <author> M. Li and P. Vitanyi. </author> <title> "An introduction to Kolmogorov complexity and its applications". </title> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: These results can be extended using the conditional Kolmogorov complexity K (xjy) instead of K (x) to define the universal conditional enumerable distribution m (xjy) that 2 verifies 8y 2 fl ; x and where the equality is known as the Conditional Coding Theorem (see <ref> [5] </ref>). Definition 2.1 A distribution D is simple iff it is multiplicatively dominated by the universal distribution. <p> Formally, the probability of drawing an example x for a target concept with representation r is given as m r (x) = r 2 K (xjr) where r satisfies r P x 2 K (xjr) = 1. By Kraft's inequality it holds that r 1 (see <ref> [5] </ref>). Concept classes such as poly-term DNF and k-reversible DFA were shown to be learnable under the PACS model [3]. In a later work [6] the learnability of DFA's was also shown. <p> It is easy to see that all m r are simple and universal for the class of simple distributions, but no one of distributios m r can be enumerable (see Chapter 4 of <ref> [5] </ref>). We use in this paper as universal distributions m r and m instead of m (jr) and m (as Li and Vitanyi proposed) in order to follow the choices in [3] and [6]. <p> The proba bility of the event S S r d (n) is at least 1 ffi. Proof We use the following well known inequalities that relate K (x) and K (xjr) (see <ref> [5] </ref>): (a) K (xjr) K (x) + O (1). 5 To prove (i) we use (b) and get r 2 K (x) r 2 K (xjr)K (r)O (1) where and r are the constants such that m (x) = 2 K (x) and m r (x) = r 2 K (xjr)
Reference: [6] <author> R. Parekh and V. Honavar. </author> <title> "Learning dfa from simple examples". </title> <booktitle> In Proc. of the 8 th International Workshop on Algorithmic Learning Theory, </booktitle> <pages> pages 116-131. </pages> <booktitle> Lecture Notes in Artificial Intelligence, </booktitle> <volume> 1361. </volume> <publisher> Springer, </publisher> <year> 1997. </year>
Reference-contexts: In this framework, called PACS model, examples with short descriptions with respect to the target concept have high probability to be drawn. We show here that any query learnable class is also learnable under this extended framework. So, for example, learnability of DFA's in the PACS model (shown in <ref> [6] </ref>) is a consequence of its query learnability [1]. We also study the relationships between simple-PAC and PACS. We show that they are correlated if some conditions hold. These conditions are not unreasonable since they hold for all known algorithms. <p> By Kraft's inequality it holds that r 1 (see [5]). Concept classes such as poly-term DNF and k-reversible DFA were shown to be learnable under the PACS model [3]. In a later work <ref> [6] </ref> the learnability of DFA's was also shown. Note that m r and m , where denotes the empty word, are similar to the normalized counterparts of the enumerable distributions m (jr) and m. <p> We use in this paper as universal distributions m r and m instead of m (jr) and m (as Li and Vitanyi proposed) in order to follow the choices in [3] and <ref> [6] </ref>. All the results below can be shown also if m and m (jr) are the choices considered and some of them can be obtained in an easier way. We note that Theorem 2.2 above remains true changing m by m . <p> l (r) c k 2 ; which shows that an appropiate choice of * 0 , namely * 0 such that * 0 l (r) c k 2 &lt; *, suffices for the result. 2 It might seem that property P is a strong requirement; but all known ([3] and <ref> [6] </ref>) PACS learning algorithms satisfy it. In particular, all algorithms obtained by applying Theorem 3.3 trivially fulfill property P . An interesting consequence of Theorems 3.3 and 4.2 is that the simple concepts of a query learnable concept class, are simple-PAC learnable in the sense of Li and Vitanyi.
Reference: [7] <author> L. Valiant. </author> <title> "A theory of the learnable". </title> <journal> Comm. </journal> <volume> ACM,27:1134-1142, </volume> <year> 1984. </year>
Reference-contexts: 1 Introduction One of the most relevant models of learning is "PAC learning", introduced by Valiant <ref> [7] </ref>, which has been widely used to investigate the phenomenon of learning from examples. <p> Section 2 is devoted to definitions and previous results, Section 3 presents the relation between query learning and PACS and Section 4 relates PACS and simple-PAC. 2 Preliminaires For definitions on learning-from-examples, PAC/query learning and representation classes we refer the reader to <ref> [2, 7, 8] </ref>. We use for the alphabet that codifies examples and l (r) to denote, in a fixed representation class, the length of the shortest representation for a concept r in that class. The simmetric difference of sets A and B is denoted by A4B.
Reference: [8] <author> O. Watanabe. </author> <title> "A framework for polynomial time query learnability". </title> <journal> Math. Systems Theory, </journal> <volume> 27 </volume> <pages> 211-229, </pages> <year> 1992. </year> <month> 8 </month>
Reference-contexts: Section 2 is devoted to definitions and previous results, Section 3 presents the relation between query learning and PACS and Section 4 relates PACS and simple-PAC. 2 Preliminaires For definitions on learning-from-examples, PAC/query learning and representation classes we refer the reader to <ref> [2, 7, 8] </ref>. We use for the alphabet that codifies examples and l (r) to denote, in a fixed representation class, the length of the shortest representation for a concept r in that class. The simmetric difference of sets A and B is denoted by A4B. <p> We restrict our attention only to query learning in the presence of bounded teachers. These teachers provide counterexamples up to a given length n or reply affirmatively if no counterexample is of length at most n, see <ref> [8] </ref> for a complete discussion. This is done for technical convenience only. Let R be a representation class of a concept class C, and let L be an exact learning algorithm for R that uses membership and equivalence queries.
References-found: 8

