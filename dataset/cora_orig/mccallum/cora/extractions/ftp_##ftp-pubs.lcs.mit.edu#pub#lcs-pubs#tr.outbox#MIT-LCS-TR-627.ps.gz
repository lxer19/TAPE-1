URL: ftp://ftp-pubs.lcs.mit.edu/pub/lcs-pubs/tr.outbox/MIT-LCS-TR-627.ps.gz
Refering-URL: http://www.sls.lcs.mit.edu/~mike/masters.html
Root-URL: 
Title: Automatic Acquisition of Language Models for Speech Recognition  
Author: by Michael Kyle McCandless James R. Glass 
Degree: 1992 Submitted to the Department of Electrical Engineering and Computer Science in partial fulfillment of the requirements for the degree of Master of Science in Electrical Engineering and Computer Science at the  All rights reserved. Author  Certified by  Scientist Thesis Supervisor Accepted by Frederic R. Morgenthaler Chairman, Departmental Committee on Graduate Students  
Date: May 1994  May 12, 1994  
Affiliation: S.B., Electrical Engineering and Computer Science Massachusetts Institute of Technology,  MASSACHUSETTS INSTITUTE OF TECHNOLOGY  c Massachusetts Institute of Technology 1994.  Department of Electrical Engineering and Computer Science  Research  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> A. Aho and J. Ullman. </author> <title> The Theory of Parsing, Translation and Compiling. </title> <address> En-glewood Cliffs, NJ: </address> <publisher> Prentice-Hall, </publisher> <year> 1972. </year>
Reference-contexts: The grammar may also be used in the other direction: given a sentence, find the sequence of reductions which, starting with S, would result in the provided sentence. This process is referred to as parsing, and there are well known search algorithms to implement it <ref> [1, 15] </ref>. The output can then be represented as a parse tree, as shown in Figure 2-2. It is possible for a given sentence to have more than one derivation with respect to a grammar. In such cases the grammar is said to be ambiguous. <p> It is a probabilistic extension to the efficient LR parsing algorithm <ref> [1] </ref>, and is designed to achieve full coverage of a test set by implementing error recovery 34 schemes when a sentence fails to parse. The LR model takes a CFG as input and uses this CFG to construct a deterministic parser.
Reference: [2] <author> J. K. Baker. </author> <title> "Trainable grammars for speech recognition", </title> <booktitle> Proc. Conference of the Acoustical Society of America, </booktitle> <pages> 547-550, </pages> <month> June </month> <year> 1979. </year>
Reference-contexts: Brill et al. [8] tested a metric based on mutual information to automatically parse sentences, starting from the parts of speech of the words. Brill [7] developed a system which learns a decision tree to bracket sentences. Pereira and Schabes [35] developed a modified version of the inside/outside algorithm <ref> [2, 29] </ref> to automatically parse test sentences, given a very small amount of training data. Bod [4] uses a monte-carlo based approach to bracket 11 sentences. <p> A SCFG may be converted into the corresponding n-gram model [43]. The inside/outside algorithm is used to optimize the probabilities such that the total probability of the training set is optimized <ref> [2, 29] </ref>. Given a sentence prefix, it is possible to predict the probabilities of all possible next words which may follow according to the SCFG. 33 2.11.2 TINA TINA [38] is a structured probabilistic language model developed explicitly for both understanding and ease of integration with a speech recognition system.
Reference: [3] <author> A. Barr, E. Feigenbaum and P. Cohen. </author> <booktitle> The Handbook of Artificial Intelligence. </booktitle> <address> Los Altos, CA: </address> <publisher> William Kaufman, </publisher> <year> 1981. </year>
Reference-contexts: It operates on the phonetic segments one at a time. The second step of the search process is a more thorough search which uses a more powerful language model. This search is done with an A* search <ref> [3] </ref>, and it operates with words as the fundamental units. It uses the results of the Viterbi search to help it through the search. 100 5.3 Search 5.3.1 Lexicon Graph A critical component in the search phase of the recognizer is the lexicon graph. <p> This leads us to the A* search. 103 5.6 A Search Because the Viterbi search cannot easily integrate more powerful language models, summit uses another search strategy, called the A fl search <ref> [3] </ref>, to tie in more powerful language models. Unlike the Viterbi search, the A fl search is a time asynchronous search which derives from the best first search formalism. It allows us to integrate more powerful language models and to compute the top N best scoring sentences from the recognizer.
Reference: [4] <author> R. </author> <title> Bod. "Monte carlo parsing", </title> <booktitle> Proc. International Workshop on Parsing Technologies, </booktitle> <pages> 1-12, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: Brill [7] developed a system which learns a decision tree to bracket sentences. Pereira and Schabes [35] developed a modified version of the inside/outside algorithm [2, 29] to automatically parse test sentences, given a very small amount of training data. Bod <ref> [4] </ref> uses a monte-carlo based approach to bracket 11 sentences.
Reference: [5] <author> L. Breiman, J. Friedman, R. Olshen and C. Stone. </author> <title> Classification and Regression Trees. </title> <address> Monterey, CA: </address> <publisher> Wadsworth and Brooks/Cole Advanced Books and Software, </publisher> <year> 1984. </year>
Reference-contexts: There are fewer contexts to store, since many word n-gram contexts will map to a single class n-gram context. 2.10 Decision Tree Models Another approach to language modeling is the decision tree language model <ref> [5, 10] </ref>. These models function like the n-gram model in that they represent a function to classify any possible context into a finite number of equivalence classes. But they differ on how the function is chosen. In particular, a binary decision tree is created from the training set.
Reference: [6] <author> E. Brill. </author> <title> "A simple rule-based part of speech tagger", </title> <booktitle> Proc. Conference on Applied Natural Language Processing, </booktitle> <pages> 152-155, </pages> <year> 1992. </year>
Reference-contexts: Brown et al. [11] use minimal loss of mutual information as the criterion for merging word classes hierarchically. Jardino and Adda [24] used simulated annealing to divide words into classes in a top-down manner. On a similar vein of automatic acquisition, Brill <ref> [6] </ref> developed a rule based system which automatically acquires rules to decide how to tag a word with its linguistic part of speech, given the word's context.
Reference: [7] <author> E. Brill. </author> <title> "A Corpus-Based Approach to Language Learning", </title> <type> Ph.D. Thesis, </type> <institution> Department of Computer and Information Science, University of Pennsylvania, </institution> <year> 1993. </year>
Reference-contexts: The second step is to then tag each of the phrases with names denoting phrasal equivalence classes. Brill et al. [8] tested a metric based on mutual information to automatically parse sentences, starting from the parts of speech of the words. Brill <ref> [7] </ref> developed a system which learns a decision tree to bracket sentences. Pereira and Schabes [35] developed a modified version of the inside/outside algorithm [2, 29] to automatically parse test sentences, given a very small amount of training data. Bod [4] uses a monte-carlo based approach to bracket 11 sentences.
Reference: [8] <author> E. Brill, D. Magerman, M. Marcus and B. Santorini. </author> <title> "Deducing linguistic structure from the statistics of large corpora", </title> <booktitle> Proc. DARPA Speech and Natural Language Workshop, </booktitle> <pages> 275-281, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Sentence bracketing is a first step in parsing the sentence it assigns the phrasal structure to the sentence. The second step is to then tag each of the phrases with names denoting phrasal equivalence classes. Brill et al. <ref> [8] </ref> tested a metric based on mutual information to automatically parse sentences, starting from the parts of speech of the words. Brill [7] developed a system which learns a decision tree to bracket sentences. <p> In this manner longer phrases than length two can be considered even though the concatenation process is binary. This metric is based on mutual information, which has been used it other grammar inference approaches as a means to acquire structure <ref> [8] </ref>. P r (u 1 ; u 2 ) fi log ( P r (u 2 ) This score is high when two units are "sticky", meaning the probability that u 2 follows u 1 is much higher than the simple frequency of u 2 .
Reference: [9] <author> E. Brill and M. Marcus. </author> <title> "Automatically acquiring phrase structure using distributional analysis", </title> <booktitle> Proc. DARPA Speech and Natural Language Workshop, </booktitle> <pages> 155-159, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: Some of these approaches utilize a word class n-gram model [11] to evaluate the output, while others simply rely on linguistic knowledge to decide if the system is learning "correct" word classes. Brill and Marcus <ref> [9] </ref> used bottom up clustering to assign words to classes, using divergence as a similarity metric. Brown et al. [11] use minimal loss of mutual information as the criterion for merging word classes hierarchically. Jardino and Adda [24] used simulated annealing to divide words into classes in a top-down manner.
Reference: [10] <author> P. Brown, S. Chen, S. DellaPietra, V. DellaPietra, R. Mercer and P. </author> <title> Resnik. "Using decision-trees in language modeling", </title> <type> Unpublished Report, </type> <year> 1991. </year>
Reference-contexts: There are fewer contexts to store, since many word n-gram contexts will map to a single class n-gram context. 2.10 Decision Tree Models Another approach to language modeling is the decision tree language model <ref> [5, 10] </ref>. These models function like the n-gram model in that they represent a function to classify any possible context into a finite number of equivalence classes. But they differ on how the function is chosen. In particular, a binary decision tree is created from the training set.
Reference: [11] <author> P. Brown, V. Della Pietra, P. de Souza, J. Lai and R. Mercer. </author> <title> "Class-based n-gram models of natural language", </title> <journal> Computational Linguistics, </journal> <volume> Vol. 18, No. 4, </volume> <pages> 467-479, </pages> <month> December </month> <year> 1992. </year> <month> 138 </month>
Reference-contexts: As a result, there have been a number of recent efforts towards empirically deriving some structure of human languages. Most of the approaches have dealt only with word classes, not trying to acquire any phrase structure. Some of these approaches utilize a word class n-gram model <ref> [11] </ref> to evaluate the output, while others simply rely on linguistic knowledge to decide if the system is learning "correct" word classes. Brill and Marcus [9] used bottom up clustering to assign words to classes, using divergence as a similarity metric. Brown et al. [11] use minimal loss of mutual information <p> utilize a word class n-gram model <ref> [11] </ref> to evaluate the output, while others simply rely on linguistic knowledge to decide if the system is learning "correct" word classes. Brill and Marcus [9] used bottom up clustering to assign words to classes, using divergence as a similarity metric. Brown et al. [11] use minimal loss of mutual information as the criterion for merging word classes hierarchically. Jardino and Adda [24] used simulated annealing to divide words into classes in a top-down manner. <p> The sentences for the trigram model are curious because locally the words are somewhat reasonable, but in the long term the sentence completely loses focus. 29 2.9.2 Word Class n-gram Models A slight modification of the n-gram model is that of the word class n-gram model <ref> [11] </ref>. <p> Another extreme grammar is one which performs only classing operations. This means words are grouped into equivalence classes, and no rule has a phrase on its right hand side. In this case, the pcng model based on this grammar will be identical to the word class n-gram model <ref> [11] </ref>, as no sequence of two or more words will ever be 81 Evolution of the bigram and trigram perplexity for the word-class-only standard inference run.
Reference: [12] <author> S. Chen. </author> <title> "The Automatic Acquisition of a Statistical Language Model for En--glish", </title> <booktitle> Ph.D. Thesis in progress, </booktitle> <publisher> Harvard University. </publisher>
Reference: [13] <author> K. Church and W. Gale. </author> <title> "A comparison of the enhanced good-turing and deleted estimation methods for estimating probabilities of english bigrams", </title> <journal> Computers, Speech and Language, </journal> <volume> Vol 5, No 1, </volume> <pages> 19-56, </pages> <year> 1991. </year>
Reference-contexts: Other approaches try to steal more probability mass from infrequent n-grams, for example back-off smoothing [27], which only removes probability mass from those n-grams that occurred fewer than a certain cutoff number of times. There are various other approaches, such as nonlinear discounting [33], and Good/Turing <ref> [21, 13] </ref>. When one uses an n-gram language model to generate random sentences, as shown in Tables 2.1, 2.2 and 2.3, very few of the resulting sentences could be considered valid. This indicates that n-gram models leave substantial room for improvement.
Reference: [14] <author> T. M. Cover and J. A. Thomas. </author> <title> Elements of Information Theory. </title> <address> New York: </address> <publisher> John Wiley and Sons Inc., </publisher> <year> 1991. </year>
Reference-contexts: They could be relaxed to some extent without changing the results proven in this appendix, but they make the proof more straightforward. The true entropy H of a source which generates output symbols w 1 ; w 2 ; ::: indefinitely is defined as <ref> [14] </ref>: H = lim n w n 2W n where the sum is taken over all possible sequences of words of length n, and P (w n ) is the true underlying probability of the sequence of words. <p> Because we've assumed the source is ergodic, if we randomly draw a particular sample of size N , according to the underlying distribution the particular sample will be "typical". Therefore, our test sample t N will be a typical sample, according to the Asymptotic Equipartition Principle for ergodic sources <ref> [14] </ref>. <p> N ) + N w N 2W N = N w N 2W N = N w N 2W N = N w N 2W N P (w N ) (A.10) 1 D (P k ^ P ) (A.11) where D (P k ^ P ) is the relative entropy <ref> [14] </ref> between the probability distributions P and ^ P . Relative entropy is a natural information theoretic distance metric between probability distributions which is always greater than or equal to zero. Equality is achieved if and only iff P = ^ P .
Reference: [15] <author> J. Earley. </author> <title> "An efficient context-free parsing algorithm", </title> <journal> Communications of the ACM, </journal> <volume> Vol. 13, </volume> <pages> 94-102, </pages> <year> 1970. </year>
Reference-contexts: The grammar may also be used in the other direction: given a sentence, find the sequence of reductions which, starting with S, would result in the provided sentence. This process is referred to as parsing, and there are well known search algorithms to implement it <ref> [1, 15] </ref>. The output can then be represented as a parse tree, as shown in Figure 2-2. It is possible for a given sentence to have more than one derivation with respect to a grammar. In such cases the grammar is said to be ambiguous. <p> Thus, whether we compute the sum or the maximum, that value will be the same when we extract a completed path. We chose to implement this procedure using a modified version of Earley's parser <ref> [15] </ref>, which is an efficient CFG parser. Earley's parser processes a sentence in one pass, keeping track of all possible rules applications at every word. Our word-by-word version of the pcng model does precisely this for every path extension.
Reference: [16] <author> K. Fu and T. L. Booth. </author> <title> "Grammatical inference: introduction and survey | part 1", </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> Vol. 5, No. 1, </volume> <pages> 95-111, </pages> <month> January </month> <year> 1975. </year>
Reference-contexts: We can measure the overall performance of these two steps by evaluating the final acquired language model according to the known objective measures for language models. In this chapter we describe an algorithm to implement the first of the above steps. This algorithm is a form of grammar inference <ref> [16] </ref>, which is an iterative process that tries to learn a grammar to describe an example set of training sentences.
Reference: [17] <author> K. Fu and T. L. Booth. </author> <title> "Grammatical inference: introduction and survey | part 2", </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> Vol. 5, No. 5, </volume> <pages> 409-423, </pages> <month> July </month> <year> 1975. </year>
Reference: [18] <author> J. Glass, D. Goddeau, D. Goodine, L. Hetherington, L. Hirschman, M. Phillips, J. Polifroni, C. Pao, S. Seneff and V. Zue. </author> <title> "The MIT atis system: January 1993 progress report", </title> <booktitle> Proc. DARPA Spoken Language Systems Technology Workshop, </booktitle> <month> January </month> <year> 1993. </year>
Reference-contexts: Summit is a segment based speech recognition system which typically employs class based n-gram models as the language model. <ref> [18] </ref>. We tested the performance of the system using acquired pcng models and found them to improve recognition performance when compared to the word trigram model. Not only is word accuracy of interest, but the computational cost is also important.
Reference: [19] <author> D. Goddeau. </author> <title> "Using probabilistic shift-reduce parsing in speech recognition systems", </title> <booktitle> Proc. International Conference on Spoken Language Processing, </booktitle> <pages> 321-324, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: It therefore has the same difficulties with robustness as other structural approaches. Recent work has been done to address this problem, mainly by resorting to standard bigram models for words that fall outside of parsable sequences [39]. 2.11.3 PLR PLR <ref> [19] </ref> is another approach to which generates a probabilistic language model from a CFG as input.
Reference: [20] <author> E. M. Gold. </author> <title> "Language identification in the limit", </title> <journal> Information and Control, </journal> <volume> Vol. 10, </volume> <pages> 447-474, </pages> <year> 1967. </year>
Reference-contexts: Many efforts have been made within this formalism to show that various concept classes are or are not learnable. Gold <ref> [20] </ref> has shown that not even the regular languages, let alone the context free languages, can be learned within this framework. The learning formalism established by Solomonof is very strict in that it demands precise learning of the concept.
Reference: [21] <author> I. J. </author> <title> Good. "The population frequencies of species and the estimation of population parameters", </title> <journal> Biometrika, </journal> <volume> Vol. 40, </volume> <pages> 237-264, </pages> <year> 1953. </year>
Reference-contexts: Other approaches try to steal more probability mass from infrequent n-grams, for example back-off smoothing [27], which only removes probability mass from those n-grams that occurred fewer than a certain cutoff number of times. There are various other approaches, such as nonlinear discounting [33], and Good/Turing <ref> [21, 13] </ref>. When one uses an n-gram language model to generate random sentences, as shown in Tables 2.1, 2.2 and 2.3, very few of the resulting sentences could be considered valid. This indicates that n-gram models leave substantial room for improvement.
Reference: [22] <author> I. Hetherington, M. Phillips, J. Glass and V. Zue. </author> <title> "A fl word network search for continuous speech recognition", </title> <booktitle> Proc. European Conference on Speech Communication and Technology, </booktitle> <pages> 1533-1536, </pages> <month> September </month> <year> 1993. </year>
Reference: [23] <author> L. Hirschman, et al., </author> <title> "Multi-site data collection for a spoken language system", </title> <booktitle> Proc. DARPA Speech and Natural Language Workshop, </booktitle> <pages> 7-14, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: test sentences, only to correctly derive their structure, which makes it difficult to extend them to language models for speech recognition. 1.2 Thesis Outline All of the experiments described in this thesis are conducted within the atis domain, which is a common evaluation domain in the ARPA speech understanding community <ref> [23] </ref>. Atis contains flight information queries solicited from users who solved problem scenarios. For example, these are some typical sentences: "what is the earliest flight from boston to pittsburgh tomorrow afternoon", or "could you tell me if american airlines flight two oh one serves breakfast".
Reference: [24] <author> M. Jardino and G. Adda. </author> <title> "Language modelling for CSR of large corpus using automatic classification of words", </title> <booktitle> Proc. European Conference on Speech Communication and Technology, </booktitle> <pages> 1191-1194, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: Brill and Marcus [9] used bottom up clustering to assign words to classes, using divergence as a similarity metric. Brown et al. [11] use minimal loss of mutual information as the criterion for merging word classes hierarchically. Jardino and Adda <ref> [24] </ref> used simulated annealing to divide words into classes in a top-down manner. On a similar vein of automatic acquisition, Brill [6] developed a rule based system which automatically acquires rules to decide how to tag a word with its linguistic part of speech, given the word's context.
Reference: [25] <author> F. Jelinek. </author> <title> "Self-Organized language modeling for speech recognition", Readings in Speech Recognition, </title> <editor> Alex Waibel and Kai-Fu Lee, eds., </editor> <month> 450-506, </month> <year> 1990. </year> <month> 139 </month>
Reference-contexts: If the model is perfect, i.e. ^ P (W ) = P (W ) for all word sequences W , then the sentences produced by this random sampling will be reasonable sentences. 24 2.8.1 Perplexity Perplexity <ref> [25, 37] </ref> is an information-theoretic measure for evaluating how well a language model predicts a particular test set. It is an excellent metric for comparing two language models because it is entirely independent of how each language model functions internally, and because it is mathematically very simple to compute. <p> The specific approaches differ only in where this probability mass is taken from and how much is taken. 28 One general approach to smoothing is referred to as interpolation <ref> [25] </ref>, where the estimated probability vectors of n-gram model are smoothed with the probability vectors of the (n 1)-gram model: = C (w 1 ; :::; w n1 ) + K s +(1 ) fi ^ P n1 (w i j w in+2 ; :::; w i1 ) where K s <p> In general, the smoothing parameter can be chosen as above by optimizing K s empirically, or through deleted interpolation techniques <ref> [25] </ref>. Typically the parameter K s is optimized so as to minimize perplexity on an independent test set. This smoothing process is recursive so that each level of the n-gram model is smoothing with the vectors from the (n 1)-gram model.
Reference: [26] <author> F. Jelinek, J. Lafferty and R. Mercer. </author> <title> "Basic methods of probabilistic context free grammars", Speech Recognition and Understanding: Recent Advances, Trends, and Applications, </title> <editor> P. Laface and R. De Mori, eds., </editor> <volume> Vol. 75, </volume> <year> 1992. </year>
Reference-contexts: But, the emphasis of these robust parsing techniques is to extract the correct meaning of these sentences in rather than to provide a reasonable estimate of the probability of the sentence. 2.11.1 SCFG One structural approach is the stochastic context free grammar (SCFG) <ref> [26] </ref>, which mirrors the context-freeness of the grammar by assigning independent probabilities to each rule. These probabilities are estimated by examining the parse trees of all of the training sentences. A sentence is then assigned a probability by multiplying the probabilities of the rules used in parsing the sentence. <p> SCFG's have been well studied by the theoretical community, and well-known algorithms have been developed to manipulate SCFG's, and to compute useful quantities for them <ref> [32, 26] </ref>. A SCFG may be converted into the corresponding n-gram model [43]. The inside/outside algorithm is used to optimize the probabilities such that the total probability of the training set is optimized [2, 29].
Reference: [27] <author> S. M. Katz. </author> <title> "Estimation of probabilities from sparse data for the language model component of a speech recognizer", </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> Vol. 35, </volume> <pages> 400-401, </pages> <year> 1987. </year>
Reference-contexts: The interpolation approach is very simple in that it uniformly transfers probability mass from all n-grams. Other approaches try to steal more probability mass from infrequent n-grams, for example back-off smoothing <ref> [27] </ref>, which only removes probability mass from those n-grams that occurred fewer than a certain cutoff number of times. There are various other approaches, such as nonlinear discounting [33], and Good/Turing [21, 13].
Reference: [28] <author> S. Kullback. </author> <title> Information Theory and Statistics. </title> <address> New York: </address> <publisher> John Wiley and Sons Inc., </publisher> <year> 1959. </year>
Reference-contexts: This smoothed distribution is computed in exactly the same way as the interpolated n-gram distribution as described in Section 2.9. There are various possible choices for "distance metrics" once we have these context probability vectors for all units. We chose as our distance metric divergence <ref> [28] </ref>, which is a natural information theoretic distance between probability vectors.
Reference: [29] <author> K. Lari and S. J. Young. </author> <title> "The estimation of stochastic context-free grammars using the inside-outside algorithm", </title> <booktitle> Computer Speech and Language, </booktitle> <volume> Vol. 4, </volume> <pages> 35-56, </pages> <year> 1990. </year>
Reference-contexts: Brill et al. [8] tested a metric based on mutual information to automatically parse sentences, starting from the parts of speech of the words. Brill [7] developed a system which learns a decision tree to bracket sentences. Pereira and Schabes [35] developed a modified version of the inside/outside algorithm <ref> [2, 29] </ref> to automatically parse test sentences, given a very small amount of training data. Bod [4] uses a monte-carlo based approach to bracket 11 sentences. <p> A SCFG may be converted into the corresponding n-gram model [43]. The inside/outside algorithm is used to optimize the probabilities such that the total probability of the training set is optimized <ref> [2, 29] </ref>. Given a sentence prefix, it is possible to predict the probabilities of all possible next words which may follow according to the SCFG. 33 2.11.2 TINA TINA [38] is a structured probabilistic language model developed explicitly for both understanding and ease of integration with a speech recognition system.
Reference: [30] <author> J. C. Martin. </author> <title> Introduction to Languages and the Theory of Computation. </title> <address> New York: </address> <publisher> McGraw-Hill Inc., </publisher> <year> 1991. </year>
Reference-contexts: While the nature of this inference process discourages acquiring an ambiguous grammar, there can be sequences of merges that do result in an ambiguous grammar. Fortunately, this seems empirically not to happen very often. Unfortunately, there is no algorithm to determine whether an arbitrary CFG is ambiguous <ref> [30] </ref> | this decision problem has been proven undecidable by the theoretical community.
Reference: [31] <author> E. Newport, H. Gleitman and E. Gleitman. </author> <title> "Mother, I'd rather do it myself: some effects and non-effects of maternal speech style", Talking to Children: Language Input and Acquisition, </title> <editor> C. E. Snow and C. A. Ferguson, eds., </editor> <address> New York: </address> <publisher> Cambridge University Press, </publisher> <year> 1977. </year>
Reference-contexts: The results from this community are of interest to the linguistic community because it is widely held that children are exposed to very little negative evidence as they learn natural languages <ref> [31, 34] </ref>. This constrains the linguistic theories of human language as they must be consistent with this evidence. For our purposes, we measure learning with far less stringent requirements. We measure the quality of the resulting language models and speech recognition systems which use these language models.
Reference: [32] <author> H. Ney. </author> <title> "Stochastic grammars and pattern recognition", Speech Recognition and Understanding: Recent Advances, Trends, and Applications, </title> <editor> P. Laface and R. De Mori, eds., </editor> <volume> Vol 75, </volume> <pages> 319-344, </pages> <year> 1992. </year>
Reference-contexts: SCFG's have been well studied by the theoretical community, and well-known algorithms have been developed to manipulate SCFG's, and to compute useful quantities for them <ref> [32, 26] </ref>. A SCFG may be converted into the corresponding n-gram model [43]. The inside/outside algorithm is used to optimize the probabilities such that the total probability of the training set is optimized [2, 29].
Reference: [33] <author> H. Ney and U. Essen. </author> <title> "On smoothing techniques for bigram-based natural language modeling", </title> <booktitle> Proc. International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> 825-828, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Other approaches try to steal more probability mass from infrequent n-grams, for example back-off smoothing [27], which only removes probability mass from those n-grams that occurred fewer than a certain cutoff number of times. There are various other approaches, such as nonlinear discounting <ref> [33] </ref>, and Good/Turing [21, 13]. When one uses an n-gram language model to generate random sentences, as shown in Tables 2.1, 2.2 and 2.3, very few of the resulting sentences could be considered valid. This indicates that n-gram models leave substantial room for improvement.
Reference: [34] <author> S. Penner. </author> <title> "Parental responses to grammatical and ungrammatical child utterances", Child Development, </title> <journal> Vol. </journal> <volume> 58, </volume> <pages> 376-384, </pages> <year> 1987. </year>
Reference-contexts: The results from this community are of interest to the linguistic community because it is widely held that children are exposed to very little negative evidence as they learn natural languages <ref> [31, 34] </ref>. This constrains the linguistic theories of human language as they must be consistent with this evidence. For our purposes, we measure learning with far less stringent requirements. We measure the quality of the resulting language models and speech recognition systems which use these language models.
Reference: [35] <author> F. Pereira and Y. Schabes. </author> <title> "Inside-outside reestimation from partially bracketed corpora", </title> <booktitle> Proc. DARPA Speech and Natural Language Workshop, </booktitle> <pages> 122-127, </pages> <year> 1992. </year>
Reference-contexts: Brill et al. [8] tested a metric based on mutual information to automatically parse sentences, starting from the parts of speech of the words. Brill [7] developed a system which learns a decision tree to bracket sentences. Pereira and Schabes <ref> [35] </ref> developed a modified version of the inside/outside algorithm [2, 29] to automatically parse test sentences, given a very small amount of training data. Bod [4] uses a monte-carlo based approach to bracket 11 sentences.
Reference: [36] <author> F. Pereira, N. Tishby and L. Lee. </author> <title> "Distributional clustering of English words", </title> <booktitle> Proc. 30th Annual Meeting of the Asoociation for Computational Linguistics, </booktitle> <pages> 128-135, </pages> <year> 1992. </year>
Reference: [37] <author> S. Roucos. </author> <title> "Measuring perplexity of language models used in speech recognizers", </title> <type> BBN Technical Report, </type> <year> 1987. </year>
Reference-contexts: If the model is perfect, i.e. ^ P (W ) = P (W ) for all word sequences W , then the sentences produced by this random sampling will be reasonable sentences. 24 2.8.1 Perplexity Perplexity <ref> [25, 37] </ref> is an information-theoretic measure for evaluating how well a language model predicts a particular test set. It is an excellent metric for comparing two language models because it is entirely independent of how each language model functions internally, and because it is mathematically very simple to compute.
Reference: [38] <author> S. Seneff. "TINA: </author> <title> A natural language system for spoken language applications", </title> <journal> Computational Linguistics, </journal> <volume> Vol. 18, No. 1, </volume> <pages> 61-86, </pages> <year> 1992. </year>
Reference-contexts: The inside/outside algorithm is used to optimize the probabilities such that the total probability of the training set is optimized [2, 29]. Given a sentence prefix, it is possible to predict the probabilities of all possible next words which may follow according to the SCFG. 33 2.11.2 TINA TINA <ref> [38] </ref> is a structured probabilistic language model developed explicitly for both understanding and ease of integration with a speech recognition system. From a hand-written input CFG, TINA constructs a probabilistic recursive transition network which records probabilities on the arcs of the network.
Reference: [39] <author> S. Seneff, H. Meng and V. Zue. </author> <title> "Language modelling for recognition and understanding using layered bigrams", </title> <booktitle> Proc. International Conference on Spoken Language Processing, </booktitle> <pages> 317-320, </pages> <month> October </month> <year> 1992. </year> <month> 140 </month>
Reference-contexts: TINA, like SCFG, only assigns probability to those sentences which it accept. It therefore has the same difficulties with robustness as other structural approaches. Recent work has been done to address this problem, mainly by resorting to standard bigram models for words that fall outside of parsable sequences <ref> [39] </ref>. 2.11.3 PLR PLR [19] is another approach to which generates a probabilistic language model from a CFG as input.
Reference: [40] <author> C. E. Shannon. </author> <title> "Prediction and entropy of printed English", </title> <journal> Bell Systems Tech--nical Journal, </journal> <volume> Vol. 30, </volume> <pages> 50-64, </pages> <month> January </month> <year> 1951. </year>
Reference: [41] <author> S. M. Shieber. </author> <title> "Evidence against the context-freeness of natural language", </title> <journal> Linguistics and Philosophy, </journal> <volume> Vol. 8, </volume> <pages> 333-343, </pages> <year> 1985. </year>
Reference: [42] <author> R. Solomonoff. </author> <title> "A Formal Theory of Inductive Inference", </title> <journal> Information and Control, </journal> <volume> Vol. 7, </volume> <pages> 1-22, 234-254, </pages> <year> 1964. </year>
Reference-contexts: The learner constructs the hypothesis that it believes is the correct one from the examples, and the performance of the learner is measured objectively based on how well it classifies future examples. One such formalism of learning, proposed by Solomonof <ref> [42] </ref>, is a strict sense of learning called learning in the limit. This formalism requires that the learner 37 This shows the function of each component. The merging unit actually transforms the grammar by selecting two similar units to merge.
Reference: [43] <author> A Stolcke and J. Segal. </author> <title> "Precise n-gram probabilities from stochastic context-free grammars", </title> <type> ICSI Technical Report, </type> <month> January </month> <year> 1994. </year>
Reference-contexts: SCFG's have been well studied by the theoretical community, and well-known algorithms have been developed to manipulate SCFG's, and to compute useful quantities for them [32, 26]. A SCFG may be converted into the corresponding n-gram model <ref> [43] </ref>. The inside/outside algorithm is used to optimize the probabilities such that the total probability of the training set is optimized [2, 29].
Reference: [44] <author> L. G. Valiant. </author> <title> "A theory of the learnable", </title> <journal> Communications of the ACM, </journal> <volume> Vol. 27, </volume> <pages> 1134-1142, </pages> <year> 1984. </year>
Reference-contexts: The learning formalism established by Solomonof is very strict in that it demands precise learning of the concept. A more relaxed and practical formalism was proposed recently by Valiant <ref> [44] </ref>, called PAC-learning, which stands for "probably approximately correct learning". Instead of requiring the learner to classify all future examples, it simply imposes a bound on the probability of misclassification. Thus, the learner is allowed to make mistakes, as long as it does not make too many.
Reference: [45] <author> A. </author> <title> Viterbi. "Error bounds for convolutional codes and an asymptotic optimal decoding algorithm", </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. 13, </volume> <pages> 260-269, </pages> <month> April </month> <year> 1967. </year>
Reference-contexts: There is a one-to-one correspondence between paths through this large graph and alignments of sentences with the acoustic evidence. Thus, we have reduced the problem of finding the best sentence and alignment to one of searching a large graph for the best path. 5.5 Viterbi Search The Viterbi search <ref> [45] </ref> is an efficient search algorithm, based on dynamic programming, which finds the best path through G s . It is a time synchronous search, which 102 means it makes a single pass through G s one boundary at a time.
Reference: [46] <author> V. Zue, J. Glass, D. Goodine, M. Phillips and S. Seneff. </author> <title> "The SUMMIT speech recognition system: phonological modelling and lexical access", </title> <booktitle> Proc. International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> 49-52, </pages> <year> 1990. </year>
Reference-contexts: The graph for each word is hand-written at first to create a base-form pronunciation. It then undergoes some automatic transformations based on phonetic pronunciation rules which attempt to model word contextual pronunciation effects <ref> [46] </ref>. The score on each arc is trained automatically using a corrective training process. The lexicon graph incorporates the word bigram model, which allows the individual word graphs to be interconnected with arcs labelled with the bigram score for the two connected words.
Reference: [47] <author> V. Zue, J. Glass, M. Phillips and S. Seneff. </author> <title> "Acoustic segmentation and phonetic classification in the summit speech recognition system", </title> <booktitle> Proc. International Conference Acoustics, Speech, and Signal Processing, </booktitle> <pages> 389-392, </pages> <year> 1989. </year> <month> 141 </month>
Reference-contexts: We decided to evaluate the acquired pcng language models by integrating them with the summit speech recognition system developed in the Spoken Language Systems group in the Laboratory for Computer Science at MIT <ref> [47] </ref>. Summit is a segment based speech recognition system which typically employs class based n-gram models as the language model. [18]. We tested the performance of the system using acquired pcng models and found them to improve recognition performance when compared to the word trigram model. <p> These 99 two issues, computational cost versus word accuracy, need to be examined closely in deciding which language model to integrate into a recognition system. 5.2 Summit The speech recognition system used to evaluate this system was the atis version of summit <ref> [47] </ref>, which has been developed over the past six years in the Spoken Language Systems group at MIT. Summit is segment based, which means the incoming speech signal is explicitly segmented into likely phonetic segments before being classified into phonetic units. Summit can be divided into two phases.
References-found: 47

