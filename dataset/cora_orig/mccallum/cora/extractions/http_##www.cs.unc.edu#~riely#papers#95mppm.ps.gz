URL: http://www.cs.unc.edu/~riely/papers/95mppm.ps.gz
Refering-URL: http://www.cs.unc.edu/~riely/papers.html
Root-URL: http://www.cs.unc.edu
Email: friely,prinsg@cs.unc.edu  purush@csc.ncsu.edu  
Phone: 2  
Title: From Programming Models for Massively Parallel Computers (MMPM'95),  Provably Correct Vectorization of Nested-Parallel Programs  
Author: James Wheelis Riely, Jan Prins and S. Purushothoman Iyer 
Address: Chapel Hill, NC 27599-3175 USA  Raleigh, NC 27695-8206 USA  
Affiliation: 1 Univ. of North Carolina  North Carolina State Univ.  
Note: IEEE Computer Society Press, 1996.  
Abstract: The work/step framework provides a high-level cost model for nested data-parallel programming languages, allowing programmers to understand the efficiency of their codes without concern for the eventual mapping of tasks to processors. Vec-torization, or flattening, is the key technique for compiling nested-parallel languages. This paper presents a formal study of vectorization, considering three low-level targets: the erew, bounded-contention crew, and crew variants of the vram. For each, we describe a variant of the cost model and prove the correctness of vectorization for that model. The models impose different constraints on the set of programs and implementations that can be considered; we discuss these in detail. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Arun-Kumar and Matthew Hennessy. </author> <title> An efficiency preorder for processes. </title> <journal> Acta Informatica, </journal> <volume> 29 </volume> <pages> 737-760, </pages> <year> 1992. </year>
Reference-contexts: Intuitively, Q is ae-greater than P if they compute the same things and Q is more efficient than P , with allowances made for constant overhead and slowdown. It is these "allowances" that make our preorder an asymptotic efficiency preorder. Simple efficiency preorders <ref> [1] </ref> do not make such allowances, but rather compare the absolute running times of programs. In addition to being more efficient, we also allow that an ae-greater program be more defined.
Reference: [2] <author> Guy E. Blelloch. </author> <title> Vector Models for Data-Parallel Computing. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: 1 Introduction Many complexity models (or cost models) have been proposed for parallel programs. High-level models such as Blelloch's step/work metrics for nesl <ref> [3, 2] </ref> and Skil-licorn's calculus for bmf [13] are based on a rich, highly-parallel expression language with compositional cost met-rics: the complexity of an expression can be understood by combining the complexities of its subexpressions. <p> Low-level models such as the pram [8], vram <ref> [2] </ref> and bsp model [14] are statement-based with very limited compo-sitionality. Cost models are important for programming because they help guide the construction of efficient code. High-level models allow each code-fragment to be considered separately, so that optimizing any fragment in isolation should improve overall performance. <p> An inaccurate cost model may be worse than no model at all; it may encourage "optimizations" that diminish the performance on an actual parallel machine. In this paper, we prove that the high-level step/work metric accurately reflects the implementation of a nested-parallel language on a vram. Blelloch <ref> [2] </ref> proved a similar result for a limited class of expressions with no free variables, but until now his work has stood in isolation. Our proofs are both more formal and more general than his, although we also must restrict our attention to contained programs (see Section 4). <p> While we would like to formalize this second step, we believe that it has already been well established by implementation, experimentation, and some less-formal proofs <ref> [4, 2, 10] </ref>. Because of space limitations, we have cut quite a bit from this extended abstract. We assume that the reader has a familiarity with the basic notions of operational semantics [15]. <p> But it is a problem for the step complexity, where the combining function does change: from max to sum. Blel-loch originally noticed this problem and defined a class of programs for which this transformation was sound: contained programs <ref> [2] </ref>. Intuitively, contained programs are those that do not have recursive calls on both sides of a conditional. Definition 2.
Reference: [3] <author> Guy E. Blelloch. NESL: </author> <title> A nested data-parallel language (version 3.0). </title> <type> Technical report, </type> <institution> Carnegie-Mellon University, Department of Computer Science, </institution> <year> 1994. </year>
Reference-contexts: 1 Introduction Many complexity models (or cost models) have been proposed for parallel programs. High-level models such as Blelloch's step/work metrics for nesl <ref> [3, 2] </ref> and Skil-licorn's calculus for bmf [13] are based on a rich, highly-parallel expression language with compositional cost met-rics: the complexity of an expression can be understood by combining the complexities of its subexpressions. <p> This allows for implementation on an erew vram but makes the cost model sensitive to the presence of free variables, leading to counterintuitive complexity results and complicating programming. This is the approach taken in a recent implementation of nesl <ref> [3] </ref>. * A construct-results semantics which strikes a middle ground. It removes sensitivity to free-variables from the cost model while allowing implementation on a low-contention crew vram. There is some loss of generality, however, which can be partially recouped using static analysis.
Reference: [4] <author> Guy E. Blelloch, Siddhartha Chatterjee, Jonathan C. Hardwick, Margaret Reid-Miller, Jay Sipelstein, and Marco Zagha. CVL: </author> <title> A C vector library. </title> <type> Technical Report CMU-CS-93-114, </type> <institution> Carnegie-Mellon University, Department of Computer Science, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: While we would like to formalize this second step, we believe that it has already been well established by implementation, experimentation, and some less-formal proofs <ref> [4, 2, 10] </ref>. Because of space limitations, we have cut quite a bit from this extended abstract. We assume that the reader has a familiarity with the basic notions of operational semantics [15]. <p> that a primitive p is non-magical iff: S ffi E P p (A 1 ; ...; A ` ) D ffi E p (A 1 ; ...; A ` ) The primitives that we use are implemented by the Data-Parallel Library (dpl) [9], an extension of the C Vector Library <ref> [4] </ref>. The implementation is non-magical and meets the other constraints given above. Theorem 4. Assume the construct-parameters semantics and a primitive specification that meets the con straints outlined in this section.
Reference: [5] <author> Guy E. Blelloch, Phillip B. Gibbons, Yossi Matias, and Marco Zagha. </author> <title> Accounting for memory bank conetention and delay in high-bandwidth multiprocessors. </title> <booktitle> In Proceedings of the ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 84-94, </pages> <address> Santa Barbara, CA, July 1995. </address> <publisher> ACM Press. </publisher>
Reference-contexts: The step/work metrics also work well for crew and crcw models as long as memory contention is bounded by a small constant. In general, however, step and work complexity fail to predict eventual performance when a vram supports crew operations with unbounded contention. For a discussion, see <ref> [5] </ref>. In Section 3 we define a profiling semantics for a simple nested-parallel programming language, giving an explicit account of the step/work paradigm for this language.
Reference: [6] <author> Guy E. Blelloch and John Greiner. </author> <title> Parallelism in sequential functional languages. </title> <booktitle> In Proceedings of the Conference on Functional Programming Languages and Computer Architecture, </booktitle> <pages> pages 226-237, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Because of space limitations, we have cut quite a bit from this extended abstract. We assume that the reader has a familiarity with the basic notions of operational semantics [15]. For pointers to the literature on high-level cost models, see the excellent summaries in articles by Blelloch and Greiner <ref> [6] </ref> and Skillicorn and Cai [13]; these two articles are closely related to ours. 2 The language Throughout the paper we use many metavariables, most of which are listed in Table 1.
Reference: [7] <author> Allen Goldberg, Peter Mills, Lars Nyland, Jan Prins, John Reif, and James Riely. </author> <title> Specification and development of parallel algorithms with the Proteus system. In G.E. Blelloch, </title> <editor> K.M. Chandy, and S.Jagannathan, editors, </editor> <booktitle> Specification of Parallel Algorithms, volume 18 of DIMACS Series in Discrete Mathematics and Theoretical Computer Science. </booktitle> <publisher> AMS Press, </publisher> <month> May </month> <year> 1994. </year>
Reference-contexts: Our proofs are both more formal and more general than his, although we also must restrict our attention to contained programs (see Section 4). Our work has been motivated by|and is complementary to|ongoing implementation and performance studies of the Proteus programming language <ref> [7, 10] </ref>. Nested-parallel languages such as nesl and Proteus are characterized by a nested sequence datatype along with a set of second-order functions to manipulate them. The main source of parallelism is the apply-to-each function; both nesl and Proteus have a special form for this, the iterator.
Reference: [8] <author> Joseph JaJa. </author> <title> An Introduction to Parallel Algorithms. </title> <publisher> Addison-Wesley, </publisher> <year> 1992. </year> <month> 10 </month>
Reference-contexts: Low-level models such as the pram <ref> [8] </ref>, vram [2] and bsp model [14] are statement-based with very limited compo-sitionality. Cost models are important for programming because they help guide the construction of efficient code. High-level models allow each code-fragment to be considered separately, so that optimizing any fragment in isolation should improve overall performance.
Reference: [9] <author> Daniel W. Palmer. DPL: </author> <title> Data Parallel Library manual. </title> <type> Technical Report 93:064, </type> <institution> University of North Carolina, Department of Computer Science, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: We say that a primitive p is non-magical iff: S ffi E P p (A 1 ; ...; A ` ) D ffi E p (A 1 ; ...; A ` ) The primitives that we use are implemented by the Data-Parallel Library (dpl) <ref> [9] </ref>, an extension of the C Vector Library [4]. The implementation is non-magical and meets the other constraints given above. Theorem 4. Assume the construct-parameters semantics and a primitive specification that meets the con straints outlined in this section.
Reference: [10] <author> Daniel W. Palmer, Jan F. Prins, and Stephen Westfold. </author> <title> Work-efficient nested data-parallelism. </title> <booktitle> In Frontiers '95, </booktitle> <year> 1995. </year>
Reference-contexts: Our proofs are both more formal and more general than his, although we also must restrict our attention to contained programs (see Section 4). Our work has been motivated by|and is complementary to|ongoing implementation and performance studies of the Proteus programming language <ref> [7, 10] </ref>. Nested-parallel languages such as nesl and Proteus are characterized by a nested sequence datatype along with a set of second-order functions to manipulate them. The main source of parallelism is the apply-to-each function; both nesl and Proteus have a special form for this, the iterator. <p> It removes sensitivity to free-variables from the cost model while allowing implementation on a low-contention crew vram. There is some loss of generality, however, which can be partially recouped using static analysis. This is the approach taken in the current implementation of Proteus <ref> [10] </ref>. <p> While we would like to formalize this second step, we believe that it has already been well established by implementation, experimentation, and some less-formal proofs <ref> [4, 2, 10] </ref>. Because of space limitations, we have cut quite a bit from this extended abstract. We assume that the reader has a familiarity with the basic notions of operational semantics [15]. <p> The running-time of a primitive depends upon its implementation; we discuss implementations and an intermediate form of primitive specification in Section 3.3. Further motivation for the use of these primitives can be found in <ref> [12, 10, 2, ] </ref>, and formal definitions in the full paper. * build ` builds a `-length sequence out of its ` arguments. <p> Palmer et al . <ref> [10] </ref> propose a solution to this problem which avoids "pushing the iterator" around a.
Reference: [11] <author> David A. Plaisted. </author> <title> Term-rewriting systems. </title> <editor> In Dov M. Gabbay, Christopher John Hogger, and J.A. Robinson, editors, </editor> <booktitle> Handbook of Logic in Artificial Intelligence and Logic Programming, volume 2 Deduction Methodologies. </booktitle> <publisher> Oxford University Press, </publisher> <year> 1993. </year>
Reference-contexts: Thus the main implementation problem is iterator removal. In Table 3, we present a simple set of conditional rewrite rules, or program transformations, which can be used to eliminate iterators from a program. These transformations can be applied to any subterm of a program, forming a rewrite system <ref> [11] </ref>. The transformations are terminating but non-confluent (const conflicts with the other rules.) The main theorems of this paper state|for each semantics|the class of programs for which this rewrite system preserves the ae-preorder.
Reference: [12] <author> Jan F. Prins and Daniel W. Palmer. </author> <title> Transforming high-level data-parallel programs into vector operations. </title> <booktitle> In Proceedings of the Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 119-128, </pages> <address> San Diego, </address> <month> May </month> <year> 1993. </year> <journal> (ACM SIGPLAN Notices, </journal> <volume> 28(7), </volume> <month> July, </month> <year> 1993). </year>
Reference-contexts: The running-time of a primitive depends upon its implementation; we discuss implementations and an intermediate form of primitive specification in Section 3.3. Further motivation for the use of these primitives can be found in <ref> [12, 10, 2, ] </ref>, and formal definitions in the full paper. * build ` builds a `-length sequence out of its ` arguments.
Reference: [13] <author> D. B. Skillicorn and W. Cai. </author> <title> A cost calculus for parallel functional programming, 1994. </title> <institution> Queens University Department of Computer Science TR-93-348. </institution>
Reference-contexts: 1 Introduction Many complexity models (or cost models) have been proposed for parallel programs. High-level models such as Blelloch's step/work metrics for nesl [3, 2] and Skil-licorn's calculus for bmf <ref> [13] </ref> are based on a rich, highly-parallel expression language with compositional cost met-rics: the complexity of an expression can be understood by combining the complexities of its subexpressions. Low-level models such as the pram [8], vram [2] and bsp model [14] are statement-based with very limited compo-sitionality. <p> We assume that the reader has a familiarity with the basic notions of operational semantics [15]. For pointers to the literature on high-level cost models, see the excellent summaries in articles by Blelloch and Greiner [6] and Skillicorn and Cai <ref> [13] </ref>; these two articles are closely related to ours. 2 The language Throughout the paper we use many metavariables, most of which are listed in Table 1.
Reference: [14] <author> Leslie G. Valiant. </author> <title> A bridging model for parallel computation. </title> <journal> Communications of the ACM, </journal> <volume> 33(8) </volume> <pages> 103-111, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: Low-level models such as the pram [8], vram [2] and bsp model <ref> [14] </ref> are statement-based with very limited compo-sitionality. Cost models are important for programming because they help guide the construction of efficient code. High-level models allow each code-fragment to be considered separately, so that optimizing any fragment in isolation should improve overall performance.
Reference: [15] <author> Glynn Winskel. </author> <title> The Formal Semantics of Programming Languages: An Introduction. </title> <publisher> MIT Press, </publisher> <year> 1993. </year> <month> 11 </month>
Reference-contexts: Because of space limitations, we have cut quite a bit from this extended abstract. We assume that the reader has a familiarity with the basic notions of operational semantics <ref> [15] </ref>. <p> Table 2: Ideal dynamic semantics. 5 We give a big-step or "natural" operational semantics <ref> [15] </ref> in Table 2.
References-found: 15

