URL: http://www.csc.ncsu.edu/faculty/mpsingh/papers/mas/jelia-90.ps
Refering-URL: http://www.csc.ncsu.edu/faculty/mpsingh/papers/mas/
Root-URL: http://www.csc.ncsu.edu
Email: msingh@mcc.com msingh@cs.utexas.edu  @relay.cs.net  
Title: TOWARDS A FORMAL THEORY OF INTENTIONS  
Author: Munindar P. Singh and Nicholas M. Asher 
Web: ims%rus.uni-stuttgart.dbp.de  
Address: Austin, TX 78759 USA  Keplerstrasse 17 D-7000 Stuttgart-1 FRG  
Affiliation: Artificial Intelligence Laboratory MCC  Universitat Stuttgart  
Date: 1990  
Note: In Proceedings of the European Workshop on Logics in Artificial Intelligence,  
Abstract: Intentions are an important concept in several subfields of Artificial Intelligence. We present a formal theory of intentions and beliefs based on Discourse Representation Theory that captures many of the important logical aspects of the functional roles of intentions and beliefs, and of relationships among intentions, and between intentions and beliefs. Unlike possible worlds approaches, this theory does not assume that agents are perfect reasoners, and gives a realistic view of their internal architecture; unlike most representational approaches, it has an objective semantics, and does not rely on an ad hoc labeling of the internal states of agents. We then describe a minimal logic for intentions and beliefs. We close with several additional inferences, and the constraints on the model that correspond to them. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Philip Agre and David Chapman. Pengi: </author> <title> An implementation of a theory of activity. </title> <booktitle> In AAAI, </booktitle> <pages> pages 268-272, </pages> <year> 1987. </year>
Reference-contexts: Note that I and B (and honesty, in general) do not apply to the semantics 3 We can thus use conceptual individuals to model the "indexical-functional" aspects of the environment <ref> [1] </ref>. This approach allows us to also make sense of higher level notions like belief and intention. 8 of all conditions, but only to the semantics of attitude reports. For frogs, these relations would be almost empty; for perfect reasoners, they would allow all valid deductions.
Reference: [2] <author> James Allen and C. Raymond Perrault. </author> <title> Participating in dialogues: Understanding via plan deduction. </title> <booktitle> In Proceedings of CSCSI, </booktitle> <year> 1978. </year>
Reference-contexts: 1 Introduction An understanding of intentions is important to several subfields of Artificial Intelligence (AI), especially, speech act theory [3, 4, 10, 11], discourse processing [17], planning [16], and plan recognition <ref> [2, 23, 27] </ref>. We present a formal theory of intentions and beliefs that is based on Discourse Representation Theory (DRT) [5, 6, 18]. Our theory involves a formal model of time and possibility, and also explicitly models the structure of the agents' internal states.
Reference: [3] <author> James F. Allen and C. Raymond Perrault. </author> <title> Analyzing intention in utterances. </title> <journal> Artificial Intelligence, </journal> <volume> 15 </volume> <pages> 143-178, </pages> <year> 1980. </year>
Reference-contexts: 1 Introduction An understanding of intentions is important to several subfields of Artificial Intelligence (AI), especially, speech act theory <ref> [3, 4, 10, 11] </ref>, discourse processing [17], planning [16], and plan recognition [2, 23, 27]. We present a formal theory of intentions and beliefs that is based on Discourse Representation Theory (DRT) [5, 6, 18].
Reference: [4] <author> Douglas Appelt. </author> <title> Planning English Sentences. </title> <publisher> Cambridge University Press, </publisher> <address> Cam-bridge, UK, </address> <year> 1986. </year>
Reference-contexts: 1 Introduction An understanding of intentions is important to several subfields of Artificial Intelligence (AI), especially, speech act theory <ref> [3, 4, 10, 11] </ref>, discourse processing [17], planning [16], and plan recognition [2, 23, 27]. We present a formal theory of intentions and beliefs that is based on Discourse Representation Theory (DRT) [5, 6, 18]. <p> Their importance to cognition and their use in AI has been defended extensively in the literature <ref> [4, 8, 10, 11, 16, 17, 29] </ref>. The roles of intentions mentioned below are especially important when one is interested in agents who would not otherwise (because of their limitations) be able to make appropriate or rational decisions. <p> We sketch just enough of DRT in this paper so that the presentation here is self-contained. A study of the (mostly AI) literature <ref> [4, 8, 10, 16, 17, 24, 26, 27, 29] </ref> yields the following important properties of intentions. Intentions are about future events. An agent with an intention should believe that it can be realized, at least along some future. <p> to determine his own plans, and to reason about their consequences [16, 24], (2) and to do the same for the plans of others [27], and (3) a principled approach to the design of multiagent intelligent systems [28]. (1) and (2) directly relate to speech act planning and discourse understanding <ref> [4, 10, 17, 26] </ref> as well. Such a logic would also be useful in multiagent systems where agents must reason about each other's intentions to negotiate among themselves. We now turn to a deductive system for our semantics.
Reference: [5] <author> Nicholas Asher. </author> <title> Notes on formalizing salience with reference to theories of centering and discourse structure. </title> <type> Manuscript, </type> <institution> Center for Cognitive Science, University of Texas at Austin, </institution> <month> November </month> <year> 1986. </year> <month> 14 </month>
Reference-contexts: We present a formal theory of intentions and beliefs that is based on Discourse Representation Theory (DRT) <ref> [5, 6, 18] </ref>. Our theory involves a formal model of time and possibility, and also explicitly models the structure of the agents' internal states. <p> Such a unified theory has been evolving in the DRT framework over the last few years. Kamp's thesis of the Unity of Thought and Information [19], and Asher's work on the attitudes <ref> [5] </ref>, on their relationship with information [7], and on the anaphoric properties of verbs [6] must be cited in this context. DRT is a useful framework for the general project for several reasons. <p> internal structure of the agent entirely; the choices made by an agent, and his (most especially, verbal) behavior does not just depend on the anchoring of his internal state in the world, but on its structure as well: Kripke's `London' versus `Londres' example [21], discussed in a DRT framework in <ref> [5, pp. 142-143] </ref> and [19, pp. 253-254], is a case in point. <p> In x3, we motivate the formal language and model. In x4, we present a minimal logic for intentions and beliefs. In x5, we list some important extensions to the basic logic. 5 3 Formal Language and Formal Semantics Our sentences (DRS's) <ref> [5] </ref> are members of the language, DRS, generated by the following semi-formal grammar. The temporal part of the grammar is inspired by CTL* [13] (with the addition of the "sometimes in the past" operator P). <p> A DRS, K is valid at M and w iff it is satisfiable at all times in M and w. Validity in a model and validity simpliciter may be defined analogously. 3.1 Satisfaction conditions The satisfaction conditions for :, _, !, and predicates are standard (e.g., see <ref> [5, 6] </ref>); the others are defined below. * M j= w;t;f EK iff (9g : g w w f w : (9S : S 2 S w;t M j= S;g K)) E stands for "in some scenario"; i.e., K is true in some future of t in world w. * M <p> While ultimately the latter would be the right alternative, we prefer the former for reasons of space and ease of exposition. The language we use is the usual first order temporal logic language augmented by predicates for belief and intention <ref> [5] </ref>. The axiomatization is then quite straightforward. 1. (WA): Weak Anticipation. (x Intends p)! (x Believes EFp) Since p occurs on all the scenarios in the content of an intention for p, those scenarios are automatically in the content of the consequent belief.
Reference: [6] <author> Nicholas Asher. </author> <title> A typology for attitude verbs and their anaphoric properties. </title> <journal> Linguistics and Philosophy, </journal> <volume> 10 </volume> <pages> 125-197, </pages> <year> 1987. </year>
Reference-contexts: We present a formal theory of intentions and beliefs that is based on Discourse Representation Theory (DRT) <ref> [5, 6, 18] </ref>. Our theory involves a formal model of time and possibility, and also explicitly models the structure of the agents' internal states. <p> Such a unified theory has been evolving in the DRT framework over the last few years. Kamp's thesis of the Unity of Thought and Information [19], and Asher's work on the attitudes [5], on their relationship with information [7], and on the anaphoric properties of verbs <ref> [6] </ref> must be cited in this context. DRT is a useful framework for the general project for several reasons. Firstly, DRT is a theory of discourse meaning that captures many aspects of the information typically encoded in natural language utterances. <p> A DRS, K is valid at M and w iff it is satisfiable at all times in M and w. Validity in a model and validity simpliciter may be defined analogously. 3.1 Satisfaction conditions The satisfaction conditions for :, _, !, and predicates are standard (e.g., see <ref> [5, 6] </ref>); the others are defined below. * M j= w;t;f EK iff (9g : g w w f w : (9S : S 2 S w;t M j= S;g K)) E stands for "in some scenario"; i.e., K is true in some future of t in world w. * M
Reference: [7] <author> Nicholas Asher. </author> <title> Information, interpretation, and attitudes. </title> <editor> In P. Hanson, editor, </editor> <booktitle> British Columbia Studies in Cognitive Science, </booktitle> <volume> volume 1. </volume> <publisher> University of British Columbia Press, </publisher> <address> Vancouver, Canada, </address> <year> 1990. </year>
Reference-contexts: Such a unified theory has been evolving in the DRT framework over the last few years. Kamp's thesis of the Unity of Thought and Information [19], and Asher's work on the attitudes [5], on their relationship with information <ref> [7] </ref>, and on the anaphoric properties of verbs [6] must be cited in this context. DRT is a useful framework for the general project for several reasons. Firstly, DRT is a theory of discourse meaning that captures many aspects of the information typically encoded in natural language utterances.
Reference: [8] <author> Michael E. Bratman. </author> <title> Intention, Plans, and Practical Reason. </title> <publisher> Harvard University Press, </publisher> <address> Cambridge, MA, </address> <year> 1987. </year>
Reference-contexts: Their importance to cognition and their use in AI has been defended extensively in the literature <ref> [4, 8, 10, 11, 16, 17, 29] </ref>. The roles of intentions mentioned below are especially important when one is interested in agents who would not otherwise (because of their limitations) be able to make appropriate or rational decisions. <p> We sketch just enough of DRT in this paper so that the presentation here is self-contained. A study of the (mostly AI) literature <ref> [4, 8, 10, 16, 17, 24, 26, 27, 29] </ref> yields the following important properties of intentions. Intentions are about future events. An agent with an intention should believe that it can be realized, at least along some future. <p> Intentions are more complex than beliefs since they are future directed; also, an agent with an intention is "constrained" by it to the futures where it is fulfilled <ref> [8] </ref>. The contents of several attitudes may be combined. <p> this definition, the following properties of intentions are accounted for: (1) an agent with an intention tacitly considers it possible that his intention will be fulfilled (this is motivated in [24]), and (2) is tacitly restricted by his intention to scenarios in which it is achieved (this is motivated in <ref> [8] </ref>). These are two of the most important properties of intentions. Now, the content of any set of attitudes is the intersection of their respective contents.
Reference: [9] <author> Brian F. Chellas. </author> <title> Modal Logic. </title> <publisher> Cambridge University Press, </publisher> <address> New York, NY, </address> <year> 1980. </year>
Reference-contexts: Schema (WA) above is validated by the definition of Content and the special clause in the definition of . As a result, it is clear that the axiomatization is sound. Completeness too is simple. The proof sketched below follows the canonical model technique of Chellas <ref> [9, pp. 60, 173] </ref>. Let the above logic be called . Construct a canonical model M = hW; T; &lt;; A; C; [[]]i for as follows: 1. Let T be the set of maximally consistent sets of DRS's (i.e., the ^'s are mapped into sets of sub-DRS's). 2.
Reference: [10] <author> Philip R. Cohen and Hector J. Levesque. </author> <title> Rational interaction as the basis for communication. </title> <type> Technical Report 433, </type> <institution> SRI International, </institution> <address> Menlo Park, CA, </address> <month> April </month> <year> 1988. </year>
Reference-contexts: 1 Introduction An understanding of intentions is important to several subfields of Artificial Intelligence (AI), especially, speech act theory <ref> [3, 4, 10, 11] </ref>, discourse processing [17], planning [16], and plan recognition [2, 23, 27]. We present a formal theory of intentions and beliefs that is based on Discourse Representation Theory (DRT) [5, 6, 18]. <p> Their importance to cognition and their use in AI has been defended extensively in the literature <ref> [4, 8, 10, 11, 16, 17, 29] </ref>. The roles of intentions mentioned below are especially important when one is interested in agents who would not otherwise (because of their limitations) be able to make appropriate or rational decisions. <p> We sketch just enough of DRT in this paper so that the presentation here is self-contained. A study of the (mostly AI) literature <ref> [4, 8, 10, 16, 17, 24, 26, 27, 29] </ref> yields the following important properties of intentions. Intentions are about future events. An agent with an intention should believe that it can be realized, at least along some future. <p> These properties, while not valid in the minimal logic, are expressible in extensions of our theory (see x5). 2 More Intuitions Our theory, like the so-called sentential theories [20], and unlike most possible-worlds based theories <ref> [10] </ref>, avoids attributing logical omniscience to agents, since it does not require that agents' intentions or beliefs be closed under logical equivalence (thus it also avoids validating closure under logical consequence). At the same time, this theory has advantages over the sentential theories as well. <p> to determine his own plans, and to reason about their consequences [16, 24], (2) and to do the same for the plans of others [27], and (3) a principled approach to the design of multiagent intelligent systems [28]. (1) and (2) directly relate to speech act planning and discourse understanding <ref> [4, 10, 17, 26] </ref> as well. Such a logic would also be useful in multiagent systems where agents must reason about each other's intentions to negotiate among themselves. We now turn to a deductive system for our semantics.
Reference: [11] <author> Philip R. Cohen and C. Raymond Perrault. </author> <title> Elements of a plan-based theory of speech acts. </title> <journal> Cognitive Science, </journal> <volume> 3 </volume> <pages> 117-212, </pages> <year> 1979. </year>
Reference-contexts: 1 Introduction An understanding of intentions is important to several subfields of Artificial Intelligence (AI), especially, speech act theory <ref> [3, 4, 10, 11] </ref>, discourse processing [17], planning [16], and plan recognition [2, 23, 27]. We present a formal theory of intentions and beliefs that is based on Discourse Representation Theory (DRT) [5, 6, 18]. <p> Their importance to cognition and their use in AI has been defended extensively in the literature <ref> [4, 8, 10, 11, 16, 17, 29] </ref>. The roles of intentions mentioned below are especially important when one is interested in agents who would not otherwise (because of their limitations) be able to make appropriate or rational decisions.
Reference: [12] <author> Daniel C. Dennett. </author> <title> The Intentional Stance. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1987. </year>
Reference-contexts: This, of course, depends on what one might want to do with such a theory. A theory of intentions is needed at the foundational level of study in AI and Cognitive Science in order to complete an account of intelligent (and possibly, rational) agency <ref> [12, 33] </ref>. Intentions are important fl We are indebted to Allen Emerson and Rob Koons for several discussions and for comments on previous versions of this paper. 2 attitudes of intelligent agents and, for resource-bounded agents, cannot be reduced to simple considerations of the optimality of decisions.
Reference: [13] <author> E. A. Emerson. </author> <title> Temporal and modal logic. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science, volume B. </booktitle> <publisher> North-Holland Publishing Company, </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1990. </year>
Reference-contexts: In x5, we list some important extensions to the basic logic. 5 3 Formal Language and Formal Semantics Our sentences (DRS's) [5] are members of the language, DRS, generated by the following semi-formal grammar. The temporal part of the grammar is inspired by CTL* <ref> [13] </ref> (with the addition of the "sometimes in the past" operator P). <p> This relativizes our axiomatization to that of the underlying logic. In practice, this is the most one can do: no axiomatization of CTL* is known <ref> [13] </ref>. The above axiomatization is quite simple. The associativity and commutativity inferences in belief and intention contexts arise since DRS's may consist of sets of sub-DRS's. The simplification inferences arise due to the embedding conditions for DRS's, and the way in which Content is defined.
Reference: [14] <author> Ronald Fagin and Joseph Y. Halpern. </author> <title> Belief, awareness, and limited reasoning. </title> <journal> Artificial Intelligence, </journal> <volume> 34 </volume> <pages> 39-76, </pages> <year> 1988. </year>
Reference-contexts: Thus our approach also avoids the charges of ad hoc-ism often levied against the sentential approaches (e.g., by Levesque [22], and others <ref> [14] </ref>). The main advantage of our approach is that it allows us to model the internal architecture of intelligent agents far more realistically than in the other formal approaches.
Reference: [15] <author> Ronald Fagin, Joseph Y. Halpern, and Moshe Y. Vardi. </author> <title> A nonstandard approach to the logical omniscience problem. </title> <booktitle> In Proceedings of the Third Conference on Theoretical Aspects of Reasoning About Knowledge. </booktitle> <publisher> Morgan Kaufmann Inc., </publisher> <year> 1990. </year>
Reference-contexts: Clearly that is not true for all such K and L. Thus this inference fails for the right reasons. This inference cannot be avoided in any possible worlds approach, not even those that consider "impossible worlds" [22], or "buddy worlds" <ref> [15] </ref> (roughly, because K and L are true at exactly the same worlds). 4.1 A Logic for Intention and Belief We have so far presented a philosophically and folk-psychologically well-motivated model theoretic approach to giving the semantics of intention and belief reports.
Reference: [16] <author> Michael P. Georgeff. </author> <title> Planning. </title> <editor> In J. F. Traub, editor, </editor> <booktitle> Annual Review of Computer Science, </booktitle> <volume> Vol 2. </volume> <publisher> Annual Reviews Inc., </publisher> <address> Palo Alto, CA, </address> <year> 1987. </year>
Reference-contexts: 1 Introduction An understanding of intentions is important to several subfields of Artificial Intelligence (AI), especially, speech act theory [3, 4, 10, 11], discourse processing [17], planning <ref> [16] </ref>, and plan recognition [2, 23, 27]. We present a formal theory of intentions and beliefs that is based on Discourse Representation Theory (DRT) [5, 6, 18]. Our theory involves a formal model of time and possibility, and also explicitly models the structure of the agents' internal states. <p> Their importance to cognition and their use in AI has been defended extensively in the literature <ref> [4, 8, 10, 11, 16, 17, 29] </ref>. The roles of intentions mentioned below are especially important when one is interested in agents who would not otherwise (because of their limitations) be able to make appropriate or rational decisions. <p> We sketch just enough of DRT in this paper so that the presentation here is self-contained. A study of the (mostly AI) literature <ref> [4, 8, 10, 16, 17, 24, 26, 27, 29] </ref> yields the following important properties of intentions. Intentions are about future events. An agent with an intention should believe that it can be realized, at least along some future. <p> However, for many purposes in several important subfields of AI, it would be useful to also have a logic that corresponds to the above semantics. These purposes include (1) reasoning by an agent to determine his own plans, and to reason about their consequences <ref> [16, 24] </ref>, (2) and to do the same for the plans of others [27], and (3) a principled approach to the design of multiagent intelligent systems [28]. (1) and (2) directly relate to speech act planning and discourse understanding [4, 10, 17, 26] as well.
Reference: [17] <author> Barbara Grosz and Candace Sidner. </author> <title> Attentions, intentions, and discourse structure. </title> <journal> Computational Linguistics, </journal> <volume> 12(3) </volume> <pages> 175-204, </pages> <year> 1986. </year>
Reference-contexts: 1 Introduction An understanding of intentions is important to several subfields of Artificial Intelligence (AI), especially, speech act theory [3, 4, 10, 11], discourse processing <ref> [17] </ref>, planning [16], and plan recognition [2, 23, 27]. We present a formal theory of intentions and beliefs that is based on Discourse Representation Theory (DRT) [5, 6, 18]. Our theory involves a formal model of time and possibility, and also explicitly models the structure of the agents' internal states. <p> Their importance to cognition and their use in AI has been defended extensively in the literature <ref> [4, 8, 10, 11, 16, 17, 29] </ref>. The roles of intentions mentioned below are especially important when one is interested in agents who would not otherwise (because of their limitations) be able to make appropriate or rational decisions. <p> We sketch just enough of DRT in this paper so that the presentation here is self-contained. A study of the (mostly AI) literature <ref> [4, 8, 10, 16, 17, 24, 26, 27, 29] </ref> yields the following important properties of intentions. Intentions are about future events. An agent with an intention should believe that it can be realized, at least along some future. <p> to determine his own plans, and to reason about their consequences [16, 24], (2) and to do the same for the plans of others [27], and (3) a principled approach to the design of multiagent intelligent systems [28]. (1) and (2) directly relate to speech act planning and discourse understanding <ref> [4, 10, 17, 26] </ref> as well. Such a logic would also be useful in multiagent systems where agents must reason about each other's intentions to negotiate among themselves. We now turn to a deductive system for our semantics.
Reference: [18] <author> Hans Kamp. </author> <title> A theory of truth and semantic representation. </title> <editor> In J. Groenendijk, T. Jansenn, and M. Stokhof, editors, Truth, </editor> <booktitle> Interpretation and Information, </booktitle> <pages> pages 1-41. </pages> <publisher> Foris Publications, </publisher> <address> Dordrecht, The Netherlands, </address> <year> 1984. </year>
Reference-contexts: We present a formal theory of intentions and beliefs that is based on Discourse Representation Theory (DRT) <ref> [5, 6, 18] </ref>. Our theory involves a formal model of time and possibility, and also explicitly models the structure of the agents' internal states.
Reference: [19] <author> Hans Kamp. </author> <title> Context, </title> <booktitle> thought and communication. The Proceedings of the Aristotelian Society, New Series, </booktitle> <address> LXXXV(XIII):239-261, 1984/1985. </address>
Reference-contexts: Such a unified theory has been evolving in the DRT framework over the last few years. Kamp's thesis of the Unity of Thought and Information <ref> [19] </ref>, and Asher's work on the attitudes [5], on their relationship with information [7], and on the anaphoric properties of verbs [6] must be cited in this context. DRT is a useful framework for the general project for several reasons. <p> agent entirely; the choices made by an agent, and his (most especially, verbal) behavior does not just depend on the anchoring of his internal state in the world, but on its structure as well: Kripke's `London' versus `Londres' example [21], discussed in a DRT framework in [5, pp. 142-143] and <ref> [19, pp. 253-254] </ref>, is a case in point.
Reference: [20] <author> Kurt Konolige. </author> <title> A Deduction Model of Belief. </title> <publisher> Morgan Kaufmann, Inc., </publisher> <year> 1986. </year>
Reference-contexts: These properties, while not valid in the minimal logic, are expressible in extensions of our theory (see x5). 2 More Intuitions Our theory, like the so-called sentential theories <ref> [20] </ref>, and unlike most possible-worlds based theories [10], avoids attributing logical omniscience to agents, since it does not require that agents' intentions or beliefs be closed under logical equivalence (thus it also avoids validating closure under logical consequence).
Reference: [21] <author> Saul Kripke. </author> <title> A puzzle about belief. </title> <editor> In A. Margalit, editor, </editor> <title> Meaning and Use. </title> <publisher> Dordrecht Reidel, </publisher> <address> Dordrecht, The Netherlands, </address> <year> 1979. </year>
Reference-contexts: it is not acceptable to ignore the internal structure of the agent entirely; the choices made by an agent, and his (most especially, verbal) behavior does not just depend on the anchoring of his internal state in the world, but on its structure as well: Kripke's `London' versus `Londres' example <ref> [21] </ref>, discussed in a DRT framework in [5, pp. 142-143] and [19, pp. 253-254], is a case in point.
Reference: [22] <author> Hector Levesque. </author> <title> A logic of implicit and explicit belief. </title> <booktitle> In AAAI, </booktitle> <year> 1984. </year> <month> 15 </month>
Reference-contexts: Thus our approach also avoids the charges of ad hoc-ism often levied against the sentential approaches (e.g., by Levesque <ref> [22] </ref>, and others [14]). The main advantage of our approach is that it allows us to model the internal architecture of intelligent agents far more realistically than in the other formal approaches. <p> Clearly that is not true for all such K and L. Thus this inference fails for the right reasons. This inference cannot be avoided in any possible worlds approach, not even those that consider "impossible worlds" <ref> [22] </ref>, or "buddy worlds" [15] (roughly, because K and L are true at exactly the same worlds). 4.1 A Logic for Intention and Belief We have so far presented a philosophically and folk-psychologically well-motivated model theoretic approach to giving the semantics of intention and belief reports.
Reference: [23] <author> Diane J. Litman and James F. Allen. </author> <title> A plan recognition model for subdialogues in conversations. </title> <journal> Cognitive Science, </journal> <volume> 11 </volume> <pages> 163-200, </pages> <year> 1987. </year>
Reference-contexts: 1 Introduction An understanding of intentions is important to several subfields of Artificial Intelligence (AI), especially, speech act theory [3, 4, 10, 11], discourse processing [17], planning [16], and plan recognition <ref> [2, 23, 27] </ref>. We present a formal theory of intentions and beliefs that is based on Discourse Representation Theory (DRT) [5, 6, 18]. Our theory involves a formal model of time and possibility, and also explicitly models the structure of the agents' internal states.
Reference: [24] <author> Drew McDermott. </author> <title> A temporal logic for reasoning about processes and plans. </title> <journal> Cognitive Science, </journal> <volume> 6(2) </volume> <pages> 101-155, </pages> <year> 1982. </year>
Reference-contexts: We sketch just enough of DRT in this paper so that the presentation here is self-contained. A study of the (mostly AI) literature <ref> [4, 8, 10, 16, 17, 24, 26, 27, 29] </ref> yields the following important properties of intentions. Intentions are about future events. An agent with an intention should believe that it can be realized, at least along some future. <p> 2 S ^ g w w f w : M j= w;t 0 ;g K))g 7 As a consequence of this definition, the following properties of intentions are accounted for: (1) an agent with an intention tacitly considers it possible that his intention will be fulfilled (this is motivated in <ref> [24] </ref>), and (2) is tacitly restricted by his intention to scenarios in which it is achieved (this is motivated in [8]). These are two of the most important properties of intentions. Now, the content of any set of attitudes is the intersection of their respective contents. <p> However, for many purposes in several important subfields of AI, it would be useful to also have a logic that corresponds to the above semantics. These purposes include (1) reasoning by an agent to determine his own plans, and to reason about their consequences <ref> [16, 24] </ref>, (2) and to do the same for the plans of others [27], and (3) a principled approach to the design of multiagent intelligent systems [28]. (1) and (2) directly relate to speech act planning and discourse understanding [4, 10, 17, 26] as well.
Reference: [25] <author> Barbara Hall Partee. </author> <title> Belief-sentences and the limits of semantics. </title> <editor> In Stanley Peters and Esa Saarinen, editors, </editor> <title> Processes, Beliefs, and Questions. </title> <address> D. </address> <publisher> Reidel Publishing Company, </publisher> <address> Dordrecht, The Netherlands, </address> <year> 1982. </year>
Reference-contexts: Barbara Partee notes that the lack of valid inferences involving beliefs (and, by extension, intentions) provides only negative evidence against specific proposals for their semantics <ref> [25, p. 95] </ref>. We feel that positive evidence may be generated when agents of different architectures and computational power 11 are considered. That is, while no inference seems to hold in general, it is important methodologically to consider inferences that hold under different conditions.
Reference: [26] <author> Raymond Perrault. </author> <title> An application of default logic to speech act theory. </title> <type> Technical Report 90, </type> <institution> Center for the Study of Language and Information, Stanford, </institution> <address> CA, </address> <month> March </month> <year> 1987. </year>
Reference-contexts: We sketch just enough of DRT in this paper so that the presentation here is self-contained. A study of the (mostly AI) literature <ref> [4, 8, 10, 16, 17, 24, 26, 27, 29] </ref> yields the following important properties of intentions. Intentions are about future events. An agent with an intention should believe that it can be realized, at least along some future. <p> to determine his own plans, and to reason about their consequences [16, 24], (2) and to do the same for the plans of others [27], and (3) a principled approach to the design of multiagent intelligent systems [28]. (1) and (2) directly relate to speech act planning and discourse understanding <ref> [4, 10, 17, 26] </ref> as well. Such a logic would also be useful in multiagent systems where agents must reason about each other's intentions to negotiate among themselves. We now turn to a deductive system for our semantics.
Reference: [27] <author> Martha E. Pollack. </author> <title> Inferring Domain Plans in Question Answering. </title> <type> PhD thesis, </type> <institution> University of Pennsylvania, </institution> <year> 1986. </year>
Reference-contexts: 1 Introduction An understanding of intentions is important to several subfields of Artificial Intelligence (AI), especially, speech act theory [3, 4, 10, 11], discourse processing [17], planning [16], and plan recognition <ref> [2, 23, 27] </ref>. We present a formal theory of intentions and beliefs that is based on Discourse Representation Theory (DRT) [5, 6, 18]. Our theory involves a formal model of time and possibility, and also explicitly models the structure of the agents' internal states. <p> We sketch just enough of DRT in this paper so that the presentation here is self-contained. A study of the (mostly AI) literature <ref> [4, 8, 10, 16, 17, 24, 26, 27, 29] </ref> yields the following important properties of intentions. Intentions are about future events. An agent with an intention should believe that it can be realized, at least along some future. <p> These purposes include (1) reasoning by an agent to determine his own plans, and to reason about their consequences [16, 24], (2) and to do the same for the plans of others <ref> [27] </ref>, and (3) a principled approach to the design of multiagent intelligent systems [28]. (1) and (2) directly relate to speech act planning and discourse understanding [4, 10, 17, 26] as well. <p> These axioms also may be used to motivate certain inferences that are invalid in general but may be acceptable in special circumstances (e.g., as models of how agents of a particular class deliberate), or used as heuristics or conjectures for reasoning in areas such as plan recognition <ref> [27] </ref>. In future work, we plan to incorporate an explicit account of actions and ability into this theory [32] and also to extend it to the intentions of groups of agents [31, 30].
Reference: [28] <author> Stanley J. Rosenschein. </author> <title> Formal theories of knowledge in AI and robotics. </title> <journal> New Generation Computing, </journal> <volume> 3(4), </volume> <year> 1985. </year>
Reference-contexts: These purposes include (1) reasoning by an agent to determine his own plans, and to reason about their consequences [16, 24], (2) and to do the same for the plans of others [27], and (3) a principled approach to the design of multiagent intelligent systems <ref> [28] </ref>. (1) and (2) directly relate to speech act planning and discourse understanding [4, 10, 17, 26] as well. Such a logic would also be useful in multiagent systems where agents must reason about each other's intentions to negotiate among themselves.
Reference: [29] <author> John R. Searle. Intentionality: </author> <title> An essay in the Philosophy of Mind. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, UK, </address> <year> 1983. </year>
Reference-contexts: Their importance to cognition and their use in AI has been defended extensively in the literature <ref> [4, 8, 10, 11, 16, 17, 29] </ref>. The roles of intentions mentioned below are especially important when one is interested in agents who would not otherwise (because of their limitations) be able to make appropriate or rational decisions. <p> We sketch just enough of DRT in this paper so that the presentation here is self-contained. A study of the (mostly AI) literature <ref> [4, 8, 10, 16, 17, 24, 26, 27, 29] </ref> yields the following important properties of intentions. Intentions are about future events. An agent with an intention should believe that it can be realized, at least along some future.
Reference: [30] <author> Munindar P. Singh. </author> <title> Group ability and structure. </title> <booktitle> In 2nd European Workshop on the Modeling of Autonomous Agents in a Multi-Agent World, </booktitle> <month> August </month> <year> 1990. </year>
Reference-contexts: In future work, we plan to incorporate an explicit account of actions and ability into this theory [32] and also to extend it to the intentions of groups of agents <ref> [31, 30] </ref>.
Reference: [31] <author> Munindar P. Singh. </author> <title> Group intentions. </title> <booktitle> In 10th Workshop on Distributed Artificial Intelligence, </booktitle> <month> October </month> <year> 1990. </year>
Reference-contexts: In future work, we plan to incorporate an explicit account of actions and ability into this theory [32] and also to extend it to the intentions of groups of agents <ref> [31, 30] </ref>.
Reference: [32] <author> Munindar P. Singh. </author> <title> Towards a theory of situated know-how. </title> <booktitle> In 9th European Conference on Artificial Intelligence, </booktitle> <month> August </month> <year> 1990. </year>
Reference-contexts: In future work, we plan to incorporate an explicit account of actions and ability into this theory <ref> [32] </ref> and also to extend it to the intentions of groups of agents [31, 30].
Reference: [33] <author> Robert C. Stalnaker. </author> <title> Inquiry. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1984. </year>
Reference-contexts: This, of course, depends on what one might want to do with such a theory. A theory of intentions is needed at the foundational level of study in AI and Cognitive Science in order to complete an account of intelligent (and possibly, rational) agency <ref> [12, 33] </ref>. Intentions are important fl We are indebted to Allen Emerson and Rob Koons for several discussions and for comments on previous versions of this paper. 2 attitudes of intelligent agents and, for resource-bounded agents, cannot be reduced to simple considerations of the optimality of decisions.
References-found: 33

