URL: http://www.iscs.nus.sg/~plong/papers/part_match.ps
Refering-URL: 
Root-URL: 
Title: Multiple-Dictionary Compression Using Partial Matching  
Author: Dzung T. Hoang Philip M. Long Jeffrey Scott Vitter 
Address: Box 90129 Durham, NC 27708-0129  
Affiliation: Department of Computer Science Duke University  
Abstract: Motivated by the desire to find text compressors that compress better than existing dictionary methods, but run faster than PPM implementations, we describe methods for text compression using multiple dictionaries, one for each context of preceeding characters, where the contexts have varying lengths. The context to be used is determined using an escape mechanism similar to that of PPM methods. We describe modifications of three popular dictionary coders along these lines and experiments evaluating their efficacy using the text files in the Calgary corpus. Our results suggest that modifying LZ77 along these lines yields an improvement in compression of about 4%, that modifying LZFG yields a compression improvement of about 8%, and that modifying LZW in this manner yields an average improvement on the order of 12%.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. C. Bell, J. G. Cleary, and I. H. Witten, </author> <title> Text Compression, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1990. </year>
Reference-contexts: Therefore, LZ77 coders reserve codespace for pointers that will never be coded, which is wasteful. The C2 coder of Fiala and Green [4] (referred to in <ref> [1] </ref> as LZFG) eliminates pointer redundancy by coding a pointer into a trie, which stores unique parsed substrings that have occurred previously in the text. Each substring stored in the trie has a unique identifier. <p> We also adjusted the maximum context order and obtained the best results for a maximum context order of one. The tabulated results, given in Table 1, are for a maximum 2 Actually a nearly flat code <ref> [1] </ref> is used if the number of internal nodes is not a power of 2. context order of one, coding escapes as nodes. 2.4 LZW-PM In the LZW algorithm [14], which is used in UNIX compress, the dictionary is initialized to contain all strings of length 1, and a flat code <p> This method of encoding escapes approximately corresponds to the method used in PPMA <ref> [1] </ref>, since it (approximately) involves the implicit assumption that the probability of seeing a previously unseen character in a given context is the inverse of the number of times the context has been seen.
Reference: [2] <author> T. C. Bell and I. H. Witten, </author> <title> "The relationship between greedy parsing and sym-bolwise text compression," </title> <note> Journal of the ACM (In press). </note>
Reference: [3] <author> J. G. Cleary and I. H. Witten, </author> <title> "Data Compression using Adaptive Coding and Partial String Matching," </title> <journal> IEEE Transactions on Communication 32 (April 1984), </journal> <pages> 396-402. </pages>
Reference: [4] <author> E.R. Fiala and D.H. Greene, </author> <title> "Data compression with finite windows," </title> <journal> Communications of the ACM 32, </journal> <pages> 490-505. </pages>
Reference-contexts: We describe implementations of coders of this type behaving, for each context, in a manner analogous to a basic LZ77 [15] coder, and in a manner analogous to the LZFG <ref> [4] </ref> variant of the basic Ziv-Lempel coder, and the LZW [14] coder. <p> We found this tool not to be useful in the LZW-based coder. We describe experiments using the Calgary corpus which show that, for a simple instantiation of the LZ77 ideas, based on the A1 coder from <ref> [4] </ref>, context-length management of this type leads to an improvement in compression of about 4%. For LZFG [4], the improvement is around 8%, and for LZW, the improvement is around 12%. <p> We describe experiments using the Calgary corpus which show that, for a simple instantiation of the LZ77 ideas, based on the A1 coder from <ref> [4] </ref>, context-length management of this type leads to an improvement in compression of about 4%. For LZFG [4], the improvement is around 8%, and for LZW, the improvement is around 12%. <p> The coder described in Section 2.2 makes these decisions in a manner analogous to the A1 instantiation <ref> [4] </ref> of the LZ77 method [15]. The coder of Section 2.3 uses as a subroutine a dictionary coder based on Slyz's implementation [12] of the C2 coder [4]. <p> The coder described in Section 2.2 makes these decisions in a manner analogous to the A1 instantiation <ref> [4] </ref> of the LZ77 method [15]. The coder of Section 2.3 uses as a subroutine a dictionary coder based on Slyz's implementation [12] of the C2 coder [4]. The coder of Section 2.4 is based on LZW [14]. 2.2 LZ77-PM We begin by describing a PM modification of a simple LZ77 coder that is almost identical to the A1 coder from [4]. The baseline coder to which we compare our results works as follows. <p> uses as a subroutine a dictionary coder based on Slyz's implementation [12] of the C2 coder <ref> [4] </ref>. The coder of Section 2.4 is based on LZW [14]. 2.2 LZ77-PM We begin by describing a PM modification of a simple LZ77 coder that is almost identical to the A1 coder from [4]. The baseline coder to which we compare our results works as follows. First, it divides the file into buffers of size 256K, and codes each buffer as if it were a separate file. <p> The results for the baseline coder and the PM modification on the 10 text files from the Calgary Corpus are given in Table 1. The A1 coder from <ref> [4] </ref>, instead of restarting from time to time, restricts pointers earlier into the file to be to positions at most 64K from the position to be coded. The distance to the position where the match begins is then transmitted using a flat code. <p> Therefore, LZ77 coders reserve codespace for pointers that will never be coded, which is wasteful. The C2 coder of Fiala and Green <ref> [4] </ref> (referred to in [1] as LZFG) eliminates pointer redundancy by coding a pointer into a trie, which stores unique parsed substrings that have occurred previously in the text. Each substring stored in the trie has a unique identifier. <p> The LZ77 coder is almost identical to the A1 coder from <ref> [4] </ref>. The LZFG implementation used is from [12]. The total bits per character is obtained taking the total number of bits in the compressed representations of the files in the corpus and dividing by the total number of characters.
Reference: [5] <author> P.C. Gutmann and T.C. Bell, </author> <title> "A hybrid approach to data compression," </title> <booktitle> Proceedings of the 1994 Data Compression Conference (1994), </booktitle> <pages> 225-233. </pages>
Reference-contexts: The idea in both is, loosely, to have a corresponding equivalent statistical algorithm start its cycle by using a context of 1 previous character, then 2, and so on, instead of starting at 0. The work of <ref> [5] </ref> placed a heavy emphasis on speed, and therefore it is unfair to compare our compression results with theirs. The compression results of the CSD algorithm [10], which was based in the LZW [14] dictionary method, are given in Table 1 for comparison.
Reference: [6] <author> P. G. Howard and J. S. Vitter, </author> <title> "Practical Implementations of Arithmetic Coding," in Images and Text Compression, </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1992, </year> <type> invited paper. </type>
Reference: [7] <author> P. G. Howard and J. S. Vitter, </author> <title> "Design and Analysis of Fast Text Compression Based on Quasi-Arithmetic Coding," </title> <booktitle> Proc. 1993 IEEE Data Compression Conference (March-April 1993), </booktitle> <pages> 98-107. </pages>
Reference-contexts: F49620-92-J-0515 and F49620-94-1-0217. y Supported in part by Air Force Office of Strategic Research grants F49620-92-J-0515 and F49620-94-1-0217. z Supported in part by Air Force Office of Scientific Research grants F49620-92-J-0515 and F49620-94-1-0217 and by a Universities Space Research Association/CESDIS associate membership. of about 2|is the PPM method developed in <ref> [7] </ref>. In this paper we describe a different approach to developing methods that give better compression than LZ methods while running faster than PPM and report on experiments evaluating the new methods. At a high level, for one setting of its parameters, the PPMC algorithm works approximately as follows.
Reference: [8] <author> D. Knuth, </author> <booktitle> in The Art of Computer Programming, </booktitle> <volume> Volume 3: </volume> <publisher> Sorting and Searching , Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1973. </year>
Reference-contexts: Each substring stored in the trie has a unique identifier. The improvement in compression with LZFG comes at a price of a more complex decoder, which has to maintain the same trie as the encoder. LZFG uses a space-efficient implementation of a trie called the PATRICIA trie <ref> [8] </ref>. A PATRICIA trie is a trie in which sequences of unary branches are compressed into a single branch. A PATRICIA trie saves space by not storing every character explicitly in the nodes.
Reference: [9] <author> G. G. Langdon, </author> <title> "A note on the Ziv-Lempel model for compressing individual sequences," </title> <journal> IEEE Transactions on Information Theory 29 (March 1983), </journal> <pages> 284-287. </pages>
Reference: [10] <author> Y. Nakano, H. Yahagi, Y. Okada, and S. Yoshida, </author> <title> "Highly efficient universal coding with classifying to subdictionaries for text compression," </title> <booktitle> Proceedings of the 1994 Data Compression Conference, </booktitle> <pages> 234-243. </pages>
Reference-contexts: The work of [5] placed a heavy emphasis on speed, and therefore it is unfair to compare our compression results with theirs. The compression results of the CSD algorithm <ref> [10] </ref>, which was based in the LZW [14] dictionary method, are given in Table 1 for comparison. The currently best known statistical method, the PPMD method, usually bases its probability estimates on contexts of three characters. <p> The compression results given by applying LZW-PM to the Calgary corpus, together with those of straight LZW, and the CSD coder <ref> [10] </ref>, which adapts LZW by having a separate context for each single previous character, with no escapes to an order-0 context, are given in Table 1. <p> The bits per character per file is calculated as the average of the bits per character over the test files. CSD results are copied from <ref> [10] </ref>. CSD used a limited dictionary size (16K), while the LZW and LZW-PM algorithms did not; the adverse affect of this is most notable on the long files. We are presently working on improving our results in a number of ways.
Reference: [11] <author> M. Ohmine and H. Yamamoto, </author> <title> "Universal data compression algorithms with multiple dictionaries," </title> <type> Tech. </type> <institution> Repo. of Inst. of Elec. Inf. Comm. Eng. </institution> <year> (1993), </year> <pages> 1-6. </pages>
Reference: [12] <author> M. Slyz, </author> <title> "Image Compression using a Ziv-Lempel type Coder," </title> <institution> University of Michigan School of Engineering, </institution> <type> Master's Thesis, </type> <year> 1991. </year>
Reference-contexts: The coder described in Section 2.2 makes these decisions in a manner analogous to the A1 instantiation [4] of the LZ77 method [15]. The coder of Section 2.3 uses as a subroutine a dictionary coder based on Slyz's implementation <ref> [12] </ref> of the C2 coder [4]. The coder of Section 2.4 is based on LZW [14]. 2.2 LZ77-PM We begin by describing a PM modification of a simple LZ77 coder that is almost identical to the A1 coder from [4]. <p> All the static variable-length codes used belong to a parameterized class called start-step-stop codes which are quickly encoded and decoded. We constructed a PM coder which we call LZFG-PM based on an existing LZFG implementation <ref> [12] </ref>. Following the outline of Section 2.1, LZFG-PM constructs separate tries for each context, including a context of order 0. A limited number of internal nodes and leaves is globally allocated. <p> The LZ77 coder is almost identical to the A1 coder from [4]. The LZFG implementation used is from <ref> [12] </ref>. The total bits per character is obtained taking the total number of bits in the compressed representations of the files in the corpus and dividing by the total number of characters.
Reference: [13] <author> J. A. Storer, </author> <title> Data Compression: Methods and Theory , CS Press, </title> <address> New York, </address> <year> 1988. </year>
Reference: [14] <author> T.A. Welch, </author> <title> "A technique for high performance data compression," </title> <booktitle> Computer (1984), </booktitle> <pages> 8-19. </pages>
Reference-contexts: The work of [5] placed a heavy emphasis on speed, and therefore it is unfair to compare our compression results with theirs. The compression results of the CSD algorithm [10], which was based in the LZW <ref> [14] </ref> dictionary method, are given in Table 1 for comparison. The currently best known statistical method, the PPMD method, usually bases its probability estimates on contexts of three characters. <p> We describe implementations of coders of this type behaving, for each context, in a manner analogous to a basic LZ77 [15] coder, and in a manner analogous to the LZFG [4] variant of the basic Ziv-Lempel coder, and the LZW <ref> [14] </ref> coder. <p> The coder of Section 2.3 uses as a subroutine a dictionary coder based on Slyz's implementation [12] of the C2 coder [4]. The coder of Section 2.4 is based on LZW <ref> [14] </ref>. 2.2 LZ77-PM We begin by describing a PM modification of a simple LZ77 coder that is almost identical to the A1 coder from [4]. The baseline coder to which we compare our results works as follows. <p> The tabulated results, given in Table 1, are for a maximum 2 Actually a nearly flat code [1] is used if the number of internal nodes is not a power of 2. context order of one, coding escapes as nodes. 2.4 LZW-PM In the LZW algorithm <ref> [14] </ref>, which is used in UNIX compress, the dictionary is initialized to contain all strings of length 1, and a flat code is used to encode the dictionary entry that is matched.
Reference: [15] <author> J. Ziv and A. Lempel, </author> <title> "A universal algorithm for sequential data compression," </title> <journal> IEEE Transactions on Information Theory 23 (1977), </journal> <pages> 337-343. </pages>
Reference-contexts: In this paper, we investigate 1 Actually, these statistics are only sometimes updated. the use of separate dictionaries for each context up to some maximum context order. We describe implementations of coders of this type behaving, for each context, in a manner analogous to a basic LZ77 <ref> [15] </ref> coder, and in a manner analogous to the LZFG [4] variant of the basic Ziv-Lempel coder, and the LZW [14] coder. <p> The coder described in Section 2.2 makes these decisions in a manner analogous to the A1 instantiation [4] of the LZ77 method <ref> [15] </ref>. The coder of Section 2.3 uses as a subroutine a dictionary coder based on Slyz's implementation [12] of the C2 coder [4].
Reference: [16] <author> J. Ziv and A. Lempel, </author> <title> "Compression of Individual Sequences via Variable-Rate Coding," </title> <journal> IEEE Transactions on Information Theory 24 (September 1978), </journal> <pages> 530-536. </pages>
References-found: 16

