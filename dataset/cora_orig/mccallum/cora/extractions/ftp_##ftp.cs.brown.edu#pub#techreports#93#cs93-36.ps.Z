URL: ftp://ftp.cs.brown.edu/pub/techreports/93/cs93-36.ps.Z
Refering-URL: http://www.cs.brown.edu/publications/techreports/reports/CS-93-36.html
Root-URL: http://www.cs.brown.edu/
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Blumer, Anselm, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth, </author> <note> "Occam's Razor", in Information Processing Letters 24 (1987) pp 377-380, also reprinted in Readings in Machine Learning by Jude W. </note> <editor> Shavlik and Thomas G. </editor> <publisher> Dietterich. </publisher>
Reference-contexts: data, many of which work only by accident. 27 "Lemma: Given any function f in a hypothesis class of r hypotheses, the probability that any hypothesis with error larger than epsilon is consistent with a sample of f of size m is less than r fl (1 *) m ." <ref> [1] </ref>.
Reference: [2] <author> Colby, Robert W., and Thomas A. Meyers, </author> <title> The Encyclopedia of Technical Stock Market Indicators, </title> <booktitle> Dow Jones-Irwin, </booktitle> <year> 1988. </year>
Reference: [3] <author> Crowder, R. Scott, </author> <title> "Predicting the Mackey-Glass Timeseries With Cascade-Correlation Learning" in David S. </title> <editor> Touretzky (ed), </editor> <title> Connectionist Models: </title> <booktitle> Proceedings of the 1990 Summer School. </booktitle>
Reference: [4] <author> Elman, Jeffrey L., </author> <title> "Distributed Representations, Simple Recurrent Networks, and Grammatical Structure", Machine Learning 7, </title> <editor> p. </editor> <month> 195 </month> <year> (1991). </year>
Reference: [5] <author> Elman, Jefferey L., post to comp.ai.neural-nets, </author> <month> August 6, </month> <year> 1992. </year>
Reference-contexts: I'd go for the most powerful machine available." <ref> [5] </ref>. My best guess is that I have stopped it before it overtrained, thereby getting something closer to an arbitrary weighting of the preprocessed inputs than the most other algorithms.
Reference: [6] <author> Fahlman, Scott E., </author> <booktitle> "The Cascade-Correlation Learning Architecture", in Advances in Neural Information Processing Systems 2, </booktitle> <publisher> Morgan Kaufman (1990). </publisher>
Reference-contexts: Linear Associator (using the Widrow-Hoff rule) (WH) A single layer linear network. 2. Backpropagation (BP) A popular algorithm which uses one or more hidden layers. 3. Cascade Correlation (CC) <ref> [6] </ref> An algorithm which adds hidden units one at a time, training several candidate units at each step and choosing the one most correlated with the error. 4.
Reference: [7] <author> Fahlman, Scott E., </author> <title> "The Recurrent Cascade-Correlation Architecture", </title> <booktitle> in Advances in Neural Information Processing Systems 3, </booktitle> <publisher> Morgan Kaufman (1991). </publisher>
Reference-contexts: I suspect from my results and from the Kadirkamanathan and Niranjan paper that the algorithm is sensitive to the threshold used to control when new units are added. 2 2.2 Recurrent Networks 1. Recurrent Cascade Correlation (RCC) <ref> [7] </ref> ( see figure 2.1 ) Similar to the cascade correlation algorithm, but with the output of each hidden unit fed back as an input to itself. 2.
Reference: [8] <author> Fahlman, Scott E., </author> <type> Personal Communication (1993). </type>
Reference-contexts: Backpropagation (BP) A popular algorithm which uses one or more hidden layers. 3. Cascade Correlation (CC) [6] An algorithm which adds hidden units one at a time, training several candidate units at each step and choosing the one most correlated with the error. 4. Cascade 2 (C2) <ref> [8] </ref> Like cascade correlation, but the candidate units are trained to minimize the sum squared difference between the unit output and the error of the output layer. I also tried: 5. RAN (Resource Allocating Network) [13] This builds up a network of radial basis units one unit at a time.
Reference: [9] <author> Fang, Yan and Terrence J. Sejnowski, </author> <title> "Faster Learning for Dynamic Recurrent Backpropagation", </title> <booktitle> Neural Computation 2, </booktitle> <month> 270-273 </month> <year> (1990). </year>
Reference-contexts: Since I did not find any indication that it has been used for anything more difficult than the (very simple) purposes to which Pearlmutter put it, and because of the following: "We replicated this result, but the original algorithm was very sensitive to the choice of parameters and initial conditions" <ref> [9] </ref>, I decided it was not promising. 3 4 5 2.3 Genetic Programming (GP) I start with several seed expressions (detailed in the appendix), and initialize the remaining individuals by mutating a randomly selected seed expression.
Reference: [10] <author> Fosback, Norman G., </author> <title> Stock Market Logic, </title> <institution> Institute for Econometric Research, </institution> <year> 1976. </year>
Reference: [11] <author> Guimares, Rui M.C., Brian G. Kingsman and Stephen J. Taylor, </author> <title> A Reappraisal of the Efficiency of Financial Markets, </title> <publisher> Springer-Verlag 1989. </publisher>
Reference-contexts: The stock market is particularly interesting because there is serious disagreement about whether better-than-random predictions of stock prices are possible <ref> [11] </ref>. There are three ways that a neural network can forecast a time series. It can be provided with inputs which enable it to find rules relating the current state of the system being predicted to future states.
Reference: [12] <author> Hertz, John, Anders Krogh, and Richard G. Palmer, </author> <title> Introduction to the Theory of Neural Computation, </title> <publisher> Addison-Wesley 1991. </publisher>
Reference: [13] <author> Kadirkamanathan, Visakan and Mahesan Niranjan, </author> <title> "A Function Estimation Approach to Sequential Learning with Neural Networks" svr-ftp.eng.cam.ac.uk:/reports/kadirkamanathan tr111.ps.Z (1992). </title>
Reference-contexts: Cascade 2 (C2) [8] Like cascade correlation, but the candidate units are trained to minimize the sum squared difference between the unit output and the error of the output layer. I also tried: 5. RAN (Resource Allocating Network) <ref> [13] </ref> This builds up a network of radial basis units one unit at a time. While I got the RAN to do some gradient descent, it did not work nearly as well as the papers indicated it should, suggesting there is still a bug in my code. <p> 20, candidate input epsilon = 100, candidate output epsilon = 10, candidate weight decay (input and output) = 0.0000000001, and output epsilon = 0.1. 3.5 Analysis of Mackey-Glass Results Recurrence is more important here than in the sunspot tests, probably because the input window was less effectively chosen (I followed <ref> [13] </ref>). There was substantially greater variation in the results for a given algorithm here. Recurrent connections are more important here than with the sunspot data, causing windowed recurrent cascade correlation to be the best.
Reference: [14] <editor> Koza, John R., </editor> <booktitle> Genetic Programming, </booktitle> <publisher> MIT Press 1992. </publisher>
Reference: [15] <author> Mandelman, Avner, </author> <title> "The Computer's Bullish!: A Money Manager's Love Affair with Neural Network Programs", </title> <address> Barron's, </address> <month> Dec 14, </month> <year> 1992. </year>
Reference: [16] <author> Pollack, Jordan B., </author> <title> "The Induction of Dynamical Recognizers", Machine Learning 7, </title> <editor> p. </editor> <month> 227 </month> <year> (1991). </year>
Reference-contexts: Real-Time Recurrent Learning (RTRL) [23, 24] ( see figure 2.3 ) A single layer network, with the outputs of all units (of which there may be more than there are external outputs) fed back as input to all the units. 4. Sequential Cascaded Network (SEQ) <ref> [16] </ref> ( see figure 2.4 ) Connects the inputs to the outputs by a single layer, whose weights are set dy namically by a matrix of context weights based on the previous output. I also did some work with a BPTT variant (Back-Propagation Through Time) [22].
Reference: [17] <author> Tong, Howell, </author> <title> Threshold Models in Non-linear Time Series Analysis, </title> <publisher> Springer, </publisher> <year> 1983. </year>
Reference-contexts: stability problems, although I do not believe this would have been a problem with the data that I used. 8 Chapter 3 Sunspot Tests I used the average annual sunspot data (approximately a count of the number of sunspots, with adjustments for sunspot groups and differences in telescopes), taken from <ref> [17] </ref>. I took the data from 1700 to 1920 for a training set, and from 1921 to 1979 for a prediction set.
Reference: [18] <author> Utans, Joachim, and John Moody, </author> <title> "Selecting Neural Network Architectures via the Prediction Risk: </title> <booktitle> Application to Corporate Bond Rating Prediction" in Proceedings of The First International Conference on Artificial Intelligence Applications on Wall Street, </booktitle> <publisher> IEEE Computer Society Press 1991. </publisher>
Reference: [19] <author> Weigend, Andreas S., David E. Rumelhart, and Bernardo A. Huberman, </author> <title> "BackPropagation, Weight Elimination, and Time Series Prediction" in David S. </title> <editor> Touret-zky (ed), </editor> <title> Connectionist Models: </title> <booktitle> Proceedings of the 1990 Summer School. </booktitle>
Reference-contexts: results are the average of 5 runs. 11 3.3 Analysis of Sunspot Results The choice of window size, and the number of hidden units for backpropagation, was influenced by a desire to compare my results with results for backpropagation and threshold autoregression as reported by Weigend, Rumelhart, and Huberman in <ref> [19, 21] </ref> With the training rate of .1 which they used for backpropagation, I got very unstable fluctuations in the error, but reducing the training rate produced results similar to theirs. For all networks, windowing was more important than the recurrent connections.
Reference: [20] <author> Weigend, Andreas S., David E. Rumelhart, and Bernardo A. Huberman, </author> <title> "Generalization by weight elimination with application to forecasting", </title> <editor> In R. Lippmann, J. Moody, and D. Touretzky (eds.) </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pp. 875-882, </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference: [21] <author> Weigend, Andreas S., Bernardo A. Huberman, and David E. Rumelhart, </author> <title> "Predicting Sunspots and Exchange Rates with Connectionist Networks", </title> <editor> in Martin Cas-dagli and Stephen Eubank (eds), </editor> <title> Nonlinear Modeling and Forecasting, </title> <publisher> Addison-Wesley, </publisher> <year> 1992. </year> <month> 47 </month>
Reference-contexts: I took the data from 1700 to 1920 for a training set, and from 1921 to 1979 for a prediction set. In addition to calculating the error for the whole prediction period, I have calculated it for what <ref> [21] </ref> calls the "early" period (1921 through 1955) and the "late" period (1956 through 1979). <p> I used the average relative variance: arv = k (target k prediction k ) 2 = P k (target k mean) 2 , where mean is the mean of the complete data set, as my measure of the results, in order to compare them to the results in <ref> [21] </ref>. 3.1 Neural Network Parameters I scaled the data to a range of .1 to .9 for both the inputs and the outputs. <p> Ideally, this decision should have been made on the basis of the training set alone or a validation set <ref> [21] </ref>. 3.2 Sunspot Results name epochs training set prediction set early late CPU time ARV RMSE ARV RMSE ARV ARV (minutes) no window RCC 0.110 0.334 0.136 0.385 0.116 0.149 14 RTRL 5000 0.249 0.501 0.202 0.468 0.206 0.200 333 SRN 1000 0.190 0.438 0.202 0.468 0.195 0.206 6 with window <p> results are the average of 5 runs. 11 3.3 Analysis of Sunspot Results The choice of window size, and the number of hidden units for backpropagation, was influenced by a desire to compare my results with results for backpropagation and threshold autoregression as reported by Weigend, Rumelhart, and Huberman in <ref> [19, 21] </ref> With the training rate of .1 which they used for backpropagation, I got very unstable fluctuations in the error, but reducing the training rate produced results similar to theirs. For all networks, windowing was more important than the recurrent connections.
Reference: [22] <author> Williams, Ronald J. and Jing Peng, </author> <title> "An Efficient Gradient-Based Algorithm for On-Line Training of Recurrent Network Trajectories", </title> <booktitle> Neural Computation 2 p. </booktitle> <month> 490 </month> <year> (1990). </year>
Reference-contexts: Sequential Cascaded Network (SEQ) [16] ( see figure 2.4 ) Connects the inputs to the outputs by a single layer, whose weights are set dy namically by a matrix of context weights based on the previous output. I also did some work with a BPTT variant (Back-Propagation Through Time) <ref> [22] </ref>. I did not get this to work.
Reference: [23] <author> Williams, Ronald J. and David Zipser, </author> <title> "Experimental Analysis of the Real-time Recurrent Learning Algorithm", </title> <journal> Connection Science, </journal> <volume> vol. 1, no. </volume> <month> 1 </month> <year> (1989). </year>
Reference-contexts: Simple Recurrent Networks (SRN)[4]( see figure 2.2 ) Like backpropagation, but with the outputs of the hidden layer fed back as inputs to that layer. 3. Real-Time Recurrent Learning (RTRL) <ref> [23, 24] </ref> ( see figure 2.3 ) A single layer network, with the outputs of all units (of which there may be more than there are external outputs) fed back as input to all the units. 4.
Reference: [24] <author> Williams, Ronald J. and David Zipser, </author> <title> "A Learning Algorithm for Continually Running Fully Recurrent Networks", </title> <booktitle> Neural Computation 1, p 270 (1989). </booktitle> <pages> 48 </pages>
Reference-contexts: Simple Recurrent Networks (SRN)[4]( see figure 2.2 ) Like backpropagation, but with the outputs of the hidden layer fed back as inputs to that layer. 3. Real-Time Recurrent Learning (RTRL) <ref> [23, 24] </ref> ( see figure 2.3 ) A single layer network, with the outputs of all units (of which there may be more than there are external outputs) fed back as input to all the units. 4.
References-found: 24

