URL: http://www.ee.ogi.edu/~ericwan/PUBS/wan_nips96a.ps
Refering-URL: 
Root-URL: 
Email: ericwan@ee.ogi.edu  atnelson@ee.ogi.edu  
Title: Dual Kalman Filtering Methods for Nonlinear Prediction, Smoothing, and Estimation  
Author: Eric A. Wan Alex T. Nelson 
Address: P.O. Box 91000 Portland, OR 97124  
Affiliation: Department of Electrical Engineering Oregon Graduate Institute  
Abstract: Prediction, estimation, and smoothing are fundamental to signal processing. To perform these interrelated tasks given noisy data, we form a time series model of the process that generates the data. Taking noise in the system explicitly into account, maximum-likelihood and Kalman frameworks are discussed which involve the dual process of estimating both the model parameters and the underlying state of the system. We review several established methods in the linear case, and propose several extensions utilizing dual Kalman filters (DKF) and forward-backward (FB) filters that are applicable to neural networks. Methods are compared on several simulations of noisy time series. We also include an example of nonlinear noise reduction in speech. 
Abstract-found: 1
Intro-found: 1
Reference: <author> S.F. Boll. </author> <title> Suppression of acoustic noise in speech using spectral subtraction. </title> <journal> IEEE ASSP-27, </journal> <pages> pp. 113-120. </pages> <month> April </month> <year> 1979. </year>
Reference-contexts: The average SNR is improved by 9.94 dB. We also ran the experiment when 2 r and 2 v were estimated using only the noisy signal (Nelson and Wan, 1997), and acheived an SNR improvement of 8.50 dB. In comparison, available "state-of-the-art" techniques of spectral subtraction <ref> (Boll, 1979) </ref> and RASTA processing (Hermansky et al., 1995), achieve SNR improvements of only .65 and 1.26 dB, respectively.
Reference: <author> J. Connor, R. Martin, L. </author> <title> Atlas. Recurrent neural networks and robust time series prediction. </title> <journal> IEEE Tr. on Neural Networks. </journal> <month> March </month> <year> 1994. </year>
Reference-contexts: For nonlinear models, we use a feedforward neural network to approximate f (), and replace the LS and KF procedures by backpropagation and extended Kalman filtering, respectively <ref> (referred to here as BP-EKF, see Connor 1994) </ref>.
Reference: <editor> F. Lewis. </editor> <publisher> Optimal Estimation John Wiley & Sons, Inc. </publisher> <address> New York. </address> <year> 1986. </year>
Reference-contexts: If the model is linear, and the parameters w are known, the Kalman filter (KF) algorithm can be readily used to estimate the states <ref> (see Lewis, 1986) </ref>. At each time step, the filter computes the linear least squares estimate ^x (k) and prediction ^x (k), as well as their error covariances, P x (k) and P x (k). In the linear case with Gaussian statistics, the estimates are the minimum mean square estimates. <p> Hence, only the estimates ^x (N ) at the final time step will match. An exact equivalence for all time is achieved by combining the Kalman filter with a backwards information filter to produce a forward-backward (FB) smoothing filter <ref> (Lewis, 1986) </ref>. 2 Effectively, an inverse co-variance is propagated backwards in time to form backwards state estimates that are combined with the forward estimates. When the data set is large, the FB filter offers significant computational advantages over the batch form.
Reference: <author> G. Goodwin, </author> <title> K.S. Sin. Adaptive Filtering Prediction and Control. </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, NJ. </address> <year> 1994. </year>
Reference-contexts: The model and time series are then estimated simultaneously by applying an EKF to the nonlinear joint state equations <ref> (see Goodwin and Sin, 1994 for the linear case) </ref>. This algorithm, however, has been known to have convergence problems.
Reference: <author> H. Hermansky, E. Wan, C. Avendano. </author> <title> Speech enhancement based on temporal processing. </title> <booktitle> ICASSP Proceedings. </booktitle> <year> 1995. </year>
Reference-contexts: We also ran the experiment when 2 r and 2 v were estimated using only the noisy signal (Nelson and Wan, 1997), and acheived an SNR improvement of 8.50 dB. In comparison, available "state-of-the-art" techniques of spectral subtraction (Boll, 1979) and RASTA processing <ref> (Hermansky et al., 1995) </ref>, achieve SNR improvements of only .65 and 1.26 dB, respectively.
Reference: <author> A. Nelson, E. Wan. </author> <title> Neural speech enhancement using dual extended Kalman filtering. </title> <note> Submitted to ICNN`97. </note>
Reference: <author> L. Nelson, E. Stear. </author> <title> The simultaneous on-line estimation of parameters and states in linear systems. </title> <journal> IEEE Tr. on Automatic Control. </journal> <month> February, </month> <year> 1976. </year>
Reference-contexts: When the unknown model is linear, the observation takes the form ^ x (k 1) T w (k). Then a pair of dual Kalman filters (DKF) can be run in parallel, one for state estimation, and one for weight estimation <ref> (see Nelson, 1976) </ref>. At each time step, all current estimates are used. The dual approach essentially allows us to separate the non-linear optimization into two linear ones. Assumptions are that ^ x and ^ w remain uncorrelated and that statistics remain Gaussian.
Reference: <author> G. Puskorious, L. Feldkamp. </author> <title> Neural control of nonlinear dynamic systems with kalman filter trained recurrent networks. </title> <journal> IEEE Trn. on NN, </journal> <volume> vol. 5, no. 2. </volume> <year> 1994. </year>
Reference: <author> G. Seber, C. Wild. </author> <title> Nonlinear Regression. </title> <publisher> John Wiley & Sons. </publisher> <year> 1989. </year>
Reference-contexts: if our model was formed as an approximation of E [x (k)j ^ x (k 1)], then we should not provide it with y (k 1) as an input in order to avoid a model mismatch. 2.1 Errors-In-Variables (EIV) Methods This method comes from the statistics literature for nonlinear regression <ref> (see Seber and Wild, 1989) </ref>, and involves batch optimization of the cost function in Equation 3. Only minor modifications are made to account for the time series model.
Reference: <author> A. Weigend, H.G. </author> <type> Zimmerman. </type> <institution> Clearning. University of Colorado Computer Science Technical Report CU-CS-772-95. </institution> <month> May, </month> <year> 1995. </year>
Reference-contexts: This is equivalent to the Clearning (CLRN) cost function <ref> (Weigend, 1995) </ref>, developed as a heuristic method for cleaning the inputs in neural network modelling problems. While this allows for stochastic optimization, the assumption in the time series formulation may lead to severely biased results. Note also that no estimate is provided for the last point ^x (N ).
References-found: 10

