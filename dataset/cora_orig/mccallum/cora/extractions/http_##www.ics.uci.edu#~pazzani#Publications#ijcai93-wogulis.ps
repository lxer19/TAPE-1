URL: http://www.ics.uci.edu/~pazzani/Publications/ijcai93-wogulis.ps
Refering-URL: http://www.ics.uci.edu/~mlearn/MLPapers.html
Root-URL: 
Email: E-mail: wogulis@ics.uci.edu pazzani@ics.uci.edu  
Author: James Wogulis and Michael J. Pazzani 
Keyword: a first-order theory revision system.  
Address: Irvine, CA 92717  
Affiliation: Department of Information Computer Science University of California  
Abstract: A Methodology for Evaluating Theory Revision Systems: Results Abstract Theory revision systems are learning systems that have a goal of making small changes to an original theory to account for new data. A measure for the distance between two theories is proposed. This measure corresponds to the minimum number of edit operations at the literal level required to transform one theory into another. By computing the distance between an original theory and a revised theory, the claim that a theory revision system makes few revisions to a theory may be quantitatively evaluated. We present data using both accuracy and the distance metric on Audrey II, with Audrey II fl
Abstract-found: 1
Intro-found: 1
Reference: [ Bergadano and Giordana, 1988 ] <author> F. Bergadano and A. Gior-dana. </author> <title> A knowledge intensive approach to concept induction. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <pages> pages 305-317, </pages> <address> Ann Arbor, Michi-gan, 1988. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Paz-zani and Kibler [ 1992 ] state that the reason for providing a domain theory is "to increase the accuracy of learned rules." A variety of systems with different learning mechanisms share this objective, e.g., ML-Smart <ref> [ Bergadano and Giordana, 1988 ] </ref> , A-EBL [ Co-hen, 1992a ] , Grendel [ Cohen, 1992b ] , IOE [ Flann and Dietterich, 1989 ] , IVSM [ Hirsh, 1989 ] .
Reference: [ Cohen, 1992a ] <author> W. Cohen. </author> <title> Abductive explanation-based learning: A solution to the multiple inconsistent explanation problem. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 167-219, </pages> <year> 1992. </year>
Reference: [ Cohen, 1992b ] <author> W. Cohen. </author> <title> Compiling prior knowledge into an explicit bias. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> pages 100-110, </pages> <address> Aberdeen, Scotland, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Paz-zani and Kibler [ 1992 ] state that the reason for providing a domain theory is "to increase the accuracy of learned rules." A variety of systems with different learning mechanisms share this objective, e.g., ML-Smart [ Bergadano and Giordana, 1988 ] , A-EBL [ Co-hen, 1992a ] , Grendel <ref> [ Cohen, 1992b ] </ref> , IOE [ Flann and Dietterich, 1989 ] , IVSM [ Hirsh, 1989 ] . These systems are typically evaluated by comparing the accuracy of the learned rules with and without an initial theory.
Reference: [ Davis, 1978 ] <author> R. T. Davis. </author> <title> Model-directed learning of production rules. </title> <editor> In D. Waterman and F. Hayes-Roth, editors, </editor> <title> Pattern-directed inference systems. </title> <publisher> Academic Press, </publisher> <year> 1978. </year>
Reference-contexts: A better approach might be to provide this knowledge to a theory revision system in the form of rule models <ref> [ Davis, 1978 ] </ref> or higher-order relationships between predicates such as those expressible in CYC [ Lenat and Guha, 1990 ] . 7 Conclusion Theory revision systems have the goal of improving the accuracy of the initial theory as well as some constraints on the type and degree of revisions that
Reference: [ Flann and Dietterich, 1989 ] <author> N. Flann and T. Dietterich. </author> <title> A study of explanation-based methods for inductive learning. </title> <journal> Machine Learning, </journal> <volume> 4 </volume> <pages> 187-226, </pages> <year> 1989. </year>
Reference-contexts: state that the reason for providing a domain theory is "to increase the accuracy of learned rules." A variety of systems with different learning mechanisms share this objective, e.g., ML-Smart [ Bergadano and Giordana, 1988 ] , A-EBL [ Co-hen, 1992a ] , Grendel [ Cohen, 1992b ] , IOE <ref> [ Flann and Dietterich, 1989 ] </ref> , IVSM [ Hirsh, 1989 ] . These systems are typically evaluated by comparing the accuracy of the learned rules with and without an initial theory. Some learning systems, called theory revision systems, impose an additional constraint on the learner.
Reference: [ Hirsh, 1989 ] <author> H. Hirsh. </author> <title> Combining empirical and analytical learning with version spaces. </title> <booktitle> In Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <pages> pages 29-33, </pages> <address> Ithaca, NY, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: theory is "to increase the accuracy of learned rules." A variety of systems with different learning mechanisms share this objective, e.g., ML-Smart [ Bergadano and Giordana, 1988 ] , A-EBL [ Co-hen, 1992a ] , Grendel [ Cohen, 1992b ] , IOE [ Flann and Dietterich, 1989 ] , IVSM <ref> [ Hirsh, 1989 ] </ref> . These systems are typically evaluated by comparing the accuracy of the learned rules with and without an initial theory. Some learning systems, called theory revision systems, impose an additional constraint on the learner.
Reference: [ Kuhn, 1962 ] <author> T. Kuhn. </author> <title> The structure of scientific revolutions. </title> <publisher> University of Chicago Press, </publisher> <year> 1962. </year>
Reference-contexts: We note that using distance favors theories that make small incremental changes rather than major re-writes of the initial theory. While we feel this is desirable in general, there are situations, perhaps corresponding to paradigm shifts <ref> [ Kuhn, 1962 ] </ref> , in which an entire theory must be discarded and replaced by a drastically different theory.
Reference: [ Lawler, 1976 ] <author> E. L. Lawler. </author> <title> Combinatorial optimization: Networks and matroids. </title> <publisher> Holt, Rinehart and Winston, </publisher> <year> 1976. </year>
Reference-contexts: As we have implemented it, the metric is exponential and somewhat costly to compute but not prohibitive on the domains we have worked with. We suspect the complexity of our problem to be polynomial since it is closely related to the assignment problem <ref> [ Lawler, 1976 ] </ref> . We are in the process of determining the complexity of this problem. 4.2 Relation Between Distance and Accuracy It is important to note that the definitions of accuracy and distance are orthogonal to one another and thus complementary.
Reference: [ Lenat and Guha, 1990 ] <author> D. Lenat and R. V. Guha. </author> <title> Building large knowledge-based systems: Representation and inference in the CYC project. </title> <publisher> Addison-Wesley, </publisher> <year> 1990. </year>
Reference-contexts: A better approach might be to provide this knowledge to a theory revision system in the form of rule models [ Davis, 1978 ] or higher-order relationships between predicates such as those expressible in CYC <ref> [ Lenat and Guha, 1990 ] </ref> . 7 Conclusion Theory revision systems have the goal of improving the accuracy of the initial theory as well as some constraints on the type and degree of revisions that are desirable.
Reference: [ Mooney and Richards, 1992 ] <author> R. Mooney and B. Richards. </author> <title> Automated debugging of logic programs via theory revision. </title> <booktitle> In Proceedings of the International Workshop on Inductive Logic Programming, </booktitle> <address> Tokyo, Japan, </address> <year> 1992. </year>
Reference: [ Muggleton and Buntine, 1988 ] <author> S. Muggleton and W. Bun-tine. </author> <title> Machine invention of first-order predicates by inverting resolution. </title> <booktitle> In Proceedings of the Fifth International Workshop on Machine Learning, </booktitle> <pages> pages 339-352, </pages> <address> Ann Arbor, MI, 1988. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [ Muggleton and Feng, 1990 ] <author> S. Muggleton and C. Feng. </author> <title> Efficient induction of logic programs. </title> <booktitle> In Proceedings of the First Conference on Algorithmic Learning Theory, </booktitle> <address> Tokyo, Japan, 1990. </address> <publisher> Ohmsha. </publisher>
Reference-contexts: Finally, we compare Audrey II's performance with other theory revision systems. 2 Representation A common method used by first-order inductive learners such as Foil [ Quinlan, 1990 ] , Focl [ Pazzani and Kibler, 1992 ] , Golem <ref> [ Muggleton and Feng, 1990 ] </ref> and first-order theory revisers such as KR-Focl, Forte and Audrey is to represent domain theories as a set of first-order Horn clauses and instances as ground literals.
Reference: [ Norvig, 1992 ] <author> P. Norvig. </author> <title> Paradigms of artificial intelligence programming: Case studies in Common Lisp. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: We will ignore this complication since the theories used by theory revision systems so far do not include complex literals such as pred (f (A),[A|B]). We note that these might be handled within this framework by breaking a complex literal into several simpler ones <ref> [ Norvig, 1992 ] </ref> pred (V1,V2) & V1 = f (A) & V2 = [A|B]. 2 modulo (A; B; M ) is defined such that M is the remainder of A divided by B. Table 2: Three theories for positive, even integers.
Reference: [ Ourston and Mooney, 1990 ] <author> D. Ourston and R. Mooney. </author> <title> Changing the rules: A comprehensive approach to theory refinement. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 815-820, </pages> <address> Boston, MA, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Some learning systems, called theory revision systems, impose an additional constraint on the learner. Mooney and Richards [ 1992 ] state that the goal of theory revision systems is "to minimally modify the theory to correctly classify all of the examples." The first theory revision systems such as Either <ref> [ Ourston and Mooney, 1990 ] </ref> and KBANN [ Towell, 1991 ] were restricted to revising theories in propositional logic.
Reference: [ Pazzani and Brunk, 1991 ] <author> M. Pazzani and C. Brunk. </author> <title> Detecting and correcting errors in rule-based expert systems: An integration of empirical and explanation-based learning. </title> <journal> Knowledge Acquisition, </journal> <volume> 3 </volume> <pages> 157-173, </pages> <year> 1991. </year>
Reference-contexts: More recently, several systems such as KR-Focl <ref> [ Pazzani and Brunk, 1991 ] </ref> , Forte [ Richards and Mooney, 1991; Richards, 1992 ] and Audrey [ Wogulis, 1991 ] have been introduced that revise theories expressed as Horn clauses. In spite of the fl This research was supported in part by grant number F49620-92-J-0430 from AFOSR.
Reference: [ Pazzani and Kibler, 1992 ] <author> M. Pazzani and D. Kibler. </author> <title> The utility of knowledge in inductive learning. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 57-94, </pages> <year> 1992. </year>
Reference-contexts: Finally, we compare Audrey II's performance with other theory revision systems. 2 Representation A common method used by first-order inductive learners such as Foil [ Quinlan, 1990 ] , Focl <ref> [ Pazzani and Kibler, 1992 ] </ref> , Golem [ Muggleton and Feng, 1990 ] and first-order theory revisers such as KR-Focl, Forte and Audrey is to represent domain theories as a set of first-order Horn clauses and instances as ground literals.
Reference: [ Quinlan, 1990 ] <author> J. R. Quinlan. </author> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 239-266, </pages> <year> 1990. </year>
Reference-contexts: Finally, we compare Audrey II's performance with other theory revision systems. 2 Representation A common method used by first-order inductive learners such as Foil <ref> [ Quinlan, 1990 ] </ref> , Focl [ Pazzani and Kibler, 1992 ] , Golem [ Muggleton and Feng, 1990 ] and first-order theory revisers such as KR-Focl, Forte and Audrey is to represent domain theories as a set of first-order Horn clauses and instances as ground literals.
Reference: [ Richards and Mooney, 1991 ] <author> B. Richards and R. Mooney. </author> <title> First-order theory revision. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <pages> pages 447-451, </pages> <address> Evanston, IL, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: More recently, several systems such as KR-Focl [ Pazzani and Brunk, 1991 ] , Forte <ref> [ Richards and Mooney, 1991; Richards, 1992 ] </ref> and Audrey [ Wogulis, 1991 ] have been introduced that revise theories expressed as Horn clauses. In spite of the fl This research was supported in part by grant number F49620-92-J-0430 from AFOSR.
Reference: [ Richards, 1992 ] <author> B. Richards. </author> <title> An operator-based approach to first-order theory revision. </title> <type> PhD thesis, </type> <institution> University of Texas, Austin, TX, </institution> <year> 1992. </year>
Reference-contexts: More recently, several systems such as KR-Focl [ Pazzani and Brunk, 1991 ] , Forte <ref> [ Richards and Mooney, 1991; Richards, 1992 ] </ref> and Audrey [ Wogulis, 1991 ] have been introduced that revise theories expressed as Horn clauses. In spite of the fl This research was supported in part by grant number F49620-92-J-0430 from AFOSR.
Reference: [ Shasha and Zhang, 1990 ] <author> D. Shasha and K. Zhang. </author> <title> Fast algorithms for the unit cost editing distance between trees. </title> <journal> Journal of Algorithms, </journal> <volume> 11 </volume> <pages> 581-621, </pages> <year> 1990. </year>
Reference-contexts: The measure is defined as the number of edit operations (i.e., additions, deletions or replacements of literals) that are needed to transform one theory into another. The notion of edit-distance has been studied by others <ref> [ Tai, 1979; Shasha and Zhang, 1990 ] </ref> and applied to problems in areas such as pattern matching, parsing, and comparing secondary structures of RNA.
Reference: [ Swartout, 1981 ] <author> W. Swartout. </author> <title> Explaining and justifying in expert consulting programs. </title> <booktitle> In Proceedings of the Seventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 203-208, </pages> <address> Vancouver, B.C., 1981. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In contrast, a theory revision system should use the abstract predicates from the initial theory, provided they are useful in making classifications. * Abstractions from the initial theory that are retained in the revised theory can support the generation of meaningful natural language explanations of the system's translations <ref> [ Swartout, 1981 ] </ref> . We note that using distance favors theories that make small incremental changes rather than major re-writes of the initial theory.
Reference: [ Tai, 1979 ] <author> K. C. Tai. </author> <title> The tree-to-tree correction problem. </title> <journal> J. Assoc. Comput. Mach., </journal> <volume> 26 </volume> <pages> 422-433, </pages> <year> 1979. </year>
Reference-contexts: The measure is defined as the number of edit operations (i.e., additions, deletions or replacements of literals) that are needed to transform one theory into another. The notion of edit-distance has been studied by others <ref> [ Tai, 1979; Shasha and Zhang, 1990 ] </ref> and applied to problems in areas such as pattern matching, parsing, and comparing secondary structures of RNA.
Reference: [ Towell, 1991 ] <author> G. Towell. </author> <title> Symbolic knowledge and neural networks: Insertion, refinement and extraction. </title> <type> PhD thesis, </type> <institution> University of Wisconsin, Madison, WI, </institution> <year> 1991. </year>
Reference-contexts: Mooney and Richards [ 1992 ] state that the goal of theory revision systems is "to minimally modify the theory to correctly classify all of the examples." The first theory revision systems such as Either [ Ourston and Mooney, 1990 ] and KBANN <ref> [ Towell, 1991 ] </ref> were restricted to revising theories in propositional logic.
Reference: [ Wogulis, 1991 ] <author> J. Wogulis. </author> <title> Revising relational domain theories. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <pages> pages 462-466, </pages> <address> Evanston, IL, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: More recently, several systems such as KR-Focl [ Pazzani and Brunk, 1991 ] , Forte [ Richards and Mooney, 1991; Richards, 1992 ] and Audrey <ref> [ Wogulis, 1991 ] </ref> have been introduced that revise theories expressed as Horn clauses. In spite of the fl This research was supported in part by grant number F49620-92-J-0430 from AFOSR. <p> The process repeats on any remaining uncovered positives until all are covered. To cover the example, Audrey II first applies the abduc-tive process used in Audrey <ref> [ Wogulis, 1991 ] </ref> to find a single literal to assume that, if true, would allow the example to be explained. <p> If deleting the literal from a clause decreases the coverage of the positive examples, then Audrey II tries replacing the 3 Audrey II differs from Audrey <ref> [ Wogulis, 1991 ] </ref> primarily in its use of a Foil-like component for inductive learning rather than forming least-general generalizations. Also, Au-drey II can specialize clauses by adding literals which Audrey wasn't able to do. literal with a new conjunction of literals determined by Foil.
References-found: 24

