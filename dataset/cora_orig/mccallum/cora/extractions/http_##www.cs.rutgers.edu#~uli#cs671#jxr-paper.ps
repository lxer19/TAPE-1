URL: http://www.cs.rutgers.edu/~uli/cs671/jxr-paper.ps
Refering-URL: http://www.cs.rutgers.edu/~uli/cs671/index.html
Root-URL: http://www.cs.rutgers.edu
Email: E-mail: jxr@ee.lsu.edu  
Title: Software pipelining of nested loops  "Optimal Software Pipelining of Nested Loops,"  
Author: J. Ramanujam 
Keyword: Instruction level parallelism, fine-grain scheduling, nested loops, software pipelining, optimal scheduling.  
Note: Note: This is an expanded version of the paper titled  that appeared in Proc. 8th International Parallel Processing Symposium,  pp. 335-342. Supported in part by an NSF Young Investigator Award CCR-9457768, and NSF grant CCR-9210422, and by the Louisiana Board of Regents through contract LEQSF (1991-94)-RD-A-09.  
Date: May 1994  (April 1994),  
Address: LA 70803  
Affiliation: Dept. of Electrical and Computer Engineering Louisiana State University, Baton Rouge,  
Abstract: This paper presents an approach to software pipelining of nested loops. While several papers have addressed software pipelining of inner loops, little work has been done in the area of extending it to nested loops. This paper solves the problem of finding the minimum iteration initiation interval (in the absence of resource constraints) for each level of a nested loop. The problem is formulated as one of finding a rational quasi-affine schedule for each statement in the body of a perfectly nested loop which is then solved using linear programming. This allows us to treat iteration-dependent statement reordering and multidimensional loop unrolling in the same framework. Unlike most work in scheduling nested loops, we treat each statement in the body as a unit of scheduling. Thus, the schedules derived allow for instances of statements from different iterations to be scheduled at the same time. Optimal schedules derived here subsume extant work on software pipelining of inner loops, in the absence of resource constraints. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Aiken. </author> <title> Compaction based parallelization. </title> <type> (Ph.D. thesis). Technical Report 88-922, </type> <institution> Cornell University, </institution> <year> 1988. </year>
Reference-contexts: Iwano and Yeh [22] use network flow algorithms for optimal loops parallelization. Software pipelining of sequential loops on limited resources is discussed in <ref> [1, 11, 15, 25] </ref>. Software pipelining of nested loops 5 Trace scheduling [7, 12, 13] is a technique used in VLIW machines that extracts parallelism in sequential loops by unrolling it several times. <p> Nicolau [29] suggests loop quantization as a technique for multidimensional loop unrolling in conjunction with tree-height reduction and percolation scheduling. He does not consider the problem of determining the optimal initiation interval for each loop. Loop quantization as described in <ref> [1, 29] </ref> deals with the problem at the iteration level rather than at the statement level. Recently, Gao et al. [17] present a technique that works for rectangular loops but requires all components of all distance vectors to be positive. <p> The solutions derived give the minimum iteration initiation interval for each level of an n-nested loop. Let G denote the statement level dependence graph. If G is acyclic, then list scheduling and tree height reduction can be used to optimally schedule the computation <ref> [1] </ref>. If G is cyclic, we use Tarjan's algorithm [36, 40, 42] to find all the strongly connected components and schedule each strongly connected component separately. For the rest of the paper, we discuss the optimal scheduling of a single strongly connected component in G. <p> ; : : : ; ffi r that minimizes P n j=1 jh j j (U j L j ) subject to dependence constraints: Minimize n X jh j j (U j L j ) subject to h d k;l ffi k ffi l + t k k; l 2 <ref> [1; r] </ref> for every edge in G. In many cases, the loop limits, though constants, are not known at compile time. In such cases, we aim at finding optimal schedules independent of the loop limits. <p> With unknown loop limits, our problem then is that of finding a schedule hh; ffii that minimizes P n j=1 jh j j subject to dependence constraints: Minimize n X jh j j subject to h d k;l ffi k ffi l + t k k; l 2 <ref> [1; r] </ref> for every edge in G. The above formulation is not in standard linear programming form for two reasons: 1. Lack of non-negativity constraints on hh; ffii 2. <p> these modifications, the problem is now in standard Linear Programming (LP) form: Minimize n X j subject to j h 1 j 0 j = 1; : : : ; n j h 2 n X j h 2 k;l k ffi 2 l ffi 2 where k; l 2 <ref> [1; r] </ref> ^ (k; l) 2 edges (G). The formulation has 2n + m constraints with 3n + 2r variables where m is the number of edges in G for an n-nested loop with r statements. <p> Earlier we had mentioned that we schedule strongly connected components separately. Next, we show an example that illustrates how we can interleave strongly connected components; we use the following example from page 124 in <ref> [1] </ref>: Example 3: for i = 1 to N do B : B [i] = f 2 (A [i]; D [i 1]) D : D [i] = f 4 (B [i]; C [i]) endfor The statement level dependence graph is shown in Figure 3. <p> In addition, our technique produces the optimal solution for codes such as the ones on pages 131 and 138 of <ref> [1] </ref>, both of which require interleaving of strongly connected components in scheduling. <p> This can be generalized to nested loops using loop quantization. Cytron [9, 10] addresses the same problem. Aiken and Nicolau <ref> [1, 29] </ref> use loop quantization and percolation scheduling. We are working on mitred quantizations [1] that keep all the available processors busy. The problem is related to iteration space tiling of nested loops [21, 30]. We are also working towards integrating conditionals in nested loops. <p> This can be generalized to nested loops using loop quantization. Cytron [9, 10] addresses the same problem. Aiken and Nicolau [1, 29] use loop quantization and percolation scheduling. We are working on mitred quantizations <ref> [1] </ref> that keep all the available processors busy. The problem is related to iteration space tiling of nested loops [21, 30]. We are also working towards integrating conditionals in nested loops.
Reference: [2] <author> A. Aiken and A. Nicolau. </author> <title> Optimal loop parallelization. </title> <booktitle> In Proc. ACM SIGPLAN Conference on Programming Languages Design and Implementation, </booktitle> <month> June </month> <year> 1988. </year>
Reference-contexts: Moreover, these approaches use an ad hoc method to decide on the degree of loop unrolling, and are unacceptable in cases where the optimal solution can be found only after a very large amount of unrolling. Aiken and Nicolau <ref> [2] </ref>, describe a procedure which yields an optimal schedule for inner sequential loops. The procedure works by simulating the execution of the loop body until a pattern evolves. The technique does not guarantee an upper bound on the amount of time it needs to find a solution. <p> Zaky and Sadayappan [41] present a novel approach that is based on eigenvalues of matrices that arise path algebra. Their algorithm has polynomial time complexity; their algorithm exploits the connectivity properties of the loop dependence graph. While the algorithm of <ref> [2] </ref> requires unrolling to detect a pattern, the algorithm in [41] does not require any unrolling. In Section 5.1, we show that the technique developed in this paper for nested loops is equally applicable to inner loops and derives the same solution as [41] using a simpler framework.
Reference: [3] <author> A. Aiken and A. Nicolau. </author> <title> A realistic resource-constrained software pipelining algorithm. </title> <booktitle> In Proc. 3rd Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Irvine, CA, </address> <month> August </month> <year> 1990. </year>
Reference: [4] <author> R. Allen and K. Kennedy. </author> <title> Automatic translation of FORTRAN programs to vector form. </title> <journal> ACM Trans. Programming Languages and Systems, </journal> <volume> 9(4) </volume> <pages> 491-542, </pages> <year> 1987. </year>
Reference-contexts: 1 Introduction Exploiting parallelism in loops in scientific programs is an important factor in realizing the potential performance of highly parallel computers today. Programming these machines remains a difficult problem. Much progress has been made resulting in a suite of techniques that extract coarse-grain parallelism from sequential programs <ref> [4, 28, 40, 42] </ref>. With the advent of VLIW, superscalar, horizontal microengines, multiple RISC and pipelined processors, the exploitation of fine-grain instruction-level parallelism has become a major challenge to parallelizing compilers [7, 13, 20, 33, 37, 38]. <p> In Section 6, we discuss work in progress aimed at integrating resource constraints and handling conditionals. Section 7 concludes with a discussion. 2 Background 2.1 Data dependence Good and thorough parallelization of a program critically depends on how precisely a compiler can discover the data dependence information <ref> [4, 5, 39, 40, 42] </ref>. These dependences imply precedence constraints among computations which have to be satisfied for a correct execution. <p> A control dependence exists between a statement with a conditional jump and another statement if the conditional jump statement controls the execution of the other statement. Control dependences can be handled by methods similar to data dependences <ref> [4] </ref>. In our analysis, we treat the different types of dependences uniformly. <p> Control dependences can be handled by methods similar to data dependences [4]. In our analysis, we treat the different types of dependences uniformly. Methods to calculate data dependence vectors can be found in <ref> [4, 5, 39, 40, 42] </ref>. 3 Related Work Software pipelining of inner loops has been considered by several authors [1, 6, 11, 14, 15, 16, 23, 24, 25, 27, 32, 35, 41]. All of these studies search for the minimum initiation interval by unrolling the loop several times.
Reference: [5] <author> U. Banerjee. </author> <title> Dependence analysis for supercomputing, </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, MA, </address> <year> 1988. </year>
Reference-contexts: In Section 6, we discuss work in progress aimed at integrating resource constraints and handling conditionals. Section 7 concludes with a discussion. 2 Background 2.1 Data dependence Good and thorough parallelization of a program critically depends on how precisely a compiler can discover the data dependence information <ref> [4, 5, 39, 40, 42] </ref>. These dependences imply precedence constraints among computations which have to be satisfied for a correct execution. <p> The loop iterations are executed in lexicographic ordering during sequential execution. Any vector x = (x 1 ; x 2 ; : : : ; x n ) is a positive vector, if its first (leading read from left to right) non-zero component is positive <ref> [5] </ref>. We say that i = (i 1 ; : : : ; i n ) precedes j = (j 1 ; : : : ; i n ), written i j, if j i is a positive vector. <p> Given two statements S k (I 1 ) and S l (I 2 ), S l (I 2 ) is dependent on S k (I 1 ) (with distance vector d k;l ) iff <ref> [5, 28, 39, 40] </ref>: * Either X (F (I 1 )) is written in statement S k (I 1 ) or X (G (I 2 )) is written in statement S l (I 2 ): A flow dependence exists from statement S k to statement S l if S k writes <p> Control dependences can be handled by methods similar to data dependences [4]. In our analysis, we treat the different types of dependences uniformly. Methods to calculate data dependence vectors can be found in <ref> [4, 5, 39, 40, 42] </ref>. 3 Related Work Software pipelining of inner loops has been considered by several authors [1, 6, 11, 14, 15, 16, 23, 24, 25, 27, 32, 35, 41]. All of these studies search for the minimum initiation interval by unrolling the loop several times. <p> If the iteration space is rectangular, i.e., L j and U j (j = 1; : : : ; n) are integer constants, we can find an expression of the Software pipelining of nested loops 7 optimal value of E using Banerjee's inequalities <ref> [5] </ref> as discussed below. Definition 1 [5]: Given a number h, its positive part, h + = max (h; 0); and its negative part, h = max (h; 0). <p> If the iteration space is rectangular, i.e., L j and U j (j = 1; : : : ; n) are integer constants, we can find an expression of the Software pipelining of nested loops 7 optimal value of E using Banerjee's inequalities <ref> [5] </ref> as discussed below. Definition 1 [5]: Given a number h, its positive part, h + = max (h; 0); and its negative part, h = max (h; 0).
Reference: [6] <author> A. Charlesworth. </author> <title> An approach to scientific array processing: The architectural design of the AP-120B/FPS-164 family. </title> <journal> Computer, </journal> <volume> 14(3) </volume> <pages> 18-27, </pages> <year> 1981. </year>
Reference: [7] <author> R. Colwell, R. Nix, J. O'Donnell, D. Papworth, and P. Rodman. </author> <title> A VLIW architecture for a trace scheduling compiler. </title> <journal> IEEE Trans. Comput., </journal> <volume> C-37(8):967-979, </volume> <month> August </month> <year> 1988. </year>
Reference-contexts: Much progress has been made resulting in a suite of techniques that extract coarse-grain parallelism from sequential programs [4, 28, 40, 42]. With the advent of VLIW, superscalar, horizontal microengines, multiple RISC and pipelined processors, the exploitation of fine-grain instruction-level parallelism has become a major challenge to parallelizing compilers <ref> [7, 13, 20, 33, 37, 38] </ref>. The problem will become even more important as these processors form the building blocks of massively parallel machines. <p> Iwano and Yeh [22] use network flow algorithms for optimal loops parallelization. Software pipelining of sequential loops on limited resources is discussed in [1, 11, 15, 25]. Software pipelining of nested loops 5 Trace scheduling <ref> [7, 12, 13] </ref> is a technique used in VLIW machines that extracts parallelism in sequential loops by unrolling it several times. The code for one iteration of the unrolled loop is then compacted using the acyclic dependence graph corresponding to only the code body of the unrolled loop.
Reference: [8] <author> D. Culler and Arvind. </author> <title> Resource requirements for dataflow programs. </title> <booktitle> In Proc. International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1988. </year>
Reference-contexts: Since scheduling in the presence of resource constraints is NP-complete for inner loops, the problem is NP-complete for nested loops as well. In the area of dataflow machines, Culler <ref> [8] </ref> has proposed a technique known as loop bounding, which limits the number of iterations that can be active (i.e., started but not yet finished) at any time. This can be generalized to nested loops using loop quantization. Cytron [9, 10] addresses the same problem.
Reference: [9] <author> R. Cytron. </author> <title> Compile-time scheduling and optimization for asynchronous machines. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, Urbana, Illinois, </institution> <year> 1984. </year> <title> Software pipelining of nested loops 15 </title>
Reference-contexts: Weiss and Smith [38] discuss an adaptation of this technique for pipelined supercomputers. While software pipelining of inner loops has received a lot of attention, very few authors have addressed software pipelining of nested loops. Cytron <ref> [9, 10] </ref> presents a technique for doacross loops that minimizes the delays between initiating successive iterations of a sequential loop with no reordering of statements in its body. Cytron [9, 10] does not explicitly attempt to exploit fine-grain parallelism. <p> Cytron <ref> [9, 10] </ref> presents a technique for doacross loops that minimizes the delays between initiating successive iterations of a sequential loop with no reordering of statements in its body. Cytron [9, 10] does not explicitly attempt to exploit fine-grain parallelism. Munshi and Simmons [27] study the problem of minimizing the iteration initiation interval which considers statement reordering. They show that a close variant of the problem is NP-complete. <p> In the area of dataflow machines, Culler [8] has proposed a technique known as loop bounding, which limits the number of iterations that can be active (i.e., started but not yet finished) at any time. This can be generalized to nested loops using loop quantization. Cytron <ref> [9, 10] </ref> addresses the same problem. Aiken and Nicolau [1, 29] use loop quantization and percolation scheduling. We are working on mitred quantizations [1] that keep all the available processors busy. The problem is related to iteration space tiling of nested loops [21, 30].
Reference: [10] <author> R. Cytron. </author> <title> doacross: Beyond vectorization for multiprocessors. </title> <booktitle> Proc. 1986 International Conference on Parallel Processing, </booktitle> <pages> pp. 836-844, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: Weiss and Smith [38] discuss an adaptation of this technique for pipelined supercomputers. While software pipelining of inner loops has received a lot of attention, very few authors have addressed software pipelining of nested loops. Cytron <ref> [9, 10] </ref> presents a technique for doacross loops that minimizes the delays between initiating successive iterations of a sequential loop with no reordering of statements in its body. Cytron [9, 10] does not explicitly attempt to exploit fine-grain parallelism. <p> Cytron <ref> [9, 10] </ref> presents a technique for doacross loops that minimizes the delays between initiating successive iterations of a sequential loop with no reordering of statements in its body. Cytron [9, 10] does not explicitly attempt to exploit fine-grain parallelism. Munshi and Simmons [27] study the problem of minimizing the iteration initiation interval which considers statement reordering. They show that a close variant of the problem is NP-complete. <p> In the area of dataflow machines, Culler [8] has proposed a technique known as loop bounding, which limits the number of iterations that can be active (i.e., started but not yet finished) at any time. This can be generalized to nested loops using loop quantization. Cytron <ref> [9, 10] </ref> addresses the same problem. Aiken and Nicolau [1, 29] use loop quantization and percolation scheduling. We are working on mitred quantizations [1] that keep all the available processors busy. The problem is related to iteration space tiling of nested loops [21, 30].
Reference: [11] <author> C. Eisenbeis. </author> <title> Optimization of horizontal microcode generation for loop structures. </title> <booktitle> In Proc. 2nd ACM International Conference on Supercomputing, </booktitle> <pages> pp. 453-465, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Iwano and Yeh [22] use network flow algorithms for optimal loops parallelization. Software pipelining of sequential loops on limited resources is discussed in <ref> [1, 11, 15, 25] </ref>. Software pipelining of nested loops 5 Trace scheduling [7, 12, 13] is a technique used in VLIW machines that extracts parallelism in sequential loops by unrolling it several times.
Reference: [12] <author> J. Fisher. </author> <title> Trace scheduling: A technique for global microcode compaction. </title> <journal> IEEE Trans. Comput., </journal> <volume> C-30(7):478-490, </volume> <month> July </month> <year> 1971. </year>
Reference-contexts: Iwano and Yeh [22] use network flow algorithms for optimal loops parallelization. Software pipelining of sequential loops on limited resources is discussed in [1, 11, 15, 25]. Software pipelining of nested loops 5 Trace scheduling <ref> [7, 12, 13] </ref> is a technique used in VLIW machines that extracts parallelism in sequential loops by unrolling it several times. The code for one iteration of the unrolled loop is then compacted using the acyclic dependence graph corresponding to only the code body of the unrolled loop.
Reference: [13] <author> J. Fisher. </author> <title> Very long instruction word architectures and the ELI-512. </title> <booktitle> In Proc. 10th International Symposium on Computer Architecture, </booktitle> <pages> pp. 140-150, </pages> <month> June </month> <year> 1983. </year>
Reference-contexts: Much progress has been made resulting in a suite of techniques that extract coarse-grain parallelism from sequential programs [4, 28, 40, 42]. With the advent of VLIW, superscalar, horizontal microengines, multiple RISC and pipelined processors, the exploitation of fine-grain instruction-level parallelism has become a major challenge to parallelizing compilers <ref> [7, 13, 20, 33, 37, 38] </ref>. The problem will become even more important as these processors form the building blocks of massively parallel machines. <p> Iwano and Yeh [22] use network flow algorithms for optimal loops parallelization. Software pipelining of sequential loops on limited resources is discussed in [1, 11, 15, 25]. Software pipelining of nested loops 5 Trace scheduling <ref> [7, 12, 13] </ref> is a technique used in VLIW machines that extracts parallelism in sequential loops by unrolling it several times. The code for one iteration of the unrolled loop is then compacted using the acyclic dependence graph corresponding to only the code body of the unrolled loop.
Reference: [14] <author> K. Ebcioglu. </author> <title> A compilation technique for software pipelining of loops with conditional jumps. </title> <booktitle> In Proc. 20th Annual Workshop on Microprogramming, </booktitle> <month> December </month> <year> 1987. </year>
Reference: [15] <author> K. Ebcioglu and A. Nicolau. </author> <title> A global resource-constrained parallelization technique. </title> <booktitle> In Proc. ACM International Conference on Supercomputing, </booktitle> <month> June </month> <year> 1989. </year>
Reference-contexts: Iwano and Yeh [22] use network flow algorithms for optimal loops parallelization. Software pipelining of sequential loops on limited resources is discussed in <ref> [1, 11, 15, 25] </ref>. Software pipelining of nested loops 5 Trace scheduling [7, 12, 13] is a technique used in VLIW machines that extracts parallelism in sequential loops by unrolling it several times.
Reference: [16] <author> G. Gao, Y. Wong, and Q. Ning. </author> <title> A Petri-Net model for fine-grain loop scheduling. </title> <booktitle> In Proc. ACM SIGPLAN Conf. Programming Language Design and Implementation, </booktitle> <pages> pp. 204-218, </pages> <address> Toronto, Canada, </address> <month> June </month> <year> 1991. </year>
Reference: [17] <author> G. Gao, Q. Ning and V. van Dongen. </author> <title> Extending software pipelining for scheduling nested loops. </title> <booktitle> In Proc. 6th Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: He does not consider the problem of determining the optimal initiation interval for each loop. Loop quantization as described in [1, 29] deals with the problem at the iteration level rather than at the statement level. Recently, Gao et al. <ref> [17] </ref> present a technique that works for rectangular loops but requires all components of all distance vectors to be positive. <p> While unimodular transformations could be used to convert all distance vectors to have non-negative entries, the transformed iteration spaces are no longer rectangular; this limits the applicability of the results in <ref> [17] </ref>.
Reference: [18] <author> F. Gasperoni. </author> <title> Compilation techniques for VLIW architectures. </title> <type> Technical Report 435, </type> <institution> Department of Computer Science, </institution> <address> New York University, </address> <month> March </month> <year> 1989. </year>
Reference-contexts: The fine grained solution runs 3:3 times faster than the best solution that can be obtained using hyperplane technique [26]. 5.1 Application to inner loops The method presented here is equally applicable to inner loops. Consider the following example from page 45 in <ref> [18] </ref>. Example 2: for i = 1 to N do Software pipelining of nested loops 11 S 1 : A [i] = C [i 1] S 3 : C [i] = B [i] endfor The statement level dependence graph for Example 2 is shown in Figure 2.
Reference: [19] <author> Saul I. Gass. </author> <title> Linear programming, methods and applications. </title> <publisher> McGraw-Hill Book Company, </publisher> <address> fourth edition, </address> <year> 1975. </year>
Reference-contexts: technique presented in this paper does not have the restriction on non-negativity and hence is more general. 4 Statement level rational affine schedules In this section, we formulate the problem of optimal fine grain scheduling of nested loops in the absence of resource constraints as a Linear Programming (LP) problem <ref> [19, 34] </ref> which admits polynomial time solutions and is extremely fast in practice. This paper generalizes the hyperplane scheduling technique of scheduling iterations of nested loops pioneered by Lamport [26] by deriving optimal schedules at the statement level rather than at the iteration level.
Reference: [20] <author> J. L. Hennessy and D. A. Patterson. </author> <title> Computer architecture: A quantitative approach, </title> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1990. </year>
Reference-contexts: Much progress has been made resulting in a suite of techniques that extract coarse-grain parallelism from sequential programs [4, 28, 40, 42]. With the advent of VLIW, superscalar, horizontal microengines, multiple RISC and pipelined processors, the exploitation of fine-grain instruction-level parallelism has become a major challenge to parallelizing compilers <ref> [7, 13, 20, 33, 37, 38] </ref>. The problem will become even more important as these processors form the building blocks of massively parallel machines.
Reference: [21] <author> F. Irigoin and R. Triolet. </author> <title> Supernode Partitioning. </title> <booktitle> Proc. 15th Annual ACM Symp. Principles of Programming Languages, </booktitle> <address> San Diego, CA, </address> <month> Jan. </month> <year> 1988, </year> <pages> pp. 319-329. </pages>
Reference-contexts: Cytron [9, 10] addresses the same problem. Aiken and Nicolau [1, 29] use loop quantization and percolation scheduling. We are working on mitred quantizations [1] that keep all the available processors busy. The problem is related to iteration space tiling of nested loops <ref> [21, 30] </ref>. We are also working towards integrating conditionals in nested loops.
Reference: [22] <author> K. Iwano and Yeh. </author> <title> An efficient algorithm for optimal loop parallelization. </title> <booktitle> Lecture Notes in Comp. Sci., </booktitle> <volume> No. 450, </volume> <publisher> Springer-Verlag, </publisher> <year> 1990, </year> <pages> pp. 201-210. </pages>
Reference-contexts: In Section 5.1, we show that the technique developed in this paper for nested loops is equally applicable to inner loops and derives the same solution as [41] using a simpler framework. Iwano and Yeh <ref> [22] </ref> use network flow algorithms for optimal loops parallelization. Software pipelining of sequential loops on limited resources is discussed in [1, 11, 15, 25].
Reference: [23] <author> R. Jones and H. Allan. </author> <title> Software pipelining: A comparison and improvement. </title> <booktitle> In Proc. 23rd Annual Workshop on Microprogramming and Microarchitectures, </booktitle> <pages> pp. 46-56, </pages> <address> Orlando, Florida, </address> <month> November </month> <year> 1990. </year>
Reference: [24] <author> M. Lam. </author> <title> A systolic array optimizing compiler. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <month> May </month> <year> 1987. </year>
Reference: [25] <author> M. Lam. </author> <title> Software pipelining: An effective scheduling technique for VLIW machines. </title> <booktitle> In Proc. ACM SIGPLAN Conf. Programming Languages Design and Implementation, </booktitle> <pages> pp. 318-328, </pages> <address> Atlanta, GA, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: Iwano and Yeh [22] use network flow algorithms for optimal loops parallelization. Software pipelining of sequential loops on limited resources is discussed in <ref> [1, 11, 15, 25] </ref>. Software pipelining of nested loops 5 Trace scheduling [7, 12, 13] is a technique used in VLIW machines that extracts parallelism in sequential loops by unrolling it several times.
Reference: [26] <author> L. Lamport. </author> <title> The Parallel Execution of DO Loops. </title> <journal> Communications of the ACM, </journal> <volume> 17(2) </volume> <pages> 83-93, </pages> <month> Feb. </month> <year> 1974. </year> <title> Software pipelining of nested loops 16 </title>
Reference-contexts: This paper generalizes the hyperplane scheduling technique of scheduling iterations of nested loops pioneered by Lamport <ref> [26] </ref> by deriving optimal schedules at the statement level rather than at the iteration level. The solutions derived give the minimum iteration initiation interval for each level of an n-nested loop. Let G denote the statement level dependence graph. <p> The best execution time that can be derived using only the iteration space distance vectors is 6N for the schedule 5i + j. The fine grained solution runs 3:3 times faster than the best solution that can be obtained using hyperplane technique <ref> [26] </ref>. 5.1 Application to inner loops The method presented here is equally applicable to inner loops. Consider the following example from page 45 in [18]. <p> The fine grained solution runs 2 times faster than the best solution that can be obtained using the hyperplane technique <ref> [26] </ref>. Earlier we had mentioned that we schedule strongly connected components separately.
Reference: [27] <author> A. Munshi and B. Simons. </author> <title> Scheduling sequential loops on parallel processors. </title> <booktitle> In Proc. ACM International Conference on Supercomputing, </booktitle> <pages> pp. 392-415, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: Cytron [9, 10] presents a technique for doacross loops that minimizes the delays between initiating successive iterations of a sequential loop with no reordering of statements in its body. Cytron [9, 10] does not explicitly attempt to exploit fine-grain parallelism. Munshi and Simmons <ref> [27] </ref> study the problem of minimizing the iteration initiation interval which considers statement reordering. They show that a close variant of the problem is NP-complete. Both these papers separate the issues of iteration initiation and the scheduling of operations within an iteration.
Reference: [28] <author> D. Padua and M. Wolfe. </author> <title> Advanced compiler optimizations for supercomputers. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1184-1201, </pages> <month> Dec. </month> <year> 1986. </year>
Reference-contexts: 1 Introduction Exploiting parallelism in loops in scientific programs is an important factor in realizing the potential performance of highly parallel computers today. Programming these machines remains a difficult problem. Much progress has been made resulting in a suite of techniques that extract coarse-grain parallelism from sequential programs <ref> [4, 28, 40, 42] </ref>. With the advent of VLIW, superscalar, horizontal microengines, multiple RISC and pipelined processors, the exploitation of fine-grain instruction-level parallelism has become a major challenge to parallelizing compilers [7, 13, 20, 33, 37, 38]. <p> Given two statements S k (I 1 ) and S l (I 2 ), S l (I 2 ) is dependent on S k (I 1 ) (with distance vector d k;l ) iff <ref> [5, 28, 39, 40] </ref>: * Either X (F (I 1 )) is written in statement S k (I 1 ) or X (G (I 2 )) is written in statement S l (I 2 ): A flow dependence exists from statement S k to statement S l if S k writes
Reference: [29] <author> A. Nicolau. </author> <title> Loop quantization: A generalized loop unwinding technique. </title> <journal> J. Parallel and Dist. Comput., </journal> <volume> 5(5) </volume> <pages> 568-586, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: They show that a close variant of the problem is NP-complete. Both these papers separate the issues of iteration initiation and the scheduling of operations within an iteration. In general, such a separation does not result in the minimum iteration initiation interval. Nicolau <ref> [29] </ref> suggests loop quantization as a technique for multidimensional loop unrolling in conjunction with tree-height reduction and percolation scheduling. He does not consider the problem of determining the optimal initiation interval for each loop. <p> Nicolau [29] suggests loop quantization as a technique for multidimensional loop unrolling in conjunction with tree-height reduction and percolation scheduling. He does not consider the problem of determining the optimal initiation interval for each loop. Loop quantization as described in <ref> [1, 29] </ref> deals with the problem at the iteration level rather than at the statement level. Recently, Gao et al. [17] present a technique that works for rectangular loops but requires all components of all distance vectors to be positive. <p> This can be generalized to nested loops using loop quantization. Cytron [9, 10] addresses the same problem. Aiken and Nicolau <ref> [1, 29] </ref> use loop quantization and percolation scheduling. We are working on mitred quantizations [1] that keep all the available processors busy. The problem is related to iteration space tiling of nested loops [21, 30]. We are also working towards integrating conditionals in nested loops.
Reference: [30] <author> J. Ramanujam and P. Sadayappan. </author> <title> Tiling multidimensional iteration spaces for nonshared memory machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 16(2) </volume> <pages> 108-120, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Cytron [9, 10] addresses the same problem. Aiken and Nicolau [1, 29] use loop quantization and percolation scheduling. We are working on mitred quantizations [1] that keep all the available processors busy. The problem is related to iteration space tiling of nested loops <ref> [21, 30] </ref>. We are also working towards integrating conditionals in nested loops.
Reference: [31] <author> J. Ramanujam. </author> <title> Optimal multidimensional loop unwinding: A framework for software pipelin-ing of nested loops. </title> <type> Technical Report TR-93-05-01, </type> <institution> Dept. of Electrical and Computer Engineering, Louisiana State University, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: Optimal schedules derived here subsume extant work on software pipelining of inner loops. Due to space constraints, we do not discuss code generation issues in this paper; the reader is referred to <ref> [31] </ref> for details. Section 2 provides the background and Section 3 discusses related work, placing this research in the context of extant work in the field. <p> In addition, our technique produces the optimal solution for codes such as the ones on pages 131 and 138 of [1], both of which require interleaving of strongly connected components in scheduling. Due to lack of space, we do not present these solutions here; see <ref> [31] </ref> for details. 6 Work in progress We briefly discuss here our effort at integrating resource constraints such as the number of processors, functional units etc. Since scheduling in the presence of resource constraints is NP-complete for inner loops, the problem is NP-complete for nested loops as well.
Reference: [32] <author> B. Rau and C. Glaeser. </author> <title> Some scheduling techniques and an easily schedulable horizontal architecture for high performance scientific computing, </title> <booktitle> In Proc. 14th Annual Workshop on Microprogramming, </booktitle> <pages> pp. 183-198, </pages> <year> 1981. </year>
Reference: [33] <author> B. Rau. </author> <title> Cydra 5 directed dataflow architecture. </title> <booktitle> In Compcon 88, </booktitle> <pages> pp. 106-113. </pages> <publisher> IEEE Computer Society, </publisher> <year> 1988. </year>
Reference-contexts: Much progress has been made resulting in a suite of techniques that extract coarse-grain parallelism from sequential programs [4, 28, 40, 42]. With the advent of VLIW, superscalar, horizontal microengines, multiple RISC and pipelined processors, the exploitation of fine-grain instruction-level parallelism has become a major challenge to parallelizing compilers <ref> [7, 13, 20, 33, 37, 38] </ref>. The problem will become even more important as these processors form the building blocks of massively parallel machines.
Reference: [34] <author> A. Schrijver. </author> <title> Theory of linear and integer programming. Wiley-Interscience series in Discrete Mathematics and Optimization, </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1986. </year>
Reference-contexts: technique presented in this paper does not have the restriction on non-negativity and hence is more general. 4 Statement level rational affine schedules In this section, we formulate the problem of optimal fine grain scheduling of nested loops in the absence of resource constraints as a Linear Programming (LP) problem <ref> [19, 34] </ref> which admits polynomial time solutions and is extremely fast in practice. This paper generalizes the hyperplane scheduling technique of scheduling iterations of nested loops pioneered by Lamport [26] by deriving optimal schedules at the statement level rather than at the iteration level. <p> With the assumption that loop bounds are affine functions of outer loop variables, the iteration space is a convex polyhedron. The extrema of affine functions over I, therefore occur at the corners of the polyhedron <ref> [34] </ref>. If the iteration space is rectangular, i.e., L j and U j (j = 1; : : : ; n) are integer constants, we can find an expression of the Software pipelining of nested loops 7 optimal value of E using Banerjee's inequalities [5] as discussed below.
Reference: [35] <author> B. Su, S. Ding and J. Xia. </author> <title> GURPR A method for global software pipelining. </title> <booktitle> In Proc. 20th Annual Workshop on Microprogramming, </booktitle> <pages> pp. 88-96, </pages> <month> December </month> <year> 1987. </year>
Reference: [36] <author> R. Tarjan. </author> <title> Depth-first search and linear graph algorithms. </title> <journal> SIAM J. Comput., </journal> <volume> 1(2) </volume> <pages> 146-160, </pages> <month> June </month> <year> 1972. </year>
Reference-contexts: Let G denote the statement level dependence graph. If G is acyclic, then list scheduling and tree height reduction can be used to optimally schedule the computation [1]. If G is cyclic, we use Tarjan's algorithm <ref> [36, 40, 42] </ref> to find all the strongly connected components and schedule each strongly connected component separately. For the rest of the paper, we discuss the optimal scheduling of a single strongly connected component in G.
Reference: [37] <author> R. Touzeau. </author> <title> A FORTRAN compiler for the FPS-164 scientific computer. </title> <booktitle> In Proc. SIGPLAN Symposium on Compiler Construction, </booktitle> <pages> pp. 48-57, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: Much progress has been made resulting in a suite of techniques that extract coarse-grain parallelism from sequential programs [4, 28, 40, 42]. With the advent of VLIW, superscalar, horizontal microengines, multiple RISC and pipelined processors, the exploitation of fine-grain instruction-level parallelism has become a major challenge to parallelizing compilers <ref> [7, 13, 20, 33, 37, 38] </ref>. The problem will become even more important as these processors form the building blocks of massively parallel machines.
Reference: [38] <author> S. Weiss and J. Smith. </author> <title> A study of scalar compilation techniques for pipelined supercomputers. </title> <booktitle> In Proc. 2nd Intl. Conf. Architectural Support for Programming Languages & Operating System, </booktitle> <pages> pp. 105-109, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: Much progress has been made resulting in a suite of techniques that extract coarse-grain parallelism from sequential programs [4, 28, 40, 42]. With the advent of VLIW, superscalar, horizontal microengines, multiple RISC and pipelined processors, the exploitation of fine-grain instruction-level parallelism has become a major challenge to parallelizing compilers <ref> [7, 13, 20, 33, 37, 38] </ref>. The problem will become even more important as these processors form the building blocks of massively parallel machines. <p> The code for one iteration of the unrolled loop is then compacted using the acyclic dependence graph corresponding to only the code body of the unrolled loop. Weiss and Smith <ref> [38] </ref> discuss an adaptation of this technique for pipelined supercomputers. While software pipelining of inner loops has received a lot of attention, very few authors have addressed software pipelining of nested loops.
Reference: [39] <author> M. Wolfe and U. Banerjee. </author> <title> Data dependence and its application to parallel processing. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 16(2) </volume> <pages> 137-178, </pages> <year> 1987. </year>
Reference-contexts: In Section 6, we discuss work in progress aimed at integrating resource constraints and handling conditionals. Section 7 concludes with a discussion. 2 Background 2.1 Data dependence Good and thorough parallelization of a program critically depends on how precisely a compiler can discover the data dependence information <ref> [4, 5, 39, 40, 42] </ref>. These dependences imply precedence constraints among computations which have to be satisfied for a correct execution. <p> Given two statements S k (I 1 ) and S l (I 2 ), S l (I 2 ) is dependent on S k (I 1 ) (with distance vector d k;l ) iff <ref> [5, 28, 39, 40] </ref>: * Either X (F (I 1 )) is written in statement S k (I 1 ) or X (G (I 2 )) is written in statement S l (I 2 ): A flow dependence exists from statement S k to statement S l if S k writes <p> Control dependences can be handled by methods similar to data dependences [4]. In our analysis, we treat the different types of dependences uniformly. Methods to calculate data dependence vectors can be found in <ref> [4, 5, 39, 40, 42] </ref>. 3 Related Work Software pipelining of inner loops has been considered by several authors [1, 6, 11, 14, 15, 16, 23, 24, 25, 27, 32, 35, 41]. All of these studies search for the minimum initiation interval by unrolling the loop several times.
Reference: [40] <author> M. Wolfe. </author> <title> Optimizing supercompilers for supercomputers, </title> <publisher> MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: 1 Introduction Exploiting parallelism in loops in scientific programs is an important factor in realizing the potential performance of highly parallel computers today. Programming these machines remains a difficult problem. Much progress has been made resulting in a suite of techniques that extract coarse-grain parallelism from sequential programs <ref> [4, 28, 40, 42] </ref>. With the advent of VLIW, superscalar, horizontal microengines, multiple RISC and pipelined processors, the exploitation of fine-grain instruction-level parallelism has become a major challenge to parallelizing compilers [7, 13, 20, 33, 37, 38]. <p> In Section 6, we discuss work in progress aimed at integrating resource constraints and handling conditionals. Section 7 concludes with a discussion. 2 Background 2.1 Data dependence Good and thorough parallelization of a program critically depends on how precisely a compiler can discover the data dependence information <ref> [4, 5, 39, 40, 42] </ref>. These dependences imply precedence constraints among computations which have to be satisfied for a correct execution. <p> Given two statements S k (I 1 ) and S l (I 2 ), S l (I 2 ) is dependent on S k (I 1 ) (with distance vector d k;l ) iff <ref> [5, 28, 39, 40] </ref>: * Either X (F (I 1 )) is written in statement S k (I 1 ) or X (G (I 2 )) is written in statement S l (I 2 ): A flow dependence exists from statement S k to statement S l if S k writes <p> Control dependences can be handled by methods similar to data dependences [4]. In our analysis, we treat the different types of dependences uniformly. Methods to calculate data dependence vectors can be found in <ref> [4, 5, 39, 40, 42] </ref>. 3 Related Work Software pipelining of inner loops has been considered by several authors [1, 6, 11, 14, 15, 16, 23, 24, 25, 27, 32, 35, 41]. All of these studies search for the minimum initiation interval by unrolling the loop several times. <p> Let G denote the statement level dependence graph. If G is acyclic, then list scheduling and tree height reduction can be used to optimally schedule the computation [1]. If G is cyclic, we use Tarjan's algorithm <ref> [36, 40, 42] </ref> to find all the strongly connected components and schedule each strongly connected component separately. For the rest of the paper, we discuss the optimal scheduling of a single strongly connected component in G.
Reference: [41] <author> A. Zaky and P. Sadayappan. </author> <title> Optimal static scheduling of sequential loops on multiprocessors. </title> <booktitle> In Proc. International Conference on Parallel Processing, </booktitle> <volume> volume 3, </volume> <pages> pp. 130-137, </pages> <year> 1989. </year>
Reference-contexts: The procedure works by simulating the execution of the loop body until a pattern evolves. The technique does not guarantee an upper bound on the amount of time it needs to find a solution. Zaky and Sadayappan <ref> [41] </ref> present a novel approach that is based on eigenvalues of matrices that arise path algebra. Their algorithm has polynomial time complexity; their algorithm exploits the connectivity properties of the loop dependence graph. While the algorithm of [2] requires unrolling to detect a pattern, the algorithm in [41] does not require <p> Zaky and Sadayappan <ref> [41] </ref> present a novel approach that is based on eigenvalues of matrices that arise path algebra. Their algorithm has polynomial time complexity; their algorithm exploits the connectivity properties of the loop dependence graph. While the algorithm of [2] requires unrolling to detect a pattern, the algorithm in [41] does not require any unrolling. In Section 5.1, we show that the technique developed in this paper for nested loops is equally applicable to inner loops and derives the same solution as [41] using a simpler framework. Iwano and Yeh [22] use network flow algorithms for optimal loops parallelization. <p> While the algorithm of [2] requires unrolling to detect a pattern, the algorithm in <ref> [41] </ref> does not require any unrolling. In Section 5.1, we show that the technique developed in this paper for nested loops is equally applicable to inner loops and derives the same solution as [41] using a simpler framework. Iwano and Yeh [22] use network flow algorithms for optimal loops parallelization. Software pipelining of sequential loops on limited resources is discussed in [1, 11, 15, 25].
Reference: [42] <author> H. Zima and B. Chapman. </author> <title> Supercompilers for parallel and vector supercomputers. </title> <publisher> ACM Press Frontier Series, </publisher> <year> 1990. </year>
Reference-contexts: 1 Introduction Exploiting parallelism in loops in scientific programs is an important factor in realizing the potential performance of highly parallel computers today. Programming these machines remains a difficult problem. Much progress has been made resulting in a suite of techniques that extract coarse-grain parallelism from sequential programs <ref> [4, 28, 40, 42] </ref>. With the advent of VLIW, superscalar, horizontal microengines, multiple RISC and pipelined processors, the exploitation of fine-grain instruction-level parallelism has become a major challenge to parallelizing compilers [7, 13, 20, 33, 37, 38]. <p> In Section 6, we discuss work in progress aimed at integrating resource constraints and handling conditionals. Section 7 concludes with a discussion. 2 Background 2.1 Data dependence Good and thorough parallelization of a program critically depends on how precisely a compiler can discover the data dependence information <ref> [4, 5, 39, 40, 42] </ref>. These dependences imply precedence constraints among computations which have to be satisfied for a correct execution. <p> Control dependences can be handled by methods similar to data dependences [4]. In our analysis, we treat the different types of dependences uniformly. Methods to calculate data dependence vectors can be found in <ref> [4, 5, 39, 40, 42] </ref>. 3 Related Work Software pipelining of inner loops has been considered by several authors [1, 6, 11, 14, 15, 16, 23, 24, 25, 27, 32, 35, 41]. All of these studies search for the minimum initiation interval by unrolling the loop several times. <p> Let G denote the statement level dependence graph. If G is acyclic, then list scheduling and tree height reduction can be used to optimally schedule the computation [1]. If G is cyclic, we use Tarjan's algorithm <ref> [36, 40, 42] </ref> to find all the strongly connected components and schedule each strongly connected component separately. For the rest of the paper, we discuss the optimal scheduling of a single strongly connected component in G.
References-found: 42

