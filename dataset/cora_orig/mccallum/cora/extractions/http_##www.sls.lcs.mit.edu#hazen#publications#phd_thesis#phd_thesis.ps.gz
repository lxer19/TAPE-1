URL: http://www.sls.lcs.mit.edu/hazen/publications/phd_thesis/phd_thesis.ps.gz
Refering-URL: http://www.sls.lcs.mit.edu/hazen/publications.html
Root-URL: 
Title: The Use of Speaker Correlation Information for Automatic Speech Recognition  
Author: by Timothy J. Hazen James R. Glass Arthur C. Smith 
Degree: 1991 Submitted to the Department of Electrical Engineering and Computer Science in Partial Fulfillment of the Requirements for the Degree of Doctor of Philosophy at the  c flMassachusetts Institute of Technology, 1998. All rights reserved. Signature of Author  Certified by  Accepted by  Chair, Department Committee on Graduate Students  
Date: January, 1998  January 30, 1998  
Address: 1993  
Affiliation: S.M., Massachusetts Institute of Technology,  S.B., Massachusetts Institute of Technology,  Massachusetts Institute of Technology  Department of Electrical Engineering and Computer Science  Principal Research Scientist Department of Electrical Engineering and Computer Science  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> M. Afify, Y. Gong, and J. Haton. </author> <title> Correlation based predictive adaptation of hidden Markov models. </title> <booktitle> In Proceedings of the 5th European Conference on Speech Communication and Technology, </booktitle> <pages> pages 2111-2114, </pages> <address> Rhodes, Greece, </address> <year> 1997. </year>
Reference-contexts: Techniques that fall into this general class of adaptation approaches have been developed by Ahadi and Woodland [2], Cox [14], Hazen [34], Chen and DeSouza [11], and Afify, et al. <ref> [1] </ref>. 70 3.4.4 Transformational Approaches Overview As an alternative to predictive methods such as EMAP, which require the training of a priori models, some researchers have investigated transformational approaches to the problem.
Reference: [2] <author> S. Ahadi and P. Woodland. </author> <title> Rapid speaker adaptation using model prediction. </title> <booktitle> In Proceedings of the 1995 International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 684-687, </pages> <address> Detroit, MI, </address> <year> 1995. </year>
Reference-contexts: Predictive methods which utilize within-speaker correlation information are then used to adapt the models for which no adaptation data has been seen. Techniques that fall into this general class of adaptation approaches have been developed by Ahadi and Woodland <ref> [2] </ref>, Cox [14], Hazen [34], Chen and DeSouza [11], and Afify, et al. [1]. 70 3.4.4 Transformational Approaches Overview As an alternative to predictive methods such as EMAP, which require the training of a priori models, some researchers have investigated transformational approaches to the problem.
Reference: [3] <author> T. Anastasakos, J. McDonough, and J. Makhoul. </author> <title> Speaker adaptive training: A maximum likelihood approach to speaker normalization. </title> <booktitle> In Proceedings of the 1997 International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 1043-1046, </pages> <address> Munich, Germany, </address> <year> 1997. </year>
Reference-contexts: Some past adaptation efforts have investigated methods for reducing the variance of this matrix to be more in line with the variance of a typical speaker dependent model. The most successful of these approaches is speaker adaptative training <ref> [4, 3] </ref>. This issue will not be investigated in this thesis. Next, each a priori density function p (~c m ) will also be modeled with a single Gaussian density function.
Reference: [4] <author> T. Anastasakos, J. McDonough, and R. Schwartz. </author> <title> A compact model for speaker-adaptive training. </title> <booktitle> In Proceedings of the 1996 International Conference on Spoken Language Processing, </booktitle> <pages> pages 1137-1140, </pages> <address> Philadelphia, PA, </address> <year> 1996. </year>
Reference-contexts: Similarly, all of the nasals are highly correlated. In some cases, it is observed that the place or articulation of a phone contributes more correlation information than the phone's manner class. For example, the phones exhibiting the most within-speaker correlation with the closure [g] are the phones [g], <ref> [4] </ref>, [g], [k] and [y]. All five of these phones share a similar place of articulation to [g], but only two of them are stop closures themselves. <p> Some past adaptation efforts have investigated methods for reducing the variance of this matrix to be more in line with the variance of a typical speaker dependent model. The most successful of these approaches is speaker adaptative training <ref> [4, 3] </ref>. This issue will not be investigated in this thesis. Next, each a priori density function p (~c m ) will also be modeled with a single Gaussian density function.
Reference: [5] <author> L. Bahl, S. Balakrishnan-Aiyer, J. Bellegarda, M. Franz, P. Gopalakrishnan, D. Nahamoo, M. Novak, M. Padmanabhan, M. Picheny, and S. Roukos. </author> <title> Performance of the IBM large vocabulary continuous speech recognition system on the ARPA Wall Street Journal task. </title> <booktitle> In Proceedings of the 1995 International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 41-44, </pages> <address> Detroit, MI, </address> <year> 1995. </year>
Reference-contexts: With the development and refinement of the Hidden Markov Model (HMM) approach [7, 8, 57], today's speech recognition systems have been shown to work effectively on various large vocabulary, continuous speech, speaker independent tasks. However, despite the high quality of today's speaker independent (SI) systems <ref> [5, 25, 49] </ref>, there can still be a significant gap in performance between these systems and their speaker dependent (SD) counterparts. As will be discussed in Chapter 3, The reduction in a system's error rate between its speaker independent mode and its speaker dependent mode can be 50% or more.
Reference: [6] <author> L. Bahl, P. Brown, P. de Souza, R. Mercer, and D. Nahamoo. </author> <title> A fast algorithm for deleted interpolation. </title> <booktitle> In Proceedings of the 2nd European Conference on Speech Communication and Technology, </booktitle> <pages> pages 1209-1212, </pages> <address> Genova, Italy, </address> <year> 1991. </year>
Reference-contexts: The method used in this thesis is called deleted interpolation <ref> [6, 37] </ref>. Deleted interpolation optimizes the values by maximizing the likelihood of data jack-knifed from the training set using the EM algorithm. A full description of the deleted interpolation algorithm is provided in Appendix D. Using the deleted interpolation algorithm, each phone model receives a different set of interpolation weights.
Reference: [7] <author> L. Bahl, F. Jelinek, and R. Mercer. </author> <title> A maximum likelihood approach to continuous speech recognition. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-5(2), </volume> <pages> pages 179-190, </pages> <month> March </month> <year> 1984. </year> <month> 171 </month>
Reference-contexts: Thus, this dissertation will specifically examine the issues involved in utilizing speaker constraint for speech recognition. Over the last ten to twenty years, dramatic improvements in the quality of speaker independent speech recognition technology have been made. With the development and refinement of the Hidden Markov Model (HMM) approach <ref> [7, 8, 57] </ref>, today's speech recognition systems have been shown to work effectively on various large vocabulary, continuous speech, speaker independent tasks.
Reference: [8] <author> J. Baker. </author> <title> Stochastic Modeling as a Means of Automatic Speech Recognition. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <month> April </month> <year> 1975. </year>
Reference-contexts: Thus, this dissertation will specifically examine the issues involved in utilizing speaker constraint for speech recognition. Over the last ten to twenty years, dramatic improvements in the quality of speaker independent speech recognition technology have been made. With the development and refinement of the Hidden Markov Model (HMM) approach <ref> [7, 8, 57] </ref>, today's speech recognition systems have been shown to work effectively on various large vocabulary, continuous speech, speaker independent tasks.
Reference: [9] <author> C. Bishop. </author> <title> Neural Networks for Pattern Recognition. </title> <publisher> Oxford University Press, </publisher> <address> New York, NY, </address> <year> 1995. </year>
Reference-contexts: Both of the methods also suffer from the disadvantage that computation is wasted. The results of only one training trial are kept, while the models from the other trials are thrown away <ref> [9] </ref>. Ideally, a training routine should be able to utilize whatever is learned from all of the training trials and not just one selected trial. <p> To counter the problems discussed above, an algorithm is needed which produces a mixture density function which can be proven to yield better accuracy, on average, than any randomly initialized density function trained using standard techniques. Aggregation is a technique which meets this criterion <ref> [9] </ref>. Aggregation improves the performance of models which exhibit uncertainty or instability during their training phase. Aggregation has been applied to a variety of types of predictors and classifiers.
Reference: [10] <author> L. Breiman. </author> <title> Bagging predictors. </title> <journal> Machine Learning, </journal> <volume> 24(2), </volume> <pages> pages 123-140, </pages> <year> 1996. </year>
Reference-contexts: Aggregation has been applied to a variety of types of predictors and classifiers. For example, Breiman has shown the effectiveness of a specific type of aggregation known as bagging (or bootstrap aggregating) on linear regression predictors and on classification trees <ref> [10] </ref>. In this thesis, aggregation is utilized to improve the likelihood estimates generated by mixture Gaussian density functions. 1 E.2 Theory Aggregation of probabilistic density functions is performed by averaging the outputs of a set of independently trained models.
Reference: [11] <author> S. Chen and P. DeSouza. </author> <title> Speaker adaptation by correlation (ABC). </title> <booktitle> In Proceedings of the 5th European Conference on Speech Communication and Technology, </booktitle> <pages> pages 2111-2114, </pages> <address> Rhodes, Greece, </address> <year> 1997. </year>
Reference-contexts: Predictive methods which utilize within-speaker correlation information are then used to adapt the models for which no adaptation data has been seen. Techniques that fall into this general class of adaptation approaches have been developed by Ahadi and Woodland [2], Cox [14], Hazen [34], Chen and DeSouza <ref> [11] </ref>, and Afify, et al. [1]. 70 3.4.4 Transformational Approaches Overview As an alternative to predictive methods such as EMAP, which require the training of a priori models, some researchers have investigated transformational approaches to the problem.
Reference: [12] <author> G. Chung and S. Seneff. </author> <title> Hierarchical duration modelling for speech recognition using the ANGIE framework. </title> <booktitle> In Proceedings of the 5th European Conference on Speech Communication and Technology, </booktitle> <pages> pages 1475-1478, </pages> <address> Rhodes, Greece, </address> <year> 1997. </year>
Reference-contexts: A hierarchical duration model incorporating higher level information about the stress pattern, speaking rate, and word string is currently under development within our group <ref> [12] </ref>. B.4 Pronunciation Modeling The pronunciation model is represented by the expression p (P jW ). Every word in the vocabulary is initially represented with a phonetic baseform pronunciation and possible alternate pronunciations which are specific to that word.
Reference: [13] <author> D. Van Compernolle, J. Smolders, P. Jaspers, and T. Hellemans. </author> <title> Speaker clustering for dialectic robustness in speaker independent recognition. </title> <booktitle> In Proceedings of the 2nd European Conference on Speech Communication and Technology, </booktitle> <pages> pages 723-726, </pages> <address> Genova, Italy, </address> <year> 1991. </year>
Reference-contexts: Siegler and Stern also created speaking rate specific acoustic and pronunciation models as well, but were not able to achieve any improvement from these new models. Speaker Clustering One of the most common approaches for providing speaker constraint to a recognition system is speaker clustering <ref> [13, 23, 24, 48, 47, 61, 65, 62, 66] </ref>. The basic of idea of speaker clustering is to create a selection of different models by clustering the training speakers based on some similarity measure.
Reference: [14] <author> S. Cox. </author> <title> Speaker adaptation using a predictive model. </title> <booktitle> In Proceedings of the 3rd European Conference on Speech Communication and Technology, </booktitle> <pages> pages 2283-2286, </pages> <address> Berlin, Germany, </address> <year> 1993. </year>
Reference-contexts: Predictive methods which utilize within-speaker correlation information are then used to adapt the models for which no adaptation data has been seen. Techniques that fall into this general class of adaptation approaches have been developed by Ahadi and Woodland [2], Cox <ref> [14] </ref>, Hazen [34], Chen and DeSouza [11], and Afify, et al. [1]. 70 3.4.4 Transformational Approaches Overview As an alternative to predictive methods such as EMAP, which require the training of a priori models, some researchers have investigated transformational approaches to the problem.
Reference: [15] <author> J. Dalby. </author> <title> Phonetic Structure of Fast Speech in American English. </title> <type> PhD thesis, </type> <institution> Indiana University, </institution> <month> December </month> <year> 1984. </year>
Reference-contexts: As a result formants move towards more central or laxer position when speaking rate is increased [50]. Overall, an increase in speaking rate generally results in speech which is more relaxed and less carefully articulated than speech which is spoken slower <ref> [15] </ref>. As a result, distinct acoustic differences in the acoustic models of fast and slow speakers can be observed. Figure 2.5 shows the high likelihood region contours for models from fast and slow speakers. <p> As the speaking rate is increased speakers tend to produce speech which is more casual in nature, tending towards pronunciations which are reduced or under-specified versions of the underlying form <ref> [15] </ref>. Consider the following phonological rules as expressed in rewrite form: 1. st- ! s 3. i ! I 4. Vd- ! VF -or- Vt- ! VF- where V represents any vowel.
Reference: [16] <author> V. Digalakis and L. Neumeyer. </author> <title> Speaker adaptation using combined transformation and Bayesian methods. </title> <booktitle> In Proceedings of the 1995 International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 680-683, </pages> <address> Detroit, MI, </address> <year> 1995. </year>
Reference-contexts: This approach was independently proposed by Leggetter and Woodland [59, 60] and by Digalakis, et al. <ref> [17, 16, 71] </ref>. <p> The second step is to apply MAP adaptation to the selected speaker cluster model. A number of research efforts have investigated the combination of two or more of the techniques presented above <ref> [16, 71, 81, 83] </ref>. The next three chapters will present three new adaptation approaches which attempt to incorporate some or all of the attributes listed above.
Reference: [17] <author> V. Digalakis, D. Rtischev, and L. Neumeyer. </author> <title> Speaker adaptation using constrained reestimation of gaussian mixtures. </title> <journal> IEEE Transactions on Speech and Audio Processing, </journal> <volume> 3(5), </volume> <pages> pages 357-366, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: This approach was independently proposed by Leggetter and Woodland [59, 60] and by Digalakis, et al. <ref> [17, 16, 71] </ref>.
Reference: [18] <institution> Dragon naturally speaking. </institution> <note> Published by Dragon Systems, Inc. Located at URL http://www.dragonsystems.com/marketing/pcproducts.html, 1997. </note>
Reference-contexts: Of the different styles, the easiest to perform is supervised, enrolled, batch adaptation. This is the approach taken by many speaker dependent dictation applications <ref> [18, 42] </ref>. For these tasks, the time spent recording the user's speech on predetermined sentences is small compared to the many hours that the system will be used by the specific individual. Thus, the time needed to provide the initial set of adaptation data is viewed as a worthwhile investment.
Reference: [19] <author> R. Duda and P. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, NY, </address> <year> 1973. </year> <month> 172 </month>
Reference-contexts: Using the definitions provided above, it can be shown that the MAP estimated value of ~c m given the adaptation data A m can be found using the following equation <ref> [19] </ref>: ~c map m + S m ) ~ ap m (N m S ap 1 m (3.27) It should be noted that ~c ml m is the ML estimated value of ~c m and is defined as: ~c ml 1 N m X ~a m;n (3.28) When examining Equation (3.27) <p> for combining different adaptation algorithms within a unified framework will be discussed in Chapter 4 and Chapter 7. 65 3.4 Past and Present Approaches 3.4.1 MAP Adaptation The standard expressions for MAP adaptation 1 of Gaussian model parameters have long been derived and can be found in many standard texts <ref> [19] </ref>. However, the standard text book implementation may encounter problems when it is implemented. Consider the MAP estimation expression for a Gaussian mean vector, as explained in Section 3.3.5. <p> First, a method for optimizing must be determined. The expectation-maximization (EM) algorithm fits this need. The EM algorithm is an iterative process which is guaranteed to adjust the parameters of a mixture density such that the total likelihood score produced over the cross-validation data is increased with every iteration <ref> [19] </ref>. Second, an adequate amount of cross-validation data must be produced in order to reliably determine the value for .
Reference: [20] <author> E. Eide and H. Gish. </author> <title> A parametric approach to vocal tract length normalization. </title> <booktitle> In Proceedings of the 1996 International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 346-349, </pages> <address> Atlanta, GA, </address> <year> 1996. </year>
Reference-contexts: A number of research efforts have attempted to account for different speaker properties within the modeling schemes of their systems. 73 Vocal Tract Length Normalization Recently there have been numerous research efforts directed at the problem of vocal tract length normalization (VTLN) <ref> [20, 43, 58, 83] </ref>. The basic idea of VTLN is to warp the spectrum of the speech through stretching or compressing so that the relative formant locations of the current speaker match the formant locations of some generic speaker model as closely as possible.
Reference: [21] <author> G. Fant. </author> <title> Acoustic Theory of Speech Production. Mouton, The Hague, </title> <booktitle> The Nether-lands, </booktitle> <year> 1970. </year>
Reference-contexts: However, the size and shape of each individual vocal tract constrains the exact acoustic realization of the various phonetic elements. In fact, there is considerable literature dedicated to the relationship between the acoustic speech signal and the underlying 27 vocal tract configuration and dimensions <ref> [21, 75] </ref>. The physical characteristics of the speaker, combined with other factors such as the speaker's regional dialect, speaking rate and emotional state all combine to constrain the acoustics produced by a speaker. <p> On average a female vocal tract is 15% shorter than a male vocal tract, with a majority of the difference occurring in the length of the pharynx <ref> [21, 46] </ref>. This results in vowel formant locations which are measurably different between male and female speakers. As a result acoustic models trained on only male speakers are distinctly different than acoustic models trained on only female speakers.
Reference: [22] <author> W. Fisher. </author> <title> The DARPA task domain speech recognition database. </title> <booktitle> In Proceedings of the DARPA Speech Recognition Workshop, </booktitle> <pages> pages 105-109, </pages> <address> San Diego, CA, </address> <month> March </month> <year> 1987. </year>
Reference-contexts: Paradigm 2 would be used in the context of direct incorporation of speaker correlation information into the speech recognition modeling scheme, such as the consistency modeling approach. In this section, these two methods will be explored on data from the DARPA Resource Management (RM) corpus <ref> [22, 69] </ref>. A complete description of the corpus is provided in Appendix C. 2.2.2 Paradigm 1 One way in which speaker correlation information can be used is to constrain the space of possible speaker dependent (SD) models. Let fi represent the set of parameters in a SD model set. <p> model but rather just a word-pair grammar which specifies which words are allowed to follow any given word. 160 Appendix C The Resource Management Corpus The DARPA Resource Management (RM) Task Domain corpus was developed for the purpose of evaluating speech recognition performance for systems operating in a constrained domain <ref> [22, 69] </ref>. The corpus contains read utterances of 2800 different artificially generated sentences constrained by a predetermined grammar. The vocabulary size of the corpus is roughly 1000 words. The sentences themselves were designed to simulate database queries that might be spoken by naval personal when performing naval resource management tasks.
Reference: [23] <author> S. Furui. </author> <title> Unsupervised speaker adaptation method based on hierarchical spectral clustering. </title> <booktitle> In Proceedings of the 1989 International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 286-289, </pages> <address> Glasgow, Scotland, </address> <year> 1989. </year>
Reference-contexts: Siegler and Stern also created speaking rate specific acoustic and pronunciation models as well, but were not able to achieve any improvement from these new models. Speaker Clustering One of the most common approaches for providing speaker constraint to a recognition system is speaker clustering <ref> [13, 23, 24, 48, 47, 61, 65, 62, 66] </ref>. The basic of idea of speaker clustering is to create a selection of different models by clustering the training speakers based on some similarity measure. <p> There are a variety of ways in which the speaker clustered tree can be constructed. The construction can be performed using unsupervised bottom-up clustering based on an acoustic similarity measure [47, 48], unsupervised top-down clustering based on an acoustic similarity measure <ref> [23, 62] </ref>, or some supervised method. In the experiments presented here, the hierarchical tree is manually created. The data is first divided by gender and then subdivided by speaking rate. Three different speaking rate classifications are utilized: fast, medium and slow.
Reference: [24] <author> Y. Gao, M. Padmanabhan, and M. Picheny. </author> <title> Speaker adaptation based on pre-clustering training speakers. </title> <booktitle> In Proceedings of the 5th European Conference on Speech Communication and Technology, </booktitle> <pages> pages 2091-2094, </pages> <address> Rhodes, Greece, </address> <year> 1997. </year>
Reference-contexts: Siegler and Stern also created speaking rate specific acoustic and pronunciation models as well, but were not able to achieve any improvement from these new models. Speaker Clustering One of the most common approaches for providing speaker constraint to a recognition system is speaker clustering <ref> [13, 23, 24, 48, 47, 61, 65, 62, 66] </ref>. The basic of idea of speaker clustering is to create a selection of different models by clustering the training speakers based on some similarity measure.
Reference: [25] <author> J. Gauvain, L. Lamel, and M. Adda-Decker. </author> <title> Developments in continuous speech dictation using the ARPA WSJ task. </title> <booktitle> In Proceedings of the 1995 International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 65-68, </pages> <address> Detroit, MI, </address> <year> 1995. </year>
Reference-contexts: With the development and refinement of the Hidden Markov Model (HMM) approach [7, 8, 57], today's speech recognition systems have been shown to work effectively on various large vocabulary, continuous speech, speaker independent tasks. However, despite the high quality of today's speaker independent (SI) systems <ref> [5, 25, 49] </ref>, there can still be a significant gap in performance between these systems and their speaker dependent (SD) counterparts. As will be discussed in Chapter 3, The reduction in a system's error rate between its speaker independent mode and its speaker dependent mode can be 50% or more.
Reference: [26] <author> J. Gauvain and C. Lee. </author> <title> Bayesian learning for hidden Markov model with Gaussian mixture state observation densities. </title> <booktitle> In Proceedings of the 2nd European Conference on Speech Communication and Technology, </booktitle> <pages> pages 939-942, </pages> <address> Genova, Italy, </address> <year> 1991. </year>
Reference-contexts: As such, it may be possible to substitute simpler interpolation methods, such as the one utilized in Section 3.3.4, in the place of more complicated MAP algorithms without sacrificing accuracy. The difficult issues surrounding MAP adaptation have been thoroughly investigated and expounded upon by Gauvain and Lee <ref> [26, 27, 28, 29, 30, 56] </ref>. In particular, in [30] they extend the mathematical framework of MAP adaptation to mixture Gaussian density functions and HMM recognizers.
Reference: [27] <author> J. Gauvain and C. Lee. </author> <title> Bayesian learning of Gaussian mixture densities for hidden Markov models. </title> <booktitle> In Proceedings of the DARPA Speech and Natural Language Workshop, </booktitle> <pages> pages 272-277, </pages> <address> Pacific Grove, CA, </address> <month> February </month> <year> 1991. </year>
Reference-contexts: As such, it may be possible to substitute simpler interpolation methods, such as the one utilized in Section 3.3.4, in the place of more complicated MAP algorithms without sacrificing accuracy. The difficult issues surrounding MAP adaptation have been thoroughly investigated and expounded upon by Gauvain and Lee <ref> [26, 27, 28, 29, 30, 56] </ref>. In particular, in [30] they extend the mathematical framework of MAP adaptation to mixture Gaussian density functions and HMM recognizers.
Reference: [28] <author> J. Gauvain and C. Lee. </author> <title> Improved acoustic modeling with Bayesian learning. </title> <booktitle> In Proceedings of the 1992 International Conference on Acoustics, Speech and Signal Processing, </booktitle> <volume> volume I, </volume> <pages> pages 481-484, </pages> <address> San Francisco, CA, </address> <year> 1992. </year>
Reference-contexts: As such, it may be possible to substitute simpler interpolation methods, such as the one utilized in Section 3.3.4, in the place of more complicated MAP algorithms without sacrificing accuracy. The difficult issues surrounding MAP adaptation have been thoroughly investigated and expounded upon by Gauvain and Lee <ref> [26, 27, 28, 29, 30, 56] </ref>. In particular, in [30] they extend the mathematical framework of MAP adaptation to mixture Gaussian density functions and HMM recognizers.
Reference: [29] <author> J. Gauvain and C. Lee. </author> <title> MAP estimation of continuous density HMM: Theory and application. </title> <booktitle> In Proceedings of the DARPA Speech and Natural Language Workshop, </booktitle> <pages> pages 185-190, </pages> <address> Harriman, NY, </address> <month> February </month> <year> 1992. </year>
Reference-contexts: As such, it may be possible to substitute simpler interpolation methods, such as the one utilized in Section 3.3.4, in the place of more complicated MAP algorithms without sacrificing accuracy. The difficult issues surrounding MAP adaptation have been thoroughly investigated and expounded upon by Gauvain and Lee <ref> [26, 27, 28, 29, 30, 56] </ref>. In particular, in [30] they extend the mathematical framework of MAP adaptation to mixture Gaussian density functions and HMM recognizers.
Reference: [30] <author> J. Gauvain and C. Lee. </author> <title> Maximum a posteriori estimation for multivariate Gaussian mixture observation of Markov chains. </title> <journal> IEEE Transactions on Speech and Audio Processing, </journal> <volume> 2(2), </volume> <pages> pages 291-298, </pages> <month> April </month> <year> 1994. </year> <month> 173 </month>
Reference-contexts: As such, it may be possible to substitute simpler interpolation methods, such as the one utilized in Section 3.3.4, in the place of more complicated MAP algorithms without sacrificing accuracy. The difficult issues surrounding MAP adaptation have been thoroughly investigated and expounded upon by Gauvain and Lee <ref> [26, 27, 28, 29, 30, 56] </ref>. In particular, in [30] they extend the mathematical framework of MAP adaptation to mixture Gaussian density functions and HMM recognizers. <p> The difficult issues surrounding MAP adaptation have been thoroughly investigated and expounded upon by Gauvain and Lee [26, 27, 28, 29, 30, 56]. In particular, in <ref> [30] </ref> they extend the mathematical framework of MAP adaptation to mixture Gaussian density functions and HMM recognizers. More recent investigations of the problem have introduced methods for performing on-line MAP adaptation [38, 39] and methods for smoothing MAP estimates of mixture Gaussian parameters (i.e., vector field smoothing) [78, 79].
Reference: [31] <author> L. Gillick and S. Cox. </author> <title> Some statistical issues in the comparison of speech recog-nition algorithms. </title> <booktitle> In Proceedings of the 1989 International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 532-535, </pages> <address> Glasgow, Scotland, </address> <year> 1989. </year>
Reference-contexts: To evaluate the significance of the results presented in Table 7.2, the matched pairs sentence segment word error test is utilized <ref> [31] </ref>. This test measures the likelihood that the differences present during an evaluation between two recognizers are a result of chance as opposed to genuine differences in the performances of the recognizers.
Reference: [32] <author> J. Glass, J. Chang, and M. McCandless. </author> <title> A probabilistic framework for feature-based speech recognition. </title> <booktitle> In Proceedings of the 1996 International Conference on Spoken Language Processing, </booktitle> <pages> pages 2277-2280, </pages> <address> Philadelphia, PA, </address> <year> 1996. </year>
Reference-contexts: speech recognition algorithms an be developed which which utilize the strong points of the HMM approach while correcting for the HMM's shortcomings. 151 152 Appendix B The SUMMIT Recognizer B.1 Probabilistic Framework The summit system is a segment-based speech recognition engine developed in the Spoken Language Systems Group at MIT <ref> [32, 85, 86, 87, 88] </ref>. The summit system is based on a probabilistic framework describing the speech recognition problem. In this thesis summit is utilized to perform word recognition. The goal during recognition is to find the word sequence which is most likely given the acoustic information.
Reference: [33] <author> D. Goddeau, E. Brill, J. Glass, C. Pao, and M. Phillips. </author> <title> GALAXY: A human-language interface to on-line travel information. </title> <booktitle> In Proceedings of the 1994 International Conference on Spoken Language Processing, </booktitle> <pages> pages 707-710, </pages> <address> Yokohama, Japan, </address> <year> 1994. </year>
Reference-contexts: However, there are many applications where supervised, enrolled, batch adaptation is not feasible. For example, the galaxy and jupiter systems typically operate with speakers who are unknown to the system and will only utilize a small number of conversational exchanges to achieve their goal <ref> [33, 54, 89, 84] </ref>. In these cases adaptation must by performed in an unsupervised fashion. Because the number of utterances is small, it is also desirable to perform adaptation in an instantaneous fashion, i.e., adaptation is applied using the same utterance that the system is trying to recognize.
Reference: [34] <author> T. Hazen. </author> <title> Probabilistic transfer vector prediction for speaker adaptation. </title> <type> Technical Report TR-IT-0124, </type> <institution> ATR, </institution> <address> Kyoto, Japan, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: Predictive methods which utilize within-speaker correlation information are then used to adapt the models for which no adaptation data has been seen. Techniques that fall into this general class of adaptation approaches have been developed by Ahadi and Woodland [2], Cox [14], Hazen <ref> [34] </ref>, Chen and DeSouza [11], and Afify, et al. [1]. 70 3.4.4 Transformational Approaches Overview As an alternative to predictive methods such as EMAP, which require the training of a priori models, some researchers have investigated transformational approaches to the problem.
Reference: [35] <author> T. Hazen and A. Halberstadt. </author> <title> Using aggregation to improve the performance of mixture Gaussian acoustic models. </title> <booktitle> In Proceedings of the The 1998 International Conference on Acoustics, Speech and Signal Processing, </booktitle> <address> Seattle, WA, </address> <year> 1998. </year>
Reference-contexts: By combining together multiple trials, a more robust mixture model can be created. Aggregation has also been shown to alleviate problems caused when individual training trials tend to over-fit the training data <ref> [35] </ref>. A full discussion of aggregation is presented in Appendix E. 115 collected from three different training speakers. In (a) diagonal Gaussian models are created for each speaker. In (b) the individual diagonal Gaussians for each speaker are combined to make one large mixture of Gaussians. <p> However, the proof does not depend in any way on how the classifiers are generated. 1 This appendix draws freely from work conducted jointly with Andrew Halberstadt which is presented in <ref> [35] </ref>. 168 In order to evaluate the accuracy of a density function, an appropriate error metric must be defined. Let the squared error of the likelihood be the error metric used to evaluate the accuracy of an estimated density function. <p> Thus, the gain in accuracy produced by aggregation comes at the cost of additional computation caused by the added number of parameters in the aggregate model. The computation issues and potential methods for computational savings are addressed in <ref> [35] </ref>. 170
Reference: [36] <author> L. Hetherington and M. McCandless. SAPPHIRE: </author> <title> An extensible speech analysis and recognition tool based on Tcl/Tk. </title> <booktitle> In Proceedings of the 1996 International Conference on Spoken Language Processing, </booktitle> <pages> pages 1942-1945, </pages> <address> Philadelphia, PA, </address> <year> 1996. </year>
Reference-contexts: Thus, summit models a word as a sequence of phones each of which occupy one segment in the segment network. Figure B.1 demonstrates a segment network generated by summit as presented by the sapphire speech analysis and recognition tool <ref> [36] </ref>. In this figure, the waveform is shown on the top. The utterance's spectrogram is shown directly below the waveform. Below the spectrogram is the segment network. The time-aligned phone and word transcriptions are shown below the segment network.
Reference: [37] <author> X. Huang, M. Hwang, L. Jiang, and M. Mahajan. </author> <title> Deleted interpolation and density sharing for continuous hidden Markov models. </title> <booktitle> In Proceedings of the 1996 International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 885-888, </pages> <address> Atlanta, GA, </address> <year> 1996. </year>
Reference-contexts: The method used in this thesis is called deleted interpolation <ref> [6, 37] </ref>. Deleted interpolation optimizes the values by maximizing the likelihood of data jack-knifed from the training set using the EM algorithm. A full description of the deleted interpolation algorithm is provided in Appendix D. Using the deleted interpolation algorithm, each phone model receives a different set of interpolation weights. <p> The more data that is taken from the training set for cross validation, the better the estimate of the interpolation weights will be, but the worse the estimates of the individual density functions will be. To alleviate this problem, deleted interpolation can be utilized <ref> [37] </ref>. The basic idea behind deleted interpolation is that multiple cross-validation data sets are created via a jack-knifing process applied to the training set.
Reference: [38] <author> Q. Huo and C. Chan. </author> <title> On-line Bayes adaptation of SCHMM parameters for speech recognition. </title> <booktitle> In Proceedings of the 1995 International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 708-711, </pages> <address> Detroit, MI, </address> <year> 1995. </year>
Reference-contexts: In particular, in [30] they extend the mathematical framework of MAP adaptation to mixture Gaussian density functions and HMM recognizers. More recent investigations of the problem have introduced methods for performing on-line MAP adaptation <ref> [38, 39] </ref> and methods for smoothing MAP estimates of mixture Gaussian parameters (i.e., vector field smoothing) [78, 79].
Reference: [39] <author> Q. Huo, C. Chan, and C. Lee. </author> <title> Bayesian learning of SCHMM parameters for speech recognition. </title> <booktitle> In Proceedings of the 1994 International Conference on Acoustics, Speech and Signal Processing, </booktitle> <volume> volume I, </volume> <pages> pages 221-224, </pages> <address> Adelaide, Australia, </address> <year> 1994. </year>
Reference-contexts: In particular, in [30] they extend the mathematical framework of MAP adaptation to mixture Gaussian density functions and HMM recognizers. More recent investigations of the problem have introduced methods for performing on-line MAP adaptation <ref> [38, 39] </ref> and methods for smoothing MAP estimates of mixture Gaussian parameters (i.e., vector field smoothing) [78, 79].
Reference: [40] <author> Q. Huo and C. Lee. </author> <title> On-line adaptive learning of the correlated continuous density hidden Markov models for speech recognition. </title> <booktitle> In Proceedings of the 1996 International Conference on Spoken Language Processing, </booktitle> <pages> pages 985-988, </pages> <address> Philadelphia, PA, </address> <year> 1996. </year> <month> 174 </month>
Reference-contexts: For comparison, full covariance MAP adaptation requires M D 2 covariance parameters be trained. Even with simplifying assumptions, EMAP adaptation is extremely difficult to implement. Despite the difficulties, research efforts by Huo and Lee <ref> [40, 41] </ref> and by Zavaliagkos, et al [81, 82], have utilized EMAP adaptation with modest success. 3.4.3 Model Prediction Approaches In addition to the EMAP approach there have been a number of approaches which follow the same motivation as EMAP, but utilize different slightly frameworks.
Reference: [41] <author> Q. Huo and C. Lee. </author> <title> Combined on-line model adaptation and Bayesian predic-tive classification for robust speecg recognition. </title> <booktitle> In Proceedings of the 5th Eu-ropean Conference on Speech Communication and Technology, </booktitle> <pages> pages 1847-1850, </pages> <address> Rhodes, Greece, </address> <year> 1997. </year>
Reference-contexts: For comparison, full covariance MAP adaptation requires M D 2 covariance parameters be trained. Even with simplifying assumptions, EMAP adaptation is extremely difficult to implement. Despite the difficulties, research efforts by Huo and Lee <ref> [40, 41] </ref> and by Zavaliagkos, et al [81, 82], have utilized EMAP adaptation with modest success. 3.4.3 Model Prediction Approaches In addition to the EMAP approach there have been a number of approaches which follow the same motivation as EMAP, but utilize different slightly frameworks.
Reference: [42] <institution> IBM ViaVoice. Published by IBM Corporation. </institution> <note> Located at URL http://www.software.ibm.com/is/voicetype/us vv.html, </note> <year> 1997. </year>
Reference-contexts: Of the different styles, the easiest to perform is supervised, enrolled, batch adaptation. This is the approach taken by many speaker dependent dictation applications <ref> [18, 42] </ref>. For these tasks, the time spent recording the user's speech on predetermined sentences is small compared to the many hours that the system will be used by the specific individual. Thus, the time needed to provide the initial set of adaptation data is viewed as a worthwhile investment.
Reference: [43] <author> T. Kamm, A. Andreou, and J. Cohen. </author> <title> Vocal tract normalization in speech recognition: compensating for systematic speaker variability. </title> <booktitle> In Proceedings of the 15th Annual Speech Research Symposium, </booktitle> <address> Baltimore, MD, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: A number of research efforts have attempted to account for different speaker properties within the modeling schemes of their systems. 73 Vocal Tract Length Normalization Recently there have been numerous research efforts directed at the problem of vocal tract length normalization (VTLN) <ref> [20, 43, 58, 83] </ref>. The basic idea of VTLN is to warp the spectrum of the speech through stretching or compressing so that the relative formant locations of the current speaker match the formant locations of some generic speaker model as closely as possible.
Reference: [44] <author> A. Kannan and M. Ostendorf. </author> <title> Modeling dependency in adaptation of acoustic models using multiscale tree processes. </title> <booktitle> In Proceedings of the 5th European Conference on Speech Communication and Technology, </booktitle> <pages> pages 1863-1866, </pages> <address> Rhodes, Greece, </address> <year> 1997. </year>
Reference-contexts: During adaption each class utilizes a set of adaptation parameters which are shared amongst all phones contained within that class. Tied Model Translation The simplest transformational approach can be referred to as tied model translation <ref> [44, 73] </ref>. In this approach it is assumed that the speaker adapted center of mass of a phone is simply the addition of the SI center of mass and a translation vector.
Reference: [45] <author> P. Kenny, R. Hollan, V. Gupta, M. Lennig, P. Mermelstein, and D. O'Shaughnessy. </author> <title> A fl -admissible heurustics for rapid lexical access. </title> <booktitle> In Proceedings of the 1991 International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 689-692, </pages> <address> Toronto, Canada, </address> <year> 1991. </year>
Reference-contexts: Furthermore, because the number of phones pairs that could be scored by the consistency model could be O (n 2 ), it may be very inefficient to incorporate the consistency model into a best-first search such as the A fl search <ref> [45, 76] </ref>. An alternative to incorporating the consistency model directly into an A fl search is to use an A fl search to generate an N -best list and then rescore the N -best hypotheses using the consistency model.
Reference: [46] <author> D. Klatt and L. Klatt. </author> <title> Analysis, synthesis, and perception of voice quality variations among female and male talkers. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 87(2), </volume> <pages> pages 820-857, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: On average a female vocal tract is 15% shorter than a male vocal tract, with a majority of the difference occurring in the length of the pharynx <ref> [21, 46] </ref>. This results in vowel formant locations which are measurably different between male and female speakers. As a result acoustic models trained on only male speakers are distinctly different than acoustic models trained on only female speakers.
Reference: [47] <author> T. Kosaka, S. Matsunaga, and S. Sagayama. </author> <title> Tree-structured speaker clustering for speaker-independent continuous speech recognition. </title> <booktitle> In Proceedings of the 1994 International Conference on Spoken Language Processing, </booktitle> <pages> pages 1375-1378, </pages> <address> Yokohama, Japan, </address> <year> 1994. </year>
Reference-contexts: Siegler and Stern also created speaking rate specific acoustic and pronunciation models as well, but were not able to achieve any improvement from these new models. Speaker Clustering One of the most common approaches for providing speaker constraint to a recognition system is speaker clustering <ref> [13, 23, 24, 48, 47, 61, 65, 62, 66] </ref>. The basic of idea of speaker clustering is to create a selection of different models by clustering the training speakers based on some similarity measure. <p> In the extreme, the leaves of the tree represent individual speakers. There are a variety of ways in which the speaker clustered tree can be constructed. The construction can be performed using unsupervised bottom-up clustering based on an acoustic similarity measure <ref> [47, 48] </ref>, unsupervised top-down clustering based on an acoustic similarity measure [23, 62], or some supervised method. In the experiments presented here, the hierarchical tree is manually created. The data is first divided by gender and then subdivided by speaking rate.
Reference: [48] <author> T. Kosaka and S. Sagayama. </author> <title> Tree-structured speaker clustering for fast speaker adaptation. </title> <booktitle> In Proceedings of the 1994 International Conference on Acoustics, Speech and Signal Processing, </booktitle> <volume> volume I, </volume> <pages> pages 245-248, </pages> <address> Adelaide, Australia, </address> <year> 1994. </year>
Reference-contexts: Siegler and Stern also created speaking rate specific acoustic and pronunciation models as well, but were not able to achieve any improvement from these new models. Speaker Clustering One of the most common approaches for providing speaker constraint to a recognition system is speaker clustering <ref> [13, 23, 24, 48, 47, 61, 65, 62, 66] </ref>. The basic of idea of speaker clustering is to create a selection of different models by clustering the training speakers based on some similarity measure. <p> In the extreme, the leaves of the tree represent individual speakers. There are a variety of ways in which the speaker clustered tree can be constructed. The construction can be performed using unsupervised bottom-up clustering based on an acoustic similarity measure <ref> [47, 48] </ref>, unsupervised top-down clustering based on an acoustic similarity measure [23, 62], or some supervised method. In the experiments presented here, the hierarchical tree is manually created. The data is first divided by gender and then subdivided by speaking rate.
Reference: [49] <author> F. Kubala, H. Jin, S. Matsoukas, L. Nguyen, R. Schwartz, and J. Makhoul. </author> <title> Advances in transcription of broadcast news. </title> <booktitle> In Proceedings of the 5th European Conference on Speech Communication and Technology, </booktitle> <pages> pages 927-930, </pages> <address> Rhodes, Greece, </address> <year> 1997. </year> <month> 175 </month>
Reference-contexts: With the development and refinement of the Hidden Markov Model (HMM) approach [7, 8, 57], today's speech recognition systems have been shown to work effectively on various large vocabulary, continuous speech, speaker independent tasks. However, despite the high quality of today's speaker independent (SI) systems <ref> [5, 25, 49] </ref>, there can still be a significant gap in performance between these systems and their speaker dependent (SD) counterparts. As will be discussed in Chapter 3, The reduction in a system's error rate between its speaker independent mode and its speaker dependent mode can be 50% or more.
Reference: [50] <author> H. Kuwabara. </author> <title> Acoustic and perceptual properties of phonemes in continuous speech as a function of speaking rate. </title> <booktitle> In Proceedings of the 5th European Conference on Speech Communication and Technology, </booktitle> <pages> pages 1003-1006, </pages> <address> Rhodes, Greece, </address> <year> 1997. </year>
Reference-contexts: For example, as speaking rate increases a person is less likely to achieve the extreme articulatory positions when producing vowels. As a result formants move towards more central or laxer position when speaking rate is increased <ref> [50] </ref>. Overall, an increase in speaking rate generally results in speech which is more relaxed and less carefully articulated than speech which is spoken slower [15]. As a result, distinct acoustic differences in the acoustic models of fast and slow speakers can be observed.
Reference: [51] <author> L. Lamel, R. Kassel, and S. Seneff. </author> <title> Speech database development: Design and analysis of the acoustic-phonetic corpus. </title> <booktitle> In Proceedings of the DARPA Speech Recognition Workshop, </booktitle> <pages> pages 100-109, </pages> <address> Palo Alto, CA, </address> <month> February </month> <year> 1986. </year>
Reference-contexts: Thus, it is reasonable to wonder whether or not an analysis of the techniques presented in this thesis on the task of phonetic recognition on the TIMIT corpus would have been fruitful <ref> [51] </ref>. In fact some preliminary experiments on TIMIT were conducted. However, these experiments were set aside because the preliminary results when using consistency modeling were not promising. There are likely two reasons why the TIMIT database was not suitable for demonstrating the capabilities of the consistency modeling approach.
Reference: [52] <author> M. Lasry and R. Stern. </author> <title> A posteriori estimation of correlated jointly Gaussian mean vectors. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-6(4), </volume> <pages> pages 530-535, </pages> <month> July </month> <year> 1984. </year>
Reference-contexts: By ignoring these correlations, adaptation towards the correct underlying parameter set for a new speaker may be slower than possible. To account for these correlations within a MAP framework Lasry and Stern developed the Extended MAP adaptation (EMAP) approach <ref> [52, 77] </ref>.
Reference: [53] <author> R. Lau. </author> <title> Adaptive Statistical Language Modelling. </title> <type> Master's thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: It is possible that performance improvements could be gained by accounting for the within-speaker correlations which exist in the duration, pronunciation and language models. The consistency modeling framework is easily extensible to these models. In fact, the consistency model framework is very similar to the trigger-based language modeling approach <ref> [55, 53] </ref>. In trigger-based language modeling, the likelihood of words are adjusted based on the observation of related trigger words. This allows the language model to adjust to the general topic being spoken in an utterance.
Reference: [54] <author> R. Lau, G. Flammia, C. Pao, and V. Zue. </author> <title> WebGalaxy integrating spoken language and hypertext navigation. </title> <booktitle> In Proceedings of the 5th European Conference on Speech Communication and Technology, </booktitle> <pages> pages 883-886, </pages> <address> Rhodes, Greece, </address> <year> 1997. </year>
Reference-contexts: However, there are many applications where supervised, enrolled, batch adaptation is not feasible. For example, the galaxy and jupiter systems typically operate with speakers who are unknown to the system and will only utilize a small number of conversational exchanges to achieve their goal <ref> [33, 54, 89, 84] </ref>. In these cases adaptation must by performed in an unsupervised fashion. Because the number of utterances is small, it is also desirable to perform adaptation in an instantaneous fashion, i.e., adaptation is applied using the same utterance that the system is trying to recognize.
Reference: [55] <author> R. Lau, R. Rosenfeld, and S. Roukos. </author> <title> Trigger-based language models: A maximum entropy approach. </title> <booktitle> In Proceedings of the 1993 International Conference on Acoustics, Speech and Signal Processing, </booktitle> <volume> volume II, </volume> <pages> pages 45-48, </pages> <address> Minneapolis, MN, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: It is possible that performance improvements could be gained by accounting for the within-speaker correlations which exist in the duration, pronunciation and language models. The consistency modeling framework is easily extensible to these models. In fact, the consistency model framework is very similar to the trigger-based language modeling approach <ref> [55, 53] </ref>. In trigger-based language modeling, the likelihood of words are adjusted based on the observation of related trigger words. This allows the language model to adjust to the general topic being spoken in an utterance.
Reference: [56] <author> C. Lee and J. Gauvain. </author> <title> Speaker adaptation based on MAP estimation of HMM parameters. </title> <booktitle> In Proceedings of the 1993 International Conference on Acoustics, Speech and Signal Processing, </booktitle> <volume> volume II, </volume> <pages> pages 558-561, </pages> <address> Minneapolis, MN, </address> <year> 1993. </year>
Reference-contexts: As such, it may be possible to substitute simpler interpolation methods, such as the one utilized in Section 3.3.4, in the place of more complicated MAP algorithms without sacrificing accuracy. The difficult issues surrounding MAP adaptation have been thoroughly investigated and expounded upon by Gauvain and Lee <ref> [26, 27, 28, 29, 30, 56] </ref>. In particular, in [30] they extend the mathematical framework of MAP adaptation to mixture Gaussian density functions and HMM recognizers.
Reference: [57] <author> K. Lee. </author> <title> Large Vocabulary Speaker-Independent Continuous Speech Recognition: The Development of the SPHINX System. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <month> April </month> <year> 1988. </year>
Reference-contexts: Thus, this dissertation will specifically examine the issues involved in utilizing speaker constraint for speech recognition. Over the last ten to twenty years, dramatic improvements in the quality of speaker independent speech recognition technology have been made. With the development and refinement of the Hidden Markov Model (HMM) approach <ref> [7, 8, 57] </ref>, today's speech recognition systems have been shown to work effectively on various large vocabulary, continuous speech, speaker independent tasks.
Reference: [58] <author> L. Lee and R. Rose. </author> <title> Speaker normalization using efficient frequency warping procedures. </title> <booktitle> In Proceedings of the 1996 International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 353-356, </pages> <address> Atlanta, GA, </address> <year> 1996. </year>
Reference-contexts: A number of research efforts have attempted to account for different speaker properties within the modeling schemes of their systems. 73 Vocal Tract Length Normalization Recently there have been numerous research efforts directed at the problem of vocal tract length normalization (VTLN) <ref> [20, 43, 58, 83] </ref>. The basic idea of VTLN is to warp the spectrum of the speech through stretching or compressing so that the relative formant locations of the current speaker match the formant locations of some generic speaker model as closely as possible.
Reference: [59] <author> C. Leggetter and P. Woodland. </author> <title> Speaker adaptation of continuous density HMMs using multivariate linear regression. </title> <booktitle> In Proceedings of the 1994 International Conference on Spoken Language Processing, </booktitle> <pages> pages 451-454, </pages> <address> Yokohama, Japan, </address> <year> 1994. </year>
Reference-contexts: This approach was independently proposed by Leggetter and Woodland <ref> [59, 60] </ref> and by Digalakis, et al. [17, 16, 71].
Reference: [60] <author> C. Leggetter and P. Woodland. </author> <title> Maximum likelihood linear regression for speaker adaptation of continuous density hidden Markov models. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 9(2), </volume> <pages> pages 171-185, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: This approach was independently proposed by Leggetter and Woodland <ref> [59, 60] </ref> and by Digalakis, et al. [17, 16, 71]. <p> A full description of this process can be found in <ref> [60] </ref>. Leggetter and Woodland call their implementation maximum likelihood linear regression (MLLR). Since its introduction, MLLR has become one of the most widely used adaptation methods. <p> These difficulties arise because of the large number of parameters in each Q i matrix which must be estimated. In fact, MLLR has been shown to harm recognition performance when the number of adaptation utterances is small (3 or less) <ref> [60, 83] </ref>. As such, it has not proven useful for rapid or instantaneous adaptation. 3.4.5 Adaptation to Speaker Properties Overview This section describes a series of adaptation techniques which capitalize on prior knowledge about the effects of various speaker properties on the speech signal.
Reference: [61] <author> A. Ljolje. </author> <title> Speaker clustering for improved speech recognition. </title> <booktitle> In Proceedings of the 3rd European Conference on Speech Communication and Technology, </booktitle> <pages> pages 631-634, </pages> <address> Berlin, Germany, </address> <year> 1993. </year>
Reference-contexts: Siegler and Stern also created speaking rate specific acoustic and pronunciation models as well, but were not able to achieve any improvement from these new models. Speaker Clustering One of the most common approaches for providing speaker constraint to a recognition system is speaker clustering <ref> [13, 23, 24, 48, 47, 61, 65, 62, 66] </ref>. The basic of idea of speaker clustering is to create a selection of different models by clustering the training speakers based on some similarity measure.
Reference: [62] <author> L. Mathan and L. Miclet. </author> <title> Speaker hierarchical clustering for improving speaker-independent HMM word recognition. </title> <booktitle> In Proceedings of the 1990 International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 149-152, </pages> <address> Albu-querque, NM, </address> <year> 1990. </year>
Reference-contexts: Siegler and Stern also created speaking rate specific acoustic and pronunciation models as well, but were not able to achieve any improvement from these new models. Speaker Clustering One of the most common approaches for providing speaker constraint to a recognition system is speaker clustering <ref> [13, 23, 24, 48, 47, 61, 65, 62, 66] </ref>. The basic of idea of speaker clustering is to create a selection of different models by clustering the training speakers based on some similarity measure. <p> There are a variety of ways in which the speaker clustered tree can be constructed. The construction can be performed using unsupervised bottom-up clustering based on an acoustic similarity measure [47, 48], unsupervised top-down clustering based on an acoustic similarity measure <ref> [23, 62] </ref>, or some supervised method. In the experiments presented here, the hierarchical tree is manually created. The data is first divided by gender and then subdivided by speaking rate. Three different speaking rate classifications are utilized: fast, medium and slow.
Reference: [63] <author> P. Mermelstein and S. Davis. </author> <title> Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> 28(4), </volume> <pages> pages 357, </pages> <month> August </month> <year> 1980. </year>
Reference-contexts: The observation vectors usually capture spectral information using a spectral representation such as Mel frequency scale cepstral coefficients (MFCC's) <ref> [63] </ref>. In frame-based approaches, such as hidden Markov models (HMM's), recognition uses the frame-based observations directly in the probabilistic framework used for scoring and search. The goal is to find a hypothesized string of words, W 0 , which is most likely given O.
Reference: [64] <author> N. Morgan, E. Fosler, and N. Mirghafori. </author> <title> Speech recognition using on-line estimation of speaking rate. </title> <booktitle> In Proceedings of the 5th European Conference on Speech Communication and Technology, </booktitle> <pages> pages 2079-2082, </pages> <address> Rhodes, Greece, </address> <year> 1997. </year>
Reference-contexts: Both Siegler and Stern [74], and Morgan, et al. <ref> [64] </ref> were able to improve their HMM recognition systems by utilizing speaking rate dependent HMM transition probabilities. The transition probabilities provide the durational modeling capability for an HMM.
Reference: [65] <author> P. Niyogi. </author> <title> Modeling Speaker Variability and Imposing Speaker Constraints in Phonetic Classification. </title> <type> Master's thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <month> December </month> <year> 1991. </year>
Reference-contexts: Siegler and Stern also created speaking rate specific acoustic and pronunciation models as well, but were not able to achieve any improvement from these new models. Speaker Clustering One of the most common approaches for providing speaker constraint to a recognition system is speaker clustering <ref> [13, 23, 24, 48, 47, 61, 65, 62, 66] </ref>. The basic of idea of speaker clustering is to create a selection of different models by clustering the training speakers based on some similarity measure.
Reference: [66] <author> M. Padmanabhan, L. Bahl, D. Nahamoo, and M. Picheny. </author> <title> Speaker clustering and transformation for speaker adaptation in large-vocabulary speech recognition systems. </title> <booktitle> In Proceedings of the 1996 International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 701-704, </pages> <address> Atlanta, GA, </address> <year> 1996. </year>
Reference-contexts: Siegler and Stern also created speaking rate specific acoustic and pronunciation models as well, but were not able to achieve any improvement from these new models. Speaker Clustering One of the most common approaches for providing speaker constraint to a recognition system is speaker clustering <ref> [13, 23, 24, 48, 47, 61, 65, 62, 66] </ref>. The basic of idea of speaker clustering is to create a selection of different models by clustering the training speakers based on some similarity measure.
Reference: [67] <author> D. Paul and J. Baker. </author> <title> The Design for the Wall Street Journal-based CSR corpus. </title> <booktitle> In Proceedings of the DARPA Speech and Natural Language Workshop, </booktitle> <pages> pages 357-362, </pages> <address> Harriman, NY, </address> <month> February </month> <year> 1992. </year>
Reference-contexts: In particular, the Wall Street Journal corpus contains more data while still containing many of the characteristics that made the RM corpus appealing. For instance, the Wall Street Journal (WSJ) corpus contains different data sets for speaker independent and speaker dependent experiments <ref> [67] </ref>. In retrospect, it is clear that the experiments conducted 145 in this thesis would have required more effort to construct and more time to run had they been performed on the WSJ corpus.
Reference: [68] <author> M. Phillips and V. Zue. </author> <title> Automatic discovery of acoustic measurements for phonetic classification. </title> <booktitle> In Proceedings of the 1992 International Conference on Spoken Language Processing, </booktitle> <pages> pages 795-798, </pages> <address> Banff, Canada, </address> <year> 1992. </year>
Reference-contexts: The measurement vectors used by the acoustic segment models are segment-based measurements primarily extracted from averages of frame-based MFCC's. In the context independent system each segment is represented with a 36 dimension vector containing segment-based measurements which were automatically selected based on their ability to perform phonetic discrimination <ref> [68] </ref>. The measurement vectors are all rotated using principal components analysis in order to remove global correlations before training. The boundary models used in the context dependent system are created in the same fashion using measurements extending forwards and backwards from each landmark.
Reference: [69] <author> P. Price, W. Fisher, J. Bernstein, and D. Pallett. </author> <title> The DARPA 1000-word Resource Management database for continuous speech recognition. </title> <booktitle> In Proceedings of the 1988 International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 651-654, </pages> <address> New York, NY, </address> <year> 1988. </year> <month> 177 </month>
Reference-contexts: Paradigm 2 would be used in the context of direct incorporation of speaker correlation information into the speech recognition modeling scheme, such as the consistency modeling approach. In this section, these two methods will be explored on data from the DARPA Resource Management (RM) corpus <ref> [22, 69] </ref>. A complete description of the corpus is provided in Appendix C. 2.2.2 Paradigm 1 One way in which speaker correlation information can be used is to constrain the space of possible speaker dependent (SD) models. Let fi represent the set of parameters in a SD model set. <p> model but rather just a word-pair grammar which specifies which words are allowed to follow any given word. 160 Appendix C The Resource Management Corpus The DARPA Resource Management (RM) Task Domain corpus was developed for the purpose of evaluating speech recognition performance for systems operating in a constrained domain <ref> [22, 69] </ref>. The corpus contains read utterances of 2800 different artificially generated sentences constrained by a predetermined grammar. The vocabulary size of the corpus is roughly 1000 words. The sentences themselves were designed to simulate database queries that might be spoken by naval personal when performing naval resource management tasks.
Reference: [70] <author> L. Rabiner and B. Juang. </author> <title> Fundamentals of Speech Recognition. </title> <publisher> PTR Prentice--Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1993. </year>
Reference-contexts: Because of this dependence on the full past context, the consistency model can not be incorporated into a standard Viterbi search <ref> [70] </ref>. Furthermore, because the number of phones pairs that could be scored by the consistency model could be O (n 2 ), it may be very inefficient to incorporate the consistency model into a best-first search such as the A fl search [45, 76]. <p> The assumptions that are made in deriving the context-independent HMM allow the use of two efficient algorithms, the Baum-Welch algorithm (used to train the probability functions used by the HMM) and the Viterbi algorithm (used to search for the most likely state sequence) <ref> [70] </ref>. However, these assumptions disregard obvious correlations in the speech signal which may be useful when decoding the underlying phonetic content of the signal. In particular, it is the independence assumptions utilized in the acoustic model that are primarily examined in this dissertation.
Reference: [71] <author> A. Sankar, L. Neumeyer, and M. Weintraub. </author> <title> An experimental study of acoustic adaptation algorithms. </title> <booktitle> In Proceedings of the 1996 International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 713-716, </pages> <address> Atlanta, GA, </address> <year> 1996. </year>
Reference-contexts: This approach was independently proposed by Leggetter and Woodland [59, 60] and by Digalakis, et al. <ref> [17, 16, 71] </ref>. <p> The second step is to apply MAP adaptation to the selected speaker cluster model. A number of research efforts have investigated the combination of two or more of the techniques presented above <ref> [16, 71, 81, 83] </ref>. The next three chapters will present three new adaptation approaches which attempt to incorporate some or all of the attributes listed above.
Reference: [72] <author> B. Serridge. </author> <title> Context-dependent Modeling in a Segment-based Speech Recognition System. </title> <type> Master's thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <month> August </month> <year> 1997. </year>
Reference-contexts: Second, an adequate amount of cross-validation data must be produced in order to reliably determine the value for . The use of deleted interpolation is one means of addressing this problem. 1 1 This appendix draws freely from the explanation of deleted interpolation provided in <ref> [72] </ref>. 163 D.2 The EM Algorithm The EM algorithm is an iterative method for learning the parameters of a density function with the goal of maximizing the likelihood score it produces on a set of data.
Reference: [73] <author> K. Shinoda and T. Watanabe. </author> <title> Speaker adaptation with autonomous model complexity control by MDL principle. </title> <booktitle> In Proceedings of the 1996 International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 717-720, </pages> <address> Atlanta, GA, </address> <year> 1996. </year>
Reference-contexts: During adaption each class utilizes a set of adaptation parameters which are shared amongst all phones contained within that class. Tied Model Translation The simplest transformational approach can be referred to as tied model translation <ref> [44, 73] </ref>. In this approach it is assumed that the speaker adapted center of mass of a phone is simply the addition of the SI center of mass and a translation vector.
Reference: [74] <author> M. Siegler and R. Stern. </author> <title> On the effects of speech rate in large vocabulary speech recognition systems. </title> <booktitle> In Proceedings of the 1995 International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 612-615, </pages> <address> Detroit, MI, </address> <year> 1995. </year>
Reference-contexts: Speaking Rate Adaptation Although there is evidence that the speaking rate can have a significant effect on the acoustic realization, duration, and pronunciation of speech, there has been relatively little research directed towards accounting for speaking rate within the models of a speech recognition system. Both Siegler and Stern <ref> [74] </ref>, and Morgan, et al. [64] were able to improve their HMM recognition systems by utilizing speaking rate dependent HMM transition probabilities. The transition probabilities provide the durational modeling capability for an HMM.
Reference: [75] <author> M. Sondhi and B. Gopinath. </author> <title> Determination of vocal-tract shape from impulse responses at the lips. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 49(6), </volume> <pages> pages 1847-1873, </pages> <month> June </month> <year> 1971. </year>
Reference-contexts: However, the size and shape of each individual vocal tract constrains the exact acoustic realization of the various phonetic elements. In fact, there is considerable literature dedicated to the relationship between the acoustic speech signal and the underlying 27 vocal tract configuration and dimensions <ref> [21, 75] </ref>. The physical characteristics of the speaker, combined with other factors such as the speaker's regional dialect, speaking rate and emotional state all combine to constrain the acoustics produced by a speaker.
Reference: [76] <author> F. Soong and E. Huang. </author> <title> A tree-trellis based fast search for finding the N best sentence hypotheses in continuous speech recognition. </title> <booktitle> In Proceedings of the 1991 International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 705-708, </pages> <address> Toronto, Canada, </address> <year> 1991. </year>
Reference-contexts: Furthermore, because the number of phones pairs that could be scored by the consistency model could be O (n 2 ), it may be very inefficient to incorporate the consistency model into a best-first search such as the A fl search <ref> [45, 76] </ref>. An alternative to incorporating the consistency model directly into an A fl search is to use an A fl search to generate an N -best list and then rescore the N -best hypotheses using the consistency model.
Reference: [77] <author> R. Stern and M. Lasry. </author> <title> Dynamic speaker adaptation for feature-based isolated word recognition. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> ASSP-35(6), </volume> <pages> pages 751-763, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: By ignoring these correlations, adaptation towards the correct underlying parameter set for a new speaker may be slower than possible. To account for these correlations within a MAP framework Lasry and Stern developed the Extended MAP adaptation (EMAP) approach <ref> [52, 77] </ref>.
Reference: [78] <author> J. Takahashi and S. Sagayama. </author> <title> Vector-field-smoothed Bayesian learning for incremental speaker adaptation. </title> <booktitle> In Proceedings of the 1995 International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 696-699, </pages> <address> Detroit, MI, </address> <year> 1995. </year>
Reference-contexts: More recent investigations of the problem have introduced methods for performing on-line MAP adaptation [38, 39] and methods for smoothing MAP estimates of mixture Gaussian parameters (i.e., vector field smoothing) <ref> [78, 79] </ref>.
Reference: [79] <author> M. Tonomura, T. Kosaka, and S. Matsunaga. </author> <title> Speaker adaptation based on transfer vector field smoothing using maximum a posteriori probability estimation. </title> <booktitle> In Proceedings of the 1995 International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 688-691, </pages> <address> Detroit, MI, </address> <year> 1995. </year>
Reference-contexts: More recent investigations of the problem have introduced methods for performing on-line MAP adaptation [38, 39] and methods for smoothing MAP estimates of mixture Gaussian parameters (i.e., vector field smoothing) <ref> [78, 79] </ref>.
Reference: [80] <author> B. Winer. </author> <title> Statistical Principles in Experimental Design. </title> <publisher> McGraw-Hill, </publisher> <address> New York, NY, </address> <year> 1971. </year> <month> 178 </month>
Reference-contexts: These measurements are rotated using eigen analysis. The rotated measurements are ranked by the amount of variance they contribute across all acoustic observations. The ranked set of rotated measurements is also known as the set of principal components of the system <ref> [80] </ref>. In order to reduce the set of measurements used by the consistency model, it is useful to learn which of the principal components account for the most within-speaker correlation information. In Section 2.2.3 of Chapter 2 a method for measuring the within-speaker correlation exhibited between two phones was presented.
Reference: [81] <author> G. Zavaliagkos, R. Schwartz, and J. Makhoul. </author> <title> Batch, incremental and instanta-neous adaptation techniques for speech recognition. </title> <booktitle> In Proceedings of the 1995 International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 676-679, </pages> <address> Detroit, MI, </address> <year> 1995. </year>
Reference-contexts: For comparison, full covariance MAP adaptation requires M D 2 covariance parameters be trained. Even with simplifying assumptions, EMAP adaptation is extremely difficult to implement. Despite the difficulties, research efforts by Huo and Lee [40, 41] and by Zavaliagkos, et al <ref> [81, 82] </ref>, have utilized EMAP adaptation with modest success. 3.4.3 Model Prediction Approaches In addition to the EMAP approach there have been a number of approaches which follow the same motivation as EMAP, but utilize different slightly frameworks. <p> The second step is to apply MAP adaptation to the selected speaker cluster model. A number of research efforts have investigated the combination of two or more of the techniques presented above <ref> [16, 71, 81, 83] </ref>. The next three chapters will present three new adaptation approaches which attempt to incorporate some or all of the attributes listed above.
Reference: [82] <author> G. Zavaliagkos, R. Schwartz, and John McDonough. </author> <title> Maximum a posteriori adaptation for large scale HMM recognizers. </title> <booktitle> In Proceedings of the 1996 International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 725-728, </pages> <address> Atlanta, GA, </address> <year> 1996. </year>
Reference-contexts: For comparison, full covariance MAP adaptation requires M D 2 covariance parameters be trained. Even with simplifying assumptions, EMAP adaptation is extremely difficult to implement. Despite the difficulties, research efforts by Huo and Lee [40, 41] and by Zavaliagkos, et al <ref> [81, 82] </ref>, have utilized EMAP adaptation with modest success. 3.4.3 Model Prediction Approaches In addition to the EMAP approach there have been a number of approaches which follow the same motivation as EMAP, but utilize different slightly frameworks.
Reference: [83] <author> P. Zhan, M. Westphal, M. Finke, and A. Waibel. </author> <title> Speaker normalization and speaker adaptation a combination for conversational speech recognition. </title> <booktitle> In Proceedings of the 5th European Conference on Speech Communication and Technology, </booktitle> <pages> pages 2087-2090, </pages> <address> Rhodes, Greece, </address> <year> 1997. </year>
Reference-contexts: These difficulties arise because of the large number of parameters in each Q i matrix which must be estimated. In fact, MLLR has been shown to harm recognition performance when the number of adaptation utterances is small (3 or less) <ref> [60, 83] </ref>. As such, it has not proven useful for rapid or instantaneous adaptation. 3.4.5 Adaptation to Speaker Properties Overview This section describes a series of adaptation techniques which capitalize on prior knowledge about the effects of various speaker properties on the speech signal. <p> A number of research efforts have attempted to account for different speaker properties within the modeling schemes of their systems. 73 Vocal Tract Length Normalization Recently there have been numerous research efforts directed at the problem of vocal tract length normalization (VTLN) <ref> [20, 43, 58, 83] </ref>. The basic idea of VTLN is to warp the spectrum of the speech through stretching or compressing so that the relative formant locations of the current speaker match the formant locations of some generic speaker model as closely as possible. <p> The second step is to apply MAP adaptation to the selected speaker cluster model. A number of research efforts have investigated the combination of two or more of the techniques presented above <ref> [16, 71, 81, 83] </ref>. The next three chapters will present three new adaptation approaches which attempt to incorporate some or all of the attributes listed above.
Reference: [84] <author> V. Zue. </author> <title> Conversational interfaces: </title> <booktitle> Advances and challenges. In Proceedings of the 5th European Conference on Speech Communication and Technology, pages KN9-KN18, </booktitle> <address> Rhodes, Greece, </address> <year> 1997. </year>
Reference-contexts: However, there are many applications where supervised, enrolled, batch adaptation is not feasible. For example, the galaxy and jupiter systems typically operate with speakers who are unknown to the system and will only utilize a small number of conversational exchanges to achieve their goal <ref> [33, 54, 89, 84] </ref>. In these cases adaptation must by performed in an unsupervised fashion. Because the number of utterances is small, it is also desirable to perform adaptation in an instantaneous fashion, i.e., adaptation is applied using the same utterance that the system is trying to recognize.
Reference: [85] <author> V. Zue, J. Glass, D. Goodine, H. Leung, M. Phillips, J. Polifroni, and S. Seneff. </author> <title> Recent progress on the summit system. </title> <booktitle> In Proceedings of the DARPA Speech and Natural Language Workshop, </booktitle> <pages> pages 380-384, </pages> <address> Hidden Valley, PA, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: speech recognition algorithms an be developed which which utilize the strong points of the HMM approach while correcting for the HMM's shortcomings. 151 152 Appendix B The SUMMIT Recognizer B.1 Probabilistic Framework The summit system is a segment-based speech recognition engine developed in the Spoken Language Systems Group at MIT <ref> [32, 85, 86, 87, 88] </ref>. The summit system is based on a probabilistic framework describing the speech recognition problem. In this thesis summit is utilized to perform word recognition. The goal during recognition is to find the word sequence which is most likely given the acoustic information.
Reference: [86] <author> V. Zue, J. Glass, D. Goodine, M. Phillips, and S. Seneff. </author> <title> The summit speech recognition system: Phonological modeling and lexical access. </title> <booktitle> In Proceedings of the 1990 International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 49-52, </pages> <address> Alburquerque, NM, </address> <year> 1990. </year>
Reference-contexts: speech recognition algorithms an be developed which which utilize the strong points of the HMM approach while correcting for the HMM's shortcomings. 151 152 Appendix B The SUMMIT Recognizer B.1 Probabilistic Framework The summit system is a segment-based speech recognition engine developed in the Spoken Language Systems Group at MIT <ref> [32, 85, 86, 87, 88] </ref>. The summit system is based on a probabilistic framework describing the speech recognition problem. In this thesis summit is utilized to perform word recognition. The goal during recognition is to find the word sequence which is most likely given the acoustic information.
Reference: [87] <author> V. Zue, J. Glass, M. Phillips, and S. Seneff. </author> <title> Acoustic segmentation and phonetic classification in the summit system. </title> <booktitle> In Proceedings of the 1989 International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 389-392, </pages> <address> Glasgow, Scotland, </address> <year> 1989. </year>
Reference-contexts: speech recognition algorithms an be developed which which utilize the strong points of the HMM approach while correcting for the HMM's shortcomings. 151 152 Appendix B The SUMMIT Recognizer B.1 Probabilistic Framework The summit system is a segment-based speech recognition engine developed in the Spoken Language Systems Group at MIT <ref> [32, 85, 86, 87, 88] </ref>. The summit system is based on a probabilistic framework describing the speech recognition problem. In this thesis summit is utilized to perform word recognition. The goal during recognition is to find the word sequence which is most likely given the acoustic information.
Reference: [88] <author> V. Zue, J. Glass, M. Phillips, and S. Seneff. </author> <title> The MIT summit speech recognition system: A progress report. </title> <booktitle> In Proceedings of the DARPA Speech and Natural Language Workshop, </booktitle> <pages> pages 179-189, </pages> <address> Philadelphia, PA, </address> <month> February </month> <year> 1989. </year>
Reference-contexts: speech recognition algorithms an be developed which which utilize the strong points of the HMM approach while correcting for the HMM's shortcomings. 151 152 Appendix B The SUMMIT Recognizer B.1 Probabilistic Framework The summit system is a segment-based speech recognition engine developed in the Spoken Language Systems Group at MIT <ref> [32, 85, 86, 87, 88] </ref>. The summit system is based on a probabilistic framework describing the speech recognition problem. In this thesis summit is utilized to perform word recognition. The goal during recognition is to find the word sequence which is most likely given the acoustic information.
Reference: [89] <author> V. Zue, S. Seneff, J. Glass, L. Hetherington, E. Hurley, H. Meng, C. Pao, J. Po-lifroni, R. Schloming, and P. Schmid. </author> <title> From interface to content: translingual access and delivery of on-line information. </title> <booktitle> In Proceedings of the 5th Euro-pean Conference on Speech Communication and Technology, </booktitle> <pages> pages 2227-2230, </pages> <address> Rhodes, Greece, </address> <year> 1997. </year> <month> 179 </month>
Reference-contexts: Thus, the time needed to provide the initial set of adaptation data is viewed as a worthwhile investment. On the other hand, there are many applications, such as the Jupiter weather information server <ref> [89] </ref>, where a user interacts with the system for only a few utterances. For applications such as these, it is not practical for the user to provide enrollment data for the purpose of adaptation. In this case the most difficult style of adaptation must be utilized: unsupervised, instantaneous, on-line adaptation. <p> However, there are many applications where supervised, enrolled, batch adaptation is not feasible. For example, the galaxy and jupiter systems typically operate with speakers who are unknown to the system and will only utilize a small number of conversational exchanges to achieve their goal <ref> [33, 54, 89, 84] </ref>. In these cases adaptation must by performed in an unsupervised fashion. Because the number of utterances is small, it is also desirable to perform adaptation in an instantaneous fashion, i.e., adaptation is applied using the same utterance that the system is trying to recognize.
References-found: 89

