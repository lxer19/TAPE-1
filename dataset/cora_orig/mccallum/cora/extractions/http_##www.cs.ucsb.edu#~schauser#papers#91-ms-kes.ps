URL: http://www.cs.ucsb.edu/~schauser/papers/91-ms-kes.ps
Refering-URL: http://www.cs.ucsb.edu/~schauser/papers/
Root-URL: http://www.cs.ucsb.edu
Title: Compiling Dataflow into Threads Efficient Compiler-Controlled Multithreading for Lenient Parallel Languages Approval for the Report
Author: KLAUS ERIK SCHAUSER Committee: Prof. David E. Culler Research Advisor // Date Prof. D. A. Patterson 
Degree: in partial satisfaction of the requirements for the degree of Master of Science,  
Note: RESEARCH PROJECT Submitted to the  Plan II.  Second Reader 7/9/91 Date  
Date: July 2, 1991  
Affiliation: Department of Electrical Engineering and Computer Science Computer Science Division University of California, Berkeley  Department of Electrical Engineering and Computer Sciences, University of California, Berkeley,  
Abstract-found: 0
Intro-found: 0
Reference: [ACC + 90] <author> R. Alverson, D. Callahan, D. Cummings, B. Koblenz, A. Porterfield, and B. Smith. </author> <title> The Tera Computer System. </title> <booktitle> In Proc. of the 1990 Int. Conf. on Supercomputing, </booktitle> <pages> pages 1-6, </pages> <address> Amsterdam, </address> <year> 1990. </year>
Reference-contexts: Many researchers suggest that processors should support multiple instruction streams and switch very rapidly between them in response to remote memory reference latencies or synchronization <ref> [AI87, Smi90, HF88, ALKK90, ACC + 90] </ref>. However, the proposed architectural solutions make thread scheduling invisible to the compiler, preventing it from applying optimizations that might reduce the cost of thread switching or improve scheduling based on analysis of the program.
Reference: [ACI + 83] <author> Arvind, D. E. Culler, R. A. Iannucci, V. Kathail, K. Pingali, and R. E. Thomas. </author> <title> The Tagged Token Dataflow Architecture. </title> <type> Technical Report FLA memo, </type> <institution> MIT Lab for Comp. Sci., </institution> <type> 545 Tech. </type> <institution> Square, </institution> <address> Cambridge, MA, </address> <month> August </month> <year> 1983. </year> <note> Revised October, </note> <year> 1984. </year>
Reference: [ACM88] <author> Arvind, D. E. Culler, and G. K. Maa. </author> <title> Assessing the Benefits of Fine-Grain Parallelism in Dataflow Programs. </title> <journal> The Int. Journal of Supercomputer Applications, </journal> <volume> 2(3), </volume> <month> November </month> <year> 1988. </year>
Reference-contexts: This results in an overlap of computation between the function body and the argument evaluation. Non-strict, but eager, parallel functional languages, such as Id90, exhibit a large amount of parallelism on all levels <ref> [ACM88, Cul90, AE88] </ref>. Id90 would would exhibit less parallelism if the language were lazy, because then evaluation of arguments could only be started when functions need the corresponding value.
Reference: [AE88] <author> Arvind and K. Ekanadham. </author> <title> Future Scientific Programming on Parallel Machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5(5) </volume> <pages> 460-493, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: This results in an overlap of computation between the function body and the argument evaluation. Non-strict, but eager, parallel functional languages, such as Id90, exhibit a large amount of parallelism on all levels <ref> [ACM88, Cul90, AE88] </ref>. Id90 would would exhibit less parallelism if the language were lazy, because then evaluation of arguments could only be started when functions need the corresponding value. <p> Paraffins [AHN88] enumerates the distinct isomers of paraffins of size up to 14. Gamteb is a Monte Carlo neutron transport code [BCS + 89]. It is highly recursive with many conditionals. Simple is a hydrodynamics and heat conduction code widely used 59 as an application benchmark, rewritten in Id90 <ref> [CHR78, AE88] </ref>. One iteration is run on 50 fi 50 matrices.
Reference: [AHN88] <author> Arvind, S. K. Heller, and R. S. Nikhil. </author> <title> Programming Generality and Parallel Computers. </title> <booktitle> In Proc. of the Fourth Int. Symp. on Biological and Artificial Intelligence Systems, </booktitle> <pages> pages 255-286. </pages> <address> ESCOM (Leider), Trento, Italy, </address> <month> September </month> <year> 1988. </year>
Reference-contexts: DTW implements a dynamic time warp algorithm used in discrete word speech recognition [Sah91]. The size of the test template and number of cepstral coefficients is 100. Speech is used to determine cepstral coefficients for speech processing. We take 10240 speech samples and compute 30 cepstral coefficients. Paraffins <ref> [AHN88] </ref> enumerates the distinct isomers of paraffins of size up to 14. Gamteb is a Monte Carlo neutron transport code [BCS + 89]. It is highly recursive with many conditionals. Simple is a hydrodynamics and heat conduction code widely used 59 as an application benchmark, rewritten in Id90 [CHR78, AE88].
Reference: [AI87] <author> Arvind and R. A. </author> <title> Iannucci. Two Fundamental Issues in Multiprocessing. </title> <booktitle> In Proc. of DFVLR - Conf. 1987 on Par. Proc. in Science and Eng., </booktitle> <address> Bonn-Bad Godesberg, </address> <publisher> W. </publisher> <address> Germany, </address> <month> June </month> <year> 1987. </year>
Reference-contexts: Many researchers suggest that processors should support multiple instruction streams and switch very rapidly between them in response to remote memory reference latencies or synchronization <ref> [AI87, Smi90, HF88, ALKK90, ACC + 90] </ref>. However, the proposed architectural solutions make thread scheduling invisible to the compiler, preventing it from applying optimizations that might reduce the cost of thread switching or improve scheduling based on analysis of the program.
Reference: [ALKK90] <author> A. Agarwal, B. Lim, D. Kranz, and J. Kubiatowicz. </author> <month> APRIL: </month> <title> A Processor Architecture for Multiprocessing. </title> <booktitle> In Proc. of the 17th Ann. Int. Symp. on Comp. Arch., </booktitle> <pages> pages 104-114, </pages> <address> Seattle, Washington, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: Many researchers suggest that processors should support multiple instruction streams and switch very rapidly between them in response to remote memory reference latencies or synchronization <ref> [AI87, Smi90, HF88, ALKK90, ACC + 90] </ref>. However, the proposed architectural solutions make thread scheduling invisible to the compiler, preventing it from applying optimizations that might reduce the cost of thread switching or improve scheduling based on analysis of the program.
Reference: [ANP87] <author> Arvind, R. S. Nikhil, and K. K. Pingali. I-Structures: </author> <title> Data Structures for Parallel Computing. </title> <type> Technical Report CSG Memo 269, </type> <institution> MIT Lab for Comp. Sci., </institution> <type> 545 Tech. </type> <institution> Square, </institution> <address> Cambridge, MA, </address> <month> February </month> <year> 1987. </year> <title> (Also in Proc. of the Graph Reduction Workshop, </title> <address> Santa Fe, NM. </address> <month> October </month> <year> 1986.). </year>
Reference-contexts: TAM provides a specialized form of send to support split-phase access to data structures. 10 The heap is assumed to be distributed over processors, so access to a data element may require interprocessor communication. In addition, accesses may be synchronizing, as with I-structures <ref> [ANP87] </ref> where a read of an empty element is deferred until the corresponding store takes place. The I-Read, I-Fetch, and I-Take operations generate a request for a particular heap location and the response is received by an inlet.
Reference: [AU77] <author> A. V. Aho and J. D. Ullman. </author> <title> Principles of Compiler Design. </title> <publisher> Addison-Wesley Pub. Co., </publisher> <address> Reading, Mass., </address> <year> 1977. </year>
Reference-contexts: TAM can be efficiently implemented on standard sequential and parallel machines, Both the compilation of sequential languages for standard architectures <ref> [AU77] </ref> as well as the compilation of non-strict parallel languages for dataflow machines [Tra86] is well understood. The storage model of conventional machines is directly reflected in most sequential languages.
Reference: [BCS + 89] <author> P. J. Burns, M. Christon, R. Schweitzer, O. M. Lubeck, H. J. Wasserman, M. L. Sim-mons, and D. V. Pryor. </author> <title> Vectorization of Monte-Carlo Particle Transport: An Architectural Study using the LANL Benchmark "Gamteb". </title> <booktitle> In Proc. Supercomputing '89. IEEE Computer Society and ACM SIGARCH, </booktitle> <address> New York, NY, </address> <month> November </month> <year> 1989. </year>
Reference-contexts: Speech is used to determine cepstral coefficients for speech processing. We take 10240 speech samples and compute 30 cepstral coefficients. Paraffins [AHN88] enumerates the distinct isomers of paraffins of size up to 14. Gamteb is a Monte Carlo neutron transport code <ref> [BCS + 89] </ref>. It is highly recursive with many conditionals. Simple is a hydrodynamics and heat conduction code widely used 59 as an application benchmark, rewritten in Id90 [CHR78, AE88]. One iteration is run on 50 fi 50 matrices.
Reference: [BP89] <author> M. Beck and K. Pingali. </author> <title> From Control Flow to Dataflow. </title> <type> Technical Report TR 89-1050, </type> <institution> CS Dep., Cornell University, </institution> <month> October </month> <year> 1989. </year>
Reference-contexts: Other backends translate program graphs for the Monsoon dataflow machine [PC90] and for the P-RISC machine [NA89]. Some other very interesting approaches compile FORTRAN programs into representations similar to program graphs <ref> [BP89, FOW87] </ref>. 13 Chapter 2 Dual Graphs Compilation of Id90 to TAM begins after generation of dataflow program graphs. The meaning of program graphs is given in terms of a dataflow firing rule, so control flow is implicitly prescribed by the dynamic propagation of values.
Reference: [CFR + 89] <author> R. Cytron, J. Ferrante, B. K. Rosen, M. N. Wegman, and F. K. Zadeck. </author> <title> An Efficient Method of Computing Static Single Assignment Form. </title> <booktitle> In Proc. of the 16th Annual ACM Symp. on Principles of Progr. Lang., </booktitle> <pages> pages 25-35, </pages> <address> Los Angeles, </address> <month> January </month> <year> 1989. </year>
Reference-contexts: Dual graphs are similar in form to the data structures used in most optimizing compilers; the key differences are that they describe parallel control flow and are in static single assignment form <ref> [CFR + 89] </ref>. Compilation to TAM involves a series of transformations on the dual graph, including partitioning, fork and join insertion, lifetime analysis, register and frame-slot allocation, scheduling and linearization.
Reference: [CHR78] <author> W. P. Crowley, C. P. Hendrickson, and T. E. Rudy. </author> <title> The SIMPLE code. </title> <type> Technical Report UCID 17715, </type> <institution> Lawrence Livermore Laboratory, </institution> <month> February </month> <year> 1978. </year>
Reference-contexts: Paraffins [AHN88] enumerates the distinct isomers of paraffins of size up to 14. Gamteb is a Monte Carlo neutron transport code [BCS + 89]. It is highly recursive with many conditionals. Simple is a hydrodynamics and heat conduction code widely used 59 as an application benchmark, rewritten in Id90 <ref> [CHR78, AE88] </ref>. One iteration is run on 50 fi 50 matrices.
Reference: [CSS + 91] <author> D. Culler, A. Sah, K. Schauser, T. von Eicken, and J. Wawrzynek. </author> <title> Fine-grain Parallelism with Minimal Hardware Support: A Compiler-Controlled Threaded Abstract Machine. </title> <booktitle> In Proc. of 4th Int. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Santa-Clara, CA, </address> <month> April </month> <year> 1991. </year> <note> (Also available as Technical Report UCB/CSD 91/591, </note> <institution> CS Div., University of California at Berkeley). </institution> <month> 70 </month>
Reference-contexts: Synchronization, thread scheduling and storage management are explicit in the machine language and, thus, exposed to the compiler. TAM is presented elsewhere <ref> [CSS + 91, vESC91] </ref>; in this section we describe the salient features of TAM as a compilation target. <p> Registers can be used to carry values between threads within a quantum. Empirical studies show that quanta are often large, crossing many points of possible suspension <ref> [CSS + 91] </ref>. Thus, it is advantageous for the compiler to be able to keep values in registers between threads that it cannot prove will execute in a single quantum. The leave and enter threads can save and restore specific registers if the guess proves incorrect.
Reference: [Cul90] <author> D. E. Culler. </author> <title> Managing Parallelism and Resources in Scientific Dataflow Programs. </title> <type> Technical Report 446, </type> <institution> MIT Lab for Comp. Sci., </institution> <month> March </month> <year> 1990. </year> <type> (PhD Thesis, </type> <institution> Dept. of EECS, MIT). </institution>
Reference-contexts: This results in an overlap of computation between the function body and the argument evaluation. Non-strict, but eager, parallel functional languages, such as Id90, exhibit a large amount of parallelism on all levels <ref> [ACM88, Cul90, AE88] </ref>. Id90 would would exhibit less parallelism if the language were lazy, because then evaluation of arguments could only be started when functions need the corresponding value. <p> the previous reconnection step has made them unnecessary. 2.4 Example The program graph for the lookup example from Figure 1.2 (page 9) includes a function DEF node, enclosing a LOOP node, enclosing an IF node, as shown in Figure 2.19. form, i.e. where only one iteration is active at once <ref> [Cul90] </ref>. The compiler actually produces a slightly more complicated dual graph; we have omitted the trigger inlet and its arcs. The four arguments enter at the inlet nodes at the top of the graph. The control outputs are joined before arriving at the merge.
Reference: [FOW87] <author> J. Ferrante, K. Ottenstein, and J. Warren. </author> <title> The Program Dependence Graph and its Use in Optimization . ACM Transactions on Programming Languages and Systems, </title> <booktitle> 9(3) </booktitle> <pages> 319-349, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: Other backends translate program graphs for the Monsoon dataflow machine [PC90] and for the P-RISC machine [NA89]. Some other very interesting approaches compile FORTRAN programs into representations similar to program graphs <ref> [BP89, FOW87] </ref>. 13 Chapter 2 Dual Graphs Compilation of Id90 to TAM begins after generation of dataflow program graphs. The meaning of program graphs is given in terms of a dataflow firing rule, so control flow is implicitly prescribed by the dynamic propagation of values.
Reference: [GH90] <author> V. G. Grafe and J. E. Hoch. </author> <title> The Epsilon-2 Hybrid Dataflow Architecture. </title> <booktitle> In Proc. of Compcon90, </booktitle> <pages> pages 88-93, </pages> <address> San Francisco, CA, </address> <month> March </month> <year> 1990. </year>
Reference-contexts: By compiling via TAM, we have achieved more than two orders of magnitude performance improvement over graph interpreters on conventional machines, making this Id90 implementation competitive with machines supporting dynamic instruction scheduling in hardware <ref> [PC90, SYH + 89, GH90, Ian88b] </ref>. Timing measurements show that our Id90 implementation on a standard RISC can achieve a performance close to Id90 on the recent dataflow machine Monsoon. <p> Ian-nucci integrates thread generation and register assignment to a limited extent; registers are assumed to vanish at every possible suspension point or control transfer. This style of register usage is incorporated in recent dataflow machines, including Monsoon [PT91], Epsilon <ref> [GH90] </ref> and EM-4 [SYH + 89], allowing partitioning similar to the hybrid model. * DE ME: For TAM we use our best partitioning: dependence sets partitioning with merging. The dynamic measurements presented in this and subsequent sections were all collected on one node of a multiprocessor nCUBE/2.
Reference: [Hal85] <author> R. H. Halstead, Jr. </author> <title> Multilisp: A Language for Concurrent Symbolic Computation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(4) </volume> <pages> 501-538, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: However, the proposed architectural solutions make thread scheduling invisible to the compiler, preventing it from applying optimizations that might reduce the cost of thread switching or improve scheduling based on analysis of the program. Inherently parallel languages, such as Id90 [Nik90] and Multilisp <ref> [Hal85] </ref>, require that small execution threads be scheduled dynamically, even if executed on a single processor [Tra88]. Traub's theoretical work demonstrates how to minimize thread switching for these languages on sequential machines.
Reference: [HF88] <author> R. H. Halstead, Jr. and T. Fujita. MASA: </author> <title> a Multithreaded Processor Architecture for Parallel Symbolic Computing. </title> <booktitle> In Proc. of the 15th Int. Symp. on Comp. Arch., </booktitle> <pages> pages 443-451, </pages> <address> Hawaii, </address> <month> May </month> <year> 1988. </year>
Reference-contexts: Many researchers suggest that processors should support multiple instruction streams and switch very rapidly between them in response to remote memory reference latencies or synchronization <ref> [AI87, Smi90, HF88, ALKK90, ACC + 90] </ref>. However, the proposed architectural solutions make thread scheduling invisible to the compiler, preventing it from applying optimizations that might reduce the cost of thread switching or improve scheduling based on analysis of the program.
Reference: [Hic91] <author> J. Hicks. </author> <title> Report on Running Id Applications on Monsoon. </title> <type> Technical Report FLA memo, </type> <institution> MIT Lab for Comp. Sci., </institution> <type> 545 Tech. </type> <institution> Square, </institution> <address> Cambridge, MA, </address> <month> March </month> <year> 1991. </year>
Reference-contexts: Column three gives the run-time for Monsoon, as reported in <ref> [Hic91] </ref>. The fourth columns uses the backend that directly translates TAM into native MIPS code, while the last column gives run-times for the backend that first expands TAM into C code and then compiles it for the MIPS.
Reference: [Ian88a] <author> R. A. </author> <title> Iannucci. A Dataflow/von Neumann Hybrid Architecture. </title> <type> Technical Report TR-418, </type> <institution> MIT Lab for Comp. Sci., </institution> <type> 545 Tech. </type> <institution> Square, </institution> <address> Cambridge, MA, </address> <month> May </month> <year> 1988. </year> <type> (PhD Thesis, </type> <institution> Dept. of EECS, MIT). </institution>
Reference-contexts: None of these examples present problems for a machine with dynamic instruction scheduling, such as Monsoon [PC90]. At the same time, none require dynamic scheduling throughout. Thus, it makes sense to investigate hybrid execution modes <ref> [Ian88a] </ref>, where statically ordered threads are scheduled dynamically. Our TAM model takes this idea one step further by exposing the scheduling of threads to the compiler as well, so even the dynamic scheduling is done without hardware support. <p> Simple, switch and outlet nodes are placed into the partition of their control predecessor. Dataflow partitioning also yields safe TAM partitions. 3.2.3 Dependence Sets Partitioning Far more powerful is dependence sets partitioning, which is based on a variant of Iannucci's method of dependence sets <ref> [Ian88a] </ref>.
Reference: [Ian88b] <author> R. A. </author> <title> Iannucci. Toward a Dataflow/von Neumann Hybrid Architecture. </title> <booktitle> In Proc. 15th Int. Symp. on Comp. Arch., </booktitle> <pages> pages 131-140, </pages> <address> Hawaii, </address> <month> May </month> <year> 1988. </year>
Reference-contexts: By compiling via TAM, we have achieved more than two orders of magnitude performance improvement over graph interpreters on conventional machines, making this Id90 implementation competitive with machines supporting dynamic instruction scheduling in hardware <ref> [PC90, SYH + 89, GH90, Ian88b] </ref>. Timing measurements show that our Id90 implementation on a standard RISC can achieve a performance close to Id90 on the recent dataflow machine Monsoon. <p> Hardware provides two-way synchronization as token matching on binary operations; unary operations do not require matching, so they can be scheduled into the pipeline following the instruction on which they depend. * DE: Threads produced using dependence sets partitioning without merging correspond closely to Scheduling Quanta in Iannucci's hybrid architecture <ref> [Ian88b] </ref>. Ian-nucci integrates thread generation and register assignment to a limited extent; registers are assumed to vanish at every possible suspension point or control transfer.
Reference: [NA89] <author> R. S. Nikhil and Arvind. </author> <title> Can Dataflow Subsume von Neumann Computing? In Proc. </title> <booktitle> of the 16th Annual Int. Symp. on Comp. Arch., </booktitle> <address> Jerusalem, Israel, </address> <month> May </month> <year> 1989. </year>
Reference-contexts: The original MIT compiler translated program graphs into dataflow graphs which then could be executed on the Tagged Token Dataflow Architecture (TTDA)[ACI + 83] or be interpreted by a graph interpreter (GITA). Other backends translate program graphs for the Monsoon dataflow machine [PC90] and for the P-RISC machine <ref> [NA89] </ref>. Some other very interesting approaches compile FORTRAN programs into representations similar to program graphs [BP89, FOW87]. 13 Chapter 2 Dual Graphs Compilation of Id90 to TAM begins after generation of dataflow program graphs. <p> It is also possible for a restricted model to avoid the need to re-initialize synchronization variables, which could slightly improve the the total number of instruction executed with dataflow partitioning. For example, Monsoon and P-RISC <ref> [NA89] </ref> provide hardware support for two-way synchronization. They associate individual bits with each synchronizing operation or thread, and toggle the bit on each synchronization access. The first operand will set this bit, the second reset it.
Reference: [Nik90] <author> R. S. Nikhil. </author> <note> Id (Version 90.0) Reference Manual. Technical Report CSG Memo, to appear, </note> <institution> MIT Lab for Comp. Sci., </institution> <type> 545 Tech. </type> <institution> Square, </institution> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: However, the proposed architectural solutions make thread scheduling invisible to the compiler, preventing it from applying optimizations that might reduce the cost of thread switching or improve scheduling based on analysis of the program. Inherently parallel languages, such as Id90 <ref> [Nik90] </ref> and Multilisp [Hal85], require that small execution threads be scheduled dynamically, even if executed on a single processor [Tra88]. Traub's theoretical work demonstrates how to minimize thread switching for these languages on sequential machines. <p> The actual thread generation, which includes frame slot and register assignment, instruction scheduling and 7 thread scheduling, is discussed in Chapter 4. Finally, preliminary results are presented in Chapter 5. 1.2 Language Issues We are using the parallel functional language Id90 <ref> [Nik90] </ref> as a starting point. Id90 is a non-strict but eager language. Traub uses the term lenient for this class of languages [Tra88]. A language is called non-strict, if it may be necessary to start computation of a function body before all (or even any) arguments have been provided.
Reference: [Nik91] <author> R. S. Nikhil. </author> <title> The Parallel Programming Language Id and its Compilation for Parallel Machines. </title> <booktitle> In Proc. Workshop on Massive Parallelism, </booktitle> <address> Amalfi, Italy, October 1989. </address> <publisher> Academic Press, </publisher> <year> 1991. </year> <note> Also: CSG Memo 313, </note> <institution> MIT Laboratory for Computer Science, 545 Technology Square, </institution> <address> Cambridge, MA 02139, USA. </address>
Reference-contexts: Flat produces a list of the leaves of a binary tree using accumulation lists which are constructed with the build-in function cons. If cons and flat were strict, this code would exhibit no parallelism. Under lenient execution, the entire list is constructed in parallel <ref> [Nik91] </ref>. Simulations showed that with non-strictness the critical path on an example of a full binary tree of depth 10 constists of 250 time steps; with a maximum parallelism of 1776 and an average parallelism of 266 instructions (assuming the resources were available).
Reference: [PC90] <author> G. M. Papadopoulos and D. E. Culler. Monsoon: </author> <title> an Explicit Token-Store Architecture. </title> <booktitle> In Proc. of the 17th Annual Int. Symp. on Comp. Arch., </booktitle> <address> Seattle, Washington, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: Because of the three mutually recursive bindings, no top to bottom static ordering of the statements yields a correct execution. None of these examples present problems for a machine with dynamic instruction scheduling, such as Monsoon <ref> [PC90] </ref>. At the same time, none require dynamic scheduling throughout. Thus, it makes sense to investigate hybrid execution modes [Ian88a], where statically ordered threads are scheduled dynamically. <p> The original MIT compiler translated program graphs into dataflow graphs which then could be executed on the Tagged Token Dataflow Architecture (TTDA)[ACI + 83] or be interpreted by a graph interpreter (GITA). Other backends translate program graphs for the Monsoon dataflow machine <ref> [PC90] </ref> and for the P-RISC machine [NA89]. Some other very interesting approaches compile FORTRAN programs into representations similar to program graphs [BP89, FOW87]. 13 Chapter 2 Dual Graphs Compilation of Id90 to TAM begins after generation of dataflow program graphs. <p> By compiling via TAM, we have achieved more than two orders of magnitude performance improvement over graph interpreters on conventional machines, making this Id90 implementation competitive with machines supporting dynamic instruction scheduling in hardware <ref> [PC90, SYH + 89, GH90, Ian88b] </ref>. Timing measurements show that our Id90 implementation on a standard RISC can achieve a performance close to Id90 on the recent dataflow machine Monsoon.
Reference: [PT91] <author> G. M. Papadopoulos and K. R. Traub. </author> <title> Multithreading: A Revisionist View of Dataflow Architectures. </title> <booktitle> In Proc. of the 18th Int. Symp. on Comp. Arch., </booktitle> <pages> pages 342-351, </pages> <address> Toronto, Canada, </address> <month> May </month> <year> 1991. </year>
Reference-contexts: Ian-nucci integrates thread generation and register assignment to a limited extent; registers are assumed to vanish at every possible suspension point or control transfer. This style of register usage is incorporated in recent dataflow machines, including Monsoon <ref> [PT91] </ref>, Epsilon [GH90] and EM-4 [SYH + 89], allowing partitioning similar to the hybrid model. * DE ME: For TAM we use our best partitioning: dependence sets partitioning with merging. The dynamic measurements presented in this and subsequent sections were all collected on one node of a multiprocessor nCUBE/2.
Reference: [Sah91] <author> A. Sah. </author> <title> Parallel Language Support for Shared memory multiprocessors. </title> <type> Master's thesis, </type> <institution> Computer Science Div., University of California at Berkeley, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: TAM is concrete enough though that it can be translated with ease in a second translation step to actual machines. One translation path has chosen C as a portable "intermediate form" and is producing code for parallel machines like the Sequent Symetry <ref> [Sah91] </ref> and Motorola Delta, as well as for various standard sequential machines. Other backends translate TAM directly into machine code. Currently there exists backends for the MIPS processor and for the parallel Ncube/2. The figure also shows in gray other compilation approaches that chose program graphs as their intermediate form. <p> Thirty iterations are run on matrices of size 100fi100. DTW implements a dynamic time warp algorithm used in discrete word speech recognition <ref> [Sah91] </ref>. The size of the test template and number of cepstral coefficients is 100. Speech is used to determine cepstral coefficients for speech processing. We take 10240 speech samples and compute 30 cepstral coefficients. Paraffins [AHN88] enumerates the distinct isomers of paraffins of size up to 14.
Reference: [Smi90] <author> B. Smith. </author> <title> Keynote Address. </title> <booktitle> Proc. of the 17th Annual Int. Symp. on Comp. Arch., </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: Many researchers suggest that processors should support multiple instruction streams and switch very rapidly between them in response to remote memory reference latencies or synchronization <ref> [AI87, Smi90, HF88, ALKK90, ACC + 90] </ref>. However, the proposed architectural solutions make thread scheduling invisible to the compiler, preventing it from applying optimizations that might reduce the cost of thread switching or improve scheduling based on analysis of the program.
Reference: [SYH + 89] <author> S. Sakai, Y. Yamaguchi, K. Hiraki, Y. Kodama, and T. Yuba. </author> <title> An Architecture of a Dataflow Single Chip Processor. </title> <booktitle> In Proc. of the 16th Annual Int. Symp. on Comp. Arch., </booktitle> <pages> pages 46-53, </pages> <address> Jerusalem, Israel, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: By compiling via TAM, we have achieved more than two orders of magnitude performance improvement over graph interpreters on conventional machines, making this Id90 implementation competitive with machines supporting dynamic instruction scheduling in hardware <ref> [PC90, SYH + 89, GH90, Ian88b] </ref>. Timing measurements show that our Id90 implementation on a standard RISC can achieve a performance close to Id90 on the recent dataflow machine Monsoon. <p> Ian-nucci integrates thread generation and register assignment to a limited extent; registers are assumed to vanish at every possible suspension point or control transfer. This style of register usage is incorporated in recent dataflow machines, including Monsoon [PT91], Epsilon [GH90] and EM-4 <ref> [SYH + 89] </ref>, allowing partitioning similar to the hybrid model. * DE ME: For TAM we use our best partitioning: dependence sets partitioning with merging. The dynamic measurements presented in this and subsequent sections were all collected on one node of a multiprocessor nCUBE/2.
Reference: [Tra86] <author> K. R. Traub. </author> <title> A Compiler for the MIT Tagged-Token Dataflow Architecture. </title> <type> Technical Report TR-370, </type> <institution> MIT Lab for Comp. Sci., </institution> <type> 545 Tech. </type> <institution> Square, </institution> <address> Cambridge, MA, </address> <month> August </month> <year> 1986. </year> <type> (MS Thesis, </type> <institution> Dept. of EECS, MIT). </institution>
Reference-contexts: TAM can be efficiently implemented on standard sequential and parallel machines, Both the compilation of sequential languages for standard architectures [AU77] as well as the compilation of non-strict parallel languages for dataflow machines <ref> [Tra86] </ref> is well understood. The storage model of conventional machines is directly reflected in most sequential languages. The control structures provided by those languages can be easily mapped to standard processors which follow only one path of execution. <p> This involves analysis of expected quantum boundaries, frame and register assignment under asynchronous thread scheduling, and generation of inlets. 1.4 Id90-to-TAM Compilation Stages are first translated into program graphs by an Id90 front-end from MIT <ref> [Tra86] </ref>. Program graphs are a hierarchical graphical intermediate form with only one kind of arc. Program graphs allow a representation of the various basic operations as well as conditionals, function definition and application. This facilitates powerful high-level optimizations, such as motion of arbitrarily large program constructs across loops or conditionals. <p> This is a local transformation and can be described by giving the expansion rules for the individual program graph nodes. Program graphs are a hierarchical graphical intermediate form and described in <ref> [Tra86] </ref>. The lowest level is represented by basic operations. Larger programs can be built up using program graph encapsulators, including conditionals, loops, function definition, and application.
Reference: [Tra88] <author> K. R. Traub. </author> <title> Sequential Implementation of Lenient Programming Languages. </title> <type> Technical Report TR-417, </type> <institution> MIT Lab for Comp. Sci., </institution> <type> 545 Tech. </type> <institution> Square, </institution> <address> Cambridge, MA, </address> <month> September </month> <year> 1988. </year> <type> (PhD Thesis, </type> <institution> Dept. of EECS, MIT). </institution>
Reference-contexts: Inherently parallel languages, such as Id90 [Nik90] and Multilisp [Hal85], require that small execution threads be scheduled dynamically, even if executed on a single processor <ref> [Tra88] </ref>. Traub's theoretical work demonstrates how to minimize thread switching for these languages on sequential machines. <p> Finally, preliminary results are presented in Chapter 5. 1.2 Language Issues We are using the parallel functional language Id90 [Nik90] as a starting point. Id90 is a non-strict but eager language. Traub uses the term lenient for this class of languages <ref> [Tra88] </ref>. A language is called non-strict, if it may be necessary to start computation of a function body before all (or even any) arguments have been provided. Conversely, it is called strict if all arguments can be evaluated before calling a function. <p> The final example, due to Traub <ref> [Tra88] </ref>, shows a cyclic dependence through a conditional that must be resolved dynamically. Because of the three mutually recursive bindings, no top to bottom static ordering of the statements yields a correct execution. None of these examples present problems for a machine with dynamic instruction scheduling, such as Monsoon [PC90]. <p> Architecturally, providing fork, rather than branch, is interesting because it allows instruction fetch and execution to be decoupled without branch prediction. From a compiling viewpoint, mixing forks and branches presents very tricky code-generation issues <ref> [Tra88] </ref>, especially in handling "non-strict" conditionals where computations must started even if not all inputs to the conditional are available. It should be noted, however, that when mapping TAM to native code for existing machines fork and stop are removed whenever possible to produce a branch or fall-through. <p> This aspect is constrained partly by the language and partly by the execution model. The language dictates which portions of the program can be scheduled statically and which require dynamic synchronization. An elegant theoretical framework for addressing the language requirements is provided by Traub's work <ref> [Tra88] </ref>. The execution model places further constraints on partitioning, since synchronization only occurs at the entry to a thread and conditional execution occurs only between threads. The second aspect is management of processor and storage resources in the context of dynamic scheduling to gain maximum performance. <p> Identifying portions of the dual graph that can be executed as a thread is called partitioning. 30 31 32 Chapter 3 Partitioning The fundamental step in compiling a lenient language for a machine that executes instruction sequences is partitioning the program into statically schedulable entities <ref> [Tra88] </ref>. Limits on partitioning are imposed by dependence cycles that can only be resolved dynamically. In Id90 these arise due to conditionals, function calls, and accesses to I-structures.
Reference: [vESC91] <author> T. von Eicken, K. E. Schauser, and D. E. Culler. TL0: </author> <title> An Implementation of the TAM Threaded Abstract Machine, Version 2.1. </title> <type> Technical Report, </type> <institution> Computer Science Div., University of California at Berkeley, </institution> <year> 1991. </year> <month> 71 </month>
Reference-contexts: Synchronization, thread scheduling and storage management are explicit in the machine language and, thus, exposed to the compiler. TAM is presented elsewhere <ref> [CSS + 91, vESC91] </ref>; in this section we describe the salient features of TAM as a compilation target.
References-found: 33

