URL: http://www.pdl.cs.cmu.edu/PDL-FTP/ParityLogging/ISCA93.ps
Refering-URL: http://www.pdl.cs.cmu.edu/PDL-FTP/RAID/ComputingSurveys.biblio.html
Root-URL: 
Title: RAID level 1: Mirroring Figure 1 Data Layouts. In nonredundant disk arrays, data units are
Author:  Daniel Stodolsky, Garth Gibson, and Mark Holland 
Address: 5000 Forbes Avenue Pittsburgh, PA 15213-3890  Diego CA.  
Affiliation: School of Computer Science and Department of Electrical and Computer Engineering Carnegie Mellon University  San  
Note: Section 1: Introduction  To appear in the 20th Annual International Symposium on Computer Architecture,  
Pubnum: D0 D1 D2 D3 D8 D9 D10 D11  D0 D0 D1 D1 D4 D4 D5 D5  D8 D8 D9 D9 3  D0 D1 D2 P0-2 D6 D7 D8 P6-8  D0 D1 D2 P0-2 D8 P6-8 D6 D7  
Email: Daniel.Stodolsky@cmu.edu  
Date: 5  May 16-19, 1993,  
Abstract: Parity encoded redundant disk arrays provide highly reliable, cost effective secondary storage with high performance for read accesses and large write accesses. Their performance on small writes, however, is much worse than mirrored disks the traditional, highly reliable, but expensive organization for secondary storage. Unfortunately, small writes are a substantial portion of the I/O workload of many important, demanding applications such as on-line transaction processing. This paper presents parity logging, a novel solution to the small write problem for redundant disk arrays. Parity logging applies journalling techniques to substantially reduce the cost of small writes. We provide a detailed analysis of parity logging and competing schemes mirroring, oating storage, and RAID level 5 and verify these models by simulation. Parity logging provides performance competitive with mirroring, the best of the alternative single failure tolerating disk array organizations. However, its overhead cost is close to the minimum offered by RAID level 5. Finally, parity logging can exploit data caching much more effectively than all three alternative approaches. The market for disk arrays, collections of independent magnetic disks linked together as a single data store, is undergoing rapid growth and has been predicted to exceed 7 billion dollars by 1994 [Jones91]. This growth has been driven by three factors. First, the growth in processor speed has outstripped the growth in disk data rates, requiring multiple disks for adequate bandwidth. Second, arrays of small diameter disks often have substantial cost, power, and performance advantages over larger drives. Third, low cost encoding schemes preserve most of these advantages while providing high data reliability (without redundancy, large disk arrays have unacceptably low data reliability because of their large number of component disks). For these three reasons, redundant disk arrays, also known as Redundant Arrays of Inexpensive Disks (RAID), are strong candidates for nearly all on-line secondary storage systems [Gibson92]. Figure 1 presents an overview of RAID systems considered in this paper. The most promising variant employs rotated parity with data striped on a unit that is one or more disk sectors [Lee91]. This configuration is commonly known as the RAID level 5 organization [Patterson88]. RAID level 5 arrays exploit the low cost of parity encoding to provide high data reliability [Gibson93]. Data is striped over all disks so that large files can be fetched with high bandwidth. By rotating the parity, many small random blocks can also be accessed in parallel without hot spots on any disk. While RAID level 5 disk arrays offer performance and reliability advantages for a wide variety of applications, they are commonly thought to possess at least one critical limitation: their throughput is penalized by a factor of four over nonredundant arrays for workloads of mostly small writes. A small write may require prereading the old value of the users data, overwriting this with new user data, prereading the old value of the corresponding parity, then overwriting this second disk block with the updated parity. In contrast, mirrored disks simply write the 
Abstract-found: 1
Intro-found: 1
Reference: [Bhide92] <author> A. Bhide and D. Dias, </author> <title> Raid Architectures for OLTP, </title> <institution> IBM Computer Science Research Report RC 17879, </institution> <year> 1992. </year>
Reference-contexts: ML F RN ML F R ML F R 4142 69 49 103 56 138 Nonredundant Mirroring Parity Logging Floating Data/Parity RAID 5 0-Failure Tolerating 1-Failure Tolerating 2-Failure Tolerating 3-Failure Tolerating Disk milliseconds per small write Section 7: Related Work Bhide and Dias <ref> [Bhide92] </ref> have independently developed a scheme similar to parity logging. Their LRAID-X4 organization maintains separate parity and parity update log disks, and periodically applies the logged updates to the parity disk.
Reference: [Bitton88] <author> D. Bitton and J. Gray, </author> <title> Disk Shadowing, </title> <booktitle> Proceedings of the 14th Conference on Very Large Data Bases, </booktitle> <year> 1988, </year> <pages> pp. 331-338. </pages>
Reference: [Gibson89] <author> G. Gibson, L. Hellerstein, R. M. Karp, R. H. Katz, and D. A. Patterson, </author> <title> Coding Techniques for Handling Failures in Large Disk Arrays, </title> <booktitle> Third International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS III), </booktitle> <publisher> ACM Press, </publisher> <year> 1989, </year> <pages> pp 123-132. </pages>
Reference-contexts: Section 6: Multiple failure tolerating arrays Another significant advantage of parity logging is its efficient extension to multiple failure tolerating arrays. Multiple failure tolerance provides much longer mean time to data loss and greater tolerance for bad blocks discovered during reconstruction <ref> [Gibson89] </ref>. Using codes more powerful than parity, RAID level 5 and its variants, oating data and parity and parity logging, can all be extended to tolerate concurrent failures. Figure 20 gives an example of one of the more easily understood double failure tolerant disk array organizations. <p> This paper does not consider the choice of codes that might be used for failure protection, except to note that these codes all have one property important to small random write performance <ref> [Gibson89] </ref>: each small write updates disks disks containing check information (generalized parity) and the disk containing the users data. This check maintenance work, which scales up with the number of failures tolerated, is exactly the work that parity logging is designed to handle more efficiently.
Reference: [Gibson92] <author> G. Gibson, </author> <title> Redundant Disk Arrays: Reliable, Parallel Secondary Storage, </title> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: Head Scheduling: FIFO Power/Cabling: Disks independently powered/cabled Array Parameters and spatial distribution are typical of OLTP workloads, while a 100% write ratio emphasizes the performance differences of the various techniques. Since the disks have independent support hardware, disk failures will be independent, allowing a single parity group <ref> [Gibson92] </ref>. Disk parameters are modeled on the IBM Lightning drive [IBM0661]. Note that the dist term in the seek time model is the number of cylinders traversed, excluding the destination.
Reference: [Gibson93] <author> G. Gibson and D. Patterson, </author> <title> Designing Disk Arrays for High Data Reliability, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <month> January, </month> <year> 1993, </year> <pages> pp. 4-27 </pages>
Reference: [Holland92] <author> M. Holland and G. Gibson, </author> <title> Parity Declustering for Continuous Operation in Redundant Disk Arrays, </title> <booktitle> Proceedings of ASPLOS-V, </booktitle> <year> 1992, </year> <pages> pp. 23-35. </pages>
Reference: [IBM0661] <author> IBM Corporation, </author> <title> IBM 0661 Disk Drive Product Description, Model 370, First Edition, Low End Storage Products, </title> <type> 504/114-2, </type> <year> 1989. </year>
Reference-contexts: Since the disks have independent support hardware, disk failures will be independent, allowing a single parity group [Gibson92]. Disk parameters are modeled on the IBM Lightning drive <ref> [IBM0661] </ref>. Note that the dist term in the seek time model is the number of cylinders traversed, excluding the destination. As is traditional, the track skew is chosen to equal the head switch time, optimizing data layout for sequential multitrack access. These disks do not support zero latency writes.
Reference: [Jones91] <author> J. Jones, Jr., and T. Liu, </author> <title> RAID: A Technology Poised for Explosive Growth, </title> <type> Montgomery Securities Industry Report, Montgomery Securities, </type> <address> San Francisco, </address> <year> 1991 </year>
Reference: [Lee91] <author> E. Lee and R. Katz, </author> <title> Performance Consequences of Parity Placement in Disk Arrays, </title> <booktitle> Proceedings of ASPLOS-IV, </booktitle> <year> 1991, </year> <pages> pp. 190-199. </pages>
Reference-contexts: Parity logging was simulated for several different degrees of log striping 5 . The RAIDSIM package, a disk array simulator derived from the Sprite operating system disk array driver <ref> [Ousterhout88, Lee91] </ref>, was extended with implementations of parity logging and oat parity and data. In each simulation, the request stream was generated by 66 processes (i.e, three per disk). Each process requests a 5. A single track was buffered per region in all parity logging simulations.
Reference: [Menon92] <author> J. Menon and J. Kasson, </author> <title> Methods for Improved Update Performance of Disk Arrays, </title> <booktitle> Proceedings of the Hawaii International Conference on System Sciences, </booktitle> <year> 1992, </year> <pages> pp. 74-83. </pages>
Reference-contexts: Section 4: Alternative Schemes Few other authors have addressed the problem of high performance yet reliable disk storage for small write work-loads. The most notable of these is oating data and parity <ref> [Menon92] </ref>. This section reviews and estimates the performance of four configurations: mirrored disks (RAID level 1), nonredundant disk arrays (RAID level 0), distributed N+1 parity (RAID level 5), and oating data and parity. The notation and analysis methodology are the same as used in the previous section.
Reference: [Ousterhout88] <author> J. Ousterhout, et. al., </author> <title> The Sprite Network Operating System, </title> <booktitle> IEEE Computer, </booktitle> <month> February </month> <year> 1988, </year> <pages> pp. 23-36. </pages>
Reference-contexts: Parity logging was simulated for several different degrees of log striping 5 . The RAIDSIM package, a disk array simulator derived from the Sprite operating system disk array driver <ref> [Ousterhout88, Lee91] </ref>, was extended with implementations of parity logging and oat parity and data. In each simulation, the request stream was generated by 66 processes (i.e, three per disk). Each process requests a 5. A single track was buffered per region in all parity logging simulations.
Reference: [Patterson88] <author> D. Patterson, G. Gibson, and R. Katz, </author> <title> A Case for Redundant Arrays of Inexpensive Disks (RAID), </title> <booktitle> Proceedings of the ACM SIGMOD Conference, </booktitle> <year> 1988, </year> <pages> pp. 109-116. </pages>
Reference: [Rosenblum91] <author> M. Rosenblum and J. Ousterhout, </author> <title> The Design and Implementation of a Log-Structured File System, </title> <booktitle> Proceedings of the 13th ACM Symposium on Operating System Principles, </booktitle> <year> 1991, </year> <pages> pp. 1-15. </pages>
Reference: [Salem86] <author> K. Salem, H. Garcia-Molina, </author> <title> Disk Striping, </title> <booktitle> Proceedings of the 2nd IEEE International Conference on Data Engineering, </booktitle> <year> 1986. </year>
Reference-contexts: When a regions fault tolerant buffers fill, the buffers will be written to one of the regions sublogs in a single K track write. The cost of this operation is the same as in the unstriped case. 3. Disks that support zero-latency writes <ref> [Salem86] </ref> can eliminate the initial rotational positioning delay.
Reference: [TPCA89] <author> The TPC-A Benchmark: </author> <title> A Standard Specification, Transaction Processing Performance Council, </title> <year> 1989. </year>
References-found: 15

