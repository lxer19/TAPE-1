URL: http://www.aic.nrl.navy.mil/~spears/papers/mlj93.ps.gz
Refering-URL: http://www.cs.gmu.edu:80/research/gag/pubs.html
Root-URL: 
Title: Using Genetic Algorithms for Concept Learning  
Author: Kenneth A. De Jong, William M. Spears and Diana F. Gordon and 
Keyword: Key words: concept learning, genetic algorithms, bias adjustment  
Address: Washington, D.C. 20375  Fairfax, VA 22030  
Affiliation: Naval Research Laboratory  George Mason University  
Abstract: In this paper, we explore the use of genetic algorithms (GAs) as a key element in the design and implementation of robust concept learning systems. We describe and evaluate a GA-based system called GABIL that continually learns and refines concept classification rules from its interaction with the environment. The use of GAs is motivated by recent studies showing the effects of various forms of bias built into different concept learning systems, resulting in systems that perform well on certain concept classes (generally, those well matched to the biases) and poorly on others. By incorporating a GA as the underlying adaptive search mechanism, we are able to construct a concept learning system that has a simple, unified architecture with several important features. First, the system is surprisingly robust even with minimal bias. Second, the system can be easily extended to incorporate traditional forms of bias found in other concept learning systems. Finally, the architecture of the system encourages explicit representation of such biases and, as a result, provides for an important additional feature: the ability to dynamically adjust system bias. The viability of this approach is illustrated by comparing the performance of GABIL with that of four other more traditional concept learners (AQ14, C4.5, ID5R, and IACL) on a variety of target concepts. We conclude with some observations about the merits of this approach and about possible extensions. 
Abstract-found: 1
Intro-found: 1
Reference: <editor> Baeck, T., Hoffmeister, F., & Schwefel, H. </editor> <year> (1991). </year> <title> A survey of evolution strategies. </title> <booktitle> Proceedings of the Fourth International Conference on Genetic Algorithms (pp. 2 - 9). </booktitle> <address> La Jolla, CA: </address> <publisher> Morgan Kaufmann. </publisher> <year> 1991. </year>
Reference-contexts: The result should be a system capable of performing the search for the best set of operators (biases) and the search for the best hypotheses in parallel (see <ref> (Baeck et. al., 1991) </ref> for related work). Such an approach is easily implemented by adding to each individual additional control bits (one for each adaptive operator). Each bit determines whether the corresponding operator can be used on that individual.
Reference: <author> Booker, L. </author> <year> (1989). </year> <title> Triggered rule discovery in classifier systems. </title> <booktitle> Proceedings of the Third International Conference on Genetic Algorithms (pp. </booktitle> <volume> 265 - 274). </volume> <publisher> Fairfax, </publisher> <address> VA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: See (Wilson, 1987) and <ref> (Booker, 1989) </ref> for examples of the Michigan approach. - 6 - rules. The number of rules in a particular individual can be unrestricted or limited by a user-defined upper bound.
Reference: <author> Braudaway, W. & Tong, C. </author> <year> (1989). </year> <title> Automated synthesis of constrained generators. </title> <booktitle> Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (pp. 583 - 589). </booktitle> <address> Detroit, MI: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Davis, L. </author> <year> (1989). </year> <title> Adapting operator probabilities in genetic algorithms. </title> <booktitle> Proceedings of the Third International Conference on Genetic Algorithms (pp. </booktitle> <volume> 61 - 69). </volume> <publisher> Fairfax, </publisher> <address> VA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This issue will be raised again later in this paper. An uncoupled approach does not rely upon the GA for the adaptive mechanism. Rather, the behavior of the underlying GA is adjusted by a separate control mechanism (see <ref> (Davis, 1989) </ref> and (Janikow, 1991) for examples). While this may alleviate the problems associated with coupling, such mechanisms appear to be difficult to construct, and involve complicated bookkeeping. Although we may explore this route in future work, we concentrate on the conceptually simpler coupled approach in this paper.
Reference: <author> De Jong, K. </author> <year> (1987). </year> <title> Using genetic algorithms to search program spaces. </title> <booktitle> Proceedings of the Second International Conference on Genetic Algorithms (pp. </booktitle> <volume> 210 - 216). </volume> <publisher> Cam-bridge, </publisher> <address> MA: </address> <publisher> Lawrence Erlbaum. - 25 - De Jong, </publisher> <editor> K. & Spears, W. </editor> <year> (1989). </year> <title> Using genetic algorithms to solve NP-complete problems. </title> <booktitle> Proceedings of the Third International Conference on Genetic Algorithms (pp. </booktitle> <volume> 124 - 132). </volume> <publisher> Fairfax, </publisher> <address> VA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This must be done carefully to preserve the properties that make the GAs effective adaptive search procedures (see <ref> (DeJong, 1987) </ref> for a more detailed discussion). The traditional internal representation of GAs involves using fixed-length (generally binary) strings to represent points in the space to be searched. <p> The few notable exceptions that adjust the algorithmic bias include the Competitive Relation Learner and Induce and Select Optimizer combination (CRL/ISO) (Tcheng et. al., 1989), Climbing in the Bias Space (ClimBS) (Provost, 1991), PEAK (Holder, 1990), the Variable Bias Management System (VBMS) <ref> (Rendell et. al., 1987) </ref>, and the Genetic-based - 23 - Inductive Learner (GIL) (Janikow, 1991). We can classify these systems according to the type of bias that they select. Adaptive GABIL shifts its bias by dynamically selecting generalization operators.
Reference: <author> De Jong, K. & Spears, W. </author> <year> (1991). </year> <title> Learning concept classification rules using genetic algorithms. </title> <booktitle> Proceedings of the Twelfth International Joint Conference on Artificial Intelligence (pp. 651 - 656). </booktitle> <address> Sydney, Australia: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: database, with 70% of the instances in the training set and 30% in the test set, 72% of the predictions made on the test set were correct predictions (see Gordon, 1990). 10 An explanation of the difficulty of systems based on ID3 on target concepts of this type is in <ref> (De Jong & Spears, 1991) </ref>. - 16 - implemented as additional "genetic" operators to be used by GABIL's GA search procedure (see Figure 2).
Reference: <author> Goldberg, D. </author> <year> (1989). </year> <title> Genetic algorithms in search, optimization, </title> <booktitle> and machine learning. </booktitle> <address> New York: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: The decision to adopt a minimalist approach has immediate implications for the choice of concept description languages. We need to identify a language that can be _______________ 1 Excellent introductions to GAs can be found in (Holland, 1975) and <ref> (Goldberg, 1989) </ref>. - 4 - effectively mapped into string representations and yet retains the necessary expressive power to represent complex concept descriptions efficiently. As a consequence, we have chosen a simple, yet general rule language for describing concepts, the details of which are presented in the following sections. 2.1.
Reference: <author> Gordon, D. </author> <year> (1990). </year> <title> Active bias adjustment for incremental, supervised concept learning. </title> <type> Doctoral dissertation, </type> <institution> Computer Science Department, University of Maryland, College Park, MD. </institution>
Reference-contexts: The chosen systems are AQ14 (Mozetic, 1985), which is based on the AQ algorithm described in (Michalski, 1983), C4.5 (Quinlan, unpublished), ID5R (Utgoff, 1988), and Iba's Algorithm Concept Learner (IACL) <ref> (Gordon, 1990) </ref>, which is based on Iba's algorithm described in (Iba, 1979). AQ14 and IACL form modified DNF hypotheses. The C4.5 and ID5R systems are based on the ID algorithm described in (Quinlan, 1986), and form decision tree hypotheses. <p> We have modified ID5R to make a random prediction in this case. 7 The fourth system to be compared with GABIL is IACL <ref> (Gordon, 1990) </ref>, a system similar to AQ14. IACL is not as well-known as the other systems described above and, therefore, we describe it in slightly more detail. IACL maintains two DNF hypotheses, one for learning the target concept, and one for learning the negation of that concept. <p> IACL's MSG operator is well-suited for learning when the instance features are structured nominals (i.e., have generalization trees to structure their values) or numeric, but is not well-suited for learning when the features are (unstructured) nominals. According to <ref> (Gordon, 1990) </ref>, IACL performs very well on the numeric form of the BC database. 9 Other experiments, which are not reported here, indicate that the conversion of the BC data to a nominal form does not adversely affect performance for AQ14 and C4.5. <p> We selected a subset of these biases to be _______________ 9 When run in batch mode on the numeric BC database, with 70% of the instances in the training set and 30% in the test set, 72% of the predictions made on the test set were correct predictions <ref> (see Gordon, 1990) </ref>. 10 An explanation of the difficulty of systems based on ID3 on target concepts of this type is in (De Jong & Spears, 1991). - 16 - implemented as additional "genetic" operators to be used by GABIL's GA search procedure (see Figure 2). <p> When this operator is applied to a particular member of the population (i.e., a particular rule set), each disjunct is deterministically checked for possible condition dropping. The decision to drop a condition is based on a criterion from <ref> (Gordon, 1990) </ref> and involves examining the bits of each feature in each disjunct. If more than half of the bits of a feature in a disjunct are 1s, then the remaining 0 bits are changed to 1s. <p> We are also extending our experimental analysis to include other systems which attempt to dynamically adjust their bias. 6. Related Work on Bias Adjustment Adaptive bias, in our context, is similar to dynamic preference (bias) adjustment for concept learning (see <ref> (Gordon, 1990) </ref> for related literature). The vast majority of concept learning systems that adjust their bias focus on changing their representational bias.
Reference: <author> Greene, D. & Smith, S. </author> <year> (1987). </year> <title> A genetic system for learning models of consumer choice. </title> <booktitle> Proceedings of the Second International Conference on Genetic Algorithms (pp. 217 - 223). </booktitle> <address> Cambridge, MA: </address> <publisher> Lawrence Erlbaum. </publisher>
Reference-contexts: There is still much to be learned about the relative merits of the two approaches. In this paper we report on results obtained from using the Pittsburgh approach. 2 That is, each individual in the population is a variable-length string representing an unordered set of fixed-length _______________ 2 <ref> (Greene & Smith, 1987) </ref> and (Janikow, 1991) have also used the Pittsburgh approach. See (Wilson, 1987) and (Booker, 1989) for examples of the Michigan approach. - 6 - rules. The number of rules in a particular individual can be unrestricted or limited by a user-defined upper bound.
Reference: <author> Grefenstette, John J. </author> <year> (1986). </year> <title> Optimization of control parameters for genetic algorithms. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> Vol. SMC-16, No. </volume> <pages> 1. </pages>
Reference-contexts: The within-problem approach adapts a GA dynamically, as it solves one problem. In contrast, the across-problem approach adapts GAs over the course of many problems. One good example of the across-problem approach is provided by <ref> (Grefenstette, 1986) </ref>. In this paper, a separate meta-GA is used to adapt a GA as it solves a suite of problems. The advantage of such an approach is that the resulting system performs robustly on a suite of problems.
Reference: <author> Grefenstette, John J. </author> <year> (1989). </year> <title> A system for learning control strategies with genetic algorithms. </title> <booktitle> Proceedings of the Third International Conference on Genetic Algorithms (pp. </booktitle> <volume> 183 - 190). </volume> <publisher> Fairfax, </publisher> <address> Virginia: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Holder, L. </author> <year> (1990). </year> <title> The general utility problem in machine learning. </title> <booktitle> Proceedings of the Seventh International Conference on Machine Learning (pp. 402 - 410). </booktitle> <address> Austin, Texas: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The few notable exceptions that adjust the algorithmic bias include the Competitive Relation Learner and Induce and Select Optimizer combination (CRL/ISO) (Tcheng et. al., 1989), Climbing in the Bias Space (ClimBS) (Provost, 1991), PEAK <ref> (Holder, 1990) </ref>, the Variable Bias Management System (VBMS) (Rendell et. al., 1987), and the Genetic-based - 23 - Inductive Learner (GIL) (Janikow, 1991). We can classify these systems according to the type of bias that they select. Adaptive GABIL shifts its bias by dynamically selecting generalization operators.
Reference: <author> Holland, J. </author> <year> (1975). </year> <booktitle> Adaptation in natural and artificial systems. </booktitle> <address> Ann Arbor, MI: </address> <publisher> The University of Michigan Press. </publisher>
Reference-contexts: The decision to adopt a minimalist approach has immediate implications for the choice of concept description languages. We need to identify a language that can be _______________ 1 Excellent introductions to GAs can be found in <ref> (Holland, 1975) </ref> and (Goldberg, 1989). - 4 - effectively mapped into string representations and yet retains the necessary expressive power to represent complex concept descriptions efficiently.
Reference: <author> Holland, J. </author> <year> (1986). </year> <title> Escaping brittleness: The possibilities of general-purpose learning algorithms applied to parallel rule-based systems. </title> <editor> In R. Michalski, J. Carbonell, </editor> <booktitle> T. </booktitle>
Reference-contexts: The right-hand side of a rule is simply the class (concept) to which the example belongs. This means that our rule language defines a "stimulus-response" system with no message passing or any other form of internal memory such as those found in <ref> (Holland, 1986) </ref>. In many of the traditional concept learning contexts, there is only a single concept to be learned. In these situations there is no need for the rules to have an explicit right-hand side, since the class is implied.
Reference: <editor> Mitchell (Eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach. </booktitle> <address> Los Altos: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Iba, G. </author> <year> (1979). </year> <title> Learning disjunctive concepts from examples. </title> <institution> Massachusetts Institute of Technology A.I. </institution> <address> Memo 548, Cambridge, MA. </address>
Reference-contexts: The chosen systems are AQ14 (Mozetic, 1985), which is based on the AQ algorithm described in (Michalski, 1983), C4.5 (Quinlan, unpublished), ID5R (Utgoff, 1988), and Iba's Algorithm Concept Learner (IACL) (Gordon, 1990), which is based on Iba's algorithm described in <ref> (Iba, 1979) </ref>. AQ14 and IACL form modified DNF hypotheses. The C4.5 and ID5R systems are based on the ID algorithm described in (Quinlan, 1986), and form decision tree hypotheses. AQ14 and C4.5 are run in batch-incremental mode since they are batch systems. ID5R and IACL are incremental.
Reference: <author> Janikow, C. </author> <year> (1991). </year> <title> Inductive learning of decision rules from attribute-based examples: A knowledge-intensive genetic algorithm approach. </title> <institution> TR91-030, The University of North Carolina at Chapel Hill, Dept. of Computer Science, Chapel Hill, NC. </institution>
Reference-contexts: In this paper we report on results obtained from using the Pittsburgh approach. 2 That is, each individual in the population is a variable-length string representing an unordered set of fixed-length _______________ 2 (Greene & Smith, 1987) and <ref> (Janikow, 1991) </ref> have also used the Pittsburgh approach. See (Wilson, 1987) and (Booker, 1989) for examples of the Michigan approach. - 6 - rules. The number of rules in a particular individual can be unrestricted or limited by a user-defined upper bound. <p> This issue will be raised again later in this paper. An uncoupled approach does not rely upon the GA for the adaptive mechanism. Rather, the behavior of the underlying GA is adjusted by a separate control mechanism (see (Davis, 1989) and <ref> (Janikow, 1991) </ref> for examples). While this may alleviate the problems associated with coupling, such mechanisms appear to be difficult to construct, and involve complicated bookkeeping. Although we may explore this route in future work, we concentrate on the conceptually simpler coupled approach in this paper. <p> adjust the algorithmic bias include the Competitive Relation Learner and Induce and Select Optimizer combination (CRL/ISO) (Tcheng et. al., 1989), Climbing in the Bias Space (ClimBS) (Provost, 1991), PEAK (Holder, 1990), the Variable Bias Management System (VBMS) (Rendell et. al., 1987), and the Genetic-based - 23 - Inductive Learner (GIL) <ref> (Janikow, 1991) </ref>. We can classify these systems according to the type of bias that they select. Adaptive GABIL shifts its bias by dynamically selecting generalization operators.
Reference: <author> Koza, J. R. </author> <year> (1991). </year> <title> Concept formation and decision tree induction using the genetic programming paradigm. </title> <editor> In H. P. Schwefel and R. Maenner (Eds.), </editor> <title> Parallel Problem - 26 - Solving from Nature. </title> <publisher> Berlin: Springer-Verlag. </publisher>
Reference: <author> Michalski, R. </author> <year> (1983). </year> <title> A theory and methodology of inductive learning. </title> <editor> In R. Michalski, J. Carbonell, T. Mitchell (Eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach. </booktitle> <address> Palo Alto: </address> <publisher> Tioga. </publisher>
Reference-contexts: The result is a modified DNF that allows internal disjunction. (See <ref> (Michalski, 1983) </ref> for a discussion of internal disjunction.) With these restrictions we can now construct a fixed-length internal representation for classification rules. Each fixed-length rule will have N feature tests, one for each feature. <p> We selected four systems to represent all four combinations of batch and incremental modes, and two popular hypothesis forms (DNF and decision trees). The chosen systems are AQ14 (Mozetic, 1985), which is based on the AQ algorithm described in <ref> (Michalski, 1983) </ref>, C4.5 (Quinlan, unpublished), ID5R (Utgoff, 1988), and Iba's Algorithm Concept Learner (IACL) (Gordon, 1990), which is based on Iba's algorithm described in (Iba, 1979). AQ14 and IACL form modified DNF hypotheses. <p> This analysis led to the addition of two new GABIL operators which add biases for simpler and more general descriptions. 4.1. The adding alternative operator One of the mechanisms AQ uses to increase the generality of its inductive hypotheses is the "adding alternative" generalization operator of <ref> (Michalski, 1983) </ref>. This operator generalizes by adding a disjunct (i.e., alternative) to the current classification rule. The most useful form of this operator, according to (Michalski, 1983), is the addition of an internal disjunct. <p> The adding alternative operator One of the mechanisms AQ uses to increase the generality of its inductive hypotheses is the "adding alternative" generalization operator of <ref> (Michalski, 1983) </ref>. This operator generalizes by adding a disjunct (i.e., alternative) to the current classification rule. The most useful form of this operator, according to (Michalski, 1983), is the addition of an internal disjunct. For example, if the disjunct is (F1 = small) and (F2 = sphere) then the adding alternative operator might create the new disjunct (F1 = small) and (F2 = sphere or cube). <p> The dropping condition operator A second, and complementary, generalization mechanism leading to simpler hypotheses involves removing what appear to be nearly irrelevant conditions from a disjunct. This operator, which we call DC (the dropping condition operator), is based on the generalization operator of the same name described in <ref> (Michalski, 1983) </ref>. For example, if the disjunct is (F1 = small or medium) and (F2 = sphere) then the DC operator might create the new disjunct (F2 = sphere). The DC operator is easily added to GABIL in the following manner.
Reference: <author> Michalski, R. </author> <year> (1986). </year> <title> Learning flexible concepts: Fundamental ideas and a method based on two-tiered representation. </title> <editor> In Y. Kodratoff, R. Michalski (Eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach. </booktitle> <address> San Mateo: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This procedure was repeated 10 times (trials) for each concept and learning algorithm pair. For Domain 2, we selected a natural domain to further test our conjectures about system biases. Domain 2 is a well-known natural database for diagnosing breast cancer <ref> (Michalski et. al., 1986) </ref>. This database has descriptions of cases for 286 patients, and each case (instance) is described in terms of 9 features. There is a small amount of noise of unknown origin in the database manifested as cases with identical features but different classifications.
Reference: <author> Michalski, R., Mozetic, I., Hong, J., & Lavrac, N. </author> <year> (1986). </year> <title> The AQ15 inductive learning system: An overview and experiments. </title> <institution> University of Illinois Technical Report Number UIUCDCS-R-86-1260, Urbana-Champaign, ILL. </institution>
Reference-contexts: This procedure was repeated 10 times (trials) for each concept and learning algorithm pair. For Domain 2, we selected a natural domain to further test our conjectures about system biases. Domain 2 is a well-known natural database for diagnosing breast cancer <ref> (Michalski et. al., 1986) </ref>. This database has descriptions of cases for 286 patients, and each case (instance) is described in terms of 9 features. There is a small amount of noise of unknown origin in the database manifested as cases with identical features but different classifications.
Reference: <author> Mozetic, I. </author> <year> (1985). </year> <title> NEWGEM: Program for learning from examples, program documentation and user's guide. </title> <institution> University of Illinois Report Number UIUCDCS-F-85-949, Urbana-Champaign, ILL. </institution>
Reference-contexts: We selected four systems to represent all four combinations of batch and incremental modes, and two popular hypothesis forms (DNF and decision trees). The chosen systems are AQ14 <ref> (Mozetic, 1985) </ref>, which is based on the AQ algorithm described in (Michalski, 1983), C4.5 (Quinlan, unpublished), ID5R (Utgoff, 1988), and Iba's Algorithm Concept Learner (IACL) (Gordon, 1990), which is based on Iba's algorithm described in (Iba, 1979). AQ14 and IACL form modified DNF hypotheses.
Reference: <author> Provost, F. </author> <year> (1991). </year> <title> Navigation of an extended bias space for inductive learning, </title> <type> Ph.D. thesis proposal, </type> <institution> University of Pittsburgh, </institution> <address> Pittsburgh, PA. </address>
Reference-contexts: The vast majority of concept learning systems that adjust their bias focus on changing their representational bias. The few notable exceptions that adjust the algorithmic bias include the Competitive Relation Learner and Induce and Select Optimizer combination (CRL/ISO) (Tcheng et. al., 1989), Climbing in the Bias Space (ClimBS) <ref> (Provost, 1991) </ref>, PEAK (Holder, 1990), the Variable Bias Management System (VBMS) (Rendell et. al., 1987), and the Genetic-based - 23 - Inductive Learner (GIL) (Janikow, 1991). We can classify these systems according to the type of bias that they select. Adaptive GABIL shifts its bias by dynamically selecting generalization operators.
Reference: <author> Quinlan, J. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1(1), </volume> <pages> 81-106. </pages>
Reference-contexts: AQ14 and IACL form modified DNF hypotheses. The C4.5 and ID5R systems are based on the ID algorithm described in <ref> (Quinlan, 1986) </ref>, and form decision tree hypotheses. AQ14 and C4.5 are run in batch-incremental mode since they are batch systems. ID5R and IACL are incremental. AQ14, like AQ, generates classification rules from instances using a beam search.
Reference: <author> Quinlan, J. </author> <year> (1989). </year> <title> Documentation and user's guide for C4.5. </title> <note> (unpublished). </note>
Reference: <author> Rendell, L. </author> <year> (1985). </year> <title> Genetic plans and the probabilistic learning system: Synthesis and results. </title> <booktitle> Proceedings of the First International Conference on Genetic Algorithms (pp. 60 - 73). </booktitle> <address> Pittsburgh, PA: </address> <publisher> Lawrence Erlbaum. </publisher>
Reference: <author> Rendell, L., Seshu, R., & Tcheng, D. </author> <year> (1987). </year> <title> More robust concept learning using dynamically-variable bias. </title> <booktitle> Proceedings of the Fourth International Workshop on Machine Learning (pp. 66 - 78). </booktitle> <address> Irvine, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The few notable exceptions that adjust the algorithmic bias include the Competitive Relation Learner and Induce and Select Optimizer combination (CRL/ISO) (Tcheng et. al., 1989), Climbing in the Bias Space (ClimBS) (Provost, 1991), PEAK (Holder, 1990), the Variable Bias Management System (VBMS) <ref> (Rendell et. al., 1987) </ref>, and the Genetic-based - 23 - Inductive Learner (GIL) (Janikow, 1991). We can classify these systems according to the type of bias that they select. Adaptive GABIL shifts its bias by dynamically selecting generalization operators.
Reference: <author> Schaffer, J. David & Morishima, A. </author> <year> (1987). </year> <title> An adaptive crossover distribution mechanism for genetic algorithms. </title> <booktitle> Proceedings of the Second International Conference on Genetic Algorithms (pp. 36 - 40). </booktitle> <address> Cambridge, MA: </address> <publisher> Lawrence Erlbaum. </publisher>
Reference-contexts: This is accomplished by using the underlying population to store - 20 - information relevant to the adaptive mechanism as well as the standard information regarding the original problem space being searched. This approach is elegant and straightforward, since no new adaptive mechanism is required (see <ref> (Schaffer, 1987) </ref> for examples of this approach). Unfortunately, this coupling also means that the additional search can be hindered by the same issues that hinder the search of the problem space. For example, one possible concern is that this mechanism may only work well with large population sizes.
Reference: <author> Smith, S. </author> <year> (1983). </year> <title> Flexible learning of problem solving heuristics through adaptive search. </title> <booktitle> Proceedings of the Eighth International Joint Conference on Artificial Intelligence (pp. </booktitle> <volume> 422 - 425). </volume> <publisher> Karlsruche, Germany: William Kaufmann. </publisher>
Reference-contexts: There are currently two basic strategies: the Michigan approach exemplified by Holland's classifier system (Hol-land, 1986), and the Pittsburgh approach exemplified by Smith's LS-1 system <ref> (Smith, 1983) </ref>. Systems using the Michigan approach maintain a population of individual rules that compete with each other for space and priority in the population.
Reference: <author> Tcheng, D., Lambert, B., Lu, S., & Rendell, R. </author> <year> (1989). </year> <title> Building robust learning systems by combining induction and optimization. </title> <booktitle> Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (pp. 806 - 812). </booktitle> <address> Detroit, MI: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The vast majority of concept learning systems that adjust their bias focus on changing their representational bias. The few notable exceptions that adjust the algorithmic bias include the Competitive Relation Learner and Induce and Select Optimizer combination (CRL/ISO) <ref> (Tcheng et. al., 1989) </ref>, Climbing in the Bias Space (ClimBS) (Provost, 1991), PEAK (Holder, 1990), the Variable Bias Management System (VBMS) (Rendell et. al., 1987), and the Genetic-based - 23 - Inductive Learner (GIL) (Janikow, 1991). We can classify these systems according to the type of bias that they select.
Reference: <author> Wilson, S. </author> <year> (1987). </year> <title> Quasi-Darwinian learning in a classifier system. </title> <booktitle> Proceedings of the - 27 - Fourth International Workshop on Machine Learning (pp. 59 - 65). </booktitle> <address> Irvine, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In this paper we report on results obtained from using the Pittsburgh approach. 2 That is, each individual in the population is a variable-length string representing an unordered set of fixed-length _______________ 2 (Greene & Smith, 1987) and (Janikow, 1991) have also used the Pittsburgh approach. See <ref> (Wilson, 1987) </ref> and (Booker, 1989) for examples of the Michigan approach. - 6 - rules. The number of rules in a particular individual can be unrestricted or limited by a user-defined upper bound.
Reference: <author> Utgoff, P. </author> <year> (1988). </year> <title> ID5R: An incremental ID3. </title> <booktitle> Proceedings of the Fifth International Conference on Machine Learning (pp. 107 - 120). </booktitle> <address> Ann Arbor, Michigan: </address> <publisher> Morgan Kaufmann. </publisher> - <pages> 28 </pages> - 
Reference-contexts: We selected four systems to represent all four combinations of batch and incremental modes, and two popular hypothesis forms (DNF and decision trees). The chosen systems are AQ14 (Mozetic, 1985), which is based on the AQ algorithm described in (Michalski, 1983), C4.5 (Quinlan, unpublished), ID5R <ref> (Utgoff, 1988) </ref>, and Iba's Algorithm Concept Learner (IACL) (Gordon, 1990), which is based on Iba's algorithm described in (Iba, 1979). AQ14 and IACL form modified DNF hypotheses. The C4.5 and ID5R systems are based on the ID algorithm described in (Quinlan, 1986), and form decision tree hypotheses.
References-found: 32

