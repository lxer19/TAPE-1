URL: http://www.ruhr-uni-bochum.de/lmi/mschmitt/spikingneurons.ps.gz
Refering-URL: http://www.ruhr-uni-bochum.de/lmi/mschmitt/
Root-URL: http://www.ruhr-uni-bochum.de/lmi/mschmitt/
Email: maass@igi.tu-graz.ac.at  mschmitt@lmi.ruhr-uni-bochum.de  
Title: On the Complexity of Learning for Spiking Neurons with Temporal Coding  
Author: Wolfgang Maass Michael Schmitt 
Note: Research for this article was partially supported by the ESPRIT Working Group NeuroCOLT, No. 8556, and the Fonds zur Forderung der wissenschaftlichen Forschung (FWF), Austria, project P12153. This research was carried out while M. Schmitt was with the Institute for Theoretical  
Address: Klosterwiesgasse 32/2 A-8010 Graz Austria  Ruhr-Universitat Bochum D-44780 Bochum Germany  
Affiliation: Institute for Theoretical Computer Science Technische Universitat Graz  Lehrstuhl Mathematik und Informatik Fakultat fur Mathematik  Computer Science at the Technische Universitat Graz.  
Abstract: Spiking neurons are models for the computational units in biological neural systems where information is considered to be encoded mainly in the temporal patterns of their activity. In a network of spiking neurons a new set of parameters becomes relevant which has no counterpart in traditional neural network models: the time that a pulse needs to travel through a connection between two neurons (also known as delay of a connection). It is known that these delays are tuned in biological neural systems through a variety of mechanisms. In this article we consider the arguably most simple model for a spiking neuron, which can also easily be implemented in pulsed VLSI. We investigate the VC dimension of networks of spiking neurons where the delays are viewed as programmable parameters and we prove tight bounds for this VC dimension. Thus we get quantitative estimates for the diversity of functions that a network with fixed architecture can compute with different settings of its delays. In particular, it turns out that a network of spiking neurons with k adjustable delays is able to compute a much richer class of functions than a threshold circuit with k adjustable weights. The results also yield bounds for the number of training examples that an algorithm needs for tuning the delays of a network of spiking neurons. Results about the computational complexity of such algorithms are also given. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Agmon-Snir, H., and Segev, I. </author> <year> (1993), </year> <title> Signal delay and input synchronization in passive dendritic structures. </title> <journal> Journal of Neurophysiology 70, </journal> <pages> 2066-2085. </pages>
Reference: <author> Anthony, M., and Biggs, N. </author> <year> (1992), </year> <title> "Computational Learning Theory", </title> <institution> Cambridge Tracts Theoret. Comput. Sci., </institution> <address> Cambridge Univ. </address> <publisher> Press, </publisher> <address> Cambridge. </address>
Reference-contexts: In a similar way, NP-completeness can be shown for the case that the delays are allowed to take on values from a bounded set f0; : : : ; k 1g where k 3. The reduction is from GRAPH-k-COLORABILITY and is basically a modification of the reduction used in <ref> (Anthony and Biggs, 1992) </ref> for the AND of k threshold gates. Again, the weights and the threshold can also be kept fixed. Combining this with Theorem 4.1 we get the following result.
Reference: <author> Blum, A. L., and Rivest, R. L. </author> <year> (1992), </year> <title> Training a 3-node neural network is NP-complete, </title> <booktitle> Neural Networks 5, </booktitle> <pages> 117-127. </pages>
Reference-contexts: Theorem 4.1 The consistency problem for a spiking neuron with delays from f0; 1g is NP-complete. The proof is by a reduction from 3SET-SPLITTING (Garey and Johnson, 1979), a problem which was also used in <ref> (Blum and Rivest, 1992) </ref> for intractability results concerning certain two-layer networks of threshold gates. In fact, the problem considered here seems to be closely related to the consistency problem for the AND of two threshold gates analyzed in (Blum and Rivest, 1992). <p> 3SET-SPLITTING (Garey and Johnson, 1979), a problem which was also used in <ref> (Blum and Rivest, 1992) </ref> for intractability results concerning certain two-layer networks of threshold gates. In fact, the problem considered here seems to be closely related to the consistency problem for the AND of two threshold gates analyzed in (Blum and Rivest, 1992).
Reference: <author> Blumer, A., Ehrenfeucht, A., Haussler, D., and Warmuth, M. K. </author> <year> (1989), </year> <title> Learn-ability and the Vapnik-Chervonenkis dimension, </title> <editor> J. </editor> <booktitle> ACM 36, </booktitle> <pages> 929-965. </pages>
Reference-contexts: Corollary 2.5 The classes S bb n and S ab n have VC dimension fi (n log n). In the proof of Theorem 2.4 we will use the following result which is a consequence of Theorem 2 in (Cover, 1965) 2 and Proposition A2.1 of <ref> (Blumer et al., 1989) </ref>. Lemma 2.6 Let m hyperplanes in IR n passing through the origin be given, where m n. They partition IR n into at most 2 (em=(n 1)) (n1) different regions. Proof. <p> They partition IR n into at most 2 (em=(n 1)) (n1) different regions. Proof. By Theorem 2 of (Cover, 1965), m hyperplanes through the origin partition IR n into at most 2 P n1 m1 different regions. By Proposition A2.1 (iii) of <ref> (Blumer et al., 1989) </ref>, 2 k=0 k 2 (e (m 1)=(n 1)) (n1) for m n. Proof of Theorem 2.4. The proof is structured as follows: We first estimate the number of dichotomies induced by a spiking neuron on an arbitrary finite set S IR n of cardinality m.
Reference: <author> Cover, T. M. </author> <year> (1965), </year> <title> Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition, </title> <journal> IEEE Trans. Electron. Comput. </journal> <volume> 14, </volume> <pages> 326-334. </pages>
Reference-contexts: Corollary 2.5 The classes S bb n and S ab n have VC dimension fi (n log n). In the proof of Theorem 2.4 we will use the following result which is a consequence of Theorem 2 in <ref> (Cover, 1965) </ref> 2 and Proposition A2.1 of (Blumer et al., 1989). Lemma 2.6 Let m hyperplanes in IR n passing through the origin be given, where m n. They partition IR n into at most 2 (em=(n 1)) (n1) different regions. Proof. By Theorem 2 of (Cover, 1965), m hyperplanes through <p> of Theorem 2 in <ref> (Cover, 1965) </ref> 2 and Proposition A2.1 of (Blumer et al., 1989). Lemma 2.6 Let m hyperplanes in IR n passing through the origin be given, where m n. They partition IR n into at most 2 (em=(n 1)) (n1) different regions. Proof. By Theorem 2 of (Cover, 1965), m hyperplanes through the origin partition IR n into at most 2 P n1 m1 different regions. By Proposition A2.1 (iii) of (Blumer et al., 1989), 2 k=0 k 2 (e (m 1)=(n 1)) (n1) for m n. Proof of Theorem 2.4.
Reference: <author> Garey, M. R., and Johnson, D. S. </author> <year> (1979), </year> <title> "Computers and Intractability: A Guide to the Theory of NP-Completeness," </title> <editor> W. H. </editor> <publisher> Freeman, </publisher> <address> New York. </address>
Reference-contexts: Moreover, the proof shows that the result also holds when the weights and the threshold are kept fixed. Theorem 4.1 The consistency problem for a spiking neuron with delays from f0; 1g is NP-complete. The proof is by a reduction from 3SET-SPLITTING <ref> (Garey and Johnson, 1979) </ref>, a problem which was also used in (Blum and Rivest, 1992) for intractability results concerning certain two-layer networks of threshold gates. <p> Then there exists c 2 C; c = fu i ; u j ; u k g and b 2 f0; 1g such that fi (u i ) = fi (u j ) = fi (u k ) = b: 4 Strictly speaking, the restriction of SET-SPLITTING as defined in <ref> (Garey and Johnson, 1979) </ref> allows that jcj 3. However, it is straightforward to define a reduction that avoids subsets of cardinality 2. 20 (i) If b = 1 then g (1 f2l1;2lg ) = 1 for each l 2 fi; j; kg.
Reference: <author> Gerstner, W. </author> <year> (1995), </year> <title> Time structure of the activity in neural network models, </title> <journal> Phys. Rev. </journal> <volume> E 51, </volume> <pages> 738-758. </pages> <note> 22 Gerstner, </note> <author> W., Kempter, R., van Hemmen, J. L., and Wagner, H. </author> <year> (1996), </year> <title> A neuronal learning rule for sub-millisecond temporal coding, </title> <booktitle> Nature 383, </booktitle> <pages> 76-78. </pages>
Reference: <author> Goldberg, P. W., and Jerrum, M. R. </author> <year> (1995), </year> <title> Bounding the Vapnik-Chervonenkis dimension of concept classes parameterized by real numbers, </title> <booktitle> Machine Learning 18, </booktitle> <pages> 131-148. </pages>
Reference-contexts: Moreover, this upper bound holds even if all delays, weights, and thresholds are programmable and even for analog coding of the inputs. The proof of this bound relies on a well-known and far-reaching result by <ref> (Goldberg and Jerrum, 1995) </ref>. In Section 4 we investigate the computational complexity of PAC-learning using a particular spiking neuron as hypothesis class. We show that for any bounded set of at least two delay values the consistency problem for the corresponding hypothesis class is NP-complete. <p> Note that the delays in M 1 ; : : : ; M m are not required to be adjustable. The following result, which employs a bound from <ref> (Goldberg and Jerrum, 1995) </ref>, shows that the lower bound of Theorem 3.1 is optimal. Theorem 3.3 Consider an SNN N with rectangular pulses where all delays, weights, and thresholds are programmable parameters, and let l be the number of edges. <p> There are at most 2 O (l) such paths. This leads to a Boolean formula k;n containing s = 2 O (l) distinct atomic predicates, where each predicate is a polynomial inequality of degree d = 1 over k + n variables. According to Theorem 2.2 of <ref> (Goldberg and Jerrum, 1995) </ref>, the VC dimension of the class of functions described by this formula is at most 2k log (8eds) = O (l 2 ). 4 Computational Complexity of Delay Learning In order to investigate the computational complexity of learning within the PAC framework one has to specify which
Reference: <author> Haussler, D. </author> <year> (1992), </year> <title> Decision theoretic generalizations of the PAC model for neural net and other learning applications, </title> <journal> Inform. and Comput. </journal> <volume> 100, </volume> <pages> 78-150. </pages>
Reference-contexts: It is well known that the VC dimension of a function class gives fairly tight bounds on the sample complexity, i.e. the number of training examples needed, for PAC-learning this class. According to <ref> (Haussler, 1992) </ref>, these estimates of the sample complexity in terms of the VC dimension hold even for agnostic PAC-learning, i.e. in the case when the training examples are generated by some arbitrary probability distribution. <p> in the light of the fact that there are at most 2 n 2 many different functions in T bb n (Muroga, 1971). 2.4 Pseudo Dimension When analyzing the PAC-learnability of real-valued function classes the pseudo dimension plays a similar role as the VC dimension does for binary-valued function classes <ref> (Haussler, 1992) </ref>. <p> Obviously, if F contains only binary-valued functions then its pseudo dimension is equal to its VC dimension. Using known results about the pseudo dimension (see, e.g., <ref> (Haussler, 1992) </ref>) it is easy to derive that the class T aa n has pseudo dimension n + 1, which is equal to the number of programmable parameters.
Reference: <author> Hoffgen, K.-U., Simon, H.-U., and Van Horn, K. S. </author> <year> (1995), </year> <title> Robust trainability of single neurons, </title> <journal> J. Comput. System Sci. </journal> <volume> 50, </volume> <pages> 114-125. </pages>
Reference: <author> Kearns, M., Li, M., and Valiant, L. </author> <year> (1994a), </year> <title> Learning Boolean formulas, </title> <editor> J. </editor> <booktitle> ACM 41, </booktitle> <pages> 1298-1328. </pages>
Reference-contexts: In one setting studied in the literature the learner is allowed to output hypotheses 18 from some polynomial-time evaluatable hypothesis class (see, e.g., <ref> (Kearns et al., 1994a) </ref> for a definition). If the class S n were PAC-learnable by such a hypothesis class then according to Theorem 2.1 (d) the same result would hold for the class DNF. <p> If the class S n were PAC-learnable by such a hypothesis class then according to Theorem 2.1 (d) the same result would hold for the class DNF. Polynomial learnability of DNF in this setting, however, implies polynomial learnability of the more general class DNF as <ref> (Kearns et al., 1994a) </ref> have shown. It is one of the major open problems in computational learning theory whether DNF can be learned in polynomial time by some polynomial-time evaluat-able hypothesis class.
Reference: <author> Kearns, M. J., Schapire, R. E., and Sellie, L. M. </author> <year> (1994b), </year> <title> Toward efficient agnostic learning, </title> <booktitle> Machine Learning 17, </booktitle> <pages> 115-141. </pages>
Reference: <author> Koiran, P., and Sontag, E. D. </author> <year> (1997), </year> <title> Neural networks with quadratic VC dimension, </title> <journal> J. Comput. System Sci. </journal> <volume> 54, </volume> <pages> 190-198. </pages>
Reference-contexts: We show that such networks can have a VC dimension that is quadratic in the number of delays that are programmable. Interestingly, this bound equals the quadratic lower bound for sigmoidal networks in terms of the number of weights due to <ref> (Koiran and Sontag, 1997) </ref>. We further show that this bound is asymptotically tight by proving that any feedforward network of spiking neurons has a VC dimension that is at most quadratic in the number of its edges. <p> The following lower bound for the VC dimension of an SNN in terms of the number of delays equals the quadratic lower bound in terms of the number of weights due to <ref> (Koiran and Sontag, 1997) </ref>, which holds for sigmoidal neural networks. The agreement between these two bounds is somewhat surprising, since the settings and the constructions are quite different.
Reference: <author> Maass, W. </author> <year> (1997a), </year> <title> Fast sigmoidal networks via spiking neurons, </title> <booktitle> Neural Computation 9, </booktitle> <pages> 279-304. </pages>
Reference: <author> Maass, W. </author> <year> (1997b), </year> <title> Networks of spiking neurons: </title> <booktitle> The third generation of neural network models, Neural Networks 10, </booktitle> <pages> 1659-1671. </pages>
Reference-contexts: We will focus in this article on a simple version of the spiking neuron model ("spiking neurons of type A" in the terminology of <ref> (Maass, 1997b) </ref>). This model allows us to study some fundamental new learning problems that arise in the context of computation with temporal coding. <p> Pulses of this shape are actually very common in silicon implementations of networks of spiking neurons (Murray and Tarassenko, 1994; Maass and Bishop, 1999). A spiking neuron of this type was called a "spiking neuron of type A" in <ref> (Maass, 1997b) </ref>. In this article we will refer to it simply as a spiking neuron. 1.2 Temporal Coding A spiking neuron may be viewed as a digital or analog computational element, depending on the type of temporal coding that is used.
Reference: <author> Maass, W., and Bishop, C. M., Eds. </author> <year> (1999), </year> <title> "Pulsed Neural Networks," </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: In addition, this simple model for a spiking neuron has the advantage that it provides a link to silicon implementations of spiking neurons in analog VLSI (see, e.g., <ref> (Maass and Bishop, 1999) </ref>). 1.1 The Model for a Spiking Neuron We consider a spiking neuron v that receives inputs in the form of short pulses, also known as spikes, from n input neurons a 1 ; : : : ; a n .
Reference: <author> Macintyre, A., and Sontag, E. D. </author> <year> (1993), </year> <title> Finiteness results for sigmoidal "neural" networks, </title> <booktitle> in "Proceedings of the Twenty-Fifth Annual ACM Symposium on the Theory of Computing," </booktitle> <pages> pp. 325-334, </pages> <publisher> ACM Press, </publisher> <address> New York. </address>
Reference-contexts: Following the terminology of <ref> (Macintyre and Son-tag, 1993) </ref> we say a set fs 1 ; : : : ; s m g IR n is H-shattered by a class F of real-valued functions if there exist real numbers x 1 ; : : : ; x m such that every dichotomy of f (s 1
Reference: <author> Muroga, S. </author> <year> (1971), </year> <title> "Threshold Logic and Its Applications," </title> <publisher> John Wiley & Sons, </publisher> <address> New York. </address>
Reference-contexts: Thus one derives the upper bound 2 n 2 +O (n log n) for the number of Boolean functions. This result is particularly interesting in the light of the fact that there are at most 2 n 2 many different functions in T bb n <ref> (Muroga, 1971) </ref>. 2.4 Pseudo Dimension When analyzing the PAC-learnability of real-valued function classes the pseudo dimension plays a similar role as the VC dimension does for binary-valued function classes (Haussler, 1992).
Reference: <author> Muroga, S., and Toda, I. </author> <year> (1966), </year> <title> Lower bound of the number of threshold functions. </title> <journal> IEEE Trans. Electron. Comput. </journal> <volume> 15, </volume> <pages> 805-806. </pages>
Reference-contexts: The bound (3) can also be used to estimate the number of Boolean functions that can be computed by a spiking neuron. Substituting m = 2 n yields the bound 2 O (n 2 ) . Combining this with the lower bound 2 (n 2 ) of <ref> (Muroga and Toda, 1966) </ref> for T bb n and our Theorem 2.1 (c), we get the upper and the lower bound almost matching.
Reference: <author> Murray, A., and Tarassenko, L. </author> <year> (1994), </year> <title> "Analogue Neural VLSI: A Pulse Stream Approach," </title> <publisher> Chapman & Hall, London. </publisher>
Reference: <author> Schlafli, L. </author> <year> (1901), </year> <institution> "Theorie der vielfachen Kontinuitat," Zurcher & Furrer, Zurich. </institution> <note> (Reprinted in Schlafli, </note> <author> L. </author> <year> (1950), </year> <title> "Gesammelte Mathematische Abhandlungen," Band I, </title> <publisher> Birkhauser, </publisher> <address> Basel.) 23 Tuckwell, </address> <publisher> H. </publisher> <address> C. </address> <year> (1988), </year> <title> "Introduction to Theoretical Neurobiology," Vols. 1 and 2, </title> <publisher> Cambridge Univ. Press, </publisher> <address> Cambridge. </address>
Reference-contexts: The neuron fires if within some interval the sum of the weights in the associated subset reaches the threshold. 2 This reference is frequently cited when using this result. Cover himself, however, attributes the first proof of this theorem to <ref> (Schlafli, 1901) </ref>. 9 In order to prove (3), we first estimate the number of different delay vectors that have to be considered.
Reference: <author> Valiant, L. G. </author> <year> (1984), </year> <title> A theory of the learnable, </title> <journal> Communications of the ACM 27, </journal> <pages> 1134-1142. </pages>
Reference: <author> Zador, A. M., and Pearlmutter, B. A. </author> <year> (1996), </year> <title> VC dimension of an integrate-and-fire neuron model, </title> <booktitle> Neural Computation 8, </booktitle> <pages> 611-624. 24 </pages>
Reference-contexts: Some other biological mechanisms for changing the effective delay between two neurons are discussed in (Agmon-Snir and Segev, 1993; Gerstner et al., 1996). Our results about the VC dimension of a spiking neuron are complementary to those achieved in <ref> (Zador and Pearlmutter, 1996) </ref>. In that article the integration 1 There is some discussion among neurobiologists whether the sign of a synaptic efficacy w i can change in the course of a learning process.
References-found: 23

