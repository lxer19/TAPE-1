URL: http://csg-www.lcs.mit.edu:8001/mcrctr/tr43/ppg.ps.Z
Refering-URL: http://csg-www.lcs.mit.edu:8001/mcrctr/
Root-URL: 
Email: nodine@mcrc.mot.com  jamey@mcrc.mot.com  cottons@mcrc.mot.com  mikeb@mcrc.mot.com  
Title: Generating Parallelism Profiles from C Programs  
Author: Mark H. Nodine James E. Hicks Cotton Seed Michael J. Beckerle 
Note: Copyright c 1994 Motorola, Inc. All Rights Reserved  
Address: One Kendall Square, Building 200 Cambridge, MA 02139  
Affiliation: Motorola Cambridge Research Center  
Abstract: Motorola Technical Report MCRC-TR-43 Version 003 September 16, 1994 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Zahira Ammarguellat. </author> <title> A control-flow normalizations algorithm and its complexity. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 18(3), </volume> <month> March </month> <year> 1992. </year>
Reference-contexts: The part of the compiler that has to do with interprocedural analysis is described by Harrison [10], while that part concerning the control-flow normalization is by Ammarguellat <ref> [1] </ref>. A significant portion of the compiler is involved in breaking dependencies in the presence of pointers and structures, so that independent parts of the computation can proceed in parallel. There is a substantial amount of work done on analyzing pointers and structures [14, 17, 7, 12, 20, 13]. <p> We are expecting a new version that will accept C with the sync declaration or the C++ language. The Miprac compiler performs three very important functions for us. 1. It does a thorough job of control flow normalization, using the algorithm of Ammarguellat <ref> [1] </ref>. This process eliminates all gotos from the program and converts any control-flow loops (including those with break and continue statements) into single-entry, single-exit while loops. It does this with no code blow-up except in the unlikely event of a program with an irreducible flow.
Reference: [2] <author> Boon S. Ang, Alejandro Caro, Stephen Glim, and Andrew Shaw. </author> <title> An introduction to the Id compiler. </title> <type> Technical Report Computation Structures Group Memo 328, </type> <institution> Massachusetts Institute of Technology Laboratory for Computer Science, </institution> <address> Cambridge MA, </address> <month> May </month> <year> 1991. </year>
Reference-contexts: Implicitly Parallel C is based on an attempt to incorporate the best functional programming ideas into a standard, deterministic, sequential language. The functional programming language that was the basis for our ideas is Id <ref> [2] </ref> and the sequential language is C. The ideas from Id that we are including are extracting implicit parallelism and using single-assignment data structures, which are called I-structures in Id [3].
Reference: [3] <author> Arvind, Rishiyur S. Nikhil, and Keshav K. Pingali. I-structures: </author> <title> Data structures for parallel computing. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 11(4) </volume> <pages> 598-632, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: The functional programming language that was the basis for our ideas is Id [2] and the sequential language is C. The ideas from Id that we are including are extracting implicit parallelism and using single-assignment data structures, which are called I-structures in Id <ref> [3] </ref>. The language is thus all of sequential C, to which we have added a new storage class for variables called sync . A sync declaration indicates that a variable's value will only be written once, after which it will never change.
Reference: [4] <author> Michael J. Beckerle. </author> <title> The parallel slowdown problem for parallel computers. </title> <booktitle> In Proceedings of the Motorola Computer Group Technical Ladder Conference, </booktitle> <address> Tempe, AZ, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: To preserve the sequential semantics of the C program, so that no non-determinacy is introduced in the process of exposing parallelism. * To accept the entire C language, including pointers, pointer arithmetic, function pointers, and gotos. * To determine efficient implementation techniques that avoid the common pitfall of parallel slowdown <ref> [4] </ref>. Implicitly Parallel C is based on an attempt to incorporate the best functional programming ideas into a standard, deterministic, sequential language. The functional programming language that was the basis for our ideas is Id [2] and the sequential language is C.
Reference: [5] <author> Rohit Chandra, Anoop Gupta, and John L. Hennessy. </author> <title> Data locality and load balanc-ing in COOL. </title> <booktitle> In Symposium on Parallel Algorithms and Architectures. Association for Computing Machinery, </booktitle> <year> 1993. </year>
Reference-contexts: There are also many C-like parallel languages in existence, such as C ? (and Parallel C) [16], C ?? [18], C with futures [21], CC++ [6], Split C, Mentat [8], COOL <ref> [5] </ref>, Charm++, Enterprise, ESP, Hypertool, Jade, MeldC, PC, pC++, C++, X3H5, and Cid.
Reference: [6] <author> K. Mani Chandy and Carl Kesselman. </author> <title> CC++: A declarative concurrent object oriented programming notation. </title> <type> Technical report, </type> <institution> California Institute of Technology, </institution> <month> March </month> <year> 1993. </year>
Reference-contexts: There is a substantial amount of work done on analyzing pointers and structures [14, 17, 7, 12, 20, 13]. There are also many C-like parallel languages in existence, such as C ? (and Parallel C) [16], C ?? [18], C with futures [21], CC++ <ref> [6] </ref>, Split C, Mentat [8], COOL [5], Charm++, Enterprise, ESP, Hypertool, Jade, MeldC, PC, pC++, C++, X3H5, and Cid.
Reference: [7] <author> David R. Chase, Mark Wegman, and F. Kenneth Zadeck. </author> <title> Analysis of pointers and struc-tures. </title> <booktitle> In SIGPLAN '90 Conference on Programming Language Design and Implementation. Association for Computing Machinery, </booktitle> <year> 1990. </year>
Reference-contexts: A significant portion of the compiler is involved in breaking dependencies in the presence of pointers and structures, so that independent parts of the computation can proceed in parallel. There is a substantial amount of work done on analyzing pointers and structures <ref> [14, 17, 7, 12, 20, 13] </ref>. There are also many C-like parallel languages in existence, such as C ? (and Parallel C) [16], C ?? [18], C with futures [21], CC++ [6], Split C, Mentat [8], COOL [5], Charm++, Enterprise, ESP, Hypertool, Jade, MeldC, PC, pC++, C++, X3H5, and Cid.
Reference: [8] <author> Andrew S. Grimshaw. </author> <title> Easy-to-use object-oriented parallel processing with Mentat. </title> <booktitle> IEEE Computer, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: There is a substantial amount of work done on analyzing pointers and structures [14, 17, 7, 12, 20, 13]. There are also many C-like parallel languages in existence, such as C ? (and Parallel C) [16], C ?? [18], C with futures [21], CC++ [6], Split C, Mentat <ref> [8] </ref>, COOL [5], Charm++, Enterprise, ESP, Hypertool, Jade, MeldC, PC, pC++, C++, X3H5, and Cid.
Reference: [9] <author> R.H. Halstead, Jr. </author> <title> Multilisp: A language for concurrent symbolic computation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(4) </volume> <pages> 501-538, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: Examples of data-parallel languages are High Performance Fortran and Data Parallel C. 2. Explicit parallelism constructs. The programmer specifies what computation can take place in parallel by using explicit constructs such as DOALL, cobegin, and futures <ref> [9] </ref>. The chief disadvantage to this approach is that determinacy may be lost if the programmer specifies computations that have race conditions. Non-deterministic programs are not, in general, repeatable, so they are very difficult to debug or to prove correct. 3. Message passing.
Reference: [10] <author> Williams Ludwell Harrison III. </author> <title> The interprocedural analysis and automatic parallelization of Scheme programs. </title> <journal> Lisp and Symbolic Computation, </journal> 2(3/4):179-396, October 1989. 
Reference-contexts: The most directly relevant research concerns the Miprac project, since the Miprac compiler is used as a "front end" in our IPC compiler. The part of the compiler that has to do with interprocedural analysis is described by Harrison <ref> [10] </ref>, while that part concerning the control-flow normalization is by Ammarguellat [1]. A significant portion of the compiler is involved in breaking dependencies in the presence of pointers and structures, so that independent parts of the computation can proceed in parallel.
Reference: [11] <author> W.L. Harrison III and Z. Ammarguellat. </author> <title> A program's eye view of Miprac. </title> <editor> In D. Gelernter, N. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing. </booktitle> <publisher> MIT Press, </publisher> <month> August </month> <year> 1992. </year>
Reference-contexts: It does interprocedural dependence analysis using abstract interpretation to "execute" the program at compile time using approximate, or summary, values for variables. It uses a notion of procedure strings, which denote the instruction pointer in terms of procedure invocations and returns, as an abstract measure of time <ref> [11] </ref>. The results of this interprocedural analysis are surprisingly precise because of the procedure strings. 3. It does closed-form replacement for variables that are described by linear recurrences. This closed-form replacement allows us to break the loop dependencies upon the iteration variable in many instances.
Reference: [12] <author> Laurie J. Hendren and Alexandru Nicolau. </author> <title> Parallelizing programs with recursive data structures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 1(1), </volume> <month> January </month> <year> 1990. </year>
Reference-contexts: A significant portion of the compiler is involved in breaking dependencies in the presence of pointers and structures, so that independent parts of the computation can proceed in parallel. There is a substantial amount of work done on analyzing pointers and structures <ref> [14, 17, 7, 12, 20, 13] </ref>. There are also many C-like parallel languages in existence, such as C ? (and Parallel C) [16], C ?? [18], C with futures [21], CC++ [6], Split C, Mentat [8], COOL [5], Charm++, Enterprise, ESP, Hypertool, Jade, MeldC, PC, pC++, C++, X3H5, and Cid.
Reference: [13] <author> Joseph Hummel, Laurie J. Hendren, and Alexandru Nicolau. </author> <title> A general data dependence test for dynamic, pointer-based data structures. </title> <booktitle> In SIGPLAN '94 Conference on Programming Language Design and Implementation, </booktitle> <year> 1994. </year>
Reference-contexts: A significant portion of the compiler is involved in breaking dependencies in the presence of pointers and structures, so that independent parts of the computation can proceed in parallel. There is a substantial amount of work done on analyzing pointers and structures <ref> [14, 17, 7, 12, 20, 13] </ref>. There are also many C-like parallel languages in existence, such as C ? (and Parallel C) [16], C ?? [18], C with futures [21], CC++ [6], Split C, Mentat [8], COOL [5], Charm++, Enterprise, ESP, Hypertool, Jade, MeldC, PC, pC++, C++, X3H5, and Cid.
Reference: [14] <editor> N.D. Jones and S. Muchnick. </editor> <title> A flexible approach to interprocedural data flow analysis and programs with recursive data structures. </title> <booktitle> In Conference Record of the 9th ACM Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 66-74, </pages> <year> 1982. </year>
Reference-contexts: A significant portion of the compiler is involved in breaking dependencies in the presence of pointers and structures, so that independent parts of the computation can proceed in parallel. There is a substantial amount of work done on analyzing pointers and structures <ref> [14, 17, 7, 12, 20, 13] </ref>. There are also many C-like parallel languages in existence, such as C ? (and Parallel C) [16], C ?? [18], C with futures [21], CC++ [6], Split C, Mentat [8], COOL [5], Charm++, Enterprise, ESP, Hypertool, Jade, MeldC, PC, pC++, C++, X3H5, and Cid.
Reference: [15] <author> Eleftherios Koutsofios and Stephen C. </author> <title> North. Drawing graphs with dot. </title> <institution> AT&T Bell Laboratories, </institution> <address> Murray Hill, NJ, </address> <month> June </month> <year> 1993. </year> <month> 8 </month>
Reference-contexts: We have implemented two back-ends to the SDFG representation. One of these is a Digraph Drawing Generator, which puts out code to draw the digraph in a graph language developed at AT&T called "dot" <ref> [15] </ref>. (An example of the output of the Digraph Drawing Generator is presented in Figure 3.) The other back end we have is for producing parallelism profiles.
Reference: [16] <author> Anthony J. Lapadula and Kathleen P. Herold. </author> <title> A retargetable C* compiler and run-time library for mesh-connected MIMD multicomputers. </title> <type> Technical Report TR 92-15, </type> <institution> Dept. of Computer Science, University of New Hampshire, Durham, NH, </institution> <year> 1992. </year>
Reference-contexts: There is a substantial amount of work done on analyzing pointers and structures [14, 17, 7, 12, 20, 13]. There are also many C-like parallel languages in existence, such as C ? (and Parallel C) <ref> [16] </ref>, C ?? [18], C with futures [21], CC++ [6], Split C, Mentat [8], COOL [5], Charm++, Enterprise, ESP, Hypertool, Jade, MeldC, PC, pC++, C++, X3H5, and Cid.
Reference: [17] <author> James R. Larus and Paul N. Hilfinger. </author> <title> Detecting conflicts between structure accesses. </title> <booktitle> In SIGPLAN '88 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 21-34, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: A significant portion of the compiler is involved in breaking dependencies in the presence of pointers and structures, so that independent parts of the computation can proceed in parallel. There is a substantial amount of work done on analyzing pointers and structures <ref> [14, 17, 7, 12, 20, 13] </ref>. There are also many C-like parallel languages in existence, such as C ? (and Parallel C) [16], C ?? [18], C with futures [21], CC++ [6], Split C, Mentat [8], COOL [5], Charm++, Enterprise, ESP, Hypertool, Jade, MeldC, PC, pC++, C++, X3H5, and Cid.
Reference: [18] <author> James R. Larus, Brad Richards, and Guhan Viswanathan. </author> <title> C ?? : A large-grain, objectoriented, data-parallel programming language. </title> <type> Technical Report 1126, </type> <institution> Computer Sciences Dept. University of Wisconsin-Madison, </institution> <address> 1210 West Dayton Street, Madison, WI 53706, USA, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: There is a substantial amount of work done on analyzing pointers and structures [14, 17, 7, 12, 20, 13]. There are also many C-like parallel languages in existence, such as C ? (and Parallel C) [16], C ?? <ref> [18] </ref>, C with futures [21], CC++ [6], Split C, Mentat [8], COOL [5], Charm++, Enterprise, ESP, Hypertool, Jade, MeldC, PC, pC++, C++, X3H5, and Cid.
Reference: [19] <author> K. Pingali, M. Beck, R. Johnson, M. Moudgill, and P. Stodghill. </author> <title> Dependence flow graphs: An algebraic approach to program dependencies. </title> <type> Technical Report TR-90-1152, </type> <institution> Dept. of Computer Science, Cornell University, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: The output of the Miprac compiler is a low-level intermediate language called MIL. From the MIL representation (augmented with the dependence information), we create an internal data structure called a Structured Dependence Flow Graph (SDFG). This data structure is similar to the Dependence Flow Graph of Pingali et al. <ref> [19] </ref>, but it is much simpler since the control flow normalization leaves us with only structured flow of control. An SDFG consists of a number of operators joined together by directed arcs. There are two types of arcs: value arcs and dependence arcs.
Reference: [20] <author> John Plevyak, Vijay Karamcheti, and Andrew A. Chien. </author> <title> Analysis of dynamic structures for efficient parallel execution. </title> <booktitle> In Workshop on Languages and Compilers for Parallel Computing. </booktitle> <year> 1993. </year>
Reference-contexts: A significant portion of the compiler is involved in breaking dependencies in the presence of pointers and structures, so that independent parts of the computation can proceed in parallel. There is a substantial amount of work done on analyzing pointers and structures <ref> [14, 17, 7, 12, 20, 13] </ref>. There are also many C-like parallel languages in existence, such as C ? (and Parallel C) [16], C ?? [18], C with futures [21], CC++ [6], Split C, Mentat [8], COOL [5], Charm++, Enterprise, ESP, Hypertool, Jade, MeldC, PC, pC++, C++, X3H5, and Cid.

References-found: 20

