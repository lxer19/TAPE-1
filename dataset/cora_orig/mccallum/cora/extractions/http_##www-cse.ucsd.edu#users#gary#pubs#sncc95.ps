URL: http://www-cse.ucsd.edu/users/gary/pubs/sncc95.ps
Refering-URL: http://www.cse.ucsd.edu/users/gary/
Root-URL: 
Title: Learning to Retrieve Information  
Author: Brian Bartell Garrison W. Cottrell Rik Belew 
Address: La Jolla, California 92093  La Jolla, California 92093  La Jolla, California 92093  
Affiliation: Encylopdia Britannica and Institute for Neural Computation Computer Science Engineering University of California, San Diego  Institute for Neural Computation Computer Science Engineering University of California, San Diego  Institute for Neural Computation Computer Science Engineering University of California, San Diego  
Abstract: Information retrieval differs significantly from function approximation in that the goal is for the system to achieve the same ranking function of documents relative to queries as the user: the outputs of the system relative to one another must be in the proper order. We hypothesize that a particular rank-order statistic, Guttman's point alienation, is the proper objective function for such a system, and demonstrate its efficacy by using it to find the optimal combination of retrieval experts. In application to a commercial retrieval system, the combination performs 47% better than any single expert.
Abstract-found: 1
Intro-found: 1
Reference: <author> Bartell, </author> <title> B.T. Optimizing Ranking Functions: A Connectionist Approach to Adaptive Information Retrieval. </title> <type> Ph.D. Thesis, UCSD, </type> <year> 1994. </year>
Reference-contexts: Thus, combining systems leads to higher achievable recall, although this does not account for all of the improvement (Saracevic & Kantor 1988). 2 Approach The criterion used in this paper is a variation on Guttman's Point Alienation (Guttman, 1978). a statistical measure of rank correlation. Previous work <ref> (Bartell et al., 1994) </ref> has demonstrated that this criterion can be highly correlated with average precision 1 , a more typical measure of performance in Information Retrieval. Thus, optimization of this criterion is likely to lead to optimized average precision performance. <p> Recall is the fraction of relevant documents that are retrieved. Average precision is precision averaged over a number of recall levels. Using the notation in <ref> (Bartell et al., 1994) </ref>, we let R fi;q (d) be a ranked retrieval function which generates a score indicating the relevance of document d to query q. R is must be differentiable in its parameters fi. <p> This equation provides the blue-print for optimizing many different models: Simply derive the gradient of the model output with respect to the parameters of the model. There are singularities in the first term, these are easily dealt with or can be ignored <ref> (Bartell, 1994) </ref>. <p> In these cases, training time is as short as a few seconds and yields performance results better than the full size experiments. Of course, training time may vary for other text collections and other models. A further discussion of computational complexity is provided in <ref> (Bartell, 1994) </ref>. 3 Optimal Performance in a Commercial System We have applied our method to combining the experts in a commercial retrieval system to validate the usefulness of the technique for real systems. We have been able to achieve very high performance relative to any of the system's individual experts. <p> The optimized system performs on average 47% higher (measured by average precision) than the best individual expert. We have compared our results with an alternative method using supervised learning methods and found they only perform on average 2% better than the best individual expert <ref> (Bartell, 1994) </ref>. The commercial system used is SmarTrieve, developed by Compton's New Media and available for the PC. The SmarTrieve (ST) ranking algorithm consults four experts when determining the relevance of a document to a query.
Reference: <author> Bartell, B.T., Cottrell G.W., and R.K. </author> <title> Belew Learning the optimal parameters in a ranked retrieval system using multi-query relevance feedback. </title> <booktitle> In Proc. Symp. on Document Analysis and Information Retrieval, </booktitle> <address> Las Vegas, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: Thus, combining systems leads to higher achievable recall, although this does not account for all of the improvement (Saracevic & Kantor 1988). 2 Approach The criterion used in this paper is a variation on Guttman's Point Alienation (Guttman, 1978). a statistical measure of rank correlation. Previous work <ref> (Bartell et al., 1994) </ref> has demonstrated that this criterion can be highly correlated with average precision 1 , a more typical measure of performance in Information Retrieval. Thus, optimization of this criterion is likely to lead to optimized average precision performance. <p> Recall is the fraction of relevant documents that are retrieved. Average precision is precision averaged over a number of recall levels. Using the notation in <ref> (Bartell et al., 1994) </ref>, we let R fi;q (d) be a ranked retrieval function which generates a score indicating the relevance of document d to query q. R is must be differentiable in its parameters fi. <p> This equation provides the blue-print for optimizing many different models: Simply derive the gradient of the model output with respect to the parameters of the model. There are singularities in the first term, these are easily dealt with or can be ignored <ref> (Bartell, 1994) </ref>. <p> In these cases, training time is as short as a few seconds and yields performance results better than the full size experiments. Of course, training time may vary for other text collections and other models. A further discussion of computational complexity is provided in <ref> (Bartell, 1994) </ref>. 3 Optimal Performance in a Commercial System We have applied our method to combining the experts in a commercial retrieval system to validate the usefulness of the technique for real systems. We have been able to achieve very high performance relative to any of the system's individual experts. <p> The optimized system performs on average 47% higher (measured by average precision) than the best individual expert. We have compared our results with an alternative method using supervised learning methods and found they only perform on average 2% better than the best individual expert <ref> (Bartell, 1994) </ref>. The commercial system used is SmarTrieve, developed by Compton's New Media and available for the PC. The SmarTrieve (ST) ranking algorithm consults four experts when determining the relevance of a document to a query.
Reference: <author> Belew. R.K. </author> <title> Adaptive information retrieval: Using a connectionist representation to retrieve and learn about documents. </title> <booktitle> In Proc. SIGIR 1989, </booktitle> <pages> pages 11-20, </pages> <address> Cambridge, MA, </address> <month> June </month> <year> 1989. </year>
Reference: <author> Bookstein, A. and D.R. Swanson. </author> <title> Probabilistic models for automatic indexing. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 25 </volume> <pages> 312-318, </pages> <year> 1974. </year>
Reference-contexts: Many current approaches in Information Retrieval are examples of ranked retrieval functions, e.g., the vector space model (Salton & McGill, 1983), the probabilistic model <ref> (Bookstein & Swanson, 1974) </ref>, spreading activation approaches (Mozer, 1984; Belew, 1989) and boolean retrieval. Therefore, this requirement on the functionality of the system does not appear overly restrictive. 1 Precision is the fraction of documents retrieved that are relevant. Recall is the fraction of relevant documents that are retrieved.
Reference: <author> Guttman, L. </author> <title> What is not what in statistics. </title> <journal> The Statistician, </journal> <volume> 26 </volume> <pages> 81-107, </pages> <year> 1978. </year>
Reference-contexts: Thus, combining systems leads to higher achievable recall, although this does not account for all of the improvement (Saracevic & Kantor 1988). 2 Approach The criterion used in this paper is a variation on Guttman's Point Alienation <ref> (Guttman, 1978) </ref>. a statistical measure of rank correlation. Previous work (Bartell et al., 1994) has demonstrated that this criterion can be highly correlated with average precision 1 , a more typical measure of performance in Information Retrieval.
Reference: <author> Harman, D. </author> <title> Overview of the first Text REtrieval Conference. </title> <booktitle> In Proceedings of the ACM SIGIR, </booktitle> <pages> pages 36-48, </pages> <address> Pittsburgh, PA, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: Expert System over Best 1 .2315 .3235 .2162 .3176 -2% 3 .3761 .4610 .3173 .3957 -14% 5 .3600 .4810 .3013 .5089 +6% 7 .3402 .3248 .2018 .2684 -17% Average .2949 .3856 .2801 .4021 +6% Table 1: Results of optimized system. oriented queries, typified by the queries in the TREC collection <ref> (Harman, 1993) </ref>, which do not have well-defined answers. Rather, a potentially large number of documents, each relating to some aspect of the broad query topic, may be relevant to each query. Nine groups of training queries were constructed, with approximately 60 queries per group.
Reference: <author> Lewis, D.D. and W.B. Croft. </author> <title> Term clustering of syntactic phrases. </title> <booktitle> In Proceedings of the ACM SIGIR, </booktitle> <address> Brussels, </address> <month> Sept </month> <year> 1990. </year>
Reference: <author> Mozer. </author> <title> M.C. Inductive information retrieval using parallel distributed computation. </title> <institution> TR8406, Institute for Cognitive Science, </institution> <address> UCSD, La Jolla, CA, </address> <month> May </month> <year> 1984. </year>
Reference: <author> Salton, G. and C. Buckley. </author> <title> Term-weighting approaches in automatic text retrieval. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 24(5) </volume> <pages> 513-523, </pages> <year> 1988. </year>
Reference-contexts: The objective is to find the best performing combination of these three experts. We note that the vector expert does not utilize state-of-the-art weighting schemes <ref> (cf. Salton & Buckley, 1988) </ref>. Therefore, we have found that the performance of the vector expert alone is not as high as could be achieved using the SMART system (Salton & McGill, 1983) and some of its possible weighting mechanisms.
Reference: <author> Salton, G. and M.J. McGill. </author> <title> Introduction to Modern Information Retrieval. </title> <publisher> McGraw-Hill, Inc., </publisher> <year> 1983. </year>
Reference-contexts: By ordering all documents with respect to their scores, the system provides a ranked list of documents estimating the order of relevance of the documents, most relevant before least relevant. Many current approaches in Information Retrieval are examples of ranked retrieval functions, e.g., the vector space model <ref> (Salton & McGill, 1983) </ref>, the probabilistic model (Bookstein & Swanson, 1974), spreading activation approaches (Mozer, 1984; Belew, 1989) and boolean retrieval. Therefore, this requirement on the functionality of the system does not appear overly restrictive. 1 Precision is the fraction of documents retrieved that are relevant. <p> We note that the vector expert does not utilize state-of-the-art weighting schemes (cf. Salton & Buckley, 1988). Therefore, we have found that the performance of the vector expert alone is not as high as could be achieved using the SMART system <ref> (Salton & McGill, 1983) </ref> and some of its possible weighting mechanisms. However, our emphasis in this work is on achieving the highest level of performance possible by combining existing systems, not on optimizing the individual experts.
Reference: <author> Saracevic, T. and P. Kantor. </author> <title> A study of information seeking and retrieving. III. Searchers, searches, overlap. </title> <journal> Journal of the ASIS, </journal> <volume> 39(3) </volume> <pages> 197-216, </pages> <year> 1988. </year>
Reference-contexts: Thus, combining systems leads to higher achievable recall, although this does not account for all of the improvement <ref> (Saracevic & Kantor 1988) </ref>. 2 Approach The criterion used in this paper is a variation on Guttman's Point Alienation (Guttman, 1978). a statistical measure of rank correlation.
Reference: <author> Wong, S.K.M., Cai, Y.J. and Y.Y. Yao. </author> <title> Computation of term associations by a neural network. </title> <booktitle> In Proceedings of SIGIR, </booktitle> <address> Pittsburgh, PA, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: It is assumed that we either know the relevance of all retrieved documents for the training queries, or we restrict the optimization to those documents that are known. This knowledge is represented by the binary relation q <ref> (a notation adopted from Wong et al. 1993) </ref>. q is a preference relation over document pairs. It is interpreted as: d q d 0 () the user prefers d to d 0 (2) This preference relation is much more expressive than common alternatives in Information Retrieval.
References-found: 12

