URL: http://euler.mcs.utulsa.edu/~sandip/aaai97ws-game.ps
Refering-URL: http://euler.mcs.utulsa.edu/~sandip/DAI.html
Root-URL: 
Email: e-mail: sandip@kolkata.mcs.utulsa.edu  
Phone: Phone: 918-631-2985  
Title: Learning to take risks  
Author: Sandip Sen Neeraj Arora 
Keyword: Opponent model, game playing, learning models  
Address: 600 South College Avenue Tulsa, OK 74104-3189  Tulsa  
Affiliation: Department of Mathematical Computer Sciences  University of  
Abstract: Agents that learn about other agents and can exploit this information possess a distinct advantage in competitive situations. Games provide stylized adversarial environments to study agent learning strategies. Researchers have developed game playing programs that learn to play better from experience. We have developed a learning program that does not learn to play better, but learns to identify and exploit the weaknesses of a particular opponent by repeatedly playing it over several games. We propose a scheme for learning opponent action probabilities and a utility maximization framework that exploits this learned opponent model. We show that the proposed expected utility maximization strategy generalizes the traditional maximin strategy, and allows players to benefit by taking calculated risks that are avoided by the max-imin strategy. Experiments in the popular board game of Connect-4 show that a learning player consistently outperforms a non-learning player when pitted against another automated player using a weaker heuristic. Though our proposed mechanism does not improve the skill level of a computer player, it does improve its ability to play more effectively against a weaker opponent. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Eichberger, J. </author> <year> 1993. </year> <title> Game Theory for Economists. </title> <publisher> Academic Press Inc. </publisher>
Reference-contexts: Game playing programs Game playing is considered a representative intelligent activity and studies in game theory have found repeated applications in a variety of social and economic settings <ref> (Eichberger 1993) </ref>. Most AI approaches that study learning in games address the problem of learning to play better (Samuel 1959; Tesauro 1992; Tesauro & Sejnowski 1989).
Reference: <author> Finlay, J., and Dix, A. </author> <year> 1996. </year> <title> An Introduction to Artificial Intelligence. </title> <address> London, UK: </address> <publisher> UCL Press. </publisher>
Reference: <author> Kaelbling, L.; Littman, M. L.; and Moore, A. W. </author> <year> 1996. </year> <title> Reinforcement learning: A survey. </title> <journal> Journal of AI Research 4 </journal> <pages> 237-285. </pages>
Reference-contexts: Our past attempts at using traditional reinforcement learning schemes in adversarial domains gave us important insights into the effectiveness and limitations of this approach (Sekaran & Sen 1994). Our experiments reinforced the notion that Q-learning and other such algorithms are not capable of handling non-stationary environments <ref> (Kaelbling, Littman, & Moore 1996) </ref>. Other researchers exploring the possibility of using Q-learning to explicitly model competitors have also stumbled against the limitations of such approaches when competitors are learning concurrently (Sandholm & R.H.Crites 1995).
Reference: <author> Littman, M. L. </author> <year> 1994. </year> <title> Markov games as a framework for multi-agent reinforcement learning. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> 157-163. </pages>
Reference-contexts: Other researchers exploring the possibility of using Q-learning to explicitly model competitors have also stumbled against the limitations of such approaches when competitors are learning concurrently (Sandholm & R.H.Crites 1995). An alternate approach to using reinforcement learning agents in a game situation utilizes the Markov games formulation <ref> (Littman 1994) </ref>. Both approaches suffer from large training time and data requirements, making them unusable in most realistic problems. The Minimax-Q algorithm developed by in the latter work (Littman 1994) is interesting in that it tries to train a player to be as conservative as a minimax approach suggests. <p> An alternate approach to using reinforcement learning agents in a game situation utilizes the Markov games formulation <ref> (Littman 1994) </ref>. Both approaches suffer from large training time and data requirements, making them unusable in most realistic problems. The Minimax-Q algorithm developed by in the latter work (Littman 1994) is interesting in that it tries to train a player to be as conservative as a minimax approach suggests.
Reference: <author> Luce, R. D., and Raiffa, H. </author> <year> 1957. </year> <title> Games and Decisions. </title> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: Then it attempts to take advantage of those shortcomings by taking calculated risks. In the following we outline our plan to enable a game playing agent to attempt the same. Game theoreticians have espoused the use of the maximin strategy for matrix games to arrive at equilibrium points <ref> (Luce & Raiffa 1957) </ref>. This strategy recommends that a player plays a move that is guaranteed to give it the maximum value if the opponent plays its optimal move. <p> The algorithm used by most AI-based game-playing programs is the minimax algorithm derived directly from the maximin strategy mentioned above. The concept of dominance of strategies have been proposed to choose between two moves that have the same security level <ref> (Luce & Raiffa 1957) </ref>. Move ff k dominates move ff l if 8fi j 2 fi; u (ff k ; fi j ) u (ff l ; fi j ).
Reference: <author> Russell, S., and Norvig, P. </author> <year> 1995. </year> <title> Artificial Intelligence: A Modern Approach. </title> <publisher> Printice Hall. </publisher>
Reference-contexts: An agent is rational if and only if it chooses an action that yields the highest expected utility, averaged over all the possible outcomes. This is called the principle of Maximum Expected Utility (MEU) <ref> (Russell & Norvig 1995) </ref>.
Reference: <author> Samuel, A. L. </author> <year> 1959. </year> <title> Some studies in machine learning using the game of checkers. </title> <journal> IBM Journal 3(3). </journal>
Reference: <author> Sandholm, T., and R.H.Crites. </author> <year> 1995. </year> <title> Multiagent reinforcement learning in the iterated prisoner's dilemma. </title> <type> Biosystems 37 </type> <pages> 147-166. </pages>
Reference-contexts: Our experiments reinforced the notion that Q-learning and other such algorithms are not capable of handling non-stationary environments (Kaelbling, Littman, & Moore 1996). Other researchers exploring the possibility of using Q-learning to explicitly model competitors have also stumbled against the limitations of such approaches when competitors are learning concurrently <ref> (Sandholm & R.H.Crites 1995) </ref>. An alternate approach to using reinforcement learning agents in a game situation utilizes the Markov games formulation (Littman 1994). Both approaches suffer from large training time and data requirements, making them unusable in most realistic problems.
Reference: <author> Sekaran, M., and Sen, S. </author> <year> 1994. </year> <title> Learning with friends and foes. </title> <booktitle> In Sixteenth Annual Conference of the Cognitive Science Society, </booktitle> <pages> 800-805. </pages>
Reference-contexts: Our past attempts at using traditional reinforcement learning schemes in adversarial domains gave us important insights into the effectiveness and limitations of this approach <ref> (Sekaran & Sen 1994) </ref>. Our experiments reinforced the notion that Q-learning and other such algorithms are not capable of handling non-stationary environments (Kaelbling, Littman, & Moore 1996).
Reference: <author> Tesauro, G., and Sejnowski, T. </author> <year> 1989. </year> <title> A parallel network that learns to play backgammon. </title> <booktitle> Artificial Intelligence 39(3) </booktitle> <pages> 357-390. </pages>
Reference: <author> Tesauro, G. </author> <year> 1992. </year> <title> Practical issues in temporal difference learning. </title> <booktitle> Machince Learning 8(3-4):257-277. </booktitle>
References-found: 11

