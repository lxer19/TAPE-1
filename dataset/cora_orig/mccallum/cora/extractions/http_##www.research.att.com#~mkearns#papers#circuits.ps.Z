URL: http://www.research.att.com/~mkearns/papers/circuits.ps.Z
Refering-URL: http://www.research.att.com/~mkearns/
Root-URL: 
Email: sg@cs.wustl.edu  mkearns@research.att.com  schapire@research.att.com  
Title: Exact Identification of Read-once Formulas Using Fixed Points of Amplification Functions  
Author: Sally A. Goldman Michael J. Kearns Robert E. Schapire 
Date: March 9, 1992  
Address: St. Louis, MO 63130  Murray Hill, NJ 07974  Murray Hill, NJ 07974  
Affiliation: Department of Computer Science Washington University  AT&T Bell Laboratories  AT&T Bell Laboratories  
Abstract: In this paper we describe a new technique for exactly identifying certain classes of read-once Boolean formulas. The method is based on sampling the input-output behavior of the target formula on a probability distribution that is determined by the fixed point of the formula's amplification function (defined as the probability that a 1 is output by the formula when each input bit is 1 independently with probability p). By performing various statistical tests on easily sampled variants of the fixed-point distribution, we are able to efficiently infer all structural information about any logarithmic-depth formula (with high probability). We apply our results to prove the existence of short universal identification sequences for large classes of formulas. We also describe extensions of our algorithms to handle high rates of noise, and to learn formulas of unbounded depth in Valiant's model with respect to specific distributions. Most of this research was carried out while all three authors were at MIT Laboratory for Computer Science with support provided by ARO Grant DAAL03-86-K-0171, DARPA Contract N00014-89-J-1988, NSF Grant CCR-88914428, and a grant from the Siemens Corporation. R. Schapire received additional support from AFOSR Grant 89-0506 while at Harvard University. S. Goldman is currently supported in part by a G.E. Foundation Junior Faculty Grant and NSF Grant CCR-9110108. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Dana Angluin, Lisa Hellerstein, and Marek Karpinski. </author> <title> Learning read-once formulas with queries. </title> <type> Technical Report UCB/CSD 89/528, </type> <institution> University of California Berkeley, Computer Science Division, </institution> <month> August </month> <year> 1989. </year> <note> To appear, Journal of the Association for Computing Machinery. </note>
Reference-contexts: For Boolean read-once formulas (a superset of the class of formulas constructed from nand gates) there is an efficient, exact-identification algorithm using membership and equivalence queries due to Angluin, Hellerstein and Karpinski <ref> [1, 11] </ref>. The class of read-once majority formulas can also be exactly identified using membership and equivalence queries, as proved by Hancock and Hellerstein [9] and Bshouty, Hancock, Hellerstein, and Karpin-ski [6].
Reference: [2] <author> Dana Angluin and Philip Laird. </author> <title> Learning from noisy examples. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 343-370, </pages> <year> 1988. </year>
Reference-contexts: We also prove that our algorithms are robust against a large amount of random misclas-sification noise, similar to, but slightly more general than that considered by Sloan [23] and Angluin and Laird <ref> [2] </ref>. <p> Although omitted, a similar (though slightly more involved) algorithm can be derived for nand formulas. Our algorithm is able to handle a kind of random misclassification noise that is similar, but slightly more general than that considered by Angluin and Laird <ref> [2] </ref>, and Sloan [23]. Specifically, the output of the target formula is "flipped" with some fixed probability that may depend on the formula's output.
Reference: [3] <author> Gyora M. Benedek and Alon Itai. </author> <title> Learnability by fixed distributions. </title> <booktitle> In Proceedings of the 1988 Workshop on Computational Learning Theory, </booktitle> <pages> pages 80-90, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: In this paper, we are primarily interested in a variant of Valiant's model in which the target distribution is known a priori to belong to a specific restricted class of distributions. This distribution-specific model has also been studied by Benedek and Itai <ref> [3] </ref>.
Reference: [4] <author> Ravi B. Boppana. </author> <title> Amplification of probabilistic Boolean formulas. </title> <booktitle> In 26th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 20-29, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: Amplification functions were first studied by Valiant [24] and Boppana <ref> [4, 5] </ref> in obtaining bounds on monotone formula size for the majority function. The method used by our algorithms is of central interest. <p> He shows that an arbitrarily good approximation of such formulas can be inferred in polynomial time against any product distribution. 2 Preliminaries Given a Boolean function f : f0; 1g n ! f0; 1g, Boppana <ref> [4, 5] </ref> defines its amplification function A f as follows: A f (p) = Pr [f (X 1 ; . . . ; X n ) = 1], where X 1 ; . . . ; X n are independent Bernoulli variables that are each 1 with probability p.
Reference: [5] <author> Ravi Babu Boppana. </author> <title> Lower Bounds for Monotone Circuits and Formulas. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1986. </year>
Reference-contexts: Amplification functions were first studied by Valiant [24] and Boppana <ref> [4, 5] </ref> in obtaining bounds on monotone formula size for the majority function. The method used by our algorithms is of central interest. <p> He shows that an arbitrarily good approximation of such formulas can be inferred in polynomial time against any product distribution. 2 Preliminaries Given a Boolean function f : f0; 1g n ! f0; 1g, Boppana <ref> [4, 5] </ref> defines its amplification function A f as follows: A f (p) = Pr [f (X 1 ; . . . ; X n ) = 1], where X 1 ; . . . ; X n are independent Bernoulli variables that are each 1 with probability p.
Reference: [6] <author> N. Bshouty, T. Hancock, L. Hellerstein, and M. Karpinski. </author> <title> Read-once formulas, justifying assignments, and generic transformations. </title> <type> Unpublished Manuscript, </type> <year> 1991. </year>
Reference-contexts: The class of read-once majority formulas can also be exactly identified using membership and equivalence queries, as proved by Hancock and Hellerstein [9] and Bshouty, Hancock, Hellerstein, and Karpin-ski <ref> [6] </ref>.
Reference: [7] <author> Merrick Furst, Jeffrey Jackson, and Sean Smith. </author> <title> Learning AC 0 functions sampled under mutually independent distributions. </title> <type> Technical Report CMU-CS-90-183, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <month> October </month> <year> 1990. </year> <month> 29 </month>
Reference-contexts: Also, Linial, Mansour and Nisan [17] used a technique based on Fourier spectra to learn 3 the class of constant-depth circuits (constructed from gates of unbounded fan-in) against the uniform distribution. Furst, Jackson and Smith <ref> [7] </ref> generalized this result to learn this same class against any product distribution (i.e., any distribution in which the setting of each variable is chosen independently of the settings of the other variables). Verbeurgt [25] gives a different algorithm for learning DNF-formulas against the uniform distribution.
Reference: [8] <author> Sally A. Goldman and Michael J. Kearns. </author> <title> On the complexity of teaching. </title> <booktitle> In Proceedings of the Fourth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 303-314, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: A universal identification sequence for a concept class C n is an instance sequence that distinguishes every concept c 2 C n . In the language used in the related work of Goldman and Kearns <ref> [8] </ref>, a sequence that distinguishes c would be called a teaching sequence for c. Thus, in their terminology, a universal identification sequence is one that acts as a teaching sequence for every c 2 C n . See also the related work of Shinohara and Miyano [22].
Reference: [9] <author> Thomas Hancock and Lisa Hellerstein. </author> <title> Learning read-once formulas over fields and extended bases. </title> <booktitle> In Proceedings of the Fourth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 326-336, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: The class of read-once majority formulas can also be exactly identified using membership and equivalence queries, as proved by Hancock and Hellerstein <ref> [9] </ref> and Bshouty, Hancock, Hellerstein, and Karpin-ski [6].
Reference: [10] <author> Thomas Hancock and Yishay Mansour. </author> <title> Learning monotone k DNF formulas on product distributions. </title> <booktitle> In Proceedings of the Fourth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 179-183, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: A similar result, though based on a different method, was obtained by Pagallo and Haussler [18]. These results were extended by Hancock and Mansour <ref> [10] </ref>, and by Schapire [21] as described below. Also, Linial, Mansour and Nisan [17] used a technique based on Fourier spectra to learn 3 the class of constant-depth circuits (constructed from gates of unbounded fan-in) against the uniform distribution.
Reference: [11] <author> Lisa Hellerstein. </author> <title> On Characterizing and Learning Some Classes of Read-Once Formulas. </title> <type> PhD thesis, </type> <institution> University of California at Berkeley, </institution> <year> 1989. </year>
Reference-contexts: For Boolean read-once formulas (a superset of the class of formulas constructed from nand gates) there is an efficient, exact-identification algorithm using membership and equivalence queries due to Angluin, Hellerstein and Karpinski <ref> [1, 11] </ref>. The class of read-once majority formulas can also be exactly identified using membership and equivalence queries, as proved by Hancock and Hellerstein [9] and Bshouty, Hancock, Hellerstein, and Karpin-ski [6].
Reference: [12] <author> Wassily Hoeffding. </author> <title> Probability inequalities for sums of bounded random variables. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 58(301) </volume> <pages> 13-30, </pages> <month> March </month> <year> 1963. </year>
Reference-contexts: Thus, if one estimates the value of the amplification function from a sample whose size is polynomial in 2 h , then with high probability one can determine which variables are relevant, as well as the sign and level of every relevant variable. Specifically, we can apply Chernoff bounds <ref> [12] </ref> to derive a sample size sufficient to ensure that all the above information is properly computed with high probability. <p> Our algorithm is described in Figure 2. Given that the estimates for the formula's amplification have the needed accuracy, the proof of correctness follows from Theorems 3.3 and 3.7. To compute the actual sample size needed to make these estimates, we use Hoeffding's inequality <ref> [12] </ref> (also known as a form of Chernoff bounds) as stated below: Lemma 3.9 (Hoeffding's Inequality) Let X 1 ; . . . ; X m be a sequence of m independent Bernoulli trials, each succeeding with probability p.
Reference: [13] <author> Michael Kearns. </author> <title> The Computational Complexity of Machine Learning. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Thus, for these classes, since the fixed point of the amplification function is the same for all formulas in the class, for each class we obtain a simple product distribution under which the class is learnable. As proved by Kearns and Valiant <ref> [16, 13] </ref>, these same classes of formulas cannot be even weakly approximated in polynomial time when no restriction is placed on the target distribution; thus, our results may be interpreted as demonstrating that while there are some distributions that in a computationally bounded setting reveal essentially no information about the target
Reference: [14] <author> Michael Kearns and Ming Li. </author> <title> Learning in the presence of malicious errors. </title> <booktitle> In Proceedings of the Twentieth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 267-280, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: Again regarding our algorithms as using "random" membership queries, these are the first efficient procedures performing exact identification in some reasonable model of noisy queries. Our algorithms can also tolerate a modest rate of malicious noise, as considered by Kearns and Li <ref> [14] </ref>. Finally, we present an algorithm that learns any (not necessarily logarithmic-depth) read-once majority formula in Valiant's model against the uniform distribution. To obtain this result we first show that the target formula can be well approximated by truncating the formula to have only logarithmic depth. <p> Finally, we comment that our algorithms can be extended to handle a modest amount of malicious noise. In this model, first considered by Kearns and Li <ref> [14] </ref>, an adversary is allowed to corrupt each example in any manner he chooses (both the labels and the variable settings) with probability .
Reference: [15] <author> Michael Kearns, Ming Li, Leonard Pitt, and Leslie Valiant. </author> <title> On the learnability of Boolean formulae. </title> <booktitle> In Proceedings of the Nineteenth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 285-295, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: A similar result also holds for read-once nand formulas of unbounded depth. The problem of learning Boolean formulas against special distributions has been considered by a number of other authors. In particular, our technique closely resembles that used by Kearns et al. <ref> [15] </ref> for learning the class of read-once formulas in disjunctive normal form (DNF) against the uniform distribution. A similar result, though based on a different method, was obtained by Pagallo and Haussler [18]. These results were extended by Hancock and Mansour [10], and by Schapire [21] as described below. <p> The resulting majority formula can further be reduced to one that is read-once using the substitution method of Kearns et al. <ref> [15] </ref>. Finally, combined with Kearns and Valiant's result that Boolean formulas are not learnable (modulo cryptographic assumptions), this shows that majority formulas are also not learnable.
Reference: [16] <author> Michael Kearns and Leslie G. Valiant. </author> <title> Cryptographic limitations on learning Boolean formulae and finite automata. </title> <booktitle> In Proceedings of the Twenty First Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 433-444, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: Thus, for these classes, since the fixed point of the amplification function is the same for all formulas in the class, for each class we obtain a simple product distribution under which the class is learnable. As proved by Kearns and Valiant <ref> [16, 13] </ref>, these same classes of formulas cannot be even weakly approximated in polynomial time when no restriction is placed on the target distribution; thus, our results may be interpreted as demonstrating that while there are some distributions that in a computationally bounded setting reveal essentially no information about the target <p> The main idea of the reduction is to replace 1 Namely, these hardness results depend on the assumed intractability of factoring Blum integers, inverting RSA functions, and recognizing quadratic residues. See Kearns and Valiant <ref> [16] </ref> for details. 6 each or gate (respectively, and gate) occurring in this formula with a maj gate, one of whose inputs is wired to a distinct variable that, under the target distribution, always has the value 1 (respectively, 0).
Reference: [17] <author> Nathan Linial, Yishay Mansour, and Noam Nisan. </author> <title> Constant depth circuits, Fourier transform, and learnability. </title> <booktitle> In 30th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 574-579, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: A similar result, though based on a different method, was obtained by Pagallo and Haussler [18]. These results were extended by Hancock and Mansour [10], and by Schapire [21] as described below. Also, Linial, Mansour and Nisan <ref> [17] </ref> used a technique based on Fourier spectra to learn 3 the class of constant-depth circuits (constructed from gates of unbounded fan-in) against the uniform distribution.
Reference: [18] <author> Giulia Pagallo and David Haussler. </author> <title> A greedy method for learning DNF functions under the uniform distribution. </title> <type> Technical Report UCSC-CRL-89-12, </type> <institution> University of California Santa Cruz, Computer Research Laboratory, </institution> <month> June </month> <year> 1989. </year>
Reference-contexts: In particular, our technique closely resembles that used by Kearns et al. [15] for learning the class of read-once formulas in disjunctive normal form (DNF) against the uniform distribution. A similar result, though based on a different method, was obtained by Pagallo and Haussler <ref> [18] </ref>. These results were extended by Hancock and Mansour [10], and by Schapire [21] as described below. Also, Linial, Mansour and Nisan [17] used a technique based on Fourier spectra to learn 3 the class of constant-depth circuits (constructed from gates of unbounded fan-in) against the uniform distribution.
Reference: [19] <author> Leonard Pitt and Manfred K. Warmuth. </author> <title> Prediction-preserving reducibility. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 41(3) </volume> <pages> 430-467, </pages> <month> December </month> <year> 1990. </year> <month> 30 </month>
Reference-contexts: basis; this can be proved, for instance, by enumerating all three-input read-once Boolean formulas.) It can be shown that the class of logarithmic-depth read-once majority formulas is not learnable in the distribution-free model, modulo assorted cryptographic assumptions. 1 Briefly, this can be proved using a Pitt and Warmuth-style "prediction-preserving reduction" <ref> [19] </ref> to show that learning read-once majority formulas is at least as hard as learning general Boolean formulas. Our reduction starts with a given Boolean formula which we can assume without loss of generality has been converted using standard techniques into an equivalent formula of logarithmic depth.
Reference: [20] <author> Robert E. Schapire. </author> <title> The strength of weak learnability. </title> <journal> Machine Learning, </journal> <volume> 5(2) </volume> <pages> 197-227, </pages> <year> 1990. </year>
Reference-contexts: This type of formula is used by Schapire <ref> [20] </ref> in his proof that a concept class is weakly learnable in polynomial time if and only if it is strongly learnable in polynomial time.
Reference: [21] <author> Robert E. Schapire. </author> <title> Learning probabilistic read-once formulas on product distributions. </title> <booktitle> In Proceedings of the Fourth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 184-198, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: A similar result, though based on a different method, was obtained by Pagallo and Haussler [18]. These results were extended by Hancock and Mansour [10], and by Schapire <ref> [21] </ref> as described below. Also, Linial, Mansour and Nisan [17] used a technique based on Fourier spectra to learn 3 the class of constant-depth circuits (constructed from gates of unbounded fan-in) against the uniform distribution. <p> Verbeurgt [25] gives a different algorithm for learning DNF-formulas against the uniform distribution. However, all three of these algorithms require quasi-polynomial (n polylog (n) ) time, though Verbeurgt's procedure only requires a polynomial-size sample. Finally, Schapire <ref> [21] </ref> has recently extended our technique to handle a probabilistic generalization of the class of all read-once Boolean formulas constructed from the usual basis fand; or; notg. <p> In other words, the behaviors of the two functions are indistinguishable with respect to the visible variables. Thus, we handle all deep variables by regarding them as hidden variables, and the target 2 Alternatively, the algorithm recently described by Schapire <ref> [21] </ref> could be used to PAC-learn unbounded-depth nand formulas. His algorithm is more general but less efficient than the approach described in this section. 23 formula as one that is partially visible.
Reference: [22] <author> Ayumi Shinohara and Satoru Miyano. </author> <title> Teachability in computational learning. </title> <journal> New Generation Computing, </journal> <volume> 8 </volume> <pages> 337-247, </pages> <year> 1991. </year>
Reference-contexts: Thus, in their terminology, a universal identification sequence is one that acts as a teaching sequence for every c 2 C n . See also the related work of Shinohara and Miyano <ref> [22] </ref>. The following theorem gives general conditions for when a deterministic exact identification algorithm implies the existence of a polynomial-length universal identification sequence. We then apply this theorem to our algorithms of the previous sections.
Reference: [23] <author> Robert H. Sloan. </author> <title> Types of noise in data for concept learning. </title> <booktitle> In Proceedings of the 1988 Workshop on Computational Learning Theory, </booktitle> <pages> pages 91-96, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: We also prove that our algorithms are robust against a large amount of random misclas-sification noise, similar to, but slightly more general than that considered by Sloan <ref> [23] </ref> and Angluin and Laird [2]. <p> Although omitted, a similar (though slightly more involved) algorithm can be derived for nand formulas. Our algorithm is able to handle a kind of random misclassification noise that is similar, but slightly more general than that considered by Angluin and Laird [2], and Sloan <ref> [23] </ref>. Specifically, the output of the target formula is "flipped" with some fixed probability that may depend on the formula's output.
Reference: [24] <author> Leslie G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <month> November </month> <year> 1984. </year>
Reference-contexts: Amplification functions were first studied by Valiant <ref> [24] </ref> and Boppana [4, 5] in obtaining bounds on monotone formula size for the majority function. The method used by our algorithms is of central interest. <p> The quantity A f (p) is called the amplification of f at p. Valiant <ref> [24] </ref> uses properties of the amplification function to prove the existence of monotone Boolean formulas of size O (n 5:3 ) for the majority function on n inputs. <p> In this setting polynomial time means polynomial in n and 1=ffi. Our algorithms achieve exact identification with high probability when the example source is a particular, fixed distribution. In the distribution-free or probably approximately correct (PAC) learning model, introduced by Valiant <ref> [24] </ref>, the learner is given access to labeled (positive and negative) examples 4 of the target concept, drawn randomly according to some unknown target distribution D. The learner is also given as input positive real numbers * and ffi.
Reference: [25] <author> Karsten Verbeurgt. </author> <title> Learning DNF under the uniform distribution in quasi-polynomial time. </title> <booktitle> In Proceedings of the Third Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 314-326, </pages> <month> August </month> <year> 1990. </year> <month> 31 </month>
Reference-contexts: Furst, Jackson and Smith [7] generalized this result to learn this same class against any product distribution (i.e., any distribution in which the setting of each variable is chosen independently of the settings of the other variables). Verbeurgt <ref> [25] </ref> gives a different algorithm for learning DNF-formulas against the uniform distribution. However, all three of these algorithms require quasi-polynomial (n polylog (n) ) time, though Verbeurgt's procedure only requires a polynomial-size sample.
References-found: 25

