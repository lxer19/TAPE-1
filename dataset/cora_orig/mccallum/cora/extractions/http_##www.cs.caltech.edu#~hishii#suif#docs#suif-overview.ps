URL: http://www.cs.caltech.edu/~hishii/suif/docs/suif-overview.ps
Refering-URL: http://www.cs.caltech.edu/~hishii/suif/docs/
Root-URL: http://www.cs.caltech.edu
Title: An Overview of the SUIF Compiler System  
Author: Robert Wilson, Robert French, Christopher Wilson, Saman Amarasinghe, Jennifer Anderson, Steve Tjiang, Shih-Wei Liao, Chau-Wen Tseng, Mary Hall, Monica Lam, and John Hennessy 
Address: CA 94305  
Affiliation: Computer Systems Lab Stanford University,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> S. P. Amarasinghe and M. S. Lam. </author> <title> Communication optimization and code generation for distributed memory machines. </title> <booktitle> In Proceedings of the ACM SIGPLAN'93 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: We are also developing algorithms to restructure arrays to enhance the performance of the memory hierarchy in a shared address space machine and algorithms to generate optimized communication code on a distributed address space machine <ref> [1] </ref>. Interprocedural analysis is another current research focus of ours. We have prototype implementations of a scalar data flow analyzer that can find loop induction and loop invariant variables across procedures and also propagate constants and linear relationships between variables across procedural boundaries.
Reference: [2] <author> J. M. Anderson and M. S. Lam. </author> <title> Global optimizations for parallelism and locality on scalable parallel machines. </title> <booktitle> In Proceedings of the ACM SIGPLAN'93 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: We are experimenting with an algorithm that can automatically decompose data and computation across multiple processors to minimize communication while preserving parallelism <ref> [2] </ref>. We are also developing algorithms to restructure arrays to enhance the performance of the memory hierarchy in a shared address space machine and algorithms to generate optimized communication code on a distributed address space machine [1]. Interprocedural analysis is another current research focus of ours.
Reference: [3] <author> C. W. Fraser and David R. Hanson. </author> <title> A retargetable compiler for ANSI c. </title> <journal> SIGPLAN Notices, </journal> <volume> 26(10), </volume> <month> October </month> <year> 1991. </year> <note> 10 http://suif.stanford.edu/suif/docs/parguide toc.html 9 </note>
Reference-contexts: Appendix A illustrates how the elements of the toolkit can be combined in a compiler. 4.1 Front Ends Our C front end is based on Fraser and Hansen's lcc front end <ref> [3] </ref>, which has been modified to generate SUIF files. We do not have a Fortran front end that directly translates Fortran into SUIF. Instead, we use f2c [4] to translate Fortran 77 into C and then use our C front end to convert the C programs into SUIF.
Reference: [4] <author> D. Gay, Stu Feldman, Mark Maimone, and Norm Schryer. </author> <note> f2c. Available via netlib@research.att.com. </note>
Reference-contexts: We do not have a Fortran front end that directly translates Fortran into SUIF. Instead, we use f2c <ref> [4] </ref> to translate Fortran 77 into C and then use our C front end to convert the C programs into SUIF. We have modified f2c to capture some of the Fortran-specific information 4 so that we can pass it down to the SUIF compiler.
Reference: [5] <author> Daniel Lenoski, James Laudon, Truman Joe, David Nakahira, Luis Stevens, Anoop Gupta, and John Hennessy. </author> <title> The DASH prototype: Implementation and performance. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 92-103, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Included 2 http://suif.stanford.edu/suif/docs/dependence toc.html 3 http://suif.stanford.edu/suif/docs/transform toc.html 4 http://suif.stanford.edu/suif/docs/parguide toc.html 5 in this release is a run-time system that supports parallel execution on SGI machines and the Stanford DASH multiprocessor <ref> [5] </ref>. 4.3 An Optimizing MIPS Back End While our focus is not on traditional compiler optimizations for sequential machines, some compiler research projects need access to an optimizing back end.
Reference: [6] <author> D. E. Maydan, J. L. Hennessy, and M. S. Lam. </author> <title> Efficient and exact data dependence analysis. </title> <booktitle> In Proceedings of the ACM SIGPLAN'91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 1-14, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Traditional data dependence analysis determines if there is an overlap between pairs of array accesses whose indices and loop bounds are affine functions of loop indices. Our dependence analyzer 2 is based on the algorithm described in Maydan et al.'s paper <ref> [6] </ref>. It consists of a series of fast exact tests, each applicable to a limited domain. Its last test is a Fourier-Motzkin elimination algorithm that has been extended to solve for integer solutions.
Reference: [7] <author> Todd C. Mowry, Monica S. Lam, and Anoop Gupta. </author> <title> Design and evaluation of a compiler algorithm for prefetching. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 62-73, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Examples of such projects that we have implemented in SUIF include a software prefetcher <ref> [7] </ref> and an instruction scheduler for superscalar machines [8]. (We plan to release the software prefetcher and instruction scheduler in the future.) The initial release includes a set of conventional data flow optimizations and a MIPS code generator.
Reference: [8] <author> Michael D. Smith, Mark A. Horowitz, and Monica S. Lam. </author> <title> Efficient superscalar performance through boosting. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 248-259, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Examples of such projects that we have implemented in SUIF include a software prefetcher [7] and an instruction scheduler for superscalar machines <ref> [8] </ref>. (We plan to release the software prefetcher and instruction scheduler in the future.) The initial release includes a set of conventional data flow optimizations and a MIPS code generator. However, this back end still uses the previous generation of the SUIF program representation.
Reference: [9] <author> S. W. K. Tjiang, M. E. Wolf, M. S. Lam, K. Pieper, and J. L. Hennessy. </author> <title> Integrating Scalar Optimizations and Parallelization, </title> <address> pages 137-151. </address> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1991. </year>
Reference-contexts: SUIF is a research compiler. It is designed to facilitate experimentation and development of new compiler algorithms, ranging from high-level transformations to conventional data flow optimizations. The current release is a major revision from an earlier version <ref> [9] </ref>. In the interest of making the system available as soon as possible, this initial release is a minimal system that we believe is useful to some compiler researchers. Not all the passes we have developed in the previous 1 system have been updated.
Reference: [10] <author> Steven W. K. Tjiang and John L. Hennessy. </author> <title> Sharlit|a tool for building optimizers. </title> <booktitle> In Proceedings of the ACM SIGPLAN'92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 82-93, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: The data flow optimizer is built using Sharlit|a data flow optimizer generator that automatically takes a data flow description and translates it into efficient data flow optimization code <ref> [10] </ref>. The optimizations implemented using Sharlit include constant propagation, partial redundancy elimination, strength reduction, and register allocation. 4.4 Compiler Development Tools The compiler toolkit includes several tools to facilitate the compiler development process.
Reference: [11] <author> M. E. Wolf and M. S. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In Proceedings of the ACM SIGPLAN'91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 30-44, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: The technique has been shown to efficiently generate exact answers to all the data dependence tests invoked on the PERFECT Club benchmarks. We have also extended our dependence analyzer to handle some simple nonlinear array accesses. Our loop transformer is based on Wolf and Lam's algorithms <ref> [11, 12] </ref>. The parallelizer optimizes the code for coarse-grain parallelism via unimodular loop transformations (loop interchange, skewing, and reversal) and blocking (or tiling). It can also optimize for cache locality on uniproces-sors and multiprocessors. The algorithm operates not just on loops with distance vectors but also those with direction vectors.
Reference: [12] <author> M. E. Wolf and M. S. Lam. </author> <title> A loop transformation theory and an algorithm to maximize parallelism. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <pages> pages 452-471, </pages> <month> October </month> <year> 1991. </year> <note> also available as technical report CSL-TR-91-457. </note>
Reference-contexts: The technique has been shown to efficiently generate exact answers to all the data dependence tests invoked on the PERFECT Club benchmarks. We have also extended our dependence analyzer to handle some simple nonlinear array accesses. Our loop transformer is based on Wolf and Lam's algorithms <ref> [11, 12] </ref>. The parallelizer optimizes the code for coarse-grain parallelism via unimodular loop transformations (loop interchange, skewing, and reversal) and blocking (or tiling). It can also optimize for cache locality on uniproces-sors and multiprocessors. The algorithm operates not just on loops with distance vectors but also those with direction vectors.
References-found: 12

