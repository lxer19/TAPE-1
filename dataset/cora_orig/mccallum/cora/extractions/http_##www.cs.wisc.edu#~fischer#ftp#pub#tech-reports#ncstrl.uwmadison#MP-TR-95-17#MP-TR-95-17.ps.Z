URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/MP-TR-95-17/MP-TR-95-17.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/MP-TR-95-17/
Root-URL: http://www.cs.wisc.edu
Title: NONLINEAR JACOBI AND *-RELAXATION METHODS FOR PARALLEL NETWORK OPTIMIZATION  
Author: By Armand A. Zakarian 
Degree: A dissertation submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy (Computer Sciences) at the  
Date: 1995  
Address: WISCONSIN MADISON  
Affiliation: UNIVERSITY OF  
Abstract-found: 0
Intro-found: 1
Reference: <author> Ali, Agha Iqbal & J. </author> <title> Kennington (1977), MNETGEN program documentation, </title> <type> Technical Report IEOR 77003, </type> <institution> Department of Industrial Engineering and Operations Research, Southern Methodist University, Dallas, Texas 75275. </institution>
Reference-contexts: In section 4.3.1 we describe a parallel implementation of the nonlinear Jacobi method on the Thinking Machines CM-5 supercomputer. The test problems solved here are linear and quadratic problems generated by MNETGEN <ref> (Ali & Kennington 1977) </ref>. The final section 4.3.2 focuses on a distributed implementation of the NJA on a cluster of workstations running PVM (a parallel programming environment for a collection of heterogeneous computers).
Reference: <author> Bertsekas, Dimitri P., Patrick A. Hosein & Paul Tseng (1987), </author> <title> `Relaxation methods for network flow problems with convex arc costs', </title> <journal> SIAM Journal on Control and Optimization 25(5), </journal> <pages> 1219-1243. </pages>
Reference-contexts: This together with its acyclicity guarantees that Algorithm 1 terminates finitely. The pair (x fl ; fl ) computed by Algorithm 1 is nearly optimal. The follow ing two lemmas (Corollary 3.1 and Proposition 3.6 from <ref> (Bertsekas et al. 1987) </ref>, respectively) give the precise statement. <p> The results for the first and third groups are comparable (after taking into account the computer performance differential) to those reported in <ref> (Bertsekas et al. 1987) </ref> for similarly modified NETGEN problems. However, the method of (Bertsekas et al. 1987) had difficulties solving problems similar to "50% large quadratic/50% small quadratic," being more than five times slower for them compared to the corresponding "50% linear/50% quadratic" problems. <p> The results for the first and third groups are comparable (after taking into account the computer performance differential) to those reported in <ref> (Bertsekas et al. 1987) </ref> for similarly modified NETGEN problems. However, the method of (Bertsekas et al. 1987) had difficulties solving problems similar to "50% large quadratic/50% small quadratic," being more than five times slower for them compared to the corresponding "50% linear/50% quadratic" problems.
Reference: <author> Bertsekas, D.P. </author> <year> (1991), </year> <title> Linear Network Optimization:Algorithms and Codes, </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts. </address>
Reference-contexts: Let x be any optimal solution of (17). Then for every (i; j) 2 A d ij jx fl Proof Fix (i; j) 2 A and assume that x fl ij 6= x ij . According to the Conformal Realization Theorem <ref> (Bertsekas 1991) </ref> x fl x can be decomposed into a sum of conforming simple cycle flows. Let Y be any of the cycles containing (i; j). <p> One way of enforcing Assumption 2.3.1 in practice is to consider the nodes in Step 1 in some fixed order that includes all nodes in N . A more economical solution is used in the linear code *-RELAX <ref> (Bertsekas 1991) </ref>. There the nodes that have positive surplus are kept in a queue. <p> The computational results in Section 4.2 demonstrate that the performance of the algorithm for nonlinear problems is significantly improved by mixing both "up" and "down" iterations. The above assumption can be computationally checked by using a device similar to the one used in the code *-RELAX-N <ref> (Bertsekas 1991) </ref>. For a node i, let p k i be the number of "up" iterations executed at node i before iteration number k.
Reference: <author> Bertsekas, </author> <title> D.P. & J.N. Tsitsiklis (1989), Parallel and Distributed Computation, </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey. </address>
Reference-contexts: In this case every processor is responsible for solving several problems (29) in Gauss-Seidel fashion. Convergence of the nonlinear Gauss-Seidel algorithm is established in <ref> (Bertsekas & Tsitsiklis 1989) </ref>. We would like to combine our results with those in (Bertsekas & Tsitsiklis 1989) to prove convergence under different coordination schemes. The parallel linear successive overrelaxation method of (Mangasarian & De Leone 1987) will usually have a better convergence speed than that of a Jacobi method. <p> In this case every processor is responsible for solving several problems (29) in Gauss-Seidel fashion. Convergence of the nonlinear Gauss-Seidel algorithm is established in <ref> (Bertsekas & Tsitsiklis 1989) </ref>. We would like to combine our results with those in (Bertsekas & Tsitsiklis 1989) to prove convergence under different coordination schemes. The parallel linear successive overrelaxation method of (Mangasarian & De Leone 1987) will usually have a better convergence speed than that of a Jacobi method. <p> As the algorithm progresses, the difference between the two nodes' estimates of the flow approaches zero. This algorithm may be viewed as a generalization of the totally asynchronous algorithm for linear network flow 81 problems of <ref> (Bertsekas & Tsitsiklis 1989) </ref>, to nonlinear cost functions.
Reference: <author> Chen, </author> <title> Chunhui & O.L. Mangasarian (1994), A class of smoothing functions for nonlinear and mixed complementarity problems, </title> <type> Technical Report 94-11, </type> <institution> Center for Parallel Optimization, University of Wisconsin|Madison. </institution>
Reference: <author> Collins, M., L. Cooper, R. Helgason, J. Kennington & L. </author> <title> LeBlanc (1978), `Solving the pipe network analysis problem using optimization techniques', </title> <booktitle> Management Science 24, </booktitle> <pages> 747-760. </pages>
Reference-contexts: Networks with nonlinear objectives arise in a variety of applications including electrical networks (Hu 1966), pipeline networks <ref> (Collins, Cooper, Helgason, Kennington & LeBlanc 1978) </ref>, urban traffic flow (Magnanti 1984) and communications networks (Monma & Segal 1982). We make the following assumptions with regard to the functions f ij .
Reference: <author> Dantzig, G.B. & P. </author> <title> Wolfe (1960), `Decomposition principle for linear programs', </title> <note> Operations Research 8, 101-111. 90 De Leone, </note> <editor> R., Manlio Gaudioso & Maria Flavia Monaco (1992), </editor> <title> Nonsmooth optimization methods for parallel decomposition of multicommodity flow problems, </title> <type> Technical Report 1080, </type> <institution> Computer Sciences Dept., University of Wisconsin|Madison. </institution>
Reference-contexts: To present a more complete picture of parallel decomposition methods, we discuss in the last subsection some resource-directive based approaches. 4 1.3.1 The Dantzig-Wolfe method Dantzig-Wolfe decomposition <ref> (Dantzig & Wolfe 1960) </ref> is a classical price-directive solution approach for linear block-angular programs. Let X i and S i be matrices whose columns are the extreme points and the extreme rays of B i , respectively.
Reference: <author> De Leone, R., R. R. Meyer, S. Kontogiorgis, A. Zakarian & G. </author> <month> Zakeri </month> <year> (1994), </year> <title> `Coordination in coarse-grained decomposition', </title> <journal> SIAM Journal on Optimization 4, </journal> <pages> 777-793. </pages>
Reference-contexts: is identified, the corresponding column is generated, updated and a pivot in (2) (an update of the basis inverse) is performed. 5 A fork-join parallel implementation of the method where one processor is dedicated to solving the master problem and another K processors solve the subproblems independently is described in <ref> (De Leone, Meyer, Kontogiorgis, Zakarian & Zakeri 1994) </ref>. <p> More effort spent in the coordination step usually produces better iterates and hence leads to faster convergence. On the other hand, a complex coordinator is essentially a serial step that brings down the parallel efficiency of the decomposition method. In <ref> (De Leone et al. 1994) </ref> we focused on the coordination mechanisms employed by several of the decomposition algorithms mentioned in Chapter 1. <p> Further, the good scalability of Step 2c makes it superior for problems with large number of commodities. The final table 6 compares number of outer/inner (augmented Lagrangian/ nonlinear Jacobi) iterations and CPU time of our algorithm with Step 2b to those reported in <ref> (De Leone et al. 1994) </ref> for an implementation of the Dantzig-Wolfe method. Both codes use the same programming model and were run on the CM-5. <p> Every machine has a dual processor CPU, 64 MB of memory and runs the Solaris 2.1 operating system. The machines are connected by an Ethernet network. We used the Parallel Virtual Machine (PVM) v3.3 package <ref> (Geist, Beguelin, Dongarra, Jiang, Manchek & Sunderam 1994) </ref> to develop a distributed version of our code. PVM makes available a rich collection of utility functions for distributed applications running on a collection of heterogeneous computers. These include creation and termination of tasks, synchronization, and communication.
Reference: <author> Evans, J.R. </author> <year> (1977), </year> <title> `Some network flow models and heuristics for multiproduct production and inventory planning', </title> <journal> AIIE Transactions 9, </journal> <pages> 75-81. </pages>
Reference-contexts: The first model is typical for applications such as railroad transportation networks or computer networks. The second type frequently arises in communication and distribution/transportation networks. Other applications of MC include optimal deployment of resources (Kaplan 1973), VLSI circuit layout (Korte 1988) and multi-product multi-stage production-inventory planning <ref> (Evans 1977) </ref>. 1.3 Overview of parallel decomposition methods In this section we briefly describe several approaches to decomposing MC for parallel solution. The first four methods share some price-directive elements.
Reference: <author> Ferris, </author> <title> M.C. & O.L. Mangasarian (1993), Parallel variable distribution, </title> <type> Technical Report 1175, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, Wisconsin. </institution>
Reference: <author> Geist, Al, Adam Beguelin, Jack Dongarra, Weicheng Jiang, Robert Manchek & Vaidy Sunderam (1994), </author> <title> PVM: Parallel Virtual Machine, A User's Guide and Tutorial for Networked Parallel Computing, </title> <publisher> The MIT Press. </publisher>
Reference-contexts: Every machine has a dual processor CPU, 64 MB of memory and runs the Solaris 2.1 operating system. The machines are connected by an Ethernet network. We used the Parallel Virtual Machine (PVM) v3.3 package <ref> (Geist, Beguelin, Dongarra, Jiang, Manchek & Sunderam 1994) </ref> to develop a distributed version of our code. PVM makes available a rich collection of utility functions for distributed applications running on a collection of heterogeneous computers. These include creation and termination of tasks, synchronization, and communication.
Reference: <author> Gill, P., S. Hammarling, W. Murray, M. Saunders & M. </author> <title> Wright (1986), User's guide for LSSOL: A FORTRAN package for constrained linear least-squares and convex quadratic programming, </title> <type> Technical Report SOL 86-1, </type> <institution> Department of Operations Research, Stanford University, Stanford, California 94305. </institution> <note> 91 Gill, </note> <author> P.E., W. Murray & M.H. </author> <title> Wright (1981), Practical Optimization, </title> <publisher> Academic Press, London. </publisher>
Reference-contexts: Appendix B). The multidimensional search coordination problem (Step 2b) of the NJA was solved using LSSOL <ref> (Gill, Ham-marling, Murray, Saunders & Wright 1986) </ref>|a popular routine for constrained least-squares optimization. 67 We used the augmented Lagrangian formulation (25). The augmented La grangian algorithm was terminated when kp s+1 p s k 1 5 10 4 where p is the vector of multipliers associated with the coupling constraints.
Reference: <author> Glowinski, R. & A. </author> <month> Marrocco </month> <year> (1975), </year> <title> `Sur l' approximation par elements finis d' order un, et la resolution par penalisation-dualite d' une classe de problemes de Dirichlet nonlineaires', </title> <journal> Revue Fran~caise d' Automatique, Informatique et Recherche Operationelle 2, </journal> <pages> 41-76. </pages>
Reference-contexts: developed by Zakeri (1995) who proposes replacing the master problem of (Schultz & Meyer 1991) by a collection of (smaller) coordinating problems that can be solved simultaneously thus improving the parallel efficiency of the original method. 1.3.4 Method of alternating directions Kontogiorgis (1994) specializes the Alternating Directions method originally of <ref> (Glowinski & Marrocco 1975) </ref> for the parallel solution of MC.
Reference: <author> Hu, T.C. </author> <year> (1966), </year> <title> `Minimum cost flows in convex cost networks', </title> <journal> Naval Research Logistics Quarterly 13, </journal> <pages> 1-9. </pages>
Reference-contexts: Networks with nonlinear objectives arise in a variety of applications including electrical networks <ref> (Hu 1966) </ref>, pipeline networks (Collins, Cooper, Helgason, Kennington & LeBlanc 1978), urban traffic flow (Magnanti 1984) and communications networks (Monma & Segal 1982). We make the following assumptions with regard to the functions f ij .
Reference: <author> Kamesam, P.V. </author> & <title> R.R. Meyer (1984), `Multipoint methods for separable nonlinear networks', </title> <booktitle> Mathematical Programming Study 22, </booktitle> <pages> 185-205. </pages>
Reference-contexts: The solution method for nonlinear network flow problems described above is described in <ref> (Kamesam & Meyer 1984) </ref>. 41 Chapter 3 Parallel decomposition of the multi-commodity flow problem 3.1 Overview In this chapter we describe a parallel decomposition method for the multi-commodity network flow problem that is based on applying a nonlinear Jacobi algorithm to the augmented Lagrangian formulation of the problem.
Reference: <author> Kaplan, S. </author> <year> (1973), </year> <title> `Readiness and the optimal redeployment of resources', </title> <journal> Naval Research Logistics Quarterly 20, </journal> <pages> 625-638. </pages>
Reference-contexts: The first model is typical for applications such as railroad transportation networks or computer networks. The second type frequently arises in communication and distribution/transportation networks. Other applications of MC include optimal deployment of resources <ref> (Kaplan 1973) </ref>, VLSI circuit layout (Korte 1988) and multi-product multi-stage production-inventory planning (Evans 1977). 1.3 Overview of parallel decomposition methods In this section we briefly describe several approaches to decomposing MC for parallel solution. The first four methods share some price-directive elements.
Reference: <author> Klingman, D., A. Napier & J. </author> <title> Stutz (1974), `NETGEN|A program for generation of large-scale (un)capacitated assignment, transportation and minimum cost network problems', </title> <booktitle> Management Science 20, </booktitle> <pages> 814-822. </pages>
Reference-contexts: In section 4.2 we describe a serial implementation of the relaxation algorithm of Chapter 2 and report results on solving mixed linear/quadratic network problems generated by NETGEN <ref> (Klingman, Napier & Stutz 1974) </ref>. We compare the relaxation method with an implementation of the multi-point method and study the effect of varying the percentage of arcs having nonlinear cost functions on the performance of the two respective algorithms. <p> given by max fi fi X x fl X x fl fi fi The computed objective value was accurate to nearly ten significant digits: fi fi fi f (x fl ) fi fi 10 10 58 4.2.1 The NETGEN problems We obtained our test problems by modifying the standard NETGEN <ref> (Klingman et al. 1974) </ref> problems 1-10 and 16-25. The first ten problems are transportation problems while the rest are transshipment problems. The transshipment problems differ in number of transshipment sources and sinks, percentage of capacitated arcs and size of upper bounds.
Reference: <author> Kontogiorgis, S. </author> <year> (1994), </year> <title> Alternating Directions methods for the parallel solution of large-scale block-structured optimization problems, </title> <type> PhD thesis, </type> <institution> University of Wisconsin, Madison, Wisconsin. </institution> <note> 92 Korte, </note> <author> B. </author> <year> (1988), </year> <title> Applications of combinatorial optimization, </title> <type> Technical Report 88541-OR, </type> <institution> Institute fur Okonometrie und Operations Research, Bonn, Germany. </institution>
Reference-contexts: is identified, the corresponding column is generated, updated and a pivot in (2) (an update of the basis inverse) is performed. 5 A fork-join parallel implementation of the method where one processor is dedicated to solving the master problem and another K processors solve the subproblems independently is described in <ref> (De Leone, Meyer, Kontogiorgis, Zakarian & Zakeri 1994) </ref>. <p> + p T (Ax + b Bz) + 2 2 repeatedly in a block Gauss-Seidel fashion: x t+1 2 argmin x 8 z t+1 2 argmin z p t+1 = p t + r p L (x t+1 ; z t+1 ; p t ) The Activity-and-Resource Proximization splitting of <ref> (Kontogiorgis 1994) </ref> casts MC in the form of (5) by introducing variables ~ d 1 ; : : : ; ~ d K representing the allocation of the common resource d to the commodities and defining h i (x i ) := &gt; &gt; &lt; f i (x i ) if <p> The matrices A and B and the vector b in (5) are taken to be identity matrices and zero, respectively. The linear constraints of (5) thus become simply x i = y i ; ~ d i = d i 8i. In <ref> (Kontogiorgis 1994) </ref> two other splittings for MC are introduced|the Resource Proximization splitting and the Activity Proximization splitting.
Reference: <author> Magnanti, T.L. </author> <year> (1984), </year> <title> Models and algorithms for predicting urban traffic equilibria, </title> <editor> in M.Florian, ed., </editor> <title> `Transportation planning models', </title> <publisher> North-Holland, Amsterdam, </publisher> <pages> pp. 153-186. </pages>
Reference-contexts: Networks with nonlinear objectives arise in a variety of applications including electrical networks (Hu 1966), pipeline networks (Collins, Cooper, Helgason, Kennington & LeBlanc 1978), urban traffic flow <ref> (Magnanti 1984) </ref> and communications networks (Monma & Segal 1982). We make the following assumptions with regard to the functions f ij .
Reference: <author> Mangasarian, O. L. & R. </author> <title> De Leone (1987), `Parallel successive overrelaxation methods for symmetric linear complementarity problems and linear programs', </title> <journal> Journal of Optimization Theory and Applications 54, </journal> <pages> 437-466. </pages>
Reference-contexts: Convergence of the nonlinear Gauss-Seidel algorithm is established in (Bertsekas & Tsitsiklis 1989). We would like to combine our results with those in (Bertsekas & Tsitsiklis 1989) to prove convergence under different coordination schemes. The parallel linear successive overrelaxation method of <ref> (Mangasarian & De Leone 1987) </ref> will usually have a better convergence speed than that of a Jacobi method. We are interested in extending the method to nonlinear objectives and performing a comparison with the nonlinear Jacobi method.
Reference: <author> Mangasarian, Olvi L. </author> <year> (1969), </year> <title> Nonlinear Programming, </title> <publisher> McGraw-Hill. </publisher>
Reference: <author> Medhi, D. </author> <year> (1987), </year> <title> Decomposition of structured large-scale optimization problems and parallel optimization, </title> <type> Technical Report 718, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, Wisconsin 53706. </institution> <type> Ph.D. thesis. </type>
Reference: <author> Monma, C.L. & M. </author> <title> Segal (1982), `A primal algorithm for finding minimum-cost flows in capacitated networks with applications', </title> <journal> Bell System Technical Journal 61, </journal> <pages> 449-468. </pages>
Reference-contexts: Networks with nonlinear objectives arise in a variety of applications including electrical networks (Hu 1966), pipeline networks (Collins, Cooper, Helgason, Kennington & LeBlanc 1978), urban traffic flow (Magnanti 1984) and communications networks <ref> (Monma & Segal 1982) </ref>. We make the following assumptions with regard to the functions f ij . Assumption 2.2.1 Each f ij is a closed proper convex function and the intersection of dom P with the set of vectors satisfying the constraints of (17) is nonempty.
Reference: <author> Nielsen, S.S. & S.A. </author> <title> Zenios (1990), Massively parallel algorithms for singly constrained nonlinear programs, </title> <type> Technical Report 90-03-01, </type> <institution> Decision Sciences 93 Department, The Wharton School, </institution> <address> Philadelphia, PA 19104. </address> <month> Revised March </month> <year> 1991. </year>
Reference: <author> Pnar, M.C. & S. </author> <title> Zenios (1992), `Parallel decomposition of multicommodity network flows using a linear-quadratic penalty algorithm', </title> <journal> ORSA Journal on Computing 4(3), </journal> <pages> 235-249. </pages> <month> Rockafellar, </month> <title> R.T (1970), Convex Analysis, </title> <publisher> Princeton University Press, </publisher> <address> Prince-ton, NJ. </address>
Reference: <author> Rockafellar, R.T. </author> <year> (1976), </year> <title> `Augmented Lagrangians and applications of the proximal point algorithm in convex programming', </title> <journal> Mathematics of Operations Research 1(2), </journal> <pages> 97-116. </pages>
Reference-contexts: If f i are convex lower semicontinuous, the method is globally convergent to the optimal set of (1) and the sequence fp k g 1 k=0 converges to a particular dual 43 optimal solution. Additionally, if f i are polyhedral, the method converges in finitely many iterations <ref> (Rockafellar 1976) </ref>. Updating in Step 3, although not necessary to establish convergence, can significantly improve the convergence speed of the algorithm in practice. A heuristic scheme that increases if the there is not a sufficient reduction in constraint violation from the previous iteration, has been successfully used.
Reference: <author> Rockafellar, R.T. </author> <year> (1984), </year> <title> Network Flows and Monotropic Optimization, </title> <publisher> John Wiley & Sons, </publisher> <address> New York. </address>
Reference: <author> Rockafellar, R.T. & R.J.-B. </author> <month> Wets </month> <year> (1991), </year> <title> `Scenarios and policy aggregation in optimization under uncertainty', </title> <journal> Mathematics of Operations Research 10, </journal> <pages> 119-147. </pages>
Reference: <author> Rosa, C. & A. </author> <title> Ruszczynski (1994), On augmented Lagrangian decomposition methods for multistage stochastic programs, </title> <type> WP 94-125, </type> <institution> IIASA, A-2361 Laxenburg, Austria. </institution> <note> 94 Ruszczynski, </note> <author> A. </author> <year> (1993a), </year> <title> Augmented Lagrangian decomposition for sparse convex optimization, </title> <type> WP 92-75, </type> <institution> IIASA, A-2361 Laxenburg, Austria. </institution> <note> revised April 1993. </note>
Reference: <author> Ruszczynski, A. </author> <year> (1993b), </year> <title> `Parallel decomposition of multistage stochastic programs', </title> <booktitle> Mathematical Programming 58, </booktitle> <pages> 201-228. </pages>
Reference-contexts: A bundle approach has also been used by Medhi (1987) to optimize the Rockafellar dual max ( K X min n o of MC in parallel using the fact that the inner minimization is separable by block. 11 1.4 The augmented Lagrangian decomposition Augmented Lagrangian decomposition has been successfully used <ref> (Ruszczynski 1993b) </ref> for solving large-scale block angular programs arising in the deterministic formulation of multi-stage stochastic programs.
Reference: <author> Schultz, G.L. </author> & <title> R.R. Meyer (1991), `An interior point method for block angular optimization', </title> <note> SIAM Journal on Optimization 1(4), </note> <month> 583-602. </month> <title> Thinking Machines Corporation (1991), The Connection Machine CM-5 Technical Summary, </title> <address> Cambridge, MA. Toint, Ph.L. & D. </address> <month> Tuyttens </month> <year> (1992), </year> <title> `LSNNO: a Fortran subroutine for solving large-scale nonlinear network opimization problems', </title> <journal> ACM Transactions on Mathematical Software 18(3), </journal> <pages> 308-328. </pages>
Reference-contexts: The resulting subproblems (which also employ trust region constraints) generate descent directions for each block in parallel. These directions are then used by a multi-dimensional line-search master procedure to find approximate solutions of (4). The method is further developed by Zakeri (1995) who proposes replacing the master problem of <ref> (Schultz & Meyer 1991) </ref> by a collection of (smaller) coordinating problems that can be solved simultaneously thus improving the parallel efficiency of the original method. 1.3.4 Method of alternating directions Kontogiorgis (1994) specializes the Alternating Directions method originally of (Glowinski & Marrocco 1975) for the parallel solution of MC.
Reference: <author> Zakeri, G. </author> <year> (1995), </year> <title> Multi-Coordination Methods for Parallel Solution of Block-Angular Programs, </title> <type> PhD thesis, </type> <institution> University of Wisconsin, Madison, Wis-consin. </institution>
Reference: <author> Zenios, S.A., </author> <title> M.C. Pnar & R.S. Dembo (1991), A smooth penalty function algorithm for network-structured problems, </title> <type> Technical Report 90-12-05, </type> <institution> Decision Sciences Department, The Wharton School, </institution> <address> Philadelphia, PA 19104. </address>
References-found: 33

