URL: http://polaris.cs.uiuc.edu/reports/1373.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Email: moreira@csrd.uiuc.edu cdp@csrd.uiuc.edu  
Title: Autoscheduling in a Distributed Shared-Memory Environment  
Author: Jose E. Moreira Constantine D. Polychronopoulos 
Date: 1373  
Address: 1308 W. Main St. Urbana, IL 61801-2307 USA  
Affiliation: Center for Supercomputing Research and Development and Coordinated Science Laboratory University of Illinois at Urbana-Champaign  
Pubnum: CSRD Report No.  
Abstract: The ease of programming and compiling for the shared memory multiprocessor model, coupled with the scalability and cost advantages of distributed memory computers, give an obvious appeal to distributed shared memory architectures. In this paper we discuss the design and implementation issues of a dynamic data management and scheduling environment for distributed shared memory architectures. Unlike the predominantly static approaches used on distributed and message passing machines, we advocate the advantages of dynamic resource allocation, especially in the case of multi-user environments. We propose hybrid data and work distribution techniques that adjust to variations in the physical partition, achieving better load balance than purely static schemes. We present the architecture of our execution environment and discuss implementation details of some of the critical components. Preliminary results using benchmarks of representative execution profiles support our main thesis: With minimal control, the load balancing and resource utilization advantages offered by dynamic methods often outweigh the disadvantage of increased memory latency stemming from slightly compromised data locality, and perhaps additional run-time overhead.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. </author> <booktitle> Compilers Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, MA, </address> <year> 1986. </year>
Reference-contexts: The execution of a program begins with its START task in the ready queue and terminates when its STOP task finishes. The activation frames of a parallel program cannot be implemented with the simple stack structure normally used in sequential programs <ref> [1] </ref>, since several instances of subroutines and loop iterations can be active at the same time. Instead, a cactus-stack [21], equivalent to a (dynamic) tree of activation frames, is used. For example, consider the code in Figure 3 (a).
Reference: [2] <author> Siegfried Benkner, Barbara M. Chapman, and Hans P. Zima. </author> <title> Vienna Fortran 90. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference SHPCC-92, </booktitle> <pages> pages 51-59, </pages> <year> 1992. </year>
Reference-contexts: A technical description of the KSR-1, a cache-only variant of distributed shared memory, is provided in [14]. Examples of languages that support the specification of data distribution include Fortran D [6], Vi-enna Fortran 90 <ref> [2] </ref>, High Performance Fortran [11, 15], and Cray MPP Fortran [20]. Compilation issues for these languages, including the actual implementation of the data distributions, code generation, and communication of distributed data across procedure boundaries, are discussed in [4, 10, 12, 13, 24].
Reference: [3] <author> Carl J. Beckmann. </author> <title> Hardware and Software for Functional and Fine Grain Parallelism. </title> <type> PhD thesis, </type> <institution> Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, </institution> <year> 1993. </year>
Reference-contexts: Information on control and data dependences allows the exploitation of functional (task level) parallelism in addition to data (loop level) parallelism. A brief summary of the properties of the HTG is given here, and details can be found in <ref> [3, 7, 16, 22] </ref>. The hierarchical task graph is a directed acyclic graph HTG = (HV ; HE ) with unique nodes START and STOP 2 HV , the set of vertices.
Reference: [4] <author> Alok Choudhary, Geoffrey Fox, Sanjay Ranka, Seema Hiranandani, Ken Kennedy, Charles Koelbel, and Chau-Wen Tseng. </author> <title> Compiling Fortran 77D and 90D for MIMD distributed-memory machines. </title> <booktitle> In Proceedings of the Fourth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 4-11, </pages> <year> 1992. </year>
Reference-contexts: Compilation issues for these languages, including the actual implementation of the data distributions, code generation, and communication of distributed data across procedure boundaries, are discussed in <ref> [4, 10, 12, 13, 24] </ref>. Research is also being conducted on the field of automatic data partitioning techniques, which can alleviate the job of data distribution from the programmer through a smart compiler.
Reference: [5] <author> Cray Research Inc. </author> <title> CRAY T3D System Architecture Overview Manual, </title> <year> 1994. </year>
Reference-contexts: They combine the scalability of message passing architectures, with the easier programming model of shared memory. For a survey of distributed shared memory architectures, see [18]. Detailed information on the architecture and programming model of the Cray T3D can be found in <ref> [5, 19, 20] </ref>. A technical description of the KSR-1, a cache-only variant of distributed shared memory, is provided in [14]. Examples of languages that support the specification of data distribution include Fortran D [6], Vi-enna Fortran 90 [2], High Performance Fortran [11, 15], and Cray MPP Fortran [20].
Reference: [6] <author> Geoffrey Fox, Seema Hiranandani, Ken Kennedy, Charles Koebel, Uli Kremer, Chau-Wen Tseng, and Min-You Wu. </author> <title> Fortran D language specification. </title> <type> Technical Report COMP TR90-141, </type> <institution> Department of Computer Science, Rice University, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: These tasks and iterations can still be executed by any processor because the single shared address space in autoscheduling allows any task to be executed in any processor. Our approach to data distribution uses features from Fortran D <ref> [6] </ref>, HPF [11], and Cray MPP Fortran [20], and works as follows. <p> Detailed information on the architecture and programming model of the Cray T3D can be found in [5, 19, 20]. A technical description of the KSR-1, a cache-only variant of distributed shared memory, is provided in [14]. Examples of languages that support the specification of data distribution include Fortran D <ref> [6] </ref>, Vi-enna Fortran 90 [2], High Performance Fortran [11, 15], and Cray MPP Fortran [20]. Compilation issues for these languages, including the actual implementation of the data distributions, code generation, and communication of distributed data across procedure boundaries, are discussed in [4, 10, 12, 13, 24].
Reference: [7] <author> M. Girkar and C. D. Polychronopoulos. </author> <title> The HTG: An intermediate representation for programs based on control and data dependences. </title> <type> Technical Report 1046, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: The distributed autoscheduling model borrows many of the characteristics of autoscheduling, which has been the subject of some of our recent work <ref> [7, 17] </ref>. Like autoscheduling, we assume that a user application is decomposed 1 into a collection of well-defined subcomputations (tasks). During execution, the firing of tasks is performed dynamically by means of compiler generated code which enforces data and control dependences. <p> Information on control and data dependences allows the exploitation of functional (task level) parallelism in addition to data (loop level) parallelism. A brief summary of the properties of the HTG is given here, and details can be found in <ref> [3, 7, 16, 22] </ref>. The hierarchical task graph is a directed acyclic graph HTG = (HV ; HE ) with unique nodes START and STOP 2 HV , the set of vertices.
Reference: [8] <author> Anoop Gupta, Andrew Tucker, and Luis Stevens. </author> <title> Making effective use of shared-memory multiprocessors: The process control approach. </title> <type> Technical Report CSL-TR-91-475A, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <year> 1991. </year> <month> 20 </month>
Reference-contexts: An example of such systems is the Alliant FX/80. A combination of the two approaches can alleviate some of these problems, but not as effectively as a scheme that dynamically partitions the processors among the executing processes. Such a scheme (an example is process control <ref> [8] </ref>) avoids unnecessary context switching, thus improving locality, and ideally keeps each process running on a partition compatible with the parallelism of the program and the total load of the system.
Reference: [9] <author> Manish Gupta and Prithviraj Banerjee. </author> <title> Demonstration of automatic data partitioning techniques for parallelizing compilers on multicomputers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(2) </volume> <pages> 179-193, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Research is also being conducted on the field of automatic data partitioning techniques, which can alleviate the job of data distribution from the programmer through a smart compiler. An example of such research is found in <ref> [9] </ref>. 11 Conclusions We have addressed many of the issues involved on the implementation of autoscheduling in a distributed shared-memory environment: data distribution, load distribution, task queue implementation, enqueueing and dequeueing policies, activation frame allocation, virtual to physical processor mapping, loop scheduling and loop distribution.
Reference: [10] <author> Mary W. Hall, Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Interprocedural compilation of Fortran D for MIMD distributed memory machines. </title> <booktitle> In Proceedings of Supercomputing'92, </booktitle> <pages> pages 522-534, </pages> <year> 1992. </year>
Reference-contexts: Compilation issues for these languages, including the actual implementation of the data distributions, code generation, and communication of distributed data across procedure boundaries, are discussed in <ref> [4, 10, 12, 13, 24] </ref>. Research is also being conducted on the field of automatic data partitioning techniques, which can alleviate the job of data distribution from the programmer through a smart compiler.
Reference: [11] <author> High Performance Fortran Forum. </author> <title> High Perfomance Fortran Language Specification, </title> <note> Version 1.0, </note> <month> May </month> <year> 1993. </year>
Reference-contexts: These goals are achieved through the following mechanism: * Data are partitioned at compile time across sets of virtual processors defined by the user (or the compiler), in a manner similar to HPF <ref> [11] </ref>. * Virtual processors are assigned on demand to physical processors during run-time. Virtual processors can migrate between physical processors during the execution of a process and may even, at times, not be assigned to any physical processor. <p> These tasks and iterations can still be executed by any processor because the single shared address space in autoscheduling allows any task to be executed in any processor. Our approach to data distribution uses features from Fortran D [6], HPF <ref> [11] </ref>, and Cray MPP Fortran [20], and works as follows. <p> A technical description of the KSR-1, a cache-only variant of distributed shared memory, is provided in [14]. Examples of languages that support the specification of data distribution include Fortran D [6], Vi-enna Fortran 90 [2], High Performance Fortran <ref> [11, 15] </ref>, and Cray MPP Fortran [20]. Compilation issues for these languages, including the actual implementation of the data distributions, code generation, and communication of distributed data across procedure boundaries, are discussed in [4, 10, 12, 13, 24].
Reference: [12] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Compiler support for machine-independent parallel programming in Fortran D. </title> <type> Technical Report COMP TR91-149, </type> <institution> Department of Computer Science, Rice University, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: Compilation issues for these languages, including the actual implementation of the data distributions, code generation, and communication of distributed data across procedure boundaries, are discussed in <ref> [4, 10, 12, 13, 24] </ref>. Research is also being conducted on the field of automatic data partitioning techniques, which can alleviate the job of data distribution from the programmer through a smart compiler.
Reference: [13] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Example (1): A particular subset of space descriptors, rectangular sections, can be easily described by regular section descriptors (RSDs) <ref> [13] </ref>, which are sequences of triples l i : u i : s i , where l i , u i and s i indicate the lower bound, upper bound, and step, respectively, along the i-th dimension of the RSD. <p> Compilation issues for these languages, including the actual implementation of the data distributions, code generation, and communication of distributed data across procedure boundaries, are discussed in <ref> [4, 10, 12, 13, 24] </ref>. Research is also being conducted on the field of automatic data partitioning techniques, which can alleviate the job of data distribution from the programmer through a smart compiler.
Reference: [14] <institution> Kendall Square Reseach Corporation. Kendall Square Research Technical Summary, </institution> <year> 1992. </year>
Reference-contexts: For a survey of distributed shared memory architectures, see [18]. Detailed information on the architecture and programming model of the Cray T3D can be found in [5, 19, 20]. A technical description of the KSR-1, a cache-only variant of distributed shared memory, is provided in <ref> [14] </ref>. Examples of languages that support the specification of data distribution include Fortran D [6], Vi-enna Fortran 90 [2], High Performance Fortran [11, 15], and Cray MPP Fortran [20].
Reference: [15] <author> David B. Loveman. </author> <title> High Performance Fortran. </title> <journal> IEEE Parallel & Distributed Technology, </journal> <volume> 1(1) </volume> <pages> 24-42, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: A technical description of the KSR-1, a cache-only variant of distributed shared memory, is provided in [14]. Examples of languages that support the specification of data distribution include Fortran D [6], Vi-enna Fortran 90 [2], High Performance Fortran <ref> [11, 15] </ref>, and Cray MPP Fortran [20]. Compilation issues for these languages, including the actual implementation of the data distributions, code generation, and communication of distributed data across procedure boundaries, are discussed in [4, 10, 12, 13, 24].
Reference: [16] <author> Milind Girkar. </author> <title> Functional Parallelism: Theoretical Foundations and Implementation. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Illinois at Urbana-Champaign, </institution> <year> 1992. </year>
Reference-contexts: Information on control and data dependences allows the exploitation of functional (task level) parallelism in addition to data (loop level) parallelism. A brief summary of the properties of the HTG is given here, and details can be found in <ref> [3, 7, 16, 22] </ref>. The hierarchical task graph is a directed acyclic graph HTG = (HV ; HE ) with unique nodes START and STOP 2 HV , the set of vertices.
Reference: [17] <author> Jose E. Moreira and Constantine D. Polychronopoulos. </author> <title> On the implementation and effectiveness of autoscheduling. </title> <type> Technical report, </type> <institution> Center for Supercoputing Research and Development, University of Illinois, </institution> <year> 1994. </year>
Reference-contexts: The distributed autoscheduling model borrows many of the characteristics of autoscheduling, which has been the subject of some of our recent work <ref> [7, 17] </ref>. Like autoscheduling, we assume that a user application is decomposed 1 into a collection of well-defined subcomputations (tasks). During execution, the firing of tasks is performed dynamically by means of compiler generated code which enforces data and control dependences.
Reference: [18] <author> Bill Nitzberg and Virginia Lo. </author> <title> Distributed shared memory: A survey of issues and algorithms. </title> <journal> IEEE Computer, </journal> <volume> 24(9), </volume> <month> August </month> <year> 1991. </year>
Reference-contexts: They combine the scalability of message passing architectures, with the easier programming model of shared memory. For a survey of distributed shared memory architectures, see <ref> [18] </ref>. Detailed information on the architecture and programming model of the Cray T3D can be found in [5, 19, 20]. A technical description of the KSR-1, a cache-only variant of distributed shared memory, is provided in [14].
Reference: [19] <author> Wilfried Oed. </author> <title> The Cray Research Massively Parallel Processor System - CRAY T3D. </title> <type> Technical report, </type> <institution> Cray Reseach GmbH, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: They combine the scalability of message passing architectures, with the easier programming model of shared memory. For a survey of distributed shared memory architectures, see [18]. Detailed information on the architecture and programming model of the Cray T3D can be found in <ref> [5, 19, 20] </ref>. A technical description of the KSR-1, a cache-only variant of distributed shared memory, is provided in [14]. Examples of languages that support the specification of data distribution include Fortran D [6], Vi-enna Fortran 90 [2], High Performance Fortran [11, 15], and Cray MPP Fortran [20].
Reference: [20] <author> Douglas M. Pase, Tom MacDonald, and Andrew Meltzer. </author> <title> MPP Fortran programming model. </title> <type> Technical report, </type> <institution> Cray Research, Inc., </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: These tasks and iterations can still be executed by any processor because the single shared address space in autoscheduling allows any task to be executed in any processor. Our approach to data distribution uses features from Fortran D [6], HPF [11], and Cray MPP Fortran <ref> [20] </ref>, and works as follows. <p> They combine the scalability of message passing architectures, with the easier programming model of shared memory. For a survey of distributed shared memory architectures, see [18]. Detailed information on the architecture and programming model of the Cray T3D can be found in <ref> [5, 19, 20] </ref>. A technical description of the KSR-1, a cache-only variant of distributed shared memory, is provided in [14]. Examples of languages that support the specification of data distribution include Fortran D [6], Vi-enna Fortran 90 [2], High Performance Fortran [11, 15], and Cray MPP Fortran [20]. <p> A technical description of the KSR-1, a cache-only variant of distributed shared memory, is provided in [14]. Examples of languages that support the specification of data distribution include Fortran D [6], Vi-enna Fortran 90 [2], High Performance Fortran [11, 15], and Cray MPP Fortran <ref> [20] </ref>. Compilation issues for these languages, including the actual implementation of the data distributions, code generation, and communication of distributed data across procedure boundaries, are discussed in [4, 10, 12, 13, 24].
Reference: [21] <author> Per Stenstrom. </author> <title> VLSI Support for Cactus Stack Oriented Memory Organization. </title> <booktitle> In Proceedings of the 21 st Annual Hawaii International Conference on System Sciences, </booktitle> <volume> vol I., </volume> <pages> pages 211-220, </pages> <year> 1988. </year>
Reference-contexts: The activation frames of a parallel program cannot be implemented with the simple stack structure normally used in sequential programs [1], since several instances of subroutines and loop iterations can be active at the same time. Instead, a cactus-stack <ref> [21] </ref>, equivalent to a (dynamic) tree of activation frames, is used. For example, consider the code in Figure 3 (a). If it is executed serially, its run-time stack evolves as shown in Figure 3 (b), where only the frame at the top of the stack is active.
Reference: [22] <author> Constantine D. Polychronopoulos. Autoscheduling: </author> <title> Control flow and data flow come together. </title> <type> Technical Report 1058, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <year> 1990. </year>
Reference-contexts: Information on control and data dependences allows the exploitation of functional (task level) parallelism in addition to data (loop level) parallelism. A brief summary of the properties of the HTG is given here, and details can be found in <ref> [3, 7, 16, 22] </ref>. The hierarchical task graph is a directed acyclic graph HTG = (HV ; HE ) with unique nodes START and STOP 2 HV , the set of vertices.
Reference: [23] <author> Ravi Ponnusamy, Joel Saltz, Raja Das, Charles Koebel, and Alok Choudhary. </author> <title> A runtime data mapping scheme for irregular problems. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference SHPCC-92, </booktitle> <pages> pages 216-219, </pages> <year> 1992. </year>
Reference: [24] <author> Hans P. Zima and Barbara Mary Chapman. </author> <title> Compiling for distributed-memory systems. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 81(2) </volume> <pages> 264-287, </pages> <month> February </month> <year> 1993. </year> <month> 21 </month>
Reference-contexts: Compilation issues for these languages, including the actual implementation of the data distributions, code generation, and communication of distributed data across procedure boundaries, are discussed in <ref> [4, 10, 12, 13, 24] </ref>. Research is also being conducted on the field of automatic data partitioning techniques, which can alleviate the job of data distribution from the programmer through a smart compiler.
References-found: 24

