URL: ftp://ftp.cs.rochester.edu/pub/papers/systems/91.TOCS.Scalable_Synch.ps.Z
Refering-URL: http://www.cs.rochester.edu/u/scott/pubs.html
Root-URL: 
Email: address: johnmc@rice.edu.  scott@cs.rochester.edu.  
Title: Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors  
Author: John M. Mellor-Crummey Michael L. Scott 
Address: P.O. Box 1892, Houston, TX 77251-1892. Internet  Rochester, Rochester, NY 14627. Internet address:  
Affiliation: Computation, Rice University,  Computer Science Department, University of  
Note: Center for Research on Parallel  Supported in part by the National Science Foundation under Cooperative Agreement CCR-8809615.  Supported in part by the National Science Foundation under Institutional Infrastruc- ture grant CDA-8822724.  
Date: January 1991  
Abstract: Busy-wait techniques are heavily used for mutual exclusion and barrier synchronization in shared-memory parallel programs. Unfortunately, typical implementations of busy-waiting tend to produce large amounts of memory and interconnect contention, introducing performance bottlenecks that become markedly more pronounced as applications scale. We argue that this problem is not fundamental, and that one can in fact construct busy-wait synchronization algorithms that induce no memory or interconnect contention. The key to these algorithms is for every processor to spin on separate locally-accessible flag variables, and for some other processor to terminate the spin with a single remote write operation at an appropriate time. Flag variables may be locally-accessible as a result of coherent caching, or by virtue of allocation in the local portion of physically distributed shared memory. We present a new scalable algorithm for spin locks that generates O(1) remote references per lock acquisition, independent of the number of processors attempting to acquire the lock. Our algorithm provides reasonable latency in the absence of contention, requires only a constant amount of space per lock, and requires no hardware support other than a swap-with-memory instruction. We also present a new scalable barrier algorithm that generates O(1) remote references per processor reaching the barrier, and observe that two previously-known barriers can likewise be cast in a form that spins only on locally-accessible flag variables. None of these barrier algorithms requires hardware support beyond the usual atomicity of memory reads and writes. We compare the performance of our scalable algorithms with other software approaches to busy-wait synchronization on both a Sequent Symmetry and a BBN Butterfly. Our principal conclusion is that contention due to synchronization need not be a problem in large-scale shared-memory multiprocessors. The existence of scalable algorithms greatly weakens the case for costly special-purpose hardware support for synchronization, and provides a case against so-called "dance hall" architectures, in which shared memory locations are equally far from all processors. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. V. Adve and M. D. Hill. </author> <title> Weak ordering|a new definition. </title> <booktitle> In Proc. of the International Symposium on Computer Architecture, </booktitle> <pages> pages 2-14, </pages> <address> Seattle, WA, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: i, sense is initially true // in nodes [i]: // havechild [j] = true if 4*i+j &lt; P; otherwise false // parentpointer = &nodes [floor ((i-1)/4)].childnotready [(i-1) mod 4], // or &dummy if i = 0 // childpointers [0] = &nodes [2*i+1].parentsense, or &dummy if 2*i+1 &gt;= P // childpointers <ref> [1] </ref> = &nodes [2*i+2].parentsense, or &dummy if 2*i+2 &gt;= P // initially childnotready = havechild and parentsense = false procedure tree_barrier with nodes [vpid] do repeat until childnotready = -false, false, false, false- childnotready := havechild // prepare for next barrier parentpointer^ := false // let parent know I'm ready // <p> false, false, false- childnotready := havechild // prepare for next barrier parentpointer^ := false // let parent know I'm ready // if not root, wait until my parent signals wakeup if vpid != 0 repeat until parentsense = sense // signal children in wakeup tree childpointers [0]^ := sense childpointers <ref> [1] </ref>^ := sense sense := not sense Algorithm 11: A scalable, distributed, tree-based barrier with only local spinning. 19 MP MP cache-coherent, shared-bus multiprocessor. <p> We believe that this cost must be extremely low to make it worth the effort. Of course, increasing the performance of busy-wait locks and barriers is not the only possible rationale for implementing synchronization mechanisms in hardware. Recent work on weakly- consistent shared memory <ref> [1, 13, 28] </ref> has suggested the need for synchronization "fences" that provide clean points for memory semantics. Combining networks, likewise, may improve the performance of memory with bursty access patterns (caused, for example, by sharing after a barrier).
Reference: [2] <author> A. Agarwal and M. Cherian. </author> <title> Adaptive backoff synchronization techniques. </title> <booktitle> In Proc. of the International Symposium on Computer Architecture, </booktitle> <pages> pages 396-406, </pages> <address> Jerusalem, Israel, </address> <month> May </month> <year> 1989. </year>
Reference-contexts: Un- fortunately, typical implementations of busy-waiting tend to produce large amounts of memory and interconnection network contention, which causes performance bottlenecks that become markedly more pronounced in larger machines and applications. As a consequence, the overhead of busy-wait synchronization is widely regarded as a serious performance problem <ref> [2, 6, 11, 14, 38, 49, 51] </ref>. When many processors busy-wait on a single synchronization variable, they create a hot spot that is the target of a disproportionate share of the network traffic. <p> Pfister and Norton [38] showed that the presence of hot spots can severely degrade performance for all traffic in multistage interconnection networks, not just traffic due to synchronizing processors. As part of a larger study, Agarwal and Cherian <ref> [2] </ref> investigated the impact of synchronization on overall program performance. Their simulations of benchmarks on a cache-coherent multiprocessor indicate that memory references due to synchronization cause cache line invalidations much more often than non-synchronization references. <p> On machines without coherent caches, however, or on machines with directory-based caches without broadcast, busy-wait references to a shared location may generate unacceptable levels of memory and interconnect contention. To reduce the interconnection network traffic caused by busy waiting on a barrier flag, Agarwal and Cherian <ref> [2] </ref> investigated the utility of adaptive backoff schemes. They arranged for processors to delay between successive polling operations for geometrically-increasing amounts of time. Their results indicate that in many cases such exponential backoff can substantially reduce the amount of network traffic required to achieve a barrier. <p> Agarwal and Cherian also note that for systems with more than 256 processors, for a range of arrival intervals and delay ratios, backoff strategies are of limited utility for barriers that spin on a single flag <ref> [2] </ref>. In such large-scale systems, the number of network accesses per processor increases sharply as collisions in the interconnection network cause processors to repeat accesses. These observations imply that centralized barrier algorithms will not scale well to large numbers of processors, even using adaptive backoff strategies. <p> These observations imply that centralized barrier algorithms will not scale well to large numbers of processors, even using adaptive backoff strategies. Our experiments (see section 4.4) confirm this conclusion. 6 Commenting on Tang and Yew's barrier algorithm (algorithm 3.1 in [48]), Agarwal and Cherian <ref> [2] </ref> show that on a machine in which contention causes memory accesses to be aborted and retried, the expected number of memory accesses initiated by each processor to achieve a single barrier is linear in the number of processors participating, even if processors arrive at the barrier at approximately the same
Reference: [3] <author> A. Agarwal, R. Simoni, J. Hennessy, and M. Horowitz. </author> <title> An evaluation of directory schemes for cache coherence. </title> <booktitle> In Proc. of the International Symposium on Computer Architecture, </booktitle> <pages> pages 280-289, </pages> <address> New York, NY, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: The class of machines for which the dissemination barrier should outperform all other algorithms includes the BBN Butterfly [8], the IBM RP3 [37], Cedar [50], the BBN Monarch [42], the NYU Ultracomputer [17], and proposed large-scale multiprocessors with directory-based cache coherence <ref> [3] </ref>. Our tree-based barrier will also perform well on these machines. It induces less network load, and requires total space proportional to P , rather than P log P , but its critical path is longer by a factor of about 1.5.
Reference: [4] <author> G. S. Almasi and A. Gottlieb. </author> <title> Highly Parallel Computing. </title> <address> Benjamin/Cummings, Redwood City, CA, </address> <year> 1989. </year>
Reference-contexts: The private flag selects which counter to use; consecutive barriers use alternate counters. Another similar barrier can be found in library packages distributed by Sequent Corporation for the Symmetry multiprocessor. Arriving processors read the current value 5 A similar technique appears in <ref> [4, p. 445] </ref>, where it is credited to Isaac Dimitrovsky. 11 shared count : integer := P shared sense : Boolean := true processor private local_sense : Boolean := true procedure central_barrier local_sense := not local_sense // each processor toggles its own sense if fetch_and_decrement (&count) = 1 count := P
Reference: [5] <author> T. E. Anderson. </author> <title> The performance of spin lock alternatives for shared-memory multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(1) </volume> <pages> 6-16, </pages> <month> Jan. </month> <year> 1990. </year>
Reference-contexts: The simplest approach employs a constant delay; more elaborate schemes use some sort of backoff on unsuccessful probes. Anderson <ref> [5] </ref> reports the best performance with exponential backoff; our experiments confirm this result. Pseudo-code for a test and set lock with exponential backoff appears in algorithm 1. <p> Anderson <ref> [5] </ref> and Graunke and Thakkar [18] have proposed locking algorithms that achieve the constant bound on cache-coherent multiprocessors that support atomic fetch and increment or fetch and store, respectively. <p> Because the atomic add instruction on the Symmetry does not return the old value of its target location, implementation of the ticket lock is not possible on the Symmetry, nor is it possible to implement Anderson's lock directly. In his implementation <ref> [5] </ref>, Anderson (who worked on a Symmetry) introduced an outer test and set lock with randomized backoff to protect the state of his queue. 12 This strategy is reasonable when the critical section protected by the outer lock (namely, acquisition or release of the inner lock) is substantially smaller than the
Reference: [6] <author> T. E. Anderson, E. D. Lazowska, and H. M. Levy. </author> <title> The performance implications of thread management alternatives for shared-memory multiprocessors. </title> <journal> Performance Evaluation Review, </journal> <volume> 17(1) </volume> <pages> 49-60, </pages> <month> May </month> <year> 1989. </year> <note> SIGMETRICS '89 Conference Paper. </note>
Reference-contexts: Un- fortunately, typical implementations of busy-waiting tend to produce large amounts of memory and interconnection network contention, which causes performance bottlenecks that become markedly more pronounced in larger machines and applications. As a consequence, the overhead of busy-wait synchronization is widely regarded as a serious performance problem <ref> [2, 6, 11, 14, 38, 49, 51] </ref>. When many processors busy-wait on a single synchronization variable, they create a hot spot that is the target of a disproportionate share of the network traffic.
Reference: [7] <author> J. Archibald and J.-L. Baer. </author> <title> An economical solution to the cache coherence problem. </title> <booktitle> In Proc. of the International Symposium on Computer Architecture, </booktitle> <pages> pages 355-362, </pages> <year> 1984. </year>
Reference-contexts: Our tree-based barrier with wakeup flag should be the fastest algorithm on large-scale multiprocessors that use broadcast to maintain cache coherence (either in snoopy cache protocols [15] or in directory-based protocols with broadcast <ref> [7] </ref>). It requires only O (P ) updates to shared variables in order to tally arrivals, compared to O (P log P ) for the dissemination barrier.
Reference: [8] <author> BBN Laboratories. </author> <title> Butterfly parallel processor overview. </title> <type> Technical Report 6148, Version 1, </type> <institution> BBN Laboratories, </institution> <address> Cambridge, MA, </address> <month> Mar. </month> <year> 1986. </year>
Reference-contexts: On a machine with coherent caches, processors spin only on locations in their caches. On a machine in which shared memory is distributed (e.g., the BBN Butterfly <ref> [8] </ref>, the IBM RP3 [37], or a shared-memory hypercube [10]), processors spin only on locations in the local portion of shared memory. The implication of our work is that efficient synchronization algorithms can be constructed in software for shared-memory multiprocessors of arbitrary size. <p> It has a shorter critical path than the tree and tournament barriers (by a constant factor), and is therefore faster. The class of machines for which the dissemination barrier should outperform all other algorithms includes the BBN Butterfly <ref> [8] </ref>, the IBM RP3 [37], Cedar [50], the BBN Monarch [42], the NYU Ultracomputer [17], and proposed large-scale multiprocessors with directory-based cache coherence [3]. Our tree-based barrier will also perform well on these machines.
Reference: [9] <author> E. D. Brooks III. </author> <title> The butterfly barrier. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 15(4) </volume> <pages> 295-307, </pages> <year> 1986. </year>
Reference-contexts: Our algorithm uses no atomic instructions other than read and write, and performs the minimum possible number of operations across the processor-memory interconnect. 3.3 The Dissemination Barrier Brooks <ref> [9] </ref> has proposed a symmetric "butterfly barrier," in which processors participate as equals, performing the same operations at each step. Each processor in a butterfly barrier participates in a sequence of ~ log 2 P pairwise synchronizations.
Reference: [10] <author> E. D. Brooks III. </author> <title> The shared memory hypercube. </title> <journal> Parallel Computing, </journal> <volume> 6 </volume> <pages> 235-245, </pages> <year> 1988. </year> <month> 38 </month>
Reference-contexts: On a machine with coherent caches, processors spin only on locations in their caches. On a machine in which shared memory is distributed (e.g., the BBN Butterfly [8], the IBM RP3 [37], or a shared-memory hypercube <ref> [10] </ref>), processors spin only on locations in the local portion of shared memory. The implication of our work is that efficient synchronization algorithms can be constructed in software for shared-memory multiprocessors of arbitrary size.
Reference: [11] <author> H. Davis and J. Hennessy. </author> <title> Characterizing the synchronization behavior of parallel programs. </title> <booktitle> In Proc. of the ACM/SIGPLAN PPEALS 1988 Conference on Parallel Programming: Experience with Applications, Languages and Systems, </booktitle> <pages> pages 198-211, </pages> <address> New Haven, CT, </address> <month> July </month> <year> 1988. </year>
Reference-contexts: Un- fortunately, typical implementations of busy-waiting tend to produce large amounts of memory and interconnection network contention, which causes performance bottlenecks that become markedly more pronounced in larger machines and applications. As a consequence, the overhead of busy-wait synchronization is widely regarded as a serious performance problem <ref> [2, 6, 11, 14, 38, 49, 51] </ref>. When many processors busy-wait on a single synchronization variable, they create a hot spot that is the target of a disproportionate share of the network traffic.
Reference: [12] <author> E. Dijkstra. </author> <title> Solution of a problem in concurrent programming and control. </title> <journal> Communications of the ACM, </journal> <volume> 8(9):569, </volume> <month> Sept. </month> <year> 1965. </year>
Reference-contexts: Early algorithms assumed only the ability to read and write individual memory locations atomically. They tended to be subtle, and costly in time 1 and space, requiring both a large number of shared variables and a large number of operations to coordinate concurrent invocations of synchronization primitives <ref> [12, 25, 26, 36, 40] </ref>. Modern multiprocessors generally include more sophisticated atomic operations, permitting simpler and faster coordination strategies. Particularly common are various fetch and operations [22], which atomically read, modify, and write a memory location.
Reference: [13] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. L. Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proc. of the International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <address> Seattle, WA, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: We believe that this cost must be extremely low to make it worth the effort. Of course, increasing the performance of busy-wait locks and barriers is not the only possible rationale for implementing synchronization mechanisms in hardware. Recent work on weakly- consistent shared memory <ref> [1, 13, 28] </ref> has suggested the need for synchronization "fences" that provide clean points for memory semantics. Combining networks, likewise, may improve the performance of memory with bursty access patterns (caused, for example, by sharing after a barrier).
Reference: [14] <author> J. R. Goodman, M. K. Vernon, and P. J. Woest. </author> <title> Efficient synchronization primitives for largescale cache-coherent multiprocessors. </title> <booktitle> In Proc. of the 3rd International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 64-75, </pages> <address> Boston, MA, </address> <month> Apr. </month> <year> 1989. </year>
Reference-contexts: Un- fortunately, typical implementations of busy-waiting tend to produce large amounts of memory and interconnection network contention, which causes performance bottlenecks that become markedly more pronounced in larger machines and applications. As a consequence, the overhead of busy-wait synchronization is widely regarded as a serious performance problem <ref> [2, 6, 11, 14, 38, 49, 51] </ref>. When many processors busy-wait on a single synchronization variable, they create a hot spot that is the target of a disproportionate share of the network traffic. <p> there have been proposals for multistage interconnection networks that combine concurrent accesses to the same memory location [17, 37, 42], multistage networks that have special synchronization variables embedded in each stage of the network [21], and special-purpose cache hardware to maintain a queue of processors waiting for the same lock <ref> [14, 28, 35] </ref>. The principal purpose of these hardware primitives is to reduce the impact of busy waiting. Before adopting them, it is worth considering the extent to which software techniques can achieve a similar result. <p> The fourth advantage is in large part a consequence of the second, and is unique to the MCS lock. Our lock was inspired by the QOLB (Queue On Lock Bit) primitive proposed for the cache controllers of the Wisconsin Multicube <ref> [14] </ref>, but is implemented entirely in software. It requires an atomic fetch and store (swap) instruction, and benefits from the availability of compare - and swap. <p> they will come close enough to provide an extremely attractive alternative to complex, expensive hardware. 14 Other researchers have suggested building special-purpose hardware mechanisms solely for synchronization, including synchronization variables in the switching nodes of multistage interconnection networks [21] and lock queuing mechanisms in the cache controllers of cache-coherent multiprocessors <ref> [14, 28, 35] </ref>. Our results suggest that simple exploitation of a multi-level memory hierarchy, in software, may provide a more cost-effective means of avoiding lock-based contention. The algorithms we present in this paper require no hardware support for synchronization other than commonly-available atomic instructions. <p> Future designs for shared memory machines should include a full set of fetch and operations, including compare and swap. Our measurements on the Sequent Symmetry indicate that special-purpose synchronization mechanisms such as the QOLB instruction <ref> [14] </ref> are unlikely to outperform our MCS lock by more than 30%. A QOLB lock will have higher single-processor latency than a test and set lock [14, p.68], and its performance should be essentially the same as the MCS lock when competition for a lock is high. <p> Our measurements on the Sequent Symmetry indicate that special-purpose synchronization mechanisms such as the QOLB instruction [14] are unlikely to outperform our MCS lock by more than 30%. A QOLB lock will have higher single-processor latency than a test and set lock <ref> [14, p.68] </ref>, and its performance should be essentially the same as the MCS lock when competition for a lock is high. Goodman, Vernon, and Woest suggest that a QOLB-like mechanism can be implemented at very little incremental cost (given that they are already constructing large coherent caches with multi-dimensional snooping).
Reference: [15] <author> J. R. Goodman and P. J. Woest. </author> <title> The Wisconsin Multicube: A new large-scale cache coherent multiprocessor. </title> <booktitle> In Proc. of the International Symposium on Computer Architecture, </booktitle> <pages> pages 422-431, </pages> <address> Honolulu, HI, </address> <month> May </month> <year> 1988. </year>
Reference-contexts: Our tree-based barrier with wakeup flag should be the fastest algorithm on large-scale multiprocessors that use broadcast to maintain cache coherence (either in snoopy cache protocols <ref> [15] </ref> or in directory-based protocols with broadcast [7]). It requires only O (P ) updates to shared variables in order to tally arrivals, compared to O (P log P ) for the dissemination barrier.
Reference: [16] <author> A. Gottlieb. </author> <title> Scalability, Combining and the NYU Ultracomputer. </title> <institution> Ohio State University Parallel Computing Workshop, </institution> <month> Mar. </month> <year> 1990. </year> <note> Invited Lecture. </note>
Reference-contexts: Gottlieb <ref> [16] </ref> indicates that combining networks are difficult to bit-slice. 36 5 Summary of Recommendations We have presented a detailed comparison of new and existing algorithms for busy-wait synchronization on shared-memory multiprocessors, with a particular eye toward minimizing the network transactions that lead to contention.
Reference: [17] <author> A. Gottlieb, R. Grishman, C. P. Kruskal, K. P. McAuliffe, L. Rudolph, and M. Snir. </author> <title> The NYU Ultracomputer|Designing an MIMD shared memory parallel computer. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-32(2):175-189, </volume> <month> Feb. </month> <year> 1983. </year>
Reference-contexts: Fetch and operations include test and - set, fetch and store (swap), fetch and add, and compare and swap. 1 More recently, there have been proposals for multistage interconnection networks that combine concurrent accesses to the same memory location <ref> [17, 37, 42] </ref>, multistage networks that have special synchronization variables embedded in each stage of the network [21], and special-purpose cache hardware to maintain a queue of processors waiting for the same lock [14, 28, 35]. <p> Like hardware combining in a multistage interconnection network <ref> [17] </ref>, a software combining tree serves to collect multiple references to the same shared variable into a single reference whose effect is the same as the combined effect of the individual references. <p> The class of machines for which the dissemination barrier should outperform all other algorithms includes the BBN Butterfly [8], the IBM RP3 [37], Cedar [50], the BBN Monarch [42], the NYU Ultracomputer <ref> [17] </ref>, and proposed large-scale multiprocessors with directory-based cache coherence [3]. Our tree-based barrier will also perform well on these machines. <p> Dance hall machines include bus-based multiprocessors without coherent caches, and multistage network architectures such as Cedar [50], the BBN Monarch [42], and the NYU Ultracomputer <ref> [17] </ref>. Both Cedar and the Ultracomputer include processor-local memory, but only for private code and data. The Monarch provides a small amount of local memory as a "poor man's instruction cache." In none of these machines can local memory be modified remotely.
Reference: [18] <author> G. Graunke and S. Thakkar. </author> <title> Synchronization algorithms for shared-memory multiprocessors. </title> <journal> Computer, </journal> <volume> 23(6) </volume> <pages> 60-69, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Anderson [5] and Graunke and Thakkar <ref> [18] </ref> have proposed locking algorithms that achieve the constant bound on cache-coherent multiprocessors that support atomic fetch and increment or fetch and store, respectively. The trick is for each processor to use the atomic operation to obtain the address of a location on which to spin.
Reference: [19] <author> D. Hensgen, R. Finkel, and U. Manber. </author> <title> Two algorithms for barrier synchronization. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 17(1) </volume> <pages> 1-17, </pages> <year> 1988. </year>
Reference-contexts: We can reduce the number of references to the shared state variables, and simultaneously eliminate one of the two spinning episodes, by "reversing the sense" of the variables (and leaving them with different values) between consecutive barriers <ref> [19] </ref>. 5 The resulting code is shown in algorithm 7. Arriving processors decrement count and then wait until sense has a different value than it did in the previous barrier. The last arriving processor resets count and reverses sense. <p> If the number of processors is not a power of 2, then existing processors stand in for the missing ones, thereby participating in as many as 2dlog 2 P e pairwise synchronizations. Hensgen, Finkel, and Manber <ref> [19] </ref> describe a "dissemination barrier" that improves on Brooks's algorithm by employing a more efficient pattern of synchronizations and by reducing the cost of each synchronization. Their barrier takes its name from an algorithm developed to disseminate information among a set of processes. <p> This pattern does not require existing processes to stand in for missing ones, and therefore requires only dlog 2 P e synchronization operations on its critical path, regardless of P . Reference <ref> [19] </ref> contains a more detailed description of the synchronization pattern and a proof of its correctness. For each signalling operation of the dissemination barrier, Hensgen, Finkel, and Manber use alternating sets of variables in consecutive barrier episodes, avoiding interference without requiring two separate spins in each operation. <p> sense parity := 1 parity Algorithm 9: The scalable, distributed dissemination barrier with only local spinning. and without coherent caches, the shared allnodes array would be scattered statically across the memory banks of the machine, or replaced by a scattered set of variables. 3.4 Tournament Barriers Hensgen, Finkel, and Manber <ref> [19] </ref> and Lubachevsky [30] have also devised tree-style "tournament" barriers. The processors involved in a tournament barrier begin at the leaves of a binary tree, much as they would in a combining tree of fan-in two. <p> Our results are consistent with those of Hensgen, Finkel, and Manber <ref> [19] </ref>, who showed their tournament barrier to be faster than their dissemination barrier on the Sequent Balance multiprocessor. They did not compare their algorithms against a centralized barrier because the lack of an atomic increment instruction on the Balance precludes efficient atomic update of a counter.
Reference: [20] <author> M. Herlihy. </author> <title> A methodology for implementing highly concurrent data structures. </title> <booktitle> In Proceedings of the Second ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP), </booktitle> <pages> pages 197-206, </pages> <address> Seattle, WA, </address> <month> Mar. </month> <year> 1990. </year>
Reference-contexts: Anderson's lock benefits from fetch - and increment, and the ticket lock requires it. All of these instructions have uses other than the construction of busy-wait locks. Fetch and store and compare and swap, for example, are essential for manipulating pointers to build concurrent data structures <ref> [20, 32] </ref>. Because of their general utility, fetch and instructions are substantially more attractive than special-purpose synchronization primitives. Future designs for shared memory machines should include a full set of fetch and operations, including compare and swap.
Reference: [21] <author> D. N. Jayasimha. </author> <title> Distributed synchronizers. </title> <booktitle> In Proc. of the 1988 International Conference on Parallel Processing, </booktitle> <pages> pages 23-27, </pages> <address> St. Charles, IL, </address> <month> Aug. </month> <year> 1988. </year>
Reference-contexts: - set, fetch and store (swap), fetch and add, and compare and swap. 1 More recently, there have been proposals for multistage interconnection networks that combine concurrent accesses to the same memory location [17, 37, 42], multistage networks that have special synchronization variables embedded in each stage of the network <ref> [21] </ref>, and special-purpose cache hardware to maintain a queue of processors waiting for the same lock [14, 28, 35]. The principal purpose of these hardware primitives is to reduce the impact of busy waiting. <p> in this paper are unlikely to match the performance of hardware combining, they will come close enough to provide an extremely attractive alternative to complex, expensive hardware. 14 Other researchers have suggested building special-purpose hardware mechanisms solely for synchronization, including synchronization variables in the switching nodes of multistage interconnection networks <ref> [21] </ref> and lock queuing mechanisms in the cache controllers of cache-coherent multiprocessors [14, 28, 35]. Our results suggest that simple exploitation of a multi-level memory hierarchy, in software, may provide a more cost-effective means of avoiding lock-based contention.
Reference: [22] <author> C. P. Kruskal, L. Rudolph, and M. Snir. </author> <title> Efficient synchronization on multiprocessors with shared memory. </title> <booktitle> In Proc. of the 5th Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 218-228, </pages> <year> 1986. </year>
Reference-contexts: Modern multiprocessors generally include more sophisticated atomic operations, permitting simpler and faster coordination strategies. Particularly common are various fetch and operations <ref> [22] </ref>, which atomically read, modify, and write a memory location.
Reference: [23] <author> M. Kumar and G. F. Pfister. </author> <title> The onset of hot spot contention. </title> <booktitle> In Proc. of the 1986 International Conference on Parallel Processing, </booktitle> <pages> pages 28-34, </pages> <year> 1986. </year>
Reference-contexts: A study by Kumar and Pfister <ref> [23] </ref> shows the onset of hot-spot contention to be rapid. Pfister and Norton argue for hardware message combining in interconnection networks to reduce the impact of hot spots.
Reference: [24] <author> L. Lamport. </author> <title> A new solution of Dijkstra's concurrent programming problem. </title> <journal> Communications of the ACM, </journal> <volume> 17(8) </volume> <pages> 453-455, </pages> <month> Aug. </month> <year> 1974. </year>
Reference-contexts: It releases the lock by incrementing the release counter. In the terminology of Reed and Kanodia [41], a ticket lock corresponds to the busy-wait implementation of a semaphore using an eventcount and a sequencer. It can also be thought of as an optimization of Lamport's bakery lock <ref> [24] </ref>, which was designed for fault-tolerance rather than performance. Instead of spinning on the release counter, processors using a bakery lock repeatedly examine the tickets of their peers.
Reference: [25] <author> L. Lamport. </author> <title> The mutual exclusion problem: Part I|A theory of interprocess communication; Part II|Statement and solutions. </title> <journal> Journal of the ACM, </journal> <volume> 33(2) </volume> <pages> 313-348, </pages> <month> Apr. </month> <year> 1986. </year> <month> 39 </month>
Reference-contexts: Early algorithms assumed only the ability to read and write individual memory locations atomically. They tended to be subtle, and costly in time 1 and space, requiring both a large number of shared variables and a large number of operations to coordinate concurrent invocations of synchronization primitives <ref> [12, 25, 26, 36, 40] </ref>. Modern multiprocessors generally include more sophisticated atomic operations, permitting simpler and faster coordination strategies. Particularly common are various fetch and operations [22], which atomically read, modify, and write a memory location.
Reference: [26] <author> L. Lamport. </author> <title> A fast mutual exclusion algorithm. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 5(1) </volume> <pages> 1-11, </pages> <month> Feb. </month> <year> 1987. </year>
Reference-contexts: Early algorithms assumed only the ability to read and write individual memory locations atomically. They tended to be subtle, and costly in time 1 and space, requiring both a large number of shared variables and a large number of operations to coordinate concurrent invocations of synchronization primitives <ref> [12, 25, 26, 36, 40] </ref>. Modern multiprocessors generally include more sophisticated atomic operations, permitting simpler and faster coordination strategies. Particularly common are various fetch and operations [22], which atomically read, modify, and write a memory location.
Reference: [27] <author> C. A. Lee. </author> <title> Barrier synchronization over multistage interconnection networks. </title> <booktitle> In Proc. of the Second IEEE Symposium on Parallel and Distributed Processing, </booktitle> <pages> pages 130-133, </pages> <address> Dallas, TX, </address> <month> Dec. </month> <year> 1990. </year>
Reference-contexts: In addition to employing a wakeup tree, we have modified Hensgen, Finkel, and Manber's algorithm to use sense reversal to avoid re-initializing flag variables in each round. These same modifications have been discovered independently by Craig Lee of Aerospace Corporation <ref> [27] </ref>. Hensgen, Finkel, and Manber provide performance figures for the Sequent Balance (a bus-based, cache-coherent multiprocessor), comparing their tournament algorithm against the dissemination barrier, as well as Brooks's butterfly barrier. They report that the tournament barrier outperforms the dissemination barrier when P &gt; 16.
Reference: [28] <author> J. Lee and U. Ramachandran. </author> <title> Synchronization with multiprocessor caches. </title> <booktitle> In Proc. of the International Symposium on Computer Architecture, </booktitle> <address> Seattle, WA, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: there have been proposals for multistage interconnection networks that combine concurrent accesses to the same memory location [17, 37, 42], multistage networks that have special synchronization variables embedded in each stage of the network [21], and special-purpose cache hardware to maintain a queue of processors waiting for the same lock <ref> [14, 28, 35] </ref>. The principal purpose of these hardware primitives is to reduce the impact of busy waiting. Before adopting them, it is worth considering the extent to which software techniques can achieve a similar result. <p> they will come close enough to provide an extremely attractive alternative to complex, expensive hardware. 14 Other researchers have suggested building special-purpose hardware mechanisms solely for synchronization, including synchronization variables in the switching nodes of multistage interconnection networks [21] and lock queuing mechanisms in the cache controllers of cache-coherent multiprocessors <ref> [14, 28, 35] </ref>. Our results suggest that simple exploitation of a multi-level memory hierarchy, in software, may provide a more cost-effective means of avoiding lock-based contention. The algorithms we present in this paper require no hardware support for synchronization other than commonly-available atomic instructions. <p> We believe that this cost must be extremely low to make it worth the effort. Of course, increasing the performance of busy-wait locks and barriers is not the only possible rationale for implementing synchronization mechanisms in hardware. Recent work on weakly- consistent shared memory <ref> [1, 13, 28] </ref> has suggested the need for synchronization "fences" that provide clean points for memory semantics. Combining networks, likewise, may improve the performance of memory with bursty access patterns (caused, for example, by sharing after a barrier).
Reference: [29] <author> B. Lubachevsky. </author> <title> An approach to automating the verification of compact parallel coordination programs. I. </title> <journal> Acta Informatica, </journal> <pages> pages 125-169, </pages> <year> 1984. </year>
Reference-contexts: The last arriving processor resets count and reverses sense. Consecutive barriers cannot interfere with each other because all operations on count occur before sense is toggled to release the waiting processors. Lubachevsky <ref> [29] </ref> presents a similar barrier algorithm that uses two shared counters and a processor private two-state flag. The private flag selects which counter to use; consecutive barriers use alternate counters. Another similar barrier can be found in library packages distributed by Sequent Corporation for the Symmetry multiprocessor.
Reference: [30] <author> B. Lubachevsky. </author> <title> Synchronization barrier and related tools for shared memory parallel pro-gramming. </title> <booktitle> In Proc. of the 1989 International Conference on Parallel Processing, </booktitle> <pages> pages II175-II-179, </pages> <month> Aug. </month> <year> 1989. </year>
Reference-contexts: 1 parity Algorithm 9: The scalable, distributed dissemination barrier with only local spinning. and without coherent caches, the shared allnodes array would be scattered statically across the memory banks of the machine, or replaced by a scattered set of variables. 3.4 Tournament Barriers Hensgen, Finkel, and Manber [19] and Lubachevsky <ref> [30] </ref> have also devised tree-style "tournament" barriers. The processors involved in a tournament barrier begin at the leaves of a binary tree, much as they would in a combining tree of fan-in two. One processor from each node continues up the tree to the next "round" of the tournament. <p> Processor j participates in the next round of the tournament. A complete tournament consists of dlog 2 P e rounds. Processor 0 sets a global flag when the tournament is over. Lubachevsky <ref> [30] </ref> presents a CREW (concurrent read, exclusive write) tournament barrier that uses a global flag for wakeup, similar to that of Hensgen, Finkel, and Manber.
Reference: [31] <author> M. Maekawa. </author> <title> A p N algorithm for mutual exclusion in decentralized systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 3(2) </volume> <pages> 145-159, </pages> <month> May </month> <year> 1985. </year>
Reference-contexts: As noted above, a substantial body of work has also addressed mutual exclusion using more primitive read and write atomicity; the complexity of the resulting solutions is the principal motivation for the development of fetch and primitives. Other researchers have considered mutual exclusion in the context of distributed systems <ref> [31, 39, 43, 45, 46] </ref>, but the characteristics of message passing are different enough from shared memory operations that solutions do not transfer from one environment to the other. Our pseudo-code notation is meant to be more-or-less self explanatory.
Reference: [32] <author> J. M. Mellor-Crummey. </author> <title> Concurrent queues: Practical fetch-and- algorithms. </title> <type> Technical Report 229, </type> <institution> Computer Science Department, University of Rochester, </institution> <month> Nov. </month> <year> 1987. </year>
Reference-contexts: Anderson's lock benefits from fetch - and increment, and the ticket lock requires it. All of these instructions have uses other than the construction of busy-wait locks. Fetch and store and compare and swap, for example, are essential for manipulating pointers to build concurrent data structures <ref> [20, 32] </ref>. Because of their general utility, fetch and instructions are substantially more attractive than special-purpose synchronization primitives. Future designs for shared memory machines should include a full set of fetch and operations, including compare and swap.
Reference: [33] <author> J. M. Mellor-Crummey and M. L. Scott. </author> <title> Algorithms for scalable synchronization on sharedmemory multiprocessors. </title> <type> Technical Report 342, </type> <institution> Computer Science Department, University of Rochester, </institution> <month> Apr. </month> <year> 1990. </year> <note> Also COMP TR90-114, </note> <institution> Department of Computer Science, Rice University, </institution> <month> May </month> <year> 1990. </year>
Reference-contexts: The other two actions are reduced to the functionality of fetch and store and compare and swap. The details of this argument are straightforward but lengthy (they appear as an appendix to the technical report version of this paper <ref> [33] </ref>); we find the informal description and pictures above more intuitively convincing. 3 Barriers Barriers have received a great deal of attention in the literature, more so even than spin locks, and the many published algorithms differ significantly in notational conventions and architectural assumptions.
Reference: [34] <author> J. K. Ousterhout, D. A. Scelza, and P. S. Sindhu. </author> <title> Medusa: An experiment in distributed operating system structure. </title> <journal> Communications of the ACM, </journal> <volume> 23(2), </volume> <month> Feb. </month> <year> 1980. </year>
Reference-contexts: This guarantee of fairness is likely to be considered an advantage in many environments, but is likely to waste CPU resources if spinning processes may be preempted. (Busy-wait barriers may also waste cycles in the presence of preemption.) We can avoid this problem by co-scheduling processes that share locks <ref> [34] </ref>. Alternatively (for mutual exclusion), a test and set lock with exponential backoff will allow latecomers to acquire the lock when processes that arrived earlier are not running. In this situation the test and set lock may be preferred to the FIFO alternatives.
Reference: [35] <institution> P1596 Working Group of the IEEE Computer Society Microprocessor Standards Committee. SCI (scalable coherent interface): </institution> <note> An overview of extended cache-coherence protocols, Feb. 5, 1990. Draft 0.59 P1596/Part III-D. </note>
Reference-contexts: there have been proposals for multistage interconnection networks that combine concurrent accesses to the same memory location [17, 37, 42], multistage networks that have special synchronization variables embedded in each stage of the network [21], and special-purpose cache hardware to maintain a queue of processors waiting for the same lock <ref> [14, 28, 35] </ref>. The principal purpose of these hardware primitives is to reduce the impact of busy waiting. Before adopting them, it is worth considering the extent to which software techniques can achieve a similar result. <p> they will come close enough to provide an extremely attractive alternative to complex, expensive hardware. 14 Other researchers have suggested building special-purpose hardware mechanisms solely for synchronization, including synchronization variables in the switching nodes of multistage interconnection networks [21] and lock queuing mechanisms in the cache controllers of cache-coherent multiprocessors <ref> [14, 28, 35] </ref>. Our results suggest that simple exploitation of a multi-level memory hierarchy, in software, may provide a more cost-effective means of avoiding lock-based contention. The algorithms we present in this paper require no hardware support for synchronization other than commonly-available atomic instructions.
Reference: [36] <author> G. L. Peterson. </author> <title> A new solution to Lamport's concurrent programming problem using small shared variables. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 5(1) </volume> <pages> 56-65, </pages> <month> Jan. </month> <year> 1983. </year>
Reference-contexts: Early algorithms assumed only the ability to read and write individual memory locations atomically. They tended to be subtle, and costly in time 1 and space, requiring both a large number of shared variables and a large number of operations to coordinate concurrent invocations of synchronization primitives <ref> [12, 25, 26, 36, 40] </ref>. Modern multiprocessors generally include more sophisticated atomic operations, permitting simpler and faster coordination strategies. Particularly common are various fetch and operations [22], which atomically read, modify, and write a memory location.
Reference: [37] <author> G. Pfister et al. </author> <title> The IBM research parallel processor prototype (RP3): Introduction and architecture. </title> <booktitle> In Proc. of the 1985 International Conference on Parallel Processing, </booktitle> <pages> pages 764-771, </pages> <address> St. Charles, Illinois, </address> <month> Aug. </month> <year> 1985. </year>
Reference-contexts: Fetch and operations include test and - set, fetch and store (swap), fetch and add, and compare and swap. 1 More recently, there have been proposals for multistage interconnection networks that combine concurrent accesses to the same memory location <ref> [17, 37, 42] </ref>, multistage networks that have special synchronization variables embedded in each stage of the network [21], and special-purpose cache hardware to maintain a queue of processors waiting for the same lock [14, 28, 35]. <p> On a machine with coherent caches, processors spin only on locations in their caches. On a machine in which shared memory is distributed (e.g., the BBN Butterfly [8], the IBM RP3 <ref> [37] </ref>, or a shared-memory hypercube [10]), processors spin only on locations in the local portion of shared memory. The implication of our work is that efficient synchronization algorithms can be constructed in software for shared-memory multiprocessors of arbitrary size. <p> It has a shorter critical path than the tree and tournament barriers (by a constant factor), and is therefore faster. The class of machines for which the dissemination barrier should outperform all other algorithms includes the BBN Butterfly [8], the IBM RP3 <ref> [37] </ref>, Cedar [50], the BBN Monarch [42], the NYU Ultracomputer [17], and proposed large-scale multiprocessors with directory-based cache coherence [3]. Our tree-based barrier will also perform well on these machines.
Reference: [38] <author> G. F. Pfister and V. A. Norton. </author> <title> "Hot spot" contention and combining in multistage intercon-nection networks. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-34(10):943-948, </volume> <month> Oct. </month> <year> 1985. </year>
Reference-contexts: Un- fortunately, typical implementations of busy-waiting tend to produce large amounts of memory and interconnection network contention, which causes performance bottlenecks that become markedly more pronounced in larger machines and applications. As a consequence, the overhead of busy-wait synchronization is widely regarded as a serious performance problem <ref> [2, 6, 11, 14, 38, 49, 51] </ref>. When many processors busy-wait on a single synchronization variable, they create a hot spot that is the target of a disproportionate share of the network traffic. <p> When many processors busy-wait on a single synchronization variable, they create a hot spot that is the target of a disproportionate share of the network traffic. Pfister and Norton <ref> [38] </ref> showed that the presence of hot spots can severely degrade performance for all traffic in multistage interconnection networks, not just traffic due to synchronizing processors. As part of a larger study, Agarwal and Cherian [2] investigated the impact of synchronization on overall program performance. <p> Simulations by Yew, Tzeng, and Lawrie [51] show that a software combining tree can significantly decrease memory contention and prevent tree saturation (a form of network congestion that delays the response of the network to large numbers of references <ref> [38] </ref>) in multistage interconnection networks by distributing accesses across the memory modules of the machine. The principal shortcoming of the combining tree barrier, from our point of view, is that is requires processors to spin on memory locations that cannot be statically determined, and on which other processors also spin. <p> Table 2 shows that when processors are able to spin on shared locations locally, average network latency increases only slightly. With only network access to shared memory, latency more than doubles. Studies by Pfister and Norton <ref> [38] </ref> show that hot-spot contention can lead to tree saturation in multistage interconnection networks with blocking switch nodes and distributed routing control, 34 0 10 20 30 40 50 60 70 80 250 750 1250 1750 2250 Processors Time (s) 4 4 4 4 4 4 4 4 4 4 4
Reference: [39] <author> K. Raymond. </author> <title> A tree-based algorithm for distributed mutual exclusion. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(1) </volume> <pages> 61-77, </pages> <month> Feb. </month> <year> 1989. </year>
Reference-contexts: As noted above, a substantial body of work has also addressed mutual exclusion using more primitive read and write atomicity; the complexity of the resulting solutions is the principal motivation for the development of fetch and primitives. Other researchers have considered mutual exclusion in the context of distributed systems <ref> [31, 39, 43, 45, 46] </ref>, but the characteristics of message passing are different enough from shared memory operations that solutions do not transfer from one environment to the other. Our pseudo-code notation is meant to be more-or-less self explanatory.
Reference: [40] <author> M. Raynal. </author> <title> Algorithms for Mutual Exclusion. </title> <publisher> MIT Press Series in Scientific Computation. MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year> <title> Translated from the French by D. </title> <publisher> Beeson. </publisher>
Reference-contexts: Early algorithms assumed only the ability to read and write individual memory locations atomically. They tended to be subtle, and costly in time 1 and space, requiring both a large number of shared variables and a large number of operations to coordinate concurrent invocations of synchronization primitives <ref> [12, 25, 26, 36, 40] </ref>. Modern multiprocessors generally include more sophisticated atomic operations, permitting simpler and faster coordination strategies. Particularly common are various fetch and operations [22], which atomically read, modify, and write a memory location.
Reference: [41] <author> D. P. Reed and R. K. Kanodia. </author> <title> Synchronization with eventcounts and sequencers. </title> <journal> Communications of the ACM, </journal> <volume> 22(2) </volume> <pages> 115-123, </pages> <month> Feb. </month> <year> 1979. </year> <month> 40 </month>
Reference-contexts: A processor acquires the lock by performing a fetch and increment operation on the request counter and waiting until the result (its ticket) is equal to the value of the release counter. It releases the lock by incrementing the release counter. In the terminology of Reed and Kanodia <ref> [41] </ref>, a ticket lock corresponds to the busy-wait implementation of a semaphore using an eventcount and a sequencer. It can also be thought of as an optimization of Lamport's bakery lock [24], which was designed for fault-tolerance rather than performance.
Reference: [42] <author> R. D. Rettberg, W. R. Crowther, P. P. Carvey, and R. S. Tomlinson. </author> <title> The Monarch parallel processor hardware design. </title> <journal> Computer, </journal> <volume> 23(4) </volume> <pages> 18-30, </pages> <month> Apr. </month> <year> 1990. </year>
Reference-contexts: Fetch and operations include test and - set, fetch and store (swap), fetch and add, and compare and swap. 1 More recently, there have been proposals for multistage interconnection networks that combine concurrent accesses to the same memory location <ref> [17, 37, 42] </ref>, multistage networks that have special synchronization variables embedded in each stage of the network [21], and special-purpose cache hardware to maintain a queue of processors waiting for the same lock [14, 28, 35]. <p> It has a shorter critical path than the tree and tournament barriers (by a constant factor), and is therefore faster. The class of machines for which the dissemination barrier should outperform all other algorithms includes the BBN Butterfly [8], the IBM RP3 [37], Cedar [50], the BBN Monarch <ref> [42] </ref>, the NYU Ultracomputer [17], and proposed large-scale multiprocessors with directory-based cache coherence [3]. Our tree-based barrier will also perform well on these machines. <p> Dance hall machines include bus-based multiprocessors without coherent caches, and multistage network architectures such as Cedar [50], the BBN Monarch <ref> [42] </ref>, and the NYU Ultracomputer [17]. Both Cedar and the Ultracomputer include processor-local memory, but only for private code and data. The Monarch provides a small amount of local memory as a "poor man's instruction cache." In none of these machines can local memory be modified remotely.
Reference: [43] <author> G. Ricart and A. K. Agrawala. </author> <title> An optimal algorithm for mutual exclusion in computer networks. </title> <journal> Communications of the ACM, </journal> <volume> 24(1) </volume> <pages> 9-17, </pages> <month> Jan. </month> <year> 1981. </year>
Reference-contexts: As noted above, a substantial body of work has also addressed mutual exclusion using more primitive read and write atomicity; the complexity of the resulting solutions is the principal motivation for the development of fetch and primitives. Other researchers have considered mutual exclusion in the context of distributed systems <ref> [31, 39, 43, 45, 46] </ref>, but the characteristics of message passing are different enough from shared memory operations that solutions do not transfer from one environment to the other. Our pseudo-code notation is meant to be more-or-less self explanatory.
Reference: [44] <author> L. Rudolph and Z. Segall. </author> <title> Dynamic decentralized cache schemes for MIMD parallel processors. </title> <booktitle> In Proc. of the International Symposium on Computer Architecture, </booktitle> <pages> pages 340-347, </pages> <year> 1984. </year>
Reference-contexts: To reduce this overhead, the test and set lock can be modified to use a test and set instruction only when a previous read indicates that the test and set might succeed. This so-called test-and-test and - set technique <ref> [44] </ref> ensures that waiting processors poll with read requests during the time that a lock is held.
Reference: [45] <author> B. A. Sanders. </author> <title> The information structure of distributed mutual exclusion algorithms. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 5(3) </volume> <pages> 284-299, </pages> <month> Aug. </month> <year> 1987. </year>
Reference-contexts: As noted above, a substantial body of work has also addressed mutual exclusion using more primitive read and write atomicity; the complexity of the resulting solutions is the principal motivation for the development of fetch and primitives. Other researchers have considered mutual exclusion in the context of distributed systems <ref> [31, 39, 43, 45, 46] </ref>, but the characteristics of message passing are different enough from shared memory operations that solutions do not transfer from one environment to the other. Our pseudo-code notation is meant to be more-or-less self explanatory.
Reference: [46] <author> F. B. Schneider. </author> <title> Synchronization in distributed programs. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 4(2) </volume> <pages> 179-195, </pages> <month> Apr. </month> <year> 1982. </year>
Reference-contexts: As noted above, a substantial body of work has also addressed mutual exclusion using more primitive read and write atomicity; the complexity of the resulting solutions is the principal motivation for the development of fetch and primitives. Other researchers have considered mutual exclusion in the context of distributed systems <ref> [31, 39, 43, 45, 46] </ref>, but the characteristics of message passing are different enough from shared memory operations that solutions do not transfer from one environment to the other. Our pseudo-code notation is meant to be more-or-less self explanatory.
Reference: [47] <author> M. L. Scott, T. J. LeBlanc, and B. D. Marsh. </author> <booktitle> Multi-model parallel programming in Psyche. In Proceedings of the Second ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP), </booktitle> <pages> pages 70-78, </pages> <address> Seattle, WA, </address> <month> Mar. </month> <year> 1990. </year>
Reference-contexts: In this situation the test and set lock may be preferred to the FIFO alternatives. Additional mechanisms can ensure that a process is not preempted while actually holding a lock <ref> [47, 52] </ref>. All of the spin lock algorithms we have considered require some sort of fetch and instructions. The test and set lock of course requires test and set. The ticket lock requires fetch and - increment.
Reference: [48] <author> P. Tang and P.-C. Yew. </author> <title> Processor self-scheduling for multiple-nested parallel loops. </title> <booktitle> In Proc. of the 1986 International Conference on Parallel Processing, </booktitle> <pages> pages 528-535, </pages> <address> St. Charles, IL, </address> <month> Aug. </month> <year> 1986. </year>
Reference-contexts: Without the first spin, it is possible for a processor to mistakenly pass through the current barrier because of state information being used by processors still leaving the previous barrier. Two barrier algorithms proposed by Tang and Yew (the first algorithm appears in <ref> [48, Algorithm 3.1] </ref> and [49, p. 3]; the second algorithm appears in [49, Algorithm 3.1]) suffer from this type of flaw. <p> These observations imply that centralized barrier algorithms will not scale well to large numbers of processors, even using adaptive backoff strategies. Our experiments (see section 4.4) confirm this conclusion. 6 Commenting on Tang and Yew's barrier algorithm (algorithm 3.1 in <ref> [48] </ref>), Agarwal and Cherian [2] show that on a machine in which contention causes memory accesses to be aborted and retried, the expected number of memory accesses initiated by each processor to achieve a single barrier is linear in the number of processors participating, even if processors arrive at the barrier
Reference: [49] <author> P. Tang and P.-C. Yew. </author> <title> Algorithms for distributing hot-spot addressing. </title> <type> CSRD report 617, </type> <institution> Center for Supercomputing Research and Development, University of Illinois UrbanaChampaign, </institution> <month> Jan. </month> <year> 1987. </year>
Reference-contexts: Un- fortunately, typical implementations of busy-waiting tend to produce large amounts of memory and interconnection network contention, which causes performance bottlenecks that become markedly more pronounced in larger machines and applications. As a consequence, the overhead of busy-wait synchronization is widely regarded as a serious performance problem <ref> [2, 6, 11, 14, 38, 49, 51] </ref>. When many processors busy-wait on a single synchronization variable, they create a hot spot that is the target of a disproportionate share of the network traffic. <p> Without the first spin, it is possible for a processor to mistakenly pass through the current barrier because of state information being used by processors still leaving the previous barrier. Two barrier algorithms proposed by Tang and Yew (the first algorithm appears in [48, Algorithm 3.1] and <ref> [49, p. 3] </ref>; the second algorithm appears in [49, Algorithm 3.1]) suffer from this type of flaw. <p> Two barrier algorithms proposed by Tang and Yew (the first algorithm appears in [48, Algorithm 3.1] and [49, p. 3]; the second algorithm appears in <ref> [49, Algorithm 3.1] </ref>) suffer from this type of flaw. <p> At every level of the tree, atomic instructions are used to combine the arguments to write operations or to split the results of read operations. In the context of this general framework, Tang and Yew <ref> [49] </ref> describe how software combining trees can be used to implement a barrier. Writes into one tree are used to determine that all processors have reached the barrier; reads out of a second are used to allow them to continue.
Reference: [50] <author> P.-C. Yew. </author> <title> Architecture of the Cedar parallel supercomputer. </title> <type> CSRD report 609, </type> <institution> Center for Supercomputing Research and Development, University of Illinois Urbana-Champaign, </institution> <month> Aug. </month> <year> 1986. </year>
Reference-contexts: It has a shorter critical path than the tree and tournament barriers (by a constant factor), and is therefore faster. The class of machines for which the dissemination barrier should outperform all other algorithms includes the BBN Butterfly [8], the IBM RP3 [37], Cedar <ref> [50] </ref>, the BBN Monarch [42], the NYU Ultracomputer [17], and proposed large-scale multiprocessors with directory-based cache coherence [3]. Our tree-based barrier will also perform well on these machines. <p> Unfortunately, on dance hall machines the load will still consume interconnect bandwidth, degrading the performance not only of synchronization operations but also of all other activity on the machine, severely constraining scalability. Dance hall machines include bus-based multiprocessors without coherent caches, and multistage network architectures such as Cedar <ref> [50] </ref>, the BBN Monarch [42], and the NYU Ultracomputer [17]. Both Cedar and the Ultracomputer include processor-local memory, but only for private code and data.
Reference: [51] <author> P.-C. Yew, N.-F. Tzeng, and D. H. Lawrie. </author> <title> Distributing hot-spot addressing in large-scale multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(4):388-395, </volume> <month> Apr. </month> <year> 1987. </year>
Reference-contexts: Un- fortunately, typical implementations of busy-waiting tend to produce large amounts of memory and interconnection network contention, which causes performance bottlenecks that become markedly more pronounced in larger machines and applications. As a consequence, the overhead of busy-wait synchronization is widely regarded as a serious performance problem <ref> [2, 6, 11, 14, 38, 49, 51] </ref>. When many processors busy-wait on a single synchronization variable, they create a hot spot that is the target of a disproportionate share of the network traffic. <p> combining_barrier_aux (parent) count := k // prepare for next barrier locksense := not locksense // release waiting processors repeat until locksense = sense Algorithm 8: A software combining tree barrier with optimized wakeup. 3.2 The Software Combining Tree Barrier To reduce hot-spot contention for synchronization variables, Yew, Tzeng, and Lawrie <ref> [51] </ref> have devised a data structure known as a software combining tree. <p> The processor that reaches the root of the tree begins a reverse wave of updates to locksense flags. As soon as it awakes, each processor retraces its path through the tree unblocking its siblings at each node along the path. Simulations by Yew, Tzeng, and Lawrie <ref> [51] </ref> show that a software combining tree can significantly decrease memory contention and prevent tree saturation (a form of network congestion that delays the response of the network to large numbers of references [38]) in multistage interconnection networks by distributing accesses across the memory modules of the machine.
Reference: [52] <author> J. Zahorjan, E. D. Lazowska, and D. L. Eager. </author> <title> The effect of scheduling discipline on spin overhead in shared memory parallel processors. </title> <type> Technical Report TR-89-07-03, </type> <institution> Computer Science Department, University of Washington, </institution> <month> July </month> <year> 1989. </year> <month> 41 </month>
Reference-contexts: In this situation the test and set lock may be preferred to the FIFO alternatives. Additional mechanisms can ensure that a process is not preempted while actually holding a lock <ref> [47, 52] </ref>. All of the spin lock algorithms we have considered require some sort of fetch and instructions. The test and set lock of course requires test and set. The ticket lock requires fetch and - increment.
References-found: 52

