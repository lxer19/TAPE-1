URL: http://ftp.eecs.umich.edu/techreports/cse/1992/cse-tr-147-92.ps.Z
Refering-URL: http://ftp.eecs.umich.edu/techreports/cse/1992/
Root-URL: http://www.eecs.umich.edu
Title: Monster: A Tool for Analyzing the Interaction between Operating Systems and Computer Architectures Monster: A
Author: David Nagle Richard Uhlig Trevor Mudge 
Keyword: Monster  
Date: 6, 1992  Abstract  
Note: May  
Abstract: University of Michigan Tech Report To enable computer designers to better evaluate the architectural needs of operating sys tems, we have developed , a tool which combines hardware and software monitoring techniques to unobtrusively obtain system performance data. This report is split into two major parts. In Part I, we argue the need for OS performance evaluation tools, summarize previous hardware and software based monitoring techniques, discuss our design of Monster and finally present an analysis of compilation workloads which test and demonstrate Monster s capabilities. In Part II, we detail our plans for future studies in which Monster plays a central role. 
Abstract-found: 1
Intro-found: 1
Reference: [Agarwal 89] <author> Anant Agarwal. </author> <title> Analysis of Cache Performance for Operat ing Systems and Multipr ogramming. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1989. </year>
Reference: [Anderson et al. 91] <author> Thomas E. Anderson, Henry M. Levy, Brian N. Bershad and Edward D. Lazowska. </author> <title> The interaction of architecture and operating system design. </title> <booktitle> In Fourth International Confer - ence on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 108-120, </pages> <year> 1991. </year>
Reference-contexts: 1 Introduction Monster was developed because of our interest in architectural support for operating systems. We would like to study the needs of applications which rely heavily on operating system services and then determine what hardware features best support these needs. Our work is motivated primarily by <ref> [Anderson et al. 91] </ref> and [Ousterhout 90] who ar gue that operating systems and computer architectures are evolving in somewhat incompatible directions. To get started, we will restrict the scope of our studies to improving application perfor - mance with respect to a machine's instruction and data CPU caches.
Reference: [DEC 92] <author> Digital Corporation. </author> <title> Digital 21064-AA Product Brief. Pre liminary Release, </title> <year> 1992. </year>
Reference: [Emer & Clark 84] <author> Joel S. Emer and Douglas W. Clark. </author> <title> A characterization of processor performance in the VAX-11/780. </title> <booktitle> In 11th Annual Symposium on Computer Architecture , pages 301-309, </booktitle> <year> 1984. </year>
Reference: [Fitzgerald & Rashid 86] <author> R. Fitzgerald and R. F . Rashid. </author> <title> The integration of virtual memory management and interprocess communication in Accent. </title> <journal> ACM Transactions on Computer systems 177, </journal> <month> May </month> <year> 1986. </year>
Reference: [Groves and Oehler 91] <author> Randy D. </author> <title> Groves and Richard Oehler . RISC System/6000 processor architecture. </title> <institution> In IBM RISC System/6000 T echnol ogy , pages 16-23, </institution> <year> 1991. </year>
Reference: [Golub et al. 90] <author> D. Golub, R. Dean, A. Forin, and R. Rashid. </author> <title> Unix as an application program. </title> <booktitle> In Pr oceedings of the Summer 1990 USENIX Conference , pages 87-95, </booktitle> <year> 1990. </year>
Reference: [Hill & Smith 89] <author> M. Hill and A. Smith. </author> <title> Evaluating associativity in CPU caches. </title> <journal> IEEE T ransactions on Computers 1630, </journal> <year> 1989. </year>
Reference-contexts: Because of the speed at which decisions must be made, the policies tend to be very simple. For example, it has been suggested that a direct-mapped scheme is often the most effective placement policy for a CPU cache <ref> [Hill & Smith 89] </ref>. But many researchers have observed that even higher performance can be obtained by combining the various soft ware based techniques (such as those discussed in the introduction) with a simple hardware implemented cache placement policy.
Reference: [Hwu & Chang 89] <author> W. Hwu and P . Chang. </author> <title> Achieving high instruction cache performance with an optimizing compiler . In 16th Annual Symposium on Computer Architecture , pages 242-251, </title> <year> 1989. </year>
Reference-contexts: In [Taylor et al. 90], a heuristic called page-coloring is used to assign page frames so that physical pages tend to be distributed in the same way that virtual pages are. Page coloring is actually one of the heuristics that Kessler uses. In <ref> [Hwu & Chang 89] </ref>, dynamic execution profi les are used by an optimizing compiler to place instructions in a way that minimizes con icts in the cache. <p> A call graph is a directed graph where every node is a function (procedure) and 1. Borrowed from <ref> [Hwu & Chang 89] </ref> Monster: A Tool for Analyzing the Interaction between Operating Systems and Computer Architectures every arc is a function call. <p> Experiment Class 2: Studies of careful page placement in isolation will implement the placement algorithms proposed by <ref> [Hwu & Chang 89] </ref> and [Lam et al. 91] and then use Monster to measure changes in cache hit rates. Experiment Class 3: Studies on integrating car eful page mapping and placement .
Reference: [Hennessy & Jouppi 91] <author> John L. Hennessy and Norman P. Jouppi. </author> <title> Computer technol ogy and architecture: an evolving interaction. </title> <booktitle> Computer 24(9); 18-29, </booktitle> <month> September, </month> <year> 1991. </year>
Reference-contexts: In a next generation computer system, it is possible that a memory access which misses the cache could result in a penalty of hundreds of CPU cycles to fetch the data from main memory [Olukotun et al. 91] <ref> [Hennessy & Jouppi 91] </ref>. This trend underscores the importance of fi nding new techniques to more effectively use CPU caches. 2.1 Previous Work The policies that dictate where data is placed in a CPU cache are usually implemented in hardware.
Reference: [Kessler & Hill 90] <author> R. Kessler and M. Hill. </author> <title> Miss reduction in large, real-indexed caches. University of Wisconsin Tech Report Monster: A Tool for Analyzing the Interaction between Operating Systems and Computer Architectures </title>
Reference-contexts: The conict misses caused by multiple processes competing for room in a cache has been studied by [Stone & Thiebaut 86] and [Mogul & Borg 91] using mathematical analysis and tracedriven simulation. In <ref> [Kessler & Hill 90] </ref>, tracedriven cache simulations are used to investigate dif ferent page mapping policies designed to minimize con ict misses from overlapping pages in the CPU cache. This work corresponds to technique (a) (careful mapping) as described in the introduction. <p> The compiler can avoid this prob lem by using the simple heuristic of placing the two functions on the same page, as shown A Poor Mapping A Good Mapping A Cache Bin Page Frames (Borrowed from <ref> [Kessler & Hill 90] </ref>) Monster: A Tool for Analyzing the Interaction between Operating Systems and Computer Architectures in placement (b). This ensures that they wont conict in the cache as shown in placement (e). As a second example, consider a compiler which knows about the D-cache. <p> This section specifies the experiments we plan to perform and the necessary modifications to our tools. 4.2.1 Experiments Here are the different classes of experiments we would like to perform: Experiment Class 1: Studies of car eful page mapping in isolation will implement the different mapping policies proposed by <ref> [Kessler & Hill 90] </ref> and then use Monster to measure changes in cache hit rates.
Reference: [Lam et al. 91] <author> Monica S. Lam, Edward E. Rothber g and Michael E. Wolf. </author> <title> The cache performance and optimizations of blocked algo rithms. </title> <booktitle> In Fourth International Conference on Architectural Support for Pr ogramming Languages and Operating Sys tems, </booktitle> <pages> pages 63-74, </pages> <year> 1991. </year>
Reference-contexts: Experiments based on tracedriven simulation show that miss rates for carefully placed code in a direct mapped instruction cache (I-cache) are consistently lower than unoptimized code in larger I-caches and I-caches with greater degrees of associativity . In <ref> [Lam et al. 91] </ref>, static code analysis and blocking algorithms are used to reorder accesses to lar ge data structures (e.g. lar ge arrays) so that they are operated on in chunks or blocks. This reduces conict misses in the data cache (D-cache) due to repeated reloading of array items. <p> Experiment Class 2: Studies of careful page placement in isolation will implement the placement algorithms proposed by [Hwu & Chang 89] and <ref> [Lam et al. 91] </ref> and then use Monster to measure changes in cache hit rates. Experiment Class 3: Studies on integrating car eful page mapping and placement .
Reference: [Larus 90] <author> James R. Larus. </author> <title> Abstract Execution: </title> <institution> A Technique for Effi - ciently T racing Pr ograms . University of Wisconsin-Madi son, Madison, WI, </institution> <year> 1990. </year>
Reference: [Li & Hudak 89] <author> Kai Li and Paul Hudak. </author> <title> Memory Coherence in Shared Vir tual Memory Systems. </title> <journal> Transactions on Computer Systems , 7(4) </journal> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference: [McNamee & Armstrong 90] <author> D. McNamee and K. Armstrong. </author> <title> Extending the Mach exter nal pager interface to accommodate user-level page replace ment policies. </title> <booktitle> In Proceedings of the USENIX Association Mach Workshop , pages 17-29, </booktitle> <address> Burlington, Vermont (USA), </address> <month> October </month> <year> 1990. </year> <institution> USENIX Association. </institution>
Reference-contexts: For OSF/1 (Mach 2.5) and Mach 3.0, we can modify a user level pager process. We hope to draw on work by [Sechrest & Park 91] and <ref> [McNamee & Armstrong 90] </ref> in which the Mach external pager interface is extended to allow the page replacement policy to be implemented in a user level pro cess. To perform the experiments in Class 2, we need to in uence the way a compiler places instructions and data.
Reference: [Mogul & Borg 91] <author> Jeffrey C. Mogul and Anita Bor g. </author> <title> The ef fect of context switches on cache performance. </title> <booktitle> In Four th International Conference on Architectural Support for Programming Lan guages and Operating Systems, </booktitle> <pages> pages 75-84, </pages> <year> 1991. </year>
Reference-contexts: The conict misses caused by multiple processes competing for room in a cache has been studied by [Stone & Thiebaut 86] and <ref> [Mogul & Borg 91] </ref> using mathematical analysis and tracedriven simulation. In [Kessler & Hill 90], tracedriven cache simulations are used to investigate dif ferent page mapping policies designed to minimize con ict misses from overlapping pages in the CPU cache.
Reference: [MIPS 88] <institution> MIPS Computer Systems, Inc. </institution> <note> RISCompiler Languages Programmers Guide </note>
Reference-contexts: To perform the experiments in Class 2, we need to in uence the way a compiler places instructions and data. For instructions, this will be easy . We have a comprehensive set of compiler optimization tools <ref> [MIPS 88] </ref>. In particular, we can: Generate dynamic execution profiles using prof pixie Translate the pixie prof outputs into the cord format by using the ftoc tool. The cord format is just an encoding of the information needed to construct a weighted call graph.
Reference: [Nagle & Uhlig 92] <author> D. Nagle, R. Uhlig and T. Mudge. </author> <title> Monster: A tool for ana lyzing the interaction between operating systems and com puter ar chitectures. </title> <institution> University of Michigan Tech Report, </institution> <year> 1992. </year>
Reference: [Nelson 89] <author> Harry Nelson. </author> <title> Experiences with performance monitors. </title> <booktitle> In Instrumentation for Futur e Parallel Computing Systems pages 201-208, </booktitle> <year> 1989. </year>
Reference: [Olukotun et al. 91] <author> O. A. Olukotun, T. N. Mudge and R. B. Brown. </author> <booktitle> Implementing a cache for a high-performance GaAs microprocessor . In The 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 138-147, </pages> <year> 1991. </year>
Reference-contexts: In a next generation computer system, it is possible that a memory access which misses the cache could result in a penalty of hundreds of CPU cycles to fetch the data from main memory <ref> [Olukotun et al. 91] </ref> [Hennessy & Jouppi 91]. This trend underscores the importance of fi nding new techniques to more effectively use CPU caches. 2.1 Previous Work The policies that dictate where data is placed in a CPU cache are usually implemented in hardware.
Reference: [Ousterhout 90] <author> J. Ousterhout. </author> <title> Why aren t operating systems getting faster as fast as hardware? In Proceedings of the Summer 1990 USENIX Conference , pages 247-256, 1990. Monster: A Tool for Analyzing the Interaction between Operating Systems and Computer Architectures </title>
Reference-contexts: We would like to study the needs of applications which rely heavily on operating system services and then determine what hardware features best support these needs. Our work is motivated primarily by [Anderson et al. 91] and <ref> [Ousterhout 90] </ref> who ar gue that operating systems and computer architectures are evolving in somewhat incompatible directions. To get started, we will restrict the scope of our studies to improving application perfor - mance with respect to a machine's instruction and data CPU caches.
Reference: [Pierce 92] <author> James Pierce. </author> <title> IDT race for the X86 Architecture. </title> <booktitle> Advanced Computer Architecture Seminar. </booktitle> <institution> The University of Michi gan, </institution> <year> 1992. </year>
Reference: [Schroeder & Burrows 90] <author> M. D. Schroeder and M. Burrows. </author> <title> Performance of Fire y RPC. </title> <journal> ACM Transactions on Computer Systems February, </journal> <year> 1990. </year>
Reference: [Sechrest & Park 91] <author> S. Sechrest and Y. Park. </author> <title> User level physical memory man agement for Mach. Extended Abstract to a USENIX Mach Workshop. </title>
Reference-contexts: Specifically, we have looked at some of the code and it appears that the modifications could be made by changing the memall () mall () routines. For OSF/1 (Mach 2.5) and Mach 3.0, we can modify a user level pager process. We hope to draw on work by <ref> [Sechrest & Park 91] </ref> and [McNamee & Armstrong 90] in which the Mach external pager interface is extended to allow the page replacement policy to be implemented in a user level pro cess.
Reference: [SPEC 91] <institution> SPEC Newsletter, </institution> <month> 3(3-4), </month> <year> 1991. </year>
Reference-contexts: A typical example of application of this sort can be found in the SPEC 1.0 benchmark suite studied in Part I. Each of these applications runs in a single UNIX process and usually makes very few system calls. <ref> [SPEC 91] </ref> 2. Conict misses occur when an item is evicted from the cache by a reference to a different item, but then must be re-fetched later on. This is in contrast with compulsory (cold-start) and capacity misses.
Reference: [Stone & Thiebaut 86] <author> H. Stone and D. Thiebaut. </author> <title> Footprints in the cache. </title> <journal> Transactions on Computer Systems </journal>
Reference-contexts: The conict misses caused by multiple processes competing for room in a cache has been studied by <ref> [Stone & Thiebaut 86] </ref> and [Mogul & Borg 91] using mathematical analysis and tracedriven simulation. In [Kessler & Hill 90], tracedriven cache simulations are used to investigate dif ferent page mapping policies designed to minimize con ict misses from overlapping pages in the CPU cache.
Reference: [Taylor et al. 90] <author> G. Taylor, P. Davies and M. Farmwald. </author> <title> The TLB slice - a low-cost highspeed address translation mechanism. </title> <booktitle> In 17th Annual International Symposium on Computer Archi tecture, </booktitle> <pages> pages 355-363, </pages> <year> 1990. </year>
Reference-contexts: Monster: A Tool for Analyzing the Interaction between Operating Systems and Computer Architectures conicts between the multiple address spaces of several processes. In <ref> [Taylor et al. 90] </ref>, a heuristic called page-coloring is used to assign page frames so that physical pages tend to be distributed in the same way that virtual pages are. Page coloring is actually one of the heuristics that Kessler uses.
References-found: 27

