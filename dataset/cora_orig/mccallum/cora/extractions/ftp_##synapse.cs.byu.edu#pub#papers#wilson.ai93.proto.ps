URL: ftp://synapse.cs.byu.edu/pub/papers/wilson.ai93.proto.ps
Refering-URL: ftp://synapse.cs.byu.edu/pub/papers/details.html
Root-URL: 
Email: email: randy@axon.cs.byu.edu, martinez@cs.byu.edu  
Title: The Potential of Prototype Styles of Generalization  
Author: D. Randall Wilson Tony R. Martinez 
Address: Provo, Utah 84602  
Affiliation: Computer Science Department, Brigham Young University,  
Date: 356-361, Nov. 1993.  
Note: Proceedings of the 6th Australian Joint Conference on Artificial Intelligence (AI93), pp.  
Abstract: There are many ways for a learning system to generalize from training set data. This paper presents several generalization styles using prototypes in an attempt to provide accurate generalization on training set data for a wide variety of applications. These generalization styles are efficient in terms of time and space, and lend themselves well to massively parallel architectures. Empirical results of generalizing on several real-world applications are given, and these results indicate that the prototype styles of generalization presented have potential to provide accurate generalization for many applications. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Lippmann, Richard P., </author> <title> An Introduction to Computing with Neural Nets, </title> <journal> IEEE ASSP Magazine, </journal> <volume> 3, no. 4, </volume> <pages> pp. 4-22, </pages> <month> April </month> <year> 1987. </year>
Reference: 2. <author> Widrow, Bernard, Rodney Winter, </author> <title> Neural Nets for Adaptive Filtering and Adaptive Pattern Recognition, </title> <journal> IEEE Computer Magazine, </journal> <pages> pp. 25-39, </pages> <month> March </month> <year> 1988. </year>
Reference: 3. <author> Widrow, Bernard, and Michael A. Lehr, </author> <title> 30 Years of Adaptive Neural Networks: Perceptron, Madaline, and Backpropagation, </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 78, no. 9, </volume> <pages> pp. 1415-1441, </pages> <month> September </month> <year> 1990. </year>
Reference: 4. <author> Rumelhart, D. E., and J. L. McClelland, </author> <title> Parallel Distributed Processing, </title> <publisher> MIT Press, Ch. </publisher> <pages> 8, pp. 318-362, </pages> <year> 1986. </year>
Reference: 5. <author> Martinez, Tony R., </author> <title> Adaptive Self-Organizing Logic Networks, </title> <type> Ph.D. Dissertation, </type> <institution> University of California Los Angeles Technical Report - CSD 860093, </institution> <note> (274 pp.), </note> <month> June, </month> <year> 1986. </year>
Reference: 6. <author> Martinez, Tony R., Douglas M. Campbell, Brent W. Hughes, </author> <title> Priority ASOCS, </title> <note> to appear in Journal of Artificial Neural Networks. </note>
Reference: 7. <author> Martinez, Tony R., </author> <title> Adaptive Self-Organizing Concurrent Systems, </title> <booktitle> Progress in Neural Networks, </booktitle> <volume> 1, ch. 5, </volume> <pages> pp. 105-126, </pages> <editor> O. Omidvar (Ed), </editor> <publisher> Ablex Publishing, </publisher> <year> 1990a. </year>
Reference: 8. <author> Rudolph, George L., and Tony R. Martinez, </author> <title> An Efficient Static Topology For Modeling ASOCS, </title> <booktitle> International Conference on Artificial Neural Networks, </booktitle> <address> Helsinki, Finland. </address> <booktitle> In Artificial Neural Networks, </booktitle> <editor> Kohonen, et. al. (Eds), </editor> <publisher> Elsevier Science Publishers, North Holland, </publisher> <pages> pp. 729-734, </pages> <year> 1991. </year>
Reference: 9. <author> Murphy, P. M., and D. W. Aha, </author> <title> UCI Repository of Machine Learning Databases. </title> <address> Irvine, CA: </address> <institution> University of California Irvine, Department of Information and Computer Science, </institution> <year> 1993. </year>
Reference-contexts: That is, given a training set of instances, they can often learn the relationship between the inputs This research was supported in part by grants from Novell, Inc., and WordPerfect Corporation. Reference: The Sixth Australian Joint Conference on Artificial Intelligence <ref> (AI 93) </ref>, pp. 356-361, Nov. 1993. and outputs well enough to be able to receive an input and produce the correct output with a high probability, even if that input did not appear in the training set. This ability is called generalization.
Reference: 10. <author> Wilson, D. Randall, </author> <title> The Importance of Using Multiple Styles of Generalization, </title> <booktitle> to appear in Proceedings of he First New Zealand International Conference on Artificial Neural Networks and Expert SystemsANNES 93. </booktitle>
References-found: 10

