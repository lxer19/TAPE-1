URL: http://www.cs.cmu.edu/~baird/papers/metric/metric.ps
Refering-URL: http://www.cs.cmu.edu/~baird/papers/index.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Metrics for Temporal Difference Learning  
Author: Mance E. Harmon Leemon C. Baird, III 
Note: United States Air Force  
Address: Dayton, OH 45433  Colorado Springs, CO 80840  
Affiliation: Wright Laboratory, Wright-Patterson AFB,  Department of Computer Science,  Academy,  
Abstract: For an absorbing Markov chain with a reinforcement on each transition, Bertsekas (1995a) gives a simple example where the function learned by TD( ll ) depends on ll . Bertsekas showed that for ll =1 the approximation is optimal with respect to a least-squares error of the value function, and that for ll =0 the approximation obtained by the TD method is poor with respect to the same metric. With respect to the error in the values, TD(1) approximates the function better than TD(0). However, with respect to the error in the differences in the values, TD(0) approximates the function better than TD(1). TD(1) is only better than TD(0) with respect to the former metric rather than the latter. In addition, direct TD( ll ) weights the errors unequally, while residual gradient methods (Baird, 1995, Harmon, Baird, & Klopf, 1995) weight the errors equally. For the case of control, a simple Markov decision process is presented for which direct TD(0) and residual gradient TD(0) both learn the optimal policy, while TD( 11 ) learns a suboptimal policy. These results suggest that, for this example, the differences in state values are more significant than the state values themselves, so TD(0) is preferable to TD(1). 
Abstract-found: 1
Intro-found: 1
Reference: <author> Baird, L.C. </author> <year> (1995). </year> <title> Residual Algorithms: Reinforcement Learning with Function Approximation. </title> <editor> In Armand Prieditis & Stuart Russell, eds. </editor> <booktitle> Machine Learning: Proceedings of the Twelfth International Conference , 9-12 July, </booktitle> <publisher> Morgan Kaufman Publishers, </publisher> <address> San Francisco, CA. </address>
Reference-contexts: A class of algorithms that does perform gradient descent on a single error function, residual algorithms <ref> (Baird, 1995, Harmon, Baird, and Klopf, 1995) </ref>, weights each temporal difference error based solely on the frequency with which it is trained and can be viewed as stochastic gradient descent on the mean squared Bellman residual.
Reference: <author> Bertsekas, D.P. </author> <year> (1995a). </year> <title> A counterexample to temporal differences learning. </title> <booktitle> Neural Computation , 7 , 270-279. </booktitle>
Reference: <author> Bertsekas, D.P. </author> <year> (1995b). </year> <title> Dynamic Programming And Optimal Control Vol. </title> <type> 1 & 2. </type> <address> Belmont, Massachusetts: </address> <publisher> Athena Scientific. </publisher>
Reference-contexts: We define this operator d f in equation (0). This operator is somewhat like a derivative or slope, particularly when g =1 (as is the case for the examples presented here), and is equivalent to the difference between the two sides of the Bellman equation <ref> (Bertsekas, 1995b) </ref>. d f ( x ) g f ( x ) - f ( x ) (0) The TD ( l ) update equation is defined as follows: N + 1 N N + 1 N l N -1 V ( x , w ) + l N -2 V
Reference: <author> Harmon, M. E., Baird, L. C., & Klopf, A. H. </author> <year> (1996). </year> <title> Reinforcement learning applied to a differential game. </title> <booktitle> Adaptive Behavior , 4 (1), </booktitle> <pages> 3-28. </pages>
Reference: <author> Sutton, R.S. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. </title> <note> Machine Learning . 3 , 9-44. </note>
References-found: 5

