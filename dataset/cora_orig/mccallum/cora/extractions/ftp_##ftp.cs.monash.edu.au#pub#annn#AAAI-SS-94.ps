URL: ftp://ftp.cs.monash.edu.au/pub/annn/AAAI-SS-94.ps
Refering-URL: http://www.cs.monash.edu.au/~annn/cv/pub.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: faen,lpkg@cs.brown.edu  
Title: Toward Approximate Planning in Very Large Stochastic Domains  
Author: Ann E. Nicholson and Leslie Pack Kaelbling 
Address: Providence, RI 02912  
Affiliation: Department of Computer Science Brown University,  
Abstract: In this paper we extend previous work on approximate planning in large stochastic domains by adding the ability to plan in automatically-generated abstract world views. The dynamics of the domain are represented compositionally using a Bayesian network. Sensitivity analysis is performed on the network to identify the aspects of the world upon which success is most highly dependent. An abstract world model is constructed by including only the most relevant aspects of the world. The world view can be refined over time, making the overall planner behave in most cases like an anytime algorithm. This paper is a pre liminary report on this ongoing work.
Abstract-found: 1
Intro-found: 1
Reference: [ Bellman, 1957 ] <author> Bellman, Richard 1957. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, New Jersey. </address>
Reference-contexts: We begin by giving a brief review of the formal definitions of the Markov decision process (mdp) model, then we consider more compact compositional representations for domains, and present an efficient approximate planning algorithm based on those representations. 2 Markov Decision Processes The work on Markov decision processes <ref> [ Bellman, 1957, Howard, 1960 ] </ref> , models the entire environment as a stochastic automaton. <p> A policy is optimal if it is not dominated by any other policy. Given a Markov decision process and a value for fl, it is possible to compute the optimal policy using either the policy iteration algorithm [ Howard, 1960 ] or the value iteration algorithm <ref> [ Bellman, 1957 ] </ref> . One of the most common goals is to achieve a certain condition p as soon as possible.
Reference: [ Dean and Boddy, 1988 ] <author> Dean, Thomas and Boddy, </author> <title> Mark 1988. An analysis of time-dependent planning. </title> <booktitle> In Proceedings of the Seventh National Conference on Artificial Intelligence, </booktitle> <address> Minneapolis-St. Paul, Min-nesota. </address>
Reference-contexts: An important property of this planning algorithm is that it works by constructing successive approximations to the optimal policy. Although there are rare cases in which approximations can get worse over time, in most cases this algorithm behaves like an anytime algorithm <ref> [ Dean and Boddy, 1988 ] </ref> , generating better and better policies the longer it has to compute. It can be interrupted at any time and a policy will be available.
Reference: [ Dean et al., 1993 ] <author> Dean, Thomas; Kael-bling, Leslie Pack; Kirman, Jak; and Nicholson, </author> <title> Ann 1993. Planning with deadlines in stochastic domains. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <address> Washington, DC. </address>
Reference-contexts: The theory of Markov decision processes provides methods for constructing policies (mappings from states to actions) that achieve robust execution by conditioning all actions on the current state of the world. These methods, unfortunately, require enumeration of the state space. Dean et al. <ref> [ Dean et al., 1993 ] </ref> have addressed this problem by arranging for the planner to consider only parts of the state space that are likely to be traversed during the execution of a plan. <p> Applying this abstraction to the world modeled in Figure 2 results in the abstract world view shown in Figure 3. robotic actions 6 Planning With Approximate World Models In this section we outline the basic approximate planning algorithm of Dean et al. <ref> [ Dean et al., 1993 ] </ref> , then show how we can extend the notion of planning in restricted state spaces by planning in restricted versions of abstract world views.
Reference: [ Howard, 1960 ] <author> Howard, Ronald A. </author> <year> 1960. </year> <title> Dynamic Programming and Markov Processes. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts. </address>
Reference-contexts: We begin by giving a brief review of the formal definitions of the Markov decision process (mdp) model, then we consider more compact compositional representations for domains, and present an efficient approximate planning algorithm based on those representations. 2 Markov Decision Processes The work on Markov decision processes <ref> [ Bellman, 1957, Howard, 1960 ] </ref> , models the entire environment as a stochastic automaton. <p> A policy is optimal if it is not dominated by any other policy. Given a Markov decision process and a value for fl, it is possible to compute the optimal policy using either the policy iteration algorithm <ref> [ Howard, 1960 ] </ref> or the value iteration algorithm [ Bellman, 1957 ] . One of the most common goals is to achieve a certain condition p as soon as possible.
Reference: [ Kemeny and Snell, 1976 ] <author> Kemeny, John G. and Snell, J. </author> <title> Laurie 1976. Finite Markov Chains. </title> <publisher> Springer-Verlag, </publisher> <address> New York. </address>
Reference-contexts: A policy is a mapping from S to A, specifying an action to be taken in each situation. An environment combined with a policy for choosing actions in that environment yields a Markov chain <ref> [ Kemeny and Snell, 1976 ] </ref> .
Reference: [ Kushmerick et al., 1993 ] <author> Kushmerick, N.; Hanks, S.; and Weld, D. </author> <year> 1993. </year> <title> An Algorithm for Probabilistic Planning. </title> <type> Technical Report 93-06-03, </type> <institution> University of Washington Department of Computer Science and Engineering. </institution> <note> To appear in Artificial Intelligence. </note>
Reference-contexts: The state transition function is given compo-sitionally, by specifying, in different rules, how different aspects of the environment change given different actions. Thus, the color of walls need only be mentioned when describing change that occurs as a result of the paint operator. Work on the Buridan planner <ref> [ Kushmerick et al., 1993 ] </ref> has extended traditional operator descriptions to describe stochastic transitions. It allows the post conditions of an operator to be a probability distribution over possible outcomes, expressed as equivalence classes of world states.
Reference: [ Pearl, 1988 ] <author> Pearl, </author> <title> Judea 1988. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California. </address>
Reference-contexts: A simplified version of the Bayesian network formalism <ref> [ Pearl, 1988 ] </ref> is well suited to specifying stochastic state transition and reward models.
Reference: [ Polya, 1945 ] <author> Polya, </author> <title> George 1945. How to Solve It. </title> <publisher> Princeton university Press, </publisher> <address> Princeton, New Jersey. </address>
Reference-contexts: In other cases, they will serve as tractable approximations to more complex models. The idea of using abstract versions of a problem space for efficient planning is by no means new. Polya mentions it as a problem solving heuristic in How to Solve It <ref> [ Polya, 1945 ] </ref> . It was articulated formally and applied to planning by Sacerdoti in his work on abstrips [ Sacer-doti, 1974 ] . abstrips planned first in abstract domains that left out preconditions of low criticality.
Reference: [ Renyi, 1984 ] <author> Renyi, </author> <title> Alfred 1984. A Diary on Information Theory. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, New York. </address>
Reference-contexts: Then a (N; N 0 ) = jj 2 n 0 where D is a measure of distance between two distributions. As a distance measure, we use the information-theoretical distance (also known as the Kullback-Leibler information distance) <ref> [ Renyi, 1984 ] </ref> as follows: D (P; Q) = x P (x) : It measures the distance between two distributions by taking the expectation (according to the first distribution) of the difference of the log probabilities at every 3 Of course, this might be a very bad approximation, because we
Reference: [ Sacerdoti, 1974 ] <author> Sacerdoti, Earl D. </author> <year> 1974. </year> <title> Planning in a hierarchy of abstraction spaces. </title> <booktitle> Artificial Intelligence 5 </booktitle> <pages> 115-135. </pages>
References-found: 10

