URL: http://www.eecs.umich.edu/~gabandah/HPL-96-157.ps
Refering-URL: http://www.eecs.umich.edu/~gabandah/research.html
Root-URL: http://www.eecs.umich.edu
Email: abandah@hpl.hp.com  
Title: Tools for Characterizing Distributed Shared Memory Applications Keywords: Distributed Shared Memory, Application Analysis, Trace Collection,
Author: Gheith A. Abandah 
Date: November 20, 1996  
Affiliation: Computer Systems Laboratory HP Laboratories  
Abstract: In order to support the design of future distributed shared memory (DSM) systems, we have developed a set of tools for characterizing DSM applications. These tools enable collecting and analyzing data, instruction, and I/O stream traces of DSM applications. They give characterization for several aspects of DSM applications including communication and sharing patterns. They also enable evaluating the performance of these applications on various DSM design alternatives. This report describes the implementation of these tools, their usage, and the format of their output files. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Protic, M. Tomasevic, and V. Milutinovic, </author> <title> "Distributed shared memory: </title> <booktitle> Concepts and systems," IEEE Parallel and Distributed Technology, </booktitle> <pages> pp. 63-79, </pages> <month> Summer </month> <year> 1996. </year>
Reference-contexts: It describes the implementation of these tools, their usage, and the format of their output files. The development of these tools was initiated to support developing new DSM multiprocessor systems, Protic et al. survey DSM concepts and systems in <ref> [1] </ref>. The tools can also aid in application performance tuning. There are three logical steps in conducting application characterization; problem definition, performance collection, and performance analysis. In problem definition, we need to specify the applications that we are interested in characterizing and the performance aspects that need to be characterized.
Reference: [2] <author> D. Clark and H. Levy, </author> <title> "Measurement and analysis of instruction use in the VAX-11/780," </title> <booktitle> in Proc. The 9th Annual Symposium on Computer Architecture, </booktitle> <pages> pp. 9-17, </pages> <year> 1982. </year>
Reference-contexts: There are several classes of techniques for performance collection, each with its own advantages and limitations. Hardware monitors, e.g. VAX microcode monitor <ref> [2] </ref>, Convex Cxpa [3], and the IBM POWER performance monitor [4] provide low-level information using event counters, but require special hardware support. Code instrumentation, e.g. RYO [5] and Pixie [6], enables collecting various performance data and traces, but requires source code availability. Direct-execution simulation, e.g.
Reference: [3] <institution> CONVEX Computer Corporation, </institution> <address> 3000 Waterview Parkway, P.O. Box 833851, Richard-son, TX 75083-3851, </address> <note> CXpa Reference Manual, second ed., </note> <month> March </month> <year> 1993. </year> <title> Order No. </title> <publisher> DSW-253. </publisher>
Reference-contexts: There are several classes of techniques for performance collection, each with its own advantages and limitations. Hardware monitors, e.g. VAX microcode monitor [2], Convex Cxpa <ref> [3] </ref>, and the IBM POWER performance monitor [4] provide low-level information using event counters, but require special hardware support. Code instrumentation, e.g. RYO [5] and Pixie [6], enables collecting various performance data and traces, but requires source code availability. Direct-execution simulation, e.g.
Reference: [4] <author> E. H. Welbon, C. C. Cha-Nui, D. J. Shippy, and D. A. Hicks, </author> <title> "The POWER2 performance monitor," </title> <journal> IBM J. Research and Development, </journal> <volume> vol. 38, </volume> <pages> pp. 545-554, </pages> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: There are several classes of techniques for performance collection, each with its own advantages and limitations. Hardware monitors, e.g. VAX microcode monitor [2], Convex Cxpa [3], and the IBM POWER performance monitor <ref> [4] </ref> provide low-level information using event counters, but require special hardware support. Code instrumentation, e.g. RYO [5] and Pixie [6], enables collecting various performance data and traces, but requires source code availability. Direct-execution simulation, e.g.
Reference: [5] <author> D. F. Zucker and A. H. Karp, "RYO: </author> <title> a versatile instruction instrumentation tool for PA-RISC," </title> <type> Technical Report CSL-TR-95-658, </type> <institution> Stanford University, </institution> <month> Jan. </month> <year> 1995. </year>
Reference-contexts: There are several classes of techniques for performance collection, each with its own advantages and limitations. Hardware monitors, e.g. VAX microcode monitor [2], Convex Cxpa [3], and the IBM POWER performance monitor [4] provide low-level information using event counters, but require special hardware support. Code instrumentation, e.g. RYO <ref> [5] </ref> and Pixie [6], enables collecting various performance data and traces, but requires source code availability. Direct-execution simulation, e.g. Proteus [7] and SimOS [8], similar to code instrumentation, enables collecting various performance data and perturbs the execution as well, but does not require source code availability. <p> Section 7 concludes the paper by stating the limitations and advantages of these tools and methodology. Appendix A specifies the formant of SMAIT trace files. Appendix B presents CIAT and CDAT command line options. Appendix C specifies CDAT trace format. 4 2 Trace Collection SMAIT is based on RYO <ref> [5] </ref>, a tool developed by Zucker and Karp for instrumenting PA-RISC [12] instruction sequences. RYO is a set of awk scripts that enable replacing individual machine instructions with calls to user written subroutines. SMAIT is designed to enable collecting traces of multi-threaded shared-memory parallel applications.
Reference: [6] <author> M. D. Smith, </author> <title> "Tracing with Pixie." </title> <note> ftp document, </note> <institution> Center for Integrated Systems, Stan-ford University, </institution> <month> Apr. </month> <year> 1991. </year>
Reference-contexts: Hardware monitors, e.g. VAX microcode monitor [2], Convex Cxpa [3], and the IBM POWER performance monitor [4] provide low-level information using event counters, but require special hardware support. Code instrumentation, e.g. RYO [5] and Pixie <ref> [6] </ref>, enables collecting various performance data and traces, but requires source code availability. Direct-execution simulation, e.g. Proteus [7] and SimOS [8], similar to code instrumentation, enables collecting various performance data and perturbs the execution as well, but does not require source code availability.
Reference: [7] <author> E. A. Brewer, C. N. Dellarocas, A. Colbrook, and W. E. Weihl, "PROTEUS: </author> <title> a high-performance parallel-architecture simulator," </title> <type> Tech. Rep. </type> <institution> MIT/LCS/TR-516, Mas-sachusetts Institute of Technology, </institution> <month> Sept. </month> <year> 1991. </year> <month> 18 </month>
Reference-contexts: VAX microcode monitor [2], Convex Cxpa [3], and the IBM POWER performance monitor [4] provide low-level information using event counters, but require special hardware support. Code instrumentation, e.g. RYO [5] and Pixie [6], enables collecting various performance data and traces, but requires source code availability. Direct-execution simulation, e.g. Proteus <ref> [7] </ref> and SimOS [8], similar to code instrumentation, enables collecting various performance data and perturbs the execution as well, but does not require source code availability. SimOS collects traces for activities within the operating system in addition to the user-space activities.
Reference: [8] <author> M. Rosenblum, S. A. Herrod, E. Witchel, and A. Gupta, </author> <title> "Complete computer simula-tion: The SimOS approach," </title> <booktitle> IEEE Parallel and Distributed Technology, </booktitle> <month> Fall </month> <year> 1995. </year>
Reference-contexts: Code instrumentation, e.g. RYO [5] and Pixie [6], enables collecting various performance data and traces, but requires source code availability. Direct-execution simulation, e.g. Proteus [7] and SimOS <ref> [8] </ref>, similar to code instrumentation, enables collecting various performance data and perturbs the execution as well, but does not require source code availability. SimOS collects traces for activities within the operating system in addition to the user-space activities.
Reference: [9] <author> D. Reed et al., </author> <title> "Scalable performance analysis: The Pablo performance analysis environment," </title> <booktitle> in Proc. IEEE Scalable Parallel Libraries Conf., </booktitle> <year> 1993. </year>
Reference-contexts: Because of the wide range of applications that we are interested in characterizing, we are collecting traces using code instrumentation and other methods. Trace collection using code instrumentation is described in this report. Most of the available parallel performance analysis tools are developed for analyzing message-passing applications, e.g. Pablo <ref> [9] </ref>, Medea [10], and Paradyn [11]. The tools presented in this report address the shortage of tools for analyzing shared-memory applications. They assist in the performance analysis of shared-memory applications and enable characterizing wide range of performance aspects. We have developed two tools for analyzing shared-memory applications.
Reference: [10] <author> M. Calzarossa, L. Massari, A. Merlo, M. Pantano, and T. Daniele, "Medea: </author> <title> A tool for workload characterization of parallel systems," </title> <journal> IEEE Parallel and Distributed Technology, </journal> <volume> vol. 3, </volume> <pages> pp. 72-80, </pages> <month> Winter </month> <year> 1995. </year>
Reference-contexts: Trace collection using code instrumentation is described in this report. Most of the available parallel performance analysis tools are developed for analyzing message-passing applications, e.g. Pablo [9], Medea <ref> [10] </ref>, and Paradyn [11]. The tools presented in this report address the shortage of tools for analyzing shared-memory applications. They assist in the performance analysis of shared-memory applications and enable characterizing wide range of performance aspects. We have developed two tools for analyzing shared-memory applications.
Reference: [11] <author> B. Miller, M. Callaghan, J. Cargille, J. Hollingsworth, R. Irvin, K. Karavanic, K. Kun-chithapadam, and T. Newhall, </author> <title> "The Paradyn parallel performance measurement tool," </title> <journal> Computer, </journal> <volume> vol. 28, </volume> <pages> pp. 37-46, </pages> <month> Nov. </month> <year> 1995. </year>
Reference-contexts: Trace collection using code instrumentation is described in this report. Most of the available parallel performance analysis tools are developed for analyzing message-passing applications, e.g. Pablo [9], Medea [10], and Paradyn <ref> [11] </ref>. The tools presented in this report address the shortage of tools for analyzing shared-memory applications. They assist in the performance analysis of shared-memory applications and enable characterizing wide range of performance aspects. We have developed two tools for analyzing shared-memory applications.
Reference: [12] <author> Hewlett-Packard, </author> <title> PA-RISC 1.1 Architecture and Instruction Set, </title> <booktitle> third ed., </booktitle> <month> Feb. </month> <year> 1994. </year>
Reference-contexts: Appendix A specifies the formant of SMAIT trace files. Appendix B presents CIAT and CDAT command line options. Appendix C specifies CDAT trace format. 4 2 Trace Collection SMAIT is based on RYO [5], a tool developed by Zucker and Karp for instrumenting PA-RISC <ref> [12] </ref> instruction sequences. RYO is a set of awk scripts that enable replacing individual machine instructions with calls to user written subroutines. SMAIT is designed to enable collecting traces of multi-threaded shared-memory parallel applications.
Reference: [13] <author> T. Brewer, </author> <title> "A highly scalable system utilizing up to 128 PA-RISC processors," </title> <booktitle> in Digest of papers, COMPCON'95, </booktitle> <pages> pp. 133-140, </pages> <month> Mar. </month> <year> 1995. </year>
Reference-contexts: The following steps outline an example how to use SMAIT to instrument a simple matrix multiplication program and to collect traces on a Convex SPP-1600 multiprocessor <ref> [13] </ref>. 1. Convert the high-level source files to assembly language files. 5 2. Instrument the assembly files using smait.perl and link with the two run-time library object files; smait.o and smait init.o.
References-found: 13

