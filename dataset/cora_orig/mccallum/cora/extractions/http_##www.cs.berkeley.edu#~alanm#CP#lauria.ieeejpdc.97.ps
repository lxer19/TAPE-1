URL: http://www.cs.berkeley.edu/~alanm/CP/lauria.ieeejpdc.97.ps
Refering-URL: http://www.cs.berkeley.edu/~alanm/CP/bib.html
Root-URL: 
Email: lauria@nadis.dis.unina.it.  achien@cs.uiuc.edu  
Title: MPI-FM: High Performance MPI on Workstation Clusters a high performance implementation of MPI for networks
Author: Mario Lauria Andrew Chien 
Note: MPI-FM is  Visiting at time of writing  
Address: via Claudio 21 80125 Napoli, Italy  1304 W. Springfield Ave. Urbana, IL 61801, USA  
Affiliation: Dipartimento di Informatica e Sistemistica Universita di Napoli "Federico II"  Department of Computer Science University of Illinois at Urbana-Champaign  
Abstract: Despite the emergence of high speed LANs, the communication performance available to applications on workstation clusters still falls short of that available on MPPs. A new generation of efficient messaging layers is needed to take advantage of the hardware performance and to deliver it to the application level. Communication software is the key element in bridging the communication performance gap separating MPPs and workstation clusters. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. M. Anderson and R. S. Cornelius. </author> <title> High-performance switching with Fibre Channel. </title> <booktitle> In Digest of Papers Compcon 1992, </booktitle> <pages> pages 261-268. </pages> <publisher> IEEE Computer Society Press, </publisher> <address> 1992. Los Alamitos, Calif. </address>
Reference-contexts: Workstation clusters have recently become attractive for high performance computation due to the introduction of new communication technologies with much improved performance. The fast Local Area Networks (LANs) available today (ATM [5], FDDI [8], Fi-brechannel <ref> [1] </ref>, Myrinet [3]) are, in terms of latency and bandwidth, comparable to the proprietary interconnect found on MPPs. However, without a radical change in the way communication protocols are implemented, applications will not be able to reap the benefits of this new technology.
Reference: [2] <author> H. E. Bal, M. F. Kaashoek, and A. S. Tanenbaum. Orca: </author> <title> A language for parallel programming of distributed systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 18(3) </volume> <pages> 190-205, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Besides the MPI message passing library, these are the BSD socket interface, the Converse compiler back-end [12], the Tempest runtime library [11], the Orca Project parallel object language <ref> [2] </ref>. 6 MPI and MPICH MPI (Message Passing Interface) is a message passing library, with primitives specifications for both C and Fortran.
Reference: [3] <author> Nanette J. Boden, Danny Cohen, Robert E. Felderman, Alan E. Kulawik, Charles L. Seitz, Jakov N. Seizovic, and Wen-King Su. </author> <title> Myrinet|a gigabit-per-second local-area network. </title> <journal> IEEE Micro, </journal> <volume> 15(1) </volume> <pages> 29-36, </pages> <month> February </month> <year> 1995. </year> <note> Available from http://www.myri.com/research/publications/Hot.ps. </note>
Reference-contexts: Workstation clusters have recently become attractive for high performance computation due to the introduction of new communication technologies with much improved performance. The fast Local Area Networks (LANs) available today (ATM [5], FDDI [8], Fi-brechannel [1], Myrinet <ref> [3] </ref>) are, in terms of latency and bandwidth, comparable to the proprietary interconnect found on MPPs. However, without a radical change in the way communication protocols are implemented, applications will not be able to reap the benefits of this new technology. <p> In the present version of FM, this represents a major bottleneck for bandwidth and directly limits performance for long messages. Network Myrinet is a high speed LAN interconnect which uses bidirectional byte-wide copper links to achieve physical bandwidth of nearly 80 MB/s in each direction <ref> [3] </ref>. It uses the interconnect technology developed for the Caltech Mosaic project [18]. A Myrinet network is composed of network interfaces connected to crossbar switches by point-to-point links. The full crossbar switches have four or eight ports, and can be interconnected in an arbitrary topology.
Reference: [4] <author> Greg Buzzard, David Jacobson, Scott Marovich, and John Wilkes. Hamlyn: </author> <title> A high-performance network interface with sender-based memory management. </title> <booktitle> In Proceedings of the IEEE Hot Interconnects Symposium, </booktitle> <year> 1995. </year> <note> Available from ftp://ftp.hpl.hp.com/wilkes/ HamlynHotIntIII.ps.Z. </note>
Reference-contexts: One problem of ATM networks is the cost of the switch, much higher than the cost of a comparable Myrinet switch. Hamlyn <ref> [4] </ref> implements a sender-based memory management scheme, and gives applications direct access to the network interface. There are plans to implement it on a proprietary version of Myrinet for HP workstations.
Reference: [5] <author> CCITT, </author> <title> SG XVIII, Report R34. Draft Recommendation I.150: B-ISDN ATM functional characteristics, </title> <month> June </month> <year> 1990. </year>
Reference-contexts: In the same time interval, a typical workstation RISC processor can execute tens to hundreds of thousands instructions. Workstation clusters have recently become attractive for high performance computation due to the introduction of new communication technologies with much improved performance. The fast Local Area Networks (LANs) available today (ATM <ref> [5] </ref>, FDDI [8], Fi-brechannel [1], Myrinet [3]) are, in terms of latency and bandwidth, comparable to the proprietary interconnect found on MPPs. However, without a radical change in the way communication protocols are implemented, applications will not be able to reap the benefits of this new technology.
Reference: [6] <author> D. Clark. </author> <title> The structure of systems using upcalls. </title> <booktitle> In ACM Symp. OS Principle '85, </booktitle> <pages> pages 171-180, </pages> <year> 1985. </year>
Reference-contexts: Rather than trying to relax these constraints, for example by duplicating on the send side the knowledge about which buffer to use, we adopted an inquiry mechanism employing an upcall <ref> [6] </ref>. An upcall is a function defined inside an upper layer and invoked by the layer underneath. Whenever the first fragment of a long message arrives, FM asks the application code for a buffer to use as reassembly buffer.
Reference: [7] <author> D. Clark, V. Jacobson, J Romkey, and H. Salwen. </author> <title> An analysis of TCP processing overhead. </title> <journal> IEEE Communication Magazine, </journal> <volume> 27(6) </volume> <pages> 23-29, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: In traditional protocol implementations, these services are provided by the operating system. But the convenience of relying on the operating system is paid for in terms of additional copies between address spaces, and by the context switch occurring at each system call <ref> [7] </ref>, both of which contribute to overhead and reduce performance. As a result, on a network of workstations, the observed communication latency over TCP/IP is on the order of one half to one millisecond.
Reference: [8] <institution> Fiber-distributed data interface (FDDI)|Token ring media access control (MAC). American National Standard for Information Systems ANSI X3.139-1987, </institution> <month> July </month> <year> 1987. </year> <institution> American National Standards Institute. </institution> <month> 25 </month>
Reference-contexts: Workstation clusters have recently become attractive for high performance computation due to the introduction of new communication technologies with much improved performance. The fast Local Area Networks (LANs) available today (ATM [5], FDDI <ref> [8] </ref>, Fi-brechannel [1], Myrinet [3]) are, in terms of latency and bandwidth, comparable to the proprietary interconnect found on MPPs. However, without a radical change in the way communication protocols are implemented, applications will not be able to reap the benefits of this new technology.
Reference: [9] <author> Message Passing Interface Forum. </author> <title> The MPI message passing interface standard. </title> <type> Technical report, </type> <institution> University of Tennessee, Knoxville, </institution> <month> April </month> <year> 1994. </year> <note> Available from http://www.mcs.anl.gov/mpi/mpi-report.ps. </note>
Reference-contexts: The MPI standard has been gaining support in the parallel computing community since the presentation of the initial draft standard in 1993 and its formalization at the Message Passing Interface Forum <ref> [9] </ref>. One of MPI largest attractions is the number of free implementations that have been made available. The quick and efficient realization of the MPI library on top of FM was made possible by the existence of one of these publicly available implementations.
Reference: [10] <author> H. Franke, C. E. Wu, M Riviere, P Pattnik, and M Snir. </author> <title> MPI programming environment for IBM SP1/SP2. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <year> 1995. </year>
Reference-contexts: Using a faster medium than Ethernet does not bring much improvement [15]. Some of the same libraries have been ported to MPPs. For example, two implementations of MPI are available on the SP2. One is a port of MPICH, the other (MPI-F) is a native implementation <ref> [10] </ref>. Both achieve 33 MB/s of peak bandwidth, with a 0-byte latency of 40.5 s for MPI-F and 55 s for the MPICH port. One feature of MPI-F is the packing/unpacking on the fly of complex data types, which exploits a pipe abstraction provided by the underlying communication layer. <p> The network interface adaptors in both Myrinet and the SP2 are quite similar, suggesting that the FM approach to guarantees and interface is responsible for the performance differences. In particular, MPI-F takes different choices in buffer allocation, notification, flow control, reliability, and packet ordering <ref> [10] </ref>. The last two points are a consequence of the assumption of unreliable, out of order delivery performed by the network. According to [10], this assumption was a choice made in order to preserve the flexibility of choosing various routing strategies and simplify error recovery. <p> In particular, MPI-F takes different choices in buffer allocation, notification, flow control, reliability, and packet ordering <ref> [10] </ref>. The last two points are a consequence of the assumption of unreliable, out of order delivery performed by the network. According to [10], this assumption was a choice made in order to preserve the flexibility of choosing various routing strategies and simplify error recovery. The graphs show that MPI-FM has better or comparable latency and bandwidth for message sizes up to 2 KB.
Reference: [11] <author> Mark D. Hill, James R. Larus, and David A. Wood. </author> <title> Tempest: A substrate for portable parallel programs. </title> <booktitle> In Compcon, </booktitle> <month> March </month> <year> 1995. </year> <note> Available from ftp://ftp.cs.wisc.edu/wwt/compcon95 tempest.ps. </note>
Reference-contexts: The range of applications being considered for development on FM, or already in the works, is testimony to the flexibility of its interface. Besides the MPI message passing library, these are the BSD socket interface, the Converse compiler back-end [12], the Tempest runtime library <ref> [11] </ref>, the Orca Project parallel object language [2]. 6 MPI and MPICH MPI (Message Passing Interface) is a message passing library, with primitives specifications for both C and Fortran.
Reference: [12] <author> Laxmikant V. Kale, Milind Bhandarkar, Narain Jagathesan, Sanjeev Krishnan, and Joshua M. Yelon. </author> <title> Converse: an interoperable framework for parallel programming. </title> <booktitle> In Proceedings of the International Parallel Processing Symposium, </booktitle> <year> 1996. </year> <note> Available from http://charm.cs.uiuc.edu/papers/converse-ipps96.ps. </note>
Reference-contexts: The range of applications being considered for development on FM, or already in the works, is testimony to the flexibility of its interface. Besides the MPI message passing library, these are the BSD socket interface, the Converse compiler back-end <ref> [12] </ref>, the Tempest runtime library [11], the Orca Project parallel object language [2]. 6 MPI and MPICH MPI (Message Passing Interface) is a message passing library, with primitives specifications for both C and Fortran.
Reference: [13] <author> Vijay Karamcheti and Andrew Chien. </author> <title> Software overhead in messaging layers: </title> <booktitle> Where does the time go? In Proceedings of the Sixth Symposium on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VI), </booktitle> <year> 1994. </year> <note> Available from http://www-csag.cs.uiuc.edu/papers/asplos94.ps. </note>
Reference-contexts: is a generalization of the Active Message model [22], in that there are no restrictions on the communication operations that can be carried out by the handler. (The user is responsible for avoiding deadlock situations.) Reliable and in-order delivery guarantees can be expensive if implemented in the upper messaging layers <ref> [13] </ref>. Their cost can be decreased if built directly into the lower level layer, where there is an opportunity to take advantage of some useful features of the network.
Reference: [14] <author> S. J. Le*er, M. K. McKusick, M. J. Karels, and J. S. Quaterman. </author> <title> The Design and Implementation of the 4.3BSD UNIX Operating System. </title> <publisher> Addison-Wesley, </publisher> <year> 1988. </year>
Reference-contexts: Software is abundant, readily available, and has a large base of established users. The use of networked machines to do distributed computing is not new, and a number of communication libraries which use TCP/IP over Ethernet have been around for some time now (BSD sockets <ref> [14] </ref>, PVM [21]). However, Ethernet and its associated networking protocols were not designed for high performance computing, and their limitations when used for this purpose severely restrict the range of applications that can be run and achieve good parallel performance.
Reference: [15] <author> M. Liu, J. Hsieh, D. Hu, J. Thomas, and J. MacDonald. </author> <title> Distributed network computing over Local ATM Networks. </title> <booktitle> In Supercomputing '94, </booktitle> <year> 1995. </year>
Reference-contexts: However, without a radical change in the way communication protocols are implemented, applications will not be able to reap the benefits of this new technology. Existing protocol implementations have been shown to achieve only modest performance improvements when used on new network hardware <ref> [15] </ref>. Our solution is to build new communication software, designed from the start with the objectives of low latency and high bandwidth communication. In the context of the Fast Messages (FM) project [17], we selected the Myrinet network due to its performance, programmability and price/performance ratio. <p> For instance, tolerance to high latency and high error rates, or protection among processes using the 8 network were primary design objectives. Using a faster medium than Ethernet does not bring much improvement <ref> [15] </ref>. Some of the same libraries have been ported to MPPs. For example, two implementations of MPI are available on the SP2. One is a port of MPICH, the other (MPI-F) is a native implementation [10].
Reference: [16] <author> Neil R. McKenzie, Kevin Bolding, Carl Ebeling, and Lawrence Snyder. Cranium: </author> <title> An interface for message passing on adaptive packet routing networks. </title> <booktitle> In Proceedings of the 1994 Parallel Computer Routing and Communication Workshop, </booktitle> <month> May </month> <year> 1994. </year> <note> Available from ftp://shrimp.cs.washington.edu/pub/chaos/docs/cranium-pcrcw.ps.Z. </note>
Reference-contexts: Hamlyn [4] implements a sender-based memory management scheme, and gives applications direct access to the network interface. There are plans to implement it on a proprietary version of Myrinet for HP workstations. Cranium <ref> [16] </ref> is in many respect similar to Hamlyn, and is to be implemented on a experimental interconnection network, Chaos. However, to our knowledge, at the time of this writing there are no high performance MPI implementations available on these interfaces.
Reference: [17] <author> Scott Pakin, Mario Lauria, and Andrew Chien. </author> <title> High performance messaging on workstations: Illinois Fast Messages (FM) for Myrinet. </title> <booktitle> In Supercomputing, </booktitle> <month> December </month> <year> 1995. </year> <note> Available from http://www-csag.cs.uiuc.edu/papers/myrinet-fm-sc95.ps. </note>
Reference-contexts: Our solution is to build new communication software, designed from the start with the objectives of low latency and high bandwidth communication. In the context of the Fast Messages (FM) project <ref> [17] </ref>, we selected the Myrinet network due to its performance, programmability and price/performance ratio. On this network we wrote the FM library, a highly optimized, low latency messaging layer providing a virtual interface for the hardware [17]. <p> In the context of the Fast Messages (FM) project <ref> [17] </ref>, we selected the Myrinet network due to its performance, programmability and price/performance ratio. On this network we wrote the FM library, a highly optimized, low latency messaging layer providing a virtual interface for the hardware [17]. The lowest layer of the communication software often loses most of the raw performance, due to architectural barriers (I/O bus) and to the large difference in abstractions (hardware device vs. programming interface). So in this first part of the project the goal was to minimize the performance loss. <p> FM achieves a short message latency of only 14 s and a peak bandwidth of 17.6 MB/s, with an Active Messages style interface. The first part of the research project is complete and is documented in <ref> [17] </ref>, and only a few details of the FM interface will be presented here. The second phase of the project constitutes the object of this work, and will be described in greater detail. After this first phase, the problem of optimizing the FM interface was tackled. <p> This technique requires additional queue management, the handling of acknowledgments, a timeout device, and provisions to deal with duplicated packets. Instead, the FM layer is built in a way not only to offer this guarantee, but also to pay little or no cost for it <ref> [17] </ref>. Some low level messaging layers offer different primitives, each targeted to a specific use, like low latency transfer, or remote memory copy. The typical utilization domain of each primitive can be best represented using a multidimensional space, with each dimension representing one of the major features differentiating the primitives.
Reference: [18] <author> C. Seitz, N. Boden, J. Seizovic, and W. Su. </author> <title> The design of the Caltech Mosaic C multicom-puter. </title> <booktitle> In Proceedings of the University of Washington Symposium on Integrated Systems, </booktitle> <year> 1993. </year> <note> Available from http://www.myri.com/research/publications/sbss.ps. </note>
Reference-contexts: Network Myrinet is a high speed LAN interconnect which uses bidirectional byte-wide copper links to achieve physical bandwidth of nearly 80 MB/s in each direction [3]. It uses the interconnect technology developed for the Caltech Mosaic project <ref> [18] </ref>. A Myrinet network is composed of network interfaces connected to crossbar switches by point-to-point links. The full crossbar switches have four or eight ports, and can be interconnected in an arbitrary topology.
Reference: [19] <author> David Sitsky, David Walsh, and Chris Johnson. </author> <title> Implementation and performance of the MPI Message Passing Interface on the Fujistu AP1000 Multicomputer. </title> <note> In Proceedings of ACSC'95. Available from ftp://dcssoft.anu.edu.au/pub/www/dcs/cap/mpi/mpi.html. </note>
Reference-contexts: MPI is available on the AP1000, where it achieves 332 s minimum latency and 2.85 MB/s peak bandwidth <ref> [19] </ref>. 4 Design The approach followed in creating MPI-FM has been that of incrementally refining a straightforward implementation of the ADI. At each step, a different optimization was tested by modifying both the ADI and the FM interface as required according to the results of the performance analysis.
Reference: [20] <author> C.B. Stunkel, D.G. Shea, D.G. Grice, P.H. Hochschild, and M. Tsao. </author> <title> The SP1 high-performance switch. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference, </booktitle> <pages> pages 150-157, </pages> <address> Knoxville, TN,, </address> <month> May </month> <year> 1994. </year> <note> Available from http://ibm.tc.cornell.edu/ibm/pps/doc/hps.ps. </note>
Reference-contexts: The SP2 is commonly regarded as one of the (few) successful commercial supercomputers available on the market today. The SP2 nodes are ordinary IBM RISC workstations running an unmodified AIX; the interconnect is a proprietary network based on the Vulcan switch <ref> [20] </ref>. From an architectural point of view, the SP2 is basically a network of workstations. The network interface adaptors in both Myrinet and the SP2 are quite similar, suggesting that the FM approach to guarantees and interface is responsible for the performance differences.
Reference: [21] <author> V. S. Sunderam. </author> <title> PVM: A framework for parallel distributed computing. </title> <journal> Concurrency, Practice and Experience, </journal> <volume> 2(4) </volume> <pages> 315-340, </pages> <month> [12] </month> <year> 1990. </year> <month> 26 </month>
Reference-contexts: Software is abundant, readily available, and has a large base of established users. The use of networked machines to do distributed computing is not new, and a number of communication libraries which use TCP/IP over Ethernet have been around for some time now (BSD sockets [14], PVM <ref> [21] </ref>). However, Ethernet and its associated networking protocols were not designed for high performance computing, and their limitations when used for this purpose severely restrict the range of applications that can be run and achieve good parallel performance.
Reference: [22] <author> T. von Eicken, D. Culler, S. Goldstein, and K. Schauser. </author> <title> Active Messages: a mechanism for integrated communication and computation. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <year> 1992. </year>
Reference-contexts: When invoked, the FM extract () function processes the pending received messages, dequeuing them and executing their handlers. FM send4 () is a specialized version for messages of no more than four words, optimized for latency. The FM interface is a generalization of the Active Message model <ref> [22] </ref>, in that there are no restrictions on the communication operations that can be carried out by the handler. (The user is responsible for avoiding deadlock situations.) Reliable and in-order delivery guarantees can be expensive if implemented in the upper messaging layers [13].
Reference: [23] <author> Thorsten von Eicken, Anindya Basu, Vineet Buch, and Werner Vogels. U-Net: </author> <title> A user-level network interface for parallel and distributed computing. </title> <booktitle> In Proceedings of the 15th ACM Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1995. </year> <note> Available from http://www.cs.cornell.edu/Info/Projects/ATM/sosp.ps. 27 </note>
Reference-contexts: U-Net <ref> [23] </ref> is built using FORE ATM interface cards. It presents an AAL5 programming interface to the applications.
References-found: 23

