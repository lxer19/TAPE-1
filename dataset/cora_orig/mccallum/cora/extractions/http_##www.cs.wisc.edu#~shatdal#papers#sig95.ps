URL: http://www.cs.wisc.edu/~shatdal/papers/sig95.ps
Refering-URL: http://www.cs.wisc.edu/~shatdal/shatdal.html
Root-URL: 
Email: fshatdal,naughtong@cs.wisc.edu  
Title: Adaptive Parallel Aggregation Algorithms  
Author: Ambuj Shatdal Jeffrey F. Naughton 
Affiliation: Computer Sciences Department University of Wisconsin-Madison  
Abstract: Aggregation and duplicate removal are common in SQL queries. However, in the parallel query processing literature, aggregate processing has received surprisingly little attention; furthermore, for each of the traditional parallel aggregation algorithms, there is a range of grouping selectivities where the algorithm performs poorly. In this work, we propose new algorithms that dynamically adapt, at query evaluation time, in response to observed grouping selectivities. Performance analysis via analytical modeling and an implementation on a workstation-cluster shows that the proposed algorithms are able to perform well for all grouping selectivities. Finally, we study the effect of data skew and show that for certain data sets the proposed algorithms can even outperform the best of traditional approaches. 
Abstract-found: 1
Intro-found: 1
Reference: [BBDW83] <author> D. Bitton, H. Boral, et al. </author> <title> Parallel algorithms for the execution of relational database operations. </title> <journal> ACM Trans. on Database Systems, </journal> <volume> 8(3), </volume> <month> Sep. </month> <year> 1983. </year>
Reference-contexts: This aspect of the algorithms allows them to proceed adaptively without any global synchronization. As mentioned, there has been little work reported in literature on aggregate processing. Epstein [Eps79] discusses some algorithms for computing scalar aggregates and aggregate functions on a uniprocessor. Bitton et al. <ref> [BBDW83] </ref> discuss two sorting based algorithms for aggregate processing on a shared disk cache architecture. The first algorithm is somewhat similar to the Two Phase approach mentioned above in that it uses local aggregation.
Reference: [BCL93] <author> K. P. Brown, M. J. Carey, and M. Livny. </author> <title> Managing Memory to Meet Multiclass Workload Response Time Goals. </title> <booktitle> In Proc. of 19th VLDB Conf., </booktitle> <year> 1993. </year>
Reference-contexts: We assume that the aggregation is being performed directly on a base relation stored on disks as in the example query. The parameters of the study are listed in Table 1 unless otherwise specified. These parameters are similar to those in previous studies e.g. <ref> [BCL93] </ref>. The CPU speed is chosen to reflect the characteristics of the current generation of commercially available microprocessors. The I/O rate was as observed on the SUN disk on the SUN SparcServer20/51.
Reference: [BF93] <author> J. Bunge and M. Fitzpatrick. </author> <title> Estimating the Number of Species: A Review. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 88(421), </volume> <month> March </month> <year> 1993. </year>
Reference-contexts: This does not require an accurate estimate of the number of groups, especially when it is large, making the problem significantly simpler than the general estimation problem which is fairly complex <ref> [BF93] </ref>. The basic scheme is as follows. First the optimizer will decide what is an appropriate switching point of the algorithms depending on the system characteristics.
Reference: [DGS + 90] <author> D. DeWitt, S. Ghandeharizadeh, et al. </author> <title> The Gamma database machine project. </title> <journal> IEEE Trans. on Knowledge and Data Engineering, </journal> <volume> 2(1), </volume> <month> March </month> <year> 1990. </year>
Reference-contexts: Further, we assume a Gamma <ref> [DGS + 90] </ref> like architecture where each relational operation is represented by operators. The data "flows" through the operators in a pipelined fashion as far as possible. For example, a join of two base relations is implemented as two select operators followed by a join operator. <p> The overflow buckets are processed one by one as in step 1 above. In the following sections we briefly describe the three traditional approaches to parallel aggregation. The first being the Centralized Two Phase algorithm (C-2P) <ref> [DGS + 90] </ref>. 2.1 Centralized Two Phase Algorithm The most simple and common approach for parallel aggregation is for each node to do aggregation on the locally generated tuples in phase one and then merge these local aggregate values at a central coordinator in phase two.
Reference: [Eps79] <author> R. Epstein. </author> <title> Techniques for Processing of Aggregates in Relational Database Systems. </title> <institution> Memo UCB/ERL M79/8, E.R.L., College of Eng., Univ. of Calif., Berkeley, </institution> <month> Feb. </month> <year> 1979. </year>
Reference-contexts: This aspect of the algorithms allows them to proceed adaptively without any global synchronization. As mentioned, there has been little work reported in literature on aggregate processing. Epstein <ref> [Eps79] </ref> discusses some algorithms for computing scalar aggregates and aggregate functions on a uniprocessor. Bitton et al. [BBDW83] discuss two sorting based algorithms for aggregate processing on a shared disk cache architecture.
Reference: [ER61] <author> P. Erdos and A. Renyi. </author> <title> On a Classical Problem of Probability Theory. </title> <journal> MTA Mat. Kut. Int. Kozl, </journal> <volume> 6A, </volume> <year> 1961. </year> <note> Also in Selected Papers of A. </note> <editor> Renyi, v. </editor> <volume> 2, </volume> <publisher> Akademiai Kiado, </publisher> <address> Budapest. </address>
Reference-contexts: This trade-off is explored later. It can be shown that the number of samples required is fairly small (about 10 times the crossover threshold) <ref> [ER61] </ref>. For example, for a crossover threshold of 320 (assuming 32 processor and 10 times as many groups) this is approximately 2563. This is likely to be less than 1% of any reasonably sized relation for small crossover thresholds.
Reference: [Gra93] <author> G. Graefe. </author> <title> Query Evaluation Techniques for Large Databases. </title> <journal> ACM Computing Surveys, </journal> <volume> 25(2), </volume> <month> June </month> <year> 1993. </year>
Reference-contexts: This is impractical on today's multiprocessor interconnects, which do not efficiently support broadcasting. Su et al. [SM82] discuss an implementation of the traditional approach. Graefe <ref> [Gra93] </ref> discusses one optimization for dealing with bucket overflow for the Two Phase algorithm. The rest of the paper is organized as follows. Section 2 introduces aggregation, the traditional approaches to aggregate processing and summarizes their performance characteristics. <p> ) fl G=P fl 2 fl IO * generating final result: jGj fl S g fl t w * I/O cost for storing result: (G fl S g =P ) fl IO The sequential bottleneck can be overcome by paral-lelizing the merging phase resulting in the Two Phase algorithm (2P) <ref> [Gra93] </ref>. 2.2 Two Phase Aggregation The algorithm is similar to above, except that the merging phase is parallelized by hash-partitioning on the GROUP BY attribute. Each node aggregates the locally generated tuples. The local aggregate values are then hash-partitioned and the nodes merge these local aggregate values in parallel. <p> The hash table will have the final aggregated values for all group values hashing to that node. Here we must mention that <ref> [Gra93] </ref> points out another optimization to the Two Phase algorithm. It suggests that in the local aggregation phase, if the hash table is full then the locally generated tuples are hash partitioned and forwarded to the local aggregation phase.
Reference: [Oak93] <institution> Oak Ridge National Lab. </institution> <note> PVM 3 User's Guide and Reference Manual, </note> <month> May </month> <year> 1993. </year>
Reference-contexts: We implemented the algorithms on top of the UNIX file system using the PVM parallel library <ref> [Oak93] </ref>. Our implementation had no concurrency control and did not use slotted pages. Hence the algorithms are significantly more CPU efficient than what would be found in a complete database system. The 2 Million 100 byte tuples were partitioned in a round-robin fashion.
Reference: [Ses92] <author> S. Seshadri. </author> <title> Probabilistic Methods in Query Process ing. </title> <type> PhD thesis, </type> <institution> Univ. of Wisconsin-Madison, Computer Sciences Department, </institution> <year> 1992. </year>
Reference-contexts: Unfortunately, the number of groups is not usually known beforehand. Sampling has been used effectively in past to estimate DBMS parameters <ref> [Ses92] </ref>. The general problem of accurately estimating the number of groups is similar to the projection estimation problem. <p> Page-oriented random sampling has been shown to be quite effective if there is no correlation between tuples in a page <ref> [Ses92] </ref>. Then the tuples in the sampled pages are aggregated using, possibly, the Centralized Two Phase algorithm. The number of groups obtained from the sample provides a lower bound on the number of groups in the relation.
Reference: [SM82] <author> S. Y. W. Su and K. P. Mikkilineni. </author> <booktitle> Parallel Algorithms and Their Implentation in MICRONET. In Proc. of 8th VLDB Conf., </booktitle> <year> 1982. </year>
Reference-contexts: This is impractical on today's multiprocessor interconnects, which do not efficiently support broadcasting. Su et al. <ref> [SM82] </ref> discuss an implementation of the traditional approach. Graefe [Gra93] discusses one optimization for dealing with bucket overflow for the Two Phase algorithm. The rest of the paper is organized as follows. Section 2 introduces aggregation, the traditional approaches to aggregate processing and summarizes their performance characteristics.
Reference: [TPC94] <author> TPC. </author> <title> TPC Benchmark TM D (Decision Support). Working draft 6.5, Transaction Processing Performance Council, </title> <month> Feb. </month> <year> 1994. </year>
Reference-contexts: 1 Introduction SQL queries are replete with aggregate and duplicate elimination operations. One measure of the perceived importance of aggregation is that in the proposed TPC-D benchmark <ref> [TPC94] </ref> 15 out of 17 queries contain aggregate operations. Yet we find that aggregate processing is an issue almost totally ignored by the researchers in the parallel database community.
Reference: [WDJ91] <author> C. B. Walton, A. G. Dale, and R. M. Jenevein. </author> <title> A taxonomy and performance model of data skew effects in parallel joins. </title> <booktitle> In Proc. of the 17th VLDB Conf., </booktitle> <year> 1991. </year> <month> 11 </month>
Reference-contexts: The number of tuples/node is same but number of groups/node is different. We call this output skew. This can be contrasted with data skew in the join algorithms, where placement skew is analogous to input skew and join product skew is analogous to output skew <ref> [WDJ91] </ref>. However, since hash partitioning is not necessary for aggregate processing, the effect of data skew is significantly different. The data skew, in essence, results in one or more nodes having to do more work than the under uniform distribution case.
References-found: 12

