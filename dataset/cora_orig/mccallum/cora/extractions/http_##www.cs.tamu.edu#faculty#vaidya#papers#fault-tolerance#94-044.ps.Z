URL: http://www.cs.tamu.edu/faculty/vaidya/papers/fault-tolerance/94-044.ps.Z
Refering-URL: http://www.cs.tamu.edu/faculty/vaidya/Vaidya-ftc.html
Root-URL: http://www.cs.tamu.edu
Email: E-mail: vaidya@cs.tamu.edu  
Phone: Phone: 409-845-0512 Fax: 409-847-8578  
Title: Some Thoughts on Distributed Recovery (preliminary version) recovery line. Relaxing the definition of orphan messages.
Author: Nitin H. Vaidya 
Note: logical  
Address: College Station, TX 77843-3112  
Affiliation: Department of Computer Science Texas A&M University  
Abstract: Technical Report 94-044 June 1994 Abstract This report deals with some aspects of distributed recovery. The report is divided into multiple parts, each part introducing a problem and a solution. The intent of this report is to present a medley of preliminary ideas, more detailed treatment may be presented elsewhere. The report deals with the following problems: * A single processor failure tolerance scheme based on the distributed recovery unit abstraction. * Staggered checkpointing and coordinated message logging to obtain a consistent 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Alvisi, B. Hoppe, and K. Marzullo, </author> <title> "Nonblocking and orphan-free message logging protocols," </title> <booktitle> in Digest of papers: The 23 rd Int. Symp. Fault-Tolerant Comp., </booktitle> <pages> pp. 145-154, </pages> <year> 1993. </year>
Reference-contexts: To this we would like to add one more non-deterministic event, namely, delivery of a message sent by the recovery unit to itself. The reason will be clearer shortly. Consider the single fault tolerance schemes proposed by Johnson [7] and Alvisi et al. <ref> [1] </ref>. Each of these schemes can tolerate failure of a single recovery unit. When multiple processes are scheduled on a single processor, failure of the processor will cause failure of all processes on that processor, not just one. <p> When a process sends a message to another process within the same DRU, it can be interpreted as a message from the DRU to itself. The schemes by Johnson [7] and Alvisi et al. <ref> [1] </ref> are not designed to handle such messages. 4 3 Suggested Solution In this section, we present a single processor failure tolerance scheme that treats the collection of all processes on a processor as a single distributed recovery unit. <p> This scheme is an extension of Johnson's sender based message logging scheme [7]. Similar modifications can also be made to the scheme presented by Alvisi et al. <ref> [1] </ref>. The recovery scheme presented here is also closely related to Elnozahy and Zwaenepoel [3, 4]. Essentially, each DRU uses Elnozahy' scheme for keeping track of intra-DRU events (using antecedence graph), the single processor fault assumption being used to allow volatile logging of intra-DRU events as well as inter-DRU messages. <p> This scheme is based on single fault tolerance schemes presented by Alvisi et al. <ref> [1] </ref> and Johnson and Zwaenepoel [7]. Although the proposed scheme may seem very expensive, for k 3, an efficient implementation of this scheme on wormhole-routed networks is possible. The implementation of the recovery scheme is sketched subsequently. <p> Another alternative is to allow the process to send a message before the acknowledgement is received, but tag the RSN information to these messages as well. This is similar to the single fault tolerance scheme presented by Alvisi et al. <ref> [1] </ref>. 11.4 Recovery Protocol Each faulty process can recover independently. When a failure occurs, a faulty process P rolls back to its most recently saved checkpoint.
Reference: [2] <author> K. M. Chandy and L. Lamport, </author> <title> "Distributed snapshots: Determining global states in distributed systems," </title> <journal> ACM Trans. Comp. Syst., </journal> <volume> vol. 3, </volume> <pages> pp. 63-75, </pages> <month> February </month> <year> 1985. </year> <month> 24 </month>
Reference-contexts: Plank [10] shows that staggering indeed reduces the overhead significantly for many applications. Here, we present a simple alternative for coordinated checkpointing that allows arbitrary staggering of checkpoints. The solution presented below is closely related to <ref> [2, 6, 10, 14] </ref>, as discussed later. 7 Suggested Solution The solution suggested here can be summarized as follows: staggered checkpoints + coordinated message logging = consistent logical checkpoints The basic idea is to coordinate logical checkpoints [14, 15] rather than physical checkpoints. <p> When the coordinator receives acknowledgement messages from all the processes indicating that they have taken a checkpoint, the coordinator initiates the consistent message logging phase of the algorithm. In this phase, any coordinated checkpointing algorithm can be used, for example, Chandy and Lamport <ref> [2] </ref>. The only difference is that when the original algorithm requires a process to take a physical checkpoint, our processes instead take a logical checkpoint by logging the relevant messages received since the physical checkpoint taken in the previous step. <p> a failure, each process rolls back to its recent physical checkpoint and reexecutes (using the logged messages) to restore the process state to the logical checkpoint that is a part of the most recent consistent recovery line. 8 Relation to Existing Work The algorithm presented above is closely related to <ref> [2, 6, 10, 14] </ref>. Our algorithm is designed to bound the rollback distance, similar to the traditional coordinated checkpointing algorithms. It may be noted that, after a failure, a process rolls back to a physical checkpoint and then executes to restore a logical checkpoint. <p> Additionally, Johnson's algorithm can result in all messages being logged (as processes may log messages asynchronously), our algorithm logs messages only until the consistent message logging phase is completed. Plank [10] presents two coordinated checkpointing algorithms (based on Chandy and Lamport <ref> [2] </ref>) that attempt to stagger the checkpoints. However, it is possible that some checkpoints taken by these algorithms cannot be staggered. In contrast, our algorithm allows arbitrary staggering of the checkpoints.
Reference: [3] <author> E. N. Elnozahy and W. Zwaenepoel, "Manetho: </author> <title> Transparent rollback-recovery with low overhead, limited rollback, and fast output commit," </title> <journal> IEEE Trans. Computers, </journal> <volume> vol. 41, </volume> <month> May </month> <year> 1992. </year>
Reference-contexts: A comparison of proposed recovery schemes with existing work is presented in the report. 2 Part I 3 2 Single Processor Fault Tolerance Using DRUs [12] The research on message logging schemes for achieving distributed recovery assumes that the system consists of a collection of recovery units <ref> [3, 11] </ref> that are deterministic. The execution of such a recovery unit consists of a sequence of piecewise deterministic state intervals, each started by a non-deterministic event. <p> Such an event can be (i) receipt of a message from another recovery unit, (ii) an internal non-deterministic event such as a kernel call, or (iii) creation of the recovery unit <ref> [3] </ref>. To this we would like to add one more non-deterministic event, namely, delivery of a message sent by the recovery unit to itself. The reason will be clearer shortly. Consider the single fault tolerance schemes proposed by Johnson [7] and Alvisi et al. [1]. <p> This scheme is an extension of Johnson's sender based message logging scheme [7]. Similar modifications can also be made to the scheme presented by Alvisi et al. [1]. The recovery scheme presented here is also closely related to Elnozahy and Zwaenepoel <ref> [3, 4] </ref>. Essentially, each DRU uses Elnozahy' scheme for keeping track of intra-DRU events (using antecedence graph), the single processor fault assumption being used to allow volatile logging of intra-DRU events as well as inter-DRU messages. <p> Along with the checkpoints, the intra-DRU messages that were sent before the recovery line but delivered after the recovery line are also logged on the stable storage. Sender based message logging: This is similar to Johnson [7] with one important modification. Each DRU maintains an antecedence graph <ref> [3] </ref> containing only the events internal to the DRU, including delivery of intra-DRU messages. Specifically, the antecedence graph does not contain events corresponding to inter-DRU messages. <p> The antecedence graph logged when committing the most recent output may also be needed to recover from the failure, if no inter-DRU message was logged subsequently. The above recovery procedure is essentially identical to that presented in <ref> [3] </ref> if the inter-DRU messages are treated as input/output (during recovery only). 4 Relation to Existing Work The approach presented above is a combination of the recovery schemes presented by Johnson [7] and Elnozahy and Zwaenepoel [4]. [4] presents a recovery scheme that uses coordinated checkpointing as well as logs the <p> It uses the techniques presented previously in <ref> [3, 4, 7, 12] </ref> to obtain a useful recovery scheme. An advantage of this scheme is that the antecedence graphs do not become bigger with the size of the system, as only intra-DRU events are included in the antecedence graphs.
Reference: [4] <author> E. N. Elnozahy and W. Zwaenepoel, </author> <title> "On the use and implementation of message logging," </title> <booktitle> in Digest of papers: The 24 th Int. Symp. Fault-Tolerant Comp., </booktitle> <pages> pp. 298-307, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: This scheme is an extension of Johnson's sender based message logging scheme [7]. Similar modifications can also be made to the scheme presented by Alvisi et al. [1]. The recovery scheme presented here is also closely related to Elnozahy and Zwaenepoel <ref> [3, 4] </ref>. Essentially, each DRU uses Elnozahy' scheme for keeping track of intra-DRU events (using antecedence graph), the single processor fault assumption being used to allow volatile logging of intra-DRU events as well as inter-DRU messages. <p> The above recovery procedure is essentially identical to that presented in [3] if the inter-DRU messages are treated as input/output (during recovery only). 4 Relation to Existing Work The approach presented above is a combination of the recovery schemes presented by Johnson [7] and Elnozahy and Zwaenepoel <ref> [4] </ref>. [4] presents a recovery scheme that uses coordinated checkpointing as well as logs the antecedence graph. Thus, the above system can be viewed as a collection of DRUs, each DRU using Elnozahy and Zwaenepoel [4] internally (to recover intra-DRU messages) and Johnson and Zwaenepoel [7] externally (to recover inter-DRU messages). <p> The above recovery procedure is essentially identical to that presented in [3] if the inter-DRU messages are treated as input/output (during recovery only). 4 Relation to Existing Work The approach presented above is a combination of the recovery schemes presented by Johnson [7] and Elnozahy and Zwaenepoel <ref> [4] </ref>. [4] presents a recovery scheme that uses coordinated checkpointing as well as logs the antecedence graph. Thus, the above system can be viewed as a collection of DRUs, each DRU using Elnozahy and Zwaenepoel [4] internally (to recover intra-DRU messages) and Johnson and Zwaenepoel [7] externally (to recover inter-DRU messages). <p> is a combination of the recovery schemes presented by Johnson [7] and Elnozahy and Zwaenepoel <ref> [4] </ref>. [4] presents a recovery scheme that uses coordinated checkpointing as well as logs the antecedence graph. Thus, the above system can be viewed as a collection of DRUs, each DRU using Elnozahy and Zwaenepoel [4] internally (to recover intra-DRU messages) and Johnson and Zwaenepoel [7] externally (to recover inter-DRU messages). Single DRU failure assumption allows the antecedence graph of each DRU to be logged at any other DRU. <p> It uses the techniques presented previously in <ref> [3, 4, 7, 12] </ref> to obtain a useful recovery scheme. An advantage of this scheme is that the antecedence graphs do not become bigger with the size of the system, as only intra-DRU events are included in the antecedence graphs. <p> The issue of dynamically changing the membership of processes to DRUs also needs to be investigated [12]. 8 Part II 9 6 Completely Staggered Checkpointing The problem of reducing the overhead of checkpointing in multicomputers has received considerable attention (e.g., <ref> [10, 4] </ref>). Plank [10] presented different techniques to reduce this overhead. One of the suggested techniques is staggering of checkpoints. When processes in a system coordinate their checkpoints, they tend to take the checkpoints at about the same time.
Reference: [5] <author> D. B. Johnson, </author> <title> Distributed System Fault Tolerance Using Message Logging and Check-pointing. </title> <type> PhD thesis, </type> <institution> Computer Science, Rice University, </institution> <month> December </month> <year> 1989. </year>
Reference-contexts: One consequence of this is that we do not need to log all messages, only those message are logged which make the logical checkpoints consistent. 13 Part III 14 9 Relaxing the Definition of Orphan Messages To our knowledge, all message logging protocols require that the receive sequence number <ref> [5] </ref> of a message must be logged before the message is considered to be fully logged. Our intent here is to show that this is not a necessary condition to be able to achieve recovery, but a performance optimization.
Reference: [6] <author> D. B. Johnson, </author> <title> "Efficient transparent optimistic rollback recovery for distributed application programs," </title> <booktitle> in Symposium on Reliable Distributed Systems, </booktitle> <pages> pp. 86-95, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: Plank [10] shows that staggering indeed reduces the overhead significantly for many applications. Here, we present a simple alternative for coordinated checkpointing that allows arbitrary staggering of checkpoints. The solution presented below is closely related to <ref> [2, 6, 10, 14] </ref>, as discussed later. 7 Suggested Solution The solution suggested here can be summarized as follows: staggered checkpoints + coordinated message logging = consistent logical checkpoints The basic idea is to coordinate logical checkpoints [14, 15] rather than physical checkpoints. <p> a failure, each process rolls back to its recent physical checkpoint and reexecutes (using the logged messages) to restore the process state to the logical checkpoint that is a part of the most recent consistent recovery line. 8 Relation to Existing Work The algorithm presented above is closely related to <ref> [2, 6, 10, 14] </ref>. Our algorithm is designed to bound the rollback distance, similar to the traditional coordinated checkpointing algorithms. It may be noted that, after a failure, a process rolls back to a physical checkpoint and then executes to restore a logical checkpoint. <p> It may be noted that, after a failure, a process rolls back to a physical checkpoint and then executes to restore a logical checkpoint. Thus, the overhead of recovery (or rollback distance) is determined by when physical checkpoints are taken. 3 Johnson <ref> [6] </ref> suggested a scheme where each process uses a similar heuristic to decide whether to log messages or not. 12 Johnson [6] presents an algorithm that forces the processes to log message on the stable storage or to take a physical checkpoint. <p> Thus, the overhead of recovery (or rollback distance) is determined by when physical checkpoints are taken. 3 Johnson <ref> [6] </ref> suggested a scheme where each process uses a similar heuristic to decide whether to log messages or not. 12 Johnson [6] presents an algorithm that forces the processes to log message on the stable storage or to take a physical checkpoint. The goal of his algorithm is to make the state of a single process committable (primarily, to allow it to commit an output).
Reference: [7] <author> D. B. Johnson and W. Zwaenepoel, </author> <title> "Sender-based message logging," </title> <booktitle> in Digest of papers: The 17 th Int. Symp. Fault-Tolerant Comp., </booktitle> <pages> pp. 14-19, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: Accordingly, the presentation is informal and no correctness proofs are presented. The report deals with the following problems: * A single processor failure tolerance scheme based on the distributed recovery unit abstraction. This scheme is derived from a single process failure tolerance scheme <ref> [7] </ref>, but tolerates simultaneous failure of all processes on a single processor. * Staggered checkpointing and coordinated message logging to obtain a consistent logical recovery line. Checkpoint staggering is useful to reduce the overhead of a recovery scheme [10]. <p> To this we would like to add one more non-deterministic event, namely, delivery of a message sent by the recovery unit to itself. The reason will be clearer shortly. Consider the single fault tolerance schemes proposed by Johnson <ref> [7] </ref> and Alvisi et al. [1]. Each of these schemes can tolerate failure of a single recovery unit. When multiple processes are scheduled on a single processor, failure of the processor will cause failure of all processes on that processor, not just one. <p> When a process sends a message to another process within the same DRU, it can be interpreted as a message from the DRU to itself. The schemes by Johnson <ref> [7] </ref> and Alvisi et al. [1] are not designed to handle such messages. 4 3 Suggested Solution In this section, we present a single processor failure tolerance scheme that treats the collection of all processes on a processor as a single distributed recovery unit. <p> This scheme is an extension of Johnson's sender based message logging scheme <ref> [7] </ref>. Similar modifications can also be made to the scheme presented by Alvisi et al. [1]. The recovery scheme presented here is also closely related to Elnozahy and Zwaenepoel [3, 4]. <p> Along with the checkpoints, the intra-DRU messages that were sent before the recovery line but delivered after the recovery line are also logged on the stable storage. Sender based message logging: This is similar to Johnson <ref> [7] </ref> with one important modification. Each DRU maintains an antecedence graph [3] containing only the events internal to the DRU, including delivery of intra-DRU messages. Specifically, the antecedence graph does not contain events corresponding to inter-DRU messages. <p> The antecedence graph is purged each time a message is received by a process in the DRU, as described below. When an intra-DRU message is delivered, the antecedence graph is updated. The intra-DRU messages need not be logged (except those logged during the checkpointing step described above). Johnson <ref> [7] </ref> logs a message and its receive sequence number (RSN) in the volatile storage of the sender. We modify his protocol as follows: When an inter-DRU message is delivered to a process, it sends the RSN of the message to the message sender. <p> The above recovery procedure is essentially identical to that presented in [3] if the inter-DRU messages are treated as input/output (during recovery only). 4 Relation to Existing Work The approach presented above is a combination of the recovery schemes presented by Johnson <ref> [7] </ref> and Elnozahy and Zwaenepoel [4]. [4] presents a recovery scheme that uses coordinated checkpointing as well as logs the antecedence graph. Thus, the above system can be viewed as a collection of DRUs, each DRU using Elnozahy and Zwaenepoel [4] internally (to recover intra-DRU messages) and Johnson and Zwaenepoel [7] <p> <ref> [7] </ref> and Elnozahy and Zwaenepoel [4]. [4] presents a recovery scheme that uses coordinated checkpointing as well as logs the antecedence graph. Thus, the above system can be viewed as a collection of DRUs, each DRU using Elnozahy and Zwaenepoel [4] internally (to recover intra-DRU messages) and Johnson and Zwaenepoel [7] externally (to recover inter-DRU messages). Single DRU failure assumption allows the antecedence graph of each DRU to be logged at any other DRU. Our approach can be viewed as combining two recovery schemes to obtain a hybrid recovery scheme [12]. <p> It uses the techniques presented previously in <ref> [3, 4, 7, 12] </ref> to obtain a useful recovery scheme. An advantage of this scheme is that the antecedence graphs do not become bigger with the size of the system, as only intra-DRU events are included in the antecedence graphs. <p> This scheme is based on single fault tolerance schemes presented by Alvisi et al. [1] and Johnson and Zwaenepoel <ref> [7] </ref>. Although the proposed scheme may seem very expensive, for k 3, an efficient implementation of this scheme on wormhole-routed networks is possible. The implementation of the recovery scheme is sketched subsequently. To simplify our discussion assume that only one process is scheduled on each processor.
Reference: [8] <author> J. H. Kim, Z. Liu, and A. A. Chien, </author> <title> "Compressionless routing: A framework for adaptive and fault-tolerant routing," </title> <booktitle> in Int. Symp. Comp. Arch., </booktitle> <pages> pp. 289-300, </pages> <year> 1994. </year>
Reference-contexts: It is possible to conceive implementation of the proposed algorithm using various wormhole routing protocols. The discussion in this report, however, utilizes a variant of the the fault-tolerant compressionless routing (FCR) recently proposed by Kim et al. <ref> [8] </ref>. The next section briefly describes FCR. The subsequent sections describe the routing protocol used by our algorithm, and the proposed implementation of the k-fault tolerance algorithm. 19 11.1 Fault-Tolerant Compressionless Routing [8] FCR tolerates failures in routing, i.e., a message is delivered to its destination in spite of few link <p> report, however, utilizes a variant of the the fault-tolerant compressionless routing (FCR) recently proposed by Kim et al. <ref> [8] </ref>. The next section briefly describes FCR. The subsequent sections describe the routing protocol used by our algorithm, and the proposed implementation of the k-fault tolerance algorithm. 19 11.1 Fault-Tolerant Compressionless Routing [8] FCR tolerates failures in routing, i.e., a message is delivered to its destination in spite of few link (or router) failures. FCR is not designed to recover from processor failures, however. FCR is used to send messages between the nodes.
Reference: [9] <author> A. Lowry, J. R. Russell, and A. P. Goldberg, </author> <title> "Optimistic failure recovery for very large networks," </title> <booktitle> in Symposium on Reliable Distributed Systems, </booktitle> <pages> pp. 66-75, </pages> <year> 1991. </year>
Reference-contexts: Single DRU failure assumption allows the antecedence graph of each DRU to be logged at any other DRU. Our approach can be viewed as combining two recovery schemes to obtain a hybrid recovery scheme [12]. Lowry et al. <ref> [9] </ref> propose an approach for hierarchical implementations, wherein different clusters of recovery units internally use different recovery schemes. The messages between different clusters are sent through interface recovery units that facilitate optimistic message passing between the clusters. Although our approach as well as [9] 7 partition the processes into clusters (or <p> Lowry et al. <ref> [9] </ref> propose an approach for hierarchical implementations, wherein different clusters of recovery units internally use different recovery schemes. The messages between different clusters are sent through interface recovery units that facilitate optimistic message passing between the clusters. Although our approach as well as [9] 7 partition the processes into clusters (or DRUs), the two approaches are complementary to each other (not identical). 5 Summary The recovery scheme presented above tolerates the failure of a single processor (independent of how many processes are scheduled on the processor).
Reference: [10] <author> J. S. Plank, </author> <title> Efficient Checkpointing on MIMD Architectures. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Princeton University, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: This scheme is derived from a single process failure tolerance scheme [7], but tolerates simultaneous failure of all processes on a single processor. * Staggered checkpointing and coordinated message logging to obtain a consistent logical recovery line. Checkpoint staggering is useful to reduce the overhead of a recovery scheme <ref> [10] </ref>. We present a simple approach that allows staggered checkpoints but bounds the rollback distance by coordinated message logging. * Relaxing the definition of orphan messages. We claim that the commonly used defini tion of orphan messages can be relaxed. <p> The issue of dynamically changing the membership of processes to DRUs also needs to be investigated [12]. 8 Part II 9 6 Completely Staggered Checkpointing The problem of reducing the overhead of checkpointing in multicomputers has received considerable attention (e.g., <ref> [10, 4] </ref>). Plank [10] presented different techniques to reduce this overhead. One of the suggested techniques is staggering of checkpoints. When processes in a system coordinate their checkpoints, they tend to take the checkpoints at about the same time. <p> The issue of dynamically changing the membership of processes to DRUs also needs to be investigated [12]. 8 Part II 9 6 Completely Staggered Checkpointing The problem of reducing the overhead of checkpointing in multicomputers has received considerable attention (e.g., [10, 4]). Plank <ref> [10] </ref> presented different techniques to reduce this overhead. One of the suggested techniques is staggering of checkpoints. When processes in a system coordinate their checkpoints, they tend to take the checkpoints at about the same time. <p> This can result in severe performance degradation in multicomputers where the I/O bandwidth is not adequate. Staggering techniques suggested by Plank alleviate this problem to some extent. However, his algorithm cannot stagger the checkpoints arbitrarily, that is, some checkpoints cannot be staggered without blocking some processes. Plank <ref> [10] </ref> shows that staggering indeed reduces the overhead significantly for many applications. Here, we present a simple alternative for coordinated checkpointing that allows arbitrary staggering of checkpoints. <p> Plank [10] shows that staggering indeed reduces the overhead significantly for many applications. Here, we present a simple alternative for coordinated checkpointing that allows arbitrary staggering of checkpoints. The solution presented below is closely related to <ref> [2, 6, 10, 14] </ref>, as discussed later. 7 Suggested Solution The solution suggested here can be summarized as follows: staggered checkpoints + coordinated message logging = consistent logical checkpoints The basic idea is to coordinate logical checkpoints [14, 15] rather than physical checkpoints. <p> The above algorithm reduces the contention for the stable storage by completely staggering the physical checkpoints. However, contention is now introduced in the second step of the algorithm when the processes coordinate message logging. This contention can be reduced by using the limited staggering techniques proposed in <ref> [10] </ref>. The proposed scheme will perform well if message volume is relatively small compared to checkpoint sizes. <p> a failure, each process rolls back to its recent physical checkpoint and reexecutes (using the logged messages) to restore the process state to the logical checkpoint that is a part of the most recent consistent recovery line. 8 Relation to Existing Work The algorithm presented above is closely related to <ref> [2, 6, 10, 14] </ref>. Our algorithm is designed to bound the rollback distance, similar to the traditional coordinated checkpointing algorithms. It may be noted that, after a failure, a process rolls back to a physical checkpoint and then executes to restore a logical checkpoint. <p> Additionally, Johnson's algorithm can result in all messages being logged (as processes may log messages asynchronously), our algorithm logs messages only until the consistent message logging phase is completed. Plank <ref> [10] </ref> presents two coordinated checkpointing algorithms (based on Chandy and Lamport [2]) that attempt to stagger the checkpoints. However, it is possible that some checkpoints taken by these algorithms cannot be staggered. In contrast, our algorithm allows arbitrary staggering of the checkpoints.
Reference: [11] <author> R. E. Strom and S. A. Yemini, </author> <title> "Optimistic recovery: An asynchronous approach to fault-tolerance in distributed systems," </title> <booktitle> Digest of papers: The 14 th Int. Symp. Fault-Tolerant Comp., </booktitle> <pages> pp. 374-379, </pages> <year> 1984. </year>
Reference-contexts: A comparison of proposed recovery schemes with existing work is presented in the report. 2 Part I 3 2 Single Processor Fault Tolerance Using DRUs [12] The research on message logging schemes for achieving distributed recovery assumes that the system consists of a collection of recovery units <ref> [3, 11] </ref> that are deterministic. The execution of such a recovery unit consists of a sequence of piecewise deterministic state intervals, each started by a non-deterministic event.
Reference: [12] <author> N. H. Vaidya, </author> <title> "Distributed recovery units: An approach for hybrid and adaptive distributed recovery," </title> <type> Tech. Rep. 93-052, </type> <institution> Computer Science Department, Texas A&M University, College Station, </institution> <month> November </month> <year> 1993. </year> <month> 25 </month>
Reference-contexts: A comparison of proposed recovery schemes with existing work is presented in the report. 2 Part I 3 2 Single Processor Fault Tolerance Using DRUs <ref> [12] </ref> The research on message logging schemes for achieving distributed recovery assumes that the system consists of a collection of recovery units [3, 11] that are deterministic. The execution of such a recovery unit consists of a sequence of piecewise deterministic state intervals, each started by a non-deterministic event. <p> This implies that, for the above schemes to be useful, the collection of processes on a processor should together be considered a recovery unit. To differentiate this from traditional recovery unit definition, we name the collection a distributed recovery unit (DRU) <ref> [12] </ref>. This definition of a recovery unit has two related consequences: * A collection of processes is not deterministic, even if each process in the collection is deterministic. <p> Single DRU failure assumption allows the antecedence graph of each DRU to be logged at any other DRU. Our approach can be viewed as combining two recovery schemes to obtain a hybrid recovery scheme <ref> [12] </ref>. Lowry et al. [9] propose an approach for hierarchical implementations, wherein different clusters of recovery units internally use different recovery schemes. The messages between different clusters are sent through interface recovery units that facilitate optimistic message passing between the clusters. <p> It uses the techniques presented previously in <ref> [3, 4, 7, 12] </ref> to obtain a useful recovery scheme. An advantage of this scheme is that the antecedence graphs do not become bigger with the size of the system, as only intra-DRU events are included in the antecedence graphs. <p> Specifically, the DRU may contain processes on different processors, also, processes on the same processor may belong to different processors. The issue of dynamically changing the membership of processes to DRUs also needs to be investigated <ref> [12] </ref>. 8 Part II 9 6 Completely Staggered Checkpointing The problem of reducing the overhead of checkpointing in multicomputers has received considerable attention (e.g., [10, 4]). Plank [10] presented different techniques to reduce this overhead. One of the suggested techniques is staggering of checkpoints.
Reference: [13] <author> N. H. Vaidya, </author> <title> "A case for multi-level distributed recovery schemes," </title> <type> Tech. Rep. 94-043, </type> <institution> Computer Science Department, Texas A&M University, College Station, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: This scheme exploits wormhole routing to minimize the number of messages. It is designed to tolerate a small number of simultaneous failures. (Motivation for design of such schemes is presented in <ref> [13] </ref>.) The work presented here demonstrates that (i) there is potential for design of efficient recovery schemes for small number of failures, and (ii) it is possible to exploit hardware to improve performance of such schemes.
Reference: [14] <author> Y. Wang, Y. Huang, and W. K. Fuchs, </author> <title> "Progressive retry for software error recovery in distributed systems," </title> <booktitle> in Digest of papers: The 23 rd Int. Symp. Fault-Tolerant Comp., </booktitle> <pages> pp. 138-144, </pages> <year> 1993. </year>
Reference-contexts: Plank [10] shows that staggering indeed reduces the overhead significantly for many applications. Here, we present a simple alternative for coordinated checkpointing that allows arbitrary staggering of checkpoints. The solution presented below is closely related to <ref> [2, 6, 10, 14] </ref>, as discussed later. 7 Suggested Solution The solution suggested here can be summarized as follows: staggered checkpoints + coordinated message logging = consistent logical checkpoints The basic idea is to coordinate logical checkpoints [14, 15] rather than physical checkpoints. <p> The solution presented below is closely related to [2, 6, 10, 14], as discussed later. 7 Suggested Solution The solution suggested here can be summarized as follows: staggered checkpoints + coordinated message logging = consistent logical checkpoints The basic idea is to coordinate logical checkpoints <ref> [14, 15] </ref> rather than physical checkpoints. A physical checkpoint of a process is taken by saving the process state on the stable storage. A logical checkpoint is taken by logging all the message received by the process since its most recent physical checkpoint on the stable storage. <p> a failure, each process rolls back to its recent physical checkpoint and reexecutes (using the logged messages) to restore the process state to the logical checkpoint that is a part of the most recent consistent recovery line. 8 Relation to Existing Work The algorithm presented above is closely related to <ref> [2, 6, 10, 14] </ref>. Our algorithm is designed to bound the rollback distance, similar to the traditional coordinated checkpointing algorithms. It may be noted that, after a failure, a process rolls back to a physical checkpoint and then executes to restore a logical checkpoint. <p> Plank [10] presents two coordinated checkpointing algorithms (based on Chandy and Lamport [2]) that attempt to stagger the checkpoints. However, it is possible that some checkpoints taken by these algorithms cannot be staggered. In contrast, our algorithm allows arbitrary staggering of the checkpoints. Wang et al. <ref> [14, 15] </ref> introduce the notion of a logical checkpoint. [14, 15] determines a recovery line consisting of consistent logical checkpoints, after a failure occurs. This recovery line is used to recover from the failure. <p> However, it is possible that some checkpoints taken by these algorithms cannot be staggered. In contrast, our algorithm allows arbitrary staggering of the checkpoints. Wang et al. <ref> [14, 15] </ref> introduce the notion of a logical checkpoint. [14, 15] determines a recovery line consisting of consistent logical checkpoints, after a failure occurs. This recovery line is used to recover from the failure. Their goal is to determine the "latest" consistent recovery line using the information saved on the stable storage.
Reference: [15] <author> Y. M. Wang, A. Lowry, and W. K. Fuchs, </author> <title> "Consistent global checkpoints based on direct dependency tracking." To appear in Inform. Process. </title> <journal> Lett. </journal> <volume> 26 </volume>
Reference-contexts: The solution presented below is closely related to [2, 6, 10, 14], as discussed later. 7 Suggested Solution The solution suggested here can be summarized as follows: staggered checkpoints + coordinated message logging = consistent logical checkpoints The basic idea is to coordinate logical checkpoints <ref> [14, 15] </ref> rather than physical checkpoints. A physical checkpoint of a process is taken by saving the process state on the stable storage. A logical checkpoint is taken by logging all the message received by the process since its most recent physical checkpoint on the stable storage. <p> Plank [10] presents two coordinated checkpointing algorithms (based on Chandy and Lamport [2]) that attempt to stagger the checkpoints. However, it is possible that some checkpoints taken by these algorithms cannot be staggered. In contrast, our algorithm allows arbitrary staggering of the checkpoints. Wang et al. <ref> [14, 15] </ref> introduce the notion of a logical checkpoint. [14, 15] determines a recovery line consisting of consistent logical checkpoints, after a failure occurs. This recovery line is used to recover from the failure. <p> However, it is possible that some checkpoints taken by these algorithms cannot be staggered. In contrast, our algorithm allows arbitrary staggering of the checkpoints. Wang et al. <ref> [14, 15] </ref> introduce the notion of a logical checkpoint. [14, 15] determines a recovery line consisting of consistent logical checkpoints, after a failure occurs. This recovery line is used to recover from the failure. Their goal is to determine the "latest" consistent recovery line using the information saved on the stable storage.
References-found: 15

