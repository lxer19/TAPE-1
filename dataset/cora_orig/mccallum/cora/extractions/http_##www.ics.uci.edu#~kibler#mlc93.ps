URL: http://www.ics.uci.edu/~kibler/mlc93.ps
Refering-URL: http://www.ics.uci.edu/~kibler/
Root-URL: 
Email: pdatta@ics.uci.edu  kibler@ics.uci.edu  
Title: Concept Sharing: A Means to Improve Multi-Concept Learning  
Author: Piew Datta Dennis Kibler 
Address: Irvine, CA 92717-3425  Irvine, CA 92717-3425  
Affiliation: Information and Computer Science Dept. University of California, Irvine  Information and Computer Science Dept. University of California, Irvine  
Abstract: This paper describes several means for sharing between related concepts to improve learning in the same domain. The sharing comes in the form of substructures or possibly entire structures of previous concepts which may aid in learning other concepts. These substructures highlight useful information in the domain. Using two domains, we evaluate the effectiveness of concept sharing with respect to accuracy, concept size, search complexity, and noise resistance.
Abstract-found: 1
Intro-found: 1
Reference: <author> Branting, K. L. </author> <year> (1991). </year> <title> Exploiting the Complementarity of rules and precedents with reciprocity and fairness. </title> <booktitle> In Proceedings: Case-Based Reasoning Workshop. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> De Raedt, L., Bruynooghe, M. </author> <year> (1992). </year> <title> Interactive concept-learning and constructive induction by analogy. </title> <journal> Machine Learning, </journal> <volume> volume 8, no. </volume> <pages> 2. </pages> <address> Boston, MA: </address> <publisher> Kluwer Academic Publishers. </publisher>
Reference: <author> Falkenhainer, B., Forbus, K., Gentner D. </author> <year> (1986). </year> <title> The structure-mapping engine. </title> <type> (Technical Report no. </type> <institution> UIUCDCS-R-86-1275). Urbana, Illinois: University of Illinois at Urbana-Champaign, Department of Computer Science. </institution>
Reference: <author> Kolodner, J. L. </author> <year> (1989). </year> <title> Judging which is the "best" case for a case-based reasoner. </title> <booktitle> Proceedings: Case-Based Reasoning Workshop. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Markman, E. </author> <year> (1989). </year> <title> Categorization and naming in children. </title> <address> Cambridge, Massachusetts: </address> <publisher> The MIT Press. </publisher>
Reference: <author> Minton, S., Carbonell, J. G. </author> <year> (1987). </year> <title> Strategies for learning search control rules: an explanation-based approach. </title> <editor> In J. McDermott, </editor> <booktitle> ed.,Proceedings of the Tenth International Joint Conference on Artificial Intelligence. </booktitle> <address> Los Altos, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Creating intensional predicates often reduces the depth of the search when a good selection is chosen, but it also increases the branching factor of the search. Therefore, the trade-offs for generating intensional predicates with these rules must be considered. This is analogous to the utility problem <ref> (Minton & Carbonell, 1987) </ref> while learning macros for planning. In accordance with the idea of concept sharing, we developed the concept, clause, and conjunct rules. These rules facilitate M-FOCL's use of prior concepts to learn subsequent concepts.
Reference: <author> Mooney, R. J., Ourston, D. </author> <year> (1991). </year> <title> Constructive induction in theory refinement. </title> <editor> In Birnbaum, Collins, eds., </editor> <booktitle> Proceedings of the Eighth International Workshop on Machine Learning. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: In addition, M-FOCL differs from these systems since it uses a more relaxed method of modifying the language, that is, it waits for regularities to occur before adding new descriptors to the language. The theory revision system, EITHER <ref> (Mooney & Ourston, 1991) </ref> creates intermediate concepts using two operators, inter-construction and absorption, that are similar to the conjunct and concept rules used in M-FOCL. Although both systems use similar opera tors, they behave differently.
Reference: <author> Pagallo, G., Haussler, D. </author> <year> (1990). </year> <title> Boolean feature discovery in empirical learning. </title> <journal> Machine Learning, </journal> <volume> volume 5, no. </volume> <pages> 1. </pages> <address> Boston, MA: </address> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: STAGGER (Schlimmer, 1987) heuristically searches through the space of conjunctions, disjunctions, and negations of the primitive predicates, using a failure driven test and the examples to see if the new constructive predicates should be added. FRINGE <ref> (Pagallo & Haussler, 1990) </ref> revises its language by finding Boolean conjuncts of features at the fringe of its decision tree and creates new features from these conjuncts. By modifying the language dynamically, it avoids the replication problem (Pagallo & Haussler, 1990). <p> FRINGE <ref> (Pagallo & Haussler, 1990) </ref> revises its language by finding Boolean conjuncts of features at the fringe of its decision tree and creates new features from these conjuncts. By modifying the language dynamically, it avoids the replication problem (Pagallo & Haussler, 1990). Although M-FOCL has this self-transference ability, it has a broader scope since it also finds repetitions using other concepts in the domain rather than just within one concept.
Reference: <author> Pazzani, M. J., Kibler, D. </author> <year> (1992). </year> <title> The utility of knowledge in inductive learning. </title> <journal> Machine Learning, </journal> <volume> volume 9, no. </volume> <pages> 1. </pages> <address> Boston, MA: </address> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: Our solution to this problem was to apply a commutative prop erty on the variables in the predicate, allowing different orderings of the variables to be considered the same. We used the commutative constraint on the predicates <ref> (Pazzani & Kibler, 1992) </ref> and extended it from working only on 2-arity predicates to n-arity predicates. This constraint reduced the amount of search, but was only originally applied to pairs of variables.
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> The effect of noise on concept learning. </title> <booktitle> Machine Learning: An Artificial Intelligence Approach, volume 2. </booktitle> <address> Los Altos, CA: </address> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: When creating attribute noise in the data, for each element in a tuple there is an n% chance that the element's value is assigned randomly, where n is the amount of attribute noise in the data <ref> (Quinlan, 1986) </ref>. In the noise test, we experimented in the chess domain with a training size of 25 examples and varied the amount of noise. We used noise levels of 10%, 20%, 30%, and 40%, and used a testing size of 200 examples.
Reference: <author> Quinlan, J. R. </author> <year> (1990). </year> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> volume 5 no. </volume> <pages> 3. </pages> <address> Boston, MA: </address> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: We contrast our work with related work in learning multiple concepts in section 5. We conclude by summarizing our contributions and discussing future work. 2 Relational Horn clause learning algorithms Before describing Multiple concept FOCL (M-FOCL) and FOCL (Pazzani & Kibler, 1990), we will describe FOIL <ref> (Quinlan, 1990) </ref> to explain the core learning method applied in these systems. Given a set of extensional predicates, positive examples, and negative examples, FOIL learns a relational Horn clause concept in terms of predefined predicates. This concept covers all of the positive examples and excludes all neg ative ones. <p> Since each hand contains five different cards, the number of possible bindings for the unbound variable ?card1 greatly increases the search space of possible horn clause theories <ref> (Quinlan, 1990) </ref>. Indeed, the search space increases by a factor of 5, the number of cards in the hand. Next FOCL would add card-in-hand (?hand,?card2) which increases the search space by another factor of 5.
Reference: <author> Ruby, D., Kibler, D. </author> <year> (1991). </year> <title> SteppingStone: An empirical and analytical evaluation. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence. </booktitle> <address> Menlo Park, CA: </address> <publisher> AAAI Press/The Mit Press. </publisher>
Reference-contexts: By using the conjunct rule, a predicate is created that is the conjunction of other predicates that form useful partial substructures. An analogy can be drawn between learning new predicates in M-FOCL and learning macros in planning systems. Macros are learned from previous successful sequences of operators <ref> (Ruby & Kibler, 1991) </ref>, whereas the concept, clause, and conjunct rules facilitate learn ing new predicates from previous concepts.
Reference: <author> Schlimmer, J. C. </author> <year> (1987). </year> <title> "Incremental adjustment of representations." </title> <editor> In P. Langley, ed., </editor> <booktitle> Proceedings of the Fourth International Workshop on Machine Learning. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Learning multiple concepts allows the system to refine its representation language dynamically. Other systems that revise their language apply a variety of methods to create new predicates. STAGGER <ref> (Schlimmer, 1987) </ref> heuristically searches through the space of conjunctions, disjunctions, and negations of the primitive predicates, using a failure driven test and the examples to see if the new constructive predicates should be added.
Reference: <author> Vere, S. A. </author> <year> (1977). </year> <title> Induction of relational productions in the presence of background information. </title> <booktitle> In Fifth International Joint Conference on Artificial Intelligence. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
References-found: 14

