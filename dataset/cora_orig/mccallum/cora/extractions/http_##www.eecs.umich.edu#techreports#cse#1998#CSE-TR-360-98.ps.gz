URL: http://www.eecs.umich.edu/techreports/cse/1998/CSE-TR-360-98.ps.gz
Refering-URL: http://www.eecs.umich.edu/home/techreports/cse98.html
Root-URL: http://www.cs.umich.edu
Title: ABSTRACT Block Enlargement Optimizations for Increasing the Instruction Fetch Rate in Block-Structured Instruction Set Architectures  
Author: by Eric Hao Chair: Yale N. Patt 
Abstract: To exploit larger amounts of instruction level parallelism, processors are being built with wider issue widths and larger numbers of functional units. Instruction fetch rate must also be increased in order to effectively exploit the performance potential of such processors. Block-structured ISAs are a new class of instruction set architectures that were designed to address the performance obstacles faced by processors attempting to exploit high levels of instruction level parallelism. The major distinguishing feature of a block-structured ISA is that it defines the architectural atomic unit (i.e. the instruction) to be a group of operations which is called an atomic block. This dissertation defines an optimization, block enlargement, that can be applied to a block-structured ISA to increase the instruction fetch rate of a processor that implements that ISA. A compiler that generates block-structured ISA code and a simulator that models the execution of that code on a block-structured ISA processor were constructed to evaluate the performance benefit of block-structured ISAs. This dissertation shows that for the SPECint95 benchmarks, the block-structured ISA processor executing enlarged atomic blocks and using simpler microarchitectural mechanisms to support wide-issue and dynamic scheduling outperforms a conventional ISA processor that also supports wide-issue and dynamic scheduling by 28% when assuming perfect branch prediction and by 15% when using real branch prediction. 
Abstract-found: 1
Intro-found: 1
Reference: <institution> 112 BIBLIOGRAPHY </institution>
Reference: [1] <author> A. V. Aho, R. Sethi, and J. D. Ullman, </author> <booktitle> Compilers Principles, Techniques, and Tools, </booktitle> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1986. </year>
Reference-contexts: Compiler optimizations can be divided into two classes: local and global <ref> [1] </ref>. Local optimizations are optimizations that are performed upon instructions that reside within a single block. They are narrow in focus. Global optimizations are optimizations that are performed across multiple basic blocks.
Reference: [2] <author> J. R. Allen, K. Kennedy, C. Porterfield, and J. Warren, </author> <title> "Conversion of control dependence to data dependence," </title> <booktitle> in 10th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pp. 177-189, </pages> <year> 1983. </year>
Reference-contexts: more accurate than static branch prediction [53], using superblock scheduling to increase the instruction fetch rate will not be as effective as an approach that uses dynamic branch prediction to select the set of blocks to be fetched each cycle. 7 2.2.2 Predicated Execution A processor that supports predicated execution <ref> [2, 20, 43, 42] </ref> associates a predicate register with each instruction issued. The execution of an instruction proceeds normally if the predicate register resolves to a true value. The execution is suppressed if that predicate value resolves to a false value.
Reference: [3] <author> M. G. Butler, </author> <title> Aggressive Execution Engines for Surpassing Single Basic Block Execution, </title> <type> PhD thesis, </type> <institution> University of Michigan, </institution> <year> 1993. </year>
Reference-contexts: The instruction trace generated by BaseSim's front end included only instructions that were on the correct path of execution. By excluding instructions from incorrect speculative (or wrong) paths, BaseSim was unable to model the effects of issuing such instructions into the machine <ref> [3, 25] </ref>. However, the execution penalty associated with the mispredicted branch that created the incorrect speculative path is still correctly modeled. When a branch misprediction occurs in a real machine, the processor continues to fetch instructions from the wrong path until the mispredicted branch is resolved. <p> This phenomenon of increased mispredicted branch resolution time was also observed by Butler when studying the performance of a conventional ISA processor that fetched and issued 65 and conventional ISA predictors. multiple basic blocks per cycle <ref> [3] </ref>. Unlike the other benchmarks, the branch resolution times for the go and m88ksim benchmark decreases (by 14% and 4%) for the block-structured ISA executables instead of increasing. Part of the decrease for the go benchmark was due to BTB misses and icache misses.
Reference: [4] <author> P. Chang, </author> <title> Compiler Support for Multiple-Instruction-Issue Architectures, </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1991. </year>
Reference-contexts: The compiler-based schemes reorganize the program to increase the amount of work that can be fetched with a single icache access, eliminating the need for extra hardware. These schemes include trace and superblock scheduling <ref> [14, 4, 22] </ref>, predicated execution [20, 42], hyperblock scheduling [29], the VLIW multi-way jump mechanism [13, 26, 12, 36], and multiscalar processors [15, 47]. 2.1 Hardware-based Approaches 2.1.1 Predicting Multiple Branches Per Cycle The branch address cache [54], the collapsing buffer [10], the subgraph-level predictor [11], and the multiple-block ahead branch <p> In addition, they showed that the trace cache works well even when compared to a single block fetch mechanism that uses an aggressive branch predictor. 6 2.2 Software-based Approaches 2.2.1 Trace and Superblock Scheduling Trace scheduling [14] and superblock scheduling <ref> [22, 4, 9] </ref> are compiler optimizations that enlarge the scope in which the compiler can schedule instructions. They use static branch prediction to determine the frequently executed program paths and place the basic blocks along these paths into consecutive locations, forming a superblock. <p> This could possibly be done by adding additional code to the trap's successor block that negates the effect of the included code from the wrong path as is done in trace scheduling and superblocks <ref> [14, 4, 22] </ref>. Given these tradeoffs, there may be situations in which it is better for the block enlargement optimization to insert a trap operation instead of a fault operation. * Further increases in prediction accuracy for block-structured ISA branch predictors can be achieved by: 1.
Reference: [5] <author> P.-Y. Chang, E. Hao, and Y. N. Patt, </author> <title> "Target prediction for indirect jumps," </title> <booktitle> To appear in the 24th Annual International Symposium on Computer Architecture. </booktitle>
Reference-contexts: On the other hand, the tomcatv and su2cor benchmarks show a significant increase in execution time despite achieving a low branch misprediction rate for conditional branches because they have a significant number of indirect branches whose targets are frequently mispredicted. Using a target cache <ref> [5] </ref> to predict the targets of these indirect branches may significantly improve 2 The shorter history register length is due to the increased size of the pattern history table entries. 83 a 32 wide machine with and without perfect branch prediction. the performance of these two benchmarks. 8.4 Future Directions The
Reference: [6] <author> P.-Y. Chang, E. Hao, and Y. N. Patt, </author> <title> "Alternative implementations of hybrid branch predictors," </title> <booktitle> in Proceedings of the 28th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <pages> pp. 252-257, </pages> <year> 1995. </year>
Reference-contexts: As a result, more accurate branch predictions can be made in the presence of BTB misses if compiler-specified target mapping is used. 6.4 A Specific Implementation For the experiments in chapter 7, the branch predictor used in the block-structured ISA processor is a hybrid branch predictor <ref> [30, 8, 6] </ref> with two component predictors. For each branch, each component predictor generates a prediction. The hybrid predictor selects the 59 prediction from the component predictor that has been more accurate for that branch in the recent past to serve as the final branch prediction.
Reference: [7] <author> P.-Y. Chang, E. Hao, Y. N. Patt, and P. P. Chang, </author> <title> "Using predicated execution to improve the performance of a dynamically scheduled machine with speculative execution," </title> <booktitle> in Proceedings of the IFIP WG10.3 Working Conference on Parallel Architectures and Compilation Techniques, PACT '95, </booktitle> <pages> pp. 99-108, </pages> <year> 1995. </year>
Reference-contexts: While predicated execution by itself may not be an effective mechanism for increasing fetch rate, it can provide a significant performance benefit when used in conjunction with speculative execution <ref> [28, 7] </ref> and other schemes for increasing fetch rate. 2.2.3 Hyperblock Scheduling Hyperblock scheduling [29, 28] is an extension of superblock scheduling that incorporates predicated execution. As discussed earlier, the processor must always follow the static branch predictions used to form the superblock.
Reference: [8] <author> P.-Y. Chang, E. Hao, T.-Y. Yeh, and Y. N. Patt, </author> <title> "Branch classification: A new mechanism for improving branch predictor performance," </title> <booktitle> in Proceedings of the 27th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <pages> pp. 22-31, </pages> <year> 1994. </year>
Reference-contexts: As a result, more accurate branch predictions can be made in the presence of BTB misses if compiler-specified target mapping is used. 6.4 A Specific Implementation For the experiments in chapter 7, the branch predictor used in the block-structured ISA processor is a hybrid branch predictor <ref> [30, 8, 6] </ref> with two component predictors. For each branch, each component predictor generates a prediction. The hybrid predictor selects the 59 prediction from the component predictor that has been more accurate for that branch in the recent past to serve as the final branch prediction.
Reference: [9] <author> P. P. Chang, S. A. Mahlke, W. Y. Chen, N. J. Warter, and W. W. Hwu, </author> <title> "IMPACT: An architectural framework for multiple-instruction-issue processors," </title> <booktitle> in Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 266-275, </pages> <year> 1991. </year>
Reference-contexts: In addition, they showed that the trace cache works well even when compared to a single block fetch mechanism that uses an aggressive branch predictor. 6 2.2 Software-based Approaches 2.2.1 Trace and Superblock Scheduling Trace scheduling [14] and superblock scheduling <ref> [22, 4, 9] </ref> are compiler optimizations that enlarge the scope in which the compiler can schedule instructions. They use static branch prediction to determine the frequently executed program paths and place the basic blocks along these paths into consecutive locations, forming a superblock.
Reference: [10] <author> T. M. Conte, K. N. Menezes, P. M. Mills, and B. Patel, </author> <title> "Optimization of instruction fetch mechanisms for high issue rates," </title> <booktitle> in Proceedings of the 22st Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 333-344, </pages> <year> 1995. </year> <month> 113 </month>
Reference-contexts: Appendix A contains the corresponding figures for the seven other SPECint95 benchmarks. Various approaches have been proposed for increasing instruction fetch rate from that of a single basic block per cycle. Some approaches <ref> [54, 10, 11, 45] </ref> extend the branch predictor and icache so that multiple branch predictions can be made each cycle and multiple, nonconsecutive cache lines can be fetched each cycle. However, this extra hardware requires extra stages in the pipeline which will increase the branch misprediction penalty, decreasing performance. <p> The hardware-based schemes extend the branch predictor and icache so that multiple branch predictions can be made each cycle and multiple non-consecutive cache lines can be fetched each cycle. They include the branch address cache [54], the collapsing buffer <ref> [10] </ref>, the subgraph-level predictor [11], the multiple-block ahead branch predictor [45], and the trace cache [35, 44, 39]. The compiler-based schemes reorganize the program to increase the amount of work that can be fetched with a single icache access, eliminating the need for extra hardware. <p> These schemes include trace and superblock scheduling [14, 4, 22], predicated execution [20, 42], hyperblock scheduling [29], the VLIW multi-way jump mechanism [13, 26, 12, 36], and multiscalar processors [15, 47]. 2.1 Hardware-based Approaches 2.1.1 Predicting Multiple Branches Per Cycle The branch address cache [54], the collapsing buffer <ref> [10] </ref>, the subgraph-level predictor [11], and the multiple-block ahead branch predictor [45] are hardware schemes that propose different ways to extend the dynamic branch predictor so that it can make multiple branch predictions each cycle.
Reference: [11] <author> S. Dutta and M. Franklin, </author> <title> "Control flow prediction with tree-like subgraphs for super-scalar processors," </title> <booktitle> in Proceedings of the 28th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <pages> pp. 258-263, </pages> <year> 1995. </year>
Reference-contexts: Appendix A contains the corresponding figures for the seven other SPECint95 benchmarks. Various approaches have been proposed for increasing instruction fetch rate from that of a single basic block per cycle. Some approaches <ref> [54, 10, 11, 45] </ref> extend the branch predictor and icache so that multiple branch predictions can be made each cycle and multiple, nonconsecutive cache lines can be fetched each cycle. However, this extra hardware requires extra stages in the pipeline which will increase the branch misprediction penalty, decreasing performance. <p> The hardware-based schemes extend the branch predictor and icache so that multiple branch predictions can be made each cycle and multiple non-consecutive cache lines can be fetched each cycle. They include the branch address cache [54], the collapsing buffer [10], the subgraph-level predictor <ref> [11] </ref>, the multiple-block ahead branch predictor [45], and the trace cache [35, 44, 39]. The compiler-based schemes reorganize the program to increase the amount of work that can be fetched with a single icache access, eliminating the need for extra hardware. <p> include trace and superblock scheduling [14, 4, 22], predicated execution [20, 42], hyperblock scheduling [29], the VLIW multi-way jump mechanism [13, 26, 12, 36], and multiscalar processors [15, 47]. 2.1 Hardware-based Approaches 2.1.1 Predicting Multiple Branches Per Cycle The branch address cache [54], the collapsing buffer [10], the subgraph-level predictor <ref> [11] </ref>, and the multiple-block ahead branch predictor [45] are hardware schemes that propose different ways to extend the dynamic branch predictor so that it can make multiple branch predictions each cycle.
Reference: [12] <author> K. Ebcioglu, </author> <title> "Some design ideas for a VLIW architecture for sequential natured software," </title> <booktitle> Parallel Processing (Proceedings of IFIP WG 10.3 Working Conference on Parallel Processing, </booktitle> <pages> pp. 3-21, </pages> <month> April </month> <year> 1988. </year>
Reference-contexts: The compiler-based schemes reorganize the program to increase the amount of work that can be fetched with a single icache access, eliminating the need for extra hardware. These schemes include trace and superblock scheduling [14, 4, 22], predicated execution [20, 42], hyperblock scheduling [29], the VLIW multi-way jump mechanism <ref> [13, 26, 12, 36] </ref>, and multiscalar processors [15, 47]. 2.1 Hardware-based Approaches 2.1.1 Predicting Multiple Branches Per Cycle The branch address cache [54], the collapsing buffer [10], the subgraph-level predictor [11], and the multiple-block ahead branch predictor [45] are hardware schemes that propose different ways to extend the dynamic branch predictor <p> Hyperblock scheduling must be carefully controlled to ensure that this tradeoff results in a performance improvement. 2.2.4 VLIW Multi-Way Jumps The VLIW multi-way jump mechanism <ref> [13, 26, 12, 36] </ref> combines multiple branches from multiple paths in the control flow graph into a single branch.
Reference: [13] <author> J. A. Fisher, </author> <title> "2 n -way jump microinstruction hardware and an effective instruction binding method," </title> <booktitle> in Proceedings of the 13th Annual Microprogramming Workshop, </booktitle> <pages> pp. 64-75, </pages> <year> 1980. </year>
Reference-contexts: The compiler-based schemes reorganize the program to increase the amount of work that can be fetched with a single icache access, eliminating the need for extra hardware. These schemes include trace and superblock scheduling [14, 4, 22], predicated execution [20, 42], hyperblock scheduling [29], the VLIW multi-way jump mechanism <ref> [13, 26, 12, 36] </ref>, and multiscalar processors [15, 47]. 2.1 Hardware-based Approaches 2.1.1 Predicting Multiple Branches Per Cycle The branch address cache [54], the collapsing buffer [10], the subgraph-level predictor [11], and the multiple-block ahead branch predictor [45] are hardware schemes that propose different ways to extend the dynamic branch predictor <p> Hyperblock scheduling must be carefully controlled to ensure that this tradeoff results in a performance improvement. 2.2.4 VLIW Multi-Way Jumps The VLIW multi-way jump mechanism <ref> [13, 26, 12, 36] </ref> combines multiple branches from multiple paths in the control flow graph into a single branch.
Reference: [14] <author> J. A. Fisher, </author> <title> "Trace scheduling: A technique for global microcode compaction," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-30, no. 7, </volume> <pages> pp. 478-490, </pages> <month> July </month> <year> 1981. </year>
Reference-contexts: However, this extra hardware requires extra stages in the pipeline which will increase the branch misprediction penalty, decreasing performance. Other approaches <ref> [14, 22] </ref> statically predict the direction to be taken by a program's branches and then based on those predictions, use the compiler to arrange the blocks so that the multiple blocks to be fetched are always placed in consecutive memory 1 gcc benchmark as the number of blocks fetched per cycle <p> The compiler-based schemes reorganize the program to increase the amount of work that can be fetched with a single icache access, eliminating the need for extra hardware. These schemes include trace and superblock scheduling <ref> [14, 4, 22] </ref>, predicated execution [20, 42], hyperblock scheduling [29], the VLIW multi-way jump mechanism [13, 26, 12, 36], and multiscalar processors [15, 47]. 2.1 Hardware-based Approaches 2.1.1 Predicting Multiple Branches Per Cycle The branch address cache [54], the collapsing buffer [10], the subgraph-level predictor [11], and the multiple-block ahead branch <p> In addition, they showed that the trace cache works well even when compared to a single block fetch mechanism that uses an aggressive branch predictor. 6 2.2 Software-based Approaches 2.2.1 Trace and Superblock Scheduling Trace scheduling <ref> [14] </ref> and superblock scheduling [22, 4, 9] are compiler optimizations that enlarge the scope in which the compiler can schedule instructions. They use static branch prediction to determine the frequently executed program paths and place the basic blocks along these paths into consecutive locations, forming a superblock. <p> branch prediction instead of dynamic branch prediction for certain select branches in the program. 4.1.2 Enlarging the Compiler's Scope for Optimization In addition to increasing the instruction rate, the block enlargement optimization also increases the compiler's scope for local optimization in the same manner as is done in trace scheduling <ref> [14] </ref> and superblock scheduling [22]. Compiler optimizations can be divided into two classes: local and global [1]. Local optimizations are optimizations that are performed upon instructions that reside within a single block. They are narrow in focus. Global optimizations are optimizations that are performed across multiple basic blocks. <p> This could possibly be done by adding additional code to the trap's successor block that negates the effect of the included code from the wrong path as is done in trace scheduling and superblocks <ref> [14, 4, 22] </ref>. Given these tradeoffs, there may be situations in which it is better for the block enlargement optimization to insert a trap operation instead of a fault operation. * Further increases in prediction accuracy for block-structured ISA branch predictors can be achieved by: 1.
Reference: [15] <author> M. Franklin and G. S. Sohi, </author> <title> "The expandable split window paradigm for exploiting fine-grain parallelism," </title> <booktitle> in Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 58-67, </pages> <year> 1992. </year>
Reference-contexts: These schemes include trace and superblock scheduling [14, 4, 22], predicated execution [20, 42], hyperblock scheduling [29], the VLIW multi-way jump mechanism [13, 26, 12, 36], and multiscalar processors <ref> [15, 47] </ref>. 2.1 Hardware-based Approaches 2.1.1 Predicting Multiple Branches Per Cycle The branch address cache [54], the collapsing buffer [10], the subgraph-level predictor [11], and the multiple-block ahead branch predictor [45] are hardware schemes that propose different ways to extend the dynamic branch predictor so that it can make multiple branch <p> The compiler may have to delay the scheduling of certain operations in order to meet these requirements, lowering the instruction fetch rate of the processor. 2.2.5 Multiscalar Processors The multiscalar processing paradigm <ref> [15, 47] </ref> applies multiprocessor concepts to the design of microprocessors. A multiscalar processor consists of a set of processing elements connected in a ring. The multiscalar processor's compiler is responsible for dividing the program up into units of work, called tasks.
Reference: [16] <author> L. Gwennap, </author> <title> "PA-8000 combines complexity and speed," Microprocessor Report, </title> <journal> vol. </journal> <volume> 8, no. 15, </volume> , <month> November </month> <year> 1994. </year>
Reference-contexts: These techniques include aggressive branch prediction, wide instruction issue, out-of-order execution, and precise exception handling and have been adopted by all currently introduced microprocessors <ref> [16, 17, 18] </ref>. The HPS implementations modeled issue sixteen instructions per cycle. This issue width was chosen for two reasons.
Reference: [17] <author> L. Gwennap, </author> <title> "Intel's P6 uses decoupled superscalar design," Microprocessor Report, </title> <journal> vol. </journal> <volume> 9, </volume> , <month> February </month> <year> 1995. </year>
Reference-contexts: These techniques include aggressive branch prediction, wide instruction issue, out-of-order execution, and precise exception handling and have been adopted by all currently introduced microprocessors <ref> [16, 17, 18] </ref>. The HPS implementations modeled issue sixteen instructions per cycle. This issue width was chosen for two reasons.
Reference: [18] <author> L. Gwennap, </author> <title> "Digital 21264 sets new standard," Microprocessor Report, </title> <journal> vol. </journal> <volume> 10, no. 14, </volume> , <month> October </month> <year> 1996. </year>
Reference-contexts: These techniques include aggressive branch prediction, wide instruction issue, out-of-order execution, and precise exception handling and have been adopted by all currently introduced microprocessors <ref> [16, 17, 18] </ref>. The HPS implementations modeled issue sixteen instructions per cycle. This issue width was chosen for two reasons. <p> However, advances in process technology will make such icaches possible in the future. Digital has already announced the 21264 Alpha processor which has a 64KB, two-way set associative icache with a single cycle access time <ref> [18] </ref>. 14 Class Latency Description Integer 1 INT add, sub and logic OPs FP Add 3 FP add, sub, and convert FP/INT Mul 3 FP mul and INT mul Load 2 Memory loads Store Memory stores Bit Field 1 Shift and bit testing Branch 1 Trap, fault, and other control instructions
Reference: [19] <author> E. Hao, P.-Y. Chang, and Y. N. Patt, </author> <title> "Increasing the instruction fetch rate via block-structured instruction set architectures," </title> <booktitle> in Proceedings of the 29th Annual IEEE/ACM International Symposium on Microarchitecture, </booktitle> <pages> pp. 191-200, </pages> <year> 1996. </year>
Reference-contexts: Block-structured ISAs exploit the advantages of both compiler-based and hardware-based solutions by merging basic blocks together at compile-time and providing support for dynamic branch prediction. Block-structured ISAs <ref> [31, 33, 32, 48, 19] </ref> are a new class of instruction set architectures that were designed to address the performance obstacles faced by processors attempting to exploit high levels of instruction level parallelism.
Reference: [20] <author> P. Hsu and E. Davidson, </author> <title> "Highly concurrent scalar processing," </title> <booktitle> in Proceedings of the 13th Annual International Symposium on Computer Architecture, </booktitle> <year> 1986. </year>
Reference-contexts: The compiler-based schemes reorganize the program to increase the amount of work that can be fetched with a single icache access, eliminating the need for extra hardware. These schemes include trace and superblock scheduling [14, 4, 22], predicated execution <ref> [20, 42] </ref>, hyperblock scheduling [29], the VLIW multi-way jump mechanism [13, 26, 12, 36], and multiscalar processors [15, 47]. 2.1 Hardware-based Approaches 2.1.1 Predicting Multiple Branches Per Cycle The branch address cache [54], the collapsing buffer [10], the subgraph-level predictor [11], and the multiple-block ahead branch predictor [45] are hardware schemes <p> more accurate than static branch prediction [53], using superblock scheduling to increase the instruction fetch rate will not be as effective as an approach that uses dynamic branch prediction to select the set of blocks to be fetched each cycle. 7 2.2.2 Predicated Execution A processor that supports predicated execution <ref> [2, 20, 43, 42] </ref> associates a predicate register with each instruction issued. The execution of an instruction proceeds normally if the predicate register resolves to a true value. The execution is suppressed if that predicate value resolves to a false value.
Reference: [21] <author> W. W. Hwu and P. P. Chang, </author> <title> "Inline function expansion for compiling c programs," </title> <booktitle> in Proceedings of the ACM SIGPLAN'89 Conference on Programming Language Design and Implementation, </booktitle> <year> 1989. </year>
Reference-contexts: To do this, only a select set of function calls (or call sites) within the program are chosen for function inlining. The block-structured ISA compiler uses a variation of the algorithm proposed by Hwu and Chang <ref> [21] </ref> to select the call sites to be inlined.
Reference: [22] <author> W. W. Hwu, S. A. Mahlke, W. Y. Chen, P. P. Chang, N. J. Warter, R. A. Bringmann, R. G. Ouellette, R. E. Hank, T. Kiyohara, G. E. Haab, J. G. Holm, and D. M. Lav-ery, </author> <title> "The superblock: An effective technique for VLIW and superscalar compilation," </title> <journal> Journal of Supercomputing, </journal> <volume> vol. 7, no. </volume> <pages> 9-50, </pages> , <year> 1993. </year>
Reference-contexts: However, this extra hardware requires extra stages in the pipeline which will increase the branch misprediction penalty, decreasing performance. Other approaches <ref> [14, 22] </ref> statically predict the direction to be taken by a program's branches and then based on those predictions, use the compiler to arrange the blocks so that the multiple blocks to be fetched are always placed in consecutive memory 1 gcc benchmark as the number of blocks fetched per cycle <p> The compiler-based schemes reorganize the program to increase the amount of work that can be fetched with a single icache access, eliminating the need for extra hardware. These schemes include trace and superblock scheduling <ref> [14, 4, 22] </ref>, predicated execution [20, 42], hyperblock scheduling [29], the VLIW multi-way jump mechanism [13, 26, 12, 36], and multiscalar processors [15, 47]. 2.1 Hardware-based Approaches 2.1.1 Predicting Multiple Branches Per Cycle The branch address cache [54], the collapsing buffer [10], the subgraph-level predictor [11], and the multiple-block ahead branch <p> In addition, they showed that the trace cache works well even when compared to a single block fetch mechanism that uses an aggressive branch predictor. 6 2.2 Software-based Approaches 2.2.1 Trace and Superblock Scheduling Trace scheduling [14] and superblock scheduling <ref> [22, 4, 9] </ref> are compiler optimizations that enlarge the scope in which the compiler can schedule instructions. They use static branch prediction to determine the frequently executed program paths and place the basic blocks along these paths into consecutive locations, forming a superblock. <p> dynamic branch prediction for certain select branches in the program. 4.1.2 Enlarging the Compiler's Scope for Optimization In addition to increasing the instruction rate, the block enlargement optimization also increases the compiler's scope for local optimization in the same manner as is done in trace scheduling [14] and superblock scheduling <ref> [22] </ref>. Compiler optimizations can be divided into two classes: local and global [1]. Local optimizations are optimizations that are performed upon instructions that reside within a single block. They are narrow in focus. Global optimizations are optimizations that are performed across multiple basic blocks. <p> This could possibly be done by adding additional code to the trap's successor block that negates the effect of the included code from the wrong path as is done in trace scheduling and superblocks <ref> [14, 4, 22] </ref>. Given these tradeoffs, there may be situations in which it is better for the block enlargement optimization to insert a trap operation instead of a fault operation. * Further increases in prediction accuracy for block-structured ISA branch predictors can be achieved by: 1.
Reference: [23] <author> W. W. Hwu and Y. N. Patt, </author> <title> "Checkpoint repair for high-performance out-of-order execution machines," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-36, no. 12, </volume> , <month> December </month> <year> 1987. </year>
Reference-contexts: Table 3.1 lists the simulated latencies for each instruction class. After execution, an instruction's result is forwarded to the register file and to the other instructions awaiting execution in the node tables. An instruction is retired when its associated checkpoint <ref> [23] </ref> is retired. This occurs after all the instructions in the checkpoint have successfully executed and all previously issued checkpoints have been retired. The HPS implementations modeled can hold up to 32 checkpoints for a total of 256 instructions.
Reference: [24] <institution> Intel Reference C Compiler User's Guide for UNIX Systems, Intel Corporation, </institution> <year> 1993. </year> <month> 114 </month>
Reference-contexts: This chapter details each component of the experimental setup. 3.1 The Compiler I implemented a compiler that was targeted for both the block-structured and the conventional ISAs. This compiler is based on the Intel Reference C Compiler <ref> [24] </ref> with the back end appropriately retargeted. The Intel Reference C Compiler generates an intermediate representation of the program being compiled and applies the standard set of optimizations to that representation. The compiler back end takes this representation and applies a set of target-specific optimizations and allocates registers.
Reference: [25] <author> S. Jourdan, T. Hsing, J. Stark, and Y. N. Patt, </author> <title> "The effects of mispredicted-path execution on branch prediction structures," </title> <booktitle> in Proceedings of the 1996 Conference on Parallel Architectures and Compilation Techniques (PACT '96), </booktitle> <pages> pp. 58-67, </pages> <year> 1996. </year>
Reference-contexts: The instruction trace generated by BaseSim's front end included only instructions that were on the correct path of execution. By excluding instructions from incorrect speculative (or wrong) paths, BaseSim was unable to model the effects of issuing such instructions into the machine <ref> [3, 25] </ref>. However, the execution penalty associated with the mispredicted branch that created the incorrect speculative path is still correctly modeled. When a branch misprediction occurs in a real machine, the processor continues to fetch instructions from the wrong path until the mispredicted branch is resolved.
Reference: [26] <author> K. Karplus and A. Nicolau, </author> <title> "Efficient hardware for multi-way jumps and prefetches," </title> <booktitle> in Proceedings of the 18th Annual Microprogramming Workshop, </booktitle> <pages> pp. 11-18, </pages> <year> 1985. </year>
Reference-contexts: The compiler-based schemes reorganize the program to increase the amount of work that can be fetched with a single icache access, eliminating the need for extra hardware. These schemes include trace and superblock scheduling [14, 4, 22], predicated execution [20, 42], hyperblock scheduling [29], the VLIW multi-way jump mechanism <ref> [13, 26, 12, 36] </ref>, and multiscalar processors [15, 47]. 2.1 Hardware-based Approaches 2.1.1 Predicting Multiple Branches Per Cycle The branch address cache [54], the collapsing buffer [10], the subgraph-level predictor [11], and the multiple-block ahead branch predictor [45] are hardware schemes that propose different ways to extend the dynamic branch predictor <p> Hyperblock scheduling must be carefully controlled to ensure that this tradeoff results in a performance improvement. 2.2.4 VLIW Multi-Way Jumps The VLIW multi-way jump mechanism <ref> [13, 26, 12, 36] </ref> combines multiple branches from multiple paths in the control flow graph into a single branch.
Reference: [27] <author> J. K. F. Lee and A. J. Smith, </author> <title> "branch prediction strategies and branch target buffer design," </title> <booktitle> IEEE Computer, </booktitle> <pages> pp. 6-22, </pages> <month> January </month> <year> 1984. </year>
Reference-contexts: As a result, the branch history register need not shift in the full target index for each prediction. 58 6.3 Extensions to the Branch Target Buffer The Branch Target Buffer (BTB) <ref> [46, 27] </ref> must be extended so that it can keep track of all the possible control flow successors for a block. For a conventional ISA, the number of successors is limited to two, the taken and fall-through targets of the block's branch.
Reference: [28] <author> S. A. Mahlke, R. E. Hank, R. A. Bringmann, J. C. Gyllenhaal, D. M. Gallagher, and W. W. Hwu, </author> <title> "Characterizing the impact of predicated execution on branch prediction," </title> <booktitle> in Proceedings of the 27th Annual ACM/IEEE International Symposium on Microar-chitecture, </booktitle> <pages> pp. 217-227, </pages> <year> 1994. </year>
Reference-contexts: While predicated execution by itself may not be an effective mechanism for increasing fetch rate, it can provide a significant performance benefit when used in conjunction with speculative execution <ref> [28, 7] </ref> and other schemes for increasing fetch rate. 2.2.3 Hyperblock Scheduling Hyperblock scheduling [29, 28] is an extension of superblock scheduling that incorporates predicated execution. As discussed earlier, the processor must always follow the static branch predictions used to form the superblock. <p> While predicated execution by itself may not be an effective mechanism for increasing fetch rate, it can provide a significant performance benefit when used in conjunction with speculative execution [28, 7] and other schemes for increasing fetch rate. 2.2.3 Hyperblock Scheduling Hyperblock scheduling <ref> [29, 28] </ref> is an extension of superblock scheduling that incorporates predicated execution. As discussed earlier, the processor must always follow the static branch predictions used to form the superblock.
Reference: [29] <author> S. A. Mahlke, D. C. Lin, W. Y. Chen, R. E. Hank, and R. A. Bringmann, </author> <title> "Effective compiler support for predicated execution using the hyperblock," </title> <booktitle> in Proceedings of the 25th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <pages> pp. 45-54, </pages> <year> 1992. </year>
Reference-contexts: The compiler-based schemes reorganize the program to increase the amount of work that can be fetched with a single icache access, eliminating the need for extra hardware. These schemes include trace and superblock scheduling [14, 4, 22], predicated execution [20, 42], hyperblock scheduling <ref> [29] </ref>, the VLIW multi-way jump mechanism [13, 26, 12, 36], and multiscalar processors [15, 47]. 2.1 Hardware-based Approaches 2.1.1 Predicting Multiple Branches Per Cycle The branch address cache [54], the collapsing buffer [10], the subgraph-level predictor [11], and the multiple-block ahead branch predictor [45] are hardware schemes that propose different ways <p> While predicated execution by itself may not be an effective mechanism for increasing fetch rate, it can provide a significant performance benefit when used in conjunction with speculative execution [28, 7] and other schemes for increasing fetch rate. 2.2.3 Hyperblock Scheduling Hyperblock scheduling <ref> [29, 28] </ref> is an extension of superblock scheduling that incorporates predicated execution. As discussed earlier, the processor must always follow the static branch predictions used to form the superblock.
Reference: [30] <author> S. McFarling, </author> <title> "Combining branch predictors," </title> <type> Technical Report TN-36, </type> <institution> Digital Western Research Laboratory, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: As a result, more accurate branch predictions can be made in the presence of BTB misses if compiler-specified target mapping is used. 6.4 A Specific Implementation For the experiments in chapter 7, the branch predictor used in the block-structured ISA processor is a hybrid branch predictor <ref> [30, 8, 6] </ref> with two component predictors. For each branch, each component predictor generates a prediction. The hybrid predictor selects the 59 prediction from the component predictor that has been more accurate for that branch in the recent past to serve as the final branch prediction. <p> The two component predictors used were the gshare variation <ref> [30] </ref> and the PAs variation [56] of the Two-Level Adaptive Branch Predictor. The gshare variation records global branch history in its branch history register. The direction of every branch that occurred in the dynamic instruction is recorded in the history register.
Reference: [31] <author> S. Melvin, </author> <title> Performance Enhancement Through Dynamic Scheduling and Lareg Execution Atomic Units in Single Instruction Stream Processors, </title> <type> PhD thesis, </type> <institution> University of California, Berkeley, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: Block-structured ISAs exploit the advantages of both compiler-based and hardware-based solutions by merging basic blocks together at compile-time and providing support for dynamic branch prediction. Block-structured ISAs <ref> [31, 33, 32, 48, 19] </ref> are a new class of instruction set architectures that were designed to address the performance obstacles faced by processors attempting to exploit high levels of instruction level parallelism. <p> All the test data sets used were abbreviated versions of the SPECfp95 refer ence input sets. 17 CHAPTER 4 Block-Structured ISAs and the Block Enlargement Optimization Block-structured ISAs <ref> [31, 33, 32, 48] </ref> were designed to help solve performance obstacles faced by wide issue processors. Their major distinguishing feature is that the architectural atomic unit is defined to be a group of operations, where an operation typically corresponds to an instruction in a load/store architecture.
Reference: [32] <author> S. Melvin and Y. Patt, </author> <title> "Enhancing instruction scheduling with a block-structured ISA," </title> <journal> International Journal on Parallel Processing, </journal> <volume> vol. 23, no. 3, </volume> <pages> pp. 221-243, </pages> <year> 1995. </year>
Reference-contexts: Block-structured ISAs exploit the advantages of both compiler-based and hardware-based solutions by merging basic blocks together at compile-time and providing support for dynamic branch prediction. Block-structured ISAs <ref> [31, 33, 32, 48, 19] </ref> are a new class of instruction set architectures that were designed to address the performance obstacles faced by processors attempting to exploit high levels of instruction level parallelism. <p> All the test data sets used were abbreviated versions of the SPECfp95 refer ence input sets. 17 CHAPTER 4 Block-Structured ISAs and the Block Enlargement Optimization Block-structured ISAs <ref> [31, 33, 32, 48] </ref> were designed to help solve performance obstacles faced by wide issue processors. Their major distinguishing feature is that the architectural atomic unit is defined to be a group of operations, where an operation typically corresponds to an instruction in a load/store architecture.
Reference: [33] <author> S. Melvin and Y. N. Patt, </author> <title> "Exploiting fine-grained parallelism through a combination of hardware and software techniques," </title> <booktitle> in Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 287-297, </pages> <year> 1991. </year>
Reference-contexts: Block-structured ISAs exploit the advantages of both compiler-based and hardware-based solutions by merging basic blocks together at compile-time and providing support for dynamic branch prediction. Block-structured ISAs <ref> [31, 33, 32, 48, 19] </ref> are a new class of instruction set architectures that were designed to address the performance obstacles faced by processors attempting to exploit high levels of instruction level parallelism. <p> All the test data sets used were abbreviated versions of the SPECfp95 refer ence input sets. 17 CHAPTER 4 Block-Structured ISAs and the Block Enlargement Optimization Block-structured ISAs <ref> [31, 33, 32, 48] </ref> were designed to help solve performance obstacles faced by wide issue processors. Their major distinguishing feature is that the architectural atomic unit is defined to be a group of operations, where an operation typically corresponds to an instruction in a load/store architecture.
Reference: [34] <author> S. W. Melvin and Y. N. Patt, </author> <title> "Performance benefits of large execution atomic units in dynamically scheduled machines," </title> <booktitle> in Proceedings of Supercomputing '89, </booktitle> <pages> pp. 427-432, </pages> <year> 1989. </year>
Reference-contexts: Each decoded instruction cache entry held the microoperations that corresponded to the set of instructions within a given basic block. Thus, a hit in the decoded instruction cache would result in the fetch and issue of an entire basic block's worth of instructions. Later, Melvin and Patt <ref> [34] </ref> suggested using the fill unit to combine instructions from different basic blocks. Rotenberg et al. [44] and Patel et al. [39] independently proposed implementations of the trace cache that combined basic blocks as suggested by Melvin and Patt.
Reference: [35] <author> S. W. Melvin, M. C. Shebanow, and Y. N. Patt, </author> <title> "Hardware support for large atomic units in dynamically scheduled machines," </title> <booktitle> in Proceedings of the 21st Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <pages> pp. 60-63, </pages> <year> 1988. </year>
Reference-contexts: They include the branch address cache [54], the collapsing buffer [10], the subgraph-level predictor [11], the multiple-block ahead branch predictor [45], and the trace cache <ref> [35, 44, 39] </ref>. The compiler-based schemes reorganize the program to increase the amount of work that can be fetched with a single icache access, eliminating the need for extra hardware. <p> If this problem is not addressed, then this increase in hardware complexity will result in increased cycle times or additional pipeline stages for an aggressive wide-issue machine, which in turn, will decrease performance. 2.1.2 The Trace Cache The trace cache <ref> [35, 44, 39] </ref> is a hardware-based scheme that does not require fetching non-consecutive blocks from the icache. Its fetch unit consists of three parts: a core fetch unit, a fill unit, and a trace cache. The core fetch unit fetches one basic block per cycle from the icache. <p> As long as the processor is fetching its instructions from the trace cache, the trace cache is an effective means for fetching multiple basic blocks each cycle without incurring the costs associated with the other hardware-based approaches. The trace cache idea was originally proposed by Melvin, Shebanow, and Patt <ref> [35] </ref>. They proposed using the trace cache (or decoded instruction cache) to ease the instruction decoding bottleneck for HPS implementations of complex instruction sets such as the VAX. The core fetch unit fetched one instruction per cycle.
Reference: [36] <author> S.-M. Moon and K. Ebcioglu, </author> <title> "An efficient resource-constrained global scheduling technique for superscalar and VLIW processors," </title> <booktitle> in Proceedings of the 25th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <pages> pp. 55-71, </pages> <year> 1992. </year>
Reference-contexts: The compiler-based schemes reorganize the program to increase the amount of work that can be fetched with a single icache access, eliminating the need for extra hardware. These schemes include trace and superblock scheduling [14, 4, 22], predicated execution [20, 42], hyperblock scheduling [29], the VLIW multi-way jump mechanism <ref> [13, 26, 12, 36] </ref>, and multiscalar processors [15, 47]. 2.1 Hardware-based Approaches 2.1.1 Predicting Multiple Branches Per Cycle The branch address cache [54], the collapsing buffer [10], the subgraph-level predictor [11], and the multiple-block ahead branch predictor [45] are hardware schemes that propose different ways to extend the dynamic branch predictor <p> Hyperblock scheduling must be carefully controlled to ensure that this tradeoff results in a performance improvement. 2.2.4 VLIW Multi-Way Jumps The VLIW multi-way jump mechanism <ref> [13, 26, 12, 36] </ref> combines multiple branches from multiple paths in the control flow graph into a single branch.
Reference: [37] <institution> MC88110 Second Generation RISC Microprocessor User's Manual, Motorola, </institution> <year> 1991. </year> <month> 115 </month>
Reference-contexts: This ISA's architectural unit is the atomic block. The operations that can be found in an atomic block were taken from a subset of the MC88000 ISA <ref> [37] </ref>. All the non-control flow operations and indirect branches were taken directly from the MC88000 ISA. Added to this core set of operations were the trap, fault, and subroutine call operations. Each atomic block can contain up to sixteen operations, the issue width of the machine 1 .
Reference: [38] <author> S.-T. Pan, K. So, and J. T. Rahmeh, </author> <title> "Improving the accuracy of dynamic branch prediction using branch correlation," </title> <booktitle> in Proceedings of the 5th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 76-84, </pages> <year> 1992. </year>
Reference-contexts: from a probabilistic viewpoint, the number of branches predicted each cycle by a predictor should not affect its overall prediction accuracy. 54 Branch Predictor. 6.2 Two-Level Adaptive Branch Prediction 6.2.1 Background The Two-level Adaptive Branch Predictor exploits the correlation among a program's branches to accurately predict their outcomes [55] [56] <ref> [38] </ref> [57]. This correlation can be detected by recording the program's branch behavior at two levels, branch execution history and pattern history. The Two-Level Adaptive Branch Predictor has two key variations: global and per-address.
Reference: [39] <author> S. J. Patel, S. W. Kim, D. H. Friendly, and Y. N. Patt, </author> <title> "Enhancing the trace cache fetch mechanism," </title> <note> To appear as a University of Michigan technical report, </note> <year> 1997. </year>
Reference-contexts: They include the branch address cache [54], the collapsing buffer [10], the subgraph-level predictor [11], the multiple-block ahead branch predictor [45], and the trace cache <ref> [35, 44, 39] </ref>. The compiler-based schemes reorganize the program to increase the amount of work that can be fetched with a single icache access, eliminating the need for extra hardware. <p> If this problem is not addressed, then this increase in hardware complexity will result in increased cycle times or additional pipeline stages for an aggressive wide-issue machine, which in turn, will decrease performance. 2.1.2 The Trace Cache The trace cache <ref> [35, 44, 39] </ref> is a hardware-based scheme that does not require fetching non-consecutive blocks from the icache. Its fetch unit consists of three parts: a core fetch unit, a fill unit, and a trace cache. The core fetch unit fetches one basic block per cycle from the icache. <p> Thus, a hit in the decoded instruction cache would result in the fetch and issue of an entire basic block's worth of instructions. Later, Melvin and Patt [34] suggested using the fill unit to combine instructions from different basic blocks. Rotenberg et al. [44] and Patel et al. <ref> [39] </ref> independently proposed implementations of the trace cache that combined basic blocks as suggested by Melvin and Patt. These two trace cache implementations showed significant performance improvements over previously proposed fetch mechanisms including the branch address cache and the collapsing buffer. <p> The implementation proposed by Rotenberg et al. used the trace cache as the secondary fetch mechanism. The majority of the hardware was devoted to the icache. The implementation proposed by Patel et al. <ref> [39] </ref> used the trace cache as the primary fetch mechanism. By devoting more hardware to the trace cache, they showed that further performance improvements could be achieved.
Reference: [40] <author> Y. Patt, W. Hwu, and M. Shebanow, "HPS, </author> <title> a new microarchitecture: Rationale and introduction," </title> <booktitle> in Proceedings of the 18th Annual Microprogramming Workshop, </booktitle> <pages> pp. 103-107, </pages> <year> 1985. </year>
Reference-contexts: The compiler back end takes this representation and applies a set of target-specific optimizations and allocates registers. By using the same core compiler for both ISAs, any unfair compiler advantages one ISA may have had over the other was eliminated. 3.2 The HPS Processor Model The HPS paradigm <ref> [40, 41] </ref> is used to model the processor implementation in this study because it encompasses a set of microarchitectural techniques that are designed to achieve high performance for single instruction stream execution.
Reference: [41] <author> Y. N. Patt, S. W. Melvin, W. Hwu, and M. C. Shebanow, </author> <title> "Critical issues regarding HPS, a high performance microarchitecture," </title> <booktitle> in Proceedings of the 18th Annual Microprogramming Workshop, </booktitle> <pages> pp. 109-116, </pages> <year> 1985. </year>
Reference-contexts: The compiler back end takes this representation and applies a set of target-specific optimizations and allocates registers. By using the same core compiler for both ISAs, any unfair compiler advantages one ISA may have had over the other was eliminated. 3.2 The HPS Processor Model The HPS paradigm <ref> [40, 41] </ref> is used to model the processor implementation in this study because it encompasses a set of microarchitectural techniques that are designed to achieve high performance for single instruction stream execution.
Reference: [42] <author> D. N. Pnevmatikatos and G. S. Sohi, </author> <title> "Guarded execution and dynamic branch prediction in dynamic ILP processors," </title> <booktitle> in Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 120-129, </pages> <year> 1994. </year>
Reference-contexts: The compiler-based schemes reorganize the program to increase the amount of work that can be fetched with a single icache access, eliminating the need for extra hardware. These schemes include trace and superblock scheduling [14, 4, 22], predicated execution <ref> [20, 42] </ref>, hyperblock scheduling [29], the VLIW multi-way jump mechanism [13, 26, 12, 36], and multiscalar processors [15, 47]. 2.1 Hardware-based Approaches 2.1.1 Predicting Multiple Branches Per Cycle The branch address cache [54], the collapsing buffer [10], the subgraph-level predictor [11], and the multiple-block ahead branch predictor [45] are hardware schemes <p> more accurate than static branch prediction [53], using superblock scheduling to increase the instruction fetch rate will not be as effective as an approach that uses dynamic branch prediction to select the set of blocks to be fetched each cycle. 7 2.2.2 Predicated Execution A processor that supports predicated execution <ref> [2, 20, 43, 42] </ref> associates a predicate register with each instruction issued. The execution of an instruction proceeds normally if the predicate register resolves to a true value. The execution is suppressed if that predicate value resolves to a false value.
Reference: [43] <author> B. R. Rau, D. W. L. Yen, W. Yen, and R. A. Towle, </author> <title> "The Cydra 5 departmental supercomputer," </title> <journal> IEEE Computer, </journal> <volume> vol. 22, </volume> <pages> pp. 12-35, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: more accurate than static branch prediction [53], using superblock scheduling to increase the instruction fetch rate will not be as effective as an approach that uses dynamic branch prediction to select the set of blocks to be fetched each cycle. 7 2.2.2 Predicated Execution A processor that supports predicated execution <ref> [2, 20, 43, 42] </ref> associates a predicate register with each instruction issued. The execution of an instruction proceeds normally if the predicate register resolves to a true value. The execution is suppressed if that predicate value resolves to a false value.
Reference: [44] <author> E. Rotenberg, S. Bennett, and J. E. Smith, </author> <title> "Trace cache: A low latency approach to high bandwidth instruction fetching," </title> <type> Technical Report 1310, </type> <institution> University of Wisconsin - Madison, </institution> <month> April </month> <year> 1996. </year>
Reference-contexts: They include the branch address cache [54], the collapsing buffer [10], the subgraph-level predictor [11], the multiple-block ahead branch predictor [45], and the trace cache <ref> [35, 44, 39] </ref>. The compiler-based schemes reorganize the program to increase the amount of work that can be fetched with a single icache access, eliminating the need for extra hardware. <p> If this problem is not addressed, then this increase in hardware complexity will result in increased cycle times or additional pipeline stages for an aggressive wide-issue machine, which in turn, will decrease performance. 2.1.2 The Trace Cache The trace cache <ref> [35, 44, 39] </ref> is a hardware-based scheme that does not require fetching non-consecutive blocks from the icache. Its fetch unit consists of three parts: a core fetch unit, a fill unit, and a trace cache. The core fetch unit fetches one basic block per cycle from the icache. <p> Thus, a hit in the decoded instruction cache would result in the fetch and issue of an entire basic block's worth of instructions. Later, Melvin and Patt [34] suggested using the fill unit to combine instructions from different basic blocks. Rotenberg et al. <ref> [44] </ref> and Patel et al. [39] independently proposed implementations of the trace cache that combined basic blocks as suggested by Melvin and Patt. These two trace cache implementations showed significant performance improvements over previously proposed fetch mechanisms including the branch address cache and the collapsing buffer.
Reference: [45] <author> A. Seznec, S. Jourdan, P. Sainrat, and P. Michaud, </author> <title> "Multiple-block ahead branch predictors," </title> <booktitle> in Proceedings of the 7th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <year> 1996. </year>
Reference-contexts: Appendix A contains the corresponding figures for the seven other SPECint95 benchmarks. Various approaches have been proposed for increasing instruction fetch rate from that of a single basic block per cycle. Some approaches <ref> [54, 10, 11, 45] </ref> extend the branch predictor and icache so that multiple branch predictions can be made each cycle and multiple, nonconsecutive cache lines can be fetched each cycle. However, this extra hardware requires extra stages in the pipeline which will increase the branch misprediction penalty, decreasing performance. <p> The hardware-based schemes extend the branch predictor and icache so that multiple branch predictions can be made each cycle and multiple non-consecutive cache lines can be fetched each cycle. They include the branch address cache [54], the collapsing buffer [10], the subgraph-level predictor [11], the multiple-block ahead branch predictor <ref> [45] </ref>, and the trace cache [35, 44, 39]. The compiler-based schemes reorganize the program to increase the amount of work that can be fetched with a single icache access, eliminating the need for extra hardware. <p> 22], predicated execution [20, 42], hyperblock scheduling [29], the VLIW multi-way jump mechanism [13, 26, 12, 36], and multiscalar processors [15, 47]. 2.1 Hardware-based Approaches 2.1.1 Predicting Multiple Branches Per Cycle The branch address cache [54], the collapsing buffer [10], the subgraph-level predictor [11], and the multiple-block ahead branch predictor <ref> [45] </ref> are hardware schemes that propose different ways to extend the dynamic branch predictor so that it can make multiple branch predictions each cycle.
Reference: [46] <author> J. E. Smith, </author> <title> "A study of branch prediction strategies," </title> <booktitle> in Proceedings of the 8th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 135-148, </pages> <year> 1981. </year>
Reference-contexts: As a result, the branch history register need not shift in the full target index for each prediction. 58 6.3 Extensions to the Branch Target Buffer The Branch Target Buffer (BTB) <ref> [46, 27] </ref> must be extended so that it can keep track of all the possible control flow successors for a block. For a conventional ISA, the number of successors is limited to two, the taken and fall-through targets of the block's branch.
Reference: [47] <author> G. S. Sohi, S. E. Breach, and T. N. Vijaykumar, </author> <title> "Multiscalar processors," </title> <booktitle> in Proceedings of the 22st Annual International Symposium on Computer Architecture, </booktitle> <year> 1995. </year>
Reference-contexts: These schemes include trace and superblock scheduling [14, 4, 22], predicated execution [20, 42], hyperblock scheduling [29], the VLIW multi-way jump mechanism [13, 26, 12, 36], and multiscalar processors <ref> [15, 47] </ref>. 2.1 Hardware-based Approaches 2.1.1 Predicting Multiple Branches Per Cycle The branch address cache [54], the collapsing buffer [10], the subgraph-level predictor [11], and the multiple-block ahead branch predictor [45] are hardware schemes that propose different ways to extend the dynamic branch predictor so that it can make multiple branch <p> The compiler may have to delay the scheduling of certain operations in order to meet these requirements, lowering the instruction fetch rate of the processor. 2.2.5 Multiscalar Processors The multiscalar processing paradigm <ref> [15, 47] </ref> applies multiprocessor concepts to the design of microprocessors. A multiscalar processor consists of a set of processing elements connected in a ring. The multiscalar processor's compiler is responsible for dividing the program up into units of work, called tasks.
Reference: [48] <author> E. Sprangle and Y. Patt, </author> <title> "Facilitating superscalar processing via a combined static/dynamic register renaming scheme," </title> <booktitle> in Proceedings of the 27th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <pages> pp. 143-147, </pages> <year> 1994. </year>
Reference-contexts: Block-structured ISAs exploit the advantages of both compiler-based and hardware-based solutions by merging basic blocks together at compile-time and providing support for dynamic branch prediction. Block-structured ISAs <ref> [31, 33, 32, 48, 19] </ref> are a new class of instruction set architectures that were designed to address the performance obstacles faced by processors attempting to exploit high levels of instruction level parallelism. <p> All the test data sets used were abbreviated versions of the SPECfp95 refer ence input sets. 17 CHAPTER 4 Block-Structured ISAs and the Block Enlargement Optimization Block-structured ISAs <ref> [31, 33, 32, 48] </ref> were designed to help solve performance obstacles faced by wide issue processors. Their major distinguishing feature is that the architectural atomic unit is defined to be a group of operations, where an operation typically corresponds to an instruction in a load/store architecture.
Reference: [49] <author> Welcome to SPEC, </author> <title> The Standard Performance Evaluation Corporation. </title> <address> http://www.specbench.org/. </address>
Reference-contexts: To accomplish this, BlockSim's back end communicates its branch predictions to the front end so that the front end knows when a branch misprediction will occur and includes the appropriate instructions in the instruction trace. 3.4 The SPEC95 Benchmarks The SPEC benchmarks <ref> [49] </ref> were used to evaluate the performance of the block-structured and conventional ISAs. Tables 3.2 lists the eight SPECint95 used along with their test and training data sets. The test data sets were used to generate the performance numbers reported.
Reference: [50] <author> J. W. Stark and Y. N. Patt, </author> <title> "The effects of memory disambiguation on the performance of wide-issue, </title> <journal> dynamically-scheduled microprocessors," </journal> <note> To appear as a University of Michigan technical report, 1995. 116 </note>
Reference-contexts: Stark has shown that such load imbalances can increase the latencies of critical 66 tional ISA branch predictors. paths in the program, lowering performance <ref> [50] </ref>.
Reference: [51] <author> R. M. Tomasulo, </author> <title> "An efficient algorithm for exploiting multiple arithmetic units," </title> <journal> IBM Journal of Research and Development, </journal> <volume> vol. 11, </volume> <pages> pp. 25-33, </pages> <month> January </month> <year> 1967. </year>
Reference-contexts: Second, the block enlargement optimizations are designed to improve performance for wide-issue machines. They have little benefit for narrow issue widths. Using a generalized version of the Tomasulo algorithm <ref> [51] </ref>, the instructions' registers are dynamically renamed in parallel and sent to an appropriate node table entry to await execution. An instruction is executed as soon as its source operands are ready and a functional unit is free.
Reference: [52] <author> J. E. Wilson, S. Melvin, M. Shebanow, W. mei Hwu, and Y. N. Patt, </author> <title> "On tuning the microarchitecture of an HPS implementation of the VAX," </title> <booktitle> in Proceedings of the 20th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <pages> pp. 162-167, </pages> <year> 1987. </year>
Reference-contexts: defined to have two explicit 1 The maximum atomic block size does not necessarily have to be restricted to the issue width of the machine if the machine has the microarchitectural support to issue atomic blocks that require more than one cycle to issue (e.g. the scratch pad alias table <ref> [52] </ref>).
Reference: [53] <author> T.-Y. Yeh, </author> <title> Two-Level Adaptive Branch Prediction and Instruction Fetch Mechanisms for High Performance Superscalar Processors, </title> <type> PhD thesis, </type> <institution> University of Michigan, </institution> <year> 1993. </year>
Reference-contexts: In effect, to fetch multiple basic blocks in a cycle, the processor must follow the static branch predictions used to form the superblocks. Because dynamic branch prediction is significantly more accurate than static branch prediction <ref> [53] </ref>, using superblock scheduling to increase the instruction fetch rate will not be as effective as an approach that uses dynamic branch prediction to select the set of blocks to be fetched each cycle. 7 2.2.2 Predicated Execution A processor that supports predicated execution [2, 20, 43, 42] associates a predicate
Reference: [54] <author> T.-Y. Yeh, D. Marr, and Y. N. Patt, </author> <title> "Increasing the instruction fetch rate via multiple branch prediction and branch address cache," </title> <booktitle> in Proceedings of the International Conference on Supercomputing, </booktitle> <pages> pp. 67-76, </pages> <year> 1993. </year>
Reference-contexts: Appendix A contains the corresponding figures for the seven other SPECint95 benchmarks. Various approaches have been proposed for increasing instruction fetch rate from that of a single basic block per cycle. Some approaches <ref> [54, 10, 11, 45] </ref> extend the branch predictor and icache so that multiple branch predictions can be made each cycle and multiple, nonconsecutive cache lines can be fetched each cycle. However, this extra hardware requires extra stages in the pipeline which will increase the branch misprediction penalty, decreasing performance. <p> The hardware-based schemes extend the branch predictor and icache so that multiple branch predictions can be made each cycle and multiple non-consecutive cache lines can be fetched each cycle. They include the branch address cache <ref> [54] </ref>, the collapsing buffer [10], the subgraph-level predictor [11], the multiple-block ahead branch predictor [45], and the trace cache [35, 44, 39]. The compiler-based schemes reorganize the program to increase the amount of work that can be fetched with a single icache access, eliminating the need for extra hardware. <p> These schemes include trace and superblock scheduling [14, 4, 22], predicated execution [20, 42], hyperblock scheduling [29], the VLIW multi-way jump mechanism [13, 26, 12, 36], and multiscalar processors [15, 47]. 2.1 Hardware-based Approaches 2.1.1 Predicting Multiple Branches Per Cycle The branch address cache <ref> [54] </ref>, the collapsing buffer [10], the subgraph-level predictor [11], and the multiple-block ahead branch predictor [45] are hardware schemes that propose different ways to extend the dynamic branch predictor so that it can make multiple branch predictions each cycle.
Reference: [55] <author> T.-Y. Yeh and Y. N. Patt, </author> <title> "Two-level adaptive branch prediction," </title> <booktitle> in Proceedings of the 24th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <pages> pp. 51-61, </pages> <year> 1991. </year>
Reference-contexts: So from a probabilistic viewpoint, the number of branches predicted each cycle by a predictor should not affect its overall prediction accuracy. 54 Branch Predictor. 6.2 Two-Level Adaptive Branch Prediction 6.2.1 Background The Two-level Adaptive Branch Predictor exploits the correlation among a program's branches to accurately predict their outcomes <ref> [55] </ref> [56] [38] [57]. This correlation can be detected by recording the program's branch behavior at two levels, branch execution history and pattern history. The Two-Level Adaptive Branch Predictor has two key variations: global and per-address.
Reference: [56] <author> T.-Y. Yeh and Y. N. Patt, </author> <title> "Alternative implementations of two-level adaptive branch prediction," </title> <booktitle> in Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 124-134, </pages> <year> 1992. </year>
Reference-contexts: So from a probabilistic viewpoint, the number of branches predicted each cycle by a predictor should not affect its overall prediction accuracy. 54 Branch Predictor. 6.2 Two-Level Adaptive Branch Prediction 6.2.1 Background The Two-level Adaptive Branch Predictor exploits the correlation among a program's branches to accurately predict their outcomes [55] <ref> [56] </ref> [38] [57]. This correlation can be detected by recording the program's branch behavior at two levels, branch execution history and pattern history. The Two-Level Adaptive Branch Predictor has two key variations: global and per-address. <p> The current value in the branch history register is used as an index into that table. The prediction is made based on the contents of the specified table entry. Research has shown that the two-bit saturating up-down counter serves as an effective PHT entry <ref> [56] </ref>. The counter is incremented each time the branch is taken and decremented each time the branch is not taken. If the counter's value falls into the lower half of the range 55 of possible values, the PHT entry produces a not taken prediction. <p> The two component predictors used were the gshare variation [30] and the PAs variation <ref> [56] </ref> of the Two-Level Adaptive Branch Predictor. The gshare variation records global branch history in its branch history register. The direction of every branch that occurred in the dynamic instruction is recorded in the history register.
Reference: [57] <author> T.-Y. Yeh and Y. N. Patt, </author> <title> "A comparison of dynamic branch predictors that use two levels of branch history," </title> <booktitle> in Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 257-266, </pages> <year> 1993. </year>
Reference-contexts: a probabilistic viewpoint, the number of branches predicted each cycle by a predictor should not affect its overall prediction accuracy. 54 Branch Predictor. 6.2 Two-Level Adaptive Branch Prediction 6.2.1 Background The Two-level Adaptive Branch Predictor exploits the correlation among a program's branches to accurately predict their outcomes [55] [56] [38] <ref> [57] </ref>. This correlation can be detected by recording the program's branch behavior at two levels, branch execution history and pattern history. The Two-Level Adaptive Branch Predictor has two key variations: global and per-address.
Reference: [58] <author> C. Young and M. D. Smith, </author> <title> "Improving the accuracy of static branch prediction using branch correlation," </title> <booktitle> in Proceedings of the 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 232-241, </pages> <year> 1994. </year> <month> cxvii </month>
Reference-contexts: By inlining the function at each of its call sites, 1 Young and Smith observed this correlation between function call site and branch behavior and used it to improve static branch prediction accuracy <ref> [58] </ref>. 73 enlargement optimization. a separate copy of the branches would be created for each call site. This would enable the branch predictor to tailor its predictions to the specific dynamic behavior of each inlined copy of the branch.
References-found: 59

