URL: http://www.cs.bu.edu/techreports/93-007-mermera-warp.ps.Z
Refering-URL: http://cs-www.bu.edu/techreports/Home.html
Root-URL: 
Title: Using Warp to Control Network Contention in Mermera  
Author: Abdelsalam Heddaya Himanshu Sinha 
Keyword: Distributed non-coherent shared memory, network contention, flow control, itera tive methods, Isis.  
Note: To appear in Proc. 27th Hawaii Int'l Conf. on System Sciences, IEEE Comp. Soc. Press,  This research was supported in part by NSF under grants IRI-8910195, IRI-9041581 and CDA-8920936. This author was additionally supported by NSF grant CCR-9204284. Present address:  
Address: Park  40 Sylvan Road, Waltham MA 02254.  
Affiliation: Kihong  GTE Laboratories,  
Email: heddaya@cs.bu.edu  park@cs.bu.edu  hss@cs.bu.edu  
Date: June 4, 1993 Revised September 25, 1993  January 1994.  
Abstract: Parallel computing on a network of workstations can saturate the communication network, leading to excessive message delays and consequently poor application performance. We examine empirically the consequences of integrating a flow control protocol, called Warp control [Par93], into Mermera, a software shared memory system that supports parallel computing on distributed systems [HS93]. For an asynchronous iterative program that solves a system of linear equations, our measurements show that Warp succeeds in stabilizing the network's behavior even under high levels of contention. As a result, the application achieves a higher effective communication throughput, and a reduced completion time. In some cases, however, Warp control does not achieve the performance attainable by fixed size buffering when using a statically optimal buffer size. Our use of Warp to regulate the allocation of network bandwidth emphasizes the possibility for integrating it with the allocation of other resources, such as CPU cycles and disk bandwidth, so as to optimize overall system throughtput, and enable fully-shared execution of parallel programs. 
Abstract-found: 1
Intro-found: 1
Reference: [Bir93] <author> Kenneth Birman. </author> <title> Congestion control in Isis. </title> <type> Private communication, </type> <month> May </month> <year> 1993. </year>
Reference-contexts: Isis uses several schemes, among them a simple heuristic that, based on the number of buffered messages at a node, determines whether congestion exists or not. Once Isis decides that congestion exists, it stops sending messages for a long period of time (1 second) <ref> [Bir93] </ref>, thus causing a large spike in warp. Smaller spikes may be explained by the Ethernet's back-off mechanism when collisions occur [Tan88]. large spikes in warp, and consequently in message delay.
Reference: [BJ87] <author> K. Birman and T.A. Joseph. </author> <title> Exploiting virtual synchrony in distributed systems. </title> <booktitle> In Proc. 11th ACM Symp. on Operating System Principles, </booktitle> <address> Austin, Texas, </address> <pages> pages 123-138, </pages> <month> Nov. </month> <year> 1987. </year>
Reference-contexts: We can say that these operations are asynchronous with respect to communication, and computations that use them can, in principle, submit new values for transmission at a sustained rate greater than the network capacity. Writes are propagated by broadcasting the values to other processes. We use the Isis toolkit <ref> [BJ87, BSS91] </ref> for our implementation because it provides a suite of multicast protocols that satisfy different ordering properties. The broadcasts of interest to us are abcast (), f bcast () and mbcast (). These primitives have different constraints on the order in which the messages are delivered to their destinations.
Reference: [BSS91] <author> Kenneth Birman, Andre Schiper, and Pat Stephenson. </author> <title> Lightweight causal and atomic group multicast. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 9(3) </volume> <pages> 272-314, </pages> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: We can say that these operations are asynchronous with respect to communication, and computations that use them can, in principle, submit new values for transmission at a sustained rate greater than the network capacity. Writes are propagated by broadcasting the values to other processes. We use the Isis toolkit <ref> [BJ87, BSS91] </ref> for our implementation because it provides a suite of multicast protocols that satisfy different ordering properties. The broadcasts of interest to us are abcast (), f bcast () and mbcast (). These primitives have different constraints on the order in which the messages are delivered to their destinations.
Reference: [BT89] <author> Dimitri P. Bertsekas and John N. Tsitsiklis. </author> <title> Parallel and Distributed Computation: Numerical Methods. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1989. </year>
Reference-contexts: By contrast, coherent write operations are inherently "synchronous" in that they do not return until some message is successfully sent and received. Applications such as asynchronous iterative algorithms <ref> [BT89] </ref> and oblivious computations [LS88], are thus capable of causing high and sustained network contention. Operating systems that aim to support parallel computing, must therefore offer mechanisms for controlling network contention to a higher degree than is available today. <p> For example, in asynchronous iterative algorithms that solve fixed-point problems arising in various applications <ref> [BT89] </ref>, convergence rate is enhanced if the application can keep its computation/communication ratio, which is variable, compatible with i b. This way, no time is wasted in generating messages that may get delayed due to unnecessary buffering.
Reference: [CBZ91] <author> John B. Carter, John K. Bennett, and Willy Zwaenopoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Proc. 13th ACM Symp. on Operating System Principles, Asilomar, California, </booktitle> <pages> pages 152-164, </pages> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: Numerous software shared memory systems have been proposed in the literature, but most of them provide coherence at the level of sequential consistency or stricter. Recently, some systems started to allow weaker conditions. Examples include the DASH multiprocessor [LLJ + 93], the Munin software shared memory system <ref> [CBZ91] </ref>, and transactional memory [HM92], which allow sequential consistency to be broken, but only during the execution of code blocks explicitly marked by the programmer.
Reference: [GK80] <author> M. Gerla and L. Kleinrock. </author> <title> Flow control: a comparative survey. </title> <journal> IEEE Trans. Comm., </journal> <volume> COM-28:553-574, </volume> <year> 1980. </year>
Reference-contexts: For a related discussion of flow control algorithms, see <ref> [GK80, HW91] </ref>. Let N be the network, and let p 1 ; p 2 ; : : : ; p n be the network nodes.
Reference: [HA90] <author> P.W. Hutto and M. Ahamad. </author> <title> Slow memory: weakening consistency to enhance concur-rency in distributed shared memories. </title> <booktitle> In Proc. 10th IEEE Intl. Conference on Distributed Computing Systems, </booktitle> <address> Paris, France, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Processes that comprise a parallel program communicate through read and write operations to the shared memory provided by Mermera. Several different memory behaviors are supported, including one coherent 1 , and three non-coherent (Pipelined Random Access Memory [LS88], Slow Memory <ref> [HA90] </ref>, and Local Consistency [HS93]). A key property of non-coherent memory operations supported by Mermera is that they are "asynchronous" in that they do not wait for any communication to be performed before they return.
Reference: [HM92] <author> M. Herlihy and J.E.B. Moss. </author> <title> Transactional Memory: architectural support for lock-free data structures. </title> <type> Technical Report CRL 92/07, </type> <institution> Digital Equipment Corp., Cambridge Research Lab, </institution> <month> Dec. </month> <year> 1992. </year> <month> 15 </month>
Reference-contexts: Recently, some systems started to allow weaker conditions. Examples include the DASH multiprocessor [LLJ + 93], the Munin software shared memory system [CBZ91], and transactional memory <ref> [HM92] </ref>, which allow sequential consistency to be broken, but only during the execution of code blocks explicitly marked by the programmer.
Reference: [HS92] <author> Abdelsalam Heddaya and Himanshu S. Sinha. </author> <title> Coherence, non-coherence and Local Con--sistency in distributed shared memory for parallel computing. </title> <type> Technical Report BU-CS-92-004, </type> <institution> Boston University, Computer Sciene Dept., </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: It consists of a protocol suite, of which rate adjustment protocol (RAP) is the basic one used in the experiments. The network estimator is called warp, and it uses time-stamps to 1 Our definition of coherence, first advanced in <ref> [HS92] </ref> is equivalent to that of sequential consistency [Lam79]. 1 estimate relative change in network delay. We will often use "Warp" (capitalized) to refer to the protocol itself. In case of single server queues, warp where is the network utilization (i.e., throughput).
Reference: [HS93] <author> Abdelsalam Heddaya and Himanshu S. Sinha. </author> <title> An overview of mermera: a system and formalism for non-coherent distributed parallel memory. </title> <booktitle> In Proc. 26th Hawaii International Conference on System Sciences, Maui, Hawaii, </booktitle> <pages> pages 164-173, </pages> <month> Jan. 5-8 </month> <year> 1993. </year>
Reference-contexts: Mermera is an example of a system that encourages applications to generate high traffic volumes, causing network congestion. It is a software shared memory system that has been developed to support parallel computing on a workstation network <ref> [HS93, Sin93] </ref>. Processes that comprise a parallel program communicate through read and write operations to the shared memory provided by Mermera. Several different memory behaviors are supported, including one coherent 1 , and three non-coherent (Pipelined Random Access Memory [LS88], Slow Memory [HA90], and Local Consistency [HS93]). <p> Processes that comprise a parallel program communicate through read and write operations to the shared memory provided by Mermera. Several different memory behaviors are supported, including one coherent 1 , and three non-coherent (Pipelined Random Access Memory [LS88], Slow Memory [HA90], and Local Consistency <ref> [HS93] </ref>). A key property of non-coherent memory operations supported by Mermera is that they are "asynchronous" in that they do not wait for any communication to be performed before they return.
Reference: [HW91] <author> Z. Haas and J. Winters. </author> <title> Congestion control by adaptive admission. </title> <booktitle> In Proc. IEEE INFOCOM, </booktitle> <pages> pages 560-569, </pages> <year> 1991. </year>
Reference-contexts: For a related discussion of flow control algorithms, see <ref> [GK80, HW91] </ref>. Let N be the network, and let p 1 ; p 2 ; : : : ; p n be the network nodes.
Reference: [Lam79] <author> Leslie Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multi-process programs. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-28(9):690-691, </volume> <month> Sep. </month> <year> 1979. </year>
Reference-contexts: It consists of a protocol suite, of which rate adjustment protocol (RAP) is the basic one used in the experiments. The network estimator is called warp, and it uses time-stamps to 1 Our definition of coherence, first advanced in [HS92] is equivalent to that of sequential consistency <ref> [Lam79] </ref>. 1 estimate relative change in network delay. We will often use "Warp" (capitalized) to refer to the protocol itself. In case of single server queues, warp where is the network utilization (i.e., throughput). <p> Section 7 discusses our conclusions. 2 Mermera Mermera supports several different memory behaviors, including: * Coherent Memory: All processes agree on the order of all writes, a definition that can be shown to be equivalent to sequential consistency <ref> [Lam79] </ref>. More precisely, if a process observes some writes in a certain order then no other process observes those writes in a different order.
Reference: [LLJ + 93] <author> D. Lenoski, J. Laudon, T. Joe, D. Nakahira, L. Stevens, A. Gupta, and J. Hennessy. </author> <title> The DASH prototype: logic overhead and performance. </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> 4(1) </volume> <pages> 41-61, </pages> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: Numerous software shared memory systems have been proposed in the literature, but most of them provide coherence at the level of sequential consistency or stricter. Recently, some systems started to allow weaker conditions. Examples include the DASH multiprocessor <ref> [LLJ + 93] </ref>, the Munin software shared memory system [CBZ91], and transactional memory [HM92], which allow sequential consistency to be broken, but only during the execution of code blocks explicitly marked by the programmer.
Reference: [LS88] <author> R.J. Lipton and J.S. Sandberg. </author> <title> PRAM: a scalable shared memory. </title> <type> Technical Report CS-TR-180-88, </type> <institution> Princeton Univ., Dept. of Computer Science, </institution> <month> Sep. </month> <year> 1988. </year>
Reference-contexts: Processes that comprise a parallel program communicate through read and write operations to the shared memory provided by Mermera. Several different memory behaviors are supported, including one coherent 1 , and three non-coherent (Pipelined Random Access Memory <ref> [LS88] </ref>, Slow Memory [HA90], and Local Consistency [HS93]). A key property of non-coherent memory operations supported by Mermera is that they are "asynchronous" in that they do not wait for any communication to be performed before they return. <p> By contrast, coherent write operations are inherently "synchronous" in that they do not return until some message is successfully sent and received. Applications such as asynchronous iterative algorithms [BT89] and oblivious computations <ref> [LS88] </ref>, are thus capable of causing high and sustained network contention. Operating systems that aim to support parallel computing, must therefore offer mechanisms for controlling network contention to a higher degree than is available today.
Reference: [Par93] <author> Kihong Park. </author> <title> Warp control: a dynamically stable congestion protocol and its analysis. </title> <booktitle> In Proc. ACM SIGCOMM '93, </booktitle> <pages> pages 137-147, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: To study the question, and to try to formulate requirements for the likely candidate mechanisms for this purpose, we implemented, tested and measured the Warp control protocol under Mermera. Warp control <ref> [Par93] </ref> is a distributed end-to-end flow control protocol which uses a time-stamp based estimator of network state to adaptively throttle arrival rates so as to optimize network throughput. It consists of a protocol suite, of which rate adjustment protocol (RAP) is the basic one used in the experiments. <p> value returned to process i by a read operation is the value written by x:w, or is a value written by a process that observed x:w. 2 One method for modeling network congestion is to assume a functional relationship between network load and service rate that has a unimodal shape <ref> [Par93] </ref>. 2 * Pipelined RAM (PRAM): This behavior shares only the acronym with the Parallel Ran--dom Access Machine, a theoretical model of parallelism often used to design simple parallel algorithms. <p> Message decoding protocol (MDP): 1. warp := t 2 hist [i]:last in m ij :time stamphist [i]:last out 2. hist [i]:last in := t 2 3. hist [i]:last out := m ij :time stamp It can be shown <ref> [Par93] </ref>, under the assumption of N being a FCFS single-server queue and jd=dtj t 1, that warp . Therefore, to achieve and maintain maximum utilization, the following control law can be employed, d = *(1 warp) (1) where * &gt; 0 is a constant adjustment factor. <p> On the positive side, this makes the system robust with respect to noise, guaranteeing asymptotic stability even under dynamic perturbation. The issues of better responsiveness, fairness and stability under unimodal load-service rate functions are discussed in <ref> [Par93] </ref>. Distributed control is achieved by noting that = P n i=1 i . That is, at every p j , the following update protocol is executed after performing MDP: Rate adjustment protocol (RAP): 1. j (t) := j (t 1) + * j (1 warp). 5 2. <p> Figure 6 shows a simulation run of a 10 node system without a fairness protocol (left figure), and with one (right figure). The output is taken from <ref> [Par93] </ref> and shows the variation in bandwidth allocation when fairness is not regulated. The message generation rates of the first 5 nodes are shown, and the drift in individual arrival rates is clearly visible. For a discussion of the fairness protocol, see [Par93]. <p> The output is taken from <ref> [Par93] </ref> and shows the variation in bandwidth allocation when fairness is not regulated. The message generation rates of the first 5 nodes are shown, and the drift in individual arrival rates is clearly visible. For a discussion of the fairness protocol, see [Par93]. A prerequisite for reducing variation and maintaining even progress in computation over all 13 participating nodes is that packets sent out from the nodes not be too long. <p> Our measurements also give experimental insight into the dynamic behavior of Warp itself, confirming the theoretical predictions and large-scale simulations of <ref> [Par93] </ref>, given for a general packet or circuit-switched queueing system. Furthermore, our data revealed to us an undocumented aspect of the Isis toolkit, which is the technique it uses to control congestion.
Reference: [Sin93] <author> Himanshu Sinha. Mermera: </author> <title> Non-coherent Distributed Shared Memory for Parallel Computing. </title> <type> PhD thesis, </type> <institution> Boston University, Computer Science Department, </institution> <address> 111 Cum-mington Street, Boston, MA 02215, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Mermera is an example of a system that encourages applications to generate high traffic volumes, causing network congestion. It is a software shared memory system that has been developed to support parallel computing on a workstation network <ref> [HS93, Sin93] </ref>. Processes that comprise a parallel program communicate through read and write operations to the shared memory provided by Mermera. Several different memory behaviors are supported, including one coherent 1 , and three non-coherent (Pipelined Random Access Memory [LS88], Slow Memory [HA90], and Local Consistency [HS93]).
Reference: [Tan88] <author> Andrew S. Tanenbaum. </author> <title> Computer Networks. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <note> second edition, 1988. 16 </note>
Reference-contexts: Once Isis decides that congestion exists, it stops sending messages for a long period of time (1 second) [Bir93], thus causing a large spike in warp. Smaller spikes may be explained by the Ethernet's back-off mechanism when collisions occur <ref> [Tan88] </ref>. large spikes in warp, and consequently in message delay. We see in the right hand plot that the spikes subside after a thousand messages or so have been delivered.
References-found: 17

