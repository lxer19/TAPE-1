URL: ftp://ftpipr.ira.uka.de/pub/papers/1996/fgml96mk.ps.gz
Refering-URL: ftp://ftpipr.ira.uka.de/.public_html/papersna.html
Root-URL: 
Title: Learning from undiscounted delayed rewards  
Author: Michael Kaiser, Markus Riepp 
Address: D-76128 Karlsruhe, Germany  
Affiliation: University of Karlsruhe, Institute for Real-Time Computer Systems Robotics,  
Note: In: GI Fachgruppentreffen Maschinelles Lernen, Chemnitz,  
Email: E-Mail: kaiser@ira.uka.de  
Phone: Phone: +49 721 6084051 Fax: +49 721 606740  
Date: August 1996  
Abstract: The general framework of reinforcement learning has been proposed by several researchers for both the solution of optimization problems and the realization of adaptive control schemes. To allow for an efficient application of reinforcement learning in either of these areas, it is necessary to solve both the structural and the temporal credit assignment problem. In this paper, we concentrate on the latter which is usually tackled through the use of learning algorithms that employ discounted rewards. We argue that for realistic problems this kind of solution is not satisfactory, since it does not address the effect of noise originating from different experiences and does not allow for an easy explanation of the parameters involved in the learning process. As a possible solution, we propose to keep the delayed reward undiscounted, but to discount the actual adaptation rate. Empirical results show that dependent on the kind of discount used amore stable convergence and even an increase in performance can be obtained. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> A. G. Barto, R. S. Sutton, and C. W. Anderson. </author> <title> Neuronlike elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <pages> pages 835-846, </pages> <year> 1983. </year>
Reference-contexts: 1 Introduction Reinforcement learning <ref> [1] </ref> is a kind of learning that can be seen as located between the completely supervised and the unsupervised paradigms. The learning system (often called an agent) receives feedback (rewards and punishments) from the world.
Reference: 2. <author> A. G. Barto, R. S. Sutton, and C. J. C. H. Watkins. </author> <title> Sequential decision problems and neural networks. </title> <booktitle> In Advances in Neural Information Processing Systems 3 (NIPS-3), </booktitle> <year> 1990. </year>
Reference-contexts: This feedback depends on the current state of the world, the agent's actions, and some notion of goal which may be given in a non-operational form, such as "survive." The learning task is to construct an optimal policy for solving the given problem <ref> [2] </ref> by learning to maximize the long-term reward (also called expected return, discounted cumulative reinforcement, etc.) that the system obtains if it takes a particular action in a particular state.
Reference: 3. <author> K. Berns, R. Dillmann, and U. Zachmann. </author> <title> Reinforcement learning for the control of an autonomous mobile robot. </title> <booktitle> In Proceedings of the IEEE/RSJ Conference on Intelligent Robots and Systems, </booktitle> <address> Raleigh, NC, </address> <year> 1992. </year>
Reference-contexts: Because of its relation to adaptive control techniques [13] and its generally low demands on available background knowledge, reinforcement learning has found wide use especially in the area of robot learning <ref> [3, 7, 9, 5, 6] </ref>. However, in real applications often problems such as strong dependency of achieved results on parameterizations of the learning technique, unsatisfactory convergence and the need for exploration combined with insecure convergence criteria occur which make the successful application of these techniques very difficult [5, 6].
Reference: 4. <author> P. Dayan. </author> <title> The convergence of TD() for general . Machine Learning, </title> <booktitle> 8 </booktitle> <pages> 341-362, </pages> <year> 1992. </year>
Reference-contexts: Both the convergence of QLearning [15] and of T D () <ref> [4] </ref> have been proven. During actual learning, also some amount of exploration has to take place. In standard QLearning, a Boltzmann distribution P (ajx) = e Q (x;a)=Temp P b2actions e Q (x;b)=T emp is used to this aim. Here, the temperature T emp determines the amount of exploration.
Reference: 5. <author> V. Gullapalli, J. A. Franklin, and H. Benbrahim. </author> <title> Acquiring robot skills via reinforcement learning. </title> <journal> IEEE Control Systems Magazine, </journal> <volume> 14(1):13 - 24, </volume> <year> 1994. </year>
Reference-contexts: Because of its relation to adaptive control techniques [13] and its generally low demands on available background knowledge, reinforcement learning has found wide use especially in the area of robot learning <ref> [3, 7, 9, 5, 6] </ref>. However, in real applications often problems such as strong dependency of achieved results on parameterizations of the learning technique, unsatisfactory convergence and the need for exploration combined with insecure convergence criteria occur which make the successful application of these techniques very difficult [5, 6]. <p> However, in real applications often problems such as strong dependency of achieved results on parameterizations of the learning technique, unsatisfactory convergence and the need for exploration combined with insecure convergence criteria occur which make the successful application of these techniques very difficult <ref> [5, 6] </ref>. Throughout this paper, we analyze the dependency of both conventional QLearning [14] and its temporal-difference [12] version (e.g., [7]) on their respective parameterization. In addition, we reconsider the problem of using exponentially discounted rewards for handling the temporal credit assignment problem.
Reference: 6. <author> M. Kaiser, A. Retey, and R. Dillmann. </author> <title> Designing neural networks for adaptive control. </title> <booktitle> In IEEE International Conference on Decision and Control (34th CDC), </booktitle> <year> 1995. </year>
Reference-contexts: Because of its relation to adaptive control techniques [13] and its generally low demands on available background knowledge, reinforcement learning has found wide use especially in the area of robot learning <ref> [3, 7, 9, 5, 6] </ref>. However, in real applications often problems such as strong dependency of achieved results on parameterizations of the learning technique, unsatisfactory convergence and the need for exploration combined with insecure convergence criteria occur which make the successful application of these techniques very difficult [5, 6]. <p> However, in real applications often problems such as strong dependency of achieved results on parameterizations of the learning technique, unsatisfactory convergence and the need for exploration combined with insecure convergence criteria occur which make the successful application of these techniques very difficult <ref> [5, 6] </ref>. Throughout this paper, we analyze the dependency of both conventional QLearning [14] and its temporal-difference [12] version (e.g., [7]) on their respective parameterization. In addition, we reconsider the problem of using exponentially discounted rewards for handling the temporal credit assignment problem.
Reference: 7. <author> L. J. Lin. </author> <title> Reinforcement learning for robots using neural networks. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <year> 1993. </year>
Reference-contexts: Because of its relation to adaptive control techniques [13] and its generally low demands on available background knowledge, reinforcement learning has found wide use especially in the area of robot learning <ref> [3, 7, 9, 5, 6] </ref>. However, in real applications often problems such as strong dependency of achieved results on parameterizations of the learning technique, unsatisfactory convergence and the need for exploration combined with insecure convergence criteria occur which make the successful application of these techniques very difficult [5, 6]. <p> Throughout this paper, we analyze the dependency of both conventional QLearning [14] and its temporal-difference [12] version (e.g., <ref> [7] </ref>) on their respective parameterization. In addition, we reconsider the problem of using exponentially discounted rewards for handling the temporal credit assignment problem. <p> (x (t); a (t)) + Q (x (t); a (t)) where Q (x (t); a (t)) = r t + fl f [(1 )Q max (x (t + 1)) + Q (x (t + 1); a (t + 1))]; such that for = 0 the original learning law is retained <ref> [7, 10] </ref>. Both the convergence of QLearning [15] and of T D () [4] have been proven. During actual learning, also some amount of exploration has to take place.
Reference: 8. <author> S. Mahadevan. </author> <title> To discount or not to discount in reinforcement learning: a case study comparing R learning and Q learning. </title> <booktitle> In Machine Learning: Proceedings of the Eleventh International Conference, </booktitle> <pages> pages 164-172, </pages> <year> 1994. </year>
Reference-contexts: A similar line of thought has motivated Schwartz' [11] work on RLearning, which is based on the idea of optimizing average rewards instead of discounted rewards. However, comparisons between QLearning and RLearning <ref> [8] </ref> have shown that RLearning depends even more on the choice of parameters and the selected exploration strategy. Therefore, the basic idea behind our DLR 1 algorithm, which can, for example, easily be integrated into QLearning has been to not discount rewards but to discount the actual adaptation rate.
Reference: 9. <author> J. del R. Millan. </author> <title> Learning efficient reactive behavioral sequences from basic reflexes in a goal-directed autonomous robot. </title> <booktitle> In Proceedings of the third International Conference on Simulation of Adaptive Behavior, </booktitle> <year> 1994. </year>
Reference-contexts: Because of its relation to adaptive control techniques [13] and its generally low demands on available background knowledge, reinforcement learning has found wide use especially in the area of robot learning <ref> [3, 7, 9, 5, 6] </ref>. However, in real applications often problems such as strong dependency of achieved results on parameterizations of the learning technique, unsatisfactory convergence and the need for exploration combined with insecure convergence criteria occur which make the successful application of these techniques very difficult [5, 6].
Reference: 10. <author> M. Riepp. </author> <title> Analyse verschiedener Ansatze zur Losung des ,,Temporal Credit Assignment" Problems. </title> <institution> Studienar-beit, Universitat Karlsruhe, Fakultat fur Informatik, Institut fur Prozerechentechnik und Robotik, </institution> <year> 1996. </year>
Reference-contexts: (x (t); a (t)) + Q (x (t); a (t)) where Q (x (t); a (t)) = r t + fl f [(1 )Q max (x (t + 1)) + Q (x (t + 1); a (t + 1))]; such that for = 0 the original learning law is retained <ref> [7, 10] </ref>. Both the convergence of QLearning [15] and of T D () [4] have been proven. During actual learning, also some amount of exploration has to take place. <p> Because results in the scene shown in Fig. 3 are rather typical, we'll describe these results. A more detailed description of experiments and results obtained in a variety of settings can be found in <ref> [10] </ref>. Fig. 3.: Task chosen for evaluation. S = start, G = goal.
Reference: 11. <author> A. Schwartz. </author> <title> A reinforcement learning method for maximizing undiscounted rewards. </title> <booktitle> In Machine Learning: Proceedings of the Tenth International Conference, </booktitle> <pages> pages 298-305, </pages> <year> 1993. </year>
Reference-contexts: While exponential temporal discount aids in many cases the formal analysis of the learning algorithms' properties, it may not be adequate with respect to the real learning task, which mostly consists of finding optimal action sequences. A similar line of thought has motivated Schwartz' <ref> [11] </ref> work on RLearning, which is based on the idea of optimizing average rewards instead of discounted rewards. However, comparisons between QLearning and RLearning [8] have shown that RLearning depends even more on the choice of parameters and the selected exploration strategy.
Reference: 12. <author> R. S. Sutton. </author> <title> Learning to predict by methods of temporal difference. </title> <journal> Machine Learning, </journal> <volume> 3:9 - 44, </volume> <year> 1988. </year>
Reference-contexts: Throughout this paper, we analyze the dependency of both conventional QLearning [14] and its temporal-difference <ref> [12] </ref> version (e.g., [7]) on their respective parameterization. In addition, we reconsider the problem of using exponentially discounted rewards for handling the temporal credit assignment problem.
Reference: 13. <author> R. S. Sutton, A. G. Barto, and R. J. Williams. </author> <title> Reinforcement learning is direct adaptive control. </title> <journal> IEEE Control Systems Magazine, </journal> <pages> pages 19 - 22, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: Because of its relation to adaptive control techniques <ref> [13] </ref> and its generally low demands on available background knowledge, reinforcement learning has found wide use especially in the area of robot learning [3, 7, 9, 5, 6].
Reference: 14. <author> C. J. C. H. Watkins. </author> <title> Learning with delayed rewards. </title> <type> PhD thesis, </type> <institution> University of Cambridge, </institution> <year> 1989. </year>
Reference-contexts: Throughout this paper, we analyze the dependency of both conventional QLearning <ref> [14] </ref> and its temporal-difference [12] version (e.g., [7]) on their respective parameterization. In addition, we reconsider the problem of using exponentially discounted rewards for handling the temporal credit assignment problem. <p> The discounted reward r t is the basis for all further learning steps, independent on the kind of learning technique that's actually used. E.g., in standard QLearning <ref> [14] </ref>, the learning law is given as Q new (x (t); a (t)) = (1 )Q old (x (t); a (t)) + (r t + fl f Q max (x (t + 1))); where r t = rfl T t ; fl f with 0 fl f 1 is an additional
Reference: 15. <author> C. J. C. H. Watkins and P. </author> <title> Dayan. </title> <journal> Q-learning. Machine Learning, </journal> <volume> 8 </volume> <pages> 279-292, </pages> <year> 1992. </year>
Reference-contexts: Both the convergence of QLearning <ref> [15] </ref> and of T D () [4] have been proven. During actual learning, also some amount of exploration has to take place. In standard QLearning, a Boltzmann distribution P (ajx) = e Q (x;a)=Temp P b2actions e Q (x;b)=T emp is used to this aim.
References-found: 15

