URL: ftp://psyche.mit.edu/pub/jordan/facthmm.ps.Z
Refering-URL: http://www.ai.mit.edu/projects/jordan.html
Root-URL: 
Email: zoubin@cs.toronto.edu  jordan@psyche.mit.edu  
Title: Factorial Hidden Markov Models  
Author: Zoubin Ghahramani Michael I. Jordan 
Address: Toronto, ON M5S 3H5, Canada  Cambridge, MA 02139, USA  
Affiliation: Department of Computer Science University of Toronto  Department of Brain Cognitive Sciences Massachusetts Institute of Technology  
Abstract: One of the basic probabilistic tools used for time series modeling is the hidden Markov model (HMM). In an HMM, information about the past of the time series is conveyed through a single discrete variable|the hidden state. We present a generalization of HMMs in which this state is factored into multiple state variables and is therefore represented in a distributed manner. Both inference and learning in this model depend critically on computing the posterior probabilities of the hidden state variables given the observations. We present an exact algorithm for inference in this model, and relate it to the Forward-Backward algorithm for HMMs and to algorithms for more general belief networks. Due to the combinatorial nature of the hidden state representation, this exact algorithm is intractable. As in other intractable systems, approximate inference can be carried out using Gibbs sampling or mean field theory. We also present a structured approximation in which the the state variables are decoupled, based on which we derive a tractable learning algorithm. Empirical comparisons suggest that these approximations are efficient and accurate alternatives to the exact methods. Finally, we use the structured approximation to model Bach's chorales and show that it outperforms HMMs in capturing the complex temporal patterns in this dataset.
Abstract-found: 1
Intro-found: 1
Reference: <author> Ackley, D., Hinton, G., and Sejnowski, T. </author> <year> (1985). </year> <title> A learning algorithm for Boltzmann machines. </title> <journal> Cognitive Science, </journal> <volume> 9 </volume> <pages> 147-169. </pages>
Reference-contexts: While the JLO algorithm can still be used to propagate information exactly in chain graphs, such undirected links cause the normalization constant of the probability distribution| the partition function|to depend on the coupling parameters. Like in Boltzmann machines <ref> (Ackley et al., 1985) </ref>, both a clamped and an unclamped phase are therefore required for learning, where the goal of the unclamped phase is to compute the derivative of the partition function with respect to the parameters (Neal, 1992). 5.3 Conditioning on inputs Like the hidden Markov model, the factorial HMM <p> M Y P (S 1 jX 1 )P (Y 1 jS 1 ; X 1 ) T Y M Y P (S t jS t1 ; X t )P (Y t jS t ; X t ): (19) 1 This is analogous to the fully-connected Boltzmann machine with N units <ref> (Ackley et al., 1985) </ref>, in which every binary unit is coupled to every other unit using O (N 2 ) parameters, rather than the O (2 N ) parameters required to specify the complete probability table. 16 The model depends on the specification of P (Y t jS (m) (m) (m)
Reference: <author> Baldi, P., Chauvin, Y., Hunkapiller, T., and McClure, M. </author> <year> (1994). </year> <title> Hidden Markov models of biological primary sequence information. </title> <booktitle> Proc. </booktitle> <institution> Nat. Acad. Sci. (USA), </institution> <month> 91(3) </month> <pages> 1059-1063. </pages>
Reference-contexts: Due to the simplicity and efficiency of its parameter estimation algorithm, the hidden Markov model (HMM) has emerged as one of the basic statistical tools for modeling discrete time series, finding widespread application in the areas of speech recognition (Rabiner and Juang, 1986) and computational molecular biology <ref> (Baldi et al., 1994) </ref>. An HMM is essentially a mixture model, encoding information about the history of a time series in the value of a single multinomial variable|the hidden state. This multinomial assumption allows an efficient parameter estimation algorithm to be derived|the Baum-Welch algorithm.
Reference: <author> Barlow, H. </author> <year> (1989). </year> <title> Unsupervised learning. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 295-311. </pages>
Reference: <author> Baum, L., Petrie, T., Soules, G., and Weiss, N. </author> <year> (1970). </year> <title> A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains. </title> <journal> The Annals of Mathematical Statistics, </journal> <volume> 41 </volume> <pages> 164-171. </pages>
Reference-contexts: We concentrate primarily on the problem of learning the parameters for a given structure. The parameters of a factorial HMM can be estimated via the Expectation Maximization (EM) algorithm (Dempster et al., 1977), which in the case of HMMs is known as the Baum-Welch algorithm <ref> (Baum et al., 1970) </ref>. This procedure iterates between assuming the current parameters to compute posterior probabilities over the 5 hidden states (E-step), and using these probabilities to maximize the expected log likelihood of the parameters (M-step).
Reference: <author> Bengio, Y. and Frasconi, P. </author> <year> (1995). </year> <title> An input-output HMM architecture. </title> <editor> In Tesauro, G., Touretzky, D. S., and Leen, T. K., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <pages> pages 427-434. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Cacciatore, T. W. and Nowlan, S. J. </author> <year> (1994). </year> <title> Mixtures of controllers for jump Linear and non-linear plants. </title> <editor> In Cowan, J. D., Tesauro, G., and Alspector, J., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 719-726. </pages> <publisher> Morgan Kaufman Publishers, </publisher> <address> San Francisco, CA. </address>
Reference: <author> Conklin, D. and Witten, I. H. </author> <year> (1995). </year> <title> Multiple viewpoint systems for music prediction. </title>
Reference: <author> J. </author> <title> of New Music Research, </title> <booktitle> 24 </booktitle> <pages> 51-73. </pages>
Reference: <author> Cover, T. and Thomas, J. </author> <year> (1991). </year> <title> Elements of Information Theory. </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference-contexts: We will limit ourselves to addressing the problem of computing the likelihood, as this already poses some severe computational problems. The interested Bayesian researcher can extend this framework An alternative and formally equivalent framework for unsupervised learning can be derived using information theory <ref> (Cover and Thomas, 1991) </ref>. The goal of the learner is to communicate the data efficiently to a receiver, thereby producing a compact representation for the data (Zemel, 1993; Hinton and Zemel, 1994). <p> t g) = log fS t g = log fS t g Q (fS t gjfY t g) " Q (fS t gjfY t g) # X Q (fS t gjfY t g) log " Q (fS t gjfY t g) # where we have made use of Jensen's inequality <ref> (Cover and Thomas, 1991) </ref> in the last step.
Reference: <author> Dempster, A., Laird, N., and Rubin, D. </author> <year> (1977). </year> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> J. Royal Statistical Society Series B, </journal> <volume> 39 </volume> <pages> 1-38. </pages> <note> 23 Geman, </note> <author> S., Bienenstock, E., and Doursat, R. </author> <year> (1992). </year> <title> Neural networks and the bias/variance dilemma. </title> <journal> Neural Computation, </journal> 4:1-58. 
Reference-contexts: Pearl, 1988; Stolcke and Omohundro, 1993). We concentrate primarily on the problem of learning the parameters for a given structure. The parameters of a factorial HMM can be estimated via the Expectation Maximization (EM) algorithm <ref> (Dempster et al., 1977) </ref>, which in the case of HMMs is known as the Baum-Welch algorithm (Baum et al., 1970).
Reference: <author> Geman, S. and Geman, D. </author> <year> (1984). </year> <title> Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 6 </volume> <pages> 721-741. </pages>
Reference-contexts: Although there are many possible sampling schemes (for a review see Neal, 1993), here we present one of the simplest ones|Gibbs sampling <ref> (Geman and Geman, 1984) </ref>. For a given observation sequence fY t g, this procedure starts with a random setting of the hidden states fS t g.
Reference: <author> Ghahramani, Z. </author> <year> (1995). </year> <title> Factorial learning and the EM algorithm. </title> <editor> In Tesauro, G., Touretzky, D., and Leen, T., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Hinton, G. E., Dayan, P., Frey, B. J., and Neal, R. M. </author> <year> (1995). </year> <title> The wake-sleep algorithm for unsupervised neural networks. </title> <journal> Science, </journal> <volume> 268 </volume> <pages> 1158-1161. </pages>
Reference: <author> Hinton, G. E. and Zemel, R. S. </author> <year> (1994). </year> <title> Autoencoders, minimum description length, and Helmholtz free energy. </title> <editor> In Cowan, J., Tesauro, G., and Alspector, J., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6. </booktitle> <publisher> Morgan Kaufmanm Publishers, </publisher> <address> San Francisco, CA. </address>
Reference: <author> Jensen, F. V., Lauritzen, S. L., and Olesen, K. G. </author> <year> (1990). </year> <title> Bayesian updating in recursive graphical models by local computations. </title> <journal> Computational Statistical Quarterly, </journal> <volume> 4 </volume> <pages> 269-282. </pages>
Reference-contexts: The relation between hidden Markov models and Bayesian belief networks has recently been reviewed in Smyth, Heckerman and Jordan (1996) . While exact belief propagation algorithms exist for belief networks <ref> (Jensen et al., 1990) </ref>, such algorithms soon become intractable for densely-connected models such as the ones we consider in this paper. Kanazawa et al. (1995) apply a stochastic simulation algorithm to overcome this difficulty.
Reference: <author> Jordan, M. I., Ghahramani, Z., and Saul, L. K. </author> <year> (1996). </year> <title> Hidden Markov decision trees. </title> <note> Submitted to NIPS 8. </note>
Reference: <author> Jordan, M. I. and Jacobs, R. </author> <year> (1994). </year> <title> Hierarchical mixtures of experts and the EM algorithm. </title> <journal> Neural Computation, </journal> <volume> 6 </volume> <pages> 181-214. </pages>
Reference-contexts: At the next time step, a similar procedure is used to generate data from the model, except that now, each decision in the decision tree is dependent on the decision taken at that node in the previous time step. Thus, the "hierarchical mixture of experts" architecture <ref> (Jordan and Jacobs, 1994) </ref> is generalized such that it includes Markovian dynamics for the decisions. Hidden Markov decision trees provide a useful starting point for modeling time series with both temporal and spatial structure at multiple resolutions.
Reference: <author> Kanazawa, K., Koller, D., and Russell, S. J. </author> <year> (1995). </year> <title> Stochastic simulation algorithms for dynamic probabilistic networks. </title> <booktitle> In Proceedings of the 11th Annual Conference on Uncertainty in AI, </booktitle> <pages> pages 346-351. </pages>
Reference: <author> Lauritzen, S. L. and Spiegelhalter, D. J. </author> <year> (1988). </year> <title> Local computations with probabilities on graphical structures and their application to expert systems. </title> <journal> J. Royal Statistical Society B, </journal> <pages> pages 157-224. </pages>
Reference: <author> Meila, M. and Jordan, M. I. </author> <year> (1996). </year> <title> Learning fine motion by Markov mixtures of experts. </title> <editor> In Touretzky, D. S., Mozer, M. C., and Hasselmo, M. E., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 8. </booktitle> <publisher> MIT Press. </publisher>
Reference: <author> Murphy, P. M. and Aha, D. W. </author> <year> (1992). </year> <title> UCI Repository of machine learning databases [Machine-readable data repository]. </title> <institution> University of California, Department of Information and Computer Science, </institution> <address> Irvine, CA. </address>
Reference: <author> Neal, R. M. </author> <year> (1992). </year> <title> Connectionist learning of belief networks. </title> <journal> Artificial Intelligence, </journal> <volume> 56 </volume> <pages> 71-113. </pages>
Reference-contexts: Thus the compactness of the representation would be entirely lost. Standard methods from belief networks suggest approximating such large matrices with "noisy-OR" (Pearl, 1988) or "sigmoid" <ref> (Neal, 1992) </ref> models of interaction. <p> Like in Boltzmann machines (Ackley et al., 1985), both a clamped and an unclamped phase are therefore required for learning, where the goal of the unclamped phase is to compute the derivative of the partition function with respect to the parameters <ref> (Neal, 1992) </ref>. 5.3 Conditioning on inputs Like the hidden Markov model, the factorial HMM provides a model of the unconditional density of the observation sequences.
Reference: <author> Neal, R. M. </author> <year> (1993). </year> <title> Probabilistic inference using Markov chain monte carlo methods. </title> <institution> University of Toronto, </institution> <note> Technical Report CRG-TR-93-1. </note>
Reference-contexts: Although there are many possible sampling schemes <ref> (for a review see Neal, 1993) </ref>, here we present one of the simplest ones|Gibbs sampling (Geman and Geman, 1984). For a given observation sequence fY t g, this procedure starts with a random setting of the hidden states fS t g.
Reference: <author> Parisi, G. </author> <year> (1988). </year> <title> Statistical Field Theory. </title> <publisher> Addison-Wesley, </publisher> <address> Redwood City, CA. </address>
Reference-contexts: t gj) = t=1 m=1 (m) (m) The parameters of this distribution, = f (m) t g, are the means of the state variables: Q (S t j t ) = t : (9) In the statistical mechanics literature, this completely factorized approximation is known as a mean field approximation <ref> (Parisi, 1988) </ref>. The approximation is made as tight as possible by varying so as to minimize the KL divergence.
Reference: <author> Pearl, J. </author> <year> (1988). </year> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: However, in the case of a factorial HMM, this matrix would have D fi K M entries for each combination of the state variables and observation. Thus the compactness of the representation would be entirely lost. Standard methods from belief networks suggest approximating such large matrices with "noisy-OR" <ref> (Pearl, 1988) </ref> or "sigmoid" (Neal, 1992) models of interaction.
Reference: <author> Rabiner, L. and Juang, B. </author> <year> (1986). </year> <title> An Introduction to hidden Markov models. </title> <journal> IEEE Acoustics, Speech & Signal Processing Magazine, </journal> <volume> 3 </volume> <pages> 4-16. </pages>
Reference-contexts: Due to the simplicity and efficiency of its parameter estimation algorithm, the hidden Markov model (HMM) has emerged as one of the basic statistical tools for modeling discrete time series, finding widespread application in the areas of speech recognition <ref> (Rabiner and Juang, 1986) </ref> and computational molecular biology (Baldi et al., 1994). An HMM is essentially a mixture model, encoding information about the history of a time series in the value of a single multinomial variable|the hidden state. <p> Note that in standard HMM notation <ref> (Rabiner and Juang, 1986) </ref>, hS (m) t i corresponds to the fl t and hS (m) (m) 0 t i corresponds to the ~ t computed during the forward-backward iterations, whereas hS (m) (n) 0 no analogue when there is only a single underlying Markov model.
Reference: <author> Redlich, A. N. </author> <year> (1993). </year> <title> Supervised factorial learning. </title> <journal> Neural Computation, </journal> <volume> 5 </volume> <pages> 750-766. </pages>
Reference: <author> Rissanen, J. </author> <year> (1989). </year> <title> Stochastic Complexity in Statistical Inquiry. </title> <publisher> World Scientific, Singapore. </publisher>
Reference-contexts: The goal of the learner is to communicate the data efficiently to a receiver, thereby producing a compact representation for the data (Zemel, 1993; Hinton and Zemel, 1994). A cost function quantifying the efficiency of this communication process can be derived from the Minimum Description Length (MDL) principle <ref> (Rissanen, 1989) </ref>. Using Shannon's coding theorem, the MDL cost function can be shown to be equal to the posterior probability of the parameters given the data.
Reference: <author> Saul, L., Jaakkola, T., and Jordan, M. I. </author> <year> (1996). </year> <title> Mean Field Theory for Sigmoid Belief Networks. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4 </volume> <pages> 61-76. </pages>
Reference-contexts: The basic idea is to approximate the posterior distribution P by a tractable distribution Q. This approximation provides a lower bound on the log likelihood which can be used to obtain an efficient learning algorithm. The argument can be formalized as follows <ref> (Saul et al., 1996) </ref>.
Reference: <author> Saul, L. and Jordan, M. I. </author> <year> (1995). </year> <title> Boltzmann chains and hidden Markov models. </title> <editor> In Tesauro, G., Touretzky, D., and Leen, T., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Saul, L. and Jordan, M. I. </author> <year> (1996). </year> <title> Exploiting tractable substructures in Intractable networks. </title> <editor> In Touretzky, D., Mozer, M., and Hasselmo, M., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 8. </booktitle> <publisher> MIT Press. </publisher>
Reference-contexts: The basic idea is to approximate the posterior distribution P by a tractable distribution Q. This approximation provides a lower bound on the log likelihood which can be used to obtain an efficient learning algorithm. The argument can be formalized as follows <ref> (Saul et al., 1996) </ref>.
Reference: <author> Saund, E. </author> <year> (1995). </year> <title> A multiple cause mixture model for unsupervised learning. </title> <journal> Neural Computation, </journal> <volume> 7(1) </volume> <pages> 51-71. </pages>
Reference: <author> Smyth, P., Heckerman, D., and Jordan, M. I. (1996.). </author> <title> Probabilistic independence networks for hidden Markov probability models. </title> <note> Microsoft Research Technical Report MSR-TR-96-03. </note>
Reference-contexts: This problem can be solved efficiently via the Forward-Backward (F-B) algorithm (Ra-biner and Juang, 1986), which can be shown to be equivalent to the Jensen, Lauritzen, and Olesen (JLO; 1990) algorithm for probability propagation in more general belief networks <ref> (Smyth et al., 1996) </ref>. In some cases, rather than a probability distribution over hidden states it is desirable to infer the single most probable hidden state sequence.
Reference: <author> Stolcke, A. and Omohundro, S. </author> <year> (1993). </year> <title> Hidden Markov model induction by Bayesian model merging. </title> <editor> In Hanson, S. J., Cowan, J. D., and Giles, C. L., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 11-18. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA. </address>
Reference: <author> Tanner, M. A. and Wong, W. H. </author> <year> (1987). </year> <title> The calculation of posterior distributions by data augmentation (with discussion). </title> <journal> Journal of the American Statistical Association, </journal> <volume> 82 </volume> <pages> 528-550. </pages>
Reference-contexts: This procedure, which can be seen as a particular form of Gibbs sampling, is also known as data augmentation <ref> (Tanner and Wong, 1987) </ref>. 3.3 Mean field approximation In this section, we present a second approximation of the posterior probability of the hidden states which is both tractable and deterministic. The basic idea is to approximate the posterior distribution P by a tractable distribution Q.
Reference: <author> Titterington, D. M., Smith, A., and Makov, U. </author> <year> (1985). </year> <title> Statistical analysis of finite mixture distributions. </title> <publisher> Wiley, </publisher> <address> Chichester. </address>
Reference-contexts: In this model a set of vector quantizers, each representing one of the factors, cooperate to reconstruct the observation. Each vector quantizer is a mixture model <ref> (Titterington et al., 1985) </ref>, and CVQ generalizes mixture models by allowing the mixture components to cooperate in modeling the data set. In simple mixture models each data point must be accounted for by a single mixture component.
Reference: <author> Williams, C. and Hinton, G. </author> <year> (1991). </year> <title> Mean field networks that learn to discriminate temporally distorted strings. </title> <editor> In Touretzky, D., Elman, J., Sejnowski, T., and Hinton, G., editors, </editor> <title> Connectionist Models: </title> <booktitle> Proceedings of the 1990 Summer School, </booktitle> <pages> pages 18-22. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> Man Mateo, CA. </address> <note> 25 Zemel, </note> <author> R. S. </author> <year> (1993). </year> <title> A minimum description length framework for unsupervised learn-ing. </title> <type> Ph.D. Thesis, </type> <institution> Dept. of Computer Science, University of Toronto, Toronto, Canada. </institution> <month> 26 </month>
Reference-contexts: For example, to represent 30 bits of information about the history of a time sequence, an HMM would need 2 30 distinct states. On the other hand an HMM with a distributed state representation could achieve the same task with 30 binary units <ref> (Williams and Hinton, 1991) </ref>. This paper addresses the problem of 1 deriving efficient learning algorithms for hidden Markov models with distributed state representations. The need for distributed state representations in HMMs can be motivated in two ways.
References-found: 37

