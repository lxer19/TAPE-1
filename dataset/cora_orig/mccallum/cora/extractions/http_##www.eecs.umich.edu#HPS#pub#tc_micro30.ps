URL: http://www.eecs.umich.edu/HPS/pub/tc_micro30.ps
Refering-URL: http://www.eecs.umich.edu/HPS/hps_tracecache.html
Root-URL: http://www.cs.umich.edu
Title: Alternative Fetch and Issue Policies for the Trace Cache Fetch Mechanism  
Abstract: Copyright 1997 IEEE. Published in the Proceedings of Micro-30, December 1-3, 1997 in Research Triangle Park, North Carolina. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works, must be obtained from the IEEE. Contact: Manager, Copyrights and Permissions / IEEE Service Center / 445 Hoes Lane / P.O. Box 1331 / Piscataway, NJ 08855-1331, USA. Telephone: + Intl. 908-562-3966. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. O. Bondi, A. K. Nanda, and S. Dutta, </author> <title> "Integrating a misprediction recovery cache (mrc) into a superscalar pipeline," </title> <booktitle> in Proceedings of the 29th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <pages> pp. 14 - 23, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: The inactive issue technique presented in section 4 is similar to the misprediction recovery cache work done by Bondi et al <ref> [1] </ref>. Both schemes reduce the effects of the branch misprediction penalty by either saving processed instructions on the other path or concurrently processing them along with the predicted path.
Reference: [2] <author> D. Burger, T. Austin, and S. Bennett, </author> <title> "Evaluating future microprocessors: The simplescalar tool set," </title> <type> Technical Report 1308, </type> <institution> University of Wis-consin - Madison Technical Report, </institution> <month> July </month> <year> 1996. </year>
Reference-contexts: For the experiments in this paper the fill unit collects instructions as they retire. 5 Experimental model An executable-driven HPS [12] simulator modeling wrong path effects was used for this study. The simulator was implemented using the simplescalar 2.0 tool suite <ref> [2] </ref>. All instructions undergo four stages of pro cessing: fetch, issue, schedule, execute. All stages take at least one cycle. Figure 3 shows a block diagram of the pipeline model.
Reference: [3] <author> P.-Y. Chang, E. Hao, T.-Y. Yeh, and Y. N. Patt, </author> <title> "Branch classification: A new mechanism for improving branch predictor performance," </title> <booktitle> in Proceedings of the 27th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <pages> pp. 22-31, </pages> <year> 1994. </year>
Reference-contexts: The selection between the components is done by a 14-bit gshare-style selector. Combining a per-address predictor with a gshare predictor was first proposed by McFarling [8]. Using a two-level mechanism to select between the two was proposed by Chang et al <ref> [3] </ref>. A similar version of this predictor is implemented in the DECChip 21264 [7]. Despite being limited to a single fetch block per cycle, such a mechanism attains a high fetch bandwidth because of the high accuracy of its branch predictor.
Reference: [4] <author> M. Franklin and M. Smotherman, </author> <title> "A fill-unit approach to multiple instruction issue," </title> <booktitle> in Proceedings of the 27th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <pages> pp. 162-171, </pages> <year> 1994. </year>
Reference-contexts: In [9] the performance implications of the fill unit are discussed and the idea of dynamically combining fetch blocks into larger "execution atomic units" (EAUs) to further increase the fetch bandwidth is first proposed. Two other extensions of the original schemes were presented by Franklin and Smotherman. In <ref> [4] </ref>, they applied the original fill unit idea to dynamically create VLIW instructions out of RISC-type operations. They reworked the fill unit finalization strategy by restricting the type of instruction dependencies allowed in a fill unit line and by filling both paths beyond a conditional branch. <p> Both schemes reduce the effects of the branch misprediction penalty by either saving processed instructions on the other path or concurrently processing them along with the predicted path. Franklin and Smotherman <ref> [4] </ref> examined creating tree-like VLIWs using a fill unit, a technique similar to what we propose in section 7. They examined the multipath trace segments in the context of statically scheduled machines. Similar work was done by Ebcioglu.
Reference: [5] <author> W. W. Hwu and Y. N. Patt, </author> <title> "Checkpoint repair for out-of-order execution machines," </title> <booktitle> in Proceedings of the 14th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 18-26, </pages> <year> 1987. </year>
Reference-contexts: The blocks that do not match the prediction are said to be issued inactively. Although these inactive instructions are renamed and receive physical registers for their destination values, the changes they make to the register alias table (RAT) <ref> [5] </ref> are not considered valid for subsequent issue cycles. Thus instructions along the predicted path view the speculative state of the processor exactly as if the inactive blocks had not been issued. <p> The functional units are uniform and capable of all operations. A 32KB L1 data cache was used. The latencies of the operations are listed in Table 1. The HPS model uses checkpoint repair <ref> [5] </ref> to recover from branch mispredictions and exceptions. The execution engine is capable of creating three checkpoints each cycle. All experiments were performed on the SPECint95 benchmarks. Table 2 lists the benchmarks and the input sets 1 .
Reference: [6] <author> E. Jacobsen, E. Rotenberg, and J. E. Smith, </author> <title> "Assigning confidence to conditional branch predictions," </title> <booktitle> in micro96, </booktitle> <pages> pp. 142-152, </pages> <year> 1996. </year>
Reference-contexts: Forks within a segment are most useful when the corresponding branch is unbiased. To make effective use of dual path segments, the fill unit must decide when a path should contain a fork and when it should not. Hardware schemes, such as a confidence mechanism proposed by Jacobsen <ref> [6] </ref>, can assist the fill unit by identifying branches which the branch predictor is not predicting well. Whenever an unbiased branch is encountered, the pending segment is a possible candidate for dual paths.
Reference: [7] <author> J. Keller, </author> <title> The 21264: A Superscalar Alpha Processor with Out-of-Order Execution, </title> <institution> Digital Equipment Corporation, Hudson, </institution> <address> MA, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: Combining a per-address predictor with a gshare predictor was first proposed by McFarling [8]. Using a two-level mechanism to select between the two was proposed by Chang et al [3]. A similar version of this predictor is implemented in the DECChip 21264 <ref> [7] </ref>. Despite being limited to a single fetch block per cycle, such a mechanism attains a high fetch bandwidth because of the high accuracy of its branch predictor. The predictor attains a 93% prediction accuracy for gcc and 85% for go.
Reference: [8] <author> S. McFarling, </author> <title> "Combining branch predictors," </title> <type> Technical Report TN-36, </type> <institution> Digital Western Research Laboratory, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: The single branch predictor is a hybrid predictor, consisting of two components: a 14-bit PAs predictor and a 14-bit gshare predictor. The selection between the components is done by a 14-bit gshare-style selector. Combining a per-address predictor with a gshare predictor was first proposed by McFarling <ref> [8] </ref>. Using a two-level mechanism to select between the two was proposed by Chang et al [3]. A similar version of this predictor is implemented in the DECChip 21264 [7].
Reference: [9] <author> S. W. Melvin and Y. N. Patt, </author> <title> "Performance benefits of large execution atomic units in dynamically scheduled machines," </title> <booktitle> in Proceedings of the 1989 International Conference on Supercomputing, </booktitle> <pages> pp. 427-432, </pages> <year> 1989. </year>
Reference-contexts: A hit in the decoded instruction cache results in a larger atomic unit of work than would be possible if the individual instructions were fetched and decoded one per cycle. In <ref> [9] </ref> the performance implications of the fill unit are discussed and the idea of dynamically combining fetch blocks into larger "execution atomic units" (EAUs) to further increase the fetch bandwidth is first proposed. Two other extensions of the original schemes were presented by Franklin and Smotherman.
Reference: [10] <author> S. W. Melvin, M. C. Shebanow, and Y. N. Patt, </author> <title> "Hardware support for large atomic units in dynamically scheduled machines," </title> <booktitle> in Proceedings of the 21st Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <pages> pp. 60-63, </pages> <year> 1988. </year>
Reference-contexts: Section 8 provides a comparison with existing fetch technology. Section 9 gives a brief summary of the work presented. 2 Related work A precursor to the trace cache was first introduced by Melvin, Shebanow and Patt <ref> [10] </ref>. They proposed the fill unit to compact a fetch block's worth of instructions into an entry in a decoded instruction cache.
Reference: [11] <author> S. J. Patel, D. H. Friendly, and Y. N. Patt, </author> <title> "Critical issues regarding the trace cache fetch mechanism," </title> <type> Technical Report CSE-TR-335-97, </type> <institution> University of Michigan Technical Report, </institution> <month> May </month> <year> 1997. </year>
Reference-contexts: A fetch block roughly corresponds to a compiler basic block: it is a dynamic sequence of instructions starting at the current fetch address and ending at the next control instruction. Recently, the trace cache <ref> [13, 14, 11] </ref> has been proposed as a technique that is able to overcome this bandwidth limitation. By placing logically contiguous instructions in physically contiguous storage, the trace cache is able to supply multiple fetch blocks each cycle. <p> Patel et al <ref> [11] </ref> examined a wide range of trace cache and fill unit characteristics. They showed that higher performance can be achieved when a fetch mechanism is partitioned into a large trace cache with a small instruction cache than with the opposite configuration. <p> Another implementation issue concerning this technique is how the fill unit should treat instructions which are issued inactively. The fill unit can collect blocks either as they are issued or as they are retired. Results presented in <ref> [11] </ref> show that the performance difference between these two options is minimal. If the fill unit is designed to collect blocks at issue time, a choice must be made whether to include inactively issued instructions or to discard them. <p> Both caches were modeled as 4-way set associative. A 256KB unified second level cache provides instruction and data with a latency of six cycles in the case of first level cache misses. The branch predictor modeled, a variation of the gshare predictor designed for use with the trace cache <ref> [11] </ref>, provides up to three predictions per cycle. The size of the pattern history table was fixed at 16K entries. An ideal return address stack was modeled. The trace cache we modeled has no path associativity. <p> An ideal return address stack was modeled. The trace cache we modeled has no path associativity. Only one segment starting at a particular fetch block can be stored in the cache at any time (ie. ABC and ABD cannot be concurrently resident). See <ref> [11] </ref> for a discussion and performance results on path associativity. <p> cycles spent by the baseline trace cache fetching wrong path instructions and the percentage difference for partial matching and inactive issue. age branch resolution time for perl to rise from 10.79 cycles with partial matching to 11.00 cycles with inactive issue. 7 Dual path extensions The trace caches explored in <ref> [13, 14, 11] </ref> contain trace segments which are dynamic sequences of blocks. Each subsequent block in a segment is the target of the previous block. Partial matching enables another option: a trace segment need not consist of blocks from a single path of execution.
Reference: [12] <author> Y. Patt, W. Hwu, and M. Shebanow, "HPS, </author> <title> a new microarchitecture: Rationale and introduction," </title> <booktitle> in Proceedings of the 18th Annual ACM/IEEE International Symposium on Mi-croarchitecture, </booktitle> <pages> pp. 103-107, </pages> <year> 1985. </year>
Reference-contexts: For the experiments in this paper the fill unit collects instructions as they retire. 5 Experimental model An executable-driven HPS <ref> [12] </ref> simulator modeling wrong path effects was used for this study. The simulator was implemented using the simplescalar 2.0 tool suite [2]. All instructions undergo four stages of pro cessing: fetch, issue, schedule, execute. All stages take at least one cycle.
Reference: [13] <author> A. Peleg and U. Weiser. </author> <title> Dynamic Flow Instruction Cache Memory Organized Around Trace Segments Independant of Virtual Address Line. </title> <type> U.S. Patent Number 5,381,533, </type> <year> 1994. </year>
Reference-contexts: A fetch block roughly corresponds to a compiler basic block: it is a dynamic sequence of instructions starting at the current fetch address and ending at the next control instruction. Recently, the trace cache <ref> [13, 14, 11] </ref> has been proposed as a technique that is able to overcome this bandwidth limitation. By placing logically contiguous instructions in physically contiguous storage, the trace cache is able to supply multiple fetch blocks each cycle. <p> In [15], they demonstrated how a fill unit could help overcome the decoder bottleneck of a Pentium Pro type processor. In 1994, Peleg and Weiser filed a patent on the trace cache concept <ref> [13] </ref>. The concept was further investigated by Rotenberg et al [14]. They presented a thorough comparison between the trace cache scheme and several hardware-based high-bandwidth fetch schemes and showed the advantage of using a trace cache, both in performance and latency. <p> cycles spent by the baseline trace cache fetching wrong path instructions and the percentage difference for partial matching and inactive issue. age branch resolution time for perl to rise from 10.79 cycles with partial matching to 11.00 cycles with inactive issue. 7 Dual path extensions The trace caches explored in <ref> [13, 14, 11] </ref> contain trace segments which are dynamic sequences of blocks. Each subsequent block in a segment is the target of the previous block. Partial matching enables another option: a trace segment need not consist of blocks from a single path of execution.
Reference: [14] <author> E. Rotenberg, S. Bennett, and J. E. Smith, </author> <title> "Trace cache: a low latency approach to high bandwidth instruction fetching," </title> <booktitle> in Proceedings of the 29th Annual ACM/IEEE International Symposium on Microarchitecture, </booktitle> <pages> pp. 24 - 34, </pages> <month> De-cember </month> <year> 1996. </year>
Reference-contexts: A fetch block roughly corresponds to a compiler basic block: it is a dynamic sequence of instructions starting at the current fetch address and ending at the next control instruction. Recently, the trace cache <ref> [13, 14, 11] </ref> has been proposed as a technique that is able to overcome this bandwidth limitation. By placing logically contiguous instructions in physically contiguous storage, the trace cache is able to supply multiple fetch blocks each cycle. <p> In this paper we examine several enhancements to the trace cache mechanism that increase the effective fetch rate without severely increasing the number of cycles lost to branch mispredictions and cache misses. The first technique, partial matching, suggested by Rotenberg et al <ref> [14] </ref>, allows the trace cache to respond with only the portion of a trace cache line selected by the predictor. Partial matching improves the effective fetch rate by increasing the percentage of instructions that are fetched from the trace cache. <p> In [15], they demonstrated how a fill unit could help overcome the decoder bottleneck of a Pentium Pro type processor. In 1994, Peleg and Weiser filed a patent on the trace cache concept [13]. The concept was further investigated by Rotenberg et al <ref> [14] </ref>. They presented a thorough comparison between the trace cache scheme and several hardware-based high-bandwidth fetch schemes and showed the advantage of using a trace cache, both in performance and latency. In this work they suggested the concept of partial matching. <p> cycles spent by the baseline trace cache fetching wrong path instructions and the percentage difference for partial matching and inactive issue. age branch resolution time for perl to rise from 10.79 cycles with partial matching to 11.00 cycles with inactive issue. 7 Dual path extensions The trace caches explored in <ref> [13, 14, 11] </ref> contain trace segments which are dynamic sequences of blocks. Each subsequent block in a segment is the target of the previous block. Partial matching enables another option: a trace segment need not consist of blocks from a single path of execution.
Reference: [15] <author> M. Smotherman and M. Franklin, </author> <title> "Improving cisc instruction decoding performance using a fill unit," </title> <booktitle> in Proceedings of the 28th Annual ACM/IEEE International Symposium on Mi-croarchitecture, </booktitle> <pages> pp. 219-229, </pages> <year> 1995. </year>
Reference-contexts: In [4], they applied the original fill unit idea to dynamically create VLIW instructions out of RISC-type operations. They reworked the fill unit finalization strategy by restricting the type of instruction dependencies allowed in a fill unit line and by filling both paths beyond a conditional branch. In <ref> [15] </ref>, they demonstrated how a fill unit could help overcome the decoder bottleneck of a Pentium Pro type processor. In 1994, Peleg and Weiser filed a patent on the trace cache concept [13]. The concept was further investigated by Rotenberg et al [14].
References-found: 15

