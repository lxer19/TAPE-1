URL: http://www.pdl.cs.cmu.edu/PDL-FTP/TIP/rhp_diss.ps
Refering-URL: http://www.cs.cmu.edu/~rhp/
Root-URL: 
Title: Informed Prefetching and Caching  
Author: Russel Hugo Patterson III Garth A. Gibson, Chair Mahadev Satyanrayanan Daniel P. Siewiorek F. Roy Carlson, Russel Hugo Patterson III 
Degree: Submitted in partial fulfillment of the requirements for the degree Doctor of Philosphy in Electrical and Computer Engineering Thesis Committee:  
Note: Quantum Corporation 1997 by  
Address: Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Date: December 1997  
Pubnum: CMU-CS-97-204  
Abstract: This research was supported in part by Advanced Research Projects Agency contracts DABT63-93-C-0054 and N00174-96-0002, in part by the Data Storage Systems Center under National Science Foundation grant number ECD-8907068, in part by an IBM Graduate Fellowship, and in part by generous contributions from the member companies of the Parallel Data Consortium: Hewlett-Packard Laboratories, Symbios Logic, Data General, Compaq, IBM Corporation, Seagate Technology, EMC Corporation, Storage Technology Corporation, and Digital Equipment Corporation. The views and conclusions contained in this document are my own and should not be interpreted as representing the official policies, either expressed or implied, of any supporting organization or the U.S. Government. 
Abstract-found: 1
Intro-found: 1
Reference: [Accetta86] <author> Accetta, M.J., Baron, R., Bolosky, W., Golub, D., Rashid, R., Tevanian, A., Young, M., </author> <title> Mach: A New Kernel Foundation for UNIX Development, </title> <booktitle> Proceedings of the Summer 1986 USENIX Conference, </booktitle> <address> Atlanta, GA, </address> <month> June, </month> <year> 1986, </year> <pages> pp. 93-112. </pages>
Reference-contexts: In the operating systems research community, efforts have been focussed on moving functionality out of the kernel and into user space where the user can customize behavior. Examples of this include external pagers [Harty92], scheduler activations [Anderson92], and, in the extreme case, micro-kernels themselves <ref> [Accetta86, Rozier88, Engler95] </ref>. Certainly, this approach can lead to dramatic performance gains for applications that are willing to rewrite significant chunks of the operating system. But, many application programmers dont wish to become systems programmers. They would prefer to focus their efforts on their own algorithms.
Reference: [Akyrek92] <author> Akyrek, S., Salem, K., </author> <title> Placing Replicated Data to Reduce Seek Delays, </title> <booktitle> Proceedings of the USENIX File System Conference, </booktitle> <month> May, </month> <year> 1992. </year> <note> Also available as Computer Science Technical Report CS-TR-2746, </note> <institution> University of Maryland, </institution> <month> August, </month> <year> 1991. </year>
Reference-contexts: For example, systems can take advantage of the fact that the distribution of accesses tends to be highly skewed to a small portion of the data stored on a disk. By profiling data accesses, disk subsystems [Vongsathorn90, Akyrek93, Akyrek93a] or file systems [Staelin90] can migrate or replicate <ref> [Akyrek92] </ref> the most active blocks to the center of the disk to reduce seek distances. The second approach to increasing sequentiality is to reorder accesses to reduce positioning delays.
Reference: [Akyrek93] <author> Akyrek, S., Salem, K., </author> <title> Adaptive Block Rearrangement, </title> <booktitle> Proceedings of IEEE International Conference on Data Engineering, </booktitle> <month> April, </month> <year> 1993, </year> <pages> pp. 182-189. </pages> <note> An expanded version of the paper is available as Computer Science Technical Report CS-TR-2854, </note> <institution> University of Maryland, </institution> <month> March, </month> <year> 1992. </year>
Reference-contexts: For example, systems can take advantage of the fact that the distribution of accesses tends to be highly skewed to a small portion of the data stored on a disk. By profiling data accesses, disk subsystems <ref> [Vongsathorn90, Akyrek93, Akyrek93a] </ref> or file systems [Staelin90] can migrate or replicate [Akyrek92] the most active blocks to the center of the disk to reduce seek distances. The second approach to increasing sequentiality is to reorder accesses to reduce positioning delays.
Reference: [Akyrek93a] <author> Akyrek, S., Salem, K., </author> <title> Adaptive Block Rearrangement under UNIX, </title> <booktitle> Proceedings of the USENIX Summer Technical Conference, </booktitle> <month> June, </month> <year> 1993, </year> <pages> pp. 307-321. </pages> <note> Also available as Computer Science Technical Report CS-TR-3054, </note> <institution> University of Maryland, </institution> <month> April , </month> <year> 1993. </year>
Reference-contexts: For example, systems can take advantage of the fact that the distribution of accesses tends to be highly skewed to a small portion of the data stored on a disk. By profiling data accesses, disk subsystems <ref> [Vongsathorn90, Akyrek93, Akyrek93a] </ref> or file systems [Staelin90] can migrate or replicate [Akyrek92] the most active blocks to the center of the disk to reduce seek distances. The second approach to increasing sequentiality is to reorder accesses to reduce positioning delays.
Reference: [Amdahl67] <editor> Validity of the Single Processor Approach to Achieving Large Scale Computing Capabilities, </editor> <booktitle> American Federation of Information Processing Societies (AFIPS) Spring Joint Conference, V 30, </booktitle> <address> Atlantic City, NJ, </address> <month> April 18-20, </month> <year> 1967, </year> <pages> pp. 483-485. 228 </pages>
Reference-contexts: Even high-performance disks have access latencies of 10 milliseconds or more. In that time, a modern processor can execute millions of instructions. And, as shown in Figure 2.1, the trend is for this performance disparity to grow, not shrink. The key implication, as Amdahls Law tells us <ref> [Amdahl67] </ref>, is that reductions in elapsed time due to increasing processor performance will ultimately be limited by the ever-larger portion of an applications elapsed time that will be spent waiting for disk accesses to complete.
Reference: [Anderson92] <author> Anderson, T.E., Bershad, B.N., Lazowska, E.D. and Levy, H.M., Sched-uler Activations: </author> <title> Effective Kernel Support for the User-Level Management of Parallelism, </title> <journal> ACM Transactions on Computer Systems (TOCS), </journal> <volume> V 10 (1), </volume> <month> February, </month> <year> 1992, </year> <pages> pp. 53-79. </pages>
Reference-contexts: In the operating systems research community, efforts have been focussed on moving functionality out of the kernel and into user space where the user can customize behavior. Examples of this include external pagers [Harty92], scheduler activations <ref> [Anderson92] </ref>, and, in the extreme case, micro-kernels themselves [Accetta86, Rozier88, Engler95]. Certainly, this approach can lead to dramatic performance gains for applications that are willing to rewrite significant chunks of the operating system. But, many application programmers dont wish to become systems programmers.
Reference: [Baker91] <author> Baker, M. G., Hartman, J. H., Kupfer, M.D., Shirriff, K.W., Ousterhout, J.K., </author> <title> Measurements of a Distributed File System, </title> <booktitle> Proceedings of the 13th Symposium on Operating System Principles (SOSP), </booktitle> <address> Pacific Grove, CA, </address> <month> October, </month> <year> 1991, </year> <pages> pp. 198-212. </pages>
Reference-contexts: Again, these techniques also increase workload sequentiality. On the write side, buffering written data and delaying the write to disk can avoid some write accesses because data may be over-written or deleted before the disk writes occur <ref> [Baker91, Kistler93] </ref>. This is particularly useful when an application is appending data to a file with small writes. <p> Is such improvement likely? Table 2.1 compares the performance predicted for a variety of operating system file cache sizes in 1985 [Ousterhout85] with that observed in 1991 <ref> [Baker91] </ref> by a group at Berkeley. The first observation, based on the 1985 data, is that increasing the size of an already large cache does not reduce the miss ratio much. <p> The closer workloads can come to this ideal, the less time they will waste on positioning delays, and the greater the disk performance they will achieve. Table 2.1. Comparison of caching performance in 1985 and 1991. The numbers in this table are drawn from [Ousterhout85] and <ref> [Baker91] </ref>. The 1985 tracing study of the UNIX 4.2 BSD file system predicted cache performance for a range of cache sizes assuming a 30 second ush back policy for writes. The 1991 study measured cache performance on a number of workstations running Sprite.
Reference: [Cabrera91] <author> Cabrera, L.-F., Long, D.D.E., Swift: </author> <title> Using Distributed Disk Striping to Provide High I/O Data Rates, </title> <journal> Computing Systems, </journal> <volume> V 4 (4), </volume> <year> 1991, </year> <pages> pp. 405-439. </pages>
Reference-contexts: There is a substantial push in the parallel computing community to support parallel file systems and I/O [Dibble88, Cao93, del Rosario94, Kotz94, Krieger94, Corbett95, Corbett96, Haskin96]. There has also been some effort in this direction in the distributed domain <ref> [Cabrera91, Hartman93, Lee96, Gibson97, Thekkath97] </ref>. But, parallel and distributed computing is not the focus of this dissertation. Certainly, using parallel threads is one approach to generating parallel I/O.
Reference: [Cao93] <author> Cao, P., Lim, S.B., Venkataraman, S., Wilkes, J., </author> <title> "The TickerTAIP Parallel RAID Architecture," </title> <booktitle> Proceedings of the 20th Annual International Symposium on Computer Architecture (ISCA), </booktitle> <month> May, </month> <year> 1993, </year> <pages> pp. 52-63. </pages> <note> Available from http://www.cs.wisc.edu/ ~cao/publications.html. </note>
Reference-contexts: For these applications, taking advantage of these calls would require many of the same code modifications needed to support asynchronous I/O. There is a substantial push in the parallel computing community to support parallel file systems and I/O <ref> [Dibble88, Cao93, del Rosario94, Kotz94, Krieger94, Corbett95, Corbett96, Haskin96] </ref>. There has also been some effort in this direction in the distributed domain [Cabrera91, Hartman93, Lee96, Gibson97, Thekkath97]. But, parallel and distributed computing is not the focus of this dissertation. Certainly, using parallel threads is one approach to generating parallel I/O.
Reference: [Cao94] <author> Cao, P., Felten, E.W., Li, K., </author> <title> Application-Controlled File Caching Policies, </title> <booktitle> Proceedings of the Summer 1994 USENIX Conference, </booktitle> <address> Boston, MA, </address> <month> June 6-10, </month> <year> 1994, </year> <pages> pp. 171-182. </pages> <note> Available from http://www.cs.wisc.edu/~cao/publications.html. </note>
Reference-contexts: More recently, Pei Cao et al. proposed that applications generate their own hints or advice about which caching policies to apply to which blocks <ref> [Cao94, Cao94a] </ref>. They showed how to apply the LRU algorithm at a global level to allocate buffers among processes while allowing each process to specify caching polices for the blocks in its partition. In subsequent work, she and collaborators proposed using application hints for prefetching as well [Cao95, Cao96]. <p> Perhaps the most familiar of these occurs in the form of policy advice from an application to the virtual-memory or file-cache modules. In these hints, the application recommends a resource management policy that has been statically or dynamically determined to improve performance for this application <ref> [Trivedi79, Sun88, Cao94, Cao94a] </ref>. In some cases this policy advice can be generated automatically from observations of file system activity. I already mentioned Korners work on automatically generating caching hints at the client for a remote file server [Korner90].
Reference: [Cao94a] <author> Cao, P., Felten, E.W., Li, K., </author> <title> Implementation and Performance of Application-Controlled File Caching, </title> <booktitle> Proceedings of the First USENIX Symposium on Operating Systems Design and Implementation (OSDI), </booktitle> <address> Monterey, CA, </address> <month> November, </month> <year> 1994, </year> <note> pp.165-178. Available from http://www.cs.wisc.edu/~cao/publications.html. </note>
Reference-contexts: More recently, Pei Cao et al. proposed that applications generate their own hints or advice about which caching policies to apply to which blocks <ref> [Cao94, Cao94a] </ref>. They showed how to apply the LRU algorithm at a global level to allocate buffers among processes while allowing each process to specify caching polices for the blocks in its partition. In subsequent work, she and collaborators proposed using application hints for prefetching as well [Cao95, Cao96]. <p> Perhaps the most familiar of these occurs in the form of policy advice from an application to the virtual-memory or file-cache modules. In these hints, the application recommends a resource management policy that has been statically or dynamically determined to improve performance for this application <ref> [Trivedi79, Sun88, Cao94, Cao94a] </ref>. In some cases this policy advice can be generated automatically from observations of file system activity. I already mentioned Korners work on automatically generating caching hints at the client for a remote file server [Korner90].
Reference: [Cao95] <author> Cao, P., Felten, E.W., Karlin, A., Li, K., </author> <title> A Study of Integrated Prefetching and Caching Strategies, </title> <booktitle> Proceedings of the Joint International Conference on Measurement & Modeling of Computer Systems (SIGMETRICS), </booktitle> <address> Ottawa, Canada, </address> <month> May, </month> <year> 1995, </year> <pages> pp. 188-197. </pages> <note> Available from http://www.cs.wisc.edu/~cao/publications.html. </note>
Reference-contexts: They showed how to apply the LRU algorithm at a global level to allocate buffers among processes while allowing each process to specify caching polices for the blocks in its partition. In subsequent work, she and collaborators proposed using application hints for prefetching as well <ref> [Cao95, Cao96] </ref>. This work is closely related to the work described in this dissertation and is discussed in more detail in Section 2.4. Caches are crucial to the performance of modern systems. Hints about future requests, whether inferred or explicit, can make them even more effective. <p> Relatively little work has been done on the combination of caching and prefetching. In one notable exception, however, Cao, Felton, Karlin and Li derive an aggressive prefetch-ing policy with excellent competitive performance characteristics in the context of complete knowledge of future accesses on a single disk <ref> [Cao95] </ref>. These same authors go on to show how to integrate prefetching according to hints into their system that exploits application-supplied cache management advice [Cao96] which I mentioned earlier. <p> This decomposition led to two studies that explored alternative solutions to the two sub-problems. 7.3.1 Prefetching and caching for a single process The aggressive algorithm was designed to prefetch and cache in the presence of complete knowledge of all future accesses <ref> [Cao95] </ref>. The algorithm is as follows: whenever the disk is free, eject the block whose next reference is furthest in the future to prefetch the block whose next reference is soonest, provided that the prefetched block will be referenced before the ejected one.
Reference: [Cao96] <author> Cao, P., Felten, E.W., Karlin, A., Li, K., </author> <title> Implementation and Performance of Integrated Application-Controlled File Caching, Prefetching and Disk Scheduling, </title> <journal> 229 ACM Transaction on Computer Systems (TOCS), </journal> <volume> V 14 (4), </volume> <month> November, </month> <year> 1996, </year> <pages> pp. 311-343. </pages> <note> Available from http://www.cs.wisc.edu/~cao/publications.html. </note>
Reference-contexts: They showed how to apply the LRU algorithm at a global level to allocate buffers among processes while allowing each process to specify caching polices for the blocks in its partition. In subsequent work, she and collaborators proposed using application hints for prefetching as well <ref> [Cao95, Cao96] </ref>. This work is closely related to the work described in this dissertation and is discussed in more detail in Section 2.4. Caches are crucial to the performance of modern systems. Hints about future requests, whether inferred or explicit, can make them even more effective. <p> And, they must manage the requests and the buffers they use. Thus, using asynchronous I/O can require substantial programmer effort. Intermediate between these two, applications can give hints about blocks they will access in the future <ref> [Gibson92, Patterson94, Patterson95, Cao96] </ref>. In some cases, compilers can generate such hints automatically [Mowry96]. This approach is the focus of this dissertation and will be discussed in some depth shortly. <p> These same authors go on to show how to integrate prefetching according to hints into their system that exploits application-supplied cache management advice <ref> [Cao96] </ref> which I mentioned earlier. The hints they use for prefetching are very similar to the disclosure hints I advocate, although they supplement them with caching advice hints whereas I rely on the one type of hint for both caching and prefetching. <p> However, if the disk is not the bottleneck when prefetching, then reordering the first request to the end would remove the chance to overlap any computation with I/O; queuing 1000 requests would increase elapsed time, not reduce it. One approach, known empirically to be effective <ref> [Cao96] </ref>, is to issue prefetches in batches. A new batch of prefetches could be issued just before the disk completes servicing the previous batch. Batches limit reordering while providing a disk scheduler the opportunity to sort requests. <p> Extended to multiple disks, whenever any disk is free, the algorithm prefetches the next-referenced block from that disk subject to the same ejection constraint as for the single disk. In an implementation of the algorithm <ref> [Cao96] </ref>, prefetches are issued in batches of 16 to provide the opportunity for disk scheduling to reduce average access time. <p> Pei Cao showed how to adapt the LRU algorithm to partition the cache buffers among competing processes while using an algorithm such as aggressive or forestall to decide within a partition when to prefetch and what to eject <ref> [Cao96] </ref>.
Reference: [Chen93] <author> Chen, C-M. M., Roussopoulos, N., </author> <title> Adaptive Database Buffer Allocation Using Query Feedback, </title> <booktitle> Proceedings of the 19th International Conference on Very Large Data Bases (VLDB), </booktitle> <address> Dublin, Ireland, </address> <year> 1993, </year> <pages> pp. 342-353. </pages>
Reference-contexts: In large integrated applications, detailed knowledge may be available. The database community has long taken advantage of this for buffer management. The buffer manager can use the access plan for a query to help determine the number of buffers to allocate <ref> [Sacco82, Chou85, Cornell89, Ng91, Chen93] </ref>. Ng, Faloutsos and Selliss work on marginal gains considered the question of how much benefit a query would derive from an additional buffer. Their work stimulated the development of my approach to cache management.
Reference: [Chen96] <author> Chen, P.M., Ng, W.T., Chandra, S., Aycock, C., Rajamani, G., Lowell, D., </author> <title> The Rio File Cache: Surviving Operating System Crashes, </title> <booktitle> Proceedings of the Seventh International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VII), </booktitle> <address> Cambridge, MA, </address> <month> October 1-5, </month> <year> 1996, </year> <pages> pp. 74-83. </pages>
Reference-contexts: But, battery-backed RAM or uninterruptable power supplies can protect from data loss due to power failure, and write-protecting the data cache can protect against software failures such as operating-system scribble bugs and crashes as shown by the Rio file system <ref> [Chen96] </ref>. For maximal security, the write buffer could be constructed with a solid-state disk made of flash memory. Prefetching data into the cache is the read-equivalent of write buffering. In its simplest form, file-system prefetching is based on the prevalence of sequential file access.
Reference: [Chou85] <author> Chou, H.T., DeWitt, </author> <title> D.J., An Evaluation of Buffer Management Strategies for Relational Database Systems, </title> <booktitle> Proceedings of the 11th International Conference on Very Large Data Bases (VLDB), </booktitle> <address> Stockholm, </address> <year> 1985, </year> <pages> pp. 127-141. </pages>
Reference-contexts: In large integrated applications, detailed knowledge may be available. The database community has long taken advantage of this for buffer management. The buffer manager can use the access plan for a query to help determine the number of buffers to allocate <ref> [Sacco82, Chou85, Cornell89, Ng91, Chen93] </ref>. Ng, Faloutsos and Selliss work on marginal gains considered the question of how much benefit a query would derive from an additional buffer. Their work stimulated the development of my approach to cache management.
Reference: [Corbett95] <author> Corbettt, P.F., Feitelson, D.G., Fineberg, S., Hsu, Y., Nitzberg, B., Prost, J.-P., Snir, M., Traversat, B., Wong, P., </author> <title> Overview of the MPI-IO Parallel I/O Interface, </title> <booktitle> IPPS '95 Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <month> April, </month> <year> 1995, </year> <pages> pp. 1-15. </pages>
Reference-contexts: For these applications, taking advantage of these calls would require many of the same code modifications needed to support asynchronous I/O. There is a substantial push in the parallel computing community to support parallel file systems and I/O <ref> [Dibble88, Cao93, del Rosario94, Kotz94, Krieger94, Corbett95, Corbett96, Haskin96] </ref>. There has also been some effort in this direction in the distributed domain [Cabrera91, Hartman93, Lee96, Gibson97, Thekkath97]. But, parallel and distributed computing is not the focus of this dissertation. Certainly, using parallel threads is one approach to generating parallel I/O.
Reference: [Corbett96] <author> Corbett, P.F., Feitelson, D.G., </author> <title> "The Vesta Parallel File System," </title> <journal> ACM Transactions on Computer Systems (TOCS), </journal> <volume> V 14 (3), </volume> <month> August, </month> <year> 1996, </year> <pages> pp. 225-264. </pages>
Reference-contexts: For these applications, taking advantage of these calls would require many of the same code modifications needed to support asynchronous I/O. There is a substantial push in the parallel computing community to support parallel file systems and I/O <ref> [Dibble88, Cao93, del Rosario94, Kotz94, Krieger94, Corbett95, Corbett96, Haskin96] </ref>. There has also been some effort in this direction in the distributed domain [Cabrera91, Hartman93, Lee96, Gibson97, Thekkath97]. But, parallel and distributed computing is not the focus of this dissertation. Certainly, using parallel threads is one approach to generating parallel I/O.
Reference: [Cornell89] <author> Cornell, D. W., Yu, P. S., </author> <title> Integration of Buffer Management and Query Optimization in Relational Database Environment, </title> <booktitle> Proceedings of the 15th International Conference on Very Large Data Bases (VLDB), </booktitle> <address> Amsterdam, </address> <month> August </month> <year> 1989, </year> <pages> pp. 247-255. </pages>
Reference-contexts: In large integrated applications, detailed knowledge may be available. The database community has long taken advantage of this for buffer management. The buffer manager can use the access plan for a query to help determine the number of buffers to allocate <ref> [Sacco82, Chou85, Cornell89, Ng91, Chen93] </ref>. Ng, Faloutsos and Selliss work on marginal gains considered the question of how much benefit a query would derive from an additional buffer. Their work stimulated the development of my approach to cache management.
Reference: [Cray93] <author> Cray Research, </author> <title> Advanced I/O Users Guide SG-3076 8.0, Cray Research, Inc., Order desk phone number (612) 683-5907, </title> <institution> Mendota Heights, MN, </institution> <year> 1993. </year> <month> 230 </month>
Reference-contexts: Batch or vector requests provide explicit parallelism with a single system call. For example, Crays UNICOS operating system supports a listio system call that initiates a list of distinct I/O requests <ref> [Cray93] </ref>. But, many applications are not written to support batch or vector processing. For these applications, taking advantage of these calls would require many of the same code modifications needed to support asynchronous I/O.
Reference: [Curewitz93] <author> Curewitz, K.M., Krishnan, P., Vitter, J.S., </author> <title> Practical Prefetching via Data Compression, </title> <booktitle> Proceedings of the 1993 ACM Conference on Management of Data (SIGMOD), </booktitle> <address> Washington, DC, </address> <month> May </month> <year> 1993, </year> <pages> pp. </pages> <note> 257-266. </note> <author> [de Jonge93] de Jonge, W., Kaashoek, M.F., Hsieh, W.C., </author> <title> The Logical Disk: A New Approach to Improving File Systems, </title> <booktitle> Proceedings of the Fourteenth ACM Symposium on Operating Systems Priciples (SOSP), </booktitle> <address> Asheville, NC, </address> <month> December 5-8, </month> <year> 1993, </year> <pages> pp. </pages> <note> 15-28. </note> <author> [del Rosario94] del Rosario, J.M., Choudhary, A., </author> <title> "High Performance I/O for Parallel Computers: Problems and Prospects," </title> <journal> IEEE Computer, </journal> <volume> V 27 (3), </volume> <month> March, </month> <year> 1994, </year> <pages> pp. 59-68. </pages>
Reference-contexts: The idea of using compression techniques for prefetching was first proposed by Vitter and Krishnan [Vitter91]. They and Curewitz applied the approach to page references in an object-oriented database <ref> [Curewitz93] </ref>. Palmer and Zdonik have also explored pattern matching for database references [Palmer90, Palmer91]. But, instead of using compression algorithms, they use an associative memory to find close matches to the current sequence of accesses.
Reference: [Denning67] <author> Denning, P.J., </author> <title> Effects of Scheduling on File Memory Operations, </title> <booktitle> American Federation of Information Processing Societies (AFIPS) Spring Joint Conference, V 30, </booktitle> <address> Atlantic City, NJ, </address> <month> April 18-20, </month> <year> 1967, </year> <pages> pp. 9-21. </pages>
Reference-contexts: The second approach to increasing sequentiality is to reorder accesses to reduce positioning delays. Scheduling requests to minimize average access time is itself an old and well-developed field of study <ref> [Denning67, Geist87, Seltzer90, Jacobson91, 20 CHAPTER 2 Worthington94] </ref>. But such scheduling techniques are only applicable if there are multiple requests to schedule. If there is only one request outstanding at the drive, there is little that can be done to reduce the service time for that request.
Reference: [Dibble88] <author> Dibble, P., Scott, M., Ellis, C., </author> <title> "Bridge: A High-Performance File System for Parallel Processors," </title> <booktitle> Proceedings of the Eighth International Conference on Distributed Computing Systems, </booktitle> <address> San Jose, CA, </address> <month> June, </month> <year> 1988, </year> <pages> pp. 154-161. </pages>
Reference-contexts: For these applications, taking advantage of these calls would require many of the same code modifications needed to support asynchronous I/O. There is a substantial push in the parallel computing community to support parallel file systems and I/O <ref> [Dibble88, Cao93, del Rosario94, Kotz94, Krieger94, Corbett95, Corbett96, Haskin96] </ref>. There has also been some effort in this direction in the distributed domain [Cabrera91, Hartman93, Lee96, Gibson97, Thekkath97]. But, parallel and distributed computing is not the focus of this dissertation. Certainly, using parallel threads is one approach to generating parallel I/O.
Reference: [Ebling94] <author> Ebling, </author> <title> M.R., Mummert, L.B., Steere, D.C., Overcoming the Network Bottleneck in Mobile Computing, </title> <booktitle> Proceedings of the IEEE Workshop on Mobile Computing Systems and Applications, </booktitle> <address> Santa Cruz, CA, </address> <month> December, </month> <year> 1994. </year>
Reference-contexts: Ghost buffers are dataless buffer headers which serve as placeholders to record when an access would have been a hit had there been more buffers in the cache. My use of ghost buffers was inspired by work by Maria Ebling on caching in a distributed file system <ref> [Ebling94] </ref>. The length of the LRU queue, including ghosts, is limited to the total number of buffers in the cache. Unfortunately, efficiently determining where in an LRU queue hits occur is not easy. After every cache miss, the buffer is released to the tail of the queue.
Reference: [Engler95] <author> Engler, D.R., Kaashoek, M.F., OToole, Jr., J., Exokernel: </author> <title> An Operating System Architecture for Application-Level Resource Management, </title> <booktitle> Proceedings of the 15th ACM Symposium on Operating Systems Principles (SOSP), </booktitle> <address> Copper Mountain Resort, CO, </address> <month> December 3-6, </month> <year> 1995, </year> <pages> pp. 251-266. </pages>
Reference-contexts: In the operating systems research community, efforts have been focussed on moving functionality out of the kernel and into user space where the user can customize behavior. Examples of this include external pagers [Harty92], scheduler activations [Anderson92], and, in the extreme case, micro-kernels themselves <ref> [Accetta86, Rozier88, Engler95] </ref>. Certainly, this approach can lead to dramatic performance gains for applications that are willing to rewrite significant chunks of the operating system. But, many application programmers dont wish to become systems programmers. They would prefer to focus their efforts on their own algorithms.
Reference: [Eustace95] <author> Eustace, A., Srivastava, A., </author> <title> ATOM: a Flexible Interface for Building High Performance Program Analysis Tools, </title> <booktitle> Proceedings USENIX Winter 1995 Technical Conference, </booktitle> <address> New Orleans, LA, </address> <month> January </month> <year> 1995, </year> <pages> pp. 303-314. 231 </pages>
Reference-contexts: Each trace record adds about 40 cycles of overhead.There is some variation due to cache effects. This overhead is substantially lower than automated techniques such as Digitals ATOM <ref> [Eustace95] </ref> package which would have added a few hundred cycles per record. 6.6.2 CPU overhead Table 6.27 analyzes the CPU activity of the seven benchmarks into six categories. User is the time spent at user level between system calls.
Reference: [Ganger97] <author> Ganger, G.R., Kaashoek, M.F., </author> <title> Embedded Inodes and Explicit Groupiing: Exploiting Disk Bandwidth for Small File, </title> <booktitle> Proceedings of the Winter 1997 USENIX Technical Conference, </booktitle> <month> January, </month> <year> 1997, </year> <pages> pp. 1-17. </pages>
Reference-contexts: Greg Ganger has shown how to embed inodes in directories to avoid some metadata accesses and how to co-locate multiple small files in the same directory with their metadata so that a single access can read all the metadata and user data for multiple files <ref> [Ganger97] </ref>. Again, these techniques also increase workload sequentiality. On the write side, buffering written data and delaying the write to disk can avoid some write accesses because data may be over-written or deleted before the disk writes occur [Baker91, Kistler93].
Reference: [Geist87] <author> Geist, R., Daniel, S., </author> <title> A Continuum of Disk Scheduling Algorithms, </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> V 5 (1), </volume> <month> February, </month> <year> 1987, </year> <pages> pp. 77-92. </pages>
Reference-contexts: The second approach to increasing sequentiality is to reorder accesses to reduce positioning delays. Scheduling requests to minimize average access time is itself an old and well-developed field of study <ref> [Denning67, Geist87, Seltzer90, Jacobson91, 20 CHAPTER 2 Worthington94] </ref>. But such scheduling techniques are only applicable if there are multiple requests to schedule. If there is only one request outstanding at the drive, there is little that can be done to reduce the service time for that request.
Reference: [Gibson92] <author> Gibson, G. A., Patterson, R. H., Satyanarayanan, M., </author> <title> Disk Reads with DRAM Latency, </title> <booktitle> Proceedings of the Third Workshop on Workstation Operating Systems, IEEE, </booktitle> <address> Key Biscayne, FL, </address> <month> April, </month> <year> 1992, </year> <pages> pp. 126-131. </pages> <note> Available from http://www.pdl.cs.cmu.edu/Publications/publications.html. </note>
Reference-contexts: And, they must manage the requests and the buffers they use. Thus, using asynchronous I/O can require substantial programmer effort. Intermediate between these two, applications can give hints about blocks they will access in the future <ref> [Gibson92, Patterson94, Patterson95, Cao96] </ref>. In some cases, compilers can generate such hints automatically [Mowry96]. This approach is the focus of this dissertation and will be discussed in some depth shortly.
Reference: [Gibson92a] <author> Gibson, G., </author> <title> Redundant Disk Arrays: Reliable, Parallel Secondary Storage, </title> <editor> Ph. D. </editor> <booktitle> thesis, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1992. </year>
Reference-contexts: The growing performance disparity between processors and disks requires a solution to secondary storage performance that can scale with time. Redundant Arrays of Inexpensive Disks (RAID) were proposed to be just such a scalable solution <ref> [Patterson88, Gibson92a] </ref>. Because arrays of any size could be built, disk arrays do provide a scalable 8 CHAPTER 2 amount of raw secondary-storage throughput. Unfortunately, as shown in Figure 2.2, many applications do not take advantage of that potential throughput. <p> Even sequential access is not increasing as fast as processor performance, but for actual disk performance to keep pace with even the increase in channel rates, workloads will have to become ever more sequential. The advent of redundant disk arrays (RAID) <ref> [Patterson88, Gibson92a, RAB96] </ref>, has added a new dimension to I/O subsystem performance, namely parallelism. <p> P 220 CHAPTER 7 221 Chapter 8 Conclusion In the late eighties and early nineties, researchers argued that storage device parallelism was required for secondary storage performance to balance increasing processor performance and proposed Redundant Arrays of Inexpensive Disks (RAID) to provide that parallelism <ref> [Patterson88, Gibson92a] </ref>. Since then, the processor and storage performance trends they identified have continued. In my analysis of the four principal virtues of storage workloads that maximize performance (ASAP or avoidance, sequentiality, asynchrony, and parallelism), I again found that only parallelism could satisfy the demand for storage throughput.
Reference: [Gibson97] <author> Gibson, Garth; Nagle, Deavid F.; Amiri, Khalil; Chang, Fay W.; Feinberg, Eugene M.; Gobioff, Howard; Lee, Chen; Ozceri, Berend; Riedel, Erik; Rochberg, David; Zelenka, Jim, </author> <title> File Server Scaling with Network-Attached Secure Disks, </title> <booktitle> Proceedings of the 1997 ACM Sigmetrics International Conference on Measurement & Modeling of Computer Systems (SIGMETRICS), </booktitle> <address> Seattle, WA, </address> <month> June 15-18, </month> <year> 1997, </year> <pages> pp. 272-284. </pages> <note> Available from http://www.pdl.cs.cmu.edu/Publications/publications.html. </note>
Reference-contexts: There is a substantial push in the parallel computing community to support parallel file systems and I/O [Dibble88, Cao93, del Rosario94, Kotz94, Krieger94, Corbett95, Corbett96, Haskin96]. There has also been some effort in this direction in the distributed domain <ref> [Cabrera91, Hartman93, Lee96, Gibson97, Thekkath97] </ref>. But, parallel and distributed computing is not the focus of this dissertation. Certainly, using parallel threads is one approach to generating parallel I/O.
Reference: [Griffioen93] <author> Griffioen, J., Appleton, R., </author> <title> Automatic Prefetching in a WAN, </title> <booktitle> Proceedings of the IEEE Workshop on Advances in Parallel and Distributed Systems, </booktitle> <address> Princeton, NJ, </address> <month> October, 6, </month> <year> 1993, </year> <pages> pp. 8-12. </pages> <note> Available from http://www.dcs.uky.edu/ ~griff/papers/mybib.html. </note>
Reference-contexts: Many researchers have explored ways to discover access patterns among files. For example Griffieon and Appleton observe the sequence of files opened and build a probability graph that records how often file B is referenced soon after file A <ref> [Griffioen93, Griffioen94, Griffioen95, Griffioen96] </ref>. Then, when A is referenced, if the likelihood that B will be referenced is above a threshold, the system prefetches file B. The system also 22 CHAPTER 2 prefetches any other files that are accessed soon after A with a frequency above the threshold.
Reference: [Griffioen94] <author> Griffioen, J., Appleton, R., </author> <title> Reducing File System Latency using a Predictive Approach, </title> <booktitle> Proceedings of the 1994 Summer USENIX Conference, </booktitle> <address> Boston, MA, </address> <year> 1994. </year> <note> Available from http://www.dcs.uky.edu/~griff/papers/mybib.html. </note>
Reference-contexts: Many researchers have explored ways to discover access patterns among files. For example Griffieon and Appleton observe the sequence of files opened and build a probability graph that records how often file B is referenced soon after file A <ref> [Griffioen93, Griffioen94, Griffioen95, Griffioen96] </ref>. Then, when A is referenced, if the likelihood that B will be referenced is above a threshold, the system prefetches file B. The system also 22 CHAPTER 2 prefetches any other files that are accessed soon after A with a frequency above the threshold.
Reference: [Griffioen95] <author> Griffioen, J., Appleton, R., </author> <title> Performance Measurements of Automatic Prefetching, </title> <booktitle> Proceedings of the International Society for Computers and their Applications (ISCA) International Conference on Parallel and Distributed Computing Sys 232 tems, </booktitle> <address> Orlando, FL, </address> <month> October, </month> <year> 1995, </year> <pages> pp. 165-170. </pages> <note> Available from http://www.dcs.uky.edu/~griff/papers/mybib.html. </note>
Reference-contexts: Many researchers have explored ways to discover access patterns among files. For example Griffieon and Appleton observe the sequence of files opened and build a probability graph that records how often file B is referenced soon after file A <ref> [Griffioen93, Griffioen94, Griffioen95, Griffioen96] </ref>. Then, when A is referenced, if the likelihood that B will be referenced is above a threshold, the system prefetches file B. The system also 22 CHAPTER 2 prefetches any other files that are accessed soon after A with a frequency above the threshold.
Reference: [Griffioen96] <author> Griffioen, J., Appleton, R., </author> <title> The Design, Implementation, and Evaluation of a Predictive Caching File System, </title> <type> Technical Report CS-264-96, </type> <institution> Department of Computer Science, University of Kentucky, </institution> <month> June, </month> <year> 1996. </year> <note> Available from http://www.dcs.uky.edu/~griff/papers/mybib.html. </note>
Reference-contexts: Many researchers have explored ways to discover access patterns among files. For example Griffieon and Appleton observe the sequence of files opened and build a probability graph that records how often file B is referenced soon after file A <ref> [Griffioen93, Griffioen94, Griffioen95, Griffioen96] </ref>. Then, when A is referenced, if the likelihood that B will be referenced is above a threshold, the system prefetches file B. The system also 22 CHAPTER 2 prefetches any other files that are accessed soon after A with a frequency above the threshold.
Reference: [Grimshaw91] <author> Grimshaw, A.S., Loyot Jr., </author> <title> E.C., ELFS: Object-Oriented Extensible File Systems, </title> <type> Technical Report TR-91-14, </type> <institution> Computer Science, University of Virginia, </institution> <year> 1991. </year>
Reference-contexts: Another example interface is an object-oriented file system implemented as library on top of the UNIX file system called ELFS <ref> [Grimshaw91] </ref>. ELFS has knowledge of file structure (e.g. 2-D matrix) and high-level file operations (e.g. FFT) that it uses for its own prefetching and caching operations. When users request these high-level operations, they in effect disclose to ELFS a large quantity of work.
Reference: [Grochowski96] <author> Grochowski, E.G., Hoyt, </author> <title> R.F., Future Trends in Hard Disk Drives, </title> <journal> IEEE Transactions on Magnetics, </journal> <volume> V 32 (3), </volume> <month> May, </month> <year> 1996, </year> <pages> pp. 1850-1854. </pages>
Reference-contexts: In the remaining chapters, I will show that my solution is in fact both feasible and effective. dramatic 40% per year, mechanical constraints have limited the growth in the number of random accesses per second a disk can perform to about 8% per year <ref> [Grochowski96] </ref>. Neither is enough to keep up with CPU performance which is increasing at about 58% per year [Hennessy96]. Setting the relative performance of each to 1.0 in 1989, this graph shows how the difference in performance growth rates leads to a growing disparity between processor and disk performance. <p> The bias in favor of sequential access is increasing. Channel data rates, which are a function of the rotational speed of the disk and the linear bit density within a track, are limited only by the speed of the channel electronics which is increasing about 40% per year <ref> [Grochowski96] </ref>. In contrast, accesses per second, which are largely determined by mechanical constraints, are increasing at a rate of only about 8% per year ASYNCHRONY + THROUGHPUT = LOW LATENCY 11 [Grochowski96]. <p> a track, are limited only by the speed of the channel electronics which is increasing about 40% per year <ref> [Grochowski96] </ref>. In contrast, accesses per second, which are largely determined by mechanical constraints, are increasing at a rate of only about 8% per year ASYNCHRONY + THROUGHPUT = LOW LATENCY 11 [Grochowski96]. Figure 2.1 shows the impact of these different growth rates on the relative performance of sequential vs. random accesses.
Reference: [Hartman93] <author> Hartman, J.H., Ousterhout, J.K., </author> <title> The Zebra Striped Network File System, </title> <booktitle> Proceedings of the Fourteenth ACM Symposium on Operating Systems Principles (SOSP), </booktitle> <address> Ashville, NC, </address> <month> December, </month> <year> 1993, </year> <pages> pp. 29-43. </pages>
Reference-contexts: There is a substantial push in the parallel computing community to support parallel file systems and I/O [Dibble88, Cao93, del Rosario94, Kotz94, Krieger94, Corbett95, Corbett96, Haskin96]. There has also been some effort in this direction in the distributed domain <ref> [Cabrera91, Hartman93, Lee96, Gibson97, Thekkath97] </ref>. But, parallel and distributed computing is not the focus of this dissertation. Certainly, using parallel threads is one approach to generating parallel I/O.
Reference: [Harty92] <author> Harty, K., Cheriton, </author> <title> D.R., Application-Controlled Physical Memory Using External Page-Cache Management, </title> <booktitle> Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASP-LOS-V), </booktitle> <address> Boston, MA, </address> <month> October, </month> <year> 1992, </year> <pages> pp. 187-199. </pages>
Reference-contexts: In the operating systems research community, efforts have been focussed on moving functionality out of the kernel and into user space where the user can customize behavior. Examples of this include external pagers <ref> [Harty92] </ref>, scheduler activations [Anderson92], and, in the extreme case, micro-kernels themselves [Accetta86, Rozier88, Engler95]. Certainly, this approach can lead to dramatic performance gains for applications that are willing to rewrite significant chunks of the operating system. But, many application programmers dont wish to become systems programmers.
Reference: [Haskin96] <author> Haskin, R., Schmuck, F., </author> <title> "The Tiger Shark File System," </title> <booktitle> Proceedings of IEEE 1996 Spring COMPCON, </booktitle> <address> Santa Clara, CA, </address> <month> February, </month> <year> 1996. </year>
Reference-contexts: For these applications, taking advantage of these calls would require many of the same code modifications needed to support asynchronous I/O. There is a substantial push in the parallel computing community to support parallel file systems and I/O <ref> [Dibble88, Cao93, del Rosario94, Kotz94, Krieger94, Corbett95, Corbett96, Haskin96] </ref>. There has also been some effort in this direction in the distributed domain [Cabrera91, Hartman93, Lee96, Gibson97, Thekkath97]. But, parallel and distributed computing is not the focus of this dissertation. Certainly, using parallel threads is one approach to generating parallel I/O.
Reference: [Hennessy96] <author> Hennessy, J.L., Patterson, D.A., </author> <title> Computer Architecture A Quantitative Approach, 2nd ed., </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Francisco, CA, </address> <year> 1996. </year>
Reference-contexts: Neither is enough to keep up with CPU performance which is increasing at about 58% per year <ref> [Hennessy96] </ref>. Setting the relative performance of each to 1.0 in 1989, this graph shows how the difference in performance growth rates leads to a growing disparity between processor and disk performance.
Reference: [IDC96] <author> International Data Corporation, </author> <title> 1996 Worldwide Disk Subsystems Market Review and Forecast, </title> <journal> International Data Corporation publication number IDC #11365, </journal> <month> June, </month> <year> 1996. </year> <month> 233 </month>
Reference: [Jacobson91] <author> Jacobson, </author> <title> D.M. and Wilkes,J., Disk Scheduling Algorithms Based on Rotational Position, </title> <type> Technical Report HPL-CSP-91-7, </type> <institution> Hewlett-Packard Laboratories, </institution> <month> February, </month> <year> 1991. </year>
Reference-contexts: The second approach to increasing sequentiality is to reorder accesses to reduce positioning delays. Scheduling requests to minimize average access time is itself an old and well-developed field of study <ref> [Denning67, Geist87, Seltzer90, Jacobson91, 20 CHAPTER 2 Worthington94] </ref>. But such scheduling techniques are only applicable if there are multiple requests to schedule. If there is only one request outstanding at the drive, there is little that can be done to reduce the service time for that request.
Reference: [Jain91] <author> Jain, Raj, </author> <title> The Art of Computer Systems Performance Analysis, </title> <publisher> John Wiley & Sons, </publisher> <address> New York, ISBN 0-471-50336-3, </address> <year> 1991. </year>
Reference-contexts: In this case, the total filesystem time and the total time spent in each component would be meaningful numbers in and of themselves and then, according to Jain <ref> [Jain91, p. 190] </ref>, the arithmetic average, and not the geometric average, would be the best estimate of the portion of time spent in each component. So, reporting the arithmetic average is not unreasonable.
Reference: [Kiczales] <author> Kiczales, G., </author> <title> Towards a New Model of Abstraction in the Engineering of Software, </title> <booktitle> Proceedings of the IMSA `92 Workshop on Reection and Meta-level Architectures, </booktitle> <year> 1992. </year>
Reference: [Kimbrel96] <author> Kimbrel, T., Tomkins, A., Patterson, R.H., Bershad, B., Cao, P., Felten, E.W., Gibson, G.A., Karlin, A.R., Li, K., </author> <title> A Trace-Driven Comparison of Algorithms for Parallel Prefetching and Caching, </title> <booktitle> Proceedings of the 2nd USENIX Symposium on Operating Systems Design and Implementation (OSDI), </booktitle> <address> Seattle, WA, </address> <month> October 28-31, </month> <year> 1996, </year> <pages> pp. 19-34. </pages> <note> Available from http://www.pdl.cs.cmu.edu/Publications/publications.html. </note>
Reference-contexts: Recent joint work compared the two approaches and found that an adaptive approach that incorporated features of each worked best across array sizes <ref> [Kimbrel96] </ref>. A further distinction is the buffer allocation algorithms. Cao et al. propose a two-level approach that uses the LRU algorithm to allocate buffers among processes and uses a local, application-controlled manager for each process buffers. <p> in the most dramatic case, to a 54% reduction in elapsed time with a 17-MByte cache. 4 6 8 10 12 14 16 18 20 22 cache size (MBytes) 0 100 200 300 elapsed time (sec) TIP, no hints TIP TIP PERFORMANCE EVALUATION 133 deeply and flush all cached blocks <ref> [Kimbrel96] </ref>. TIP effectively balances the allocation of cache buffers between prefetching and caching. In general, the benefit of informed caching is sensitive to the spacial locality of an applications I/O workload and how well conventional caching is working. <p> Ideally, prefetching would be sensitive to load imbalances and be deeper when beneficial. This observation has led to work developing such adaptability <ref> [Kimbrel95, Kimbrel96a, Kimbrel96, Tomkins97] </ref>. I will discuss the relationship of that work to the approach described here in Chapter 7. <p> Randomized striping could help balance the load within a single device, but there will inevitably be imbalances among devices. Ideally, prefetching and caching should be sensitive to such imbalances and adapt accordingly. Recent work has shown how this can be done <ref> [Kimbrel96, Tomkins97] </ref>. 6.6 System overhead TIPs cost-benefit cache management adds both CPU and memory overheads to the system. In this section, I quantify these overheads. <p> A large collaboration, which included the developers of both algorithms, used trace-driven simulation to compare the performance of the aggressive and cost-benefit algorithms when all accesses are known in advance <ref> [Kimbrel96] </ref>. 3 Also studied was a third algorithm, reverse aggressive, which was designed to take disk load into account when making ejection/prefetching decisions. <p> As described in Chapter 7, the same recent study and another <ref> [Kimbrel96] </ref>, showed how to improve the specific prefetching-benefit and hinted-block-ejection-cost estimates proposed in this dissertation in Chapter 4. No claim is made that the estimators proposed here are optimal. To the contrary, my hope was that a framework for resource management based on cost-benefit analysis would be flexible and extensible.
Reference: [Kistler93] <author> Kistler, J.J., </author> <title> Disconnected Operation in a Distributed File System, </title> <type> Ph. D. thesis, Technical Report CMU-CS-93-156, </type> <institution> School of Computer Science, Carnegie Mel-lon University, </institution> <year> 1993. </year> <note> Available from http://www.cs.cmu.edu/afs/cs.cmu.edu/ project/coda/Web/docs-coda.html. </note>
Reference-contexts: Again, these techniques also increase workload sequentiality. On the write side, buffering written data and delaying the write to disk can avoid some write accesses because data may be over-written or deleted before the disk writes occur <ref> [Baker91, Kistler93] </ref>. This is particularly useful when an application is appending data to a file with small writes.
Reference: [Korner90] <author> Korner, K., </author> <title> Intelligent Caching for Remote File Service, </title> <booktitle> Proceedings of the 10th International Conference on Distributed Computing Systems, </booktitle> <year> 1990, </year> <pages> pp. 220-226. </pages>
Reference-contexts: For example, Kim Korner proposed analyzing traces of file activity to correlate programs with their access patterns to files in particular directories or to files whose names end with a particular extension <ref> [Korner90] </ref>. For example, an assembler typically reads a file in the /tmp directory and then deletes it, so there is no point in caching any blocks that it reads from that directory. <p> In some cases this policy advice can be generated automatically from observations of file system activity. I already mentioned Korners work on automatically generating caching hints at the client for a remote file server <ref> [Korner90] </ref>. An example from parallel computing is Madhyasthas work on automatically classifying access patterns to set parallel file system policies [Madhyastha97]. In large integrated applications, detailed knowledge may be available. The database community has long taken advantage of this for buffer management.
Reference: [Kotz90] <author> Kotz, D., Ellis, C.S., </author> <title> Prefetching in File Systems for MIMD Multiprocessors, </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> V 1 (2), </volume> <month> April, </month> <year> 1990, </year> <pages> pp. 218-230. </pages>
Reference-contexts: Digital UNIX takes a more aggressive approach and prefetches ahead roughly the same number of blocks that have been read sequentially up to a maximum of 8 clusters of 8 blocks. David Kotz has looked at detecting and prefetching for strided access patterns within a file <ref> [Kotz90, Kotz91, Kotz93] </ref>. This work primarily focussed on the parallel computing domain, and there is more about it in the next section. Many researchers have explored ways to discover access patterns among files. <p> Prefetching has been studied specifically in the parallel domain by David Kotz who was perhaps the first to emphasize its importance for increasing I/O parallelism <ref> [Kotz90, Kotz91, Kotz93] </ref>. He explored techniques for detecting sequential and strided access patterns and prefetching in parallel for them with the goal of increasing array utilization. He was able to demonstrate significant reductions in elapsed time for the parallel computations he studied.
Reference: [Kotz91] <author> Kotz, D., </author> <title> "Prefetching and Caching Techniques in File Systems for MIMD Multiprocessors, </title> <type> Ph. D. thesis, Technical Report CS-1991-16, </type> <institution> Department of Computer Science, Duke University, </institution> <year> 1991. </year> <month> 234 </month>
Reference-contexts: Digital UNIX takes a more aggressive approach and prefetches ahead roughly the same number of blocks that have been read sequentially up to a maximum of 8 clusters of 8 blocks. David Kotz has looked at detecting and prefetching for strided access patterns within a file <ref> [Kotz90, Kotz91, Kotz93] </ref>. This work primarily focussed on the parallel computing domain, and there is more about it in the next section. Many researchers have explored ways to discover access patterns among files. <p> Prefetching has been studied specifically in the parallel domain by David Kotz who was perhaps the first to emphasize its importance for increasing I/O parallelism <ref> [Kotz90, Kotz91, Kotz93] </ref>. He explored techniques for detecting sequential and strided access patterns and prefetching in parallel for them with the goal of increasing array utilization. He was able to demonstrate significant reductions in elapsed time for the parallel computations he studied.
Reference: [Kotz93] <author> Kotz, D., Ellis, C.S., </author> <title> Practical Prefetching Techniques for Multiprocessor File Systems, Distributed and Parallel Databases, </title> <type> V 1 (1), </type> <month> January, </month> <year> 1993, </year> <pages> pp. 33-51. </pages>
Reference-contexts: Digital UNIX takes a more aggressive approach and prefetches ahead roughly the same number of blocks that have been read sequentially up to a maximum of 8 clusters of 8 blocks. David Kotz has looked at detecting and prefetching for strided access patterns within a file <ref> [Kotz90, Kotz91, Kotz93] </ref>. This work primarily focussed on the parallel computing domain, and there is more about it in the next section. Many researchers have explored ways to discover access patterns among files. <p> Prefetching has been studied specifically in the parallel domain by David Kotz who was perhaps the first to emphasize its importance for increasing I/O parallelism <ref> [Kotz90, Kotz91, Kotz93] </ref>. He explored techniques for detecting sequential and strided access patterns and prefetching in parallel for them with the goal of increasing array utilization. He was able to demonstrate significant reductions in elapsed time for the parallel computations he studied.
Reference: [Kotz94] <author> Kotz, D., </author> <title> Disk-directed I/O for MIMD Multiprocessors, </title> <booktitle> Proceedings of the 1st USENIX Symposium on Operating Systems Design and Implementation, </booktitle> <address> Monterey, CA, </address> <month> November, </month> <year> 1994, </year> <pages> pp. 61-74. </pages>
Reference-contexts: For these applications, taking advantage of these calls would require many of the same code modifications needed to support asynchronous I/O. There is a substantial push in the parallel computing community to support parallel file systems and I/O <ref> [Dibble88, Cao93, del Rosario94, Kotz94, Krieger94, Corbett95, Corbett96, Haskin96] </ref>. There has also been some effort in this direction in the distributed domain [Cabrera91, Hartman93, Lee96, Gibson97, Thekkath97]. But, parallel and distributed computing is not the focus of this dissertation. Certainly, using parallel threads is one approach to generating parallel I/O. <p> It ASYNCHRONY + THROUGHPUT = LOW LATENCY 33 would be interesting to incorporate estimates of the benefit of heuristic, sequential prefetching into the TIP system described here. Researchers have considered a variety of rich languages for expressing and exploiting disclosure. One example is collective I/O <ref> [Kotz94] </ref> in which collections of processes running on a parallel machine describe their related accesses so that the underlying I/O subsystem can optimize across the accesses.
Reference: [Krieger94] <author> Krieger, O., </author> <title> "HFS: A Flexible File System for Shared Memory Multiprocessors," </title> <type> Ph. D. thesis, </type> <institution> Department of Electrical and Computer Engineering, University of Toronto, </institution> <year> 1994. </year>
Reference-contexts: For these applications, taking advantage of these calls would require many of the same code modifications needed to support asynchronous I/O. There is a substantial push in the parallel computing community to support parallel file systems and I/O <ref> [Dibble88, Cao93, del Rosario94, Kotz94, Krieger94, Corbett95, Corbett96, Haskin96] </ref>. There has also been some effort in this direction in the distributed domain [Cabrera91, Hartman93, Lee96, Gibson97, Thekkath97]. But, parallel and distributed computing is not the focus of this dissertation. Certainly, using parallel threads is one approach to generating parallel I/O.
Reference: [Kroeger96] <author> Kroeger, </author> <title> T.M., Long, D.D.E., Predicting File System Actions from Prior Events, </title> <booktitle> Proceedings of the USENIX 1996 Annual Technical Conference, </booktitle> <address> San Diego, CA, </address> <month> January 22-26, </month> <year> 1996, </year> <pages> pp. 319-328. </pages>
Reference-contexts: The multiple access trees allow the systems to distinguish different patterns of use for the same files and prefetch for the currently occurring pattern. Kroeger and Long have explored using data compression techniques to discover frequently occurring sequences of file references <ref> [Kroeger96] </ref>. When the current sequence matches one or more prior sequences, the system prefetches the next file in each sequence whose frequency of occurrence is above a threshold. The idea of using compression techniques for prefetching was first proposed by Vitter and Krishnan [Vitter91].
Reference: [Lampson83] <author> Lampson, B.W., </author> <title> Hints for Computer System Design, </title> <booktitle> Proceedings of the 9th Symposium on Operating System Principles (SOSP), </booktitle> <address> Bretton Woods, N.H., </address> <year> 1983, </year> <pages> pp. 33-48. </pages>
Reference-contexts: Lampson reports their use in operating systems (Alto, Pilot), networking (Arpa-net, Ethernet), and language implementation (Smalltalk) <ref> [Lampson83] </ref>. Terry proposes their use for distributed systems [Terry87]. Broadly, these examples consult a possibly out-of-date cache as a hint to short-circuit some expensive computation or blocking event. 32 CHAPTER 2 An alternate class of hints are those that express one system components advance knowledge of its impact on another.
Reference: [Lee96] <author> Lee, E.K., Thekkath, </author> <title> C.A., "Petal: Distributed Virtual Disks," </title> <booktitle> Proceedings of the Seventh International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VII), </booktitle> <address> Cambridge, MA, </address> <month> October 1-5, </month> <year> 1996, </year> <pages> pp. 84-92. </pages>
Reference-contexts: There is a substantial push in the parallel computing community to support parallel file systems and I/O [Dibble88, Cao93, del Rosario94, Kotz94, Krieger94, Corbett95, Corbett96, Haskin96]. There has also been some effort in this direction in the distributed domain <ref> [Cabrera91, Hartman93, Lee96, Gibson97, Thekkath97] </ref>. But, parallel and distributed computing is not the focus of this dissertation. Certainly, using parallel threads is one approach to generating parallel I/O.
Reference: [Lee90] <author> Lee, K.-F., Hon, H.-W., Reddy, R., </author> <title> An Overview of the SPHINX Speech Recognition System, </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> V 38 (1), </volume> <month> January, </month> <year> 1990, </year> <pages> pp. 35-45. </pages>
Reference-contexts: I use this same array to store the translated coordinates from the first loop and iterate over these in a second loop. Thus, the original loop is split to deliver hints efficiently. 3.4.6 Sphinx Sphinx <ref> [Lee90] </ref> is a high-quality, speaker-independent, continuous-voice, speech-recognition system developed at Carnegie Mellon. In the benchmark, Sphinx recognizes an 18-second recording commonly used in Sphinx regression testing. Sphinx represents acoustics with Hidden Markov Models and uses a Viterbi beam search to prune unpromising word combinations from these models.
Reference: [Lei97] <author> Lei, H., Duchamp, D., </author> <title> An Analytical Approach to File Prefetching, </title> <booktitle> Proceedings 1997 USENIX Annual Technical Conference, </booktitle> <month> January, </month> <year> 1997. </year> <note> Also available from http://www.cs.columbia.edu/~lei/resume.html. 235 </note>
Reference-contexts: Using this technique, they were able to initiate prefetches for many files in advance of their use. Duchamp and collaborators observe the sequence of files, including other programs, that a program accesses during the course of a single run <ref> [Tait91, Lei97] </ref>. They store the pattern in an access tree. Over time, the system may build up multiple pattern trees for each program. When the program is later run again, its sequence of accesses is compared to the ones stored in the access trees for the program. <p> On a more speculative level, it might be possible to combine simple compiler techniques with access profiling. For example, it might be possible to augment Lei and Duch-amps access pattern trees <ref> [Lei97] </ref> with the disclosure of the arguments passed to a program to arrive at a more accurate prediction of accesses. Such a simple disclosure would not be hard to generate automatically.
Reference: [Madhyastha97] <author> Madhyastha, T.M., Reed, D.A., </author> <title> Input/Output Access Pattern Classification Using Hidden Markov Models, </title> <booktitle> Workshop on Input/Output in Parallel and Distributed Systems (IOPADS), </booktitle> <month> November, </month> <year> 1997. </year>
Reference-contexts: I already mentioned Korners work on automatically generating caching hints at the client for a remote file server [Korner90]. An example from parallel computing is Madhyasthas work on automatically classifying access patterns to set parallel file system policies <ref> [Madhyastha97] </ref>. In large integrated applications, detailed knowledge may be available. The database community has long taken advantage of this for buffer management. The buffer manager can use the access plan for a query to help determine the number of buffers to allocate [Sacco82, Chou85, Cornell89, Ng91, Chen93].
Reference: [Mattson70] <author> Mattson, R.L., Gecsei, J., Slutz, </author> <title> D.R., Traiger, I.L., Evaluation Techniques for Storage Hierarchies, </title> <journal> IBM Systems Journal, </journal> <volume> V 9 (2), </volume> <year> 1970, </year> <pages> pp. 78-117. </pages>
Reference-contexts: Thus, cache management is a game of predicting the future. Most current systems use a simple, his ASYNCHRONY + THROUGHPUT = LOW LATENCY 15 tory-based mechanism for choosing what to cache called the Least-Recently-Used (LRU) algorithm <ref> [Mattson70] </ref>. When a buffer is needed, LRU ejects the block that has not been accessed for the longest time. This heuristic has proven quite effective for general work-loads which often have high temporal locality, that is, for which recently accessed blocks tend to be reaccessed again soon. <p> I conclude with the description of an algorithm with a reasonable overhead that implements the min-max valuation of buffers. Chapter 5 describes the implementation in more detail. 4.3.1 The LRU estimator LRU block replacement is a stack algorithm <ref> [Mattson70] </ref>, which means that the ordering of blocks in the LRU queue is independent of the length of the queue. Consequently, cache hits occur at the same depth in the queue for all cache sizes.
Reference: [McKellar69] <author> McKellar, A.C., Coffman, Jr., E.G., </author> <title> Organizing Matrices and Matrix Operations for Paged Memory Systems, </title> <journal> Communications of the ACM, </journal> <volume> V 12 (3), </volume> <month> March </month> <year> 1969, </year> <pages> pp. 153-165. </pages>
Reference-contexts: Restructuring programs and reorganizing file data so that data accessed together are stored together can substantially reduce the number of accesses. Techniques such as blocking and tiling regular data structures have been developed to store data for more efficient access <ref> [McKellar69, Wolfe96] </ref>. When applications do need to read data, the system can have an impact on how many accesses it takes to satisfy those requests.
Reference: [McKusick84] <author> McKusick, M.K., Joy, W.J., Lefer, S.J., Fabry, </author> <title> R.S., A Fast File System for UNIX, </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> V 2 (3), </volume> <month> August </month> <year> 1984, </year> <pages> pp. 181-197. </pages>
Reference-contexts: Next best is to minimize seek and rotational delays and therefore the time that data channel is not utilized. For example, the Berkeley Fast File System (FFS) stores files from the same directory in the same group of neighboring cylinders to reduce seek distances <ref> [McKusick84] </ref>. 3. Asynchrony. Asynchronous accesses mask disk latency by allowing computation to continue while disk operations complete. Buffered writes and readahead are com mon examples of such asynchronous accesses. 4. Parallelism. Taking advantage of multiple disks through parallel transfer or multiple concurrent requests is a relatively recent innovation. <p> Even if the same number of bytes is transferred in the two cases, reducing the number of accesses reduces CPU access overhead and request-processing delays at the drive. A simple technique for accomplishing this is to group file data into larger blocks <ref> [McKusick84] </ref> which reduces the number of accesses when application data requests have high spacial locality, that is, when applications are likely to access data logically near data recently accessed in the same file. <p> Larger blocks ensure greater sequentiality (less external fragmentation) at the cost of more wasted space (internal fragmentation). Smaller blocks make the opposite trade-off. The Berkeley FFS tried to have the best of both worlds by subdividing whole file blocks into fragments when needed to recapture lost space <ref> [McKusick84] </ref>. An alternative is the dynamic solution adopted by Microsoft for its MS-DOS file system: periodically run a defragmentation utility that moves blocks around to form new, long sequential runs of free space [Microsoft93]. Larger blocks increase sequentiality for reasons beyond a reduction in fragmentation. <p> TIP provides conventional file caching service for the several file systems that DU supports including UFS, a variant of the Fast File System (FFS) <ref> [McKusick84] </ref> and the Network File System (NFS) [Sandberg85]. Application file requests first pass through the Virtual File System (VFS) [Sandberg85, Kleiman86] which forwards them to the target file system (UFS, NFS, or some other). The target asks the 94 CHAPTER 5 cache if it has the referenced block.
Reference: [McVoy91] <author> McVoy, L.W., Kleiman, </author> <title> S.R., Extent-like Performance from a UNIX File System, </title> <booktitle> Proceedings of the Winter 1991 USENIX Conference, </booktitle> <address> Dallas, TX, </address> <month> January, </month> <year> 1991, </year> <pages> pp. 33-43. </pages>
Reference-contexts: Similarly, reading multiple blocks in a single cluster or extent <ref> [Peacock88, McVoy91] </ref> can reduce the number of accesses. Another approach is to focus on avoiding metadata accesses which can be a substantial portion of the total workload, especially when there are accesses to many small files. <p> In practice, SunOS prefetches one block ahead when the last two blocks referenced were sequential, or for clustered I/Os, it prefetches the next cluster when the last cluster was read sequentially <ref> [McVoy91] </ref>. Digital UNIX takes a more aggressive approach and prefetches ahead roughly the same number of blocks that have been read sequentially up to a maximum of 8 clusters of 8 blocks. David Kotz has looked at detecting and prefetching for strided access patterns within a file [Kotz90, Kotz91, Kotz93].
Reference: [Microsoft93] <author> Microsoft Corporation, </author> <title> Microsoft Windows & MS-DOS 6 Users Guide, </title> <publisher> Microsoft Press, </publisher> <address> Redmond, WA, </address> <year> 1993. </year>
Reference-contexts: An alternative is the dynamic solution adopted by Microsoft for its MS-DOS file system: periodically run a defragmentation utility that moves blocks around to form new, long sequential runs of free space <ref> [Microsoft93] </ref>. Larger blocks increase sequentiality for reasons beyond a reduction in fragmentation. At first glance, it may appear that sequential accesses to a large number of small blocks and to a small number of large blocks would have the same sequentiality.
Reference: [Mogi94] <author> Mogi, K., Kitsuregawa, M., </author> <title> Dynamic Parity Stripe Reorganizations for RAID5 Disk Arrays, </title> <booktitle> Proceedings of the Third International Conference on Parallel and Distributed Information Systems (PDIS), </booktitle> <address> Austin, TX, </address> <month> September 28-30, </month> <year> 1994, </year> <pages> pp. 17-26. </pages>
Reference-contexts: Other researchers have looked at dynamically building new stripes to avoid parity updates <ref> [Mogi94] </ref>. Most of the foregoing techniques use static policies to govern storage reallocation. Autoraid goes a step further and allocates mirrored or RAID 5 storage depending on the rate of updates to the blocks.
Reference: [Mowry96] <author> Mowry, T., Demke, A., Krieger, O., </author> <title> Automatic Compiler-Inserted I/O Prefetching for Out-of-Core Applications, </title> <booktitle> Proceedings of Second USENIX Symposium on Operating Systems Design and Implementation (OSDI), </booktitle> <address> Seattle, WA, </address> <month> October 28-31, </month> <year> 1996, </year> <pages> pp. 3-17. </pages>
Reference-contexts: And, they must manage the requests and the buffers they use. Thus, using asynchronous I/O can require substantial programmer effort. Intermediate between these two, applications can give hints about blocks they will access in the future [Gibson92, Patterson94, Patterson95, Cao96]. In some cases, compilers can generate such hints automatically <ref> [Mowry96] </ref>. This approach is the focus of this dissertation and will be discussed in some depth shortly. Can asynchrony continue to mask disk latency even as processor performance continues to increase? For it to do so, three issues must be addressed. <p> In Chapter 3, I show that three straight-forward techniques are sufficient for annotating a broad range of applications to give hints. It is already possible for a compiler to generate some hints without programmer intervention <ref> [Mowry96] </ref> and there is a reasonable expectation that compiler and other techniques will eventually be able to generate hints for a broader range of applications automatically. <p> In the long term, I hope that compilers and pro-filers may generate reliable hints automatically. Indeed, recent work has already produced promising results. For example, Todd Mowry led a group that showed how compilers can apply memory prefetching techniques in the I/O domain <ref> [Mowry96] </ref>. But, in the short term, manual techniques for annotating applications to give hints remain the most powerful. Section 3.3 identifies three techniques for manual hint annotation. <p> However, I am sure that many more programs would give hints if annotations could be added automatically. We have already seen that compilers can generate hints for scientific applications <ref> [Mowry96] </ref>, but much remains to be done for irregular programs. On a more speculative level, it might be possible to combine simple compiler techniques with access profiling.
Reference: [NCSA89] <institution> National Center for Supercomputing Applications. XDataSlice for the X Window System, http://www.ncsa.uiuc.edu/, Univ. of Illinois at Urbana-Champaign, </institution> <year> 1989. </year> <month> 236 </month>
Reference-contexts: The first sixty of Davidsons hints are accurate. When the algorithm terminates, one, unconsumed hint is left outstanding. 3.4.5 XDataSlice XDataSlice (XDS) is a data visualization package developed by the National Center for Supercomputing Applications (NCSA) at the University of Illinois at Urbana-Cham-paign <ref> [NCSA89] </ref>. Among other features, XDS lets scientists select and view a false-color representation of an arbitrary planar slice through their 3-dimensional scientific dataset. The datasets may originate from a broad range of applications such as airflow simulations, pollution modeling, or magnetic resonance imaging, and tend to be very large.
Reference: [Ng91] <author> Ng, R., Faloutsos, C., Sellis, T., </author> <title> Flexible Buffer Allocation Based on Marginal Gains, </title> <booktitle> Proceedings of the 1991 ACM Conference on Management of Data (SIG-MOD), </booktitle> <pages> pp. 387-396. </pages>
Reference-contexts: In large integrated applications, detailed knowledge may be available. The database community has long taken advantage of this for buffer management. The buffer manager can use the access plan for a query to help determine the number of buffers to allocate <ref> [Sacco82, Chou85, Cornell89, Ng91, Chen93] </ref>. Ng, Faloutsos and Selliss work on marginal gains considered the question of how much benefit a query would derive from an additional buffer. Their work stimulated the development of my approach to cache management.
Reference: [Ousterhout85] <author> Ousterhout, J.K., Da Costa, H., Harrison, D., Kunze, J.A., Kupfer, M., Thompson, J.G., </author> <title> A Trace-Driven Analysis of the UNIX 4.2 BSD File System, </title> <booktitle> Proceedings of the 10th Symposium on Operating System Principles (SOSP), </booktitle> <address> Orcas Island, WA, </address> <month> December, </month> <year> 1985, </year> <pages> pp. 15-24. </pages>
Reference-contexts: Is such improvement likely? Table 2.1 compares the performance predicted for a variety of operating system file cache sizes in 1985 <ref> [Ousterhout85] </ref> with that observed in 1991 [Baker91] by a group at Berkeley. The first observation, based on the 1985 data, is that increasing the size of an already large cache does not reduce the miss ratio much. <p> The closer workloads can come to this ideal, the less time they will waste on positioning delays, and the greater the disk performance they will achieve. Table 2.1. Comparison of caching performance in 1985 and 1991. The numbers in this table are drawn from <ref> [Ousterhout85] </ref> and [Baker91]. The 1985 tracing study of the UNIX 4.2 BSD file system predicted cache performance for a range of cache sizes assuming a 30 second ush back policy for writes. The 1991 study measured cache performance on a number of workstations running Sprite.
Reference: [Ousterhout89] <author> Ousterhout, J., Douglis, F., </author> <title> Beating the I/O Bottleneck: A Case for Log-Structured File Systems, </title> <journal> ACM Operating Systems Review, </journal> <volume> V 23 (1), </volume> <month> January, </month> <year> 1989, </year> <pages> pp. 11-28. </pages> <note> Also available as Technical Report UCB/CSD 88/467, </note> <institution> University of Cali-fornia-Berkeley, </institution> <year> 1988. </year>
Reference-contexts: Hints about future requests, whether inferred or explicit, can make them even more effective. Could caches be made so effective that they could permanently relieve the I/O bottleneck? At one time, it appeared 16 CHAPTER 2 that by increasing cache size, caches could virtually eliminate slow synchronous data reads <ref> [Ousterhout89] </ref>. But, for caches to compensate for the growing disparity between processor and disk performance, their miss ratios will have to drop proportionately so that an ever-smaller proportion of data accesses actually suffer the full latency of a disk read.
Reference: [Palmer90] <author> Palmer, </author> <title> M.L., Zdonik, S.B., Predictive Caching, </title> <type> Technical Report CS-90-29, </type> <institution> Computer Science, Brown University, </institution> <year> 1990. </year>
Reference-contexts: The idea of using compression techniques for prefetching was first proposed by Vitter and Krishnan [Vitter91]. They and Curewitz applied the approach to page references in an object-oriented database [Curewitz93]. Palmer and Zdonik have also explored pattern matching for database references <ref> [Palmer90, Palmer91] </ref>. But, instead of using compression algorithms, they use an associative memory to find close matches to the current sequence of accesses. All of these approaches to prefetching have the attractive advantage of being transparent to the user.
Reference: [Palmer91] <author> Palmer, </author> <title> M.L., Zdonik, S.B., FIDO: A Cache that Learns to Fetch, </title> <type> Technical Report CS-90-15, </type> <institution> Computer Science, Brown University, </institution> <year> 1991. </year>
Reference-contexts: The idea of using compression techniques for prefetching was first proposed by Vitter and Krishnan [Vitter91]. They and Curewitz applied the approach to page references in an object-oriented database [Curewitz93]. Palmer and Zdonik have also explored pattern matching for database references <ref> [Palmer90, Palmer91] </ref>. But, instead of using compression algorithms, they use an associative memory to find close matches to the current sequence of accesses. All of these approaches to prefetching have the attractive advantage of being transparent to the user.
Reference: [Parsons97] <author> Parsons, I., Unrau, R., Schaeffer, J., Szafron, D., PI/OT: </author> <title> Parallel I/O Templates, </title> <booktitle> Parallel Computing, </booktitle> <pages> V 23 (4-5), </pages> <month> June, </month> <year> 1997, </year> <pages> pp. 543-570. </pages>
Reference-contexts: This is particularly useful when, for example, each processor is performing strided access to a matrix, but collectively they are accessing the matrix in its entirety. Another example in the parallel domain is the use of templates to specify parallel I/O access patterns <ref> [Parsons97] </ref>. In this approach, users specify how the system should coordinate the file accesses of multiple parallel processes. For example, the processes might share a common file pointer so that accesses are serialized, or the file might be broken into distinct segments for each process.
Reference: [Patterson88] <author> Patterson, D., Gibson, G., Katz, R., </author> <title> A Case for Redundant Arrays of Inexpensive Disks (RAID), </title> <booktitle> Proceedings of the 1988 ACM Conference on Management of Data (SIGMOD), </booktitle> <address> Chicago, IL, </address> <month> June, </month> <year> 1988, </year> <pages> pp. 109-116. </pages>
Reference-contexts: The growing performance disparity between processors and disks requires a solution to secondary storage performance that can scale with time. Redundant Arrays of Inexpensive Disks (RAID) were proposed to be just such a scalable solution <ref> [Patterson88, Gibson92a] </ref>. Because arrays of any size could be built, disk arrays do provide a scalable 8 CHAPTER 2 amount of raw secondary-storage throughput. Unfortunately, as shown in Figure 2.2, many applications do not take advantage of that potential throughput. <p> Even sequential access is not increasing as fast as processor performance, but for actual disk performance to keep pace with even the increase in channel rates, workloads will have to become ever more sequential. The advent of redundant disk arrays (RAID) <ref> [Patterson88, Gibson92a, RAB96] </ref>, has added a new dimension to I/O subsystem performance, namely parallelism. <p> P 220 CHAPTER 7 221 Chapter 8 Conclusion In the late eighties and early nineties, researchers argued that storage device parallelism was required for secondary storage performance to balance increasing processor performance and proposed Redundant Arrays of Inexpensive Disks (RAID) to provide that parallelism <ref> [Patterson88, Gibson92a] </ref>. Since then, the processor and storage performance trends they identified have continued. In my analysis of the four principal virtues of storage workloads that maximize performance (ASAP or avoidance, sequentiality, asynchrony, and parallelism), I again found that only parallelism could satisfy the demand for storage throughput.
Reference: [Patterson93] <author> Patterson, R.H., Gibson, G.A., Satyanarayanan, M., </author> <title> A Status Report on Research in Transparent Informed Prefetching, </title> <journal> ACM Operating Systems Review, </journal> <volume> V 27 (2), </volume> <month> April, </month> <year> 1993, </year> <pages> pp. 21-34. </pages> <note> Available from http://www.pdl.cs.cmu.edu/Publications/publications.html. </note>
Reference-contexts: to thousands of accesses at once thereby exposing 38 CHAPTER 3 I/O concurrency and, as we will see in later chapters, enabling the system to add much needed read parallelism to these applications workloads. 3.1 Hints that disclose I advocate a form of hints based on advance knowledge called disclosure <ref> [Patterson93] </ref>. An application discloses its future resource requirements when its hints describe its future requests in terms of the existing request interface. For example, a disclosing hint might indicate that a particular file is going to be read sequentially four times in succession.
Reference: [Patterson94] <author> Patterson, R.H., Gibson, </author> <title> G.A., Exposing I/O Concurrency with Informed Prefetching, </title> <booktitle> Proceedings of the 3rd IEEE International Conference on Parallel and 237 Distributed Information Systems (PDIS), </booktitle> <address> Austin, TX, </address> <month> September 28-30, </month> <year> 1994, </year> <pages> pp. 7-16. </pages> <note> Available from http://www.pdl.cs.cmu.edu/Publications/publications.html. </note>
Reference-contexts: And, they must manage the requests and the buffers they use. Thus, using asynchronous I/O can require substantial programmer effort. Intermediate between these two, applications can give hints about blocks they will access in the future <ref> [Gibson92, Patterson94, Patterson95, Cao96] </ref>. In some cases, compilers can generate such hints automatically [Mowry96]. This approach is the focus of this dissertation and will be discussed in some depth shortly.
Reference: [Patterson95] <author> Patterson, R.H., Gibson, G.A., Ginting, E., Stodolsky, D., Zelenka, J., </author> <title> Informed Prefetching and Caching, </title> <booktitle> Proceedings of the 15th ACM Symposium on Operating Systems Principles (SOSP), </booktitle> <address> Copper Mountain Resort, CO, </address> <month> December 3-6, </month> <year> 1995, </year> <pages> pp. 79-95. </pages> <note> Available from http://www.pdl.cs.cmu.edu/Publications/publications.html. </note>
Reference-contexts: And, they must manage the requests and the buffers they use. Thus, using asynchronous I/O can require substantial programmer effort. Intermediate between these two, applications can give hints about blocks they will access in the future <ref> [Gibson92, Patterson94, Patterson95, Cao96] </ref>. In some cases, compilers can generate such hints automatically [Mowry96]. This approach is the focus of this dissertation and will be discussed in some depth shortly. <p> In addition to the latency of the fetch, T disk , these requests suffer the computational overhead, T driver , of allocating a buffer, queuing the request at the 1 Note that T app T CPU in the terminology of the paper, Informed Prefetching and Caching <ref> [Patterson95] </ref>. T N I O T app T I O +( ) ,= file disk VM, net, etc. user kernel disk T I/O has two components, computation and I/O. <p> Under the assumption of no disk congestion, a prefetch of this xth future block would complete in T disk time. Thus, the stall time when requesting this block is at most 2 This formulation is slightly different from that presented in the paper, Informed Prefetching and Caching <ref> [Patterson95] </ref> in that it compares prefetching x vs. x-1 accesses in advance instead of x+1 vs. x. This reformulation lets us ask: assuming blocks 1 x-1 have been prefetched, what is the benefit of prefetching the next block x accesses in advance.
Reference: [RAB96] <author> Raid Advisory Board, </author> <title> The RAIDbook, A Source Book for Disk Array Technology, 5th ed., edited by Paul Massiglia, ISBN 1-879936-90-9, The RAID Advisory Board, </title> <type> 13 Marie Lane, </type> <institution> St. Peter, MN, </institution> <year> 1996. </year>
Reference-contexts: Even sequential access is not increasing as fast as processor performance, but for actual disk performance to keep pace with even the increase in channel rates, workloads will have to become ever more sequential. The advent of redundant disk arrays (RAID) <ref> [Patterson88, Gibson92a, RAB96] </ref>, has added a new dimension to I/O subsystem performance, namely parallelism.
Reference: [Peacock88] <author> Peacock, J.K., </author> <title> The Counterpoint Fast File System, </title> <booktitle> Proceedings of the USENIX Winter Conference, </booktitle> <address> Dallas, TX, </address> <month> February 9-12, </month> <year> 1988, </year> <pages> pp. 243-249. </pages>
Reference-contexts: Similarly, reading multiple blocks in a single cluster or extent <ref> [Peacock88, McVoy91] </ref> can reduce the number of accesses. Another approach is to focus on avoiding metadata accesses which can be a substantial portion of the total workload, especially when there are accesses to many small files.
Reference: [Rochberg97] <author> Rochberg, D., Gibson, G., </author> <title> Prefetching Over a Network: Early Experience with CTIP, </title> <journal> ACM SIGMETRICS Performance Evaluation Review, </journal> <volume> V 25 (3), </volume> <month> December, </month> <year> 1997, </year> <pages> pp. 29-36. </pages> <note> Also available at http://www.pdl.cs.cmu.edu/Publications/publications.html. </note>
Reference-contexts: I have only added such support for TIP to the UFS file system, but other researchers are working to add support to other file systems including NFS <ref> [Rochberg97] </ref>. As a last note on the relationship between TIP and the rest of the system, I should point out that the original UBC shares memory pages with the Virtual Memory (VM) system, and the partition between the two varies dynamically. <p> The fact that the improved estimators could be integrated into the existing framework argues that this is indeed the case. Recent work by David Rochberg extending TIP to prefetch from a distributed file system further strengthens this argument <ref> [Rochberg97] </ref>. Collectively, these results show that disclosure hints are a feasible and effective mechanism for passing I/O optimization information across the file-system interface that frees applications from the burden of buffer management and scheduling their own disk accesses.
Reference: [Rosenblum92] <author> Rosenblum, M., Ousterhout, J.K., </author> <title> The Design and Implementation of a Log-Structured File System, </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> V 10 (1), Feb-ruary, </volume> <year> 1992, </year> <pages> pp. 26-52. </pages>
Reference-contexts: Parallelism. Taking advantage of multiple disks through parallel transfer or multiple concurrent requests is a relatively recent innovation. Beyond disk-array architecture itself, relatively little effort has been devoted to increasing the parallelism of disk workloads. An exception is the Log-Structured File System (LFS) <ref> [Rosenblum92] </ref> which organizes multiple small writes into large writes that can take advantage of parallel transfer. One goal of this dissertation is to explore informed prefetching as a technique for taking advantage of array parallelism for reads. <p> Another approach is to focus on avoiding metadata accesses which can be a substantial portion of the total workload, especially when there are accesses to many small files. For example, the Log-Structured File System (LFS) avoids some synchronous metadata writes by appending new data followed by the updated metadata <ref> [Rosenblum92] </ref>. Because the appends are sequential, LFS also enables more clustered writes and even the RAID 5 large-write optimization.
Reference: [Rozier88] <author> Rozier, M., Abrossimov, V., Armand, F., Boule, I., Gien, M., Guillemont, M., Herrmann, F., Kaiser, C., Langlois, S., Leonard, P., Neuhauser, W., </author> <title> CHORUS Distributed Operating System, </title> <journal> Computing Systems, </journal> <volume> V 1 (4), </volume> <year> 1988, </year> <pages> pp. 305-370. </pages>
Reference-contexts: In the operating systems research community, efforts have been focussed on moving functionality out of the kernel and into user space where the user can customize behavior. Examples of this include external pagers [Harty92], scheduler activations [Anderson92], and, in the extreme case, micro-kernels themselves <ref> [Accetta86, Rozier88, Engler95] </ref>. Certainly, this approach can lead to dramatic performance gains for applications that are willing to rewrite significant chunks of the operating system. But, many application programmers dont wish to become systems programmers. They would prefer to focus their efforts on their own algorithms.
Reference: [Ruemmler91] <author> Ruemmler, C., Wilkes, J., </author> <title> Disk Shufing, </title> <type> Technical Report HPL-CSP-91-30, </type> <institution> Hewlett-Packard Laboratories, </institution> <month> October, </month> <year> 1991. </year>
Reference-contexts: One approach is to use a greedy algorithm that assigns the free block with lowest access latency given the current head position to blocks as they are written <ref> [Ruemmler91] </ref>. Unfortunately, this approach can leave the data in non-sequential locations which can reduce physical sequentiality for reads. The Logical Disk interface extends SCSIs linear block address space to a two-dimensional space of a meta-list of lists of blocks [de Jonge93].
Reference: [Sacco82] <author> Sacco, G.M., Schkolnick, M., </author> <title> A Mechanism for Managing the Buffer Pool in a Relational Database System Using the Hot Set Model, </title> <booktitle> Proceedings of the 8th Inter 238 national Conference on Very Large Data Bases (VLDB), </booktitle> <month> September, </month> <year> 1982, </year> <pages> pp. 257-262. </pages>
Reference-contexts: In large integrated applications, detailed knowledge may be available. The database community has long taken advantage of this for buffer management. The buffer manager can use the access plan for a query to help determine the number of buffers to allocate <ref> [Sacco82, Chou85, Cornell89, Ng91, Chen93] </ref>. Ng, Faloutsos and Selliss work on marginal gains considered the question of how much benefit a query would derive from an additional buffer. Their work stimulated the development of my approach to cache management.
Reference: [Salem86] <author> Salem, K. Garcia-Molina, H., </author> <title> Disk Striping, </title> <booktitle> Proceedings of the 2nd IEEE International Conference on Data Engineering, </booktitle> <year> 1986. </year>
Reference: [Seltzer90] <author> Seltzer, M. I., Chen, P. M., Ousterhout, J. K., </author> <title> Disk Scheduling Revisted, </title> <booktitle> Proceedings of the Winter 1990 USENIX Technical Conference, </booktitle> <address> Washinton, DC, </address> <month> January, </month> <year> 1990. </year>
Reference-contexts: The second approach to increasing sequentiality is to reorder accesses to reduce positioning delays. Scheduling requests to minimize average access time is itself an old and well-developed field of study <ref> [Denning67, Geist87, Seltzer90, Jacobson91, 20 CHAPTER 2 Worthington94] </ref>. But such scheduling techniques are only applicable if there are multiple requests to schedule. If there is only one request outstanding at the drive, there is little that can be done to reduce the service time for that request. <p> FFS does this when it buffers data for up to 30 seconds as described above. Some have advocated buffering thousands of writes <ref> [Seltzer90] </ref>. The problem comes on the read side. Because many applications only issue one read request at a time, there is often no opportunity to schedule reads. One solution is issuing prefetches of additional blocks along with the read of the requested block.
Reference: [Smith78] <author> Smith, A.J., </author> <title> Sequentiality and Prefetching in Database Systems, </title> <journal> ACM Transactions on Database Systems, </journal> <volume> V 3 (3), </volume> <month> September, </month> <year> 1978, </year> <pages> pp. 223-247. </pages>
Reference-contexts: More explicitly, the file system can readahead sequential blocks of a file. But, because not all accesses are sequential, and because it can hurt performance to prefetch unused data, it is advantageous to scale the depth of prefetching according to the length of a run of sequential accesses <ref> [Smith78, Smith85] </ref>. In practice, SunOS prefetches one block ahead when the last two blocks referenced were sequential, or for clustered I/Os, it prefetches the next cluster when the last cluster was read sequentially [McVoy91]. <p> The use of estimates of the cost of an operation have long been used to develop allocation policies. For example, in his study of sequential prefetching <ref> [Smith78] </ref>, Smith developed estimates of the cost of prefetching a block, the cost of a demand miss, and the cost of the loss of cache effectiveness due to the early ejection of a block to reuse a buffer for prefetching.
Reference: [Smith85] <author> Smith, A.J., </author> <title> Disk Cache Miss Ratio Analysis and Design Considerations, </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> V 3 (3), </volume> <month> August </month> <year> 1985, </year> <pages> pp. 161-203. </pages>
Reference-contexts: More explicitly, the file system can readahead sequential blocks of a file. But, because not all accesses are sequential, and because it can hurt performance to prefetch unused data, it is advantageous to scale the depth of prefetching according to the length of a run of sequential accesses <ref> [Smith78, Smith85] </ref>. In practice, SunOS prefetches one block ahead when the last two blocks referenced were sequential, or for clustered I/Os, it prefetches the next cluster when the last cluster was read sequentially [McVoy91].
Reference: [Staelin90] <author> Staelin, C., Garcia-Molina, H., </author> <title> Clustering Active Disk Data to Improve Disk Performance, </title> <type> Technical Report CS-TR-283-90, </type> <institution> Computer Science, Princeton University, </institution> <month> September, </month> <year> 1990. </year>
Reference-contexts: For example, systems can take advantage of the fact that the distribution of accesses tends to be highly skewed to a small portion of the data stored on a disk. By profiling data accesses, disk subsystems [Vongsathorn90, Akyrek93, Akyrek93a] or file systems <ref> [Staelin90] </ref> can migrate or replicate [Akyrek92] the most active blocks to the center of the disk to reduce seek distances. The second approach to increasing sequentiality is to reorder accesses to reduce positioning delays.
Reference: [Stathopoulos94] <author> Stathopoulos, A., Fischer, </author> <title> C.F., A Davidson Program for Finding a Few Selected Extreme Eigenpairs of a Large, Sparse, Real, Symmetric Matrix, </title> <journal> Computer Physics Communications, </journal> <volume> V 79, </volume> <year> 1994, </year> <pages> pp. 268-290. </pages>
Reference-contexts: The Davidson algorithm <ref> [Stathopoulos94] </ref> is an element of the suite that computes, by successive refinement, the extreme eigenvalue-eigenvector pairs of a large, sparse, real, symmetric matrix stored on disk. In the benchmark, the size of this matrix is 2089 8-KByte blocks or 16.3 MBytes.
Reference: [Steere97] <author> Steere, </author> <title> D.C., Exploiting the Non-Determinism and Asynchrony of Set Itera-tors to Reduce Aggregate File I/O Latency, </title> <booktitle> Proceedings of the 16th ACM Symposium on Operating Systems Principles (SOSP), </booktitle> <address> Saint-Malo, France, </address> <month> October 5-8, </month> <year> 1997, </year> <pages> pp. 252-263. </pages>
Reference-contexts: Many different patterns are possible, but the use of the template to specify them means that the system can be aware of what access patterns it will be asked to support. Another example is Dynamic Sets <ref> [Steere97] </ref> in which users specify a set of files over which they will iterate performing some operation. The specification of a set of files discloses likely access to all files in the set.
Reference: [Stodolsky93] <author> Stodolsky, D., Gibson, G., Holland, M., </author> <title> Parity Logging: Overcoming the Small Write Problem in Redundant Disk Arrays, </title> <booktitle> Proceedings of the 21st Annual International Symposium on Computer Architecture (ISCA), </booktitle> <month> May, </month> <year> 1993, </year> <pages> pp. 64-75. </pages> <note> Available at http://www.pdl.cs.cmu.edu/Publications/publications.html. 239 </note>
Reference-contexts: log, applying the updates to a RAID 5 array, but taking advantage of multiple writes to the same region of the array to avoid multiple overwrites, to coalesce neighboring writes to achieve the large-write optimization, or just to increase the locality and therefore decrease the latency of more isolated updates <ref> [Stodolsky93] </ref>. Other researchers have looked at dynamically building new stripes to avoid parity updates [Mogi94]. Most of the foregoing techniques use static policies to govern storage reallocation. Autoraid goes a step further and allocates mirrored or RAID 5 storage depending on the rate of updates to the blocks.
Reference: [Stonebraker86] <author> Stonebraker, M., Rowe, </author> <title> L, The Design of Postgres, </title> <booktitle> Proceedings of 1986 ACM International Conference on Management of Data (SIGMOD), </booktitle> <address> Washington, DC, </address> <month> May 28-30, </month> <year> 1986. </year>
Reference-contexts: As shown in lists to give hints for the final passes. 3.4.3 Postgres Postgres version 4.2 <ref> [Stonebraker86, Stonebraker90] </ref> is an extensible, object-oriented relational database system from the University of California at Berkeley. In our bench mark, Postgres executes a join of two relations.
Reference: [Stonebraker90] <author> Stonebraker, M., Rowe, L.A., Hirohama, M., </author> <title> The implementation of POSTGRES, </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> V 2 (1), </volume> <month> March, </month> <year> 1990, </year> <pages> pp. 125-142. </pages>
Reference-contexts: As shown in lists to give hints for the final passes. 3.4.3 Postgres Postgres version 4.2 <ref> [Stonebraker86, Stonebraker90] </ref> is an extensible, object-oriented relational database system from the University of California at Berkeley. In our bench mark, Postgres executes a join of two relations.
Reference: [Sun88] <author> Sun Microsystems, Inc., </author> <title> Sun OS Reference Manual, Part Number 800-1751-10, Revision A, </title> <month> May 9, </month> <year> 1988. </year>
Reference-contexts: Perhaps the most familiar of these occurs in the form of policy advice from an application to the virtual-memory or file-cache modules. In these hints, the application recommends a resource management policy that has been statically or dynamically determined to improve performance for this application <ref> [Trivedi79, Sun88, Cao94, Cao94a] </ref>. In some cases this policy advice can be generated automatically from observations of file system activity. I already mentioned Korners work on automatically generating caching hints at the client for a remote file server [Korner90].
Reference: [Tait91] <author> Tait, C.D., Duchamp, D., </author> <title> Detection and Exploitation of File Working Sets, </title> <booktitle> Proceedings of the 11th International Conference on Distributed Computing Systems, </booktitle> <address> Arlington, TX, </address> <month> May, </month> <year> 1991, </year> <pages> pp. 2-9. </pages>
Reference-contexts: Using this technique, they were able to initiate prefetches for many files in advance of their use. Duchamp and collaborators observe the sequence of files, including other programs, that a program accesses during the course of a single run <ref> [Tait91, Lei97] </ref>. They store the pattern in an access tree. Over time, the system may build up multiple pattern trees for each program. When the program is later run again, its sequence of accesses is compared to the ones stored in the access trees for the program.
Reference: [Terry87] <author> Terry, </author> <title> D.B., Caching Hints in Distributed Systems, </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> V SE-13 (1), </volume> <month> January, </month> <year> 1987. </year>
Reference-contexts: Lampson reports their use in operating systems (Alto, Pilot), networking (Arpa-net, Ethernet), and language implementation (Smalltalk) [Lampson83]. Terry proposes their use for distributed systems <ref> [Terry87] </ref>. Broadly, these examples consult a possibly out-of-date cache as a hint to short-circuit some expensive computation or blocking event. 32 CHAPTER 2 An alternate class of hints are those that express one system components advance knowledge of its impact on another.
Reference: [Thekkath97] <author> Thekkath, C.A., Mann, T., Lee, E.K., "Frangipani: </author> <title> A Scalable Distributed File System," </title> <booktitle> Proceedings of the Sixteenth ACM Symposium on Operating Systems Principles (SOSP), </booktitle> <address> Saint-Malo, France, </address> <month> October 5-8, </month> <year> 1997, </year> <pages> pp. 224-237. </pages>
Reference-contexts: There is a substantial push in the parallel computing community to support parallel file systems and I/O [Dibble88, Cao93, del Rosario94, Kotz94, Krieger94, Corbett95, Corbett96, Haskin96]. There has also been some effort in this direction in the distributed domain <ref> [Cabrera91, Hartman93, Lee96, Gibson97, Thekkath97] </ref>. But, parallel and distributed computing is not the focus of this dissertation. Certainly, using parallel threads is one approach to generating parallel I/O.
Reference: [Tomkins97] <author> Tomkins, A., Patterson, R.H., Gibson, </author> <title> G.A., Informed Multi-Process Prefetching and Caching, </title> <booktitle> Proceedings of the 1997 ACM Sigmetrics International Conference on Measurement & Modeling of Computer Systems (SIGMETRICS), </booktitle> <address> Seat-tle, WA, </address> <month> June 15-18, </month> <year> 1997, </year> <pages> pp. 100-114. </pages> <note> Available from http://www.pdl.cs.cmu.edu/ Publications/publications.html. </note>
Reference-contexts: My proposal is for a single unified manager that uses locally-generated cost and benefit estimates to find the best global allocation. A recent study compared and contrasted these two approaches <ref> [Tomkins97] </ref>. <p> Ideally, prefetching would be sensitive to load imbalances and be deeper when beneficial. This observation has led to work developing such adaptability <ref> [Kimbrel95, Kimbrel96a, Kimbrel96, Tomkins97] </ref>. I will discuss the relationship of that work to the approach described here in Chapter 7. <p> Randomized striping could help balance the load within a single device, but there will inevitably be imbalances among devices. Ideally, prefetching and caching should be sensitive to such imbalances and adapt accordingly. Recent work has shown how this can be done <ref> [Kimbrel96, Tomkins97] </ref>. 6.6 System overhead TIPs cost-benefit cache management adds both CPU and memory overheads to the system. In this section, I quantify these overheads. <p> work show how to incorporate disk load not only into the prefetching-benefit estimate, but also into the ejection-cost estimate. 7.3.2 Allocating resources among multiple processes A second comparative study investigated the second sub-problem: how to prefetch and cache when there are multiple processes and when not all accesses are hinted <ref> [Tomkins97] </ref>. The study compared using the time-tested LRU algorithm to make global allocation decisions to the cost-benefit approach. <p> Recent work has shown how to adapt the disk-load-sensitive forestall algorithm to the cost-benefit approach to build a modified TIP system called TIPTOE (TIP with temporal overload estimators) <ref> [Tomkins97, Tomkins97a] </ref>. The adaptation requires generating a benefit estimate in terms of the common currency. The fundamental modeling insight of Chapter 4 remains the basis of TIPTOE: the benefit of prefetching is the reduction of stall. <p> with Andrew Tomkins, we found, in simulation, that such a post-hint estimator could reduce elapsed time for a pair of applications by as much as 30%, and that the average reduction for a set of seven single-process and 11 multi-process experiments on a range of array sizes was nearly 5% <ref> [Tomkins97] </ref>. Implementing a post-hint estimator remains an area for future work. The only stumbling block I anticipate is the active region of the LRU queue (see Section 5.3.3 for a description of the active region). <p> CHAPTER 8 In recent joint work with Andrew Tomkins, we provide additional evidence in support of the claim that allocation based on cost-benefit analysis is effective by showing that, in simulation, the cost-benefit approach outperforms a competing approach which uses an LRU queue to allocate buffers at a global level <ref> [Tomkins97] </ref>. As described in Chapter 7, the same recent study and another [Kimbrel96], showed how to improve the specific prefetching-benefit and hinted-block-ejection-cost estimates proposed in this dissertation in Chapter 4. No claim is made that the estimators proposed here are optimal.
Reference: [Tomkins97a] <author> Tomkins, A., Ph. D. </author> <type> thesis, Technical Report CMU-CS-97-181, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <year> 1997. </year> <note> Available from http://www.cs.cmu.edu/~andrewt. </note>
Reference-contexts: Recent work has shown how to adapt the disk-load-sensitive forestall algorithm to the cost-benefit approach to build a modified TIP system called TIPTOE (TIP with temporal overload estimators) <ref> [Tomkins97, Tomkins97a] </ref>. The adaptation requires generating a benefit estimate in terms of the common currency. The fundamental modeling insight of Chapter 4 remains the basis of TIPTOE: the benefit of prefetching is the reduction of stall.
Reference: [Trivedi79] <author> Trivedi, </author> <title> K.S., An Analysis of Prepaging, </title> <journal> Computing, </journal> <volume> V 22 (3), </volume> <year> 1979, </year> <pages> pp. 191-210. 240 </pages>
Reference-contexts: Perhaps the most familiar of these occurs in the form of policy advice from an application to the virtual-memory or file-cache modules. In these hints, the application recommends a resource management policy that has been statically or dynamically determined to improve performance for this application <ref> [Trivedi79, Sun88, Cao94, Cao94a] </ref>. In some cases this policy advice can be generated automatically from observations of file system activity. I already mentioned Korners work on automatically generating caching hints at the client for a remote file server [Korner90].
Reference: [Vitter91] <author> Vitter, J.S., Krishnan, P., </author> <title> Optimal Prefetching via Data Compression, </title> <type> Technical Report CS-91-46, </type> <institution> Computer Science, Brown University, </institution> <month> July, </month> <year> 1991. </year> <booktitle> An extended abstract appears in Proceedings of the 32nd Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <address> Puerto Rico, </address> <month> October, </month> <year> 1991. </year>
Reference-contexts: When the current sequence matches one or more prior sequences, the system prefetches the next file in each sequence whose frequency of occurrence is above a threshold. The idea of using compression techniques for prefetching was first proposed by Vitter and Krishnan <ref> [Vitter91] </ref>. They and Curewitz applied the approach to page references in an object-oriented database [Curewitz93]. Palmer and Zdonik have also explored pattern matching for database references [Palmer90, Palmer91]. But, instead of using compression algorithms, they use an associative memory to find close matches to the current sequence of accesses.
Reference: [Vongsathorn90] <author> Vongsathorn, P., Carson, </author> <title> S.D., A System for Adaptive Disk Rearrangement, </title> <journal> Software - Practice and Experience (UK), </journal> <volume> V 20 (3), </volume> <month> March </month> <year> 1990, </year> <pages> pp. 225-242. </pages>
Reference-contexts: For example, systems can take advantage of the fact that the distribution of accesses tends to be highly skewed to a small portion of the data stored on a disk. By profiling data accesses, disk subsystems <ref> [Vongsathorn90, Akyrek93, Akyrek93a] </ref> or file systems [Staelin90] can migrate or replicate [Akyrek92] the most active blocks to the center of the disk to reduce seek distances. The second approach to increasing sequentiality is to reorder accesses to reduce positioning delays.
Reference: [Wilkes96] <author> Wilkes, J., Golding, R., Staelin, C., Sullivan, T., </author> <title> The HP AutoRAID Hierarchical Storage System, </title> <journal> ACM Transactions on Computer Systems (TOCS), </journal> <volume> V 14 (1), </volume> <month> February </month> <year> 1996, </year> <pages> pp. 108-136. </pages>
Reference-contexts: Examples include HP Autoraid which initially writes to mirrored storage and later migrates data in large chunks to a RAID 5 arrays to reduce the space overhead of redundancy information and provide higher, parallel bandwidth for subsequent reads <ref> [Wilkes96] </ref>.
Reference: [Wolfe96] <author> Wolfe, M.J., </author> <title> High Performance Compilers for Parallel Computing, </title> <publisher> Addison-Wesley, </publisher> <address> Redwood City, CA, </address> <year> 1996. </year>
Reference-contexts: Restructuring programs and reorganizing file data so that data accessed together are stored together can substantially reduce the number of accesses. Techniques such as blocking and tiling regular data structures have been developed to store data for more efficient access <ref> [McKellar69, Wolfe96] </ref>. When applications do need to read data, the system can have an impact on how many accesses it takes to satisfy those requests. <p> This has the disadvantage of requiring that the entire data object be read from disk to render a slice that cuts across all rows. To make loading arbitrary slices efficient, and in keeping with state-of-the-art tiling techniques <ref> [Wolfe96] </ref>, I reorganized the object into submatrices as shown in Figure 3.5 and extended the DFSD layer to export a blocked view of the scientific data object.
Reference: [Worthington94] <author> Worthington, B. L., Ganger, G. R., Patt, Y. N., </author> <title> Scheduling algorithms for modern disk drives, </title> <booktitle> Proceedings of the 1994 ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems (SIGMETRICS), </booktitle> <month> May, </month> <year> 1994, </year> <pages> pp. 241-51. </pages>
Reference-contexts: Thus, CSCAN scans the disk surface in increasing order and then seeks back to begin a new scan. Some researchers refer to this algorithm as CLOOK <ref> [Worthington94] </ref>. IMPLEMENTATION OF INFORMED PREFETCHING AND CACHING 121 5.4 Conclusion Taking advantage of application disclosure of future file accesses for prefetching and caching is a bookkeeping challenge. To hold a block in the cache, a hint for the block must be found before the block is ejected.
Reference: [Wu92] <author> Wu, S. and Manber, U. </author> <title> AGREP - a Fast Approximate Pattern-Matching Tool, </title> <booktitle> Proceedings of the 1992 Winter USENIX Conference, </booktitle> <address> San Francisco, CA, </address> <month> January, </month> <year> 1992, </year> <pages> pp. 20-24. </pages>
Reference-contexts: both as case studies in annotating applications with hints and as benchmarks for evaluating the performance benefits of informed prefetching and caching based on these hints. 3.4.1 Agrep Agrep, version 2.04, a variant of the standard UNIX Grep utility, was written by Wu and Manber at the University of Arizona <ref> [Wu92] </ref>. It is a fast full-text pattern matching program that allows matching errors. Invoked in its simplest form, it opens the files specified on its command line one at a time, in argument order, and reads each sequentially.
References-found: 107

