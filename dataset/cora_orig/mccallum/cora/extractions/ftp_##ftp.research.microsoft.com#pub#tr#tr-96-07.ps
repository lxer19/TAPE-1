URL: ftp://ftp.research.microsoft.com/pub/tr/tr-96-07.ps
Refering-URL: http://www.research.microsoft.com/~meek/meek.htm
Root-URL: http://www.research.microsoft.com
Email: dang@cs.technion.ac.il  heckerma@microsoft.com  meek@cmu.edu  
Title: Asymptotic Model Selection for Directed Networks with Hidden Variables  
Author: Dan Geiger David Heckerman Christopher Meek 
Address: One Microsoft Way Redmond, WA 98052  
Affiliation: Microsoft Research Advanced Technology Division Microsoft Corporation  
Date: May 1996  
Abstract: Technical Report MSR-TR-96-07 
Abstract-found: 1
Intro-found: 1
Reference: [Bouckaert, 1995] <author> Bouckaert, R. </author> <year> (1995). </year> <title> Bayesian belief networks: From construction to inference. </title> <type> PhD thesis, </type> <institution> University Utrecht. </institution>
Reference: [Buntine, 1996] <author> Buntine (1996). </author> <title> A guide to the literature on learning graphical models. </title> <journal> IEEE KDE, </journal> <note> to appear. </note>
Reference: [Buntine, 1994] <author> Buntine, W. </author> <year> (1994). </year> <title> Operations for learning with graphical models. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2 </volume> <pages> 159-225. </pages>
Reference: [Cheeseman and Stutz, 1995] <author> Cheeseman, P. and Stutz, J. </author> <year> (1995). </year> <title> Bayesian classification (AutoClass): Theory and results. </title> <editor> In Fayyad, U., Piatesky-Shapiro, G., Smyth, P., and Uthurusamy, R., editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <publisher> AAAI Press, </publisher> <address> Menlo Park, CA. </address>
Reference: [Chickering and Heckerman, 1996] <author> Chickering, D. and Heckerman, D. </author> <year> (1996). </year> <title> Efficient approximations for the marginal likelihood of incomplete data given a Bayesian network. </title> <booktitle> In Proceedings of Twelfth Conference on Uncertainty in Artificial Intelligence, </booktitle> <address> Portland, OR. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 15 </pages>
Reference-contexts: Although Cheeseman and Stutz suggested this approximation in the context of simple AutoClass models, it can be used to score any Bayesian network with discrete variables as well as other models <ref> [Chickering and Heckerman, 1996] </ref>. We call this approximation the CS scoring function.
Reference: [Cooper and Herskovits, 1992] <author> Cooper, G. and Herskovits, E. </author> <year> (1992). </year> <title> A Bayesian method for the induction of probabilistic networks from data. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 309-347. </pages>
Reference: [Geiger and Heckerman, 1995] <author> Geiger, D. and Heckerman, D. </author> <year> (1995). </year> <title> A characterization of the Dirichlet distribution with application to learning Bayesian networks. </title> <booktitle> In Proceedings of Eleventh Conference on Uncertainty in Artificial Intelligence, </booktitle> <address> Montreal, </address> <publisher> QU, </publisher> <pages> pages 196-207. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: [Glymour et al., 1987] <author> Glymour, C., Scheines, R., Spirtes, P., and Kelly, K. </author> <year> (1987). </year> <title> Discovering Causal Structure. </title> <publisher> Academic Press. </publisher>
Reference: [Haughton, 1988] <author> Haughton, D. </author> <year> (1988). </year> <title> On the choice of a model to fit data from an exponential family. </title> <journal> Annals of Statistics, </journal> <volume> 16 </volume> <pages> 342-355. </pages>
Reference: [Heckerman, 1995a] <author> Heckerman, D. </author> <year> (1995a). </year> <title> A Bayesian approach for learning causal networks. </title> <booktitle> In Proceedings of Eleventh Conference on Uncertainty in Artificial Intelligence, </booktitle> <address> Montreal, </address> <publisher> QU, </publisher> <pages> pages 285-295. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This assumption was made explicit, because it does not hold for causal networks where two arcs with opposing directions corre spond to distinct hypotheses <ref> [Heckerman, 1995a] </ref>.
Reference: [Heckerman, 1995b] <author> Heckerman, D. </author> <year> (1995b). </year> <title> A tutorial on learning Bayesian networks. </title> <type> Technical Report MSR-TR-95-06, </type> <institution> Microsoft, Redmond, WA. </institution> <note> Revised January, </note> <year> 1996. </year>
Reference: [Heckerman et al., 1994] <author> Heckerman, D., Geiger, D., and Chickering, D. </author> <year> (1994). </year> <title> Learning Bayesian networks: The combination of knowledge and statistical data. </title> <booktitle> In Proceedings of Tenth Conference on Uncertainty in Artificial Intelligence, </booktitle> <address> Seattle, WA, </address> <pages> pages 293-301. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Third, if a node has the same parents in two distinct networks, then the distribution of the parameters associated with this node are identical in both networks (parameter modularity <ref> [Heckerman et al., 1994] </ref>). Fourth, each case is complete.
Reference: [Heckerman et al., 1995] <author> Heckerman, D., Geiger, D., and Chickering, D. </author> <year> (1995). </year> <title> Learning Bayesian networks: The combination of knowledge and statistical data. </title> <journal> Machine Learning, </journal> <volume> 20 </volume> <pages> 197-243. </pages>
Reference-contexts: fifth assumption is implied from the first three assumptions and from one additional assumption that if S 1 and S 2 are equivalent Bayesian networks (i.e., they represent the same sets of joint distributions), then the events S h 1 and S h 2 are equivalent as well (hypothesis equivalence <ref> [Heckerman et al., 1995] </ref>). This assumption was made explicit, because it does not hold for causal networks where two arcs with opposing directions corre spond to distinct hypotheses [Heckerman, 1995a].
Reference: [Herskovits, 1991] <author> Herskovits, E. </author> <year> (1991). </year> <title> Computer-based probabilistic network construction. </title> <type> PhD thesis, </type> <institution> Medical Information Sciences, Stanford University, Stanford, </institution> <address> CA. </address>
Reference: [Lam and Bacchus, 1993] <author> Lam, W. and Bacchus, F. </author> <year> (1993). </year> <title> Using causal information and local measures to learn Bayesian networks. </title> <booktitle> In Proceedings of Ninth Conference on Uncertainty in Artificial Intelligence, </booktitle> <address> Washington, DC, </address> <pages> pages 243-250. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: [Neal, 1992] <author> Neal, R. </author> <year> (1992). </year> <title> Connectionist learning of belief networks. </title> <journal> Artificial Intelligence, </journal> <volume> 56 </volume> <pages> 71-113. </pages>
Reference-contexts: These models, which we call sigmoid networks, are useful for learning relationships among discrete variables, because these models capture non-linear relationships among variables yet employ only a small number of parameters <ref> [Neal, 1992, Saul et al., 1996] </ref>. Using techniques similar to those in Section 5, we can compute the rank of the Jacobian matrix h @W .
Reference: [Rissanen, 1987] <author> Rissanen, J. </author> <year> (1987). </year> <title> Stochastic complexity (with discussion). </title> <journal> Journal of the Royal Statistical Society, Series B, </journal> <volume> 49 </volume> <pages> 223-239 and 253-265. 16 </pages>
Reference: [Saul et al., 1996] <author> Saul, L., Jaakkola, T., and Jordan, M. </author> <year> (1996). </year> <title> Mean field theory for sigmoid belief networks. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4 </volume> <pages> 61-76. </pages>
Reference-contexts: These models, which we call sigmoid networks, are useful for learning relationships among discrete variables, because these models capture non-linear relationships among variables yet employ only a small number of parameters <ref> [Neal, 1992, Saul et al., 1996] </ref>. Using techniques similar to those in Section 5, we can compute the rank of the Jacobian matrix h @W .
Reference: [Schwarz, 1978] <author> Schwarz, G. </author> <year> (1978). </year> <title> Estimating the dimension of a model. </title> <journal> Annals of Statistics, </journal> <volume> 6 </volume> <pages> 461-464. </pages>
Reference-contexts: noting that det h i is proportional to N yields the BIC: p (D N jS h ) p (D N j ^ ; S h ) d=2 log (N ) (7) A careful derivation in this spirit shows that the error in this approximation does not depend on N <ref> [Schwarz, 1978] </ref>. For Bayesian networks, the function f () is known. Thus, all the assumptions about this function can be verified.
Reference: [Spiegelhalter et al., 1993] <author> Spiegelhalter, D., Dawid, A., Lauritzen, S., and Cowell, R. </author> <year> (1993). </year> <title> Bayesian analysis in expert systems. </title> <journal> Statistical Science, </journal> <volume> 8 </volume> <pages> 219-282. </pages>
Reference: [Spiegelhalter and Lauritzen, 1990] <author> Spiegelhalter, D. and Lauritzen, S. </author> <year> (1990). </year> <title> Sequential updating of conditional probabilities on directed graphical structures. </title> <journal> Networks, </journal> <volume> 20 </volume> <pages> 579-605. </pages>
Reference-contexts: First, the data D is assumed to be a random sample from some Bayesian network (S; s ). Second, for each network structure, the parameter sets 1 ; : : : ; n are mutually independent (global independence <ref> [Spiegelhalter and Lauritzen, 1990] </ref>), and the parameter sets i1 ; : : :; iq i for each i are assumed to be mutually independent (local independence [Spiegelhalter and Lauritzen, 1990]). <p> Second, for each network structure, the parameter sets 1 ; : : : ; n are mutually independent (global independence <ref> [Spiegelhalter and Lauritzen, 1990] </ref>), and the parameter sets i1 ; : : :; iq i for each i are assumed to be mutually independent (local independence [Spiegelhalter and Lauritzen, 1990]). Third, if a node has the same parents in two distinct networks, then the distribution of the parameters associated with this node are identical in both networks (parameter modularity [Heckerman et al., 1994]). Fourth, each case is complete.
Reference: [Spivak, 1979] <author> Spivak, M. </author> <year> (1979). </year> <title> A Comprehensive Introduction to Differential Geometry 1, </title> <booktitle> 2nd edition. </booktitle> <address> Publish or Perish, Berkeley, CA. </address>
Reference: [Suzuki, 1993] <author> Suzuki, J. </author> <year> (1993). </year> <title> A construction of Bayesian networks from databases based on an MDL scheme. </title> <booktitle> In Proceedings of Ninth Conference on Uncertainty in Artificial Intelligence, </booktitle> <address> Washington, DC, </address> <pages> pages 266-273. </pages> <publisher> Morgan Kaufmann. </publisher> <pages> 17 </pages>
References-found: 23

