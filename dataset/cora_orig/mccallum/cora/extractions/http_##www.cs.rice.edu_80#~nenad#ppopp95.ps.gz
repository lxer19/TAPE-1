URL: http://www.cs.rice.edu:80/~nenad/ppopp95.ps.gz
Refering-URL: http://www.cs.rice.edu:80/~nenad/papers.html
Root-URL: 
Email: ken@cs.rice.edu nenad@cs.rice.edu sethi@cs.rice.edu  
Title: A Linear-Time Algorithm for Computing the Memory Access Sequence in Data-Parallel Programs  
Author: Ken Kennedy Nenad Nedeljkovic Ajay Sethi 
Affiliation: Department of Computer Science, Rice University  
Note: Center for Research on Parallel Computation  
Abstract: Data-parallel languages, such as High Performance Fortran, are widely regarded as a promising means for writing portable programs for distributed-memory machines. Novel features of these languages call for the development of new techniques in both compilers and run-time systems. In this paper, we present an improved algorithm for finding the local memory access sequence in computations involving regular sections of arrays with cyclic(k) distributions. After establishing the fact that regular section indices correspond to elements of an integer lattice, we show how to find a lattice basis that allows for simple and fast enumeration of memory accesses. The complexity of our algorithm is shown to be lower than that of the previous solution for the same problem. In addition, the experimental results demonstrate the efficiency of our method in practice. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Ancourt, F. Coelho, F. Irigoin, and R. Keryell. </author> <title> A linear algebra framework for static HPF code distribution. </title> <booktitle> In Proceedings of the Fourth Workshop on Compilers for Parallel Computers, </booktitle> <address> Delft, The Netherlands, </address> <month> December </month> <year> 1993. </year>
Reference-contexts: Since this point belongs to processor 1, no adjustment to M [0] is necessary. In the next iteration of the outer loop, we visit index 76 in the inner loop, setting M <ref> [1] </ref> = 12 (lines 36-37). After terminating the inner loop, the next index visited is 103, which does not belong to processor 1, and therefore we move to the point 139, setting M [2] = 15 (lines 45-46). <p> Ancourt et al. describe a linear algebra framework for compiling independent loops in HPF <ref> [1] </ref>. The assumption of independent parallelism allows them to enumerate loop iterations in any order. Although their method can handle arbitrary affine subscripts and alignments, generated loop bounds and local array subscripts can be quite complex, and thus introduce a significant overhead.
Reference: [2] <author> B. Chapman, P. Mehrotra, and H. Zima. </author> <title> Programming in Vienna Fortran. </title> <journal> Scientific Programming, </journal> <volume> 1(1) </volume> <pages> 31-50, </pages> <month> Fall </month> <year> 1992. </year>
Reference-contexts: Using this data mapping specification, the compiler must partition the arrays and generate SPMD (Single Program Multiple Data) code which will be executed on each processor. Several variants of data-parallel Fortran that preceded HPF, such as Fortran D [10] and Vienna Fortran <ref> [2] </ref>, also provided ways for the programmer to specify the mapping fl This work was supported in part by ARPA contract DABT63-92-C-0038 and NSF Cooperative Agreement Number CCR-9120008. <p> After terminating the inner loop, the next index visited is 103, which does not belong to processor 1, and therefore we move to the point 139, setting M <ref> [2] </ref> = 15 (lines 45-46).
Reference: [3] <author> S. Chatterjee. </author> <title> Private communication, </title> <month> October </month> <year> 1994. </year>
Reference-contexts: The process is con tinued until we reach the first point of the next cycle, index 301, and at the end, M = <ref> [3, 12, 15, 12, 3, 12, 3, 12] </ref>. k = 8, l = 4, s = 9, and m = 1. 5.1 Complexity The running time of the extended Euclid's algorithm is O (log min (s; pk)) [5]. <p> In order to perform a correct comparison with the algorithm by Chatterjee et al., we modified the code provided to us by S. Chatterjee <ref> [3] </ref> so that the segments common to both methods (lines 3-11 in Figure 5) were coded identically. Moreover, since their method requires sorting of the initial cycle of memory accesses, we tried to use the most efficient sorting routines available to us, so as not to obtain an unfair advantage.
Reference: [4] <author> S. Chatterjee, J. Gilbert, F. Long, R. Schreiber, and S. Teng. </author> <title> Generating local addresses and communication sets for data-parallel programs. </title> <booktitle> In Proceedings of the Fourth ACM SIG-PLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: This problem and its solution were first described by Chatterjee et al. <ref> [4] </ref>. They give an algorithm for solving the memory address problem in O (k log k + min (log s; log p)) time, and show that any algorithm for this problem takes (k) time. <p> with lower bound l = 0 and stride s = 9. 2 Problem statement Given an array A distributed across p processors using a cyclic (k) distribution, we can visualize the layout of its elements in local processor memories as a two-dimensional matrix, with each row divided into p blocks <ref> [4] </ref>. The location of array element A (i) is determined by the processor holding A (i), the block within this processor containing A (i), and the offset of A (i) within the block. <p> Chatterjee et al. visualize the table containing the offset and memory gap sequences as the transition diagram of a finite state machine <ref> [4] </ref>. State transitions depend only on p, k, and s, whereas a processor's start state in the transition table also depends on the lower bound of the array section l and that processor's number m. <p> As mentioned in Section 2, we use the approach described by Chatterjee et al. <ref> [4] </ref> to find the starting location for a given processor m. The extended Euclid's algorithm (line 3) computes d = gcd (s; pk), and x and y, such that s x + pk y = d. <p> We now describe our implementation experience, which shows that our algorithm is more efficient in practice than the method developed by Chatterjee et al. <ref> [4] </ref>. <p> The C code fragments correspond to the simple array assignment statement A (l : u : s) = 100:0. The code in Figure 8 (a) is identical to that proposed by Chatterjee et al. <ref> [4] </ref>. Expensive mod operations can be replaced by simple tests, as in Figure 8 (b). 2 A slight modification of the same idea is shown in Figure 8 (c). <p> In order to index M table using the local offsets and to compute NextOffset table used in be changed as follows: 2 S. Chatterjee pointed out that the code template from Figure 8 (a) was given in reference <ref> [4] </ref> only for conceptual reasons. The implementation actually used the code from Figure 8 (b). <p> The details of this technique, which eliminates memory overhead with only a small penalty in the execution time, are described in our related work [12]. 7 Related work The work by Chatterjee et al. <ref> [4] </ref>, which has been extensively cited throughout this paper, describes a method for enumeration of local regular section indices in increasing order.
Reference: [5] <author> T.H. Cormen, C.E. Leiserson, and R.L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: point of the next cycle, index 301, and at the end, M = [3, 12, 15, 12, 3, 12, 3, 12]. k = 8, l = 4, s = 9, and m = 1. 5.1 Complexity The running time of the extended Euclid's algorithm is O (log min (s; pk)) <ref> [5] </ref>. The loops in lines 4-11 and 19-26 of lines 34-49 does only O (k) work; in fact, we show that, in the worst case, at most 2k + 1 points are examined.
Reference: [6] <author> J. Dongarra, R. van de Geijn, and D. Walker. </author> <title> A look at scalable dense linear algebra libraries. </title> <booktitle> In Proceedings of the 1992 Scalable High Performance Computing Conference, </booktitle> <pages> pages 372-379, </pages> <address> Williamsburg, VA, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: Nevertheless, Dongarra, van de Geijn, and Walker emphasize the importance of this, in their terms "block scattered," distribution in the design of scalable libraries for dense linear algebra computations <ref> [6] </ref>. An array A distributed with a cyclic (k) distribution is effectively split into p subarrays, each being local to one processor.
Reference: [7] <author> S.K.S. Gupta, S.D. Kaushik, C.-H. Huang, and P. Sadayap-pan. </author> <title> On compiling array expressions for efficient execution on distributed-memory machines. </title> <type> Technical Report OSE-CISRC-4/94-TR19, </type> <institution> Department of Computer and Information Science, The Ohio State University, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: Gupta et al. address the problem of array statements (A (l a : u a : s a ) = f (B (l b : u b : s b ))) involving cyclic (k) distributions using the virtual processor approach <ref> [7] </ref>. In their virtual-cyclic scheme, only array elements that have the same offset are accessed in increasing order, while the order of accesses for elements with different offsets is determined by the values of the offsets, and not by the array indices.
Reference: [8] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran language specification. </title> <booktitle> Scientific Programming, </booktitle> <address> 2(1-2):1-170, </address> <year> 1993. </year>
Reference-contexts: 1 Introduction High Performance Fortran (HPF) <ref> [8, 14] </ref> incorporates a set of Fortran extensions for portable data-parallel programming on distributed-memory machines. The most important of these extensions are the align and distribute directives, which are used to describe how data should be distributed across processors in a parallel computer.
Reference: [9] <author> S. Hiranandani, K. Kennedy, J. Mellor-Crummey, and A. Sethi. </author> <title> Compilation techniques for block-cyclic distributions. </title> <booktitle> In Proceedings of the 1994 ACM International Conference on Supercomputing, </booktitle> <address> Manchester, England, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: Hiranandani et al. present an algorithm that works in O (k) time, but only if some special conditions are satisfied (s mod pk &lt; k) <ref> [9] </ref>. In this paper, we describe an improved algorithm that computes the memory address sequence for the general case in O (k+min (log s; log p)) time. <p> Hiranandani et al. present a linear-time algorithm for computing the memory gap table when s mod pk &lt; k <ref> [9] </ref>; the simplicity of the resulting access pattern allows them to base = startmem; i = 0; while (base &lt;= lastmem) f *base = 100.0; base += deltaM [i]; i = (i+1) % length; g base = startmem; i = 0; while (base &lt;= lastmem) f *base = 100.0; base +=
Reference: [10] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Using this data mapping specification, the compiler must partition the arrays and generate SPMD (Single Program Multiple Data) code which will be executed on each processor. Several variants of data-parallel Fortran that preceded HPF, such as Fortran D <ref> [10] </ref> and Vienna Fortran [2], also provided ways for the programmer to specify the mapping fl This work was supported in part by ARPA contract DABT63-92-C-0038 and NSF Cooperative Agreement Number CCR-9120008.
Reference: [11] <author> R. Kannan. </author> <title> Algorithmic geometry of numbers. </title> <editor> In J. Traub, editor, </editor> <booktitle> Annual Review of Computer Science. </booktitle> <publisher> Annual Reviews Inc., </publisher> <address> Palo Alto, CA, </address> <year> 1987. </year>
Reference-contexts: Theorem 1 Set fl = f (b; a) 2 Z 2 j pk a + b = i s; i 2 Zg is an integer lattice 1 . Proof: Every discrete subset of R n closed under subtraction is a lattice <ref> [11] </ref>. Let (b 1 ; a 1 ) : pk a 1 + b 1 = i 1 s and (b 2 ; a 2 ) : pk a 2 + b 2 = i 2 s (i 1 ; i 2 2 Z) be two arbitrary points in fl.
Reference: [12] <author> K. Kennedy, N. Nedeljkovic, and A. Sethi. </author> <title> Efficient address generation for block-cyclic distributions. </title> <booktitle> In Proceedings of the 1995 ACM International Conference on Supercomputing, </booktitle> <address> Barcelona, Spain, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Consequently, we describe our algorithm only for one-dimensional array sections. Extensions necessary to handle coupled subscripts and subscripts containing multiple index variables are described in our related work <ref> [12] </ref>. 3 Integer lattice Our approach is based on treating each array element as a point in R 2 space with the origin corresponding to the array element with index 0, positive y-axis in the direction of increasing row numbers, and positive x-axis in the direction of increasing offsets. <p> The process is con tinued until we reach the first point of the next cycle, index 301, and at the end, M = <ref> [3, 12, 15, 12, 3, 12, 3, 12] </ref>. k = 8, l = 4, s = 9, and m = 1. 5.1 Complexity The running time of the extended Euclid's algorithm is O (log min (s; pk)) [5]. <p> The details of this technique, which eliminates memory overhead with only a small penalty in the execution time, are described in our related work <ref> [12] </ref>. 7 Related work The work by Chatterjee et al. [4], which has been extensively cited throughout this paper, describes a method for enumeration of local regular section indices in increasing order.
Reference: [13] <author> A. Knies, M. O'Keefe, and T. MacDonald. </author> <title> High Performance Fortran: A practical analysis. </title> <journal> Scientific Programming, </journal> <volume> 3(3) </volume> <pages> 187-199, </pages> <month> Fall </month> <year> 1994. </year>
Reference-contexts: Although this code requires two table lookups per array access, its simple structure makes it more efficient than the others, especially for smaller values of k. Knies, O'Keefe, and MacDonald point out that the address generation scheme based on table lookup makes a time versus space tradeoff <ref> [13] </ref>. This is particularly true for the code in Figure 8 (d), which while being the fastest, requires two tables to be stored.
Reference: [14] <author> C. Koelbel, D. Loveman, R. Schreiber, G. Steele, Jr., and M. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: 1 Introduction High Performance Fortran (HPF) <ref> [8, 14] </ref> incorporates a set of Fortran extensions for portable data-parallel programming on distributed-memory machines. The most important of these extensions are the align and distribute directives, which are used to describe how data should be distributed across processors in a parallel computer.
Reference: [15] <author> C. van Reeuwijk, H. J. Sips, W. Denissen, and E. M. Paal-vast. </author> <title> Implementing HPF distributed arrays on a mesage-passing parallel computer. </title> <type> Technical report, </type> <institution> Advanced School of Computing and Imaging, Delft University of Technology, </institution> <month> February </month> <year> 1995. </year>
Reference-contexts: The process is con tinued until we reach the first point of the next cycle, index 301, and at the end, M = <ref> [3, 12, 15, 12, 3, 12, 3, 12] </ref>. k = 8, l = 4, s = 9, and m = 1. 5.1 Complexity The running time of the extended Euclid's algorithm is O (log min (s; pk)) [5]. <p> They use intersections of array slices to generate communication for array statements, but support only a restricted class of alignments. Reeuwijk et al. present methods for rowwise and column-wise scanning of arrays in forall loops <ref> [15] </ref>. The two orders of traversal are derived from different decompositions of the position equation, which specifies the relation between array indices and alignment and distribution parameters.
Reference: [16] <author> J. Stichnoth, D. O'Hallaron, and T. Gross. </author> <title> Generating communication for array statements: Design, implementation, and evaluation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 21(1) </volume> <pages> 150-159, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: The method described by Stichnoth, O'Hallaron, and Gross <ref> [16] </ref> is similar to the virtual-cyclic scheme mentioned above. They use intersections of array slices to generate communication for array statements, but support only a restricted class of alignments. Reeuwijk et al. present methods for rowwise and column-wise scanning of arrays in forall loops [15].
References-found: 16

