URL: ftp://ftp.cs.toronto.edu/pub/parallel/Unrau_etal_OSDI94.ps.Z
Refering-URL: http://www.eecg.toronto.edu/~okrieg/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: Email: unrau@eecg.toronto.edu  
Title: Operating System Design and Implementation 1994. Experiences with Locking in a NUMA Multiprocessor Operating System
Author: Ronald C. Unrau Orran Krieger Benjamin Gamsa Michael Stumm 
Note: a  
Address: Toronto  
Affiliation: Department of Electrical and Computer Engineering Department of Computer Science University of  
Abstract: We describe the locking architecture of a new operating system, HURRICANE, designed for large scale shared-memory multiprocessors. Many papers already describe kernel locking techniques, and some of the techniques we use have been previously described by others. However, our work is novel in the particular combination of techniques used, as well as several of the individual techniques themselves. Moreover, it is the way the techniques work together that is the source of our performance advantages and scalability. Briefly, we use: 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> David L. Black, Avadis Tevanian Jr. , David B. Golub, and Michael W. Young. </author> <title> Locking and reference counting in the mach kernel. </title> <booktitle> In Proc. 1991 ICPP, volume II, Software, pages II-167-II-173, </booktitle> <address> Boca Raton, FL, August 1991. </address> <publisher> CRC Press. </publisher>
Reference-contexts: If memory was arbitrarily recycled, then it would be necessary to either (i) periodically check, while spinning, that the data structure is still the one being sought, or (ii) use reference counts <ref> [1] </ref> to ensure that objects are not deallocated while a process is spinning on them. processors in the cluster may access the cluster-local hash table.
Reference: [2] <author> H.H.Y. Chang and B. Rosenburg. </author> <title> Experience porting mach to the RP3 large-scale shared-memory multiprocessor. </title> <booktitle> Future Generation Computer Systems, </booktitle> <address> 7(2-3):259-267, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: One way to address this problem is to use an adaptive technique, where the locks switch between spin and distributed locks, depending on the amount of contention observed <ref> [2, 15] </ref>. We instead found that two simple modifications to the original distributed locking algorithm could improve the uncontended latency to make it competitive with that of simple spin locks (on our system), while preserving the advantages of distributed locks in the contended case.
Reference: [3] <author> E. Chaves, P.C Das, T. J. LeBlanc, B. D. Marsh, and M. L. Scott. </author> <title> Kernel-kernel communication in a shared memory multiprocessor. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 5(3) </volume> <pages> 171-191, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: From the point of view of locking, hierarchical clustering provides two major benefits. First, it bounds contention for both coarse-grained locks and reserve bits, since RPCs are used instead of shared memory to access remote entries (Chaves et al discuss some of the trade-offs between using RPC and shared-memory <ref> [3] </ref>). Second, it increases lock bandwidth by i) instantiating per-cluster system data structures (such as the hash table in Figure 2), each with its own coarse-grained lock, and by ii) replicating read-shared objects (such as X in Figure 2), each with its own reserve bit.
Reference: [4] <author> J. Bradley Chen and Brian N. Bershad. </author> <title> The impact of operating system structure on memory system performance. </title> <booktitle> In Proc. 14th ACM SOSP, </booktitle> <pages> pages 120-133, </pages> <year> 1993. </year>
Reference-contexts: Running uncached eliminates cache-line-based false sharing, and with it the cache-line ping-pong effects that often occur when data with different access patterns share cache lines. Also, temporal and spatial locality in current operating systems is often quite poor <ref> [4, 24] </ref>, leading to very low cache hit rates, reducing the benefits of cache-coherence.) Advanced atomic primitives The atomic-swap operation supported by our processor requires two main memory accesses and is hence relatively slow compared to cache-based atomic operations which permit a lock to be acquired without going to memory (provided <p> This multiprocessor will have an order of magnitude faster processors, cache-coherence support in hardware, cache-based LL/SC instructions, and network caches. Our initial design considerations include: * Operating systems have traditionally had poor caching behavior <ref> [4, 24] </ref>. However, we believe this is primarily because caching and multiprocessor cache-coherence 7 An alternative is to use a wait-free approach, but this is generally much more expensive [10]. effects have been largely ignored in the design of op-erating systems.
Reference: [5] <author> Travis S. Craig. </author> <title> Building FIFO and priority-queuing spin locks from atomic swap. </title> <type> Technical Report TR 93-02-02, </type> <institution> University of Washington, </institution> <note> 02 1993. (ftp tr/1993/02/UW-CSE-93-02-02.PS.Z from cs.washington.edu). </note>
Reference-contexts: The queue structures from failed TryLock requests are garbage collected by Release lock operations. This implementation of TryLock is similar to the timeout mechanism for the queueing lock, developed independently by Craig <ref> [5] </ref>. Unfortunately, we found that this second variant of Try-Lock discriminated against RPC operations and favored local operations. In hindsight, we realized that this use of TryLock was fundamentally incompatible with Distributed Locks, since Distributed Locks are inherently fair, while retry-based locking is only probabilistically fair. <p> Distributed Locks are affected by cache-based locks in a number of ways. The trade-off between regular spin locks, our version of MCS Distributed Locks, and newer cache-based queueing locks which are optimized for the contended case <ref> [5, 17] </ref> depends on three primary factors: 1) the degree of sharing of the locks (and thus its hit rate); 2) the amount of steady-state contention expected; and 3) the probability of bursts of very high contention.
Reference: [6] <author> R. P. Draves, B. N. Bershad, R. F. Rashid, and R. W. Dean. </author> <title> Using continuations to implement thread management and communication in operating systems. </title> <booktitle> In Proc. 13th ACM SOSP, </booktitle> <pages> page 122, </pages> <address> Pacific Grove, CA, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: In our case, the design is influenced by the fact that 1) atomic operations are expensive relative to normal memory accesses, 2) swap is the only atomic operation supported, 3) our operating system is an exception-based micro-kernel (as opposed to process-based <ref> [6] </ref>), and 4) many of the kernel data structures are left uncached because our hardware does not support cache-coherence. Nevertheless, we believe that elements of our architecture are relevant to a wide variety of system architectures. <p> In this section we attempt to generalize our results to other hardware and software system architectures. 5.1 Other Operating System Designs Process-based operating systems Because our operating system is exception-based, as opposed to process-based <ref> [6] </ref>, non-blocking locks are the most natural approach to locking, since there is no process in the kernel to block.
Reference: [7] <author> S. Frank, J. Rothnie, and H. Burkhardt. </author> <title> The KSR1: Bridging the gap between shared memory and MPPs. </title> <booktitle> In IEEE Compcon 1993 Digest of Papers, </booktitle> <pages> pages 285-294, </pages> <year> 1993. </year>
Reference-contexts: It is natural, therefore, to question how our results might apply to more modern systems, with faster processors and interconnection networks, cache-coherence or COMA (Cache-Only Memory Access <ref> [7, 9] </ref>) support, and more powerful cache-based atomic primitives. Although such architectural changes will shift the tradeoff points, we believe that many of the techniques described in this paper would still apply to these systems. We address each of these issues in turn.
Reference: [8] <author> B. Gamsa, O. Krieger, and M. Stumm. </author> <title> Optimizing IPC performance for shared-memory multiprocessors. </title> <booktitle> In Proc. 1994 ICPP, </booktitle> <pages> pages 208-211, </pages> <address> Boca Raton, FL, August 1994. </address> <publisher> CRC Press. </publisher>
Reference-contexts: Finally, we conclude in Section 6 with a summary of this paper. 2 Locking Architecture This section describes the locking architecture of the HURRICANE <ref> [8, 13, 25, 26] </ref> operating system. <p> simplicity of the pessimistic approach, and the performance of the optimistic approach. * Finally, we are starting with a more process-oriented kernel, in part to remove some of the complications of clustering and deadlock, and in part because we believe dynamic process creation can be made to be very fast <ref> [8] </ref>. We will be reducing our reliance on spin locks, choosing instead to use either lock-free data structures or spin-then-block locks, depending on the situation.
Reference: [9] <author> Erik Hagersten, Anders Landin, and Seif Haridi. </author> <title> DDM A Cache-Only Memory Architecture. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 44-54, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: It is natural, therefore, to question how our results might apply to more modern systems, with faster processors and interconnection networks, cache-coherence or COMA (Cache-Only Memory Access <ref> [7, 9] </ref>) support, and more powerful cache-based atomic primitives. Although such architectural changes will shift the tradeoff points, we believe that many of the techniques described in this paper would still apply to these systems. We address each of these issues in turn.
Reference: [10] <author> Maurice Herlihy. </author> <title> Wait-free synchronization. </title> <journal> ACM TOPLAS, </journal> <volume> 13(1) </volume> <pages> 124-149, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: Our initial design considerations include: * Operating systems have traditionally had poor caching behavior [4, 24]. However, we believe this is primarily because caching and multiprocessor cache-coherence 7 An alternative is to use a wait-free approach, but this is generally much more expensive <ref> [10] </ref>. effects have been largely ignored in the design of op-erating systems. Today's processor speeds relative to memory speeds make it imperative to seriously consider the caching effects. We believe it is possible to design the data structures and the locking architecture of an operating system to be cache friendly.
Reference: [11] <author> Maurice Herlihy. </author> <title> A methodology for implementing highly concurrent objects. </title> <journal> ACM TOPLAS, </journal> <volume> 15(5) </volume> <pages> 745-770, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Because only a single word (or double word) can be updated atomically, modifications often become more complex: either an entire data structure is copied, changes made to the copy, and a pointer to the copy atomically swapped in (provided the previous pointer still points to the original copy <ref> [11] </ref>); or the changes can be performed as a series of atomic operations on single words, but only if each change leaves the full data structures in a valid, consistent state [18].
Reference: [12] <author> J. Kent Peacock, S. Saxena, D. Thomas, F. Yang, and W. Yu. </author> <title> Experiences from multithreading system V release 4. </title> <booktitle> In SEDMS III, </booktitle> <pages> pages 77-91. </pages> <publisher> Usenix Assoc, </publisher> <month> March </month> <year> 1992. </year>
Reference-contexts: When the bit is released, the waiting processors re-acquire the coarse-grained lock and search again. 2 1 Our hybrid locking approach is in some ways similar to the locking strategy used by Peacock, et al, for locking cache elements in a multiprocessor version of System V Release 4 <ref> [12, 21] </ref>. 2 Currently in our kernel, memory used for an object is always reused for objects of the same type. Hence, there is no danger that a process could cluster has a separate instantiation of the hash table and the locks that protect it.
Reference: [13] <author> Orran Krieger. </author> <title> HFS: A flexible file system for shared memory multiprocessors. </title> <type> PhD thesis, </type> <institution> Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada, </institution> <year> 1994. </year>
Reference-contexts: Finally, we conclude in Section 6 with a summary of this paper. 2 Locking Architecture This section describes the locking architecture of the HURRICANE <ref> [8, 13, 25, 26] </ref> operating system. <p> Monolithic operating systems Another question concerns how our locking strategy might apply to monolithic operating systems. We have applied the techniques described in this paper to several of our system servers, in particular the file system <ref> [13] </ref>, and have found the benefits of reduced latency and increased concurrency that stem from the use of both hierarchical clustering and our hybrid locks apply.
Reference: [14] <author> Orran Krieger, Michael Stumm, and Ronald Un-rau. </author> <title> The Alloc Stream Facility: A redesign of application-level stream I/O. </title> <type> Technical Report CSRI-275, </type> <institution> Computer Systems Research Institute, University of Toronto, Toronto, Canada, M5S 1A1, </institution> <month> October </month> <year> 1992. </year>
Reference-contexts: Non-concurrent requests: The only important goal for this workload is to minimize latency. With our hybrid locking strategy, many kernel requests require only a single atomic operation. Hence, HURRICANE is able to achieve uncontended response times comparable to uniprocessor systems <ref> [14] </ref>. Concurrent independent requests: The important goal for this workload is to maximize concurrency, so an optimal strategy would be to use fine grain locks for these requests.
Reference: [15] <author> Beng-Hong Lim and Anant Agarwal. </author> <title> Reactive synchronization algorithms for multiprocessors. </title> <booktitle> In ASPLOS-VI, </booktitle> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: have learned from using Distributed Locks. 3.1 Latency in the uncontended case The high uncontended latency of Distributed Locks relative to spin locks was originally a concern to us, since other researchers had found that it could be as much as twice as high as that of simple spin locks <ref> [15] </ref>. One way to address this problem is to use an adaptive technique, where the locks switch between spin and distributed locks, depending on the amount of contention observed [2, 15]. <p> One way to address this problem is to use an adaptive technique, where the locks switch between spin and distributed locks, depending on the amount of contention observed <ref> [2, 15] </ref>. We instead found that two simple modifications to the original distributed locking algorithm could improve the uncontended latency to make it competitive with that of simple spin locks (on our system), while preserving the advantages of distributed locks in the contended case.
Reference: [16] <author> Susan LoVerso, Noemi Paciorek, Alan Langerman, and George Feinberg. </author> <title> The OSF/1 UNIX filesystem (UFS). </title> <booktitle> In USENIX Conference Proceedings, </booktitle> <pages> pages 207-218, </pages> <address> Dallas, TX, </address> <month> January 21-25 </month> <year> 1991. </year> <booktitle> USENIX. </booktitle>
Reference-contexts: will no longer apply to our new system, their ability to reduce the number of critical sections and to simplify atomic operations involving multiple data structures is still valuable. * We are currently investigating alternative deadlock management schemes such as the timestamp based approach used in the OSF/1 UFS implementation <ref> [16] </ref>, to be used in conjunction with hierarchical clustering.
Reference: [17] <author> Peter Magnussen, Anders Landin, and Erik Hagersten. </author> <title> Queue locks on cache coherent multiprocessors. </title> <booktitle> In 8th IPPS, </booktitle> <pages> pages 26-29, </pages> <year> 1994. </year>
Reference-contexts: Distributed Locks are affected by cache-based locks in a number of ways. The trade-off between regular spin locks, our version of MCS Distributed Locks, and newer cache-based queueing locks which are optimized for the contended case <ref> [5, 17] </ref> depends on three primary factors: 1) the degree of sharing of the locks (and thus its hit rate); 2) the amount of steady-state contention expected; and 3) the probability of bursts of very high contention.
Reference: [18] <author> H. Massalin and C. Pu. </author> <title> A lock-free multiprocessor OS kernel. </title> <type> Technical Report CUCS-005-91, </type> <institution> Department of Computer Science, Columbia University, </institution> <month> February </month> <year> 1991. </year>
Reference-contexts: Finally, if high contention is common, the cache-based queueing locks would be the better choice, since their contended-case performance is better than the MCS locks. Finally, LL/SC or CAS instructions, whether cached or not, can be used to implement lock-free operations, which can remove the need for locks entirely <ref> [18] </ref>. Lock-free data structures have a number of benefits, both in terms of performance (by removing the extra space and time cost of locks) and in terms of functionality (they eliminate deadlock), but also have a number of disadvantages. <p> and a pointer to the copy atomically swapped in (provided the previous pointer still points to the original copy [11]); or the changes can be performed as a series of atomic operations on single words, but only if each change leaves the full data structures in a valid, consistent state <ref> [18] </ref>. The first approach can be very expensive if the data structure is large, while the second approach requires finding safe states for each atomic change, which can be difficult and error prone.
Reference: [19] <author> J.M. Mellor-Crummey and M.L. Scott. </author> <title> Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(1) </volume> <pages> 21-65, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: Finally, we make extensive use of Distributed Locks proposed by Mellor-Crummey and Scott <ref> [19] </ref>, in order to reduce second-order effects for those cases where contention cannot be otherwise avoided. We have improved on the basic algorithm and optimized Distributed Locks for use in a kernel environment. <p> Had the family tree been implemented as a separate data structure, it would have been possible to exploit the hierarchy of the tree to enforce a lock ordering that would have allowed us to avoid the RPC retries described above. 3 Using Distributed Locks Distributed Locks <ref> [19] </ref> are used in our system primarily for per-cluster coarse-grained locks, since cross-cluster interactions most often occur through RPCs.
Reference: [20] <author> Noemi Paciorek, Susan Lo Verso, and Alan Langer-man. </author> <title> Debugging multiprocessor operating system kernels. </title> <booktitle> In SEDMS II, </booktitle> <pages> pages 185-202. </pages> <publisher> USENIX, </publisher> <address> Atlanta GA, </address> <month> March 21 - 22 </month> <year> 1991. </year>
Reference-contexts: To avoid the overhead of re-establishing state for the common case, we have implemented an optimistic algorithm that avoids deadlock and is similar to that described by Paciorek <ref> [20] </ref>. Before releasing the local locks, a reserve bit is set in any structure that might be needed after completing the call. The reserve bit may act as an exclusive lock or a reader-writer lock, depending on the data it protects.
Reference: [21] <author> J. Kent Peacock. </author> <title> File system multithreading in system V release 4 MP. </title> <booktitle> In USENIX Conference Proceedings, </booktitle> <pages> pages 19-30, </pages> <address> San Antonio, TX, </address> <month> Summer </month> <year> 1992. </year> <booktitle> USENIX. </booktitle>
Reference-contexts: When the bit is released, the waiting processors re-acquire the coarse-grained lock and search again. 2 1 Our hybrid locking approach is in some ways similar to the locking strategy used by Peacock, et al, for locking cache elements in a multiprocessor version of System V Release 4 <ref> [12, 21] </ref>. 2 Currently in our kernel, memory used for an object is always reused for objects of the same type. Hence, there is no danger that a process could cluster has a separate instantiation of the hash table and the locks that protect it.
Reference: [22] <author> Abraham Silberschatz, James L. Peterson, and Peter Galvin. </author> <title> Operating Systems Concepts. </title> <publisher> Addison-Wesley, </publisher> <address> third edition edition, </address> <year> 1991. </year>
Reference-contexts: Although our algorithm has been presented in the context of a clustered system, it is important to note that the same protocol could be applied to any system that requires multiple locks simultaneously. Also, we chose not to use the more common global ordering approach <ref> [22] </ref> within a particular class of locks, because the only ordering that makes sense in a clustered system is by cluster number.
Reference: [23] <author> Daniel Stodolsky, J. Bradley Chen, and Brian N. Ber-shad. </author> <title> Fast interrupt priority management in operating system kernels. </title> <booktitle> In USENIX Microkernels Workshop. USENIX, </booktitle> <year> 1993. </year>
Reference-contexts: Unfortunately, our hardware only provides the ability to enable and disable all interrupts, and for a number of reasons the HURRICANE kernel always runs with interrupts on. We therefore adapted a strategy first suggested by Stodolsky et al <ref> [23] </ref>. Inter-processor interrupts are treated as a separate interrupt class that can be logically masked. A per-processor flag is set whenever a lock is about to be acquired that could cause deadlock with an interrupt handler.
Reference: [24] <author> Josep Torrellas, Anoop Gupta, and John L. Hennessy. </author> <title> Characterizing the caching and synchronization performance of a multiprocessor operating system. </title> <booktitle> In ASPLOS-IV Proceedings, </booktitle> <pages> pages 162-174, </pages> <address> Boston, Massachusetts, </address> <year> 1992. </year>
Reference-contexts: Running uncached eliminates cache-line-based false sharing, and with it the cache-line ping-pong effects that often occur when data with different access patterns share cache lines. Also, temporal and spatial locality in current operating systems is often quite poor <ref> [4, 24] </ref>, leading to very low cache hit rates, reducing the benefits of cache-coherence.) Advanced atomic primitives The atomic-swap operation supported by our processor requires two main memory accesses and is hence relatively slow compared to cache-based atomic operations which permit a lock to be acquired without going to memory (provided <p> This multiprocessor will have an order of magnitude faster processors, cache-coherence support in hardware, cache-based LL/SC instructions, and network caches. Our initial design considerations include: * Operating systems have traditionally had poor caching behavior <ref> [4, 24] </ref>. However, we believe this is primarily because caching and multiprocessor cache-coherence 7 An alternative is to use a wait-free approach, but this is generally much more expensive [10]. effects have been largely ignored in the design of op-erating systems.
Reference: [25] <author> R. Unrau, M. Stumm, O. Krieger, and B. Gamsa. </author> <title> Hierarchical clustering: A structure for scalable multiprocessor operating system design. </title> <journal> Journal of Supercomputing. </journal> <note> To appear. Also available as technical report CSRI-268 from ftp.csri.toronto.edu. </note>
Reference-contexts: Finally, we conclude in Section 6 with a summary of this paper. 2 Locking Architecture This section describes the locking architecture of the HURRICANE <ref> [8, 13, 25, 26] </ref> operating system. <p> Briefly, hierarchical clustering is a framework for managing locality in a scalable shared-memory multiprocessor <ref> [25] </ref>. Instead of having one set of system data structures shared by all processors in the system, the processors are grouped into clusters and a complete set of system data structures are instantiated within each cluster. <p> Consider again the clustered hash table of Figure 2. Assume a local copy of element Z is required by cluster 1. A search of the local hash table reveals that it is not present, at which point a data specific location resolution technique <ref> [25] </ref> indicates that the item resides in cluster 2. The kernel on cluster 1 then allocates a local instance of the element, Z 0 , and initiates an RPC call to cluster 2 to retrieve the data. <p> Using synthetic tests instead of real applications has the advantage that it allows us to focus our attention on results of interest to this paper. Application results on our system are presented in <ref> [25] </ref>. The two synthetic tests used are: Independent faults (Figure 6a): p processes repeatedly fault on a per-process private region of local memory. Because the faults are to different physical resources (i.e., different pages) the only lock contention in this experiment is due to unnecessary locking conflicts in the kernel.
Reference: [26] <author> Ronald C. Unrau. </author> <title> Scalable Memory Management through Hierarchical Symmetric Multiprocessing. </title> <type> PhD thesis, </type> <institution> Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: Finally, we conclude in Section 6 with a summary of this paper. 2 Locking Architecture This section describes the locking architecture of the HURRICANE <ref> [8, 13, 25, 26] </ref> operating system. <p> Although hierarchical clustering bounds lock contention and increases lock bandwidth, it does complicate locking protocols <ref> [26] </ref>. For example, a needed data structure may not be present in the local cluster, requiring a remote operation to get it. If the data is replicated, then it must be kept consistent, which also requires remote operations.
Reference: [27] <author> Zvonko G. Vranesic, Michael Stumm, Ron White, and David Lewis. </author> <title> The Hector Multiprocessor. </title> <journal> Computer, </journal> <volume> 24(1), </volume> <month> January </month> <year> 1991. </year>
Reference-contexts: The experiments were run on a fully configured version of HURRICANE with all servers active, but with no other applications running at the time. The operating system was running on a 16 processor HECTOR prototype with 16 MHz MC88100 processors <ref> [27] </ref>. The particular hardware configuration used in our experiments consists of 4 processor-memory modules per station (a shared bus) and 4 stations connected by a ring.
References-found: 27

