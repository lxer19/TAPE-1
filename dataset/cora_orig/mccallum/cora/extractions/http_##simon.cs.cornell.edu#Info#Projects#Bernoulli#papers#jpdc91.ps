URL: http://simon.cs.cornell.edu/Info/Projects/Bernoulli/papers/jpdc91.ps
Refering-URL: 
Root-URL: 
Title: Dependence Flow Graphs: An Algebraic Approach to Program Dependencies  
Author: Keshav Pingali Micah Beck Richard Johnson Mayan Moudgill Paul Stodghill 
Address: Ithaca, NY 14853  
Affiliation: Department of Computer Science, Cornell University,  
Abstract: The topic of intermediate languages for optimizing and parallelizing compilers has received much attention lately. In this paper, we argue that any good representation of a program must have two crucial properties: first, it must be a data structure that can be rapidly traversed to determine dependence information, and second this representation must be a program in its own right, with a parallel, local model of execution. In this paper, we illustrate the importance of these points by examining algorithms for a standard optimization | global constant propagation. We discuss the problems in working with current representations. Then, we propose a novel representation called the dependence flow graph which has each of the properties mentioned above. We show that this representation leads to a simple algorithm, based on abstract interpretation, for solving the constant propagation problem. Our algorithm is simpler than, and as efficient as, the best known algorithms for this problem. An interesting feature of our representation is that it naturally incorporates the best aspects of many other representations, including continuation-passing style, data and program dependence graphs, static single assignment form and dataflow program graphs. 
Abstract-found: 1
Intro-found: 1
Reference: [AA89] <author> Zena Ariola and Arvind. PTAC: </author> <title> A parallel intermediate language. </title> <booktitle> In Proceedings of the Functional Programming Languages and Computer Architecture, </booktitle> <address> London, </address> <month> September </month> <year> 1989. </year>
Reference-contexts: Some representations replace loops with tail-recursive procedures <ref> [AA89, Eka90] </ref>. In our experience, this transformation is not desirable since many important loop transformations, such as loop interchange, have no natural analog in the context of tail-recursive procedures. * The storage model should include an updatable, imperative store.
Reference: [Ack84] <author> W. B. Ackerman. </author> <title> Efficient implementation of applicative languages. </title> <type> Technical Report TR-323, </type> <institution> M.I.T. Laboratory for Computer Science, </institution> <month> April </month> <year> 1984. </year>
Reference-contexts: Some well-known representations are: control flow graphs [ASU86], def-use chains [ASU86], data dependence graphs [Kuc78], program dependence graphs and webs [FOW87, BMO90], program representation graphs [CF89], static single assignment form [CFR + 89], continuation-passing style [Ste78], and program graphs <ref> [Ack84] </ref>. The choice of program representation has a profound effect on the design, asymptotic complexity, and implementation of optimizing and parallelizing transformations. As an analogy, consider Hindu numerals 2 , which are more convenient than Roman numerals for performing arithmetic operations, while representing the same information. <p> Updat-able storage locations can be eliminated in a dependence flow graph through a simple program transformation | in essence, imperative dependencies are converted into functional dependencies [BP90]. In translating the dataflow language VAL into static dataflow graphs, Ackerman defined an intermediate representation called VAL program graphs <ref> [Ack84] </ref>. This representation has become popular in the dataflow world; with minor modifications, it has been used by Traub to translate the dataflow language Id into dynamic dataflow graphs [Tra86].
Reference: [AH82] <author> M. Auslander and M. Hopkins. </author> <title> An overview of the PL.8 compiler. </title> <booktitle> Proceedings of the 1982 SIGPLAN Symposium on Compiler Construction, </booktitle> <volume> 17(6) </volume> <pages> 22-31, </pages> <month> June </month> <year> 1982. </year>
Reference-contexts: Producing a token carrying a value on an arc is similar to storing that value in the corresponding register. Explicit load and store operators to transfer values between the global store and a set of registers/temporaries have been used in the PL.8 compiler <ref> [AH82] </ref> and many Scheme compilers [Ste78]. We develop this point of view in the rest of this section. 3.1 Acyclic Dependence Flow Graphs: Formal Semantics From a formal perspective, a dependence flow graph is a set of declarations followed by a set of definitions.
Reference: [AM87] <author> A. W. Appel and D. B. MacQueen. </author> <title> A Standard ML compiler. </title> <booktitle> Lecture Notes In Computer Science, </booktitle> <month> September </month> <year> 1987. </year>
Reference-contexts: Continuation passing style (CPS) is an executable representation that was proposed by Steele as a suitable Page 10 intermediate language for compiling Scheme [Ste78]. Since then, it has been used in a number of other compilers such as the Standard ML compiler <ref> [AM87] </ref>. Informally, the continuation of an operator is a representation of the effect of the rest of the program after the operator is executed. The execution model of CPS is sequential. Arcs in dependence flow graphs can be viewed as continuations in a parallel execution model.
Reference: [ANP89] <author> Arvind, R. Nikhil, and K. Pingali. I-structures: </author> <title> Data structures for parallel computing. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 11, </volume> <month> October </month> <year> 1989. </year>
Reference-contexts: Page 1 imperative language is phrased naturally in terms of an updatable store. While it is possible to treat the store functionally (as is done in denotational semantics), such treatments are quite clumsy in dealing with data structures, especially arrays <ref> [ANP89] </ref>. * The representation should be compact. A new program representation whose size is asymptotically bigger than that of well-accepted representations (such as def-use chains) is unlikely to gain accep tance. In this paper, we illustrate the importance of these issues by examining a particular optimization | global constant propagation.
Reference: [ASU86] <author> A. V. Aho, R. Sethi, and J. D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1986. </year>
Reference-contexts: 1 Introduction The growing complexity of optimizing and parallelizing compilers has re-focused the attention of the programming languages community on the design of intermediate program representations. Some well-known representations are: control flow graphs <ref> [ASU86] </ref>, def-use chains [ASU86], data dependence graphs [Kuc78], program dependence graphs and webs [FOW87, BMO90], program representation graphs [CF89], static single assignment form [CFR + 89], continuation-passing style [Ste78], and program graphs [Ack84]. <p> 1 Introduction The growing complexity of optimizing and parallelizing compilers has re-focused the attention of the programming languages community on the design of intermediate program representations. Some well-known representations are: control flow graphs <ref> [ASU86] </ref>, def-use chains [ASU86], data dependence graphs [Kuc78], program dependence graphs and webs [FOW87, BMO90], program representation graphs [CF89], static single assignment form [CFR + 89], continuation-passing style [Ste78], and program graphs [Ack84]. <p> We say that a definition of x reaches a use of x if execution of the definition may be followed by execution of the use without intervening execution of any other definition of x. As is standard, this definition assumes that conditional branches may go either way <ref> [ASU86] </ref>. If the right hand side of a definition of x is a constant c, we can sometimes substitute c for a use of x without changing the meaning of the program. <p> In Figure 1 (b), the use of x in the last statement is a possible-paths constant with value 1. Note that this use is not an all-paths constant. A variety of algorithms for constant propagation have been proposed in the literature <ref> [ASU86, Kil73, RL77, WZ84] </ref>. Some of these algorithms are more powerful than others | for example, only the algorithm of Weg-man and Zadeck [WZ84] finds possible-paths constants in a single pass. Repeated application of the less powerful algorithms, combined with dead code elimination, will find all possible-paths constants. <p> Page 3 x := 1 if (x =1) then y := 3 ...y... (a) Source Program (b) Control flow Graph (c) Data Dependence Graph well-known <ref> [ASU86] </ref>. Control flow graphs have a simple sequential semantics based on transforming a global imperative store. A simple algorithm based on abstract interpretation finds possible-paths constants in the control flow graph. At each node, we maintain a vector of values from Lat.
Reference: [Ber66] <author> A. J. Bernstein. </author> <title> Analysis of programs for parallel processing. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 1(5) </volume> <pages> 757-762, </pages> <month> October </month> <year> 1966. </year>
Reference-contexts: From an analysis of the construction, we show two facts. * Dependence flow graphs constructed by our algorithm satisfy Bernstein's conditions: that is, a store operator can never be enabled for execution simultaneously with another store or load operator on the same storage location <ref> [Ber66] </ref>. * The dependence flow graph of a program whose control flow graph has E edges and V variables has size O (EV ). Although token-pushing provides useful intuition, we adopt a different style of operational semantics in the formal development. <p> The proof, which we have omitted for lack of space, rests on the fact that dependence flow graphs, by construction, satisfy Bern-stein's conditions <ref> [Ber66] </ref>. We refer the interested reader to a companion technical report [PBJ + 90]. We can exploit the one-step Church-Rosser property to define a simple interpreter for dependence flow graphs. The interpreter maintains an environment and a store, and keeps a worklist of definitions that may be ready for execution.
Reference: [BMO90] <author> Robert A. Ballance, Arthur B. Maccabe, and Karl J. Ottenstein. </author> <title> The Program Dependence Web: A representation supporting control-, data-, and demand-driven interpretation of imperative languages. </title> <booktitle> Proceedings of the 1990 SIG-PLAN Conference on Programming Language Design and Implementation, </booktitle> <volume> 25(6) </volume> <pages> 257-271, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: 1 Introduction The growing complexity of optimizing and parallelizing compilers has re-focused the attention of the programming languages community on the design of intermediate program representations. Some well-known representations are: control flow graphs [ASU86], def-use chains [ASU86], data dependence graphs [Kuc78], program dependence graphs and webs <ref> [FOW87, BMO90] </ref>, program representation graphs [CF89], static single assignment form [CFR + 89], continuation-passing style [Ste78], and program graphs [Ack84]. The choice of program representation has a profound effect on the design, asymptotic complexity, and implementation of optimizing and parallelizing transformations. <p> In an earlier paper [BP89], we solved this problem completely, and pointed out the advantages of basing intermediate languages on the dependence-driven execution model. While our suggestion has been taken to heart by these researchers <ref> [BMO90] </ref>, it is too early to tell if there will be a convergence of these ongoing efforts. Future Research The ideas presented in this paper form the basis of the Typhoon parallelizing compiler project at Cornell University.
Reference: [BP89] <author> M. Beck and K. Pingali. </author> <title> From control flow to dataflow. </title> <type> Technical Report TR89-1050, </type> <institution> Cornell University, </institution> <month> October </month> <year> 1989. </year>
Reference-contexts: Suhler at IBM, and Ballance, Maccabe and Ottenstein at Los Alamos, have been working on implementing fortran on dataflow machines. In an earlier paper <ref> [BP89] </ref>, we solved this problem completely, and pointed out the advantages of basing intermediate languages on the dependence-driven execution model. While our suggestion has been taken to heart by these researchers [BMO90], it is too early to tell if there will be a convergence of these ongoing efforts.
Reference: [BP90] <author> Micah Beck and Keshav Pingali. </author> <title> From control flow to dataflow. </title> <booktitle> In Proceedings of the 1990 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1990. </year>
Reference-contexts: In a forthcoming paper, we will describe how dependence flow graphs are constructed, starting from the control-flow graph of a program. This construction can handle unstructured control-flow. Some preliminary ideas are presented in an earlier paper <ref> [BP90] </ref>. <p> In our representation, the unique naming of dependencies is fundamental and not a variant used for optimizations. Updat-able storage locations can be eliminated in a dependence flow graph through a simple program transformation | in essence, imperative dependencies are converted into functional dependencies <ref> [BP90] </ref>. In translating the dataflow language VAL into static dataflow graphs, Ackerman defined an intermediate representation called VAL program graphs [Ack84]. This representation has become popular in the dataflow world; with minor modifications, it has been used by Traub to translate the dataflow language Id into dynamic dataflow graphs [Tra86].
Reference: [CC77] <author> P. Cousout and R. Cousout. </author> <title> Abstract Interpretation: A unified lattice model for static analysis of programs by construction of approximations of fixpoints. </title> <booktitle> Proceedings of the 4th ACM Symposium on Principles of Programming Languages, </booktitle> <month> January </month> <year> 1977. </year>
Reference-contexts: That is, it should be a language with a well-defined, compositional operational semantics. This allows abstract interpretation to be employed when designing algorithms, which facilitates systematic algorithm development and proof of correctness <ref> [CC77, CC79] </ref>. * It should be possible to view the representation as a data structure that can be traversed efficiently for data dependence information, as required by many compiler transformations [Kuc78]. * Loops should be represented explicitly. Some representations replace loops with tail-recursive procedures [AA89, Eka90].
Reference: [CC79] <author> P. Cousout and R. Cousout. </author> <title> Systematic design of program analysis frameworks. </title> <booktitle> Proceedings of the 6th ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 269-282, </pages> <month> January </month> <year> 1979. </year>
Reference-contexts: That is, it should be a language with a well-defined, compositional operational semantics. This allows abstract interpretation to be employed when designing algorithms, which facilitates systematic algorithm development and proof of correctness <ref> [CC77, CC79] </ref>. * It should be possible to view the representation as a data structure that can be traversed efficiently for data dependence information, as required by many compiler transformations [Kuc78]. * Loops should be represented explicitly. Some representations replace loops with tail-recursive procedures [AA89, Eka90].
Reference: [CF89] <author> R. Cartwright and M. Felleisen. </author> <title> The semantics of program dependence. </title> <booktitle> Proceedings of the 1989 SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <volume> 25(6), </volume> <month> June </month> <year> 1989. </year>
Reference-contexts: Some well-known representations are: control flow graphs [ASU86], def-use chains [ASU86], data dependence graphs [Kuc78], program dependence graphs and webs [FOW87, BMO90], program representation graphs <ref> [CF89] </ref>, static single assignment form [CFR + 89], continuation-passing style [Ste78], and program graphs [Ack84]. The choice of program representation has a profound effect on the design, asymptotic complexity, and implementation of optimizing and parallelizing transformations. <p> Moreover, they do not have a simple, local execution semantics <ref> [CF89] </ref>. 2.5 Summary The control flow graph allows us to formulate a simple algorithm, based on abstract interpretation, that finds possible-paths constants without the need for program transformations. However, its asymptotic complexity is poor.
Reference: [CFR + 89] <author> Ron Cytron, Jeanne Ferrante, Barry K. Rosen, Mark N. Wegman, and F. Kenneth Zadeck. </author> <title> An efficient method of computing static single assignment form. </title> <booktitle> In Proceedings of the 16th ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 25-35, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: Some well-known representations are: control flow graphs [ASU86], def-use chains [ASU86], data dependence graphs [Kuc78], program dependence graphs and webs [FOW87, BMO90], program representation graphs [CF89], static single assignment form <ref> [CFR + 89] </ref>, continuation-passing style [Ste78], and program graphs [Ack84]. The choice of program representation has a profound effect on the design, asymptotic complexity, and implementation of optimizing and parallelizing transformations. <p> However, in CPS, continuations are first-class citizens in the sense that they can be passed into and out of functions. Static single assignment (SSA) is a compact representation of def-use chains that uses so-called -functions to combine def-use arcs <ref> [CFR + 89] </ref>. -functions are similar to our merge nodes and we get the same compactness advantage in our representation. In addition, unaliased variables are renamed so that each variable is assigned by just one statement.
Reference: [Den74] <author> J. B. Dennis. </author> <title> First version of a data flow procedure language. </title> <booktitle> In Proceedings of the Colloque sur la Programmation, </booktitle> <volume> Vol. 19, </volume> <booktitle> Lecture Notes in Computer Science, </booktitle> <pages> pages 362-376, </pages> <year> 1974. </year>
Reference-contexts: To date, these machines execute only functional languages. The availability of fortran will make these machines acceptable to a much wider group of users. * We propose a provocative view of the future of the dataflow model of computation <ref> [Den74] </ref>. Conventionally, the dataflow model is viewed as a way of organizing parallel architectures for executing functional language programs, but these ideas have not had a major impact on mainline architectures.
Reference: [Eka90] <author> K. Ekanadham. </author> <month> Kudos. </month> <institution> IBM Yorktown Heights, </institution> <year> 1990. </year>
Reference-contexts: Some representations replace loops with tail-recursive procedures <ref> [AA89, Eka90] </ref>. In our experience, this transformation is not desirable since many important loop transformations, such as loop interchange, have no natural analog in the context of tail-recursive procedures. * The storage model should include an updatable, imperative store. <p> At HP Labs, Rau and Schlansker are investigating an intermediate form called PIF (parallel intermediate form) for compiling fortran to VLIW machines. 6 Ekanadham at IBM has an intermediate form called Kudos which is used to translate fortran and functional languages into code for the Empire hybrid dataflow machine <ref> [Eka90] </ref>. Suhler at IBM, and Ballance, Maccabe and Ottenstein at Los Alamos, have been working on implementing fortran on dataflow machines. In an earlier paper [BP89], we solved this problem completely, and pointed out the advantages of basing intermediate languages on the dependence-driven execution model.
Reference: [FOW87] <author> J. Ferrante, K. J. Ottenstein, and J. D. Warren. </author> <title> The program dependency graph and its uses in optimization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(3) </volume> <pages> 319-349, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: 1 Introduction The growing complexity of optimizing and parallelizing compilers has re-focused the attention of the programming languages community on the design of intermediate program representations. Some well-known representations are: control flow graphs [ASU86], def-use chains [ASU86], data dependence graphs [Kuc78], program dependence graphs and webs <ref> [FOW87, BMO90] </ref>, program representation graphs [CF89], static single assignment form [CFR + 89], continuation-passing style [Ste78], and program graphs [Ack84]. The choice of program representation has a profound effect on the design, asymptotic complexity, and implementation of optimizing and parallelizing transformations.
Reference: [Kil73] <author> G. A. Kildall. </author> <title> A unified approach to global program optimization. </title> <booktitle> In Proceedings of the 1st ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 194-206, </pages> <month> October </month> <year> 1973. </year>
Reference-contexts: In Figure 1 (b), the use of x in the last statement is a possible-paths constant with value 1. Note that this use is not an all-paths constant. A variety of algorithms for constant propagation have been proposed in the literature <ref> [ASU86, Kil73, RL77, WZ84] </ref>. Some of these algorithms are more powerful than others | for example, only the algorithm of Weg-man and Zadeck [WZ84] finds possible-paths constants in a single pass. Repeated application of the less powerful algorithms, combined with dead code elimination, will find all possible-paths constants. <p> As we will see, the choice of program representation plays a critical role in this task. It is standard to express constant propagation algorithms in the framework due to Kildall <ref> [Kil73] </ref>. We define a lattice Lat shown in Figure 2, consisting of all the constant values and two distinguished values &gt; and ?. The special constant $ is used only in dependence flow graphs, and plays no part in the algorithms described in this section. <p> Algorithms for constructing the control flow graph representation of a program are 3 Note that the sense of &gt; and ? in the lattice are reversed with respect to the lattice used by previous researchers <ref> [Kil73, RL77, WZ84] </ref>. These researchers viewed constant propagation as an all-paths data flow problem; such problems are traditionally formulated so that the desired solution is the greatest fixed point of a set of equations.
Reference: [Kuc78] <author> D. J. Kuck. </author> <title> The Structure of Computers and Computations, volume 1. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: 1 Introduction The growing complexity of optimizing and parallelizing compilers has re-focused the attention of the programming languages community on the design of intermediate program representations. Some well-known representations are: control flow graphs [ASU86], def-use chains [ASU86], data dependence graphs <ref> [Kuc78] </ref>, program dependence graphs and webs [FOW87, BMO90], program representation graphs [CF89], static single assignment form [CFR + 89], continuation-passing style [Ste78], and program graphs [Ack84]. The choice of program representation has a profound effect on the design, asymptotic complexity, and implementation of optimizing and parallelizing transformations. <p> This allows abstract interpretation to be employed when designing algorithms, which facilitates systematic algorithm development and proof of correctness [CC77, CC79]. * It should be possible to view the representation as a data structure that can be traversed efficiently for data dependence information, as required by many compiler transformations <ref> [Kuc78] </ref>. * Loops should be represented explicitly. Some representations replace loops with tail-recursive procedures [AA89, Eka90]. <p> For compilers that perform wholesale reorganization of programs, a generalization of def-use chains called the data dependence graph <ref> [Kuc78] </ref> is commonly used. The data dependence graph for our example is shown in Figure 3 (c). Edges in the graph represent dependencies that are classified as flow (def-use), anti (use-def), or output (def-def) dependences.
Reference: [Man81] <author> Z. Manna. </author> <title> Mathematical Theory of Computation. </title> <publisher> McGraw-Hill Publishing Company, </publisher> <year> 1981. </year>
Reference-contexts: For any dependence flow graph, we can write down a set of semantic equations over Lat in which the functions on the right hand side are monotonic and continuous. It is a well-known result that such a system of equations has a least solution <ref> [Man81] </ref>. Figure 10 shows these values for the program of Figure 5. The possible-paths constants can be read off from the least solution of the semantic equations as follows. Let C : D ! Lat be the least solution.
Reference: [PBJ + 90] <author> Keshav Pingali, Micah Beck, Richard Johnson, Mayan Moudgill, and Paul Stodghill. </author> <title> Dependence Flow Graphs: An algebraic approach to program dependencies. </title> <type> Technical Report TR 90-1152, </type> <institution> Cornell University, </institution> <year> 1990. </year>
Reference-contexts: The proof, which we have omitted for lack of space, rests on the fact that dependence flow graphs, by construction, satisfy Bern-stein's conditions [Ber66]. We refer the interested reader to a companion technical report <ref> [PBJ + 90] </ref>. We can exploit the one-step Church-Rosser property to define a simple interpreter for dependence flow graphs. The interpreter maintains an environment and a store, and keeps a worklist of definitions that may be ready for execution. <p> This algorithm can be proved correct by a simple induction on the length of the execution sequence. We omit the proof for lack of space and refer the interested reader to the technical report <ref> [PBJ + 90] </ref>. 5 Conclusions An interesting aspect of dependence flow graphs is that they exhibit many features of previously proposed representations. These connections will be discussed in full in another paper | here, we summarize them.
Reference: [Plo81] <author> Gordon D. Plotkin. </author> <title> A structural approach to operational semantics. </title> <type> Technical Report DAIMI FN-19, </type> <institution> Aarhus University, </institution> <year> 1981. </year>
Reference-contexts: A dependence has exactly one source but can have many sinks. We now give a Plotkin-style, formal operational semantics for dependence flow graphs <ref> [Plo81] </ref>. Rather than rewrite programs, as is common in this style of semantics, we will define a state transition semantics in which we rewrite configurations. Informally, a configuration represents the state of the computation and a transition represents a step in the computation.
Reference: [PW86] <author> D. Padua and M. Wolfe. </author> <title> Advanced compiler optimization for supercomputers. </title> <journal> Communications of the ACM, </journal> <pages> pages 1184-1201, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: For any index vector I, the add operator can execute as soon as its operands are available, i.e. as soon as t 1 :I and t 2 :I are defined. 5 This device is like scalar expansion <ref> [PW86] </ref>.
Reference: [RL77] <author> John H. Reif and H. R. Lewis. </author> <title> Symbolic evaluation and the global value graph. </title> <booktitle> In Proceedings of the 14th ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 104-118, </pages> <month> Jan-uary </month> <year> 1977. </year>
Reference-contexts: In Figure 1 (b), the use of x in the last statement is a possible-paths constant with value 1. Note that this use is not an all-paths constant. A variety of algorithms for constant propagation have been proposed in the literature <ref> [ASU86, Kil73, RL77, WZ84] </ref>. Some of these algorithms are more powerful than others | for example, only the algorithm of Weg-man and Zadeck [WZ84] finds possible-paths constants in a single pass. Repeated application of the less powerful algorithms, combined with dead code elimination, will find all possible-paths constants. <p> Algorithms for constructing the control flow graph representation of a program are 3 Note that the sense of &gt; and ? in the lattice are reversed with respect to the lattice used by previous researchers <ref> [Kil73, RL77, WZ84] </ref>. These researchers viewed constant propagation as an all-paths data flow problem; such problems are traditionally formulated so that the desired solution is the greatest fixed point of a set of equations. <p> However, Reif and Lewis have shown that a factored form of def-use chains can be represented in size O (EV ) <ref> [RL77] </ref>. This yields an algorithm which is a factor of V faster than the one which uses the control flow graph. Although this algorithm will find all-paths constants as in Figure 1 (a), it will not find possible-paths constants as in Figure 1 (b).
Reference: [Ste78] <author> G. Steele. RABBIT: </author> <title> A compiler for SCHEME. </title> <type> Technical Report AI memo 474, </type> <institution> M.I.T. Laboratory for Artificial Intelligence, </institution> <month> May </month> <year> 1978. </year>
Reference-contexts: Some well-known representations are: control flow graphs [ASU86], def-use chains [ASU86], data dependence graphs [Kuc78], program dependence graphs and webs [FOW87, BMO90], program representation graphs [CF89], static single assignment form [CFR + 89], continuation-passing style <ref> [Ste78] </ref>, and program graphs [Ack84]. The choice of program representation has a profound effect on the design, asymptotic complexity, and implementation of optimizing and parallelizing transformations. As an analogy, consider Hindu numerals 2 , which are more convenient than Roman numerals for performing arithmetic operations, while representing the same information. <p> Producing a token carrying a value on an arc is similar to storing that value in the corresponding register. Explicit load and store operators to transfer values between the global store and a set of registers/temporaries have been used in the PL.8 compiler [AH82] and many Scheme compilers <ref> [Ste78] </ref>. We develop this point of view in the rest of this section. 3.1 Acyclic Dependence Flow Graphs: Formal Semantics From a formal perspective, a dependence flow graph is a set of declarations followed by a set of definitions. <p> Connections with data and program dependence graphs have already been discussed in Section 2. Continuation passing style (CPS) is an executable representation that was proposed by Steele as a suitable Page 10 intermediate language for compiling Scheme <ref> [Ste78] </ref>. Since then, it has been used in a number of other compilers such as the Standard ML compiler [AM87]. Informally, the continuation of an operator is a representation of the effect of the rest of the program after the operator is executed. The execution model of CPS is sequential.
Reference: [Tra86] <author> K. R. Traub. </author> <title> A compiler for the MIT tagged-token dataflow architecture. </title> <type> Technical report, </type> <institution> M.I.T. Laboratory for Computer Science, Cam-bridge, Massachusetts, </institution> <month> August </month> <year> 1986. </year>
Reference-contexts: In translating the dataflow language VAL into static dataflow graphs, Ackerman defined an intermediate representation called VAL program graphs [Ack84]. This representation has become popular in the dataflow world; with minor modifications, it has been used by Traub to translate the dataflow language Id into dynamic dataflow graphs <ref> [Tra86] </ref>. However, this representation supports only functional languages, and cannot handle imperative updates or arbitrary flow of control as dependence flow graphs can.
Reference: [WZ84] <author> M. N. Wegman and F. K. Zadeck. </author> <title> Constant propagation with conditional branches. </title> <booktitle> In Proceedings of the 11th ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 291-299, </pages> <year> 1984. </year> <note> REFERENCES Page 12 </note>
Reference-contexts: This algorithm is simpler than, and as efficient as, the most powerful algorithms to date, such as the one due to Wegman and Zadeck <ref> [WZ84] </ref>. * For dataflow researchers, we demonstrate a way of implementing imperative languages like fortran on dataflow machines. To date, these machines execute only functional languages. <p> We define a particularly ambitious class of constants, the possible-paths constants, which is discovered by an algorithm due to Wegman and Zadeck <ref> [WZ84] </ref>. We then consider a number of intermediate forms most commonly used for optimization in imperative language compilers. <p> However, if the predicate of a conditional can be determined to be constant, then we can ignore the effect of definitions on the side that is never executed. If we modify the definition of all-paths constants to exclude such definitions, the result is the class of possible-paths constants <ref> [WZ84] </ref>. In Figure 1 (b), the use of x in the last statement is a possible-paths constant with value 1. Note that this use is not an all-paths constant. A variety of algorithms for constant propagation have been proposed in the literature [ASU86, Kil73, RL77, WZ84]. <p> In Figure 1 (b), the use of x in the last statement is a possible-paths constant with value 1. Note that this use is not an all-paths constant. A variety of algorithms for constant propagation have been proposed in the literature <ref> [ASU86, Kil73, RL77, WZ84] </ref>. Some of these algorithms are more powerful than others | for example, only the algorithm of Weg-man and Zadeck [WZ84] finds possible-paths constants in a single pass. Repeated application of the less powerful algorithms, combined with dead code elimination, will find all possible-paths constants. <p> Note that this use is not an all-paths constant. A variety of algorithms for constant propagation have been proposed in the literature [ASU86, Kil73, RL77, WZ84]. Some of these algorithms are more powerful than others | for example, only the algorithm of Weg-man and Zadeck <ref> [WZ84] </ref> finds possible-paths constants in a single pass. Repeated application of the less powerful algorithms, combined with dead code elimination, will find all possible-paths constants. <p> Algorithms for constructing the control flow graph representation of a program are 3 Note that the sense of &gt; and ? in the lattice are reversed with respect to the lattice used by previous researchers <ref> [Kil73, RL77, WZ84] </ref>. These researchers viewed constant propagation as an all-paths data flow problem; such problems are traditionally formulated so that the desired solution is the greatest fixed point of a set of equations. <p> This suggests the development of "hybrid" algorithms that use both data structures. The constant propagation algorithm described next is adapted from that of Wegman and Zadeck <ref> [WZ84] </ref>. To find possible-paths constants while still obtaining the efficiency of def-use chains, Wegman and Zadeck refer back to the control flow graph. To keep propagation of values from bypassing conditionals, a boolean executable flag is added to each statement, and is initially set to false, except for START. <p> Next, we show that this solution can be computed efficiently, thereby developing an algorithm that has the same asymptotic complexity as the algorithm due to Wegman and Zadeck <ref> [WZ84] </ref>. This algorithm can be proved correct by an induction on the length of the computation. 4.1 Equational Characterization of Constants equations from a dependence flow graph representation of a program.
References-found: 27

