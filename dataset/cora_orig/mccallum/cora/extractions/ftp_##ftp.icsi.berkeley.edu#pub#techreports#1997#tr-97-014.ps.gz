URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1997/tr-97-014.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1997.html
Root-URL: http://www.icsi.berkeley.edu
Title: Parallel Optimizations Advanced Constructs and Compiler Optimizations for a Parallel, Object Oriented, Shared Memory Language
Phone: (510) 643-9153 FAX (510) 643-7684  
Author: Claudio Fleiner 
Date: April 1997  
Address: I 1947 Center St. Suite 600 Berkeley, California 94704-1198  
Affiliation: INTERNATIONAL COMPUTER SCIENCE INSTITUTE  
Pubnum: TR-97-014  
Abstract-found: 0
Intro-found: 1
Reference: [ADA 95] <institution> Ada 95 Reference Manual, </institution> <note> version 6.0, ISO/IEC/ANSI 8652:1995. </note>
Reference-contexts: The disjunctive lock statement in pSather replaces the standard synchronization mechanisms found in other languages with a flexible and more secure one. It replaces and combines locks, semaphores, regions, monitors (see [Silberschatz 88] for a description of those constructs), the select statement in Ada <ref> [ADA 95] </ref> and the ALT statement in OCCAM [Barret 92]. It allows the implementation of standard mutual exclusion, reader/writer locks, rendezvous and much more. It also allows the user to implement new synchronization mechanisms by providing an interface to the lock manager 11 . <p> It was inspired by the select statement in Ada <ref> [ADA 95] </ref> and the alt statement in OCCAM [Barret 92], although they are both used to synchronize two threads, while the disjunctive lock can be used for many different tasks.
Reference: [Adve 95] <author> Sarita V. Adve and Kourosh Gharachorloo, </author> <title> Shared Memory Consistency Models: A Tutorial, </title> <note> WRL Research Report 95/7, </note> <institution> Digital Equipment Corporation, </institution> <month> September </month> <year> 1995. </year>
Reference-contexts: Code that works correctly by accident when using the sequential consistency model will probably break under the weak memory model. A more detailed introduction to the memory models described here and several others can be found in <ref> [Adve 95] </ref>. 4.1 Sequential Consistency [Lamport 79] defines sequential consistency as follows: A system is sequentially consistent if the result of execution is the same as if the operations of all the processors were executed in some sequential order, and the operations of each individual processor appear in this sequence in
Reference: [Aho 86] <author> Alfred V. Aho, Ravi Sethi, Jeffrey D. Ullman, </author> <booktitle> Compilers: Principles, Techniques and Tools, </booktitle> <publisher> Addison-Wesley Publishing Company. </publisher>
Reference: [Amarasinghe 93] <author> Saman P. Amarasinghe and Monica S. Lam, </author> <title> Communication Optimization and Code Generation for Distributed Memory Machines, </title> <booktitle> Proceedings of the SIGPLAN 93 Conference on Programming Language Design and Implementation, </booktitle> <pages> pp. 126-138, </pages> <address> Albuquerque, NM, USA, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: A similar problem, namely analyzing array accesses in nested loops, is solved in <ref> [Amarasinghe 93] </ref>. In [Hirandani 94] several different optimization techniques for Fortran D are outlined, among others combining messages, message pipelining (similar to prefetching and postwriting described in chapter 6) and different pipelining optimizations. Scheduling of threads and distributing threads to different processors automatically has been extensively studied.
Reference: [Anderson 93] <author> Jennifer M. Anderson and Monica S. Lam, </author> <title> Global optimizations for Parallelism and Locality on Scalable Parallel Machines, </title> <booktitle> Proceedings of the SIGPLAN 93 Conference on Programming Language Design and Implementation, </booktitle> <pages> pp. 112-125, </pages> <address> Albuquerque, NM, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: While Fortran D [Hirandani 92] allows the user to specify the correct alignment, [Gupta 92] and [Philippsen 93] among others showed that in fact in many cases the compiler is able to do the correct alignment and distribution of the data without user annotations. <ref> [Anderson 93] </ref> describes an algorithm to automatically find computation and data decomposition for dense matrix calculations that optimize parallelism and locality. A similar problem, namely analyzing array accesses in nested loops, is solved in [Amarasinghe 93].
Reference: [Anderson 95] <author> T. Anderson, M. Dahlin, J. Neefe, D. Patterson, D. Roselli and R. </author> <title> Wang , Serverless Network File Systems, </title> <booktitle> 15th Symposium on Operating Systems Principles, ACM Transactions on Computer Systems, </booktitle> <year> 1995. </year>
Reference-contexts: The goal of the NOW group is to create a distributed UNIX called Glunix which will, among others, offer the following features: fast communication and support for parallel programs [Mainwaring 95] Network Ram <ref> [Anderson 95] </ref>. To access a memory page on a remote system is faster than accessing it on the local disc, provided that there is a fast communication system in place. <p> Therefore it is better to use the memory of idle workstations in the LAN as virtual memory rather than a local or remote disk. 8 pSather uses clusters as a synonym for nodes. 12 2. Related Work serverless, distributed file system <ref> [Anderson 95] </ref>, that uses the memory of idle ma chines as cache.
Reference: [Anderson 95b] <author> Thomas E. Anderson, David E. Culler, David A. </author> <title> Patterson and the NOW team, "A case for NOW (Network Of Workstations)", </title> <journal> IEEE Micro, </journal> <volume> 15(1) </volume> <pages> 54-64, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: to two years behind the fastest serial computers. [Krishnamurthy 96] describes several more large computers, namely Thinking Machines CM-5, the Intel Paragon and the Cray T3D and their networks and how they can be used to efficiently implement shared memory. 2.1.1 NOW The goal of the NOW (Network Of Workstations) <ref> [Anderson 95b] </ref> project of the University of California at Berkeley is to build support for using a network of workstation as a building-wide distributed supercomputer, as off-the-shelf workstations offer much better price / performance than dedicated supercomputers. <p> an efficient implementation. [Larus 93] however showed that compiler implemented shared memory has the potential to exploit more effectively the resources in a distributed system. pSather uses the last one described, the Active Message system developed at the University of California at Berkeley in the context of the NOW project <ref> [Anderson 95b] </ref>. 14 2. Related Work 2.2.1 PVM PVM (Parallel Virtual Machine) has over 8 years of history, as outlined in [Geist 94]: The PVM project began in the summer of 1989 at Oak Ridge National Laboratory.
Reference: [Bacon 93] <author> David F. Bacon, Susan L. Graham and Oliver J. Sharp, </author> <title> Compiler Transformations for High-Performance Computing, </title> <type> Technical Report UCB/CSD-93-781, </type> <institution> University of California at Berkeley, </institution> <year> 1993. </year>
Reference-contexts: disjunctive lock statement. 2.4 Optimizations for Parallel Languages An excellent overview of the different optimizations (inlining, common subexpression elimination, loop invariant hoisting, loop unrolling and others) used in todays Fortran and C compilers including several optimizations to parallelize High Performance Fortran [Richardson 96] (vectorization) is available as a technical report <ref> [Bacon 93] </ref>. Therefore this section will only describe some optimizations for parallel languages. For an overview of most of the currently available parallel languages see [Philippsen 95].
Reference: [Barret 92] <author> Geoff Barret, </author> <title> OCCAM 3 reference manual, </title> <type> Inmos, </type> <year> 1992. </year>
Reference-contexts: It replaces and combines locks, semaphores, regions, monitors (see [Silberschatz 88] for a description of those constructs), the select statement in Ada [ADA 95] and the ALT statement in OCCAM <ref> [Barret 92] </ref>. It allows the implementation of standard mutual exclusion, reader/writer locks, rendezvous and much more. It also allows the user to implement new synchronization mechanisms by providing an interface to the lock manager 11 . <p> It was inspired by the select statement in Ada [ADA 95] and the alt statement in OCCAM <ref> [Barret 92] </ref>, although they are both used to synchronize two threads, while the disjunctive lock can be used for many different tasks. The new statement allows a thread to try to lock one of several synchronization objects 16 , and execute some code, depending on the locks it got.
Reference: [Berners-Lee 96] <author> T. Berners-Lee, R. Fielding and H. Frystyk, </author> <title> Hypertext Transfer Protocol HTTP/1.0, Network Working Group, </title> <note> RFC 1945, http://ds.internic.net/rfc/rfc1945.txt , May 1996. </note>
Reference-contexts: Message Passing Systems 13 2.1.3 WEB Computer Several massively parallel programs have been run by using the protocols used by the World Wide Web, namely HTTP <ref> [Berners-Lee 96] </ref> and standard eMail. An example is the factoring of a 130 digit integer published by RSA, described in [Cooperating 96]. A new effort to factorize an even larger number has been proposed at the Supercomputing 95 in San Diego [Bhatt 95].
Reference: [Bernstein 84] <author> P. A. Bernstein and N. Goodman, </author> <title> An Algorithm for Concurrency Control and Recovery in Replicated Distributed Databases, </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 9(4), </volume> <month> September </month> <year> 1987. </year>
Reference-contexts: Distributed Reader/Writer locks have also been proposed, mostly to access shared memory pages in a distributed system that simulates shared memory by copying the pages from one processor to another. One, called Available Copies, was proposed in <ref> [Bernstein 84] </ref> and another one, Weighted Voting in [Gifford 79].
Reference: [Bhatt 95] <author> Sandeep Bhatt, Arjen Lenstra, Marina Chen, James Cowie, Geof-frey Fox and Wojtek Furmanski, </author> <title> Factoring on the WorldWide Computer (WWC), </title> <booktitle> Supercomputing 95, </booktitle> <address> San Diego. Bibliography 153 </address>
Reference-contexts: An example is the factoring of a 130 digit integer published by RSA, described in [Cooperating 96]. A new effort to factorize an even larger number has been proposed at the Supercomputing 95 in San Diego <ref> [Bhatt 95] </ref>. Several new problems arise with this kind of widespread computer system: Network Reliability : as the network is not reliable, connections between different parts of the computer may shut down at any time, and therefore only very loosely coupled algorithms are useful in this environment.
Reference: [Boden 95] <author> Nanette J. Boden, Danny Cohen, Robert E. Felderman, Alan E. Kulawik, Charles L. Seitz, Jakov N. Seizovic and Wen-King Su, </author> <title> Myrinet - A Gigabit-per-Second Local Area Network, </title> <booktitle> in IEEE-Micro, </booktitle> <volume> Vol. 15, No. 1, </volume> <pages> pp. 29-36, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: NOW defines an interface for distributed applications running on network of workstations connected over a high speed network, namely the Myrinet <ref> [Boden 95] </ref>. A Myrinet LAN is composed of point-to-point links that connect hosts and switches as shown in figure 2-1. The one way speed between two Sparc20 workstations connected with a Myrinet Switch is about 136 Mbits/sec (the complete performance data is available online from http://www.myri.com/myrinet/performance ).
Reference: [Carvalho 83] <author> O. Carvalho and G. Roucairol, </author> <title> On mutual exclusion in computer networks, </title> <journal> Communications of the ACM, </journal> <volume> 26, </volume> <pages> pp. 146-147, </pages> <year> 1983. </year>
Reference-contexts: A software protocol has been proposed by [Lamport 78], where a processor that needs the token broadcasts requests and as soon as it received an answer from all processors it implicitly got the token. This protocol was enhanced by [Ricart 81] and <ref> [Carvalho 83] </ref> by reducing the numbers of messages sent. A distributed priority lock algorithm has been developed by [Goscinski 90] and a similar one, although faster and with less overhead, was also proposed in [Johnson 94].
Reference: [Chatterjee 95] <author> Siddhartha Chatterjee, John R. Gilbert, Robert Schreiber and Shang-Hua Teng, </author> <title> Optimal Evaluation of Array Expressions on Massively Parallel Machines, </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 17(1), </volume> <month> January </month> <year> 1995, </year> <pages> pp 123-156. </pages>
Reference-contexts: Another area of research for data parallel languages is the alignment of arrays on massively parallel computers like the MasPar. Several algorithms for the optimal evaluation of array expressions in Fortran 90 are presented in <ref> [Chatterjee 95] </ref>.
Reference: [Chen 92] <author> Tien-Fu Chen and Jean-Loup Baer, </author> <title> Reducing Memory Latency via Non-blocking and Prefetching Caches, </title> <booktitle> ASPLOS-V Proceedings - Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 51-61, </pages> <year> 1992. </year>
Reference-contexts: The compiler algorithm described by [Mowry 92] introduces prefetching calls in dense matrix operations (in [Mowry 94] the algorithm is extended to sparse matrix code) to avoid latency due to cache misses, while <ref> [Chen 92] </ref> describes a more general prefetching algorithm.
Reference: [Cooperating 96] <institution> Cooperating Systems Corporation, </institution> <month> FAFNER: </month> <title> Web Server Support for Factoring RSA130, </title> <type> Technical Report TR049601, </type> <institution> Cooperating Systems, </institution> <address> 12 Hollywood Drive, Chestnut Hill MA 02167-2711, USA. </address>
Reference-contexts: Message Passing Systems 13 2.1.3 WEB Computer Several massively parallel programs have been run by using the protocols used by the World Wide Web, namely HTTP [Berners-Lee 96] and standard eMail. An example is the factoring of a 130 digit integer published by RSA, described in <ref> [Cooperating 96] </ref>. A new effort to factorize an even larger number has been proposed at the Supercomputing 95 in San Diego [Bhatt 95].
Reference: [Culler 93] <author> David E. Culler, Andrea Dusseau, Seth Copen Goldstein, Arvind Krishnamurthy, Steven Lumetta, Thorsten von Eicken, and Kath-erine Yelick, </author> <title> Parallel Programming in Split-C, </title> <booktitle> Proceedings of Supercomputing, </booktitle> <month> November </month> <year> 1993. </year>
Reference-contexts: PVM and MPI (in their current form) would actually need a thread that receives those messages, checks what functions should be executed and then executes those functions. pSather and Split-C <ref> [Culler 93] </ref> (another parallel language that runs on distributed computer systems) have the property that most of their message handlers are few in numbers and very small.
Reference: [Culler 94] <author> David E. Culler, Kim Keeton, Cedric Krumbein, Lok Tin Liu, Alan Mainwaring, Rich Martin, Steve Rodrigues, Kristin Wright, Chad Yoshikawa. </author> <title> "The Generic Active Message Interface Specification", UC Berkeley Network of Workstations (NOW) white paper, </title> <year> 1994. </year>
Reference-contexts: This last requirement ensures that deadlocks because of buffer overruns are avoided. The original Active Message interface described in <ref> [Culler 94] </ref> has been replaced by a new version, which, among other things, supports threads [Mainwaring 95]. pSather still uses the old version, but extended it with thread creation routines as there were no implementations available that supported the new interface. <p> The following active message libraries used for the pSather do not guarantee ordering (note that the generic active message definition <ref> [Culler 94] </ref> does not guarantee the ordering anyway): shared memory active message library by [Fleiner 95], Myrinet Active Message Library, by [Martin 95], Meiko CS-2 Active Message Library, by [Yoshikawa 95]. 4.4. <p> Message Passing: The functions provided by the active message interface <ref> [Culler 94] </ref> are used in the pSather runtime ( am_reply_n () , am_request_n () , am_poll () , am_put () and am_get () ).
Reference: [Cytron 90] <author> Ron Cytron, Jim Lipkis and Edith Schonberg, </author> <title> A Compiler-Assisted Approach to SPMD Execution, </title> <booktitle> Proceedings of Supercomputing 90, </booktitle> <pages> pp. 398-406, </pages> <address> New York, NY, </address> <month> November </month> <year> 1990. </year>
Reference-contexts: Several synchronization optimizations have been implemented for data parallel languages. [Philippsen 95b] removes synchronizations in FORTRANs FORALLs since many loops do not need all the synchronization required by the language and [Tseng 95] presents algorithms to remove normal barrier synchronizations. A similar problem is described in <ref> [Cytron 90] </ref>, where programs using the doall (similar to the FORALL ) are converted to execute in the SPMD model using dopar/endpar and serbegin/serend . The author argues that by eliminating the forks used in normal FORALL and reusing threads the program executes faster.
Reference: [Diderich 95] <author> Claude G. Diderich, and Marc Gengler, </author> <title> Some Strategies for Load Balancing Parallel, Algorithms for Irregular Problems: State of the Art, (IRREGULAR '94), Chapter 16, </title> <editor> (A. Ferreira and J. D. P. Rolim, </editor> <booktitle> eds.), </booktitle> <pages> pp. 323-338, </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1995. </year>
Reference-contexts: In many cases this done by using libraries, not by changing the compiler or the runtime system. [Stoffel 94] for example tried successfully to use fuzzy logic and a neuronal network to achieve load balancing on a MIMD System. <ref> [Diderich 95] </ref> describes two algorithms for dynamic load balancing after 18 2. Related Work showing that for many problems it is not possible to make statically decisions regarding load balancing. One algorithm runs locally on each node, while the other makes global decisions.
Reference: [Dongara 93] <author> Jack Dongara, Rolf Hempel, Anthony J. G. Hey and David W. Walker, </author> <title> A Proposal for a User-Level, Message Passing Interface in a Distributed Memory Environment, </title> <institution> ORNL/TM-12231, Oak Ridge National Laboratory, Tennessee, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: Like PVM, it is a standard used on many different systems to exchange messages among different nodes. A preliminary report was available in November 1992 and a revised version in February 1993 <ref> [Dongara 93] </ref>. It had only point-to-point communication, but no collective communication. Also, it was, 2.2. Message Passing Systems 15 like PVM, not threadsafe. Later versions of MPI however, were threadsafe, a major advantage over PVM.
Reference: [Dubois 90] <author> Michel Dubois and Christoph Scheurich, </author> <title> Memory Access Dependencies in Shared-Memory Multiprocessors, </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 16(6), </volume> <pages> pp. 660-673, </pages> <month> June </month> <year> 1990. </year> <note> 154 Bibliography </note>
Reference-contexts: Only at some memory synchronization points there will be an exchange of the global memory information. A model that supports this is the weak memory model described here and the release consistency model described below. The weak memory model introduced by <ref> [Dubois 90] </ref> defines that memory writes are observed by other threads at latest at synchronization points as shown in figure 4-6.
Reference: [Feldman 91] <author> Jerome A. Feldman, Chu-Cheow Lim and F Mazzanti, </author> <title> pSather monitors: Design, tutorial, rationale and implementation, </title> <type> Technical Report, </type> <institution> TR-91-031, International Computer Science Institute, Berkeley, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: The first pSather compiler was implemented by Chu-Cheow Lim on the Sequent Symmetry, workstations and the CM-5 [Lim 93]. This version of pSather included among others the lock statement (but not the disjunctive lock statement), typed and untyped gates (first introduced as monitors in <ref> [Feldman 91] </ref>) as builtin classes. Sather 1.0 was a major language change that introduced several new features (among others iterators, bound routines, separation of type and code inclusion, exception and a new library design). It was released in the Summer of 1994.
Reference: [Feldman 94] <author> Jerome A. Feldman, </author> <title> Universal High Performance Computing we have just begun, </title> <editor> in Uzi Vishkin, editor, </editor> <booktitle> Developing a Computer Science Agenda for High-Performance Computing, </booktitle> <pages> pp. 26-29, </pages> <publisher> ACM Press, </publisher> <year> 1994. </year>
Reference: [Feldman 96] <author> Jerome Feldman, </author> <title> A Language Manual For pSather 1.1, </title> <note> available at http://www.icsi.berkeley.edu , August 1996. </note>
Reference-contexts: The example uses a GATE and GATE-T as an attach object. Gates were already available in earlier versions of pSather as builtin special objects, in the current version they are just ordinary library classes. For more information about GATEs check out <ref> [Feldman 96] </ref>. - create several threads, do some work and wait until the threads end g:GATE:=#; - create a non parameterized gate g :- some_function_1; g :- some_function_2; - do some work lock g.no_threads then end; - wait until the threads end; g2:GATE-INT-:=#; - create a parameterized gate g2 :- search_function_1;
Reference: [Ferrari 94] <author> Adam Ferrari, TPVM, </author> <title> A Thread-Based Interface and Subsystem for PVM, </title> <type> Technical Report, </type> <institution> CSTR-940802, Emory University, </institution> <address> At-lanta, GA, USA, </address> <year> 1994. </year>
Reference-contexts: PVM does not support active messages and is not thread safe, as it uses global buffers to create a message, and then the same buffer to send it out. However, three PVM versions that support threads are available, namely PT-PVM by [Krone 95], TPVM, see <ref> [Ferrari 94] </ref> and [Ferrari 95] and LPVM described in [Zhou 95]. PT-PVM lets several thread run inside a PVM process. Local threads communicate by using shared memory.
Reference: [Ferrari 95] <author> Adam Ferrari, </author> <title> Mutliparadigm Distributed Computing with TPVM, </title> <type> Technical Report, </type> <institution> CSTR-951201, Emory University, </institution> <address> Atlanta, GA, USA, </address> <year> 1995. </year>
Reference-contexts: PVM does not support active messages and is not thread safe, as it uses global buffers to create a message, and then the same buffer to send it out. However, three PVM versions that support threads are available, namely PT-PVM by [Krone 95], TPVM, see [Ferrari 94] and <ref> [Ferrari 95] </ref> and LPVM described in [Zhou 95]. PT-PVM lets several thread run inside a PVM process. Local threads communicate by using shared memory.
Reference: [Fleiner 95] <author> Claudio Fleiner, </author> <title> Active Message Implementation for Shared Memory Machines, </title> <publisher> ftp://ftp.icsi.berkeley.edu/ pub/sather/am-0.07.tar.gz </publisher>
Reference-contexts: The following active message libraries used for the pSather do not guarantee ordering (note that the generic active message definition [Culler 94] does not guarantee the ordering anyway): shared memory active message library by <ref> [Fleiner 95] </ref>, Myrinet Active Message Library, by [Martin 95], Meiko CS-2 Active Message Library, by [Yoshikawa 95]. 4.4.
Reference: [Fleiner 96] <author> Claudio Fleiner, </author> <title> Killing Threads Considered Dangerous, </title> <booktitle> Proceedings of the 1996 Parallel ObjectOriented Methods and Applications Conference, </booktitle> <address> Santa Fe, NM, USA, </address> <month> February </month> <year> 1996. </year>
Reference-contexts: Any function that that reads more than one attribute cannot be a simple status check, unless one of the attributes is a constant that was set during the creation of the object, like the size of an array. We implement the protocol with a reader/writer lock (see <ref> [Fleiner 96] </ref> for a more detailed description of the visitor/mutator protocol). While designing a library, one has to identify the different visitors and mutators and add the corresponding lock statements. <p> any time (see figure 3-10 for an example). protect f:=#INPUT_FILE ("passwd"); when TERMINATION then - we don't know if the file is open or not, or if f is a valid file variable. end ; A more detailed description of the problems with the termination exception can be found in <ref> [Fleiner 96] </ref>. We tried several different designs and implementations of the termination of threads, and finally removed this feature from the language specification of pSather 1.0.
Reference: [Fleiner 96b] <author> Claudio Fleiner, </author> <title> Impact of the Different Serial Optimizations of the Sather 1.1 Compiler, </title> <address> http://www.icsi.berkeley.edu/~fleiner/benchmarks </address> . 
Reference: [Fox 87] <author> M. Fox and J. Ywoskus, </author> <title> Local Area VAXcluster Systems, </title> <journal> Digital Technical Journal, </journal> <month> September </month> <year> 1987. </year>
Reference-contexts: All those algorithm suffer from the same problems as the ones described above. The VAX / VMS cluster system <ref> [Fox 87] </ref> uses a more sophisticated distributed lock system described in [Snaman 87] that allows a lock to be locked in one of several different ways, namely for concurrent reading or writing, protected reading or writing or as an exclusive lock.
Reference: [Geist 93] <author> A. Geist, A. Beguelin, J. Dongarra, W. Jiang, R. Manchek, and V. Sunderam, </author> <title> PVM 3.0 user's guide and reference manual, </title> <type> Technical Report ORNL TN 37831-8083, </type> <institution> Oak Ridge National Laboratory, </institution> <year> 1993. </year>
Reference: [Geist 94] <author> Al Geist, Adam Beguelin, Jack Dongarra, Weicheng Jiang, Robert Manchek and Vaidy Sunderam, </author> <title> PVM: Parallel Virtual Machine, A Users' Guide and Tutorial for Networked Parallel Computing, </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: Related Work 2.2.1 PVM PVM (Parallel Virtual Machine) has over 8 years of history, as outlined in <ref> [Geist 94] </ref>: The PVM project began in the summer of 1989 at Oak Ridge National Laboratory. The prototype system, PVM 1.0, was constructed by Vaidy Sunderam and Al Geist; this version of the system was used internally at the Lab and was not released to the outside.
Reference: [Gharachorloo 90] <author> Kourosh Gharachorloo, Daniel Lenoski, James Laudon, Philip Gibbons and Anoop Gupta, </author> <title> Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors, </title> <booktitle> Proceedings of the 17th Annual Symposium on Computer Architecture, </booktitle> <pages> pp. 15-26, </pages> <month> June </month> <year> 1990. </year> <note> Bibliography 155 </note>
Reference-contexts: Thread 1 Thread 2 Thread 3 write to global memory write message synchronizaton points Within this model all optimizations can be applied, as it no longer guarantees that any thread sees writes from some other thread in a given order. 4.5 Release Consistency The release consistency model was introduced by <ref> [Gharachorloo 90] </ref> as an extension of the weak consistency model. It distinguishes two kinds of synchronization points, namely acquire and release synchronization points. Acquire synchronization points 56 4.
Reference: [Gharachorloo 91] <author> Kourosh Garachorloo, Anoop Gupta and John Hennessy, </author> <title> Performance Evaluation of Memory Consistency Models for Shared-Memory Multiprocessors, </title> <booktitle> Proceedings of the 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 245-257, </pages> <publisher> ACM, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: Writes of any processors are to be observed in the same order by any other processor, while a processor may observe writes issued by two different proces sors in different orders. This model was proposed to relax the restrictions of the sequential consistent memory model and <ref> [Gharachorloo 91] </ref> shows that it performs significantly better than sequential consistency on shared memory multiprocessors. Processor consistency does not work much better for distributed systems as each processor executes several threads that use the sequential memory consistency model among themselves.
Reference: [Gifford 79] <author> D. K. Gifford, </author> <title> Weighted Voting for Replicated Data, </title> <booktitle> Proceedings of the 7th Symposium on Operating System Principles, </booktitle> <pages> pp. 150-162, </pages> <month> December </month> <year> 1979. </year>
Reference-contexts: Distributed Reader/Writer locks have also been proposed, mostly to access shared memory pages in a distributed system that simulates shared memory by copying the pages from one processor to another. One, called Available Copies, was proposed in [Bernstein 84] and another one, Weighted Voting in <ref> [Gifford 79] </ref>. This one works as follows: each copy of the data is assigned a weight; to read the object, the process must collect as many copies as necessary so that the sum of all weights is larger than a predefined value, called the read quorum.
Reference: [Ghring 93] <author> Hans-Georg Ghring, Franz-Joachim Kauffels, </author> <title> Token Ring, Principles, Perspectives and Strategies, </title> <publisher> Addison-Wesley, </publisher> <year> 1993. </year>
Reference-contexts: Some of the solutions require the process that needs exclusive access to acquire a token. Such algorithms have been implemented mostly in software, a very well known version however is implemented in hardware, namely in the Token Ring Network <ref> [Ghring 93] </ref>. A free token is constantly passed from one card to the next. If a card needs to send a message, it removes the token and replaces it with a busy token.
Reference: [Gomes 96] <author> Benedict Gomes, David Stoutamire, Boris Weissman and Holger Klawitter, </author> <title> A Language Manual For Sather 1.1, </title> <note> available at http://www.icsi.berkeley.edu , August 1996. </note>
Reference: [Goodman 89] <author> James R. Doodman, </author> <title> Cache consistency and sequential consistency, </title> <type> Technical Report no. 61, </type> <institution> SCI Committee, </institution> <month> March </month> <year> 1989. </year>
Reference-contexts: However, this requires complex inter procedural analysis and cannot be done in many cases. 4.2. Processor Consistency 53 4.2 Processor Consistency <ref> [Goodman 89] </ref> introduced processor consistency. Writes of any processors are to be observed in the same order by any other processor, while a processor may observe writes issued by two different proces sors in different orders.
Reference: [Goos 95] <author> Gerhard Goos, Sather-K, </author> <title> The Language, </title> <type> Report 8/95, </type> <institution> University of Karlsruhe, </institution> <year> 1995. </year>
Reference-contexts: This includes for the first time the pSather extension used in this thesis, running on Workstations, Network of Workstations connected with either Myrinet or TCP/IP and the Meiko CS-2 Computer. A group at the University of Karlsruhe created a compiler for Sather 0.1. The language their compiler supports, Sather-K <ref> [Goos 95] </ref>, diverged from the ICSI specification when Sather 1.0 was released. Sather/pSather has its own home on the World Wide Web at http://www.icsi.berkeley.edu/~sather , and also its own newsgroup named 2.5. History of Sather / pSather 19 comp.lang.sather.
Reference: [Goscinski 90] <author> A. Goscinski, </author> <title> Two algorithms for mutual exclusion in real-time distributed computer systems, </title> <journal> The Journal of Parallel and Distributed Computing, </journal> <volume> 9, </volume> <pages> pp. 77-82, </pages> <year> 1990. </year>
Reference-contexts: This protocol was enhanced by [Ricart 81] and [Carvalho 83] by reducing the numbers of messages sent. A distributed priority lock algorithm has been developed by <ref> [Goscinski 90] </ref> and a similar one, although faster and with less overhead, was also proposed in [Johnson 94]. However, neither of those algorithms addresses the problem of deadlocks when acquiring more than one lock at the same time, although they do have a first-come-first-serve policy.
Reference: [Gunzinger 92] <author> A. Gunzinger, U. A. Mller, W. Scott, B. Bumle, P. Kohler, W. Guggenbhl, </author> <title> Architecture and Realization of a Multi Signalproc-essor System, International Conference On Application Specific Array Processors, </title> <publisher> IEEE Computer Society Press, </publisher> <year> 1992. </year>
Reference: [Gupta 92] <author> Manish Gupta, </author> <title> Automatic Data Partitioning on Distributed Memory Multicomputers, </title> <type> Ph.D. Thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1992. </year>
Reference-contexts: Several algorithms for the optimal evaluation of array expressions in Fortran 90 are presented in [Chatterjee 95]. While Fortran D [Hirandani 92] allows the user to specify the correct alignment, <ref> [Gupta 92] </ref> and [Philippsen 93] among others showed that in fact in many cases the compiler is able to do the correct alignment and distribution of the data without user annotations. [Anderson 93] describes an algorithm to automatically find computation and data decomposition for dense matrix calculations that optimize parallelism and
Reference: [Hall 95] <author> Mary W. Hall, Brian R. Murphy, Saman P. Amarasinghe, Shih-Wei Liao and Monica S. Lam, </author> <title> Interprocedural Analysis for Paralleliza-tion, </title> <booktitle> Proceedings of the Seventh SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pp. 650-655, </pages> <address> San Francisco, CA, USA, </address> <month> February </month> <year> 1995. </year>
Reference-contexts: A parallelizing compiler that uses only information available at compile time is described in <ref> [Hall 95] </ref>.
Reference: [Hirandani 92] <author> Seema Hiranandani, Ken Kennedy and Chau-Wen Tseng, </author> <title> Compiling Fortran D for MIMD Distributed Memory Machines, </title> <journal> Communications of the ACM, </journal> <volume> 35, (8): </volume> <pages> pp. 66-80, </pages> <year> 1992. </year>
Reference-contexts: Another area of research for data parallel languages is the alignment of arrays on massively parallel computers like the MasPar. Several algorithms for the optimal evaluation of array expressions in Fortran 90 are presented in [Chatterjee 95]. While Fortran D <ref> [Hirandani 92] </ref> allows the user to specify the correct alignment, [Gupta 92] and [Philippsen 93] among others showed that in fact in many cases the compiler is able to do the correct alignment and distribution of the data without user annotations. [Anderson 93] describes an algorithm to automatically find computation and
Reference: [Hirandani 94] <author> Seema Hirandani, Ken Kennedy and Chau-Wen Tseng, </author> <title> Evaluating Compiler Optimizations For Fortran D, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 21(1) </volume> <pages> 27-45, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: A similar problem, namely analyzing array accesses in nested loops, is solved in [Amarasinghe 93]. In <ref> [Hirandani 94] </ref> several different optimization techniques for Fortran D are outlined, among others combining messages, message pipelining (similar to prefetching and postwriting described in chapter 6) and different pipelining optimizations. Scheduling of threads and distributing threads to different processors automatically has been extensively studied.
Reference: [Johnson 94] <author> Theodore Johnson and Richard Newman-Wolfe, </author> <title> A Fast and Low Overhead Distributed Priority Lock, </title> <type> Technical Report TR 10-94, </type> <institution> Dept. of CIS, University of Florida, </institution> <address> Gainesville. 156 Bibliography </address>
Reference-contexts: This protocol was enhanced by [Ricart 81] and [Carvalho 83] by reducing the numbers of messages sent. A distributed priority lock algorithm has been developed by [Goscinski 90] and a similar one, although faster and with less overhead, was also proposed in <ref> [Johnson 94] </ref>. However, neither of those algorithms addresses the problem of deadlocks when acquiring more than one lock at the same time, although they do have a first-come-first-serve policy.
Reference: [Keckler 92] <author> Stephen W. Kecjler and William J. Dally, </author> <title> Processor Coupling: Integrating Compile Time and Runtime Scheduling for Parallelism, </title> <booktitle> 19th Annual International Symposium in Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: One algorithm runs locally on each node, while the other makes global decisions. The decision about which one to use depends on the parallel program. Another load balancing problem is studied in <ref> [Keckler 92] </ref>, where the compiler tries to schedule different threads such that they utilize 4 high performance ALUs controlled by one CPU as much as possible by exploiting instruction-level and inter-thread parallelism.
Reference: [Kranz 93] <author> David Kranz, Kirk Johnson, Anant Agarwal, John Kubiatowicz and Beng-Hong Lim, </author> <title> Integrating Message-Passing and Shared-Memory: Early Experience, </title> <booktitle> Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pp. 54-63, </pages> <address> San Diego, CA, USA, </address> <year> 1993. </year>
Reference-contexts: This chapter describes several different standards used on many different systems and for large applications. To use a distributed memory system to simulate shared memory seems to be strange, as it would probably be better to use hardware to create a real shared memory system. <ref> [Kranz 93] </ref> argues that a modern computer should support both, a message passing interface and shared memory, as, depending on the problem, either one or the other is necessary to get an efficient implementation. [Larus 93] however showed that compiler implemented shared memory has the potential to exploit more effectively the
Reference: [Krishnamurthy 94] <author> Arvind Krishnamurthy and Katherine Yelick, </author> <title> Optimizing Parallel SPMD Programs, </title> <booktitle> Seventh Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Ithaca, New York, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: Both work however on singe processors to reduce the waiting time during cache misses, but not for distributed memory systems that simulate shared memory as does the pSather language. <ref> [Krishnamurthy 94] </ref> describes how optimizations can be applied to a program using the sequential consistency memory model by detecting which memory access would result in a violation of the memory model if moved. A parallelizing compiler that uses only information available at compile time is described in [Hall 95].
Reference: [Krishnamurthy 96] <author> Arvind Krishnamurthy, Klaus E. Schauser, Chris J. Scheiman, Randolph Y. Wang, David E. Culler and Kathrine Yelick, </author> <title> Evaluation of Architectural Support for Global Address-Based Communication in Large-Scale Parallel Machines, </title> <institution> University of California at Berkeley, </institution> <address> CA, USA, </address> <year> 1996. </year>
Reference-contexts: While systems with custom built hardware are generally slightly faster, they are more expensive to build and upgrade, and generally use hardware that is one to two years behind the fastest serial computers. <ref> [Krishnamurthy 96] </ref> describes several more large computers, namely Thinking Machines CM-5, the Intel Paragon and the Cray T3D and their networks and how they can be used to efficiently implement shared memory. 2.1.1 NOW The goal of the NOW (Network Of Workstations) [Anderson 95b] project of the University of California at
Reference: [Krone 95] <author> Oliver Krone, Christian Renevey and Bat Hirsbrunner, </author> <month> PT-PVM: </month>
Reference-contexts: PVM does not support active messages and is not thread safe, as it uses global buffers to create a message, and then the same buffer to send it out. However, three PVM versions that support threads are available, namely PT-PVM by <ref> [Krone 95] </ref>, TPVM, see [Ferrari 94] and [Ferrari 95] and LPVM described in [Zhou 95]. PT-PVM lets several thread run inside a PVM process. Local threads communicate by using shared memory.
References-found: 53

