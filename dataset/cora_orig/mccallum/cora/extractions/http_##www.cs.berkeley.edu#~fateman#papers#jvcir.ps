URL: http://www.cs.berkeley.edu/~fateman/papers/jvcir.ps
Refering-URL: http://www.cs.berkeley.edu/~fateman/ocrpapers.html
Root-URL: 
Title: Optical Character Recognition and Parsing of Typeset Mathematics  
Author: Richard J. Fateman Taku Tokuyasu Benjamin P. Berman Nicholas Mitchell 
Address: Engineering, Mail Code 0114, La Jolla, CA 92093-0114.  
Note: This work was supported in part by NSF Grants numbers CCR-9214963 and IRI-9411334, and by NSF Infrastructure Grant number CDA-8722788. Present Address:  
Date: October 30, 1995  
Affiliation: Computer Science Division, EECS Department University of California at Berkeley  University of California, San Diego, Department of Computer Science and  
Abstract: There is a wealth of mathematical knowledge that could be potentially very useful in many computational applications, but is not available in electronic form. This knowledge comes in the form of mechanically typeset books and journals going back more than one hundred years. Besides these older sources, there are a great many current publications, filled with useful mathematical information, which are difficult if not impossible to obtain in electronic form. Our work intends to encode, for use by computer algebra systems, integral tables and other documents currently available in hardcopy only. Our strategy is to extract character information from these documents, which is then passed to higher-level parsing routines for further extraction of mathematical content (or any other useful two-dimensional semantic content). This information can then be output as, for example, a Lisp or T E X expression. We have also developed routines for rapid access to this information, specifically for finding matches with formulas in a table of integrals. This paper reviews our current efforts, and summarizes our results and the problems we have encountered. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Bierens de Haan. </author> <title> Nouvelles Tables d'Integrales De finies edition of 1867 corrected, with an English Translation of the Introduction by J.F. </title> <editor> Ritt. G. E. </editor> <publisher> Stechert & Co. </publisher> <address> NY, </address> <year> 1939. </year>
Reference-contexts: 1 Introduction This is our problem: There exist many integral tables in printed form, such as the one compiled by Bierens de Haan <ref> [1] </ref>. Figure 1 is a sample entry from this table. It would be very useful to have this information easily accessible in electronic form. For example, during a session with one of the popular computer algebra systems (CAS), an integral formula might be necessary to continue a computation. <p> We note in passing that there is not a "single" mathematical language. The typeset characteristics of some glyphs varies substantially with historical and aesthetic criteria. One source of material (de Haan <ref> [1] </ref>) sets the "+" with a descender as low as the tail of a "p" and uses the form Cos for the mathematical cosine. The standard size of font used for T E X mathematics has a smaller "+" and uses cos for cosine. <p> exists (at least) one valid Macsyma expression corresponding to the display we are parsing. 6 2.5 Timing To get an idea of the time this all takes, let us assume that prior to our looking at an equation, the page processing has identified a "zone" containing a single equation from <ref> [1] </ref>, as in Figure 1. This was scanned at 200 dots per inch (dpi), and slightly cleaned up so that our naive OCR would work. <p> Both size and font information provide useful hints to the parser, as the use of large letters, small letters, bold-face, italics, and different fonts can provide additional semantics. We have found references (e.g. de Haan <ref> [1] </ref>) using different fonts and sizing for numbers depending upon their use as (for example) page numbers, equation labels, reference numbers, base-line coefficients, exponents, or footnotes. <p> Some characters are raised up|that is, they have baselines below their lowest extent, including "-" and "=". Oddly, the character "+" in one source <ref> [1] </ref> is so large that it must be treated as having a descender, but not (for example) in output from T E X. A significant exception to the left-to-right method occurs in the case of a multiple-line formula. <p> The cropped rectangle is then processed further. Special fonts and abbreviations Some texts may use additional type fonts (Gothic, Black Forest, an occasional Hebrew letter); some use l for log, sn for sin, or T ang 19 for tan. de Haan <ref> [1] </ref> for example has a particular notation that changes the mean-ing of the solidus (=) in exponents via the "Notation de Kramp" such that c a=b is c (c + b)(c + 2b) (c + (a 1)b) and thus 1 a=1 is our usual a!. <p> Therefore, it is possible to catalog all the conventions that are specific to a book (or perhaps to a book designer and publisher). Then this dictionary will be another input to the parsing algorithm. Although we now have separate dictionaries for T E X, de Haan <ref> [1] </ref>, and Gradshteyn [2], not all such conventions can be predicted; an author is relatively free to invent new conventions for notation, including temporary abbreviations, and draw upon additional character sets. 4.5 Outline of our parsing algorithm 4.5.1 Input A page of glyphs and locations is presented to us as a
Reference: [2] <author> I. S. Gradshteyn and I. M. Ryzhik, </author> <title> Table of Integrals, Series, and Products, </title> <publisher> Academic Press, </publisher> <address> NY, </address> <year> 1980. </year>
Reference-contexts: We have also kept in mind how this work might fit into the overall framework of document processing and the possibility of using a more general formulation of the 1 Academic Press has recently advertised that the integral table by Gradshteyn and Ryzhik <ref> [2] </ref> will soon be available on CD-ROM. While we have yet to see the CD in action, we believe our problem is of sufficient general interest to warrant further research, independent of whether the AP product has achieved full recognition of the printed text. 2 problem [4]. <p> Therefore, it is possible to catalog all the conventions that are specific to a book (or perhaps to a book designer and publisher). Then this dictionary will be another input to the parsing algorithm. Although we now have separate dictionaries for T E X, de Haan [1], and Gradshteyn <ref> [2] </ref>, not all such conventions can be predicted; an author is relatively free to invent new conventions for notation, including temporary abbreviations, and draw upon additional character sets. 4.5 Outline of our parsing algorithm 4.5.1 Input A page of glyphs and locations is presented to us as a set of 5-tuples.
Reference: [3] <institution> Third International Conference on Document Analysis and Recognition - ICDAR '95, Montreal, Canada, </institution> <month> August 14-16, </month> <title> 1995, </title> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1995. </year>
Reference-contexts: Recognition of mathematics has in fact been enjoying a resurgence of interest. For instance, the most recent ICDAR proceedings <ref> [3] </ref> contains at least five papers related to this area. Our application differs from these presentations in that we cannot assume ideal print quality, as appears to be standard in papers in this area.
Reference: [4] <author> P. Chou and G. Kopec. </author> <title> A stochastic attribute grammar model of document production and its use in document recognition. </title> <booktitle> First International Workshop on Principles of Document Processing, </booktitle> <address> Washington, DC, </address> <month> Oct. </month> <pages> 21-23, </pages> <year> 1992. </year>
Reference-contexts: While we have yet to see the CD in action, we believe our problem is of sufficient general interest to warrant further research, independent of whether the AP product has achieved full recognition of the printed text. 2 problem <ref> [4] </ref>. As our major interest lies in ways to make mathematical information ac-cessible, a solution which is not completely satisfying from a theoretical point of view may nevertheless be sufficient, and, with the occasional call on human intervention for corrections, possibly faster. <p> Chou's work [15] by contrast views the recognition of equations as a particular domain within an overall framework, which regards recognition of 2-D images as fundamentally a signal processing task to decode a structural (syntactic) signal corrupted by noise <ref> [4] </ref>. This is a more elegant, but perhaps less pragmatic approach, especially with full 2-D structure. <p> In particular, for dealing with noise, building recovery into a recursive descent parser is one possible approach. We may find it advantageous to consider Chou's techniques <ref> [4] </ref> as a fall-back position based on stochastic parsing and probabilistic recognition of equations. We believe this will be by comparison much slower. 5 Table lookup The preceding sections described how we extract information from a printed page of integrals.
Reference: [5] <author> T.H. Einwohner and Richard J. Fateman, </author> <title> Searching techniques for integral tables, </title> <editor> in A.H.M. Levelt, ed., </editor> <booktitle> Proceedings of the 1995 International Symposium on Symbolic and Algebraic Computation - ISSAC '95, </booktitle> <address> July 10-12, 1995, Montreal, Canada, </address> <publisher> ACM Press, </publisher> <address> NY, </address> <year> 1995. </year>
Reference-contexts: implemented a storage and retrieval method to access stored formulas by "approximate" matching. 2 This table and access method would normally be used by a computer algebra system, but could be used by a human user to "look up an integral." As the table has already been described in detail <ref> [5] </ref>, we concentrate in this paper on the OCR and parsing algorithms. The paper is organized as follows. In the next section, we describe a run through the system as it presently stands. Following that, two sections describe our current approach to OCR and 2-D parsing in more detail. <p> We give a brief and somewhat simplified account of the structure of the system below. Details of the system can be found in Einwohner <ref> [5] </ref>. 5.1 Table design An entry in the table consists of information including the integrand, the limits of integration, regions of validity of the answer, and the source of the entry. The "answer" is typically a closed form result, a reduction formula, or a program to evaluate the integral.
Reference: [6] <author> H.S. Baird, H. Bunke, K. Yamamoto, eds., </author> <title> Structured Document Image Analysis, </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1992. </year>
Reference-contexts: Such a program would still make mistakes on some inputs. 3 We claim no particular originality in any of the given algorithms, for which Baird <ref> [6] </ref> and O'Gorman [7] provide keys to the literature. In order to demonstrate the flow of our ideas, we have concentrated on developing a prototype to handle the perfectly-typeset case.
Reference: [7] <editor> Lawrence O'Gorman and Rangachar Kasturi, eds., </editor> <title> Document Image Analysis, </title> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1995. </year>
Reference-contexts: Such a program would still make mistakes on some inputs. 3 We claim no particular originality in any of the given algorithms, for which Baird [6] and O'Gorman <ref> [7] </ref> provide keys to the literature. In order to demonstrate the flow of our ideas, we have concentrated on developing a prototype to handle the perfectly-typeset case. <p> Thus we have found ourselves writing our own character recognition software. Other specialized tasks (besides equations) may yield to the same general approach. Joined with some graph-type recognition, certain types of chemical and physical diagrams could be tackled as well (see e.g., O'Gorman and Kasturi <ref> [7] </ref> for discussions of related topics). 3.1 Objectives Our objective for OCR is to start from a scanned binary representation of the mathematical expression (a "bitmap") and end up with a set of pairs: (glyphs, location).
Reference: [8] <author> Frank Jenkins and Junichi Kanai, </author> <title> The use of synthesized images to evaluate the performance of optical character recognition devices and algorithms, </title> <editor> in Luc M. Vincent, Theo Pavlidis, eds., </editor> <title> Document Recognition, </title> <booktitle> Proceedings of SPIE; v. 2181, </booktitle> <address> Bellingham, WA, </address> <year> 1994. </year>
Reference-contexts: We are fairly comfortable with the subset of T E X used by CASs. The above method is useful for developing databases of ideal characters <ref> [8] </ref>. Alternatively, we can use a printed source such as an integral table and scan it into the computer.
Reference: [9] <author> M. Bokser. </author> <booktitle> Omnidocument technologies, Proceedings of the IEEE, </booktitle> <month> July </month> <year> 1992, </year> <note> vol.80, (no.7) 1066|78. </note>
Reference-contexts: A useful general reference is the paper by Bokser <ref> [9] </ref>, which discusses in some detail the Calera scheme; other competing commercial systems include Caere and Xerox Imaging Systems. 8 While representations such as the XDOC format from Xerox may in principle be more powerful, the associated OCR mechanisms do not seem to support recognition of images such as complex fractions <p> The variations are nevertheless small enough that we can reasonably envision learning them all. 3.2 General methods A variety of methods have been developed for OCR, based on character morphology [10], character "skeletons" [10, 11], feature vectors <ref> [9] </ref>, Hausdorff distances [12, 13], etc. Each method works well in certain circumstances. Further progress in this area depends in part on coming up with systematic ways of testing these various methods. <p> We back off from this admirably general long-term approach to explore what we actually must do to get OCR to work on examples of interest in the short term. The method we describe here for recognizing characters is a simple one based on "property vectors" (as suggested, e.g., in <ref> [9] </ref>). This straightforward approach works well for our present purposes. As we better understand how to deal with non-ideal input, the role of the recognition module may also change.
Reference: [10] <author> Paul D. Gader, Edward R. Dougherty, and Jean C. Serra, eds., </author> <title> Image algebra and morphological image processing III, </title> <booktitle> Proceedings of SPIE; v. 1769, </booktitle> <address> Bellingham, WA, </address> <year> 1992. </year>
Reference-contexts: The variations are nevertheless small enough that we can reasonably envision learning them all. 3.2 General methods A variety of methods have been developed for OCR, based on character morphology <ref> [10] </ref>, character "skeletons" [10, 11], feature vectors [9], Hausdorff distances [12, 13], etc. Each method works well in certain circumstances. Further progress in this area depends in part on coming up with systematic ways of testing these various methods. <p> The variations are nevertheless small enough that we can reasonably envision learning them all. 3.2 General methods A variety of methods have been developed for OCR, based on character morphology [10], character "skeletons" <ref> [10, 11] </ref>, feature vectors [9], Hausdorff distances [12, 13], etc. Each method works well in certain circumstances. Further progress in this area depends in part on coming up with systematic ways of testing these various methods.
Reference: [11] <author> Simon Kahan, Theo Pavlidis, and Henry S. Baird, </author> <title> "On the Recognition of Printed Characters of Any Font and Size," </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence, PAMI-9, </journal> <volume> No. 2, </volume> <month> March, </month> <year> 1987. </year> <month> 28 </month>
Reference-contexts: The variations are nevertheless small enough that we can reasonably envision learning them all. 3.2 General methods A variety of methods have been developed for OCR, based on character morphology [10], character "skeletons" <ref> [10, 11] </ref>, feature vectors [9], Hausdorff distances [12, 13], etc. Each method works well in certain circumstances. Further progress in this area depends in part on coming up with systematic ways of testing these various methods.
Reference: [12] <author> B. P. Berman and R. J. Fateman. </author> <title> Optical Character Recognition for Typset Mathematics, </title> <booktitle> Proceedings of the 1994 International Symposium on Symbolic and Algebraic Computation - ISSAC '94, </booktitle> <address> July, 1994, Oxford, UK, </address> <publisher> ACM Press, </publisher> <address> NY, </address> <year> 1994. </year>
Reference-contexts: Cleaning constituted breaking connected glyphs in Sin and Cos; we also removed the dots above the i's, but that could also have been handled in other ways. In rewriting our code and data structures from an earlier 1994 program <ref> [12] </ref>, we substantially sped up most parts of the process. <p> The variations are nevertheless small enough that we can reasonably envision learning them all. 3.2 General methods A variety of methods have been developed for OCR, based on character morphology [10], character "skeletons" [10, 11], feature vectors [9], Hausdorff distances <ref> [12, 13] </ref>, etc. Each method works well in certain circumstances. Further progress in this area depends in part on coming up with systematic ways of testing these various methods. <p> These templates could be bitmaps of the characters themselves, as used for instance by the Hausdorff distance method <ref> [12] </ref>.
Reference: [13] <author> D.P. Huttenlocher, Gregory A. Klandermand, William J. Rucklidge. </author> <title> Comparing images using the Hausdorff distance, </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 15, </volume> <year> 1993, </year> <pages> 850-63. </pages>
Reference-contexts: The variations are nevertheless small enough that we can reasonably envision learning them all. 3.2 General methods A variety of methods have been developed for OCR, based on character morphology [10], character "skeletons" [10, 11], feature vectors [9], Hausdorff distances <ref> [12, 13] </ref>, etc. Each method works well in certain circumstances. Further progress in this area depends in part on coming up with systematic ways of testing these various methods.
Reference: [14] <author> Henry S. Baird, </author> <title> Document Image Defect Models, </title> <editor> in H.S. Baird, H. Bunke, K. Yamamoto, eds., </editor> <title> Structured document image analysis, </title> <publisher> Berlin, Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: Each method works well in certain circumstances. Further progress in this area depends in part on coming up with systematic ways of testing these various methods. For instance, databases of English text with their "truthed" equivalents and models of noise <ref> [14] </ref> have been developed to address this issue. Probably the most fully-formed and described theory for the recognition of equations is one that emphasizes the impact of noise, in work by P. Chou [15].
Reference: [15] <author> P. Chou, </author> <title> Recognition of equations using a two-dimensional stochastic context-free grammar, </title> <booktitle> Proceedings, SPIE Conference on Visual Communications and Image Processing IV, </booktitle> <address> Philadelphia, PA, </address> <year> 1989, </year> <pages> pp. 852-863. </pages>
Reference-contexts: For instance, databases of English text with their "truthed" equivalents and models of noise [14] have been developed to address this issue. Probably the most fully-formed and described theory for the recognition of equations is one that emphasizes the impact of noise, in work by P. Chou <ref> [15] </ref>. This paper describes a 2-D parser which runs on a bit map produced by eqn and then peppered with noise. <p> A more complete system must devote some effort to correcting glyph errors. We leave this for future work. 14 4.1 Prior work in 2-D Parsing Two-dimensional mathematical expression parsing has been addressed at several times; in the late 1960's [19, 20, 21] and more recently <ref> [15] </ref>. The 60's research can be characterized as a mixture of, on the one hand, pragmatic work, and on the other, rigorous but impractical syntactic treatment of the subject. For example, Anderson's parser [19] handled a very large domain of mathematical expressions. <p> However, the language we need to parse is more complicated than that used by Lewis, or the more tentative study by Martin [23]. Chou's work <ref> [15] </ref> by contrast views the recognition of equations as a particular domain within an overall framework, which regards recognition of 2-D images as fundamentally a signal processing task to decode a structural (syntactic) signal corrupted by noise [4].
Reference: [16] <author> Steven C. Bagley and Gary E. Kopec, </author> <title> Editing Images of Text, </title> <journal> Commun. ACM, </journal> <note> Dec. 1994, vol.37, (no.12):63-72. </note>
Reference-contexts: Since a human is already involved at the identification stage, he or she could also "edit" the bounding box for such multiple-component characters, which are not too great in number. Alternatively, a heuristic such as looking for a dot within a fixed space above the ambiguous segment <ref> [16] </ref> can be used, at the cost of introducing a possibly document-specific parameter. Character recognition proceeds along similar lines to that in the training stage. We form the property vector for an input set of pixels and calculate its distance from the property vectors in the database.
Reference: [17] <author> M. Okamoto and A. Miyazawa. </author> <title> An Experimental Implementation of a Document Recognition System for Papers Containing Mathematical Expressions, </title> <editor> in H.S. Baird, H. Bunke, K. Yamamoto, eds., </editor> <title> Structured document image analysis, </title> <publisher> Berlin, Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: Related work has been done recently by, e.g. Okamoto <ref> [17] </ref>. The goal is to segment the image as far as possible before presenting a boxed region to the OCR engine.
Reference: [18] <author> Melvin Klerer and Fred Grossman, </author> <title> Error rates in tables of indefinite integrals, </title> <journal> Industrial Math., </journal> <volume> 18, </volume> <year> 1968, </year> <month> 31-62. </month> <title> See also, by the same authors, A new table of indefinite integrals; computer processed, </title> <publisher> Dover, </publisher> <address> New York, </address> <year> 1971. </year>
Reference-contexts: Quite the contrary. Substitution of numbers for parameters and numerical integration can form a partial confirmation <ref> [18] </ref>, but this too may be inadequate|the prospect for errors with regard to validity of domains is also substantial, and a particular weak point with computer algebra systems today. Only with indefinite integral formulas is there a prospect of checking by differentiating the result.
Reference: [19] <author> R. H. Anderson. </author> <title> Syntax-Directed Recognition of Hand-Printed Two-Dimensional Mathematics, </title> <type> Ph.D dissertation Harvard Univ.,Cambridge, </type> <address> MA, </address> <month> Jan. </month> <year> 1968. </year> <note> Also (shorter version) in M. </note> <editor> Klerer and J. Reinfelds (eds). </editor> <title> Interactive Systems for Experimental Applied Mathematics, </title> <publisher> Acad. Press, </publisher> <year> 1968. </year> <pages> 436-459. </pages>
Reference-contexts: A more complete system must devote some effort to correcting glyph errors. We leave this for future work. 14 4.1 Prior work in 2-D Parsing Two-dimensional mathematical expression parsing has been addressed at several times; in the late 1960's <ref> [19, 20, 21] </ref> and more recently [15]. The 60's research can be characterized as a mixture of, on the one hand, pragmatic work, and on the other, rigorous but impractical syntactic treatment of the subject. For example, Anderson's parser [19] handled a very large domain of mathematical expressions. <p> The 60's research can be characterized as a mixture of, on the one hand, pragmatic work, and on the other, rigorous but impractical syntactic treatment of the subject. For example, Anderson's parser <ref> [19] </ref> handled a very large domain of mathematical expressions. However, at the time of his work (1969), his recognizer appears to have been tested to its limits with equations having about eight symbols. <p> Our current program is not based on an automatic parser-generator, but on a somewhat mechanically generated variant of a "recursive descent" parser, and hence a grammar that maps into such a parser is particularly desirable. A heuristic suggested at least as long ago as 1968 <ref> [19] </ref> claims that for the usual mathematical notation, a parse proceeding left-to-right along a baseline can output ordered expressions: for example, we will see the horizontal divide bars before the numerator or denominators; likewise we will see the integral (or summation or product) sign before the limits or integrand of the <p> Adjacency can mean function application as in sin x, multiplication ax; but adjacency of dx signals "integration with respect to x". Spaces also are seen separating the formula number, the formula, and the reference citation in the equation "decorations". 4.4 Problems and some solutions As discussed by Anderson <ref> [19] </ref>, the distinguishing feature of 2-D parsing (compared to linear parsing) is the need to keep track of additional relationships between the 18 terminals and non-terminals of the language.
Reference: [20] <author> W. A. Martin. </author> <title> Computer Input/Output of Mathematical Expressions, </title> <type> Ph.D dissertation, </type> <institution> M.I.T. EECS Dep't, </institution> <year> 1967. </year>
Reference-contexts: A more complete system must devote some effort to correcting glyph errors. We leave this for future work. 14 4.1 Prior work in 2-D Parsing Two-dimensional mathematical expression parsing has been addressed at several times; in the late 1960's <ref> [19, 20, 21] </ref> and more recently [15]. The 60's research can be characterized as a mixture of, on the one hand, pragmatic work, and on the other, rigorous but impractical syntactic treatment of the subject. For example, Anderson's parser [19] handled a very large domain of mathematical expressions.
Reference: [21] <author> Mark B. Wells and James B. Morris, eds., </author> <title> Proceedings of a Symposium on Two-Dimensional Man-Machine Communication, </title> <journal> SIGPLAN Notices 7, </journal> <volume> No. 10, </volume> <booktitle> Octo-ber, 1972 (ACM). </booktitle>
Reference-contexts: A more complete system must devote some effort to correcting glyph errors. We leave this for future work. 14 4.1 Prior work in 2-D Parsing Two-dimensional mathematical expression parsing has been addressed at several times; in the late 1960's <ref> [19, 20, 21] </ref> and more recently [15]. The 60's research can be characterized as a mixture of, on the one hand, pragmatic work, and on the other, rigorous but impractical syntactic treatment of the subject. For example, Anderson's parser [19] handled a very large domain of mathematical expressions.
Reference: [22] <author> H. R. Lewis, </author> <title> Two Applications of Hand-Printed Two-Dimensional Computer Input, </title> <type> Senior Honors thesis, </type> <institution> Committee on Applied Mathematics, Harvard College. </institution> <note> (also, Proj. TACT Rpt. 2), 1968. 29 </note>
Reference-contexts: Furthermore, some of our expressions continue for several lines, and in some cases span pages. A much faster implementation of a parser for a subset of Anderson's language was implemented by Lewis <ref> [22] </ref>, in an impressive demonstration of a handwriting-driven conformal mapping program. However, the language we need to parse is more complicated than that used by Lewis, or the more tentative study by Martin [23].
Reference: [23] <author> W. A. Martin, </author> <title> A Fast Parsing Scheme for Hand-Printed Mthematical Expres--sions, MIT AI Project Memo 145, Project MAC Memo 360, </title> <month> October, </month> <year> 1967. </year> <month> 30 </month>
Reference-contexts: However, the language we need to parse is more complicated than that used by Lewis, or the more tentative study by Martin <ref> [23] </ref>. Chou's work [15] by contrast views the recognition of equations as a particular domain within an overall framework, which regards recognition of 2-D images as fundamentally a signal processing task to decode a structural (syntactic) signal corrupted by noise [4].
References-found: 23

