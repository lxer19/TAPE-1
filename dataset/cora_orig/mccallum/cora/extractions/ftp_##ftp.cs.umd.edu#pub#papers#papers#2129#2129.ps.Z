URL: ftp://ftp.cs.umd.edu/pub/papers/papers/2129/2129.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Title: STOCHASTIC PERTURBATION THEORY  
Author: G. W. STEWART 
Keyword: Key words. perturbation theory, random matrix, linear system, least squares, eigenvalue, eigenvector, invariant subspace, singular value  
Note: AMS(MOS) subject classifications. 15A06, 15A12, 15A18, 15A52, 15A60  
Abstract: In this paper classical matrix perturbation theory is approached from a probabilistic point of view. The perturbed quantity is approximated by a first-order perturbation expansion, in which the perturbation is assumed to be random. This permits the computation of statistics estimating the variation in the perturbed quantity. Up to the higher-order terms that are ignored in the expansion, these statistics tend to be more realistic than perturbation bounds obtained in terms of norms. The technique is applied to a number of problems in matrix perturbation theory, including least squares and the eigenvalue problem. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. E. Beaton, D. B. Rubin, and J. L. Barone, </author> <title> The acceptability of regression solutions: Another look at computational accuracy, </title> <journal> Journal of the American Statistical Association, </journal> <volume> 71 (1976), </volume> <pages> pp. 158-168. </pages>
Reference-contexts: See also [2] for applications to econometrics. x3.1.2. The presence of bias in regression coefficients with errors in the variables has long been known to statisticians under the name asymptotic inconsistency; e.g., see [23, x9.4], and <ref> [1, 40] </ref>. x3.2.1. As mentioned above, the perturbation expansion is due to Golub and Wilkinson [18]. x3.2.2. The quantities in (3.10) were derived in [36] as sensitivity coefficients for linear regression. x3.2.3.
Reference: [2] <author> G. F. Brown, J. B. Kadane, and J. G. Ramage, </author> <title> The asymptotic bias and mean-squared error of double K-class estimators when the distrubances are small, </title> <journal> International Economic Review, </journal> <volume> 15 (1974), </volume> <pages> pp. 667-679. </pages>
Reference-contexts: In the statistics literature, Hodges and Moore [19] and Davies and Hutton [7] have used first order approximations to least squares solutions to assess the effects of errors in the regression matrix|the problem of errors in the variables as it is known. See also <ref> [2] </ref> for applications to econometrics. x3.1.2. The presence of bias in regression coefficients with errors in the variables has long been known to statisticians under the name asymptotic inconsistency; e.g., see [23, x9.4], and [1, 40]. x3.2.1.
Reference: [3] <author> F. </author> <type> Chatelin, </type> <institution> Analyse statistique de la qualite numerique et arithmetic de la resolution ap-prochee d'equations par calcul sur ordinateur, Etude F.133, Centre Scientifique de Paris, 1988. [4] , De l'utilisation en calcul matriciel de modeles probabilistes pour la simulation des erreurs de calcul, Comptes Rendus de l'Academie des Sciences, Paris, Serie I, </institution> <month> 307 </month> <year> (1988), </year> <pages> pp. </pages> <month> 847-850. </month> <title> [5] , A probabilistic round-off error propagation model. application to the eigenvalue problem, in Reliable Numerical Software, </title> <editor> D. Cox and S. Hammarling, eds., </editor> <publisher> Oxford, </publisher> <year> 1990, </year> <note> Oxford University Press. To appear. </note>
Reference: [6] <author> N. David and G. W. Stewart, </author> <title> Hypothesis testing with errors in the variables, </title> <type> Technical Report TR-1735, </type> <institution> University of Maryland Department of Computer Science, </institution> <year> 1988. </year>
Reference-contexts: Thus the bounds estimate how far the column space of ~ A deviates from that of A. x3.4. The basic idea underlying this section is due to David and Stewart <ref> [6] </ref>. The present treatment is take from [41]. For an introduction to regression analysis with a survey of the errors-in-the-variables problem see [31]. STOCHASTIC PERTURBATION THEORY 19 When t is too large, one must have recourse to other techniques|techniques that require a precise knowledge of .
Reference: [7] <author> R. B. Davies and B. Hutton, </author> <title> The effects of errors in the independent variables in linear regression, </title> <journal> Biometrika, </journal> <volume> 62 (1975), </volume> <pages> pp. 383-391. </pages>
Reference-contexts: For surveys of the theory, see [44, 35, 42]. For extensions to more general problems, see [10, 29]. In the statistics literature, Hodges and Moore [19] and Davies and Hutton <ref> [7] </ref> have used first order approximations to least squares solutions to assess the effects of errors in the regression matrix|the problem of errors in the variables as it is known. See also [2] for applications to econometrics. x3.1.2.
Reference: [8] <author> C. Davis and W. M. Kahan, </author> <title> The rotation of eigenvectors by a perturbation. III, </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 7 (1970), </volume> <pages> pp. 1-46. </pages>
Reference-contexts: shown that the singular values of P are the tangents of the canonical angles between the subspaces X and ~ X (see <ref> [8] </ref>,[35] for definitions). Consequently, kP k = k ~ X Xk is a bound on the separation of the two subspaces. x4.1.5. Theorem 4.5 may be found in [8]. For generalization, see [24]. x4.1.6. A proof of Theorem 4.6 may be found in [34]. x4.1.8. Equation (4.24) does not represent all possible normalizations. In [27], which treats only eigenvectors, the normalizing function is allowed to be any differentiable function. Theorem 4.7 appears to be new. x4.1.9. <p> However, if we require both properties, then we must take w = y, the left eigenvector corresponding to . x4.2. A completely different approach to perturbation theory for Hermitian matrices is given by Davis and Kahan <ref> [8] </ref>. The Hoffman-Wielandt theorem appears in [20], and Wilkinson [46] gives an elementary proof. x4.4. A proof of Theorem 4.8 may be found in [39].
Reference: [9] <author> J. W. Demmel, </author> <title> The probability that a numerical analysis problem is difficult, </title> <journal> Mathematics of Computation, </journal> <volume> 50 (1988), </volume> <pages> pp. 449-480. </pages>
Reference: [10] <author> L. </author> <title> Eld en, Perturbation theory for the least squares problem with linear equality constraints, </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 17 (1980), </volume> <pages> pp. 338-350. </pages> <publisher> 32 G. W. STEWART </publisher>
Reference-contexts: Notes and references. Perturbation theory for the least squares problem begins with Golub and Wilkinson [18], who produced first-order expansions for least squares solutions. For surveys of the theory, see [44, 35, 42]. For extensions to more general problems, see <ref> [10, 29] </ref>. In the statistics literature, Hodges and Moore [19] and Davies and Hutton [7] have used first order approximations to least squares solutions to assess the effects of errors in the regression matrix|the problem of errors in the variables as it is known.
Reference: [11] <author> W. A. Fuller, </author> <title> Measurement Error Models, </title> <publisher> John Wiley, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: STOCHASTIC PERTURBATION THEORY 19 When t is too large, one must have recourse to other techniques|techniques that require a precise knowledge of . As usual, statisticians and numerical analysts have worked on the problem without consulting one another. Fuller's book on measurement error models <ref> [11] </ref> is the definitive source for the statistical approach (it contains much more than the model treated here).
Reference: [12] <editor> C. F. Gauss, Theoria Motus Corporum Coelestium in Sectionibus Conicis Solem Ambientium, Perthes and Besser, </editor> <title> Hamburg, 1809. [13] , Theory of the Motion of the Heavenly Bodies Moving about the Sun in Conic Sections, </title> <publisher> Dover, </publisher> <address> New York (1963), </address> <note> 1809. </note> <author> C. H. Davis, </author> <note> Trans. [14] , Theoria combinations observationum erroribus minimis obnoxiae, </note> <editor> pars prior, in Werke, </editor> <booktitle> IV, Koniglichen Gessellshaft der Wissenschaften zu Gottinging (1880), </booktitle> <volume> 1821, </volume> <pages> pp. 1-26. </pages>
Reference: [15] <author> A. Geman, </author> <title> A limit theorm for the norm of random matrices, </title> <journal> The Annals of Probability, </journal> <volume> 8 (1980), </volume> <pages> pp. 252-261. </pages>
Reference: [16] <author> G. H. Golub and C. F. Van Loan, </author> <title> An analysis of the total least squares problem, </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 17 (1980), </volume> <pages> pp. </pages> <month> 883-893. </month> <title> [17] , Matrix Computations, </title> <publisher> Johns Hopkins University Press, </publisher> <address> Baltimore, Maryland, </address> <year> 1983. </year>
Reference-contexts: As usual, statisticians and numerical analysts have worked on the problem without consulting one another. Fuller's book on measurement error models [11] is the definitive source for the statistical approach (it contains much more than the model treated here). On the numerical side, Golub and Van Loan <ref> [16] </ref> (also see [43]) have created a technique based on the singular value decomposition known as total least squares, which is closely related to the statisticians technique. When t is small, total least squares and least squares give essentially the same results [38]. 4. Eigenvalue problems.
Reference: [18] <author> G. H. Golub and J. H. Wilkinson, </author> <title> Note on the iterative refinement of least squares solution, </title> <journal> Numerische Mathematik, </journal> <volume> 9 (1966), </volume> <pages> pp. 139-148. </pages>
Reference-contexts: Note that we do not require a detailed knowledge of ; crude information sufficient to bound t is sufficient. Simulations suggest that least squares analysis can be trusted when t is less than 0:3. Notes and references. Perturbation theory for the least squares problem begins with Golub and Wilkinson <ref> [18] </ref>, who produced first-order expansions for least squares solutions. For surveys of the theory, see [44, 35, 42]. For extensions to more general problems, see [10, 29]. <p> The presence of bias in regression coefficients with errors in the variables has long been known to statisticians under the name asymptotic inconsistency; e.g., see [23, x9.4], and [1, 40]. x3.2.1. As mentioned above, the perturbation expansion is due to Golub and Wilkinson <ref> [18] </ref>. x3.2.2. The quantities in (3.10) were derived in [36] as sensitivity coefficients for linear regression. x3.2.3.
Reference: [19] <author> S. D. Hodges and P. G. Moore, </author> <title> Data uncertainties and least squares regression, </title> <journal> Applied Statistics, </journal> <volume> 21 (1972), </volume> <pages> pp. 185-195. </pages>
Reference-contexts: Notes and references. Perturbation theory for the least squares problem begins with Golub and Wilkinson [18], who produced first-order expansions for least squares solutions. For surveys of the theory, see [44, 35, 42]. For extensions to more general problems, see [10, 29]. In the statistics literature, Hodges and Moore <ref> [19] </ref> and Davies and Hutton [7] have used first order approximations to least squares solutions to assess the effects of errors in the regression matrix|the problem of errors in the variables as it is known. See also [2] for applications to econometrics. x3.1.2.
Reference: [20] <author> A. J. Hoffman and H. W. Wielandt, </author> <title> The variation of the spectrum of a normal matrix, </title> <journal> Duke Mathematical Journal, </journal> <volume> 20 (1953), </volume> <pages> pp. 37-39. </pages>
Reference-contexts: However, if we require both properties, then we must take w = y, the left eigenvector corresponding to . x4.2. A completely different approach to perturbation theory for Hermitian matrices is given by Davis and Kahan [8]. The Hoffman-Wielandt theorem appears in <ref> [20] </ref>, and Wilkinson [46] gives an elementary proof. x4.4. A proof of Theorem 4.8 may be found in [39]. It is possible to develop perturbation bound for spaces of singular vectors corresponding to clusters of singular values [34]; however, the bounds are not pretty. 5. Conclusions.
Reference: [21] <author> R. V. Hogg and A. T. Craig, </author> <title> Introduction to Mathematical Statistics, </title> <publisher> Macmillan, </publisher> <address> New York, </address> <year> 1978. </year> <note> 4th Edition. </note>
Reference: [22] <author> H. Hotelling, </author> <title> The selection of variates for use in prediction with some comments on the general problem of nuisance parameters, </title> <journal> The Annals of Mathematical Statistics, </journal> <volume> 11 (1940), </volume> <pages> pp. 271-283. </pages>
Reference: [23] <author> J. Johnston, </author> <title> Econometric Methods, </title> <publisher> Mc Graw-Hill, </publisher> <address> New York, </address> <publisher> 2nd ed., </publisher> <year> 1972. </year>
Reference-contexts: See also [2] for applications to econometrics. x3.1.2. The presence of bias in regression coefficients with errors in the variables has long been known to statisticians under the name asymptotic inconsistency; e.g., see <ref> [23, x9.4] </ref>, and [1, 40]. x3.2.1. As mentioned above, the perturbation expansion is due to Golub and Wilkinson [18]. x3.2.2. The quantities in (3.10) were derived in [36] as sensitivity coefficients for linear regression. x3.2.3.
Reference: [24] <author> W. Kahan, B. N. Parlett, and E. Jiang, </author> <title> Residual bounds on approximate eigensystems of nonnormal matrices, </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 19 (1982), </volume> <pages> pp. 470-484. </pages>
Reference-contexts: Consequently, kP k = k ~ X Xk is a bound on the separation of the two subspaces. x4.1.5. Theorem 4.5 may be found in [8]. For generalization, see <ref> [24] </ref>. x4.1.6. A proof of Theorem 4.6 may be found in [34]. x4.1.8. Equation (4.24) does not represent all possible normalizations. In [27], which treats only eigenvectors, the normalizing function is allowed to be any differentiable function. Theorem 4.7 appears to be new. x4.1.9.
Reference: [25] <author> T. Kato, </author> <title> Perturbation Theory for Linear Operators, </title> <publisher> Springer Verlag, </publisher> <address> New York, </address> <year> 1966. </year>
Reference-contexts: W. STEWART Notes and references. The general approach to invariant subspaces taken here is due to the author [33],[34]. For another view of the subject the reader reader is referred to Kato's work <ref> [25] </ref>, which also treats the perturbation of operators in infinite-dimensional settings. x4.1.2. The term "simple" referring to an invariant subspace does not seem to have appeared in the literature before. Theorem 4.3 is important in many of areas, and it has a number of proofs.
Reference: [26] <author> D. G. Kendall, </author> <title> Stochastic processes occuring in the theory of queues and their analysis by the method of the imbedded markov chain, </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 24 (1953), </volume> <pages> pp. 338-354. </pages>
Reference-contexts: Elementary treatments may be found in [21],[30]. The notation G (M; ) was suggested by the use of the letter G in queuing theory to stand for a general distribution, a practice started by Kendal <ref> [26] </ref>. x2.1. The material in this section appeared in some lecture notes by the author (c. 1982). Theorem 2.3 has been published by Neudecker and Wansbeek [28]. Although their paper treats normal matrices, their proof is quite general. x2.2.
Reference: [27] <author> C. Meyer and G. W. Stewart, </author> <title> Derivatives and perturbations of eigenvectors, </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 25 (1988), </volume> <pages> pp. 679-691. </pages>
Reference-contexts: Theorem 4.5 may be found in [8]. For generalization, see [24]. x4.1.6. A proof of Theorem 4.6 may be found in [34]. x4.1.8. Equation (4.24) does not represent all possible normalizations. In <ref> [27] </ref>, which treats only eigenvectors, the normalizing function is allowed to be any differentiable function. Theorem 4.7 appears to be new. x4.1.9.
Reference: [28] <author> H. Neudecker and T. Wansbeek, </author> <title> Fourth-order properties of normally distributed random matrices, Linear Algebra and Its Applications, </title> <booktitle> 97 (1987), </booktitle> <pages> pp. 13-22. </pages>
Reference-contexts: The material in this section appeared in some lecture notes by the author (c. 1982). Theorem 2.3 has been published by Neudecker and Wansbeek <ref> [28] </ref>. Although their paper treats normal matrices, their proof is quite general. x2.2. The formal use of the function E [trace (X T X)] as a norm on random matrices appears to be new. Its major problem is that the submultiplicative inequality (2.8) can fail.
Reference: [29] <author> C. C. Paige, </author> <title> Computer solution and perturbation analysis of generalized linear least squares problems, </title> <journal> Mathematics of Computation, </journal> <volume> 33 (1979), </volume> <pages> pp. 171-184. </pages>
Reference-contexts: Notes and references. Perturbation theory for the least squares problem begins with Golub and Wilkinson [18], who produced first-order expansions for least squares solutions. For surveys of the theory, see [44, 35, 42]. For extensions to more general problems, see <ref> [10, 29] </ref>. In the statistics literature, Hodges and Moore [19] and Davies and Hutton [7] have used first order approximations to least squares solutions to assess the effects of errors in the regression matrix|the problem of errors in the variables as it is known.
Reference: [30] <author> E. Parzen, </author> <title> Modern Probability Theory and Its Applications, </title> <publisher> John Wiley, </publisher> <address> New York, </address> <year> 1960. </year>
Reference: [31] <author> G. A. F. Seber, </author> <title> Linear Regression Analysis, </title> <publisher> John Wiley, </publisher> <address> New York, </address> <year> 1977. </year>
Reference-contexts: The basic idea underlying this section is due to David and Stewart [6]. The present treatment is take from [41]. For an introduction to regression analysis with a survey of the errors-in-the-variables problem see <ref> [31] </ref>. STOCHASTIC PERTURBATION THEORY 19 When t is too large, one must have recourse to other techniques|techniques that require a precise knowledge of . As usual, statisticians and numerical analysts have worked on the problem without consulting one another.
Reference: [32] <author> R. J. Serfling, </author> <title> Approximation Theorems of Mathematical Statistics, </title> <publisher> John Wiley, </publisher> <address> New York, </address> <year> 1980. </year>
Reference-contexts: For example, if e is distributed normally with mean zero and variance 1, then ke ek 2 S kek 2 The inequality can even fail for uncorrelated matrices. x2.4. Theorem 2.8 is the author's, who first proved (2.13) in [37]. Serfling <ref> [32, Thm. 3.3A] </ref> gives a similar theorem, but with e normal, of the form b n 0 for fixed 0 , and convergence in distribution.

Reference: [42] <author> G. W. Stewart and G.-J. Sun, </author> <title> Matrix Perturbation Theory, </title> <publisher> Academic Press, </publisher> <address> Boston, </address> <year> 1990. </year>
Reference-contexts: Simulations suggest that least squares analysis can be trusted when t is less than 0:3. Notes and references. Perturbation theory for the least squares problem begins with Golub and Wilkinson [18], who produced first-order expansions for least squares solutions. For surveys of the theory, see <ref> [44, 35, 42] </ref>. For extensions to more general problems, see [10, 29].
Reference: [43] <author> S. Van Huffel, </author> <title> Analysis of the Total Least Squares Problem and Its Use in Parameter Esti STOCHASTIC PERTURBATION THEORY 33 mation, </title> <type> PhD thesis, </type> <institution> Katholeike Universiteit Leuven, </institution> <year> 1987. </year>
Reference-contexts: Fuller's book on measurement error models [11] is the definitive source for the statistical approach (it contains much more than the model treated here). On the numerical side, Golub and Van Loan [16] (also see <ref> [43] </ref>) have created a technique based on the singular value decomposition known as total least squares, which is closely related to the statisticians technique. When t is small, total least squares and least squares give essentially the same results [38]. 4. Eigenvalue problems.
Reference: [44] <author> P. A. Wedin, </author> <title> Pertubation theory for pseudo-inverses, </title> <journal> BIT, </journal> <volume> 13 (1973), </volume> <pages> pp. 217-232. </pages>
Reference-contexts: Simulations suggest that least squares analysis can be trusted when t is less than 0:3. Notes and references. Perturbation theory for the least squares problem begins with Golub and Wilkinson [18], who produced first-order expansions for least squares solutions. For surveys of the theory, see <ref> [44, 35, 42] </ref>. For extensions to more general problems, see [10, 29].
Reference: [45] <author> N. Weis, G. W. Wasilkowski, H. Wo zniakowski, and M. Shub, </author> <title> Average condition number for solving linear equations, Linear Algebra and Its Applications, </title> <booktitle> 83 (1986), </booktitle> <pages> pp. 79-102. </pages>
Reference: [46] <author> J. H. Wilkinson, </author> <title> The Algebraic Eigenvalue Problem, </title> <publisher> Clarendon Press, Oxford, </publisher> <address> England, </address> <year> 1965. </year>
Reference-contexts: However, if we require both properties, then we must take w = y, the left eigenvector corresponding to . x4.2. A completely different approach to perturbation theory for Hermitian matrices is given by Davis and Kahan [8]. The Hoffman-Wielandt theorem appears in [20], and Wilkinson <ref> [46] </ref> gives an elementary proof. x4.4. A proof of Theorem 4.8 may be found in [39]. It is possible to develop perturbation bound for spaces of singular vectors corresponding to clusters of singular values [34]; however, the bounds are not pretty. 5. Conclusions.
References-found: 32

