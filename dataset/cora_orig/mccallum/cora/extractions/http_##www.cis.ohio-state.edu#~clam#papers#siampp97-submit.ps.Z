URL: http://www.cis.ohio-state.edu/~clam/papers/siampp97-submit.ps.Z
Refering-URL: http://www.cis.ohio-state.edu/~clam/
Root-URL: http://www.cis.ohio-state.edu
Email: e-mail: -clam, saday, wenger-@cis.ohio-state.edu  
Title: Optimization of a Class of Multi-Dimensional Integrals on Parallel Machines  
Author: Chi-Chung Lam*, P. Sadayappan*, and Rephael Wenger 
Address: Columbus, OH 43210  
Affiliation: Department of Computer and Information Science The Ohio State University  
Note: Supported in part by NSF grant DMR-9520319.  
Abstract: Multi-dimensional summations involving products of arrays arise in certain kinds of computational physics calculations that model electronic structure. Besides the issue of optimal distribution of the arrays among the processors, there is also scope for reordering of the operations using algebraic properties (commutativity, associativity, distributive law) to significantly reduce the number of operations executed. A framework for optimization of computational cost and communication cost has been developed, that can be used to synthesize efficient code. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. M. Anderson and M. S. Lam. </author> <title> Global Optimizations for Parallelism and Locality on Scalable Parallel Machines. </title> <booktitle> In Proceedings of the ACM SIGPLAN93 Conference on Programming Language Design and Implementation, </booktitle> <address> New York, </address> <year> 1993, </year> <pages> pp. 112-125. </pages>
Reference-contexts: This paper addresses the issue of optimizing communication and computational costs. Several others have investigated issues pertaining to automatic mapping of data and computation onto parallel machines. Chatterjee et. al. consider the optimal alignment of arrays in evaluating array expression on massively parallel machines [2,3]. Anderson and Lam <ref> [1] </ref> develop global loop transformations under a linear algebraic framework that handles very general loop forms. Gupta and Banerjee [4] have developed a constraint-based heuristic strategy for automatic data distribution in the PARADIGM compiler.
Reference: [2] <author> S. Chatterjee, J. R. Gilbert, R. Schreiber, and S.-H. Teng. </author> <title> Optimal Evaulation of Array Expressions on Massively Parallel Machines. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 17 (1), </volume> <month> January </month> <year> 1995, </year> <pages> pp. 123-156. </pages>
Reference: [3] <author> S. Chatterjee, J. R. Gilbert, R. Schreiber, and S.-H. Teng. </author> <title> Automatic Array Alignment in Data-Parallel Programs. </title> <booktitle> In Proceedings of the 20th Annual ACM SIGACT/SIGPLAN Symposium on Principles of Programming Languages, </booktitle> <address> New York, </address> <year> 1993, </year> <pages> pp. 16-28. </pages>
Reference: [4] <author> M. Gupta and P. Banerjee. </author> <title> PARADIGM: A Compiler for Automatic Data Distribution on Multicomputers. </title> <booktitle> In Proceedings of the ACM International Conference on Supercomputing, </booktitle> <address> Tokyo, Japan, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: Chatterjee et. al. consider the optimal alignment of arrays in evaluating array expression on massively parallel machines [2,3]. Anderson and Lam [1] develop global loop transformations under a linear algebraic framework that handles very general loop forms. Gupta and Banerjee <ref> [4] </ref> have developed a constraint-based heuristic strategy for automatic data distribution in the PARADIGM compiler. Our work differs from these other related works in that we address a more restricted form of the data/computation mapping problem, but evaluate many more data/computation mapping possibilities, including data/computation replication.
Reference: [5] <author> V. Kumar, A. Grama, A. Gupta, and G. Karypis. </author> <title> Introduction to Parallel Computing: Design and Analysis of Algorithms. </title> <address> RedWood City, CA: Benjamin/Cummings, </address> <year> 1994. </year>
Reference-contexts: Several existing matrix multiplication algorithms for parallel computers fit well into this model. One of them <ref> [5] </ref> is a simple parallel implementation of the serial block matrix multiplication algorithm. In this simple parallel algorithm, the processors form a 2-dimensional array. Initially, arrays A and B are fully block-distributed along both processor dimensions. <p> The cost of the summation is . No further data movement is required. Hence, the total execution time of this algorithm is represented as: Another matrix multiplication algorithm that fits into our model is known as the DNS algorithm <ref> [5] </ref>, which is based on a 3-dimensional processor view. The source arrays A and B are initially distributed the same way on the bottom processor plane where the third processor dimension is 1. Thus, their initial distributions can be specified by the 3-tuples for array A and for array B.
Reference: [6] <author> Chi-Chung Lam, P. Sadayappan and Rephael Wenger. </author> <title> Optimal Reordering and Mapping of a Class of Nested-Loops for Parallel Execution. </title> <booktitle> In Proceedings of Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> San Jose, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: Thus, automatic compile-time support for operation minimization and mapping onto a parallel machine is desirable. The operation minimization problem has been addressed in our previous work <ref> [6] </ref>. This paper addresses the issue of optimizing communication and computational costs. Several others have investigated issues pertaining to automatic mapping of data and computation onto parallel machines. Chatterjee et. al. consider the optimal alignment of arrays in evaluating array expression on massively parallel machines [2,3]. <p> For a summation formula of the form f r [...] = X [...], the cost is . The term arises because adding N i numbers requires additions. For example, the formula has a cost of ( )N i N j N t . In <ref> [6] </ref>, we proved that the operation minimization problem is NP-complete and provided a search procedure for obtaining the minimal-cost formula sequence. <p> Our earlier work <ref> [6] </ref> has considered the case where the processors form a logical one-dimensional chain and each array is either block-distributed among the processors along one of its dimensions or replicated on all processors.

References-found: 6

