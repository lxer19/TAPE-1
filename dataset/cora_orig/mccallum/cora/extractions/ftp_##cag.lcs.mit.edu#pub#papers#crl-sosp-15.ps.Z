URL: ftp://cag.lcs.mit.edu/pub/papers/crl-sosp-15.ps.Z
Refering-URL: http://www.pdos.lcs.mit.edu/~kerr/papers.html
Root-URL: 
Email: ftuna, kaashoek, kerrg@lcs.mit.edu  
Title: CRL: High-Performance All-Software Distributed Shared Memory  
Author: Kirk L. Johnson, M. Frans Kaashoek, and Deborah A. Wallach 
Address: Cambridge, MA 02139, U.S.A.  
Affiliation: MIT Laboratory for Computer Science  
Abstract: The C Region Library (CRL) is a new all-software distributed shared memory (DSM) system. CRL requires no special compiler, hardware, or operating system support beyond the ability to send and receive messages. It provides a simple, portable, region-based shared address space programming model that is capable of delivering good performance on a wide range of multiprocessor and distributed system architectures. Each region is an arbitrarily sized, contiguous area of memory. The programmer defines regions and delimits accesses to them using annotations. We have developed CRL implementations for two platforms: the Thinking Machines CM-5, a commercial multicomputer, and the MIT Alewife machine, an experimental multiprocessor offering efficient support for both message passing and shared memory. We present results for up to 128 processors on the CM-5 and up to 32 processors on Alewife. In a set of controlled experiments, we demonstrate that CRL is the first all-software DSM system capable of delivering performance competitive with hardware DSMs. CRL achieves speedups within 15% of those provided by Alewife's native support for shared memory, even for challenging applications (e.g., Barnes-Hut) and small problem sizes. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anant Agarwal, Ricardo Bianchini, David Chaiken, Kirk L. Johnson, David Kranz, John Kubiatowicz, Beng-Hong Lim, Ken Mackenzie, and Donald Yeung. </author> <title> The MIT Alewife Machine: Architecture and Performance. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-13, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Using the CM-5 implementation of CRL, we have run applications on systems with up to 128 processors. CRL is the first all-software DSM system to deliver performance competitive with hardware DSMs. To demonstrate this fact, we ported our CRL implementation to the MIT Alewife machine <ref> [1] </ref>. <p> This organization is motivated by studies indicating that small-scale sharing of data is the common case [9, 38, 51]; data shared more widely is relatively uncommon. In general, Alewife's shared memory system performs quite well, enabling speedups comparable to or better than other scalable hardware-based DSM systems <ref> [1, 33] </ref>. In addition to providing support for coherent shared memory, Alewife provides the processor with direct access to the interconnection network for sending and receiving messages [30]. Efficient mechanisms are provided for sending and receiving both short (register-to-register) and long (memory-to-memory, block transfer) messages.
Reference: [2] <author> Anant Agarwal, John Kubiatowicz, David Kranz, Beng-Hong Lim, Donald Yeung, Godfrey D'Souza, and Mike Parkin. Sparcle: </author> <title> An Evolutionary Processor Design for Multiprocessors. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 48-61, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: The basic Alewife architecture consists of processor/memory nodes communicating over a packet-switched interconnection network organized as a low-dimensional mesh (see Figure 2). Each processor/memory node consists of a Sparcle processor <ref> [2] </ref>, an off-the-shelf floating-point unit (FPU), a 64-kilobyte unified instruction/data cache (direct mapped, 16-byte lines), eight megabytes of DRAM, the local portion of the interconnection network, and a Communications and Memory Management Unit (CMMU).
Reference: [3] <author> H.E. Bal, M.F. Kaashoek, </author> <title> and A.S. Tanenbaum. Orca: A language for Parallel Programming of Distributed Systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <pages> pages 190-205, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: By manipulating the error correcting code bits associated with every memory block, Blizzard can control access on a cache-line by cache-line basis. All-Software In an all-software DSM system, all three of the mechanisms identified above are implemented entirely in software (e.g., Orca <ref> [3] </ref>). Several researchers have recently reported on experiences with all-software DSM systems obtained by modifying mostly software DSM systems such that the hit/miss check functionality is provided in software [43, 54]. <p> CRL runs on two large-scale platforms and has been shown to deliver performance competitive with hardware DSM systems. Several all-software DSM systems that employ an object-based approach have been developed (e.g., Amber [14], Orca <ref> [3] </ref>, Concert [24]). Like these systems, CRL effects coherence at the level of application-defined regions of memory (objects). Any necessary synchronization, data replication, or thread migration functionality is provided automatically at the entry and exit of methods on shared objects. <p> Any necessary synchronization, data replication, or thread migration functionality is provided automatically at the entry and exit of methods on shared objects. Existing systems of this type either require the use of an entirely new object-oriented language <ref> [3, 24] </ref> or only allow the use of a subset of an existing one [14]. In contrast, CRL is not language specific; the basic CRL interface could easily be provided in any imperative programming language. Scales and Lam [42] have described SAM, a shared object system for distributed memory machines.
Reference: [4] <author> Henri E. Bal and M. Frans Kaashoek. </author> <title> Object Distribution in Orca using Compile-Time and Run-Time Techniques. </title> <booktitle> In Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications (OOPSLA'93), </booktitle> <pages> pages 162-177, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: Other types of DSM systems are also possible (e.g., those in which threads of computation are migrated to the data they reference <ref> [4, 7, 14, 23] </ref>). We classify systems by three basic mechanisms required to implement DSM and whether those mechanisms are implemented in hardware or software.
Reference: [5] <author> Brian N. Bershad, Matthew J. Zekauskas, and Wayne A. Saw-don. </author> <title> The Midway Distributed Shared Memory System. </title> <booktitle> In Proceedings of the 38th IEEE Computer Society International Conference(COMPCON'93), </booktitle> <pages> pages 528-537, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: From this perspective, CRL provides sequential consistency for read and write operations in the same sense that a sequentially consistent hardware-based DSM does for individual loads and stores. In terms of individual loads and stores, CRL provides a memory/coherence model similar to entry <ref> [5] </ref> or release consistency [20]. Loads and stores to global data are allowed only within properly synchronized sections (operations), and modifications to a region are only made visible to other processors after the appropriate release operation (a call to rgn_end_write). <p> Annotations similar to those required by CRL are necessary in aggressive hardware DSM implementations (e.g., those providing release consistency) when writing to shared data. CRL requires such annotations whether reading or writing shared data, similar to entry consistency <ref> [5] </ref>. Based on our experience with the applications described in this paper, we feel that the additional programming overhead of doing so is minimal (see Table 4). Therefore, we believe CRL is an effective approach to providing a distributed shared memory abstraction. <p> In addition, the software overhead of systems like this (e.g., from manipulating virtual memory mechanisms and computing diffs) can be large enough to significantly impact delivered application performance [17]. Midway is a software DSM system based on entry consistency <ref> [5] </ref>. As discussed in Section 3.3, CRL's programming model is similar to that provided by Midway. An important difference, however, is that Midway requires a compiler that can cull user-provided annotations that relate data and synchronization objects from the source code and provide these to the Midway run-time system.
Reference: [6] <author> Nanette J. Boden, Danny Cohen, Robert E. Felderman, Alan E. Kulawik, Charles L. Seitz, Jakov N. Seizovic, and Wen-King Su. Myrinet: </author> <title> A Gigabit-per-Second Local Area Network. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 29-36, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: While this approach cuts the effective transfer bandwidth roughly in half, it provides significantly reduced latencies for small transfers by avoiding the need for prene-gotiation with the receiving node. Networks of workstations with interprocessor communication performance rivaling that of the CM-5 are rapidly becoming reality <ref> [6, 36, 47, 49] </ref>. For example, Thekkath et al. [48] describe the implementation of a specialized data-transfer mechanism implemented on a pair of 25 MHz DECstations connected with a FORE ATM network.
Reference: [7] <author> Martin C. Carlisle and Anne Rogers. </author> <title> Software Caching and Computation Migration in Olden. </title> <booktitle> In Proceedings of the Fifth Symposium on Principles and Practices of Parallel Programming, </booktitle> <pages> pages 29-38, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: Other types of DSM systems are also possible (e.g., those in which threads of computation are migrated to the data they reference <ref> [4, 7, 14, 23] </ref>). We classify systems by three basic mechanisms required to implement DSM and whether those mechanisms are implemented in hardware or software.
Reference: [8] <author> John B. Carter. </author> <title> Efficient Distributed Shared Memory Based On Multi-Protocol Release Consistency. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: Mostly Software Many software DSM systems are actually mostly software systems in which the hit/miss check functionality is implemented in hardware (e.g., by leveraging off of virtual memory protection mechanisms). Typical examples of mostly software systems include Ivy [34], Munin <ref> [8] </ref>, and Tread-Marks [25]; coherence units in these systems are the size of virtual memory pages. Blizzard [43] implements a similar scheme on the CM-5 at the granularity of individual cache lines. <p> In terms of comparing message passing and shared memory, most other previous work has either compared the performance of applications written and tuned specifically for each programming model <ref> [8, 13] </ref> or looked at the performance gains made possible by augmenting a hardware DSM system with message passing primitives [29].
Reference: [9] <author> David Chaiken, Craig Fields, Kiyoshi Kurihara, and Anant Agarwal. </author> <title> Directory-Based Cache-Coherence in Large-Scale Multiprocessors. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 41-58, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: This organization is motivated by studies indicating that small-scale sharing of data is the common case <ref> [9, 38, 51] </ref>; data shared more widely is relatively uncommon. In general, Alewife's shared memory system performs quite well, enabling speedups comparable to or better than other scalable hardware-based DSM systems [1, 33].
Reference: [10] <author> David L. Chaiken and Anant Agarwal. </author> <title> Software-Extended Coherent Shared Memory: Performance and Cost. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 314-324, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Mostly Hardware As discussed in Section 4, Alewife implements a mostly hardware DSM systemthe processor-side mechanisms are always implemented in hardware, but memory-side support is handled in software when widespread sharing is detected <ref> [10] </ref>. Dir 1 SW and its variations [21, 53] are also mostly hardware schemes. The Stanford FLASH multiprocessor [31] and Wisconsin Typhoon architecture [39] represent a different kind of mostly hardware DSM system. <p> Alewife provides efficient support for both coherent shared-memory and message-passing communication styles. Shared memory support is provided through an implementation of the LimitLESS cache coherence scheme <ref> [10] </ref>: limited sharing of memory blocks (up to five remote readers) is supported in hardware; higher-degree sharing is handled by trapping the processor on the home memory node and extending the small hardware directory in software. <p> vs. message passing) and the use of simulation to obtain controlled comparisons with cache-coherent hardware DSM (when such a comparison is provided). 7.3 Comparative Studies Several researchers have reported results comparing the performance of systems at adjacent levels of the classification presented in Section 2 (e.g., all-hardware vs. mostly hardware <ref> [10, 21, 53] </ref>, mostly software vs. all-software [43, 54]), but to our knowledge, only Cox et al. [15] have published results from a relatively controlled comparison of hardware and software DSM systems.
Reference: [11] <author> Rohit Chandra, Kourosh Gharachorloo, Vijayaraghavan Soundararajan, and Anoop Gupta. </author> <title> Performance Evaluation of Hybrid Hardware and Software Distributed Shared Memory Protocols. </title> <booktitle> In Proceedings of the Eighth International Conference on Supercomputing, </booktitle> <pages> pages 274-288, </pages> <month> July </month> <year> 1994. </year> <month> 14 </month>
Reference-contexts: CRL could be provided on such systems using similar implementation techniques and defining rgn_map and rgn_unmap to be null macros. Chandra et al. <ref> [11] </ref> propose a hybrid DSM protocol in which region-like annotations are used to demark access to regions of shared data. Coherence for regions thus annotated is provided using software DSM techniques analogous to those used by CRL; hardware DSM mechanisms are used for coherence on all other memory references.
Reference: [12] <author> Rohit Chandra, Anoop Gupta, and John L. Hennessy. </author> <title> Data Locality and Load Balancing in COOL. </title> <booktitle> In Proceedings of the Fourth Symposium on Principles and Practices of Parallel Programming, </booktitle> <pages> pages 249-259, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Except for the notion of mapping and unmapping regions, the programming interface CRL presents to the end user is similar to that provided by Shared Regions [41]; the same basic notion of synchronized access (operations) to regions (objects) also exists in other programming systems for hardware-based DSM systems (e.g., COOL <ref> [12] </ref>). The Shared Regions work arrived at this interface from a different set of constraints, however: their goal was to provide software coherence mechanisms on machines that support non-cache-coherent shared memory in hardware.
Reference: [13] <author> Satish Chandra, James R. Larus, and Anne Rogers. </author> <booktitle> Where is Time Spent in Message-Passing and Shared-Memory Programs? In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 61-73, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: In terms of comparing message passing and shared memory, most other previous work has either compared the performance of applications written and tuned specifically for each programming model <ref> [8, 13] </ref> or looked at the performance gains made possible by augmenting a hardware DSM system with message passing primitives [29].
Reference: [14] <author> Jeffrey S. Chase, Franz G. Amador, Edward D. Lazowska, Henry M. Levy, and Richard J. Littlefield. </author> <title> The Amber System: Parallel Programming on a Network of Multiprocessors. </title> <booktitle> In Proceedings of the Twelfth Symposium on Operating Systems Principles, </booktitle> <pages> pages 147-158, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Other types of DSM systems are also possible (e.g., those in which threads of computation are migrated to the data they reference <ref> [4, 7, 14, 23] </ref>). We classify systems by three basic mechanisms required to implement DSM and whether those mechanisms are implemented in hardware or software. <p> CRL runs on two large-scale platforms and has been shown to deliver performance competitive with hardware DSM systems. Several all-software DSM systems that employ an object-based approach have been developed (e.g., Amber <ref> [14] </ref>, Orca [3], Concert [24]). Like these systems, CRL effects coherence at the level of application-defined regions of memory (objects). Any necessary synchronization, data replication, or thread migration functionality is provided automatically at the entry and exit of methods on shared objects. <p> Existing systems of this type either require the use of an entirely new object-oriented language [3, 24] or only allow the use of a subset of an existing one <ref> [14] </ref>. In contrast, CRL is not language specific; the basic CRL interface could easily be provided in any imperative programming language. Scales and Lam [42] have described SAM, a shared object system for distributed memory machines.
Reference: [15] <author> Alan L. Cox, Sandhya Dwarkadas, Pete Keleher, Honghui Lu, Ramakrishnan Rajamony, and Willy Zwaenepoel. </author> <title> Software Versus Hardware Shared-Memory Implementation: A Case Study. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 106-117, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: plotting the performance of the CM-5 CRL version of Barnes-Hut on up to 128 processors for both the 4,096 body problem size discussed above and the suggested problem size of 16,384 2 The SPLASH-2 version of Water used in this paper incorporates the M-Water modifications suggested by Cox et al. <ref> [15] </ref>. 11 version of Barnes-Hut (4,096 bodies). bodies. For the larger machine sizes (64, 96, and 128 processors), the increased problem size enables speedups 40 to 70 percent better than those for 4,096 bodies. <p> such a comparison is provided). 7.3 Comparative Studies Several researchers have reported results comparing the performance of systems at adjacent levels of the classification presented in Section 2 (e.g., all-hardware vs. mostly hardware [10, 21, 53], mostly software vs. all-software [43, 54]), but to our knowledge, only Cox et al. <ref> [15] </ref> have published results from a relatively controlled comparison of hardware and software DSM systems.
Reference: [16] <author> Alan L. Cox and Robert J. Fowler. </author> <title> The Implementation of a Coherent Memory Abstraction on a NUMA Multiprocessor: Experiences with PLATINUM. </title> <booktitle> In Proceedings of the Twelfth Symposium on Operating Systems Principles, </booktitle> <pages> pages 32-44, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: To the best of our knowledge, Midway has only been implemented on a small cluster of workstations connected with an ATM network. A number of other approaches to providing coherence in software on top of non-cache-coherent shared-memory hardware have also been explored <ref> [16, 28] </ref>.
Reference: [17] <author> Sandhya Dwarkadas, Pete Keleher, Alan L. Cox, and Willy Zwaenepoel. </author> <title> Evaluation of Release Consistent Software Distributed Shared Memory on Emerging Network Technology. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 144-155, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: In addition, the software overhead of systems like this (e.g., from manipulating virtual memory mechanisms and computing diffs) can be large enough to significantly impact delivered application performance <ref> [17] </ref>. Midway is a software DSM system based on entry consistency [5]. As discussed in Section 3.3, CRL's programming model is similar to that provided by Midway.
Reference: [18] <author> Dawson R. Engler, M. Frans Kaashoek, and James O'Toole Jr. Exokernel: </author> <title> An Operating System Architecture for Application-Level Resource Management. </title> <booktitle> In Proceedings of the Fifteenth Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: We believe that the value of doing so will probably be greatly enhanced by operating system structures that allow low-overhead, flexible access to these mechanisms from user level <ref> [18] </ref>. Second, the results presented in this paper reiterate the need for network interfaces that provide not only low latency but also high bandwidth.
Reference: [19] <author> A. Geist, A. Beguelin, J. J. Dongarra, W. Jiang, R. Manchek, and V. S. Sunderam. </author> <title> PVM 3 User's Guide and Reference Manual. </title> <type> Technical Report ORNL/TM-12187, </type> <institution> Oak Ridge National Laboratory, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: In spite of this fact, message passing environments such as PVM (Parallel Virtual Machine) <ref> [19] </ref> and MPI (Message Passing Interface) [35] are often the de facto standards for programming multicomputers and networks of workstations. <p> Our current CRL implementation and CRL versions of several applications are available on the World Wide Web at URL http://www.pdos.lcs.mit.edu/crl/. In addition to the platforms described in this paper (Alewife and CM-5), this distribution can also be compiled for use with PVM <ref> [19] </ref> on a network of Sun workstations communicating with one another using TCP. 4 Experimental Platforms This section describes the two platforms that were used for the experiments described in this paper: Thinking Machines' CM-5 family of multiprocessors and the MIT Alewife machine. 4.1 CM-5 The CM-5 [32] is a commercially
Reference: [20] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: From this perspective, CRL provides sequential consistency for read and write operations in the same sense that a sequentially consistent hardware-based DSM does for individual loads and stores. In terms of individual loads and stores, CRL provides a memory/coherence model similar to entry [5] or release consistency <ref> [20] </ref>. Loads and stores to global data are allowed only within properly synchronized sections (operations), and modifications to a region are only made visible to other processors after the appropriate release operation (a call to rgn_end_write).
Reference: [21] <author> Mark D. Hill, James R. Larus, Steven K. Reinhardt, and David A. Wood. </author> <title> Cooperative Shared Memory: Software and Hardware for Scalable Multiprocessors. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 262-273, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Mostly Hardware As discussed in Section 4, Alewife implements a mostly hardware DSM systemthe processor-side mechanisms are always implemented in hardware, but memory-side support is handled in software when widespread sharing is detected [10]. Dir 1 SW and its variations <ref> [21, 53] </ref> are also mostly hardware schemes. The Stanford FLASH multiprocessor [31] and Wisconsin Typhoon architecture [39] represent a different kind of mostly hardware DSM system. <p> vs. message passing) and the use of simulation to obtain controlled comparisons with cache-coherent hardware DSM (when such a comparison is provided). 7.3 Comparative Studies Several researchers have reported results comparing the performance of systems at adjacent levels of the classification presented in Section 2 (e.g., all-hardware vs. mostly hardware <ref> [10, 21, 53] </ref>, mostly software vs. all-software [43, 54]), but to our knowledge, only Cox et al. [15] have published results from a relatively controlled comparison of hardware and software DSM systems.
Reference: [22] <author> Wilson C. Hsieh. </author> <title> Dynamic Computation Migration in Distributed Shared Memory Systems. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science, </institution> <year> 1995. </year>
Reference-contexts: Interprocessor synchronization can be effected through region operations, barriers, broadcasts, and reductions. Many shared memory applications (e.g., the SPLASH application suites [46, 52]) are written in this style. Although an experimental version of CRL that supports multiple user threads per processor and migration of threads between processors is operational <ref> [22] </ref>, all results reported in this paper were obtained using the single-threaded version. 3.3 Memory/Coherence Model The simplest explanation of the coherence model provided by CRL considers entire operations on regions as indivisible units.
Reference: [23] <author> Wilson C. Hsieh, Paul Wang, and William E. Weihl. </author> <title> Computation Migration: Enhancing Locality for Distributed-Memory Parallel Systems. </title> <booktitle> In Proceedings of the Fourth Symposium on Principles and Practice of Parallel Programming (PPoPP), </booktitle> <pages> pages 239-248, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Other types of DSM systems are also possible (e.g., those in which threads of computation are migrated to the data they reference <ref> [4, 7, 14, 23] </ref>). We classify systems by three basic mechanisms required to implement DSM and whether those mechanisms are implemented in hardware or software.
Reference: [24] <author> Vijay Karamcheti and Andrew Chien. </author> <title> Concert Efficient Runtime Support for Concurrent Object-Oriented Programming Languages on Stock Hardware. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 598-607, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: CRL runs on two large-scale platforms and has been shown to deliver performance competitive with hardware DSM systems. Several all-software DSM systems that employ an object-based approach have been developed (e.g., Amber [14], Orca [3], Concert <ref> [24] </ref>). Like these systems, CRL effects coherence at the level of application-defined regions of memory (objects). Any necessary synchronization, data replication, or thread migration functionality is provided automatically at the entry and exit of methods on shared objects. <p> Any necessary synchronization, data replication, or thread migration functionality is provided automatically at the entry and exit of methods on shared objects. Existing systems of this type either require the use of an entirely new object-oriented language <ref> [3, 24] </ref> or only allow the use of a subset of an existing one [14]. In contrast, CRL is not language specific; the basic CRL interface could easily be provided in any imperative programming language. Scales and Lam [42] have described SAM, a shared object system for distributed memory machines.
Reference: [25] <author> Pete Keleher, Sandhya Dwarkadas, Alan Cox, and Willy Zwaenepoel. TreadMarks: </author> <title> Distributed Shared Memory on Standard Workstations and Operating Systems. </title> <booktitle> In Proceedings of the 1994 Winter Usenix Conference, </booktitle> <pages> pages 115-131, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: Mostly Software Many software DSM systems are actually mostly software systems in which the hit/miss check functionality is implemented in hardware (e.g., by leveraging off of virtual memory protection mechanisms). Typical examples of mostly software systems include Ivy [34], Munin [8], and Tread-Marks <ref> [25] </ref>; coherence units in these systems are the size of virtual memory pages. Blizzard [43] implements a similar scheme on the CM-5 at the granularity of individual cache lines. <p> In spite of this performance gap, CM-5 CRL performs comparably with existing mostly software DSM systems. The CM-5 CRL speedup (5.3 on eight processors) for Water (without reductions) is slightly better than that reported for TreadMarks <ref> [25] </ref>, a second-generation page-based mostly software DSM system (a speedup of 4.0 on an ATM network of DECstation 5000/240 workstations, the largest configuration that results have been reported for) 2 . <p> SAM's performance edge comes at a cost, however: Because the primitives SAM offers are significantly different than standard shared memory models, converting existing shared-memory applications to use SAM is likely to be more difficult than converting them to use CRL. 7.2 Other Software DSM Systems TreadMarks <ref> [25] </ref> is a second-generation page-based (mostly software) DSM system that implements a release consistent memory model. Unlike many page-based systems, TreadMarks is implemented entirely in user space; virtual memory protection mechanisms are manipulated through library wrappers around system calls into the kernel.
Reference: [26] <institution> Kendall Square Research. </institution> <type> KSR-1 Technical Summary, </type> <year> 1992. </year>
Reference-contexts: Using these three characteristics, we obtain the following breakdown of the spectrum of DSM implementation techniques that have been discussed in the literature. All-Hardware In all-hardware DSM systems, all three of these mechanisms are implemented in specialized hardware; the Stan-ford DASH multiprocessor [33] and KSR-1 <ref> [26] </ref> are typical all hardware systems. Mostly Hardware As discussed in Section 4, Alewife implements a mostly hardware DSM systemthe processor-side mechanisms are always implemented in hardware, but memory-side support is handled in software when widespread sharing is detected [10].
Reference: [27] <author> Alexander C. Klaiber and Henry M. Levy. </author> <title> A Comparison of Message Passing and Shared Memory Architectures for Data Parallel Programs. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 94-105, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Experimental results comparing hardware and software DSM performance are shown for up to 32 processors (Alewife); software DSM results are shown for up to 128 processors (CM-5). 13 Klaiber and Levy <ref> [27] </ref> describe a set of experiments in which data-parallel (C*) applications are compiled such that all interpro-cessor communication is provided through a very simple library interface.
Reference: [28] <author> Leonidas I. Kontothanassis and Michael L. Scott. </author> <title> Software Cache Coherence for Large Scale Multiprocessors. </title> <booktitle> In Proceedings of the First Symposium on High-Performance Computer Architecture, </booktitle> <pages> pages 286-295, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: To the best of our knowledge, Midway has only been implemented on a small cluster of workstations connected with an ATM network. A number of other approaches to providing coherence in software on top of non-cache-coherent shared-memory hardware have also been explored <ref> [16, 28] </ref>.
Reference: [29] <author> David Kranz, Kirk Johnson, Anant Agarwal, John Kubia-towicz, and Beng-Hong Lim. </author> <title> Integrating Message-Passing and Shared-Memory: Early Experience. </title> <booktitle> In Proceedings of the Fourth Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 54-63, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: CRL is the first all-software DSM system to deliver performance competitive with hardware DSMs. To demonstrate this fact, we ported our CRL implementation to the MIT Alewife machine [1]. Since Alewife provides efficient hardware support for both message passing and shared memory communication styles <ref> [29] </ref>, the perfor Appears in Proceedings of the Fifteenth Symposium on Operating Systems Principles, December 1995. mance of applications running under CRL (using only message passing for communication) can be readily compared to the performance of the same applications when hardware-supported shared memory is used instead. <p> In terms of comparing message passing and shared memory, most other previous work has either compared the performance of applications written and tuned specifically for each programming model [8, 13] or looked at the performance gains made possible by augmenting a hardware DSM system with message passing primitives <ref> [29] </ref>. Such research addresses a different set of issues than those discussed in this paper, which takes a distributed shared memory programming model as a given and provides a controlled comparison of hardware and software implementations.
Reference: [30] <author> John Kubiatowicz and Anant Agarwal. </author> <title> Anatomy of a Message in the Alewife Multiprocessor. </title> <booktitle> In Proceedings of the International Conference on Supercomputing, </booktitle> <pages> pages 195-206, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: In general, Alewife's shared memory system performs quite well, enabling speedups comparable to or better than other scalable hardware-based DSM systems [1, 33]. In addition to providing support for coherent shared memory, Alewife provides the processor with direct access to the interconnection network for sending and receiving messages <ref> [30] </ref>. Efficient mechanisms are provided for sending and receiving both short (register-to-register) and long (memory-to-memory, block transfer) messages. Using Alewife's message-passing mechanisms, a processor can send a message with just a few user-level instructions.
Reference: [31] <author> Jeffrey Kuskin, David Ofelt, Mark Heinrich, John Heinlein, Richard Simoni, Kourosh Gharachorloo, John Chapin, David Nakahira, Joel Baxter, Mark Horowitz, Anoop Gupta, Mendel Rosenblum, and John Hennessy. </author> <title> The Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 302-313, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Dir 1 SW and its variations [21, 53] are also mostly hardware schemes. The Stanford FLASH multiprocessor <ref> [31] </ref> and Wisconsin Typhoon architecture [39] represent a different kind of mostly hardware DSM system.
Reference: [32] <author> Charles E. Leiserson, Zahi S. Abuhamdeh, David C. Dou-glas, Carl R. Feynman, Mahesh N. Ganmukhi, Jeffrey V. Hill, W. Daniel Hillis, Bradley C. Kuszmaul, Margaret A. St. Pierre, David S. Wells, Monica C. Wong, Shaw-Wen Yang, and Robert Zak. </author> <title> The Network Architecture of the Connection Machine CM-5. </title> <booktitle> In Proceedings of the Fourth Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 272-285, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: use with PVM [19] on a network of Sun workstations communicating with one another using TCP. 4 Experimental Platforms This section describes the two platforms that were used for the experiments described in this paper: Thinking Machines' CM-5 family of multiprocessors and the MIT Alewife machine. 4.1 CM-5 The CM-5 <ref> [32] </ref> is a commercially available message-passing mul-ticomputer with relatively efficient support for low-overhead, fine-grained message passing. The experiments described in this paper were run on a 128-node CM-5 system running version 7.4 Final of the CMOST operating system and version 3.3 of the CMMD message-passing library.
Reference: [33] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. Lam. </author> <title> The Stanford Dash Multiprocessor. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Using these three characteristics, we obtain the following breakdown of the spectrum of DSM implementation techniques that have been discussed in the literature. All-Hardware In all-hardware DSM systems, all three of these mechanisms are implemented in specialized hardware; the Stan-ford DASH multiprocessor <ref> [33] </ref> and KSR-1 [26] are typical all hardware systems. Mostly Hardware As discussed in Section 4, Alewife implements a mostly hardware DSM systemthe processor-side mechanisms are always implemented in hardware, but memory-side support is handled in software when widespread sharing is detected [10]. <p> This organization is motivated by studies indicating that small-scale sharing of data is the common case [9, 38, 51]; data shared more widely is relatively uncommon. In general, Alewife's shared memory system performs quite well, enabling speedups comparable to or better than other scalable hardware-based DSM systems <ref> [1, 33] </ref>. In addition to providing support for coherent shared memory, Alewife provides the processor with direct access to the interconnection network for sending and receiving messages [30]. Efficient mechanisms are provided for sending and receiving both short (register-to-register) and long (memory-to-memory, block transfer) messages.
Reference: [34] <author> Kai Li. IVY: </author> <title> A Shared Virtual Memory System for Parallel Computing. </title> <booktitle> In Proceedings of the International Conference on Parallel Computing, </booktitle> <pages> pages 94-101, </pages> <year> 1988. </year>
Reference-contexts: Mostly Software Many software DSM systems are actually mostly software systems in which the hit/miss check functionality is implemented in hardware (e.g., by leveraging off of virtual memory protection mechanisms). Typical examples of mostly software systems include Ivy <ref> [34] </ref>, Munin [8], and Tread-Marks [25]; coherence units in these systems are the size of virtual memory pages. Blizzard [43] implements a similar scheme on the CM-5 at the granularity of individual cache lines.
Reference: [35] <author> Message Passing Interface Forum. </author> <title> MPI: A Message-Passing Interface Standard, </title> <month> May </month> <year> 1994. </year>
Reference-contexts: In spite of this fact, message passing environments such as PVM (Parallel Virtual Machine) [19] and MPI (Message Passing Interface) <ref> [35] </ref> are often the de facto standards for programming multicomputers and networks of workstations. We believe that this is primarily due to the fact that these systems require no special hardware, compiler, or operating system support, thus enabling them to run entirely at user level on unmodified, stock systems.
Reference: [36] <author> Ron Minnich, Dan Burns, and Frank Hady. </author> <title> The Memory-Integrated Network Interface. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 11-20, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: While this approach cuts the effective transfer bandwidth roughly in half, it provides significantly reduced latencies for small transfers by avoiding the need for prene-gotiation with the receiving node. Networks of workstations with interprocessor communication performance rivaling that of the CM-5 are rapidly becoming reality <ref> [6, 36, 47, 49] </ref>. For example, Thekkath et al. [48] describe the implementation of a specialized data-transfer mechanism implemented on a pair of 25 MHz DECstations connected with a FORE ATM network.
Reference: [37] <author> Rishiyur S. Nikhil. Cid: </author> <title> A Parallel, Shared-memory C for Distributed-Memory Machines. </title> <booktitle> In Proceedings of the Seventh Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1994. </year> <month> 15 </month>
Reference-contexts: All synchronization must be effected through hardware DSM mechanisms. In contrast, CRL is an all-software DSM system in which all communication and synchronization is implemented using software DSM techniques. Of all other software DSM systems, Cid <ref> [37] </ref> is perhaps closest in spirit to CRL. Like CRL, Cid is an all-software DSM system in which coherence is effected on regions (global objects) according to source code annotations provided by the programmer.
Reference: [38] <author> Brian W. O'Krafka and A. Richard Newton. </author> <title> An Empirical Evaluation of Two Memory-Efficient Directory Methods. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 138-147, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: This organization is motivated by studies indicating that small-scale sharing of data is the common case <ref> [9, 38, 51] </ref>; data shared more widely is relatively uncommon. In general, Alewife's shared memory system performs quite well, enabling speedups comparable to or better than other scalable hardware-based DSM systems [1, 33].
Reference: [39] <author> Steve K. Reinhardt, James R. Larus, and David A. Wood. Tempest and Typhoon: </author> <title> User-Level Shared Memory. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 325-336, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Dir 1 SW and its variations [21, 53] are also mostly hardware schemes. The Stanford FLASH multiprocessor [31] and Wisconsin Typhoon architecture <ref> [39] </ref> represent a different kind of mostly hardware DSM system.
Reference: [40] <author> Edward Rothberg, Jaswinder Pal Singh, and Anoop Gupta. </author> <title> Working Sets, Cache Sizes, and Node Granularity Issues for Large-Scale Multiprocessors. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 14-25, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: None of the applications employ any prefetching. 5.2.1 Blocked LU Blocked LU implements LU factorization of a dense matrix; the version used in this study is based on one described by Rothberg et al. <ref> [40] </ref>. The results presented in this section are for a 500x500 matrix using 10x10 blocks.
Reference: [41] <author> Harjinder S. Sandhu, Benjamin Gamsa, and Songnian Zhou. </author> <title> The Shared Regions Approach to Software Cache Coherence on Multiprocessors. </title> <booktitle> In Proceedings of the Fourth Symposium on Principles and Practices of Parallel Programming, </booktitle> <pages> pages 229-238, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: third subsection discusses other research that provides comparative results (e.g., all-software vs. mostly software, message passing vs. shared memory). 7.1 Region- and Object-Based Systems Except for the notion of mapping and unmapping regions, the programming interface CRL presents to the end user is similar to that provided by Shared Regions <ref> [41] </ref>; the same basic notion of synchronized access (operations) to regions (objects) also exists in other programming systems for hardware-based DSM systems (e.g., COOL [12]).
Reference: [42] <author> Daniel J. Scales and Monica S. Lam. </author> <title> The Design and Evaluation of a Shared Object System for Distributed Memory Machines. </title> <booktitle> In Proceedings of the First USENIX Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 101-114, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: In contrast, CRL is not language specific; the basic CRL interface could easily be provided in any imperative programming language. Scales and Lam <ref> [42] </ref> have described SAM, a shared object system for distributed memory machines. SAM is based on a new set of primitives that are motivated by optimizations commonly used on distributed memory machines. Like CRL, SAM is implemented as a portable C library.
Reference: [43] <author> Ioannis Schoinas, Babak Falsafi, Alvin R. Lebeck, Steven K. Reinhardt, James R. Larus, and David A. Wood. </author> <title> Fine-grain Access Control for Distributed Shared Memory. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 297-306, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Typical examples of mostly software systems include Ivy [34], Munin [8], and Tread-Marks [25]; coherence units in these systems are the size of virtual memory pages. Blizzard <ref> [43] </ref> implements a similar scheme on the CM-5 at the granularity of individual cache lines. By manipulating the error correcting code bits associated with every memory block, Blizzard can control access on a cache-line by cache-line basis. <p> All-Software In an all-software DSM system, all three of the mechanisms identified above are implemented entirely in software (e.g., Orca [3]). Several researchers have recently reported on experiences with all-software DSM systems obtained by modifying mostly software DSM systems such that the hit/miss check functionality is provided in software <ref> [43, 54] </ref>. Generally speaking, increased use of software to provide shared-memory functionality tends to decrease application performance because processor cycles spent implementing memory system functionality might otherwise have been spent in application code. <p> simulation to obtain controlled comparisons with cache-coherent hardware DSM (when such a comparison is provided). 7.3 Comparative Studies Several researchers have reported results comparing the performance of systems at adjacent levels of the classification presented in Section 2 (e.g., all-hardware vs. mostly hardware [10, 21, 53], mostly software vs. all-software <ref> [43, 54] </ref>), but to our knowledge, only Cox et al. [15] have published results from a relatively controlled comparison of hardware and software DSM systems. <p> Such research addresses a different set of issues than those discussed in this paper, which takes a distributed shared memory programming model as a given and provides a controlled comparison of hardware and software implementations. Finally, Schoinas et al. <ref> [43] </ref> describe a taxonomy of shared-memory systems that is similar in spirit to that provided in Section 2.
Reference: [44] <author> Jaswinder Pal Singh, Anoop Gupta, and John L. Hennessy. </author> <title> Implications of Hierarchical N-Body Techniques for Multiprocessor Architecture. </title> <journal> ACM Transactions on Computer Systems, </journal> <pages> pages 141-202, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: be managed without adversely affecting performance for relatively simple applications (e.g., those that communicate infrequently or have relatively simple communication patterns), the task can be far more difficult for large, complex applications, particularly those in which data is shared at a fine granularity or according to irregular, dynamic communication patterns <ref> [44, 45] </ref>. In spite of this fact, message passing environments such as PVM (Parallel Virtual Machine) [19] and MPI (Message Passing Interface) [35] are often the de facto standards for programming multicomputers and networks of workstations. <p> In fact, Barnes-Hut and related hierarchical n-body methods present a challenging enough communication workload that they have been used by some authors as the basis of an argument in favor of aggressive hardware support for cache-coherent shared memory <ref> [44, 45] </ref>. 5.2.4 Performance Table 5 summarizes the running times for the sequential, CRL, and shared memory (SM) versions of the three applications.
Reference: [45] <author> Jaswinder Pal Singh, Anoop Gupta, and Marc Levoy. </author> <title> Parallel Visualization Algorithms: Performance and Architectural Implications. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 45-55, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: be managed without adversely affecting performance for relatively simple applications (e.g., those that communicate infrequently or have relatively simple communication patterns), the task can be far more difficult for large, complex applications, particularly those in which data is shared at a fine granularity or according to irregular, dynamic communication patterns <ref> [44, 45] </ref>. In spite of this fact, message passing environments such as PVM (Parallel Virtual Machine) [19] and MPI (Message Passing Interface) [35] are often the de facto standards for programming multicomputers and networks of workstations. <p> In fact, Barnes-Hut and related hierarchical n-body methods present a challenging enough communication workload that they have been used by some authors as the basis of an argument in favor of aggressive hardware support for cache-coherent shared memory <ref> [44, 45] </ref>. 5.2.4 Performance Table 5 summarizes the running times for the sequential, CRL, and shared memory (SM) versions of the three applications.
Reference: [46] <author> Jaswinder Pal Singh, Wolf-Dietrich Weber, and Anoop Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title> <booktitle> Computer Architecture News, </booktitle> <pages> pages 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Interprocessor synchronization can be effected through region operations, barriers, broadcasts, and reductions. Many shared memory applications (e.g., the SPLASH application suites <ref> [46, 52] </ref>) are written in this style. <p> The results presented in this section are for a problem size of 4,096 bodies (one-quarter of the suggested base problem size). Other application parameters (Dt and ) are scaled appropriately for the smaller problem size <ref> [46] </ref>. In the CRL version of the code, a region is created for each of the octree data structure elements in the original code: bodies (108 bytes), tree cells (88 bytes), and tree leaves (100 bytes).
Reference: [47] <author> Chandramohan A. Thekkath and Henry M. Levy. </author> <title> Limits to Low-Latency Communication on High-Speed Networks. </title> <journal> ACM Transactions on Computer Systems, </journal> <pages> pages 179-203, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: While this approach cuts the effective transfer bandwidth roughly in half, it provides significantly reduced latencies for small transfers by avoiding the need for prene-gotiation with the receiving node. Networks of workstations with interprocessor communication performance rivaling that of the CM-5 are rapidly becoming reality <ref> [6, 36, 47, 49] </ref>. For example, Thekkath et al. [48] describe the implementation of a specialized data-transfer mechanism implemented on a pair of 25 MHz DECstations connected with a FORE ATM network.
Reference: [48] <author> Chandramohan A. Thekkath, Henry M. Levy, and Edward D. Lazowska. </author> <title> Separating Data and Control Transfer in Distributed Operating Systems. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languagesand Operating Systems, </booktitle> <pages> pages 2-11, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Networks of workstations with interprocessor communication performance rivaling that of the CM-5 are rapidly becoming reality [6, 36, 47, 49]. For example, Thekkath et al. <ref> [48] </ref> describe the implementation of a specialized data-transfer mechanism implemented on a pair of 25 MHz DECstations connected with a FORE ATM network. <p> To gauge the sensitivity of CRL's performance to increased communication costs, we compare the behavior of applications running under Alewife CRL and CM-5 CRL. Although the CM-5 is a tightly-coupled multiprocessor, current-generation network-of-workstations technology is capable of providing similar communication performance <ref> [48] </ref>, so we believe that the results for CM-5 CRL are indicative of what should be possible for implementations targeting networks of workstations using current- or next-generation technology.
Reference: [49] <author> Thorsten von Eicken, Anindya Basu, </author> <title> and Vineet Buch. Low-Latency Communication Over ATM Networks Using Active Messages. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 46-53, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: While this approach cuts the effective transfer bandwidth roughly in half, it provides significantly reduced latencies for small transfers by avoiding the need for prene-gotiation with the receiving node. Networks of workstations with interprocessor communication performance rivaling that of the CM-5 are rapidly becoming reality <ref> [6, 36, 47, 49] </ref>. For example, Thekkath et al. [48] describe the implementation of a specialized data-transfer mechanism implemented on a pair of 25 MHz DECstations connected with a FORE ATM network.
Reference: [50] <author> Thorsten von Eicken, David E. Culler, Seth Copen Goldstein, and Klaus Erik Schauser. </author> <title> Active Messages: A Mechanism for Integrated Communication and Computation. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 256-266, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: In both cases, all communication is effected using active messages <ref> [50] </ref>. CRL is implemented as a library against which user programs are linked; it is written entirely in C. Both CM-5 and Alewife versions can be compiled from a single set of sources with conditionally compiled sections to handle machine-specific details (e.g., different message-passing interfaces).
Reference: [51] <author> Wolf-Dietrich Weber and Anoop Gupta. </author> <title> Analysis of Cache Invalidation Patterns in Multiprocessors. </title> <booktitle> In Proceedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 243-256, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: This organization is motivated by studies indicating that small-scale sharing of data is the common case <ref> [9, 38, 51] </ref>; data shared more widely is relatively uncommon. In general, Alewife's shared memory system performs quite well, enabling speedups comparable to or better than other scalable hardware-based DSM systems [1, 33].
Reference: [52] <author> Steven Cameron Woo, Moriyoshi Ohara, Evan Torrie, Jaswinder Pal Singh, and Anoop Gupta. </author> <title> The SPLASH-2 Programs: Characterization and Methodological Considerations. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 24-36, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Interprocessor synchronization can be effected through region operations, barriers, broadcasts, and reductions. Many shared memory applications (e.g., the SPLASH application suites <ref> [46, 52] </ref>) are written in this style. <p> CRL currently uses a fully-associative, fixed-size URC with 1024 entries. 3.4.3 Status Our CRL implementation has been operational for several months. We have used it to run a handful of shared-memory-style applications, including two from the SPLASH-2 suite <ref> [52] </ref>, on a 32-node Alewife system and CM-5 systems with up to 128 processors.
Reference: [53] <author> David A. Wood, Satish Chandra, Babak Falsafi, Mark D. Hill, James R. Larus, Alvin R. Lebeck, James C. Lewis, Shub-hendu S. Mukherjee, Subbarao Palacharla, and Steven K. Reinhardt. </author> <title> Mechanisms for Cooperative Shared Memory. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 156-167, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Mostly Hardware As discussed in Section 4, Alewife implements a mostly hardware DSM systemthe processor-side mechanisms are always implemented in hardware, but memory-side support is handled in software when widespread sharing is detected [10]. Dir 1 SW and its variations <ref> [21, 53] </ref> are also mostly hardware schemes. The Stanford FLASH multiprocessor [31] and Wisconsin Typhoon architecture [39] represent a different kind of mostly hardware DSM system. <p> vs. message passing) and the use of simulation to obtain controlled comparisons with cache-coherent hardware DSM (when such a comparison is provided). 7.3 Comparative Studies Several researchers have reported results comparing the performance of systems at adjacent levels of the classification presented in Section 2 (e.g., all-hardware vs. mostly hardware <ref> [10, 21, 53] </ref>, mostly software vs. all-software [43, 54]), but to our knowledge, only Cox et al. [15] have published results from a relatively controlled comparison of hardware and software DSM systems.
Reference: [54] <author> Matthew J. Zekauskas, Wayne A. Sawdon, and Brian N. Ber-shad. </author> <title> Software Write Detection for a Distributed Shared Memory. </title> <booktitle> In Proceedings of the First USENIX Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 87-100, </pages> <month> November </month> <year> 1994. </year> <month> 16 </month>
Reference-contexts: All-Software In an all-software DSM system, all three of the mechanisms identified above are implemented entirely in software (e.g., Orca [3]). Several researchers have recently reported on experiences with all-software DSM systems obtained by modifying mostly software DSM systems such that the hit/miss check functionality is provided in software <ref> [43, 54] </ref>. Generally speaking, increased use of software to provide shared-memory functionality tends to decrease application performance because processor cycles spent implementing memory system functionality might otherwise have been spent in application code. <p> Systems that take advantage of more complex hardware or operating system functionality (e.g., page-based mostly software DSM systems) can suffer a performance penalty because of inefficient interfaces for accessing such features <ref> [54] </ref>. Finally, we wanted a system that would be amenable to simple and lean implementations in which only a small amount of software overhead sits between applications and the message-passing infrastructure used for communication. 3.2 Programming Model In the CRL programming model, communication is effected through operations on regions. <p> By bundling an implicit synchronization object with every region, CRL obviates the need for special compiler support of this sort. Both mostly software and all-software versions of Midway have been implemented <ref> [54] </ref>. To the best of our knowledge, Midway has only been implemented on a small cluster of workstations connected with an ATM network. A number of other approaches to providing coherence in software on top of non-cache-coherent shared-memory hardware have also been explored [16, 28]. <p> simulation to obtain controlled comparisons with cache-coherent hardware DSM (when such a comparison is provided). 7.3 Comparative Studies Several researchers have reported results comparing the performance of systems at adjacent levels of the classification presented in Section 2 (e.g., all-hardware vs. mostly hardware [10, 21, 53], mostly software vs. all-software <ref> [43, 54] </ref>), but to our knowledge, only Cox et al. [15] have published results from a relatively controlled comparison of hardware and software DSM systems.
References-found: 54

