URL: ftp://borneo.gmd.de/pub/as/ga/gmd_as_ga-94_09.ps
Refering-URL: http://www.cs.bham.ac.uk/~wbl/biblio/gp-bibliography.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: E-mail: zhang@gmd.de, muehlenbein@gmd.de  
Title: Balancing Accuracy and Parsimony in Genetic Programming 1  
Author: Byoung-Tak Zhang Heinz Muhlenbein 
Keyword: Machine learning, Tree induction, Genetic programming, Minimum description length principle, Bayesian model comparison, Evolving neural networks.  
Address: Schloss Birlinghoven, D-53754 Sankt Augustin, Germany  
Affiliation: German National Research Center for Computer Science (GMD)  
Abstract: Genetic programming is distinguished from other evolutionary algorithms in that it uses tree representations of variable size instead of linear strings of fixed length. The flexible representation scheme is very important because it allows the underlying structure of the data to be discovered automatically. One primary difficulty, however, is that the solutions may grow too big without any improvement of their generalization ability. In this paper we investigate the fundamental relationship between the performance and complexity of the evolved structures. The essence of the parsimony problem is demonstrated empirically by analyzing error landscapes of programs evolved for neural network synthesis. We consider genetic programming as a statistical inference problem and apply the Bayesian model-comparison framework to introduce a class of fitness functions with error and complexity terms. An adaptive learning method is then presented that automatically balances the model-complexity factor to evolve parsimonious programs without losing the diversity of the population needed for achieving the desired training accuracy. The effectiveness of this approach is empirically shown on the induction of sigma-pi neural networks for solving a real-world medical diagnosis problem as well as benchmark tasks. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Amari, S.-I. </author> <year> (1991). </year> <title> Dualistic geometry of the manifold of higher-order neurons. </title> <booktitle> Neural Networks, </booktitle> <volume> 4 </volume> <pages> 443-451. </pages>
Reference-contexts: Another problem in using pi units is the combinatorial explosion of the number of terms <ref> (Amari, 1991) </ref>. The number of parameters necessary for specifying an order k neuron is r k = P k i=0 n C i , where n is the total number of inputs and n C m are the binomial coefficients.
Reference: <author> Angeline, P. J. and Pollack, J. B. </author> <year> (1993). </year> <title> Competitive environments evolve better solutions for complex tasks. </title> <editor> In Forrest, S., editor, </editor> <booktitle> Proceedings of the Fifth International Conference on Genetic Algorithms (ICGA-93), </booktitle> <pages> pages 264-270. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo. </address> <note> 30 Back, </note> <editor> T. and Schwefel, H.-P. </editor> <year> (1993). </year> <title> An overview of evolutionary algorithms for parameter opti-mization. </title> <journal> Evolutionary Computation, </journal> <volume> 1(1) </volume> <pages> 1-23. </pages>
Reference-contexts: Though other MDL-based tree induction methods also reward parsimony, the adaptive Occam approach is different in that it dynamically balances error and complexity costs. While proposed in a different context, the adaptive fitness function presented in this paper has some similarity in spirit to competitive fitness functions <ref> (Angeline and Pollack, 1993) </ref>. Standard fitness functions return the same fitness for an individual regardless of what other members are present in the population, demanding an accurate and consistent fitness measure throughout the evolutionary process.
Reference: <author> Blumer, A., Ehrenfeucht, A., Haussler, D., and Warmuth, M. K. </author> <year> (1987). </year> <title> Occam's Razor. </title> <journal> Information Processing Letters, </journal> <volume> 24 </volume> <pages> 377-380. </pages>
Reference: <author> Breiman, L., Friedman, J. H., Olshen, R. A., and Stone, C. J. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <publisher> Wadsworth Int. Group, </publisher> <address> Belmont, C.A. </address>
Reference-contexts: Compared with the standard method, the adaptive Occam method converged more than three times faster for this problem. 7 Discussion The minimum description length principle has also been used in other tree-based learning algorithms such as CART <ref> (Breiman et al., 1984) </ref> and ID3 (Quinlan and Rivest, 1989). For the induction of parsimonious decision trees, both CART and ID3 use a two step process.
Reference: <author> Carbonell, J. G. </author> <year> (1990). </year> <title> Introduction: Paradigms for machine learning. </title> <booktitle> In Machine Learning: Paradigms and Methods, </booktitle> <pages> pages 1-9. </pages> <publisher> MIT Press, </publisher> <address> Cambridge. </address>
Reference-contexts: 1 Introduction Machine learning has been a major research area since the birth of artificial intelligence as a discipline (Steinbuch, 1961; Samuel, 1963; Nilsson, 1965; Winston, 1975). Learning is not only an inalienable component of human intelligence, but it also plays an important role in constructing high-performance application systems <ref> (Carbonell, 1990) </ref>. Recently, Koza introduced a new learning paradigm, called genetic programming (Koza, 1992a), which extends conventional evolutionary algorithms (Back and Schwefel, 1993; Goldberg, 1989; Muhlenbein, 1993) in that the structures undergoing adaptation are hierarchical computer programs instead of bitstrings.
Reference: <author> Durbin, R. and Rumelhart, D. E. </author> <year> (1989). </year> <title> Product units: A computationally powerful and biologically plausible extension to backpropagation networks. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 133-142. </pages>
Reference-contexts: One main reason was the difficulty of training. While some special class of networks consisting solely of pi units can be trained by the gradient method, either the architecture should be very simple (Giles and Maxwell, 1987) or the solution involves the manipulation of complex-valued expressions <ref> (Durbin and Rumelhart, 1989) </ref>. Another problem in using pi units is the combinatorial explosion of the number of terms (Amari, 1991).
Reference: <author> Feldman, J. A. and Ballard, D. H. </author> <year> (1982). </year> <title> Connectionist models and their properties. </title> <journal> Cognitive Science, </journal> <volume> 6 </volume> <pages> 205-254. </pages>
Reference: <author> Fogel, D. B. </author> <year> (1991). </year> <title> An information criterion for optimal neural network selection. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 2(5) </volume> <pages> 490-497. </pages>
Reference: <author> Geman, S., Bienenstock, E., and Doursat, R. </author> <year> (1992). </year> <title> Neural networks and the bias/variance dilemma. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 1-58. </pages>
Reference: <author> Giles, C. L. and Maxwell, T. </author> <year> (1987). </year> <title> Learning, invariance, and generalization in high-order neural networks. </title> <journal> Applied Optics, </journal> <volume> 26(23) </volume> <pages> 4972-4978. </pages>
Reference-contexts: One main reason was the difficulty of training. While some special class of networks consisting solely of pi units can be trained by the gradient method, either the architecture should be very simple <ref> (Giles and Maxwell, 1987) </ref> or the solution involves the manipulation of complex-valued expressions (Durbin and Rumelhart, 1989). Another problem in using pi units is the combinatorial explosion of the number of terms (Amari, 1991).
Reference: <author> Goldberg, D. E. </author> <year> (1989). </year> <title> Genetic Algorithms in Search, Optimization & Machine Learning. </title> <publisher> Addison Wesley. </publisher>
Reference: <author> Iba, H., de Garis, H., and Sato, T. </author> <year> (1994). </year> <title> Genetic programming using a minimum description length principle. </title> <editor> In Kinnear, K. E., editor, </editor> <booktitle> Advances in Genetic Programming, </booktitle> <pages> pages 265-284. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, Cambridge. </address>
Reference-contexts: In fact, the authors remark that this kind of MDL approach should be used carefully when evolving general programs with genetic programming <ref> (Iba et al., 1994) </ref>. Note also that this strategy is far from the two step approach of ID3, taken to ensure parsimonious solutions without losing good performance.
Reference: <author> Iba, H., Kurita, T., de Garis, H., and Sato, T. </author> <year> (1993). </year> <title> System identification using structured genetic algorithms. </title> <booktitle> In Proceedings of the Fifth International Conference on Genetic Algorithms (ICGA-93), </booktitle> <pages> pages 279-286. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Ivakhnenko, A. G. </author> <year> (1971). </year> <title> Polynomial theory of complex systems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 1(4) </volume> <pages> 364-378. </pages> <note> 31 Kinnear, </note> <author> K. E. </author> <year> (1993). </year> <title> Generality and difficulty in genetic programming: Evolving a sort. </title> <editor> In Forrest, S., editor, </editor> <booktitle> Proceedings of the Fifth International Conference on Genetic Algorithms (ICGA-93), </booktitle> <pages> pages 287-294. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo. </address>
Reference-contexts: This is primarily done by the crossover operator, which generates new trees that can be larger or smaller than their parents. Iba et al. have used the MDL principle in genetic programming to evolve GMDH networks <ref> (Ivakhnenko, 1971) </ref> and decision trees (Iba et al., 1993; Iba et al., 1994).
Reference: <editor> Kinnear, K. E., editor (1994a). </editor> <booktitle> Advances in Genetic Programming. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, Cam-bridge. </address>
Reference: <author> Kinnear, K. E. </author> <year> (1994b). </year> <title> Fitness landscapes and difficulty in genetic programming. </title> <booktitle> In Proceedings of IEEE International Conference on Evolutionary Computation (ICEC-94), World Congress on Computational Intelligence, </booktitle> <pages> pages 142-147, </pages> <address> New York. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: Complexity In an attempt to examine the relationship between accuracy and structural complexity, we analyzed the error landscape of sigma-pi neural networks (Zhang, 1994). Landscape analysis techniques have also been used to characterize the difficulty of the tasks in genetic algorithms (Manderick et al., 1991) and in genetic programming <ref> (Kinnear, 1994b) </ref>. We used the parity problem with input size n = 7. We first generated a clean data set ~ D N of size N = 2 7 .
Reference: <author> Koza, J. R. </author> <year> (1992a). </year> <title> Genetic Programming: On the Programming of Computers by Means of Natural Selection. </title> <publisher> MIT Press, </publisher> <address> Cambridge. </address>
Reference-contexts: Learning is not only an inalienable component of human intelligence, but it also plays an important role in constructing high-performance application systems (Carbonell, 1990). Recently, Koza introduced a new learning paradigm, called genetic programming <ref> (Koza, 1992a) </ref>, which extends conventional evolutionary algorithms (Back and Schwefel, 1993; Goldberg, 1989; Muhlenbein, 1993) in that the structures undergoing adaptation are hierarchical computer programs instead of bitstrings.
Reference: <author> Koza, J. R. </author> <year> (1992b). </year> <title> Hierarchical automatic function definition in genetic programming. </title> <editor> In Whitley, D., editor, </editor> <booktitle> Proceeedings of Workshop on Foundations of Genetic Algorithms and Classifier Systems. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In addition, as their size increases, the programs frequently become hopelessly opaque to human understanding (Kinnear, 1993). One approach to dealing with this problem is to define and reuse submodules. Koza suggests defining potentially useful subroutines called automatically defined functions (ADF's) during a run <ref> (Koza, 1992b) </ref>. Genetic programming with automatic function definition significantly reduces the aver 2 age structural complexity of the solutions and the computational effort as compared to genetic programming without automatic function definition (Koza, 1993).
Reference: <author> Koza, J. R. </author> <year> (1993). </year> <title> Simultaneous discovery of reusable detectors and subroutines using genetic programming. </title> <editor> In Forrest, S., editor, </editor> <booktitle> Proceedings of the Fifth International Conference on Genetic Algorithms, </booktitle> <pages> pages 295-302. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Koza suggests defining potentially useful subroutines called automatically defined functions (ADF's) during a run (Koza, 1992b). Genetic programming with automatic function definition significantly reduces the aver 2 age structural complexity of the solutions and the computational effort as compared to genetic programming without automatic function definition <ref> (Koza, 1993) </ref>. However, even with reusable submodules the program size may still grow without bound if the training data is noisy or incomplete. Empirical studies report that, when their training accuracy is comparable, smaller solutions usually demonstrate better generalization performance than larger solutions.
Reference: <author> Koza, J. R. </author> <year> (1994). </year> <title> Genetic Programming II: Automatic Discovery of Reusable Programs. </title> <publisher> MIT Press, </publisher> <address> Cambridge. </address>
Reference: <author> Manderick, B., de Weger, M., and Spiessens, P. </author> <year> (1991). </year> <title> The genetic algorithm and the structure of the fitness landscape. </title> <booktitle> In Proceedings of the Fourth International Conference on Genetic Algorithms, </booktitle> <pages> pages 143-150. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Complexity In an attempt to examine the relationship between accuracy and structural complexity, we analyzed the error landscape of sigma-pi neural networks (Zhang, 1994). Landscape analysis techniques have also been used to characterize the difficulty of the tasks in genetic algorithms <ref> (Manderick et al., 1991) </ref> and in genetic programming (Kinnear, 1994b). We used the parity problem with input size n = 7. We first generated a clean data set ~ D N of size N = 2 7 .
Reference: <author> Muhlenbein, H. </author> <year> (1993). </year> <title> Evolutionary algorithms: Theory and applications. </title> <editor> In Aarts, E. H. L. and Lenstra, J. K., editors, </editor> <title> Local Search in Combinatorial Optimization. </title> <publisher> Wiley. </publisher>
Reference-contexts: The best t % of the hillclimbed population of generation g are selected into the mating pool B (g), where t 2 (0; 1] is the truncation threshold <ref> (Muhlenbein and Schlierkamp-Voosen, 1993) </ref>. The (g + 1)th generation of size M is produced by applying crossover and mutation operators to the parent networks in the mating pool B (g).
Reference: <author> Muhlenbein, H. and Schlierkamp-Voosen, D. </author> <year> (1993). </year> <title> Predictive models for the breeder genetic algorithm I: Continuous parameter optimization. </title> <journal> Evolutionary Computation, </journal> <volume> 1(1) </volume> <pages> 25-49. </pages>
Reference-contexts: The best t % of the hillclimbed population of generation g are selected into the mating pool B (g), where t 2 (0; 1] is the truncation threshold <ref> (Muhlenbein and Schlierkamp-Voosen, 1993) </ref>. The (g + 1)th generation of size M is produced by applying crossover and mutation operators to the parent networks in the mating pool B (g).
Reference: <author> Murphy, P. M. and Aha, D. W. </author> <year> (1994). </year> <title> UCI repository of machine learning databases. </title> <institution> Department of Information and Computer Science, University of California, </institution> <address> Irvine, C.A. </address>
Reference-contexts: The first 5 variables of the input are all blood tests which are thought to be sensitive to liver disorders that might arise from excessive alcohol consumption. The sixth input variable is the number of half-pint equivalents of alcoholic beverages drunk per day. The original data values <ref> (Murphy and Aha, 1994) </ref> were normalized into the interval [-1, 1] before being used to train the networks. 19 attribute name description x 1 mcv mean corpuscular volume x 2 alkphos alkaline phosphotase x 3 sgpt alamine aminotransferase x 4 sgot aspartate aminotransferase x 5 gammagt gamma-glutamyl transpeptidase x 6 drinks
Reference: <author> Nilsson, N. </author> <year> (1965). </year> <title> Learning Machines. </title> <publisher> McGraw-Hill, </publisher> <address> New York. </address> <note> 32 Quinlan, </note> <author> J. R. and Rivest, R. L. </author> <year> (1989). </year> <title> Inferring decision trees using the minimum description length principle. </title> <journal> Information and Computation, </journal> <volume> 80 </volume> <pages> 227-248. </pages>
Reference: <author> Rissanen, J. </author> <year> (1984). </year> <title> Universal coding, information, prediction, and estimation. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 30(4) </volume> <pages> 629-636. </pages>
Reference-contexts: Alternatively, we can use the model complexity; according to coding theory <ref> (Rissanen, 1984) </ref>, if P (x) is given, then its code length is given as L (P (x)) = log (P (x)).
Reference: <author> Rissanen, J. </author> <year> (1986). </year> <title> Stochastic complexity and modeling. </title> <journal> The Annals of Statistics, </journal> <volume> 14 </volume> <pages> 1080-1100. </pages>
Reference: <author> Rumelhart, D. E., Hinton, G. E., and McClelland, J. L. </author> <year> (1986). </year> <title> A general framework for parallel distributed processing. </title> <editor> In Rumelhart, D. E. and McClelland, J. L., editors, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> volume I, </volume> <pages> pages 45-76. </pages> <publisher> MIT Press, </publisher> <address> Cambridge. </address>
Reference: <author> Samuel, A. L. </author> <year> (1963). </year> <title> Some studies in machine learning using the game of checkers. </title> <editor> In Feigenbaum, E. and Feldman, J., editors, </editor> <booktitle> Computers and Thought, </booktitle> <pages> pages 71-105. </pages> <publisher> McGraw-Hill, </publisher> <address> New York. </address>
Reference: <author> Steinbuch, K. </author> <year> (1961). </year> <title> Die Lernmatrix. </title> <journal> Kybernetik, </journal> <volume> 1(1) </volume> <pages> 36-45. </pages>
Reference: <author> Tackett, W. A. </author> <year> (1993). </year> <title> Genetic programming for feature discovery and image discrimination. </title> <editor> In Forrest, S., editor, </editor> <booktitle> Proceedings of the Fifth International Conference on Genetic Algorithms (ICGA-93), </booktitle> <pages> pages 303-309. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo. </address>
Reference-contexts: Tackett, for instance, observes in his pattern classification experiments that a high degree of correlation exists between tree size and performance: "among the set of `pretty good' solutions, the smallest within the set usually achieved the highest performance" <ref> (Tackett, 1993) </ref>. He also observed that, as the size and complexity of trees grew, a point was eventually reached in most runs where performance dropped.
Reference: <author> Winston, P. </author> <year> (1975). </year> <title> Learning structural descriptions from examples. </title> <editor> In P., W., editor, </editor> <booktitle> The Psychology of Computer Vision. </booktitle> <publisher> McGraw-Hill, </publisher> <address> New York. </address>
Reference: <author> Zhang, B.-T. </author> <year> (1994). </year> <title> Effects of Occam's razor in evolving sigma-pi neural networks. </title> <editor> In Davidor, Y., Schwefel, H.-P., and R., M., editors, </editor> <booktitle> Lecture Notes in Computer Science 866: Parallel Problem Solving from Nature), </booktitle> <pages> pages 462-471. </pages> <publisher> Springer-Verlag, London. </publisher>
Reference-contexts: Genetic programming starts with an initial population A of randomly generated computer programs composed of elementary functions and terminals chosen by the domain expert. The elementary functions may be arithmetic operations, logical functions, standard programming operations, or domain-specific functions. In the synthesis of sigma-pi neural networks <ref> (Zhang and Muhlenbein, 1994) </ref>, for instance, the terminal set X consists of n input variables: X = fx 1 ; x 2 ; ; x n g (2) and the elementary function set U contains sigma (S) and pi (P ) units: U = fS; P g: (3) An instance of <p> In the next section we empirically study this phenomenon by examining the error landscape. 7 3 Generalization vs. Complexity In an attempt to examine the relationship between accuracy and structural complexity, we analyzed the error landscape of sigma-pi neural networks <ref> (Zhang, 1994) </ref>. Landscape analysis techniques have also been used to characterize the difficulty of the tasks in genetic algorithms (Manderick et al., 1991) and in genetic programming (Kinnear, 1994b). We used the parity problem with input size n = 7.
Reference: <author> Zhang, B.-T. and Muhlenbein, H. </author> <year> (1993a). </year> <title> Evolving optimal neural networks using genetic algorithms with Occam's razor. </title> <journal> Complex Systems, </journal> <volume> 7(3) </volume> <pages> 199-220. </pages>
Reference: <author> Zhang, B.-T. and Muhlenbein, H. </author> <year> (1993b). </year> <title> Genetic programming of minimal neural nets using Occam's razor. </title> <editor> In Forrest, S., editor, </editor> <booktitle> Proceedings of the Fifth International Conference on Genetic Algorithms, </booktitle> <pages> pages 342-349. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo. </address>
Reference: <author> Zhang, B.-T. and Muhlenbein, H. </author> <year> (1994). </year> <title> Synthesis of sigma-pi neural networks by the breeder genetic programming. </title> <booktitle> In Proceedings of IEEE International Conference on Evolutionary Computation (ICEC-94), World Congress on Computational Intelligence, </booktitle> <pages> pages 318-323. </pages> <publisher> IEEE Computer Society Press, </publisher> <address> New York. </address> <month> 33 </month>
Reference-contexts: Genetic programming starts with an initial population A of randomly generated computer programs composed of elementary functions and terminals chosen by the domain expert. The elementary functions may be arithmetic operations, logical functions, standard programming operations, or domain-specific functions. In the synthesis of sigma-pi neural networks <ref> (Zhang and Muhlenbein, 1994) </ref>, for instance, the terminal set X consists of n input variables: X = fx 1 ; x 2 ; ; x n g (2) and the elementary function set U contains sigma (S) and pi (P ) units: U = fS; P g: (3) An instance of <p> In the next section we empirically study this phenomenon by examining the error landscape. 7 3 Generalization vs. Complexity In an attempt to examine the relationship between accuracy and structural complexity, we analyzed the error landscape of sigma-pi neural networks <ref> (Zhang, 1994) </ref>. Landscape analysis techniques have also been used to characterize the difficulty of the tasks in genetic algorithms (Manderick et al., 1991) and in genetic programming (Kinnear, 1994b). We used the parity problem with input size n = 7.
References-found: 36

