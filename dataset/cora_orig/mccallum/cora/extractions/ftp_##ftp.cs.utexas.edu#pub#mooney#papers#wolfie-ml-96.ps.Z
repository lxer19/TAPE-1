URL: ftp://ftp.cs.utexas.edu/pub/mooney/papers/wolfie-ml-96.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/ml/abstracts.html
Root-URL: 
Email: cthomp@cs.utexas.edu, mooney@cs.utexas.edu  
Title: Lexical Acquisition: A Novel Machine Learning Problem  Areas: Inductive Learning, Natural Language Acquisition  
Author: Cynthia A. Thompson and Raymond J. Mooney 
Address: Austin, TX 78712  
Affiliation: Department of Computer Sciences University of Texas  
Date: 1996  January 19, 1996  
Note: Submitted to International Conference of Machine Learning,  
Abstract: This paper defines a new machine learning problem to which standard machine learning algorithms cannot easily be applied. The problem occurs in the domain of lexical acquisition. The ambiguous and synonymous nature of words causes the difficulty of using standard induction techniques to learn a lexicon. Additionally, negative examples are typically unavailable or difficult to construct in this domain. One approach to solve the lexical acquisition problem is presented, along with preliminary experimental results on an artificial corpus. Future work includes extending the algorithm and performing tests on a more realistic corpus. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Anderson, J. R. </author> <year> (1977). </year> <title> Induction of augmented transition networks. </title> <journal> Cognitive Science, </journal> <volume> 1, </volume> <pages> 125-157. </pages>
Reference: <author> Fillmore, C. J. </author> <year> (1968). </year> <title> The case for case. In Bach, </title> <editor> E., & Harms, R. T. (Eds.), </editor> <booktitle> Universals in Linguistic Theory. </booktitle> <publisher> Holt, Reinhart and Winston, </publisher> <address> New York. </address>
Reference-contexts: Part of the input to the problem is the procedure, C, for building up and breaking down sentence meanings. The semantic representation currently used is a tree-based representation, derived from the CD representational theory and using case-role structures <ref> (Fillmore, 1968) </ref>. For this representation, the method for breaking down sentence meanings is to enumerate all connected subgraphs of the sentence-representation tree. Because of the compositional assumption, these subgraphs are possible meanings for the individual words in the 5 sentence.
Reference: <author> McClelland, J. L., & Kawamoto, A. H. </author> <year> (1986). </year> <title> Mechanisms of sentence processing: Assigning roles to constituents of sentences. </title> <editor> In Rumelhart, D. E., & McClelland, J. L. (Eds.), </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> Vol. II, </volume> <pages> pp. 318-362. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Muggleton, S., & Feng, C. </author> <year> (1992). </year> <title> Efficient induction of logic programs. </title> <editor> In Muggleton, S. (Ed.), </editor> <booktitle> Inductive Logic Programming, </booktitle> <pages> pp. 281-297. </pages> <publisher> Academic Press, </publisher> <address> New York. </address> <note> 13 Plotkin, </note> <author> G. D. </author> <year> (1970). </year> <title> A note on inductive generalization. </title> <editor> In Meltzer, B., & Michie, D. (Eds.), </editor> <booktitle> Machine Intelligence (Vol. </booktitle> <volume> 5). </volume> <publisher> Elsevier North-Holland, </publisher> <address> New York. </address>
Reference-contexts: TLGGs between pairs of sentence representations are used to help narrow down the number of hypothesized meanings for a word. The maximum number of pairings per word is a parameter provided by the user. This effectively builds generalizations of sentence representations, in a manner similar to Golem <ref> (Muggleton & Feng, 1992) </ref>. Each choice of a (word, meaning) pair eliminates some possible meanings for other words in I, by removing elements from their TLGG list. A sample corpus that could be used as input to the learning problem is as follows: 1.
Reference: <author> Schank, R. C. </author> <year> (1975). </year> <title> Conceptual Information Processing. </title> <publisher> North-Holland, Oxford. </publisher>
Reference-contexts: To give a simple example of the lexical learning problem with a Conceptual Dependency (CD) representation <ref> (Schank, 1975) </ref>, let I be: f ([the,man,ate], [ingest,agt:[person,sex:male,age:adult]] ), ([the,woman,ate], [ingest,agt:[person,sex:female,age:adult]] ), ([the,sheep,ate], [ingest,agt:[animal,type:sheep]] )g.
Reference: <author> Siskind, J. M. </author> <year> (1992). </year> <title> Naive Physics, Event Perception, Lexical Semantics and Language Acquisition. </title> <type> Ph.D. thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Mas-sachusetts Institute of Technology, </institution> <address> Cambridge, MA. </address>
Reference: <author> Siskind, J. M. </author> <year> (1994). </year> <title> Lexical acquisition in the presence of noise and homonymy. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pp. 760-766. </pages>
Reference: <author> Zelle, J. M. </author> <year> (1995). </year> <title> Using Inductive Logic Programming to Automate the Construction of Natural Language Parsers. </title> <type> Ph.D. thesis, </type> <institution> University of Texas, Austin, TX. </institution>
Reference: <author> Zelle, J. M., & Mooney, R. J. </author> <year> (1993). </year> <title> Learning semantic grammars with constructive inductive logic programming. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 817-822 Washington, D.C. </address>
Reference: <author> Zelle, J. M., & Mooney, R. J. </author> <year> (1994). </year> <title> Inducing deterministic Prolog parsers from treebanks: A machine learning approach. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 748-753 Seattle, WA. </address> <month> 14 </month>
References-found: 10

