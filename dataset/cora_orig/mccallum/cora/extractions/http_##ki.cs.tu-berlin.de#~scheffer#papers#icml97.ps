URL: http://ki.cs.tu-berlin.de/~scheffer/papers/icml97.ps
Refering-URL: http://ki.cs.tu-berlin.de/~scheffer/publications.html
Root-URL: 
Email: scheffer@cs.tu-berlin.de  fgreiner, darkeng@scr.siemens.com  
Title: Why Experimentation can be better than "Perfect Guidance"  
Author: Tobias Scheffer Russell Greiner and Christian Darken 
Address: FR 5-8 Franklinstr. 28/29 10587 Berlin, Germany  755 College Road East Princeton, NJ 08540, USA  
Affiliation: Technische Universitat Berlin Artificial Intelligence Group,  Siemens Corporate Research Adaptive Signal and Information Processing  
Abstract: Many problems correspond to the classical control task of determining the appropriate control action to take, given some (sequence of) observations. One standard approach to learning these control rules, called behavior cloning, involves watching a perfect operator operate a plant, and then trying to emulate its behavior. In the experimental learning approach, by contrast, the learner first guesses an initial operation-to-action policy and tries it out. If this policy performs sub-optimally, the learner can modify it to produce a new policy, and recur. This paper discusses the relative effectiveness of these two approaches, especially in the presence of perceptual aliasing, showing in particular that the experimental learner can often learn more effectively than the cloning one.
Abstract-found: 1
Intro-found: 1
Reference: [BMSJ78] <author> Bruce G. Buchanan, Thomas M. Mitchell, Reid G. Smith, and C. R. Johnson, Jr. </author> <title> Models of learning systems. </title> <booktitle> In Encyclopedia of Computer Science and Technology, </booktitle> <volume> volume 11. </volume> <publisher> Dekker, </publisher> <year> 1978. </year>
Reference-contexts: Behavior cloning has an obvious connection to the standard machine learning work on classification <ref> [BMSJ78] </ref>: In each case, the learner explicitly sees the optimal answer for each given situation. Here, however, both the training and the test samples are missing some information | i.e., the state.
Reference: [Bro93] <author> Martin Brooks. </author> <title> Proposal for a pattern matching task controller for sensor-based coordination of robot motions. In Robots and Biological Systems. </title> <publisher> Springer-Verlag NATO ASI series F, </publisher> <year> 1993. </year>
Reference-contexts: 1 INTRODUCTION Many real-world tasks require controlling some type of "plant" (e.g., factory [DB95], grinding tool <ref> [Bro93] </ref>, robot [Kae93]); in each case, using information received from the sensors to decide on a control action | e.g., using temperature and pressure readings to decide whether to open some valve. <p> Obvious examples include learning to fly an airplane [SHKM92], to control a satellite [MW95], to operate a grinding machine <ref> [Bro93] </ref> or to play Wum-pus effectively [RN95]; related techniques are also used to acquire expert-level knowledge [SCG91]. Of course, this "to-be-mimicked" operator can be a problem solver that generates optimal actions for a set of sample states [MW95, BSW96].
Reference: [BSA83] <author> A. B. Barto, R. S. Sutton, and C. W. An-derson. </author> <title> Neuronlike adaptive elements that can solve difficult learning control problems. </title> <journal> In IEEE Transactions on Systems, Man, and Cybernetics, </journal> <year> 1983. </year>
Reference-contexts: Alternatively, an "experimental learner" L E would initially guess a control policy and test it out. This learner may then tweak the control policy, producing a new one that is (it hopes) superior, and so forth. This is the underlying principle of many reinforcement learning schemes <ref> [MC68, BSA83] </ref>. This paper compares these two classes of learners. Section 2 first provides the necessary definitions, defining in particular the "control" performance task, the basic learning task, and the two learning paradigms. Section 3 then describes when each paradigm should work most effectively.
Reference: [BSW96] <author> L. Briesemeister, T. Scheffer, and F. Wysotzki. </author> <title> A concept-formation oriented approach to skill acquisition. </title> <booktitle> In Proc. European Workshop on Cognitive Modelling, </booktitle> <year> 1996. </year>
Reference-contexts: Of course, this "to-be-mimicked" operator can be a problem solver that generates optimal actions for a set of sample states <ref> [MW95, BSW96] </ref>. Alternatively, an "experimental learner" L E would initially guess a control policy and test it out. This learner may then tweak the control policy, producing a new one that is (it hopes) superior, and so forth. This is the underlying principle of many reinforcement learning schemes [MC68, BSA83]. <p> r (j) = hhy (j) (j) (j) (j) 2 i; : : :i, L E is told the cost of this trajectory c (r (j) ). 3 It may then revise 2 While most work on behavioral cloning assumes a human operator is operating the plant optimally, we (along with <ref> [MW95, BSW96] </ref>) model this ideal operator as a problem solver that determines optimal actions by search; see Section 4. 3 Note that L E is not told the cost of each state encountered.
Reference: [DB95] <author> Richard C. Dorf and Robert H. Bishop. </author> <title> Modern Control Systems. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., 7th edition, </address> <year> 1995. </year>
Reference-contexts: 1 INTRODUCTION Many real-world tasks require controlling some type of "plant" (e.g., factory <ref> [DB95] </ref>, grinding tool [Bro93], robot [Kae93]); in each case, using information received from the sensors to decide on a control action | e.g., using temperature and pressure readings to decide whether to open some valve.
Reference: [Eps94] <author> Susan L. Epstein. </author> <title> Toward an ideal trainer. </title> <journal> Machine Learning, </journal> <volume> 15 </volume> <pages> 251-277, </pages> <year> 1994. </year>
Reference-contexts: As a third example of this phenomenon, Epstein also realized that her game-playing program would not work robustly against a range of opponents unless it was trained against a similarly large range of challengers <ref> [Eps94] </ref>.
Reference: [Kae93] <author> Leslie Pack Kaelbling. </author> <title> Learning in Embedded Systems. </title> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: 1 INTRODUCTION Many real-world tasks require controlling some type of "plant" (e.g., factory [DB95], grinding tool [Bro93], robot <ref> [Kae93] </ref>); in each case, using information received from the sensors to decide on a control action | e.g., using temperature and pressure readings to decide whether to open some valve.
Reference: [Kha96] <author> Roni Khardon. </author> <title> Learning to act. </title> <booktitle> In AAAI96, </booktitle> <address> Portland, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: Our task is to learn an optimal controller, based on information available about the plant. In the "behavioral cloning" approach <ref> [SHKM92, SMB95, Kha96] </ref>, a learner L B watches an "optimal" operator (i.e., an operator that always performs the best possible action) operate a plant, and then tries to emulate this operator.
Reference: [MC68] <author> D. Michie and A. Chambers. </author> <title> Boxes: An experiment in adaptive control. </title> <booktitle> In Machine Intelligence 2, </booktitle> <year> 1968. </year>
Reference-contexts: Alternatively, an "experimental learner" L E would initially guess a control policy and test it out. This learner may then tweak the control policy, producing a new one that is (it hopes) superior, and so forth. This is the underlying principle of many reinforcement learning schemes <ref> [MC68, BSA83] </ref>. This paper compares these two classes of learners. Section 2 first provides the necessary definitions, defining in particular the "control" performance task, the basic learning task, and the two learning paradigms. Section 3 then describes when each paradigm should work most effectively.
Reference: [MST94] <author> D. Michie, D. J. Spiegelhalter, and C. C. Taylor. </author> <title> Machine Learning, Neural and Statistical Classification. </title> <publisher> Ellis Horwood, </publisher> <year> 1994. </year>
Reference-contexts: Of course, this rule does not effectively control the plant! A controller learned by the DIPOL classification system [SW94] | a hybrid statistical/neural algorithm, that was ranked the best learning algorithm, on average, on the European StatLog project <ref> [MST94] </ref> | kept the system in a stable state, but the controller consumed an extremely large amount of oxygen. Providing the second observable parameter and the past measurements of both observable parameters as input reduced the oxygen consumption slightly (1-2%) for both algorithms, but the results were still unacceptable.
Reference: [MW95] <author> Wolfgang Muller and Fritz Wysotzki. </author> <title> Automatic synthesis of control programs by combination of learning and problem solving methods (extended abstract). </title> <booktitle> In Machine Learning: ECML-95, </booktitle> <pages> pages 323 - 326, </pages> <year> 1995. </year>
Reference-contexts: In the "behavioral cloning" approach [SHKM92, SMB95, Kha96], a learner L B watches an "optimal" operator (i.e., an operator that always performs the best possible action) operate a plant, and then tries to emulate this operator. Obvious examples include learning to fly an airplane [SHKM92], to control a satellite <ref> [MW95] </ref>, to operate a grinding machine [Bro93] or to play Wum-pus effectively [RN95]; related techniques are also used to acquire expert-level knowledge [SCG91]. Of course, this "to-be-mimicked" operator can be a problem solver that generates optimal actions for a set of sample states [MW95, BSW96]. <p> Of course, this "to-be-mimicked" operator can be a problem solver that generates optimal actions for a set of sample states <ref> [MW95, BSW96] </ref>. Alternatively, an "experimental learner" L E would initially guess a control policy and test it out. This learner may then tweak the control policy, producing a new one that is (it hopes) superior, and so forth. This is the underlying principle of many reinforcement learning schemes [MC68, BSA83]. <p> r (j) = hhy (j) (j) (j) (j) 2 i; : : :i, L E is told the cost of this trajectory c (r (j) ). 3 It may then revise 2 While most work on behavioral cloning assumes a human operator is operating the plant optimally, we (along with <ref> [MW95, BSW96] </ref>) model this ideal operator as a problem solver that determines optimal actions by search; see Section 4. 3 Note that L E is not told the cost of each state encountered.
Reference: [NA89] <author> K. Narendra and A. Annaswamy. </author> <title> Stable Adaptive Systems. </title> <publisher> Prentice Hall, </publisher> <year> 1989. </year>
Reference-contexts: As a third example of this phenomenon, Epstein also realized that her game-playing program would not work robustly against a range of opponents unless it was trained against a similarly large range of challengers [Eps94]. Finally, our model has several significant differences from the standard (adaptive) linear control framework <ref> [NA89] </ref>, which usually assumes the learner knows a lot about the plant's basic model in advance (e.g., that the plant is linear, and perhaps order bounds on the numerator and denominator of the transfer function that represents the system), and tends to evaluate learners on-line, in terms of a loss function.
Reference: [Pom93] <author> Dean A. Pomerleau. </author> <title> Neural Network Perception for Mobile Robot Guidance. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1993. </year>
Reference-contexts: This argues that a good learning environment should give the learner access to a wide range of situations, to allow (force?) the L B -ish learner to explore other regions of the space. Several researchers have provided such environments: For example, Pomerleau's NavLab system <ref> [Pom93] </ref> learned to drive by watching a human driver's steer, in response to the visual observations of the current road. Unfortunately for NavLab (but fortunately for these drivers), people tended to drive effectively | which meant NavLab had little experience not being in the appropriate lane position.
Reference: [PW93] <author> L. Pitt and M. Warmuth. </author> <title> The minimum consistent DFA problem cannot be approximated within any polynomial. </title> <journal> J. ACM, </journal> <volume> 40(1) </volume> <pages> 95-142, </pages> <year> 1993. </year>
Reference-contexts: This challenge | of producing a small DFA consistent with a set of observations | is intractable, and in fact, is not even approximatable <ref> [PW93] </ref>. 3.1 DISCUSSION AND RELATED WORK Note that the different learners | L B and L E | are receiving very different information: L B does get perfect information (i.e., the optimal action to take) but only about a very narrow of situations; by contrast, L E can obtain information about
Reference: [Rec94] <author> I. </author> <title> Rechenberg. </title> <type> Evolutionsstrategie '94. </type> <institution> Frommann-Holzboog, </institution> <year> 1994. </year>
Reference-contexts: The overall costs are returned to the learning algorithm, which then performs a gradient descent search for a good controller. We modified the controllers by adding normally distributed random numbers to all interval boundaries, and further used a simple mechanism for step-size adaptation, as used in evolutionary strategies <ref> [Rec94, Rec89] </ref>, based on comparing two successors generated with different step sizes. After about 10 minutes of computation time, we found a fairly good controller, which takes only one observable parameter as input. Figure 7 shows its ammonium (left) and oxygen (right) curves.
Reference: [Rec89] <editor> I. Rechenberg. </editor> <booktitle> Artificial evolution and artificial intelligence. </booktitle> <editor> In R. Forsyth, editor, </editor> <booktitle> Machine Learning, </booktitle> <pages> pages 83-103, </pages> <address> London, 89. </address> <publisher> Chapman. </publisher>
Reference-contexts: The overall costs are returned to the learning algorithm, which then performs a gradient descent search for a good controller. We modified the controllers by adding normally distributed random numbers to all interval boundaries, and further used a simple mechanism for step-size adaptation, as used in evolutionary strategies <ref> [Rec94, Rec89] </ref>, based on comparing two successors generated with different step sizes. After about 10 minutes of computation time, we found a fairly good controller, which takes only one observable parameter as input. Figure 7 shows its ammonium (left) and oxygen (right) curves.
Reference: [RN95] <author> Stuart Russell and Peter Norvig. </author> <title> Artificial Intelligence: A Modern Approach. </title> <publisher> Prentice Hall, </publisher> <year> 1995. </year>
Reference-contexts: Obvious examples include learning to fly an airplane [SHKM92], to control a satellite [MW95], to operate a grinding machine [Bro93] or to play Wum-pus effectively <ref> [RN95] </ref>; related techniques are also used to acquire expert-level knowledge [SCG91]. Of course, this "to-be-mimicked" operator can be a problem solver that generates optimal actions for a set of sample states [MW95, BSW96]. Alternatively, an "experimental learner" L E would initially guess a control policy and test it out.
Reference: [SCG91] <author> A. Carlisle Scott, Jan E. Clayton, and Eliza-beth L. Gibson. </author> <title> A Practical guide to knowledge acquisition. </title> <publisher> Addison-Wesley Pub Co., </publisher> <address> Reading, MA, </address> <year> 1991. </year>
Reference-contexts: Obvious examples include learning to fly an airplane [SHKM92], to control a satellite [MW95], to operate a grinding machine [Bro93] or to play Wum-pus effectively [RN95]; related techniques are also used to acquire expert-level knowledge <ref> [SCG91] </ref>. Of course, this "to-be-mimicked" operator can be a problem solver that generates optimal actions for a set of sample states [MW95, BSW96]. Alternatively, an "experimental learner" L E would initially guess a control policy and test it out.
Reference: [Sch95] <author> T. Scheffer. </author> <title> Learning rules with nested exceptions. </title> <booktitle> In Proc. International Workshop on Artificial Intelligence Techniques, </booktitle> <address> Brno, Czech Republic, </address> <year> 1995. </year>
Reference-contexts: Clearly, there is no obvious correlation between these values. We used an L B -style algorithm for induction of ripple down rules (i.e., rules with nested exceptions <ref> [Sch96, Sch95] </ref>) to describe the relation between this observable parameter and the optimal action via rules. We learned rules with an accuracy of up to 25%, but the rules were completely unable to keep the plant in a stable state.
Reference: [Sch96] <author> T. Scheffer. </author> <title> Algebraic foundation and improved methods of induction of ripple down rules. </title> <booktitle> In Proc. Pacific Knowledge Acquisition Workshop, </booktitle> <year> 1996. </year>
Reference-contexts: Clearly, there is no obvious correlation between these values. We used an L B -style algorithm for induction of ripple down rules (i.e., rules with nested exceptions <ref> [Sch96, Sch95] </ref>) to describe the relation between this observable parameter and the optimal action via rules. We learned rules with an accuracy of up to 25%, but the rules were completely unable to keep the plant in a stable state.
Reference: [SGD97] <author> Tobias Scheffer, Russell Greiner, and Chris-tian Darken. </author> <title> Why experimentation can be better than "perfect guidance". </title> <type> Technical report, </type> <institution> Siemens Corporate Research, </institution> <year> 1997. </year>
Reference-contexts: Conditions (1) and (2) above guarantee that s B will eventually be able to identify an appropriate ideal trajectory, and conditions (1) and (3) guarantee that s B will be able to follow this trajectory. (() The comments above sketch the proof of this di-rection; the extended technical report <ref> [SGD97] </ref> provides more details. As an immediate corollary, note that L B is a good learner if (it has arbitrary memory and) there is no perceptual aliasing; i.e., if ` XY (x) = ` XY (x 0 ) , x = x 0 .
Reference: [SHKM92] <author> C. Sammut, S. Hurst, D. Kedzier, and D. Michie. </author> <title> Learning to fly. </title> <booktitle> In ICML92, </booktitle> <address> Aberdeen, </address> <year> 1992. </year>
Reference-contexts: Our task is to learn an optimal controller, based on information available about the plant. In the "behavioral cloning" approach <ref> [SHKM92, SMB95, Kha96] </ref>, a learner L B watches an "optimal" operator (i.e., an operator that always performs the best possible action) operate a plant, and then tries to emulate this operator. <p> In the "behavioral cloning" approach [SHKM92, SMB95, Kha96], a learner L B watches an "optimal" operator (i.e., an operator that always performs the best possible action) operate a plant, and then tries to emulate this operator. Obvious examples include learning to fly an airplane <ref> [SHKM92] </ref>, to control a satellite [MW95], to operate a grinding machine [Bro93] or to play Wum-pus effectively [RN95]; related techniques are also used to acquire expert-level knowledge [SCG91]. Of course, this "to-be-mimicked" operator can be a problem solver that generates optimal actions for a set of sample states [MW95, BSW96]. <p> An ideal strategy op fl is a strategy with minimum expected cost. 1 Learning: The classic control problem involves learning an optimal strategy, given some training information. The "behavioral cloning" <ref> [SHKM92, SMB95] </ref> ap 1 We also consider strategies that take as input a short history of observations; here op : Y K 7! U for some integer K.
Reference: [SMB95] <author> B. Schulmeister, W. Muller, and M. Bleich. </author> <title> Modelling the expert's control behavior by machine learning algorithms. </title> <booktitle> In Proc. International Workshop on Artificial Intelligence Techniques, </booktitle> <year> 1995. </year>
Reference-contexts: Our task is to learn an optimal controller, based on information available about the plant. In the "behavioral cloning" approach <ref> [SHKM92, SMB95, Kha96] </ref>, a learner L B watches an "optimal" operator (i.e., an operator that always performs the best possible action) operate a plant, and then tries to emulate this operator. <p> An ideal strategy op fl is a strategy with minimum expected cost. 1 Learning: The classic control problem involves learning an optimal strategy, given some training information. The "behavioral cloning" <ref> [SHKM92, SMB95] </ref> ap 1 We also consider strategies that take as input a short history of observations; here op : Y K 7! U for some integer K.
Reference: [SP93] <author> Kah-Kay Sung and Tomaso Poggio. </author> <title> Example-based learning for view-based human face detection. </title> <type> Technical report, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1993. </year>
Reference-contexts: Pomerleau therefore presented NavLab with a variety of artificially produced scenes (formed by distorted some real scenes), each coupled with the proper steering-action. Similar techniques have been used to produce effective Face Recognition systems, that can work robustly over a wide range of conditions <ref> [SP93] </ref>. As a third example of this phenomenon, Epstein also realized that her game-playing program would not work robustly against a range of opponents unless it was trained against a similarly large range of challengers [Eps94].
Reference: [SW94] <author> B. Schulmeister and F. Wysotzki. </author> <title> The piecewise linear classifier DIPOL92. </title> <editor> In F. Bergadano and L. De Raedt, editors, </editor> <booktitle> Machine Learning: </booktitle> <address> ECML-94, </address> <publisher> LNAI 784. Springer Verlag, </publisher> <year> 1994. </year>
Reference-contexts: The rules minimizing the expected misclassification costs therefore contained the single rule: always predicting that action. Of course, this rule does not effectively control the plant! A controller learned by the DIPOL classification system <ref> [SW94] </ref> | a hybrid statistical/neural algorithm, that was ranked the best learning algorithm, on average, on the European StatLog project [MST94] | kept the system in a stable state, but the controller consumed an extremely large amount of oxygen.
Reference: [WB91] <author> S. D. Whitehead and D. H. Ballard. </author> <title> Learning to perceive and act. </title> <journal> Machine Learning, </journal> <volume> 7 </volume> <pages> 45-83, </pages> <year> 1991. </year>
Reference-contexts: In general a control policy specifies the action to take given any current (history of) sensor readings; and a controller is optimal if it minimizes some user-specified cost function. This task is complicated by several factors <ref> [WB91] </ref>, including perceptual aliasing (i.e., several different states may produce the same observations, which means the fl Appeared in Proc.
References-found: 26

