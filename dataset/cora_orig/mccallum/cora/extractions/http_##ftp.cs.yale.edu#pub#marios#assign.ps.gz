URL: http://ftp.cs.yale.edu/pub/marios/assign.ps.gz
Refering-URL: http://ftp.cs.yale.edu/pub/marios/
Root-URL: http://www.cs.yale.edu
Phone: 2  
Title: Memory Assignment for Multiprocessor Caches through Grey Coloring  
Author: Anant Agarwal John V. Guttag Christoforos N. Hadjicostis Marios C. Papaefthymiou ; 
Keyword: multiprocessor design issues, cache organization, compilers, cache coherence, direct-mapped caches, data partitioning, graph-coloring.  
Address: Cambridge, MA 02139  New Haven, CT 06520  
Affiliation: 1 Laboratory for Computer Science Massachusetts Institute of Technology  Department of Electrical Engineering and Department of Computer Science Yale University  
Abstract: The achieved performance of multiprocessors is heavily dependent on the performance of their caches. Cache performance is severely degraded when data tiles used by a program conflict in the caches. This paper explores techniques for improving multiprocessor performance by improving cache utilization. Specifically, we investigate the problem of statically assigning data tiles to memory in a way that minimizes the impact of collisions in multiprocessor caches. We define the problem precisely and present an efficient procedure for finding solutions to it. The procedure incorporates a new technique, grey coloring, that reduces the maximum number of conflicts in any cache in the system by distributing cache misses evenly among processors. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal, D. Chaiken, G. D'Souza, K. Johnson, D. Kranz, J. Kubiatowicz, K. Kurihara, B. Lim, G. Maa, D. Nussbaum, M. Parkin, and D. Yeung. </author> <title> The MIT Alewife Machine: A Large-Scale Distributed-Memory Multiprocessor. </title> <booktitle> In Workshop on Scalable Shared Memory Multiprocessors. </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year> <note> Also appears as MIT/LCS Memo TM-454, </note> <year> 1991. </year>
Reference-contexts: In bus-based machines, the memory is implemented as a single module accessed over the bus, while in most large-scale machines the memory is physically distributed among all the processing nodes. Virtually all machines that support the shared-memory programming abstraction provide local caches at each processor <ref> [1, 17, 18, 20, 21] </ref>. Caches automatically replicate memory locations close to the processor and avoid expensive network traversals for most memory accesses. Authors' e-mail: fagarwal,guttag,chadjic,mariosg@lcs.mit.edu. 1 The existence of caches does not change the memory abstraction. <p> This phenomenon is illustrated by the graph of Figure 1. This graph gives the activity profile of a blocked matrix multiply program running on a simulator of the Alewife machine <ref> [1] </ref>, using 64 processors. After a start-up transient (not shown in the graph), the number of operations going on in parallel becomes high. Subsequently, this number decreases drastically and remains at low levels for most of the rest of the computation.
Reference: [2] <author> Anant Agarwal, David Kranz, and Venkat Natarajan. </author> <title> Automatic Partitioning of Parallel Loops and Data Arrays for Distributed Shared Memory Multiprocessors. </title> <type> Technical Report MIT/LCS TM-481, </type> <institution> Massachusetts Institute of Technology, </institution> <month> December </month> <year> 1992. </year> <note> A short version also appears in ICPP 1993. </note>
Reference-contexts: The problem of partitioning data aggregates (such as arrays) into tiles of data that are individually placed in the memory modules of multiprocessors commonly called the data partitioning problem or the domain decomposition problem is non-trivial and has been the focus of much recent attention <ref> [2, 3, 8, 16, 19] </ref>, but is not treated in this paper.
Reference: [3] <author> Jennifer M. Anderson and Monica S. Lam. </author> <title> Global Optimizations for Parallelism and Locality on Scalable Parallel Machines. </title> <booktitle> In Proceedings of SIGPLAN '93, Conference on Programming Languages Design and Implementation, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: The problem of partitioning data aggregates (such as arrays) into tiles of data that are individually placed in the memory modules of multiprocessors commonly called the data partitioning problem or the domain decomposition problem is non-trivial and has been the focus of much recent attention <ref> [2, 3, 8, 16, 19] </ref>, but is not treated in this paper.
Reference: [4] <author> R. Barua, A. Agarwal, D. Kranz, and V. Natarajan. </author> <title> Addressing Partitioned Arrays in Distributed Shared Memory. </title> <note> In preparation, </note> <month> August </month> <year> 1993. </year> <month> 19 </month>
Reference: [5] <author> B. Berger and J. Rompel. </author> <title> A better performance guarantee for approximate graph coloring. </title> <journal> Algorithmica, </journal> <year> 1988. </year>
Reference-contexts: Theorem 1 The zero-conflict memory assignment problem is computationally equivalent to the graph k-colorability problem. Proof. Straightforward reduction. The complete proof will be included in the final version of the paper. Graph k-colorability is among the most difficult intractable problems <ref> [5, 6, 7, 14, 22] </ref>.
Reference: [6] <author> A. Blum. </author> <title> An ~ O(n 0:4 )-approximation algorithm for 3-coloring (and improved approximation algorithms for k-coloring). </title> <booktitle> Proc. of the 21st Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 535-542, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: Theorem 1 The zero-conflict memory assignment problem is computationally equivalent to the graph k-colorability problem. Proof. Straightforward reduction. The complete proof will be included in the final version of the paper. Graph k-colorability is among the most difficult intractable problems <ref> [5, 6, 7, 14, 22] </ref>.
Reference: [7] <author> A. Blum. </author> <title> Some tools for approximate 3-coloring. </title> <booktitle> Proc. of the 31st Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 554-562, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: Theorem 1 The zero-conflict memory assignment problem is computationally equivalent to the graph k-colorability problem. Proof. Straightforward reduction. The complete proof will be included in the final version of the paper. Graph k-colorability is among the most difficult intractable problems <ref> [5, 6, 7, 14, 22] </ref>.
Reference: [8] <author> David Callahan and Ken Kennedy. </author> <title> Compiling Programs for Distributed-Memory Multiprocessors. </title> <journal> Journal of Supercomputing, </journal> <pages> 2(151-169), </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: The problem of partitioning data aggregates (such as arrays) into tiles of data that are individually placed in the memory modules of multiprocessors commonly called the data partitioning problem or the domain decomposition problem is non-trivial and has been the focus of much recent attention <ref> [2, 3, 8, 16, 19] </ref>, but is not treated in this paper.
Reference: [9] <author> David Chaiken, Craig Fields, Kiyoshi Kurihara, and Anant Agarwal. </author> <title> Directory-Based Cache-Coherence in Large-Scale Multiprocessors. </title> <journal> IEEE Computer, </journal> <volume> 23(6) </volume> <pages> 41-58, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: The hardware is responsible for maintaining the illusion that all reads and writes take place directly to main memory using some cache coherency protocol (see [13] for a description and evaluation of several bus-based protocols, and <ref> [9] </ref> for an evaluation of interconnection-network based protocols.) This paper focuses on direct-mapped caches. (We note, however, that this work applies equally to set-associative caches, as outlined in Appendix B.) In a direct-mapped cache, each line in main memory corresponds to a unique entry in the cache memory.
Reference: [10] <author> G. J. Chaitin. </author> <title> Register allocation and spilling via graph coloring. </title> <booktitle> In Proc. of the ACM Sigplan '82 Symposium on Compiler Construction, </booktitle> <pages> pages 22-31, </pages> <month> June </month> <year> 1982. </year>
Reference-contexts: If no interesting special case is detected, we move on to color the conflict graph G by applying a minor variation of the graph-coloring scheme in <ref> [10, 11] </ref> that is used in the context of register allocation for sequential processors.
Reference: [11] <author> F. C. Chow and J. L. Hennessy. </author> <title> The prioriry-based coloring approach to register allocation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 12(4), </volume> <month> October </month> <year> 1990. </year>
Reference-contexts: If no interesting special case is detected, we move on to color the conflict graph G by applying a minor variation of the graph-coloring scheme in <ref> [10, 11] </ref> that is used in the context of register allocation for sequential processors.
Reference: [12] <author> Jr. Donald O. Tanguay. </author> <title> Compile-Time Loop Splitting for Distributed Memory Multiprocessors. </title> <type> Technical Report TM-490, </type> <institution> MIT, </institution> <month> May </month> <year> 1993. </year>
Reference: [13] <author> S. J. Eggers and R. H. Katz. </author> <title> Evaluating the performance of four snooping cache coherency protocols. </title> <booktitle> In Proceedings of the 16th International Symposium on Computer Architecture, </booktitle> <address> New York, </address> <month> June </month> <year> 1989. </year> <note> IEEE. </note>
Reference-contexts: Authors' e-mail: fagarwal,guttag,chadjic,mariosg@lcs.mit.edu. 1 The existence of caches does not change the memory abstraction. The hardware is responsible for maintaining the illusion that all reads and writes take place directly to main memory using some cache coherency protocol (see <ref> [13] </ref> for a description and evaluation of several bus-based protocols, and [9] for an evaluation of interconnection-network based protocols.) This paper focuses on direct-mapped caches. (We note, however, that this work applies equally to set-associative caches, as outlined in Appendix B.) In a direct-mapped cache, each line in main memory corresponds
Reference: [14] <author> M. R. Garey and D. S. Johnson. </author> <title> Computers and Intractability. </title> <editor> W. H. </editor> <publisher> Freeman and Co., </publisher> <address> San Francisco, </address> <year> 1979. </year>
Reference-contexts: The graph k-colorability problem is defined as follows <ref> [14] </ref>. <p> Theorem 1 The zero-conflict memory assignment problem is computationally equivalent to the graph k-colorability problem. Proof. Straightforward reduction. The complete proof will be included in the final version of the paper. Graph k-colorability is among the most difficult intractable problems <ref> [5, 6, 7, 14, 22] </ref>.
Reference: [15] <author> Christoforos N. Hadjicostis. </author> <title> Heuristics for solving the memory assignment problem in multiprocessor caches. </title> <type> Bachelor's thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: If a processor's cache was already full, we simply ignored the special tile for this specific processor, but we allowed the rest of them to access it in a regular way. More details about the generation of the Jacobian test inputs can be found in <ref> [15] </ref>. We generated the following kinds of input instances. For j = 3; 5; 7, we created two groups of inputs, J j (6) and J j (8), which we employed to experiment with caches that have 6 and 8 slots, respectively.
Reference: [16] <author> Charles Koelbel, Piyush Mehrotra, and John Van Rosendale. </author> <title> Supporting Shared Data Structures on Distributed Memory Architectures. </title> <booktitle> In Proceedings Principles and Practice of Parallel Programming II, ACM, </booktitle> <month> March </month> <year> 1990. </year> <note> ACM. </note>
Reference-contexts: The problem of partitioning data aggregates (such as arrays) into tiles of data that are individually placed in the memory modules of multiprocessors commonly called the data partitioning problem or the domain decomposition problem is non-trivial and has been the focus of much recent attention <ref> [2, 3, 8, 16, 19] </ref>, but is not treated in this paper.
Reference: [17] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, J. Hennessy, M. Horowitz, and M. Lam. </author> <title> Design of the Stanford DASH Multiprocessor. </title> <institution> Computer Systems Laboratory TR 89-403, Stanford University, </institution> <month> December </month> <year> 1989. </year>
Reference-contexts: In bus-based machines, the memory is implemented as a single module accessed over the bus, while in most large-scale machines the memory is physically distributed among all the processing nodes. Virtually all machines that support the shared-memory programming abstraction provide local caches at each processor <ref> [1, 17, 18, 20, 21] </ref>. Caches automatically replicate memory locations close to the processor and avoid expensive network traversals for most memory accesses. Authors' e-mail: fagarwal,guttag,chadjic,mariosg@lcs.mit.edu. 1 The existence of caches does not change the memory abstraction.
Reference: [18] <institution> Encore Multimax. Encore, Marlboro, Massachusetts. </institution>
Reference-contexts: In bus-based machines, the memory is implemented as a single module accessed over the bus, while in most large-scale machines the memory is physically distributed among all the processing nodes. Virtually all machines that support the shared-memory programming abstraction provide local caches at each processor <ref> [1, 17, 18, 20, 21] </ref>. Caches automatically replicate memory locations close to the processor and avoid expensive network traversals for most memory accesses. Authors' e-mail: fagarwal,guttag,chadjic,mariosg@lcs.mit.edu. 1 The existence of caches does not change the memory abstraction.
Reference: [19] <author> J. Ramanujam and P. Sadayappan. </author> <title> Compile-Time Techniques for Data Distribution in Distributed Memory Machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 472-482, </pages> <month> October </month> <year> 1991. </year> <month> 20 </month>
Reference-contexts: The problem of partitioning data aggregates (such as arrays) into tiles of data that are individually placed in the memory modules of multiprocessors commonly called the data partitioning problem or the domain decomposition problem is non-trivial and has been the focus of much recent attention <ref> [2, 3, 8, 16, 19] </ref>, but is not treated in this paper.
Reference: [20] <author> James B. Rothnie. </author> <title> Architecture of the KSR1 Computer System, </title> <address> March 1992. </address> <publisher> MIT LCS Seminar, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: In bus-based machines, the memory is implemented as a single module accessed over the bus, while in most large-scale machines the memory is physically distributed among all the processing nodes. Virtually all machines that support the shared-memory programming abstraction provide local caches at each processor <ref> [1, 17, 18, 20, 21] </ref>. Caches automatically replicate memory locations close to the processor and avoid expensive network traversals for most memory accesses. Authors' e-mail: fagarwal,guttag,chadjic,mariosg@lcs.mit.edu. 1 The existence of caches does not change the memory abstraction.
Reference: [21] <institution> Sequent Symmetry. Sequent, Portland, Oregon. </institution>
Reference-contexts: In bus-based machines, the memory is implemented as a single module accessed over the bus, while in most large-scale machines the memory is physically distributed among all the processing nodes. Virtually all machines that support the shared-memory programming abstraction provide local caches at each processor <ref> [1, 17, 18, 20, 21] </ref>. Caches automatically replicate memory locations close to the processor and avoid expensive network traversals for most memory accesses. Authors' e-mail: fagarwal,guttag,chadjic,mariosg@lcs.mit.edu. 1 The existence of caches does not change the memory abstraction.
Reference: [22] <author> A. Wigderson. </author> <title> Improving the performance guarantee for approximate graph coloring. </title> <journal> JACM, </journal> <volume> 30(4) </volume> <pages> 729-735, </pages> <year> 1983. </year> <month> 21 </month>
Reference-contexts: Theorem 1 The zero-conflict memory assignment problem is computationally equivalent to the graph k-colorability problem. Proof. Straightforward reduction. The complete proof will be included in the final version of the paper. Graph k-colorability is among the most difficult intractable problems <ref> [5, 6, 7, 14, 22] </ref>.
References-found: 22

