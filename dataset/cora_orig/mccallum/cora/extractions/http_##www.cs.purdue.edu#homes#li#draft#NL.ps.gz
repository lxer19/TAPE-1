URL: http://www.cs.purdue.edu/homes/li/draft/NL.ps.gz
Refering-URL: http://www.cs.purdue.edu/homes/li/publications.html
Root-URL: http://www.cs.purdue.edu
Title: Interprocedural Analysis for Loop Scheduling and Data Allocation  
Author: Trung N. Nguyen a Zhiyuan Li b 
Address: 55906, USA  W. Lafayette, IN 47907, USA  
Affiliation: a IBM, Rochester, Minnesota  b Department of Computer Science, Purdue University,  
Abstract: In order to reduce remote memory accesses on CC-NUMA multiprocessors, we present an interprocedural analysis to support static loop scheduling and data allocation. Given a parallelized program, the compiler constructs graphs which represent globally and interprocedurally the remote reference penalties associated with different choices for loop scheduling and data allocation. After deriving an optimal solution according to those graphs, the compiler generates data allocation directives and schedules DOALL loops. Experiments indicate that the proposed compiler scheme is efficient and simulation results show good performance of the parallel code. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal, D. Kranz, and V. Natarajan. </author> <title> Automatic partitioning of parallel loops and data arrays for distributed shared memory multiprocessors. </title> <booktitle> In Proc. International Conference on Parallel Processing, volume I: Architecture, </booktitle> <pages> pages 2-11, </pages> <address> St. Charles, IL, </address> <year> 1993. </year>
Reference-contexts: Li and M. Chen [20] and by U. Kremer et al. [16] which target distributed memory multicomputers. W. Li and K. Pingali [21] consider loop transformations to enhance data locality, whereas we consider data allocation in conjunction with loop scheduling. Agarwal et al. <ref> [1] </ref> consider a single loop nest and we considers the whole program. Our work shares several aspects with the works done by the Stanford SUIF group: We both target the CC-NUMA architecture and both schedule multiple loop nests simultaneously. However, the work by A. Lim and M.
Reference: [2] <author> S. P. Amarasinghe, J. M. Anderson, M. S. Lam, and A. W. Lim. </author> <title> An overview of a compiler for scalable parallel machines. </title> <booktitle> In Proc. of the Sixth Workshop on Languages and Compilers for Parallel Computing. </booktitle> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1993. </year>
Reference-contexts: Where such solutions do not exist, they execute some of the loops as DOACROSS loops, while serializing some DOALL loops. (Details can be found in <ref> [4, 2] </ref>). Our method, on the other hand, primarily reduces decision-graphs, which leads to minimal remote references. We execute DOALL loops in parallel but not DOACROSS loops, because the former requires only barrier synchronization, while the latter requires point-to-point data synchronization.
Reference: [3] <author> J. M. Anderson, S. P. Amarasinghe, and M. S. Lam. </author> <title> Data and computation transformations for multiprocessors. </title> <booktitle> In Proc. of Fifth ACM SIGPLAN symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 166-178, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: This issue has not been addressed in previous works. A recent paper by J. Anderson, S. Amarasinghe and M. Lam, however, mentions that they have implemented an interprocedural version of their algorithm in SUIF <ref> [3] </ref>. Anderson and Lam's paper, on the other hand, considers more general array addressing functions than our work.
Reference: [4] <author> J. M. Anderson and M. S. Lam. </author> <title> Global optimizations for parallelism and locality on scalable parallel machines. </title> <booktitle> In Proc. ACM SIGPLAN Conf. on Prog. Lang. Design and Imp., </booktitle> <pages> pages 112-125, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Static loop scheduling is most suitable for such programs, not only because it reduces the scheduling overhead, but also because it can enhance data locality. The compiler can assign loop iterations which frequently access the same memory locations to the same processor <ref> [4, 15, 27, 19, 12, 18, 28] </ref>. Moreover, on cache-coherent non-uniform memory-access (CC-NUMA) multiprocessors, data frequently accessed in a particular loop iteration can be statically allocated to the processor that executes the iteration. <p> Our work is closer to the work done by J. Anderson and M. Lam <ref> [4] </ref> in that their objective is also to reduce remote memory references. <p> Where such solutions do not exist, they execute some of the loops as DOACROSS loops, while serializing some DOALL loops. (Details can be found in <ref> [4, 2] </ref>). Our method, on the other hand, primarily reduces decision-graphs, which leads to minimal remote references. We execute DOALL loops in parallel but not DOACROSS loops, because the former requires only barrier synchronization, while the latter requires point-to-point data synchronization. <p> The graph construction algorithms presented in this work can be extended to accommodate a wider class of array references if such extension is needed in practice. 7 As Anderson and Lam pointed out <ref> [4] </ref>, the alignment problem can be broken down into two optimization phases: 1. Phase I: Optimizing the orientation of the data allocation and the assignment of the parallel iterations. Suppose the VP's are numbered from 1 to P . <p> If a displacement is small, then its effect is not as important as choosing a good orientation because its effect usually diminishes due to large data cache lines in practice. 4.1 Constructing the connection graph Before starting the two optimizing phases, we create a connection graph <ref> [4] </ref> to allow the compiler to know which DOALL loops and arrays should be considered together. A connection graph is denoted by CG (d; a; e) where d and a are two sets of vertices and e is a set of undirected edges.
Reference: [5] <author> M. Berry, D. Chen, P. Koss, D. Kuck, and S. Lo. </author> <title> The PERFECT club benchmarks: Effective performance evaluation of supercomputers. </title> <type> Technical Report CSRD-827, </type> <institution> University of Illinois, Urbana, IL, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: Program # of array refs % parameter refs adi 75 28 tomcatv 67 0 mgrid 130 86 su2cor 2095 16 hydro2d 919 28 dyfesm 338 54 flo52q 550 49 track 131 16 in mgrid and some programs in the Perfect benchmarking suite <ref> [5] </ref>), our current implementation ignores these array references. 6 Simulation Results and Summary Through execution-driven simulations, we examine the effect of static scheduling of DOALL loops and data allocation on a CC-NUMA architecture.
Reference: [6] <author> K. Cooper, M. Hall, and K. Kennedy. </author> <title> Procedure cloning. </title> <booktitle> In Proc. IEEE International Conference on Computer Language, </booktitle> <address> Oakland, CA, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: Hence, the memory location represented by a formal parameter may vary from one invocation of the subroutine to another. Subroutine in-lining [7, 8, 11] can eliminate all formal parameters, and subroutine cloning <ref> [6] </ref> can result in a unique virtual address for every formal parameter. However, in-lining and cloning may make the resulting object code overly large and, thus, are performed to limited extent in practice.
Reference: [7] <author> K. Cooper, M. Hall, and L. Torczon. </author> <title> An experiment with inline substitution. </title> <journal> Software-Practice and Experience, </journal> <volume> 21(6) </volume> <pages> 581-601, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: In this example, A and B map to X and Y respectively at the first call statement and to C and D respectively at the second call statement. Hence, the memory location represented by a formal parameter may vary from one invocation of the subroutine to another. Subroutine in-lining <ref> [7, 8, 11] </ref> can eliminate all formal parameters, and subroutine cloning [6] can result in a unique virtual address for every formal parameter. However, in-lining and cloning may make the resulting object code overly large and, thus, are performed to limited extent in practice.
Reference: [8] <author> K. Cooper, M. Hall, and L. Torczon. </author> <title> Unexpected side effects of inline substitution a case study. </title> <journal> Letters on Programming Languages and Systems, </journal> <volume> 1(1), </volume> <month> March </month> <year> 1992. </year>
Reference-contexts: In this example, A and B map to X and Y respectively at the first call statement and to C and D respectively at the second call statement. Hence, the memory location represented by a formal parameter may vary from one invocation of the subroutine to another. Subroutine in-lining <ref> [7, 8, 11] </ref> can eliminate all formal parameters, and subroutine cloning [6] can result in a unique virtual address for every formal parameter. However, in-lining and cloning may make the resulting object code overly large and, thus, are performed to limited extent in practice.
Reference: [9] <author> G. W. Elsesser, V. N. Ngo, S. Bhattacharya, and W.-T. Tsai. </author> <title> Processor preallocation and load balancing of doall loops. </title> <journal> Journal of Supercomputing, </journal> <volume> 8 </volume> <pages> 135-161, </pages> <year> 1994. </year>
Reference-contexts: Many parallel loops in well-known benchmarking programs are found to have little variance in the workload of different iterations and such loops can be recognized by the compiler <ref> [9, 22] </ref>. Static loop scheduling is most suitable for such programs, not only because it reduces the scheduling overhead, but also because it can enhance data locality.
Reference: [10] <author> J. Gu, Z. Li, and G. Lee. </author> <title> Experience with efficient array data-flow analysis for array privatization. </title> <booktitle> In Proc. 6th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 157-167, </pages> <month> June </month> <year> 1997. </year> <month> 36 </month>
Reference-contexts: Based on the analysis result, the compiler performs loop scheduling and generates data allocation directives. We perform experiments within an advanced interprocedural parallelizing compiler, Panorama <ref> [10] </ref>, and conduct execution-driven simulations using a detailed CC-NUMA simulator. In the rest of this paper, we first review related work (Section 2) and give an overview of Panorama (Section 3). We then present a graph-based compiler analysis to determine loop iteration assignment and data allocation (Section 4). <p> Anderson and Lam's paper, on the other hand, considers more general array addressing functions than our work. They also consider multi-dimensional processor topology, while we consider one dimension only. 3 The Panorama parallelizing compiler Panorama is a source-to-source parallelizing Fortran compiler <ref> [10] </ref> developed when the authors were at the University of Minnesota. It uses a hierarchical supergraph (HSG), similar to the HSCG of the the PIPS project [14], to represent program control flow both intraprocedurally and interprocedurally. <p> As far as we know, the use of an interprocedural control flow graph and its associated array reference information for static loop scheduling and data allocation is unique to this work. Panorama performs efficient interprocedural array data flow analysis to enable array privatization and loop parallelization <ref> [10] </ref>. Loop compound nodes are marked as DOALL loops if the represented loops do not contain loop-carried dependences after program transformations. A set of symbolic evaluation utility routines are implemented to support array data flow analysis.
Reference: [11] <author> A. Holler. </author> <title> A Study of the Effects of Subprogram Inlining. </title> <type> PhD thesis, </type> <institution> University of Virginia, </institution> <address> Charlottesville, VA, </address> <month> May </month> <year> 1991. </year>
Reference-contexts: In this example, A and B map to X and Y respectively at the first call statement and to C and D respectively at the second call statement. Hence, the memory location represented by a formal parameter may vary from one invocation of the subroutine to another. Subroutine in-lining <ref> [7, 8, 11] </ref> can eliminate all formal parameters, and subroutine cloning [6] can result in a unique virtual address for every formal parameter. However, in-lining and cloning may make the resulting object code overly large and, thus, are performed to limited extent in practice.
Reference: [12] <author> C.H. Huang and P. Sadayappan. </author> <title> Communication-free hyperplane positioning of nested loops. </title> <booktitle> In Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 186-200. </pages> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: Static loop scheduling is most suitable for such programs, not only because it reduces the scheduling overhead, but also because it can enhance data locality. The compiler can assign loop iterations which frequently access the same memory locations to the same processor <ref> [4, 15, 27, 19, 12, 18, 28] </ref>. Moreover, on cache-coherent non-uniform memory-access (CC-NUMA) multiprocessors, data frequently accessed in a particular loop iteration can be statically allocated to the processor that executes the iteration.
Reference: [13] <author> Silicon Graphics Inc. </author> <title> POWER Fortran Accelerator User's Guide. </title>
Reference-contexts: Static scheduling methods perform such assignment at compile time, while dynamic scheduling methods leave the assignment decision to program execution time. 1 Corresponding author. E-mail li@cs.purdue.edu. The Silicon Graphics Challenge multiprocessor, for example, provides two static scheduling methods, simple and interleave, and two dynamic scheduling methods, gss and dynamic <ref> [13] </ref>. The simple scheduling method, also known as block scheduling, divides the loop iterations by the number of processors and then assigns each chunk of consecutive iterations to one processor.
Reference: [14] <author> F. Irigoin, P. Jouvelot, and R. Triolet. </author> <title> Semantical interprocedural parallelization: An overview of the pips project. </title> <booktitle> In Proc. Int. Conf. on Supercomputing, </booktitle> <pages> pages 244-251, </pages> <year> 1991. </year>
Reference-contexts: It uses a hierarchical supergraph (HSG), similar to the HSCG of the the PIPS project <ref> [14] </ref>, to represent program control flow both intraprocedurally and interprocedurally. The HSG provides sufficient interprocedural information to the compiler such that static scheduling and data allocation can be coordinated across procedure boundaries. 4 The HSG is a composition of the control flow subgraphs of all routines in a program.
Reference: [15] <author> Y. Ju and H. Dietz. </author> <title> Reduction of cache coherence overhead by compiler data layout and loop transformation. </title> <booktitle> In Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 344-358. </pages> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: Static loop scheduling is most suitable for such programs, not only because it reduces the scheduling overhead, but also because it can enhance data locality. The compiler can assign loop iterations which frequently access the same memory locations to the same processor <ref> [4, 15, 27, 19, 12, 18, 28] </ref>. Moreover, on cache-coherent non-uniform memory-access (CC-NUMA) multiprocessors, data frequently accessed in a particular loop iteration can be statically allocated to the processor that executes the iteration.
Reference: [16] <author> K. Kennedy and U. Kremer. </author> <title> Automatic data layout for High Performance Fortran. </title> <booktitle> In Proc. Supercomputing '95, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: In Section 6, we present simulation results and summarize this work. 2 Related Work This work targets shared-memory multiprocessors, which is different from previous works by J. Li and M. Chen [20] and by U. Kremer et al. <ref> [16] </ref> which target distributed memory multicomputers. W. Li and K. Pingali [21] consider loop transformations to enhance data locality, whereas we consider data allocation in conjunction with loop scheduling. Agarwal et al. [1] consider a single loop nest and we considers the whole program.
Reference: [17] <author> R. E. Kessler and M. D. Hill. </author> <title> Page placement algorithms for large real-indixed caches. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 10(4), </volume> <month> November </month> <year> 1992. </year>
Reference-contexts: Using this information, the simulator changes the virtual addresses of array data on the fly. In addition, a page-coloring page mapping scheme <ref> [17] </ref> is adopted for the mapping from a virtual page to a physical page.
Reference: [18] <author> D. Kulkarni, K.G. Kuman, A. Basu, and A. Paulraj. </author> <title> Loop partitioning for distributed memory multiprocessors as unimodular transformations. </title> <booktitle> In ACM Int. Conf. on Supercomputing, </booktitle> <pages> pages 206-215, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Static loop scheduling is most suitable for such programs, not only because it reduces the scheduling overhead, but also because it can enhance data locality. The compiler can assign loop iterations which frequently access the same memory locations to the same processor <ref> [4, 15, 27, 19, 12, 18, 28] </ref>. Moreover, on cache-coherent non-uniform memory-access (CC-NUMA) multiprocessors, data frequently accessed in a particular loop iteration can be statically allocated to the processor that executes the iteration.
Reference: [19] <author> J. Li and M. Chen. </author> <title> Index domain alignment: Minimizing cost of cross-referencing between distributed arrays. </title> <booktitle> In Frontiers '90: The Third Sym. on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 424-432. </pages> <publisher> IEEE, </publisher> <month> October </month> <year> 1990. </year>
Reference-contexts: Static loop scheduling is most suitable for such programs, not only because it reduces the scheduling overhead, but also because it can enhance data locality. The compiler can assign loop iterations which frequently access the same memory locations to the same processor <ref> [4, 15, 27, 19, 12, 18, 28] </ref>. Moreover, on cache-coherent non-uniform memory-access (CC-NUMA) multiprocessors, data frequently accessed in a particular loop iteration can be statically allocated to the processor that executes the iteration.
Reference: [20] <author> J. Li and M. Chen. </author> <title> The data alignment phase in compiling programs for distributed-memory machines. </title> <journal> J. Par. and Dist. Computing, </journal> <volume> 13 </volume> <pages> 213-221, </pages> <year> 1991. </year>
Reference-contexts: Both Section 4 and Section 5 contain static program statistics collected in our implementation. In Section 6, we present simulation results and summarize this work. 2 Related Work This work targets shared-memory multiprocessors, which is different from previous works by J. Li and M. Chen <ref> [20] </ref> and by U. Kremer et al. [16] which target distributed memory multicomputers. W. Li and K. Pingali [21] consider loop transformations to enhance data locality, whereas we consider data allocation in conjunction with loop scheduling. <p> The orientation graph augments the component affinity graph of J. Li and M. Chen <ref> [20] </ref> by adding nodes which represent DOALL loops to handle loop iteration assignment. We also introduce a "demand-driven" algorithm 9 for the construction of the OG, which reduces the size of the graph, an important consideration when we analyze large programs.
Reference: [21] <author> W. Li and K. Pingali. </author> <title> Access normalization: Loop restructuring for NUMA computers. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 11(4), </volume> <month> November </month> <year> 1993. </year>
Reference-contexts: In Section 6, we present simulation results and summarize this work. 2 Related Work This work targets shared-memory multiprocessors, which is different from previous works by J. Li and M. Chen [20] and by U. Kremer et al. [16] which target distributed memory multicomputers. W. Li and K. Pingali <ref> [21] </ref> consider loop transformations to enhance data locality, whereas we consider data allocation in conjunction with loop scheduling. Agarwal et al. [1] consider a single loop nest and we considers the whole program.
Reference: [22] <author> Z. Li and T. N. Nguyen. </author> <title> An empirical study of the work load distribution under static scheduling. </title> <booktitle> In Proc. International Conference on Parallel Processing, volume II: Software, </booktitle> <address> St. Charles, IL, </address> <year> 1994. </year>
Reference-contexts: Many parallel loops in well-known benchmarking programs are found to have little variance in the workload of different iterations and such loops can be recognized by the compiler <ref> [9, 22] </ref>. Static loop scheduling is most suitable for such programs, not only because it reduces the scheduling overhead, but also because it can enhance data locality.
Reference: [23] <author> A. W. Lim and M. S. Lam. </author> <title> Maximizing parallelism and minimizing synchronization with affine transforms. </title> <booktitle> In Proc. of the 24th SIGPLAN-SIGACT symposium on Principles of Programming Languages, </booktitle> <month> January </month> <year> 1997. </year>
Reference-contexts: Our work shares several aspects with the works done by the Stanford SUIF group: We both target the CC-NUMA architecture and both schedule multiple loop nests simultaneously. However, the work by A. Lim and M. Lam <ref> [23] </ref> addresses a different issue from ours. Their objective is to maximize parallelism and minimize barrier synchronization, whereas ours is to minimize remote memory references subject to the condition that the DOALL loop parallelism available both in the program and on the given machine will not be sacrificed.
Reference: [24] <author> T. N. Nguyen. </author> <title> Interprocedural compiler analysis for reducing memory latency. </title> <type> Technical report, PhD thesis, </type> <institution> University of Minnesota, </institution> <month> November </month> <year> 1996. </year>
Reference-contexts: This problem is NP-hard and we use a heuristics for its solution. The heuristics itself is not the emphasis of this paper and the interested readers are referred to <ref> [24] </ref>. Obviously, if we include all possible orientation nodes for each array or each DOALL loop in the OG, then there exist a number of possible SOG's which are all equivalent. <p> We use a linear-time heuristic algorithm to find a SDG <ref> [24] </ref>, but we leave out the algorithm because it is not the focus of this paper. 4.4 Estimating the mismatch costs The mismatch cost associated with each edge is the main factor in deciding what nodes are included in SOG and SDG. <p> The computation is dominated by the construction of DG whose time complexity was discussed earlier. (Our heuristic algorithm for finding SOG and SDG is a linear algorithm with respect to the number of nodes and edges <ref> [24] </ref>.) Table 1 shows the size of the connection graph and the number of connected components found in the connection graph for several numerical programs. We will give more descriptions of some of these programs in Section 6. <p> These results are intended as an overview to show that, as expected, alignment of DOALL loops and array data can improve the performance considerably. More detailed discussions can be found in <ref> [24] </ref>. The Panorama parallelizing compiler (c.f. Section 3) is used to parallelize and instrument the benchmarking programs for the experiments. It also provides the framework for static scheduling and data allocation. <p> Since Panorama is a source-to-source compiler, it does not generate virtual addresses for the machine code. Instead, Panorama inserts data allocation directives in the program, and in the NUMAsim simulator, we re-map the virtual addresses, generated by the SGI's f77 compiler, to reflect the selected data allocation scheme <ref> [24] </ref>. As mentioned previously, the directives inserted by Panorama allow the simulator to identify the starting address and the size of each array and what data allocation scheme to be used for that array. Using this information, the simulator changes the virtual addresses of array data on the fly. <p> Loops are not interchanged in those sequential codes, since interchange did not improve the sequential execution. The results achieved by our compiler have been compared favorably against several straightforward data allocation schemes which do not perform alignment <ref> [24] </ref>. To simplify the graphs which exhibit the performance curves, however, we pick only two schemes as reference points in this paper. The first reference scheme used for comparison is flat-space interleave, which partitions the entire address space into chunks of cache line size.
Reference: [25] <author> C. Polychronopoulos and D. J. Kuck. </author> <title> Guided self scheduling: A practical scheduling scheme for parallel computers. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-36(12):1425-1439, </volume> <month> December </month> <year> 1987. </year> <month> 37 </month>
Reference-contexts: With dynamic scheduling, the iterations are also divided into CHUNK-sized chunks. As each processor finishes a chunk, however, it enters a critical section to grab the next available chunk. With gss scheduling <ref> [25] </ref> , the chunk size is varied, depending on the number of iterations remaining. The speedup of parallel loop execution on shared memory machines is determined by program parallelism, data locality, scheduling and synchronization overhead, and load balance.
Reference: [26] <author> W. H. Press, B. P. Flannery, S. A. Teukolsky, and W. T. Vetterling. </author> <title> Numerical Recipes: The Art of Scientific Computing (Fortran Version). </title> <publisher> Cambridge University Press, </publisher> <year> 1989. </year>
Reference-contexts: These programs are chosen for their relatively high degree of parallelism and their reasonable simulation time. This section provides a short descriptions of these test programs. ADI is a numerical algorithm used to solve partial differential equations <ref> [26] </ref>. The main data in ADI is nine 256x256 arrays of double words. There are two main DOALL loop nests with 254 iterations at each level. One array is swept along the columns in one loop nest and then along the rows in the other.
Reference: [27] <author> V. Sarkar and G. R. Gao. </author> <title> Optimization of array accesses by collective loop transformations. </title> <booktitle> In ACM Int. Conf.s on Supercomputing, </booktitle> <pages> pages 194-204, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Static loop scheduling is most suitable for such programs, not only because it reduces the scheduling overhead, but also because it can enhance data locality. The compiler can assign loop iterations which frequently access the same memory locations to the same processor <ref> [4, 15, 27, 19, 12, 18, 28] </ref>. Moreover, on cache-coherent non-uniform memory-access (CC-NUMA) multiprocessors, data frequently accessed in a particular loop iteration can be statically allocated to the processor that executes the iteration.
Reference: [28] <author> P.-S. Tseng. </author> <title> A parallelizing compiler for distributed memory parallel computers. </title> <type> Technical Report CMU-CS-91-121, PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: Static loop scheduling is most suitable for such programs, not only because it reduces the scheduling overhead, but also because it can enhance data locality. The compiler can assign loop iterations which frequently access the same memory locations to the same processor <ref> [4, 15, 27, 19, 12, 18, 28] </ref>. Moreover, on cache-coherent non-uniform memory-access (CC-NUMA) multiprocessors, data frequently accessed in a particular loop iteration can be statically allocated to the processor that executes the iteration.
Reference: [29] <author> J. E. Veenstra and R. J. Fowler. </author> <title> MINT tutorial and user manual. </title> <type> Technical Report 452, </type> <institution> University of Rochester, </institution> <month> June </month> <year> 1993. </year> <institution> Dep. of Computer Science. </institution> <month> 38 </month>
Reference-contexts: The SGI Challenge provides run-time libraries which implement the loop scheduling methods mentioned in the introduction. The executable object code is simulated by NUMASim, an execution-driven CC-NUMA cache simulator. NUMAsim consists of two main parts: the MINT event generator front-end <ref> [29] </ref> and our detailed CC-NUMA cache simulator back-end. The MINT front-end interprets each instruction in the executable object 24 code and schedules events such as loads and stores for the back-end to sim-ulate.
References-found: 29

