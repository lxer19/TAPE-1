URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/project/link/www/papers/ps/repulsion.ps
Refering-URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/project/link/www/homepage.html
Root-URL: 
Email: &lt;dougb,aberger,lafferty&gt;@cs.cmu.edu  
Title: A Model of Lexical Attraction and Repulsion  
Author: Doug Beeferman Adam Berger John Lafferty 
Address: Pittsburgh, PA 15213 USA  
Affiliation: School of Computer Science Carnegie Mellon University  
Abstract: This paper introduces new methods based on exponential families for modeling the correlations between words in text and speech. While previous work assumed the effects of word co-occurrence statistics to be constant over a window of several hundred words, we show that their influence is nonstationary on a much smaller time scale. Empirical data drawn from English and Japanese text, as well as conversational speech, reveals that the "attraction" between words decays exponentially, while stylistic and syntactic contraints create a "repulsion" between words that discourages close co-occurrence. We show that these characteristics are well described by simple mixture models based on two-stage exponential distributions which can be trained using the EM algorithm. The resulting distance distributions can then be incorporated as penalizing features in an exponential language model.
Abstract-found: 1
Intro-found: 1
Reference: <author> Berger, A., S. Della Pietra, and V. Della Pietra. </author> <year> 1996. </year> <title> A maximum entropy approach to natural language processing. </title> <journal> Computational Linguistics, </journal> <volume> 22(1) </volume> <pages> 39-71. </pages>
Reference: <author> Cover, T.M. and J.A. Thomas. </author> <year> 1991. </year> <title> Elements of Information Theory. </title> <publisher> John Wiley. </publisher>
Reference-contexts: The second is ignorance: we know so little about how to model effectively the nonstationary characteristics of language that we have for the most part completely neglected the problem. From a theoretical standpoint, we appeal to the Shannon-McMillan-Breiman theorem <ref> (Cover and Thomas, 1991) </ref> whenever computing perplexities on test data; yet this result only rigorously applies to stationary and ergodic sources.
Reference: <author> Csiszar, I. </author> <year> 1996. </year> <note> Maxent, mathematics, and information theory. </note> <editor> In K. Hanson and R. Silver, editors, </editor> <title> Maximum Entropy and Bayesian Methods. </title> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: In the models that we built, feature f i is an indicator function, testing for the occurrence of a trigger pair (s i ; t i ): n 0 otherwise. The use of the trigram model as a default distribution <ref> (Csiszar, 1996) </ref> in this manner is new in language modeling. (One might also use the term prior , although q (w j H ) is not a prior in the strict Bayesian sense.) Previous work using maximum entropy methods incorporated trigram constraints as p 1 ; 2 ;ff using the EM
Reference: <author> Della Pietra, S., V. Della Pietra, and J. Lafferty. </author> <year> 1997. </year> <title> Inducing features of random fields. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 19(3), </volume> <month> March. </month>
Reference-contexts: Finally, assuming robust estimates for the parameters i , the resulting model is essentially guaranteed to be superior to the trigram model. The training algorithm we use for estimating the parameters is the Improved Iterative Scaling (IIS) algorithm introduced in <ref> (Della Pietra, Della Pietra, and Lafferty, 1997) </ref>. To include distance models in the word predictions, we treat the distribution on the separation k between s i and t i in a trigger pair (s i ; t i ) as a prior.
Reference: <author> Dempster, A.P., N.M. Laird, and D.B. Rubin. </author> <year> 1977. </year> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, 39(B):1-38. </journal>
Reference-contexts: The difficulty is that there is a sum inside the logarithm, and direct differentiation results in coupled equations for 1 and 2 . Our solution to this problem is to view the con-volving index j as a hidden variable and apply the EM algorithm <ref> (Dempster, Laird, and Rubin, 1977) </ref>. Recall that the interpretation of j is the time used to pass through the first queue; that is, the number of words used to satisfy the linguistic constraints of lexical exclusion.
Reference: <author> Gelman, A., J. Carlin, H. Stern, and D. Rubin. </author> <year> 1995. </year> <title> Bayesian Data Analysis. </title> <publisher> Chapman & Hall, London. </publisher>
Reference-contexts: This is the Gamma distribution p ff;fi (x) = (ff) This distribution has mean ff=fi and variance ff=fi 2 and thus can afford greater flexibility in fitting the empirical data. For Bayesian analysis, this distribution is appropriate as the conjugate prior for the exponential parameter <ref> (Gelman et al., 1995) </ref>. Using this family, however, sacrifices the linguistic interpretation of the two-stage model. 4 Estimating the Parameters In this section we present a solution to the problem of estimating the parameters of the distance models introduced in the previous section.
Reference: <author> Godfrey, J., E. Holliman, and J. McDaniel. </author> <year> 1992. </year> <title> SWITCHBOARD: Telephone speech corpus for research and development. </title> <booktitle> In Proc. </booktitle> <address> ICASSP-92. </address>
Reference-contexts: An example of this law of large numbers is shown in Figure 4. These empirical phenomena are not restricted to the Wall Street Journal corpus. In fact, we have observed similar behavior in conversational speech and Japanese text. The corresponding data for self triggers in the Switchboard data <ref> (Godfrey, Holliman, and McDaniel, 1992) </ref>, for instance, exhibits the same bump in p (k) for small k, though the peak is closer to zero.
Reference: <author> Jelinek, F., B. Merialdo, S. Roukos, and M. Strauss. </author> <year> 1991. </year> <title> A dynamic language model for speech recognition. </title> <booktitle> In Proceedings of the DARPA Speech and Natural Language Workshop, </booktitle> <pages> pages 293-295, </pages> <month> February. </month>
Reference: <author> Katz, S. </author> <year> 1987. </year> <title> Estimation of probabilities from sparse data for the langauge model component of a speech recognizer. </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> ASSP-35(3):400-401, </volume> <month> March. </month>
Reference-contexts: In algorithm at work. 5 A Nonstationary Language Model To incorporate triggers and distance models into a long-distance language model, we begin by constructing a standard, static backoff trigram model <ref> (Katz, 1987) </ref>, which we will denote as q (w 0 j w 1 ; w 2 ). For the purposes of building a model for the Wall Street Journal data, this trigram model is quickly trained on the entire 38-million word corpus.
Reference: <author> Kleinrock, L. </author> <year> 1975. </year> <title> Queueing Systems. Volume 1: Theory. </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference: <author> Kuhn, R. and R. de Mori. </author> <year> 1990. </year> <title> A cache-based natural language model for speech recognition. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 12 </volume> <pages> 570-583. </pages>
Reference: <author> Ney, H., U. Essen, and R. Kneser. </author> <year> 1994. </year> <title> On structuring probabilistic dependencies in stochastic language modeling. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 8 </volume> <pages> 1-38. </pages>
Reference-contexts: Indeed, there are tables in (Rosenfeld, 1996) which suggest that this is so, and distance-dependent "memory weights" are proposed in <ref> (Ney, Essen, and Kneser, 1994) </ref>. We decided to investigate the effect of distance in more detail, and were surprised by what we found. corpus|for one of the non-self trigger groups (left) and one of the self trigger groups (right).
Reference: <author> Niesler, T. and P. Woodland. </author> <year> 1997. </year> <title> Modelling word-pair relations in a category-based language model. </title> <booktitle> In Proceedings of ICASSP-97, </booktitle> <address> Munich, Germany, </address> <month> April. </month>
Reference-contexts: In the following section we present examples of the empirical evidence for the effects of distance. In Section 3 we outline the class of statistical models that we propose to model this data. After completing this work we learned of a related paper <ref> (Niesler and Woodland, 1997) </ref> which constructs similar models. In Section 4 we present a parameter estimation algorithm, based on the EM algorithm, for determining the maximum likelihood estimates within the class.
Reference: <author> Rosenfeld, R. </author> <year> 1996. </year> <title> A maximum entropy approach to adaptive statistical language modeling. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 10 </volume> <pages> 187-228. </pages>
Reference-contexts: Another approach, using maximum entropy methods similar to those that we present here, introduces a parameter for trigger pairs of mutually informative words, so that the occurrence of certain words in recent context boosts the probability of the words that they trigger <ref> (Rosenfeld, 1996) </ref>. Triggers have also been incorporated through different methods (Kuhn and de Mori, 1990; Ney, Essen, and Kneser, 1994). <p> The features we used were simple "trigger pairs" of words that were chosen on the basis of mutual information. Figure 1 provides a small sample of the 41; 263 (s; t) trigger pairs used in most of the experiments we will describe. In earlier work, for example <ref> (Rosenfeld, 1996) </ref>, the distance between the words of a trigger pair (s; t) plays no role in the model, meaning that the "boost" in probability which t receives following its trigger s is independent of how long ago s occurred, so long as s appeared somewhere in the history H, a <p> Hungary Bulgaria UP#H HS Japan Air to fly cargo VJ EI sentence proposed punishment !L ND transplant organ M ( "A forest wastepaper 1?7:T3T 925 computer host 33 million word Nikkei corpus. further and further back into the context. Indeed, there are tables in <ref> (Rosenfeld, 1996) </ref> which suggest that this is so, and distance-dependent "memory weights" are proposed in (Ney, Essen, and Kneser, 1994). <p> The dashed line is the fitted curve. For the non-self trigger plot 1 = 7, 2 = 0:0148, and ff = 0:253. For the self trigger plot 1 = 0:29, 2 = 0:0168, and ff = 0:224. explicit features <ref> (Rosenfeld, 1996) </ref>, using the uniform distribution as the default model. There are several advantages to incorporating trigrams in this way. The trigram component can be efficiently constructed over a large volume of data, using standard software or including the various sophisticated techniques for smoothing that have been developed.
References-found: 14

