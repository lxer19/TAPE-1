URL: http://www.pdos.lcs.mit.edu/~kaashoek/papers/tse.ps
Refering-URL: http://www.pdos.lcs.mit.edu/~kaashoek/papers.html
Root-URL: 
Title: ORCA: A LANGUAGE FOR PARALLEL PROGRAMMING OF DISTRIBUTED SYSTEMS  
Author: Henri E. Bal M. Frans Kaashoek Andrew S. Tanenbaum 
Address: Amsterdam, The Netherlands  
Affiliation: Dept. of Mathematics and Computer Science Vrije Universiteit  
Abstract: Orca is a language for implementing parallel applications on loosely coupled distributed systems. Unlike most languages for distributed programming, it allows processes on different machines to share data. Such data are encapsulated in data-objects, which are instances of user-defined abstract data types. The implementation of Orca takes care of the physical distribution of objects among the local memories of the processors. In particular, an implementation may replicate and/or migrate objects in order to decrease access times to objects and increase parallelism. This paper gives a detailed description of the Orca language design and motivates the design choices. Orca is intended for applications programmers rather than systems programmers. This is reflected in its design goals to provide a simple, easy to use language that is type-secure and provides clean semantics. The paper discusses three example parallel applications in Orca, one of which is described in detail. It also describes one of the existing implementations, which is based on reliable broadcasting. Performance measurements of this system are given for three parallel applications. The measurements show that significant speedups can be obtained for all three applications. Finally, the paper compares Orca with several related languages and systems.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> A.S. Tanenbaum, R. van Renesse, H. van Staveren, G.J. Sharp, S.J. Mullender, A.J. Jan-sen, and G. van Rossum, </author> <title> ``Experiences with the Amoeba Distributed Operating System,'' </title> <journal> Comm. ACM 33(2), </journal> <pages> pp. </pages> <month> 46-63 (Dec. </month> <year> 1990). </year>
Reference-contexts: 1. INTRODUCTION As communication in loosely coupled distributed computing systems gets faster, such systems become more and more attractive for running parallel applications. In the Amoeba system, for example, the cost of sending a short message between Sun workstations over an Ethernet is 1.1 milliseconds <ref> [1] </ref>. Although this is still slower than communication in most multi-computers (e.g., hypercubes and transputer grids), it is fast enough for many coarse-grained parallel applications.
Reference: 2. <author> H.E. Bal, R. van Renesse, </author> <title> and A.S. Tanenbaum, ``Implementing Distributed Algorithms Using Remote Procedure Calls,'' </title> <booktitle> Proc. AFIPS Nat. Computer Conf., Chicago, Ill. </booktitle> <volume> 56, </volume> <pages> pp. 499-506, </pages> <note> AFIPS Press (June 1987). </note>
Reference-contexts: In our research, we are studying the implementation of parallel applications on distributed systems. We started out by implementing several coarse-grained parallel applications on top of the Amoeba system, using an existing sequential language extended with message passing for interprocess communication <ref> [2] </ref>. We felt that, for parallel applications, both the use of message passing and a sequential base language have many disadvantages, making them complicated for applications programmers to use. Since then, we have developed a new language for distributed programming, called Orca [3, 4, 5].
Reference: 3. <author> H.E. Bal, </author> <title> Programming Distributed Systems, </title> <publisher> Silicon Press, Summit, </publisher> <address> NJ (1990). </address>
Reference-contexts: We felt that, for parallel applications, both the use of message passing and a sequential base language have many disadvantages, making them complicated for applications programmers to use. Since then, we have developed a new language for distributed programming, called Orca <ref> [3, 4, 5] </ref>. Orca is intended for distributed applications programming rather than systems programming, and is therefore designed to be a simple, expressive, and efficient language with clean semantics. Below, we will briefly discuss the most important novelties in the language design. <p> If separate graphs are used, one will have to be copied into the other. Another disadvantage is the run-time overhead of graphs. A graph is represented as a table with pointers to the actual nodes, so the nodes are accessed indirectly through this table <ref> [3] </ref>. Also, there is a cost in making graphs type-secure, since each node access has to be validated. We are currently working on decreasing these costs through global optimizations. 3. <p> The implementation of the language therefore should hide the physical distribution of the hardware and simulate shared data in an efficient way. We have several implementations of the language <ref> [3] </ref>.
Reference: 4. <author> H.E. Bal and A.S. Tanenbaum, </author> <title> ``Distributed Programming with Shared Data,'' </title> <booktitle> Proc. IEEE CS 1988 Int. Conf. on Computer Languages, </booktitle> <address> Miami, Fl., </address> <pages> pp. </pages> <month> 82-91 (Oct. </month> <year> 1988). </year>
Reference-contexts: We felt that, for parallel applications, both the use of message passing and a sequential base language have many disadvantages, making them complicated for applications programmers to use. Since then, we have developed a new language for distributed programming, called Orca <ref> [3, 4, 5] </ref>. Orca is intended for distributed applications programming rather than systems programming, and is therefore designed to be a simple, expressive, and efficient language with clean semantics. Below, we will briefly discuss the most important novelties in the language design.
Reference: 5. <author> H.E. Bal, M.F. Kaashoek, </author> <title> and A.S. Tanenbaum, ``Experience with Distributed Programming in Orca,'' </title> <booktitle> Proceedings IEEE CS 1990 International Conference on Computer Languages, </booktitle> <address> New Orleans, LA, </address> <pages> pp. </pages> <month> 79-89 (March </month> <year> 1990). </year>
Reference-contexts: We felt that, for parallel applications, both the use of message passing and a sequential base language have many disadvantages, making them complicated for applications programmers to use. Since then, we have developed a new language for distributed programming, called Orca <ref> [3, 4, 5] </ref>. Orca is intended for distributed applications programming rather than systems programming, and is therefore designed to be a simple, expressive, and efficient language with clean semantics. Below, we will briefly discuss the most important novelties in the language design.
Reference: 6. <author> K. Li and P. Hudak, </author> <title> ``Memory Coherence in Shared Virtual Memory Systems,'' </title> <booktitle> Proc. 5th Ann. ACM Symp. on Princ. of Distr. Computing, </booktitle> <address> Calgary, Canada, </address> <pages> pp. </pages> <month> 229-239 (Aug. </month> <year> 1986). </year>
Reference-contexts: Processes in Orca can communicate through shared data, even if the processors on which they run do not have physical shared memory. The main novelty of our approach is the way access to shared data is expressed. Unlike shared physical memory (or distributed shared memory <ref> [6] </ref>), shared data in Orca are accessed through user-defined high-level operations, which, as we will see, has many important implications. Supporting shared data on a distributed system imposes some challenging implementation problems. We have worked on several implementations of Orca, one of which we will describe in the paper. <p> A memory model that looks to the user as a shared memory but is implemented on disjoint machines is referred to as Distributed Shared Memory (DSM). Many different forms of DSM exist. Li's Shared Virtual Memory (SVM) <ref> [6] </ref> is perhaps the best-known example. It simulates physical shared-memory on a distributed system. The SVM distributes the pages of the memory space over the local memories. Read-only pages may also be replicated. SVM provides a clean, simple model, but unfortunately there are many problems in implementing it efficiently. <p> As discussed above, one way of doing this is by distinguishing between read and write operations and executing reads in parallel on local copies; more advanced implementations are also feasible. Shared Virtual Memory Shared Virtual Memory (SVM) <ref> [6] </ref> simulates physical shared memory on a distributed system. It partitions the global address space into fixed-sized pages, just as with virtual memory. Each processor contains some portion of the pages. <p> Consequently, we have a choice between invalidating objects after a write operation or updating them by applying the operation to all copies (or, alternatively, sending the new value). With SVM, there is no such choice; only invalidating pages is viable <ref> [6] </ref>. In many cases, however, invalidating copies will be far less efficient than updating them. Several researchers have tried to solve this performance problem by relaxing the consistency constraints of the memory (e.g., [37, 38]).
Reference: 7. <author> C. Ghezzi and M. Jazayeri, </author> <title> Programming Language Concepts, </title> <publisher> John Wiley, </publisher> <address> New York, NY (1982). </address>
Reference-contexts: Language designers frequently have to choose between adding language features or adding compiler optimizations. In general, we prefer the latter option. We will discuss several examples of this design principle in the paper. Finally, the principle of orthogonality <ref> [7] </ref> is used with care, but it is not a design goal by itself. - 3 - Another issue we have taken into account is that of debugging.
Reference: 8. <author> A.D. Birrell and B.J. Nelson, </author> <title> ``Implementing Remote Procedure Calls,'' </title> <journal> ACM Trans. Comp. Syst. </journal> <volume> 2(1), </volume> <pages> pp. </pages> <month> 39-59 (Feb. </month> <year> 1984). </year>
Reference-contexts: In Section 4, we will discuss one implementation of Orca, based on reliable broadcast. We will also describe how to implement this broadcast primitive on top of LANs that only support unreliable broadcast. We will briefly compare this system with another implementation of Orca that uses Remote Procedure Call <ref> [8] </ref> rather than broadcasting. In Section 5, we will give performance measurements for several applications. In Section 6, we will compare our approach with those of related languages and systems. Finally, in Section 7 we will present our conclusions. 2. ORCA Orca is a procedural, strongly typed language.
Reference: 9. <author> H.E. Bal, J.G. </author> <title> Steiner, and A.S. Tanenbaum, </title> <booktitle> ``Programming Languages for Distributed Computing Systems,'' ACM Computing Surveys 21(3) (Sept. </booktitle> <year> 1989). </year>
Reference-contexts: Synchronization of operations on shared objects is discussed next, followed by a discussion of hierarchically used objects. Finally, we will look at Orca's data structures. 2.1. Distributed Shared Memory Most languages for distributed programming are based on message passing <ref> [9] </ref>. This choice seems obvious, since the underlying hardware already supports message passing. Still, there are many cases in which message passing is not the appropriate programming model. Message passing is a form of communication between two parties, which interact explicitly by sending and receiving messages.
Reference: 10. <author> R. Bisiani and A. Forin, </author> <title> ``Architectural Support for Multilanguage Parallel Programming on Heterogenous Systems,'' </title> <booktitle> Proc. 2nd Int. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Palo Alto, Calif., </address> <pages> pp. </pages> <month> 21-30 (Oct. </month> <year> 1987). </year>
Reference-contexts: The literature contains numerous other examples of distributed applications and - 4 - algorithms that would greatly benefit from support for shared data, even if no physical shared memory is available. Applications described in the literature include: a distributed speech recognition system <ref> [10] </ref>; linear equation solving, three-dimensional partial differential equations, and split-merge sort [11]; computer chess [12]; distributed system services (e.g., name service, time service), global scheduling, and replicated files [13]. So, the difficulty in providing (logically) shared data makes message passing a poor match for many applications.
Reference: 11. <author> K. Li, ``IVY: </author> <title> A Shared Virtual Memory System for Parallel Computing,'' </title> <booktitle> Proc. 1988 Int. Conf. Parallel Processing (Vol. II), </booktitle> <address> St. Charles, </address> <publisher> Ill., </publisher> <pages> pp. </pages> <month> 94-101 (Aug. </month> <year> 1988). </year>
Reference-contexts: Applications described in the literature include: a distributed speech recognition system [10]; linear equation solving, three-dimensional partial differential equations, and split-merge sort <ref> [11] </ref>; computer chess [12]; distributed system services (e.g., name service, time service), global scheduling, and replicated files [13]. So, the difficulty in providing (logically) shared data makes message passing a poor match for many applications.
Reference: 12. <author> E.W. Felten and S.W. Otto, </author> <title> ``A Highly Parallel Chess Program,'' </title> <booktitle> Proc. of the Int. Conf. - 32 - on Fifth Generation Computer Systems 1988, Tokyo, </booktitle> <pages> pp. </pages> <month> 1001-1009 (Nov. </month> <year> 1988). </year>
Reference-contexts: Applications described in the literature include: a distributed speech recognition system [10]; linear equation solving, three-dimensional partial differential equations, and split-merge sort [11]; computer chess <ref> [12] </ref>; distributed system services (e.g., name service, time service), global scheduling, and replicated files [13]. So, the difficulty in providing (logically) shared data makes message passing a poor match for many applications. Several researchers have therefore worked on communication models based on logically shared data rather than message passing.
Reference: 13. <author> D.R. Cheriton, </author> <title> ``Preliminary Thoughts on Problem-oriented Shared Memory: A Decentralized Approach to Distributed Systems,'' </title> <booktitle> ACM Operating Systems Review 19(4), </booktitle> <pages> pp. </pages> <month> 26-33 (Oct. </month> <year> 1985). </year>
Reference-contexts: Applications described in the literature include: a distributed speech recognition system [10]; linear equation solving, three-dimensional partial differential equations, and split-merge sort [11]; computer chess [12]; distributed system services (e.g., name service, time service), global scheduling, and replicated files <ref> [13] </ref>. So, the difficulty in providing (logically) shared data makes message passing a poor match for many applications. Several researchers have therefore worked on communication models based on logically shared data rather than message passing.
Reference: 14. <author> S. Ahuja, N. Carriero, and D. Gelernter, ``Linda and Friends,'' </author> <booktitle> IEEE Computer 19(8), </booktitle> <pages> pp. </pages> <month> 26-34 (Aug. </month> <year> 1986). </year>
Reference-contexts: The SVM distributes the pages of the memory space over the local memories. Read-only pages may also be replicated. SVM provides a clean, simple model, but unfortunately there are many problems in implementing it efficiently. A few existing programming languages also fall into the DSM class. Linda <ref> [14] </ref> supports a globally shared Tuple Space, which processes can access using a form of associative addressing. On distributed systems, Tuple Space can be replicated or partitioned, much as pages in SVM are. <p> Read operations can be applied to the local copy, without any message passing being involved. Moreover, processes located on different processors can apply read operations simultaneously, without losing any parallelism. Linda's Tuple Space Linda <ref> [14] </ref> is one of the first languages to recognize the disadvantages of central manager processes for guarding shared data. Linda supports so-called distributed data structures, which can be accessed simultaneously by multiple processes. In contrast, object-based languages typically serialize access to shared data structures.
Reference: 15. <author> E. Jul, H. Levy, N. Hutchinson, and A. Black, </author> <title> ``Fine-Grained Mobility in the Emerald System,'' </title> <journal> ACM Trans. Comp. Syst. </journal> <volume> 6(1), </volume> <pages> pp. </pages> <month> 109-133 (Feb. </month> <year> 1988). </year>
Reference-contexts: On distributed systems, Tuple Space can be replicated or partitioned, much as pages in SVM are. The operations allowed on Tuple Space are low-level and built-in, which, as we will argue later, complicates programming and makes an efficient distributed implementation hard. The Emerald language <ref> [15] </ref> is related to the DSM class, in that it provides a shared name space for objects, together with a location-transparent invocation mechanism. Emerald does not use any of the replication techniques that are typical of DSM systems, however. <p> In particular, we will look at objects (as used in parallel object-based languages), Linda's Tuple Space, and Shared Virtual Memory. - 28 - Objects Objects are used in many object-based languages for parallel or distributed programming, such as Emerald <ref> [15] </ref>, Amber [34], and ALPS [35]. Objects in such languages typically have two parts: 1. Encapsulated data. 2. A manager process that controls access to the data. The data are accessed by sending a message to the manager process, asking it to perform a certain operation on the data.
Reference: 16. <author> B. Liskov, </author> <title> ``Distributed Programming in Argus,'' </title> <journal> Commun. ACM 31(3), </journal> <pages> pp. </pages> <month> 300-312 (March </month> <year> 1988). </year>
Reference-contexts: All other objects are local and are treated as normal variables of an abstract data type. Most other languages use different mechanisms for these two purposes. Argus <ref> [16] </ref>, for example, uses clusters for local data and guardians for shared data; clusters and guardians are completely different. SR [17] provides a single mechanism (resources), but the overhead of operations on resources is far too high to be useful for sequential abstract data types [18].
Reference: 17. <author> G.R. Andrews, R.A. Olsson, M. Coffin, I. Elshoff, K. Nilsen, T. Purdin, and G. Town-send, </author> <title> ``An Overview of the SR Language and Implementation,'' </title> <journal> ACM Trans. Program. Lang. Syst. </journal> <volume> 10(1), </volume> <pages> pp. </pages> <month> 51-86 (Jan. </month> <year> 1988). </year>
Reference-contexts: All other objects are local and are treated as normal variables of an abstract data type. Most other languages use different mechanisms for these two purposes. Argus [16], for example, uses clusters for local data and guardians for shared data; clusters and guardians are completely different. SR <ref> [17] </ref> provides a single mechanism (resources), but the overhead of operations on resources is far too high to be useful for sequential abstract data types [18]. The fact that shared data are accessed through user-defined operations is an important distinction between our model and other models.
Reference: 18. <author> H.E. Bal, </author> <title> ``An Evaluation of the SR Language Design,'' </title> <type> report IR-219, </type> <institution> Vrije Universi-teit, </institution> <note> Amsterdam (August 1990). </note>
Reference-contexts: Argus [16], for example, uses clusters for local data and guardians for shared data; clusters and guardians are completely different. SR [17] provides a single mechanism (resources), but the overhead of operations on resources is far too high to be useful for sequential abstract data types <ref> [18] </ref>. The fact that shared data are accessed through user-defined operations is an important distinction between our model and other models. Shared virtual memory, for example, simulates physical shared memory, so shared data are accessed through low-level read and write operations.
Reference: 19. <author> C.A.R. Hoare, </author> <title> ``Monitors: An Operating System Structuring Concept,'' </title> <journal> Commun. ACM 17(10), </journal> <pages> pp. </pages> <month> 549-557 (Oct. </month> <year> 1974). </year>
Reference-contexts: As we will see in Section 4, one way to achieve this goal is to replicate shared data-objects. By replicating objects, access control to shared objects is decentralized, which decreases access costs and increases parallelism. This is a major difference with, say, monitors <ref> [19] </ref>, which centralize control to shared data. 2.4. Synchronization An abstract data type in Orca can be used for creating shared as well as local objects. For objects that are shared among multiple processes, the issue of synchronization arises.
Reference: 20. <author> G.R. Andrews and F.B. Schneider, </author> <title> ``Concepts and Notations for Concurrent Programming,'' </title> <journal> ACM Computing Surveys 15(1), </journal> <pages> pp. </pages> <month> 3-43 (March </month> <year> 1983). </year>
Reference-contexts: Synchronization An abstract data type in Orca can be used for creating shared as well as local objects. For objects that are shared among multiple processes, the issue of synchronization arises. Two types of synchronization exist: mutual exclusion synchronization and condition synchronization <ref> [20] </ref>. We will look at them in turn. Mutual exclusion synchronization Mutual exclusion in our model is done implicitly, by executing all operations on objects indivisibly. Conceptually, each operation locks the entire object it is applied to, does the work, and releases the lock only when it is finished.
Reference: 21. <author> K.P. Eswaran, J.N. Gray, R.A. Lorie, </author> <title> and I.L. Traiger, ``The Notions of Consistency and Predicate Locks in a Database System,'' </title> <journal> Commun. ACM 19(11), </journal> <pages> pp. </pages> <month> 624-633 (Nov. </month> <year> 1976). </year>
Reference-contexts: Mutual exclusion synchronization Mutual exclusion in our model is done implicitly, by executing all operations on objects indivisibly. Conceptually, each operation locks the entire object it is applied to, does the work, and releases the lock only when it is finished. To be more precise, the model guarantees seri-alizability <ref> [21] </ref> of operation invocations: if two operations are applied simultaneously to the same data-object, then the result is as if one of them is executed before the other; the order of - 8 - invocation, however, is nondeterministic.
Reference: 22. <author> S.E. Lucco, </author> <title> ``Parallel Programming in a Virtual Object Space,'' </title> <booktitle> SIGPLAN Notices (Proc. Object-Oriented Programming Systems, Languages and Applications 1987), </booktitle> <address> Orlando, FL 22(12), </address> <pages> pp. </pages> <month> 26-34 (Dec. </month> <year> 1987). </year>
Reference-contexts: This rule for defining which actions are indivisible and which are not is both easy to understand and flexible: single operations are indivisible; sequences of operations are not. The model does not provide mutual exclusion at a granularity lower than the object level. Other languages (e.g., Sloop <ref> [22] </ref>) give programmers more accurate control over mutual exclusion synchronization. Our model does not support indivisible operations on a collection of objects. Operations on multiple objects require a distributed locking protocol, which is complicated to implement efficiently. Moreover, this generality is seldom needed by parallel applications.
Reference: 23. <author> R.C.B. Cooper and K.G. Hamilton, </author> <title> ``Preserving Abstraction in Concurrent Programming,'' </title> <journal> IEEE Trans. Softw. Eng. </journal> <volume> SE-14(2), </volume> <pages> pp. </pages> <month> 258-263 (Feb. </month> <year> 1988). </year>
Reference-contexts: One could solve this problem by disallowing blocking operations on nested objects, but again this requires looking at the implementation of an operation to see how it may be used. Cooper and Hamilton have observed similar conflicts between parallel programming and data abstraction in the context of monitors <ref> [23] </ref>. They propose extending operation specifications with information about their implementation, such as whether or not the operation suspends or has any side effects. We feel it is not very elegant to make such concessions, however. The specification of an abstract data type should not reveal information about the implementation.
Reference: 24. <author> N. Wirth, </author> <title> ``The Programming Language Pascal,'' </title> <journal> Acta Informatica 1(1), </journal> <pages> pp. </pages> <month> 35-63 </month> <year> (1971). </year>
Reference-contexts: The run time system also automatically allocates memory for the new node. In this sense, addnode is similar to the standard procedure new in Pascal <ref> [24] </ref>. As a crucial difference between the two primitives, however, the addnode construct specifies the data structure for which the new block of memory is intended. Unlike in Pascal, the run time system of Orca can keep track of the nodes that belong to a certain graph.
Reference: 25. <author> T.A. Joseph and K.P. Birman, </author> <title> ``Low Cost Management of Replicated Data in Fault-Tolerant Distributed Systems,'' </title> <journal> ACM Trans. Comp. Syst. </journal> <month> 4(1) (Feb. </month> <year> 1987). </year>
Reference-contexts: Minimum is a data-object of type IntObject; it is read and written by all workers. based on replication and reliable broadcasting. We will briefly discuss a second implementation in Section 4.4. Replication of data is used in several fault-tolerant systems (e.g., ISIS <ref> [25] </ref>) to increase the availability of data in the presence of processor failures. Orca, in contrast, is not intended for fault-tolerant applications. In our implementation, replication is used to decrease the access costs to shared data. Briefly stated, each processor keeps a local copy of each shared data-object.
Reference: 26. <author> H.E. Bal, M.F. Kaashoek, A.S. Tanenbaum, and J. Jansen, </author> <title> ``Replication Techniques for Speeding up Parallel Applications on Distributed Systems,'' </title> <type> Report IR-202, </type> <institution> Vrije Universiteit, </institution> <address> Amsterdam, The Netherlands (Oct. </address> <year> 1989). </year>
Reference-contexts: There are many different design choices to be made related to replication, such as where to replicate objects, how to synchronize write operations to replicated objects, and whether to update or invalidate copies after a write operation. We have looked at many alternative strategies <ref> [26] </ref>. The RTS described in this paper uses full replication of objects, updates replicas by applying write operations to all replicas, and implements mutual exclusion synchronization through a distributed update protocol. The full replication scheme was chosen for its simplicity and good performance for many applications. <p> The full replication scheme was chosen for its simplicity and good performance for many applications. An alternative is to let the RTS decide dynamically where to store replicas. This strategy is employed in another implementation of Orca <ref> [26] </ref>. We have chosen to use an update scheme rather than an invalidation scheme for two reasons. First, in many applications objects contain large amounts of data (e.g., a 100K bit vector).
Reference: 27. <author> J. Chang and N.F. Maxemchuk, </author> <title> ``Reliable Broadcast Protocols,'' </title> <journal> ACM Trans. Comp. Syst. </journal> <volume> 2(3), </volume> <pages> pp. </pages> <month> 251-273 (Aug. </month> <year> 1984). </year>
Reference-contexts: In philosophy, the protocol described above somewhat resembles the one described by Chang and Maxemchuk <ref> [27] </ref>, but they differ in some major aspects. With our protocol, messages can be delivered to the user as soon as one (special) node has acknowledged the message. In addition, fewer control messages are needed in the normal case (no lost messages).
Reference: 28. <author> K.P. Birman and T.A. Joseph, </author> <title> ``Reliable Communication in the Presence of Failures,'' </title> <journal> ACM Trans. Comp. Syst. </journal> <volume> 5(1), </volume> <pages> pp. </pages> <month> 47-76 (Feb. </month> <year> 1987). </year>
Reference-contexts: A comparison between our protocol and other well known protocols (e.g., those of Birman and Joseph <ref> [28] </ref>, Garcia-Molina and Spauster [29], and several others) is given in [30]. - 24 - 4.4. Comparison with an RPC-based Protocol Above, we have described one implementation of Orca, based on full replication of objects and on a distributed update protocol using indivisible broadcasting.
Reference: 29. <author> H. Garcia-Molina and A. Spauster, </author> <title> ``Message Ordering in a Multicast Environment,'' </title> <booktitle> Proc. 9th Int. Conf. on Distr. Comp. Syst., </booktitle> <address> Newport Beach, CA, </address> <pages> pp. </pages> <month> 354-361 (June </month> <year> 1989). </year> <month> - 33 </month> - 
Reference-contexts: A comparison between our protocol and other well known protocols (e.g., those of Birman and Joseph [28], Garcia-Molina and Spauster <ref> [29] </ref>, and several others) is given in [30]. - 24 - 4.4. Comparison with an RPC-based Protocol Above, we have described one implementation of Orca, based on full replication of objects and on a distributed update protocol using indivisible broadcasting.
Reference: 30. <author> M.F. Kaashoek and A.S. Tanenbaum, </author> <title> ``Group Communication in the Amoeba Distributed Operating System,'' </title> <booktitle> 11th Int'l Conf. on Distributed Computing Systems, Arling-ton, Texas, </booktitle> <pages> pp. </pages> <month> 222-230 (20-24 May </month> <year> 1991). </year>
Reference-contexts: A comparison between our protocol and other well known protocols (e.g., those of Birman and Joseph [28], Garcia-Molina and Spauster [29], and several others) is given in <ref> [30] </ref>. - 24 - 4.4. Comparison with an RPC-based Protocol Above, we have described one implementation of Orca, based on full replication of objects and on a distributed update protocol using indivisible broadcasting. <p> The implementation uses Ethernet multicast communication to broadcast a message to a group of processors. All processors are on one Ethernet and are connected to it by Lance chip interfaces. The performance of the broadcast protocol on the Ethernet system is described in <ref> [30] </ref>. The time needed for multicasting a short message reliably to two processors is 2.6 msec. With 16 receivers, a multicast takes 2.7 msec. 4 This high performance is due to the fact that 333333333333333 4 In an earlier implementation of the protocol [32] the delay was 1.4 msec.
Reference: 31. <author> R.M. Metcalfe and D.R. Boggs, </author> <title> ``Ethernet: Distributed Packet Switching for Local Computer Networks,'' </title> <journal> Commun. ACM 19(7), </journal> <pages> pp. </pages> <month> 395-404 (July </month> <year> 1976). </year>
Reference-contexts: The prototype runs on top of the Amoeba system, which has been extended with the broadcast protocol described earlier. The implementation runs on a distributed system, containing 16 MC68030 CPUs (running at 16 Mhz) connected to each other through an 10 Mbit/s Ethernet <ref> [31] </ref>. The implementation uses Ethernet multicast communication to broadcast a message to a group of processors. All processors are on one Ethernet and are connected to it by Lance chip interfaces. The performance of the broadcast protocol on the Ethernet system is described in [30].
Reference: 32. <author> M.F. Kaashoek, A.S. Tanenbaum, S. Flynn Hummel, and H.E. Bal, </author> <title> ``An Efficient Reliable Broadcast Protocol,'' </title> <booktitle> ACM Operating Systems Review 23(4), </booktitle> <pages> pp. </pages> <month> 5-20 (Oct. </month> <year> 1989). </year>
Reference-contexts: The time needed for multicasting a short message reliably to two processors is 2.6 msec. With 16 receivers, a multicast takes 2.7 msec. 4 This high performance is due to the fact that 333333333333333 4 In an earlier implementation of the protocol <ref> [32] </ref> the delay was 1.4 msec.
Reference: 33. <author> J.-F. Jenq and S. Sahni, </author> <title> ``All Pairs Shortest Paths on a Hypercube Multiprocessor,'' </title> <booktitle> Proc. of the 1987 Int. Conf. on Parallel Processing, </booktitle> <address> St. Charles, </address> <publisher> Ill., </publisher> <pages> pp. </pages> <month> 713-716 (Aug. </month> <year> 1987). </year>
Reference-contexts: In this problem it is desired to find the length of the shortest path from any node i to any other node j in a given graph. The parallel algorithm we use is similar to the one given in <ref> [33] </ref>, which is a parallel version of Floyd's algorithm. The distances between the nodes are represented in a matrix. Each processor computes part of the result matrix. The algorithm requires a nontrivial amount of communication and synchronization among the processors.
Reference: 34. <author> J.S. Chase, F.G. Amador, E.D. Lazowska, H.M. Levy, and R.J. Littlefield, </author> <title> ``The Amber System: Parallel Programming on a Network of Multiprocessors,'' </title> <booktitle> Proc. of the 12th ACM Symp. on Operating System Principles, </booktitle> <address> Litchfield Park, AZ, </address> <pages> pp. </pages> <month> 147-158 (Dec. </month> <year> 1989). </year>
Reference-contexts: In particular, we will look at objects (as used in parallel object-based languages), Linda's Tuple Space, and Shared Virtual Memory. - 28 - Objects Objects are used in many object-based languages for parallel or distributed programming, such as Emerald [15], Amber <ref> [34] </ref>, and ALPS [35]. Objects in such languages typically have two parts: 1. Encapsulated data. 2. A manager process that controls access to the data. The data are accessed by sending a message to the manager process, asking it to perform a certain operation on the data.
Reference: 35. <author> P. Vishnubhotia, </author> <title> ``Synchronization and Scheduling in ALPS Objects,'' </title> <booktitle> Proc. 8th Int. Conf. on Distributed Computing Systems, </booktitle> <address> San Jose, CA, </address> <pages> pp. </pages> <month> 256-264 (June </month> <year> 1988). </year>
Reference-contexts: In particular, we will look at objects (as used in parallel object-based languages), Linda's Tuple Space, and Shared Virtual Memory. - 28 - Objects Objects are used in many object-based languages for parallel or distributed programming, such as Emerald [15], Amber [34], and ALPS <ref> [35] </ref>. Objects in such languages typically have two parts: 1. Encapsulated data. 2. A manager process that controls access to the data. The data are accessed by sending a message to the manager process, asking it to perform a certain operation on the data.
Reference: 36. <author> M.F. Kaashoek, H.E. Bal, </author> <title> and A.S. Tanenbaum, ``Experience with the Distributed Data Structure Paradigm in Linda,'' Workshop on Experiences with Building Distributed and Multiprocessor Systems, </title> <address> Ft. Lauderdale, FL. </address> <month> (Oct. </month> <year> 1989a). </year>
Reference-contexts: Operations on complex data structures (built out of multiple tuples), however, - 29 - have to be synchronized explicitly by the programmer. In essence, Tuple Space supports a fixed number of built-in operations that are executed indivisibly, but its support for building more complex indivisible operations is too low-level <ref> [36] </ref>. In Orca, on the other hand, programmers can define operations of arbitrary complexity on shared data structures; all these operations are executed indivisibly, so mutual exclusion synchronization is always done automatically by the run time system.
Reference: 37. <author> R.G. Minnich and D.J. Farber, </author> <title> ``Reducing Host Load, Network Load, and Latency in a Distributed Shared Memory,'' </title> <booktitle> Proc. 10th Int. Conf. on Distributed Computing Systems, Paris, </booktitle> <pages> pp. </pages> <month> 468-475 (May </month> <year> 1990). </year>
Reference-contexts: With SVM, there is no such choice; only invalidating pages is viable [6]. In many cases, however, invalidating copies will be far less efficient than updating them. Several researchers have tried to solve this performance problem by relaxing the consistency constraints of the memory (e.g., <ref> [37, 38] </ref>). Although these weakly consistent memory models may have better performance, we fear that they also ruin the ease of programming for which DSM was designed in the first place.
Reference: 38. <author> P.W. Hutto and M. Ahamad, </author> <title> ``Slow Memory: Weakening Consistency to Enhance Con-currency in Distributed Shared Memories,'' </title> <booktitle> Proceedings 10th International Conference on Distributed Computing Systems, Paris, </booktitle> <pages> pp. </pages> <month> 302-309 (May </month> <year> 1990). </year>
Reference-contexts: With SVM, there is no such choice; only invalidating pages is viable [6]. In many cases, however, invalidating copies will be far less efficient than updating them. Several researchers have tried to solve this performance problem by relaxing the consistency constraints of the memory (e.g., <ref> [37, 38] </ref>). Although these weakly consistent memory models may have better performance, we fear that they also ruin the ease of programming for which DSM was designed in the first place.
Reference: 39. <author> J.K. Bennet, J.B. Carter, and W. Zwaenepoel, ``Munin: </author> <title> Distributed Shared Memory Based on Type-Specific Memory Coherence,'' </title> <booktitle> Proceedings 2nd Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Seattle, WA (March 1990). </address>
Reference-contexts: Since Orca is intended to simplify applications programming, Orca programmers should not have to worry about consistency. (In the future, we may investigate whether a compiler is able to relax the consistency transparently, much as is done in the Munin system <ref> [39] </ref>.) A second important difference between Orca and SVM is the granularity of the shared data. In SVM, the granularity is the page-size, which is fixed (e.g. 4K). In Orca, the granularity is the object, which is determined by the user.
Reference: 40. <author> W.G. Levelt, M.F. Kaashoek, H.E. Bal, </author> <title> and A.S. Tanenbaum, ``A Comparison of Two Paradigms for Distributed Shared Memory,'' </title> <publisher> IR-221, Vrije Universiteit, </publisher> <address> Amsterdam, The Netherlands (August 1990). </address>
Reference-contexts: In Orca, this problem does not occur, since X and Y would be separate objects and would be treated independently. A more detailed comparison between our work and Shared Virtual Memory is given in <ref> [40] </ref>. 7. CONCLUSION We have described a new model and language for parallel programming of distributed systems. In contrast with most other models for distributed programming, our model allows processes on different machines to share data.
References-found: 40

