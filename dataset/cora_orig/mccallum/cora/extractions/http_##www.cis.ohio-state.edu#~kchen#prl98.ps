URL: http://www.cis.ohio-state.edu/~kchen/prl98.ps
Refering-URL: http://www.cis.ohio-state.edu/~kchen/on-line.html
Root-URL: 
Title: A Connectionist Method for Pattern Classification with Diverse Features with diverse features; mixture of experts;
Author: Ke Chen a;b; 
Note: b National Laboratory of Machine Perception and Center for Information Science Peking University, Beijing 100871, China Pattern Recognition Letters, Vol. 19, 1998. (in press)  Classification  
Address: Columbus, OH 43210-1277, USA  
Affiliation: a Department of Computer and Information Science and Center for Cognitive Science The Ohio State University,  
Abstract: A novel connectionist method is proposed to simultaneously use diverse features in an optimal way for pattern classification. Unlike methods of combining multiple classifiers, a modular neural network architecture is proposed through use of soft competition among diverse features. Parameter estimation in the proposed architecture is treated as a maximum likelihood problem, and an Expectation-Maximization (EM) learning algorithm is developed for adjusting the parameters of the architecture. Comparative simulation results are presented for the real world problem of speaker identification. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Campbell, F. P. </author> <year> (1997). </year> <title> Speaker recognition: A tutorial, </title> <booktitle> Proceedings of the IEEE 85(9): </booktitle> <pages> 14371463. </pages>
Reference: <author> Chen, K. & Chi, H. </author> <year> (1996). </year> <title> A modular neural network architecture for pattern classification with diverse features, </title> <institution> Technical Report PKU-CIS-96-TR07 , National Laboratory of Machine Perception and Center for Information Science, Peking University. </institution>
Reference-contexts: Consequently, the first two problems in Eqns (15) and (16) can be solved by the Newton-Raphson method or its approximation, a faster learning algorithm, proposed in our earlier work <ref> (Chen, Xu & Chi 1996) </ref>, while the last problem in Eq.(17) can be analytically solved by ff k = T t=1 (t) 3 Simulations Speaker identification is to classify an unlabeled voice token as belonging to one of reference speakers. <p> As an extension of the mixture of experts model, the hierarchical mixtures of experts architecture has been shown its effectiveness in many complicated supervised learning tasks. Similarly, the proposed modular neural network architecture can be also extended to a hierarchical structure for classification with diverse features <ref> (Chen & Chi 1996) </ref>. We expect that such a hierarchical structure yields improved performance in complicated pattern classification tasks. As a new method for classification with diverse features, our method adopts a single stage learning process rather than a two-stage sequential learning procedure used in methods of combining multiple classifiers.
Reference: <author> Chen, K., Wang, L. & Chi, H. </author> <year> (1997). </year> <title> Methods of combining multiple classifiers with different features and their applications to text-independent speaker recognition, </title> <journal> Int. J. Pattern Recogn. Artificial Intell. </journal> <volume> 11(3): </volume> <pages> 417445. </pages>
Reference-contexts: It has been reported that the BKS method achieves a promising performance and outperforms several classical combination methods in an unconstrained handwritten numerals recognition problem (Huang & Suen 1995), while the BAYES method also readily yields good performance in speaker identification <ref> (Chen et al. 1997) </ref> and the hand-written optical character recognition (Xu et al. 1992, Perrone 1993). In simulations, the aforementioned HME classifiers trained on different feature sets were used as individual classifiers.
Reference: <author> Chen, K., Xie, D. & Chi, H. </author> <year> (1996a). </year> <title> A modified HME architecture for text-dependent speaker identification, </title> <journal> IEEE Trans. Neural Net. </journal> <volume> 7(5): </volume> <pages> 13091313. </pages>
Reference-contexts: Our earlier work showed that the hierarchical mixtures of experts (HME) architecture (Jordan & Jacobs 1994), a variant of the mixture of experts model, outperforms the ME architecture in speaker identification <ref> (Chen, Xie & Chi 1996a, Chen, Xie & Chi 1996b) </ref>. For comparison, we also used 40 HME classifiers to deal with the same problem and each HME classifier was used to handle the utterances of a digit based on a specific feature set.
Reference: <author> Chen, K., Xie, D. & Chi, H. </author> <year> (1996b). </year> <title> Speaker identification using time-delay HMEs, </title> <journal> Int. J. Neural Syst. </journal> <volume> 7(1): </volume> <pages> 2943. </pages>
Reference-contexts: terms of pattern classification, the statistical model of an expert network, P (y (t) jx i ; W ij ), is assumed to be the Bernoulli distribution (Jordan & Jacobs 1994) in the case of binary classification, and the multinomial distribution (Jordan & Jacobs 1994) or the generalized Bernoulli distribution <ref> (Chen, Xie & Chi 1996b, Chen, Xu & Chi 1996) </ref> in the case of multicategory classification. <p> Corresponding to 10 isolated digits, 10 AME classifiers were employed in simulations. The generalized Bernoulli distribution was used as the statistical model of each expert network <ref> (Chen, Xie & Chi 1996b, Chen, Xu & Chi 1996) </ref>. The structure was chosen from four AME candidates ranging from 12 to 24 expert networks using the two-fold cross-validation method.
Reference: <author> Chen, K., Xu, L. & Chi, H. </author> <year> (1996). </year> <title> Improved learning algorithms for mixtures of experts in multiclass classification, Neural Networks, (in revision). </title>
Reference-contexts: Consequently, the first two problems in Eqns (15) and (16) can be solved by the Newton-Raphson method or its approximation, a faster learning algorithm, proposed in our earlier work <ref> (Chen, Xu & Chi 1996) </ref>, while the last problem in Eq.(17) can be analytically solved by ff k = T t=1 (t) 3 Simulations Speaker identification is to classify an unlabeled voice token as belonging to one of reference speakers. <p> As an extension of the mixture of experts model, the hierarchical mixtures of experts architecture has been shown its effectiveness in many complicated supervised learning tasks. Similarly, the proposed modular neural network architecture can be also extended to a hierarchical structure for classification with diverse features <ref> (Chen & Chi 1996) </ref>. We expect that such a hierarchical structure yields improved performance in complicated pattern classification tasks. As a new method for classification with diverse features, our method adopts a single stage learning process rather than a two-stage sequential learning procedure used in methods of combining multiple classifiers.
Reference: <author> Dempster, A., Laird, N. & Rubin, D. </author> <year> (1977). </year> <title> Maximum-likelihood from incomplete data via the EM algorithm, </title> <journal> J. Royal Stat. Soc. </journal> <volume> B 39(1): </volume> <pages> 138. </pages>
Reference-contexts: In this paper, we adopt an Expectation-Maximization (EM) algorithm <ref> (Dempster et al. 1977) </ref> to solve the problem. To develop the EM algorithm, we introduce an additional set of missing data besides the observable data in X in order to simplify the likelihood function.
Reference: <author> Doddington, G. </author> <year> (1986). </year> <title> Speaker recognition identifying people by their voice, </title> <booktitle> Proceedings of the IEEE 73(11): </booktitle> <volume> 1651 1664. </volume>
Reference-contexts: Moreover, the training data was divided into two sets with the same amount of utterances. They were used as the training set and the cross-validation set, respectively. In simulations, we adopted four common speech spectral features widely used in text-dependent speaker identification <ref> (Doddington 1986, Campbell 1997, Furui 1997) </ref>, i.e. 17-order delta-cepstrum, 17-order LPC based cepstrum, 17-order Mel-scale cepstrum, and 13-order LPC coefficients. Corresponding to 10 isolated digits, 10 AME classifiers were employed in simulations. <p> As a result, there were 10057 frames in SET-1, 4270 frames in SET-2, and 4604 frames in SET-3. We adopted three common speech spectral features extensively used for text-independent 7 speaker identification <ref> (Doddington 1986, Campbell 1997, Furui 1997) </ref>, i.e. 19-order LPC based cepstrum (19--LPCCEP), 19-order Mel-scale cepstrum (19-MELCEP), and 19-order LPC coefficients (19-LPCCOE). The evaluation method used is briefly described as follows. The sequence of feature vectors corresponding to testing data is denoted as ff 1 ; ; f T g.
Reference: <author> Fogelman-Soulie, F., Viennet, E. & Lamy, B. </author> <year> (1993). </year> <title> Multi-modular neural network architectures: applications in optical character and human face recognition, </title> <journal> Int. J. Pattern Recogn. Artificial Intell. </journal> <volume> 7(4): </volume> <pages> 721755. </pages>
Reference: <author> Furui, S. </author> <year> (1997). </year> <title> An overview of speaker recognition technology, </title> <journal> Pattern Recognition Letters 18(9): </journal> <volume> 859872. </volume>
Reference: <author> Ho, T., Hull, J. & Srihari, S. </author> <year> (1994). </year> <title> Decision combination in multiple classifier systems, </title> <journal> IEEE Trans. Pattern Anal. Machine Intell. </journal> <volume> 16(1): </volume> <pages> 6675. </pages>
Reference-contexts: Although some ad hoc methods have been explored to constrain the combinations based on correlation of classifiers, a systematic approach is still a challenging open problem <ref> (Ho et al. 1994) </ref>. In contrast, such a cross-validation data set is not necessary to be used in our method, and simulations have shown that less training data is required in our method to achieve a similar performance.
Reference: <author> Huang, Y. & Suen, C. </author> <year> (1995). </year> <title> A method of combining multiple experts for the recognition of unconstrained handwritten numerals, </title> <journal> IEEE Trans. Pattern Anal. Machine Intell. </journal> <volume> 17(1): </volume> <pages> 9094. </pages>
Reference-contexts: Combination of multiple classifiers turns out a good way to handle a task of classification with diverse features. For comparison, we also applied two recent combination methods in the same problem. They are the Bayesian reasoning (BAYES) method (Xu et al. 1992) and the behavior-knowledge space (BKS) method <ref> (Huang & Suen 1995) </ref>, respectively. It has been reported that the BKS method achieves a promising performance and outperforms several classical combination methods in an unconstrained handwritten numerals recognition problem (Huang & Suen 1995), while the BAYES method also readily yields good performance in speaker identification (Chen et al. 1997) and <p> They are the Bayesian reasoning (BAYES) method (Xu et al. 1992) and the behavior-knowledge space (BKS) method <ref> (Huang & Suen 1995) </ref>, respectively. It has been reported that the BKS method achieves a promising performance and outperforms several classical combination methods in an unconstrained handwritten numerals recognition problem (Huang & Suen 1995), while the BAYES method also readily yields good performance in speaker identification (Chen et al. 1997) and the hand-written optical character recognition (Xu et al. 1992, Perrone 1993). In simulations, the aforementioned HME classifiers trained on different feature sets were used as individual classifiers. <p> In simulations, the aforementioned HME classifiers trained on different feature sets were used as individual classifiers. The cross-validation set was used to calculate the confusion matrix in the BAYES method (Xu et al. 1992) and to acquire the behavior-knowledge space in the BKS method <ref> (Huang & Suen 1995) </ref>. Identification rates produced by the two combination methods are illustrated in Figure 3. For comparison in training time, we also show CPU time of training the two combination schemes in Figure 5.
Reference: <author> Jacobs, R., Jordan, M., Nowlan, S. & Hinton, G. </author> <year> (1991). </year> <title> Adaptive mixture of local experts, </title> <booktitle> Neural Computation 3(1): </booktitle> <volume> 79 87. </volume>
Reference-contexts: In addition, the proposed architecture can be viewed as an extension of the original mixture experts architecture <ref> (Jacobs et al. 1991) </ref> for classification with diverse features. When a single feature set is used, our architecture will be equivalent to the mixture experts architecture.
Reference: <author> Jordan, M. & Jacobs, R. </author> <year> (1994). </year> <title> Hierarchical mixtures of experts and the EM algorithm, </title> <booktitle> Neural Computation 6(3): </booktitle> <volume> 181 214. </volume>
Reference-contexts: In terms of pattern classification, the statistical model of an expert network, P (y (t) jx i ; W ij ), is assumed to be the Bernoulli distribution <ref> (Jordan & Jacobs 1994) </ref> in the case of binary classification, and the multinomial distribution (Jordan & Jacobs 1994) or the generalized Bernoulli distribution (Chen, Xie & Chi 1996b, Chen, Xu & Chi 1996) in the case of multicategory classification. <p> In terms of pattern classification, the statistical model of an expert network, P (y (t) jx i ; W ij ), is assumed to be the Bernoulli distribution <ref> (Jordan & Jacobs 1994) </ref> in the case of binary classification, and the multinomial distribution (Jordan & Jacobs 1994) or the generalized Bernoulli distribution (Chen, Xie & Chi 1996b, Chen, Xu & Chi 1996) in the case of multicategory classification. <p> Accordingly, testing results are illustrated in Figures 2 and 3, respectively, and the CPU time of training those AME classifiers is shown in Figures 4 and 5, respectively, in terms of two different training sets. Our earlier work showed that the hierarchical mixtures of experts (HME) architecture <ref> (Jordan & Jacobs 1994) </ref>, a variant of the mixture of experts model, outperforms the ME architecture in speaker identification (Chen, Xie & Chi 1996a, Chen, Xie & Chi 1996b).
Reference: <author> Perrone, M. </author> <year> (1993). </year> <title> Improving regression estimation: averaging methods of variance reduction with extensions to general convex measure optimization, </title> <type> Ph.D. thesis, </type> <institution> Department of Physics, Brown University. </institution>
Reference: <author> Suen, C., Legault, R., Nadal, C., Cheriet, M. & Lam, L. </author> <year> (1993). </year> <title> Building a new generation of handwriting recognition systems, </title> <journal> Pattern Recognition Letters 14(4): </journal> <volume> 303315. </volume>

References-found: 16

