URL: ftp://ai.iit.nrc.ca/pub/iit-papers/NRC-38313.ps.Z
Refering-URL: http://ai.iit.nrc.ca/cgi-bin/ftpsearch/?turney
Root-URL: 
Title: Bias and the Quantification of Stability Bias and the Quantification of Stability Bias and the
Author: Peter Turney 
Keyword: Key Words: stability, bias, accuracy, repeatability, agreement, similarity.  
Address: Ottawa, Ontario, Canada K1A 0R6  
Affiliation: Knowledge Systems Laboratory Institute for Information Technology National Research Council Canada  
Note: Submitted to Machine Learning  Running Head:  
Pubnum: Technical Note:  
Email: peter@ai.iit.nrc.ca  
Date: August 9, 1994 1  
Abstract: Research on bias in machine learning algorithms has generally been concerned with the impact of bias on predictive accuracy. We believe that there are other factors that should also play a role in the evaluation of bias. One such factor is the stability of the algorithm; in other words, the repeatability of the results. If we obtain two sets of data from the same phenomenon, with the same underlying probability distribution, then we would like our learning algorithm to induce approximately the same concepts from both sets of data. This paper introduces a method for quantifying stability, based on a measure of the agreement between concepts. We also discuss the relationships among stability, predictive accuracy, and bias. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Carnap, R. </author> <year> (1947). </year> <title> Meaning and necessity: A study in semantics and modal logic. </title> <publisher> Chicago: University of Chicago Press. </publisher>
Reference: <author> Famili, A., & Turney, P. </author> <year> (1991). </year> <title> Intelligently helping the human planner in industrial process planning. </title> <booktitle> Artificial Intelligence for Engineering Design, Analysis and Manufacturing, </booktitle> <volume> 5, </volume> <pages> 109-124. </pages>
Reference-contexts: We suggest another criterion: the stability of the algorithm. The stability of a classification algorithm is the degree to which it generates repeatable results, given different batches of data from the same process. In our work with industrial applications of decision tree induction algorithms <ref> (Famili & Turney, 1991) </ref>, we have discovered the importance of stability. We have used decision tree induction to generate rules that can predict low yield in a manufacturing process. The rules are used by process engineers to help them understand the causes of low yield.
Reference: <author> Fraser, D.A.S. </author> <year> (1976). </year> <title> Probability and statistics: Theory and applications. </title> <address> Massachusetts: </address> <publisher> Duxbury Press. </publisher>
Reference-contexts: There is a certain probability that will be 1. We say that is a sample from a Bernoulli ( ) distribution. Let be samples from a Bernoulli ( ) distribution. Consider the average of . This average has a mean of and a standard deviation of <ref> (Fraser, 1976) </ref>. The worst case (largest standard deviation) is , where the standard deviation is . If we set , then the standard deviation is 0.005 (in the worst case), or 0.5%, which seems acceptably low. Suppose , so . The truth-tables for and would have rows. <p> Since has a Bernoulli ( ) distribution, has a binomial (n, p) distribution <ref> (Fraser, 1976) </ref>. The stability of , , is estimated by , the average of for i equal 1 to m. Therefore is the sum of m random variables with binomial distributions, binomial (n, ), , binomial (n, ).
Reference: <author> Haussler, D. </author> <title> (1988) Quantifying inductive bias: AI learning systems and Valiants learning framework. </title> <journal> Artificial Intelligence, </journal> <volume> 36, </volume> <pages> 177-221. </pages>
Reference-contexts: The difficulty is determining the right direction for bias. Utgoffs (1986) definition of bias correctness implicitly assumes that predictive accuracy is the measure of the correctness of a bias. PAC (probably approximately correct) learning provides another measure of bias correctness <ref> (Haussler, 1988) </ref>. We propose that the notion of bias correctness should be extended to include stability.
Reference: <author> Honavar, V. </author> <year> (1992). </year> <title> Inductive learning using generalized distance measures. </title> <booktitle> Proceedings of the 1992 SPIE Conference on Adaptive and Learning Systems. </booktitle> <address> Orlando, Florida. </address>
Reference: <author> Levenshtein, A. </author> <year> (1966). </year> <title> Binary codes capable of correcting deletions, </title> <journal> insertions, and reversals. Soviet Physics, </journal> <volume> 10, </volume> <pages> 703-710. </pages>
Reference: <author> Murphy, P.M. & Pazzani, M.J. </author> <year> (1994). </year> <title> Exploring the decision forest: an empirical investigation of Occams razor in decision tree induction. </title> <journal> Journal for AI Research, ftp p.gp.cs.cmu.edu, cd /usr/jair/pub, </journal> <volume> 1, </volume> <pages> 257-275. </pages>
Reference-contexts: However, in most applications, we are not interested in stability alone, but stability in conjunction with accuracy. We do not claim that stability is always desirable. For example, in some situations, we may want to discover all concepts that are consistent with the training data <ref> (Murphy & Pazzani, 1994) </ref>. Thus we may sometimes define a measure of bias correctness that is based on accuracy alone, but there would not be much interest in a measure based on stability alone.
Reference: <author> Quinlan, J.R. </author> <year> (1992). </year> <title> C4.5: Programs for machine learning. </title> <address> California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: We use to denote the stability of , given and . Combining Definitions 2 and 3, the stability of is: (1) Definition 3 is new. Again, may be completely unrelated to . Suppose that the attributes and are highly correlated, given the distribution . Suppose that we use C4.5 <ref> (Quinlan, 1992) </ref> to learn decision trees on two sets of data sampled from . When building a decision tree, C4.5 selects attributes using the information gain ratio.
Reference: <author> Rendell, L. </author> <year> (1986). </year> <title> A general framework for induction and a study of selective induction. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 177-226. </pages>
Reference: <author> Schaffer, C. </author> <year> (1992). </year> <title> An empirical technique for quantifying preferential bias in inductive concept learners. </title> <type> Unpublished manuscript. </type> <institution> Department of Computer Science, CUNY/Hunter College, </institution> <address> New York. </address>
Reference-contexts: A syntactic measure of similarity (e.g., the percentage of overlap in the attributes used in two different decision trees) is likely to be ad hoc and specific to a particular concept representation. We use a semantic measure of similarity called agreement <ref> (Schaffer, 1992) </ref>. Schaffer (1992) introduced the idea of agreement in his analysis of bias, but we use agreement here to analyze stability. Section 2 presents a formal definition of stability, based on agreement. An empirical method for estimating stability is also presented. <p> Thus the strength of a preferential bias ranges from greater than 0.5 to less than or equal to 1.0. Speaking metaphorically, represents the force that the data must exert to overcome the bias of . This definition readily lends itself to empirical measurement of bias strength <ref> (Schaffer, 1992) </ref>. We can generate artificial data, using the distribution , and we can vary the value of to discover the strength of the preferential bias of . A strong exclusive bias accelerates learning by reducing the learners search space.
Reference: <author> Schaffer, C. </author> <year> (1993). </year> <title> Overfitting avoidance as bias. </title> <journal> Machine Learning, </journal> <volume> 10, </volume> <pages> 153-178. </pages>
Reference-contexts: A strong bias (either exclusive or preferential) also improves resistance to noise in the training data. Increasing bias can increase accuracy, if the bias pushes the learner towards more accurate concepts. However, increasing bias can also decrease accuracy, if the bias pushes the learner in the wrong direction <ref> (Schaffer, 1993) </ref>. For example, if we restrict the representational power of the learner too severely, the target concept may lie outside of what the learner can represent. The difficulty is determining the right direction for bias.
Reference: <author> Utgoff, P.E. </author> <year> (1986). </year> <title> Shift of bias for inductive concept learning. </title> <editor> In J.G. Carbonell, </editor> <publisher> R.S. </publisher>
Reference: <author> Michalski, </author> <title> and T.M. Mitchell (eds) Machine Learning: </title> <booktitle> An Artificial Intelligence Approach, Volume II. </booktitle> <address> California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Vapnik, V.N. </author> <year> (1982). </year> <title> Estimation of dependencies based on empirical data. </title> <address> New York: Springer. </address> <note> correlation a 1 a 2 ,( ) 1= </note>
References-found: 14

