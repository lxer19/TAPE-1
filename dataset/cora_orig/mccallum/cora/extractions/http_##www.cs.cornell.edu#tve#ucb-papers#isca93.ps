URL: http://www.cs.cornell.edu/tve/ucb-papers/isca93.ps
Refering-URL: 
Root-URL: 
Email: fellens,billdg@ai.mit.edu tam@cs.berkeley.edu  
Title: Evaluation of Mechanisms for Fine-Grained Parallel Programs in the J-Machine and the CM-5  
Author: Ellen Spertus Seth Copen Goldstein Klaus Erik Schauser Thorsten von Eicken David E. Culler William J. Dally 
Keyword: Parallel Processing, Performance Analysis, Compilation.  
Address: 545 Technology Square  Cambridge, MA 02139 Berkeley, CA 94720  
Affiliation: yMIT Artifical Intelligence Laboratory zComputer Science Division EECS  University of California  
Abstract: This paper uses an abstract machine approach to compare the mechanisms of two parallel machines: the J-Machine and the CM-5. High-level parallel programs are translated by a single optimizing compiler to a fine-grained abstract parallel machine, TAM. A final compilation step is unique to each machine and optimizes for specifics of the architecture. By determining the cost of the primitives and weighting them by their dynamic frequency in parallel programs, we quantify the effectiveness of the following mechanisms individually and in combination. Efficient processor/network coupling proves valuable. Message dispatch is found to be less valuable without atomic operations that allow the scheduling levels to cooperate. Multiple hardware contexts are of small value when the contexts cooperate and the compiler can partition the register set. Tagged memory provides little gain. Finally, the performance of the overall system is strongly influenced by the performance of the memory system and the frequency of control operations. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Arvind and K. Ekanadham. </author> <title> Future Scientific Programming on Parallel Machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5(5) </volume> <pages> 460-493, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: The input is a list of random numbers. Gamteb is a Monte Carlo neutron transport code [4]. It is highly recursive with many conditionals. Paraffins [2] enumerates the distinct isomers of paraffins. Simple is a hydrodynamics and heat conduction code widely used as an application benchmark, rewritten in Id90 <ref> [5, 1] </ref>. Speech determines cepstral coefficients for speech processing. MMT is a simple matrix operation test using 4x4 blocks; two double precision identity matrices are created, multiplied, and subtracted from a third. The programs toward the left of Figure 1 represent fine-grained parallelism.
Reference: [2] <author> Arvind, S. K. Heller, and R. S. Nikhil. </author> <title> Programming Generality and Parallel Computers. </title> <booktitle> In Proc. of the Fourth Int. Symp. on Biological and Artificial Intelligence Systems, </booktitle> <pages> pages 255-286. </pages> <address> ESCOM (Leider), Trento, Italy, </address> <month> September </month> <year> 1988. </year>
Reference-contexts: Six benchmark programs ranging from 50 to 1,100 lines are used. QS is a simple quick-sort using accumulation lists. The input is a list of random numbers. Gamteb is a Monte Carlo neutron transport code [4]. It is highly recursive with many conditionals. Paraffins <ref> [2] </ref> enumerates the distinct isomers of paraffins. Simple is a hydrodynamics and heat conduction code widely used as an application benchmark, rewritten in Id90 [5, 1]. Speech determines cepstral coefficients for speech processing.
Reference: [3] <author> D. H. Bailey et al. </author> <title> The NAS Parallel Benchmarks Summary and Preliminary Results. </title> <booktitle> In Proc. </booktitle> <address> Supercomputing'91, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: There is not even a consensus on the programming languages of choice. Where benchmarks exist, they have been developed specifically for the machine that they are intended to evaluate [14, 9] or specifically avoid emerging languages and the novel mechanisms which could bring them within practical reach <ref> [3] </ref>. It is also difficult to obtain high-quality compilers for such new languages on more than one machine, yet it is well understood that the architectural support can only be evaluated in the context of sophisticated compilation, rather than direct execution of high-level constructs.
Reference: [4] <author> P. J. Burns, M. Christon, R. Schweitzer, O. M. Lubeck, H. J. Wasserman, M. L. Simmons, and D. V. Pryor. </author> <title> Vectorization of Monte-Carlo Particle Transport: An Architectural Study using the LANL Benchmark Gamteb. </title> <booktitle> In Proc. Supercomputing '89. IEEE Computer Society and ACM SIGARCH, </booktitle> <address> New York, NY, </address> <month> November </month> <year> 1989. </year>
Reference-contexts: Six benchmark programs ranging from 50 to 1,100 lines are used. QS is a simple quick-sort using accumulation lists. The input is a list of random numbers. Gamteb is a Monte Carlo neutron transport code <ref> [4] </ref>. It is highly recursive with many conditionals. Paraffins [2] enumerates the distinct isomers of paraffins. Simple is a hydrodynamics and heat conduction code widely used as an application benchmark, rewritten in Id90 [5, 1]. Speech determines cepstral coefficients for speech processing.
Reference: [5] <author> W. P. Crowley, C. P. Hendrickson, and T. E. Rudy. </author> <title> The SIMPLE code. </title> <type> Technical Report UCID 17715, </type> <institution> Lawrence Livermore Laboratory, </institution> <month> February </month> <year> 1978. </year>
Reference-contexts: The input is a list of random numbers. Gamteb is a Monte Carlo neutron transport code [4]. It is highly recursive with many conditionals. Paraffins [2] enumerates the distinct isomers of paraffins. Simple is a hydrodynamics and heat conduction code widely used as an application benchmark, rewritten in Id90 <ref> [5, 1] </ref>. Speech determines cepstral coefficients for speech processing. MMT is a simple matrix operation test using 4x4 blocks; two double precision identity matrices are created, multiplied, and subtracted from a third. The programs toward the left of Figure 1 represent fine-grained parallelism.
Reference: [6] <author> D. Culler, A. Sah, K. Schauser, T. von Eicken, and J. Wawrzynek. </author> <title> Fine-grain Parallelism with Minimal Hardware Support: A Compiler-Controlled Threaded Abstract Machine. </title> <booktitle> In Proc. of 4th Int. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Santa-Clara, CA, </address> <month> April </month> <year> 1991. </year> <note> (Also available as Technical Report UCB/CSD 91/591, </note> <institution> CS Div., University of California at Berkeley). </institution>
Reference-contexts: The compiler performs a variety of high-level optimizations in translating the language down to code for a simple abstract machine, TAM <ref> [6, 13] </ref>. The TAM code is identical for the two machines, controlling for effects of high-level optimizations. The translator from TAM code to machine language, however, employs a variety of machine-specific optimizations reflecting the most advantageous use of the available mechanisms. <p> We will focus primarily on the two largest programs, Gamteb and Simple. The Id90 implementations of these programs take about twice as long on a single processor as implementations in standard languages like C or Fortran <ref> [6] </ref>.
Reference: [7] <author> D. E. Culler. </author> <title> Managing Parallelism and Resources in Scientific Dataflow Programs. </title> <type> Technical Report 446, </type> <institution> MIT Lab for Comp. Sci., </institution> <month> March </month> <year> 1990. </year> <type> (PhD Thesis, </type> <institution> Dept. of EECS, MIT). </institution>
Reference-contexts: Program code is placed on every processor, but a given code-block invocation takes place on a single processor. Because the compiler may pull loops out into separate code-blocks, these can be spread across the machine to implement parallel loops <ref> [7] </ref>. The memory on each processor is divided into two areas. One holds small arrays and activation frames. The other holds large arrays, which are spread across all the processors such that logically consecutive elements are on different processors. Memory is managed explicitly through library routines.
Reference: [8] <author> S. C. Goldstein. </author> <title> Implementation of a Threaded Abstract Machine on Sequential and Multiprocessors. </title> <type> Master's thesis, </type> <institution> Computer Science Division EECS, U.C. Berkeley, </institution> <year> 1993. </year> <note> (In preparation, to appear as UCB/CSD Technical Report). </note>
Reference-contexts: Frames are stored in main memory. A similar approach is followed on the Sparc with inlets using a new register window. However, due to the tight coupling between threads and inlets, it proves to be more efficient to simply partition a single window. The CM-5 implementation <ref> [8] </ref> uses 32 registers divided into three classes: global registers which hold frequently-used values, TAM registers which are preserved for the duration of a quantum, and inlet registers, used during inlet execution and to pass information from threads to inlets.
Reference: [9] <author> J. Gustafson, G. Montry, and Benner R. </author> <title> Development of Parallel Methods for a 1024-Processor Hypercube. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 9, </volume> <year> 1988. </year>
Reference-contexts: There is not even a consensus on the programming languages of choice. Where benchmarks exist, they have been developed specifically for the machine that they are intended to evaluate <ref> [14, 9] </ref> or specifically avoid emerging languages and the novel mechanisms which could bring them within practical reach [3].
Reference: [10] <author> R. S. Nikhil. </author> <title> ID Language Reference Manual Version 90.1. </title> <type> Technical Report CSG Memo 284-2, </type> <institution> MIT Lab for Comp. Sci., </institution> <type> 545 Tech. </type> <institution> Square, </institution> <address> Cambridge, MA, </address> <year> 1991. </year>
Reference-contexts: Each processor maintains a queue of ready frames with non-empty CVs. A new frame is activated from the queue when a quantum completes. Global data structures in TAM provide synchronization on a per-element basis to support I-structure and M-structure semantics <ref> [10] </ref>. In particular, reads of empty elements are deferred until the corresponding write occurs.
Reference: [11] <author> R. S. Nikhil, G. M. Papadopoulos, and Arvind. </author> <title> *T: A Killer Micro for A Brave New World. </title> <type> Technical Report CSG Memo 325, </type> <institution> MIT Lab for Comp. Sci., </institution> <type> 545 Tech. </type> <institution> Square, </institution> <address> Cambridge, MA, </address> <month> January </month> <year> 1991. </year>
Reference-contexts: These are combined with the machine cost coefficients to obtain the average cycles per TAM instruction. We look forward to a much broader set of studies following a similar methodology using additional machines and additional language frameworks. Clearly it should be possible to evaluate proposals such as *T <ref> [11] </ref> in this framework. Although other parallel language implementations may differ from TAM in many ways, the primary ingredients are likely to be similar: message exchange, remote references, synchronization, control flow, and dynamic scheduling. We do anticipate that the granularity of parallelism will have a significant impact on the evaluation.
Reference: [12] <author> R. H. Saavedra-Barrera and A. J. Smith. </author> <title> Benchmarking and The Abstract Machine Characterization Model. </title> <type> Technical Report UCB/CSD 90/607, </type> <institution> U.C. Berkeley, Computer Science Div., </institution> <month> November </month> <year> 1990. </year>
Reference-contexts: We follow a method of analysis similar to the Abstract Machine Characterization Model used to evaluate a wide range of conventional machines and benchmarks <ref> [12] </ref>. We normalize for software effects, including programming language, programming style, and high-level compiler optimizations by using a common low-level representation of each program in terms of a Threaded Abstract Machine. Machine-specific optimizations are realized in compiling the TAM code to each machine.
Reference: [13] <author> K. E. Schauser, D. Culler, and T. von Eicken. </author> <title> Compiler-controlled Multithreading for Lenient Parallel Languages. </title> <booktitle> In Proceedings of the 1991 Conference on Functional Programming Languages and Computer Architecture, </booktitle> <address> Cam-bridge, MA, </address> <month> August </month> <year> 1991. </year> <note> (Also available as Technical Report UCB/CSD 91/640, </note> <institution> CS Div., University of California at Berkeley). </institution>
Reference-contexts: The compiler performs a variety of high-level optimizations in translating the language down to code for a simple abstract machine, TAM <ref> [6, 13] </ref>. The TAM code is identical for the two machines, controlling for effects of high-level optimizations. The translator from TAM code to machine language, however, employs a variety of machine-specific optimizations reflecting the most advantageous use of the available mechanisms.
Reference: [14] <author> J. P. Singh, W.-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title> <type> Technical Report CSL-TR-91-469, </type> <institution> Stanford University, </institution> <year> 1991. </year>
Reference-contexts: There is not even a consensus on the programming languages of choice. Where benchmarks exist, they have been developed specifically for the machine that they are intended to evaluate <ref> [14, 9] </ref> or specifically avoid emerging languages and the novel mechanisms which could bring them within practical reach [3].
Reference: [15] <author> E. Spertus. </author> <title> Execution of Dataflow Programs on General-Purpose Hardware. </title> <type> Master's thesis, </type> <institution> Department of EECS, Massachusetts Institute of Technology, </institution> <type> 545 Tech. </type> <institution> Square, </institution> <address> Cambridge, MA, </address> <month> August </month> <year> 1992. </year> <note> To be expanded and released as MIT AI Lab Technical Report 1380. </note>
Reference-contexts: One holds small arrays and activation frames. The other holds large arrays, which are spread across all the processors such that logically consecutive elements are on different processors. Memory is managed explicitly through library routines. The J-Machine implementation of TAM <ref> [15] </ref> makes direct use of the hardware support for different priority levels.
Reference: [16] <institution> Thinking Machines Corporation, </institution> <address> Cambridge, MA. </address> <booktitle> The Connection Machine CM-5 Technical Summary, </booktitle> <month> January </month> <year> 1992. </year>
Reference-contexts: First, novel mechanisms do not substitute for solid engineering of the processor pipeline and storage hierarchy. Second, mechanisms should not be evaluated in isolation, but in how they work together in the compilation framework for the programming language. 2 Background 2.1 CM-5 The CM-5 <ref> [16] </ref> is a massively-parallel MIMD computer based on the Sparc processor, interconnected in two identical disjoint hypertree networks.
References-found: 16

