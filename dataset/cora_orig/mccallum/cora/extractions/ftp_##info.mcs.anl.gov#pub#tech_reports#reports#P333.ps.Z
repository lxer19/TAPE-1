URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P333.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts93.htm
Root-URL: http://www.mcs.anl.gov
Title: Derivative Convergence for Iterative Equation Solvers  
Author: Andreas Griewank and Christian Bischof, Alan Carle and Karen Williamson, 
Keyword: KEY WORDS: Derivative convergence, automatic differentiation, implicit functions, preconditioning, Newton-like methods, secant updates.  
Date: 321-355, 1993.  
Note: Preprint ANL-MCS-P333-1192 appeared in Optimization Methods and Software, volume 2, pages  
Affiliation: Argonne National Laboratory George Corliss, Marquette University and Argonne National Laboratory  Rice University Argonne  
Abstract: When nonlinear equation solvers are applied to parameter-dependent problems, their iterates can be interpreted as functions of these variable parameters. The derivatives (if they exist) of these iterated functions can be recursively evaluated by the forward mode of automatic differentiation. Then one may ask whether and how fast these derivative values converge to the derivative of the implicit solution function, which may be needed for parameter identification, sensitivity studies, or design optimization. It is shown here that derivative convergence is achieved with an R-linear or possibly R-superlinear rate for a large class of memoryless contractions or secant updating methods. For a wider class of multistep contractions, we obtain R-linear convergence of a simplified derivative recurrence, which is more economical and can be easily generalized to higher-order derivatives. We also formulate a constructive criterion for derivative convergence based on the implicit function theorem. All theoretical results are confirmed by numerical experiments on small test examples. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Christian Bischof, Alan Carle, George Corliss, Andreas Griewank, and Paul Hovland. </author> <title> ADIFOR-Generating Derivative codes from Fortran Programs. </title> <journal> Scientific Programming, </journal> <volume> 1 </volume> <pages> 11-29, </pages> <year> 1992. </year>
Reference-contexts: The Jacobian can be computed by finite differences, or it can be constructed from values of dx (p i )=dp. We will apply automatic differentiation to the nonlinear equation solver to obtain an iterative procedure for computing dx (p i )=dp. Applying the automatic differentiation tool ADIFOR <ref> [1] </ref> to LMDER and HYBRJ, which are both written in Fortran, yields a fully differentiated Newton's method and a fully differentiated Broyden's method. <p> However, the simplified approach shows the most promise for generating fast, accurate derivatives. The numerical results reported in this section were obtained in double precision on a SPARCstation 2 using the automatic differentiation package ADIFOR described in <ref> [1] </ref>. 22 Problem Additional Relative Error steps 1 6 5 fi 10 16 1a 2 2 fi 10 11 2 4 3 fi 10 12 3 1 2 fi 10 12 4 1 1 fi 10 12 4a 1 1 fi 10 13 4b 1 1 fi 10 12 5 2
Reference: [2] <author> Christian Bischof, George Corliss, and Andreas Griewank. </author> <title> Structured second- and higher-order derivatives through univariate Taylor series. </title> <type> Preprint MCS-P296-0392, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Ill., </institution> <month> March </month> <year> 1992. </year> <note> ADIFOR Working Note #6. </note>
Reference-contexts: Without loss of generality, we have restricted our framework to the case of a single scalar parameter t 2 IR since multivariate derivatives can always be constructed from families of univariate derivatives <ref> [2] </ref>. Total derivatives with respect to t will be denoted by primes, and partial derivatives (with x kept constant) by the subscript t. In this paper, we consider two approaches to computing the desired implicitly defined derivative x 0 (t).
Reference: [3] <author> Christian Bischof, George Corliss, Larry Green, Andreas Griewank, K. Haigler, and Perry Newman. </author> <title> Automatic differentiation of advanced CFD codes for multidisciplinary design. </title> <type> Preprint MCS-P339-1192, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Ill., </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: This fundamental result has removed some serious doubts regarding the general applicability of automatic differentiation. It has been verified on several large codes, including cases <ref> [3] </ref> where the assumptions of Gilbert's theorem do not appear to be satisfied. Therefore, we wish to relax the hypothesis and avoid derivatives that are not needed either from a theoretical or from a practical point of view. <p> This approach has long been used by engineers, as evidenced for example in some references of <ref> [3] </ref>. It certainly may be advantageous to start the derivative recurrences (7) or (10) with an initial x 0 k = 0 only when the underlying iteration has reached the vicinity of the solution point.
Reference: [4] <author> C. G. </author> <title> Broyden. A class of methods for solving nonlinear simultaneous equations. </title> <journal> Math. Comp. </journal> <volume> 19 </volume> <pages> 577-593, </pages> <year> 1965. </year>
Reference-contexts: Before we formulate the second major result, let us briefly show that the Broyden update <ref> [4] </ref> and the DFP formula [9], [14], which do not explicitly depend on (x; t), satisfy the condition above.
Reference: [5] <author> C. G. </author> <title> Broyden. The convergence of a class of double rank minimization algorithms. </title> <journal> J. Inst. Math. Appl. </journal> <volume> 6 </volume> <pages> 222-231, </pages> <year> 1970. </year>
Reference-contexts: We hope this terminology helps avoid the danger of confusion with the Jacobian and Hessian update formulas ([4] and <ref> [5] </ref>) that lie at the heart of secant methods. 4 Provided the P 0 k stay bounded or do not blow up too fast with increasing k, the last term in the linear recurrence (11) becomes more and more negligible as the residual F (x k ; t) approaches zero.
Reference: [6] <author> C. G. Broyden, J. E. Dennis, J. J. </author> <title> More. On the local and superlinear convergence of quasi-Newton methods. </title> <journal> J. Inst. Math. Appl. </journal> <volume> 12 </volume> <pages> 223-245, </pages> <year> 1973. </year>
Reference-contexts: However, under the usual assumption for local convergence of secant updating methods, it can be shown <ref> [6] </ref> that ff k ! 1:0 and that kD k k &lt; 0:5 in the l 2 norm for all k. Then it follows from Proposition 1 that the simplified recurrence (7) must converge to the unique limit x 0 fl .
Reference: [7] <author> Bruce Christianson. </author> <title> Reverse Accumulation and Attractive Fixed Points. </title> <type> Numerical Optimisation Centre Report 258, </type> <institution> University of Hertfordshire, </institution> <year> 1992. </year>
Reference-contexts: This approach is particularly useful if t is actually a vector so that several linear systems need to be solved for computing x 0 k . This iterative variant of the reverse mode for implicit gradients has been advocated and analyzed by Christianson in <ref> [7] </ref>. However, it should be noted that his analysis, if not the method itself, assumes that the Jacobian of the iteration function is not only contractive but also Lipschitz continuous in the current argument. This condition is certainly not satisfied by secant updating methods.
Reference: [8] <author> George F. Corliss. </author> <title> Overloading point and interval Taylor operators. </title> <editor> In Andreas Griewank and George F. Corliss, editors, </editor> <title> Automatic Differentiation of Algorithms: Theory, </title> <booktitle> Implementation, and Application, </booktitle> <pages> pages 139-146. </pages> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1991. </year>
Reference-contexts: (j) k t; x k ; ~x 0 k : While this may seem a very messy expression, the residual vectors F x (x k (t); t) x (i+1) can be evaluated simultaneously for any given t and (~x (i) k ) i=0;1;:::;j+1 by one forward sweep of automatic differentiation <ref> [8] </ref>. The complexity of this Taylor series propagation is O (j 2 ) times that of one function evaluation F (x; t) if ordinary polynomial arithmetic is used.
Reference: [9] <author> W. C. Davidon. </author> <title> Variable metric method for minimization. </title> <type> Report ANL-5990 (Rev.), </type> <institution> Argonne National Laboratory, Argonne, Illinois, </institution> <year> 1959. </year>
Reference-contexts: Before we formulate the second major result, let us briefly show that the Broyden update [4] and the DFP formula <ref> [9] </ref>, [14], which do not explicitly depend on (x; t), satisfy the condition above.
Reference: [10] <author> J. E. Dennis, Jr., D. M. Gay and R. E. Welsch. </author> <title> An Adaptive Nonlinear Least-Squares Algorithm. </title> <journal> TOMS, </journal> <volume> 7 </volume> <pages> 348-368, </pages> <year> 1981. </year>
Reference-contexts: To solve the resulting optimization problem (28), we will use the nonlinear least-squares package NL2SOL [11]. NL2SOL is an implementation of an augmented Gauss-Newton trust region method <ref> [10] </ref> that exploits the structure of the nonlinear least-squares problem.
Reference: [11] <author> J. E. Dennis, Jr., D. M. Gay and R. E. Welsch. </author> <title> Algorithm 573 NL2SOL-An Adaptive Nonlinear Least-Squares Algorithm. </title> <journal> TOMS, </journal> <volume> 7 </volume> <pages> 369-383, </pages> <year> 1981. </year>
Reference-contexts: To solve the resulting optimization problem (28), we will use the nonlinear least-squares package NL2SOL <ref> [11] </ref>. NL2SOL is an implementation of an augmented Gauss-Newton trust region method [10] that exploits the structure of the nonlinear least-squares problem.
Reference: [12] <author> John E. Dennis, Jr., Guangye Li and Karen A. Williamson. </author> <title> Optimization Algorithms for Parameter Identification. </title> <type> Technical Report CRPC-TR92277, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <year> 1992. </year>
Reference-contexts: A more detailed description of the problem formulation and the collocation scheme can be found in <ref> [12] </ref>. Thus, using collocation, we can replace the ODE in (27) with a nonlinear system of equations. <p> We used six parameter identification problems from <ref> [12] </ref> to evaluate the effectiveness of the algorithms. <p> While this discretization is adequate to solve all of the problems, a more efficient discretization could be chosen for each individual problem. In addition to the standard starting point, (which can be found in <ref> [12] </ref>), problems 1a and 1ai used p 0 = (15; 20) T , problems 4a and 4ai used p 0 = (1 fi 10 5 ; 1 fi 10 5 ) T , and problems 4b and 4bi used p 0 = (1 fi 10 4 ; 1 fi 10 4
Reference: [13] <author> J. E. Dennis and J. J. </author> <title> More. Quasi-Newton methods, motivation and theory. </title> <journal> SIAM Review, </journal> <volume> 19 </volume> <pages> 46-89, </pages> <year> 1977. </year>
Reference-contexts: As an immediate consequence of Assumptions 1 and 2, we note that by standard arguments kP k k c 0 (1 + ffi) and kP 1 In the case of secant methods <ref> [13] </ref>, the condition (2) is usually imposed for k = 0 and deduced for k &gt; 1 to guarantee local convergence. If one assumes a certain kind of uniform linear independence for the sequence of the search directions, it can be shown [19] that ffi fl = 0. <p> In the general nonlinear equations case, the progress towards the solution is usually gauged in terms of some norm of the residual vector F . Often a monotonic decrease of such a merit function is enforced by a suitable line-search or trust-region strategy <ref> [13] </ref>. In the optimization case, one may use the objective function f itself, which we have utilized for a line-search consisting of a single parabolic interpolation. This simple strategy works here because the objective function is convex and very smooth.
Reference: [14] <author> R. Fletcher and M. J. D. Powell. </author> <title> A rapidly convergent descent method for minimization. </title> <journal> Comput. J., </journal> <volume> 6 </volume> <pages> 163-168, </pages> <year> 1963. </year>
Reference-contexts: Before we formulate the second major result, let us briefly show that the Broyden update [4] and the DFP formula [9], <ref> [14] </ref>, which do not explicitly depend on (x; t), satisfy the condition above.
Reference: [15] <author> J. Ch. Gilbert. </author> <title> Automatic differentiation and iterative processes. </title> <journal> Optimization Methods and Software, </journal> <volume> 1 </volume> <pages> 13-21, </pages> <year> 1992. </year> <note> Also appeared as a preprint, </note> <institution> INRIA, Le Chesnay, France, </institution> <year> 1991. </year>
Reference-contexts: Fortunately, we can constructively check the accuracy of any derivative approximation so that a premature termination can be avoided if accurate derivative values are required. Gilbert showed in <ref> [15] </ref> that the derivatives dx k =dt converge in the limit to the desired tangent x 0 = x 0 (t) provided that the spectral radius of @(x; t)=@x is less than one in the vicinity of (x fl ; t). <p> This condition was implied by the hypothesis of Gilbert's theorem <ref> [15] </ref> but must be considered quite restrictive. For example, the condition does not hold for Broyden's method nor for other popular quasi-Newton schemes, where P k = ff k B 1 k . <p> This indicates that the convergence of derivative did not noticeably lag behind the convergence of the x k iterates for Newton's method. This is not surprising, for the work of Gilbert <ref> [15] </ref> shows that theoretically Newton's method is "special" in that the derivative convergence should only lag one iteration behind the iterate convergence. On the other hand, based on the step size, we would estimate a relative error of around 10 8 for the finite difference Jacobian.
Reference: [16] <author> Andreas Griewank. </author> <title> Automatic evaluation of first- and higher-derivative vectors. </title> <editor> In R. Seydel, F. W. Schneider, T. Kupper, and H. Troger, editors, </editor> <booktitle> Proceedings of the Conference at Wurzburg, </booktitle> <month> Aug. </month> <year> 1990, </year> <title> Bifurcation and Chaos: Analysis, Algorithms, </title> <journal> Applications, </journal> <volume> volume 97, </volume> <pages> pages 135-148. </pages> <publisher> Birkhauser Verlag, </publisher> <address> Basel, Switzerland, </address> <year> 1991. </year> <month> 26 </month>
Reference-contexts: These local contingencies are easily detected and arise only in marginal situations where the undifferentiated evaluation algorithm is already poorly conditioned. For a general review of the theory, implementation, and application of automatic differentiation, see <ref> [16] </ref>.
Reference: [17] <author> Andreas Griewank, David Juedes, Jay Srinivasan, and Charles Tyner. ADOL-C, </author> <title> a package for the automatic differentiation of algorithms written in C/C++. </title> <journal> ACM Trans. Math. Software, </journal> <note> to appear. Also appeared as Preprint MCS-P180-1190, </note> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Ill., </institution> <month> November </month> <year> 1990. </year>
Reference-contexts: The numerical results reported in this section were obtained in double precision on a SPARCstation 2 using the automatic differentiation package ADOL-C described in <ref> [17] </ref>. 4.2 APPLICATION TO PARAMETER IDENTIFICATION PROBLEMS In this section, we study the effect of using the automatic differentiation of an implicitly defined function to generate the first-order derivative information needed at each iteration of an optimization algorithm.
Reference: [18] <author> J. J. More, B. S. Garbow, and K. E. Hillstrom. </author> <title> User guide for MINPACK-1. </title> <type> Technical Report ANL-80-74, </type> <institution> Argonne National Laboratory, </institution> <year> 1980. </year>
Reference-contexts: We consider both Newton's method and Broyden's method for the solution of this nonlinear system; in particular, the implementations found in MINPACK <ref> [18] </ref>, (LMDER and HYBRJ).
Reference: [19] <author> M. J. D. Powell. </author> <title> A hybrid method for nonlinear equations. </title> <editor> In P. Rabinowitz, editor, </editor> <booktitle> Numerical Methods for Nonlinear Algebraic Equations, </booktitle> <pages> pages 87-114. </pages> <publisher> Gordon and Breach, </publisher> <address> London, </address> <year> 1970. </year>
Reference-contexts: If one assumes a certain kind of uniform linear independence for the sequence of the search directions, it can be shown <ref> [19] </ref> that ffi fl = 0. This is a sufficient, but by no means necessary, condition for Q-superlinear convergence. It can be enforced by taking so-called special steps [19] for the sole purpose of reducing the discrepancy D k . <p> If one assumes a certain kind of uniform linear independence for the sequence of the search directions, it can be shown <ref> [19] </ref> that ffi fl = 0. This is a sufficient, but by no means necessary, condition for Q-superlinear convergence. It can be enforced by taking so-called special steps [19] for the sole purpose of reducing the discrepancy D k . We will see that ffi fl = 0 implies R-superlinear rather than just R-linear convergence of the derivatives. Hence, the extra expense of special updating steps might be justified on parameter-dependent problems.
Reference: [20] <author> J. M. Ortega and W. C. Rheinboldt. </author> <title> Iterative Solution of Nonlinear Equations in Several Variables, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1970. </year> <month> 27 </month>
Reference-contexts: derived, we get k kF 1 k )k + c 0 L c 1 ae k =2 : One can replace the inverse F 1 x (x k ; t) in the first term on the right-hand side by P k , noting that by the Banach perturbation lemma (e.g., <ref> [20] </ref>) and the definition of D k in Assumption 2 kF 1 k k = k (I D k ) 1 k 1=(1 kD k k) ; which establishes the first assertion. <p> This implies the R-linear convergence assertion for the x k by well-known results <ref> [20] </ref> and by taking the infimum of ffi m over m.
References-found: 20

