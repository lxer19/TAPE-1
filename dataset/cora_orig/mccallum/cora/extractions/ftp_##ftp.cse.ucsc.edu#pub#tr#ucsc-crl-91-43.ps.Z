URL: ftp://ftp.cse.ucsc.edu/pub/tr/ucsc-crl-91-43.ps.Z
Refering-URL: ftp://ftp.cse.ucsc.edu/pub/tr/README.html
Root-URL: http://www.cse.ucsc.edu
Email: maopper@dgihrz01.bitnet  haussler@cis.ucsc.edu  
Title: Calculation of the Learning Curve of Bayes Optimal Classification Algorithm for Learning a Perceptron With
Author: Manfred Opper David Haussler 
Note: 2 as the  
Address: Justus-Liebig-Universitaet Giessen Giessen, Germany  Santa Cruz, CA 95064  
Affiliation: Institut fuer Theoretische Physik  Computer and Information Sciences U.C. Santa Cruz  
Abstract: The learning curve of Bayes optimal classification algorithm when learning a perceptron from noisy random training examples is calculated exactly in the limit of large training sample size and large instance space dimension using methods of statistical mechanics. It is shown that under certain assumptions, in this "thermodynamic" limit, the probability of misclassification of Bayes optimal algorithm is less than that of a canonical stochastic learning algorithm, by a factor approaching 
Abstract-found: 1
Intro-found: 1
Reference: [BH89] <author> E. Baum and D. Haussler. </author> <title> What size net gives valid generalization? Neural Computation, </title> <booktitle> 1(1) </booktitle> <pages> 151-160, </pages> <year> 1989. </year>
Reference-contexts: Recent work [DSW + 87] [HLW88] <ref> [BH89] </ref> [VJP89] [LTS89] [GT90] [HS90] [STS90] [OKKN90] has focused on quantifying what is known in the neural net literature as the generalization performance of learning algorithms.
Reference: [DMW88] <author> Alfredo DeSantis, George Markowski, and Mark N. Wegman. </author> <title> Learning probabilistic prediction functions. </title> <booktitle> In Proceedings of the 1988 Workshop on Computational Learning Theory, </booktitle> <pages> pages 312-328, </pages> <address> San Ma-teo, CA, </address> <year> 1988. </year> <note> Published by Morgan Kauf-mann. </note>
Reference-contexts: Here we apply similar methods from statistical physics to study Bayes optimal classification algorithm, a special case of the weighted majority algorithm [Lit89, LW89, Vov90] (see also <ref> [DMW88] </ref>). Further investigation of the Bayes and Gibbs algorithms appears in [HKS91], from both an information theory and a Vapnik-Chervonenkis theory perspective. The performance of any learning algorithm will depend on the target function, i.e. the input/output mapping to be learned.
Reference: [DSW + 87] <author> J. Denker, D. Schwartz, B. Wittner, S. Solla, R. Howard, L. Jackel, and J. Hop-field. </author> <title> Automatic learning, rule extraction and generalization. </title> <journal> Complex Syst., </journal> <volume> 1 </volume> <pages> 877-922, </pages> <year> 1987. </year>
Reference-contexts: Recent work <ref> [DSW + 87] </ref> [HLW88] [BH89] [VJP89] [LTS89] [GT90] [HS90] [STS90] [OKKN90] has focused on quantifying what is known in the neural net literature as the generalization performance of learning algorithms.
Reference: [Gar88] <author> E. Gardner. </author> <title> The space of interactions in neural networks. </title> <journal> J. Physics A, </journal> <volume> 21 </volume> <pages> 257-270, </pages> <year> 1988. </year>
Reference-contexts: 1 Introduction Extending a line of research initiated by Elizabeth Gardner <ref> [Gar88, GD88] </ref>, exceptional progress has been made in recent years in applying the methods of statistical mechanics to the analysis of the process of learning from random examples, as exemplified in the learning algorithms used to train neural networks.
Reference: [GD88] <author> E. Gardner and B. Derrida. </author> <title> Optimal storage properties of neural network models. </title> <journal> J. Physics A, </journal> <volume> 21 </volume> <pages> 271-284, </pages> <year> 1988. </year>
Reference-contexts: 1 Introduction Extending a line of research initiated by Elizabeth Gardner <ref> [Gar88, GD88] </ref>, exceptional progress has been made in recent years in applying the methods of statistical mechanics to the analysis of the process of learning from random examples, as exemplified in the learning algorithms used to train neural networks.
Reference: [GT90] <author> G. Gyorgyi and N. Tishby. </author> <title> Statistical theory of learning a rule. </title> <editor> In K. Thuemann and R. Koeberle, editors, </editor> <title> Neural Networks and Spin Glasses. </title> <publisher> World Scientific, </publisher> <year> 1990. </year>
Reference-contexts: Recent work [DSW + 87] [HLW88] [BH89] [VJP89] [LTS89] <ref> [GT90] </ref> [HS90] [STS90] [OKKN90] has focused on quantifying what is known in the neural net literature as the generalization performance of learning algorithms. <p> A canonical algorithm of this type, which we call the Gibbs algorithm 1 , was studied from a statistical mechanics perspective in <ref> [GT90, HS90, STS90] </ref>, and in a more abstract setting in [LW89] (as the randomized weighted majority algorithm) and [HKS91]. <p> Learning a perceptron was recently investigated by several authors from a statistical mechanics perspective <ref> [VJP89, Gyo90b, GT90, OKKN90] </ref>. We look at a simple model of learning where we assume that the function f fl can be learned perfectly by the neural network, i.e. f fl = f ~w fl for some weight vector ~w fl . We will call ~w fl the target vector. <p> This is the stochastic learning algorithm discussed in <ref> [GT90, STS90, LW89] </ref>. Z right m = f~w:f ~w (~x m+1 )= m+1 g i.e. the a posteriori volume (after the first m examples) of those hypotheses that predict correctly on the m + 1st example. <p> is defined by fiF (m) D E = (1 + e fi ) m X Z ( m ) ln Z ( m ) x m Using the replica trick, this is given by fiF (m) n!1 @n * m 2f1g m + By a calculation similar to that in <ref> [GT90] </ref>, one finds q (ff) by extremizing the expression f (ff) = lim fiF (m)=N 1 ln (1 q (ff)) + 2 + 1 + e fi 1 (22) where ^ H (t; ff) = e fi + (1 e fi )H (tfl (ff)): Plugging this value of q (ff) into <p> A slower convergence ( ff 1 2 ) of the generalization error has been predicted in <ref> [GT90] </ref> for a deterministic learning algorithm that tries to learn a perceptron with noise by minimizing the number of errors on the first m examples. Though the type of noise discussed in [GT90] is not the same as ours, we expect that the use of a nonzero temperature in the algorithm <p> A slower convergence ( ff 1 2 ) of the generalization error has been predicted in <ref> [GT90] </ref> for a deterministic learning algorithm that tries to learn a perceptron with noise by minimizing the number of errors on the first m examples. Though the type of noise discussed in [GT90] is not the same as ours, we expect that the use of a nonzero temperature in the algorithm is essential in obtaining faster convergence. A similar result has been given in [Gyo90b]. The function C ()(12) 2 is shown in Figure 4.
Reference: [Gyo90a] <author> G. Gyorgyi. </author> <title> First order transition to perfect generalization in a neural network with binary synapses. </title> <journal> Phys. Rev. A, </journal> <volume> 41:7097, </volume> <year> 1990. </year>
Reference-contexts: Using this distribution on the weight vectors, when the instances are distributed uniformly on the sphere, replica symmetry is known to be valid at = 0 for values of ff up to 1:245. Above this value a discontinuous transition to perfect generalization takes place <ref> [Gyo90a] </ref>. Figures 6 and 7 display results of our simulations for m = 4; N = 14 and m = 16; N = 20 respectively, averaged over many samples. The smooth curves are the theoretical predictions from Equation (24). The histograms represent the experimental results.
Reference: [Gyo90b] <author> G. Gyorgyi. </author> <title> Inference of a rule by a neural network with thermal noise. </title> <journal> Phys. Rev. Lett., </journal> <volume> 64:2957, </volume> <year> 1990. </year>
Reference-contexts: Learning a perceptron was recently investigated by several authors from a statistical mechanics perspective <ref> [VJP89, Gyo90b, GT90, OKKN90] </ref>. We look at a simple model of learning where we assume that the function f fl can be learned perfectly by the neural network, i.e. f fl = f ~w fl for some weight vector ~w fl . We will call ~w fl the target vector. <p> Though the type of noise discussed in [GT90] is not the same as ours, we expect that the use of a nonzero temperature in the algorithm is essential in obtaining faster convergence. A similar result has been given in <ref> [Gyo90b] </ref>. The function C ()(12) 2 is shown in Figure 4. Since C (0) 0:62, as a special case we obtain the generalization error without classification noise: ffi Gibbs (ff; 0) 0:62=ff = 0:62N=m, as in [OH91].
Reference: [HKS91] <author> D. Haussler, M. Kearns, and R. Schapire. </author> <title> Bounds on the sample complexity of Bayesian learning using information theory and the VC dimension. </title> <booktitle> In Proceedings of the Fourth Workshop on Computational Learning Theory, </booktitle> <year> 1991. </year>
Reference-contexts: A canonical algorithm of this type, which we call the Gibbs algorithm 1 , was studied from a statistical mechanics perspective in [GT90, HS90, STS90], and in a more abstract setting in [LW89] (as the randomized weighted majority algorithm) and <ref> [HKS91] </ref>. For noise-free training examples, the extreme "zero temperature" version of this algorithm simply chooses a hypothesis at random from among those that are consistent with all the training examples, as in [Maa91]. <p> Here we apply similar methods from statistical physics to study Bayes optimal classification algorithm, a special case of the weighted majority algorithm [Lit89, LW89, Vov90] (see also [DMW88]). Further investigation of the Bayes and Gibbs algorithms appears in <ref> [HKS91] </ref>, from both an information theory and a Vapnik-Chervonenkis theory perspective. The performance of any learning algorithm will depend on the target function, i.e. the input/output mapping to be learned. <p> For large ff and noise-free training examples, this probability is approximately 0:44=ff for Bayes algorithm and 0:62=ff for the Gibbs algorithm. These results show that the general upper bounds derived in <ref> [HKS91] </ref> for the performance of the Bayes and Gibbs algorithms on hypothesis spaces of finite VC dimension are tight to within a relatively small constant in this case. <p> a fraction of the volume of the original hypothesis space, is given by the partition function Z m , which in this case is just the measure of the version space under the a priori density d. (For the zero temperature case this volume is denoted by V m in <ref> [OH91, HKS91] </ref>.) Even in the finite fi case, it is useful to think of Z m as a kind of a posteriori volume measure on the hypothesis space, which decreases as the number m of training examples increases. 2.3 Gibbs and Bayes Algorithms In order to make its prediction on the <p> Each hidden unit output (i), i = 1; : : :; n, is passed to 2 Here we assume that the probability that 1+e fi 2 = Z m is zero, so it doesn't matter what value we choose for fi (0). A more general treatment is given in <ref> [HKS91] </ref>. a fixed output perceptron that computes the majority function maj = sign ( P n i=1 (i)) as the final output.
Reference: [HLW88] <author> D. Haussler, N. Littlestone, and M. War-muth. </author> <title> Predicting 0,1-functions on randomly drawn points. </title> <booktitle> In Proceedings of the 29th Annual Symposium on the Foundations of Computer Science, </booktitle> <pages> pages 100-109. </pages> <publisher> IEEE, </publisher> <year> 1988. </year>
Reference-contexts: Recent work [DSW + 87] <ref> [HLW88] </ref> [BH89] [VJP89] [LTS89] [GT90] [HS90] [STS90] [OKKN90] has focused on quantifying what is known in the neural net literature as the generalization performance of learning algorithms.
Reference: [HS90] <author> D. Hansel and H. Sompolinsky. </author> <title> Learning from examples in a single-layer neural network. </title> <journal> Europhys. Lett., </journal> <volume> 11(7) </volume> <pages> 687-692, </pages> <year> 1990. </year>
Reference-contexts: Recent work [DSW + 87] [HLW88] [BH89] [VJP89] [LTS89] [GT90] <ref> [HS90] </ref> [STS90] [OKKN90] has focused on quantifying what is known in the neural net literature as the generalization performance of learning algorithms. <p> A canonical algorithm of this type, which we call the Gibbs algorithm 1 , was studied from a statistical mechanics perspective in <ref> [GT90, HS90, STS90] </ref>, and in a more abstract setting in [LW89] (as the randomized weighted majority algorithm) and [HKS91].
Reference: [KS78] <author> S. Kirkpatrick and D. Sherrington. </author> <title> Infinite ranged models of spin glasses. </title> <journal> Phys. Rev. B, </journal> <volume> 17:4384, </volume> <year> 1978. </year>
Reference-contexts: C ij = ffi ij , and corresponding uniform spherical prior distribution on the target vector ~w fl (see Figure 2). In this case the order parameter q (ff) becomes identical to the Edwards-Anderson parameter <ref> [KS78] </ref>. It naturally appears in the calculation of the averaged free energy F (m).
Reference: [Lit89] <author> N. Littlestone. </author> <title> Mistake Bounds and Logarithmic Linear-threshold Learning Algorithms. </title> <type> PhD thesis, </type> <institution> University of Calif., Santa Cruz, </institution> <year> 1989. </year>
Reference-contexts: Here we apply similar methods from statistical physics to study Bayes optimal classification algorithm, a special case of the weighted majority algorithm <ref> [Lit89, LW89, Vov90] </ref> (see also [DMW88]). Further investigation of the Bayes and Gibbs algorithms appears in [HKS91], from both an information theory and a Vapnik-Chervonenkis theory perspective. The performance of any learning algorithm will depend on the target function, i.e. the input/output mapping to be learned. <p> is the correct Bayesian posterior density over all possible target vectors ~w, assuming the prior density is d ( ~w) and we are given the examples (~x 1 ; 1 ); (~x 2 ; 2 ); : : : ; (~x m ; m ). (This is also shown in <ref> [Lit89] </ref>). This means that for any (measurable) set of weight vectors W , R the conditional probability that the target vector was chosen from W , given the observed examples (~x 1 ; 1 ); (~x 2 ; 2 ); : : : ; (~x m ; m ). <p> This strategy is known as Bayes optimal classification algorithm or Bayes algorithm for short. In its general form, where fi is not necessarily set according to the noise rate , the algorithm is called the weighted majority algorithm <ref> [Lit89, LW89] </ref>. It is clear that Bayes algorithm makes an mistake in predition only when Z wrong m Z right m .
Reference: [LTS89] <author> E. Levin, N. Tishby, and S. Solla. </author> <title> A statistical approach to learning and generalization in neural networks. </title> <editor> In R. Rivest, editor, </editor> <booktitle> Proc. 2nd Workshop on Computational Learning Theory. </booktitle> <publisher> Morgan Kauf-mann, </publisher> <year> 1989. </year>
Reference-contexts: Recent work [DSW + 87] [HLW88] [BH89] [VJP89] <ref> [LTS89] </ref> [GT90] [HS90] [STS90] [OKKN90] has focused on quantifying what is known in the neural net literature as the generalization performance of learning algorithms. <p> )Z wrong m : Thus, since the Gibbs algorithm chooses its hypothesis at random according to the posterior density d m , it makes a mistake in predicting m+1 with probability Z wrong m = 1 e fi 1 Z m : (3) A similar formulation has been obtained in <ref> [LTS89] </ref> and [LW89].
Reference: [LW89] <author> N. Littlestone and M. Warmuth. </author> <title> The weighted majority algorithm. </title> <booktitle> In 30th Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 256-261, </pages> <year> 1989. </year>
Reference-contexts: A canonical algorithm of this type, which we call the Gibbs algorithm 1 , was studied from a statistical mechanics perspective in [GT90, HS90, STS90], and in a more abstract setting in <ref> [LW89] </ref> (as the randomized weighted majority algorithm) and [HKS91]. For noise-free training examples, the extreme "zero temperature" version of this algorithm simply chooses a hypothesis at random from among those that are consistent with all the training examples, as in [Maa91]. <p> Here we apply similar methods from statistical physics to study Bayes optimal classification algorithm, a special case of the weighted majority algorithm <ref> [Lit89, LW89, Vov90] </ref> (see also [DMW88]). Further investigation of the Bayes and Gibbs algorithms appears in [HKS91], from both an information theory and a Vapnik-Chervonenkis theory perspective. The performance of any learning algorithm will depend on the target function, i.e. the input/output mapping to be learned. <p> In statistical mechanics, Z m is called the partition function. Note that in the posterior density, the (unnor-malized) weight of a hypothesis is reduced exponentially in proportion to the number of times it is incorrect on the training sequence, as in the weighted majority algorithm <ref> [LW89] </ref>. <p> This is the stochastic learning algorithm discussed in <ref> [GT90, STS90, LW89] </ref>. Z right m = f~w:f ~w (~x m+1 )= m+1 g i.e. the a posteriori volume (after the first m examples) of those hypotheses that predict correctly on the m + 1st example. <p> m : Thus, since the Gibbs algorithm chooses its hypothesis at random according to the posterior density d m , it makes a mistake in predicting m+1 with probability Z wrong m = 1 e fi 1 Z m : (3) A similar formulation has been obtained in [LTS89] and <ref> [LW89] </ref>. <p> This strategy is known as Bayes optimal classification algorithm or Bayes algorithm for short. In its general form, where fi is not necessarily set according to the noise rate , the algorithm is called the weighted majority algorithm <ref> [Lit89, LW89] </ref>. It is clear that Bayes algorithm makes an mistake in predition only when Z wrong m Z right m .
Reference: [Maa91] <author> Wolfgang Maass. </author> <title> On-line learning with an oblivious environment and the power of randomization. </title> <booktitle> In Proceedings of the Fourth Workshop on Computational Learning Theory, </booktitle> <year> 1991. </year>
Reference-contexts: For noise-free training examples, the extreme "zero temperature" version of this algorithm simply chooses a hypothesis at random from among those that are consistent with all the training examples, as in <ref> [Maa91] </ref>. Here we apply similar methods from statistical physics to study Bayes optimal classification algorithm, a special case of the weighted majority algorithm [Lit89, LW89, Vov90] (see also [DMW88]). Further investigation of the Bayes and Gibbs algorithms appears in [HKS91], from both an information theory and a Vapnik-Chervonenkis theory perspective.
Reference: [Mit82] <author> T. M. Mitchell. </author> <title> Generalization as search. </title> <booktitle> Art. Intell., </booktitle> <volume> 18 </volume> <pages> 203-226, </pages> <year> 1982. </year>
Reference-contexts: This is called the version space in the AI literature <ref> [Mit82] </ref>. Thus in the zero temperature limit, all hypotheses that contradict even one training example are eliminated from consideration.
Reference: [MPV87] <author> M. Mezard, G. Parisi, and M.A. Virasoro. </author> <title> Spin Glass Theory and Beyond, </title> <booktitle> volume 9 of Lecture Notes in Physics. World Scientific, </booktitle> <year> 1987. </year>
Reference-contexts: Let ff = m=N . We will calculate the limit y (ff; r) of y (m; r) as m; N ! 1 with ff held constant. To do this, we make the crucial assumption of replica symmetry <ref> [MPV87] </ref>.
Reference: [OH91] <author> M. Opper and D. Haussler. </author> <title> Generalization performance of Bayes optimal classification algorithm for learning a perceptron. </title> <journal> Physical Review Letters, </journal> <month> May </month> <year> 1991. </year> <note> to appear. </note>
Reference-contexts: We assume that a target perceptron is selected at random according to a prior distribution, and that noisy training examples are generated from this target, where classification label of each example is flipped independently with some probability 0 1=2. The noise-free case ( = 0) was investigated previously in <ref> [OH91] </ref>. 1 This algorithm was called the Boltzmann algorithm in [OH91]. <p> The noise-free case ( = 0) was investigated previously in <ref> [OH91] </ref>. 1 This algorithm was called the Boltzmann algorithm in [OH91]. In the noisy case, one can measure the probability of misclassification as either the probability that a mistake is made in predicting the noisy label, or the probability that a mistake is made predicting the underlying classification label, before the noise is added. <p> a fraction of the volume of the original hypothesis space, is given by the partition function Z m , which in this case is just the measure of the version space under the a priori density d. (For the zero temperature case this volume is denoted by V m in <ref> [OH91, HKS91] </ref>.) Even in the finite fi case, it is useful to think of Z m as a kind of a posteriori volume measure on the hypothesis space, which decreases as the number m of training examples increases. 2.3 Gibbs and Bayes Algorithms In order to make its prediction on the <p> A similar result has been given in [Gyo90b]. The function C ()(12) 2 is shown in Figure 4. Since C (0) 0:62, as a special case we obtain the generalization error without classification noise: ffi Gibbs (ff; 0) 0:62=ff = 0:62N=m, as in <ref> [OH91] </ref>. As already noted, the corresponding Bayes value is smaller by a factor of p 2. In Figure 2 the generalization errors for the noise-free case = 0 are depicted as functions of ff.
Reference: [OKKN90] <author> M. Opper, W. Kinzel, J. Kleinz, and R. Nehl. </author> <title> On the ability of the optimal percepton to generalize. </title> <address> J.Phys.A.: Math.Gen., 23:L581-L586, </address> <year> 1990. </year>
Reference-contexts: Recent work [DSW + 87] [HLW88] [BH89] [VJP89] [LTS89] [GT90] [HS90] [STS90] <ref> [OKKN90] </ref> has focused on quantifying what is known in the neural net literature as the generalization performance of learning algorithms. <p> Learning a perceptron was recently investigated by several authors from a statistical mechanics perspective <ref> [VJP89, Gyo90b, GT90, OKKN90] </ref>. We look at a simple model of learning where we assume that the function f fl can be learned perfectly by the neural network, i.e. f fl = f ~w fl for some weight vector ~w fl . We will call ~w fl the target vector.
Reference: [STS90] <author> H. Sompolinsky, N. Tishby, and H.S. Se-ung. </author> <title> Learning from examples in large neural networks. </title> <journal> Phys.Rev.Lett., </journal> <volume> 65 </volume> <pages> 1683-1686, </pages> <year> 1990. </year>
Reference-contexts: Recent work [DSW + 87] [HLW88] [BH89] [VJP89] [LTS89] [GT90] [HS90] <ref> [STS90] </ref> [OKKN90] has focused on quantifying what is known in the neural net literature as the generalization performance of learning algorithms. <p> A canonical algorithm of this type, which we call the Gibbs algorithm 1 , was studied from a statistical mechanics perspective in <ref> [GT90, HS90, STS90] </ref>, and in a more abstract setting in [LW89] (as the randomized weighted majority algorithm) and [HKS91]. <p> This is the stochastic learning algorithm discussed in <ref> [GT90, STS90, LW89] </ref>. Z right m = f~w:f ~w (~x m+1 )= m+1 g i.e. the a posteriori volume (after the first m examples) of those hypotheses that predict correctly on the m + 1st example. <p> For fi 2 ff ! 1 we find ffi Gibbs (ff; ) 4 (1 2) 2 ff which is the dashed line in Figure 3. It follows from this and Equation (1) that * Gibbs (ff; ) + 2 fiff + (1 2)ff In <ref> [STS90] </ref> related results are given for the Gibbs algorithm in a general but somewhat different setting. 3 Conclusion Our results show that in the limit the relationship between the generalization error of the Bayes and the Gibbs algorithms is independent of the particular assumptions made about the densities used to choose
Reference: [VJP89] <author> F. Vallet, J.Cailton, and P.Refregier. </author> <title> Linear and nonlinear extensions of the pseudo inverse for learning Boolean functions. </title> <journal> Eu-rophys.Lett., </journal> <volume> 9 </volume> <pages> 315-320, </pages> <year> 1989. </year>
Reference-contexts: Recent work [DSW + 87] [HLW88] [BH89] <ref> [VJP89] </ref> [LTS89] [GT90] [HS90] [STS90] [OKKN90] has focused on quantifying what is known in the neural net literature as the generalization performance of learning algorithms. <p> Learning a perceptron was recently investigated by several authors from a statistical mechanics perspective <ref> [VJP89, Gyo90b, GT90, OKKN90] </ref>. We look at a simple model of learning where we assume that the function f fl can be learned perfectly by the neural network, i.e. f fl = f ~w fl for some weight vector ~w fl . We will call ~w fl the target vector.

References-found: 22

