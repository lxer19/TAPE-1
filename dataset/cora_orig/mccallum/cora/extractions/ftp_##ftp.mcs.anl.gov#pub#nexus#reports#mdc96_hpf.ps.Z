URL: ftp://ftp.mcs.anl.gov/pub/nexus/reports/mdc96_hpf.ps.Z
Refering-URL: http://www.mcs.anl.gov/fortran-m/
Root-URL: http://www.mcs.anl.gov
Email: ffoster,kohrg@mcs.anl.gov  frakesh,choudharg@cat.syr.edu  
Title: MPI as a Coordination Layer for Communicating HPF Tasks  
Author: Ian T. Foster David R. Kohr, Jr. Rakesh Krishnaiyer Alok Choudhary 
Address: Argonne, IL 60439  Syracuse, NY 13244  
Affiliation: Mathematics and Computer Science Div. Argonne National Laboratory  Dept. of Computer and Information Science  Dept. of Electrical and Computer Engineering Syracuse University  
Abstract: Data-parallel languages such as High Performance Fortran (HPF) present a simple execution model in which a single thread of control performs high-level operations on distributed arrays. These languages can greatly ease the development of parallel programs. Yet there are large classes of applications for which a mixture of task and data parallelism is most appropriate. Such applications can be structured as collections of data-parallel tasks that communicate by using explicit message passing. Because the Message Passing Interface (MPI) defines standardized, familiar mechanisms for this communication model, we propose that HPF tasks communicate by making calls to a coordination library that provides an HPF binding for MPI. The semantics of a communication interface for sequential languages can be ambiguous when the interface is invoked from a parallel language; we show how these ambiguities can be resolved by describing one possible HPF binding for MPI. We then present the design of a library that implements this binding, discuss issues that influenced our design decisions, and evaluate the performance of a prototype HPF/MPI library using a communications microbenchmark and application kernel. Finally, we discuss how MPI features might be incorporated into our design framework. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. N. Choudhary, Narahari, D. M. Nicol, and R. Simha. </author> <title> Optimal processor assignment for pipeline computations. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 5(4) </volume> <pages> 439-445, </pages> <year> 1994. </year>
Reference-contexts: Therefore, even if communication overhead causes the latency of a pipelined version to rise above that of a purely data-parallel version, so that the speedup of the pipeline at processing one dataset is actually lower, the pipeline may still be preferable because its throughput is higher <ref> [1] </ref>. 1.2. <p> We note, however, that HPF/MPI can be layered atop other communication substrates. In Section 4, we discuss how functionality beyond that provided by MPI could aid in extending our subset library. Many of the applications we wish to support require low latency for certain communications which are repeated frequently <ref> [1] </ref>. MPI includes the functions MPI Send init and MPI Recv init for defining persistent requests for sends and receives; persistent requests allow an implementation to recognize and optimize such repeated operations. Therefore we selected persistent requests as the first MPI optimization facility to add to our library.
Reference: [2] <author> I. Foster, B. Avalani, A. Choudhary, and M. Xu. </author> <title> A compilation system that integrates High Performance Fortran and Fortran M. </title> <booktitle> In Proceedings of the 1994 Scalable High Performance Computing Conference, </booktitle> <pages> pages 293-300. </pages> <publisher> IEEE Computer Society, </publisher> <year> 1994. </year>
Reference-contexts: Fortunately, many such programs can be decomposed into independent data-parallel tasks that can execute in parallel on a subset of the available processors at higher parallel efficiency than the original program running on all processors <ref> [2, 6] </ref>. <p> Approaches based on compilers rely on sophisticated source code analyses and programmer-supplied directives to extract implicit task parallelism from programs [6]. In language-based approaches, language extensions permit programmers to explicitly specify the division of a computation into tasks, the mapping of tasks to processors, and communication between tasks <ref> [2] </ref>. Further comparison with other approaches appears in [3]. We have presented a design for the subset binding of MPI.
Reference: [3] <author> I. Foster, D. R. Kohr, Jr., R. Krishnaiyer, and A. Choudhary. </author> <title> Double standards: Bringing task parallelism to HPF via the Message Passing Interface. </title> <type> Preprint, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Ill., </institution> <year> 1996. </year>
Reference-contexts: In language-based approaches, language extensions permit programmers to explicitly specify the division of a computation into tasks, the mapping of tasks to processors, and communication between tasks [2]. Further comparison with other approaches appears in <ref> [3] </ref>. We have presented a design for the subset binding of MPI.
Reference: [4] <author> W. Gropp, E. Lusk, N. Doss, and A. Skjellum. </author> <title> A high-performance, portable implementation of the MPI message passing interface standard. </title> <type> Technical Report ANL/MCS-TM-213, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Ar-gonne, Ill., </institution> <year> 1996. </year>
Reference-contexts: All experiments were performed on Argonne's IBM SP system, which consists of 128 Power 1 processors linked by an SP2 interconnection network. The underlying sequential MPI library was MPICH <ref> [4] </ref>. All HPF programs were compiled with pghpf, using what we determined to be the most effective optimization switches. 3.1. Communication performance To evaluate the performance of our library at transferring distributed arrays between tasks, we use a data-parallel variant of the standard "ping-pong" communication benchmark.
Reference: [5] <author> W. Gropp, E. Lusk, and A. Skjellum. </author> <title> Using MPI: Portable Parallel Processing with the Message-Passing Interface. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1994. </year>
Reference-contexts: 1. Introduction Message-passing libraries such as the Message Passing Interface (MPI) provide programmers with a high degree of control over the mapping of a parallel program's tasks to processors, and over inter-processor fl To whom correspondence should be addressed. communications <ref> [5] </ref>. However, this control comes at a high price: programmers must explicitly manage all details relating to parallelism, such as synchronization and data transfer.
Reference: [6] <author> T. Gross, D. O'Hallaron, and J. Subhlok. </author> <title> Task parallelism in a High Performance Fortran framework. </title> <journal> IEEE Parallel and Distributed Technology, </journal> <volume> 2(2) </volume> <pages> 16-26, </pages> <month> Fall </month> <year> 1994. </year>
Reference-contexts: Fortunately, many such programs can be decomposed into independent data-parallel tasks that can execute in parallel on a subset of the available processors at higher parallel efficiency than the original program running on all processors <ref> [2, 6] </ref>. <p> However, many other techniques 10 have been used to introduce task parallelism into data--parallel languages. These other techniques fall into two major categories: compiler-based approaches and language-based approaches. Approaches based on compilers rely on sophisticated source code analyses and programmer-supplied directives to extract implicit task parallelism from programs <ref> [6] </ref>. In language-based approaches, language extensions permit programmers to explicitly specify the division of a computation into tasks, the mapping of tasks to processors, and communication between tasks [2]. Further comparison with other approaches appears in [3]. We have presented a design for the subset binding of MPI.
Reference: [7] <author> C. Koelbel, D. Loveman, R. Schreiber, G. Steele Jr., and M. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> The MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: In contrast, data-parallel languages such as High Performance Fortran (HPF) provide a simple programming model in which all processors execute a single, logical thread of control that performs high-level operations on distributed arrays; many tedious details are managed automatically by the compiler <ref> [7] </ref>. 1.1. Limitations of data parallelism While data-parallel languages such as HPF can greatly ease development of concise solutions to many parallel programming problems, the rate of improvement of speedup of many data-parallel programs diminishes sharply as more processors are used to execute a program.
Reference: [8] <author> S. Ramaswamy and P. Banerjee. </author> <title> Automatic generation of efficient array redistribution routines for distributed memory multicomputers. </title> <booktitle> In Frontiers '95: The 5th Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 342-349, </pages> <address> McLean, Va., </address> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: The schedule is computed by algorithms based on the FALLS rep resentation of Ramaswamy and Banerjee <ref> [8] </ref>. 5. Transfer buffer packing: The elements to be sent to a single processor of task 1 are packed (gath ered) into a single contiguous transfer buffer . 6.
Reference: [9] <institution> The Portland Group, Inc. </institution> <note> pghpf Reference Manual. </note> <institution> 9150 SW Pioneer Ct., Suite H, Wilsonville, Oregon 97070. </institution>
Reference-contexts: Because the implementation of all of MPI is a daunting task, we have restricted our efforts to a small subset so that we can focus on analyzing and understanding design and performance issues. Our HPF/MPI implementation operates with the commercial HPF compiler pghpf, developed by the Portland Group, Inc. <ref> [9] </ref> The design of our HPF/MPI library was guided from the outset by several underlying assumptions and objectives, including the following: * The primary target platforms on which we would run HPF/MPI applications would be distributed memory multicomputers. * We wished to maintain a high degree of portability across hardware and
Reference: [10] <author> T. von Eicken, D. Culler, S. Goldstein, and K. Schauser. </author> <title> Active messages: a mechanism for integrated communication and computation. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year> <month> 11 </month>
Reference-contexts: In general, what is needed to permit maximum overlap between HPF/MPI library processing and application processing is some form of message-driven execution: the ability for some computation specified by HPF/MPI to occur upon arrival of certain messages <ref> [10] </ref>. When a message with an array descriptor arrives (Step 3), communication schedule computation should begin (Step 4), and when a data message arrives at a receiver (Step 5), it should be unpacked (Step 6).
References-found: 10

