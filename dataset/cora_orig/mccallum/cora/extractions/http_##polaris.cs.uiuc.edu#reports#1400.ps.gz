URL: http://polaris.cs.uiuc.edu/reports/1400.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/polaris/rep2.html
Root-URL: http://www.cs.uiuc.edu
Title: Run-Time Methods for Parallelizing Partially Parallel Loops  
Author: Lawrence Rauchwerger Nancy M. Amato David A. Padua 
Affiliation: University of Illinois Texas A&M University University of Illinois  
Abstract: In this paper we give a new run-time technique for finding an optimal parallel execution schedule for a partially parallel loop, i.e., a loop whose parallelization requires synchronization to ensure that the iterations are executed in the correct order. Given the original loop, the compiler generates inspector code that performs run-time preprocessing of the loop's access pattern, and scheduler code that schedules (and executes) the loop iterations. The inspector is fully parallel, uses no synchronization, and can be applied to any loop. In addition, it can implement at run-time the two most effective transformations for increasing the amount of parallelism in a loop: array privatization and reduction parallelization (element-wise). We also describe a new scheme for constructing an optimal parallel execution schedule for the iterations of the loop. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Alliant Computer Systems Corporation. FX/Series Architecture Manual, </institution> <year> 1986. </year>
Reference-contexts: Our interest in fully parallel loops is motivated by the observation that they arise frequently in real programs. 5 Implementation and Experimental Results We present experimental results obtained on two modestly parallel machines with 8 (Alliant FX/80 <ref> [1] </ref>) and 14 processors (Alliant FX/2800 [2]). However, we remark that the results scale with the number of processors and the data size and thus they may be extrapolated for massively parallel processors (MPPs), the actual target of our run-time methods.
Reference: [2] <institution> Alliant Computers Systems Corporation. Alliant FX/2800 Series System Description, </institution> <year> 1991. </year>
Reference-contexts: Our interest in fully parallel loops is motivated by the observation that they arise frequently in real programs. 5 Implementation and Experimental Results We present experimental results obtained on two modestly parallel machines with 8 (Alliant FX/80 [1]) and 14 processors (Alliant FX/2800 <ref> [2] </ref>). However, we remark that the results scale with the number of processors and the data size and thus they may be extrapolated for massively parallel processors (MPPs), the actual target of our run-time methods.
Reference: [3] <author> U. Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer. </publisher> <address> Boston, MA., </address> <year> 1988. </year>
Reference-contexts: since none of them has all of these properties (a comparison to previous work is contained in Section 4). 2 Preliminaries In order to guarantee the semantics of a loop, the parallel execution schedule for its iterations must respect the data dependence relations between the statements in the loop body <ref> [22, 15, 3, 32, 35] </ref>. There are three possible types of dependences between two statements that access the same memory location: flow (read after write), anti (write after read), and output (write after write). Flow dependences express a fundamental relationship about the data flow in the program. <p> If adjacent references are both reads, then there is no dependence between the elements, but they may have a common parent (child) in D x : the last write preceding (first write following) them in R x . For example, the dependence graph D 3 for A <ref> [3] </ref> is shown in Figure 2 (c). Our goal is to encode the predecessor/successor information of the (conceptual) dependence graph D x in a hierarchy vector H x so that the scheduler can easily look-up the dependence information for the references to A [x]. <p> We now give an example of how the hierarchy vector serves as a look-up table for the predecessors and successors of all the accesses. Consider the read access to A <ref> [3] </ref> in the 6th iteration, which appears as the 6th entry in R 3 . <p> Processor i can find the predecessors (successors) needed for its hierarchy vectors by scanning the arrays of the processors less than (larger than) i. For example, the ? at the end of pH <ref> [3] </ref> for processor 1 in Figure 3 would be filled in with a pointer to the first element in the array pR [3] of processor 2. Hence, the initial and final entries in the hierarchy vectors also need to store the processor number that contains the predecessor and successor. <p> For example, the ? at the end of pH <ref> [3] </ref> for processor 1 in Figure 3 would be filled in with a pointer to the first element in the array pR [3] of processor 2. Hence, the initial and final entries in the hierarchy vectors also need to store the processor number that contains the predecessor and successor.
Reference: [4] <author> M. Berry and others. </author> <title> The PERFECT club benchmarks: Effective performance evaluation of supercomputers. </title> <type> TR. 827, Ctr. </type> <institution> for Supercomputing R.&D., Univ. of Illinois, Urbana, IL, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: To demonstrate that the new methods can achieve speedups, we applied them to three loops contained in the PERFECT Benchmarks <ref> [4] </ref>. To analyze the overhead incurred by the methods we applied them to access patterns taken from actual programs and to synthetic access patterns. The methods were implemented in Cedar Fortran [12]. The inspector was essentially as described in Section 3.1. <p> Larger speedups were not obtained since the loop is heavily imbalanced due to the blocked nature of the algorithm used in MA28. Perfect Benchmark Loops We applied the methods to three loops contained in the PERFECT Benchmarks <ref> [4] </ref>. In the analysis phase it was found that one of the loops was fully parallel, and that the other two could be transformed into doalls by privatizing the shared array under test.
Reference: [5] <author> H. Berryman and J. Saltz. </author> <title> A manual for PARTI runtime primitives. </title> <type> Interim Report 90-13, </type> <institution> ICASE, </institution> <year> 1990. </year>
Reference-contexts: Since compile-time data dependence analysis techniques cannot be used on such programs, methods of performing the analysis at run-time are required. Several techniques have been developed for the run-time analysis and scheduling of loops with cross-iteration dependences <ref> [5, 9, 13, 17, 20, 23, 28, 29, 30, 33, 34] </ref>. However, for various reasons, such techniques have not achieved wide-spread use in current parallelizing compilers. In the following we describe a new run-time scheme for constructing a parallel execution schedule for the iterations of a loop. <p> Although this scheme potentially has less communication overhead than [34], it is still sensitive to hot spots and there are cases (e.g., doalls) in which it proves inferior to [34]. Methods for loops without output dependences. This problem has also been studied extensively by Saltz et al. <ref> [5, 28, 29, 30, 33] </ref>. Most of their work assumes that there are no output dependences in the source loop. In doacross parallelization [28], an inspector finds the (at most one) iteration in which each variable is written.
Reference: [6] <author> W. Blume and R. Eigenmann. </author> <title> Performance analysis of parallelizing compilers on the Perfect Benchmarks T M Programs. </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> 3(6) </volume> <pages> 643-656, </pages> <month> Nov. </month> <year> 1992. </year>
Reference-contexts: Research supported in part by an AT&T Bell Laboratories Graduate Fellowship, NSF Grant CCR-9315696, and the International Computer Science Institute, Berkeley, CA. Thus, since the available parallelism in theses types of applications cannot be determined statically by present parallelizing compilers <ref> [6, 8] </ref>, compile-time analysis must be complemented by new methods capable of automatically extracting parallelism at run-time. Run-time techniques are needed because the access pattern of some programs cannot be statically determined, either because of limitations of current analysis algorithms or because the access pattern is input data dependent.
Reference: [7] <author> M. Burke, R. Cytron, J. Ferrante, and W. Hsieh. </author> <title> Automatic generation of nested, fork-join parallelism. </title> <editor> J. </editor> <booktitle> of Supercomputing, </booktitle> <pages> pp. 71-88, </pages> <year> 1989. </year>
Reference-contexts: In order to remove certain types of dependences two transformations can be applied to the loop: privatization and reduction parallelization. Privatization creates, for each processor cooperating on the execution of the loop, private copies of the program variables that give rise to anti or output dependences (see, e.g., <ref> [7, 18, 19, 31] </ref>).
Reference: [8] <author> W. J. Camp, S. J. Plimpton, B. A. Hendrickson, and R. W. Leland. </author> <title> Massively parallel methods for engineering and science problems. </title> <journal> Comm. ACM, </journal> <volume> 37(4) </volume> <pages> 31-41, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Examples include SPICE for circuit simulation, DYNA-3D and PRONTO-3D for structural mechanics modeling, GAUSSIAN and DMOL for quantum mechanical simulation of molecules, CHARMM and DISCOVER for molecular dynamics simulation of organic systems, and FIDAP for modeling complex fluid flows <ref> [8] </ref>. x Due to space limitations, this paper is an extended abstract of [24]. y Center for Supercomputing Research & Development, 1308 W. Main St., Urbana, IL 61801, email: rwerger,padua@csrd.uiuc.edu. Research supported in part by Intel and NASA Graduate Fellowships, and Army contract #DABT63-92-C-0033. <p> Research supported in part by an AT&T Bell Laboratories Graduate Fellowship, NSF Grant CCR-9315696, and the International Computer Science Institute, Berkeley, CA. Thus, since the available parallelism in theses types of applications cannot be determined statically by present parallelizing compilers <ref> [6, 8] </ref>, compile-time analysis must be complemented by new methods capable of automatically extracting parallelism at run-time. Run-time techniques are needed because the access pattern of some programs cannot be statically determined, either because of limitations of current analysis algorithms or because the access pattern is input data dependent.
Reference: [9] <author> D. K. Chen, P. C. Yew, and J. Torrellas. </author> <title> An efficient algorithm for the run-time parallelization of doacross loops. </title> <booktitle> In Proc. of Supercomputing 1994, </booktitle> <pages> pp. 518-527, </pages> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: Since compile-time data dependence analysis techniques cannot be used on such programs, methods of performing the analysis at run-time are required. Several techniques have been developed for the run-time analysis and scheduling of loops with cross-iteration dependences <ref> [5, 9, 13, 17, 20, 23, 28, 29, 30, 33, 34] </ref>. However, for various reasons, such techniques have not achieved wide-spread use in current parallelizing compilers. In the following we describe a new run-time scheme for constructing a parallel execution schedule for the iterations of a loop. <p> The method of Zhu and Yew [34] computes the wavefronts one after another using a method simi obtains contains requires restricts privat optimal serial global type of or Method sched portions synch loop reduct New Yes No No No P,R MP [20] Yes No Yes 2 No No CYT <ref> [9] </ref> No 1;3 No Yes No No SMC [30] Yes Yes 4 Yes Yes 5 No P [23] No No No No No Table 1: A comparison of run-time parallelization techniques for do loops. <p> Flow dependences are enforced using full/empty bits. To our knowledge, this is the only other run-time privatization technique except for the one described in [25, 26]. Recently, Chen, Yew, and Torrellas <ref> [9] </ref> proposed an inspector that first builds (in private storage) access lists for each memory location referenced in a processor's assigned iterations (similar to [13] and our inspector's marking phase, except they serialize read accesses), and then links them across processors using a global Zhu/Yew algorithm [34].
Reference: [10] <author> I. S. Duff. </author> <title> Ma28-a set of Fortran subroutines for sparse unsymmetric linear equations. </title> <type> Tech. </type> <address> Rept. AERE R8730, HMSO, London, </address> <year> 1977. </year>
Reference-contexts: For this purpose we chose Loop MA30cd/DO 120 from MA28 (a blocked sparse non-symmetric linear solver <ref> [10] </ref>). We selected this loop, which performs the forward-backward substitution in the final phase of the blocked sparse linear system solver, because it can generate many diverse access patterns when using the Harwell-Boeing matrices as input.
Reference: [11] <author> R. Eigenmann, J. Hoeflinger, Z. Li, and D. Padua. </author> <title> Experience in the automatic parallelization of four Perfect-Benchmark programs. </title> <booktitle> In Lecture Notes in Comp. Science 589. Proc. of the 4th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Santa Clara, CA, </address> <pages> pp. 65-83, </pages> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: Once reduction variables are identified, methods are known for performing the reduction operation in parallel (see, e.g., <ref> [11, 14, 16, 35] </ref>). 3 Run-Time Analysis of Loops Given a do loop whose access pattern cannot be statically analyzed, compilers have traditionally generated sequential code. Since compile-time data dependence analysis techniques cannot be used on such programs, methods of performing the analysis at run-time are required.
Reference: [12] <author> M. Guzzi, D. Padua, J. Hoeflinger, and D. Lawrie. </author> <title> Cedar Fortran and other vector and parallel Fortran dialects. </title> <journal> J. Supercomput., </journal> <volume> 4(1) </volume> <pages> 37-62, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: To analyze the overhead incurred by the methods we applied them to access patterns taken from actual programs and to synthetic access patterns. The methods were implemented in Cedar Fortran <ref> [12] </ref>. The inspector was essentially as described in Section 3.1. In particular, we implemented the bucket sort version using separate pR and pH data structures for each processor. Each processor constructed a linked list of the non-empty rows in its pR array during the marking phase.
Reference: [13] <author> V. Krothapalli and P. Sadayappan. </author> <title> An approach to synchronization of parallel computing. </title> <booktitle> In Proc. of the 1988 Int. Conf. on Supercomputing, </booktitle> <pages> pp. 573-581, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Since compile-time data dependence analysis techniques cannot be used on such programs, methods of performing the analysis at run-time are required. Several techniques have been developed for the run-time analysis and scheduling of loops with cross-iteration dependences <ref> [5, 9, 13, 17, 20, 23, 28, 29, 30, 33, 34] </ref>. However, for various reasons, such techniques have not achieved wide-spread use in current parallelizing compilers. In the following we describe a new run-time scheme for constructing a parallel execution schedule for the iterations of a loop. <p> A feature of them is that they use only a shadow version of the shared array whereas all other methods (except [23, 25, 26]) unroll the loop and store all accesses to the shared array. Krothapalli and Sadayappan <ref> [13] </ref> proposed a run-time scheme for removing anti and output dependences from loops. For each memory location, their inspector counts the number references to it (using critical sections as in [34]), places them in a dynamically allocated array, and then sorts them by iteration number. <p> To our knowledge, this is the only other run-time privatization technique except for the one described in [25, 26]. Recently, Chen, Yew, and Torrellas [9] proposed an inspector that first builds (in private storage) access lists for each memory location referenced in a processor's assigned iterations (similar to <ref> [13] </ref> and our inspector's marking phase, except they serialize read accesses), and then links them across processors using a global Zhu/Yew algorithm [34]. Their scheduler/executor uses doacross parallelization [28] (see below).
Reference: [14] <author> C. Kruskal. </author> <title> Efficient parallel algorithms for graph problems. </title> <booktitle> In Proc. of the 1986 Int. Conf. on Parallel Processing, </booktitle> <pages> pp. 869-876, </pages> <month> Aug. </month> <year> 1986. </year>
Reference-contexts: Once reduction variables are identified, methods are known for performing the reduction operation in parallel (see, e.g., <ref> [11, 14, 16, 35] </ref>). 3 Run-Time Analysis of Loops Given a do loop whose access pattern cannot be statically analyzed, compilers have traditionally generated sequential code. Since compile-time data dependence analysis techniques cannot be used on such programs, methods of performing the analysis at run-time are required.
Reference: [15] <author> D. J. Kuck, R. H. Kuhn, B. Leasure, D. A. Padua, and M. Wolfe. </author> <title> Dependence graphs and compiler optimizations. </title> <booktitle> In Proc. of 8th ACM Symp. Princip. Prog. Lang., </booktitle> <pages> pp. 207-218, </pages> <month> Jan. </month> <year> 1981. </year>
Reference-contexts: since none of them has all of these properties (a comparison to previous work is contained in Section 4). 2 Preliminaries In order to guarantee the semantics of a loop, the parallel execution schedule for its iterations must respect the data dependence relations between the statements in the loop body <ref> [22, 15, 3, 32, 35] </ref>. There are three possible types of dependences between two statements that access the same memory location: flow (read after write), anti (write after read), and output (write after write). Flow dependences express a fundamental relationship about the data flow in the program.
Reference: [16] <author> F. Thomson Leighton. </author> <title> Introduction to Parallel Algorithms and Architectures: Arrays, Trees, Hypercubes. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: Once reduction variables are identified, methods are known for performing the reduction operation in parallel (see, e.g., <ref> [11, 14, 16, 35] </ref>). 3 Run-Time Analysis of Loops Given a do loop whose access pattern cannot be statically analyzed, compilers have traditionally generated sequential code. Since compile-time data dependence analysis techniques cannot be used on such programs, methods of performing the analysis at run-time are required. <p> The cost of these searches can be reduced from p to O (log p) using a standard parallel divide-and-conquer pair-wise merging approach <ref> [16] </ref>, where p is the total number of processors. Privatization and Reduction Recognition. The basic inspector described above can easily be augmented to find the array elements that are independent (i.e., accessed in only one iteration), read-only, privatizable, or reduction variables. <p> In particular, using the bucket sort implementation, each processor spends constant time on each of its O (a) accesses in the marking phase, and the analysis phase takes time O (a log p) using a parallel divide-and-conquer pair-wise merging strategy <ref> [16] </ref>.
Reference: [17] <author> S. Leung and J. Zahorjan. </author> <title> Improving the performance of runtime parallelization. </title> <booktitle> In 4th PPOPP, </booktitle> <pages> pp. 83-91, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Since compile-time data dependence analysis techniques cannot be used on such programs, methods of performing the analysis at run-time are required. Several techniques have been developed for the run-time analysis and scheduling of loops with cross-iteration dependences <ref> [5, 9, 13, 17, 20, 23, 28, 29, 30, 33, 34] </ref>. However, for various reasons, such techniques have not achieved wide-spread use in current parallelizing compilers. In the following we describe a new run-time scheme for constructing a parallel execution schedule for the iterations of a loop. <p> The topological sort can be parallelized somewhat using doacross parallelization. Leung and Zahorjan <ref> [17] </ref> proposed methods of parallelizing the sequential inspector of [30]. In theit sectioning method, the loop is chunked and each processor computes an optimal schedule for its chunk, and then these schedules are concatenated together separated by synchronization barriers.
Reference: [18] <author> Zhiyuan Li. </author> <title> Array privatization for parallel execution of loops. </title> <booktitle> In Proc. of the 19th Int. Symp. on Comput. Arch., </booktitle> <pages> pp. 313-322, </pages> <year> 1992. </year>
Reference-contexts: In order to remove certain types of dependences two transformations can be applied to the loop: privatization and reduction parallelization. Privatization creates, for each processor cooperating on the execution of the loop, private copies of the program variables that give rise to anti or output dependences (see, e.g., <ref> [7, 18, 19, 31] </ref>).
Reference: [19] <author> D. E. Maydan, S. P. Amarasinghe, and M. S. Lam. </author> <title> Data dependence and data-flow analysis of arrays. </title> <booktitle> In Proc. 5th Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <month> Aug. </month> <year> 1992. </year>
Reference-contexts: In order to remove certain types of dependences two transformations can be applied to the loop: privatization and reduction parallelization. Privatization creates, for each processor cooperating on the execution of the loop, private copies of the program variables that give rise to anti or output dependences (see, e.g., <ref> [7, 18, 19, 31] </ref>).
Reference: [20] <author> S. Midkiff and D. Padua. </author> <title> Compiler algorithms for synchronization. </title> <journal> IEEE Trans. Comput., </journal> <volume> C-36(12):1485-1495, </volume> <year> 1987. </year>
Reference-contexts: Since compile-time data dependence analysis techniques cannot be used on such programs, methods of performing the analysis at run-time are required. Several techniques have been developed for the run-time analysis and scheduling of loops with cross-iteration dependences <ref> [5, 9, 13, 17, 20, 23, 28, 29, 30, 33, 34] </ref>. However, for various reasons, such techniques have not achieved wide-spread use in current parallelizing compilers. In the following we describe a new run-time scheme for constructing a parallel execution schedule for the iterations of a loop. <p> Methods utilizing critical sections. The method of Zhu and Yew [34] computes the wavefronts one after another using a method simi obtains contains requires restricts privat optimal serial global type of or Method sched portions synch loop reduct New Yes No No No P,R MP <ref> [20] </ref> Yes No Yes 2 No No CYT [9] No 1;3 No Yes No No SMC [30] Yes Yes 4 Yes Yes 5 No P [23] No No No No No Table 1: A comparison of run-time parallelization techniques for do loops. <p> Midkiff and Padua <ref> [20] </ref> extended this method to allow concurrent reads from a memory location in multiple iterations. These methods run the risk of a severe degradation in performance for access patterns containing hot spots (i.e., many accesses to the same memory location).
Reference: [21] <author> J. Moreira and C. Polychronopoulos. </author> <title> Autoscheduling in a distributed shared-memory environment . TR. </title> <type> 1373, Ctr. </type> <institution> for Supercomputing R.&D., Univ. of Illinois, Urbana, </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: Hence, the determination of which version to use should be made using knowledge gained about the access pattern by the inspector. In [24], we discuss ways to reduce scheduling overhead such as overlapping wavefront computation with actual loop execution and using dynamic ready queues <ref> [21] </ref>. 4 A Comparison with Previous Methods We now compare the methods described in this paper to several other techniques that have been proposed for analyzing and scheduling do loops at run-time. Most of this work has concentrated on developing inspectors.
Reference: [22] <author> D. Padua and M. Wolfe. </author> <title> Advanced compiler optimizations for supercomputers. </title> <journal> Communications of the ACM, </journal> <volume> 29 </volume> <pages> 1184-1201, </pages> <month> Dec. </month> <year> 1986. </year>
Reference-contexts: Such hand-coding is difficult, error-prone, and often not portable to different machines. Restructuring, or parallelizing, compilers address these problems by detecting and exploiting parallelism in sequential programs written in conventional languages. Although compiler techniques for the automatic detection of parallelism have been studied extensively over the last two decades <ref> [22, 32] </ref>, current parallelizing compilers cannot extract a significant fraction of the available parallelism in a loop if it has a complex and/or statically insufficiently defined access pattern. <p> since none of them has all of these properties (a comparison to previous work is contained in Section 4). 2 Preliminaries In order to guarantee the semantics of a loop, the parallel execution schedule for its iterations must respect the data dependence relations between the statements in the loop body <ref> [22, 15, 3, 32, 35] </ref>. There are three possible types of dependences between two statements that access the same memory location: flow (read after write), anti (write after read), and output (write after write). Flow dependences express a fundamental relationship about the data flow in the program.
Reference: [23] <author> C. Polychronopoulos. </author> <title> Compiler optimizations for enhancing parallelism and their Impact on architecture design. </title> <journal> IEEE Trans. Comput., </journal> <volume> C-37(8):991-1004, </volume> <month> Aug. </month> <year> 1988. </year>
Reference-contexts: Since compile-time data dependence analysis techniques cannot be used on such programs, methods of performing the analysis at run-time are required. Several techniques have been developed for the run-time analysis and scheduling of loops with cross-iteration dependences <ref> [5, 9, 13, 17, 20, 23, 28, 29, 30, 33, 34] </ref>. However, for various reasons, such techniques have not achieved wide-spread use in current parallelizing compilers. In the following we describe a new run-time scheme for constructing a parallel execution schedule for the iterations of a loop. <p> simi obtains contains requires restricts privat optimal serial global type of or Method sched portions synch loop reduct New Yes No No No P,R MP [20] Yes No Yes 2 No No CYT [9] No 1;3 No Yes No No SMC [30] Yes Yes 4 Yes Yes 5 No P <ref> [23] </ref> No No No No No Table 1: A comparison of run-time parallelization techniques for do loops. In the table entries, P and R show that the method identities privatizable and reduction variables, respectively. <p> These methods run the risk of a severe degradation in performance for access patterns containing hot spots (i.e., many accesses to the same memory location). A feature of them is that they use only a shadow version of the shared array whereas all other methods (except <ref> [23, 25, 26] </ref>) unroll the loop and store all accesses to the shared array. Krothapalli and Sadayappan [13] proposed a run-time scheme for removing anti and output dependences from loops. <p> In bootstrapping technique, the inspector is parallelized (not optimally) using sectioning, but an optimal schedule is produced. Other methods. In contrast to the above methods which place iterations in the lowest possible wavefront, Polychronopolous <ref> [23] </ref> gives a method where wavefronts are maximal sets of contiguous iterations with no cross-iteration dependences. Dependences are detected using shadow versions of the variables, either sequentially, or in parallel with the aid of critical sections as in [34].
Reference: [24] <author> L. Rauchwerger, N. Amato and D .Padua. </author> <title> Run-time methods for par-allelizing partially parallel loops. </title> <type> TR. 1400, Ctr. </type> <institution> for Supercomputing R.&D., Univ. of Illinois, Urbana, IL, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: SPICE for circuit simulation, DYNA-3D and PRONTO-3D for structural mechanics modeling, GAUSSIAN and DMOL for quantum mechanical simulation of molecules, CHARMM and DISCOVER for molecular dynamics simulation of organic systems, and FIDAP for modeling complex fluid flows [8]. x Due to space limitations, this paper is an extended abstract of <ref> [24] </ref>. y Center for Supercomputing Research & Development, 1308 W. Main St., Urbana, IL 61801, email: rwerger,padua@csrd.uiuc.edu. Research supported in part by Intel and NASA Graduate Fellowships, and Army contract #DABT63-92-C-0033. <p> Again, the final status of each element is determined in the cross-processor analysis phase, i.e., an element is a reduction variable if and only if it was not invalidated as such by any processor. This basic strategy can be extended to handle more complex reduction operations (refer to <ref> [24] </ref> for details). Complexity of the Inspector. The worst case complexity of the inspector is O (a log p), where a is the maximum number of references assigned to each processor and p is the total number of processors. <p> In summary, we would expect the optimized version to outperform the original scheduler if there are multiple levels in the array element dependence graphs. Hence, the determination of which version to use should be made using knowledge gained about the access pattern by the inspector. In <ref> [24] </ref>, we discuss ways to reduce scheduling overhead such as overlapping wavefront computation with actual loop execution and using dynamic ready queues [21]. 4 A Comparison with Previous Methods We now compare the methods described in this paper to several other techniques that have been proposed for analyzing and scheduling do <p> We remark that there are other issues to be considered when applying these methods in a real application environment such as memory requirements and known bounds on the source loop's available parallelism (refer to <ref> [24] </ref> for more details). Synthetic Loops Using synthetic loops, we studied the sensitivity of the overhead of the methods to two characteristics of the source do loop: its average parallelism (#iterations/cpl) and its hotspot degree (the maximum number of repeated accesses to any array element).
Reference: [25] <author> L. Rauchwerger and D. Padua. </author> <title> The privatizing doall test: A run-time technique for doall loop identification and array privatization. </title> <booktitle> In Proc. of the 1994 Int. Conf. on Supercomputing, </booktitle> <pages> pp. 33-43, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: These methods run the risk of a severe degradation in performance for access patterns containing hot spots (i.e., many accesses to the same memory location). A feature of them is that they use only a shadow version of the shared array whereas all other methods (except <ref> [23, 25, 26] </ref>) unroll the loop and store all accesses to the shared array. Krothapalli and Sadayappan [13] proposed a run-time scheme for removing anti and output dependences from loops. <p> Flow dependences are enforced using full/empty bits. To our knowledge, this is the only other run-time privatization technique except for the one described in <ref> [25, 26] </ref>. <p> Dependences are detected using shadow versions of the variables, either sequentially, or in parallel with the aid of critical sections as in [34]. All of the above mentioned methods attempt to find a valid parallel execution schedule for the source do loop. Recently, we considered a related problem <ref> [25, 26] </ref>: testing at run-time whether the loop is fully parallel, i.e., whether there are any cross-iteration dependences in the loop. <p> These graphs show that the speedup scales with the number of processors and is a significant percentage of the ideal speedup. We note that these loops could also be identified by the LRPD test <ref> [25, 26] </ref>, a run-time test for identifying fully parallel loops, i.e., loops that can be transformed into doalls using privatization and reduction parallelization. Although the LRPD test has a smaller overhead than the methods presented here, it cannot extract partial parallelism. <p> It should be noted that run-time overhead could be significantly reduced through architectural support. We view the methods described in this paper as a building block in an evolving framework of run-time parallelization as a complement to the existing techniques <ref> [25, 26, 27] </ref>. Acknowledgment. We would like to thank Paul Petersen for his useful advice, and William Blume and Brett Marsolf for identifying and clarifying applications for our experiments. We are also grateful to Richard Cole for suggestions regarding sorting algorithms.
Reference: [26] <author> L. Rauchwerger and D. Padua. </author> <title> The LRPD test: Speculative run-time parallelization of loops with privatization and reduction paralleliza-tion. </title> <booktitle> In ACM SIGPLAN Conf. on Programming Language Design and Implementation, </booktitle> <month> June, </month> <year> 1995. </year>
Reference-contexts: These methods run the risk of a severe degradation in performance for access patterns containing hot spots (i.e., many accesses to the same memory location). A feature of them is that they use only a shadow version of the shared array whereas all other methods (except <ref> [23, 25, 26] </ref>) unroll the loop and store all accesses to the shared array. Krothapalli and Sadayappan [13] proposed a run-time scheme for removing anti and output dependences from loops. <p> Flow dependences are enforced using full/empty bits. To our knowledge, this is the only other run-time privatization technique except for the one described in <ref> [25, 26] </ref>. <p> Dependences are detected using shadow versions of the variables, either sequentially, or in parallel with the aid of critical sections as in [34]. All of the above mentioned methods attempt to find a valid parallel execution schedule for the source do loop. Recently, we considered a related problem <ref> [25, 26] </ref>: testing at run-time whether the loop is fully parallel, i.e., whether there are any cross-iteration dependences in the loop. <p> These graphs show that the speedup scales with the number of processors and is a significant percentage of the ideal speedup. We note that these loops could also be identified by the LRPD test <ref> [25, 26] </ref>, a run-time test for identifying fully parallel loops, i.e., loops that can be transformed into doalls using privatization and reduction parallelization. Although the LRPD test has a smaller overhead than the methods presented here, it cannot extract partial parallelism. <p> It should be noted that run-time overhead could be significantly reduced through architectural support. We view the methods described in this paper as a building block in an evolving framework of run-time parallelization as a complement to the existing techniques <ref> [25, 26, 27] </ref>. Acknowledgment. We would like to thank Paul Petersen for his useful advice, and William Blume and Brett Marsolf for identifying and clarifying applications for our experiments. We are also grateful to Richard Cole for suggestions regarding sorting algorithms.
Reference: [27] <author> L. Rauchwerger and D. Padua. </author> <title> Parallelizing while loops for multiprocessor systems. </title> <booktitle> In 9th Int. Parallel Process. Symp., </booktitle> <month> April, </month> <year> 1995. </year>
Reference-contexts: We discuss two input sets: gemat12, which generates 4929 iterations, and bp 1600, which generates 822 iterations. After extracting and precomputing the linear recurrences from the source loop (based on the methods in <ref> [27] </ref>), we generated a parallel inspector and computed an optimal parallel execution schedule for the loop. The parallelism profiles obtained (Figures 11 and 12) show the wavefront sizes of the optimal parallel execution schedule and illustrate how the same loop can generate vastly different dependence graphs given different input. <p> It should be noted that run-time overhead could be significantly reduced through architectural support. We view the methods described in this paper as a building block in an evolving framework of run-time parallelization as a complement to the existing techniques <ref> [25, 26, 27] </ref>. Acknowledgment. We would like to thank Paul Petersen for his useful advice, and William Blume and Brett Marsolf for identifying and clarifying applications for our experiments. We are also grateful to Richard Cole for suggestions regarding sorting algorithms.
Reference: [28] <author> J. Saltz and R. Mirchandaney. </author> <title> The preprocessed doacross loop. In Dr. </title> <editor> H.D. Schwetman, editor, </editor> <booktitle> Proc. of the 1991 Int. Conf. on Parallel Processing, </booktitle> <pages> pp. 174-178. </pages> <publisher> CRC Press, Inc., </publisher> <year> 1991. </year> <title> Vol. </title> <booktitle> II Software. </booktitle>
Reference-contexts: Since compile-time data dependence analysis techniques cannot be used on such programs, methods of performing the analysis at run-time are required. Several techniques have been developed for the run-time analysis and scheduling of loops with cross-iteration dependences <ref> [5, 9, 13, 17, 20, 23, 28, 29, 30, 33, 34] </ref>. However, for various reasons, such techniques have not achieved wide-spread use in current parallelizing compilers. In the following we describe a new run-time scheme for constructing a parallel execution schedule for the iterations of a loop. <p> Their scheduler/executor uses doacross parallelization <ref> [28] </ref> (see below). Although this scheme potentially has less communication overhead than [34], it is still sensitive to hot spots and there are cases (e.g., doalls) in which it proves inferior to [34]. Methods for loops without output dependences. <p> Although this scheme potentially has less communication overhead than [34], it is still sensitive to hot spots and there are cases (e.g., doalls) in which it proves inferior to [34]. Methods for loops without output dependences. This problem has also been studied extensively by Saltz et al. <ref> [5, 28, 29, 30, 33] </ref>. Most of their work assumes that there are no output dependences in the source loop. In doacross parallelization [28], an inspector finds the (at most one) iteration in which each variable is written. <p> Methods for loops without output dependences. This problem has also been studied extensively by Saltz et al. [5, 28, 29, 30, 33]. Most of their work assumes that there are no output dependences in the source loop. In doacross parallelization <ref> [28] </ref>, an inspector finds the (at most one) iteration in which each variable is written. The scheduler/executor starts iterations in a wrapped manner and processors busy wait until their operands are available.
Reference: [29] <author> J. Saltz, R. Mirchandaney, and K. Crowley. </author> <title> The doconsider loop. </title> <booktitle> In Proc. of the 1989 Int. Conf. on Supercomputing, </booktitle> <pages> pp. 29-40, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: Since compile-time data dependence analysis techniques cannot be used on such programs, methods of performing the analysis at run-time are required. Several techniques have been developed for the run-time analysis and scheduling of loops with cross-iteration dependences <ref> [5, 9, 13, 17, 20, 23, 28, 29, 30, 33, 34] </ref>. However, for various reasons, such techniques have not achieved wide-spread use in current parallelizing compilers. In the following we describe a new run-time scheme for constructing a parallel execution schedule for the iterations of a loop. <p> Although this scheme potentially has less communication overhead than [34], it is still sensitive to hot spots and there are cases (e.g., doalls) in which it proves inferior to [34]. Methods for loops without output dependences. This problem has also been studied extensively by Saltz et al. <ref> [5, 28, 29, 30, 33] </ref>. Most of their work assumes that there are no output dependences in the source loop. In doacross parallelization [28], an inspector finds the (at most one) iteration in which each variable is written.
Reference: [30] <author> J. Saltz, R. Mirchandaney, and K. Crowley. </author> <title> Run-time parallelization and scheduling of loops. </title> <journal> IEEE Trans. Comput., </journal> <volume> 40(5), </volume> <month> May </month> <year> 1991. </year>
Reference-contexts: Given the original, or source loop, most of these techniques generate inspector code that analyzes, at run-time, the cross-iteration dependences in the loop, and scheduler/executor code that schedules and executes the loop iterations using the dependence information extracted by the inspector <ref> [30] </ref>. Our Results. We give a new inspector/scheduler/executor method for finding an optimal parallel execution schedule for a partially parallel loop. Our inspector is fully parallel, uses no synchronization, and can be applied to any loop (from which an inspector can be extracted). <p> Since compile-time data dependence analysis techniques cannot be used on such programs, methods of performing the analysis at run-time are required. Several techniques have been developed for the run-time analysis and scheduling of loops with cross-iteration dependences <ref> [5, 9, 13, 17, 20, 23, 28, 29, 30, 33, 34] </ref>. However, for various reasons, such techniques have not achieved wide-spread use in current parallelizing compilers. In the following we describe a new run-time scheme for constructing a parallel execution schedule for the iterations of a loop. <p> In the previous techniques, the scheduler and the executor are tightly coupled codes which are collectively referred to as the executor, and the inspector and the scheduler/executor codes are usually decoupled <ref> [30] </ref>. <p> computes the wavefronts one after another using a method simi obtains contains requires restricts privat optimal serial global type of or Method sched portions synch loop reduct New Yes No No No P,R MP [20] Yes No Yes 2 No No CYT [9] No 1;3 No Yes No No SMC <ref> [30] </ref> Yes Yes 4 Yes Yes 5 No P [23] No No No No No Table 1: A comparison of run-time parallelization techniques for do loops. In the table entries, P and R show that the method identities privatizable and reduction variables, respectively. <p> Although this scheme potentially has less communication overhead than [34], it is still sensitive to hot spots and there are cases (e.g., doalls) in which it proves inferior to [34]. Methods for loops without output dependences. This problem has also been studied extensively by Saltz et al. <ref> [5, 28, 29, 30, 33] </ref>. Most of their work assumes that there are no output dependences in the source loop. In doacross parallelization [28], an inspector finds the (at most one) iteration in which each variable is written. <p> In doacross parallelization [28], an inspector finds the (at most one) iteration in which each variable is written. The scheduler/executor starts iterations in a wrapped manner and processors busy wait until their operands are available. In <ref> [30] </ref>, the inspector constructs wavefronts that respect the flow dependencesby performing a sequential topological sort of the accesses in the loop, and the scheduler/executor enforces any anti dependences using old and new versions of each variable (possible since each variable in the source loop is written at most once). <p> The topological sort can be parallelized somewhat using doacross parallelization. Leung and Zahorjan [17] proposed methods of parallelizing the sequential inspector of <ref> [30] </ref>. In theit sectioning method, the loop is chunked and each processor computes an optimal schedule for its chunk, and then these schedules are concatenated together separated by synchronization barriers. In bootstrapping technique, the inspector is parallelized (not optimally) using sectioning, but an optimal schedule is produced. Other methods. <p> In OCEAN-FTRVMT-Loop 109, all accesses to the shared array are found to be unique in the analysis phase. Since this loop is invoked 26,000 times, and accounts for 40% of the sequential execution time of the program, it is an excellent candidate for schedule reuse <ref> [30] </ref>. The access pattern for each instantiation of the loop is determined by a set of five scalars. In order to apply schedule reuse, we checked whether the current set of scalars matched a previously analyzed set.
Reference: [31] <author> P. Tu and D. Padua. </author> <title> Automatic array privatization. </title> <booktitle> In Proc. 6th Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> Aug. </month> <year> 1993. </year>
Reference-contexts: In order to remove certain types of dependences two transformations can be applied to the loop: privatization and reduction parallelization. Privatization creates, for each processor cooperating on the execution of the loop, private copies of the program variables that give rise to anti or output dependences (see, e.g., <ref> [7, 18, 19, 31] </ref>).
Reference: [32] <author> M. Wolfe. </author> <title> Optimizing Compilers for Supercomputers. </title> <publisher> The MIT Press, </publisher> <address> Boston, MA, </address> <year> 1989. </year>
Reference-contexts: Such hand-coding is difficult, error-prone, and often not portable to different machines. Restructuring, or parallelizing, compilers address these problems by detecting and exploiting parallelism in sequential programs written in conventional languages. Although compiler techniques for the automatic detection of parallelism have been studied extensively over the last two decades <ref> [22, 32] </ref>, current parallelizing compilers cannot extract a significant fraction of the available parallelism in a loop if it has a complex and/or statically insufficiently defined access pattern. <p> since none of them has all of these properties (a comparison to previous work is contained in Section 4). 2 Preliminaries In order to guarantee the semantics of a loop, the parallel execution schedule for its iterations must respect the data dependence relations between the statements in the loop body <ref> [22, 15, 3, 32, 35] </ref>. There are three possible types of dependences between two statements that access the same memory location: flow (read after write), anti (write after read), and output (write after write). Flow dependences express a fundamental relationship about the data flow in the program.
Reference: [33] <author> J. Wu, J. Saltz, S. Hiranandani, and H. Berryman. </author> <title> Runtime compilation methods for multicomputers. In Dr. </title> <editor> H.D. Schwetman, editor, </editor> <booktitle> Proc. of the 1991 Int. Conf. on Parallel Processing, </booktitle> <pages> pp. 26-30. </pages> <publisher> CRC Press, Inc., </publisher> <year> 1991. </year> <title> Vol. </title> <booktitle> II Software. </booktitle>
Reference-contexts: Since compile-time data dependence analysis techniques cannot be used on such programs, methods of performing the analysis at run-time are required. Several techniques have been developed for the run-time analysis and scheduling of loops with cross-iteration dependences <ref> [5, 9, 13, 17, 20, 23, 28, 29, 30, 33, 34] </ref>. However, for various reasons, such techniques have not achieved wide-spread use in current parallelizing compilers. In the following we describe a new run-time scheme for constructing a parallel execution schedule for the iterations of a loop. <p> Although this scheme potentially has less communication overhead than [34], it is still sensitive to hot spots and there are cases (e.g., doalls) in which it proves inferior to [34]. Methods for loops without output dependences. This problem has also been studied extensively by Saltz et al. <ref> [5, 28, 29, 30, 33] </ref>. Most of their work assumes that there are no output dependences in the source loop. In doacross parallelization [28], an inspector finds the (at most one) iteration in which each variable is written.
Reference: [34] <author> C. Zhu and P. C. Yew. </author> <title> A scheme to enforce data dependence on large multiprocessor systems. </title> <journal> IEEE Trans. Softw. Eng., </journal> <volume> 13(6) </volume> <pages> 726-739, </pages> <year> 1987. </year>
Reference-contexts: Since compile-time data dependence analysis techniques cannot be used on such programs, methods of performing the analysis at run-time are required. Several techniques have been developed for the run-time analysis and scheduling of loops with cross-iteration dependences <ref> [5, 9, 13, 17, 20, 23, 28, 29, 30, 33, 34] </ref>. However, for various reasons, such techniques have not achieved wide-spread use in current parallelizing compilers. In the following we describe a new run-time scheme for constructing a parallel execution schedule for the iterations of a loop. <p> Most of this work has concentrated on developing inspectors. A high level comparison of the various methods is given in Table 1. Methods utilizing critical sections. The method of Zhu and Yew <ref> [34] </ref> computes the wavefronts one after another using a method simi obtains contains requires restricts privat optimal serial global type of or Method sched portions synch loop reduct New Yes No No No P,R MP [20] Yes No Yes 2 No No CYT [9] No 1;3 No Yes No No SMC <p> Krothapalli and Sadayappan [13] proposed a run-time scheme for removing anti and output dependences from loops. For each memory location, their inspector counts the number references to it (using critical sections as in <ref> [34] </ref>), places them in a dynamically allocated array, and then sorts them by iteration number. <p> Chen, Yew, and Torrellas [9] proposed an inspector that first builds (in private storage) access lists for each memory location referenced in a processor's assigned iterations (similar to [13] and our inspector's marking phase, except they serialize read accesses), and then links them across processors using a global Zhu/Yew algorithm <ref> [34] </ref>. Their scheduler/executor uses doacross parallelization [28] (see below). Although this scheme potentially has less communication overhead than [34], it is still sensitive to hot spots and there are cases (e.g., doalls) in which it proves inferior to [34]. Methods for loops without output dependences. <p> memory location referenced in a processor's assigned iterations (similar to [13] and our inspector's marking phase, except they serialize read accesses), and then links them across processors using a global Zhu/Yew algorithm <ref> [34] </ref>. Their scheduler/executor uses doacross parallelization [28] (see below). Although this scheme potentially has less communication overhead than [34], it is still sensitive to hot spots and there are cases (e.g., doalls) in which it proves inferior to [34]. Methods for loops without output dependences. This problem has also been studied extensively by Saltz et al. [5, 28, 29, 30, 33]. <p> accesses), and then links them across processors using a global Zhu/Yew algorithm <ref> [34] </ref>. Their scheduler/executor uses doacross parallelization [28] (see below). Although this scheme potentially has less communication overhead than [34], it is still sensitive to hot spots and there are cases (e.g., doalls) in which it proves inferior to [34]. Methods for loops without output dependences. This problem has also been studied extensively by Saltz et al. [5, 28, 29, 30, 33]. Most of their work assumes that there are no output dependences in the source loop. <p> Dependences are detected using shadow versions of the variables, either sequentially, or in parallel with the aid of critical sections as in <ref> [34] </ref>. All of the above mentioned methods attempt to find a valid parallel execution schedule for the source do loop. Recently, we considered a related problem [25, 26]: testing at run-time whether the loop is fully parallel, i.e., whether there are any cross-iteration dependences in the loop.
Reference: [35] <author> H. Zima. </author> <title> Supercompilers for Parallel and Vector Computers. </title> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1991. </year>
Reference-contexts: since none of them has all of these properties (a comparison to previous work is contained in Section 4). 2 Preliminaries In order to guarantee the semantics of a loop, the parallel execution schedule for its iterations must respect the data dependence relations between the statements in the loop body <ref> [22, 15, 3, 32, 35] </ref>. There are three possible types of dependences between two statements that access the same memory location: flow (read after write), anti (write after read), and output (write after write). Flow dependences express a fundamental relationship about the data flow in the program. <p> Once reduction variables are identified, methods are known for performing the reduction operation in parallel (see, e.g., <ref> [11, 14, 16, 35] </ref>). 3 Run-Time Analysis of Loops Given a do loop whose access pattern cannot be statically analyzed, compilers have traditionally generated sequential code. Since compile-time data dependence analysis techniques cannot be used on such programs, methods of performing the analysis at run-time are required.
References-found: 35

