URL: http://www.research.att.com/~mkearns/papers/klv.ps.Z
Refering-URL: http://www.research.att.com/~mkearns/
Root-URL: 
Title: Learning Boolean Formulae  
Author: Michael Kearns Ming Li Leslie Valiant 
Keyword: Key words: Machine Learning, Inductive Inference.  
Address: Waterloo, Waterloo, Ontario N2L 3G1 Canada.  Cambridge, MA 02138.  
Note: all positive or all negative, must be correctly classified with high probability.  This research was done while the author was at Harvard University and supported by grants NSF-DCR-8600379, DAAL03-86-K-0171, and ONR-N00014-85-K-0445. Author's current address:  Supported by grants NSF-DCR-8600379 and ONR-N00014-85-K-0445. Author's address:  
Affiliation: AT&T Bell Laboratories  University of Waterloo  Harvard University  Department of Computer Science, University of  Aiken Computation Laboratory, Harvard University,  
Abstract: Efficient distribution-free learning of Boolean formulae from positive and negative examples is considered. It is shown that classes of formulae that are efficiently learnable from only positive examples or only negative examples have certain closure properties. A new substitution technique is used to show that in the distribution-free case learning DNF (disjunctive normal form formulae) is no harder than learning monotone DNF. We prove that monomials cannot be efficiently learned from negative examples alone, even if the negative examples are uniformly distributed. It is also shown that if the examples are drawn from uniform distributions then the class of DNF in which each variable occurs at most once is efficiently learnable, while the class of all monotone Boolean functions is efficiently weakly learnable (i.e., individual examples are correctly classified with a probability larger than 1 2 + 1 p , where p is a polynomial in the relevant parameters of the learning problem). We then show an equivalence between the notion of weak fl Earlier versions of most of the results presented here were described in "On the learnability of Boolean formulae", by M. Kearns, M. Li, L. Pitt and L. Valiant, Proceedings of the 19th A.C.M. Symposium on the Theory of Computing, 1987, pp. 285-295. Earlier versions of Theorems 15 and 22 were announced in "Cryptographic limitations on learning Boolean formulae and finite automata", by M. Kearns and L. Valiant, Proceedings of the 21st A.C.M. Symposium on the Theory of Computing, 1989, pp. 433-444. y This research was done while the author was at Harvard University and supported by grants NSF-DCR-8600379, ONR-N00014-85-K-0445, and an A.T. & T. Bell Laboratories Scholarship. Author's current address: AT&T Bell Laboratories, Room 2A-423, 600 Mountain Avenue, P.O. Box 636, Murray Hill, NJ 07974-0636. learning and the notion of group learning, where a group of examples of polynomial size, either
Abstract-found: 1
Intro-found: 1
Reference: [A86] <author> D. Aldous. </author> <title> On the Markov chain simulation method for uniform combinatorial distributions and simulated annealing. </title> <institution> University of California at Berkeley Statistics Department, </institution> <type> technical report number 60, </type> <year> 1986. </year>
Reference-contexts: For 1 i n let e (i) be the vector with the ith bit set to 1 and all other bits set to 0. The following lemma is due to Aldous <ref> [A86] </ref>. Lemma 14 [A86] Let T f0; 1g n be such that jT j 2 n 2 . <p> For 1 i n let e (i) be the vector with the ith bit set to 1 and all other bits set to 0. The following lemma is due to Aldous <ref> [A86] </ref>. Lemma 14 [A86] Let T f0; 1g n be such that jT j 2 n 2 . Then for some 1 i n, jT e (i) T j 2n Theorem 15 The class of all monotone Boolean functions is polynomially weakly learnable under uniform D + and uniform D .
Reference: [AV79] <author> D. Angluin, L.G. Valiant. </author> <title> Fast probabilistic algorithms for Hamiltonian circuits and matchings. </title> <journal> Journal of Computer and Systems Sciences, </journal> <volume> 18, </volume> <year> 1979, </year> <pages> pp. 155-193. </pages>
Reference-contexts: Then for 0 ff 1, Fact CB1. LE (p; m; (1 ff)mp) e ff 2 mp=2 and Fact CB2. GE (p; m; (1 + ff)mp) e ff 2 mp=3 These bounds in the form they are stated are from <ref> [AV79] </ref>; see also [C52]. Although we will make frequent use of Fact CB1 and Fact CB2, we will do so in varying levels of detail, depending on the complexity of the calculation involved.
Reference: [BI88] <author> G.M. Benedek, A. Itai. </author> <title> Learnability by fixed distributions. </title> <booktitle> Proceedings of the 1988 Workshop on Computational Learning Theory, </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1988, </year> <pages> pp. 80-90. </pages>
Reference-contexts: In this section we describe results in the latter direction. Distribution-specific learning is also considered in <ref> [BI88, N87] </ref>. We describe polynomial-time algorithms for learning under uniform distributions representa tion classes for which the learning problem under arbitrary distributions is either intractable or unresolved.
Reference: [BEHW86] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, M. Warmuth. </author> <title> Classifying learnable geometric concepts with the Vapnik-Chervonenkis dimension. </title> <booktitle> Proceedings of the 18th A.C.M. Symposium on the Theory of Computing, </booktitle> <year> 1986, </year> <pages> pp. 273-282. </pages>
Reference-contexts: The elucidation of this boundary, for Boolean expressions and other knowledge representations, is an example of the potential contribution of complexity theory to artificial intelligence. We employ the distribution-free model of learning introduced in [V84]. A more complete discussion and justification of this model can be found in <ref> [V85, BEHW86, KLPV87b] </ref>. The paper [BEHW86] includes some discussion that is relevant more particularly to infinite representations, such as geometric ones, rather than the finite case of Boolean functions. <p> We employ the distribution-free model of learning introduced in [V84]. A more complete discussion and justification of this model can be found in [V85, BEHW86, KLPV87b]. The paper <ref> [BEHW86] </ref> includes some discussion that is relevant more particularly to infinite representations, such as geometric ones, rather than the finite case of Boolean functions. <p> ~w)), where the infinite bit sequence ~r is drawn uniformly at random and the infinite example sequence ~w is drawn randomly according to the target distributions D + and D . 20 The basic format of the proof of the lower bound of Theorem 12 (as well as those of <ref> [BEHW86] </ref> and [EHKV88]) is to give specific distributions such that a random sample of size at most B has probability at least p of causing any learning algorithm to fail to output an *-good hypothesis. <p> It is shown in <ref> [BEHW86] </ref> (see also [EHKV88]) that the number of examples needed for learning (and therefore the computation time required) is bounded below by the Vapnik-Chervonenkis dimension of the target class; furthermore, this lower bound is proved using the uniform distribution over a shattered set and holds even for the weak learning model
Reference: [C52] <author> H. Chernoff. </author> <title> A measure of asymptotic efficiency for tests of a hypothesis based on the sum of observations. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 23, </volume> <year> 1952, </year> <pages> pp. 493-509. </pages>
Reference-contexts: Then for 0 ff 1, Fact CB1. LE (p; m; (1 ff)mp) e ff 2 mp=2 and Fact CB2. GE (p; m; (1 + ff)mp) e ff 2 mp=3 These bounds in the form they are stated are from [AV79]; see also <ref> [C52] </ref>. Although we will make frequent use of Fact CB1 and Fact CB2, we will do so in varying levels of detail, depending on the complexity of the calculation involved.
Reference: [EHKV88] <author> A. Ehrenfeucht, D. Haussler, M. Kearns. L.G. Valiant. </author> <title> A general lower bound on the number of examples needed for learning. </title> <booktitle> 33 Proceedings of the 1988 Workshop on Computational Learning Theory, </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1988, </year> <pages> pp. 139-154. </pages>
Reference-contexts: the infinite bit sequence ~r is drawn uniformly at random and the infinite example sequence ~w is drawn randomly according to the target distributions D + and D . 20 The basic format of the proof of the lower bound of Theorem 12 (as well as those of [BEHW86] and <ref> [EHKV88] </ref>) is to give specific distributions such that a random sample of size at most B has probability at least p of causing any learning algorithm to fail to output an *-good hypothesis. <p> It is shown in [BEHW86] (see also <ref> [EHKV88] </ref>) that the number of examples needed for learning (and therefore the computation time required) is bounded below by the Vapnik-Chervonenkis dimension of the target class; furthermore, this lower bound is proved using the uniform distribution over a shattered set and holds even for the weak learning model in the case
Reference: [GJ79] <author> M. Garey, D. Johnson. </author> <title> Computers and intractability: a guide to the theory of NP-completeness. </title> <publisher> Freeman, </publisher> <year> 1979. </year>
Reference-contexts: We assume that domain points x 2 X and representations c 2 C are efficiently encoded using any of the standard schemes 3 (see e.g. <ref> [GJ79] </ref>), and denote by jxj and jcj the length of these encodings measured in bits. Parameterized representation classes. We will often study parameterized classes of representations. Here we have a stratified domain X = [ n1 X n and representation class C = [ n1 C n .
Reference: [G89] <author> M. Gereb-Graus. </author> <title> Complexity of learning from one-sided examples. </title> <institution> Harvard University, </institution> <type> unpublished manuscript, </type> <year> 1989. </year>
Reference-contexts: In this section we prove a superpolynomial lower bound on the number of examples required for learning monomials from negative examples. The proof can actually be tightened to give a strictly exponential lower bound, and the proof technique has been generalized in <ref> [G89] </ref>. A necessary condition for (general) learning from positive examples is given in [S90]. Our bound is information-theoretic in the sense that it holds regardless of the computational complexity and hypothesis class of the negative-only learning algorithm and is independent of any complexity-theoretic assumptions.
Reference: [H88] <author> D. Haussler. </author> <title> Quantifying inductive bias: AI learning algorithms and Valiant's model. </title> <journal> Artificial Intelligence, </journal> <volume> 36(2), </volume> <year> 1988, </year> <pages> pp. 177-221. </pages>
Reference-contexts: This contrasts with the fact that monomials can be learned from positive examples alone [V84], and can be learned from very few examples if both kinds are available <ref> [H88, L88] </ref>. The class of DNF expressions in which each variable occurs at most once is called DNF. In Section 5, we consider learning DNF and the class of arbitrary monotone Boolean functions, under the restriction that both the positive and negative examples are drawn from uniform distributions.
Reference: [HKLW88] <author> D. Haussler, M. Kearns, N. Littlestone, M. Warmuth. </author> <title> Equivalence of models for polynomial learnability. </title> <booktitle> Proceedings of the 1988 Workshop on Computational Learning Theory, </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1988, </year> <pages> pp. 42-55, </pages> <institution> and University of California at Santa Cruz Information Sciences Department, </institution> <type> technical report number UCSC-CRL-88-06, </type> <year> 1988. </year>
Reference-contexts: These results also demonstrate the robustness of our underlying model of learnability, since it is invariant under these apparently significant but reasonable modifications. Related equivalences are given in <ref> [HKLW88] </ref>. Our equivalence proof also holds in both directions under fixed target distributions: thus, C is polynomially group learnable under a restricted class of distributions if and only if C is polynomially weakly learnable under these same distributions.
Reference: [HSW88] <author> D. Helmbold, R. Sloan, M. Warmuth. </author> <title> Bootstrapping one-sided learning. </title> <type> Unpublished manuscript, </type> <year> 1988. </year>
Reference-contexts: We apply the results to obtain polynomial-time learning algorithms for two classes of Boolean formulae not previously known to be polynomially learnable. Recently in <ref> [HSW88] </ref> a general composition technique has been proposed and carefully analyzed in several models of learnability.
Reference: [KLPV87a] <author> M. Kearns, M. Li, L. Pitt, L.G. Valiant. </author> <title> On the learnability of Boolean formulae. </title> <booktitle> Proceedings of the 19th A.C.M. Symposium on the Theory of Computing, </booktitle> <year> 1987, </year> <pages> pp. 285-295. </pages>
Reference: [KLPV87b] <author> M. Kearns, M. Li, L. Pitt, L.G. Valiant. </author> <title> Recent results on Boolean concept learning. </title> <booktitle> Proceedings of the 4th International Workshop on Machine Learning, </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1987, </year> <pages> pp. 337-352. </pages>
Reference-contexts: The elucidation of this boundary, for Boolean expressions and other knowledge representations, is an example of the potential contribution of complexity theory to artificial intelligence. We employ the distribution-free model of learning introduced in [V84]. A more complete discussion and justification of this model can be found in <ref> [V85, BEHW86, KLPV87b] </ref>. The paper [BEHW86] includes some discussion that is relevant more particularly to infinite representations, such as geometric ones, rather than the finite case of Boolean functions.
Reference: [KV89] <author> M. Kearns, L.G. Valiant. </author> <title> Cryptographic limitations on learning Boolean formulae and finite automata. </title> <booktitle> 34 Proceedings of the 21st A.C.M. Symposium on the Theory of Computing, </booktitle> <year> 1989, </year> <pages> pp. 433-444. </pages>
Reference-contexts: of any negative-only learning algorithm for monomials. 5 Distribution-specific Learning in Polynomial Time It has been shown elsewhere that for several natural representation classes the learning problem is computationally intractable (modulo various complexity-theoretic or cryptographic assumptions), in some cases even if we allow arbitrary polynomially evaluatable hypothesis representations (see e.g. <ref> [KV89, PV88, PW88] </ref> for hardness results in the distribution-free model). In other cases, most notably the class of unrestricted DNF formulae, researchers have been unable to provide firm evidence for either the polynomial-time learnability or the intractability of learning.
Reference: [L88] <author> N. Littlestone. </author> <title> Learning quickly when irrelevant attributes abound: a new linear threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2(4), </volume> <year> 1988, </year> <pages> pp. 245-318. </pages>
Reference-contexts: This contrasts with the fact that monomials can be learned from positive examples alone [V84], and can be learned from very few examples if both kinds are available <ref> [H88, L88] </ref>. The class of DNF expressions in which each variable occurs at most once is called DNF. In Section 5, we consider learning DNF and the class of arbitrary monotone Boolean functions, under the restriction that both the positive and negative examples are drawn from uniform distributions. <p> This suggests a notion of reducibility between learning problems. In this section we describe polynomial-time reductions between learning problems for classes of Boolean formulae. These reductions are very general and involve simple variable substitutions. Similar transformations have been given for the mistake-bounded model of learning in <ref> [L88] </ref>. Recently the notion of reducibility among learning problems has been elegantly generalized and developed into a complexity theory for polynomial-time learnability [PW88].
Reference: [N87] <author> B.K. Natarajan. </author> <title> On learning Boolean functions. </title> <booktitle> Proceedings of the 19th A.C.M. Symposium on the Theory of Computing, </booktitle> <year> 1987, </year> <pages> pp. 296-304. </pages>
Reference-contexts: In this section we describe results in the latter direction. Distribution-specific learning is also considered in <ref> [BI88, N87] </ref>. We describe polynomial-time algorithms for learning under uniform distributions representa tion classes for which the learning problem under arbitrary distributions is either intractable or unresolved.
Reference: [PV88] <author> L. Pitt, L.G. Valiant. </author> <title> Computational limitations on learning from examples. </title> <journal> Journal of the A.C.M., </journal> <volume> 35(4), </volume> <year> 1988, </year> <pages> pp. 965-984. </pages>
Reference-contexts: We show that DNF is learnable (with DNF as the hypothesis representation) in this distribution-specific case. In the distribution-free setting, learning DNF with DNF as the hypothesis representation is known to be NP-hard <ref> [PV88] </ref>. We also show that arbitrary monotone Boolean functions are weakly learnable under uniform distributions, with a polynomial-time program as the hypothesis representation. <p> All NP-hardness results follow from the results of <ref> [PV88] </ref>. <p> of any negative-only learning algorithm for monomials. 5 Distribution-specific Learning in Polynomial Time It has been shown elsewhere that for several natural representation classes the learning problem is computationally intractable (modulo various complexity-theoretic or cryptographic assumptions), in some cases even if we allow arbitrary polynomially evaluatable hypothesis representations (see e.g. <ref> [KV89, PV88, PW88] </ref> for hardness results in the distribution-free model). In other cases, most notably the class of unrestricted DNF formulae, researchers have been unable to provide firm evidence for either the polynomial-time learnability or the intractability of learning. <p> This completes the proof of Theorem 16. 28 The results of <ref> [PV88] </ref> show that k-term DNF is not learnable by k-term DNF unless NP = RP . However, the algorithm of Theorem 16 outputs an hypothesis with at most the same number of terms as the target formula.
Reference: [PW88] <author> L. Pitt, </author> <title> M.K. Warmuth. Reductions among prediction problems: on the difficulty of predicting automata. </title> <booktitle> Proceedings of the 3rd I.E.E.E. Conference on Structure in Complexity Theory, </booktitle> <year> 1988, </year> <pages> pp. 60-69. </pages>
Reference-contexts: These reductions are very general and involve simple variable substitutions. Similar transformations have been given for the mistake-bounded model of learning in [L88]. Recently the notion of reducibility among learning problems has been elegantly generalized and developed into a complexity theory for polynomial-time learnability <ref> [PW88] </ref>. <p> of any negative-only learning algorithm for monomials. 5 Distribution-specific Learning in Polynomial Time It has been shown elsewhere that for several natural representation classes the learning problem is computationally intractable (modulo various complexity-theoretic or cryptographic assumptions), in some cases even if we allow arbitrary polynomially evaluatable hypothesis representations (see e.g. <ref> [KV89, PV88, PW88] </ref> for hardness results in the distribution-free model). In other cases, most notably the class of unrestricted DNF formulae, researchers have been unable to provide firm evidence for either the polynomial-time learnability or the intractability of learning.
Reference: [R87] <author> R. Rivest. </author> <title> Learning decision lists. </title> <journal> Machine Learning, </journal> <volume> 2(3), </volume> <year> 1987, </year> <pages> pp. 229-246. </pages>
Reference-contexts: Following Theorem 12 of Section 4 we show that the representation classes kCNF _ kDNF and kCNF ^ kDNF require both positive and negative examples for polynomial learnability, regardless of the hypothesis class. We note that the more recent results of Rivest <ref> [R87] </ref> imply that the above classes are learnable by the class of k-decision lists, a class that properly includes them. <p> Further, Theorem 12 implies that the polynomially learnable classes kCNF _ kDNF and kCNF ^ kDNF of Corollaries 3 and 4 require both positive and negative examples for polynomial learnability. The same is true for the class of decision lists studied by Rivest <ref> [R87] </ref>. We also note that it is possible to obtain similar but weaker results with simpler proofs.
Reference: [S89] <author> R. Schapire. </author> <title> The strength of weak learnability. </title> <journal> Machine Learning, </journal> <volume> 5(2), </volume> <year> 1990, </year> <pages> pp. 197-227. </pages>
Reference-contexts: While the motivation for weak learning may not be as apparent as that for strong learning, we may intuitively think of weak learning as the ability to detect some slight bias separating positive and negative examples. In fact, Schapire <ref> [S89] </ref> has shown that in the distribution-free setting, polynomial-time weak learning is equivalent to polynomial-time learning. Positive-only and negative-only learning algorithms. We will sometimes study learning algorithms that need only positive examples or only negative examples. <p> The question we wish to address here is whether learning becomes easier in some cases if the group size is allowed to be larger. Recently it has been shown by Schapire <ref> [S89] </ref> that in the distribution-free setting, polynomial-time weak learning is in fact equivalent to polynomial-time strong learning. His proof gives a recursive technique for taking an algorithm outputting hypotheses with accuracy slightly above 1 2 and constructing hypotheses of accuracy 1 *.
Reference: [S90] <author> H. Shvayster. </author> <title> A necessary condition for learning from positive examples. </title> <journal> Machine Learning, </journal> <volume> 5(1), </volume> <year> 1990, </year> <pages> pp. 101-113. </pages>
Reference-contexts: The proof can actually be tightened to give a strictly exponential lower bound, and the proof technique has been generalized in [G89]. A necessary condition for (general) learning from positive examples is given in <ref> [S90] </ref>. Our bound is information-theoretic in the sense that it holds regardless of the computational complexity and hypothesis class of the negative-only learning algorithm and is independent of any complexity-theoretic assumptions.
Reference: [V84] <author> L.G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the A.C.M., </journal> <volume> 27(11), </volume> <year> 1984, </year> <pages> pp. 1134-1142. </pages>
Reference-contexts: The elucidation of this boundary, for Boolean expressions and other knowledge representations, is an example of the potential contribution of complexity theory to artificial intelligence. We employ the distribution-free model of learning introduced in <ref> [V84] </ref>. A more complete discussion and justification of this model can be found in [V85, BEHW86, KLPV87b]. The paper [BEHW86] includes some discussion that is relevant more particularly to infinite representations, such as geometric ones, rather than the finite case of Boolean functions. <p> We show that for purely information-theoretic reasons, monomials cannot be learned from negative examples alone, regardless of the hypothesis representation, and even if the distribution on the negative examples is uniform. This contrasts with the fact that monomials can be learned from positive examples alone <ref> [V84] </ref>, and can be learned from very few examples if both kinds are available [H88, L88]. The class of DNF expressions in which each variable occurs at most once is called DNF. <p> In the distribution-free case, monotone functions are not weakly learnable by any hypothesis representation, independent of any complexity-theoretic assumptions. 2 Definitions and Notation In this section we give definitions for the model of machine learning we study. This model was first defined in <ref> [V84] </ref>. 2.1 Representing subsets of a domain Concept classes and their representation. Let X be a set called a domain. We may think of X as containing encodings of all objects of interest to us in our learning problem. <p> Corollary 4 For any fixed k, let kCNF ^ kDNF = [ n1 (kCNF n ^ kDNF n ). Then kCNF ^ kDNF is polynomially learnable by kCNF ^ kDNF . Proofs of Corollaries 3 and 4 follow from Theorems 1 and 2 and the algorithms in <ref> [V84] </ref> for learning kCNF from positive examples and kDNF from negative examples. Note that algorithms obtained in Corollaries 3 and 4 use both positive and negative examples. <p> An immediate consequence of Theorem 12 is that monomials are not polynomially learnable from negative examples (regardless of the hypothesis class). This is in contrast to the fact that monomials are polynomially learnable (by monomials) from positive examples <ref> [V84] </ref>. It also follows that any class that contains the class of monotone monomials (e.g., kCNF ) is not polynomially learnable from negative examples.
Reference: [V85] <author> L.G. Valiant. </author> <title> Learning disjunctions of conjunctions. </title> <booktitle> Proceedings of the 9th International Joint Conference on Artificial Intelligence, </booktitle> <year> 1985, </year> <pages> pp. 560-566. </pages>
Reference-contexts: The elucidation of this boundary, for Boolean expressions and other knowledge representations, is an example of the potential contribution of complexity theory to artificial intelligence. We employ the distribution-free model of learning introduced in [V84]. A more complete discussion and justification of this model can be found in <ref> [V85, BEHW86, KLPV87b] </ref>. The paper [BEHW86] includes some discussion that is relevant more particularly to infinite representations, such as geometric ones, rather than the finite case of Boolean functions.
References-found: 23

