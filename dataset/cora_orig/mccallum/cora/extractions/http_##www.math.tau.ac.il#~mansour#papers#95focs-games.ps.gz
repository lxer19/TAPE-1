URL: http://www.math.tau.ac.il/~mansour/papers/95focs-games.ps.gz
Refering-URL: 
Root-URL: 
Title: Efficient Algorithms for Learning to Play Repeated Games Against Computationally Bounded Adversaries  
Author: Yoav Freund Michael Kearns Yishay Mansour Dana Ron Ronitt Rubinfeld Robert E. Schapire 
Date: April 1995  
Affiliation: AT&T Bell Laboratories  AT&T Bell Laboratories  Tel-Aviv University  Hebrew University  Cornell University  AT&T Bell Laboratories  
Abstract: We study the problem of efficiently learning to play a game optimally against an unknown adversary chosen from a computationally bounded class. We both contribute to the line of research on playing games against finite automata, and expand the scope of this research by considering new classes of adversaries. We introduce the natural notions of games against recent history adversaries (whose current action is determined by some simple boolean formula on the recent history of play), and games against statistical adversaries (whose current action is determined by some simple function of the statistics of the entire history of play). In both cases we give efficient algorithms for learning to play penny-matching and a more difficult game called contract . We also give the most powerful positive result to date for learning to play against finite automata, an efficient algorithm for learning to play any game against any finite automata with probabilistic actions and low cover time. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Dana Angluin. </author> <title> Learning regular sets from queries and counterexamples. </title> <journal> Information and Computation, </journal> <volume> 75 </volume> <pages> 87-106, </pages> <month> November </month> <year> 1987. </year>
Reference-contexts: Where no confusion will result, we drop the superscript N indicating the current round. As a simple example, we might consider a contract adversary that plays 1 in the current round if and only if p 11 &lt; fi for some threshold fi 2 <ref> [0; 1] </ref>. This adversary is willing to let the strategy learning algorithm receive a payoff only if it is not too wealthy already, a rather natural strategy. <p> Here, however, the situation is considerably more favorable. Let us say that p 2 <ref> [0; 1] </ref> has rational complexity at most N if p is a rational number whose numerator and denominator are both at most N . <p> Again, N = O ((1=*) log (1=*)) suffices. 2 Armed with the analysis of algorithm A 1 , we now return to the more general two-dimensional adversary f ff;fi (p 00 ; p 11 ). In this case, the optimal contract payoff is the largest value p 2 <ref> [0; 1] </ref> such that f ff;fi (1p; p) = 1 | in other words, an optimal strategy tries to maintain p 00 = 1 p and p 11 = p for p as large as possible; see Figure 1. <p> Our algorithm will still attempt to penny-match, but using a different strategy. The algorithm partitions the p 11 axis into disjoint subintervals. Initially, there is only the single subinter-val <ref> [0; 1] </ref>. <p> More formally, a Probabilistic Action Automaton (PAA) M is a tuple (Q; q 0 ; fl; t ), where Q is a finite set of n state, q 0 2 Q is the designated starting state, fl : Q ! <ref> [0; 1] </ref>, is the state action probability function, and t : Q fi f0; 1g ! Q is the transition function. The adversary automaton M is initially at its 8 starting state, q 0 . <p> Given such an orientation procedure, the PAA learning problem reduces [11] to what we call PAA learning with reset. Under assumption (1) above (as well as the low cover time assumption) the latter problem can be solved using a variant of Angluin's algorithm <ref> [1] </ref> (for learning DFAs with reset) that is similar to the one described in [12]; thus we concentrate here on the issue of orientation. A well-studied procedure [11] for orientation when learning DFAs uses a homing sequence [6].
Reference: [2] <author> Thomas Dean, Dana Angluin, Kenneth Basye, Sean Engelson, Leslie Kaelbling, Evangelos Kokkevis, and Oded Maron. </author> <title> Inferring finite automata with stochastic output functions and an application to map learning. </title> <journal> Machine Learning, </journal> <volume> 18(1) </volume> <pages> 81-108, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: This provides us with a sufficient orientation procedure for PAAs. We hope that this procedure may find other applications as well. The idea we employ follows a technique used by Dean et. al. <ref> [2] </ref> in the related setting of learning DFAs with noisy outputs.
Reference: [3] <author> Lance Fortnow and Duke Whang. </author> <title> Optimality and domination in repeated games with bounded players. </title> <booktitle> In The 25th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 741-749, </pages> <year> 1994. </year>
Reference-contexts: These works usually do not explicitly take into account the computational efficiency of the learning algorithm, and often give algorithms whose running time is exponential in some natural measure of the adversary's complexity; a notable recent exception is the work of Fortnow and Whang <ref> [3] </ref>. In this paper, we seek to contribute to and expand the scope of this line of research. <p> As an intuitive illustration of the difference between penny-matching and contract, consider playing these two games against the same finite automaton M . For playing penny-matching, we may not need to build a detailed model of M | it suffices to discover a "penny-matching cycle" of M <ref> [3] </ref>. For contract, while an exact model of M may still be unnecessary, we must at least do enough exploration to find any regions where M plays 1 frequently. <p> This continues a line of research investigated by Gilboa and Samet [4], and most recently by Fortnow and Whang <ref> [3] </ref>. We describe an efficient algorithm for learning to play any game optimally against any finite automaton with probabilistic actions and low cover time. <p> However, as in their original strategy for DFAs it might take an exponential number of rounds (in the number of states of the adversary automaton) to achieve payoff which is close to optimal. Fortnow and Whang <ref> [3] </ref> show that if the underlying game played has a certain property (held by contract but not by penny-matching), then there is also an exponential lower bound for the number of rounds needed before approaching optimality.
Reference: [4] <author> Itzhak Gilboa and Dov Samet. </author> <title> Bounded versus unbounded rationality: The tyranny of the weak. </title> <journal> Games and Economic Behavior, </journal> <volume> 1(3) </volume> <pages> 213-221, </pages> <year> 1989. </year>
Reference-contexts: previous papers examine how various aspects of classical game theory change in this setting (for example, whether cooperation is a stable solution for prisoner's dilemma when the adversary is a finite automaton [8, 10]), some have examined the natural question of learning to play optimally against a computationally bounded adversary <ref> [4] </ref>. These works usually do not explicitly take into account the computational efficiency of the learning algorithm, and often give algorithms whose running time is exponential in some natural measure of the adversary's complexity; a notable recent exception is the work of Fortnow and Whang [3]. <p> This continues a line of research investigated by Gilboa and Samet <ref> [4] </ref>, and most recently by Fortnow and Whang [3]. We describe an efficient algorithm for learning to play any game optimally against any finite automaton with probabilistic actions and low cover time. <p> A cycle is simple if no state is visited more than once on the cycle. As in the case of DFAs, for every PAA there exists an optimal strategy that is a repeated simple cycle <ref> [4] </ref>. Moreover, if the automaton is given, such a cycle can be found efficiently using dynamic programming. In the learning setting, Gilboa and Samet [4] note that their algorithm for learning an optimal cycle strategy against DFAs can be easily adapted to PAA's. <p> As in the case of DFAs, for every PAA there exists an optimal strategy that is a repeated simple cycle <ref> [4] </ref>. Moreover, if the automaton is given, such a cycle can be found efficiently using dynamic programming. In the learning setting, Gilboa and Samet [4] note that their algorithm for learning an optimal cycle strategy against DFAs can be easily adapted to PAA's.
Reference: [5] <author> Ehud Kalai. </author> <title> Bounded rationality and strategic complexity in repeated games. </title> <booktitle> Games Theory and Applications, </booktitle> <pages> pages 131-157, </pages> <year> 1990. </year>
Reference-contexts: The typical approach in this area has been to assume that the adversary's strategy is a member of some natural class of computationally bounded strategies | most often, a class of finite automata (for a survey on the area of "bounded rationality", see the paper of Kalai <ref> [5] </ref>).
Reference: [6] <author> Zvi Kohavi. </author> <title> Switching and Finite Automata Theory. </title> <publisher> McGraw-Hill, </publisher> <address> second edition, </address> <year> 1978. </year>
Reference-contexts: A well-studied procedure [11] for orientation when learning DFAs uses a homing sequence <ref> [6] </ref>. A homing sequence is an action sequence h such that regardless of the starting state, the sequence of state labels observed during the execution of h uniquely determines the state reached by executing h.
Reference: [7] <author> N. Littlestone. </author> <title> Learning when irrelevant attributes abound: A new linear-threshold algorith m. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 285-318, </pages> <year> 1988. </year>
Reference-contexts: Similar arguments apply to contract. On the positive side, suppose that the recent history adversary f (~u; ~v) is drawn from a class A of boolean functions for which there is an absolute mistake-bounded algorithm <ref> [7] </ref> | that is, a prediction algorithm A that makes at most m (`; size (f )) mistakes in predicting the values of any target function chosen from A on any sequence of 2`-bit inputs, where m (`; size (f )) is a polynomial. <p> an index set and f S (~u; ~v) = i2S Note that f S is a 2-DNF formula over the variables u 1 ; : : : ; u n ; v 1 ; : : : ; v n ; since such formulae have an efficient absolute mistake-bounded algorithm <ref> [7] </ref>, our comments above imply that learning to play penny-matching optimally against such adversaries is already a solved problem. Let us now examine the more subtle problem of playing contract against this class. <p> been at least halved. 1 Note that there are really only three dimensions here since P a;b2f0;1g p ab = 1 always holds. 2 This statement does not hold for ff &lt; 0, but this case can be handled by separate methods. 6 In the usual models of on-line prediction <ref> [7] </ref>, the sequence of inputs x i to f fi to be classified (matched) is arbitrary, and the halving algorithm may be forced to make N prediction mistakes in N trials due to the arbitrary precision of the x i (an unfavorable sequence would always arrange the next x i to
Reference: [8] <author> A. Neyman. </author> <title> Bounded complexity justifies cooperation in the finitely repeated prisoners' dilemma. </title> <journal> Economics Letters, </journal> <volume> 19 </volume> <pages> 227-229, </pages> <year> 1985. </year>
Reference-contexts: While most previous papers examine how various aspects of classical game theory change in this setting (for example, whether cooperation is a stable solution for prisoner's dilemma when the adversary is a finite automaton <ref> [8, 10] </ref>), some have examined the natural question of learning to play optimally against a computationally bounded adversary [4].
Reference: [9] <author> Christos H. Papadimitriou and John N. Tsitsiklis. </author> <title> The complexity of markov decision processes. </title> <journal> Mathematics of Operations Research, </journal> <volume> 12(3):441 - 450, </volume> <year> 1987. </year>
Reference-contexts: powerful positive result to date for learning to play against such automata | an efficient algorithm for learning to play any game optimally against any finite automaton with probabilistic actions and low cover time. (For the case of randomized transitions, we improve the PSPACE -hardness result of Papadimitriou and Tsitsiklis <ref> [9] </ref>.) 1 2 Models and Definitions In this paper, games are played by two players, one of which will be called the adversary and the other the strategy learning algorithm. We think of the adversary as being implemented by a resource-bounded computational mechanism. <p> In Appendix A we show that if the adversary automaton has probabilistic transitions then the problem of approximating the optimal strategy is PSPACE - complete even if the automaton is known to the algorithm, thus improving on a result of Papadimitriou and Tsitsiklis <ref> [9] </ref>. In the setting studied here, the action played by the adversary automaton is determined according to a probability distribution over the actions f0; 1g associated with the current state | thus, each state of the adversary automaton can be thought of as containing a different biased coin.
Reference: [10] <author> Christos H. Papadimitriou and Mihalis Yannakakis. </author> <title> On complexity as bounded rationality. </title> <booktitle> In Proceedings of the Twenty Sixth Annual ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 726-733, </pages> <year> 1994. </year>
Reference-contexts: While most previous papers examine how various aspects of classical game theory change in this setting (for example, whether cooperation is a stable solution for prisoner's dilemma when the adversary is a finite automaton <ref> [8, 10] </ref>), some have examined the natural question of learning to play optimally against a computationally bounded adversary [4].
Reference: [11] <author> Ronald. L. Rivest and Robert. E. Schapire. </author> <title> Inference of finite automata using homing sequences. </title> <journal> Information and Computation, </journal> <volume> 103(2) </volume> <pages> 299-347, </pages> <year> 1993. </year>
Reference-contexts: Given such an orientation procedure, the PAA learning problem reduces <ref> [11] </ref> to what we call PAA learning with reset. <p> A well-studied procedure <ref> [11] </ref> for orientation when learning DFAs uses a homing sequence [6]. A homing sequence is an action sequence h such that regardless of the starting state, the sequence of state labels observed during the execution of h uniquely determines the state reached by executing h.
Reference: [12] <author> Dana Ron and Ronitt Rubinfeld. </author> <title> Exactly learning automata with small cover time. </title> <booktitle> 1995. To appear in Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory. </booktitle>
Reference-contexts: Under assumption (1) above (as well as the low cover time assumption) the latter problem can be solved using a variant of Angluin's algorithm [1] (for learning DFAs with reset) that is similar to the one described in <ref> [12] </ref>; thus we concentrate here on the issue of orientation. A well-studied procedure [11] for orientation when learning DFAs uses a homing sequence [6].
Reference: [13] <author> Dana Ron and Ronitt Rubinfeld. </author> <title> Learning fallible finite state automata. </title> <journal> Machine Learning, </journal> <volume> 18 </volume> <pages> 149-185, </pages> <year> 1995. </year>
Reference-contexts: In order to simplify the presentation we start by making two assumptions which can be removed using a technique described in <ref> [13] </ref> (details in the full paper). (1) There exists some value = poly (*=n), such 3 The cover time of a graph is defined to be the smallest integer ` such that for every vertex v, a random walk of length ` starting from v, will pass through every other vertex
Reference: [14] <author> Adi Shamir. </author> <title> IP = PSPACE. </title> <booktitle> In Proceedings of the Thirty First Annual Symposium on Foundations of Computer Science, </booktitle> <year> 1990. </year> <month> 11 </month>
References-found: 14

