URL: http://www.cs.utah.edu/~beazley/s2.ps
Refering-URL: http://www.cs.utah.edu/~beazley/publications.html
Root-URL: 
Title: Large-scale Molecular Dynamics on MPPs (Part 2)  
Author: David M. Beazley and Peter S. Lomdahl 
Date: July 1994  
Address: Los Alamos, NM 87545  
Affiliation: Los Alamos National Laboratory  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> D.M. </author> <title> Beazley and P.S. Lomdahl, Message-passing multi-cell molecular dynamics on the Connection Machine 5, </title> <booktitle> Parallel Computing, 20(1994), </booktitle> <pages> pp: 173-195. </pages>
Reference-contexts: These are the kinds of problems facing applications programmers (and compiler writers) on machines such as the T3D. To illustrate the problem, consider the following two code fragments from the force calculation in two short-range MD codes <ref> [1, 3] </ref>. Each code fragment calculates r 2 , the square of the distance between two different atoms.
Reference: [2] <author> D.M. Beazley, P.S. Lomdahl, P. Tamayo and N. Grtnbech-Jensen, </author> <title> A high performance communications and memory caching scheme for molecular dynamics on the CM-5, </title> <booktitle> Proceedings of the 8th International Parallel Processing Symposium (IPPS'94), IEEE Computer Society, </booktitle> <year> 1994, </year> <pages> pp. 800-809. </pages>
Reference-contexts: By controlling the data-layout we can organize data in packets that allow fast communications and computation. Consequently, this allows us to minimize the communications between processors and as a result, communications only takes approximately 5-20% of the overall processing time in a typical simulation <ref> [2] </ref>. <p> As a result, the C programmer is given the sometimes daunting task of not only vectorizing a calculation, but doing so in SIMD mode with special data-layouts. While a full discussion of programming the VUs is not possible here, we decided to write our force calculation entirely in CDPEAC <ref> [2] </ref>. To solve data-layout problems and improve memory bandwidth to the VUs, we implemented a memory caching and management scheme that doubled our VU performance.
Reference: [3] <author> S. Chynoweth, Y. Michopoulos, and U.C. Klomp, </author> <title> A fast algorithm for the computation of interparticle forces in molecular dynamics simulations, Parallel Computing and Transputer Applications, </title> <publisher> IOS Press/CIMNE, </publisher> <year> 1992, </year> <pages> pp. 128-137. </pages>
Reference-contexts: These are the kinds of problems facing applications programmers (and compiler writers) on machines such as the T3D. To illustrate the problem, consider the following two code fragments from the force calculation in two short-range MD codes <ref> [1, 3] </ref>. Each code fragment calculates r 2 , the square of the distance between two different atoms.
Reference: [4] <author> P.S. Lomdahl, P. Tamayo, N. </author> <title> Grtnbech-Jensen and D.M. Beazley, </title> <booktitle> Proc. of Supercomputing 93, IEEE Computer Society (1993), </booktitle> <pages> pp: 520-527. </pages>
Reference-contexts: Furthermore, floating point performance is better than that seen in most data-parallel applications. In November 1993 our MD code, SPaSM, was one of the winners in the IEEE Gordon Bell Prize for sustaining 50 Gflops on the 1024 processor CM-5 at Los Alamos National Laboratory <ref> [4] </ref>. It is interesting to note that performance problems on the CM-5 are not limited to C codes and that the overall winner in the 1993 Gordon Bell Prize was a CM Fortran code that also used CDPEAC assembler kernels for high performance [5].
Reference: [5] <author> L. N. Long and J. </author> <title> Myczkowski, </title> <booktitle> Proc. of Supercomputing 93, IEEE Computer Society (1993), </booktitle> <pages> pp. 528-534. </pages>
Reference-contexts: It is interesting to note that performance problems on the CM-5 are not limited to C codes and that the overall winner in the 1993 Gordon Bell Prize was a CM Fortran code that also used CDPEAC assembler kernels for high performance <ref> [5] </ref>. While many cringe at the thought of having to write assembler code, it substantially improved code performance. Fortunately, we were able to isolate all of the assembler coding into a single code module.
Reference: [6] <editor> R.C. Giles and P. Tamayo, </editor> <booktitle> Proceedings of SHPCC'92, IEEE Computer Society, </booktitle> <year> 1992, </year> <pages> pp. 240. </pages>
Reference-contexts: virtually all massively parallel machines now support some form of message-passing. 5 Why message passing? Since MD is an inherently unstructured problem, data-parallel languages such as (CM Fortran, Cfl, and HPF) are not well suited to solve our problem (although data parallel MD codes have been developed for the CM-2 <ref> [6, 7] </ref>). The message-passing programming model maps very cleanly onto our algorithm. This allows us to explicitly control our data-layout and communications. By controlling the data-layout we can organize data in packets that allow fast communications and computation.
Reference: [7] <author> P. Tamayo, J.P. Mesirov and B.M. </author> <title> Boghosian, </title> <booktitle> Proceedings of Supercomputing '91, IEEE Computer Society, </booktitle> <year> 1991, </year> <pages> pp. 462. </pages>
Reference-contexts: virtually all massively parallel machines now support some form of message-passing. 5 Why message passing? Since MD is an inherently unstructured problem, data-parallel languages such as (CM Fortran, Cfl, and HPF) are not well suited to solve our problem (although data parallel MD codes have been developed for the CM-2 <ref> [6, 7] </ref>). The message-passing programming model maps very cleanly onto our algorithm. This allows us to explicitly control our data-layout and communications. By controlling the data-layout we can organize data in packets that allow fast communications and computation.
References-found: 7

