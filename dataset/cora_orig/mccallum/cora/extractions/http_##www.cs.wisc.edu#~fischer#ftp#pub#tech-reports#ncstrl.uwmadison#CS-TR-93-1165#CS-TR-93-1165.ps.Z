URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-93-1165/CS-TR-93-1165.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-93-1165/
Root-URL: http://www.cs.wisc.edu
Title: Serial and Parallel Multicategory Discrimination  
Author: Kristin P. Bennett O. L. Mangasarian 
Note: 5000/125 and a Thinking Machines Corporation CM-5 multiprocessor.  
Date: July 30, 1993  
Abstract: A parallel algorithm is proposed for a fundamental problem of machine learning, that of mul-ticategory discrimination. The algorithm is based on minimizing an error function associated with a set of highly structured linear inequalities. These inequalities characterize piecewise-linear separation of k sets by the maximum of k affine functions. The error function has a Lipschitz continuous gradient that allows the use of fast serial and parallel unconstrained minimization algorithms. A serial quasi-Newton algorithm is considerably faster than previous linear programming formulations. A parallel gradient distribution algorithm is used to parallelize the error-minimization problem. Preliminary computational results are given for both a DECstation 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Aeberhard, D. Coomans, and O. de Vel. </author> <title> Comparison of classifiers in high dimensional settings. </title> <type> Technical Report 92-02, </type> <institution> Departments of Computer Science and of Mathematics and Statistics, James Cook University of North Queensland, </institution> <year> 1992. </year>
Reference-contexts: Actual discrimination problems were used to compare the algorithms. These data sets are available via anonymous ftp (file transfer protocol) from the University of California-Irvine Repository of Machine Learning Databases and Domain Theories [17]. The wine recognition data <ref> [1] </ref>, referred to as wine, uses the chemical analysis of wine to determine the cultivar. This wine set is piecewise-linear separable. Fisher's classical Iris identification problem [10], referred to as Iris, used physical attributes of Iris blossoms to determine the type of Iris. The Iris data is almost piecewise-linear separable.
Reference: [2] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Multicategory separation via linear programming. </title> <institution> Computer Sciences Department Technical Report 1127, University of Wisconsin, Madison, Wisconsin, </institution> <year> 1992. </year> <note> To appear in Optimization Methods and Software. </note>
Reference-contexts: The function can then be used to classify future points that belong to one of the sets. We propose a piecewise-linear convex function which is the maximum of k linear (affine) functions. This function has proven to be very useful in decision-tree learning methods [5]. In <ref> [2] </ref>, a linear programming approach was proposed for constructing the function by minimizing the average classification error. In the present work we formulate a 2-norm approach that involves the minimization of an unconstrained piecewise-quadratic convex function with a Lipschitz-continuous gradient. <p> We establish a new simple condition (9) for the occurrence of the null solution for the 2-norm problem (6), which turns out to be equivalent to that for the 1-norm formulation (5) <ref> [2] </ref>. fl Computer Sciences Department, University of Wisconsin, 1210 West Dayton Street, Madison, WI 53706, email: bennett@cs.wisc.edu, olvi@cs.wisc.edu. <p> j = 1; : : : ; k: If f has continuous first partial derivatives on R h ; we write f 2 C 1 (R h ): 2 Multicategory Separation by a Piecewise-Linear Surface We begin by defining the concept of piecewise-linear separation of k sets in R n <ref> [19, 2] </ref> and formu lating the problem of minimizing the 1-norm error as a linear-program. <p> ` ; (2) p (x) = xw i fl i + f or j 6= i 2 w 3 x fl 3 w 4 w 1 x fl 3 w 1 w 3 = 2 x fl w 1 = w 2 x fl 2 A 2 A 4 In <ref> [2] </ref>, it was shown that the inequalities of the piecewise-linear separator are satisfied if and only if the minimum of the 1-norm of the average violations of the inequalities (1) is zero, namely 0 = min k X k X j6=i m i (A i (w i w j ) + <p> As shown in <ref> [2] </ref>, the linear program (5) is quite effective on real-world problems. In practice such problems are rarely piecewise-linear separable and thus a multivariate decision tree must be used. A multivariate decision tree works by applying the linear program or another algorithm to a k-class 3 classification problem. <p> It is also true for the LP (5), that the null solution occurs if and only if condition (9) holds <ref> [2] </ref>. However, condition (9) was written in a slightly more complex form in [2, Equation (10)]. For real-world classification problems, all k classes rarely have the same mean. <p> It is also true for the LP (5), that the null solution occurs if and only if condition (9) holds [2]. However, condition (9) was written in a slightly more complex form in <ref> [2, Equation (10)] </ref>. For real-world classification problems, all k classes rarely have the same mean.
Reference: [3] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Neural network training via linear programming. </title> <editor> In P. M. Pardalos, editor, </editor> <booktitle> Advances in Optimization and Parallel Computing, </booktitle> <pages> pages 56-67, </pages> <address> Amsterdam, 1992. </address> <publisher> North Holland. </publisher>
Reference-contexts: As was the case for linear and piecewise-linear separation of two sets by linear programming <ref> [3, 4] </ref>, it is important to determine when the useless null solution occurs for sets A i ; i = 1; : : : ; k 4 A 1 A 2 A 4 First piecewise-linear separation Second piecewise-linear separation Third piecewise-linear separation distinguishing 4 classes in R 2 5 A 1
Reference: [4] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Robust linear programming discrimination of two linearly inseparable sets. </title> <journal> Optimization Methods and Software, </journal> <volume> 1 </volume> <pages> 23-34, </pages> <year> 1992. </year>
Reference-contexts: As was the case for linear and piecewise-linear separation of two sets by linear programming <ref> [3, 4] </ref>, it is important to determine when the useless null solution occurs for sets A i ; i = 1; : : : ; k 4 A 1 A 2 A 4 First piecewise-linear separation Second piecewise-linear separation Third piecewise-linear separation distinguishing 4 classes in R 2 5 A 1
Reference: [5] <author> C. E. Brodley and P. E. Utgoff. </author> <title> Multivariate decision trees. </title> <type> COINS Technical Report 92-83, </type> <institution> University of Massachussets, Amherst, Massachusetts, </institution> <year> 1992. </year> <note> To appear in Machine Learning. </note>
Reference-contexts: The function can then be used to classify future points that belong to one of the sets. We propose a piecewise-linear convex function which is the maximum of k linear (affine) functions. This function has proven to be very useful in decision-tree learning methods <ref> [5] </ref>. In [2], a linear programming approach was proposed for constructing the function by minimizing the average classification error. In the present work we formulate a 2-norm approach that involves the minimization of an unconstrained piecewise-quadratic convex function with a Lipschitz-continuous gradient. <p> Ideally, an iterative method is also desirable in case new points are added and the tree needs to be adjusted [21]. Previous iterative approaches based on extensions to the perceptron algorithm [19, 6, 7] do not have stable performance for the inseparable case and as a result heuristic methods <ref> [12, 5] </ref> have been developed to get around this deficiency. Ideally we would like to have a fast parallelizable algorithm that can be shown to converge for both separable and the more common inseparable problems. By starting from Definition 2.1 and reformulating the problem we can accomplish this. <p> The Iris data is almost piecewise-linear separable. In the forensic glass identification data [8], referred to as glass, the chemical analysis of forensic glass is used to determine the origin of the glass. The glass data is not piecewise-linear separable. In the image segmentation problem <ref> [5] </ref>, low-level real-valued image features are used to determine the image segment: sky, cement, window, brick, grass, foliage, or path. This image data was generated by the Vision Group at the University of Massachusetts.
Reference: [6] <author> R. O. Duda and H. Fossum. </author> <title> Pattern classification by iteratively determined linear and piecewise linear discriminant functions. </title> <journal> IEEE Transactions on Electronic Computers, </journal> <volume> 15 </volume> <pages> 220-232, </pages> <year> 1966. </year>
Reference-contexts: Nilsson [19], Duda-Fossum <ref> [6] </ref>, Duda-Hart [7], and Fukunaga [11] considered iterative methods that are extensions of the perceptron algorithm or the Motzkin-Schoenberg algorithm [16] for determining a piecewise-linear separator provided one exists. <p> Since many such LPs may be needed to find a single decision tree, a fast method is desired. Ideally, an iterative method is also desirable in case new points are added and the tree needs to be adjusted [21]. Previous iterative approaches based on extensions to the perceptron algorithm <ref> [19, 6, 7] </ref> do not have stable performance for the inseparable case and as a result heuristic methods [12, 5] have been developed to get around this deficiency.
Reference: [7] <author> R. O. Duda and P. E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: Nilsson [19], Duda-Fossum [6], Duda-Hart <ref> [7] </ref>, and Fukunaga [11] considered iterative methods that are extensions of the perceptron algorithm or the Motzkin-Schoenberg algorithm [16] for determining a piecewise-linear separator provided one exists. Unlike our proposed approach, convergence of these iterative methods is not known if a separating piecewise-linear surface does not exist [11, p. 374]. <p> Since many such LPs may be needed to find a single decision tree, a fast method is desired. Ideally, an iterative method is also desirable in case new points are added and the tree needs to be adjusted [21]. Previous iterative approaches based on extensions to the perceptron algorithm <ref> [19, 6, 7] </ref> do not have stable performance for the inseparable case and as a result heuristic methods [12, 5] have been developed to get around this deficiency.
Reference: [8] <author> I. W. Evett and E.J. Spiehler. </author> <title> Rule induction in forensic science. </title> <type> Technical report, </type> <institution> Central Research Establishment, Home Office Forensic Science Service, </institution> <address> Aldermaston, Reading, Berkshire RG7 4PN, </address> <year> 1987. </year>
Reference-contexts: This wine set is piecewise-linear separable. Fisher's classical Iris identification problem [10], referred to as Iris, used physical attributes of Iris blossoms to determine the type of Iris. The Iris data is almost piecewise-linear separable. In the forensic glass identification data <ref> [8] </ref>, referred to as glass, the chemical analysis of forensic glass is used to determine the origin of the glass. The glass data is not piecewise-linear separable.
Reference: [9] <author> M. C. Ferris and O. L. Mangasarian. </author> <title> Parallel variable distribution. </title> <booktitle> Symposium on Parallel Optimization 3, </booktitle> <address> Madison, Wisconsin, </address> <month> July 7-9, </month> <year> 1993. </year>
Reference-contexts: Hence (16) is satisfied. Consequently we have the following parallel algorithm that we propose for our multicategory discrimination problem. This algorithm can also be considered as a parallel variable distribution algorithm <ref> [9] </ref>. Theorem 3.2 Parallel gradient distribution algorithm theorem 2 (PGD) Let f 2 C 1 (R h ). Start with any x 0 2 R h : Having x i stop if rf (x i ) = 0; else compute x i+1 as follows.
Reference: [10] <author> R.A. Fisher. </author> <title> The use of multiple measurements in taxonomic problems. </title> <booktitle> Annual Eugenics, </booktitle> <address> 7(Part II):179-188, </address> <year> 1936. </year>
Reference-contexts: The wine recognition data [1], referred to as wine, uses the chemical analysis of wine to determine the cultivar. This wine set is piecewise-linear separable. Fisher's classical Iris identification problem <ref> [10] </ref>, referred to as Iris, used physical attributes of Iris blossoms to determine the type of Iris. The Iris data is almost piecewise-linear separable. In the forensic glass identification data [8], referred to as glass, the chemical analysis of forensic glass is used to determine the origin of the glass.
Reference: [11] <author> K. Fukunaga. </author> <title> Statistical Pattern Recognition. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1990. </year> <month> 15 </month>
Reference-contexts: Nilsson [19], Duda-Fossum [6], Duda-Hart [7], and Fukunaga <ref> [11] </ref> considered iterative methods that are extensions of the perceptron algorithm or the Motzkin-Schoenberg algorithm [16] for determining a piecewise-linear separator provided one exists. Unlike our proposed approach, convergence of these iterative methods is not known if a separating piecewise-linear surface does not exist [11, p. 374]. <p> Unlike our proposed approach, convergence of these iterative methods is not known if a separating piecewise-linear surface does not exist <ref> [11, p. 374] </ref>. We give now an outline of the paper. In Section 2 we review the linear-programming formulation (5) for piecewise-linear separation using the 1-norm error formulation, and then give the 2-norm formulation (6).
Reference: [12] <author> S. Gallant. </author> <title> Optimal linear discriminants. </title> <booktitle> In Proceedings of the International Conference on Pattern Recognition, </booktitle> <pages> pages 849-852. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1986. </year>
Reference-contexts: Ideally, an iterative method is also desirable in case new points are added and the tree needs to be adjusted [21]. Previous iterative approaches based on extensions to the perceptron algorithm [19, 6, 7] do not have stable performance for the inseparable case and as a result heuristic methods <ref> [12, 5] </ref> have been developed to get around this deficiency. Ideally we would like to have a fast parallelizable algorithm that can be shown to converge for both separable and the more common inseparable problems. By starting from Definition 2.1 and reformulating the problem we can accomplish this.
Reference: [13] <author> P. A. Lachenbruch and R. M. Mickey. </author> <title> Estimation of error rates in discriminant analysis. </title> <journal> Technometrics, </journal> <volume> 10 </volume> <pages> 1-11, </pages> <year> 1968. </year>
Reference-contexts: Thus we used three criteria to evaluate the algorithms: the time to construct the function (the training time), the percent correctness on the training set, and the percent correctness on unseen points. We used 10-fold cross-validation <ref> [13] </ref> to estimate these criteria. In 10-fold cross-validation, 9 10 of the points were used for training and 1 10 of the points were held out and tested on the resulting function. This is repeated 10 times, once for each 1 10 used as the testing set.
Reference: [14] <author> K. Lang, A. Waibel, and G. Hinton. </author> <title> A time-delay neural network architecture for isolated word recognition. </title> <booktitle> Neural Networks, </booktitle> <volume> 3 </volume> <pages> 23-43, </pages> <year> 1990. </year>
Reference-contexts: 0.4 0.1 &lt; 0:00001 Glass 231.2 25.2 12.4 25.1 &lt; 0:00001 Image-s 610.5 107.0 96.9 13.1 &lt; 0:00001 12 algorithm [20], one successful stopping criteria is to reserve part of the training set as a tuning set, and to stop the algorithm when the accuracy on the tuning set decreases <ref> [14, p. 41-42] </ref>. We plan to investigate in the future the use of such tuning sets to halt the algorithm. The training times for the MCD-PGD and quasi-Newton algorithms were competitive. For small problems the quasi-Newton algorithm is clearly a better choice.
Reference: [15] <author> O. L. Mangasarian. </author> <title> Parallel gradient distribution in unconstrained optimization. </title> <type> Computer Sciences Technical Report 1145, </type> <institution> University of Wisconsin, Madison, Wisconsin 53706, </institution> <year> 1993. </year>
Reference-contexts: The two principal advantages of the new formulation over the linear programming approach are that (i) the serial version of the new approach is much faster than the linear programming formulation, and (ii) the new approach is much easier to parallelize via an iterative parallel gradient distribution algorithm <ref> [15] </ref>. Nilsson [19], Duda-Fossum [6], Duda-Hart [7], and Fukunaga [11] considered iterative methods that are extensions of the perceptron algorithm or the Motzkin-Schoenberg algorithm [16] for determining a piecewise-linear separator provided one exists. <p> The pertinent theorem (Theorem 3.1) of the parallel gradient distribution method <ref> [15] </ref> is given and an algorithm based on it is described (Theorem 3.2). Details of the algorithms and implementation are given in Section 3. <p> We took advantage of the structure of the problem, and applied the parallel gradient distribution (MCD-PGD) method <ref> [15] </ref> that is described below. We refer the reader to [15] for more details of MCD-PGD. The parallel gradient distribution algorithm theorem is based on forcing function arguments. The definition of a forcing function is provided below. <p> We took advantage of the structure of the problem, and applied the parallel gradient distribution (MCD-PGD) method <ref> [15] </ref> that is described below. We refer the reader to [15] for more details of MCD-PGD. The parallel gradient distribution algorithm theorem is based on forcing function arguments. The definition of a forcing function is provided below. <p> See <ref> [15] </ref> for the convergence proof of this theorem and other related algorithms. Theorem 3.1 Parallel gradient distribution algorithm theorem 1 [15, Corollary 3.2] Let f 2 C 1 (R h ). <p> See [15] for the convergence proof of this theorem and other related algorithms. Theorem 3.1 Parallel gradient distribution algorithm theorem 1 <ref> [15, Corollary 3.2] </ref> Let f 2 C 1 (R h ). <p> that f (x i+1 ) min f (y ` ` Then, either fx i g terminates at a stationary point x i of min x f (x), or for each accumulation point (x; d) of fx i ; d i g, x is a stationary point of min x In <ref> [15] </ref> a number of implementations of Theorem 3.1 were proposed including gradient descent and quasi-Newton directions and stepsizes such as the Armijo and minimization stepsizes. We shall use another implementation of Theorem 3.1 based on the following simple remarks.
Reference: [16] <author> T. S. Motzkin and I. J. </author> <title> Schoenberg. The relaxation method for linear inequalities. </title> <journal> Canadian Journal of Mathematics, </journal> <volume> 6 </volume> <pages> 393-404, </pages> <year> 1954. </year>
Reference-contexts: Nilsson [19], Duda-Fossum [6], Duda-Hart [7], and Fukunaga [11] considered iterative methods that are extensions of the perceptron algorithm or the Motzkin-Schoenberg algorithm <ref> [16] </ref> for determining a piecewise-linear separator provided one exists. Unlike our proposed approach, convergence of these iterative methods is not known if a separating piecewise-linear surface does not exist [11, p. 374]. We give now an outline of the paper.
Reference: [17] <author> P. M. Murphy and D. W. Aha. </author> <title> UCI repository of machine learning databases. </title> <type> Technical report, </type> <institution> Department of Information and Computer Science, University of California, </institution> <year> 1992. </year>
Reference-contexts: The linear programming and quadratic subproblems were solved using the MINOS [18] package. Actual discrimination problems were used to compare the algorithms. These data sets are available via anonymous ftp (file transfer protocol) from the University of California-Irvine Repository of Machine Learning Databases and Domain Theories <ref> [17] </ref>. The wine recognition data [1], referred to as wine, uses the chemical analysis of wine to determine the cultivar. This wine set is piecewise-linear separable. Fisher's classical Iris identification problem [10], referred to as Iris, used physical attributes of Iris blossoms to determine the type of Iris.
Reference: [18] <author> B.A. Murtagh and M.A. Saunders. </author> <title> MINOS 5.1 user's guide. </title> <type> Technical Report SOL 83.20R, </type> <institution> Stanford University, </institution> <month> January </month> <year> 1987. </year>
Reference-contexts: This problem may be solved serially by any unconstrained first-order optimization method. Our computational results, presented in Section 4, indicate that a quasi-Newton method <ref> [18, p.2] </ref> was considerably faster than solving the corresponding linear program (5). Parallel approaches are attractive for machine learning problems because the problem size may be quite large and the problem may need to be solved many times in the course of a decision-tree construction. <p> the difference between the mean of class ` and the mean of all the points: w ` e ` A ` k X e j A j j=1 ; fl l 9 The unconstrained convex minimization subproblems, (20) and (21), were solved by using the quasi-Newton algorithm in the MINOS <ref> [18] </ref> optimization package. Many variations of the direction and synchronization steps of Algorithm 3.1 are possible under Theorem 3.1. The algorithm presented was the best we found computationally. The algorithm is easily parallelized by distributing each of the subproblems (20) among k processors. <p> The parallel experiments were performed on a Thinking Machines CM-5 parallel processor. The linear programming and quadratic subproblems were solved using the MINOS <ref> [18] </ref> package. Actual discrimination problems were used to compare the algorithms. These data sets are available via anonymous ftp (file transfer protocol) from the University of California-Irvine Repository of Machine Learning Databases and Domain Theories [17]. <p> The LP (5) was solved serially using MINOS <ref> [18] </ref>, and PQM (6) was solved serially by a quasi-Newton method employed by MINOS. Note that the goal of these problems is to construct a function for classifying future unseen points.
Reference: [19] <author> N. J. Nilsson. </author> <title> Learning Machines. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1966. </year>
Reference-contexts: Nilsson <ref> [19] </ref>, Duda-Fossum [6], Duda-Hart [7], and Fukunaga [11] considered iterative methods that are extensions of the perceptron algorithm or the Motzkin-Schoenberg algorithm [16] for determining a piecewise-linear separator provided one exists. <p> j = 1; : : : ; k: If f has continuous first partial derivatives on R h ; we write f 2 C 1 (R h ): 2 Multicategory Separation by a Piecewise-Linear Surface We begin by defining the concept of piecewise-linear separation of k sets in R n <ref> [19, 2] </ref> and formu lating the problem of minimizing the 1-norm error as a linear-program. <p> Since many such LPs may be needed to find a single decision tree, a fast method is desired. Ideally, an iterative method is also desirable in case new points are added and the tree needs to be adjusted [21]. Previous iterative approaches based on extensions to the perceptron algorithm <ref> [19, 6, 7] </ref> do not have stable performance for the inseparable case and as a result heuristic methods [12, 5] have been developed to get around this deficiency.
Reference: [20] <author> D.E. Rumelhart, G.E. Hinton, and J.L. McClelland. </author> <title> Learning internal representations. </title> <editor> In D.E. Rumelhart and J.L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <pages> pages 318-362, </pages> <address> Cambridge, Massachusetts, 1986. </address> <publisher> MIT Press. </publisher>
Reference-contexts: 11.4 61.3 13.1 0.74 Image-s 79.1 0.1 85.3 7.4 0.08 Training Time Training Time (secs) t-test Dataset LP PQM p Wine 14.3 1.2 5.2 1.5 &lt; 0:00001 Iris 5.8 0.7 0.4 0.1 &lt; 0:00001 Glass 231.2 25.2 12.4 25.1 &lt; 0:00001 Image-s 610.5 107.0 96.9 13.1 &lt; 0:00001 12 algorithm <ref> [20] </ref>, one successful stopping criteria is to reserve part of the training set as a tuning set, and to stop the algorithm when the accuracy on the tuning set decreases [14, p. 41-42]. We plan to investigate in the future the use of such tuning sets to halt the algorithm.
Reference: [21] <author> P. E. Utgoff and C. E. Brodley. </author> <title> An incremental method for finding multivariate splits for decision trees. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> pages 58-65, </pages> <address> Los Altos, CA, 1990. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 16 </pages>
Reference-contexts: Since many such LPs may be needed to find a single decision tree, a fast method is desired. Ideally, an iterative method is also desirable in case new points are added and the tree needs to be adjusted <ref> [21] </ref>. Previous iterative approaches based on extensions to the perceptron algorithm [19, 6, 7] do not have stable performance for the inseparable case and as a result heuristic methods [12, 5] have been developed to get around this deficiency.
References-found: 21

