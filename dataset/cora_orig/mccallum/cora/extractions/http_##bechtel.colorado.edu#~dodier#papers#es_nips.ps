URL: http://bechtel.colorado.edu/~dodier/papers/es_nips.ps
Refering-URL: http://www.cs.cmu.edu/Web/Groups/NIPS/NIPS95/Papers.html
Root-URL: 
Title: Geometry of Early Stopping in Linear Networks  
Author: Robert Dodier 
Address: Boulder, CO 80309  
Affiliation: Dept. of Computer Science University of Colorado  
Abstract:  
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Baldi, P., and Y. Chauvin. </author> <title> "Temporal Evolution of Generalization during Learning in Linear Networks," </title> <booktitle> Neural Computation 3, </booktitle> <month> 589-603 (Winter </month> <year> 1991). </year>
Reference-contexts: While the goal of a theory of early stopping is to analyze its application to nonlinear approximators such as sig-moidal networks, this paper will deal mainly with linear systems and only marginally with nonlinear systems. Baldi and Chauvin <ref> [1] </ref> and Wang et al. [6] have also analyzed linear systems. The main result of this paper can be summarized as follows.
Reference: [2] <author> Finnoff, W., F. Hergert, and H. G. Zimmermann. </author> <title> "Extended Regularization Methods for Nonconvergent Model Selection," </title> <booktitle> in Advances in NIPS 5, </booktitle> <editor> S. Han-son, J. Cowan, and C. L. Giles, </editor> <booktitle> eds., </booktitle> <pages> pp 228-235. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann Publishers. </publisher> <year> 1993. </year> <title> [3] von Mises, </title> <editor> R. </editor> <booktitle> Mathematical Theory of Probability and Statistics. </booktitle> <address> New York: </address> <publisher> Academic Press. </publisher> <year> 1964. </year>
Reference-contexts: If the error on the valida tion set increases over time, stop training. This training method, as applied to neural networks, is of relatively recent origin. The earliest references include Morgan and Bourlard [4] and Weigend et al. [7]. fl Address correspondence to: dodier@cs.colorado.edu Finnoff et al. <ref> [2] </ref> studied early stopping empirically. While the goal of a theory of early stopping is to analyze its application to nonlinear approximators such as sig-moidal networks, this paper will deal mainly with linear systems and only marginally with nonlinear systems.
Reference: [4] <author> Morgan, N., and H. Bourlard. </author> <title> "Generalization and Parameter Estimation in Feedforward Nets: Some Experiments," </title> <booktitle> in Advances in NIPS 2, </booktitle> <editor> D. Touretzky, </editor> <publisher> ed., </publisher> <pages> pp 630-637. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher> <year> 1990. </year>
Reference-contexts: Start with initial weights close to zero. Apply gradient descent (backpropagation) on the training data. If the error on the valida tion set increases over time, stop training. This training method, as applied to neural networks, is of relatively recent origin. The earliest references include Morgan and Bourlard <ref> [4] </ref> and Weigend et al. [7]. fl Address correspondence to: dodier@cs.colorado.edu Finnoff et al. [2] studied early stopping empirically.
Reference: [5] <author> Wang, C., and S. Venkatesh. </author> <title> "Temporal Dynamics of Generalization in Neural Networks," </title> <booktitle> in Advances in NIPS 7, </booktitle> <editor> G. Tesauro, D. Touretzky, and T. Leen, </editor> <booktitle> eds. </booktitle> <pages> pp 263-270. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher> <year> 1995. </year>
Reference-contexts: Nonlinear effects such as local minima and non-quadratic basins cannot be accounted for by a linear or asymptotically linear theory, and these may play important roles in nonlinear regression problems. This may invalidate direct extrapolations of linear results to nonlinear networks, such as that given by Wang and Venkatesh <ref> [5] </ref>. 7 ACKNOWLEDGMENTS This research was supported by NSF Presidential Young Investigator award IRI-9058450 and grant 90-21 from the James S. McDonnell Foundation to Michael C. Mozer.
Reference: [6] <author> Wang, C., S. Venkatesh, J. S. Judd. </author> <title> "Optimal Stopping and Effective Machine Complexity in Learning," </title> <booktitle> in Advances in NIPS 6, </booktitle> <editor> J. Cowan, G. Tesauro, and J. Alspector, </editor> <booktitle> eds., </booktitle> <pages> pp 303-310. </pages> <address> San Francisco: </address> <publisher> Morgan Kaufmann. </publisher> <year> 1994. </year>
Reference-contexts: While the goal of a theory of early stopping is to analyze its application to nonlinear approximators such as sig-moidal networks, this paper will deal mainly with linear systems and only marginally with nonlinear systems. Baldi and Chauvin [1] and Wang et al. <ref> [6] </ref> have also analyzed linear systems. The main result of this paper can be summarized as follows.
Reference: [7] <author> Weigend, A., B. Huberman, and D. Rumelhart. </author> <title> "Predicting the Future: A Connectionist Approach," </title> <booktitle> Int'l J. Neural Systems 1, </booktitle> <month> 193-209 </month> <year> (1990). </year>
Reference-contexts: Apply gradient descent (backpropagation) on the training data. If the error on the valida tion set increases over time, stop training. This training method, as applied to neural networks, is of relatively recent origin. The earliest references include Morgan and Bourlard [4] and Weigend et al. <ref> [7] </ref>. fl Address correspondence to: dodier@cs.colorado.edu Finnoff et al. [2] studied early stopping empirically. While the goal of a theory of early stopping is to analyze its application to nonlinear approximators such as sig-moidal networks, this paper will deal mainly with linear systems and only marginally with nonlinear systems.
References-found: 6

