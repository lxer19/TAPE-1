URL: http://www.cs.wisc.edu/wpis/papers/sas97.ps
Refering-URL: http://www.cs.wisc.edu/wpis/papers/
Root-URL: 
Email: fmds,horwitzg@cs.wisc.edu  
Title: The Effects of the Precision of Pointer Analysis effects of pointer analysis (the sizes of
Author: Marc Shapiro and Susan Horwitz 
Note: How good are the "direct"  
Address: 1210 West Dayton Street, Madison, WI 53706 USA  
Affiliation: University of Wisconsin Madison  
Abstract: In order to analyze programs that manipulate pointers, it is necessary to have safe information about what each pointer might point to. There are many algorithms that can be used to determine this information, with varying degrees of accuracy. However, there has been very little previous work that addresses how much the relative accuracies of different pointer-analysis algorithms affect "transitive" results: the results of a subsequent analysis. We have carried out a number of experiments with flow-insensitive, context-insensitive pointer analyses to address the following questions: How are the transitive effects of pointer analysis affected by the precision of the analysis? What are the time tradeoffs? We found that using a more precise pointer analysis does in general lead to more precise transitive results. However, the magnitude of the payoff in precision depends on the particular use of the points-to information. We also found that direct effects are good predictors of transitive effects, and that increased precision in points-to information not only causes a subsequent analysis to produce more precise results, it also causes the subsequent analysis to run faster.
Abstract-found: 1
Intro-found: 1
Reference: [ABS94] <author> T. Austin, S. Breach, and G. Sohi. </author> <title> Efficient detection of all pointer and array access errors. </title> <booktitle> In SIGPLAN Conference on Programming Languages Design and Implementation, </booktitle> <pages> pages 290-301, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: analysis depends, of course, on the relative time requirements of the pointer analysis and the subsequent analysis.) 2 Background to the experiments Our experiments were run on a set of 61 C programs including Gnu Unix utilities, Spec benchmarks, and programs used for benchmarking by Landi [LRZ93] and by Austin <ref> [ABS94] </ref>. Tests were carried out on a Sparc 20/71 with 256 MB of RAM. Our experiments in which pointer analysis was followed by dataflow analysis all involved the following five steps: Step 1: Parse a program, building its control-flow graph.
Reference: [And94] <author> L. O. Andersen. </author> <title> Program Analysis and Specialization for the C Programming Language. </title> <type> PhD thesis, </type> <institution> DIKU, University of Copenhagen, </institution> <month> May </month> <year> 1994. </year> <type> (DIKU report 94/19). </type>
Reference-contexts: the new algorithm, this data is usually limited to reporting the times required to analyze programs, plus some information about the sizes of the points-to sets that were produced (possibly comparing the times and sizes of the new algorithm to those required/produced by some other algorithm). (For examples, see [LR92], <ref> [And94] </ref>, [Ruf95], [Ste96], [EGH94], [WL95] or [SH97].) While this is certainly interesting, it is not clear whether this information can be used to predict the usefulness of the pointer-analysis algorithm in a larger context. <p> The term ff (N ) is the (very slowly growing) inverse Ackermann's function that arises in the context of fast union/find data structures [Tar83]. The four pointer analyses we used were: 1. The analysis defined by Andersen in <ref> [And94] </ref>, which has worst-case time O (N 3 ). 2. The 3-category, log-N-runs analysis defined by Shapiro and Horwitz in [SH97], which has worst-case time O (N ff (N ) log N ). 3.
Reference: [EGH94] <author> M. Emami, R. Ghiya, and L. Hendren. </author> <title> Context-sensitive interprocedural points-to analysis in the presence of function pointers. </title> <booktitle> In SIGPLAN Conference on Programming Languages Design and Implementation, </booktitle> <year> 1994. </year>
Reference-contexts: this data is usually limited to reporting the times required to analyze programs, plus some information about the sizes of the points-to sets that were produced (possibly comparing the times and sizes of the new algorithm to those required/produced by some other algorithm). (For examples, see [LR92], [And94], [Ruf95], [Ste96], <ref> [EGH94] </ref>, [WL95] or [SH97].) While this is certainly interesting, it is not clear whether this information can be used to predict the usefulness of the pointer-analysis algorithm in a larger context.
Reference: [GMW81] <author> R. Giegerich, U. Moncke, and R. Wilhelm. </author> <title> Invariance of approximative semantics with respect to program transformation. </title> <booktitle> In GI 81 11th GI Annual Conference, </booktitle> <volume> Informatik-Fachberichte 50, </volume> <pages> pages 1-10, </pages> <address> New York, NY, 1981. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: By definition, a use in a truly live context means: in a predicate, or in a call to a library function, or in an expression whose value is assigned to a truly live variable <ref> [GMW81] </ref>. Because it is non-locally-separable, the truly-live-variables problem is in some sense a harder problem than the live-variables problem; the worst-case time for this analysis is O (sum of sizes of control-flow graphs fi max (number of visible variables) 3 ).
Reference: [HRB90] <author> S. Horwitz, T. Reps, and D. Binkley. </author> <title> Interprocedural slicing using dependence graphs. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 12(1) </volume> <pages> 26-60, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: Step 5: Slice the system dependence graph with respect to all output statements (i.e., calls to library functions like printf). The system dependence graphs were built and sliced using the algorithms defined in <ref> [HRB90] </ref> [HRSR94] (see Section 2.3 for more detail). 2 A variable x is considered to be killed at a node n if the value of x is definitely overwritten when n is executed. <p> An efficient method for computing context-sensitive interprocedural slices was described in <ref> [HRB90] </ref> [HRSR94]; that method was used in our implementation. The method involves the use of a program representation called the system dependence graph, or SDG.
Reference: [HRSR94] <author> S. Horwitz, T. Reps, M. Sagiv, and G. Rosay. </author> <title> Speeding up slicing. </title> <booktitle> In Proceedings of the Third ACM SIGSOFT Symposium on the Foundations of Software Engineering, </booktitle> <pages> pages 11-20, </pages> <month> December </month> <year> 1994. </year> <note> (Available on the WWW from URL http://www.cs.wisc.edu/wpis/papers/fse94.ps). </note>
Reference-contexts: Step 5: Slice the system dependence graph with respect to all output statements (i.e., calls to library functions like printf). The system dependence graphs were built and sliced using the algorithms defined in [HRB90] <ref> [HRSR94] </ref> (see Section 2.3 for more detail). 2 A variable x is considered to be killed at a node n if the value of x is definitely overwritten when n is executed. <p> An efficient method for computing context-sensitive interprocedural slices was described in [HRB90] <ref> [HRSR94] </ref>; that method was used in our implementation. The method involves the use of a program representation called the system dependence graph, or SDG.
Reference: [Kil73] <author> G.A. Kildall. </author> <title> A unified approach to global program optimization. </title> <booktitle> In ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 194-206, </pages> <month> Jan-uary </month> <year> 1973. </year>
Reference-contexts: The output is the set of dataflow facts that hold at each node of the graph. If the dataflow problem is intraprocedural (i.e., there is only one function), the dataflow facts provide the "meet over all paths" solution <ref> [Kil73] </ref>. If the dataflow problem is interprocedural, the dataflow facts provide the "meet over all interprocedurally valid paths" solution to the dataflow problem (i.e., the algorithm performs precise-or contextsensitive-interprocedural dataflow analysis).
Reference: [LR92] <author> W. Landi and B. Ryder. </author> <title> A safe approximate algorithm for interprocedu-ral pointer aliasing. </title> <booktitle> In SIGPLAN Conference on Programming Languages Design and Implementation, </booktitle> <pages> pages 235-248, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: running the new algorithm, this data is usually limited to reporting the times required to analyze programs, plus some information about the sizes of the points-to sets that were produced (possibly comparing the times and sizes of the new algorithm to those required/produced by some other algorithm). (For examples, see <ref> [LR92] </ref>, [And94], [Ruf95], [Ste96], [EGH94], [WL95] or [SH97].) While this is certainly interesting, it is not clear whether this information can be used to predict the usefulness of the pointer-analysis algorithm in a larger context.
Reference: [LRZ93] <author> W. Landi, B. Ryder, and S. Zhang. </author> <title> Interprocedural modification side effect analysis with pointer aliasing. </title> <booktitle> In SIGPLAN Conference on Programming Languages Design and Implementation, </booktitle> <pages> pages 56-67, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: the more precise pointer analysis depends, of course, on the relative time requirements of the pointer analysis and the subsequent analysis.) 2 Background to the experiments Our experiments were run on a set of 61 C programs including Gnu Unix utilities, Spec benchmarks, and programs used for benchmarking by Landi <ref> [LRZ93] </ref> and by Austin [ABS94]. Tests were carried out on a Sparc 20/71 with 256 MB of RAM. Our experiments in which pointer analysis was followed by dataflow analysis all involved the following five steps: Step 1: Parse a program, building its control-flow graph.
Reference: [NW74] <author> John Neter and William Wasserman. </author> <title> Applied Linear Statistical Models, </title> <booktitle> chapter 5.5, </booktitle> <pages> pages 156-159. </pages> <editor> Richard D. Irwin, </editor> <publisher> Inc., </publisher> <year> 1974. </year>
Reference-contexts: Recall that for deadS and tdeadS, more precise values are larger, while for the other quantities more precise values are smaller. and we can use standard statistical methods for regression through the origin <ref> [NW74] </ref>. As before, we minimize P j (log E ij ) 2 and find ffi = ij log Q Aj S ij P S ij ) 2 The values for ffi are given in Figure 7.
Reference: [RHS95] <author> T. Reps, S. Horwitz, and M. Sagiv. </author> <title> Precise interprocedural dataflow analysis via graph reachability. </title> <booktitle> In ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 49-61, </pages> <month> January </month> <year> 1995. </year> <note> (Available on the WWW from URL http://www.cs.wisc.edu/wpis/papers/popl95.ps). </note>
Reference-contexts: Step 5: Perform the dataflow analysis. The dataflow analysis problems were solved using the method described in <ref> [RHS95] </ref>. The input to the dataflow "engine" is a set of graphs (one for each function in the program), and a set of dataflow functions (one for each edge of the graph).
Reference: [Ruf95] <author> E. Ruf. </author> <title> Context-sensitive alias analysis reconsidered. </title> <booktitle> In SIGPLAN Conference on Programming Languages Design and Implementation, </booktitle> <pages> pages 13-22, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: new algorithm, this data is usually limited to reporting the times required to analyze programs, plus some information about the sizes of the points-to sets that were produced (possibly comparing the times and sizes of the new algorithm to those required/produced by some other algorithm). (For examples, see [LR92], [And94], <ref> [Ruf95] </ref>, [Ste96], [EGH94], [WL95] or [SH97].) While this is certainly interesting, it is not clear whether this information can be used to predict the usefulness of the pointer-analysis algorithm in a larger context. <p> However, for gmodS, gmodT, and tliveT, the value P A=B is always 1.3 or more; the choice of pointer analysis has a very strong effect on these. These results contrast with those reported by Ruf in <ref> [Ruf95] </ref>. Ruf studied the impact of context sensitivity on flow-sensitive pointer analysis. When he considered the sizes of the points-to sets at all program points, he found that using a context-sensitive algorithm did make a difference (the sets were significantly smaller).
Reference: [SH97] <author> M. Shapiro and S. Horwitz. </author> <title> Fast and accurate flow-insensitive points-to analysis. </title> <booktitle> In ACM Symposium on Principles of Programming Languages. ACM, </booktitle> <address> New York, </address> <month> January </month> <year> 1997. </year>
Reference-contexts: usually limited to reporting the times required to analyze programs, plus some information about the sizes of the points-to sets that were produced (possibly comparing the times and sizes of the new algorithm to those required/produced by some other algorithm). (For examples, see [LR92], [And94], [Ruf95], [Ste96], [EGH94], [WL95] or <ref> [SH97] </ref>.) While this is certainly interesting, it is not clear whether this information can be used to predict the usefulness of the pointer-analysis algorithm in a larger context. <p> The four pointer analyses we used were: 1. The analysis defined by Andersen in [And94], which has worst-case time O (N 3 ). 2. The 3-category, log-N-runs analysis defined by Shapiro and Horwitz in <ref> [SH97] </ref>, which has worst-case time O (N ff (N ) log N ). 3. The analysis defined by Steensgaard in [Ste96], which has worst-case time O (N ff (N )). 4.
Reference: [Ste96] <author> B. Steensgaard. </author> <title> Points-to analysis in almost linear time. </title> <booktitle> In ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 32-41, </pages> <month> January </month> <year> 1996. </year>
Reference-contexts: algorithm, this data is usually limited to reporting the times required to analyze programs, plus some information about the sizes of the points-to sets that were produced (possibly comparing the times and sizes of the new algorithm to those required/produced by some other algorithm). (For examples, see [LR92], [And94], [Ruf95], <ref> [Ste96] </ref>, [EGH94], [WL95] or [SH97].) While this is certainly interesting, it is not clear whether this information can be used to predict the usefulness of the pointer-analysis algorithm in a larger context. <p> The analysis defined by Andersen in [And94], which has worst-case time O (N 3 ). 2. The 3-category, log-N-runs analysis defined by Shapiro and Horwitz in [SH97], which has worst-case time O (N ff (N ) log N ). 3. The analysis defined by Steensgaard in <ref> [Ste96] </ref>, which has worst-case time O (N ff (N )). 4. A naive analysis that simply assumes that every pointer variable can point to any variable whose address is taken somewhere in the program, or to any location allocated from the heap. This analysis has worst-case time O (N ).
Reference: [Tar83] <author> R. Tarjan. </author> <title> Data structures and network flow algorithms. </title> <booktitle> volume CMBS44 of Regional Conference Series in Applied Mathematics. </booktitle> <publisher> SIAM, </publisher> <year> 1983. </year>
Reference-contexts: The term ff (N ) is the (very slowly growing) inverse Ackermann's function that arises in the context of fast union/find data structures <ref> [Tar83] </ref>. The four pointer analyses we used were: 1. The analysis defined by Andersen in [And94], which has worst-case time O (N 3 ). 2. The 3-category, log-N-runs analysis defined by Shapiro and Horwitz in [SH97], which has worst-case time O (N ff (N ) log N ). 3.
Reference: [Wei84] <author> M. Weiser. </author> <title> Program slicing. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-10(4):352-357, </volume> <month> July </month> <year> 1984. </year>
Reference-contexts: a projection of the program such that for all possible inputs, the same value is output at S in both the original program and the slice (if statement S is executed more than once, then the two versions of the program must produce the same sequence of values at S) <ref> [Wei84] </ref>. An efficient method for computing context-sensitive interprocedural slices was described in [HRB90] [HRSR94]; that method was used in our implementation. The method involves the use of a program representation called the system dependence graph, or SDG.
Reference: [WL95] <author> R. Wilson and M. Lam. </author> <title> Efficient context-sensitive pointer analysis for C programs. </title> <booktitle> In SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 1-12, </pages> <month> June </month> <year> 1995. </year> <title> This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: data is usually limited to reporting the times required to analyze programs, plus some information about the sizes of the points-to sets that were produced (possibly comparing the times and sizes of the new algorithm to those required/produced by some other algorithm). (For examples, see [LR92], [And94], [Ruf95], [Ste96], [EGH94], <ref> [WL95] </ref> or [SH97].) While this is certainly interesting, it is not clear whether this information can be used to predict the usefulness of the pointer-analysis algorithm in a larger context.
References-found: 17

