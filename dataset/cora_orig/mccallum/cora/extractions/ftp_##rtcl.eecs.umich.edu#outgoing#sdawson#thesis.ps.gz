URL: ftp://rtcl.eecs.umich.edu/outgoing/sdawson/thesis.ps.gz
Refering-URL: http://www.eecs.umich.edu/~sdawson/general.html
Root-URL: http://www.eecs.umich.edu
Title: ABSTRACT Message Level Fault Injection in Distributed Systems  
Author: by Scott David Dawson 
Degree: Chair: Farnam Jahanian  
Abstract: Ensuring that a system meets its prescribed specification is a growing challenge that confronts software developers, especially for distributed applications with strict dependability and timeliness constraints. This dissertation presents orchestra, an architecture for building fault injection layers that are inserted into protocol stacks. These fault injectors are used for identifying design and implementation errors in distributed protocols and applications. orchestra is highly portable. It can be inserted into different protocol stacks and can be placed at different layers within a single stack. This dissertation makes several key contributions to the state of the art in fault injection of distributed communication protocols. First, the orchestra framework supports fault injection with the aim of discovering and removing faults from protocol implementations. The state of a participant in a distributed protocol depends largely on the messages exchanged between participants. orchestra employs a technique called message-level fault injection, which allows system designers and test engineers to manipulate messages as they are exchanged, effectively manipulating the state of a participant. Second, orchestra addresses the intrusiveness of fault injection on target protocols, taking advantage of real-time operating system features, when possible, to quantify and to compensate for intrusiveness. Third, orchestra has been applied to testing protocols that have many participants. Coordination between orchestra fault injectors, and the semantics of data collected at different places in the system are addressed. Finally, this research introduces techniques that allow orchestra to interpret and manipulate the headers of multiple protocols, resulting in an orchestra-based approach to testing services comprised of multiple protocols. 
Abstract-found: 1
Intro-found: 1
Reference: <institution> 137 BIBLIOGRAPHY </institution>
Reference: [1] <author> J. Arlat, Y. Crouzet, and J.-C. Laprie, </author> <title> "Fault injection for dependability validation of fault-tolerant computing systems," </title> <booktitle> in Proc. Int'l Symp. on Fault-Tolerant Computing, </booktitle> <pages> pp. 348-355, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: This approach uses fault injection at the message level for testing fault tolerance and identifying design and implementation errors in protocols. Fault injection for this purpose has been referred to as "fault removal" in the literature <ref> [1] </ref>. The message-level fault injection approach presented here for testing fault-tolerant protocols is motivated by several observations: First, in testing distributed systems, one may wish to coerce the system into certain states to ensure that specific execution paths are taken. <p> The work on fault injection can be broken down roughly into three areas: hardware fault injection and software simulation of hardware faults, system and protocol simulation techniques, and communication protocol fault injection. These areas are presented below. 8 Hardware Fault Injection and Software Simulation of Hardware Faults Hardware fault-injection <ref> [1, 17] </ref> and simulation approaches for injecting hardware failures [18] have received much attention in the past. Shin and Lee [17] inject hardware faults of stuck-at-0, stuck-at-1, and inverted signal at the pin level into the hardware for the Fault-Tolerant Multiprocessor (FTMP) at the NASA Airlab. <p> This methodology allows circuits with long fault latencies to be targeted for on-line diagnostics, which reduces the accumulation of latent faults, i.e., faults that have not yet manifested themselves as errors. The Messaline tool <ref> [1] </ref> uses active probes and hardware sockets to induce pin-level faults into hardware. Faults may be stuck-at, open, and bridging faults, among others. Messaline was used to test a prototype of a computerized interlocking system for railway control applications. <p> Faults may be stuck-at, open, and bridging faults, among others. Messaline was used to test a prototype of a computerized interlocking system for railway control applications. Messaline has been used to test the AMp atomic multicast protocol <ref> [1, 19] </ref>. In these tests, failures were injected at the hardware level to test the properties of the system in the presence of the faults. Gate-level faults are simulated to study their effects on program behavior on the IBM RT PC platform [18]. <p> CmdName: random_seed_proc TclName: random_seed END_USERCMDS_SECTION _PFI_CMD_PROC (random_num_proc) - pfi_TclArg arg = (pfi_TclArg)cldata; char result [100]; int retcode; int num; int low, high, range; if (argc != 3) - sprintf (result, "wrong # args: should be ""%s low high""", argv [0]); retcode = TCL_ERROR; goto done; - low = atoi (argv <ref> [1] </ref>); high = atoi (argv [2]); range = high low; num = rand (); num %= range; num += low; sprintf (result, "%d", num); retcode = TCL_OK; done: Tcl_SetResult (interp, result, TCL_VOLATILE); return retcode; - 27 allow different injection activities to be performed, based on message attributes or other script state.
Reference: [2] <author> N. C. Hutchinson and L. L. Peterson, </author> <title> "The x-Kernel: An architecture for implementing network protocols," </title> <journal> IEEE Trans. Software Engineering, </journal> <volume> vol. 17, no. 1, </volume> <pages> pp. 1-13, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: Message-level fault injection allows the system designer to inject faults into distributed protocols by manipulating messages. In the model underlying message-level fault injection, a distributed protocol can be viewed as an abstraction through which a collection of participants communicate by exchanging messages, in the same spirit as the x-kernel <ref> [2] </ref>. Protocols are layered in a protocol stack, and no distinction is made between application-level protocols, communication protocols, or device layer protocols. In message-level fault injection, a protocol fault injection (PFI) layer is inserted into the protocol stack below the protocol to be tested, called the target protocol. <p> that orchestra attempts to minimize intrusiveness of the fault injection mechanism, and Section 2.7 concludes the chapter. 2.2 Message-Level Fault Injection In message-level fault injection, a distributed protocol is viewed as an abstraction through which a collection of participants communicate by exchanging a set of messages, as in the x-kernel <ref> [2] </ref>. Each layer provides an abstract communication service to higher layers, and there is no distinction made between particular layers of the protocol stack. Problems occurring in the network or on other nodes in the system are experienced by protocol layers at their lower-layer interface. <p> Many protocol stack frameworks do provide upcalls to higher layers when messages arrive. The Communication Object for Real-Time Dependable Systems (CORDS) framework [33], based on the x-kernel <ref> [2] </ref>, is an example of such a system. In CORDS, shepherd threads carry each arriving message up the protocol stack. Because the protocol stack provides threads for carrying messages through the protocol stack, it is not necessary for the orchestra implementation on this platform to contain its own thread. <p> _PFI_CMD_PROC (random_num_proc) - pfi_TclArg arg = (pfi_TclArg)cldata; char result [100]; int retcode; int num; int low, high, range; if (argc != 3) - sprintf (result, "wrong # args: should be ""%s low high""", argv [0]); retcode = TCL_ERROR; goto done; - low = atoi (argv [1]); high = atoi (argv <ref> [2] </ref>); range = high low; num = rand (); num %= range; num += low; sprintf (result, "%d", num); retcode = TCL_OK; done: Tcl_SetResult (interp, result, TCL_VOLATILE); return retcode; - 27 allow different injection activities to be performed, based on message attributes or other script state. <p> Unilateral testing was used, and one machine on the network was instrumented with the fault injection layer, as described in Section 2.2.2. Vendor TCP implementations were then tested by opening connections from the vendor TCP stack to the instrumented stack. The x-kernel <ref> [2] </ref> system was chosen as the protocol stack architecture 32 into which the fault injection layer would be inserted. The x-kernel-based fault injection layer, called a Protocol Fault Injection (PFI) layer, may be placed at any level in an x-kernel protocol stack. <p> CORDS is based on the x-kernel <ref> [2] </ref> framework developed at the University of Arizona, but it contains significant extensions for controlled resource allocation. The primary contribution of the CORDS 60 framework is the provision of paths [42], which are per-session resource pools. <p> It is based on the x-kernel object-oriented networking framework developed at the University of Arizona <ref> [2] </ref>, with some significant extensions for controlled allocation of resources. System designers describe the relationships between protocols. This description is used to build a protocol graph at compile time, resulting in a CORDS protocol stack.
Reference: [3] <author> V. Hadzilacos and S. Toueg, </author> <title> "Fault-tolerant broadcasts and related problems," in Distributed Systems, </title> <editor> S. Mullender, editor, </editor> <publisher> Addison Wesley, </publisher> <year> 1993. </year> <note> Second Edition. </note>
Reference-contexts: Hadzilacos and Toueg offer a more formal treatment of different failure models <ref> [3] </ref>. Process crash failures: A process/processor fails by halting prematurely and doing noth ing from that point on. Before stopping, it behaves correctly. Link crash failures: A link fails by losing messages, but does not delay, duplicate, or 3 corrupt messages. Before ceasing to transport messages, it behaves correctly.
Reference: [4] <author> J. Mogul, R. Rashid, and M. Accetta, </author> <title> "The packet filter: An efficient mechanism for user-level network code," </title> <booktitle> in Proc. ACM Symp. on Operating Systems Principles, </booktitle> <pages> pp. 39-51, </pages> <address> Austin, Texas, </address> <month> November </month> <year> 1987, </year> <note> ACM. </note>
Reference-contexts: Because copying network data from kernel- to user-space is expensive, a kernel agent, or packet filter, determines which packets should be given to user-level processes. Past work on packet filters includes the pioneering work on the CMU/Stanford Packet Filter <ref> [4] </ref>, the BSD packet filter (BPF) [5], the Mach Packet Filter (MPF) [6], PathFinder [7], and the Dynamic Packet Filter (DPF) [8]. The CMU/Stanford Packet Filter is a kernel-resident, protocol-independent, packet de-multiplexor.
Reference: [5] <author> S. McCanne and V. Jacobson, </author> <title> "The BSD Packet Filter: A New Architecture for User-level Packet Capture," </title> <booktitle> in Winter USENIX Conference, </booktitle> <pages> pp. 259-269, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: Because copying network data from kernel- to user-space is expensive, a kernel agent, or packet filter, determines which packets should be given to user-level processes. Past work on packet filters includes the pioneering work on the CMU/Stanford Packet Filter [4], the BSD packet filter (BPF) <ref> [5] </ref>, the Mach Packet Filter (MPF) [6], PathFinder [7], and the Dynamic Packet Filter (DPF) [8]. The CMU/Stanford Packet Filter is a kernel-resident, protocol-independent, packet de-multiplexor. User processes specify in a stack-based language a filter that describes how packets are matched to a user process. <p> After 30 ACKs have been delayed, the send filter triggers the receive filter to begin dropping incoming segments. Each incoming segment is logged by the receive filter with a timestamp. It is noteworthy that approaches depending on packet filtering <ref> [5, 11] </ref> cannot perform this type of experiment because they do not have the ability to manipulate messages. In particular, they cannot direct the system to perform a task such as delaying ACK segments.
Reference: [6] <author> M. Yuhara, B. N. Bershad, C. Maeda, and J. E. B. Moss, </author> <title> "Efficient packet demulti-plexing for multiple endpoints and large messages," </title> <booktitle> in Winter USENIX Conference, </booktitle> <month> January </month> <year> 1994. </year> <note> Second Edition. </note>
Reference-contexts: Past work on packet filters includes the pioneering work on the CMU/Stanford Packet Filter [4], the BSD packet filter (BPF) [5], the Mach Packet Filter (MPF) <ref> [6] </ref>, PathFinder [7], and the Dynamic Packet Filter (DPF) [8]. The CMU/Stanford Packet Filter is a kernel-resident, protocol-independent, packet de-multiplexor. User processes specify in a stack-based language a filter that describes how packets are matched to a user process.
Reference: [7] <author> M. L. Bailey, B. Gopal, M. A. Pagels, L. L. Peterson, and P. Sarkar, "PathFinder: </author> <title> A pattern-based packet classifier," </title> <booktitle> in Proceedings of the First Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pp. 115-123, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Past work on packet filters includes the pioneering work on the CMU/Stanford Packet Filter [4], the BSD packet filter (BPF) [5], the Mach Packet Filter (MPF) [6], PathFinder <ref> [7] </ref>, and the Dynamic Packet Filter (DPF) [8]. The CMU/Stanford Packet Filter is a kernel-resident, protocol-independent, packet de-multiplexor. User processes specify in a stack-based language a filter that describes how packets are matched to a user process. <p> Instead of applying packet filters sequentially (and possibly repeating operations), the Mach packet filter collapses individual filter requests so that redundant operations are 5 performed only once. PathFinder <ref> [7] </ref> is a packet classifier that can be implemented very efficiently, in both software and hardware.
Reference: [8] <author> D. R. Engler and M. F. Kaashoek, "DPF: </author> <title> Fast, flexible message demultiplexing using dynamic code generation," </title> <booktitle> in Proc. of ACM SIGCOMM, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: Past work on packet filters includes the pioneering work on the CMU/Stanford Packet Filter [4], the BSD packet filter (BPF) [5], the Mach Packet Filter (MPF) [6], PathFinder [7], and the Dynamic Packet Filter (DPF) <ref> [8] </ref>. The CMU/Stanford Packet Filter is a kernel-resident, protocol-independent, packet de-multiplexor. User processes specify in a stack-based language a filter that describes how packets are matched to a user process. <p> The software implementation of PathFinder runs about twice as fast as the MPF, and the hardware implementation at publication time was able to classify traffic quickly enough to keep up with OC-12 (622Mbps) network links. The Dynamic Packet Filter (DPF) <ref> [8] </ref> allows users to install filters written in a declarative packet-filter language. The filter language is amenable to aggressive dynamic code generation, allowing executable code for a filter to be generated at run-time. <p> Another possibility is to use the `C (tick-C) language. `C [46] allows code to be generated dynamically and has been used to build very fast packet filtering agents <ref> [8] </ref>. `C would give users flexibility to change scripts without recompiling, while achieving excellent performance. 72 CHAPTER 5 Fault Injection of Multiparticipant Protocols 5.1 Introduction Performing fault injection on protocols with multiple participants is considerably more complex than doing so on protocols with only two participants. <p> Two promising avenues exist: compiled Tcl [45] and `C [46]. Compiled Tcl has achieved performance increases an order of magnitude faster than regular Tcl. `C is a language for efficient dynamic code generation that has been used in the DPF packet filter <ref> [8] </ref> to achieve excellent performance. orchestra test scripts written in `C would be dynamically compiled when the protocol stack begins running, and would provide fast execution when they are called. orchestra fault injection can also be applied to other protocols.
Reference: [9] <author> J. Postel, "RFC-793: </author> <title> Transmission control protocol," Request for Comments, Septem-ber 1981. Network Information Center. </title>
Reference-contexts: Network filtering is also used to collect data for analysis of the the behavior of the Transmission Control Protocol (TCP) <ref> [9, 10] </ref> in the presence of network faults [11, 12]. Finally, approaches such as the Packet Shell [13] allow system 4 designers to generate network traffic in order to test other protocol participants. The relation between orchestra and these three areas of study is presented in this subsection. <p> In this chapter, we describe a prototype implementation of the orchestra architecture that was built to demonstrate the viability of message-level fault injection. This prototype tool was used to perform experiments on six different implementations of the Internet Transmission Control Protocol (TCP) <ref> [9, 10] </ref>. <p> When a message is pushed, it is passed down to the next lower layer of the protocol stack. When a message is popped, it is passed up to the next higher layer of the stack. 33 reliability. TCP was originally defined in RFC-793 <ref> [9] </ref> and was updated in RFC-1122 [10]. In order to meet the TCP standard, an implementation must follow both RFCs. When testing vendor implementations of TCP, source code is generally unavailable. For this reason, the technique used cannot rely on instrumenting the protocol stack on the vendor platforms. <p> If the sender transmits more data than the receiver is willing to accept, the receiver may drop the data (unless the window has reopened). Probing zero-size receive windows must be supported <ref> [9, 10] </ref> because an ACK segment that reopens the window may be lost if it contains no data. The reason is that ACK segments that carry no data are not transmitted reliably. <p> This is probably to allow Windows 95 to operate over highly congested networks and networks prone to connection outages, such as networks supporting mobile hosts. Relation of RTO to RTT TCP implementations adapt to different network speeds and conditions by adjusting the retransmission timeout. RFC-793 <ref> [9] </ref> recommended setting RT O = fiRT T , with a recommended fi = 2. Since then, Van Jacobson has shown that this setting will adapt to load increases of at most 30%[36].
Reference: [10] <author> R. Braden, "RFC-1122: </author> <title> Requirements for internet hosts," Request for Comments, </title> <month> October </month> <year> 1989. </year> <institution> Network Information Center. </institution>
Reference-contexts: Network filtering is also used to collect data for analysis of the the behavior of the Transmission Control Protocol (TCP) <ref> [9, 10] </ref> in the presence of network faults [11, 12]. Finally, approaches such as the Packet Shell [13] allow system 4 designers to generate network traffic in order to test other protocol participants. The relation between orchestra and these three areas of study is presented in this subsection. <p> In this chapter, we describe a prototype implementation of the orchestra architecture that was built to demonstrate the viability of message-level fault injection. This prototype tool was used to perform experiments on six different implementations of the Internet Transmission Control Protocol (TCP) <ref> [9, 10] </ref>. <p> When a message is pushed, it is passed down to the next lower layer of the protocol stack. When a message is popped, it is passed up to the next higher layer of the stack. 33 reliability. TCP was originally defined in RFC-793 [9] and was updated in RFC-1122 <ref> [10] </ref>. In order to meet the TCP standard, an implementation must follow both RFCs. When testing vendor implementations of TCP, source code is generally unavailable. For this reason, the technique used cannot rely on instrumenting the protocol stack on the vendor platforms. <p> If the sender transmits more data than the receiver is willing to accept, the receiver may drop the data (unless the window has reopened). Probing zero-size receive windows must be supported <ref> [9, 10] </ref> because an ACK segment that reopens the window may be lost if it contains no data. The reason is that ACK segments that carry no data are not transmitted reliably. <p> The RTO starts at a value based on the RTT, and exponentially backs off for retransmissions of the same segment. In addition, lower and upper bounds are imposed on the RTO. Part of the TCP specification <ref> [10] </ref> stated in 1989 that commonly used RTO bounds were known to be inadequate on large internets. It stated that the lower bound should be measured in fractions of a second, and that the upper bound should be 2M SL (Maximum Segment Lifetime), which is 240 seconds.
Reference: [11] <author> D. E. Comer and J. C. Lin, </author> <title> "Probing TCP implementations," </title> <booktitle> in Proc. Summer USENIX Conference, </booktitle> <month> June </month> <year> 1994. </year>
Reference-contexts: Network filtering is also used to collect data for analysis of the the behavior of the Transmission Control Protocol (TCP) [9, 10] in the presence of network faults <ref> [11, 12] </ref>. Finally, approaches such as the Packet Shell [13] allow system 4 designers to generate network traffic in order to test other protocol participants. The relation between orchestra and these three areas of study is presented in this subsection. <p> As such, orchestra provides a method not only for collecting traffic, but also for modifying message sequences to test protocol responses to various situations. The active probing approach has been used to study five TCP implementations <ref> [11] </ref>. In active probing, a network analyzer is attached to the same network as two TCP peers. This analyzer captures all network traffic between the peers. The TCP implementations are monitored, or crash faults are effected by turning off the network interface on one of the two machines running TCP. <p> While Solaris 2.3 retransmits the segment nine times before dropping the connection, Windows 95 retransmits the segment only five times. Because of the small number of retransmissions, it does not reach an upper bound before the connection is dropped. Win 3 Comer and Lin presented a similar result <ref> [11] </ref>. 36 dows 95 increases the retransmission timeout exponentially for each of the retransmissions of the dropped segment. As with Solaris 2.3, Windows 95 does not send an RST segment when the connection is dropped. <p> After 30 ACKs have been delayed, the send filter triggers the receive filter to begin dropping incoming segments. Each incoming segment is logged by the receive filter with a timestamp. It is noteworthy that approaches depending on packet filtering <ref> [5, 11] </ref> cannot perform this type of experiment because they do not have the ability to manipulate messages. In particular, they cannot direct the system to perform a task such as delaying ACK segments. <p> Passive filters may not even run on the same processor as the target protocol; they may simply be connected to the same network. Passive probing has been used to analyze the behavior of different TCP implementations <ref> [11, 12] </ref>. One disadvantage to passive filtering is that because the filter is not located at the same place in the protocol stack (or even necessarily on the same machine) as the target protocol, vantage point effects can occur [12]. <p> Although vantage point differences do exist, it is possible to account for them, as shown in [12]. Another disadvantage to passive filtering is that, by definition, it cannot manipulate any packets which travel on the network. In <ref> [11] </ref>, which analyzed performance of several vendor TCP implementations, passive filtering was used to collect data regarding what messages the TCP implementations exchanged. It was not possible, however, for any control to be exercised over the messages.
Reference: [12] <author> V. Paxson, </author> <title> "Automated packet trace analysis of TCP implementations," </title> <booktitle> in Proc. of ACM SIGCOMM, </booktitle> <pages> pp. 167-179, </pages> <address> Cannes, France, </address> <month> September </month> <year> 1997. </year> <month> 138 </month>
Reference-contexts: Network filtering is also used to collect data for analysis of the the behavior of the Transmission Control Protocol (TCP) [9, 10] in the presence of network faults <ref> [11, 12] </ref>. Finally, approaches such as the Packet Shell [13] allow system 4 designers to generate network traffic in order to test other protocol participants. The relation between orchestra and these three areas of study is presented in this subsection. <p> For example, active probing does not allow the generation of network delays or message 6 reordering. A recently developed tool, called tcpanaly <ref> [12] </ref>, provides a mechanism for analyzing traces of TCP behavior generated by the tcpdump utility [14]. tcpanaly employs various methods for coping with packet filter measurement errors, ambiguous data due to network distance between the measurement point and the participant TCP, and a large range in the behavior of different TCP <p> Passive filters may not even run on the same processor as the target protocol; they may simply be connected to the same network. Passive probing has been used to analyze the behavior of different TCP implementations <ref> [11, 12] </ref>. One disadvantage to passive filtering is that because the filter is not located at the same place in the protocol stack (or even necessarily on the same machine) as the target protocol, vantage point effects can occur [12]. <p> One disadvantage to passive filtering is that because the filter is not located at the same place in the protocol stack (or even necessarily on the same machine) as the target protocol, vantage point effects can occur <ref> [12] </ref>. Vantage point refers to differences between target protocol and packet filter views of the network traffic. <p> Although vantage point differences do exist, it is possible to account for them, as shown in <ref> [12] </ref>. Another disadvantage to passive filtering is that, by definition, it cannot manipulate any packets which travel on the network. In [11], which analyzed performance of several vendor TCP implementations, passive filtering was used to collect data regarding what messages the TCP implementations exchanged. <p> In such experiments, an active participant may be used on one participant, coupled with a passive filter located either on or near the peer machine. The passive filter provides a more accurate sequence of messages seen by the peer, although, as mentioned in <ref> [12] </ref>, vantage point differences are possible even when the passive filter resides on the same machine as the protocol participant. These differences occur because of variances in timing of traffic processing between the passive filter and the protocol participant. <p> In some cases, such tools may already exist; wherever possible, orchestra output would be filtered through these tools rather than duplicating the work of others. One possibility would be generating output that a tool such as tcpanaly <ref> [12] </ref> could interpret and analyze. Chapter 4 discussed some of the problems associated with testing real-time protocols. In particular, the intrusiveness of the fault injection mechanism must be both quantified and compensated for, so that it does not perturb the timing of the target protocol.
Reference: [13] <author> C. Schmechel and S. Parker. </author> <title> The Packet Shell. Presented at IETF TCP Working Group Meeting, </title> <note> slides available at ftp://playground.sun.com/pub/sparker/psh-ietf-pres.fm.ps, </note> <month> December </month> <year> 1996. </year>
Reference-contexts: Network filtering is also used to collect data for analysis of the the behavior of the Transmission Control Protocol (TCP) [9, 10] in the presence of network faults [11, 12]. Finally, approaches such as the Packet Shell <ref> [13] </ref> allow system 4 designers to generate network traffic in order to test other protocol participants. The relation between orchestra and these three areas of study is presented in this subsection. <p> These tools are useful both for monitoring and analyzing network traffic. In contrast, the Packet Shell <ref> [13] </ref> (psh), developed at Sun Microsystems, is a tool for protocol development and testing that allows users to generate messages. psh is an extensible software toolset for protocol development and testing that allows the user to create connections at different layers in the protocol stack and to send and receive packets
Reference: [14] <author> V. Jacobson, C. Leres, and S. McCanne. tcpdump. </author> <note> Available via anonymous ftp to ftp.ee.lbl.gov, </note> <month> June </month> <year> 1989. </year>
Reference-contexts: For example, active probing does not allow the generation of network delays or message 6 reordering. A recently developed tool, called tcpanaly [12], provides a mechanism for analyzing traces of TCP behavior generated by the tcpdump utility <ref> [14] </ref>. tcpanaly employs various methods for coping with packet filter measurement errors, ambiguous data due to network distance between the measurement point and the participant TCP, and a large range in the behavior of different TCP implementations. tcpanaly can be tuned to recognize a particular TCP implementation and has already been <p> Applications such as tcpdump <ref> [14] </ref> use packet filters [4-6] for accessing network traffic and can be configured to collect packets that match certain characteristics. Passive filtering is similar to using a hardware logic probe to snoop on a data bus. The use of a passive software filter is completely non-intrusive to the network traffic.
Reference: [15] <author> J. A. Clark and D. K. Pradhan, </author> <title> "Fault injection: A method for validating computer-system dependability," </title> <booktitle> IEEE Computer, </booktitle> <pages> pp. 47-56, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Two recent survey papers <ref> [15, 16] </ref> present fault injection studies and discuss different tools that facilitate the application of fault injection in various environments.
Reference: [16] <author> M.-C. Hsueh, T. K. Tsai, and R. K. Iyer, </author> <title> "Fault injection techniques and tools," </title> <booktitle> IEEE Computer, </booktitle> <pages> pp. 75-82, </pages> <month> April </month> <year> 1997. </year>
Reference-contexts: Two recent survey papers <ref> [15, 16] </ref> present fault injection studies and discuss different tools that facilitate the application of fault injection in various environments.
Reference: [17] <author> K. G. Shin and Y. H. Lee, </author> <title> "Measurement and application of fault latency," </title> <journal> IEEE Trans. Computers, </journal> <volume> vol. C-35, no. 4, </volume> <pages> pp. 370-375, </pages> <month> April </month> <year> 1986. </year>
Reference-contexts: The work on fault injection can be broken down roughly into three areas: hardware fault injection and software simulation of hardware faults, system and protocol simulation techniques, and communication protocol fault injection. These areas are presented below. 8 Hardware Fault Injection and Software Simulation of Hardware Faults Hardware fault-injection <ref> [1, 17] </ref> and simulation approaches for injecting hardware failures [18] have received much attention in the past. Shin and Lee [17] inject hardware faults of stuck-at-0, stuck-at-1, and inverted signal at the pin level into the hardware for the Fault-Tolerant Multiprocessor (FTMP) at the NASA Airlab. <p> These areas are presented below. 8 Hardware Fault Injection and Software Simulation of Hardware Faults Hardware fault-injection [1, 17] and simulation approaches for injecting hardware failures [18] have received much attention in the past. Shin and Lee <ref> [17] </ref> inject hardware faults of stuck-at-0, stuck-at-1, and inverted signal at the pin level into the hardware for the Fault-Tolerant Multiprocessor (FTMP) at the NASA Airlab. They introduce an indirect method for separating fault and error latencies, based on the distribution of the fault latency.
Reference: [18] <author> E. Czeck and D. Siewiorek, </author> <title> "Effects of transient gate-level faults on program be-haviour," </title> <booktitle> in Proc. International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pp. 236-243. </pages> <publisher> IEEE, </publisher> <year> 1990. </year>
Reference-contexts: These areas are presented below. 8 Hardware Fault Injection and Software Simulation of Hardware Faults Hardware fault-injection [1, 17] and simulation approaches for injecting hardware failures <ref> [18] </ref> have received much attention in the past. Shin and Lee [17] inject hardware faults of stuck-at-0, stuck-at-1, and inverted signal at the pin level into the hardware for the Fault-Tolerant Multiprocessor (FTMP) at the NASA Airlab. <p> In these tests, failures were injected at the hardware level to test the properties of the system in the presence of the faults. Gate-level faults are simulated to study their effects on program behavior on the IBM RT PC platform <ref> [18] </ref>. One-cycle inversion faults are injected into 10 key CPU locations for the entire execution of a matrix multiplication and a recursive Fibonacci program.
Reference: [19] <author> P. Verissimo, L. Rodrigues, and M. Batista, </author> <title> "Amp: A highly parallel atomic multicast protocol," </title> <booktitle> in Proc. of ACM SIGCOMM, </booktitle> <pages> pp. 83-93, </pages> <address> Austin, TX, </address> <month> September </month> <year> 1990. </year>
Reference-contexts: Faults may be stuck-at, open, and bridging faults, among others. Messaline was used to test a prototype of a computerized interlocking system for railway control applications. Messaline has been used to test the AMp atomic multicast protocol <ref> [1, 19] </ref>. In these tests, failures were injected at the hardware level to test the properties of the system in the presence of the faults. Gate-level faults are simulated to study their effects on program behavior on the IBM RT PC platform [18].
Reference: [20] <author> R. Chillarege and N. S. Bowen, </author> <title> "Understanding large system failures | a fault injection experiment," </title> <booktitle> in Proc. Int'l Symp. on Fault-Tolerant Computing, </booktitle> <pages> pp. 356-363, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: Software fault injection has been used for inserting faults into system memory for emulating hardware errors. In the failure acceleration technique, the transition from an injected fault into a realized error occurs more quickly than usual <ref> [20] </ref>. This technique allows experiments to be run without wasting time waiting for faults to manifest themselves. In the experiments, pages of memory are written with faulty values, emulating the effects of a software overlay, which occurs when a program writes into an incorrect storage area.
Reference: [21] <author> Z. Segall et al., </author> <title> "Fiat fault injection based automated testing environment," </title> <booktitle> in FTCS-18, </booktitle> <pages> pp. 102-107, </pages> <year> 1988. </year>
Reference-contexts: Partial failures occurred in 33% of the experiments, meaning that some service was disrupted, but the primary service (the transaction system) was not affected. Overall, 60% of the faults caused were candidates for failure prevention or error repair. 9 The FIAT system <ref> [21] </ref> uses software control and hardware emulation to evaluate depend-ability of fault-tolerant distributed systems. Software fault injection is used to set and clear bytes of memory in program images. These programs execute on a network of machines configured to model a particular system architecture.
Reference: [22] <author> G. A. Kanawati, N. A. Kanawati, and J. A. Abraham, "FERRARI: </author> <title> A flexible software-based fault and error injection system," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 44, no. 2, </volume> <pages> pp. 248-260, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: These programs execute on a network of machines configured to model a particular system architecture. FIAT has been used to measure coverage and latency, classify failures, and investigate the effects of fault type and workload on these metrics. FERRARI <ref> [22] </ref> is a software fault injection system based on a process model. In the system, faults are injected into a target process by a controlling process. The controlling process creates the target process and waits for either a timing event or a software trap.
Reference: [23] <author> S. Han, K. G. Shin, and H. A. Rosenberg, "DOCTOR: </author> <title> an integrateD sOftware fault injeCTiOn enviRonment for distributed real-time systems," </title> <booktitle> in Proceedings of the IEEE International Computer Performance and Dependability Symposium, </booktitle> <pages> pp. 204-213, </pages> <address> Er-langen, Germany, </address> <year> 1995. </year>
Reference-contexts: FERRARI has been used to evaluate the effectiveness of several concurrent error-detection techniques embedded in application software. The DOCTOR system <ref> [23] </ref> runs on the HARTS real-time multicomputer platform. It allows the user to inject memory, CPU, and communication faults into the system. Communication faults are injected according to probability distributions. There are two main differences between orchestra and DOCTOR.
Reference: [24] <author> K. K. Goswami, R. K. Iyer, and L. Young, </author> <title> "DEPEND: A simulation-based environment for system level dependability analysis," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 46, no. 1, </volume> <pages> pp. 60-74, </pages> <month> January </month> <year> 1997. </year>
Reference-contexts: This provides a much more powerful tool than simply allowing for probabilistic fault generation alone. System and Protocol Simulation Techniques Fault injection into simulated systems has also been used for system-level testing. The DEPEND <ref> [24, 25] </ref> environment presents a software simulation environment for performing functional and dependability analysis. Fault injection is performed at the interface between simulated functional modules. The DEPEND simulation environment employs acceleration techniques to speed simulation time.
Reference: [25] <author> K. Goswami and R. Iyer, </author> <title> "Simulation of software behaviour under hardware faults," </title> <booktitle> in Proc. International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pp. 218-227. </pages> <publisher> IEEE, </publisher> <year> 1993. </year>
Reference-contexts: This provides a much more powerful tool than simply allowing for probabilistic fault generation alone. System and Protocol Simulation Techniques Fault injection into simulated systems has also been used for system-level testing. The DEPEND <ref> [24, 25] </ref> environment presents a software simulation environment for performing functional and dependability analysis. Fault injection is performed at the interface between simulated functional modules. The DEPEND simulation environment employs acceleration techniques to speed simulation time.
Reference: [26] <author> G. A. Alvarez and F. Cristian, </author> <title> "Centralized failure injection for distributed, fault-tolerant protocol testing," </title> <booktitle> in Proc. Int. Conf. on Distributed Computer Systems, </booktitle> <pages> pp. 78-85, </pages> <address> Baltimore, Maryland, </address> <month> May </month> <year> 1997. </year>
Reference-contexts: Communication faults are as in a noisy channel, with bit corruption in messages and destruction of messages altogether. I/O and memory faults are specified either as bits flipped within a memory word 10 or as a raised flag indicating the fault. The CESIUM <ref> [26] </ref> tool for testing distributed protocols allows for testing of distributed protocols written in Java. In this system, all participants in a protocol run inside a simulator on one machine. <p> The CESIUM user provides lower protocol layer services that inject appropriate faults as messages are sent and received during an experiment. An important advantage of the CESIUM approach is repeatability. Since everything is simulated, experiments can be repeated with accuracy. However, although <ref> [26] </ref> proved equivalence between CESIUM and the real system being evaluated, situations arise in distributed systems that may not occur in a simulated environment. In addition, because CESIUM allows only for testing of protocols written in Java, its applicability to software not written in Java is limited.
Reference: [27] <author> D. B. Ingham and G. D. Parrington, "Delayline: </author> <title> A Wide-Area Network Emulation Tool," </title> <journal> Computing Systems, </journal> <volume> vol. 7, no. 3, </volume> <pages> pp. 313-332, </pages> <month> Summer </month> <year> 1994. </year> <month> 139 </month>
Reference-contexts: However, there are situations in which a protocol run depends on timing rather than solely on messages. It is difficult for orchestra to construct repeatable experiments in these situations because timing may not be finely controlled, as in CESIUM. Communication Protocol Fault Injection The Delayline <ref> [27] </ref> tool and the EFA fault injection environment [28] are closest to the orchestra approach described here. Delayline allows the user to introduce delays into user-level protocols.
Reference: [28] <author> K. Echtle and M. Leu, </author> <title> "The EFA Fault Injector for Fault-Tolerant Distributed System Testing," </title> <booktitle> in Workshop on Fault-Tolerant Parallel and Distributed Systems, </booktitle> <pages> pp. 28-35. </pages> <publisher> IEEE, </publisher> <year> 1992. </year>
Reference-contexts: It is difficult for orchestra to construct repeatable experiments in these situations because timing may not be finely controlled, as in CESIUM. Communication Protocol Fault Injection The Delayline [27] tool and the EFA fault injection environment <ref> [28] </ref> are closest to the orchestra approach described here. Delayline allows the user to introduce delays into user-level protocols. The tool is used mainly for emulating a wide-area network in a local network development environment and allows the user to specify delays on certain paths that the application is using. <p> Hence, deterministic control on ordering of certain concurrent messages is necessary. Finally, orchestra focuses on real-time as well as fault tolerance characteristics of distributed protocols. The EFA fault injector <ref> [28] </ref> inserts a fault injection layer below the fault-tolerant target protocol layer. This approach differs from orchestra's in two ways.
Reference: [29] <author> K. Echtle and Y. Chen, </author> <title> "Evaluation of deterministic fault injection for fault-tolerant protocol testing," </title> <booktitle> in Proc. Int'l Symp. on Fault-Tolerant Computing, </booktitle> <pages> pp. 418-425. </pages> <publisher> IEEE, </publisher> <year> 1991. </year>
Reference-contexts: However, the tool can be used only for delay injection; it is not intended for more general packet manipulation or injection of new messages. An important area of fault injection research is the generation of the test sequences used to test a system <ref> [29, 30] </ref>. Avresky et al. propose a framework to generate test sequences for identifying design and implementation faults in complex fault-tolerant protocols. These test sequences are applied to a simulation of the fault tolerance mechanism being tested. <p> Avresky et al. propose a framework to generate test sequences for identifying design and implementation faults in complex fault-tolerant protocols. These test sequences are applied to a simulation of the fault tolerance mechanism being tested. Echtle and Chen <ref> [29] </ref> aim at designing test sequences for evaluating the design error coverage, which is the probability of revealing an existing design error.
Reference: [30] <author> D. Avresky, J. Arlat, J. Laprie, and Y. Crouzet, </author> <title> "Fault injection for the formal testing of fault tolerance," </title> <booktitle> in Proc. Int'l Symp. on Fault-Tolerant Computing, </booktitle> <pages> pp. 345-354. </pages> <publisher> IEEE, </publisher> <year> 1992. </year>
Reference-contexts: However, the tool can be used only for delay injection; it is not intended for more general packet manipulation or injection of new messages. An important area of fault injection research is the generation of the test sequences used to test a system <ref> [29, 30] </ref>. Avresky et al. propose a framework to generate test sequences for identifying design and implementation faults in complex fault-tolerant protocols. These test sequences are applied to a simulation of the fault tolerance mechanism being tested.
Reference: [31] <institution> OSF RI MK 7.2 Release Notes, OSF Reasearch Institute Real-Time Group, </institution> <month> October </month> <year> 1996. </year>
Reference-contexts: Second, the EFA fault injection layer is fixed at the data link layer, whereas the orchestra fault injection layer can be placed at any layer in the protocol stack. In fact, the implementation of orchestra on the MK 7.2 <ref> [31] </ref> microkernel can be placed between any two protocol layers simply by changing a protocol specification file. Recently, work on the EFA fault injector has concentrated on automatic generation of fault cases to be injected [32]. <p> In particular, experiments performed using orchestra on commercial TCP implementations resulted in interaction with TCP vendors in the form of feedback about their implementations. In addition, the orchestra fault injection toolset implemented on the MK 7.2 operating system <ref> [31] </ref> has been transferred to TOG RI. It is used internally to test protocols developed there and is planned for release with a later version of the MK operating system. <p> The thread that performs all communication protocol processing is bound to a capacity reserve, ensuring that processor capacity will be available at periodic intervals for processing network traffic. 4.2.2 CORDS Protocol Framework The MK 7.2 microkernel <ref> [31] </ref> from The Open Group Research Institute (TOG RI) contains a facility for building real-time communication protocol stacks called the Communication Objects for Real-time Dependable Systems (CORDS) [33]. <p> in specifying fault injection scripts tailored to a specific protocol. 105 USERCMDS_SECTION CmdName: random_num TclName: random_num CmdName: random_seed TclName: random_seed CmdName: sub_times TclName: sub_times END_USERCMDS_SECTION 6.3 Orchestra Implementation for Multilayer Protocol Test ing An implementation of orchestra has been developed within the CORDS [33] framework on the MK 7.2 microkernel <ref> [31] </ref> from TOG RI. This implementation uses the technique for fault injection of multilayer services presented in the previous section.
Reference: [32] <author> K. Echtle and M. Leu, </author> <title> "Test of fault tolerant distributed systems by fault injection," IEEE Fault Tolerant Parallel and Distributed Systems, </title> <month> June </month> <year> 1995. </year>
Reference-contexts: In fact, the implementation of orchestra on the MK 7.2 [31] microkernel can be placed between any two protocol layers simply by changing a protocol specification file. Recently, work on the EFA fault injector has concentrated on automatic generation of fault cases to be injected <ref> [32] </ref>. An attributed Petri net model is used to derive the fault cases by a reachability analysis. <p> The implementation of orchestra takes advantage of Real-Time Mach features to compensate for the intrusiveness of the fault injection layer for real-time applications, as described in Chapter 4. Echtle and Lee's Petri net model in <ref> [32] </ref> does allow for specification of real-time protocols. However, to our knowledge, the EFA work does not attempt to quantify or compensate for intrusiveness of the fault injector. 1.3 Research Contributions This dissertation introduces a framework for performing message-level fault injection of distributed protocols and applications.
Reference: [33] <author> E. F. Menze and F. Travostino, </author> <title> The CORDS Book, </title> <booktitle> Open Software Foundation Research Institute, </booktitle> <address> Cambridge, MA, 2.0 edition, </address> <month> September </month> <year> 1996. </year>
Reference-contexts: Release of orchestra The orchestra fault injection tool that was implemented within the Communication 1 The Open Group Research Institute is an international consortium of vendors, ISVs and end-user customers from industry, government, and academia, dedicated to the advancement of multi-vendor information systems. 14 Object for Real-Time Dependable Systems (CORDS) <ref> [33] </ref> framework from TOG RI is freely available to the public [34]. This tool is a reference platform, including both a portable fault injection core library and a protocol layer implementation for the CORDS framework that makes use of the fault injection core. <p> The socket-based orchestra fault injection tool provides a thread for both event support and message handling and is described in detail in Chapter 4. Many protocol stack frameworks do provide upcalls to higher layers when messages arrive. The Communication Object for Real-Time Dependable Systems (CORDS) framework <ref> [33] </ref>, based on the x-kernel [2], is an example of such a system. In CORDS, shepherd threads carry each arriving message up the protocol stack. <p> reserve, ensuring that processor capacity will be available at periodic intervals for processing network traffic. 4.2.2 CORDS Protocol Framework The MK 7.2 microkernel [31] from The Open Group Research Institute (TOG RI) contains a facility for building real-time communication protocol stacks called the Communication Objects for Real-time Dependable Systems (CORDS) <ref> [33] </ref>. CORDS is based on the x-kernel [2] framework developed at the University of Arizona, but it contains significant extensions for controlled resource allocation. The primary contribution of the CORDS 60 framework is the provision of paths [42], which are per-session resource pools. <p> the Tcl interpreter gives them considerable power in specifying fault injection scripts tailored to a specific protocol. 105 USERCMDS_SECTION CmdName: random_num TclName: random_num CmdName: random_seed TclName: random_seed CmdName: sub_times TclName: sub_times END_USERCMDS_SECTION 6.3 Orchestra Implementation for Multilayer Protocol Test ing An implementation of orchestra has been developed within the CORDS <ref> [33] </ref> framework on the MK 7.2 microkernel [31] from TOG RI. This implementation uses the technique for fault injection of multilayer services presented in the previous section. <p> This fault injection layer can perform fault injection on all messages that the service sends and receives and can also collect appropriate information for evaluation of service performance. An implementation of orchestra within the Communication Object for Real-Time Dependable Systems (CORDS) <ref> [33] </ref> framework was used to test both functional and performance aspects of the multilayer Group Inter-Process Communication (GIPC) [42] service from Open Group Research Institute (TOG RI). The CORDS implementation of orchestra and the tests on the GIPC protocol suite impacted several industry groups, particularly TOG RI.
Reference: [34] <author> S. Dawson. </author> <note> Orchestra Toolkit. Available via anonymous ftp to rtcl.eecs.umich.edu, </note> <month> August </month> <year> 1997. </year>
Reference-contexts: within the Communication 1 The Open Group Research Institute is an international consortium of vendors, ISVs and end-user customers from industry, government, and academia, dedicated to the advancement of multi-vendor information systems. 14 Object for Real-Time Dependable Systems (CORDS) [33] framework from TOG RI is freely available to the public <ref> [34] </ref>. This tool is a reference platform, including both a portable fault injection core library and a protocol layer implementation for the CORDS framework that makes use of the fault injection core. The tool can be used as is to test other protocols that have been developed using CORDS. <p> The results of these experiments have been fed back to the protocol developers. Furthermore, this implementation of orchestra on MK 7.2 has been transferred to TOG RI and Honeywell Technology Center (HTC). Finally, the implementation of orchestra described in this chapter has been publicly released <ref> [34] </ref>. The remainder of this chapter is organized as follows. Section 6.2 presents two different methods of performing fault injection on multilayered protocols. <p> The CORDS 128 0.0 20.0 40.0 60.0 80.0 100.0 Packet loss rate (%) 0.0 100.0 200.0 Time to failure (s) 2/1 Heartbeat ratio 3/1 Heartbeat ratio 4/1 Heartbeat ratio 5/1 Heartbeat ratio 10/1 Heartbeat ratio cies and Packet Drop Probabilities 129 orchestra tool has also been publicly released <ref> [34] </ref>. Through using the CORDS orchestra implementation, several strengths and weaknesses of the tool have emerged. <p> The CORDS orchestra tool has also been publicly released <ref> [34] </ref>. This tool is a reference platform, including both the portable fault injection core library and a CORDS protocol layer implementation that makes use of the fault injection core. The tool can be used to test other protocols that have been developed within the CORDS framework.
Reference: [35] <author> S. Dawson, F. Jahanian, and T. Mitton, "ORCHESTRA: </author> <title> A fault injection environment for distributed systems," </title> <type> Technical Report CSE-TR-318-96, </type> <institution> University of Michigan, </institution> <address> Ann Arbor, MI, </address> <year> 1996. </year>
Reference-contexts: Scripts based on this model can be represented graphically, and a graphical script editor has been written which facilitates script specification. This script editor is presented in detail in a technical report <ref> [35] </ref>. messages. The code at the beginning of the script sets the initial state the first time the script is executed. The remainder of the script is a while loop representing the state machine. There are three states: initial, drop, and exit.
Reference: [36] <author> V. Jacobson, </author> <title> "Congestion avoidance and control," </title> <booktitle> in Proc. of ACM SIGCOMM, </booktitle> <pages> pp. 314-329, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: The RTO value for a TCP connection is calculated based on the measured round trip time (RTT) from the time each segment is sent until the acknowledgment (ACK) for the segment is received. RFC-1122 specifies that a TCP must use Jacobson's algorithm <ref> [36] </ref> for computing the RTO, coupled with Karn's algorithm [37] for selecting the RTT measurements. Karn's algorithm ensures that ambiguous round-trip times will not corrupt the calculation of the smoothed round-trip time.
Reference: [37] <author> P. Karn and C. Partridge, </author> <title> "Round trip time estimation," </title> <booktitle> in Proc. SIGCOMM 87, </booktitle> <address> Stowe, Vermont, </address> <month> August </month> <year> 1987. </year>
Reference-contexts: RFC-1122 specifies that a TCP must use Jacobson's algorithm [36] for computing the RTO, coupled with Karn's algorithm <ref> [37] </ref> for selecting the RTT measurements. Karn's algorithm ensures that ambiguous round-trip times will not corrupt the calculation of the smoothed round-trip time.
Reference: [38] <author> W. R. Stevens, </author> <title> TCP Illustrated, Volume 3: TCP for Transactions, HTTP, NNTP, and the UNIX Domain Protocols, </title> <publisher> Addison Wesley, </publisher> <year> 1996. </year>
Reference-contexts: Only Windows 95 drops the connection after five retransmissions of the zero-window probe are sent using exponential back-off 5 . The problem of zero-window probes being sent indefinitely has also been mentioned by Stevens <ref> [38] </ref>, whose book presents the code modification that appeared in 4.4BSD-Lite2 to fix the problem. 3.4.5 Message Reordering and Buffering This experiment examines how different TCP implementations deal with messages that are received out of order.
Reference: [39] <author> H. Tokuda, T. Nakajima, and P. Rao, </author> <title> "Real-time Mach: Towards a predictable real-time system," </title> <booktitle> in Proc. of USENIX Mach Workshop, </booktitle> <pages> pp. 1-10, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: The target system is not modified in this approach, so no perturbation of the system occurs. In testing real-time protocols, orchestra attempts to exploit operating system features of the target system whenever possible to quantify and to compensate for the intrusiveness of the fault injection mechanism. The Real-Time Mach <ref> [39] </ref> operating system, for example, provides an abstraction, called processor capacity reserve [40], which allows users to specify a processor reservation in terms of time units needed per time period. Such a facility can be employed directly by orchestra to request processing capacity for performing fault injection. <p> Synchronized clocks, which can be used to provide common views of time across a set of machines, are also discussed. 4.2.1 Real-Time Mach Capacity Reserve The Real-Time Mach operating system developed at Carnegie Mellon University <ref> [39] </ref> provides the processor capacity reserve abstraction [40]. Processor capacity reserve allows application threads to specify their CPU requirements in terms of their timing requirements. In other words, a process is able to reserve periodic computational power to ensure that it will get its work done.
Reference: [40] <author> C. W. Mercer, J. Zelenka, and R. Rajkumar, </author> <title> "On Predictable Operating System Protocol Processing," </title> <type> Technical Report CMU-CS-94-165, </type> <institution> Carnegie Mellon University, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: In testing real-time protocols, orchestra attempts to exploit operating system features of the target system whenever possible to quantify and to compensate for the intrusiveness of the fault injection mechanism. The Real-Time Mach [39] operating system, for example, provides an abstraction, called processor capacity reserve <ref> [40] </ref>, which allows users to specify a processor reservation in terms of time units needed per time period. Such a facility can be employed directly by orchestra to request processing capacity for performing fault injection. <p> Synchronized clocks, which can be used to provide common views of time across a set of machines, are also discussed. 4.2.1 Real-Time Mach Capacity Reserve The Real-Time Mach operating system developed at Carnegie Mellon University [39] provides the processor capacity reserve abstraction <ref> [40] </ref>. Processor capacity reserve allows application threads to specify their CPU requirements in terms of their timing requirements. In other words, a process is able to reserve periodic computational power to ensure that it will get its work done. <p> Furthermore, reserve parameters can be dynamically adjusted, subject to the admission policy of the kernel. Hence, timing behavior of application reserves can be monitored and changed dynamically. Processor capacity reserves have been used successfully to provide deterministic protocol processing on the Real-Time Mach platform, as described in <ref> [40] </ref>. By combining the use of a user-level socket library, which contains a thread for processing all socket traffic, and the processor capacity reserve, predictable processing can be achieved. <p> In Real-Time Mach, user-level sockets have been used in conjunction with the processor reserve abstraction to achieve predictable protocol processing <ref> [40] </ref>. Predictable processing is accomplished by binding the protocol processing thread to a processor reserve, ensuring that sufficient time is set aside for communication.
Reference: [41] <author> C. Lee, R. Rajkumar, and C. Mercer, </author> <title> "Experiences with processor reservation and dynamic qos in real-time mach," </title> <booktitle> in Proceedings of Multimedia Japan 96, </booktitle> <month> April </month> <year> 1996. </year>
Reference-contexts: Section 4.3 presents several techniques that are useful in testing real-time protocols and applications. Section 4.4 discusses the implementation of orchestra for testing socket-based applications. Section 4.5 presents the results of a set of experiments that were performed on RT-Phone, a real-time audio-conferencing application from Carnegie Mellon University <ref> [41] </ref>. Section 4.6 presents lessons learned from the experiments and implementation. 4.2 Real-Time Operating System Support Real-time operating systems often provide support for real-time communication. This support ranges from scheduling support for communication threads to hardware support for guaranteed networking delays. <p> These experiments illustrate that by measuring the intrusiveness of the fault injection layer, one can compensate for timing perturbation with significant accuracy. RT-Phone is a distributed teleconferencing application with a telephone-pad-like, Motif-based graphical interface <ref> [41] </ref>. A caller and a callee can establish a two-way audio connection across the network using the graphical user interface shown in Figure 4.2. <p> When the reservation period decreases (more frequent sampling and transmission of audio data), the end-to-end delay decreases proportionally and the CPU load increases nonlinearly <ref> [41] </ref>. Stage 1 Stage 2 Processor 1 Stage 5Stage 4 Processor 2 Stage 3 Network Delay T TTT one host to another. First, in Stage 1, the audio input from the microphone is sampled by the audio driver in the kernel. <p> 4T + D N . 5 ) A typical value for T is 16 ms for a connection with 16KHz sampling and 5 The reason being that the data begins playing on the speaker in Stage 5, so Stage 5 is not part of the end-to-end delay. 68 1 byte/sample <ref> [41] </ref>. The list of experiments conducted on RT-Phone follows: Performance measurement of network delay component of end-to-end delay In this experiment, orchestra was used to measure the network delay component of the end-to-end delay shown in Figure 4.4.
Reference: [42] <author> F. Travostino, E. Menze, and F. Reynolds, </author> <title> "Paths: Programming with system resources in support of real-time distributed applications," </title> <booktitle> in Proc. of IEEE Workshop on Object-Oriented Real-Time Dependable Systems, </booktitle> <pages> pp. 36-45, </pages> <address> Laguna Beach, CA, </address> <month> February </month> <year> 1996. </year>
Reference-contexts: CORDS is based on the x-kernel [2] framework developed at the University of Arizona, but it contains significant extensions for controlled resource allocation. The primary contribution of the CORDS 60 framework is the provision of paths <ref> [42] </ref>, which are per-session resource pools. Each path provides its own dynamically allocated memory, input buffers, and input threads for shep-herding messages up the protocol stack. Paths are typically coupled with allocators, which provide a common allocation interface to different kinds of memory pools. <p> An implementation of orchestra within the Communication Object for Real-Time Dependable Systems (CORDS) [33] framework was used to test both functional and performance aspects of the multilayer Group Inter-Process Communication (GIPC) <ref> [42] </ref> service from Open Group Research Institute (TOG RI). The CORDS implementation of orchestra and the tests on the GIPC protocol suite impacted several industry groups, particularly TOG RI.
Reference: [43] <author> F. Cristian, </author> <title> "Probabilistic clock synchronization," </title> <journal> Distributed Computing, </journal> <volume> vol. 4, no. 3, </volume> <pages> pp. 146-158, </pages> <year> 1989. </year>
Reference-contexts: This facility will allow protocols developed for the CORDS framework on the MK kernel to reserve capacity for the threads associated with CORDS paths, as was done in the Real-Time Mach system with user-level protocol processing. 4.2.3 Synchronized Clocks Many distributed real-time systems provide clock synchronization services <ref> [43, 44] </ref>. Synchronized clocks provide processes running on different machines with access to a clock value that is the same (within the variance of the synchronization algorithm) across all machines in the system.
Reference: [44] <author> A. S. Tannenbaum, </author> <title> "Synchronization in distributed systems," </title> <booktitle> in Distributed Operating Systems. </booktitle>
Reference-contexts: This facility will allow protocols developed for the CORDS framework on the MK kernel to reserve capacity for the threads associated with CORDS paths, as was done in the Real-Time Mach system with user-level protocol processing. 4.2.3 Synchronized Clocks Many distributed real-time systems provide clock synchronization services <ref> [43, 44] </ref>. Synchronized clocks provide processes running on different machines with access to a clock value that is the same (within the variance of the synchronization algorithm) across all machines in the system.
Reference: [45] <author> A. Sah and J. Blow, </author> <title> "A compiler for the Tcl language," </title> <booktitle> in Proceedings of the Tcl'93 Workshop, </booktitle> <address> Berkeley, California, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: Standard deviations on these measurements were 35 s and 36 s respectively. The fact that different scripts have execution times differences on the order of hundreds of microseconds is due mainly to the fact that Tcl is interpreted. Work presented in <ref> [45] </ref> addresses this problem by providing a version of Tcl that accepts compiled scripts as input. Speed increases of 8-12 times over the interpreted case were presented. Such increases would result in measurement differences on the order of tens of microseconds for the scripts used in this experiment. <p> It may be absolutely necessary to use compiled code for speed purposes, but because changing test cases will require recompilation of the fault injector, other solutions should be considered first. Two promising avenues exist: compiled Tcl <ref> [45] </ref> and `C [46].
Reference: [46] <author> D. R. Engler, W. C. Hsieh, and M. F. Kaashoek, </author> <title> "`C: A language for high-level, efficient, and machine-independent dynamic code generation," </title> <booktitle> in Proceedings of the 23rd Annual ACM Symposium on Principles of Programming Languages, </booktitle> <address> St. Petersburg, Florida, </address> <month> January </month> <year> 1996. </year>
Reference-contexts: However, the use of C, which must be compiled and relinked into the application or protocol stack, must be weighed against the flexibility gained by using an interpreted language such as Tcl. Another possibility is to use the `C (tick-C) language. `C <ref> [46] </ref> allows code to be generated dynamically and has been used to build very fast packet filtering agents [8]. `C would give users flexibility to change scripts without recompiling, while achieving excellent performance. 72 CHAPTER 5 Fault Injection of Multiparticipant Protocols 5.1 Introduction Performing fault injection on protocols with multiple participants <p> Rebooting can be avoided by finding another interpreted (or byte-compiled) language suitable for describing fault injection experiments that could be embedded in the kernel. Dynamic code generation techniques, such as the `C language <ref> [46] </ref>, may be applicable here and are a subject of future work, as mentioned in Chapter 7. 6.4 Experiments on a Group Interprocess-Communication Mechanism The Open Group Research Institute has developed a suite of services, called Group Inter-Process Communication (GIPC), within the CORDS framework. <p> It may be absolutely necessary to use compiled code for speed purposes, but because changing test cases will require recompilation of the fault injector, other solutions should be considered first. Two promising avenues exist: compiled Tcl [45] and `C <ref> [46] </ref>.
Reference: [47] <author> S. Deering, "RFC-1122: </author> <title> Host extensions for ip multicasting," Request for Comments, </title> <month> August </month> <year> 1989. </year>
Reference-contexts: Tunneling has been used in other network protocols, such as the Internet Protocol (IP). The idea behind tunneling is to take a packet of a given protocol and encapsulate it by sending it across a different connection. This technique has been used in IP multicast <ref> [47] </ref>, for example, to encapsulate IP multicast packets inside regular IP packets. In this manner, IP multicast traffic can be sent across networks that may contain routers and subnets that cannot process IP multicast traffic.
Reference: [48] <author> D. L. Tennenhouse and D. J. Wetherall, </author> <title> "Toward an active network architecture," </title> <journal> Computer Communication Review, </journal> <volume> vol. 26, no. 2, </volume> <pages> pp. 5-18, </pages> <month> April </month> <year> 1996. </year>
Reference-contexts: Per-Message Action It is sometimes desirable to perform specific actions on messages as they are delivered. Using fault injection control headers, messages can be modified to contain commands that run when the message is received. This technique is similar to the idea of active networks <ref> [48] </ref>. In the most general case, these messages simply contain a Tcl script fragment run by the fault injector on the receiving machine. Such control messages can even be encoded to 83 perform actions differently on different machines.
Reference: [49] <author> K. Birman, A. Schiper, and P. Stephenson, </author> <title> "Lightweight causal and atomic group multicast," </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> vol. 9, </volume> <pages> pp. 272-314, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: This technique can be used in conjunction with reordering of messages using a fault injection layer. Causal multicast order delivery checking A more complex example is delivery semantic checking on a causal multicast protocol. The Isis <ref> [49] </ref> system, for example, contains a multicast protocol that ensures causal order on the messages that are multicast to a group. To test such a protocol, it is necessary to add information inside a subscriber filtering agent when messages are sent. <p> In both experiment variations, the fact that two master sequencers are able to pick the same group incarnation number seemed to be a serious problem. Many other group protocols <ref> [49, 50] </ref> use a group identifier that depends both on the value of a counter and on a fixed value unique to each group member, such as its network address.
Reference: [50] <author> F. Jahanian, R. Rajkumar, and S. Fakhouri, </author> <title> "Processor group membership protocols: Specification, </title> <booktitle> design and implementation," in Proceedings of the 12th Symposium on Reliable Distributed Systems, </booktitle> <pages> pp. 2-11, </pages> <address> Princeton, New Jersey, </address> <month> October </month> <year> 1993. </year>
Reference-contexts: causally precedes another is delivered by the target protocol after that message, the vector timestamp will reveal the delivery error. 5.4 Experiments on a Group Membership Protocol As an example of testing multiparticipant protocols, a set of experiments was performed on a prototype implementation of the strong group membership protocol <ref> [50] </ref>. The experiments were run using a version of the socket-based orchestra tool described in Chapter 4 that was ported to the Solaris operating system. In a distributed environment, a collection of processes (or processors) can be grouped together to provide a service. <p> A detailed exposition of this problem is beyond the scope of this dissertation. The strong group membership protocol, as described in <ref> [50] </ref>, ensures that membership changes are seen in the same order by all members. In this protocol, a group of processors have a unique leader based on the processor identifier of each member. <p> In both experiment variations, the fact that two master sequencers are able to pick the same group incarnation number seemed to be a serious problem. Many other group protocols <ref> [49, 50] </ref> use a group identifier that depends both on the value of a counter and on a fixed value unique to each group member, such as its network address.
Reference: [51] <author> F. Cristian, </author> <title> "Reaching agreement on processor-group membership in synchronous distributed systems," </title> <journal> Distributed Computing, </journal> <volume> vol. 4, no. 4, </volume> <pages> pp. 175-188, </pages> <year> 1991. </year>
Reference: [52] <author> A. M. Ricciardi and K. P. Birman, </author> <title> "Using process groups to implement failure detection in asynchronous environments," </title> <booktitle> in Proceedings of the 11th ACM Symposium on Principles of Distributed Computing, </booktitle> <address> Montreal, Quebec, </address> <month> August </month> <year> 1991. </year>
Reference: [53] <author> S. Mishra, L. L. Peterson, and R. D. Schlichting, </author> <title> "A membership protocol based on partial order," </title> <booktitle> in Second Working Conference on Dependable Computing for Critical Applications, </booktitle> <month> February </month> <year> 1990. </year>
Reference: [54] <author> J. K. Ousterhout, </author> <title> "Tcl: An embeddable command language," </title> <booktitle> in Winter USENIX Conference, </booktitle> <pages> pp. 133-146, </pages> <month> January </month> <year> 1990. </year> <month> 141 </month>
Reference-contexts: The orchestra fault injection library makes calls to routines that are registered with it, allowing it to create and schedule events. The CORDS orchestra implementation provides wrappers around the CORDS event routines that are registered with the fault injection library. Currently, because orchestra uses Tcl <ref> [54] </ref>, it can only be used to test protocols executing in a user-level CORDS server outside the kernel. However, because CORDS provides a migration path for protocols to move from user- to kernel-level, many protocols are typically developed at user-level.
References-found: 55

