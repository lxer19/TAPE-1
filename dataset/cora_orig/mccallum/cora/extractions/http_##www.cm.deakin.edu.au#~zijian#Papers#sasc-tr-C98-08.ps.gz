URL: http://www.cm.deakin.edu.au/~zijian/Papers/sasc-tr-C98-08.ps.gz
Refering-URL: http://www.cm.deakin.edu.au/~zijian/publications.html
Root-URL: 
Title: Stochastic Attribute Selection Committees  
Author: Zijian Zheng and Geoffrey I. Webb 
Keyword: machine learning, decision tree learning, inductive learning, committee learning, datamining  
Address: Geelong Victoria 3217, Australia  
Affiliation: School of Computing and Mathematics Deakin University,  
Pubnum: Technical Report (TR C98/08)  
Email: fzijian,webbg@deakin.edu.au  
Date: March, 1998  
Abstract: Classifier committee learning methods generate multiple classifiers to form a committee by repeatedly applying a single base learning algorithm. The committee members vote to decide the final classification. Two such methods, Bagging and Boosting, have shown great success with decision tree learning. They create different classifiers by modifying the distribution of the training set. This paper studies a different approach: the Stochastic Attribute Selection Committee learning method with decision tree learning. It generates classifier committees by stochastically modifying the set of attributes but keeping the distribution of the training set unchanged. An empirical evaluation of a variant of this method, namely Sasc, in a representative collection of natural domains shows that the SASC method can significantly reduce the error rate of decision tree learning. On average Sasc is more accurate than Bagging and less accurate than Boosting, although a one-tailed sign-test fails to show that these differences are significant at a level of 0.05. In addition, it is found that, like Bagging, Sasc is more stable than Boosting in terms of less frequently obtaining significantly higher error rates than C4.5 and obtaining lower error rate increases over C4.5. Moreover, like Bagging, Sasc is amenable to parallel and distributed processing while Boosting is not.
Abstract-found: 1
Intro-found: 1
Reference: <author> Ali, K.M.: </author> <title> Learning Probabilistic Relational Concept Descriptions. </title> <type> PhD. Thesis, </type> <institution> Dept of Info. and Computer Science, Univ. of California, </institution> <address> Irvine (1996). </address>
Reference-contexts: networks by manually selecting attribute subsets (Cherkauer, 1996; Tumer and Ghosh, 1996), learning naive Bayesian classifier committees by randomly choosing attribute subsets (Zheng, 1998), creating Gaussian classifier committees by varying attribute sets (Asker and Maclin, 1997), and creating committees for first-order learning by adding random selection of conditions to FOIL <ref> (Ali and Pazzani, 1996) </ref>. Finally, dif 2 ferent base learning algorithms can be used for learning different classifiers in committees (Wolpert, 1992; Zhang, Mesirov, and Waltz, 1992).
Reference: <author> Ali, K.M. and Pazzani, M.J.: </author> <title> Error reduction through learning multiple descriptions. </title> <note> Machine Learning 24 (1996) 173-202. </note>
Reference-contexts: networks by manually selecting attribute subsets (Cherkauer, 1996; Tumer and Ghosh, 1996), learning naive Bayesian classifier committees by randomly choosing attribute subsets (Zheng, 1998), creating Gaussian classifier committees by varying attribute sets (Asker and Maclin, 1997), and creating committees for first-order learning by adding random selection of conditions to FOIL <ref> (Ali and Pazzani, 1996) </ref>. Finally, dif 2 ferent base learning algorithms can be used for learning different classifiers in committees (Wolpert, 1992; Zhang, Mesirov, and Waltz, 1992).
Reference: <author> Asker, L. and Maclin, R.: </author> <title> Ensembles as a sequence of classifiers. </title> <booktitle> Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence. </booktitle> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann (1997) 860-865. </publisher>
Reference-contexts: Kong, 1995; Ali, 1996), learning option trees (Buntine, 1990; Kohavi and Kunz, 1997), training a committee of neural networks by manually selecting attribute subsets (Cherkauer, 1996; Tumer and Ghosh, 1996), learning naive Bayesian classifier committees by randomly choosing attribute subsets (Zheng, 1998), creating Gaussian classifier committees by varying attribute sets <ref> (Asker and Maclin, 1997) </ref>, and creating committees for first-order learning by adding random selection of conditions to FOIL (Ali and Pazzani, 1996). Finally, dif 2 ferent base learning algorithms can be used for learning different classifiers in committees (Wolpert, 1992; Zhang, Mesirov, and Waltz, 1992).
Reference: <author> Bauer, E. and Kohavi, R.: </author> <title> An empirical comparison of voting classification algorithms: Bagging, Boosting, and variants. </title> <note> Submitted to Machine Learning (1998) (available at: http://reality.sgi.com/ronnyk/vote.ps.gz). </note>
Reference: <author> Breiman, L.: </author> <note> Bagging predictors. Machine Learning 24 (1996a) 123-140. </note>
Reference-contexts: If classifiers in a committee partition the instance space differently, and most points in the instance space are correctly covered by the majority of the committee, the committee has a lower error rate than the individual classifiers. Bagging <ref> (Breiman, 1996a) </ref> and Boosting (Schapire, 1990; Freund, 1996; Fre-und and Schapire, 1996a; 1996b; Schapire et al., 1997), as two representative methods of this type, can significantly decrease the error rate of decision tree learning (Quinlan, 1996; Freund and Schapire, 1996b; Bauer and Kohavi, 1998). <p> These weights are, then, renormalized so that they sum to m. where ff t = 1 2 ln ((1 * t )=* t ); d (x) = 1 if H t correctly classifies x and d (x) = 0 otherwise. The primary idea of Bagging <ref> (Breiman, 1996a) </ref> is to generate a committee of classifiers with each from a bootstrap sample of the original training set. Bag, our implementation of Bagging, uses C4.5 (Quinlan, 1993) as its base classifier learning algorithm. <p> The first one is using the probabilistic predictions produced by all H t s, without voting weights. The second one is using the categorical predictions provided by all H t s, without voting weights. This voting method corresponds to the method used in the original Bagging <ref> (Breiman, 1996a) </ref>. The other two methods are the same as these two but each tree H t is given a weight ff t for voting.
Reference: <author> Breiman, L.: </author> <note> Arcing classifiers. Technical Report (available at: http://www.stat. Berkeley.EDU/users/breiman/). </note> <institution> Department of Statistics, University of California, Berkeley, </institution> <address> CA (1996b). </address>
Reference: <author> Breiman, L., Friedman, J.H., Olshen, R.A., and Stone, C.J.: </author> <title> Classification And Regres sion Trees. </title> <address> Belmont, CA: </address> <publisher> Wadsworth (1984). </publisher>
Reference: <author> Buntine, W.: </author> <title> A Theory of Learning Classification Rules. </title> <type> PhD. Thesis, </type> <institution> School of Com puting Science, University of Technology, </institution> <address> Sydney (1990). </address>
Reference: <author> Chan, P., Stolfo, S., and Wolpert, D. (eds): </author> <title> Working Notes of AAAI Workshop on Integrating Multiple Learned Models for Improving and Scaling Machine Learning Algorithms (available at http://www.cs.fit.edu/~imlm/papers.html), Portland, </title> <booktitle> Oregon (1996). </booktitle>
Reference: <author> Cherkauer, K.J.: </author> <title> Human expert-level performance on a science image analysis task by a system using combined artificial neural networks. </title> <editor> Chan, P., Stolfo, S., and Wolpert, D. </editor> <title> (eds) Working Notes of AAAI Workshop on Integrating Multiple Learned Models for Improving and Scaling Machine Learning Algorithms (available at http://www.cs.fit.edu/~imlm/papers.html), Portland, </title> <address> Oregon (1996) 15-21. </address>
Reference: <author> Dietterich, T.G. and Bakiri, G.: </author> <title> Solving multiclass learning problems via error correcting output codes. </title> <note> Journal of Artificial Intelligence Research 2 (1995) 263-286. </note>
Reference-contexts: This makes Bagging appropriate for parallel machine learning and datamining. While much recent attention has focused on Boosting and Bagging, other classifier committee learning approaches have also been developed, including generating multiple trees by manually changing learning parameters (Kwok and Carter, 1990), error-correcting output codes <ref> (Dietterich and Bakiri, 1995) </ref>, generating different classifiers by randomizing the base learning process (Dietterich and Kong, 1995; Ali, 1996), learning option trees (Buntine, 1990; Kohavi and Kunz, 1997), training a committee of neural networks by manually selecting attribute subsets (Cherkauer, 1996; Tumer and Ghosh, 1996), learning naive Bayesian classifier committees by
Reference: <author> Dietterich, T.G. and Kong, E.B.: </author> <title> Machine learning bias, statistical bias, and statis tical variance of decision tree algorithms. </title> <type> Technical Report, </type> <institution> Dept of Computer Science, Oregon State University, Corvallis, </institution> <note> Oregon (1995) (available at ftp://ftp. cs.orst.edu/pub/tgd/papers/tr-bias.ps.gz). </note>
Reference-contexts: This makes Bagging appropriate for parallel machine learning and datamining. While much recent attention has focused on Boosting and Bagging, other classifier committee learning approaches have also been developed, including generating multiple trees by manually changing learning parameters (Kwok and Carter, 1990), error-correcting output codes <ref> (Dietterich and Bakiri, 1995) </ref>, generating different classifiers by randomizing the base learning process (Dietterich and Kong, 1995; Ali, 1996), learning option trees (Buntine, 1990; Kohavi and Kunz, 1997), training a committee of neural networks by manually selecting attribute subsets (Cherkauer, 1996; Tumer and Ghosh, 1996), learning naive Bayesian classifier committees by
Reference: <author> Dietterich, T.G.: </author> <note> Machine learning research. AI Magazine 18 (1997) 97-136. </note>
Reference: <author> Domingos, P.: </author> <title> Why does Bagging work? a Bayesian account and its implications. </title> <booktitle> Pro ceedings of the Third International Conference on Knowledge Discovery and Data Mining. </booktitle> <publisher> AAAI Press (1997) 155-158. </publisher>
Reference: <author> Freund, Y.: </author> <title> Boosting a weak learning algorithm by majority. </title> <note> Information and Compu tation 121 (1996) 256-285. </note>
Reference: <author> Freund, Y. and Schapire, R.E.: </author> <title> A decision-theoretic generalization of on-line learn ing and an application to Boosting. </title> <note> Unpublished manuscript (1996a) (available at http://www.research.att.com/~yoav). </note>
Reference: <author> Freund, Y. and Schapire, R.E.: </author> <title> Experiments with a new Boosting algorithm. </title> <booktitle> Proceed ings of the Thirteenth International Conference on Machine Learning. </booktitle> <address> San Fran-cisco, CA: </address> <note> Morgan Kaufmann (1996b) 148-156. 16 Kohavi, </note> <author> R.: </author> <title> A study of cross-validation and bootstrap for accuracy estimation and model selection. </title> <booktitle> Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann (1995) 1137-1143. </publisher>
Reference: <author> Kohavi, R. and Kunz, C.: </author> <title> Option decision trees with majority votes. </title> <booktitle> Proceedings of the Fourteenth International Conference on Machine Learning. </booktitle> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann (1997) 161-169. </publisher>
Reference: <author> Kwok, S.W. and Carter, C.: </author> <title> Multiple decision trees. </title> <editor> Schachter, R.D., Levitt, T.S., Kanal, L.N., and Lemmer, J.F. </editor> <booktitle> (eds) Uncertainty in Artificial Intelligence. </booktitle> <address> Elsevier Science (1990) 327-335. </address>
Reference-contexts: This makes Bagging appropriate for parallel machine learning and datamining. While much recent attention has focused on Boosting and Bagging, other classifier committee learning approaches have also been developed, including generating multiple trees by manually changing learning parameters <ref> (Kwok and Carter, 1990) </ref>, error-correcting output codes (Dietterich and Bakiri, 1995), generating different classifiers by randomizing the base learning process (Dietterich and Kong, 1995; Ali, 1996), learning option trees (Buntine, 1990; Kohavi and Kunz, 1997), training a committee of neural networks by manually selecting attribute subsets (Cherkauer, 1996; Tumer and Ghosh,
Reference: <author> Merz, C.J. and Murphy, </author> <title> P.M.: UCI Repository of machine learning databases [http://www.ics.uci.edu/~mlearn/MLRepository.html]. Irvine, </title> <institution> CA: Univ of Cali-fornia, Dept of Info and Computer Science (1997). </institution>
Reference-contexts: We will empirically compare these four voting methods later in this section. 3.2 Experimental Domains and Methods Forty natural domains from the UCI machine learning repository <ref> (Merz and Murphy, 1997) </ref> are used. They include all the domains used by Quinlan (1996) for studying Boosting and Bagging. Table 1 summarizes the characteristics of these domains, including dataset size, the number of classes, the number of continuous attributes, and the number of discrete attributes.
Reference: <author> Quinlan, J.R.: C4.5: </author> <title> Program for Machine Learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kauf mann (1993). </publisher>
Reference-contexts: Two variants of this method which have been proposed are Dietterich and Kong's (1995) randomization trees and Ali's (1996) decision tree ensembles. The former generates decision tree committees through repeatedly applying a base learning algorithm that is derived by modifying C4.5 <ref> (Quinlan, 1993) </ref> to choose randomly among the top 20 tests in terms of information gain ratio at each decision node. The latter employs a very similar technique. <p> Finally, we conclude with future work. 2 SASC for Decision Tree Learning During the growth of a decision tree, at each decision node, a decision tree learning algorithm searches for the best attribute to form a test based on some test selection functions <ref> (Quinlan, 1993) </ref>. The key idea of SASC is to vary the members of a decision tree committee by stochastic manipulation of the set of attributes available for selection at decision nodes. This creates decision trees that each partition the instance space differently. <p> OUTPUT: a pruned tree. C := the majority class in D RawTree := Grow-Tree-SAS (Att, D, C, P ) PrunedTree := Prune-Tree (RawTree, Att, D ) RETURN PrunedTree Fig. 1. The C4.5Sas decision tree learning algorithm We use C4.5 <ref> (Quinlan, 1993) </ref> with the modifications described below as the base classifier learning algorithm in our stochastic attribute selection committee learning algorithm, Sasc, although any conventional decision tree learning algorithm can be used. <p> When building a decision node, by default C4.5 uses the information gain ratio to search for the best attribute to form a test <ref> (Quinlan, 1993) </ref>. To force C4.5 to generate different trees using the same training set, we modify C4.5 by stochastic restriction of the attributes available for selection at a decision node. <p> The Sasc learning algorithm ple down to a leaf of the tree. The class distribution for the example is estimated using the proportion of the training examples of each class at the leaf, if the leaf is not empty. This is the same as C4.5 <ref> (Quinlan, 1993) </ref>. When the leaf contains no training examples, 3 C4.5 produces a class distribution with the labeled class of the leaf having the probability 1, and all other classes having the probability 0. In this case, Sasc is different from C4.5. <p> training set, and is defined in Equation 1 later. 4 These three voting methods perform either worse than or similarly to the method that we use here. 5 This issue will be addressed in Section 3.4. 3 For multi-branch trees created by C4.5, some leaves may contain no training instances <ref> (Quinlan, 1993) </ref>. 4 The weight of each training example at any trial t is 1 when computing ff t in Sasc. 5 Quinlan (1996) uses categorical predictions with the confidence with which a tree classifies a test instance as the weight of this tree for voting in the Boosted C4.5 algorithm. <p> Given a training set D consisting of m instances and an integer T, the number of trials, Boost builds T pruned trees over T trials by repeatedly invoking C4.5 <ref> (Quinlan, 1993) </ref>. Let w t (x) denote the weight of instance x in D at trial t. At the first trial, each instance has weight 1; that is, w 1 (x) = 1 for each x. <p> The primary idea of Bagging (Breiman, 1996a) is to generate a committee of classifiers with each from a bootstrap sample of the original training set. Bag, our implementation of Bagging, uses C4.5 <ref> (Quinlan, 1993) </ref> as its base classifier learning algorithm. Given a committee size T and a training set D consisting of m instances, Bag generates T 1 bootstrap samples with each being created by uniformly sampling m instances from D with replacement.
Reference: <author> Quinlan, J.R.: Bagging, </author> <title> Boosting, </title> <booktitle> and C4.5. Proceedings of the Thirteenth National Conference on Artificial Intelligence. </booktitle> <address> Menlo Park, CA: </address> <publisher> AAAI Press (1996) 725-730. </publisher>
Reference-contexts: At the current stage, the prediction accuracy is our primary concern about classifier committee learning. 3.1 The Comparison Algorithms: BOOST and BAG Boost is our implementation of the Boosting algorithm with decision tree learning. It follows the Boosted C4.5 algorithm (AdaBoost.M1) <ref> (Quinlan, 1996) </ref> but uses a new Boosting equation as shown in Equation 1, derived from Schapire et al. (1997). Given a training set D consisting of m instances and an integer T, the number of trials, Boost builds T pruned trees over T trials by repeatedly invoking C4.5 (Quinlan, 1993).
Reference: <author> Schapire, R.E.: </author> <title> The strength of weak learnability. </title> <note> Machine Learning 5 (1990) 197-227. </note>
Reference: <author> Schapire, R.E., Freund, Y., Bartlett, P., and Lee, </author> <title> W.S.: Boosting the margin: A new explanation for the effectiveness of voting methods. </title> <booktitle> Proceedings of the Fourteenth International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann (1997) 322-330. </publisher>
Reference: <author> Tumer, K. and Ghosh, J.: </author> <title> Error correction and error reduction in ensemble classifiers. </title> <note> Connection Science 8 (1996) 385-404. </note>
Reference: <author> Wolpert, D.H.: </author> <title> Stacked generalization. </title> <booktitle> Neural Networks 5 (1992) 241-259. </booktitle>
Reference: <author> Zhang, X., Mesirrov, J.P., and Waltz, </author> <title> D.L.: Hybrid system for protein secondary struc ture prediction. </title> <note> Journal of Molecular Biology 225 (1992) 1049-1063. </note>
Reference: <author> Zheng, Z.: </author> <title> Naive Bayesian classifier committees. </title> <booktitle> To appear in Proceedings of the Tenth European Conference on Machine Learning (ECML-98). </booktitle> <address> Berlin: </address> <month> Springet-Verlag </month> <year> (1998). </year>
Reference-contexts: different classifiers by randomizing the base learning process (Dietterich and Kong, 1995; Ali, 1996), learning option trees (Buntine, 1990; Kohavi and Kunz, 1997), training a committee of neural networks by manually selecting attribute subsets (Cherkauer, 1996; Tumer and Ghosh, 1996), learning naive Bayesian classifier committees by randomly choosing attribute subsets <ref> (Zheng, 1998) </ref>, creating Gaussian classifier committees by varying attribute sets (Asker and Maclin, 1997), and creating committees for first-order learning by adding random selection of conditions to FOIL (Ali and Pazzani, 1996).
References-found: 28

