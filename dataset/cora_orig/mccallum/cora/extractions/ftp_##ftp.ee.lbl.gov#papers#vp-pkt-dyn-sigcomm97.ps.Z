URL: ftp://ftp.ee.lbl.gov/papers/vp-pkt-dyn-sigcomm97.ps.Z
Refering-URL: http://www.cs.washington.edu/education/courses/590s/w98/index.html
Root-URL: 
Email: vern@ee.lbl.gov  
Title: End-to-End Internet Packet Dynamics  
Author: Vern Paxson 
Date: June 23, 1997  
Address: Berkeley  
Affiliation: Network Research Group Lawrence Berkeley National Laboratory University of California,  
Pubnum: LBNL-40488  
Abstract: We discuss findings from a large-scale study of Internet packet dynamics conducted by tracing 20,000 TCP bulk transfers between 35 Internet sites. Because we traced each 100 Kbyte transfer at both the sender and the receiver, the measurements allow us to distinguish between the end-to-end behaviors due to the different directions of the Internet paths, which often exhibit asymmetries. We characterize the prevalence of unusual network events such as out-of-order delivery and packet corruption; discuss a robust receiver-based algorithm for estimating bottleneck bandwidth that addresses deficiencies discovered in techniques based on packet pair; investigate patterns of packet loss, finding that loss events are not well-modeled as independent and, furthermore, that the distribution of the duration of loss events exhibits infinite variance; and analyze variations in packet transit delays as indicators of congestion periods, finding that congestion periods also span a wide range of time scales. 
Abstract-found: 1
Intro-found: 1
Reference: [A+96] <author> G. Almes et al, </author> <title> Framework for IP Provider Metrics, Internet draft, </title> <address> ftp://ftp.isi.edu/internet-drafts/draft-ietf-bmwg-ippm-framework-00.txt, </address> <month> Nov. </month> <year> 1996. </year> <month> 16 </month>
Reference: [Bo93] <author> J-C. Bolot, </author> <title> End-to-End Packet Delay and Loss Behavior in the Internet, </title> <booktitle> Proc. SIGCOMM '93, </booktitle> <pages> pp. 289-298, </pages> <month> Sept. </month> <year> 1993. </year>
Reference-contexts: Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. paths, because of the great logistical difficulties presented by larger-scale measurement <ref> [Mo92, Bo93, CPB93, Mu94] </ref>. Consequently, it is hard to gauge how representative their findings are for today's Internet. Recently, we devised a measurement framework in which a number of sites run special measurement daemons (NPDs) to facilitate measurement. <p> In this study we report on a large-scale experiment to study end-to-end Internet packet dynamics. 1 Our analysis is based on measurements of TCP bulk transfers conducted between 35 NPD sites (x 2). Using TCPrather than fixed-rate UDP or ICMP echo packets as done in <ref> [Bo93, CPB93, Mu94] </ref> reaps significant benefits. First, TCP traffic is real world, since TCP is widely used in today's Internet. Consequently, any network path properties we can derive from measurements of a TCP transfer can potentially be directly applied to tuning TCP performance. <p> If we do not do so, then we will skew our analysis by mixing together measurements with built-in delays (due to queueing at the bottleneck) with measurements that do not reflect built-in delays. 4.1 Packet pair The bottleneck estimation technique used in previous work is based on packet pair <ref> [Ke91, Bo93, CC96] </ref>. <p> But if, as for us, the goal is to estimate B , then these variations instead become noise we must deal with. Bolot used a stream of packets sent at fixed intervals to probe several Internet paths in order to characterize delay and loss <ref> [Bo93] </ref>. He measured round-trip delay of UDP echo packets and, among other analysis, applied the packet pair technique to form estimates of bottleneck bandwidths. <p> We find in practice this additional noise can be quite large. 6 4.2 Difficulties with packet pair As shown in <ref> [Bo93] </ref> and [CC96], packet pair techniques often provide good estimates of bottleneck bandwidth. We find, however, four potential problems (in addition to noise on the return path for SBPP). Three of these problem can often be addressed, but the fourth is more fundamental. Out-of-order delivery. <p> bottleneck from host A to host B the same as that from B to A? Bottleneck asymmetries are an important consideration for sender-based echo measurement techniques, since these will observe the minimum 4 Recall that we compute B in terms of TCP payload bytes. 8 bottleneck of the two directions <ref> [Bo93, CC96] </ref>. We find that for a given pair of hosts, the median estimates in the two directions differ by more than 20% about 20% of the time. This finding agrees with the observation that Internet paths often exhibit major routing asymmetries [Pa96]. <p> The first question we address is the degree to which packet losses are well-modeled as independent. In <ref> [Bo93] </ref>, Bolot investigated this question by comparing the unconditional loss probability, P u l , with the conditional loss probability, P c l , where P c l is conditioned on the fact that the previous packet was also lost.
Reference: [BOP94] <author> L. Brakmo, S. O'Malley and L. Peterson, </author> <title> TCP Vegas: New Techniques for Congestion Detection and Avoidance, </title> <booktitle> Proc. SIGCOMM '94, </booktitle> <pages> pp. 24-35, </pages> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: This last point argues that when the measurement of interest concerns a unidirectional pathbe it for measurement-based adaptive transport techniques such as TCP Vegas <ref> [BOP94] </ref>, or general Internet performance metrics such as those in development by the IPPM effort [A+96]the extra complications incurred by coordinating sender and receiver are worth the effort. 8 Acknowledgements This work would not have been possible without the efforts of the many volunteers who installed the Network Probe Daemon at
Reference: [CC96] <author> R. Carter and M. Crovella, </author> <title> Measuring Bottleneck Link Speed in Packet-Switched Networks, </title> <type> Tech. Report BU-CS-96-006, </type> <institution> Computer Science Department, Boston University, </institution> <month> Mar. </month> <year> 1996. </year>
Reference-contexts: If we do not do so, then we will skew our analysis by mixing together measurements with built-in delays (due to queueing at the bottleneck) with measurements that do not reflect built-in delays. 4.1 Packet pair The bottleneck estimation technique used in previous work is based on packet pair <ref> [Ke91, Bo93, CC96] </ref>. <p> He found good agreement with known link capacities, though a limitation of his study is that the measurements were confined to a small number of Internet paths. Recent work by Carter and Crovella also investigates the utility of using packet pair in the Internet for estimating B <ref> [CC96] </ref>. Their work focusses on bprobe, a tool they devised for estimating B by transmitting 10 consecutive ICMP echo packets and recording the interarrival times of the consecutive replies. <p> We find in practice this additional noise can be quite large. 6 4.2 Difficulties with packet pair As shown in [Bo93] and <ref> [CC96] </ref>, packet pair techniques often provide good estimates of bottleneck bandwidth. We find, however, four potential problems (in addition to noise on the return path for SBPP). Three of these problem can often be addressed, but the fourth is more fundamental. Out-of-order delivery. <p> Since all but single bottlenecks are rare, we defer discussion of the others to [Pa97b], and focus here on the usual case of finding a single bottleneck. Unlike <ref> [CC96] </ref>, we do not know a priori the bottleneck bandwidths for many of the paths in our study. We thus must fall back on self-consistency checks in order to gauge the accuracy of PBM. <p> bottleneck from host A to host B the same as that from B to A? Bottleneck asymmetries are an important consideration for sender-based echo measurement techniques, since these will observe the minimum 4 Recall that we compute B in terms of TCP payload bytes. 8 bottleneck of the two directions <ref> [Bo93, CC96] </ref>. We find that for a given pair of hosts, the median estimates in the two directions differ by more than 20% about 20% of the time. This finding agrees with the observation that Internet paths often exhibit major routing asymmetries [Pa96].
Reference: [CPB93] <author> K. Claffy, G. Polyzos and H-W. Braun, </author> <title> Measurement Considerations for Assessing Unidirectional Latencies, Internetworking: </title> <journal> Research and Experience, </journal> <volume> 4 (3), </volume> <pages> pp. 121-132, </pages> <month> Sept. </month> <year> 1993. </year>
Reference-contexts: Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. paths, because of the great logistical difficulties presented by larger-scale measurement <ref> [Mo92, Bo93, CPB93, Mu94] </ref>. Consequently, it is hard to gauge how representative their findings are for today's Internet. Recently, we devised a measurement framework in which a number of sites run special measurement daemons (NPDs) to facilitate measurement. <p> In this study we report on a large-scale experiment to study end-to-end Internet packet dynamics. 1 Our analysis is based on measurements of TCP bulk transfers conducted between 35 NPD sites (x 2). Using TCPrather than fixed-rate UDP or ICMP echo packets as done in <ref> [Bo93, CPB93, Mu94] </ref> reaps significant benefits. First, TCP traffic is real world, since TCP is widely used in today's Internet. Consequently, any network path properties we can derive from measurements of a TCP transfer can potentially be directly applied to tuning TCP performance. <p> The analysis in this section assumes these algorithms have first been used to reject or adjust traces with clock errors. OTT variation was previously analyzed by Claffy and colleagues in a study of four Internet paths <ref> [CPB93] </ref>. They found that mean OTTs are often not well approximated by dividing RTTs in half, and that variations in the paths' OTTs are often asymmetric. Our measurements confirm this latter finding.
Reference: [DMT96] <author> R. Durst, G. Miller and E. Travis, </author> <title> TCP Extensions for Space Communications, </title> <booktitle> Proc. </booktitle> <volume> MOBI-COM '96, </volume> <pages> pp. 15-26, </pages> <month> Nov. </month> <year> 1996. </year>
Reference-contexts: The bottleneck could differ in the two directions due to asymmetric routing, for example [Pa96], or because some media, such as satellite links, can have significant bandwidth asymmetries depending on the direction traversed <ref> [DMT96] </ref>. For estimating bottleneck bandwidth by measuring TCP traffic, a second problem arises: if the only measurements available are those at the sender, then ack compression (x 6.1) can significantly alter the spacing of the small ack packets as they return through the network, distorting the bandwidth estimate.
Reference: [FJ93] <author> S. Floyd and V. Jacobson, </author> <title> Random Early Detection Gateways for Congestion Avoidance, </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> 1(4), </volume> <pages> pp. 397-413, </pages> <month> Aug. </month> <year> 1993. </year>
Reference-contexts: Finally, we note that the patterns of loss bursts we observe might be greatly shaped by use of drop tail queueing. In particular, deployment of Random Early Detection could significantly affect these patterns and the corresponding con nection dynamics <ref> [FJ93] </ref>. 12 Type of RR Solaris 1 Solaris 2 Other 1 Other 2 % all packets 6% 6% 1% 2% % retrans. 66% 59% 26% 28% Unavoidable 14% 33% 44% 17% Coarse feed. 1% 1% 51% 80% Bad RTO 84% 66% 4% 3% Table 3: Proportion of redundant retransmissions (RRs) due
Reference: [FJ94] <author> S. Floyd and V. Jacobson, </author> <title> The Synchronization of Periodic Routing Messages, </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> 2(2), </volume> <pages> pp. 122-136, </pages> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: What has apparently happened is that a router with Ethernet-limited connectivity to the receiver stopped forwarding packets for 110 msec just as sequence 72,705 arrived, most likely because at that point it processed a routing update <ref> [FJ94] </ref>. It finished between the arrival of 91,137 and 91,649, and began forwarding packets normally again at their arrival rate, namely T1 speed. <p> Furthermore, while less prevalent, bt values all the way up to 65 sec remain common, with N 1 having a strong peak at 65 sec (which appears genuine; perhaps due to periodic outages caused by router synchronization <ref> [FJ94] </ref>, eliminated by the end of 1995).
Reference: [Ja88] <author> V. Jacobson, </author> <title> Congestion Avoidance and Control, </title> <booktitle> Proc. SIGCOMM '88, </booktitle> <pages> pp. 314-329, </pages> <month> Aug. </month> <year> 1988. </year>
Reference-contexts: Barring subsequent delay variations, they will then arrive at the receiver spaced not T s apart, but T r = Q b . We then compute B via Eqn 1. The principle of the bottleneck spacing effect was noted in Jacobson's classic congestion paper <ref> [Ja88] </ref>, where it in turn leads to the self-clocking mechanism. <p> RTO RRs are rare, providing solid evidence that the standard TCP RTO estimation algorithm developed in <ref> [Ja88] </ref> performs quite well for avoiding RRs. A separate question is whether the RTO estimation is overly conservative. A thorough investigation of this question is complex because a revised estimator might take advantage of both higher-resolution clocks and the opportunity to time multiple packets per flight. <p> Were ack compression frequent, it would present two problems. First, as acks arrive they advance TCP's sliding window and clock out new data packets at the rate reflected by their arrival <ref> [Ja88] </ref>. For compressed acks, this means that the data packets go out faster than previously, which can result in network stress. Second, sender-based measurement techniques such as SBPP (x 4.1) can misinterpret compressed acks as reflecting greater bandwidth than truly available.
Reference: [JLM89] <author> V. Jacobson, C. Leres, and S. McCanne, tcpdump, </author> <note> available via anonymous ftp to ftp.ee.lbl.gov, </note> <month> June </month> <year> 1989. </year>
Reference-contexts: Each measurement was made by instructing daemons running at two of the sites to send or receive a 100 Kbyte TCP bulk transfer, and to trace the results using tcpdump <ref> [JLM89] </ref>. Measurements occurred at Poisson intervals, which, in principle, results in unbiased measurement, even if the sampling rate varies [Pa96]. In N 1 , the mean per-site sampling interval was about 2 hours, with each site randomly paired with another.
Reference: [Ja90] <author> V. Jacobson, </author> <title> Compressing TCP/IP headers for low-speed serial links, </title> <type> RFC 1144, </type> <institution> Network Information Center, SRI International, </institution> <address> Menlo Park, CA, </address> <month> February, </month> <year> 1990. </year>
Reference-contexts: This discrepancy can be partially addressed by accounting for the different lengths of data packets versus pure acks. It can be further reconciled if header compression such as CSLIP is used along the Internet paths in our study <ref> [Ja90] </ref>, as that would greatly increase the relative size of data packets to that of pure acks. But it seems unlikely that header compression is widely used for high-speed links, and most of the inferred N 2 data packet corruptions occurred for T1 and faster network paths.
Reference: [Ke91] <author> S. Keshav, </author> <title> A Control-Theoretic Approach to Flow Control, </title> <booktitle> Proc. SIGCOMM '91, </booktitle> <pages> pp. 3-15, </pages> <month> Sept. </month> <year> 1991. </year>
Reference-contexts: If we do not do so, then we will skew our analysis by mixing together measurements with built-in delays (due to queueing at the bottleneck) with measurements that do not reflect built-in delays. 4.1 Packet pair The bottleneck estimation technique used in previous work is based on packet pair <ref> [Ke91, Bo93, CC96] </ref>. <p> Keshav formally analyzed the behavior of packet pair for a network of routers that all obey the fair queueing scheduling discipline (not presently used in the Internet), and developed a provably stable flow control scheme based on packet pair measurements <ref> [Ke91] </ref>. Both Jacobson and Keshav were interested in estimating available rather than bottleneck bandwidth, and for this variations from Q b due to queueing are of primary concern (x 6.3).
Reference: [MMSR96] <author> M. Mathis, J. Mahdavi, S. Floyd and A. Ro-manow, </author> <title> TCP Selective Acknowledgment Options, RFC 2018, </title> <institution> DDN Network Information Center, </institution> <month> Oct. </month> <year> 1995. </year>
Reference-contexts: We note that the TCP selective acknowledgement (SACK) option, now pending standardization, also holds promise for honing TCP retransmission <ref> [MMSR96] </ref>. SACK provides sufficiently fine-grained acknowledgement information that the sending TCP can generally tell which packets require retransmission and which have safely arrived (x 5.4).
Reference: [Mo92] <author> J. Mogul, </author> <title> Observing TCP Dynamics in Real Networks, </title> <booktitle> Proc. SIGCOMM '92, </booktitle> <pages> pp. 305-317, </pages> <month> Aug. </month> <year> 1992. </year>
Reference-contexts: Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. paths, because of the great logistical difficulties presented by larger-scale measurement <ref> [Mo92, Bo93, CPB93, Mu94] </ref>. Consequently, it is hard to gauge how representative their findings are for today's Internet. Recently, we devised a measurement framework in which a number of sites run special measurement daemons (NPDs) to facilitate measurement. <p> of out-of-order delivery, since the latter indicates that a first-in-first-out (FIFO) queueing model of the network does not apply. 3.1 Out-of-order delivery Even though Internet routers employ FIFO queueing, any time a route changes, if the new route offers a lower delay than the old one, then reordering can occur <ref> [Mo92] </ref>. Since we recorded packets at both ends of each TCP connection, we can detect network reordering, as follows. First, we remove from our analysis any trace pairs suffering packet filter errors [Pa97a]. <p> Mogul subsequently analyzed a trace of Internet traffic and confirmed the presence of ack compression <ref> [Mo92] </ref>. His definition of ack compression is somewhat complex since he had to infer endpoint behavior from an observation point inside the network. Since we can compute from our data both T s and T r , we can instead directly evaluate the presence of compression.
Reference: [Mu94] <author> A. Mukherjee, </author> <title> On the Dynamics and Significance of Low Frequency Components of Internet Load, Internetworking: </title> <journal> Research and Experience, </journal> <volume> Vol. 5, </volume> <pages> pp. 163-205, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. paths, because of the great logistical difficulties presented by larger-scale measurement <ref> [Mo92, Bo93, CPB93, Mu94] </ref>. Consequently, it is hard to gauge how representative their findings are for today's Internet. Recently, we devised a measurement framework in which a number of sites run special measurement daemons (NPDs) to facilitate measurement. <p> In this study we report on a large-scale experiment to study end-to-end Internet packet dynamics. 1 Our analysis is based on measurements of TCP bulk transfers conducted between 35 NPD sites (x 2). Using TCPrather than fixed-rate UDP or ICMP echo packets as done in <ref> [Bo93, CPB93, Mu94] </ref> reaps significant benefits. First, TCP traffic is real world, since TCP is widely used in today's Internet. Consequently, any network path properties we can derive from measurements of a TCP transfer can potentially be directly applied to tuning TCP performance. <p> Consequently, in this paper we do not attempt these sorts of analyses, though we hope to pursue them in future work. See also <ref> [Mu94] </ref> for previous work in applying frequency-domain analysis to Internet paths. In x 3 we characterize unusual network behavior: out-of-order delivery, replication, and packet corruption. Then in x 4 we discuss a robust algorithm for estimating the bottleneck bandwidth that limits a connection's maximum rate. <p> We also do not summarize the marginal distribution of packet delays. Mukherjee found that packet delay along a particular Internet path is well-modeled using a shifted gamma distribution, but the parameters of the distribution vary from path to path and over the course of the day <ref> [Mu94] </ref>. Since we have about 1,000 distinct paths in our study, measured at all hours of the day, and since the gamma distribution varies considerably as its parameters are varied, it is difficult to see how to summarize the delay distributions in a useful fashion.
Reference: [Pa96] <author> V. Paxson, </author> <title> End-to-End Routing Behavior in the Internet, </title> <booktitle> Proc. SIGCOMM '96, </booktitle> <pages> pp. 25-38, </pages> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: With this framework, the number of Internet paths available for measurement grows as N 2 for N sites, yielding an attractive scaling. We previously used the framework with N = 37 sites to study end-to-end routing dynamics of about 1,000 Internet paths <ref> [Pa96] </ref>. In this study we report on a large-scale experiment to study end-to-end Internet packet dynamics. 1 Our analysis is based on measurements of TCP bulk transfers conducted between 35 NPD sites (x 2). <p> In x 7 we briefly summarize our findings, a number of which challenge commonly-held assumptions about network behavior. 2 The Measurements We gathered our measurements using the NPD measurement framework we developed and discussed in <ref> [Pa96] </ref>. 35 sites participated in two experimental runs. The sites include educational institutes, research labs, network service providers, and commercial companies, in 9 countries. We conducted the first run, N 1 , during Dec. 1994, and the second, N 2 , during NovDec. 1995. <p> Each measurement was made by instructing daemons running at two of the sites to send or receive a 100 Kbyte TCP bulk transfer, and to trace the results using tcpdump [JLM89]. Measurements occurred at Poisson intervals, which, in principle, results in unbiased measurement, even if the sampling rate varies <ref> [Pa96] </ref>. In N 1 , the mean per-site sampling interval was about 2 hours, with each site randomly paired with another. Sites typically participated in about 200 measurements, and we gathered a total of 2,800 pairs of traces. <p> For example, fully 15% of the data packets sent by the ucol site 2 during N 1 arrived out of order, much higher than the 2.0% overall average. As discussed in <ref> [Pa96] </ref>, we do not claim that the individual sites participating in the measurement framework are plausibly representative of Internet sites in general, so site-specific behavior cannot be argued to reflect general Internet behavior. Reordering is also highly asymmetric. <p> The site-to-site variation in reordering coincides with our earlier findings concerning route flutter among the same sites <ref> [Pa96] </ref>. We identified two sites as particularly exhibiting flutter, ucol and the wustl site. For the part of N 1 during which wustl exhibited route flutter, 24% of all of the data packets it sent arrived out of order, a rather stunning degree of reordering. <p> Interestingly, some of the most highly reordered connections did not suffer any packet loss, and no needless retransmissions due to false signals from duplicate acks. We also occasionally observed humongous reordering gaps. However, the evidence suggests that 2 See <ref> [Pa96] </ref> for specifics concerning the sites mentioned in this paper. these gaps are not due to route changes, but a different effect. of an arriving data packet. All packets were sent in increasing sequence order. <p> The bottleneck could differ in the two directions due to asymmetric routing, for example <ref> [Pa96] </ref>, or because some media, such as satellite links, can have significant bandwidth asymmetries depending on the direction traversed [DMT96]. <p> We find that for a given pair of hosts, the median estimates in the two directions differ by more than 20% about 20% of the time. This finding agrees with the observation that Internet paths often exhibit major routing asymmetries <ref> [Pa96] </ref>. The bottleneck differences can be quite large, with for example some paths T1-limited in one direction but Ethernet-limited in the other. <p> Indeed, the coefficient of correlation between combined (loaded and unloaded) data packet loss rates and ack loss rates in N 1 is 0.21, and in N 2 , the loss rates appear uncorrelated (coefficient of 0.02), perhaps due to the greater prevalence of significant routing asymmetry <ref> [Pa96] </ref>. Further investigating the loss rate distributions, one interesting feature we find is that the non-zero portions of both the unloaded and loaded data packet loss rates agree closely with exponential distributions, while that for acks is not so persuasive a match.
Reference: [Pa97a] <author> V. Paxson, </author> <title> Automated Packet Trace Analysis of TCP Implementations, </title> <booktitle> Proc. SIGCOMM '97, </booktitle> <month> Sep. </month> <year> 1997. </year>
Reference-contexts: First, we need to distinguish between the apparently intertwined effects of the transport protocol and the network. To do so, we developed tcpanaly, a program that understands the specifics of the different TCP implementations in our study and thus can separate TCP behavior from network behavior <ref> [Pa97a] </ref>. tcpanaly also forms the basis for the analysis in this paper: after removing TCP effects, it then computes a wide range of statistics concerning network dynamics. <p> Since we recorded packets at both ends of each TCP connection, we can detect network reordering, as follows. First, we remove from our analysis any trace pairs suffering packet filter errors <ref> [Pa97a] </ref>. Then, for each arriving packet p i , we check whether it was sent after the last non-reordered packet. If so, then it becomes the new such packet. Otherwise, we count its arrival as an instance of a network reordering. <p> Surprisingly, packets can also be replicated at the sender, before the network has had much of a chance to perturb them. We know these are true replications and not packet filter duplications, as discussed in <ref> [Pa97a] </ref>, because the copies have had their TTL fields decremented. <p> verify the checksum because the packet filter used in our study only recorded the packet headers, and not the payload. (For pure acks, i.e., acknowledgement packets with no data payload, it directly verifies the checksum.) Consequently, tcpanaly includes algorithms to infer whether data packets arrive with invalid checksums, discussed in <ref> [Pa97a] </ref>. Using that analysis, we first found that one site, lbli, was much more prone to checksum errors than any other. Since lbli's Internet link is via an ISDN link, it appears quite likely that these are due to noise on the ISDN channels. <p> All it requires is knowing when packets were sent relative to one another, how they arrived relative to one another, and their size. We applied PBM to N 1 and N 2 for those traces for which tcpanaly's packet filter and clock analysis did not uncover any uncorrectable problems <ref> [Pa97a, Pa97b] </ref>. <p> Here is where the effort to ensure that tcpanaly understands the details of the TCP implementations in our study pays off <ref> [Pa97a] </ref>. Because we can determine whether traces suffer from measurement drops, we can exclude those that do from our packet loss analysis and avoid what could otherwise be significant inaccuracies. <p> Table 3 summarizes the prevalence of the different types of RRs in N 1 and N 2 . We divide the analysis into So-laris 2.3/2.4 TCP senders and others because in <ref> [Pa97a] </ref> we identified the Solaris 2.3/2.4 TCP as suffering from significant errors in computing RTO, which the other implementations do not exhibit.
Reference: [Pa97b] <author> V. Paxson, </author> <title> Measurements and Analysis of End-to-End Internet Dynamics, </title> <type> Ph.D. dissertation, </type> <institution> University of California, Berkeley, </institution> <month> April </month> <year> 1997. </year>
Reference-contexts: Second, TCP packets are sent over a wide range of time scales, from milliseconds to many seconds between consecu 1 This paper is necessarily terse due to space limitations. A longer version is available <ref> [Pa97b] </ref>. tive packets. Such irregular spacing greatly complicates cor-relational and frequency-domain analysis, because a stream of TCP packets does not give us a traditional time series of constant-rate observations to work with. <p> We limited measurements to a total of 10 minutes. This limit leads to under-representation of those times during which network conditions were poor enough to make it difficult to complete a 100 Kbyte transfer in that much time. Thus, our measurements are biased towards more favorable network conditions. In <ref> [Pa97b] </ref> we show that the bias is negligible for North American sites, but noticeable for European sites. 3 Network Pathologies We begin with an analysis of network behavior we might consider pathological, meaning unusual or unexpected: out-of-order delivery, packet replication, and packet corruption. <p> Due to space limitations, we defer the particulars to <ref> [Pa97b] </ref>. <p> All it requires is knowing when packets were sent relative to one another, how they arrived relative to one another, and their size. We applied PBM to N 1 and N 2 for those traces for which tcpanaly's packet filter and clock analysis did not uncover any uncorrectable problems <ref> [Pa97a, Pa97b] </ref>. <p> Since all but single bottlenecks are rare, we defer discussion of the others to <ref> [Pa97b] </ref>, and focus here on the usual case of finding a single bottleneck. Unlike [CC96], we do not know a priori the bottleneck bandwidths for many of the paths in our study. We thus must fall back on self-consistency checks in order to gauge the accuracy of PBM. <p> SBPP is of considerable interest because a sender can use it without any cooperation from the receiver, making it easy to deploy in the Internet. To fairly evaluate SBPP, we assume use by the sender of a number of considerations for forming sound bandwidth estimates, detailed in <ref> [Pa97b] </ref>. Even so, we find, unfortunately, that SBPP does not work especially well. In both datasets, the SBPP bottleneck estimate agrees with PBM only about 60% of the time. <p> Within regions, we find considerable site-to-site variation in loss rates, as well as variation between loss rates for packets inbound to the site and those outbound (x 5.2). We did not, however, find any sites that seriously skewed the above figures. In <ref> [Pa97b] </ref> we also analyze loss rates over the course of the day, here omitted due to limited space. We find an un-surprising diurnal pattern of busy periods corresponding to working hours and quiescent periods to late night and especially early morning hours. <p> We find that observing a zero-loss connection at a given point in time is quite a good predictor of observing zero-loss connections up to several hours in the future, and remains a useful predictor, though not as strong, even for time scales of days and weeks <ref> [Pa97b] </ref>. Similarly, observing a connection that suffered loss is also a good predictor that future connections will suffer loss. <p> This problem is particularly pronounced when measuring OTTs since doing so involves comparing measurements from two separate clocks. Accordingly, we developed robust algorithms for detecting clock adjustments and relative skew by inspecting sets of OTT measurements, described in <ref> [Pa97b] </ref>. The analysis in this section assumes these algorithms have first been used to reject or adjust traces with clock errors. OTT variation was previously analyzed by Claffy and colleagues in a study of four Internet paths [CPB93]. <p> packet dynamics. * We find wide ranges of behavior, such that we must exercise great caution in regarding any aspect of packet dynamics as typical. * Some common assumptions such as in-order packet delivery, FIFO bottleneck queueing, independent loss 8 The depressed density at fi 0 reflects a measurement bias <ref> [Pa97b] </ref>. events, single congestion time scales, and path symme tries are all violated, sometimes frequently. * When implemented correctly, TCP's retransmission strategies work in a sufficiently conservative fashion. * The combination of path asymmetries and reverse-path noise render sender-only measurement techniques markedly inferior to those that include receiver cooperation.
Reference: [WTSW95] <author> W. Willinger, M. Taqqu, R. Sherman, and D. Wilson, </author> <title> Self-Similarity Through High-Variability: Statistical Analysis of Ethernet LAN Traffic at the Source Level, </title> <booktitle> Proc. SIGCOMM '95, </booktitle> <pages> pp. 100-113, </pages> <month> Sept. </month> <year> 1995. </year>
Reference-contexts: A shape parameter ff 2 means that the distribution has infinite variance, indicating immense variability. Pareto distributions for activity and inactivity periods play key roles in some models of self-similar network traffic <ref> [WTSW95] </ref>, suggesting that packet loss outages could contribute to how TCP network traffic might fit to ON/OFF-based self-similarity models. Finally, we note that the patterns of loss bursts we observe might be greatly shaped by use of drop tail queueing.
Reference: [ZSC91] <author> L. Zhang, S. Shenker, and D. Clark, </author> <title> Observations on the Dynamics of a Congestion Control Algorithm: The Effects of Two-Way Traffic, </title> <booktitle> Proc. SIGCOMM '91, </booktitle> <pages> pp. 133-147, </pages> <month> Sept. </month> <year> 1991. </year> <month> 17 </month>
Reference-contexts: Zhang et al. predicted from theory and simulation that acks could be compressed (ack compression) if a flight arrived at a router without any intervening packets from cross traffic (hence, the router's queue is draining) <ref> [ZSC91] </ref>. Mogul subsequently analyzed a trace of Internet traffic and confirmed the presence of ack compression [Mo92]. His definition of ack compression is somewhat complex since he had to infer endpoint behavior from an observation point inside the network.
References-found: 20

