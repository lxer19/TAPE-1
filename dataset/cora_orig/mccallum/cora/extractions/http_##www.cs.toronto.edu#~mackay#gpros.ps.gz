URL: http://www.cs.toronto.edu/~mackay/gpros.ps.gz
Refering-URL: http://131.111.48.24/mackay/README.html
Root-URL: 
Title: Efficient Implementation of Gaussian Processes  
Author: Mark Gibbs David J.C. MacKay 
Date: May 28, 1997  
Address: Cambridge CB3 0HE United Kingdom  Cambridge CB3 0HE United Kingdom  
Affiliation: Cavendish Laboratory  Cavendish Laboratory  
Abstract: Neural networks and Bayesian inference provide a useful framework within which to solve regression problems. However their parameterization means that the Bayesian analysis of neural networks can be difficult. In this paper, we investigate a method for regression using Gaussian process priors which allows exact Bayesian analysis using matrix manipulations. We discuss the workings of the method in detail. We will also detail a range of mathematical and numerical techniques that are useful in applying Gaussian processes to general problems including efficient approximate matrix inversion methods developed by Skilling.
Abstract-found: 1
Intro-found: 1
Reference: <author> Barnett, S. </author> <title> (1979) Matrix Methods for Engineers and Scientists. </title> <publisher> McGraw-Hill. </publisher>
Reference: <author> Brown, J. D., Chu, M. T., Ellison, D. C., and Plemmons, R. J. </author> <title> eds. </title> <booktitle> (1994) Proceedings of the Cornelius Lanczos International Centenary Conference. </booktitle> <address> Philadelphia,PA: </address> <publisher> SIAM Publications. </publisher>
Reference-contexts: Because we are dealing with a quadratic form, the line minimizations are replaced by simple matrix manipulations. Conjugate gradient methods for inverting matrices are well established and further details can be found in Lanczos (1950), (G. H. Golub 1990) and <ref> (Brown et al. 1994) </ref>. It is useful to be able to gauge the accuracy of the approximation y K to C 1 N u. We can then stop the maximization procedure when we have achieved a specified accuracy and, hopefully, when K is significantly smaller than N .
Reference: <author> Cressie, N. </author> <title> (1993) Statistics for Spatial Data. </title> <publisher> Wiley. </publisher>
Reference-contexts: k T N+1 C 1 N k N+1 can be calculated in a similar way to C 1 N t N and hence we can find C 1 N+1 t N+1 using the form of C 1 N+1 given above. 4.2 Multiple Outputs The subject of multiple outputs (or co-kriging <ref> (Cressie 1993) </ref>) is problematic. It is possible to re-formulate Gaussian processes such that they can deal with multiple outputs.
Reference: <author> G. H. Golub, C. V. L. </author> <title> (1990) Matrix Computation. Baltimore: John Hopkins University Press. 15 for noisy data sets of different sizes generated from the function shown in Figure 5. Times are shown for both the LU decomposition method and the approximate method. For small data sets the LU method is significantly quicker than the approximate approach. At around 200 data points we can see that the O(N 2 ) scaling of the approximate method begins to yield benifits. For 400 and 800 data points the approximate method is significantly faster than the LU decomposition method. </title>
Reference-contexts: This maximization procedure is analogous to the conjugate gradient algorithm. Because we are dealing with a quadratic form, the line minimizations are replaced by simple matrix manipulations. Conjugate gradient methods for inverting matrices are well established and further details can be found in Lanczos (1950), <ref> (G. H. Golub 1990) </ref> and (Brown et al. 1994). It is useful to be able to gauge the accuracy of the approximation y K to C 1 N u.
Reference: <author> Kitanidis, P. K. </author> <title> (1986) Parameter uncertainty in estimation of spatial functions: Bayesian analysis. </title> <booktitle> Water Resources Research 22: </booktitle> <pages> 499-507. </pages>
Reference: <author> Lanczos, C. </author> <title> (1950) An iteration method for the solution of the eigenvalue problem for linear differential and integral operators. </title> <journal> Journal of Research (National Bureau of Standards) 45: </journal> <pages> 255-282. </pages>
Reference: <author> Lowe, D. G. </author> <title> (1995) Similarity metric learning for a variable kernel classifier. </title> <booktitle> Neural Computation 7: </booktitle> <pages> 72-85. </pages>
Reference-contexts: The Gaussian process framework encompasses a wide range of different regression models. O'Hagan (1978) introduced an approach which is essentially similar to Gaussian processes. Generalized radial basis functions (Poggio and Girosi 1989), ARMA models (Wahba 1990) and variable metric kernel methods <ref> (Lowe 1995) </ref> are all closely related to Gaussian processes. The present interest in the area has been initiated by the work of Neal (1996) on priors for infinite networks.
Reference: <author> MacKay, D. J. C. </author> <title> (1992a) Bayesian interpolation. </title> <booktitle> Neural Computation 4 (3): </booktitle> <pages> 415-447. </pages>
Reference-contexts: 1 Introduction Neural networks and Bayesian inference have provided a useful framework within which to solve regression problems <ref> (MacKay 1992a) </ref> (MacKay 1992b). However due to the parameterization of a neural network, implementations of the Bayesian analysis of a neural network require either maximum aposte-riori approximations (MacKay 1992b) or the evaluation of integrals using Monte Carlo methods (Neal 1993). <p> For arbitrary C (:) this is analytically intractable. There are two approaches we can take to evaluating equation 13. Either we can approximate the average prediction over all the possible values of the hyperparameters using the most probable values of hyperparameters (the Evidence framework <ref> (MacKay 1992a) </ref>) or we can perform the integration over fi numerically using Monte Carlo methods (Williams and Rasmussen 1996; Neal 1993). In this paper we shall only consider an Evidence framework approach.
Reference: <author> MacKay, D. J. C. </author> <title> (1992b) A practical Bayesian framework for backpropagation networks. </title> <booktitle> Neural Computation 4 (3): </booktitle> <pages> 448-472. </pages>
Reference-contexts: 1 Introduction Neural networks and Bayesian inference have provided a useful framework within which to solve regression problems (MacKay 1992a) <ref> (MacKay 1992b) </ref>. However due to the parameterization of a neural network, implementations of the Bayesian analysis of a neural network require either maximum aposte-riori approximations (MacKay 1992b) or the evaluation of integrals using Monte Carlo methods (Neal 1993). <p> 1 Introduction Neural networks and Bayesian inference have provided a useful framework within which to solve regression problems (MacKay 1992a) <ref> (MacKay 1992b) </ref>. However due to the parameterization of a neural network, implementations of the Bayesian analysis of a neural network require either maximum aposte-riori approximations (MacKay 1992b) or the evaluation of integrals using Monte Carlo methods (Neal 1993). In this paper, we will review a framework within which to solve regression problems using parameterized Gaussian processes (Cressie 1993; Williams 1995; Williams and Rasmussen 1996).
Reference: <author> MacKay, D. J. C. </author> <title> (1994) Bayesian methods for backpropagation networks. In Models of Neural Networks III , ed. by E. </title> <editor> Domany, J. L. van Hemmen, and K. Schulten, </editor> <booktitle> chapter 6. </booktitle> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: A very large length scale means that the predictions made using the Gaussian process would have little or no bearing on the corresponding input. Such an input could be said to be insignificant. This interpretation is closely related to Automatic Relevance Determination <ref> (MacKay 1994) </ref>, (Neal 1996). The 1 hyperparameter gives the overall vertical scale relative to the mean of the Gaussian process in output space. The 2 hyperparameter gives the vertical uncertainty.
Reference: <author> MacKay, D. J. C. </author> <year> (1996) </year> <month> Hyperparameters: </month> <title> Optimize, or integrate out? In Maximum Entropy and Bayesian Methods, </title> <editor> Santa Barbara 1993 , ed. by G. </editor> <booktitle> Heidbreder, </booktitle> <pages> pp. 43-60, </pages> <address> Dordrecht. </address> <publisher> Kluwer. </publisher>
Reference-contexts: This approximation is generally very good and the Evidence framework predictions are often identical to those found using the true predictive distribution <ref> (MacKay 1996) </ref>. In order to make use of the Evidence approximation we need to find the most probable hyperparam-eters. We can do this using a gradient based optimization routine 1 such as conjugate gradients, if we can find the derivatives of the posterior distribution of fi.
Reference: <author> Matheron, G. </author> <booktitle> (1963) Principles of geostatistics. Economic Geology 58: </booktitle> <pages> 1246-1266. </pages>
Reference: <author> Neal, R. M. </author> <title> (1993) Probabilistic inference using Markov chain Monte Carlo methods. </title> <type> Technical Report CRG-TR-93-1, </type> <institution> Dept. of Computer Science, University of Toronto. </institution>
Reference-contexts: However due to the parameterization of a neural network, implementations of the Bayesian analysis of a neural network require either maximum aposte-riori approximations (MacKay 1992b) or the evaluation of integrals using Monte Carlo methods <ref> (Neal 1993) </ref>. In this paper, we will review a framework within which to solve regression problems using parameterized Gaussian processes (Cressie 1993; Williams 1995; Williams and Rasmussen 1996). This framework allows us to evaluate the integrals which occur in the associated Bayesian analysis exactly using matrix manipulations. <p> The standard approach to solving such problems has been either to make approximations in order to evaluate equation 1 (MacKay (1992a)'s Evidence framework) or to evaluate the integral numerically using Monte Carlo methods <ref> (Neal 1993) </ref>.
Reference: <author> Neal, R. M. </author> <title> (1996) Bayesian Learning for Neural Networks. </title> <booktitle> Number 118 in Lecture Notes in Statistics. </booktitle> <address> New York: </address> <publisher> Springer. 16 Neal, </publisher> <editor> R. M. </editor> <title> (1997) Monte Carlo Implementation of Gaussian process Models for Bayesian Regression and Classification. </title> <type> Technical Report CRG-TR-97-2, </type> <institution> Dept. of Computer Science, University of Toronto. </institution>
Reference-contexts: A very large length scale means that the predictions made using the Gaussian process would have little or no bearing on the corresponding input. Such an input could be said to be insignificant. This interpretation is closely related to Automatic Relevance Determination (MacKay 1994), <ref> (Neal 1996) </ref>. The 1 hyperparameter gives the overall vertical scale relative to the mean of the Gaussian process in output space. The 2 hyperparameter gives the vertical uncertainty. This reflects how far we expect the true mean of the data to fluctuate from the mean of the Gaussian process.
Reference: <author> O'Hagan, A. </author> <title> (1978) On curve fitting and optimal design for regression. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> B 40: </volume> <pages> 1-42. </pages>
Reference: <author> Omre, H. </author> <title> (1987) Bayesian kriging merging observations and qualified guesses in kriging. </title> <booktitle> Mathematical Geology 19: </booktitle> <pages> 25-39. </pages>
Reference: <author> Poggio, T., and Girosi, F. </author> <title> (1989) A theory of networks for approximation and learning. </title> <type> Technical Report A.I. 1140, </type> <institution> M.I.T. </institution>
Reference-contexts: The Gaussian process framework encompasses a wide range of different regression models. O'Hagan (1978) introduced an approach which is essentially similar to Gaussian processes. Generalized radial basis functions <ref> (Poggio and Girosi 1989) </ref>, ARMA models (Wahba 1990) and variable metric kernel methods (Lowe 1995) are all closely related to Gaussian processes. The present interest in the area has been initiated by the work of Neal (1996) on priors for infinite networks.
Reference: <author> Rasmussen, C. E., </author> <title> (1996) Evaluation of Gaussian Processes and Other Methods for Non-Linear Regression. </title> <institution> University of Toronto dissertation. </institution>
Reference: <author> Skilling, J. </author> <title> (1993) Bayesian numerical analysis. In Physics and Probability, </title> <editor> ed. by W. T. G. Jr. and P. Milonni. </editor> <publisher> C.U.P. </publisher>
Reference: <author> Wahba, G. </author> <title> (1990) Spline Models for Observational Data. </title> <booktitle> Society for Industrial and Applied Mathematics. CBMS-NSF Regional Conference series in applied mathematics. </booktitle>
Reference-contexts: The Gaussian process framework encompasses a wide range of different regression models. O'Hagan (1978) introduced an approach which is essentially similar to Gaussian processes. Generalized radial basis functions (Poggio and Girosi 1989), ARMA models <ref> (Wahba 1990) </ref> and variable metric kernel methods (Lowe 1995) are all closely related to Gaussian processes. The present interest in the area has been initiated by the work of Neal (1996) on priors for infinite networks.
Reference: <author> Williams, C. K. I., </author> <title> (1995) Regression with Gaussian processes. </title> <note> To appear in Annals of Mathematics and Artificial Intelligence: Models, Algorithms and Applications. </note>
Reference: <author> Williams, C. K. I., and Rasmussen, C. E. </author> <title> (1996) Gaussian processes for regression. </title> <booktitle> In Advances in Neural Information Processing Systems 8 , ed. </booktitle> <editor> by D. S. Touretzky, M. C. Mozer, and M. E. Hasselmo. </editor> <publisher> MIT Press. </publisher> <pages> 17 </pages>
References-found: 22

