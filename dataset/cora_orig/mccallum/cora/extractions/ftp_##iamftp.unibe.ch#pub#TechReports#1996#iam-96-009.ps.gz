URL: ftp://iamftp.unibe.ch/pub/TechReports/1996/iam-96-009.ps.gz
Refering-URL: 
Root-URL: 
Title: Application of the Optimal Class-Selective Rejection Rule to the Detection of Abnormalities in OCR Databases  
Author: Thien M. HA 
Keyword: CR Categories and Subject Descriptors: I.5.0 [Pattern Recognition]: General; I.5.1 [Pattern Recognition]: Models; I.5.2 [Pattern Recognition]: Design Methodol ogy; I.5.m [Pattern Recognition]: Decision. Key Words: error estimation, truthing error, foreign handwriting style, segmenta tion error, sloppy handwriting.  
Address: Neubruckstr. 10, CH-3012 Berne, Switzerland  
Affiliation: University of Berne Institut fur Informatik und Angewandte Mathematik  
Email: E-Mail: haminh@iam.unibe.ch  
Phone: Phone: +41 31 631 86 99 Fax.: +41 31 631 39 65  
Date: May 22, 1996  
Abstract: Building large Optical Character Recognition (OCR) databases is a time-consuming and tedious work. Moreover, the process is error-prone due to the difficulty in segmentation and the uncertainty in labelling. When the database is very large, say one million patterns, human errors due to fatigue and inattention become a critical factor. This report discusses one method to alleviate the burden caused by these problems. Specifically, the method allows an automatic detection of abnormalities, e.g. mislabelling, and thus may contribute to clean up a labelled database. The method is based on a recently proposed optimum class-selective rejection rule. As a test case, the method is applied to the NIST databases containing nearly 300'000 handwritten numerals. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C.K. Chow, </author> <title> "On Optimum Recognition Error and Reject Tradeoff," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. IT-16, No. 1, </volume> <pages> pp. 41-46, </pages> <month> January </month> <year> 1970. </year>
Reference-contexts: In other words, the error rate at any level can be estimated without knowing the true classes of the patterns. (Such a property is also shared by other rules <ref> [1, 2] </ref>.) In particular, the Bayes error rate is given by e Bayes = e (t ope = 1 ) = 2 t dn (t) (10) 3 Detection of Abnormalities The detection of abnormalities is based on the comparison of two estimations of the e (n) curve using two different methods. <p> In the second stage the final i th output is computed by linear interpolation between C low and C high via a weighting factor w 2 <ref> [0; 1] </ref>, followed by a normalisation so that the final outputs sum up to 1: ^ P i (x) = F norm [(1 w) ^ P i (x) ^ P i (x) + w (1) (2) 2 Of course, the weighting factor must be experimentally determined.
Reference: [2] <author> P.A. Devijver, </author> <title> "Error and Reject Tradeoff for Nearest Neighbor Decision Rules," </title> <editor> in G. Tacconi (Ed.) </editor> <booktitle> Aspects of Signal Processing,, Part 2, </booktitle> <address> D. </address> <publisher> Reidel Publishing Company, Dordrecht-Holland, </publisher> <pages> pp. 525-538, </pages> <year> 1977. </year>
Reference-contexts: In other words, the error rate at any level can be estimated without knowing the true classes of the patterns. (Such a property is also shared by other rules <ref> [1, 2] </ref>.) In particular, the Bayes error rate is given by e Bayes = e (t ope = 1 ) = 2 t dn (t) (10) 3 Detection of Abnormalities The detection of abnormalities is based on the comparison of two estimations of the e (n) curve using two different methods.
Reference: [3] <author> R.O. Duda and P.E. Hart, </author> <title> Pattern Classification and Scene Analysis, </title> <publisher> John Wiley & Sons, </publisher> <year> 1973. </year>
Reference-contexts: P i (x) P (i=x) = p (x) where p (x=i) is the i th class conditional probability density function (p.d.f.), i is the a priori probability of observing the i th class, P N p (x) = j=1 is the absolute probability density function <ref> [3, 5] </ref>. It follows immediately that the posterior probabilities sum up to 1, i.e., N X P i (x) = 1 (3) A decision rule examines the posterior probabilities P i (x) and assigns to the input pattern x a number of classes.
Reference: [4] <author> K. Fukunaga and D.L. Kessel, </author> <title> "Application of Optimum Error-Reject Functions," </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. IT-18, </volume> <pages> pp. 814-817, </pages> <month> November </month> <year> 1972. </year>
Reference-contexts: In our study we use the validation set from SD3 and thus N s = 10000. The second estimation method was proposed by Fukunaga and Kessel <ref> [4] </ref>. This method estimates the error rate at zero rejection level by using only unlabelled patterns.
Reference: [5] <author> K. Fukunaga, </author> <title> Introduction to Statistical Pattern Recognition, second edition, </title> <publisher> Academic Press, </publisher> <year> 1990. </year>
Reference-contexts: P i (x) P (i=x) = p (x) where p (x=i) is the i th class conditional probability density function (p.d.f.), i is the a priori probability of observing the i th class, P N p (x) = j=1 is the absolute probability density function <ref> [3, 5] </ref>. It follows immediately that the posterior probabilities sum up to 1, i.e., N X P i (x) = 1 (3) A decision rule examines the posterior probabilities P i (x) and assigns to the input pattern x a number of classes.
Reference: [6] <author> Thien M. Ha and H. Bunke, </author> <title> "Handwritten Numeral Recognition by Perturbation Method", </title> <booktitle> Proc. of the Fourth International Workshop on Frontiers of Handwriting Recognition, </booktitle> <address> Taipei, Taiwan, </address> <month> Dec. </month> <pages> 7-9, </pages> <year> 1994, </year> <pages> pp. 97-106. 15 </pages>
Reference-contexts: Subsystem 2 estimates the posterior probabilities by first extracting a contour-based feature vector from the input pattern and then feeding it to a fully connected feed-forward multi-layer perceptron with architecture 104 : 60 : 10 (60 hidden nodes). Details about the two feature extraction methods can be found in <ref> [6] </ref>. Both neural networks are trained with the back-propagation algorithm [11] using the first 40000 numerals from SD3 and the next 10000 numerals are used to control the stopping of the training process. The combined system is built via a two-stage combination scheme.
Reference: [7] <author> Thien M. Ha, </author> <title> "An Optimum Decision Rule for Pattern Recognition," </title> <type> Tech--nical Report IAM-95-009, </type> <institution> Institute of Computer Science and Applied Mathematics, University of Berne, Switzerland, </institution> <month> November </month> <year> 1995. </year> <note> (Anonymous ftp to iamftp.unibe.ch, Directory pub/TechReports/1995/) </note>
Reference-contexts: The method allows an efficient detection of abnormal patterns resulted from the building process. The detection is based on the comparison between two error estimation methods. The first is the standard error-count method whereas the second results from the application of a recently proposed optimum class-selective rejection rule <ref> [7, 9] </ref>. The latter estimation method does not need the knowledge of the label of each pattern and is thus independent of the labelling [8]. Since the latter method does not make use of the labelling, it is insensitive to an eventual mislabelling which affects the first estimation method. <p> The way in which assignment is achieved defines the decision rule. See [8] for a review of various decision rules. By definition the optimum class-selective rejection rule minimises the error rate for a given average number of classes <ref> [7] </ref>. <p> The optimum class-selective rejection rule assigns to pattern x all classes whose posterior probability is greater than a pre-specified threshold t. If there exist no such classes, the rule simply selects the (a) single best class <ref> [7, 9] </ref>. The domain of the pre-specified threshold is 0 t 2 When t = 1 2 , the rule is equivalent to the Bayes rule, i.e., select the (a) single best class. When t = 0, the rule assigns to a pattern all classes whose posterior probability is non-zero.
Reference: [8] <author> Thien M. Ha, </author> <title> "On Functional Relation between Recognition Error and Class-Selective Reject," </title> <type> Technical Report IAM-96-007, </type> <institution> Institute of Computer Science and Applied Mathematics, University of Berne, Switzerland, </institution> <month> March </month> <year> 1996. </year> <note> (Anonymous ftp to iamftp.unibe.ch, Directory pub/TechReports/1996/) </note>
Reference-contexts: The first is the standard error-count method whereas the second results from the application of a recently proposed optimum class-selective rejection rule [7, 9]. The latter estimation method does not need the knowledge of the label of each pattern and is thus independent of the labelling <ref> [8] </ref>. Since the latter method does not make use of the labelling, it is insensitive to an eventual mislabelling which affects the first estimation method. Therefore, a discrepancy between the two methods would indicate the presence of some abnormalities in the building process. <p> The way in which assignment is achieved defines the decision rule. See <ref> [8] </ref> for a review of various decision rules. By definition the optimum class-selective rejection rule minimises the error rate for a given average number of classes [7]. <p> It should be clear that the time complexity of this rule is O (N ). It turns out that it is possible to express the error rate directly as a function of the average number of classes via the Stieltjes integral <ref> [8] </ref> e (t ope ) = t=0 where `ope' stands for operating. (For an introduction to the Stieltjes integral, see [13].) The marvelous feature of the above equation is that it allows the computation of the error rate at any level t ope from n (t) solely and that the latter
Reference: [9] <author> Thien M. Ha, </author> <title> "An Optimum Class-Selective Rejection Rule for Pattern Recognition," </title> <booktitle> The 13th International Conference on Pattern Recognition, </booktitle> <month> Aug. </month> <pages> 25-30, </pages> <year> 1996, </year> <institution> Vienna, Austria. </institution> <note> To appear. </note>
Reference-contexts: The method allows an efficient detection of abnormal patterns resulted from the building process. The detection is based on the comparison between two error estimation methods. The first is the standard error-count method whereas the second results from the application of a recently proposed optimum class-selective rejection rule <ref> [7, 9] </ref>. The latter estimation method does not need the knowledge of the label of each pattern and is thus independent of the labelling [8]. Since the latter method does not make use of the labelling, it is insensitive to an eventual mislabelling which affects the first estimation method. <p> The optimum class-selective rejection rule assigns to pattern x all classes whose posterior probability is greater than a pre-specified threshold t. If there exist no such classes, the rule simply selects the (a) single best class <ref> [7, 9] </ref>. The domain of the pre-specified threshold is 0 t 2 When t = 1 2 , the rule is equivalent to the Bayes rule, i.e., select the (a) single best class. When t = 0, the rule assigns to a pattern all classes whose posterior probability is non-zero.
Reference: [10] <author> J.J. Hull, </author> <title> "A Database for Handwritten Text Recognition Research," </title> <journal> IEEE Transaction on Pattern Analysis and Machine Intelligence, </journal> <volume> Vol. 16, No. 5, </volume> <pages> pp. 550-554, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: 1 Introduction It has been recognised since a long time that the successful design of a pattern recognition system strongly depends on the availability of a large, representative and correctly labelled training set of data <ref> [12, 10] </ref>. Unfortunately, building such a database is a very time-consuming, tedious, and error-prone task. This report discusses one method that could help alleviating the burden of the building process. The method allows an efficient detection of abnormal patterns resulted from the building process.
Reference: [11] <editor> C.G.Y. Lau (Editor), </editor> <booktitle> Neural Networks: Theoretical Foundations and Analysis, </booktitle> <publisher> IEEE Press, </publisher> <year> 1992. </year>
Reference-contexts: Details about the two feature extraction methods can be found in [6]. Both neural networks are trained with the back-propagation algorithm <ref> [11] </ref> using the first 40000 numerals from SD3 and the next 10000 numerals are used to control the stopping of the training process. The combined system is built via a two-stage combination scheme.
Reference: [12] <author> G. Nagy, </author> <title> "At the Frontiers of OCR," </title> <journal> Proceedings of the IEEE, Special Issue on Optical Character Recognition, </journal> <volume> Vol. 80, No. 7, </volume> <pages> pp. 1093-1100, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: 1 Introduction It has been recognised since a long time that the successful design of a pattern recognition system strongly depends on the availability of a large, representative and correctly labelled training set of data <ref> [12, 10] </ref>. Unfortunately, building such a database is a very time-consuming, tedious, and error-prone task. This report discusses one method that could help alleviating the burden of the building process. The method allows an efficient detection of abnormal patterns resulted from the building process.
Reference: [13] <author> S.M. Ross, </author> <title> A First Course in Probability, third edition, </title> <publisher> Macmillan Publishing Company, </publisher> <year> 1988. </year>
Reference-contexts: It turns out that it is possible to express the error rate directly as a function of the average number of classes via the Stieltjes integral [8] e (t ope ) = t=0 where `ope' stands for operating. (For an introduction to the Stieltjes integral, see <ref> [13] </ref>.) The marvelous feature of the above equation is that it allows the computation of the error rate at any level t ope from n (t) solely and that the latter can be estimated from unlabelled patterns, by just counting the average number of selected classes, see Eq. (7).
Reference: [14] <author> R.A. Wilkinson, J. Geist, S. Janet, P.J. Grother, C.J.C. Burges, R. Creecy, B. Hammond, J.J. Hull, N.W. Larsen, T.P. Vogl, and C.L. Wilson, </author> <title> The First Census Optical Character Recognition Systems Conference, The U.S. </title> <institution> Bureau of Census and the National Institute of Standards and Technology, </institution> <type> Technical Report #NISTIR 4912, </type> <address> Gaithersburg, MD, </address> <month> Aug. </month> <year> 1992. </year> <month> 16 </month>
Reference-contexts: the detection method and the next section proposes a nomenclature of detected abnormalities. 1.1 NIST Databases Two databases, namely, SD3 and SD7, were provided by the American National Institute of Standards and Technology (NIST) in 1992 as parts of a conference to assess the state-of-the-art in isolated handwritten character recognition <ref> [14] </ref>. Twenty-nine groups from Europe and North America participated to compare the performance of their OCR systems. In total, 47 systems, both commercial and research, were presented. The databases contain isolated numerals (digits) as well as upper- and lower-case letters. <p> The training and validation sets as well as the first test set (Test1) are subsets of SD3. The second test set (Test2) is identical to SD7, which has been recognised as having a statistical distribution different from that of SD3 <ref> [14] </ref>. 2 Optimum Class-Selective Rejection Rule In statistical pattern recognition, the probability that a given sample or pattern x 2 X, the pattern space, belongs to the i th class, in a N -class problem, is provided by the posterior probability P (i=x) through the Bayes formula: 3 Table 1: Partition
References-found: 14

