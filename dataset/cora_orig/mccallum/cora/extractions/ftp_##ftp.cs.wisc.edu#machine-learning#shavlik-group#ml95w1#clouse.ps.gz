URL: ftp://ftp.cs.wisc.edu/machine-learning/shavlik-group/ml95w1/clouse.ps.gz
Refering-URL: http://www.cs.wisc.edu/~shavlik/ml95w1/procs.html
Root-URL: 
Email: clouse@cs.umass.edu  
Title: Learning from an Automated Training Agent  
Author: Jeffery A. Clouse 
Address: Amherst, MA 01003  
Affiliation: Department of Computer Science University of Massachusetts  
Abstract: A learning agent employing reinforcement learning is hindered because it only receives the critic's sparse and weakly informative training information. We present an approach in which an automated training agent may also provide occasional instruction to the learner in the form of actions for the learner to perform. The learner has access to both the critic's feedback and the trainer's instruction. In the experiments, we vary the level of the trainer's interaction with the learner, from allowing the trainer to instruct the learner at almost every time step, to not allowing the trainer to respond at all. We also vary a parameter that controls how the learner incorporates the trainer's actions. The results show significant reductions in the average number of training trials necessary to learn to perform the task.
Abstract-found: 1
Intro-found: 1
Reference: <author> Barto, A. G., Bradtke, S. J., & Singh, S. P. </author> <year> (1993). </year> <title> Learning to act using real-time dynamic programming, </title> <type> (COINS Technical Report 93-02), </type> <institution> Amherst, MA: University of Massachusetts, Department of Computer and Information Science. </institution>
Reference: <author> Clouse, J. A., & Utgoff, P. E. </author> <year> (1992). </year> <title> A teaching method for reinforcement learning. </title> <booktitle> Machine Learning: Proceedings of the Ninth International Conference (pp. </booktitle> <pages> 92-101). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Gardner, M. </author> <year> (1973). </year> <title> Mathematical games. </title> <journal> Scientific American, </journal> <volume> 228, </volume> <pages> 108. </pages>
Reference-contexts: We present the details of the experiments and a discussion of the results in the following sections. 3.1 PROBLEM DOMAIN The problem domain we use is a car navigation task based on the Race Track game <ref> (Gardner, 1973) </ref>. Except for minor differences in the simulation, this is the same domain presented in Clouse & Utgoff (1992).
Reference: <author> Gordon, D., & Subramanian, D. </author> <year> (1994). </year> <title> A multistrat-egy learning scheme for agent knowledge acquisition. </title> <journal> Informatica , 17, </journal> <pages> 331-346. </pages> <address> Gullapalli, </address> <month> Vijaykumar </month> <year> (1992). </year> <title> Reinforcement learning and its application to control. </title> <type> Doctoral dissertation, </type> <institution> Computer and Information Science, University of Massachusetts, </institution> <address> Amherst, MA. </address>
Reference: <author> Lin, </author> <month> Long-Ji </month> <year> (1991). </year> <title> Programming robots using reinforcement learning and teaching. </title> <booktitle> Proceedings of the Ninth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 781-786). </pages> <address> Anaheim, CA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Lin, </author> <month> Long-Ji </month> <year> (1992). </year> <title> Self-improving reactive agents based on reinforcement learning, planning and teaching. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> 293-321. </pages>
Reference: <author> Maclin, R., & Shavlik, J. W. </author> <year> (1994). </year> <title> Incorporating advice into agents that learn from reinforcements. </title> <booktitle> Proceedings of the Twelfth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 694-699). </pages> <address> Seattle, WA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Sutton, R. S. </author> <year> (1984). </year> <title> Temporal credit assignment in reinforcement learning. </title> <type> Doctoral dissertation, </type> <institution> Department of Computer and Information Science, University of Massachusetts, </institution> <address> Amherst, MA. </address>
Reference-contexts: The automated agent performs on-line, trial-and-error experiments in the task, receiving occasional feedback from an environmental critic in the form of simple, scalar signals that indicate the value of the state in which the feedback is received. Using methods such as actor/critic architectures <ref> (Sutton, 1984) </ref> and Q-learning (Watkins, 1989), the learner adapts its policy to ensure that it acquires high levels of scalar feedback in the future. Many researchers have demonstrated success with reinforcement learning methods (Gullapalli, 1992; Lin, 1992; Whitehead, 1992; Barto, Bradtke & Singh, 1993).
Reference: <author> Thrun, Sebastian B., & Moller, </author> <title> Knut (1992). Active exploration in dynamic environments. </title> <editor> In Moody, Hanson & Lippman (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: At the 95% instruction level for the 0:01 reinforcement level, the average number of trials was over 15; 000. These very high averages occur possibly because, as the learner relies increasingly on the trainer, it performs less exploration, which is an important aspect of reinforcement learning <ref> (Thrun & Moller, 1992) </ref>. Also, at these high levels of instruction, almost all of the actions are coming from the trainer, which doesn't fail at the task. The learner may be performing poorly because it is not allowed to learn from failure.
Reference: <author> Towell, G., Shavlik, J., & Noordewier, M. </author> <year> (1990). </year> <title> Refinement of approximate domain theories by knowledge-based neural networks. </title> <booktitle> Proceedings of the Eighth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 861-866). </pages> <address> Boston, MA: </address> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: Two related systems address the question by circumventing the reinforcement learning methods. Maclin and Shavlik (1994) apply the the trainer's compiled instruction directly to the learner's policy using techniques from knowledge-based neural networks <ref> (Towell, Shavlik & Noordewier, 1990) </ref>; thus, converting high-level advice into weight changes applied to the network that represents the learner's policy. Gordon and Sub-ramanian (1994) rely on genetic algorithms, instead of reinforcement learning, in order to learn from environmental feedback.
Reference: <author> Utgoff, P. E., & Clouse, J. A. </author> <year> (1991). </year> <title> Two kinds of training information for evaluation function learning. </title> <booktitle> Proceedings of the Ninth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 596-600). </pages> <address> Anaheim, CA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: presenting how our approach handles each, and describing the approaches of related research. * What form of instruction does the training agent provide? Related work has examined training agents that provide the learner with: actions that the learner should perform (Clouse & Utgoff, 1992; Lin, 1992), the exact state values <ref> (Utgoff & Clouse, 1991) </ref>, and if-then rules (Gordon & Subramanian, 1994; Maclin & Shav-lik, 1994). In our approach, the automated trainer instructs the learning agent by giving actions that the learner should perform. We choose this form of advice for two main reasons. <p> In an early system that employs an automated strategy for supplying instruction, the learner improves its performance by asking for help when it has low confidence in its own action choices, based on the size of the difference of previous state evaluations <ref> (Utgoff & Clouse, 1991) </ref>. Deciding when the trainer should instruct is still an open question.
Reference: <author> Watkins, C.J.C.H. </author> <year> (1989). </year> <title> Learning with delayed rewards. </title> <type> Doctoral dissertation, </type> <institution> Psychology Department, Cambridge University. </institution>
Reference-contexts: The automated agent performs on-line, trial-and-error experiments in the task, receiving occasional feedback from an environmental critic in the form of simple, scalar signals that indicate the value of the state in which the feedback is received. Using methods such as actor/critic architectures (Sutton, 1984) and Q-learning <ref> (Watkins, 1989) </ref>, the learner adapts its policy to ensure that it acquires high levels of scalar feedback in the future. Many researchers have demonstrated success with reinforcement learning methods (Gullapalli, 1992; Lin, 1992; Whitehead, 1992; Barto, Bradtke & Singh, 1993). <p> The environmental critic gives one of three reinforcement signals at each step: 1:0 when the car crosses the finish line, 1:0 when the car crosses a track boundary, and 0:01 otherwise, to simulate fuel consumption. 3.2 REINFORCEMENT LEARNING METHOD The learner employs a modified Q-learning reinforcement learning method <ref> (Watkins, 1989) </ref> to develop its policy. The two modifications allows the learner to incorporate the trainer's proffered actions. Each action that the learner can choose has a separate Q-function, each of which is a linear neuron represented as a weight vector.

References-found: 12

