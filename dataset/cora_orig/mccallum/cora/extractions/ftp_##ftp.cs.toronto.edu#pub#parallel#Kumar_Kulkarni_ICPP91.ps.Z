URL: ftp://ftp.cs.toronto.edu/pub/parallel/Kumar_Kulkarni_ICPP91.ps.Z
Refering-URL: http://www.cs.toronto.edu/~kulki/pubs_abs.html
Root-URL: 
Title: Generalized Unimodular Loop Transformations for Distributed Memory Multiprocessors  
Author: K G Kumar D Kulkarni, A Basu 
Keyword: Parallelizing Compilers, Restructuring Transformations, Loop Partitioning, Iteration Spaces, Dependence Vectors.  
Address: 2/1 Brunton Road, Bangalore 560 025, India  
Affiliation: Center for Development of Advanced Computing  
Note: International Conference of Parallel Processing, 1991.  
Abstract: In this paper, we present a generalized unimodular loop transformation as a simple, systematic and elegant method for partitioning the iteration spaces of nested loops for execution on distributed memory multiprocessors. We present a methodology for deriving the transformations that internalize multiple dependences in a multidimensional iteration space without resulting in a deadlocking situation. We then derive the general expression for the bounds of the transformed loops in terms of the bounds of the original space and the transformation matrix elements.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> U. Banerjee, </author> <title> Dependence Analysis for Supercomputing, </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1988. </year>
Reference-contexts: A dependence distance vector is positive if corresponding direction vector has a nonzero leading element as +1 <ref> [1] </ref>. A distance vector is negative otherwise. For example, in Figure 2 the distance vectors are (2,1) and (3,-2), and the corresponding direction vectors (1,1) and (1,-1). Both the vectors are positive. <p> Set i = 1. Step 2: If i = N , go to step 6. Step 3: If U 0 <ref> [1; i] </ref> = 0, then set i to i + 1 and go to step 2. Step 4: If U 0 [1; i] U 0 [1; i + 1] then set q = b (jU 0 [1; i + 1]j=jU 0 [1; i]j)c; set s = sign (U 0 [1; i <p> Set i = 1. Step 2: If i = N , go to step 6. Step 3: If U 0 <ref> [1; i] </ref> = 0, then set i to i + 1 and go to step 2. Step 4: If U 0 [1; i] U 0 [1; i + 1] then set q = b (jU 0 [1; i + 1]j=jU 0 [1; i]j)c; set s = sign (U 0 [1; i + 1]=U 0 [1; i]); and subtract s fl q times column i from column i + 1. <p> Set i = 1. Step 2: If i = N , go to step 6. Step 3: If U 0 [1; i] = 0, then set i to i + 1 and go to step 2. Step 4: If U 0 [1; i] U 0 <ref> [1; i + 1] </ref> then set q = b (jU 0 [1; i + 1]j=jU 0 [1; i]j)c; set s = sign (U 0 [1; i + 1]=U 0 [1; i]); and subtract s fl q times column i from column i + 1. <p> Step 3: If U 0 [1; i] = 0, then set i to i + 1 and go to step 2. Step 4: If U 0 [1; i] U 0 <ref> [1; i + 1] </ref> then set q = b (jU 0 [1; i + 1]j=jU 0 [1; i]j)c; set s = sign (U 0 [1; i + 1]=U 0 [1; i]); and subtract s fl q times column i from column i + 1. Step 5: Interchange columns i and i + 1. Go to step 2. <p> Step 3: If U 0 <ref> [1; i] </ref> = 0, then set i to i + 1 and go to step 2. Step 4: If U 0 [1; i] U 0 [1; i + 1] then set q = b (jU 0 [1; i + 1]j=jU 0 [1; i]j)c; set s = sign (U 0 [1; i + 1]=U 0 [1; i]); and subtract s fl q times column i from column i + 1. Step 5: Interchange columns i and i + 1. Go to step 2. <p> Step 4: If U 0 [1; i] U 0 <ref> [1; i + 1] </ref> then set q = b (jU 0 [1; i + 1]j=jU 0 [1; i]j)c; set s = sign (U 0 [1; i + 1]=U 0 [1; i]); and subtract s fl q times column i from column i + 1. Step 5: Interchange columns i and i + 1. Go to step 2. Step 6: Now each of U 0 [1; 1] to U 0 [1; N 1] is zero. <p> Step 4: If U 0 <ref> [1; i] </ref> U 0 [1; i + 1] then set q = b (jU 0 [1; i + 1]j=jU 0 [1; i]j)c; set s = sign (U 0 [1; i + 1]=U 0 [1; i]); and subtract s fl q times column i from column i + 1. Step 5: Interchange columns i and i + 1. Go to step 2. Step 6: Now each of U 0 [1; 1] to U 0 [1; N 1] is zero. <p> Step 5: Interchange columns i and i + 1. Go to step 2. Step 6: Now each of U 0 <ref> [1; 1] </ref> to U 0 [1; N 1] is zero. If U 0 [1; N ] is negative, multiply column N of U 0 by -1. Stop. <p> Step 5: Interchange columns i and i + 1. Go to step 2. Step 6: Now each of U 0 [1; 1] to U 0 <ref> [1; N 1] </ref> is zero. If U 0 [1; N ] is negative, multiply column N of U 0 by -1. Stop. <p> Step 5: Interchange columns i and i + 1. Go to step 2. Step 6: Now each of U 0 [1; 1] to U 0 [1; N 1] is zero. If U 0 <ref> [1; N ] </ref> is negative, multiply column N of U 0 by -1. Stop. The matrix U resulting from the removal of the first row of U 0 at this stage is the desired unimodular matrix. 2 Example 1: Consider a dependence vector (3,1,0) in a 3-D iteration space.
Reference: [2] <author> U. Banerjee, </author> <title> A Theory of Loop Permutations, </title> <booktitle> proceedings of the second Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <year> 1989. </year> <month> 17 </month>
Reference: [3] <author> U. Banerjee, </author> <title> Unimodular Transformations of Double Loops, </title> <booktitle> proceedings of the third Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <address> Irvine, California, </address> <month> August 1-3, </month> <year> 1990. </year>
Reference-contexts: Further, unlike techniques in some earlier work [10] that are based on the hyperplane method [8], and which are more appropriate for inner loop parallelization, we regard coarse grain or outer loop parallelization as a more natural choice for distributed memory multiprocessors. fl kumar@cdacb.ernet.in 1 Unimodular transformations introduced in <ref> [3] </ref> are an innovative and unified way of viewing a host of different loop transformations. It is shown in [3] that the general hyper-plane method, loop reversal, and loop interchange can be viewed in the uniform frame work of unimodular transformations. <p> are more appropriate for inner loop parallelization, we regard coarse grain or outer loop parallelization as a more natural choice for distributed memory multiprocessors. fl kumar@cdacb.ernet.in 1 Unimodular transformations introduced in <ref> [3] </ref> are an innovative and unified way of viewing a host of different loop transformations. It is shown in [3] that the general hyper-plane method, loop reversal, and loop interchange can be viewed in the uniform frame work of unimodular transformations. The optimism has it that all possible loop transformations can be formulated as unimodular transformations [3]. <p> It is shown in <ref> [3] </ref> that the general hyper-plane method, loop reversal, and loop interchange can be viewed in the uniform frame work of unimodular transformations. The optimism has it that all possible loop transformations can be formulated as unimodular transformations [3]. <p> We achieve the desired partitioning through unimodular transformations. The concept of unimodular transformations, introduced by Banerjee <ref> [3] </ref>, provides an elegant generalization of various transformations of a nested loop. It is a new perspective of the whole list of transformations: loop interchange, loop reversal, loop skewing and inner/outer loop parallelization. <p> That is, u 11 = b and u 21 = a. And u 12 ; u 22 any pair of integers such that au 12 + bu 22 0 and U is unimodular. In the shared memory case, according to the principle of program transformations <ref> [3] </ref>, the transformation is valid iff each distance vector d 00 = (a 00 ; b 00 ) remains positive after the transformation. <p> Proof: The proof is constructive and is based on Algorithm 5.4.1 suggested in <ref> [3] </ref>. For any dependence vector d of length N , the algorithm yields a N by N unimodular matrix U such that U:d = (g; 0; 0; :::; 0).
Reference: [4] <author> E.H. D'Hollander, </author> <title> Partitioning and Labeling of Index Sets in Do Loops with Constant Dependence Vectors, </title> <booktitle> proceedings of International Conference on Parallel Processing, </booktitle> <volume> Vol. 2, </volume> <pages> pp 139- 144, </pages> <year> 1989. </year>
Reference-contexts: Efficient execution of these programs on distributed memory systems demands a methodology for partitioning the iteration spaces of these nested loops, so that different partitions can be mapped to different processors. Schemes such as <ref> [4, 9, 12] </ref> look for independent iteration partitions, such that iterations within a partition are dependent whereas those belonging to different partitions are totally independent. <p> It is our view that flow dependence taken abstractly, without reference to the individual variables involved, should form the basis for partitioning. Examples of those that achieve the latter objective are <ref> [9, 12, 4] </ref>. In [9] and [12] the maximum cardinality of partitions of the iteration spaces into independent groups is estimated. They however do not provide a method of identifying the elements of the groups. In [4] the above two methods are extended. <p> Examples of those that achieve the latter objective are [9, 12, 4]. In [9] and [12] the maximum cardinality of partitions of the iteration spaces into independent groups is estimated. They however do not provide a method of identifying the elements of the groups. In <ref> [4] </ref> the above two methods are extended. It also provides a polynomial time algorithm to compute the elements in each partition. Since these methods look for complete independence of the partitions they result in very little parallelism.
Reference: [5] <author> C.T. King and L.M. Ni, </author> <title> Grouping in Nested Loops for Parallel Execution on Mul-ticomputers, </title> <booktitle> proceedings of International Conference on Parallel Processing, </booktitle> <volume> Vol. 2, </volume> <pages> pp 31-38, </pages> <year> 1989. </year>
Reference-contexts: Since these methods look for complete independence of the partitions they result in very little parallelism. A related area, which has received considerable attention lately, is that of grouping together iterations to obtain reduced iteration spaces. These techniques, which have been termed tiling [13, 11] or grouping <ref> [5] </ref> serve the purpose of combining the communications associated with each group in order to save on the relatively high setup time for individual communications. They also result in smaller and less complex iteration space for further partitioning. <p> They also result in smaller and less complex iteration space for further partitioning. In [13], tiling is achieved by multidimensional strip mining and in [11] by identifying suitable tiling hyperplanes. Both works describe the choice of optimum shapes and sizes of the tiles and conditions for deadlock free tiling. <ref> [5] </ref> investigates the formation of groups of fixed size along the direction of any dependence vector.
Reference: [6] <author> D Kulkarni, K.G. Kumar, A. Basu, and A. Paulraj, </author> <title> Loop Partitioning Unimodular Transformations for Distributed Memory Multiprocessors, </title> <booktitle> to be presented at the Fifth International Parallel Processing Symposium, </booktitle> <month> March, </month> <year> 1991. </year>
Reference-contexts: It is shown in [3] that the general hyper-plane method, loop reversal, and loop interchange can be viewed in the uniform frame work of unimodular transformations. The optimism has it that all possible loop transformations can be formulated as unimodular transformations [3]. In our earlier work <ref> [6, 7] </ref> we have presented the problem of partitioning the iteration space of a nested double loop for parallel execution on a distributed memory machine as one of finding a unimodular transformation called rotation which changes a nested loop into a new loop with outer loop parallelization. <p> We also provided a combined metric that helps choose the best direction among the available dependence distance vectors. In this paper, we present a generalization of the results of <ref> [6, 7] </ref> for arbitrary loop nest levels and to internalize several dependences. We present a systematic method for obtaining the unimodular matrix defining the transformation which is also guaranteed not to give rise to a deadlocking situation. <p> We then derive the generalized expressions for the bounds of the transformed loops for arbitrary iteration space dimensions. The rest of the paper is organized as follows. Section 2 introduces the problem of iteration space partitioning and summarizes the results in <ref> [6, 7] </ref>. Section 3 presents our methodology of obtaining a generalized transformation for n-dimensional iteration spaces and for internalizing several dependences. Section 4 presents the derivation of the general expressions for loop bounds of the transformed space. <p> We then provide a brief introduction to our approach to the problem in the context of perfectly nested double loops. The details can be found in <ref> [6, 7] </ref>. 2.1 Iteration Space Partitioning We consider only perfectly nested loops with constant loop limits and arbitrary nesting levels. The set of iterations defined by a nested loop of level n can be denoted as a subset of R n , the set of all real n-vectors. <p> We can form a metric that combines the parameters P; V andC in an architecture dependent way <ref> [6, 7] </ref>. 3 Extension to N-D Iteration Spaces Discussion in the previous section was restricted to a doubly nested loop and involved the internalization of exactly one dependence vector.In this section we extend the framework introduced in [6, 7] to N-dimensional (N-D) iteration spaces and for internalizing more than one dependence <p> metric that combines the parameters P; V andC in an architecture dependent way <ref> [6, 7] </ref>. 3 Extension to N-D Iteration Spaces Discussion in the previous section was restricted to a doubly nested loop and involved the internalization of exactly one dependence vector.In this section we extend the framework introduced in [6, 7] to N-dimensional (N-D) iteration spaces and for internalizing more than one dependence vector. <p> We are currently looking for transformations which permute the initial iteration space before applying a unimodular loop transformation. These unified transformations can then internalize any subset of the dependences without giving rise to deadlocks. We are also deriving the expressions for the parameters of the transformations suggested in <ref> [6, 7] </ref> for the generalized transformation presented in this paper. These parameters are parallelism factor, the load imbalance and the volume of communication. We are also interested in investigating the correlation of these parameters with a chosen machine model and the choice of an optimum transformation for any given model.
Reference: [7] <author> D Kulkarni, K.G. Kumar, A. Basu, and A. Paulraj, </author> <title> Loop Partitioning for Distributed Memory Multiprocessors as Unimodular Transformations, </title> <booktitle> submitted to the ACM International Conference on Supercomputing, </booktitle> <month> June, </month> <year> 1991. </year>
Reference-contexts: It is shown in [3] that the general hyper-plane method, loop reversal, and loop interchange can be viewed in the uniform frame work of unimodular transformations. The optimism has it that all possible loop transformations can be formulated as unimodular transformations [3]. In our earlier work <ref> [6, 7] </ref> we have presented the problem of partitioning the iteration space of a nested double loop for parallel execution on a distributed memory machine as one of finding a unimodular transformation called rotation which changes a nested loop into a new loop with outer loop parallelization. <p> We also provided a combined metric that helps choose the best direction among the available dependence distance vectors. In this paper, we present a generalization of the results of <ref> [6, 7] </ref> for arbitrary loop nest levels and to internalize several dependences. We present a systematic method for obtaining the unimodular matrix defining the transformation which is also guaranteed not to give rise to a deadlocking situation. <p> We then derive the generalized expressions for the bounds of the transformed loops for arbitrary iteration space dimensions. The rest of the paper is organized as follows. Section 2 introduces the problem of iteration space partitioning and summarizes the results in <ref> [6, 7] </ref>. Section 3 presents our methodology of obtaining a generalized transformation for n-dimensional iteration spaces and for internalizing several dependences. Section 4 presents the derivation of the general expressions for loop bounds of the transformed space. <p> We then provide a brief introduction to our approach to the problem in the context of perfectly nested double loops. The details can be found in <ref> [6, 7] </ref>. 2.1 Iteration Space Partitioning We consider only perfectly nested loops with constant loop limits and arbitrary nesting levels. The set of iterations defined by a nested loop of level n can be denoted as a subset of R n , the set of all real n-vectors. <p> We can form a metric that combines the parameters P; V andC in an architecture dependent way <ref> [6, 7] </ref>. 3 Extension to N-D Iteration Spaces Discussion in the previous section was restricted to a doubly nested loop and involved the internalization of exactly one dependence vector.In this section we extend the framework introduced in [6, 7] to N-dimensional (N-D) iteration spaces and for internalizing more than one dependence <p> metric that combines the parameters P; V andC in an architecture dependent way <ref> [6, 7] </ref>. 3 Extension to N-D Iteration Spaces Discussion in the previous section was restricted to a doubly nested loop and involved the internalization of exactly one dependence vector.In this section we extend the framework introduced in [6, 7] to N-dimensional (N-D) iteration spaces and for internalizing more than one dependence vector. <p> We are currently looking for transformations which permute the initial iteration space before applying a unimodular loop transformation. These unified transformations can then internalize any subset of the dependences without giving rise to deadlocks. We are also deriving the expressions for the parameters of the transformations suggested in <ref> [6, 7] </ref> for the generalized transformation presented in this paper. These parameters are parallelism factor, the load imbalance and the volume of communication. We are also interested in investigating the correlation of these parameters with a chosen machine model and the choice of an optimum transformation for any given model.
Reference: [8] <author> L. Lamport, </author> <title> The Parallel Execution of DO Loops, </title> <journal> communications of the ACM, </journal> <volume> 17(2), </volume> <year> 1974. </year>
Reference-contexts: Further, unlike techniques in some earlier work [10] that are based on the hyperplane method <ref> [8] </ref>, and which are more appropriate for inner loop parallelization, we regard coarse grain or outer loop parallelization as a more natural choice for distributed memory multiprocessors. fl kumar@cdacb.ernet.in 1 Unimodular transformations introduced in [3] are an innovative and unified way of viewing a host of different loop transformations. <p> The hyperplane or wavefront method suggested by Lamport <ref> [8] </ref> for parallelizing do loops is the most prominent among the ones which achieve the former objective.
Reference: [9] <author> J.K. Peir and Ron Cytron, </author> <title> Minimum Distance A Method for Partitioning Recurrences for Multiprocessors, </title> <booktitle> proceedings of International Conference on Parallel Processing, </booktitle> <pages> pp 217-224, </pages> <year> 1987. </year>
Reference-contexts: Efficient execution of these programs on distributed memory systems demands a methodology for partitioning the iteration spaces of these nested loops, so that different partitions can be mapped to different processors. Schemes such as <ref> [4, 9, 12] </ref> look for independent iteration partitions, such that iterations within a partition are dependent whereas those belonging to different partitions are totally independent. <p> It is our view that flow dependence taken abstractly, without reference to the individual variables involved, should form the basis for partitioning. Examples of those that achieve the latter objective are <ref> [9, 12, 4] </ref>. In [9] and [12] the maximum cardinality of partitions of the iteration spaces into independent groups is estimated. They however do not provide a method of identifying the elements of the groups. In [4] the above two methods are extended. <p> It is our view that flow dependence taken abstractly, without reference to the individual variables involved, should form the basis for partitioning. Examples of those that achieve the latter objective are [9, 12, 4]. In <ref> [9] </ref> and [12] the maximum cardinality of partitions of the iteration spaces into independent groups is estimated. They however do not provide a method of identifying the elements of the groups. In [4] the above two methods are extended.
Reference: [10] <author> J. Ramanujam and P. Sadayappan, </author> <title> A Methodology for Parallelizing Programs for Multicomputers and Complex Memory Multiprocessors, </title> <booktitle> ACM Conference on Supercomputing, </booktitle> <year> 1989. </year>
Reference-contexts: Thus we view the problem of loop partitioning as one of partitioning the iteration space in a way that all the iterations in a partition are dependent and there is minimal data dependence among different partitions. Further, unlike techniques in some earlier work <ref> [10] </ref> that are based on the hyperplane method [8], and which are more appropriate for inner loop parallelization, we regard coarse grain or outer loop parallelization as a more natural choice for distributed memory multiprocessors. fl kumar@cdacb.ernet.in 1 Unimodular transformations introduced in [3] are an innovative and unified way of viewing <p> The resulting transformation is essentially inner loop parallelization with the planes, corresponding to each of the outer loop instances, executing strictly sequentially. This however is not particularly suited for distributed memory multiprocessors. The wavefront method has served as the basis for several later work. A recent work <ref> [10] </ref> that is aimed at distributed memory systems attempts to partition the nested loops using both iteration and data space graphs.
Reference: [11] <author> J. Ramanujam and P. Sadayappan, </author> <title> Tiling of Iteration Spaces for Multicomputers, </title> <booktitle> proceedings of International Conference on Parallel Processing, </booktitle> <volume> Vol. 2, </volume> <pages> pp 179-186, </pages> <year> 1990. </year>
Reference-contexts: Since these methods look for complete independence of the partitions they result in very little parallelism. A related area, which has received considerable attention lately, is that of grouping together iterations to obtain reduced iteration spaces. These techniques, which have been termed tiling <ref> [13, 11] </ref> or grouping [5] serve the purpose of combining the communications associated with each group in order to save on the relatively high setup time for individual communications. They also result in smaller and less complex iteration space for further partitioning. <p> They also result in smaller and less complex iteration space for further partitioning. In [13], tiling is achieved by multidimensional strip mining and in <ref> [11] </ref> by identifying suitable tiling hyperplanes. Both works describe the choice of optimum shapes and sizes of the tiles and conditions for deadlock free tiling. [5] investigates the formation of groups of fixed size along the direction of any dependence vector.
Reference: [12] <author> W. Shang and J.A.B. Fortes, </author> <title> Independent Partitioning of Algorithms with Uniform Dependencies, </title> <booktitle> proceedings of International Conference on Parallel Processing, </booktitle> <volume> Vol. 2, </volume> <pages> pp 26-33, </pages> <year> 1988. </year>
Reference-contexts: Efficient execution of these programs on distributed memory systems demands a methodology for partitioning the iteration spaces of these nested loops, so that different partitions can be mapped to different processors. Schemes such as <ref> [4, 9, 12] </ref> look for independent iteration partitions, such that iterations within a partition are dependent whereas those belonging to different partitions are totally independent. <p> It is our view that flow dependence taken abstractly, without reference to the individual variables involved, should form the basis for partitioning. Examples of those that achieve the latter objective are <ref> [9, 12, 4] </ref>. In [9] and [12] the maximum cardinality of partitions of the iteration spaces into independent groups is estimated. They however do not provide a method of identifying the elements of the groups. In [4] the above two methods are extended. <p> It is our view that flow dependence taken abstractly, without reference to the individual variables involved, should form the basis for partitioning. Examples of those that achieve the latter objective are [9, 12, 4]. In [9] and <ref> [12] </ref> the maximum cardinality of partitions of the iteration spaces into independent groups is estimated. They however do not provide a method of identifying the elements of the groups. In [4] the above two methods are extended.
Reference: [13] <author> M.J. Wolfe, </author> <title> More Iteration Space Tiling, </title> <booktitle> ACM Conference on Supercomputing, </booktitle> <pages> pp. 655-664, </pages> <year> 1989. </year>
Reference-contexts: Since these methods look for complete independence of the partitions they result in very little parallelism. A related area, which has received considerable attention lately, is that of grouping together iterations to obtain reduced iteration spaces. These techniques, which have been termed tiling <ref> [13, 11] </ref> or grouping [5] serve the purpose of combining the communications associated with each group in order to save on the relatively high setup time for individual communications. They also result in smaller and less complex iteration space for further partitioning. <p> They also result in smaller and less complex iteration space for further partitioning. In <ref> [13] </ref>, tiling is achieved by multidimensional strip mining and in [11] by identifying suitable tiling hyperplanes.
References-found: 13

