URL: ftp://ftp.cs.dartmouth.edu/TR/TR92-179.ps.Z
Refering-URL: http://www.cs.dartmouth.edu/reports/abstracts/TR92-179/
Root-URL: http://www.cs.dartmouth.edu
Email: David.Kotz@Dartmouth.edu  
Author: David Kotz 
Note: (Abstract appeared in 1992 USENIX Workshop on File Systems)  
Date: (revised)  March 17, 1992 Revised May 19, 1992  
Address: Hanover, NH 03755-3551  
Affiliation: Department of Math and Computer Science Dartmouth College  
Pubnum: Dartmouth PCS-TR92-179  
Abstract: Available at URL ftp://ftp.cs.dartmouth.edu/pub/CS-techreports/TR92-179.ps.Z Multiprocessor File System Interfaces fl Abstract Increasingly, file systems for multiprocessors are designed with parallel access to multiple disks, to keep I/O from becoming a serious bottleneck for parallel applications. Although file system software can transparently provide high-performance access to parallel disks, a new file system interface is needed to facilitate parallel access to a file from a parallel application. We describe the difficulties faced when using the conventional (Unix-like) interface in parallel applications, and then outline ways to extend the conventional interface to provide convenient access to the file for parallel programs, while retaining the traditional interface for programs that have no need for explicitly parallel file access. Our interface includes a single naming scheme, a mul-tiopen operation, local and global file pointers, mapped file pointers, logical records, multifiles, and logical coercion for backward compatibility.
Abstract-found: 1
Intro-found: 1
Reference: [Arm90] <author> Katherine Jean Armstrong. </author> <title> Improving file access performance: Cache management for mapped files. </title> <type> Master's thesis, </type> <institution> Univ. of Washington, </institution> <year> 1990. </year>
Reference-contexts: While memory-mapped files have many advantages, they have many disadvantages as a general solution. Unless the address space is segmented, writing segmented files may be difficult. Files typically have different access patterns than virtual memory, possibly requiring different memory management techniques <ref> [Arm90] </ref>. If files are mapped into a 9 distributed shared memory (DSM) system, consistency protocols may need adjustment (since they are normally designed for virtual memory access patterns). Indeed, many operating systems for distributed memory machines do not support DSM, and thus could not easily support memory-mapped files.
Reference: [AS89] <author> Raymond K. Asbury and David S. Scott. </author> <title> FORTRAN I/O on the iPSC/2: </title> <booktitle> Is there read after write? In Fourth Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 129-132, </pages> <year> 1989. </year>
Reference-contexts: The standard interface is there for compatibility, the tools for performance, and the parallel-open interface for a compromise. Intel's file system for their iPSC/2 and iPSC/860 multiprocessors, CFS [Pie89], also provides three interfaces <ref> [AS89] </ref>: standard (conventional); random-sequential access, which uses a self 8 scheduled shared file pointer (allowing atomic append); and coordinated, which is for interleaved access with either a fixed or variable record size. CFS forces each process to open the file independently.
Reference: [Cro88] <author> Thomas W. Crockett. </author> <title> Specification of the operating system interface for parallel file organizations. </title> <type> Publication status unknown (ICASE technical report), </type> <year> 1988. </year>
Reference-contexts: If the file system does not maintain the declustering information for each file, forcing the programmer to specify the set of disks, disk files, or disk blocks, then transparency is lost and the interface is much harder to use. An example of this situation is in <ref> [Cro88] </ref>. Another example is the nCUBE file system prior to 1992, which does not distribute a single file across disks [PFDJ89]. <p> The file system for the newer Intel Paragon appears to be a Unix file system, based on the OSF/1 operating system [Int91], although CFS access modes are still available. Another parallel file system is based on ways to lay out a file on parallel disks <ref> [Cro89, Cro88] </ref>. One interface provides self-scheduled access with a shared file pointer. Another provides individual file pointers. A unified access mode provides the standard interface for compatibility. One deficiency in this interface is that the user must supply a list of disks to the open operation. <p> Each concept directly addresses one or more of the problems outlined in the previous sections. 6.1 Concepts Directory Structure. There should be a single file-naming directory structure for the entire parallel file system. The user should not have to specify the list of disks involved <ref> [Cro88] </ref> or the list of local disk files [PFDJ89] when opening a file. The name structure should be the same for parallel applications as for sequential applications (such as file-maintenance and directory-listing tools). For maximum portability and interoperability, it should appear to be a Unix file system. Multiopen.
Reference: [Cro89] <author> Thomas W. Crockett. </author> <title> File concepts for parallel I/O. </title> <booktitle> In Proceedings of Supercomputing '89, </booktitle> <pages> pages 574-579, </pages> <year> 1989. </year>
Reference-contexts: Typically, these arise from self-scheduled access to the file <ref> [Cro89] </ref>. We call these globally sequential access patterns, or just global access patterns. <p> The file system for the newer Intel Paragon appears to be a Unix file system, based on the OSF/1 operating system [Int91], although CFS access modes are still available. Another parallel file system is based on ways to lay out a file on parallel disks <ref> [Cro89, Cro88] </ref>. One interface provides self-scheduled access with a shared file pointer. Another provides individual file pointers. A unified access mode provides the standard interface for compatibility. One deficiency in this interface is that the user must supply a list of disks to the open operation.
Reference: [DdR92] <author> Erik DeBenedictus and Juan Miguel del Rosario. </author> <title> nCUBE parallel I/O software. </title> <booktitle> In Eleventh Annual IEEE International Phoenix Conference on Computers and Communications (IPCCC), </booktitle> <month> April </month> <year> 1992. </year> <note> To appear. </note>
Reference-contexts: In either case files are declustered over many disks. We call the latter structure Parallel Independent Disks (PID). Examples of multiprocessors using a PID architecture include the Intel [Int88, Int91], nCUBE <ref> [nC90, DdR92, PFDJ89] </ref>, and Kendall Square Research [KSR92] multiprocessors. 3 The Workload Parallel file systems and the applications that use them are not sufficiently mature for us to know what access patterns might be typical. Here we define our expectations for parallel file access patterns in a scientific workload. <p> The original file system for the nCUBE hypercube multiprocessor [PFDJ89] is primitive, in the sense that each disk has a local file system independent of the others, and no global file system is provided. In a new nCUBE file system <ref> [DM91, DdR92, dR92] </ref>, designed around the Unix model, each process specifies a mapping from the bytes of the file to the bytes in its own access stream. The file system specifies a similar mapping, from the bytes in the file to positions on the disks. <p> We define the readp and writep operations, which are the same as read and write, respectively, except that they also return the original file pointer position. Mapped File Pointers. One of the advantages of the nCUBE's mapping functions <ref> [DdR92] </ref> is their ability to remap the address space of the whole file into smaller, contiguous address spaces for each process. Their mapping function maps from (process, pointer) to (position). Each process then sees a single byte stream, indexed by its file pointer, whereas the file is indexed by position.
Reference: [Dib90] <author> Peter C. Dibble. </author> <title> A Parallel Interleaved File System. </title> <type> PhD thesis, </type> <institution> University of Rochester, </institution> <month> March </month> <year> 1990. </year> <month> 16 </month>
Reference-contexts: Programmers need a higher-level interface to easily take advantage of parallel I/O. 5 Existing Multiprocessor File System Interfaces Several researchers have discussed parallel I/O interfaces for MIMD multiprocessors. Dibble, in his design of the Bridge file system <ref> [Dib90] </ref>, defines three interfaces: standard, which is essentially our conventional interface; parallel open, in which a control process issues all the read and write requests, automatically transferring one record in or out of every process; and tools. <p> A given file position may be mapped by any number of processes (including zero). Also note that self-scheduled access, through a global file pointer, is still possible. Logical Records. Dibble <ref> [Dib90] </ref> argues for direct support for logical records in the file system. The Unix file system does not have any built-in support for logical records, in contrast to some traditional systems (typified by commercial mainframes).
Reference: [DM91] <author> Erik DeBenedictus and Peter Madams. </author> <title> nCUBE's parallel I/O with Unix capability. </title> <booktitle> In Sixth Annual Distributed-Memory Computer Conference, </booktitle> <pages> pages 270-277, </pages> <year> 1991. </year>
Reference-contexts: The original file system for the nCUBE hypercube multiprocessor [PFDJ89] is primitive, in the sense that each disk has a local file system independent of the others, and no global file system is provided. In a new nCUBE file system <ref> [DM91, DdR92, dR92] </ref>, designed around the Unix model, each process specifies a mapping from the bytes of the file to the bytes in its own access stream. The file system specifies a similar mapping, from the bytes in the file to positions on the disks.
Reference: [dR92] <author> Juan Miguel del Rosario. </author> <title> High performance parallel I/O on the nCUBE 2. </title> <journal> Institute of Electronics, Information and Communications Engineers (Transactions), </journal> <month> August </month> <year> 1992. </year> <note> To appear. </note>
Reference-contexts: The original file system for the nCUBE hypercube multiprocessor [PFDJ89] is primitive, in the sense that each disk has a local file system independent of the others, and no global file system is provided. In a new nCUBE file system <ref> [DM91, DdR92, dR92] </ref>, designed around the Unix model, each process specifies a mapping from the bytes of the file to the bytes in its own access stream. The file system specifies a similar mapping, from the bytes in the file to positions on the disks.
Reference: [ELS88] <author> Jan Edler, Jim Lipkis, and Edith Schonberg. </author> <title> Memory management in Symunix II: A design for large-scale shared memory multiprocessors. </title> <booktitle> In Proceedings of the 1988 Usenix Supercomputer Workshop, </booktitle> <pages> pages 151-168, </pages> <year> 1988. </year>
Reference-contexts: Multiopen opens the file only once, avoiding repeated directory searches and other overhead, and gives each process in the application its own file descriptor (through some implementation-dependent mechanism, e.g., shared memory; Symunix II supports a pdup system call <ref> [ELS88] </ref>). If processes may join the process group, then they must be able to access previously-opened files, and participate in future multiopens. Multiopen can optionally create a file if it does not exist. File pointer.
Reference: [FJL + 88] <author> G. Fox, M. Johnson, G. Lyzenga, S. Otto, J. Salmon, and D. Walker. </author> <title> Solving Problems on Concurrent Processors, </title> <booktitle> volume 1, chapter 6 and 15. </booktitle> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year>
Reference-contexts: This mechanism is extended to pipes between parallel programs and to graphics output. Self-scheduled global access is not possible. The CUBIX file system for the CrOS system on hypercubes <ref> [FJL + 88] </ref> connects a sequential file server on a host processor to a parallel application program on the hypercube. It has two interfaces: singular, in which all processes simultaneously write the same data, and multiple, in which variable-length records are interleaved by process.
Reference: [Flo86] <author> Rick Floyd. </author> <title> Short-term file reference patterns in a UNIX environment. </title> <type> Technical Report 177, </type> <institution> Dept. of Computer Science, Univ. of Rochester, </institution> <month> March </month> <year> 1986. </year>
Reference-contexts: Since we concentrate on the programmer's interface to the file system, we work with file access patterns, rather than disk access patterns. In our research we do not investigate read/write file access patterns, because most files are opened for either reading or writing, with few files updated <ref> [Flo86, OCH + 85] </ref>. We expect this to be especially true for the large files used in scientific applications. Thus, we consider primarily sequential, read-only and write-only patterns of access to the records of a file. All sequential patterns consist of a sequence of accesses to sequential portions.
Reference: [GL91] <author> Andrew S. Grimshaw and Edmond C. Loyot, Jr. </author> <title> ELFS: object-oriented extensible file systems. </title> <type> Technical Report TR-91-14, </type> <institution> Univ. of Virginia Computer Science Department, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: Indeed, many operating systems for distributed memory machines do not support DSM, and thus could not easily support memory-mapped files. Grimshaw, Loyot, and Prem <ref> [GP91, GL91] </ref> outline an extensible object-oriented interface based on a simple low-level, Unix-like file system interface. The object-oriented front-end encapsulates access methods, caching, prefetching, and file layout in application-specific ways. They focus on providing the mechanism without specifying particular access methods.
Reference: [GLR83] <author> Allan Gottlieb, B. D. Lubachevsky, and Larry Rudolph. </author> <title> Basic techniques for the efficient coordination of very large numbers of cooperating sequential processors. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 5(2) </volume> <pages> 164-189, </pages> <month> April </month> <year> 1983. </year>
Reference-contexts: If the division into segments is a simple matter of dividing the file length by the number of processes, then little work is needed. If, however, the file contains logical records, care must be used to divide the file at 4 Fetch-and-add is described in <ref> [GLR83] </ref>.
Reference: [GP91] <author> Andrew S. Grimshaw and Jeff Prem. </author> <title> High performance parallel file objects. </title> <booktitle> In Sixth Annual Distributed-Memory Computer Conference, </booktitle> <pages> pages 720-723, </pages> <year> 1991. </year> <title> [Int88] iPSC/2 I/O facilities. </title> <publisher> Intel Corporation, </publisher> <year> 1988. </year> <title> Order number 280120-001. </title>
Reference-contexts: Indeed, many operating systems for distributed memory machines do not support DSM, and thus could not easily support memory-mapped files. Grimshaw, Loyot, and Prem <ref> [GP91, GL91] </ref> outline an extensible object-oriented interface based on a simple low-level, Unix-like file system interface. The object-oriented front-end encapsulates access methods, caching, prefetching, and file layout in application-specific ways. They focus on providing the mechanism without specifying particular access methods.
Reference: [Int91] <institution> Paragon XP/S product overview. Intel Corporation, </institution> <year> 1991. </year>
Reference-contexts: In either case files are declustered over many disks. We call the latter structure Parallel Independent Disks (PID). Examples of multiprocessors using a PID architecture include the Intel <ref> [Int88, Int91] </ref>, nCUBE [nC90, DdR92, PFDJ89], and Kendall Square Research [KSR92] multiprocessors. 3 The Workload Parallel file systems and the applications that use them are not sufficiently mature for us to know what access patterns might be typical. <p> This is particularly difficult when creating a file: one process creates the file, all processes synchronize at a barrier, and then the others open the file. The file system for the newer Intel Paragon appears to be a Unix file system, based on the OSF/1 operating system <ref> [Int91] </ref>, although CFS access modes are still available. Another parallel file system is based on ways to lay out a file on parallel disks [Cro89, Cro88]. One interface provides self-scheduled access with a shared file pointer. Another provides individual file pointers.
Reference: [KE91a] <author> David Kotz and Carla Schlatter Ellis. </author> <title> Caching and writeback policies in parallel file systems. </title> <booktitle> In 1991 IEEE Symposium on Parallel and Distributed Processing, </booktitle> <pages> pages 60-67, </pages> <month> December </month> <year> 1991. </year> <note> Submitted to the Journal of Parallel and Distributed Computing. 17 </note>
Reference-contexts: These hide the underlying parallel nature of the file, providing portability. Although sequential applications can access parallel file systems with high performance, parallel applications with all processes participating in reading or writing the file are more successful <ref> [KE91b, KE91a] </ref>. To scale without the limitations of Amdahl's Law, parallel programs must parallelize file access. For concreteness, we use the Unix file system interface [RT78] as an example of a conventional interface.
Reference: [KE91b] <author> David Kotz and Carla Schlatter Ellis. </author> <title> Practical prefetching techniques for parallel file systems. </title> <booktitle> In Proceedings of the First International Conference on Parallel and Distributed Information Systems, </booktitle> <pages> pages 182-189, </pages> <month> December </month> <year> 1991. </year> <note> To appear in Distributed and Parallel Databases. </note>
Reference-contexts: These hide the underlying parallel nature of the file, providing portability. Although sequential applications can access parallel file systems with high performance, parallel applications with all processes participating in reading or writing the file are more successful <ref> [KE91b, KE91a] </ref>. To scale without the limitations of Amdahl's Law, parallel programs must parallelize file access. For concreteness, we use the Unix file system interface [RT78] as an example of a conventional interface.
Reference: [Kim86] <author> Michelle Y. Kim. </author> <title> Synchronized disk interleaving. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-35(11):978-988, </volume> <month> November </month> <year> 1986. </year>
Reference-contexts: In this technique, a file is interleaved across numerous disks and accessed in parallel to simultaneously obtain many blocks of the file with the positioning overhead of one block <ref> [SGM86, Kim86, PGK88] </ref>. All of these schemes rely on a single controller to manage all of the disks, and are intended for uniprocessors. There are two ways to attach multiple disks to a multiprocessor.
Reference: [Kot91] <author> David Kotz. </author> <title> Prefetching and Caching Techniques in File Systems for MIMD Multiprocessors. </title> <type> PhD thesis, </type> <institution> Duke University, </institution> <month> April </month> <year> 1991. </year> <note> Available as technical report CS-1991-016. </note>
Reference-contexts: This is particularly useful if the records have variable length. * By understanding logical records, the file system can avoid splitting a record over two blocks. This increases concurrency in some parallel access patterns <ref> [Kot91] </ref>. It can also increase performance in random access patterns (at the cost of wasted space). In our interface, then, we divide the files into byte files and record files. The file type is an attribute of the file.
Reference: [KSR92] <institution> KSR1 technology background. Kendall Square Research, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: In either case files are declustered over many disks. We call the latter structure Parallel Independent Disks (PID). Examples of multiprocessors using a PID architecture include the Intel [Int88, Int91], nCUBE [nC90, DdR92, PFDJ89], and Kendall Square Research <ref> [KSR92] </ref> multiprocessors. 3 The Workload Parallel file systems and the applications that use them are not sufficiently mature for us to know what access patterns might be typical. Here we define our expectations for parallel file access patterns in a scientific workload. <p> Variable-length records are buffered until complete, then atomically written to the file. To the best of our knowledge, the interface on the BBN, Sequent, and Encore multiprocessors is simply the conventional interface. The Kendall Square Research KSR1 multiprocessor <ref> [KSR92] </ref> uses a PID structure with a RAID attached to individual processors. Files are mapped into the shared memory address space and accessed with normal memory operations. While memory-mapped files have many advantages, they have many disadvantages as a general solution.
Reference: [LMKQ89] <author> Samuel J. Le*er, Marshall Kirk McKusick, Michael J. Karels, and John S. Quarterman. </author> <title> The Design and Implementation of the 4.3BSD UNIX Operating System. </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year> <title> [nC90] nCUBE Corporation. nCUBE 2 supercomputers: </title> <type> Technical overview. </type> <institution> nCUBE brochure, </institution> <year> 1990. </year>
Reference: [OCH + 85] <author> John Ousterhout, Herve Da Costa, David Harrison, John Kunze, Mike Kupfer, and James Thompson. </author> <title> A trace driven analysis of the UNIX 4.2 BSD file system. </title> <booktitle> In Proceedings of the Tenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 15-24, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: Since we concentrate on the programmer's interface to the file system, we work with file access patterns, rather than disk access patterns. In our research we do not investigate read/write file access patterns, because most files are opened for either reading or writing, with few files updated <ref> [Flo86, OCH + 85] </ref>. We expect this to be especially true for the large files used in scientific applications. Thus, we consider primarily sequential, read-only and write-only patterns of access to the records of a file. All sequential patterns consist of a sequence of accesses to sequential portions.
Reference: [PFDJ89] <author> Terrence W. Pratt, James C. French, Phillip M. Dickens, and Stanley A. Janet, Jr. </author> <title> A comparison of the architecture and performance of two parallel file systems. </title> <booktitle> In Fourth Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 161-166, </pages> <year> 1989. </year>
Reference-contexts: In either case files are declustered over many disks. We call the latter structure Parallel Independent Disks (PID). Examples of multiprocessors using a PID architecture include the Intel [Int88, Int91], nCUBE <ref> [nC90, DdR92, PFDJ89] </ref>, and Kendall Square Research [KSR92] multiprocessors. 3 The Workload Parallel file systems and the applications that use them are not sufficiently mature for us to know what access patterns might be typical. Here we define our expectations for parallel file access patterns in a scientific workload. <p> An example of this situation is in [Cro88]. Another example is the nCUBE file system prior to 1992, which does not distribute a single file across disks <ref> [PFDJ89] </ref>. We believe that it is important to have a single name (e.g., Unix pathname) that defines the parallel file, and to leave the rest to the file system. 4.4 Segmented files Consider programming the read-only segmented access pattern. <p> Another provides individual file pointers. A unified access mode provides the standard interface for compatibility. One deficiency in this interface is that the user must supply a list of disks to the open operation. The original file system for the nCUBE hypercube multiprocessor <ref> [PFDJ89] </ref> is primitive, in the sense that each disk has a local file system independent of the others, and no global file system is provided. <p> There should be a single file-naming directory structure for the entire parallel file system. The user should not have to specify the list of disks involved [Cro88] or the list of local disk files <ref> [PFDJ89] </ref> when opening a file. The name structure should be the same for parallel applications as for sequential applications (such as file-maintenance and directory-listing tools). For maximum portability and interoperability, it should appear to be a Unix file system. Multiopen.
Reference: [PGK88] <author> David Patterson, Garth Gibson, and Randy Katz. </author> <title> A case for redundant arrays of inexpensive disks (RAID). </title> <booktitle> In ACM SIGMOD Conference, </booktitle> <pages> pages 109-116, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: In this technique, a file is interleaved across numerous disks and accessed in parallel to simultaneously obtain many blocks of the file with the positioning overhead of one block <ref> [SGM86, Kim86, PGK88] </ref>. All of these schemes rely on a single controller to manage all of the disks, and are intended for uniprocessors. There are two ways to attach multiple disks to a multiprocessor. <p> There are two ways to attach multiple disks to a multiprocessor. The first is to use a striped array of disks (e.g., a Redundant Array of Inexpensive Disks, or RAID <ref> [PGK88] </ref>), and attach the array's controller to a processor or to the interconnection network, as shown in Figure 1. The second is to attach independent controllers and disks to separate processors or ports on the interconnection network, as shown in Figure 2.
Reference: [Pie89] <author> Paul Pierce. </author> <title> A concurrent file system for a highly parallel mass storage system. </title> <booktitle> In Fourth Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 155-160, </pages> <year> 1989. </year>
Reference-contexts: The standard interface is there for compatibility, the tools for performance, and the parallel-open interface for a compromise. Intel's file system for their iPSC/2 and iPSC/860 multiprocessors, CFS <ref> [Pie89] </ref>, also provides three interfaces [AS89]: standard (conventional); random-sequential access, which uses a self 8 scheduled shared file pointer (allowing atomic append); and coordinated, which is for interleaved access with either a fixed or variable record size. CFS forces each process to open the file independently.
Reference: [RT78] <author> D. M. Ritchie and K. Thompson. </author> <title> The UNIX time-sharing system. </title> <journal> The Bell System Technical Journal, </journal> <volume> 6(2) </volume> <pages> 1905-1930, </pages> <month> July-August </month> <year> 1978. </year>
Reference-contexts: To scale without the limitations of Amdahl's Law, parallel programs must parallelize file access. For concreteness, we use the Unix file system interface <ref> [RT78] </ref> as an example of a conventional interface. Advantages to using the Unix (or similar) interface for a multiprocessor include application portability, programmer familiarity, and simplicity. This interface does not, however, directly support parallel file access. <p> In the Unix file system a file is modeled as an addressable sequence of bytes (sometimes referred to as a "seekable stream"). The interface is defined by the kernel file system calls <ref> [RT78] </ref>. The operations provided are open, create (called creat in Unix), close, read, write, and seek (called lseek in Unix). The open and close operations mark the start and end of activity on a given file. Create creates a file if necessary.
Reference: [SGM86] <author> Kenneth Salem and Hector Garcia-Molina. </author> <title> Disk striping. </title> <booktitle> In IEEE 1986 Conference on Data Engineering, </booktitle> <pages> pages 336-342, </pages> <year> 1986. </year> <month> 19 </month>
Reference-contexts: In this technique, a file is interleaved across numerous disks and accessed in parallel to simultaneously obtain many blocks of the file with the positioning overhead of one block <ref> [SGM86, Kim86, PGK88] </ref>. All of these schemes rely on a single controller to manage all of the disks, and are intended for uniprocessors. There are two ways to attach multiple disks to a multiprocessor.
References-found: 27

