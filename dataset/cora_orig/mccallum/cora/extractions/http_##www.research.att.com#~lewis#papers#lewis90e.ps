URL: http://www.research.att.com/~lewis/papers/lewis90e.ps
Refering-URL: http://www.research.att.com/~lewis/chronobib.html
Root-URL: 
Title: Term Clustering of Syntactic Phrases  
Author: David D. Lewis W. Bruce Croft 
Date: April 30, 1990  
Address: Amherst, MA 01003  
Affiliation: Computer and Information Science Department University of Massachusetts,  
Abstract: Term clustering and syntactic phrase formation are methods for transforming natural language text. Both have had only mixed success as strategies for improving the quality of text representations for document retrieval. Since the strengths of these methods are complementary, we have explored combining them to produce superior representations. In this paper we discuss our implementation of a syntactic phrase generator, as well as our preliminary experiments with producing phrase clusters. These experiments show small improvements in retrieval effectiveness resulting from the use of phrase clusters, but it is clear that corpora much larger than standard information retrieval test collections will be required to thoroughly evaluate the use of this technique.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Michael R. Anderberg. </author> <title> Cluster Analysis for Applications. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: Our goal in this survey is to identify what has been learned about these techniques and how they might be combined. 2.1 Term Clustering Term clustering is the application of cluster analysis <ref> [1] </ref> to forming groups of terms drawn from an existing text representation. From a pattern recognition viewpoint, term clustering is a form of feature extraction|a way of transforming an initial set of features into a new set that is more useful for classifying patterns (in this case, documents) [15].
Reference: [2] <author> Georges Antoniadis, Genevieve Lallich-Boidin, Yolla Polity, and Jacques Rouault. </author> <title> A French text recognition model for information retrieval system. </title> <booktitle> In Eleventh International Conference on Research & Development in Information Retrieval, </booktitle> <pages> pages 67-84, </pages> <year> 1988. </year>
Reference-contexts: Antoniadis, et al describe a similar method, but it is not clear if it was actually used in their system <ref> [2] </ref>. More traditional statistical clustering techniques have been used in at least two IR interfaces to suggest terms, including syntactically formed phrasal terms, that a user might want to include in their query.
Reference: [3] <author> Bran Boguraev and Ted Briscoe. </author> <title> Large lexicons for natural language processing: Util ising the grammar coding system of LDOCE. </title> <note> Computational Linguistics, 13(3-4):203-218, 1987. Special Issue on the Lexicon. </note>
Reference-contexts: Limiting the complexity of analysis does not limit the need for a large lexicon, since every word still had to be interpreted. We used the machine-readable version of the Longman Dictionary of Contemporary English (LDOCE) <ref> [3] </ref>, which provided syntactic categories for about 35,000 words. By using a morphological analyzer for inflectional suffixes we extended the effective vocabulary of the system to perhaps 100,000 words. Even so, a substantial number of words encountered in text were not present in the dictionary.
Reference: [4] <author> C. Buckley and A. F. Lewit. </author> <title> Optimization of inverted vector searches. </title> <booktitle> In ACM SI GIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 97-110, </pages> <year> 1985. </year>
Reference-contexts: This normalization for category size means that the k nearest neighbors of a seed phrase will almost always share some relatively specific CR category with the seed. We can therefore adapt the technique, first proposed for document ranking <ref> [4] </ref>, of searching inverted lists in order of their length and testing at the end of each list whether any unseen item can possibly be more similar than the k best items already seen.
Reference: [5] <author> W. Bruce Croft. </author> <title> Clustering large files of documents using the single-link method. </title> <journal> Journal of the American Society for Information Science, </journal> <pages> pages 341-344, </pages> <month> November </month> <year> 1977. </year>
Reference-contexts: This means that efficiency of clustering will be of considerable importance. We therefore conducted some preliminary investigations into efficiency methods. The use of an inverted file to speed up the finding of nearest neighbors is a technique that has been applied to both document clustering <ref> [5, 39] </ref> and term clustering [25]. The main advantage cited for this technique is the avoidance of calculating the large number of similarity values of 0 present in typical term-term or document-document matrices.
Reference: [6] <author> W. Bruce Croft and Raj Das. </author> <title> Experiments with query acquisition and use in document retrieval systems. </title> <booktitle> In Thirteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <year> 1990. </year> <month> 17 </month>
Reference-contexts: Of course, as with clustering, a large text corpus is needed to obtain this distributional information. Another potential source of high quality phrases is the user of the IR system <ref> [6] </ref>. While the user cannot control which phrases take part in clusters, he or she can control which phrases are extracted from the query, and thus used to match clusters. <p> On the other hand, existing probabilistic retrieval models are inadequate for use with phrases and clusters, particularly in handling the known dependencies between terms and phrases and terms and clusters. Network models [38] and probabilistic models incorporating explicit dependencies are two promising alternatives <ref> [6] </ref>. 3 The set of phrases used was generated by a graduate student who was not involved in the experiments on syntactic phrase formation. 16 7 Conclusions Term clustering is a natural approach to remedying the poor statistical properties of syntactic phrases.
Reference: [7] <author> Carolyn J. Crouch. </author> <title> A cluster-based approach to thesaurus construction. </title> <booktitle> In Eleventh International Conference on Research & Development in Information Retrieval, </booktitle> <pages> pages 309-320, </pages> <year> 1988. </year>
Reference-contexts: Crouch recently achieved significant performance improvements on two collections by first clustering documents, and then grouping low frequency terms that occurred in all documents of a document cluster <ref> [7] </ref>. 2.2 Research on Syntactic Phrase Indexing The use of syntactic information for phrasal indexing has been surveyed elsewhere [9, 31, 21], so we discuss this area only briefly. These techniques break down into two major classes: template-based and parser-based. <p> The stem weights themselves are computed as usual for the vector space model. The inner products were computed separately for terms and phrases and then added together, potentially with different weightings. Crouch <ref> [7] </ref> used a very similar method for clusters, giving them a weight in a query (or a document) equal to the mean of the weights of the cluster members in the query (or the document).
Reference: [8] <author> Martin Dillon and Ann S. Gray. FASIT: </author> <title> A fully automatic syntactically based indexing system. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 34(2) </volume> <pages> 99-108, </pages> <month> March </month> <year> 1983. </year>
Reference-contexts: These techniques break down into two major classes: template-based and parser-based. Dillon and Gray's FASIT system <ref> [8] </ref> is typical of template-based phrasal indexers. Adjacent groups of words from documents are matched against a library of templates, such 3 as &lt;JJ-NN NN&gt; (adjective noun), and &lt;NN PP NN&gt; (noun preposition noun), and those matching some template are retained.
Reference: [9] <author> Joel L. Fagan. </author> <title> Experiments in Automatic Phrase Indexing for Document Retrieval: </title>
Reference-contexts: Crouch recently achieved significant performance improvements on two collections by first clustering documents, and then grouping low frequency terms that occurred in all documents of a document cluster [7]. 2.2 Research on Syntactic Phrase Indexing The use of syntactic information for phrasal indexing has been surveyed elsewhere <ref> [9, 31, 21] </ref>, so we discuss this area only briefly. These techniques break down into two major classes: template-based and parser-based. Dillon and Gray's FASIT system [8] is typical of template-based phrasal indexers. <p> Phrases are normalized by stemming and removal of function words. Klingbiel's MAI system used a similar strategy [16], while the TMC Indexer [24] and LEADER [13] combined limited parsing with templates. Parser-based strategies attempt to analyze entire sentences or significant parts of them in producing syntactic phrases. Fagan <ref> [9] </ref>, for example, used the PLNLP parser to completely parse the text of two test collections and extract indexing phrases. The sophistication of the PLNLP grammar enabled Fagan to handle complex noun phrases with prepositional and clausal postmodifiers, as well as some adjectival constructions. <p> Examples are a verb and the head noun of noun phrase which is its subject, a noun and a modifying adjective, a noun and the head noun of a modifying prepositional phrase, and so on. This is essentially the definition used by Fagan <ref> [9] </ref>, except that we form phrases from all verbal, adverbial, and adjectival constructions, and do not maintain exclusion lists of specially treated words. It is important to distinguish the definition of syntactic phrases used by a system from the actual set of phrases produced. <p> We chose to use the same weighting methods used by Fagan for phrases and by Crouch for clusters, since these methods have shown some effectiveness in the past. Fagan <ref> [9, 10] </ref> assigned a two-word phrase a weight (in both queries and documents) equal to the mean of the weights of its component stems. The stem weights themselves are computed as usual for the vector space model. <p> The use of inverse document frequency weighting may make exclusion of high frequency terms less important. A final possibility is that collection frequency is not as good an indicator of quality for phrases as it is for single terms. This view is supported by the fact that Fagan <ref> [9] </ref> found only trivial improvements in retrieval performance were possible from excluding high frequency syntactic phrases. The exclusion of low quality phrases is clearly an important issue both for phrasal indexing and clustering of phrases.
References-found: 9

