URL: http://theory.lcs.mit.edu/~sivan/iopads.ps.gz
Refering-URL: http://theory.lcs.mit.edu/~sivan/papers.html
Root-URL: 
Title: The Design and Implementation of SOLAR, a Portable Library for Scalable Out-of-Core Linear Algebra Computations  
Author: Sivan Toledo Fred G. Gustavson 
Affiliation: IBM T.J. Watson Research Center  
Abstract: SOLAR is a portable high-performance library for out-of-core dense matrix computations. It combines portability with high performance by using existing high-performance in-core subroutine libraries and by using an optimized matrix input-output library. SOLAR works on parallel computers, workstations, and personal computers. It supports in-core computations on both shared-memory and distributed-memory machines, and its matrix input-output library supports both conventional I/O interfaces and parallel I/O interfaces. This paper discusses the overall design of SOLAR, its interfaces, and the design of several important subroutines. Experimental results show that SOLAR can factor on a single workstation an out-of-core positive-definite symmetric matrix at a rate exceeding 215 Mflops, and an out-of-core general matrix at a rate exceeding 195 Mflops. Less than 16% of the running time is spent on I/O in these computations. These results indicate that SOLAR's portability does not compromise its performance. We expect that the combination of portability, modularity, and the use of a high-level I/O interface will make the library an important platform for research on out-of-core algorithms and on parallel I/O. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. Agerwala, J. L. Martin, J. H. Mirza, D. C. Sadler, D. M. Dias, and M. Snir. </author> <title> SP2 system architecture. </title> <journal> IBM Systems Journal, </journal> <volume> 34(2) </volume> <pages> 152-184, </pages> <year> 1995. </year>
Reference-contexts: We conclude that whereas LAPACK's block-iterative algorithms can deliver acceptable performance, specialized out-of-core algorithms can run several times faster. Experiments on Parallel Computers The experiments were performed on two IBM SP2 parallel computers <ref> [1] </ref> which we denote by machine T and machine W. Machine T was configured with so-called thin nodes with 128 Mbytes of main memory as both the compute and I/O nodes and ran AIX version 4.1.3.
Reference: [2] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. Du Croz, A. Greenbaum, S. Hammarling, A. McKenney, S. Ostrouchov, and D. Sorensen. </author> <title> LAPACK User's Guide. </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA, 2nd edition, </address> <year> 1994. </year> <note> Also available online from http://www.netlib.org. </note>
Reference-contexts: 1 Introduction This paper describes the design and implementation of SOLAR, a high-performance portable library for out-of-core dense matrix computations. SOLAR is designed to meet three main objectives. First, the library should deliver as much of the functionality of LAPACK <ref> [2] </ref>, a public domain library for in-core matrix computations, as possible. Second, the library should be portable across a wide variety of architectures. Third, the library should deliver high performance. <p> The in-core subroutines consist of the so- called Basic Linear Algebra Subroutines [9, 10, 17] (BLAS), the parallel BLAS [5], LAPACK <ref> [2] </ref>, and ScaLAPACK [6]. The generic components of the library consist of subroutines that schedule I/O operations and call the in-core subroutines to manipulate submatrices stored in in-core buffers. 2. The library will provide a large set of solvers by following a traditional layered approach to linear algebra software.
Reference: [3] <author> D. W. Barron and H. P. F. Swinnerton-Dyerm. </author> <title> Solution of simul-taneous linear equations using a magnetic tape store. </title> <journal> Computer Journal, </journal> <volume> 3 </volume> <pages> 28-33, </pages> <year> 1960. </year>
Reference: [4] <author> Jean-Philippe Brunet, Palle Pedersen, and S. Lennart Johnsson. </author> <title> Load-balanced LU and QR factor and solve routines for scalable processors with scalable I/O. </title> <booktitle> In Proceedings of the 17th IMACS World Congress, </booktitle> <address> Atlanta, Georgia, </address> <month> July </month> <year> 1994. </year> <note> Also available as Harvard University Computer Science Technical Report TR-2094. </note>
Reference-contexts: A testimony for the need for out-of-core software is the steady stream of implementations of such codes over the last 45 years [3, 4, 11, 12, 13, 14, 15, 20, 21, 19, 22, 25], including a number of recent implementations (for example, <ref> [4, 12, 13, 15, 25] </ref>). Unfor- tunately, none of these codes appears to be portable, and all of them provide only a few subroutines that the implementors needed, rather than a full set of dense solvers. Consequently, many users cannot take advantage of these codes.
Reference: [5] <author> J. Choi, J. Dongarra, S. Ostrouchov, A. Petitet, D. Walker, and R. C. Whaley. </author> <title> A proposal for a set of parallel basic linear algebra subprograms. </title> <type> Technical Report CS-95-292, </type> <institution> University of Tennessee, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: The in-core subroutines consist of the so- called Basic Linear Algebra Subroutines [9, 10, 17] (BLAS), the parallel BLAS <ref> [5] </ref>, LAPACK [2], and ScaLAPACK [6]. The generic components of the library consist of subroutines that schedule I/O operations and call the in-core subroutines to manipulate submatrices stored in in-core buffers. 2.
Reference: [6] <author> J. Choi, J. Dongarra, R. Pozo, and D. Walker. </author> <title> ScaLAPACK: A scalable linear algebra for distributed memory concurrent com-puters. </title> <booktitle> In Proceedings of the 4th Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 120-127, </pages> <year> 1992. </year> <note> Also available as University of Tennessee Technical Report CS-92181. </note>
Reference-contexts: The in-core subroutines consist of the so- called Basic Linear Algebra Subroutines [9, 10, 17] (BLAS), the parallel BLAS [5], LAPACK [2], and ScaLAPACK <ref> [6] </ref>. The generic components of the library consist of subroutines that schedule I/O operations and call the in-core subroutines to manipulate submatrices stored in in-core buffers. 2. The library will provide a large set of solvers by following a traditional layered approach to linear algebra software. <p> In all the experiments the message passing layer used the network interface in user-space mode and did not use interrupts. For in-core computations on distributed arrays we used IBM's Parallel Engineering and Scientific Subroutine Library (PESSL) version 1.1, which based on and completely compatible with ScaLAPACK <ref> [6] </ref>, a public domain linear algebra package for linear algebra computations. 3 For in-core computations on arrays that are local to a processor we used IBM's Engineering and Scientific Subrou- tine Library (ESSL) version 2.2. We used POWER2-specific versions of all the libraries.
Reference: [7] <author> P. F. Corbett, D. G. Feitelson, J.-P. Prost, G. S. Almasi, S. J. Baylor, A. S. Bolmarcich, Y. Shu, J. Satran, M. Snir, R. Colao, B. D. Herr, J. Kavaky, T. R. Morgan, and A. Zlotek. </author> <title> Parallel file systems for the IBM SP computers. </title> <journal> IBM Systems Journal, </journal> <volume> 34(2) </volume> <pages> 222-248, </pages> <year> 1995. </year>
Reference-contexts: Wide-2 nodes have a 77 MHz POWER2 processor, 256 Kbytes 4-way set associative level-1 data-cache, no level-2 cache, and a 256-bit-wide main memory bus. Thin nodes are the slowest SP2 nodes and wide-2 nodes are currently the fastest. We used IBM's parallel file system <ref> [7] </ref> (PIOFS) version 1.1. On machine T PIOFS used 4 I/O nodes. The parallel file system used 1 Gbytes on each node, allocated on a 2.2 Gbytes 16-bit SCSI disk. On machine W PIOFS used 4 I/O nodes.
Reference: [8] <author> Thomas H. Cormen and David Kotz. </author> <title> Integrating theory and practice in parallel file systems. </title> <type> Technical Report PCS-TR93188, </type> <institution> Dept. of Math and Computer Science, Dartmouth College, </institution> <month> March </month> <year> 1993. </year> <note> Revised 9/20/94. An ealier version appeared in the Proceedings of the 1993 DAGS/PC Symposium. </note>
Reference-contexts: Second, the specification of out-of-core data mappings depends on how I/O requests and file structures are specified to the operating system or to the file system, and it seems that too early to settle this issue <ref> [8] </ref>. Fortunately, it is possible to allow such diversity, because only the MIOS need to know the details of the out-of-core data mapping. Hence, the out-of-core storage descriptor can be viewed as an opaque object accessible only to the MIOS.
Reference: [9] <author> Jack J. Dongarra, Jeremy Du Cruz, Sven Hammarling, and Ian Duff. </author> <title> An set of level 3 basic linear algebra subprograms. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 16(1) </volume> <pages> 1-17, </pages> <year> 1990. </year>
Reference-contexts: The platform dependent layer of the library consists of a set of functions that perform I/O on submatrices, as well as of high- performance in-core subroutines that are already implemented on most platforms. The in-core subroutines consist of the so- called Basic Linear Algebra Subroutines <ref> [9, 10, 17] </ref> (BLAS), the parallel BLAS [5], LAPACK [2], and ScaLAPACK [6]. The generic components of the library consist of subroutines that schedule I/O operations and call the in-core subroutines to manipulate submatrices stored in in-core buffers. 2. <p> SOLAR creates a complex variable with the value 1, for example, by calling a function that returns a pointer to a complex datum with a given initial value, F77types get value ("C",1.0). Subroutines for Out-of-Core Computations We have already implemented three out of the nine level-3 BLAS <ref> [9] </ref>, namely general matrix multiply-add, symmetric rank-k update, and triangular solve, as well as factor and solve subroutines for positive- definite symmetric and general matrices. The BLAS use block-iterative algorithms.
Reference: [10] <author> Jack J. Dongarra, Jeremy Du Cruz, Sven Hammarling, and Richard J. Hanson. </author> <title> An extended set of FORTRAN basic lin-ear algebra subprograms. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 14(1) </volume> <pages> 1-17, </pages> <year> 1988. </year>
Reference-contexts: The platform dependent layer of the library consists of a set of functions that perform I/O on submatrices, as well as of high- performance in-core subroutines that are already implemented on most platforms. The in-core subroutines consist of the so- called Basic Linear Algebra Subroutines <ref> [9, 10, 17] </ref> (BLAS), the parallel BLAS [5], LAPACK [2], and ScaLAPACK [6]. The generic components of the library consist of subroutines that schedule I/O operations and call the in-core subroutines to manipulate submatrices stored in in-core buffers. 2.
Reference: [11] <author> J. J. Du Cruz, S. M. Nugent, J. K. Reid, and D. B. Taylor. </author> <title> Solving large full sets of linear equations in a paged virtual store. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 7(4) </volume> <pages> 527-536, </pages> <year> 1981. </year>
Reference-contexts: When the factorization is complete, each block is read again, the row exchanges that were generated after it was factored are applied, and it is written back. This algorithm, which was first proposed in <ref> [11] </ref>, has two desirable features. First, the amount of I/O required for row exchanges is only one read and one write for every element in the strictly block-lower-triangular part of the matrix. Second, except for these writes, each element is written only once.
Reference: [12] <author> Charbel Farhat. </author> <title> Large out-of-core calculation runs on the IBM SP2. </title> <journal> NAS News, </journal> <volume> 2(11), </volume> <month> August </month> <year> 1995. </year>
Reference-contexts: A testimony for the need for out-of-core software is the steady stream of implementations of such codes over the last 45 years [3, 4, 11, 12, 13, 14, 15, 20, 21, 19, 22, 25], including a number of recent implementations (for example, <ref> [4, 12, 13, 15, 25] </ref>). Unfor- tunately, none of these codes appears to be portable, and all of them provide only a few subroutines that the implementors needed, rather than a full set of dense solvers. Consequently, many users cannot take advantage of these codes.
Reference: [13] <author> Nikolaus Geers and Roland Klees. </author> <title> Out-of-core solver for large dense nonsymmetric linear systems. </title> <journal> Manuscripta Geodetica, </journal> <volume> 18(6) </volume> <pages> 331-342, </pages> <year> 1993. </year>
Reference-contexts: A testimony for the need for out-of-core software is the steady stream of implementations of such codes over the last 45 years [3, 4, 11, 12, 13, 14, 15, 20, 21, 19, 22, 25], including a number of recent implementations (for example, <ref> [4, 12, 13, 15, 25] </ref>). Unfor- tunately, none of these codes appears to be portable, and all of them provide only a few subroutines that the implementors needed, rather than a full set of dense solvers. Consequently, many users cannot take advantage of these codes.
Reference: [14] <author> Roger G. Grimes and Horst D. Simon. </author> <title> Solution of large, dense symmetric generalized eigenvalue problems using sec-ondary storage. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 14(3) </volume> <pages> 241-256, </pages> <year> 1988. </year>
Reference: [15] <author> Kenneth Klimkowski and Robert van de Geijn. </author> <title> Anatomy of an out-of-core dense linear solver. </title> <booktitle> In Proceedings of the 1995 International Conference on Parallel Processing, </booktitle> <year> 1995. </year> <note> To appear. </note>
Reference-contexts: A testimony for the need for out-of-core software is the steady stream of implementations of such codes over the last 45 years [3, 4, 11, 12, 13, 14, 15, 20, 21, 19, 22, 25], including a number of recent implementations (for example, <ref> [4, 12, 13, 15, 25] </ref>). Unfor- tunately, none of these codes appears to be portable, and all of them provide only a few subroutines that the implementors needed, rather than a full set of dense solvers. Consequently, many users cannot take advantage of these codes.
Reference: [16] <author> David Kotz. </author> <title> Disk-directed I/O for an out-of-core computation. </title> <booktitle> In Proceedings of the Fourth IEEE International Symposium on High Performance Distributed Computing, </booktitle> <pages> pages 159-166, </pages> <month> August </month> <year> 1995. </year> <note> Also available as Dartmouth College Technical Report PCS-TR95-251. </note>
Reference-contexts: We believe that the main reason for using toy applications in past research on I/O has been the lack of portable out-of-core software, and our library should eliminate this problem. The high-level I/O interface that SOLAR uses is compatible with current research on parallel I/O (see <ref> [16, 23] </ref>, for example) and the library should therefore provide a convenient platform for research on I/O optimization. 1 Our library uses two main mechanisms to achieves our design objectives: 1. <p> First, such an interface allows an out-of-core subroutine to use multiple out-of-core storage layouts and multiple low-level I/O interfaces without explicit reference to all of these options. Second, a high level interface allows for I/O optimizations such as two-phase I/O [23] and disk-directed I/O <ref> [16] </ref>. The contract between the out-of-core subroutines and the MIOS is simple. The MIOS can transfer any rectangular block of a matrix between primary and secondary memories.
Reference: [17] <author> C. L. Lawson, R. J. Hanson, D. R. Kincaid, and F. T. Krogh. </author> <title> Basic linear algebra subprogram for Fortran usage. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 5(3) </volume> <pages> 308-323, </pages> <year> 1979. </year>
Reference-contexts: The platform dependent layer of the library consists of a set of functions that perform I/O on submatrices, as well as of high- performance in-core subroutines that are already implemented on most platforms. The in-core subroutines consist of the so- called Basic Linear Algebra Subroutines <ref> [9, 10, 17] </ref> (BLAS), the parallel BLAS [5], LAPACK [2], and ScaLAPACK [6]. The generic components of the library consist of subroutines that schedule I/O operations and call the in-core subroutines to manipulate submatrices stored in in-core buffers. 2.
Reference: [18] <author> David A. Patterson and John L. Hennessy. </author> <title> Computer Architecture A Quantitative Approach, Second Edition. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, </address> <year> 1995. </year>
Reference-contexts: Although current computers, especially parallel computers, have large amounts of memory, there is still a need for software for out <p>- of-core matrix computations. Since DRAM is about 100 times more expensive than disk <ref> [18] </ref> and since out-of-core software for dense matrix computations can run at almost the rate of in-core software (see Section 4), out-of-core software can solve problems at a much lower cost-performance ratio than in-core software.
Reference: [19] <author> Hans Riesel. </author> <title> A note on large linear systems. Mathematical Tables and Other Aids to Computation, </title> <booktitle> 10 </booktitle> <pages> 226-227, </pages> <year> 1956. </year>
Reference: [20] <author> J. Rutledge and H. Rubinstein. </author> <title> High order matrix computation on the UNIVAC. </title> <booktitle> Presented at the meeting of the Association for Computing Machinery, </booktitle> <month> May </month> <year> 1952. </year>
Reference: [21] <author> Joseph Rutledge and Harvey Rubinstein. </author> <title> Matrix algebra pro-grams for the UNIVAC. </title> <booktitle> Presented at the Wayne Conference on Automatic Computing Machinery and Applications, </booktitle> <month> March </month> <year> 1951. </year>
Reference: [22] <author> M. M. Stabrowski. </author> <title> A block equation solver for large unsymmetric linear equation systems with dense coefficient matrices. </title> <journal> International Journal for Numerical Methods in Engineering, </journal> <volume> 24 </volume> <pages> 289-300, </pages> <year> 1982. </year>
Reference: [23] <author> Rajeev Thakur and Alok Choudhary. </author> <title> An extended two-phase method for accessing sections of out-of-core arrays. </title> <type> Technical Report CACR-103, </type> <institution> Scalable I/O Initiative, Center for Advanced Computing Research, Caltech, </institution> <month> June </month> <year> 1995. </year> <note> Submitted to a special issue of Scientific Programming on implementations of HPF. </note>
Reference-contexts: We believe that the main reason for using toy applications in past research on I/O has been the lack of portable out-of-core software, and our library should eliminate this problem. The high-level I/O interface that SOLAR uses is compatible with current research on parallel I/O (see <ref> [16, 23] </ref>, for example) and the library should therefore provide a convenient platform for research on I/O optimization. 1 Our library uses two main mechanisms to achieves our design objectives: 1. <p> First, such an interface allows an out-of-core subroutine to use multiple out-of-core storage layouts and multiple low-level I/O interfaces without explicit reference to all of these options. Second, a high level interface allows for I/O optimizations such as two-phase I/O <ref> [23] </ref> and disk-directed I/O [16]. The contract between the out-of-core subroutines and the MIOS is simple. The MIOS can transfer any rectangular block of a matrix between primary and secondary memories.
Reference: [24] <author> Sivan Toledo. </author> <title> Locality of reference in LU decomposition with partial pivoting. </title> <type> Technical Report RC20344, </type> <institution> IBM T.J. Watson Research Center, </institution> <address> Yorktown Heights, NY, </address> <month> January </month> <year> 1996. </year>
Reference-contexts: Each half is factored using the same strategy. Toledo <ref> [24] </ref> recently proved that this algorithm uses only fi (n 3 = p M ) words worth of I/O in a simple two level memory hierarchy, where n is the order of the matrix and M is the size of main memory.
Reference: [25] <author> David Womble, David Greenberg, Stephen Wheat, and Rolf Riesen. </author> <title> Beyond core: Making parallel computer I/O prac-tical. </title> <booktitle> In Proceeings of the 1993 DAGS/PC Symposium, </booktitle> <pages> pages 56-63, </pages> <address> Hanover, NH, </address> <month> June </month> <year> 1993. </year> <institution> Dartmouth Institute for Advanced Graduate Studies. </institution> <note> Also available online from http://www.cs.sandia.gov/~ dewombl. </note>
Reference-contexts: A testimony for the need for out-of-core software is the steady stream of implementations of such codes over the last 45 years [3, 4, 11, 12, 13, 14, 15, 20, 21, 19, 22, 25], including a number of recent implementations (for example, <ref> [4, 12, 13, 15, 25] </ref>). Unfor- tunately, none of these codes appears to be portable, and all of them provide only a few subroutines that the implementors needed, rather than a full set of dense solvers. Consequently, many users cannot take advantage of these codes.
References-found: 25

