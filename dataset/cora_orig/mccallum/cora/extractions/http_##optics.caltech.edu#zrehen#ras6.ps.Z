URL: http://optics.caltech.edu/zrehen/ras6.ps.Z
Refering-URL: http://optics.caltech.edu/zrehen/sz_publis.html
Root-URL: http://www.cs.caltech.edu
Email: e-mail:  
Title: Living in a partially structured environment: How to bypass the limitations of classical reinforcement techniques  
Author: P. Gaussier*, A. Revel*, C. Joulain*, S. Zrehen** 
Keyword: Neural Networks, Unsupervised Learning, Topological Maps, Reinforcement, Autonomous System, All-or-none learning, hyspotheses test.  
Note: gaussier or revel@ensea.fr Hedco Neurosciences  
Address: 6 Av. du Ponceau 95014 Cergy Pontoise Cedex France  Building, Room 07E  Los Angeles Ca 900089-2520  
Affiliation: ETIS ENSEA Cergy University  University of Southern California University Part,  
Abstract: In this paper, we propose an unsupervised neural network allowing a robot to learn sensori-motor associations with a delayed reward. The robot task is to learn the "meaning" of pictograms in order to "survive" in a maze. First, we introduce a new neural conditioning rule (PCR: Probabilistic Conditioning Rule) allowing to test hypotheses (associations between visual categories and movements) during a given time span. Second, we describe a real maze experiment with our mobile robot. We propose a neural architecture to solve this problem and we discuss the difficulty to build visual categories dynamically while associating them to movements. Third, we propose to use our algorithm on a simulation in order to test it exhaustively. We give the results for different kind of mazes and we compare our system to an adapted version of the Q-learning algorithm. Finally, we conclude by showing the limitations of approaches that do not take into account the intrinsic complexity of a reasonning based on image recognition. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J.S. Albus. </author> <title> Outline for a theory of intelligence. </title> <journal> IEEE trans. on syst. and cybern., </journal> <volume> 21(3) </volume> <pages> 473-509, </pages> <month> may/june </month> <year> 1991. </year>
Reference-contexts: It represents a way to measure robot satisfaction over time. lf B (W ij ) = 1 if W ij = 1 ff is the delayed conditioning learning rate ~ is a constant fixed by the experimenter Rnd is a random value in <ref> [0; 1] </ref> p ij 2 [0; 1], W ij 2 f0; 1g. The input/output normalized correlation is given by equation 5. This term does not depend on "appearance" probabilities of input-output pairs. <p> It represents a way to measure robot satisfaction over time. lf B (W ij ) = 1 if W ij = 1 ff is the delayed conditioning learning rate ~ is a constant fixed by the experimenter Rnd is a random value in <ref> [0; 1] </ref> p ij 2 [0; 1], W ij 2 f0; 1g. The input/output normalized correlation is given by equation 5. This term does not depend on "appearance" probabilities of input-output pairs. <p> classical conditioning possible and p ij (t) I i (t) allows to avoid unstable behavior of the p ij (t) terms (pseudo normalization of the p ij ). 9 Box 1: The PerAc architecture The PerAc (Perception-Action) block has been proposed as an elementary generic brick of neuronal computation (Hecht-Nielsen <ref> [1, 24, 8] </ref>). It enables on-line learning of sensory-motor associations. A PerAc block is divided into two levels corresponding to the action and to the perception data flows. <p> In our current research, we try to fill the gap between models of outdoor navigation [21] and goal seeking [19] and cortical explanation of planification <ref> [1, 2] </ref>. Then, we hope to have at the same time reinforcement and planification capabilities. Another important point will be to modify the neuron model so as to introduce continuous time considerations.
Reference: [2] <author> F. Alexandre, F. Guyot, J.P. Haton, and Y. Burnod. </author> <title> The corical column :a new processing unit for multilayered networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 4 </volume> <pages> 15-25, </pages> <year> 1991. </year>
Reference-contexts: In our current research, we try to fill the gap between models of outdoor navigation [21] and goal seeking [19] and cortical explanation of planification <ref> [1, 2] </ref>. Then, we hope to have at the same time reinforcement and planification capabilities. Another important point will be to modify the neuron model so as to introduce continuous time considerations.
Reference: [3] <author> A.A. Baloch and A.M. Waxman. </author> <title> Visual learning, adaptive expectations and behavioral conditionning of the mobile robot mavin. </title> <booktitle> Neural Networks, </booktitle> <volume> 4 </volume> <pages> 271-302, </pages> <year> 1991. </year>
Reference-contexts: In the 1 case of unsupervised learning, it can be a WTA group (Winner Take All [40] ), an ART architecture (Adaptive Resonance Theory - [9], <ref> [3] </ref> for an application), or a quickly learnable version of the Kohonen map ([27], [45] , [47] for applications) which remains a statistical classifier: the most examples of a class are presented, the most neurons are used to represent the class on the map. <p> Accordingly, it distinguishes a reflex response (Unconditional Response - UR) and a forced response (Conditional Response - CR). Conditioning phenomenon can, then, be divided in two types, whether they lean on a UR or not (see [46, 28, 53, 54], and [14, 32] for a review and <ref> [63, 3] </ref> for application). In the first case, a link already exists between US and UR. The problem is then to reinforce the link between a CS and the UR. After learning, even if the CS is presented alone, the robot should respond appropriately.
Reference: [4] <author> A.G. Barto and R.S. Sutton. </author> <title> Landmark learning : an illustration of associative search. </title> <journal> Biological cybernetics, </journal> <volume> 42 </volume> <pages> 1-8, </pages> <year> 1981. </year>
Reference-contexts: In the second case, no US is available. Therefore a global reward signal must be introduced to tell whether the association between the CS and the CR is correct or not <ref> [4, 6] </ref>. Those models work quite well if the reward is given immediately after correct or incorrect action was performed. But, in most cases, and particularly for our task, the reward can only occur at the very end. <p> The number of stimuli and responses must not be too large, otherwise the probability of finding the right association will be very low (NP-complete problem [35]). In that context, Barto and Sutton <ref> [4, 6] </ref> propose an efficient solution using both a diversity generator (selection at random) added to the neuron activity, and an equation adapted from the Hebb rule: 3 Act j = W 0j + i=1 W ij I i + noise O j = 1 if Act j &gt; 0 0 <p> This normalized hebbian term is very similar to the eligibility term used in Barto and Sutton <ref> [4, 6] </ref>. Each time the reinforcement signal P (t) varies enough ( fi fi @t fi fi &gt; ~), confidence terms are updated according to the formula 6.
Reference: [5] <author> A.G. Barto, R.S. Sutton, and C.W. Anderson. </author> <title> Neuronlike adaptive elements that can solve difficult control problems. </title> <journal> IEEE transactions on system, man and cybernetics, </journal> <volume> SMC-13(5):834-846, </volume> <month> Sep/Oct </month> <year> 1983. </year>
Reference: [6] <author> A.G. Barto, </author> <title> R.S. Sutton, and D.S. Brouwer. Associative search network : A reinforcement learning associative memory. </title> <journal> Biological cybernetics, </journal> <volume> 40 </volume> <pages> 201-211, </pages> <year> 1981. </year>
Reference-contexts: In the second case, no US is available. Therefore a global reward signal must be introduced to tell whether the association between the CS and the CR is correct or not <ref> [4, 6] </ref>. Those models work quite well if the reward is given immediately after correct or incorrect action was performed. But, in most cases, and particularly for our task, the reward can only occur at the very end. <p> The number of stimuli and responses must not be too large, otherwise the probability of finding the right association will be very low (NP-complete problem [35]). In that context, Barto and Sutton <ref> [4, 6] </ref> propose an efficient solution using both a diversity generator (selection at random) added to the neuron activity, and an equation adapted from the Hebb rule: 3 Act j = W 0j + i=1 W ij I i + noise O j = 1 if Act j &gt; 0 0 <p> This normalized hebbian term is very similar to the eligibility term used in Barto and Sutton <ref> [4, 6] </ref>. Each time the reinforcement signal P (t) varies enough ( fi fi @t fi fi &gt; ~), confidence terms are updated according to the formula 6.
Reference: [7] <author> M. Bishay, R.A. Peters II, and K. Kawamura. </author> <title> Object detection in indoor scenes using log-polar mapping. </title> <booktitle> In International Conference on Robotics & Automation, </booktitle> <pages> pages 775-780, </pages> <address> San Diego, 1994. </address> <publisher> IEEE. </publisher>
Reference-contexts: If there is a vanishing point on the left of the image it is because the robot is too much on the right of the corridor and it should turn left a little. The localization of vanishing points can be obtained by different methods such as a log-polar transform <ref> [7] </ref> or a Hough transform [41]. Yet, in a neural context, a vanishing point detector can be implemented with orientation sensible cells.
Reference: [8] <author> Rodney A. Brooks. </author> <title> A robust layered control system for a mobile robot. </title> <journal> IEEE Journal of Robotics and Automation, R.A. </journal> <volume> 2(1) </volume> <pages> 14-23, </pages> <month> March </month> <year> 1986. </year>
Reference-contexts: classical conditioning possible and p ij (t) I i (t) allows to avoid unstable behavior of the p ij (t) terms (pseudo normalization of the p ij ). 9 Box 1: The PerAc architecture The PerAc (Perception-Action) block has been proposed as an elementary generic brick of neuronal computation (Hecht-Nielsen <ref> [1, 24, 8] </ref>). It enables on-line learning of sensory-motor associations. A PerAc block is divided into two levels corresponding to the action and to the perception data flows.
Reference: [9] <author> G.A. Carpenter and S. Grossberg. </author> <title> Invariant pattern recognition and recall by an attentive self-organizing art architecture in a nonstationary world. </title> <booktitle> Proceeding of Neural Network, </booktitle> <volume> 2 </volume> <pages> 737-745, </pages> <year> 1987. </year>
Reference-contexts: In the 1 case of unsupervised learning, it can be a WTA group (Winner Take All [40] ), an ART architecture (Adaptive Resonance Theory - <ref> [9] </ref>, [3] for an application), or a quickly learnable version of the Kohonen map ([27], [45] , [47] for applications) which remains a statistical classifier: the most examples of a class are presented, the most neurons are used to represent the class on the map. <p> is really the one which permits to reach the exit of the maze is solved by the PCR process as described in the previous section. 14 Box 2: The Probabilistic Topological Map (PTM) PTM has been imagined to take advantage of the main features of Carpenter&Grossberg's Adaptive Resonance Theory (ART <ref> [9] </ref>) and Kohonen's topological map [27]: ART models are very interesting because they allow on-line learning (as autonomy is concerned), while Kohonen's maps allow a priori topological generalization. Topological representations are interesting for two main reasons. First they allow to preserve, at least locally, the topology of the input information.
Reference: [10] <author> D. </author> <title> Chapman and L.P. Kaelbling. Input generalization in delayed reinforcement learning: An algorithm and performance comparisons. </title> <booktitle> In International Joint Conference on Artificial Intelligence, </booktitle> <address> Sydney, Australia, </address> <year> 1991. </year>
Reference-contexts: Second, at the opposite of immediate reinforcement techniques, most of the systems using delayed rewards were tested by simulation to prove their theoritical properties <ref> [57, 52, 10, 33] </ref> but only some of them were validated on real experiments [58, 55]. In addition to the delayed assignement problem, those algorithms face the same categorization problems (state identification) as in the immediate reinforcement case. Both categorization and association should be considered at the same time.
Reference: [11] <author> R. Chatila. </author> <title> Deliberation and reactivity in autonomous mobile robots. </title> <booktitle> Robotics and Autonomous System, </booktitle> <address> 16(2-4):197-211, </address> <month> December </month> <year> 1995. </year>
Reference: [12] <author> J.L. Denebourg, S. Goss, N. Franks, A. Sendova-Franks, C. Detrain, and L. Chretien. </author> <title> The dynamics of collective sorting: Robot-like ants and ant-like robots. </title> <editor> In J.A. Meyer and S. Wilson, editors, </editor> <booktitle> Conference on Simulation of Adaptive Behavior, </booktitle> <address> Cambridge, 1990. </address> <publisher> MIT Press. </publisher>
Reference-contexts: In Artificial Life (AL) approach, nowadays, most contributions on autonomous robots deal with simple universes in which evolve one or several simple robot (s). The collective tasks implemented are obviously very interesting because of their emergent behaviors: simple robots are able to generate complex behaviors <ref> [12, 38, 18, 47] </ref>. The adaptation capability of those robots does not lean on any internal adaptation mechanism but only supposes to take into account the feedback loop on the environment during the design process (constructivist approach [39, 62, 56]).
Reference: [13] <author> G. Edelman. </author> <title> Neural Darwinism: The Theory of Neuronal Group Selection. </title> <publisher> Basic Books, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: In fact, our mechanism is 5 much more similar to the neural-darwinism proposed by Edelman <ref> [13] </ref> and his notion of reentrant maps [50] (but here the link suppression can be performed according to a delayed fitness value). In order to be able to modify the probability p ij associated to a synaptic weight, an input-output correlation measure must be stored.
Reference: [14] <author> C.R. Gallistel. </author> <title> The organization of learning. </title> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: Accordingly, it distinguishes a reflex response (Unconditional Response - UR) and a forced response (Conditional Response - CR). Conditioning phenomenon can, then, be divided in two types, whether they lean on a UR or not (see [46, 28, 53, 54], and <ref> [14, 32] </ref> for a review and [63, 3] for application). In the first case, a link already exists between US and UR. The problem is then to reinforce the link between a CS and the UR. After learning, even if the CS is presented alone, the robot should respond appropriately. <p> We have shown that the PerAc architecture coupled with the Probabilistic Conditionning rule PCR is an efficient approach to solve delayed credit assignments in an indoor environment. However, in the described N.N. we have deliberately forgotten that some animals can create and use "cognitive maps" of their environment <ref> [59, 14] </ref> to improve their navigation performances.
Reference: [15] <author> P. Gaussier and J.P. Cocquerez. </author> <title> Neural networks for complex scene recognition : simulation of a visual system with several cortical areas. </title> <booktitle> In IJCNN Baltimore, </booktitle> <pages> pages 233-259, </pages> <year> 1992. </year> <month> 26 </month>
Reference-contexts: However, in our previous systems, the robot was only able to perform instantaneous learning, such as associative learning or immediate reinforcement <ref> [15, 20, 21] </ref>. In this paper, we extend our previous on line and immediate learning mechanisms to delayed reinforcement signals so that the robot will be able to learn a sequence of Perception-Actions associations according to a late internal or external reinforcement signal. <p> Shapes are detected by a series of Gabor filters. For legibility reasons we did not give details of the complete visual system (for further explanation, see <ref> [15, 16] </ref>). In fact, it is only important to understand that the visual system enables the robot to perceive the pictograms as if they were in the center of the image. Even if in reality the images can be slightly translated or rotated.
Reference: [16] <author> P. Gaussier and J.P. Cocquerez. </author> <title> A robot that learns to look a scene : Simulation of a neural network with parietal and temporal cortical areas. To be published in Neural Networks, </title> <year> 1995. </year>
Reference-contexts: Shapes are detected by a series of Gabor filters. For legibility reasons we did not give details of the complete visual system (for further explanation, see <ref> [15, 16] </ref>). In fact, it is only important to understand that the visual system enables the robot to perceive the pictograms as if they were in the center of the image. Even if in reality the images can be slightly translated or rotated.
Reference: [17] <author> P. Gaussier, A. Revel, C. Joulain, and B. </author> <title> Gas. Living in a partially strctured environment: How to bypass the limitation of classical reinforcement techniques. </title> <type> Technical report, </type> <institution> ETIS-ENSEA, </institution> <year> 1996. </year>
Reference-contexts: This mechanism is equivalent to testing several hypotheses during a single exploration. For sake of space and clarity, the internal reward 8 system (limbic system) will not be described in this paper (see <ref> [17] </ref> for more detail). 2.5 Discussion on the PCR algorithm First of all, the choice of binary weights can be justified by the will to be homogeneous with the probabilistic topological map (PTM see box 2) which is used for on line learning of the perceived situations.
Reference: [18] <author> P. Gaussier and S. Zrehen. </author> <title> Avoiding the world model trap: An acting robot does not need to be so smart! Robotics and Computer-Integrated Manufacturing, </title> <booktitle> 11(4) </booktitle> <pages> 279-286, </pages> <year> 1994. </year>
Reference-contexts: In Artificial Life (AL) approach, nowadays, most contributions on autonomous robots deal with simple universes in which evolve one or several simple robot (s). The collective tasks implemented are obviously very interesting because of their emergent behaviors: simple robots are able to generate complex behaviors <ref> [12, 38, 18, 47] </ref>. The adaptation capability of those robots does not lean on any internal adaptation mechanism but only supposes to take into account the feedback loop on the environment during the design process (constructivist approach [39, 62, 56]).
Reference: [19] <author> P. Gaussier and S. Zrehen. </author> <title> Navigating with an animal brain : a neural network for landmark identification and navigation. </title> <booktitle> In Intelligent Vehicles, </booktitle> <pages> pages 399-404, </pages> <year> 1994. </year>
Reference-contexts: In our current research, we try to fill the gap between models of outdoor navigation [21] and goal seeking <ref> [19] </ref> and cortical explanation of planification [1, 2]. Then, we hope to have at the same time reinforcement and planification capabilities. Another important point will be to modify the neuron model so as to introduce continuous time considerations.
Reference: [20] <author> P. Gaussier and S. Zrehen. </author> <title> A topological map for on-line learning : Emergence of obstacle avoidance. </title> <booktitle> In SAB, From Animals to Animats 94, </booktitle> <pages> pages 282-290, </pages> <address> Brighton, 1994. </address> <publisher> MIT Press. </publisher>
Reference-contexts: That is the reason why we have previously developed a Probabilistic Topological Map (PTM) which allows one-shot learning by using only binary weights and by acting on the probabilities of the weights modifications (See box 2 <ref> [20] </ref>). Second, at the opposite of immediate reinforcement techniques, most of the systems using delayed rewards were tested by simulation to prove their theoritical properties [57, 52, 10, 33] but only some of them were validated on real experiments [58, 55]. <p> However, in our previous systems, the robot was only able to perform instantaneous learning, such as associative learning or immediate reinforcement <ref> [15, 20, 21] </ref>. In this paper, we extend our previous on line and immediate learning mechanisms to delayed reinforcement signals so that the robot will be able to learn a sequence of Perception-Actions associations according to a late internal or external reinforcement signal.
Reference: [21] <author> P. Gaussier and S. Zrehen. Perac: </author> <title> A neural architecture to control artificial animals. </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <address> 16(2-4):291-320, </address> <year> 1995. </year>
Reference-contexts: Our neural network architectures are built while studying the dynamical interactions of the robot in its environment. We use a bottom-up approach (constructivism [62, 56]) that consists in making more and more complex systems relying on simpler ones. For instance, as we have shown in a previous paper <ref> [21] </ref> a focus of attention reflex can be a good starting point to build a robot which is able to explore and to analyse a visual scene. That mechanism is performed by a single PerAc block. <p> Moreover, we have also shown that the results of that box can be used by a second PerAc box to learn to recognize a place in an indoor or outdoor environment and to learn how to reach it from any position in the surrounding environment <ref> [21] </ref>. However, in our previous systems, the robot was only able to perform instantaneous learning, such as associative learning or immediate reinforcement [15, 20, 21]. <p> However, in our previous systems, the robot was only able to perform instantaneous learning, such as associative learning or immediate reinforcement <ref> [15, 20, 21] </ref>. In this paper, we extend our previous on line and immediate learning mechanisms to delayed reinforcement signals so that the robot will be able to learn a sequence of Perception-Actions associations according to a late internal or external reinforcement signal. <p> In our current research, we try to fill the gap between models of outdoor navigation <ref> [21] </ref> and goal seeking [19] and cortical explanation of planification [1, 2]. Then, we hope to have at the same time reinforcement and planification capabilities. Another important point will be to modify the neuron model so as to introduce continuous time considerations.
Reference: [22] <author> S. Harnad. </author> <title> The symbol grounding problem. </title> <journal> Physica D, </journal> <volume> 42 </volume> <pages> 335-346, </pages> <year> 1990. </year>
Reference-contexts: New ways to build autonomous robots (partly addressed by the Animat and/or the Artificial Life communities) promote models or concepts that should solve the limitations of the symbolic approaches of Artificial Intelligence such as the "symbol grounding problem" <ref> [22] </ref> and the "frame problem". In Artificial Life (AL) approach, nowadays, most contributions on autonomous robots deal with simple universes in which evolve one or several simple robot (s).
Reference: [23] <author> D.O. Hebb. </author> <title> The Organization of Behavior. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1949. </year>
Reference-contexts: The problem is then to reinforce the link between a CS and the UR. After learning, even if the CS is presented alone, the robot should respond appropriately. Such simple conditioning mechanisms are based on the Hebb rule <ref> [23] </ref> (see equation 1) and on classical model of formal neurons. The weight of each link between neurons i and j is given by W ij .
Reference: [24] <author> R. Hecht-Nielsen. </author> <title> Counterpropagation networks. </title> <journal> Applied Optics, </journal> <volume> 26(23) </volume> <pages> 4979-4984, </pages> <year> 1987. </year>
Reference-contexts: classical conditioning possible and p ij (t) I i (t) allows to avoid unstable behavior of the p ij (t) terms (pseudo normalization of the p ij ). 9 Box 1: The PerAc architecture The PerAc (Perception-Action) block has been proposed as an elementary generic brick of neuronal computation (Hecht-Nielsen <ref> [1, 24, 8] </ref>). It enables on-line learning of sensory-motor associations. A PerAc block is divided into two levels corresponding to the action and to the perception data flows.
Reference: [25] <author> L. Pack Kaelbling, M.L. Littman, and A.W. Moore. </author> <title> Reinforcement learning: A survey. </title> <journal> Journal of Artificial Intelligence Research, </journal> <note> to appear. </note>
Reference-contexts: Watkins has proved analytically that his algorithm converges to an optimal policy (the best final reward) in the Markov case. Only a very simple fitness function (a reinforcement signal r 2 IR) is used to evaluate the system behavior (the succession of state-actions). For a review see <ref> [25] </ref>. 4.3.2 Comparison with a Q-learning implementation Our sensory-motor association problem consists in learning given associations between immediate observation and actions. Yet, it has been shown that this problem was NP-complete and not Markovian [35, 67]. The Q-learning algorithm is thus not guaranteed to converge.
Reference: [26] <author> J. Kodjabachian and J.A. Meyer. </author> <title> Evolution and development of control architectures in animats. </title> <booktitle> Robotics and Autonomous System, </booktitle> <address> 16(2-4):161-182, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: Yet, a random draw is done in order to change weights whose confidence term is low. Such a mechanism brings the robot to behave as if it was testing hypotheses. It is interesting to draw a parallel between the PCR algorithm and Genetic Algorithms (GA) (see <ref> [26, 43] </ref>). The way binary weights are switched can be compared to the GA mutation process which allows bits change in the DNA sequence of an individual.
Reference: [27] <author> T. Kohonen. </author> <title> Self-Organization and Associative Memory. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1984. </year>
Reference-contexts: permits to reach the exit of the maze is solved by the PCR process as described in the previous section. 14 Box 2: The Probabilistic Topological Map (PTM) PTM has been imagined to take advantage of the main features of Carpenter&Grossberg's Adaptive Resonance Theory (ART [9]) and Kohonen's topological map <ref> [27] </ref>: ART models are very interesting because they allow on-line learning (as autonomy is concerned), while Kohonen's maps allow a priori topological generalization. Topological representations are interesting for two main reasons. First they allow to preserve, at least locally, the topology of the input information.
Reference: [28] <author> J. Konorski. </author> <title> Conditioned reflexes and neuron organisation. </title> <publisher> The university press, </publisher> <address> Cambridge, Eng-land, </address> <year> 1948. </year>
Reference-contexts: Accordingly, it distinguishes a reflex response (Unconditional Response - UR) and a forced response (Conditional Response - CR). Conditioning phenomenon can, then, be divided in two types, whether they lean on a UR or not (see <ref> [46, 28, 53, 54] </ref>, and [14, 32] for a review and [63, 3] for application). In the first case, a link already exists between US and UR. The problem is then to reinforce the link between a CS and the UR.
Reference: [29] <author> I. Krechevsky. </author> <title> The genesis of "hyootheses" in rats. </title> <institution> Univ. Calif. Publ. Psychol., 6(4):46, </institution> <year> 1932. </year>
Reference-contexts: Moreover, a position cannot be recognized just by its (x,y) coordinates. 2.3 Making and testing hypotheses : the PCR algorithm Studies of animal strategies in maze experiments provide good hints to imagine robot control architectures that solve problems noticed in the previous section. For instance, Krechevsky <ref> [29] </ref> has proposed an interesting experiment in which a random search is unable to explain the rat capacity to solve a high dimension association problem. In his experiment, the rat must pass 10 times a day through 4 identical discrimination-boxes (see figure 4.
Reference: [30] <author> M. Levine. </author> <title> A model of hypothesis behavior in discrimination learning set. </title> <journal> Psychological Review, </journal> <volume> 66(6) </volume> <pages> 353-366, </pages> <year> 1959. </year>
Reference-contexts: So, there are 40 choices to be tested. Krechevsky found that the rats mainly performed a systematic well-above-chance choice called "hypothesis" (see also <ref> [30, 31] </ref> and [61] for all-or-none learning). the correct door may be determined by the experimenter in terms of its being light or dark, left or right. In this section, we propose a neural learning rule that tries to model such a behavior. <p> But, those mechanisms cannot explain everything. Indeed, there can be several well recognized objects in a visual scene (the best recognized object can be not relevant for the task). An intelligent system should be able to build a set of hypotheses as human being do <ref> [30] </ref>. Obviously, if the focus of attention mechanism used to select possibly salient objects choose an incomplete or biased set of hybotheses, the system will be unable to solve the problem. It is exactly what seems to happen for children who do badly at school [31].
Reference: [31] <author> M. Levine. </author> <title> Hypothesis theory and nonlearning despite ideal s-r-reinforcement contingencies. </title> <journal> Psychological Review, </journal> <volume> 78(2) </volume> <pages> 130-140, </pages> <year> 1971. </year>
Reference-contexts: So, there are 40 choices to be tested. Krechevsky found that the rats mainly performed a systematic well-above-chance choice called "hypothesis" (see also <ref> [30, 31] </ref> and [61] for all-or-none learning). the correct door may be determined by the experimenter in terms of its being light or dark, left or right. In this section, we propose a neural learning rule that tries to model such a behavior. <p> Obviously, if the focus of attention mechanism used to select possibly salient objects choose an incomplete or biased set of hybotheses, the system will be unable to solve the problem. It is exactly what seems to happen for children who do badly at school <ref> [31] </ref>. Thus, the architecture for the control of an autonomous robot that use complex sensors (such as vision) cannot be reduced to simple sensori-motor associations. One cannot hope to scale up behavior based learning strategies to this kind of problems.
Reference: [32] <author> D.A. Lieberman. </author> <title> Learning: Behavior and Cognition. </title> <booktitle> 2nd edition, </booktitle> <year> 1993. </year>
Reference-contexts: Accordingly, it distinguishes a reflex response (Unconditional Response - UR) and a forced response (Conditional Response - CR). Conditioning phenomenon can, then, be divided in two types, whether they lean on a UR or not (see [46, 28, 53, 54], and <ref> [14, 32] </ref> for a review and [63, 3] for application). In the first case, a link already exists between US and UR. The problem is then to reinforce the link between a CS and the UR. After learning, even if the CS is presented alone, the robot should respond appropriately.
Reference: [33] <author> Long-Ji Lin. </author> <title> Programming robots using reinforcement learning and teaching. </title> <booktitle> In Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 781-786, </pages> <year> 1991. </year>
Reference-contexts: Second, at the opposite of immediate reinforcement techniques, most of the systems using delayed rewards were tested by simulation to prove their theoritical properties <ref> [57, 52, 10, 33] </ref> but only some of them were validated on real experiments [58, 55]. In addition to the delayed assignement problem, those algorithms face the same categorization problems (state identification) as in the immediate reinforcement case. Both categorization and association should be considered at the same time. <p> Hence, it will obtain a positive reward which allows the learning of the correct set of Perception-Action pairs (see figure 6). side of the maze. Most methods <ref> [66, 33] </ref> do not take into account that situations are not equiprobable. For instance, they suppose, that a corridor can be easily recognized as such. <p> Of course, the associations between the vanishing cells and the robot movements to avoid obstacles could have been learned ([64, 20],...). However, this "reflex learning" must be over before the beginning of the maze learning in order to overcome combinational explosion (hierarchical learning <ref> [33, 34] </ref>). The complete architecture of our neural network is displayed figure 18. 3.3.2 Comment on the robot trajectories In figure 19, we present a trajectory of the robot in a simple T-maze. The maze is 3m wide and 1.8m long.
Reference: [34] <author> Long-Ji Lin. </author> <title> Self-improving reactive agents based on reinforcement learning, planning and teaching. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 293-321, </pages> <year> 1992. </year>
Reference-contexts: Of course, the associations between the vanishing cells and the robot movements to avoid obstacles could have been learned ([64, 20],...). However, this "reflex learning" must be over before the beginning of the maze learning in order to overcome combinational explosion (hierarchical learning <ref> [33, 34] </ref>). The complete architecture of our neural network is displayed figure 18. 3.3.2 Comment on the robot trajectories In figure 19, we present a trajectory of the robot in a simple T-maze. The maze is 3m wide and 1.8m long.
Reference: [35] <author> M.L. Littman. </author> <title> Memoryless policies: Theoritical limitations and practical results. </title> <editor> In D. Cliff, P. Husbands, J.A.Meyer, and S.W.Wilson, editors, </editor> <booktitle> Third International Conference on Simulation of Ada-tive Behavior: From Animals to Animats, </booktitle> <pages> pages 238-245, </pages> <address> Cambridge, MA, 1994. </address> <publisher> MIT Press. </publisher>
Reference-contexts: An intuitive solution consists in selecting a random response (by adding noise to the neuron output see figure 5-a ) and testing the consequences. The number of stimuli and responses must not be too large, otherwise the probability of finding the right association will be very low (NP-complete problem <ref> [35] </ref>). <p> For a review see [25]. 4.3.2 Comparison with a Q-learning implementation Our sensory-motor association problem consists in learning given associations between immediate observation and actions. Yet, it has been shown that this problem was NP-complete and not Markovian <ref> [35, 67] </ref>. The Q-learning algorithm is thus not guaranteed to converge. Yet, practically, it is shown that it is possible to find the solution.
Reference: [36] <author> S. Mahadevan and J. Connell. </author> <title> Automatic programming of behavior -based robots using reinforcement learning. </title> <booktitle> In Ninth National Conference on Artificial Intelligence, </booktitle> <address> Menlo Park, CA, </address> <year> 1991. </year>
Reference: [37] <author> M.J. Mataric. </author> <title> Learning to behave socially. </title> <editor> In D.Cliff, P.Husbands, J.A. Meyer, and S.Wilson, editors, </editor> <booktitle> From Animals to Animats: International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pages 453-462, </pages> <year> 1994. </year>
Reference-contexts: Generally, the robot learning needs two successive phases: off-line categorization of the possible situations and on-line learning of the associations between the categories and the actions. The categorization is often performed according to statistical data or even designed by hand (thresholds on the sensor values provided by the engineers <ref> [37] </ref>). The first well investigated case of reinforcement learning (both by simulation and by real experiments) is the one in which the reward or the fitness value is available at each time step ([6, 5] for a review see Torras [60]).
Reference: [38] <author> M.J. Mataric. </author> <title> Issues and approaches in the design of collective autonomous agents. </title> <booktitle> Robotics and Autonomous System, </booktitle> <address> 16(2-4):321-331, </address> <month> December </month> <year> 1995. </year> <month> 27 </month>
Reference-contexts: In Artificial Life (AL) approach, nowadays, most contributions on autonomous robots deal with simple universes in which evolve one or several simple robot (s). The collective tasks implemented are obviously very interesting because of their emergent behaviors: simple robots are able to generate complex behaviors <ref> [12, 38, 18, 47] </ref>. The adaptation capability of those robots does not lean on any internal adaptation mechanism but only supposes to take into account the feedback loop on the environment during the design process (constructivist approach [39, 62, 56]).
Reference: [39] <author> H.R. Mataruna and F.J. Varela. </author> <title> Autopoiesis and Cognition: the realization of the living. </title> <publisher> Reidel, </publisher> <address> Dordrecht, </address> <year> 1980. </year>
Reference-contexts: The adaptation capability of those robots does not lean on any internal adaptation mechanism but only supposes to take into account the feedback loop on the environment during the design process (constructivist approach <ref> [39, 62, 56] </ref>). However, those robots require simple and explicit sensors that must directly fit an interpretation (for instance: to detect the only red objects into a scene, you can use a photosensor sensible to red color!).
Reference: [40] <author> J.L. McClelland, D.E. Rumelhart, and G.E. Hinton. </author> <title> PDP, </title> <booktitle> chapter The Appeal of Parallel Distributed Processing. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1986. </year>
Reference-contexts: When the categories are not linearly separable according to the motor actions, a "hidden layer" of neurons must be added between the input and the output of the system. In the 1 case of unsupervised learning, it can be a WTA group (Winner Take All <ref> [40] </ref> ), an ART architecture (Adaptive Resonance Theory - [9], [3] for an application), or a quickly learnable version of the Kohonen map ([27], [45] , [47] for applications) which remains a statistical classifier: the most examples of a class are presented, the most neurons are used to represent the class
Reference: [41] <author> M. Meng and A.C. Kak. </author> <title> Mobile robot navigation using neural networks and nonmetrical environment models. </title> <journal> IEEE Control Systems, </journal> <pages> pages 30-39, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: The localization of vanishing points can be obtained by different methods such as a log-polar transform [7] or a Hough transform <ref> [41] </ref>. Yet, in a neural context, a vanishing point detector can be implemented with orientation sensible cells. In the simplest case, 3 vanishing detection points are used to sum the activity of orientation sensible cells distributed in the alignment of the vanishing point (see figure 17 -a) ).
Reference: [42] <author> J.R. Millan. </author> <title> Learning efficient reactive behavioral sequences from basic reflexes in a goal-directed autonomous robot. </title> <editor> In D.Cliff, P. Husbands, J.A. Meyer, and S.W. Wilson, editors, SAB: </editor> <booktitle> From Animals to Animats, </booktitle> <pages> pages 266-274, </pages> <year> 1994. </year>
Reference: [43] <author> F. Mondada and D. Floreano. </author> <title> Evolution of neural control structures: some experiments on mobile robots. </title> <booktitle> Robotics and Autonomous System, </booktitle> <address> 16(2-4):183-195, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: Yet, a random draw is done in order to change weights whose confidence term is low. Such a mechanism brings the robot to behave as if it was testing hypotheses. It is interesting to draw a parallel between the PCR algorithm and Genetic Algorithms (GA) (see <ref> [26, 43] </ref>). The way binary weights are switched can be compared to the GA mutation process which allows bits change in the DNA sequence of an individual.
Reference: [44] <author> U. Nehmzow and B. McGonigle. </author> <title> Robot navigation by light. </title> <booktitle> In European Conference on Artificial Life, </booktitle> <address> Brussels, </address> <year> 1993. </year>
Reference-contexts: Demonstrative results were obtained either for already defined categories ([42, 36] ) or for input information simple enough to be directly used for a resensory-motor association (linearly separable categories: [48], <ref> [44] </ref>). When the categories are not linearly separable according to the motor actions, a "hidden layer" of neurons must be added between the input and the output of the system.
Reference: [45] <author> U. Nehmzow and T.Smithers. </author> <title> Mapbuilding using self-organasing networks. </title> <editor> In J.A. Meyer and S. Wilson, editors, </editor> <booktitle> From Animals to Animats: 1st International Conference on Simulation of Adaptive Behavior, </booktitle> <address> Cambridge, MA, 1991. </address> <publisher> MIT Press. </publisher>
Reference-contexts: In the 1 case of unsupervised learning, it can be a WTA group (Winner Take All [40] ), an ART architecture (Adaptive Resonance Theory - [9], [3] for an application), or a quickly learnable version of the Kohonen map ([27], <ref> [45] </ref> , [47] for applications) which remains a statistical classifier: the most examples of a class are presented, the most neurons are used to represent the class on the map.
Reference: [46] <author> I.P. Pavlov. </author> <title> Conditioned Reflexes. </title> <publisher> Oxford University Press, </publisher> <year> 1927. </year>
Reference-contexts: Accordingly, it distinguishes a reflex response (Unconditional Response - UR) and a forced response (Conditional Response - CR). Conditioning phenomenon can, then, be divided in two types, whether they lean on a UR or not (see <ref> [46, 28, 53, 54] </ref>, and [14, 32] for a review and [63, 3] for application). In the first case, a link already exists between US and UR. The problem is then to reinforce the link between a CS and the UR.
Reference: [47] <author> R. Pfeifer and C. Scheier. </author> <title> Sensory-motor coordination: the metaphor and beyond. </title> <note> this issue, </note> <year> 1996. </year>
Reference-contexts: In the 1 case of unsupervised learning, it can be a WTA group (Winner Take All [40] ), an ART architecture (Adaptive Resonance Theory - [9], [3] for an application), or a quickly learnable version of the Kohonen map ([27], [45] , <ref> [47] </ref> for applications) which remains a statistical classifier: the most examples of a class are presented, the most neurons are used to represent the class on the map. <p> In Artificial Life (AL) approach, nowadays, most contributions on autonomous robots deal with simple universes in which evolve one or several simple robot (s). The collective tasks implemented are obviously very interesting because of their emergent behaviors: simple robots are able to generate complex behaviors <ref> [12, 38, 18, 47] </ref>. The adaptation capability of those robots does not lean on any internal adaptation mechanism but only supposes to take into account the feedback loop on the environment during the design process (constructivist approach [39, 62, 56]).
Reference: [48] <author> R. Pfeifer and P. Verschure. </author> <title> The Artificial Life Route to Artificial Intelligence, chapter The challenge of autonomous systems: Pitfalls and how to avoid them. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: Demonstrative results were obtained either for already defined categories ([42, 36] ) or for input information simple enough to be directly used for a resensory-motor association (linearly separable categories: <ref> [48] </ref>, [44]). When the categories are not linearly separable according to the motor actions, a "hidden layer" of neurons must be added between the input and the output of the system.
Reference: [49] <editor> J. Piaget. La naissance de l'intelligence chez l'enfant. Delachaux et Niestle Editions, </editor> <address> Geneve, </address> <year> 1936. </year>
Reference-contexts: The limitations of reinforcement mechanisms would make it impossible for an animal or a robot to address very complex universes such as outdoor environments, which is nonsense. That combinatorial problem could be partly solved assuming a darwinian process and/or an ontological development of the Perception-Action system <ref> [49] </ref>. But, those mechanisms cannot explain everything. Indeed, there can be several well recognized objects in a visual scene (the best recognized object can be not relevant for the task). An intelligent system should be able to build a set of hypotheses as human being do [30].
Reference: [50] <author> G.N. Reeke, O. Sporns, and G.M. Edelman. </author> <title> Synthetic neural modeling: The "darwin" series of recognition automata. </title> <editor> In C. Lau and B. Widrow, editors, </editor> <booktitle> Special issue on Neural Networks, </booktitle> <pages> pages 1498-1530. </pages> <publisher> IEEE, </publisher> <month> September </month> <year> 1990. </year>
Reference-contexts: In fact, our mechanism is 5 much more similar to the neural-darwinism proposed by Edelman [13] and his notion of reentrant maps <ref> [50] </ref> (but here the link suppression can be performed according to a delayed fitness value). In order to be able to modify the probability p ij associated to a synaptic weight, an input-output correlation measure must be stored.
Reference: [51] <author> G. Schoner, M. Dose, and C. Engels. </author> <title> Dynamics of behavior: theory and applications for autonomous robot architectures. </title> <booktitle> Robotics and Autonomous System, </booktitle> <address> 16(2-4):213-245, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: Another important point will be to modify the neuron model so as to introduce continuous time considerations. Neuron simulating by differential equations should avoid harshly neuron activation and allow the use of dynamic fields modelling for the control of autonomous robots <ref> [51] </ref>. In conclusion, we would like to point out what has been already done in the field of autonomous robots to better understand what are the next problems to solve.
Reference: [52] <author> S.P. Singh and R.S. Sutton. </author> <title> Reinforcement learning with replacing eligibility traces. </title> <booktitle> Machine Learning, </booktitle> <year> 1996. </year>
Reference-contexts: Second, at the opposite of immediate reinforcement techniques, most of the systems using delayed rewards were tested by simulation to prove their theoritical properties <ref> [57, 52, 10, 33] </ref> but only some of them were validated on real experiments [58, 55]. In addition to the delayed assignement problem, those algorithms face the same categorization problems (state identification) as in the immediate reinforcement case. Both categorization and association should be considered at the same time.
Reference: [53] <editor> B.F. Skinner. </editor> <booktitle> Science and human behavior. </booktitle> <publisher> Macmillan Company, </publisher> <address> New York, </address> <year> 1953. </year>
Reference-contexts: Accordingly, it distinguishes a reflex response (Unconditional Response - UR) and a forced response (Conditional Response - CR). Conditioning phenomenon can, then, be divided in two types, whether they lean on a UR or not (see <ref> [46, 28, 53, 54] </ref>, and [14, 32] for a review and [63, 3] for application). In the first case, a link already exists between US and UR. The problem is then to reinforce the link between a CS and the UR.
Reference: [54] <author> K.W. Spence. </author> <title> Behavior theory and conditioning. </title> <publisher> Yale University Press, </publisher> <address> New Haven, </address> <year> 1956. </year>
Reference-contexts: Accordingly, it distinguishes a reflex response (Unconditional Response - UR) and a forced response (Conditional Response - CR). Conditioning phenomenon can, then, be divided in two types, whether they lean on a UR or not (see <ref> [46, 28, 53, 54] </ref>, and [14, 32] for a review and [63, 3] for application). In the first case, a link already exists between US and UR. The problem is then to reinforce the link between a CS and the UR.
Reference: [55] <author> L. Steels. </author> <title> A selectionist mechanism for autonomous behavior acquisition. </title> <note> this issue, </note> <year> 1996. </year>
Reference-contexts: Second, at the opposite of immediate reinforcement techniques, most of the systems using delayed rewards were tested by simulation to prove their theoritical properties [57, 52, 10, 33] but only some of them were validated on real experiments <ref> [58, 55] </ref>. In addition to the delayed assignement problem, those algorithms face the same categorization problems (state identification) as in the immediate reinforcement case. Both categorization and association should be considered at the same time.
Reference: [56] <author> J. Stewart. </author> <title> The implication for understanding high-level cognition of a grounding in elementary adaptive systems. </title> <booktitle> Robotics and Autonomous System, </booktitle> <address> 16(2-4):107-116, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: Our neural network architectures are built while studying the dynamical interactions of the robot in its environment. We use a bottom-up approach (constructivism <ref> [62, 56] </ref>) that consists in making more and more complex systems relying on simpler ones. <p> Practically, the robot task is to learn the "meaning" of pictograms such as turning right when it sees a "right-turn" arrow (see figure 8-b ). The task is defined with proscriptive constraints only <ref> [56] </ref> linked to internal motivations (we only define what is the viability domain of the system). <p> The adaptation capability of those robots does not lean on any internal adaptation mechanism but only supposes to take into account the feedback loop on the environment during the design process (constructivist approach <ref> [39, 62, 56] </ref>). However, those robots require simple and explicit sensors that must directly fit an interpretation (for instance: to detect the only red objects into a scene, you can use a photosensor sensible to red color!).
Reference: [57] <author> R.S. Sutton and A.G. Barto. </author> <title> Learning and computational neuroscience: </title> <booktitle> foundations of adaptive networks, chapter Time-Derivative Models of Pavlovian Reinforcement, </booktitle> <pages> pages 497-537. </pages> <editor> M. Gabriel and J. Moore, </editor> <year> 1990. </year>
Reference-contexts: Second, at the opposite of immediate reinforcement techniques, most of the systems using delayed rewards were tested by simulation to prove their theoritical properties <ref> [57, 52, 10, 33] </ref> but only some of them were validated on real experiments [58, 55]. In addition to the delayed assignement problem, those algorithms face the same categorization problems (state identification) as in the immediate reinforcement case. Both categorization and association should be considered at the same time.
Reference: [58] <author> Sebastian Thrun and Tom M. Mitchell. </author> <title> Lifelong robot learning. </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <volume> 15 </volume> <pages> 25-46, </pages> <year> 1995. </year>
Reference-contexts: Second, at the opposite of immediate reinforcement techniques, most of the systems using delayed rewards were tested by simulation to prove their theoritical properties [57, 52, 10, 33] but only some of them were validated on real experiments <ref> [58, 55] </ref>. In addition to the delayed assignement problem, those algorithms face the same categorization problems (state identification) as in the immediate reinforcement case. Both categorization and association should be considered at the same time. <p> In practical realizations, this problem is got round by an off-line statistical learning of all the possible states. For instance, multilayers perceptron using backpropagation algorithm can be used to define a pseudo-continuous state space <ref> [58] </ref>. In even more difficult cases, an infinite number of states can be generated because of loops or because of non-planar mazes.
Reference: [59] <author> E.C. Tolman. </author> <title> Cognitive maps in rats and men. </title> <journal> The Psychological Review, </journal> <volume> 55(4), </volume> <year> 1948. </year>
Reference-contexts: We have shown that the PerAc architecture coupled with the Probabilistic Conditionning rule PCR is an efficient approach to solve delayed credit assignments in an indoor environment. However, in the described N.N. we have deliberately forgotten that some animals can create and use "cognitive maps" of their environment <ref> [59, 14] </ref> to improve their navigation performances.
Reference: [60] <author> C. Torras. </author> <title> Robot adaptivity. </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <volume> 15 </volume> <pages> 11-23, </pages> <year> 1995. </year>
Reference-contexts: The first well investigated case of reinforcement learning (both by simulation and by real experiments) is the one in which the reward or the fitness value is available at each time step ([6, 5] for a review see Torras <ref> [60] </ref>). Demonstrative results were obtained either for already defined categories ([42, 36] ) or for input information simple enough to be directly used for a resensory-motor association (linearly separable categories: [48], [44]).
Reference: [61] <author> T. Trabasso. </author> <title> Stimulus emphasis and all-or-none learning of concept identification. </title> <journal> Journal of Experimental Psychology, </journal> <volume> 65 </volume> <pages> 395-406, </pages> <year> 1963. </year>
Reference-contexts: So, there are 40 choices to be tested. Krechevsky found that the rats mainly performed a systematic well-above-chance choice called "hypothesis" (see also [30, 31] and <ref> [61] </ref> for all-or-none learning). the correct door may be determined by the experimenter in terms of its being light or dark, left or right. In this section, we propose a neural learning rule that tries to model such a behavior.
Reference: [62] <author> F. Varela, E. Thompson, and E. Rosch. </author> <title> The Embodied Mind. </title> <publisher> MIT Press, </publisher> <year> 1993. </year> <month> 28 </month>
Reference-contexts: Our neural network architectures are built while studying the dynamical interactions of the robot in its environment. We use a bottom-up approach (constructivism <ref> [62, 56] </ref>) that consists in making more and more complex systems relying on simpler ones. <p> The adaptation capability of those robots does not lean on any internal adaptation mechanism but only supposes to take into account the feedback loop on the environment during the design process (constructivist approach <ref> [39, 62, 56] </ref>). However, those robots require simple and explicit sensors that must directly fit an interpretation (for instance: to detect the only red objects into a scene, you can use a photosensor sensible to red color!).
Reference: [63] <author> F.M.J. Verschure, J. Wray, O. Sporns, G. Tononi, and G.M. Edelman. </author> <title> Multilevel analysis of a behaving real world artifact: an illustration of synthetic neural modeling. </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <pages> 16(2-4), </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: Accordingly, it distinguishes a reflex response (Unconditional Response - UR) and a forced response (Conditional Response - CR). Conditioning phenomenon can, then, be divided in two types, whether they lean on a UR or not (see [46, 28, 53, 54], and [14, 32] for a review and <ref> [63, 3] </ref> for application). In the first case, a link already exists between US and UR. The problem is then to reinforce the link between a CS and the UR. After learning, even if the CS is presented alone, the robot should respond appropriately.
Reference: [64] <author> P.F.M.J Verschure and R.Pfeifer. </author> <title> Categorization, representation, and the dynamics of system-environment interaction. </title> <booktitle> In SAB-92, From animals to animats, </booktitle> <pages> pages 210-217, </pages> <year> 1992. </year>
Reference: [65] <author> C.J.C.H. Watkins. </author> <title> Learning from delayed rewards. </title> <type> PhD thesis, </type> <institution> Psychology Department, Cambridge University, </institution> <address> Cambridge, England, </address> <year> 1989. </year>
Reference-contexts: patterns Kind of maze 3 4 Short 34 34 Long 67 67 Loop maze ? ? Table 3: Performances on different mazes for our adapted version of the Q-learning algorithm (median values). 4.3.1 A state space description A general frame for solving delayed credit assignment has been proposed by Watkins <ref> [65] </ref>: Q-learning supposes that a set of situations or states is available. The algorithm tries to attribute to each pair of state-action a saliency measurement Q (s; a). In table 2 a), for instance, there are four different states and four possible actions.
Reference: [66] <author> S.E. Weaver, A.H. Klopf, and J.S. Morgan. </author> <title> A hierarchical network of control systems that learn : modeling nervous system function during classical and instrumental conditioning. </title> <booktitle> Adaptive behavior, </booktitle> <volume> 1(3) </volume> <pages> 263-319, </pages> <year> 1993. </year>
Reference-contexts: Hence, it will obtain a positive reward which allows the learning of the correct set of Perception-Action pairs (see figure 6). side of the maze. Most methods <ref> [66, 33] </ref> do not take into account that situations are not equiprobable. For instance, they suppose, that a corridor can be easily recognized as such.
Reference: [67] <author> Steven D. Whitehead. </author> <title> Complexity and cooperation in q-learning. </title> <booktitle> In Eight International Conference on Machine Learning, </booktitle> <pages> pages 363-367, </pages> <address> Evanston, IL, 1991. </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: For a review see [25]. 4.3.2 Comparison with a Q-learning implementation Our sensory-motor association problem consists in learning given associations between immediate observation and actions. Yet, it has been shown that this problem was NP-complete and not Markovian <ref> [35, 67] </ref>. The Q-learning algorithm is thus not guaranteed to converge. Yet, practically, it is shown that it is possible to find the solution.
Reference: [68] <author> S. Zrehen and P. Gaussier. </author> <title> Why topological maps are useful for learning in an autonomous agent. In From perception to action, </title> <booktitle> 1994. </booktitle> <pages> 29 </pages>
Reference-contexts: On the contrary, with topological preservation, a neuron in the neighborhood of a given winner will further respond depending on the distance to it. It thus gives interesting generalization capabilities <ref> [68] </ref>.
References-found: 68

