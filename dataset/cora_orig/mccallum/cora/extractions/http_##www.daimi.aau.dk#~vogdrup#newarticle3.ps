URL: http://www.daimi.aau.dk/~vogdrup/newarticle3.ps
Refering-URL: http://www.daimi.aau.dk/~vogdrup/Work.html
Root-URL: http://www.daimi.aau.dk
Email: E-mail: vogdrup@daimi.aau.dk  
Phone: Phone: 45 89 42 33 55  
Title: Combining Predictors: Some Old Methods and a New Method  
Author: Jakob Vogdrup Hansen 
Note: In tests DynCo compares favorably with the four other methods.  
Date: September 22, 1998  
Address: Ny Munkegade, Bldg. 540 DK-8000 Aarhus C Denmark  
Affiliation: Department of Computer Science University of Aarhus  
Abstract: For some years there has been a trend toward combining predictors, and away from big monolithic predictors. Two groups of predictor combination methods are ensemble methods and mixture of experts methods. In this paper, five different representatives, three from the ensemble group, and two from the mixture of expert group, are presented, discussed, and compared. The selected methods are Simple ensemble, AdaBoost, Bagging, Hierarchical Mixtures of Experts, and a new mixture of experts method: DynCo. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Bishop, C. M. </author> <title> Neural Networks for Pattern recognition. </title> <publisher> Oxford University Press, </publisher> <address> Great Clarendon Street, Oxford OX2 6DP, </address> <year> 1995. </year>
Reference-contexts: A number of such machine learning methods have been developed, e.g. k-nearest neighbor and Kernel functions (see e.g. <ref> [1] </ref>, [10], or [11]), Neural Networks [7], CART [4], C4.5 [22], and support vectors [6]. Several of these are universal machine learning methods, since they can approximate a function arbitrarily well, but it is often problematic to achieve a match. In the nineties, meta machine learning methods have been developed. <p> The output of the experts are the estimated mean: ^ ~y j . The experts are linear predictors: ^ ~y j = w T j <ref> [~x; 1] </ref>, where w j are the weights and ~x is the input, so j is f j ; w j g and is S i j . We have used the notation [~x; 1] for the vector (x 1 ; : : : ; x n ; 1) T if <p> The experts are linear predictors: ^ ~y j = w T j <ref> [~x; 1] </ref>, where w j are the weights and ~x is the input, so j is f j ; w j g and is S i j . We have used the notation [~x; 1] for the vector (x 1 ; : : : ; x n ; 1) T if ~x = (x 1 ; : : : ; x n ) T . <p> The vector ~ d k and the matrix A are constructed as follows: ~ d k = t (k) A = t The update rules for new j are: j = t h j (~y t j~x t )(~y t (w new ) T <ref> [~x t ; 1] </ref>)(~y t (w new ) T [~x t ; 1]) T t h j (~y t j~x t ) 3.5 DynCo DynCo can be seen as a generalized regression ME method. <p> d k and the matrix A are constructed as follows: ~ d k = t (k) A = t The update rules for new j are: j = t h j (~y t j~x t )(~y t (w new ) T <ref> [~x t ; 1] </ref>)(~y t (w new ) T [~x t ; 1]) T t h j (~y t j~x t ) 3.5 DynCo DynCo can be seen as a generalized regression ME method. It can be used on any iterative machine learning methods that can be trained with gradient descent. <p> Construct the combined function F (~x) = P M i=1 c i (~x)f i (~x), where c i is defined in equation (17). 8 As mentioned the learning algorithm, L, in algorithm 3, step 3 and 4 is based upon gradient descent (see e.g. chapter 7.5 in <ref> [1] </ref>). The update rule for a pa rameter g in gradient descent is: g new = g old @E (g) @g .
Reference: [2] <author> Breiman, L. </author> <title> Bagging Predictors. </title> <booktitle> Machine Learning 24 (1996), </booktitle> <pages> 123-140. </pages>
Reference-contexts: In the nineties, meta machine learning methods have been developed. We define meta machine learning (MML) as methods for combining predictors generated by machine learning methods. Two groups of methods exist: Ensemble methods, e.g. AdaBoost [9] and Bagging <ref> [2] </ref> and Mixture of Experts (ME) methods ([13] and [15]). There are different reasons for combining predictors. In some cases, training a number of small predictors and combining them, can be more efficient measured in training time, than training one large predictor (e.g. see [18]). <p> For all i 2 (1; : : : ; M ): f i = L (T i ). 4. Construct ensemble predictor F (~x) = 1 M i=1 f i (~x). Bagging can be used to combine any kind of predictors. In <ref> [2] </ref> Breiman states that the benefit of Bagging lies in variance reductions (for a description of bias and variance see [3], [10], or [14]) This is achieved by generating an ensemble of different predictors, where the variance is "averaged out" in the combined 3 predictor. Algorithm 1 describes Bagging.
Reference: [3] <author> Breiman, L. Bias,Variance, </author> <title> and Arcing Classifiers. </title> <type> Tech. Rep. 460, </type> <institution> Statistics Department, University of California, Berkeley, </institution> <address> CA 94720, </address> <month> Apr. </month> <year> 1996. </year>
Reference-contexts: Construct ensemble predictor F (~x) = 1 M i=1 f i (~x). Bagging can be used to combine any kind of predictors. In [2] Breiman states that the benefit of Bagging lies in variance reductions (for a description of bias and variance see <ref> [3] </ref>, [10], or [14]) This is achieved by generating an ensemble of different predictors, where the variance is "averaged out" in the combined 3 predictor. Algorithm 1 describes Bagging.
Reference: [4] <author> Breiman, L., Friedman, J. H., Olshen, R. A., and Stone, C. J. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth and Brooks/Cole, </publisher> <address> Mon-terey, CA, </address> <year> 1984. </year>
Reference-contexts: A number of such machine learning methods have been developed, e.g. k-nearest neighbor and Kernel functions (see e.g. [1], [10], or [11]), Neural Networks [7], CART <ref> [4] </ref>, C4.5 [22], and support vectors [6]. Several of these are universal machine learning methods, since they can approximate a function arbitrarily well, but it is often problematic to achieve a match. In the nineties, meta machine learning methods have been developed.
Reference: [5] <author> Bridle, J. </author> <title> Probabilistic Interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition. </title> <booktitle> Neuro-computing: Algorithms, Architectures and Applications (1990). </booktitle>
Reference-contexts: The combined function is: F (~x) = i=1 The coefficients are defined by the SOFTMAX function: c i (~x) = P M (17) The SOFTMAX function (see <ref> [5] </ref>) is used to automatically ensure that the coefficients obey 0 c i (~x) 1 and P M Definition 2 (DynCo Training Method) In algorithm 3 the DynCo Training Method is shown in algorithm form.
Reference: [6] <author> Cortes, C., and Vapnik, V. </author> <title> Support-vector networks. </title> <type> Tech. rep., </type> <institution> AT&T Bell Laboratories, </institution> <address> Holmdel, NJ 07733, USA, </address> <year> 1995. </year>
Reference-contexts: A number of such machine learning methods have been developed, e.g. k-nearest neighbor and Kernel functions (see e.g. [1], [10], or [11]), Neural Networks [7], CART [4], C4.5 [22], and support vectors <ref> [6] </ref>. Several of these are universal machine learning methods, since they can approximate a function arbitrarily well, but it is often problematic to achieve a match. In the nineties, meta machine learning methods have been developed.
Reference: [7] <author> D. E. Rummelhart, G. E. Hinton, and R. J. Williams. </author> <title> Learning Internal Representation by Error Propagation. </title> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition 1 (1986). </booktitle>
Reference-contexts: A number of such machine learning methods have been developed, e.g. k-nearest neighbor and Kernel functions (see e.g. [1], [10], or [11]), Neural Networks <ref> [7] </ref>, CART [4], C4.5 [22], and support vectors [6]. Several of these are universal machine learning methods, since they can approximate a function arbitrarily well, but it is often problematic to achieve a match. In the nineties, meta machine learning methods have been developed.
Reference: [8] <author> Drucker, H. </author> <title> Improving Regressors using Boosting Techniques. </title> <booktitle> Machine Learning: Proceedings of the Fourteenth International Conference (1997). </booktitle>
Reference-contexts: f i (~x)) 2 and the ambiguity is 1 2 i c i (f i (~x) F (~x)) 2 , since ambiguity is positive, we see that the generalization error is less than or equal to the average error. 3.3 AdaBoost The regressor version of AdaBoost presented here is from <ref> [8] </ref>, which is a modi fication of AdaBoost.R in [24]. Algorithm 2 describes AdaBoost. The AdaBoost method consists of a number of cycles. In each cycle a new training set is generated by sampling with replacement from the original training set, and a predictor is trained using this training set.
Reference: [9] <author> Freund, Y., and Schapire, R. E. </author> <title> A decision-theoretic generalization of on-line learning and an application to boosting. </title> <booktitle> In Proceedings of the Second European Conference on Computational Learning Theory (Mar. 1995), </booktitle> <publisher> Springer-Verlag, </publisher> <pages> pp. 23-37. 20 </pages>
Reference-contexts: In the nineties, meta machine learning methods have been developed. We define meta machine learning (MML) as methods for combining predictors generated by machine learning methods. Two groups of methods exist: Ensemble methods, e.g. AdaBoost <ref> [9] </ref> and Bagging [2] and Mixture of Experts (ME) methods ([13] and [15]). There are different reasons for combining predictors. In some cases, training a number of small predictors and combining them, can be more efficient measured in training time, than training one large predictor (e.g. see [18]).
Reference: [10] <author> Geman, S., and Bienenstock, E. </author> <title> Neural Networks and the Bias/Variance Dilemma. </title> <booktitle> Neural Computation 4 (1992). </booktitle>
Reference-contexts: A number of such machine learning methods have been developed, e.g. k-nearest neighbor and Kernel functions (see e.g. [1], <ref> [10] </ref>, or [11]), Neural Networks [7], CART [4], C4.5 [22], and support vectors [6]. Several of these are universal machine learning methods, since they can approximate a function arbitrarily well, but it is often problematic to achieve a match. In the nineties, meta machine learning methods have been developed. <p> Construct ensemble predictor F (~x) = 1 M i=1 f i (~x). Bagging can be used to combine any kind of predictors. In [2] Breiman states that the benefit of Bagging lies in variance reductions (for a description of bias and variance see [3], <ref> [10] </ref>, or [14]) This is achieved by generating an ensemble of different predictors, where the variance is "averaged out" in the combined 3 predictor. Algorithm 1 describes Bagging.
Reference: [11] <author> H ardle, W. </author> <title> Smoothing techniques with implementation in s. </title> <booktitle> Springer Series in Statistics (1990). </booktitle>
Reference-contexts: A number of such machine learning methods have been developed, e.g. k-nearest neighbor and Kernel functions (see e.g. [1], [10], or <ref> [11] </ref>), Neural Networks [7], CART [4], C4.5 [22], and support vectors [6]. Several of these are universal machine learning methods, since they can approximate a function arbitrarily well, but it is often problematic to achieve a match. In the nineties, meta machine learning methods have been developed.
Reference: [12] <author> Jacobs, R. A., Jordan, M. I., and Barto, A. G. </author> <title> Task decomposition through competition in a modular connectionist architecture: The what and where vision tasks. </title> <type> Tech. rep., </type> <institution> Department of Computer & Information Science, University of Massachusetts, </institution> <address> Amherts, MA 01003, </address> <month> Mar. </month> <year> 1990. </year>
Reference-contexts: The results for the XuME method do not seem to follow the interpretation of fl (see section 3.6), and we conclude that the information is method-dependent. 3.5.3 DynCo Compared to Known Methods DynCo is, as mentioned, a ME method and is related to methods presented in <ref> [12] </ref>, [13], and [16]. There are a number of similarities: A group of predictors are combined using weighting with non-constant coefficients (or "gating network"). All the different combined predictors are trained using gradient descent. The differences are to be found in the error function and the architecture. In [12] there are <p> presented in <ref> [12] </ref>, [13], and [16]. There are a number of similarities: A group of predictors are combined using weighting with non-constant coefficients (or "gating network"). All the different combined predictors are trained using gradient descent. The differences are to be found in the error function and the architecture. In [12] there are two error functions, one for the experts and one for the gating networks. The expert error function is the sum of the square error function for each expert.
Reference: [13] <author> Jacobs, R. A., Jordan, M. I., Nowlan, S. J., and Hinton, G. E. </author> <title> Adaptive mixtures of local experts. </title> <booktitle> Neural Computation 3, 1 (1991), </booktitle> <pages> 79-87. </pages>
Reference-contexts: The results for the XuME method do not seem to follow the interpretation of fl (see section 3.6), and we conclude that the information is method-dependent. 3.5.3 DynCo Compared to Known Methods DynCo is, as mentioned, a ME method and is related to methods presented in [12], <ref> [13] </ref>, and [16]. There are a number of similarities: A group of predictors are combined using weighting with non-constant coefficients (or "gating network"). All the different combined predictors are trained using gradient descent. The differences are to be found in the error function and the architecture. <p> At the same time, the error function forces the coefficients towards either one or zero and the sum of the coefficients towards one. In contrast: DynCo, the methods in <ref> [13] </ref> and [16] use the SOFTMAX function to obtain auto-normalization of the coefficients. In [13] (Jacobs et al.) three different error functions are discussed. The first is (y P j c j f j ) 2 , which (apart from a constant) is the error function used in DynCo. <p> At the same time, the error function forces the coefficients towards either one or zero and the sum of the coefficients towards one. In contrast: DynCo, the methods in <ref> [13] </ref> and [16] use the SOFTMAX function to obtain auto-normalization of the coefficients. In [13] (Jacobs et al.) three different error functions are discussed. The first is (y P j c j f j ) 2 , which (apart from a constant) is the error function used in DynCo.
Reference: [14] <author> James, G., and Hastie, T. </author> <title> Generalizations of the bias/variance decomposition for prediction error. </title> <type> Tech. rep., </type> <institution> Dept. of Statistics, Stanford University, </institution> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: Construct ensemble predictor F (~x) = 1 M i=1 f i (~x). Bagging can be used to combine any kind of predictors. In [2] Breiman states that the benefit of Bagging lies in variance reductions (for a description of bias and variance see [3], [10], or <ref> [14] </ref>) This is achieved by generating an ensemble of different predictors, where the variance is "averaged out" in the combined 3 predictor. Algorithm 1 describes Bagging.
Reference: [15] <author> Jordan, M., and Jacobs, R. </author> <title> Hierarchical mixtures of experts and the EM algorithm. </title> <booktitle> Neural Computation 6, 2 (1994), </booktitle> <pages> 181-214. </pages>
Reference-contexts: In the nineties, meta machine learning methods have been developed. We define meta machine learning (MML) as methods for combining predictors generated by machine learning methods. Two groups of methods exist: Ensemble methods, e.g. AdaBoost [9] and Bagging [2] and Mixture of Experts (ME) methods ([13] and <ref> [15] </ref>). There are different reasons for combining predictors. In some cases, training a number of small predictors and combining them, can be more efficient measured in training time, than training one large predictor (e.g. see [18]). <p> Construct ensemble predictor: F (~x) = inf (y : k:f k (~x)y 1 X log (fi k )) (2) 3.4 Mixture of Experts The term Mixtures of Experts covers a variety of methods. We will present the extension of 'Hierarchical Mixtures of Experts' <ref> [15] </ref> that can be found in [23]. We call the method XuME after the author and use the notation from the article. <p> The P (~xj j ) depends on the parameters f j ; m j g, so j is f j ; m j g and is S The EM algorithm (see <ref> [15] </ref>) is used to maximize a log likelihood function, which in the case of XuME is: l (; ) = log ( j X log (P (~y j j~x j )): (7) The theoretical background is well described in the literature, so we will skip right to the update rules after
Reference: [16] <author> Jordan, M. I., and Jacobs, R. A. </author> <title> Hierarchies of adaptive experts. </title> <booktitle> In Advances in Neural Information Processing Systems (1992), </booktitle> <editor> J. E. Moody, S. J. Hanson, and R. P. Lippmann, Eds., </editor> <volume> vol. 4, </volume> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <pages> pp. 985-992. </pages>
Reference-contexts: The results for the XuME method do not seem to follow the interpretation of fl (see section 3.6), and we conclude that the information is method-dependent. 3.5.3 DynCo Compared to Known Methods DynCo is, as mentioned, a ME method and is related to methods presented in [12], [13], and <ref> [16] </ref>. There are a number of similarities: A group of predictors are combined using weighting with non-constant coefficients (or "gating network"). All the different combined predictors are trained using gradient descent. The differences are to be found in the error function and the architecture. <p> At the same time, the error function forces the coefficients towards either one or zero and the sum of the coefficients towards one. In contrast: DynCo, the methods in [13] and <ref> [16] </ref> use the SOFTMAX function to obtain auto-normalization of the coefficients. In [13] (Jacobs et al.) three different error functions are discussed. The first is (y P j c j f j ) 2 , which (apart from a constant) is the error function used in DynCo. <p> Our experiments suggest that the first error function is the better (see section 3.5.4). Jacobs et al. end up with a third error function log ( j c j e 1 ), which is derived from a likelihood measure. The error function in <ref> [16] </ref> is also a log likelihood measure. The experts are still neural networks in [16], but the gating networks are simple affine predictors before they are combined using the SOFTMAX function. The architecture in [16] is hierarchical with gating networks forming a binary tree, which differs from the architecture of DynCo. <p> Jacobs et al. end up with a third error function log ( j c j e 1 ), which is derived from a likelihood measure. The error function in <ref> [16] </ref> is also a log likelihood measure. The experts are still neural networks in [16], but the gating networks are simple affine predictors before they are combined using the SOFTMAX function. The architecture in [16] is hierarchical with gating networks forming a binary tree, which differs from the architecture of DynCo. 3.5.4 Cooperation or Competition? As mentioned above Jacobs et al. prefer the error function <p> The error function in <ref> [16] </ref> is also a log likelihood measure. The experts are still neural networks in [16], but the gating networks are simple affine predictors before they are combined using the SOFTMAX function. The architecture in [16] is hierarchical with gating networks forming a binary tree, which differs from the architecture of DynCo. 3.5.4 Cooperation or Competition? As mentioned above Jacobs et al. prefer the error function P (y j c j f j ) 2 .
Reference: [17] <author> Krogh, A., and Vedelsby, J. </author> <title> Neural network ensembles, cross validation, and active learning. </title> <booktitle> In Advances in Neural Information Processing Systems (1995), </booktitle> <editor> G. Tesauro, D. Touretzky, and T. Leen, Eds., </editor> <volume> vol. 7, </volume> <publisher> The MIT Press, </publisher> <pages> pp. 231-238. </pages>
Reference-contexts: The purpose can also be to achieve lower generalization error as the combined predictor can be better than the single predictors (e.g. see <ref> [17] </ref>). Another purpose is to prevent overfitting, which is the foundation of many ensemble methods. In this article we will compare some of the methods. We will also introduce a new method, called DynCo that can be regarded as a ME method. <p> We call this method for 'Simple'. In the case of a Simple ensemble using a mean square error function, it is shown in <ref> [17] </ref> that the generalization error ( 1 2 (y F (~x)) 2 ) can be split into the average error for the ensemble members and a term called ambiguity: 1 (y F (~x)) 2 = 2 i 1 X c i (f i (~x) F (~x)) 2 ; (1) where F
Reference: [18] <author> Lu, B.-L., and Ito, M. </author> <title> Task decomposition and module combination based on class relations: A modular neural network for pattern classifica 21 tion. </title> <type> Tech. rep., </type> <institution> Nio-Mimetic Control Research Center, The Institute of Physical and Chemical Research (RIKEN), </institution> <address> 2271-130 Anagahora, Shimoshi-dami, Moriyama-ku, Nagoya 463-0003, Japan, </address> <month> Mar. </month> <year> 1998. </year>
Reference-contexts: AdaBoost [9] and Bagging [2] and Mixture of Experts (ME) methods ([13] and [15]). There are different reasons for combining predictors. In some cases, training a number of small predictors and combining them, can be more efficient measured in training time, than training one large predictor (e.g. see <ref> [18] </ref>). The purpose can also be to achieve lower generalization error as the combined predictor can be better than the single predictors (e.g. see [17]). Another purpose is to prevent overfitting, which is the foundation of many ensemble methods. In this article we will compare some of the methods.
Reference: [19] <author> Merz, C., and Murphy, P. </author> <title> UCI repository of machine learning databases, </title> <year> 1998. </year>
Reference-contexts: constructed with 784 examples. * Brain II: Another Brain scan picture example set from the PET Center, Aarhus University Hospital, DK 8000 Aarhus, Denmark, containing 1000 examples. 5 Align in three dimensions. 6 Positron Emission Tomography. 16 * Abalone: The Abalone example set is from the UCI Machine Learning Repository <ref> [19] </ref>. One input is symbolic (male, female, and infant), which is converted into the values (-0.9, 0.9 and 0). All other values are scaled to have mean zero and standard deviation 0.71. <p> All other values are scaled to have mean zero and standard deviation 0.71. There are 4177 examples. * Thyroid: This example set can be found in various version at both the PROBEN1 database and the UCI Machine Learning Repository <ref> [19] </ref>. The set used contains 7200 examples. * Spiral: This is an example set for the intertwined spiral problem. An illustration of the training set can be seen in figure 14. The spiral set contains 1800 examples.
Reference: [20] <author> Munk, O. L. </author> <title> Anvendelse af neurale netvrk til registrering af hjernebilleder fra positron emissions tomografi. </title> <type> Tech. rep., </type> <institution> PET Center, Aarhus University Hospital, </institution> <address> DK 8000 Aarhus, Denmark, </address> <year> 1998. </year>
Reference-contexts: In the PET 6 Center, Aarhus University Hospital, DK 8000 Aarhus, Denmark, a project has been carried out, where this problem has been studied (see <ref> [20] </ref>).
Reference: [21] <author> Prechelt, L. </author> <title> PROBEN1 | A set of benchmarks and benchmarking rules for neural network training algorithms. </title> <type> Tech. Rep. 21/94, </type> <institution> Fakultat fur Informatik, Universitat Karlsruhe, D-76128 Karlsruhe, Germany, </institution> <month> Sept. </month> <year> 1994. </year> <note> Anonymous FTP: /pub/papers/techreports/1994/1994-21.ps.Z on ftp.ira.uka.de. </note>
Reference-contexts: Below is a description of the six sets of examples. All example sets are divided into a test, validation, and training set of relative size 1 4 ; 1 2 . * Building: An example set from the PROBEN1 <ref> [21] </ref> data base. The set contains 4208 examples. * Brain I: In neurology it is a problem of great importance to coregistrate 5 two brain scan pictures exactly.
Reference: [22] <author> Quinlan, J. R. </author> <title> C4. 5: Programs for Machine Learning. </title> <publisher> Morgan Kauf-mann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: A number of such machine learning methods have been developed, e.g. k-nearest neighbor and Kernel functions (see e.g. [1], [10], or [11]), Neural Networks [7], CART [4], C4.5 <ref> [22] </ref>, and support vectors [6]. Several of these are universal machine learning methods, since they can approximate a function arbitrarily well, but it is often problematic to achieve a match. In the nineties, meta machine learning methods have been developed.
Reference: [23] <author> Xu, L., Jordan, M. I., and Hinton, G. E. </author> <title> An alternative model for mixtures of experts. </title> <booktitle> In Advances in Neural Information Processing Systems (1995), </booktitle> <editor> G. Tesauro, D. Touretzky, and T. Leen, Eds., </editor> <volume> vol. 7, </volume> <publisher> The MIT Press, </publisher> <pages> pp. 633-640. </pages>
Reference-contexts: Construct ensemble predictor: F (~x) = inf (y : k:f k (~x)y 1 X log (fi k )) (2) 3.4 Mixture of Experts The term Mixtures of Experts covers a variety of methods. We will present the extension of 'Hierarchical Mixtures of Experts' [15] that can be found in <ref> [23] </ref>. We call the method XuME after the author and use the notation from the article.

References-found: 23

