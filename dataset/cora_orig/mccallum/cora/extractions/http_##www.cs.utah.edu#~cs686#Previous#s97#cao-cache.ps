URL: http://www.cs.utah.edu/~cs686/Previous/s97/cao-cache.ps
Refering-URL: http://www.cs.utah.edu/~cs686/Previous/s97/
Root-URL: 
Title: Maintaining Strong Cache Consistency in the World-Wide Web  
Author: Chengjie Liu and Pei Cao 
Address: Madison, WI 53706  
Affiliation: Department of Computer Science University of Wisconsin-Madison  
Abstract: As the Web continues to explode in size, caching becomes increasingly important. With caching comes the problem of cache consistency. Conventional wisdom holds that strong cache consistency is too expensive for the Web, and weak consistency methods such as Time-To-Live (TTL) are most appropriate. This study compares three consistency approaches: adaptive TTL, polling-every-time and invalidation, using prototype implementation and trace replay in a simulated environment. Our results show that invalidation generates less or a comparable amount of network traffic and server workload than adaptive TTL and has a slightly lower average client response time, while polling-every-time generates more network traffic and longer client response times. We show that, contrary to popular belief, strong cache consistency can be maintained for the Web with little or no extra cost than the current weak consistency approaches, and it should be maintained using an invalidation-based protocol. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Thomas E. Anderson, Michael D. Dahlin, Jeanna M. Neefe, David A. Patterson, Drew S. Roselli, and Randolph Y. Wang. </author> <title> Serverless network file systems. </title> <booktitle> In Proceedings of 15th ACM Symposium on Operating Systems Principles, </booktitle> <month> De-cember </month> <year> 1995. </year>
Reference-contexts: Many recent studies on distributed file systems fol lowed the trend of letting client workstations assume more responsibilities, including caching, consistency maintanence and failure resilience <ref> [4, 1] </ref>. These techniques do not easily apply to the current Web because most web clients have limited resources.
Reference: [2] <author> Mary Baker, John H. Hartman, Michael D. Kupfer, Ken W. Shirriff, and John Ousterhout. </author> <title> Measurements of a distributed file system. </title> <booktitle> In Proceedings of 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 198-211, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: The adaptive TTL approach handles the problem by adjusting a document's time-to-live based on observations of its lifetime. The approach, also called the Alex protocol, was first proposed in [5]. Adaptive TTL takes advantage of the fact that file lifetime distributions tend to be bimodal <ref> [4, 2] </ref>; if a file has not been modified for a long time, it tends to stay unchanged.
Reference: [3] <author> A. Bestavros. </author> <title> Demand-based resource allocation to reduce traffic and balance load in distributed information systems. </title> <booktitle> In Proceedings of the 1995 IEEE Symposium on Parallel and Distributed Processing, </booktitle> <month> October </month> <year> 1995. </year>
Reference-contexts: The average file life times are chosen to sample the life time range of 2 to 50 days. (Studies <ref> [3, 8] </ref> report that the average file life time found on university servers is 50 days.) Looking at the number of cache hits , we see that the three approaches have fairly similar numbers in most experiments, except for SASK.
Reference: [4] <author> Matthew A. </author> <title> Blaze. Caching in Large-Scale Distributed File Systems. </title> <type> PhD thesis, </type> <institution> Princeton University, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: Many recent studies on distributed file systems fol lowed the trend of letting client workstations assume more responsibilities, including caching, consistency maintanence and failure resilience <ref> [4, 1] </ref>. These techniques do not easily apply to the current Web because most web clients have limited resources. <p> The adaptive TTL approach handles the problem by adjusting a document's time-to-live based on observations of its lifetime. The approach, also called the Alex protocol, was first proposed in [5]. Adaptive TTL takes advantage of the fact that file lifetime distributions tend to be bimodal <ref> [4, 2] </ref>; if a file has not been modified for a long time, it tends to stay unchanged.
Reference: [5] <author> V. Cate. </author> <title> Alex a global file system. </title> <booktitle> In Proceedings of the 1992 USENIX File System Workshop, </booktitle> <pages> pages 1-12, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: We implemented invalidation and polling-every-time in the popular web caching system Harvest [6], and compared their performance with the existing weak consistency approach in Harvest, adaptive TTL (also called the Alex protocol <ref> [5] </ref>). Our method is to replay Web server traces through the prototypes running on workstations connected by an Ethernet. <p> Similar difficulties exist with the client polling approach in deciding when to send "if-modified-since" requests. The adaptive TTL approach handles the problem by adjusting a document's time-to-live based on observations of its lifetime. The approach, also called the Alex protocol, was first proposed in <ref> [5] </ref>. Adaptive TTL takes advantage of the fact that file lifetime distributions tend to be bimodal [4, 2]; if a file has not been modified for a long time, it tends to stay unchanged. <p> Studies <ref> [5, 8] </ref> have shown that adaptive TTL can keep the probability of stale documents within reasonable bounds (&lt; 5%). The Harvest cache manager [6] mainly uses this approach to maintain cache consistency, with the percentage set to 50% 1 .
Reference: [6] <author> A. Chankhunthod, P. Danzig, C. Neerdaels, M. Schwartz, and K. Worrell. </author> <title> A hierarchical internet object cache. </title> <booktitle> In Proceedings of the 1996 USENIX Technical Conference, </booktitle> <address> San Diego, CA, </address> <month> January </month> <year> 1996. </year>
Reference-contexts: In addition, depending on the relative frequency of accesses and modifications, polling-every-time may perform better than invalidation. To answer these questions, we felt that the best way is to implement these approaches and conduct experiments. We implemented invalidation and polling-every-time in the popular web caching system Harvest <ref> [6] </ref>, and compared their performance with the existing weak consistency approach in Harvest, adaptive TTL (also called the Alex protocol [5]). Our method is to replay Web server traces through the prototypes running on workstations connected by an Ethernet. <p> Another study [16] describes a light-weight caching server that employs both adaptive TTL and invalidation for cache consistency. However, the paper focuses on comparing the performance differences between the light-weight server and the CERN proxy server, and does not compare the consistency approaches. Though we chose the Harvest <ref> [7, 6] </ref> system for our implementations, there are many other Web caching software. Popular browsers such as Netscape and the Internet Explorer all offer some form of client caching. CERN proxy [13] offers caching for all its clients, using TTL as the consistency mechanism. <p> Studies [5, 8] have shown that adaptive TTL can keep the probability of stale documents within reasonable bounds (&lt; 5%). The Harvest cache manager <ref> [6] </ref> mainly uses this approach to maintain cache consistency, with the percentage set to 50% 1 . Though adaptive TTL keeps the frequency of stale documents low, it does not eliminate its occurance. Two other approaches can provide stronger consistency guarantees.
Reference: [7] <author> P. B. Danzig, R. S. Hall, and M. F. Schwartz. </author> <title> A case for caching file objects inside internetworks. </title> <booktitle> In Proceedings of SIGCOMM '93, </booktitle> <pages> pages 239-248, </pages> <year> 1993. </year>
Reference-contexts: In this approach, a write is complete when the modifica-tion is registered in the server's file system. Part of the motivation of this study is to determine whether strong consistency can be maintained in a large scale wide-area system such as the Web. Previous studies <ref> [7, 8] </ref> concluded that both invalidation and polling-every-time are much more expensive than weak consistency approaches. Our analysis in Section 3.2, however, shows that this is not necessarily the case, that weak consistency saves network load over strong consistency mainly at the expense of stale documents. <p> Another study [16] describes a light-weight caching server that employs both adaptive TTL and invalidation for cache consistency. However, the paper focuses on comparing the performance differences between the light-weight server and the CERN proxy server, and does not compare the consistency approaches. Though we chose the Harvest <ref> [7, 6] </ref> system for our implementations, there are many other Web caching software. Popular browsers such as Netscape and the Internet Explorer all offer some form of client caching. CERN proxy [13] offers caching for all its clients, using TTL as the consistency mechanism.
Reference: [8] <author> James Gwertzman and Margo Seltzer. </author> <title> Worldwide web cache consistency. </title> <booktitle> In Proceedings of the 1996 USENIX Technical Conference, </booktitle> <address> San Diego, CA, </address> <month> January </month> <year> 1996. </year>
Reference-contexts: In this approach, a write is complete when the modifica-tion is registered in the server's file system. Part of the motivation of this study is to determine whether strong consistency can be maintained in a large scale wide-area system such as the Web. Previous studies <ref> [7, 8] </ref> concluded that both invalidation and polling-every-time are much more expensive than weak consistency approaches. Our analysis in Section 3.2, however, shows that this is not necessarily the case, that weak consistency saves network load over strong consistency mainly at the expense of stale documents. <p> In particular, Gwertzman and Seltzer's paper <ref> [8] </ref> gave an excellent comparison of cache consistency approaches via simulation, and concluded that a weak-consistency approach such as adaptive TTL would be best for Web caching. The main metric used in [8] is network traffic. <p> In particular, Gwertzman and Seltzer's paper <ref> [8] </ref> gave an excellent comparison of cache consistency approaches via simulation, and concluded that a weak-consistency approach such as adaptive TTL would be best for Web caching. The main metric used in [8] is network traffic. The study did not address many other important questions, such as server loads, client response times, and consistency message latency. Another study that is similar to ours is Worrell's thesis [17]. The study investigates using invalidation as the consistency approach in hierarchical network object caches. <p> Studies <ref> [5, 8] </ref> have shown that adaptive TTL can keep the probability of stale documents within reasonable bounds (&lt; 5%). The Harvest cache manager [6] mainly uses this approach to maintain cache consistency, with the percentage set to 50% 1 . <p> The average file life times are chosen to sample the life time range of 2 to 50 days. (Studies <ref> [3, 8] </ref> report that the average file life time found on university servers is 50 days.) Looking at the number of cache hits , we see that the three approaches have fairly similar numbers in most experiments, except for SASK.
Reference: [9] <author> John H. Howard, Michael Kazar, Sherri G. Me-nees, David A. Nichols, M. Satyanarayanan, Robert N. Sidebotham, and Michael J. West. </author> <title> Scale and performance in a distributed file system. </title> <journal> ACM Transactions on Computer Systems, </journal> <pages> pages 6(1) 51-81, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: CERN proxy [13] offers caching for all its clients, using TTL as the consistency mechanism. We choose Harvest due to its source code availability and good performance [16]. Finally, the Web cache consistency problem is the same cache consistency problem in network and distributed file systems <ref> [15, 9, 14] </ref>. However, the contexts are quite different. The Web is orders of magnitudes bigger than any distributed file system. The systems participating in the Web are heterogeneous and use different operating systems. The Web currently restricts most documents to single-writer/multiple-reader, while most distributed file systems support concurrent write-sharing. <p> In particular, the TTL approach is similar to the NFS protocol for cache consistency [15], the polling-every-time approach is similar to what is adopted in the Sprite file systems (clients contact the server on every file open/close) [14], and the invalidation approach is similar to the callbacks in AFS <ref> [9] </ref>. Many recent studies on distributed file systems fol lowed the trend of letting client workstations assume more responsibilities, including caching, consistency maintanence and failure resilience [4, 1]. These techniques do not easily apply to the current Web because most web clients have limited resources.
Reference: [10] <author> Stefanos Kaxiras and James Goodman. </author> <title> Implementation and performance of the GLOW kilo-processor extensions to sci on the wisconsin wind tunnel. </title> <booktitle> In Proceedings of the 2nd International Workshop on SCI-Based High-Performance Low-Cost Computing, </booktitle> <month> March </month> <year> 1995. </year>
Reference-contexts: These techniques do not easily apply to the current Web because most web clients have limited resources. Our initial interest in this work came from discussions with Stefanos Kaxiras and James Goodman at University of Wisconsin at Madison on applying the GLOW scalable hardware shared-memory coherence protocol <ref> [10] </ref> to cache consistency on the Web. GLOW uses hierarchical caching and hardware multicast network for invalidation messages.
Reference: [11] <author> Paul Leach and Jeff Mogul. </author> <title> The HTTP hit-metering protocol. </title> <type> Technical report, </type> <institution> ftp://ieft.org/internet-draft/draft-mogul-http-hit-metering-00.txt, </institution> <month> November </month> <year> 1996. </year>
Reference-contexts: What we have shown is that, for those that do care, invalidation is feasible, and it is a better approach than TTL or client polling. For those commercial Web sites that want to control the accesses to its contents, invalidation should be merged with other hit-metering protocols <ref> [11] </ref> to pro-vide both the benefits of caching and the access control.
Reference: [12] <author> Chengjie Liu and Pei Cao. </author> <title> Maintaining strong cache consistency for the world-wide web. </title> <type> Technical report, Technical Report, </type> <institution> Univ. of Wiscon-sin, Department of Computer Science, CS-TR-96-1318, </institution> <month> October </month> <year> 1996. </year>
Reference-contexts: Due to space limitations, we present a subset of our experiments; for a more complete set of results see <ref> [12] </ref>. Shown in the table are the replays of EPA with average file life time of 50 days, SASK with average file life time of 14 days, ClarkNet with 50 days, NASA with 7 days, and SDSC with 2.5 days and 25 days.
Reference: [13] <author> A. Luotonen, H. Frystyk, and T. Berners-Lee. </author> <title> CERN HTTPD public domain full-featured hypertext/proxy server with caching. </title> <type> Technical report, </type> <note> Available from http://www.w3.org/hypertext/WWW/Daemon/Status.html, 1994. </note>
Reference-contexts: Though we chose the Harvest [7, 6] system for our implementations, there are many other Web caching software. Popular browsers such as Netscape and the Internet Explorer all offer some form of client caching. CERN proxy <ref> [13] </ref> offers caching for all its clients, using TTL as the consistency mechanism. We choose Harvest due to its source code availability and good performance [16]. Finally, the Web cache consistency problem is the same cache consistency problem in network and distributed file systems [15, 9, 14].
Reference: [14] <author> Michael N. Nelson, Brent B. Welch, and John K. Ousterhout. </author> <title> Caching in the Sprite file system. </title> <journal> ACM Transactions on Computer Systems, </journal> <pages> pages 6(1) 134-154, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: CERN proxy [13] offers caching for all its clients, using TTL as the consistency mechanism. We choose Harvest due to its source code availability and good performance [16]. Finally, the Web cache consistency problem is the same cache consistency problem in network and distributed file systems <ref> [15, 9, 14] </ref>. However, the contexts are quite different. The Web is orders of magnitudes bigger than any distributed file system. The systems participating in the Web are heterogeneous and use different operating systems. The Web currently restricts most documents to single-writer/multiple-reader, while most distributed file systems support concurrent write-sharing. <p> Despite the differences, there are similarities in the solutions. In particular, the TTL approach is similar to the NFS protocol for cache consistency [15], the polling-every-time approach is similar to what is adopted in the Sprite file systems (clients contact the server on every file open/close) <ref> [14] </ref>, and the invalidation approach is similar to the callbacks in AFS [9]. Many recent studies on distributed file systems fol lowed the trend of letting client workstations assume more responsibilities, including caching, consistency maintanence and failure resilience [4, 1].
Reference: [15] <author> R. Sandberg, D. Boldberg, S. Kleiman, D. Walsh, and B. Lyon. </author> <title> Design and implementation of the Sun network filesystem. </title> <booktitle> In Summer Usenix Conference Proceedings, </booktitle> <pages> pages 119-130, </pages> <month> June </month> <year> 1985. </year>
Reference-contexts: CERN proxy [13] offers caching for all its clients, using TTL as the consistency mechanism. We choose Harvest due to its source code availability and good performance [16]. Finally, the Web cache consistency problem is the same cache consistency problem in network and distributed file systems <ref> [15, 9, 14] </ref>. However, the contexts are quite different. The Web is orders of magnitudes bigger than any distributed file system. The systems participating in the Web are heterogeneous and use different operating systems. The Web currently restricts most documents to single-writer/multiple-reader, while most distributed file systems support concurrent write-sharing. <p> The Web currently restricts most documents to single-writer/multiple-reader, while most distributed file systems support concurrent write-sharing. Despite the differences, there are similarities in the solutions. In particular, the TTL approach is similar to the NFS protocol for cache consistency <ref> [15] </ref>, the polling-every-time approach is similar to what is adopted in the Sprite file systems (clients contact the server on every file open/close) [14], and the invalidation approach is similar to the callbacks in AFS [9].
Reference: [16] <author> D. Wessels. </author> <title> Intelligent caching for the world-wide web objects. </title> <booktitle> In Proceedings of INET-95, </booktitle> <year> 1995. </year>
Reference-contexts: However, the results in [17] relies on the existence of a hierarchical caching structure, which significantly reduces the overhead for invalidation. Unfortunately, hierarchical caches are not yet widely present in the Internet. Thus, we focus on invalidation in the absence of caching hierarchies. Another study <ref> [16] </ref> describes a light-weight caching server that employs both adaptive TTL and invalidation for cache consistency. However, the paper focuses on comparing the performance differences between the light-weight server and the CERN proxy server, and does not compare the consistency approaches. <p> Popular browsers such as Netscape and the Internet Explorer all offer some form of client caching. CERN proxy [13] offers caching for all its clients, using TTL as the consistency mechanism. We choose Harvest due to its source code availability and good performance <ref> [16] </ref>. Finally, the Web cache consistency problem is the same cache consistency problem in network and distributed file systems [15, 9, 14]. However, the contexts are quite different. The Web is orders of magnitudes bigger than any distributed file system.
Reference: [17] <author> K. Worrell. </author> <title> Invalidation in large scale network object caches. </title> <type> Technical report, Master's Thesis, </type> <institution> University of Colorado, Boulder, </institution> <year> 1994, 1994. </year>
Reference-contexts: The main metric used in [8] is network traffic. The study did not address many other important questions, such as server loads, client response times, and consistency message latency. Another study that is similar to ours is Worrell's thesis <ref> [17] </ref>. The study investigates using invalidation as the consistency approach in hierarchical network object caches. It compares invalidation with a fixed TTL approach, in which a single time-to-live is assigned to all files. The study concludes that invalidation is a better approach for cache consistency. However, the results in [17] relies <p> thesis <ref> [17] </ref>. The study investigates using invalidation as the consistency approach in hierarchical network object caches. It compares invalidation with a fixed TTL approach, in which a single time-to-live is assigned to all files. The study concludes that invalidation is a better approach for cache consistency. However, the results in [17] relies on the existence of a hierarchical caching structure, which significantly reduces the overhead for invalidation. Unfortunately, hierarchical caches are not yet widely present in the Internet. Thus, we focus on invalidation in the absence of caching hierarchies.
References-found: 17

