URL: ftp://ftp.cs.dartmouth.edu/pub/kotz/papers/kotz:workload.ps.Z
Refering-URL: http://www.cs.dartmouth.edu/~dfk/papers/workload.html
Root-URL: http://www.cs.dartmouth.edu
Email: fdfk,nilsg@cs.dartmouth.edu  
Title: Dynamic File-Access Characteristics of a Production Parallel Scientific Workload  
Author: David Kotz Nils Nieuwejaar 
Address: College, Hanover, NH 03755-3510  
Affiliation: Department of Computer Science Dartmouth  
Web: URL ftp://ftp.cs.dartmouth.edu/pub/CS-papers/Kotz/kotz:workload.ps.Z  
Note: Copyright 1994 by IEEE. Appeared in Supercomputing '94, pages 640-649. Available at  
Abstract: Most successful systems are based on a solid understanding of the expected workload, but thus far there have been no comprehensive workload characterizations of multiprocessor file systems. This paper presents the results of a three week tracing study in which all file-related activity on a massively parallel computer was recorded. Our instrumentation differs from previous efforts in that it collects information about every I/O request and about the mix of jobs running in a production environment. We also present the results of a trace-driven caching simulation and recommendations for designers of multiprocessor file systems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. G. Baker, J. H. Hartman, M. D. Kupfer, K. W. Shirriff, and J. K. Ousterhout. </author> <title> Measurements of a distributed file system. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 198-212, </pages> <year> 1991. </year>
Reference-contexts: General-purpose workstations. Uniprocessor file access patterns have been measured many times. Floyd and Ellis [12, 13] and Ousterhout et al. [28] measured isolated Unix workstations, and Baker et al. measured a distributed Unix (Sprite) system <ref> [1] </ref>. All of these studies cover general-purpose (engineering and office) workloads with uniprocessor applications. Scientific vector applications. Some studies specifically examined scientific workloads. Del Rosario and Choudhary provide an informal characterization of grand-challenge applications [10]. Powell measured file sizes on a Cray-1 file system [31]. <p> Although these files were larger than those in a general-purpose file system <ref> [1] </ref>, they were smaller than we would expect to see in a scientific supercomputing environment [25]. <p> believe that the preponderance of small request sizes is the natural result of parallelization by distributing file data across many processors, and would be found in other work-loads using a similar file-system interface. 4.4 Sequentiality A common characteristic of file workloads, particularly scientific workloads, is that files are accessed sequentially <ref> [28, 1, 25] </ref>. <p> It is concurrently shared if the opens overlap in time. It is write-shared if one of the opens involves writing the file. In uniprocessor and distributed-system workloads, concurrent sharing is known to be uncommon, and concurrent write sharing rare <ref> [1] </ref>. In a parallel file system, of course, concurrent file sharing among processes within a job is presumably the norm, while concurrent file sharing between jobs is likely to be rare.
Reference: [2] <author> M. L. Best, A. Greenberg, C. Stanfill, and L. W. Tucker. </author> <title> CMMD I/O: A parallel Unix I/O. </title> <booktitle> In Proceedings of the Seventh International Parallel Processing Symposium, </booktitle> <pages> pages 489-495, </pages> <year> 1993. </year>
Reference-contexts: Most extend a traditional file abstraction (a growable, addressable sequence of bytes) with some parallel file-access methods. The most common provide I/O "modes" that specify whether and how parallel processes share a file pointer <ref> [7, 30, 33, 2, 17] </ref>. Some are based on a memory-mapped interface [23, 22]. Some provide a way for the user to specify per-process logical views of the file [5, 9]. Some provide SIMD-style transfers [34, 24, 16].
Reference: [3] <author> R. Bordawekar, A. Choudhary, and J. M. D. Rosario. </author> <title> An experimental performance evaluation of Touchstone Delta Concurrent File System. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <pages> pages 367-376, </pages> <year> 1993. </year>
Reference-contexts: Recent studies have found that CFS caching and prefetching work well in limited situations, but that the throughput of CFS can be disappointing relative to the capabilities of the hardware <ref> [27, 3] </ref>. Miller and Katz drove a cache simulation using traces from a Cray supercomputer and found that access locality was not high enough for significant benefits to be realized from a file system cache [25]. 2.4 Intel iPSC/860 and CFS The iPSC/860 is a distributed-memory, message-passing, MIMD machine.
Reference: [4] <author> R. Carter, B. Ciotti, S. Fineberg, and B. Nitzberg. </author> <title> NHT-1 I/O benchmarks. </title> <type> Technical Report RND-92-016, </type> <institution> NAS Systems Division, NASA Ames, </institution> <month> Nov. </month> <year> 1992. </year>
Reference-contexts: Simple benchmarking of the instrumented library revealed that the overhead added by our instrumentation was virtually undetectable in many cases. The worst case we found was a 7% increase in execution time on one run of the NAS NHT-1 Application-I/O Benchmark <ref> [4] </ref>. After the instrumented library was put into production use, anecdotal evidence suggests that there was no noticeable performance loss. 3.2 Analysis The raw trace files required some simple postpro-cessing before they could be easily analyzed. This postprocessing included data realignment, clock synchronization, and chronological sorting.
Reference: [5] <author> P. F. Corbett, D. G. Feitelson, J.-P. Prost, and S. J. Baylor. </author> <title> Parallel access to files in the Vesta file system. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 472-481, </pages> <year> 1993. </year>
Reference-contexts: The most common provide I/O "modes" that specify whether and how parallel processes share a file pointer [7, 30, 33, 2, 17]. Some are based on a memory-mapped interface [23, 22]. Some provide a way for the user to specify per-process logical views of the file <ref> [5, 9] </ref>. Some provide SIMD-style transfers [34, 24, 16]. PIFS (Bridge) [11] allows the file system to control which processor handles which parts of the file, to encourage memory locality. Clearly, the industrial and research communities have not yet settled on a single new model for file access. <p> A strided request can express a regular request and interval size (which were common in our workload), effectively increasing the request size, lowering overhead, and perhaps eliminating the need for compute-node buffers. Strided requests are available in some file-system interfaces <ref> [5, 9, 17] </ref>. For some applications, collective I/O requests can lead to even better performance [18]. Dependence on Intel CFS. We caution that some of our results may be specific to workloads on Intel CFS file systems, or to NASA Ames's workload (computational fluid dynamics).
Reference: [6] <author> T. H. Cormen and D. Kotz. </author> <title> Integrating theory and practice in parallel file systems. </title> <booktitle> In Proceedings of the 1993 DAGS/PC Symposium, </booktitle> <pages> pages 64-74, </pages> <address> Hanover, NH, </address> <month> June </month> <year> 1993. </year> <institution> Dartmouth Institute for Advanced Graduate Studies. </institution> <note> Revised from Dartmouth PCS-TR93-188. </note>
Reference-contexts: Pasquale and Polyzos studied I/O-intensive Cray applications, focusing on patterns in the I/O rate [29]. All of these studies are limited to uniprocess applications on vector supercomputers. Scientific parallel applications. Crockett [7] and Kotz [20] hypothesize about the character of a parallel scientific file-system workload. Cormen and Kotz <ref> [6] </ref> discuss the needs of parallel-I/O algorithms. Reddy et al. chose five sequential scientific applications from the PERFECT benchmarks and parallelized them for an eight-processor Alliant, finding only sequential file-access patterns [32].
Reference: [7] <author> T. W. Crockett. </author> <title> File concepts for parallel I/O. </title> <booktitle> In Proceedings of Supercomputing '89, </booktitle> <pages> pages 574-579, </pages> <year> 1989. </year>
Reference-contexts: Pasquale and Polyzos studied I/O-intensive Cray applications, focusing on patterns in the I/O rate [29]. All of these studies are limited to uniprocess applications on vector supercomputers. Scientific parallel applications. Crockett <ref> [7] </ref> and Kotz [20] hypothesize about the character of a parallel scientific file-system workload. Cormen and Kotz [6] discuss the needs of parallel-I/O algorithms. Reddy et al. chose five sequential scientific applications from the PERFECT benchmarks and parallelized them for an eight-processor Alliant, finding only sequential file-access patterns [32]. <p> Most extend a traditional file abstraction (a growable, addressable sequence of bytes) with some parallel file-access methods. The most common provide I/O "modes" that specify whether and how parallel processes share a file pointer <ref> [7, 30, 33, 2, 17] </ref>. Some are based on a memory-mapped interface [23, 22]. Some provide a way for the user to specify per-process logical views of the file [5, 9]. Some provide SIMD-style transfers [34, 24, 16].
Reference: [8] <author> R. Cypher, A. Ho, S. Konstantinidou, and P. Messina. </author> <title> Architectural requirements of parallel scientific applications with explicit communication. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-13, </pages> <year> 1993. </year>
Reference-contexts: This study is interesting, but far from what we need: the sample size is small; the programs are parallelized sequential programs, not parallel programs per se; and the I/O itself was not par-allelized. Cypher et al. <ref> [8] </ref> studied individual parallel scientific applications, measuring temporal patterns in I/O rates. Galbreath et al. [16] present a useful high-level characterization based on anecdotal evidence. 2.2 Existing file systems To increase parallelism, all large multiprocessor file systems decluster blocks of a file across many disks, which are accessed in parallel.
Reference: [9] <author> E. DeBenedictis and J. M. del Rosario. </author> <title> nCUBE parallel I/O software. </title> <booktitle> In Eleventh Annual IEEE International Phoenix Conference on Computers and Communications (IPCCC), </booktitle> <pages> pages 0117-0124, </pages> <month> Apr. </month> <year> 1992. </year>
Reference-contexts: The most common provide I/O "modes" that specify whether and how parallel processes share a file pointer [7, 30, 33, 2, 17]. Some are based on a memory-mapped interface [23, 22]. Some provide a way for the user to specify per-process logical views of the file <ref> [5, 9] </ref>. Some provide SIMD-style transfers [34, 24, 16]. PIFS (Bridge) [11] allows the file system to control which processor handles which parts of the file, to encourage memory locality. Clearly, the industrial and research communities have not yet settled on a single new model for file access. <p> A strided request can express a regular request and interval size (which were common in our workload), effectively increasing the request size, lowering overhead, and perhaps eliminating the need for compute-node buffers. Strided requests are available in some file-system interfaces <ref> [5, 9, 17] </ref>. For some applications, collective I/O requests can lead to even better performance [18]. Dependence on Intel CFS. We caution that some of our results may be specific to workloads on Intel CFS file systems, or to NASA Ames's workload (computational fluid dynamics).
Reference: [10] <author> J. M. del Rosario and A. Choudhary. </author> <title> High performance I/O for parallel computers: Problems and prospects. </title> <journal> IEEE Computer, </journal> <volume> 27(3) </volume> <pages> 59-68, </pages> <month> Mar. </month> <year> 1994. </year>
Reference-contexts: All of these studies cover general-purpose (engineering and office) workloads with uniprocessor applications. Scientific vector applications. Some studies specifically examined scientific workloads. Del Rosario and Choudhary provide an informal characterization of grand-challenge applications <ref> [10] </ref>. Powell measured file sizes on a Cray-1 file system [31]. Miller and Katz traced specific I/O-intensive Cray applications to determine the per-file access patterns [25], focusing primarily on access rates. Pasquale and Polyzos studied I/O-intensive Cray applications, focusing on patterns in the I/O rate [29].
Reference: [11] <author> P. C. Dibble. </author> <title> A Parallel Interleaved File System. </title> <type> PhD thesis, </type> <institution> University of Rochester, </institution> <month> Mar. </month> <year> 1990. </year>
Reference-contexts: Some are based on a memory-mapped interface [23, 22]. Some provide a way for the user to specify per-process logical views of the file [5, 9]. Some provide SIMD-style transfers [34, 24, 16]. PIFS (Bridge) <ref> [11] </ref> allows the file system to control which processor handles which parts of the file, to encourage memory locality. Clearly, the industrial and research communities have not yet settled on a single new model for file access.
Reference: [12] <author> R. Floyd. </author> <title> Short-term file reference patterns in a UNIX environment. </title> <type> Technical Report 177, </type> <institution> Dept. of Computer Science, Univ. of Rochester, </institution> <month> Mar. </month> <year> 1986. </year>
Reference-contexts: Related file-system workload studies can be classified as characterizing general-purpose workstations (or workstation networks), scientific vector applications, or scientific parallel applications. General-purpose workstations. Uniprocessor file access patterns have been measured many times. Floyd and Ellis <ref> [12, 13] </ref> and Ousterhout et al. [28] measured isolated Unix workstations, and Baker et al. measured a distributed Unix (Sprite) system [1]. All of these studies cover general-purpose (engineering and office) workloads with uniprocessor applications. Scientific vector applications. Some studies specifically examined scientific workloads. <p> There were very few (less than 2300) files that were read and written in the same open. This behavior is also common in Unix file systems <ref> [12] </ref> and may be accentuated here by the difficulty in coordinating concurrent reads and writes to the same file (note the CFS file-access modes are of little help for read-write access). Finally, there were nearly 2500 files which were opened but neither read nor written.
Reference: [13] <author> R. A. Floyd and C. S. Ellis. </author> <title> Directory reference patterns in hierarchical file systems. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 1(2) </volume> <pages> 238-247, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: Related file-system workload studies can be classified as characterizing general-purpose workstations (or workstation networks), scientific vector applications, or scientific parallel applications. General-purpose workstations. Uniprocessor file access patterns have been measured many times. Floyd and Ellis <ref> [12, 13] </ref> and Ousterhout et al. [28] measured isolated Unix workstations, and Baker et al. measured a distributed Unix (Sprite) system [1]. All of these studies cover general-purpose (engineering and office) workloads with uniprocessor applications. Scientific vector applications. Some studies specifically examined scientific workloads.
Reference: [14] <author> J. C. </author> <title> French. A global time reference for hypercube multiprocessors. </title> <booktitle> In Fourth Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 217-220, </pages> <year> 1989. </year>
Reference-contexts: Ordering the records was complicated by the lack of synchronized clocks on the iPSC/860. Each node maintains its own clock; the clocks are synchronized at system startup but each drifts significantly and differently after that <ref> [14] </ref>. We partially compensated for the asynchrony by timestamping each block of records when it left the node and again when it was received at the data collector.
Reference: [15] <author> J. C. French, T. W. Pratt, and M. Das. </author> <title> Perfor--mance measurement of the Concurrent File System of the Intel iPSC/2 hypercube. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> 17(1-2):115-121, January and February 1993. </note>
Reference-contexts: The implications of this fact for our study are discussed in Section 5. 2.3 Multiprocessor file system caching Caching and prefetching are successful in multiprocessor file systems [19, 20]. Pratt and French found that the caching and prefetching supplied with In-tel's Concurrent File System (CFS) does improve performance <ref> [15] </ref>. Recent studies have found that CFS caching and prefetching work well in limited situations, but that the throughput of CFS can be disappointing relative to the capabilities of the hardware [27, 3]. <p> The I/O nodes are based on the Intel i386 processor and each has a port for SCSI disk drives. There may also be one or more service nodes that handle as Ethernet connections or interactive shells [26]. Intel's Concurrent File System (CFS) <ref> [30, 15, 27] </ref> provides a Unix-like interface to the user with the addition of four I/O modes to help the programmer coordinate parallel access to files.
Reference: [16] <author> N. Galbreath, W. Gropp, and D. Levine. </author> <title> Applications-driven parallel I/O. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 462-471, </pages> <year> 1993. </year>
Reference-contexts: Cypher et al. [8] studied individual parallel scientific applications, measuring temporal patterns in I/O rates. Galbreath et al. <ref> [16] </ref> present a useful high-level characterization based on anecdotal evidence. 2.2 Existing file systems To increase parallelism, all large multiprocessor file systems decluster blocks of a file across many disks, which are accessed in parallel. <p> Some are based on a memory-mapped interface [23, 22]. Some provide a way for the user to specify per-process logical views of the file [5, 9]. Some provide SIMD-style transfers <ref> [34, 24, 16] </ref>. PIFS (Bridge) [11] allows the file system to control which processor handles which parts of the file, to encourage memory locality. Clearly, the industrial and research communities have not yet settled on a single new model for file access.
Reference: [17] <author> D. Kotz. </author> <title> Multiprocessor file system interfaces. </title> <booktitle> In Proceedings of the Second International Conference on Parallel and Distributed Information Systems, </booktitle> <pages> pages 194-201, </pages> <year> 1993. </year>
Reference-contexts: Most extend a traditional file abstraction (a growable, addressable sequence of bytes) with some parallel file-access methods. The most common provide I/O "modes" that specify whether and how parallel processes share a file pointer <ref> [7, 30, 33, 2, 17] </ref>. Some are based on a memory-mapped interface [23, 22]. Some provide a way for the user to specify per-process logical views of the file [5, 9]. Some provide SIMD-style transfers [34, 24, 16]. <p> A strided request can express a regular request and interval size (which were common in our workload), effectively increasing the request size, lowering overhead, and perhaps eliminating the need for compute-node buffers. Strided requests are available in some file-system interfaces <ref> [5, 9, 17] </ref>. For some applications, collective I/O requests can lead to even better performance [18]. Dependence on Intel CFS. We caution that some of our results may be specific to workloads on Intel CFS file systems, or to NASA Ames's workload (computational fluid dynamics).
Reference: [18] <author> D. Kotz. </author> <title> Disk-directed I/O for MIMD multiprocessors. </title> <type> Technical Report PCS-TR94-226, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: Strided requests are available in some file-system interfaces [5, 9, 17]. For some applications, collective I/O requests can lead to even better performance <ref> [18] </ref>. Dependence on Intel CFS. We caution that some of our results may be specific to workloads on Intel CFS file systems, or to NASA Ames's workload (computational fluid dynamics).
Reference: [19] <author> D. Kotz and C. S. Ellis. </author> <title> Caching and writeback policies in parallel file systems. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> 17(1-2):140-145, January and February 1993. </note>
Reference-contexts: Some aspects of the workload, therefore, are dependent on the particular file-access model provided to the user. The implications of this fact for our study are discussed in Section 5. 2.3 Multiprocessor file system caching Caching and prefetching are successful in multiprocessor file systems <ref> [19, 20] </ref>. Pratt and French found that the caching and prefetching supplied with In-tel's Concurrent File System (CFS) does improve performance [15]. <p> Replacement policies other than LRU or FIFO should be developed (e.g., <ref> [19] </ref>), to optimize for interprocess locality rather than traditional spatial and temporal locality. Ultimately, we believe that the file-system interface must change. The current interface forces the programmer to break down large parallel I/O activities into small, non-contiguous requests.
Reference: [20] <author> D. Kotz and C. S. Ellis. </author> <title> Practical prefetching techniques for multiprocessor file systems. </title> <journal> Journal of Distributed and Parallel Databases, </journal> <volume> 1(1) </volume> <pages> 33-51, </pages> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: Pasquale and Polyzos studied I/O-intensive Cray applications, focusing on patterns in the I/O rate [29]. All of these studies are limited to uniprocess applications on vector supercomputers. Scientific parallel applications. Crockett [7] and Kotz <ref> [20] </ref> hypothesize about the character of a parallel scientific file-system workload. Cormen and Kotz [6] discuss the needs of parallel-I/O algorithms. Reddy et al. chose five sequential scientific applications from the PERFECT benchmarks and parallelized them for an eight-processor Alliant, finding only sequential file-access patterns [32]. <p> Some aspects of the workload, therefore, are dependent on the particular file-access model provided to the user. The implications of this fact for our study are discussed in Section 5. 2.3 Multiprocessor file system caching Caching and prefetching are successful in multiprocessor file systems <ref> [19, 20] </ref>. Pratt and French found that the caching and prefetching supplied with In-tel's Concurrent File System (CFS) does improve performance [15].
Reference: [21] <author> D. Kotz and N. Nieuwejaar. </author> <title> Dynamic file-access characteristics of a production parallel scientific workload. </title> <type> Technical Report PCS-TR94-211, </type> <institution> Dept. of Math and Computer Science, Dartmouth College, </institution> <month> Apr. </month> <year> 1994. </year> <note> To appear in Supercomputing '94. Revised May 11, </note> <year> 1994. </year>
Reference-contexts: Since one of the goals of the CHARISMA project is to organize and facilitate a multi-platform file system tracing effort, we have defined a large set of event records suitable for both SIMD and MIMD systems <ref> [21] </ref>. On the iPSC/860, high-level CFS calls are implemented in a library that is linked with the user's program. We instrumented the library calls to generate an event record each time they were called. <p> We then examine individual I/O requests by looking for sequentiality, regularity, and sharing in the access pattern. Finally, we evaluate the effect on caching through trace-driven simulation. More detail may be found in <ref> [21] </ref>. 4.1 Jobs spent running a given number of jobs. For more than given number of jobs running. This data includes all jobs, even if their file access could not be traced. used by jobs in our workload (even those whose file access could not be traced).
Reference: [22] <author> O. Krieger and M. Stumm. </author> <title> HFS: a flexible file system for large-scale multiprocessors. </title> <booktitle> In Proceedings of the 1993 DAGS/PC Symposium, </booktitle> <pages> pages 6-14, </pages> <address> Hanover, NH, </address> <month> June </month> <year> 1993. </year> <institution> Dartmouth Institute for Advanced Graduate Studies. </institution>
Reference-contexts: Most extend a traditional file abstraction (a growable, addressable sequence of bytes) with some parallel file-access methods. The most common provide I/O "modes" that specify whether and how parallel processes share a file pointer [7, 30, 33, 2, 17]. Some are based on a memory-mapped interface <ref> [23, 22] </ref>. Some provide a way for the user to specify per-process logical views of the file [5, 9]. Some provide SIMD-style transfers [34, 24, 16]. PIFS (Bridge) [11] allows the file system to control which processor handles which parts of the file, to encourage memory locality.
Reference: [23] <institution> KSR1 technology background. Kendall Square Research, </institution> <month> Jan. </month> <year> 1992. </year>
Reference-contexts: Most extend a traditional file abstraction (a growable, addressable sequence of bytes) with some parallel file-access methods. The most common provide I/O "modes" that specify whether and how parallel processes share a file pointer [7, 30, 33, 2, 17]. Some are based on a memory-mapped interface <ref> [23, 22] </ref>. Some provide a way for the user to specify per-process logical views of the file [5, 9]. Some provide SIMD-style transfers [34, 24, 16]. PIFS (Bridge) [11] allows the file system to control which processor handles which parts of the file, to encourage memory locality.
Reference: [24] <institution> Parallel file I/O routines. MasPar Computer Corporation, </institution> <year> 1992. </year>
Reference-contexts: Some are based on a memory-mapped interface [23, 22]. Some provide a way for the user to specify per-process logical views of the file [5, 9]. Some provide SIMD-style transfers <ref> [34, 24, 16] </ref>. PIFS (Bridge) [11] allows the file system to control which processor handles which parts of the file, to encourage memory locality. Clearly, the industrial and research communities have not yet settled on a single new model for file access.
Reference: [25] <author> E. L. Miller and R. H. Katz. </author> <title> Input/output behavior of supercomputer applications. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <pages> pages 567-576, </pages> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: Scientific vector applications. Some studies specifically examined scientific workloads. Del Rosario and Choudhary provide an informal characterization of grand-challenge applications [10]. Powell measured file sizes on a Cray-1 file system [31]. Miller and Katz traced specific I/O-intensive Cray applications to determine the per-file access patterns <ref> [25] </ref>, focusing primarily on access rates. Pasquale and Polyzos studied I/O-intensive Cray applications, focusing on patterns in the I/O rate [29]. All of these studies are limited to uniprocess applications on vector supercomputers. Scientific parallel applications. <p> Miller and Katz drove a cache simulation using traces from a Cray supercomputer and found that access locality was not high enough for significant benefits to be realized from a file system cache <ref> [25] </ref>. 2.4 Intel iPSC/860 and CFS The iPSC/860 is a distributed-memory, message-passing, MIMD machine. The compute nodes are based on the Intel i860 processor and are connected by a hypercube network. <p> Although these files were larger than those in a general-purpose file system [1], they were smaller than we would expect to see in a scientific supercomputing environment <ref> [25] </ref>. We suspect that users limited their file sizes due to the small disk capacity (7.2 GB) and limited disk bandwidth (10 MB/s peak). 4.3 I/O request sizes small, but that most bytes are transferred through large reads. <p> believe that the preponderance of small request sizes is the natural result of parallelization by distributing file data across many processors, and would be found in other work-loads using a similar file-system interface. 4.4 Sequentiality A common characteristic of file workloads, particularly scientific workloads, is that files are accessed sequentially <ref> [28, 1, 25] </ref>. <p> This further suggests that most of the hits in the I/O node cache were indeed a result of interprocess locality because, as Figure 8 shows, the limited intraprocess locality was filtered out by the compute-node cache. Note the contrast with Miller and Katz's tracing study <ref> [25] </ref>, which found little benefit from caching. (They did notice a benefit from prefetching and write-behind.) Both their workload and ours involve sequential access patterns; the difference is that the small requests in our access pattern lead to intraprocess spatial locality, and the distribution of a sequential pattern across parallel compute
Reference: [26] <institution> NASA Ames Research Center, Moffet Field, CA. NAS User Guide, </institution> <address> 6.1 edition, </address> <month> Mar. </month> <year> 1993. </year>
Reference-contexts: The I/O nodes are based on the Intel i386 processor and each has a port for SCSI disk drives. There may also be one or more service nodes that handle as Ethernet connections or interactive shells <ref> [26] </ref>. Intel's Concurrent File System (CFS) [30, 15, 27] provides a Unix-like interface to the user with the addition of four I/O modes to help the programmer coordinate parallel access to files. <p> Their iPSC has 128 compute nodes, each with 8 MB of memory, and 10 I/O nodes, each with 4 MB of memory and a single 760 MB disk drive <ref> [26] </ref>. There is also a single service node that handles a 10-Mbit Ethernet connection to the host computer. The total I/O capacity is 7.6 GB and the total bandwidth is less than 10 MB/s.
Reference: [27] <author> B. Nitzberg. </author> <title> Performance of the iPSC/860 Concurrent File System. </title> <type> Technical Report RND-92-020, </type> <institution> NAS Systems Division, NASA Ames, </institution> <month> Dec. </month> <year> 1992. </year>
Reference-contexts: Recent studies have found that CFS caching and prefetching work well in limited situations, but that the throughput of CFS can be disappointing relative to the capabilities of the hardware <ref> [27, 3] </ref>. Miller and Katz drove a cache simulation using traces from a Cray supercomputer and found that access locality was not high enough for significant benefits to be realized from a file system cache [25]. 2.4 Intel iPSC/860 and CFS The iPSC/860 is a distributed-memory, message-passing, MIMD machine. <p> The I/O nodes are based on the Intel i386 processor and each has a port for SCSI disk drives. There may also be one or more service nodes that handle as Ethernet connections or interactive shells [26]. Intel's Concurrent File System (CFS) <ref> [30, 15, 27] </ref> provides a Unix-like interface to the user with the addition of four I/O modes to help the programmer coordinate parallel access to files. <p> Similarly, 89.4% of all writes were for fewer than 4000 bytes, but those writes transferred only 3% of all data written (not shown). The number of small requests is surprising due to their poor performance in node basis. CFS <ref> [27] </ref>. The small peak at 4 KB indicates that some users have optimized for the file-system block size, but it appears that most users prefer ease of programming over performance. requests as well as in the data transferred by 1 MB requests.
Reference: [28] <author> J. Ousterhout, H. D. Costa, D. Harrison, J. Kunze, M. Kupfer, and J. Thompson. </author> <title> A trace driven analysis of the UNIX 4.2 BSD file system. </title> <booktitle> In Proceedings of the Tenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 15-24, </pages> <month> Dec. </month> <year> 1985. </year>
Reference-contexts: Related file-system workload studies can be classified as characterizing general-purpose workstations (or workstation networks), scientific vector applications, or scientific parallel applications. General-purpose workstations. Uniprocessor file access patterns have been measured many times. Floyd and Ellis [12, 13] and Ousterhout et al. <ref> [28] </ref> measured isolated Unix workstations, and Baker et al. measured a distributed Unix (Sprite) system [1]. All of these studies cover general-purpose (engineering and office) workloads with uniprocessor applications. Scientific vector applications. Some studies specifically examined scientific workloads. Del Rosario and Choudhary provide an informal characterization of grand-challenge applications [10]. <p> believe that the preponderance of small request sizes is the natural result of parallelization by distributing file data across many processors, and would be found in other work-loads using a similar file-system interface. 4.4 Sequentiality A common characteristic of file workloads, particularly scientific workloads, is that files are accessed sequentially <ref> [28, 1, 25] </ref>.
Reference: [29] <author> B. K. Pasquale and G. C. Polyzos. </author> <title> A static analysis of I/O characteristics of scientific applications in a production workload. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 388-397, </pages> <year> 1993. </year>
Reference-contexts: Powell measured file sizes on a Cray-1 file system [31]. Miller and Katz traced specific I/O-intensive Cray applications to determine the per-file access patterns [25], focusing primarily on access rates. Pasquale and Polyzos studied I/O-intensive Cray applications, focusing on patterns in the I/O rate <ref> [29] </ref>. All of these studies are limited to uniprocess applications on vector supercomputers. Scientific parallel applications. Crockett [7] and Kotz [20] hypothesize about the character of a parallel scientific file-system workload. Cormen and Kotz [6] discuss the needs of parallel-I/O algorithms.
Reference: [30] <author> P. Pierce. </author> <title> A concurrent file system for a highly parallel mass storage system. </title> <booktitle> In Fourth Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 155-160, </pages> <year> 1989. </year>
Reference-contexts: Most extend a traditional file abstraction (a growable, addressable sequence of bytes) with some parallel file-access methods. The most common provide I/O "modes" that specify whether and how parallel processes share a file pointer <ref> [7, 30, 33, 2, 17] </ref>. Some are based on a memory-mapped interface [23, 22]. Some provide a way for the user to specify per-process logical views of the file [5, 9]. Some provide SIMD-style transfers [34, 24, 16]. <p> The I/O nodes are based on the Intel i386 processor and each has a port for SCSI disk drives. There may also be one or more service nodes that handle as Ethernet connections or interactive shells [26]. Intel's Concurrent File System (CFS) <ref> [30, 15, 27] </ref> provides a Unix-like interface to the user with the addition of four I/O modes to help the programmer coordinate parallel access to files.
Reference: [31] <author> M. L. Powell. </author> <title> The DEMOS File System. </title> <booktitle> In Proceedings of the Sixth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 33-42, </pages> <month> Nov. </month> <year> 1977. </year>
Reference-contexts: All of these studies cover general-purpose (engineering and office) workloads with uniprocessor applications. Scientific vector applications. Some studies specifically examined scientific workloads. Del Rosario and Choudhary provide an informal characterization of grand-challenge applications [10]. Powell measured file sizes on a Cray-1 file system <ref> [31] </ref>. Miller and Katz traced specific I/O-intensive Cray applications to determine the per-file access patterns [25], focusing primarily on access rates. Pasquale and Polyzos studied I/O-intensive Cray applications, focusing on patterns in the I/O rate [29]. All of these studies are limited to uniprocess applications on vector supercomputers.
Reference: [32] <author> A. L. N. Reddy and P. Banerjee. </author> <title> A study of I/O behavior of Perfect benchmarks on a multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 312-321, </pages> <year> 1990. </year>
Reference-contexts: Crockett [7] and Kotz [20] hypothesize about the character of a parallel scientific file-system workload. Cormen and Kotz [6] discuss the needs of parallel-I/O algorithms. Reddy et al. chose five sequential scientific applications from the PERFECT benchmarks and parallelized them for an eight-processor Alliant, finding only sequential file-access patterns <ref> [32] </ref>. This study is interesting, but far from what we need: the sample size is small; the programs are parallelized sequential programs, not parallel programs per se; and the I/O itself was not par-allelized. Cypher et al. [8] studied individual parallel scientific applications, measuring temporal patterns in I/O rates.
Reference: [33] <author> P. J. Roy. </author> <title> Unix file access and caching in a multicom-puter environment. </title> <booktitle> In Proceedings of the Usenix Mach III Symposium, </booktitle> <pages> pages 21-37, </pages> <year> 1993. </year>
Reference-contexts: Most extend a traditional file abstraction (a growable, addressable sequence of bytes) with some parallel file-access methods. The most common provide I/O "modes" that specify whether and how parallel processes share a file pointer <ref> [7, 30, 33, 2, 17] </ref>. Some are based on a memory-mapped interface [23, 22]. Some provide a way for the user to specify per-process logical views of the file [5, 9]. Some provide SIMD-style transfers [34, 24, 16].
Reference: [34] <institution> Connection Machine model CM-2 technical summary. </institution> <type> Technical Report HA87-4, </type> <institution> Thinking Machines, </institution> <month> Apr. </month> <year> 1987. </year>
Reference-contexts: Some are based on a memory-mapped interface [23, 22]. Some provide a way for the user to specify per-process logical views of the file [5, 9]. Some provide SIMD-style transfers <ref> [34, 24, 16] </ref>. PIFS (Bridge) [11] allows the file system to control which processor handles which parts of the file, to encourage memory locality. Clearly, the industrial and research communities have not yet settled on a single new model for file access.
References-found: 34

