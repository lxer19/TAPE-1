URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/aberger/www/ps/oracle.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/aberger/www/web.html
Root-URL: 
Email: aberger,rcm@cs.cmu.edu  
Title: JUST-IN-TIME LANGUAGE MODELLING  
Author: Adam Berger and Robert Miller 
Address: Pittsburgh PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Abstract: Traditional approaches to language modelling have relied on a fixed corpus of text to inform the parameters of a probability distribution over word sequences. Increasing the corpus size often leads to better-performing language models, but no matter how large, the corpus is a static entity, unable to reflect information about events which postdate it. In these pages we introduce an online paradigm which interleaves the estimation and application of a language model. We present a Bayesian approach to online language modelling, in which the marginal probabilities of a static trigram model are dynamically updated to match the topic being dictated to the system. We also describe the architecture of a prototype we have implemented which uses the World Wide Web (WWW) as a source of information, and provide results from some initial proof of concept experiments. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. Seymore and R. </author> <title> Rosenfeld (1997). Using story topics for language model adaptation. </title> <booktitle> Eurospeech '97 </booktitle>
Reference-contexts: When applying this composite model, one needs somehow to detect the topic at hand and select the model appropriate to the topic. A somewhat more refined approach is to allow a few topics to be active at once, and apply a weighted average of the individual topic models <ref> [1] </ref>. These approaches rely on a training corpus fixed offline, prior to applying the model. Such an approach works well when the topic at hand is to be found in the training corpus, but not when Adam Berger is partially supported by an IBM Cooperative Fellowship.
Reference: [2] <author> P. Clarkson and R. </author> <title> Rosenfeld (1997) Statistical language modeling using the CMU-Cambridge toolkit. </title> <address> Eu-rospeech '97 </address>
Reference-contexts: Our concern here is to construct a language model which incorporates knowledge gleaned from C U into S. Some desired properties of : 1 We make no explicit assumptions about the form of S in what follows, though the prototype described later uses the trigram model of <ref> [2] </ref> (1) The influence which the update corpus C U plays should increase with its size (number of words) N U .
Reference: [3] <author> R. </author> <title> Szeliski (1989) Bayesian modelling of uncertainty in low level vision. </title> <address> Boston: </address> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: Notice that in the case of a uniform prior p (), the optimization problem posed by (5) reduces to maximum-likelihood: find that which assigns maximal probability to the data C U . It has been recognized for some time in the computer vision community <ref> [3] </ref> that this type of Bayesian approach can be viewed as an instance of regularization, a popular method for solving ill-posed optimization problems.
Reference: [4] <author> A. Berger and R. </author> <title> Miller (1998). A real-time system for language modelling. </title> <type> CMU CS Department Technical Report. (Forthcoming). </type>
Reference-contexts: Q () = p Y exp 2 x;y e y s (yjx) y e y s (yjx) It is a straightforward exercise in calculus to derive the con dition for ? y , the optimal value of y (Space precludes a more thorough treatment here, but details are available in <ref> [4] </ref>): ? N U 2 + p u (y) x where p u (x; y) c u (x; y)=N U . Some observations about (6): * As N U gets small, ? y tends to zero.
Reference: [5] <author> S. Della Pietra, V. Della Pietra and J. </author> <title> Lafferty (1997) Inducing features of random fields. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence:19(4), </journal> <volume> 380393. </volume>
Reference-contexts: There are some technical issues to contend with when the optimal setting for y is non-finite; these and other aspects of the iterative scaling are addressed in <ref> [5] </ref>. In seeking ? , it helps to recast the problem to a slightly different form. Starting from some model , we seek the optimal change y to each parameter y . We denote the vector of (additive) changes by .
Reference: [6] <institution> DEC, AltaVista. </institution> <note> http://altavista.digital.com. </note>
Reference-contexts: As depicted in figure 2, the query engine first filters noise words from the query, and then passes the query to a Web search engine. We experimented with two search engines: AltaVista <ref> [6] </ref>, which indexes over 100 million Web pages of all kinds; and News Index [7], which indexes about 200 online news sources but revisits them frequently.
Reference: [7] <institution> News Index. </institution> <note> http://www.newsindex.com. </note>
Reference-contexts: As depicted in figure 2, the query engine first filters noise words from the query, and then passes the query to a Web search engine. We experimented with two search engines: AltaVista [6], which indexes over 100 million Web pages of all kinds; and News Index <ref> [7] </ref>, which indexes about 200 online news sources but revisits them frequently. Web pages found by the search engine are fetched in order of relevance in a multithreaded manner, stripped of HTML formatting tags, and added to the update corpus until it reaches the desired size.
Reference: [8] <author> K.F. Lee, H.W. Hon and R. </author> <title> Reddy (1990) An overview of the SPHINX speech recognition system. </title> <journal> Journal of Acoustics, Speech, and Signal Processing: 38:1. </journal>
Reference-contexts: We are pursuing more refined approaches to generating an update corpus, and plan to incorporate the system into the CMU Sphinx <ref> [8] </ref> speech recognition system to determine what effect this approach can have on word error rate. Acknowledgments The authors thank John Lafferty and Stanley Chen for technical guidance.
References-found: 8

