URL: http://www.cam.sri.com/tr/crc051/paper.ps.Z
Refering-URL: http://www.cam.sri.com/tr/ABSTRACTS.html
Root-URL: 
Title: THE SPEECH-LANGUAGE INTERFACE IN THE SPOKEN LANGUAGE TRANSLATOR  
Author: David Carter and Manny Rayner 
Address: 23 Millers Yard Cambridge CB2 1RQ, U.K.  
Affiliation: SRI International Cambridge Computer Science Research Centre  
Note: of TWLT-8, Twente Workshop on Language Technology, University of Twente, Holland,  
Email: dmc@cam.sri.com, manny@cam.sri.com  
Web: URL: http://www.cam.sri.com/tr/crc051/paper.ps.ZProceedings  
Date: 1994  
Abstract: The Spoken Language Translator (SLT) is a prototype for practically useful systems capable of translating continuous spoken language within restricted domains. The prototype system translates air travel (ATIS) queries from spoken En- glish to spoken Swedish and to French. It is constructed, with as few modifications as possible, from existing pieces of speech and language processing software. The speech recognizer and language under- stander are connected by a fairly conventional pipelined N-best interface. This paper focuses on the ways in which the language processor makes intelligent use of the sentence hypotheses delivered by the recognizer. These ways include (1) producing modified hypotheses to reflect the possible presence of repairs in the uttered word sequence; (2) fast parsing with a version of the grammar automatically specialized to the more frequent constructions in the training corpus; and (3) allowing syntactic and semantic factors to interact with acoustic ones in the choice of a meaning structure for translation, so that the acoustically preferred hypothesis is not always selected even if it is within linguistic coverage. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Agnas, M-S., </author> <title> and 17 others (1994). Spoken Language Translator: First Year Report. Joint report by SRI International (Cambridge) and SICS. Order from preben@sics.se. Alshawi, Hiyan, editor (1992). The Core Language Engine. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts. </address>
Reference: <author> Alshawi, Hiyan, and David Carter (1994). </author> <title> "Training and Scaling Preference Functions for Disambiguation". </title> <note> Computational Linguistics, 20:4. </note>
Reference: <author> Alshawi, Hiyan, and Richard Crouch (1992). </author> <title> "Monotonic Semantic Interpretation". </title> <booktitle> In Proceedings of 30th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. 32-39, </pages> <address> Newark, Delaware. </address>
Reference-contexts: Language analysis in SLT is performed by the SRI Core Language Engine (CLE), a general natural-language processing system developed at SRI Cambridge <ref> (Alshawi, 1992) </ref>. The English grammar used for this is a large general-purpose feature grammar, which has been augmented with a small number of domain-specific rules. It associates surface strings with meaning representations in Quasi Logical Form (QLF; Alshawi and Crouch, 1992).
Reference: <author> Bear, J., J. Dowding, and E. </author> <title> Shriberg (1992). "Integrating multiple knowledge sources for the detection and correction of repairs in humancomputer dialog". </title> <booktitle> In Proceedings of 30th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. 56-63, </pages> <editor> Newark, Delaware. van Harmelen, Frank, and Alan Bundy (1988). </editor> <title> "Explanation-Based Generalization = Partial Evaluation" (Research Note), </title> <journal> Artificial Intelligence 36 , pp. </journal> <pages> 401-412. </pages>
Reference: <author> Hirsh, </author> <title> Haym (1987). "Explanation-Based Generalization in a Logic-Programming Environment", </title> <booktitle> Proceedings of the Tenth International Joint Conference on Artificial Intelligence, Milan, </booktitle> <pages> pp. 221227. </pages>
Reference: <author> Murveit, Hy, John Butzberger, Vassilios Digalakis, and Mitch Weintraub (1993). </author> <title> "Large Vocabulary Dictation using SRI's DECIPHER(TM) Speech Recognition System: Progressive Search Techniques". </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, </booktitle> <volume> volume 2, </volume> <pages> pp. 319-322, </pages> <address> Minneapolis, Minnesota. </address>
Reference-contexts: The models are gender-independent and the system is trained on 19,000 sentences and has a 1381-word vocabulary. A bigram language model is used. The output is an N-best hypothesis list, produced using a progressive recognition search <ref> (Murveit et al, 1993) </ref> in which the space of possible utterances is pruned by successively more powerful but more costly techniques. The motiva <p>- 1 tion for this kind of search is to avoid making hard decisions without sufficient evidence, while at the same time maintaining reasonable efficiency.
Reference: <author> Murveit, Hy, John Butzberger, and Mitch Weintraub (1991). </author> <title> "Speech Recognition in SRI's Resource Management and ATIS Systems". </title> <booktitle> In Proceedings of the 4th Speech and Natural Language Workshop. DARPA, </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The speech recognizer used is a fast version of SRI's DECIPHER [TM] speaker-independent continuous speech recognition system <ref> (Murveit et al, 1991) </ref>. It uses context-dependent phonetic- based hidden Markov models (HMMs) with discrete observation distributions for four features: cepstrum, delta-cepstrum, energy and delta- energy. The models are gender-independent and the system is trained on 19,000 sentences and has a 1381-word vocabulary. A bigram language model is used.
Reference: <author> Nakatani, C., and J. </author> <title> Hirschberg (1993). "A speech-first model of repair detection and cor-rection". </title> <booktitle> In Proceedings of 31th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. 46-53, </pages> <address> Columbus, Ohio. </address>
Reference: <author> Rayner, M. </author> <year> (1988). </year> <title> "Applying Explanation-Based Generalization to Natural-Language Processing". </title> <booktitle> Proceedings of the Conference on Fifth Generation Computer Systems, </booktitle> <address> Tokyo. </address>
Reference: <author> Rayner, M., and C. </author> <title> Samuelsson (1990). "Using Explanation-Based Learning to Increase Performance in a Large-Scale NL Query Interface". </title> <booktitle> Proceedings of the 3rd DARPA Speech and Natural Language Workshop, </booktitle> <address> Hidden Valley. </address>
Reference: <author> Rayner, M., </author> <title> and 11 others (1993). "Spoken Language Translation with Mid-90's Technology: a Case Study". </title> <booktitle> Proceedings of Eurospeech-93, </booktitle> <address> Berlin. </address>
Reference: <author> Rayner, M., D. Carter, V. Digalakis and P. </author> <title> Price (1994). "Combining Knowledge Sources to Reorder N-Best Speech Hypothesis Lists". </title> <booktitle> Proceedings of the 1994 ARPA Workshop on Human Language Technology, </booktitle> <address> Princeton. </address>
Reference-contexts: Nevertheless, it seemed likely that introducing linguistic factors, if done optimally, should improve sentence accuracy by more than a couple of per cent. We therefore carried out some experiments <ref> (reported in full in Rayner et al, 1994) </ref> in which several preference functions were trained on N-best data as in SLT-1, but with sentence hypothesis selection, rather than QLF selection, as the objective. The value of N was chosen to be 10, rather than 5 as in the run-time system.
Reference: <author> Samuelsson, C., and M. </author> <title> Rayner (1991). "Quantitative Evaluation of Explanation-Based Learning as a Tuning Tool for a Large-Scale Natural Language System". </title> <booktitle> Proceedings of the 12th International Joint Conference on Artificial Intelligence, </booktitle> <address> Sydney. </address>
Reference: <author> Samuelsson, C. </author> <year> (1994). </year> <title> Fast Natural Language Parsing Using Explanation-Based Learning, </title> <type> PhD thesis, </type> <institution> Royal Institute of Technology, Stockholm. </institution> <month> 9 </month>
Reference-contexts: This loss of coverage is more than counterbalanced by the greatly simplified structure of the specialized grammar, which can be parsed nearly two orders of magnitude more quickly than the general one, using an LR parsing algorithm <ref> (Samuelsson, 1994) </ref>. The gain in speed is due to the fact that the grammar, after specialization, is nearly finite-state; we have in effect automatically squeezed a general grammar into a finite-state format, after cutting off the few pieces that refuse to fit.
References-found: 14

