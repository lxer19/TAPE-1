URL: ftp://psyche.mit.edu/pub/jordan/AIM-1458.ps.Z
Refering-URL: http://www.ai.mit.edu/projects/jordan.html
Root-URL: 
Author: I. Jordan and Lei Xu 
Note: Michael  Copyright c Massachusetts Institute of Technology, 1993  
Date: 1458 November 18, 1993  87  
Affiliation: MASSACHUSETTS INSTITUTE OF TECHNOLOGY ARTIFICIAL INTELLIGENCE LABORATORY and CENTER FOR BIOLOGICAL AND COMPUTATIONAL LEARNING DEPARTMENT OF BRAIN AND COGNITIVE SCIENCES  
Pubnum: A.I. Memo No.  C.B.C.L. Memo No.  
Abstract: Convergence Results for the EM Approach to Abstract The Expectation-Maximization (EM) algorithm is an iterative approach to maximum likelihood parameter estimation. Jordan and Jacobs (1993) recently proposed an EM algorithm for the mixture of experts architecture of Jacobs, Jordan, Nowlan and Hinton (1991) and the hierarchical mixture of experts architecture of Jordan and Jacobs (1992). They showed empirically that the EM algorithm for these architectures yields significantly faster convergence than gradient ascent. In the current paper we provide a theoretical analysis of this algorithm. We show that the algorithm can be regarded as a variable metric algorithm with its searching direction having a positive projection on the gradient of the log likelihood. We also analyze the convergence of the algorithm and provide an explicit expression for the convergence rate. In addition, we describe an acceleration technique that yields a significant speedup in simulation experiments. This report describes research done at the Dept. of Brain and Cognitive Sciences, the Center for Biological and Computational Learning, and the Artificial Intelligence Laboratory of the Massachusetts Institute of Technology. Support for CBCL is provided in part by a grant from the NSF (ASC-9217041). Support for the laboratory's artificial intelligence research is provided in part by the Advanced Research Projects Agency of the Dept. of Defense. The authors were supported by a grant from the McDonnell-Pew Foundation, by a grant from ATR Human Information Processing Research Laboratories, by a grant from Siemens Corporation, by by grant IRI-9013991 from the National Science Foundation, by grant N00014-90-J-1942 from the Office of Naval Research, and by NSF grant ECS-9216531 to support an Initiative in Intelligent Control at MIT. Michael I. Jordan is a NSF Presidential Young Investigator. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Baum, L.E., & Sell, G.R. </author> <year> (1968). </year> <title> Growth transformation for functions on manifolds. Pac. </title> <journal> J. Math., </journal> <volume> 27, </volume> <pages> 211-227. </pages>
Reference: <author> Baum, L.E., Petrie, T., Soules, G., & Weiss, N. </author> <year> (1970). </year> <title> A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains. </title> <journal> The Annals of Mathematical Statistics, </journal> <volume> 41, </volume> <pages> 164-171. </pages>
Reference: <author> Breiman, L., Friedman, J.H., Olshen, R.A., & Stone, C.J. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <address> Belmont, CA: </address> <publisher> Wadsworth International Group. </publisher>
Reference-contexts: This assumption does not allow for piecewise variation in the form of the regression surface; all of the regression components contribute throughout the input space. Switching regression can be viewed as one end of a continuum in which the overlap in the regression components is total; decision tree models <ref> (e.g., Breiman et al., 1984) </ref> are the other end of the continuum in which the overlap is zero. The ME model interpolates smoothly between these extremes. An EM algorithm for training the mixture of experts In many estimation problems the likelihood is a complicated nonlinear function of the parameters.
Reference: <author> De Veaux, R.D. </author> <year> (1986). </year> <title> Parameter estimation for a mixture of linear regressions. </title> <type> Ph.D. </type> <institution> Dissertation and Tech. Rept. No. 247, Department of Statistics, Stanford University, Stanford, </institution> <address> CA. </address>
Reference: <author> Dempster, A.P., Laird, N.M., & Rubin, D.B. </author> <year> (1977). </year> <title> Maximum-likelihood from incomplete 30 data via the EM algorithm. </title> <journal> J. of Royal Statistical Society, B39, </journal> <pages> 1-38. </pages>
Reference-contexts: An algorithm with an M-step given by Eq. (13) is referred to as a generalized EM (GEM) algorithm <ref> (Dempster, Laird & Rubin, 1977) </ref>.
Reference: <author> Friedman, J.H. </author> <year> (1991). </year> <title> Multivariate adaptive regression splines. </title> <journal> The Annals of Statistics, </journal> <volume> 19, </volume> <pages> 1-141. </pages>
Reference: <author> Jacobs, R.A., Jordan, M.I., Nowlan, S.J., & Hinton, G.E. </author> <year> (1991). </year> <title> Adaptive mixtures of local experts. </title> <journal> Neural Computation, </journal> <volume> 3, </volume> <pages> 79-87. </pages>
Reference: <author> Jordan, M.I. & Jacobs, R.A. </author> <year> (1992). </year> <title> Hierarchies of adaptive experts. </title> <editor> In J.E. Moody, S. Hanson & R.P. Lippmann, (Eds.), </editor> <booktitle> Advances in Neural Information Processing System 4. </booktitle> <address> San Mateo: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 985-992. </pages>
Reference-contexts: The hierarchical mixture of experts (HME) architecture generalizes this idea to a nested model in which regions in the input space are split recursively into subregions <ref> (Jordan & Jacobs, 1992) </ref>. The resulting tree-structured architecture can be viewed as a multi-resolution function approximator in which smoothed piecewise functions are fit at a variety of levels of resolution. As shown in Figure 2, the HME architecture is a tree.
Reference: <author> Jordan, M.I. & Jacobs, R.A. </author> <title> (in press). Hierarchical mixtures of experts and the EM algorithm. </title> <booktitle> Neural Computation. </booktitle>
Reference: <author> Little, R.J.A., & Rubin, D.B. </author> <year> (1987). </year> <title> Statistical Analysis with Missing Data. </title> <address> New York: </address> <publisher> John Wiley. </publisher>
Reference-contexts: Given an observed data set Y, we augment Y with a set of additional variables Y mis , called "missing" or "hidden" variables, and consider a maximum likelihood problem for a "complete-data" set Z = fY; Y mis g <ref> (cf. Little & Rubin, 1987) </ref>. We choose the missing variables in such 4 a way that the resulting "complete-data log likelihood," given by l c (fi; Z) = ln P (Y; Y mis jfi), is easy to maximize with respect to fi.
Reference: <author> McCullagh, P. & Nelder, J.A. </author> <year> (1983). </year> <title> Generalized Linear Models. </title> <publisher> London: Chapman and Hall. </publisher>
Reference: <author> McLachlan, G.J., & Basford, K.E. </author> <year> (1988). </year> <title> Mixture Models: Inference and Application to Clustering. </title> <address> New York: </address> <publisher> Dekker. </publisher>
Reference-contexts: The advantage of this niche in statistical theory is that these models have much of the flexibility of nonparametric approaches, but retain some of the analytical advantages of parametric approaches <ref> (McLachlan & Basford, 1988) </ref>. Similar remarks can be made in the case of supervised learning: The ME architecture and the HME architecture provide flexible models for general nonlinear regression while retaining a strong flavor of parametric statistics.
Reference: <author> Meilijson, I. </author> <year> (1989). </year> <title> A fast improvement to the EM algorithm on its own terms. </title> <journal> J. of Royal Statistical Society, </journal> <volume> B51, </volume> <pages> 127-138. </pages>
Reference: <author> Peters, B.C. & Walker, H.F. </author> <year> (1978a). </year> <title> An iterative procedure for obtaining maximum-likelihood estimates of the parameters for a mixture of normal distributions. </title> <journal> SIAM J. Applied Mathematics, </journal> <volume> 35, </volume> <pages> 362-378. </pages>
Reference: <author> Peters, B.C., & Walker, H.F. </author> <year> (1978b). </year> <title> The numerical evaluation of the maximum-likelihood estimates of a subset of mixture proportions, </title> <journal> SIAM J. Applied Mathematics, </journal> <volume> 35, </volume> <pages> 447-452. </pages>
Reference: <author> Quandt, R.E. & Ramsey, J.B. </author> <year> (1972). </year> <title> A new approach to estimating switching regressions. </title> <journal> J. American Statistical Society, </journal> <volume> 67, </volume> <pages> 306-310. </pages>
Reference-contexts: Thus model (7) represents a unconditional probability, appropriate for unsupervised learning, while model (4) represents a conditional probability, appropriate for supervised learning. There is another model studied in statistics, the switching regression model <ref> (Quandt & Ramsey, 1972, 1978, De Veaux, 1986) </ref>, that is intermediate between model (7) and model (4).
Reference: <author> Quandt, R.E. & Ramsey, J.B. </author> <year> (1978). </year> <title> Estimating mixtures of normal distribution and switching regressions. </title> <journal> J. American Statistical Society, </journal> <volume> 73, </volume> <pages> 730-738. </pages>
Reference: <author> Redner, R.A., & Walker, H.F. </author> <year> (1984). </year> <title> Mixture densities, maximum likelihood, and the EM algorithm. </title> <journal> SIAM Review 26, </journal> <pages> 195-239. </pages>
Reference-contexts: EM is based on the idea of solving a succession of simplified problems that are obtained by augmenting the original observed variables with a set of additional "hidden" variables. Unconditional mixture models are particularly amenable to the EM approach <ref> (Redner & Walker, 1984) </ref> and, as observed by Jordan and Jacobs (in press), the conditional mixture of experts model is also amenable to an EM approach. <p> When a smaller value was used ( k = 0:5), the algorithm converged after 24 steps (Figure 6 (a)). Trying other values of k , we verified that k &lt; 1 slows down the convergence, while k &gt; 1 may speed up the convergence <ref> (cf. Redner & Walker, 1984) </ref>. We found, however, that the outcome was quite sensitive to the selection of the value of k . For example, setting k = 1:2 led the algorithm to diverge.
Reference: <author> Wu, C.F.J. </author> <year> (1983). </year> <title> On the convergence properties of the EM algorithm. </title> <journal> The Annals of Statistics, </journal> <volume> 11, </volume> <pages> 95-103. </pages>
Reference: <author> Xu, L., & Jordan, M.I. </author> <year> (1993). </year> <title> Theoretical and experimental studies of the EM algorithm for unsupervised learning based on finite Gaussian mixtures. </title> <institution> MIT Computational Cognitive Science Tech. </institution> <type> Rep. 9302, </type> <institution> MIT, </institution> <address> Cambridge, MA. </address>
Reference-contexts: We have shown that the algorithm converges linearly, with a rate determined by the difference between the minimal and maximal eigenvalues of a negative definite matrix. Similar results to those obtained here can also be obtained for the case of the unsupervised learning of finite mixtures <ref> (Xu & Jordan, 1993) </ref>.
References-found: 20

