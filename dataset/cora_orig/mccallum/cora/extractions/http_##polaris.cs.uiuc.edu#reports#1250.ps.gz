URL: http://polaris.cs.uiuc.edu/reports/1250.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: Automatic Program Parallelization  
Author: Utpal Banerjee Rudolf Eigenmann Alexandru Nicolau David A. Padua 
Date: February 1993  
Affiliation: Intel Corporation  University of Illinois at Urbana-Champaign  University of California at Irvine  University of Illinois at Urbana-Champaign  
Abstract: This paper presents an overview of automatic program parallelization techniques. It covers dependence analysis techniques, followed by a discussion of program transformations, including straight-line code parallelization, do loop transformations, and parallelization of recursive routines. The last section of the paper surveys several experimental studies on the effectiveness of parallelizing compilers. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. J. Kuck, R. H. Kuhn, D. A. Padua, B. Leasure, and M. Wolfe. </author> <title> Dependence Graphs and Compiler Optimizations. </title> <booktitle> Proceedings of the 8th ACM Symp. on Principles of Programming Languages, </booktitle> <pages> pages 207-218, </pages> <month> Jan. </month> <year> 1981. </year>
Reference-contexts: There are several surveys of automatic parallelization <ref> [1, 2, 3] </ref> and several descriptions of experimental systems [4, 5, 6, 7]. However, this paper is, hopefully, a useful contribution because it presents an up-to-date overview that includes references to the most recent literature and discusses both instruction-level and coarse-grain paral 1 lelization techniques.
Reference: [2] <author> D. Padua and M. Wolfe. </author> <title> Advanced Compiler Optimization for Supercomputers. </title> <journal> CACM, </journal> <volume> 29(12) </volume> <pages> 1184-1201, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: There are several surveys of automatic parallelization <ref> [1, 2, 3] </ref> and several descriptions of experimental systems [4, 5, 6, 7]. However, this paper is, hopefully, a useful contribution because it presents an up-to-date overview that includes references to the most recent literature and discusses both instruction-level and coarse-grain paral 1 lelization techniques.
Reference: [3] <author> J. Allen and K. Kennedy. </author> <title> Automatic translation of Fortran programs to vector form. </title> <journal> ACM Trans. on Programming Languages and Systems, </journal> <volume> 9(4) </volume> <pages> 491-542, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: There are several surveys of automatic parallelization <ref> [1, 2, 3] </ref> and several descriptions of experimental systems [4, 5, 6, 7]. However, this paper is, hopefully, a useful contribution because it presents an up-to-date overview that includes references to the most recent literature and discusses both instruction-level and coarse-grain paral 1 lelization techniques. <p> There is also a generalized gcd test that works for a system of linear diophantine equations [10]. The exact method of data dependence computation illustrated in examples 3 and 4 is described in [11], [12] and [10] The approximate method of Example 5 is described in [13], <ref> [3] </ref>, [10] and [14]. The approximate method described here is a very simple example of a linear programming problem. We did not have to use any general algorithm (like the simplex method, for example) since the feasible region is so simple that the corner points are obvious.
Reference: [4] <author> D. J. Kuck, R. H. Kuhn, B. Leasure, and M. Wolfe. </author> <title> The Structure of an Advanced Vectorizer for Pipelined Processors. </title> <booktitle> Proceedings of COMPSAC 80, The 4th Int'l. Computer Software and Applications Conf., </booktitle> <pages> pages 709-715, </pages> <month> Oct. </month> <year> 1980. </year>
Reference-contexts: There are several surveys of automatic parallelization [1, 2, 3] and several descriptions of experimental systems <ref> [4, 5, 6, 7] </ref>. However, this paper is, hopefully, a useful contribution because it presents an up-to-date overview that includes references to the most recent literature and discusses both instruction-level and coarse-grain paral 1 lelization techniques. The rest of the paper is organized as follows. <p> After the transformation, the control dependences of T , U , and V on S become flow dependences generated by the variable b. 2 The previous transformation from control to data dependence was used in Parafrase <ref> [4] </ref>, an experimental par-allelizing compiler. The transformation is described by Banerjee [11] and by Allen and Kennedy [28]. In the recent past, there have been several intermediate language proposals that can be used to represent both control and data dependences in a consistent and convenient manner. <p> Examples of such techniques are loop fusion [87], loop collapsing <ref> [4] </ref>, loop coalescing [88], and tiling. Loop fusion transforms two disjoint do loops into a single loop. If both loops are parallel, fusing them decreases the overhead because, for example, only one parallel loop has to be started instead of two. <p> Another approach is to discriminate among individual compiler techniques. Thus, Cytron et al. [155] studied the performance degradation of the EISPACK algorithms after disabling various restructuring techniques of Parafrase <ref> [4] </ref>. Of the measured analysis and transformation steps, scalar expansion was the most effective, followed by conversion of control dependence into data dependence, a sharp data-dependence test analysis pass, and the recurrence recognition and substitution pass.
Reference: [5] <author> J.R. Allen and K. Kennedy. </author> <title> Pfc: A program to convert fortran to parallel form. </title> <editor> In K. Hwang, editor, </editor> <booktitle> Supercomputers: Design and Applications, </booktitle> <pages> pages 186-205. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1985. </year>
Reference-contexts: There are several surveys of automatic parallelization [1, 2, 3] and several descriptions of experimental systems <ref> [4, 5, 6, 7] </ref>. However, this paper is, hopefully, a useful contribution because it presents an up-to-date overview that includes references to the most recent literature and discusses both instruction-level and coarse-grain paral 1 lelization techniques. The rest of the paper is organized as follows.
Reference: [6] <author> Fran Allen, Michael Burke, Philippe Charles, Ron Cytron, and Jeanne Ferrante. </author> <title> An Overview of the PTRAN Analysis System for Multiprocessing. </title> <booktitle> In Proceeding of Int'l. Conf. on Supercomputing, </booktitle> <pages> pages 194-211, </pages> <year> 1987. </year>
Reference-contexts: There are several surveys of automatic parallelization [1, 2, 3] and several descriptions of experimental systems <ref> [4, 5, 6, 7] </ref>. However, this paper is, hopefully, a useful contribution because it presents an up-to-date overview that includes references to the most recent literature and discusses both instruction-level and coarse-grain paral 1 lelization techniques. The rest of the paper is organized as follows.
Reference: [7] <author> Constantine Polychronopoulos, Milind Girkar, Moham-mad Reza Haghighat, Chia-Ling Lee, Bruce Leung, and Dale Schouten. </author> <title> Parafrase-2: A new generation parallelizing compiler. </title> <booktitle> Int'l. Jour. of High Speed Computing, </booktitle> <volume> 1(1) </volume> <pages> 45-72, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: There are several surveys of automatic parallelization [1, 2, 3] and several descriptions of experimental systems <ref> [4, 5, 6, 7] </ref>. However, this paper is, hopefully, a useful contribution because it presents an up-to-date overview that includes references to the most recent literature and discusses both instruction-level and coarse-grain paral 1 lelization techniques. The rest of the paper is organized as follows.
Reference: [8] <author> R. Cytron, J. Ferrante, and V. Sarkar. </author> <title> Experiences using control dependence in PTRAN. </title> <editor> In D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 186-212. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1990. </year>
Reference-contexts: We discuss transformations from a generic point of view and make only a few observations on how those techniques can be used to generate code for particular machines. Parallelizers should incorporate an economic model of the target machine <ref> [8] </ref>, which is used to determine when a particular transformation is profitable or to select one from a collection of possible transformations. Except for the techniques discussed in Section 3.2.4 to manage memory hierarchies and increase data locality, nothing is said in this paper about memory management and data allocation.
Reference: [9] <author> D. Knuth. </author> <booktitle> The Art of Computer Programming, </booktitle> <volume> Vol. 2: </volume> <booktitle> Seminumerical Algorithms. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1981. </year> <note> Second Edition. </note>
Reference-contexts: Equation (1) is a linear diophantine equation in two variables. The method for solving such equations is well known and is based on the Extended Euclid's Algorithm <ref> [9] </ref>. Let g denote the greatest common divisor (gcd) of a and b. Then (1) has a solution if and only if g (evenly) divides b 0 a 0 .
Reference: [10] <author> U. Banerjee. </author> <title> Dependence analysis for supercomputing. </title> <publisher> Kluwer Academic Publishers, Norwell, </publisher> <address> Mass., </address> <year> 1988. </year>
Reference-contexts: But, it is still unknown whether or not (6) has a solution satisfying (7). There is also a generalized gcd test that works for a system of linear diophantine equations <ref> [10] </ref>. The exact method of data dependence computation illustrated in examples 3 and 4 is described in [11], [12] and [10] The approximate method of Example 5 is described in [13], [3], [10] and [14]. The approximate method described here is a very simple example of a linear programming problem. <p> There is also a generalized gcd test that works for a system of linear diophantine equations <ref> [10] </ref>. The exact method of data dependence computation illustrated in examples 3 and 4 is described in [11], [12] and [10] The approximate method of Example 5 is described in [13], [3], [10] and [14]. The approximate method described here is a very simple example of a linear programming problem. <p> There is also a generalized gcd test that works for a system of linear diophantine equations <ref> [10] </ref>. The exact method of data dependence computation illustrated in examples 3 and 4 is described in [11], [12] and [10] The approximate method of Example 5 is described in [13], [3], [10] and [14]. The approximate method described here is a very simple example of a linear programming problem. We did not have to use any general algorithm (like the simplex method, for example) since the feasible region is so simple that the corner points are obvious.
Reference: [11] <author> Utpal Banerjee. </author> <title> Speedup of Ordinary Programs. </title> <type> PhD thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, Dept. of Computer Sci., </institution> <month> Oct. </month> <year> 1979. </year>
Reference-contexts: But, it is still unknown whether or not (6) has a solution satisfying (7). There is also a generalized gcd test that works for a system of linear diophantine equations [10]. The exact method of data dependence computation illustrated in examples 3 and 4 is described in <ref> [11] </ref>, [12] and [10] The approximate method of Example 5 is described in [13], [3], [10] and [14]. The approximate method described here is a very simple example of a linear programming problem. <p> The notion of control dependence has been discussed by several authors including Towle [26] and Banerjee <ref> [11] </ref>. The definition that is most frequently used today is that of Ferrante, Ottenstein, and Warren [27]. They assume control-flow graphs with only one sink, that is, a node with no outgoing arcs. Clearly, all control-flow graphs can be represented in this form. <p> After the transformation, the control dependences of T , U , and V on S become flow dependences generated by the variable b. 2 The previous transformation from control to data dependence was used in Parafrase [4], an experimental par-allelizing compiler. The transformation is described by Banerjee <ref> [11] </ref> and by Allen and Kennedy [28]. In the recent past, there have been several intermediate language proposals that can be used to represent both control and data dependences in a consistent and convenient manner.
Reference: [12] <author> M. Wolfe and U. Banerjee. </author> <title> Data dependence and its application to parallel processing. </title> <journal> Int'l. Journal of Parallel Programming, </journal> <volume> 16(2) </volume> <pages> 137-178, </pages> <month> April </month> <year> 1987. </year>
Reference-contexts: But, it is still unknown whether or not (6) has a solution satisfying (7). There is also a generalized gcd test that works for a system of linear diophantine equations [10]. The exact method of data dependence computation illustrated in examples 3 and 4 is described in [11], <ref> [12] </ref> and [10] The approximate method of Example 5 is described in [13], [3], [10] and [14]. The approximate method described here is a very simple example of a linear programming problem.
Reference: [13] <author> Utpal Banerjee. </author> <title> Data Dependence in Ordinary Programs. </title> <type> Master's thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, Dept. of Computer Sci., </institution> <month> Nov. </month> <year> 1976. </year>
Reference-contexts: There is also a generalized gcd test that works for a system of linear diophantine equations [10]. The exact method of data dependence computation illustrated in examples 3 and 4 is described in [11], [12] and [10] The approximate method of Example 5 is described in <ref> [13] </ref>, [3], [10] and [14]. The approximate method described here is a very simple example of a linear programming problem. We did not have to use any general algorithm (like the simplex method, for example) since the feasible region is so simple that the corner points are obvious.
Reference: [14] <author> M. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1989. </year>
Reference-contexts: The exact method of data dependence computation illustrated in examples 3 and 4 is described in [11], [12] and [10] The approximate method of Example 5 is described in [13], [3], [10] and <ref> [14] </ref>. The approximate method described here is a very simple example of a linear programming problem. We did not have to use any general algorithm (like the simplex method, for example) since the feasible region is so simple that the corner points are obvious. <p> This is equivalent to saying that any two statements belonging to a cycle in the statement dependence graph have to belong to the same subsequence, which is the traditional condition presented in the literature <ref> [14, 68] </ref>. Loop distribution in the presence of conditional statements can be done by transforming the control dependences into data dependences as discussed in Section 2.4. This was the approach followed by Parafrase. Another technique to distribute loops with conditional statements is presented by Kennedy and McKinley [69]. <p> This method was developed by Steve Chen for the Burroughs Scientific Processor. Loop interchanging is described in detail by Wolfe <ref> [14, 78] </ref>, who also studied how it can be applied to triangular loops. Further discussions on interchanging can be found in the work of Allen and Kennedy [79]. <p> Equivalent transformations can of course also be applied to scalars. In fact, several of the existing parallelizers are only capable of expanding or privatizing scalars, and most of the literature on paral-lelizers only discusses the case of scalars <ref> [14, 68] </ref>.
Reference: [15] <author> Alexander Schrijver. </author> <title> Theory of Linear and Integer Programming. </title> <publisher> John Wiley, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: Any known general integer programming method is time consuming. A number of data dependence tests have been proposed in recent years with the goal of extending the scope and/or accuracy of the basic methods illustrated above, without incurring the complexity of a general linear/integer programming algorithm. The Fourier-Motzkin method <ref> [15] </ref> of elimination has been used in many of those tests in place of the simplex or the cutting plane method. This method of elimination is simple to understand, but it is not a polynomial method. <p> This method of elimination is simple to understand, but it is not a polynomial method. It can be applied by hand to a small system, but can be quite time consuming for problems in many variables <ref> [15] </ref>. For a large system, the simplex method is expected to be much more efficient. Also, the elimination method decides if there is a real solution to a system of linear inequalities; it cannot say whether or not there is an integer solution.
Reference: [16] <author> Z. Li, P. Yew, and C. Zhu. </author> <title> An efficient data dependence analysis for parallelizing compilers. </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> 1(1) </volume> <pages> 26-34, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: Also, the elimination method decides if there is a real solution to a system of linear inequalities; it cannot say whether or not there is an integer solution. In fact, the technique illustrated in Example 5 can be derived from elimination. The -test <ref> [16] </ref> is an approximate test that tries to decide if there is a real solution to the whole system of data dependence equations satisfying the constraints. It assumes that no subscript tested can be formed by a linear combination of other subscripts.
Reference: [17] <author> X. Kong, D. Klappholz, and K. Psarris. </author> <title> The i test: An improved dependence test for automatic parallelization and vectorization. </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> 2(3), </volume> <month> July </month> <year> 1991. </year>
Reference-contexts: The -test [16] is an approximate test that tries to decide if there is a real solution to the whole system of data dependence equations satisfying the constraints. It assumes that no subscript tested can be formed by a linear combination of other subscripts. The I-test <ref> [17] </ref> combines the approximate method of Example 5 and the gcd test. It isolates the case in which the approximate method is exact, and therefore can decide if there is an integer solution in that case.
Reference: [18] <author> William Pugh. </author> <title> The Omega Test: A Fast and Practical Integer Programming Algorithm for Dependence Analysis. </title> <booktitle> Proceedings of Supercomputing'91, </booktitle> <month> November </month> <year> 1991. </year>
Reference-contexts: It is applicable when the array is one-dimensional, and the coefficients of the data dependence equation are `small' in a sense (at least one coefficient must be 1). The Omega test <ref> [18] </ref> uses an extension of the Fourier-Motzkin method to integer programming.
Reference: [19] <author> D. Maydan, J. Hennessy, and M. Lam. </author> <title> Efficient and exact data dependence analysis. </title> <booktitle> In Proceedings of the ACM SIG-PLAN `91 Conf. on Programming Language Design and Implementation, </booktitle> <address> Toronto, </address> <month> June </month> <year> 1991. </year> <journal> Available as SIGPLAN Notices, </journal> <volume> vol. 26, no. 6, </volume> <pages> pp. 1-14, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: The Omega test [18] uses an extension of the Fourier-Motzkin method to integer programming. Although its worst-case time complexity is exponential, it is claimed to be a "fast and practical method for performing data dependence analysis." Two recent papers, <ref> [19] </ref> and [20], describe practical experiences with sets of data dependence testing algorithms actually used by the authors. Brief descriptions of several tests and a large number of references on data dependence analysis can be found in [20]. <p> Table 1 summarizes these results. Data-dependence tests, as described in Section 2, are crucial for the successful recognition of parallel loops. Early evaluation work for these techniques was done by Shen et.al. [146] who have analyzed subscript patterns 28 that arise in real programs. Maydan et.al. <ref> [19] </ref> and Goff et.al. [20] present statistics on the success rates of data-dependence tests.
Reference: [20] <author> G. Goff, K. Kennedy, and C. Tseng. </author> <title> Practical dependence testing. </title> <booktitle> In Proceedings of the ACM SIGPLAN `91 Conf. on Programming Language Design and Implementation, </booktitle> <address> Toronto, </address> <month> June </month> <year> 1991. </year> <journal> Available as SIGPLAN Notices, </journal> <volume> vol. 26, no. 6, </volume> <pages> pp. 15-29. </pages>
Reference-contexts: The Omega test [18] uses an extension of the Fourier-Motzkin method to integer programming. Although its worst-case time complexity is exponential, it is claimed to be a "fast and practical method for performing data dependence analysis." Two recent papers, [19] and <ref> [20] </ref>, describe practical experiences with sets of data dependence testing algorithms actually used by the authors. Brief descriptions of several tests and a large number of references on data dependence analysis can be found in [20]. <p> a "fast and practical method for performing data dependence analysis." Two recent papers, [19] and <ref> [20] </ref>, describe practical experiences with sets of data dependence testing algorithms actually used by the authors. Brief descriptions of several tests and a large number of references on data dependence analysis can be found in [20]. The presence of subroutine or function invocations raises some important practical issues in relation to data dependence analysis. One simple solution is to expand inline (or integrate) the subroutine or function [21], and then perform dependence analysis on the resulting pro gram. <p> Data-dependence tests, as described in Section 2, are crucial for the successful recognition of parallel loops. Early evaluation work for these techniques was done by Shen et.al. [146] who have analyzed subscript patterns 28 that arise in real programs. Maydan et.al. [19] and Goff et.al. <ref> [20] </ref> present statistics on the success rates of data-dependence tests. Recently, Petersen and Padua [147] have extended this work by relating these numbers to program performance of a suite of Benchmark programs. 4.3.2 Comparing Performance Measurements Other researchers have focussed on actual timing measurements of automatically parallelized code.
Reference: [21] <author> Christopher Alan Huson. </author> <title> An In-Line Subroutine Expander for Parafrase. </title> <type> Master's thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, Dept. of Computer Sci., </institution> <month> Dec. </month> <year> 1982. </year>
Reference-contexts: The presence of subroutine or function invocations raises some important practical issues in relation to data dependence analysis. One simple solution is to expand inline (or integrate) the subroutine or function <ref> [21] </ref>, and then perform dependence analysis on the resulting pro gram. The major technical difficulty in this case is that it is necessary to reflect in the inlined code the effect of aliasing between formal and actual parameters.
Reference: [22] <author> K. Cooper and K. Kennedy. </author> <title> Efficient computation of flow insensitive interprocedural summary information. </title> <booktitle> In Proceedings of the ACM SIGPLAN'84 Symp. on Compiler Construction, SIGPLAN Notices vol. </booktitle> <volume> 19, no. 6, </volume> <month> June </month> <year> 1984. </year>
Reference-contexts: For this reason, several other techniques for interprocedural data dependence analysis have been developed. For lack of space we cannot describe them in this paper, but the reader is referred to the papers by Cooper and Kennedy <ref> [22] </ref>, Triolet et al. [23], Burke and Cytron [24], and Li and Yew [25] which describe some of the better known techniques. 2.4 Control Dependences As mentioned above, the control dependence relation represents that part of the control structure of the source program that is important to determine which transformations are
Reference: [23] <author> R. Triolet, F. Irigoin, and P. Feautrier. </author> <title> Direct parallelization of call statements. </title> <booktitle> In Proceedings of the ACM SIGPLAN'86 Symp. on Compiler Construction, SIGPLAN Notices, </booktitle> <volume> vol. 21, no. 7, </volume> <month> June </month> <year> 1986. </year>
Reference-contexts: For this reason, several other techniques for interprocedural data dependence analysis have been developed. For lack of space we cannot describe them in this paper, but the reader is referred to the papers by Cooper and Kennedy [22], Triolet et al. <ref> [23] </ref>, Burke and Cytron [24], and Li and Yew [25] which describe some of the better known techniques. 2.4 Control Dependences As mentioned above, the control dependence relation represents that part of the control structure of the source program that is important to determine which transformations are valid.
Reference: [24] <author> M. Burke and R. Cytron. </author> <title> Interprocedural dependence analysis and parallelization. </title> <booktitle> In Proceedings of the ACM SIG 33 PLAN'86 Symp. on Compiler Construction, SIGPLAN No--tices, </booktitle> <volume> vol. 21, no. 7, </volume> <month> June </month> <year> 1986. </year>
Reference-contexts: For this reason, several other techniques for interprocedural data dependence analysis have been developed. For lack of space we cannot describe them in this paper, but the reader is referred to the papers by Cooper and Kennedy [22], Triolet et al. [23], Burke and Cytron <ref> [24] </ref>, and Li and Yew [25] which describe some of the better known techniques. 2.4 Control Dependences As mentioned above, the control dependence relation represents that part of the control structure of the source program that is important to determine which transformations are valid.
Reference: [25] <author> Zhiyuan Li and Pen-Chung Yew. </author> <title> Efficient Interprocedu-ral Analysis for Program Parallelization and Restructuring. </title> <booktitle> Proceedings of ACM/SIGPLAN PPEALS, </booktitle> <pages> pages 85-99, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: For lack of space we cannot describe them in this paper, but the reader is referred to the papers by Cooper and Kennedy [22], Triolet et al. [23], Burke and Cytron [24], and Li and Yew <ref> [25] </ref> which describe some of the better known techniques. 2.4 Control Dependences As mentioned above, the control dependence relation represents that part of the control structure of the source program that is important to determine which transformations are valid.
Reference: [26] <author> Ross Albert Towle. </author> <title> Control and Data Dependence for Program Transformations. </title> <type> PhD thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, Dept. of Computer Sci., </institution> <month> Mar. </month> <year> 1976. </year>
Reference-contexts: The notion of control dependence has been discussed by several authors including Towle <ref> [26] </ref> and Banerjee [11]. The definition that is most frequently used today is that of Ferrante, Ottenstein, and Warren [27]. They assume control-flow graphs with only one sink, that is, a node with no outgoing arcs. Clearly, all control-flow graphs can be represented in this form.
Reference: [27] <author> J. Ferrante, K. J. Ottenstein, and J. D. Warren. </author> <title> The Program Dependency Graph and its Uses in Optimization. </title> <journal> ACM Trans. on Programming Languages and Systems, </journal> <volume> 9(3) </volume> <pages> 319-349, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: The notion of control dependence has been discussed by several authors including Towle [26] and Banerjee [11]. The definition that is most frequently used today is that of Ferrante, Ottenstein, and Warren <ref> [27] </ref>. They assume control-flow graphs with only one sink, that is, a node with no outgoing arcs. Clearly, all control-flow graphs can be represented in this form. In such a graph, a node Y post dominates a node X if all paths from X to the sink include Y . <p> In the recent past, there have been several intermediate language proposals that can be used to represent both control and data dependences in a consistent and convenient manner. The reader is referred to the papers by Ferrante et al. <ref> [27] </ref>, Pingali et al. [29], and Girkar and Polychronopoulos [30] for examples of those intermediate languages. 3 Program Transformation In this section we discuss a collection of parallelization techniques, most of which are either based on dependence analysis or are designed to change the dependence structure of the program to increase
Reference: [28] <author> J. R. Allen and K. Kennedy. </author> <title> Conversion of Control Dependence to Data Dependence. </title> <booktitle> Proceedings of the 10th ACM Symp. on Principles of Programming Languages, </booktitle> <month> Jan. </month> <year> 1983. </year>
Reference-contexts: The transformation is described by Banerjee [11] and by Allen and Kennedy <ref> [28] </ref>. In the recent past, there have been several intermediate language proposals that can be used to represent both control and data dependences in a consistent and convenient manner.
Reference: [29] <author> Keshav Pingali, Micah Beck, Richard Johnson, Mayan Moudgill, and Paul Stodghill. </author> <title> Dependence Flow Graphs: An Algebraic Approach to Program Dependencies. </title> <booktitle> In Proceedings of the 18th ACM Symp. on Principles of Programming Languages, </booktitle> <pages> pages 67-78, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: In the recent past, there have been several intermediate language proposals that can be used to represent both control and data dependences in a consistent and convenient manner. The reader is referred to the papers by Ferrante et al. [27], Pingali et al. <ref> [29] </ref>, and Girkar and Polychronopoulos [30] for examples of those intermediate languages. 3 Program Transformation In this section we discuss a collection of parallelization techniques, most of which are either based on dependence analysis or are designed to change the dependence structure of the program to increase its intrinsic parallelism.
Reference: [30] <author> Milind Girkar and Constantine D. Polychronopoulos. </author> <title> The HTG: An Intermediate Representation for Programs Based on Control and Data Dependences. </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> 3(2) </volume> <pages> 166-178, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: In the recent past, there have been several intermediate language proposals that can be used to represent both control and data dependences in a consistent and convenient manner. The reader is referred to the papers by Ferrante et al. [27], Pingali et al. [29], and Girkar and Polychronopoulos <ref> [30] </ref> for examples of those intermediate languages. 3 Program Transformation In this section we discuss a collection of parallelization techniques, most of which are either based on dependence analysis or are designed to change the dependence structure of the program to increase its intrinsic parallelism.
Reference: [31] <author> Y. Wu and T. Lewis. </author> <title> Parallelizing WHILE loops. </title> <editor> In D. Padua, editor, </editor> <booktitle> Proceedings of the 1990 Int'l. Conf. on Parallel Processing, </booktitle> <address> St. Charles, Ill., </address> <month> August 13-17, </month> <pages> pages 1-8. </pages> <publisher> The Pennsylvania State University Press, </publisher> <year> 1990. </year> <title> Vol. </title> <booktitle> II - Software. </booktitle>
Reference-contexts: A topic not covered in this survey is the parallelization of while loops. The reader is referred to the papers by Wu and Lewis <ref> [31] </ref>, and Harrison [32] for parallelization techniques that apply to this type of construct. In Section 3.3 we discuss program transformations which postpone the decision of what to execute in parallel to execution time. Finally, in Section 3.4 we discuss translation techniques to deal with pointers and recursion.
Reference: [32] <author> W. Ludwell Harrison. </author> <title> Compiling Lisp for Evaluation on a Tightly Coupled Multiprocessor. </title> <type> Technical Report 565, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomputing Res. & Dev., </institution> <month> Mar. </month> <year> 1986. </year>
Reference-contexts: A topic not covered in this survey is the parallelization of while loops. The reader is referred to the papers by Wu and Lewis [31], and Harrison <ref> [32] </ref> for parallelization techniques that apply to this type of construct. In Section 3.3 we discuss program transformations which postpone the decision of what to execute in parallel to execution time. Finally, in Section 3.4 we discuss translation techniques to deal with pointers and recursion.
Reference: [33] <author> E. Dijkstra. </author> <title> Co-operating sequential processes. </title> <editor> In F. Genuys, editor, </editor> <booktitle> Programming Languages, </booktitle> <pages> pages 43-112, </pages> <address> New York, 1968. </address> <publisher> Academic Press. </publisher>
Reference-contexts: Synchronization instructions should be inserted in such a way that the order implied by the data and control dependences is guaranteed to be followed during execution. The parallel code resulting from acyclic code will be represented below by means of the cobegin-coend construct <ref> [33] </ref>, and the post and wait synchronization primitives.
Reference: [34] <author> E. Coffman and P. Denning. </author> <title> Operating Systems Theory. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1973. </year>
Reference-contexts: However, finding a good schedule is in general more difficult. In fact, it is well known that the general problem of finding an optimal schedule is NP-hard <ref> [34] </ref> and, therefore, compile-time scheduling algorithms are usually based on heuristics. 3.1.1 Coarse-Grain Parallelization When the target machine is a conventional multiprocessor, one objective of the acyclic code parallelization techniques is to generate relatively long sequential segments of code or threads to overcome the overhead.
Reference: [35] <author> D. D. Gajski, D. J. Kuck, and D. A. Padua. </author> <title> Dependence Driven Computation. </title> <booktitle> Proceedings of the COMPCON 81 Spring Computer Conf., </booktitle> <pages> pages 168-172, </pages> <month> Feb. </month> <year> 1981. </year>
Reference-contexts: For this reason, the parallelization techniques usually operate on compound statements such as loops, basic blocks, and sequences of these two. Furthermore, it is sometimes better to leave some of the scheduling decisions to the run-time system, especially when the statement execution time cannot be estimated at compile time <ref> [35] </ref>. In this case, it may be profitable to generate more parallel components than processors to enhance the load balance between processors and, as a consequence, decrease execution time. It is not always convenient to generate a pair of synchronization operations for each dependence relation.
Reference: [36] <author> S. Midkiff and D. Padua. </author> <title> Compiler algorithm for synchronization. </title> <journal> IEEE Trans. Comput., </journal> <volume> C-36(12):1485-1495, </volume> <year> 1987. </year>
Reference-contexts: It is not always convenient to generate a pair of synchronization operations for each dependence relation. This naive approach usually leads to the generation of unnecessary operations <ref> [36] </ref> because two statements may be ordered by more than one collection of dependences. Avoiding redundant control and data dependences may reduce not only the number of synchronization operations, but also the complexity of the boolean expressions in some of the resulting if statements. <p> However, some of the syn (a) (a) Iteration space of the original loop; and (b) iteration space after skewing. chronization operations could be redundant. Techniques to avoid this redundancy are described in <ref> [75, 36, 76] </ref>. Another approach to avoid unnecessary synchronization operations is to skew the loop body to decrease the number of cross-iteration dependences. This technique, also called alignment, is described in [74, 36, 77]. <p> Techniques to avoid this redundancy are described in [75, 36, 76]. Another approach to avoid unnecessary synchronization operations is to skew the loop body to decrease the number of cross-iteration dependences. This technique, also called alignment, is described in <ref> [74, 36, 77] </ref>. Example 19 Consider the following loop: do I = 0; N S 2 : C (I) = A (I 1) + 1 enddo Its iteration dependence graph is shown in Fig. 12. Horizontal parallelization could be applied to this program, but this would require synchronization.
Reference: [37] <author> H. Kasahara, H. Honda, A. Mogi, A. Ogura, K. Fujiwara, and S. Narita. </author> <title> A multi-grain parallelizing compilation scheme for OSCAR (optimally scheduled advanced multiprocessor). </title> <editor> In U. Banerjee, D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, Lecture Notes in Computer Science, </booktitle> <volume> No. </volume> <pages> 589. </pages> <address> New York: </address> <publisher> Springer-Verlag, </publisher> <pages> pages 283-297, </pages> <year> 1992. </year>
Reference-contexts: Avoiding redundant control and data dependences may reduce not only the number of synchronization operations, but also the complexity of the boolean expressions in some of the resulting if statements. Techniques to avoid redundant dependences in acyclic code have been studied by Kasahara et al. <ref> [37] </ref> and Girkar and Polychronopoulos [38]. 3.1.2 Instruction Level Transformations|Code Compaction The great importance of the techniques for the extraction of instruction-level parallelism arises from today's widespread use of superscalar and VLIW processors and from the difficulty associated with the explicit parallel programming of such machines.
Reference: [38] <author> M. Girkar and C. Polychronopoulos. </author> <title> Optimization of data/control conditions in task graphs. </title> <editor> In U. Banerjee, D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, Lecture Notes in Computer Science, </booktitle> <volume> No. </volume> <pages> 589. </pages> <address> New York: </address> <publisher> Springer-Verlag, </publisher> <pages> pp. 152-168, </pages> <year> 1992. </year>
Reference-contexts: Techniques to avoid redundant dependences in acyclic code have been studied by Kasahara et al. [37] and Girkar and Polychronopoulos <ref> [38] </ref>. 3.1.2 Instruction Level Transformations|Code Compaction The great importance of the techniques for the extraction of instruction-level parallelism arises from today's widespread use of superscalar and VLIW processors and from the difficulty associated with the explicit parallel programming of such machines.
Reference: [39] <author> J. Fisher. </author> <title> Trace scheduling: A technique for global microcode compaction. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-30(7):478-490, </volume> <month> July </month> <year> 1981. </year>
Reference-contexts: The rest of this section discusses transformations on sequences of macronodes which rearrange operations and if statements to shorten or compact the program graph and thereby speed up execution. Trace Scheduling Early instruction-level parallelization techniques confined their activities to basic blocks. Trace scheduling was developed by Fisher <ref> [39] </ref> and was the first technique to operate across conditional jumps and jump targets enhancing in this way the process of parallelization by increasing the length of the sequence to be parallelized. Trace scheduling is discussed in detail by Fisher et al. [40], Ellis [41], and Colwell et al. [42].
Reference: [40] <author> J. Fisher, J. Ellis, J. Ruttenberg, and A. Nicolau. </author> <title> Parallel processing: A smart compiler and a dumb machine. </title> <booktitle> In Proceedings of the 1984 SIGPLAN Symp. on Compiler Construction, </booktitle> <pages> pages 37-47, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: Trace scheduling was developed by Fisher [39] and was the first technique to operate across conditional jumps and jump targets enhancing in this way the process of parallelization by increasing the length of the sequence to be parallelized. Trace scheduling is discussed in detail by Fisher et al. <ref> [40] </ref>, Ellis [41], and Colwell et al. [42]. A formal definition of trace scheduling and discussions of its correctness, termination, and incremental updating of dependence information is presented by Nicolau [43].
Reference: [41] <author> J. Ellis. Bulldog: </author> <title> A Compiler for VLIW Architectures. </title> <type> PhD thesis, </type> <institution> Yale University, Department of Computer Science, </institution> <month> February </month> <year> 1985. </year>
Reference-contexts: Trace scheduling is discussed in detail by Fisher et al. [40], Ellis <ref> [41] </ref>, and Colwell et al. [42]. A formal definition of trace scheduling and discussions of its correctness, termination, and incremental updating of dependence information is presented by Nicolau [43]. Trace scheduling uses information on the probability that the program would follow a given branch of a conditional jump 5 . <p> This study also con 8. A reasonable restriction given that no global|i.e., beyond basic block boundaries|instruction-level parallelization techniques had been yet developed at the time. 27 firmed the previous results regarding the small speedups achievable within basic blocks. In more recent studies Ellis <ref> [41] </ref> and Lam [63] have taken into account the development of global instruction-level parallelization techniques. The former effort utilized trace-scheduling in the context of simulated VLIW architectures and achieved speedups of over 10-fold over sequential code.
Reference: [42] <author> R. Colwell, R. Nix, J. O'Donnell, D. Papworth, and P. Rod-man. </author> <title> A VLIW architecture for a trace scheduling compiler. </title> <booktitle> In Proceedings of the Second Int'l. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1987. </year>
Reference-contexts: Trace scheduling is discussed in detail by Fisher et al. [40], Ellis [41], and Colwell et al. <ref> [42] </ref>. A formal definition of trace scheduling and discussions of its correctness, termination, and incremental updating of dependence information is presented by Nicolau [43]. Trace scheduling uses information on the probability that the program would follow a given branch of a conditional jump 5 . <p> Average speedups of 11-fold over sequential execution were obtained, given sufficient resources. Perhaps the most robust results to date, using state-of-the-art compilation techniques for a relatively large instruction-level machine, come from the Multiflow Trace by Colwell et. al. <ref> [42] </ref>. This paper reports 5-6 fold speedups on full scientific applications on a 7-functional-unit Trace machine using their trace-scheduling compiler. This speedup was relative to a Vax 8700 10 .
Reference: [43] <author> A. Nicolau. </author> <title> Parallelism, Memory Anti-Aliasing, and Correctness for Trace Scheduling Compilers. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Yale University, </institution> <year> 1984. </year>
Reference-contexts: Trace scheduling is discussed in detail by Fisher et al. [40], Ellis [41], and Colwell et al. [42]. A formal definition of trace scheduling and discussions of its correctness, termination, and incremental updating of dependence information is presented by Nicolau <ref> [43] </ref>. Trace scheduling uses information on the probability that the program would follow a given branch of a conditional jump 5 . The most probable path or trace through the code is selected and parallelized subject only to the restrictions imposed by the data dependences.
Reference: [44] <author> J. L. Linn. </author> <title> Srdag CompAction: a generalization of trace scheduling to increase the use of global context information. </title> <booktitle> In Proceedings of the 16th Annual Workshop on Microprogramming, </booktitle> <month> Oct. </month> <year> 1983. </year>
Reference-contexts: Trace Scheduling is intrinsically designed around the assumptions that conditional jump directions are statically predictable most of the time. An early technique that generalized trace scheduling by enhancing its ability to deal with conditional jumps, SRDAG Compaction, is described by Linn <ref> [44] </ref>. Another technique is region scheduling, introduced by Gupta and Soffa [45]. It uses the program dependence graph to perform large, non local code motions in a rel 5.
Reference: [45] <author> R. Gupta and M. Soffa. </author> <title> Region scheduling. </title> <booktitle> In Proceedings of the 2nd Int'l. Conf. on Supercomputing, </booktitle> <month> May </month> <year> 1987. </year>
Reference-contexts: An early technique that generalized trace scheduling by enhancing its ability to deal with conditional jumps, SRDAG Compaction, is described by Linn [44]. Another technique is region scheduling, introduced by Gupta and Soffa <ref> [45] </ref>. It uses the program dependence graph to perform large, non local code motions in a rel 5. These probabilities may be computed heuristically, or based on profiling information. 10 atively inexpensive way once the dependence graph has been computed.
Reference: [46] <author> Y. Patt and W. W. Hwu. </author> <title> HPSm a High Performance Restricted Data Flow Architecture Having Minimal Functionality. </title> <booktitle> In Conference Proceedings of the 13th Annual Symposium on Computer Architecture, </booktitle> <pages> page 297, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: Also, the motion of regions as a whole may create more code duplication than strictly necessary. Patt and Hwu <ref> [46] </ref> have designed an architecture, HPS, that attempts to utilize small-scale data-flow techniques (within a window of limited size) to dynamically dispatch operations, while utilizing instruction-level compiler technology to reorder the code to increase the number of independent instructions within each window.
Reference: [47] <author> P. P. Chang, S. A. Mahlke, W. Y. Chen, N. J. Warter, and W. W. Hwu. </author> <title> IMPACT: An Architectural Framework for Multiple Instruction Issue Processors. </title> <booktitle> In Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 266-, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: More recently, Chang et al. <ref> [47] </ref> studied means of improving commercial architectures (e.g., RS6000,i860) to make better use of instruction-level parallelization techniques. Percolation Scheduling Percolation scheduling was developed by Nicolau from the work on trace scheduling [48, 49].
Reference: [48] <author> A. Nicolau. </author> <title> Uniform parallelism exploitation in ordinary programs. </title> <booktitle> In Proceedings of the Int'l. Conf. on Parallel Processing, </booktitle> <pages> pages 614-618, </pages> <month> August </month> <year> 1985. </year>
Reference-contexts: More recently, Chang et al. [47] studied means of improving commercial architectures (e.g., RS6000,i860) to make better use of instruction-level parallelization techniques. Percolation Scheduling Percolation scheduling was developed by Nicolau from the work on trace scheduling <ref> [48, 49] </ref>. It is based on three elementary transformations which can be combined to create an effective set of parallelizing transformations. These transformations are driven by heuristics which may depend on the target machine and the nature of the source programs. The three elementary transformations are move-op, move-cj, and unify.
Reference: [49] <author> A. Aiken and A. Nicolau. </author> <title> A development environment for horizontal microcode. </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> 14(5) </volume> <pages> 584-594, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: More recently, Chang et al. [47] studied means of improving commercial architectures (e.g., RS6000,i860) to make better use of instruction-level parallelization techniques. Percolation Scheduling Percolation scheduling was developed by Nicolau from the work on trace scheduling <ref> [48, 49] </ref>. It is based on three elementary transformations which can be combined to create an effective set of parallelizing transformations. These transformations are driven by heuristics which may depend on the target machine and the nature of the source programs. The three elementary transformations are move-op, move-cj, and unify. <p> (K 1)) coend cobegin S 2 : B (K) = F 2 (A (K); C (K 1)) S 4 : D (K 1) = F 4 (D (K 1); C (K 1)) coend enddo Notice that this code is slightly different from the one obtained by skewing. 2 Perfect pipelining <ref> [57, 49] </ref>, when applied to loops which, like that in Example 13, do not contain conditional statements, has been proven to generate optimal code. The optimality is subject only to the availabil 16 ity of sufficient resources, and limited by the depen-dences of the initial loop.
Reference: [50] <author> A. Aiken. </author> <title> Compaction-Based Parallelization. </title> <type> PhD thesis, </type> <institution> Cornell University, Department of Computer Science, </institution> <year> 1988. </year>
Reference-contexts: cobegin y = y 1 k if true then goto B coend S 8 : cobegin a = x + 1 k if true then goto B coend 2 Formal definitions of the transformations, as well as proofs of correctness, termination, and completeness of percolation scheduling are discussed by Aiken <ref> [50] </ref>. A slightly different implementation of the transformations is described by Ebcioglu [51]. It is worth pointing out that in percolation scheduling and in trace scheduling, data-dependence information is computed when needed in the course of the transformations. <p> The flow information used (live-dead and reaching definitions) are initially computed and dynamically updated as part of the percolation transformations. Also, it is possible to compose a compaction algorithm based on the three elementary operations that subsumes the effect of trace scheduling <ref> [50] </ref>.
Reference: [51] <author> K. Ebcioglu. </author> <title> A compilation technique for software pipelin-ing of loops with conditional jumps. </title> <booktitle> In Proceedings of the 20th Annual Workshop on Microprogramming, </booktitle> <pages> pages 69-79, </pages> <month> December </month> <year> 1987. </year>
Reference-contexts: A slightly different implementation of the transformations is described by Ebcioglu <ref> [51] </ref>. It is worth pointing out that in percolation scheduling and in trace scheduling, data-dependence information is computed when needed in the course of the transformations. The flow information used (live-dead and reaching definitions) are initially computed and dynamically updated as part of the percolation transformations. <p> Another early approach to software pipelining, Modulo Scheduling, was proposed by Rau and Glaeser [62]. These techniques were limited to loops without tests. Lam [63, 64] integrates within Modulo Scheduling, heuristics for resource constraints with a limited form of conditional-handling. An alternative approach to perfect pipelining due to Ebcioglu <ref> [51] </ref> has the potential for faster compilation time at the expense of optimality. Still another approach is discussed by Su et.al. [65]; it operates by pipelining individual paths using a compaction technique similar to trace scheduling.
Reference: [52] <author> K. Ebcioglu and A. Nicolau. </author> <title> A global resource-constrained parallelizationtechnique. </title> <booktitle> In Proceedings of ACM SIGARCH ICS-89: Int'l. Conf. on Supercomputing, </booktitle> <address> Crete, Greece, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: It is relatively easy to incorporate resource constrained heuristics, register allocation, 13 and pipelined operations as well as other transforma-tions such as renaming and tree height reduction within the percolation scheduling framework as discussed by Ebcioglu and Nicolau <ref> [52] </ref> and by Potasman [53]. 3.2 Parallelization of DO Loops Because of their importance in the typical supercomputer workload, the discussion of do loops dominates the literature on automatic parallelization.
Reference: [53] <author> R. Potasman. </author> <title> Percolation-Based Compiling for Evaluation of Parallelism and Hardware Design Tradeoffs. </title> <type> PhD thesis, </type> <institution> Department of Information and Computer Science, University of Calif., Irvine, </institution> <month> December </month> <year> 1991. </year>
Reference-contexts: It is relatively easy to incorporate resource constrained heuristics, register allocation, 13 and pipelined operations as well as other transforma-tions such as renaming and tree height reduction within the percolation scheduling framework as discussed by Ebcioglu and Nicolau [52] and by Potasman <ref> [53] </ref>. 3.2 Parallelization of DO Loops Because of their importance in the typical supercomputer workload, the discussion of do loops dominates the literature on automatic parallelization. In fact, do loops are the only construct that most of today's compilers attempt to parallelize whenever the objective is to exploit coarse-grain parallelism. <p> In a related paper Nakatani and Ebcioglu [140] showed that average speedups of 5.4-fold could still be obtained in systems and AI codes, even when percolation of operations is limited to a relatively small (moving) window in order to reduce code explosion and compilation-time. In an independent effort Potasman <ref> [53] </ref> evaluated the effect of percolation scheduling used in conjunction with software pipelining and various auxiliary techniques (e.g., renaming) on a variety of kernels from numerical as well as systems codes. Average speedups of 11-fold over sequential execution were obtained, given sufficient resources.
Reference: [54] <author> Constantine Polychronopoulos. </author> <title> Compiler Optimizations for Enhancing Parallelism and Their Impact on Architecture Design. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-37(8):991-1004, </volume> <month> August </month> <year> 1988. </year>
Reference-contexts: 1 (A (I 2); C (I + 1)) 2 : D (I + 1) = F 2 (A (I + 1); D (I 1)) coend enddo 2 A generalization of the technique used in this last example, which also works for the case of multiple loops, was developed by Polychronopoulos <ref> [54] </ref> under the name of cycle shrinking. Loop unrolling has also been applied in conjunction with forward substitution to increase parallelism of the loop body.
Reference: [55] <author> D. J. Kuck. </author> <title> The Structure of Computers and Computations,, volume I. </title> <publisher> John Wiley, </publisher> <address> NY, </address> <year> 1978. </year>
Reference-contexts: Clearly, such a substitution is only done when it does not change the outcome of the program. Forward substitution increases the length of the right-hand side of assignment statements and usually enhances the opportunities for paralleliza-tion, especially if tree-height reduction is applied <ref> [55] </ref>. Tree-height reduction techniques use associativity, com-mutativity, and distributivity to decrease the height of an expression tree and therefore decrease the best parallel execution time of an expression.
Reference: [56] <author> D. Kuck, P. Budnik, S-C. Chen, Jr. E. Davis, J. Han, P. Kraska, D. Lawrie, Y. Muraoka, R. Strebendt, and R. Towle. </author> <title> Measurements of Parallelism in Ordinary FORTRAN Programs. </title> <journal> Computer, </journal> <volume> 7(1) </volume> <pages> 37-46, </pages> <month> January </month> <year> 1974. </year>
Reference-contexts: In the first version of Parafrase, forward substitution and tree-height reduction were used in conjunction with loop unrolling to parallelize loops with loop-carried dependences <ref> [56] </ref>. This approach, however, has been abandoned, and today forward substitution is used mostly 15 to help expose the nature of array subscripts in order to allow a more accurate dependence analysis. <p> The following reports contribute to this goal. In an early study, Kuck et. al. <ref> [56] </ref> have determined the parallelism available in a set of algorithms. Their analyzer detects parallelism in do loops and parallelism from tree height reduction. The authors conclude that there is a potential average speedup of about 10. <p> Third and fourth line: Improvements over serial program execution on Alliant FX8 [153]. Fifth line: manual improvements over serial program execution on Alliant FX8 [154] Study Test Suite Measures Machines Compilers K A P V N T S I F <ref> [56] </ref> x x simulated [149] x x x x x Cyber 203/5 FTN200, KAP, VAST [158] x x simulated Parafrase [155] x x x simulated Parafrase [144, 143] x x x see Table 1 [150] x x x x Cyber 205 FTN200, KAP, VAST [145] x x see Table 1 [151]
Reference: [57] <author> A. Aiken and A. Nicolau. </author> <title> Perfect pipelining: A new loop parallelization technique. </title> <booktitle> In Proceedings of the 1988 Eu-ropean Symp. on Programming, </booktitle> <pages> pages 221-235. </pages> <editor> Springer-Verlag, </editor> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> No. 300, </volume> <month> March </month> <year> 1988. </year> <note> Also available as Cornell Technical Report TR 87-873. </note>
Reference-contexts: (K 1)) coend cobegin S 2 : B (K) = F 2 (A (K); C (K 1)) S 4 : D (K 1) = F 4 (D (K 1); C (K 1)) coend enddo Notice that this code is slightly different from the one obtained by skewing. 2 Perfect pipelining <ref> [57, 49] </ref>, when applied to loops which, like that in Example 13, do not contain conditional statements, has been proven to generate optimal code. The optimality is subject only to the availabil 16 ity of sufficient resources, and limited by the depen-dences of the initial loop. <p> On the other hand the skewing technique discussed above does not always produce optimal parallel code. Perfect pipelining produces optimal schedules even when the source loops contain conditional jumps <ref> [57] </ref>, subject to the same conditions, plus the limitations of the compaction algorithm employed 6 . The literature on software pipelining is extensive. This technique was applied by hand by microprogrammers for decades [59].
Reference: [58] <author> U. Schwiegelshohn, F. Gasperoni, and K. Ebcioglu. </author> <title> On Optimal Loop Parallelization. </title> <type> Technical report. </type> <institution> IBM T.J. Wat-son Research Center, </institution> <address> Yorktown Hts., NY, </address> <year> 1989. </year>
Reference-contexts: These constraints are necessary for convergence in the presence of conditional jumps, and are minimal in the sense that better results (i.e., absolute optimal software pipelining) are impossible to guarantee in general for such code <ref> [58] </ref>. enddo 2 We can represent the dependence relation in a distributed loop as an iteration dependence graph where the statement instances in the jth loop are shifted to the right (j 1) fl (N + 1) positions, where N is the upper limit of the original, normalized loop.
Reference: [59] <author> P. Kogge. </author> <title> The microprogramming of pipelined processors. </title> <booktitle> In Proceedings of the 4th Annual Int'l. Symp. on Computer Architecture, </booktitle> <year> 1977. </year>
Reference-contexts: Perfect pipelining produces optimal schedules even when the source loops contain conditional jumps [57], subject to the same conditions, plus the limitations of the compaction algorithm employed 6 . The literature on software pipelining is extensive. This technique was applied by hand by microprogrammers for decades <ref> [59] </ref>. An algorithm based on the first semiautomatic software pipelining technique [60], was implemented in an FPS compiler [61]. Another early approach to software pipelining, Modulo Scheduling, was proposed by Rau and Glaeser [62]. These techniques were limited to loops without tests.
Reference: [60] <author> A. Charlesworth. </author> <title> An approach to scientific array processing: The architectural design of the AP-120b/FPS-164 family. </title> <journal> IEEE Computer, </journal> <volume> 14(3) </volume> <pages> 18-27, </pages> <year> 1981. </year>
Reference-contexts: The literature on software pipelining is extensive. This technique was applied by hand by microprogrammers for decades [59]. An algorithm based on the first semiautomatic software pipelining technique <ref> [60] </ref>, was implemented in an FPS compiler [61]. Another early approach to software pipelining, Modulo Scheduling, was proposed by Rau and Glaeser [62]. These techniques were limited to loops without tests. Lam [63, 64] integrates within Modulo Scheduling, heuristics for resource constraints with a limited form of conditional-handling.
Reference: [61] <author> R. Touzeau. </author> <title> A Fortran compiler for the FPS-164 scientific computer. </title> <booktitle> In Proceedings of the 1984 ACM SIGPLAN Symp. on Compiler Construction, </booktitle> <pages> pages 48-57, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: The literature on software pipelining is extensive. This technique was applied by hand by microprogrammers for decades [59]. An algorithm based on the first semiautomatic software pipelining technique [60], was implemented in an FPS compiler <ref> [61] </ref>. Another early approach to software pipelining, Modulo Scheduling, was proposed by Rau and Glaeser [62]. These techniques were limited to loops without tests. Lam [63, 64] integrates within Modulo Scheduling, heuristics for resource constraints with a limited form of conditional-handling.
Reference: [62] <author> B. Rau and C. Glaeser. </author> <title> Some scheduling techniques and an easily schedulable horizontal architecture for high performance scientific computing. </title> <booktitle> In Proceedings of the 14th 34 Annual Workshop on Microprogramming, </booktitle> <pages> pages 183-198, </pages> <month> October </month> <year> 1981. </year>
Reference-contexts: This technique was applied by hand by microprogrammers for decades [59]. An algorithm based on the first semiautomatic software pipelining technique [60], was implemented in an FPS compiler [61]. Another early approach to software pipelining, Modulo Scheduling, was proposed by Rau and Glaeser <ref> [62] </ref>. These techniques were limited to loops without tests. Lam [63, 64] integrates within Modulo Scheduling, heuristics for resource constraints with a limited form of conditional-handling. An alternative approach to perfect pipelining due to Ebcioglu [51] has the potential for faster compilation time at the expense of optimality.
Reference: [63] <author> M. Lam. </author> <title> A Systolic Array Optimizing Compiler. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Carnegie Mellon University, </institution> <year> 1987. </year>
Reference-contexts: An algorithm based on the first semiautomatic software pipelining technique [60], was implemented in an FPS compiler [61]. Another early approach to software pipelining, Modulo Scheduling, was proposed by Rau and Glaeser [62]. These techniques were limited to loops without tests. Lam <ref> [63, 64] </ref> integrates within Modulo Scheduling, heuristics for resource constraints with a limited form of conditional-handling. An alternative approach to perfect pipelining due to Ebcioglu [51] has the potential for faster compilation time at the expense of optimality. <p> This study also con 8. A reasonable restriction given that no global|i.e., beyond basic block boundaries|instruction-level parallelization techniques had been yet developed at the time. 27 firmed the previous results regarding the small speedups achievable within basic blocks. In more recent studies Ellis [41] and Lam <ref> [63] </ref> have taken into account the development of global instruction-level parallelization techniques. The former effort utilized trace-scheduling in the context of simulated VLIW architectures and achieved speedups of over 10-fold over sequential code. The latter effort performed extensive experiments with software pipelining and hierarchical reduction 9 on the Warp machine.
Reference: [64] <author> M. Lam. </author> <title> Software pipelining: An effective scheduling technique for VLIW machines. </title> <booktitle> In Proceedings of the SIG-PLAN'88 Conf. on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1988. </year>
Reference-contexts: An algorithm based on the first semiautomatic software pipelining technique [60], was implemented in an FPS compiler [61]. Another early approach to software pipelining, Modulo Scheduling, was proposed by Rau and Glaeser [62]. These techniques were limited to loops without tests. Lam <ref> [63, 64] </ref> integrates within Modulo Scheduling, heuristics for resource constraints with a limited form of conditional-handling. An alternative approach to perfect pipelining due to Ebcioglu [51] has the potential for faster compilation time at the expense of optimality.
Reference: [65] <author> B. Su, S. Ding, and J. Xia. </author> <title> GURPR|a method for global software pipelining. </title> <booktitle> In Proceedings of 20th Annual Workshop on Microprogramming, </booktitle> <pages> pages 88-96, </pages> <month> December </month> <year> 1987. </year>
Reference-contexts: Lam [63, 64] integrates within Modulo Scheduling, heuristics for resource constraints with a limited form of conditional-handling. An alternative approach to perfect pipelining due to Ebcioglu [51] has the potential for faster compilation time at the expense of optimality. Still another approach is discussed by Su et.al. <ref> [65] </ref>; it operates by pipelining individual paths using a compaction technique similar to trace scheduling. Generating Multiple Sequential Loops The second type of technique that generates heterogeneous parallel code transforms serial do loops into two or more serial loops that execute in parallel with each other.
Reference: [66] <author> Yoichi Muraoka. </author> <title> Parallelism Exposure and Exploitation in Programs. </title> <type> PhD thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, Dept. of Computer Sci., </institution> <month> Feb. </month> <year> 1971. </year>
Reference-contexts: Generating Multiple Sequential Loops The second type of technique that generates heterogeneous parallel code transforms serial do loops into two or more serial loops that execute in parallel with each other. The technique is based on a transformation called loop distribution, developed by Muraoka <ref> [66] </ref>, and also described by Banerjee et al. [67] which partitions the statements in the loop body into a sequence of subsequences and creates a separate loop for each subsequence. Example 14 Consider the loop of Example 11.
Reference: [67] <author> U. Banerjee, S. C. Chen, D. J. Kuck, and R. A. Towle. </author> <title> Time and Parallel Processor Bounds for Fortran-Like Loops. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-28(9):660-670, </volume> <month> Sept. </month> <year> 1979. </year>
Reference-contexts: The technique is based on a transformation called loop distribution, developed by Muraoka [66], and also described by Banerjee et al. <ref> [67] </ref> which partitions the statements in the loop body into a sequence of subsequences and creates a separate loop for each subsequence. Example 14 Consider the loop of Example 11.
Reference: [68] <author> H. Zima. </author> <title> Supercompilers for Parallel and Vector Computers. </title> <publisher> ACM Press, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: This is equivalent to saying that any two statements belonging to a cycle in the statement dependence graph have to belong to the same subsequence, which is the traditional condition presented in the literature <ref> [14, 68] </ref>. Loop distribution in the presence of conditional statements can be done by transforming the control dependences into data dependences as discussed in Section 2.4. This was the approach followed by Parafrase. Another technique to distribute loops with conditional statements is presented by Kennedy and McKinley [69]. <p> Equivalent transformations can of course also be applied to scalars. In fact, several of the existing parallelizers are only capable of expanding or privatizing scalars, and most of the literature on paral-lelizers only discusses the case of scalars <ref> [14, 68] </ref>.
Reference: [69] <author> K. Kennedy and K.S. McKinley. </author> <title> Loop distribution with arbitrary control flow. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <pages> pages 407-417. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1990. </year>
Reference-contexts: Loop distribution in the presence of conditional statements can be done by transforming the control dependences into data dependences as discussed in Section 2.4. This was the approach followed by Parafrase. Another technique to distribute loops with conditional statements is presented by Kennedy and McKinley <ref> [69] </ref>. The last parallelization technique to be described in this section distributes the original loop and generates a thread for each resulting loop [70]. Synchronization instructions are inserted where indicated by the dependences to guarantee correctness.
Reference: [70] <author> D. J. Kuck and D. A. Padua. </author> <title> High-Speed Multiprocessors and Their Compilers. </title> <booktitle> Proceedings of the 1979 Int'l. Conf. on Parallel Processing, </booktitle> <pages> pages 5-16, </pages> <month> Aug. </month> <year> 1979. </year>
Reference-contexts: This was the approach followed by Parafrase. Another technique to distribute loops with conditional statements is presented by Kennedy and McKinley [69]. The last parallelization technique to be described in this section distributes the original loop and generates a thread for each resulting loop <ref> [70] </ref>. Synchronization instructions are inserted where indicated by the dependences to guarantee correctness.
Reference: [71] <author> D. A. Padua, D. J. Kuck, and D. H. Lawrie. </author> <title> High-Speed Multiprocessors and Compilation Techniques. </title> <journal> Special Issue on Parallel Processing, IEEE Trans. on Computers, </journal> <volume> C-29(9):763-776, </volume> <month> Sept. </month> <year> 1980. </year>
Reference-contexts: (0 : N ) The function partialsums (W (0 : N ); A (1)) computes the recurrence A (K 1 ) = A (K 1 1)+W (K 1 ); K 1 = 0; : : : ; N in parallel. 2 The second strategy is discussed by Padua et al. <ref> [71] </ref> and Cytron [72, 73]. It transforms the original loop into a parallel do, and cross-iteration synchronization is inserted to enforce the data dependences. Parallel loops with cross-iteration synchronization are called doacross loops.
Reference: [72] <author> Ronald Gary Cytron. </author> <title> Compile-Time Scheduling and Optimization for Asynchronous Machines. </title> <type> PhD thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, Dept. of Computer Sci., </institution> <month> Oct. </month> <year> 1984. </year>
Reference-contexts: ) The function partialsums (W (0 : N ); A (1)) computes the recurrence A (K 1 ) = A (K 1 1)+W (K 1 ); K 1 = 0; : : : ; N in parallel. 2 The second strategy is discussed by Padua et al. [71] and Cytron <ref> [72, 73] </ref>. It transforms the original loop into a parallel do, and cross-iteration synchronization is inserted to enforce the data dependences. Parallel loops with cross-iteration synchronization are called doacross loops. <p> Also, in the case of transforming into doacross, reordering the statements in the loop body may impact performance. Finding an optimal solution to each of those two problems has been shown to be NP-hard <ref> [74, 72] </ref>. Simple heuristics can be used instead, but this problem has not been studied extensively.
Reference: [73] <author> R. Cytron. </author> <title> Doacross: Beyond vectorization for multiprocessors. </title> <booktitle> In Proceedings of the Int'l. Conf. on Parallel Processing, </booktitle> <pages> pages 836-844, </pages> <year> 1986. </year>
Reference-contexts: ) The function partialsums (W (0 : N ); A (1)) computes the recurrence A (K 1 ) = A (K 1 1)+W (K 1 ); K 1 = 0; : : : ; N in parallel. 2 The second strategy is discussed by Padua et al. [71] and Cytron <ref> [72, 73] </ref>. It transforms the original loop into a parallel do, and cross-iteration synchronization is inserted to enforce the data dependences. Parallel loops with cross-iteration synchronization are called doacross loops.
Reference: [74] <author> D. A. Padua. </author> <title> Multiprocessors: Discussion of Some Theoretical and Practical Problems. </title> <type> PhD thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, Dept. of Computer Sci., </institution> <month> Oct. </month> <year> 1979. </year>
Reference-contexts: Also, in the case of transforming into doacross, reordering the statements in the loop body may impact performance. Finding an optimal solution to each of those two problems has been shown to be NP-hard <ref> [74, 72] </ref>. Simple heuristics can be used instead, but this problem has not been studied extensively. <p> Techniques to avoid this redundancy are described in [75, 36, 76]. Another approach to avoid unnecessary synchronization operations is to skew the loop body to decrease the number of cross-iteration dependences. This technique, also called alignment, is described in <ref> [74, 36, 77] </ref>. Example 19 Consider the following loop: do I = 0; N S 2 : C (I) = A (I 1) + 1 enddo Its iteration dependence graph is shown in Fig. 12. Horizontal parallelization could be applied to this program, but this would require synchronization. <p> For example, the choice between transforming into doacross or applying distributed loop parallelization could be influenced by the way in which the data are allocated. The third and last technique to be discussed in this section is known as partitioning. It was first discussed by Padua <ref> [74] </ref> for single loops. It works by computing the greatest common divisor of the cross-iteration dependence distances.
Reference: [75] <author> Zhiyuan Li and Walid Abu-Sufah. </author> <title> On Reducing Data Synchronization in Multiprocessed Loops. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-36(1):105-109, </volume> <month> Jan. </month> <year> 1987. </year>
Reference-contexts: However, some of the syn (a) (a) Iteration space of the original loop; and (b) iteration space after skewing. chronization operations could be redundant. Techniques to avoid this redundancy are described in <ref> [75, 36, 76] </ref>. Another approach to avoid unnecessary synchronization operations is to skew the loop body to decrease the number of cross-iteration dependences. This technique, also called alignment, is described in [74, 36, 77].
Reference: [76] <author> V. Krothapalli and P. Sadayappan. </author> <title> Removal of redundant dependences in DOACROSS loops with constant dependences. </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 281-289, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: However, some of the syn (a) (a) Iteration space of the original loop; and (b) iteration space after skewing. chronization operations could be redundant. Techniques to avoid this redundancy are described in <ref> [75, 36, 76] </ref>. Another approach to avoid unnecessary synchronization operations is to skew the loop body to decrease the number of cross-iteration dependences. This technique, also called alignment, is described in [74, 36, 77].
Reference: [77] <author> R. Allen, D. Callahan, and K. Kennedy. </author> <title> Automatic decomposition of scientific programs for parallel execution. </title> <booktitle> In Proceedings of the Fourteenth Annual ACM Symp. on Principles of Programming Languages, </booktitle> <pages> pages 63-76, </pages> <month> Jan. </month> <year> 1987. </year>
Reference-contexts: Techniques to avoid this redundancy are described in [75, 36, 76]. Another approach to avoid unnecessary synchronization operations is to skew the loop body to decrease the number of cross-iteration dependences. This technique, also called alignment, is described in <ref> [74, 36, 77] </ref>. Example 19 Consider the following loop: do I = 0; N S 2 : C (I) = A (I 1) + 1 enddo Its iteration dependence graph is shown in Fig. 12. Horizontal parallelization could be applied to this program, but this would require synchronization.
Reference: [78] <author> M. Wolfe. </author> <title> Advanced loop interchanging. </title> <editor> In K. Hwang, S. Jacobs, and E. Swartzlander, editors, </editor> <booktitle> Proceedings of the 1986 Int'l. Conf. on Parallel Processing, </booktitle> <pages> pages 536-543, </pages> <address> St. Charles, Ill., August 1986. </address> <publisher> IEEE Computer Society Press, </publisher> <address> Washington, DC. </address>
Reference-contexts: This method was developed by Steve Chen for the Burroughs Scientific Processor. Loop interchanging is described in detail by Wolfe <ref> [14, 78] </ref>, who also studied how it can be applied to triangular loops. Further discussions on interchanging can be found in the work of Allen and Kennedy [79].
Reference: [79] <author> J. Allen and K. Kennedy. </author> <title> Automatic loop interchange. </title> <booktitle> In Proceedings of the 1984 SIGPLAN Symp. on Compiler Construction, </booktitle> <volume> volume 19, </volume> <pages> pages 233-246, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: This method was developed by Steve Chen for the Burroughs Scientific Processor. Loop interchanging is described in detail by Wolfe [14, 78], who also studied how it can be applied to triangular loops. Further discussions on interchanging can be found in the work of Allen and Kennedy <ref> [79] </ref>. The second technique discussed here is skewing, which is very similar to the technique of the same name presented above for single loops, except that in the present case skewing is uniform along a particular dimension.
Reference: [80] <author> L. Lamport. </author> <title> The parallel execution of DO loops. </title> <journal> Comm. of the ACM, </journal> <volume> 17(2) </volume> <pages> 83-93, </pages> <month> February </month> <year> 1974. </year>
Reference-contexts: loop can be parallelized: do K 1 = 0; 1 S: A (K 2 ; 1 K 1 ) = F 1 (A (K 2 1; 1 K 1 + 1)) enddo enddo 2 The transformations of the iteration dependence graph illustrated above are special cases of Lamport's wavefront method <ref> [80] </ref>.
Reference: [81] <author> U. Banerjee. </author> <title> Unimodular transformations of double loops. </title> <booktitle> In Advances in Languages and Compilers for Parallel Processing,Cambridge, </booktitle> <address> MA: </address> <publisher> MIT Press, </publisher> <pages> pages 192-219, </pages> <year> 1991. </year>
Reference-contexts: Transformations based on operations with unimodular matrices are called unimodular transformations. Unimodular transformations have been studied by Banerjee <ref> [81] </ref> and by Wolf and Lam [82]. Determining the combination of transformations that produces the best code is the main objective of the compiler.
Reference: [82] <author> M.E. Wolf and M. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In SIGPLAN NOTICES: Proceedings of the ACM SIGPLAN 91 Conf. on Programming Language Design and Implementation, </booktitle> <address> Toronto, Ontario, </address> <pages> pages 30-44. </pages> <publisher> ACM Press, </publisher> <month> June </month> <year> 1991. </year>
Reference-contexts: Transformations based on operations with unimodular matrices are called unimodular transformations. Unimodular transformations have been studied by Banerjee [81] and by Wolf and Lam <ref> [82] </ref>. Determining the combination of transformations that produces the best code is the main objective of the compiler. <p> These techniques have been used extensively. For example, they were applied by hand to improve the performance of matrix multiplication on the Alliant FX/80 [91]. Tiling is discussed by Wolfe [92], Irigoin and Tri-olet [93], Ancourt and Irigoin [94], Wolf and Lam <ref> [82] </ref>, and Schreiber and Dongarra [95]. Tiling influences the behavior of the memory hierarchy indirectly by reorganizing the code to increase the effectiveness of pre-defined memory management policy. An alternative strategy is to control directly the movement of data across the different levels of the memory hierarchy.
Reference: [83] <author> W. Shang and J. Fortes. </author> <title> Time optimal linear schedules for algorithms with uniform dependencies. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 40(6) </volume> <pages> 723-742, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Unimodular transformations have been studied by Banerjee [81] and by Wolf and Lam [82]. Determining the combination of transformations that produces the best code is the main objective of the compiler. One strategy to achieve this goal, presented by Shang and Fortes <ref> [83] </ref>, uses linear programming techniques to find the loop reorganization that produces the optimum execution time assuming an unlimited number of processors and ignoring overhead. Other transformations can be applied in addition to combinations of the previously described three transformations to obtain parallel loops. <p> Other transformations can be applied in addition to combinations of the previously described three transformations to obtain parallel loops. For example, partitioning has been extended to multiple loops by Peir and Cytron [84], Shang and Fortes <ref> [83] </ref>, and D'Hollander [85]. Also, multiple loops can be transformed into doacross form, One complication that arises is that there are several possible valid orderings in which the iterations of a parallel do loop can be stored in the scheduling queue.
Reference: [84] <author> J.-K. Peir and R. Cytron. </author> <title> Minimum Distance: A Method for Partitioning Recurrences for Multiprocessors. </title> <editor> In S. K. Sahni, editor, </editor> <booktitle> Proceedings of the 1987 Int'l. Conf. on Parallel Processing, </booktitle> <pages> pages 217-241. </pages> <publisher> Pennsylvania State University Press, </publisher> <year> 1987. </year>
Reference-contexts: Other transformations can be applied in addition to combinations of the previously described three transformations to obtain parallel loops. For example, partitioning has been extended to multiple loops by Peir and Cytron <ref> [84] </ref>, Shang and Fortes [83], and D'Hollander [85]. Also, multiple loops can be transformed into doacross form, One complication that arises is that there are several possible valid orderings in which the iterations of a parallel do loop can be stored in the scheduling queue.
Reference: [85] <author> E. H. D'Hollander. </author> <title> Partitioning and labeling of loops by unimodular transformations. </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> 3(4) </volume> <pages> 465-476, </pages> <year> 1992. </year>
Reference-contexts: Other transformations can be applied in addition to combinations of the previously described three transformations to obtain parallel loops. For example, partitioning has been extended to multiple loops by Peir and Cytron [84], Shang and Fortes [83], and D'Hollander <ref> [85] </ref>. Also, multiple loops can be transformed into doacross form, One complication that arises is that there are several possible valid orderings in which the iterations of a parallel do loop can be stored in the scheduling queue.
Reference: [86] <author> Peiyi Tang, Pen-Chung Yew, and Chuan-Qi Zhu. </author> <title> Impact of Self-Scheduling Orders on Performance. </title> <booktitle> Proceedings of 1988 Int'l. Conf. on Supercomputing, </booktitle> <address> St. Malo, France, </address> <pages> pages 593-603, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: Also, multiple loops can be transformed into doacross form, One complication that arises is that there are several possible valid orderings in which the iterations of a parallel do loop can be stored in the scheduling queue. The ordering has performance implications, as discussed by Tang el al. <ref> [86] </ref>. 3.2.4 Locality Enhancement and Overhead Reduction A number of restructuring techniques deal with the transformation of the iteration space of a loop (or multiple loops) to reduce synchronization overhead and improve data locality. Examples of such techniques are loop fusion [87], loop collapsing [4], loop coalescing [88], and tiling.
Reference: [87] <author> D. Loveman. </author> <title> Program improvement by source-to-source transformation. </title> <journal> Journal of the ACM, </journal> <volume> 24(1) </volume> <pages> 121-145, </pages> <month> Jan. </month> <year> 1977. </year>
Reference-contexts: Examples of such techniques are loop fusion <ref> [87] </ref>, loop collapsing [4], loop coalescing [88], and tiling. Loop fusion transforms two disjoint do loops into a single loop. If both loops are parallel, fusing them decreases the overhead because, for example, only one parallel loop has to be started instead of two. <p> Tiling partitions the iteration dependence of a loop graph into blocks of adjacent nodes. All the blocks have the same size and shape with the possible exception of those at the extremes of the graph. In the case of a single loop, tiling is done by strip-mining <ref> [87] </ref>, a transformation that changes a single loop into a double one. The outer loop steps across the blocks, and the inner loop steps through the elements of the block.
Reference: [88] <author> Constantine Polychronopoulos. </author> <title> Loop Coalescing: A Compiler Transformation for Parallel Machines. </title> <booktitle> Proceedings of 1987 Int'l. Conf. on Parallel Processing, </booktitle> <address> St. Charles, </address> <publisher> Ill., </publisher> <pages> pages 235-242, </pages> <month> August </month> <year> 1987. </year>
Reference-contexts: Examples of such techniques are loop fusion [87], loop collapsing [4], loop coalescing <ref> [88] </ref>, and tiling. Loop fusion transforms two disjoint do loops into a single loop. If both loops are parallel, fusing them decreases the overhead because, for example, only one parallel loop has to be started instead of two.
Reference: [89] <author> W. Abu-Sufah, D. J. Kuck, and D. H. Lawrie. </author> <title> On the Performance Enhancement of Paging Systems Through Program Analysis and Transformations. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-30(5):341-356, </volume> <month> May </month> <year> 1981. </year>
Reference-contexts: If both loops are parallel, fusing them decreases the overhead because, for example, only one parallel loop has to be started instead of two. Loop fusion is also useful to increase data locality as discussed by Abu Sufah et al. <ref> [89] </ref>. Loop collapsing and loop coalescing transform multiple loops into single form. These transformations are useful to enhance vectorization and to improve load balancing at execution time. Tiling partitions the iteration dependence of a loop graph into blocks of adjacent nodes. <p> Their work was extended and developed into automatic strategies by Abu-Sufah et al. <ref> [89] </ref>. These techniques have been used extensively. For example, they were applied by hand to improve the performance of matrix multiplication on the Alliant FX/80 [91]. Tiling is discussed by Wolfe [92], Irigoin and Tri-olet [93], Ancourt and Irigoin [94], Wolf and Lam [82], and Schreiber and Dongarra [95].
Reference: [90] <author> A. McKellar and Jr. E. Coffman. </author> <title> Organizing matrices and matrix operations for paged memory systems. </title> <journal> Comm. of the ACM, </journal> <volume> 12 </volume> <pages> 153-165, </pages> <year> 1974. </year>
Reference-contexts: Now, if the outer loop is parallelized, the number of cache block transfers decreases to 2=IB per assignment. 2 One of the earliest discussions on program transformations to improve locality was presented by McKellar and Coffman <ref> [90] </ref>. Their work was extended and developed into automatic strategies by Abu-Sufah et al. [89]. These techniques have been used extensively. For example, they were applied by hand to improve the performance of matrix multiplication on the Alliant FX/80 [91].
Reference: [91] <author> William Jalby and Ulrike Meier. </author> <title> Optimizing Matrix Operations on a Parallel Multiprocessor with a Memory Hierarchy. </title> <booktitle> Proceedings of the 1986 Int'l. Conf. on Parallel Processing, </booktitle> <address> St. Charles, </address> <publisher> Ill., </publisher> <pages> pages 429-432, </pages> <month> Aug. </month> <year> 1986. </year>
Reference-contexts: Their work was extended and developed into automatic strategies by Abu-Sufah et al. [89]. These techniques have been used extensively. For example, they were applied by hand to improve the performance of matrix multiplication on the Alliant FX/80 <ref> [91] </ref>. Tiling is discussed by Wolfe [92], Irigoin and Tri-olet [93], Ancourt and Irigoin [94], Wolf and Lam [82], and Schreiber and Dongarra [95]. Tiling influences the behavior of the memory hierarchy indirectly by reorganizing the code to increase the effectiveness of pre-defined memory management policy.
Reference: [92] <author> M.J. Wolfe. </author> <title> More iteration space tiling. </title> <booktitle> In Proceedings of Supercomputing '89, </booktitle> <address> Reno, Nevada, </address> <month> November 13-17, </month> <pages> pages 655-664. </pages> <publisher> ACM Press, </publisher> <year> 1989. </year>
Reference-contexts: Their work was extended and developed into automatic strategies by Abu-Sufah et al. [89]. These techniques have been used extensively. For example, they were applied by hand to improve the performance of matrix multiplication on the Alliant FX/80 [91]. Tiling is discussed by Wolfe <ref> [92] </ref>, Irigoin and Tri-olet [93], Ancourt and Irigoin [94], Wolf and Lam [82], and Schreiber and Dongarra [95]. Tiling influences the behavior of the memory hierarchy indirectly by reorganizing the code to increase the effectiveness of pre-defined memory management policy.
Reference: [93] <author> F. Irigoin and R. Triolet. </author> <title> Supernode partitioning. </title> <booktitle> In Proceedings of the Fifteenth Annual ACM Symp. on Principles of Programming Languages, </booktitle> <pages> pages 319-329, </pages> <month> Jan. </month> <year> 1988. </year>
Reference-contexts: Their work was extended and developed into automatic strategies by Abu-Sufah et al. [89]. These techniques have been used extensively. For example, they were applied by hand to improve the performance of matrix multiplication on the Alliant FX/80 [91]. Tiling is discussed by Wolfe [92], Irigoin and Tri-olet <ref> [93] </ref>, Ancourt and Irigoin [94], Wolf and Lam [82], and Schreiber and Dongarra [95]. Tiling influences the behavior of the memory hierarchy indirectly by reorganizing the code to increase the effectiveness of pre-defined memory management policy.
Reference: [94] <author> C. Ancourt and F. Irigoin. </author> <title> Scanning polyhedra with do loops. </title> <booktitle> In Proceedings of the Third ACM SIGPLAN Symp. on Principles and Practice of Parallel Programming (PPOPP), </booktitle> <pages> pages 39-50, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: These techniques have been used extensively. For example, they were applied by hand to improve the performance of matrix multiplication on the Alliant FX/80 [91]. Tiling is discussed by Wolfe [92], Irigoin and Tri-olet [93], Ancourt and Irigoin <ref> [94] </ref>, Wolf and Lam [82], and Schreiber and Dongarra [95]. Tiling influences the behavior of the memory hierarchy indirectly by reorganizing the code to increase the effectiveness of pre-defined memory management policy.
Reference: [95] <author> R. Schreiber and J. Dongarra. </author> <title> Automatic blocking of nested loops. </title> <type> Technical report, </type> <institution> RIACS, </institution> <month> August </month> <year> 1990. </year>
Reference-contexts: These techniques have been used extensively. For example, they were applied by hand to improve the performance of matrix multiplication on the Alliant FX/80 [91]. Tiling is discussed by Wolfe [92], Irigoin and Tri-olet [93], Ancourt and Irigoin [94], Wolf and Lam [82], and Schreiber and Dongarra <ref> [95] </ref>. Tiling influences the behavior of the memory hierarchy indirectly by reorganizing the code to increase the effectiveness of pre-defined memory management policy. An alternative strategy is to control directly the movement of data across the different levels of the memory hierarchy.
Reference: [96] <author> R. Cytron, S. Karlovsky, and K. McAuliffe. </author> <title> Automatic management of programmable caches. </title> <booktitle> Proceedings of 1988 Int'l. Conf. on Parallel Processing, </booktitle> <address> St. Charles, </address> <publisher> Ill., </publisher> <pages> pages 229-238, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: An alternative strategy is to control directly the movement of data across the different levels of the memory hierarchy. Such techniques have been studied by Cytron et al. <ref> [96] </ref>, Gornish et al. [97], Callahan et al. [98], and Darnell et al. [99]. 3.2.5 Dependence Breaking Techniques In this section we discuss the two transformations most frequently used to eliminate cross-iteration dependences. The first eliminates from a loop L all assignments to induction variables.
Reference: [97] <author> Edward H. Gornish, Elana D. Granston, and Alexander V. Veidenbaum. </author> <booktitle> Compiler-directed Data Prefetching in Multiprocessors with Memory Hierarchies . Proceedings of Int'l Conf. on Supercomputing, </booktitle> <volume> 1 </volume> <pages> 342-353, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: An alternative strategy is to control directly the movement of data across the different levels of the memory hierarchy. Such techniques have been studied by Cytron et al. [96], Gornish et al. <ref> [97] </ref>, Callahan et al. [98], and Darnell et al. [99]. 3.2.5 Dependence Breaking Techniques In this section we discuss the two transformations most frequently used to eliminate cross-iteration dependences. The first eliminates from a loop L all assignments to induction variables.
Reference: [98] <author> D. Callahan, K. Kennedy, and A. Porterfield. </author> <title> Software prefetching. </title> <booktitle> In Proceedings of the Fourth Int'l. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 40-52, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: An alternative strategy is to control directly the movement of data across the different levels of the memory hierarchy. Such techniques have been studied by Cytron et al. [96], Gornish et al. [97], Callahan et al. <ref> [98] </ref>, and Darnell et al. [99]. 3.2.5 Dependence Breaking Techniques In this section we discuss the two transformations most frequently used to eliminate cross-iteration dependences. The first eliminates from a loop L all assignments to induction variables.
Reference: [99] <author> E. Darnell, J. M. Mellor-Crummey, and K. Kennedy. </author> <title> Automatic Software Cache Coherence through Vectorization. </title> <booktitle> In Proceeding of Int'l. Conf. on Supercomputing, </booktitle> <pages> pages 129-139, </pages> <year> 1992. </year>
Reference-contexts: An alternative strategy is to control directly the movement of data across the different levels of the memory hierarchy. Such techniques have been studied by Cytron et al. [96], Gornish et al. [97], Callahan et al. [98], and Darnell et al. <ref> [99] </ref>. 3.2.5 Dependence Breaking Techniques In this section we discuss the two transformations most frequently used to eliminate cross-iteration dependences. The first eliminates from a loop L all assignments to induction variables.
Reference: [100] <author> A. Aho and J. Ullman. </author> <title> The Theory of Parsing, Translation, and Compiling. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1973. </year>
Reference-contexts: Notice that our definition of induction variable is more general than the traditional one <ref> [100] </ref>, which is restricted to the case where Example 28 Consider the loop: do I = 0; N S 1 : K = K + 1 enddo enddo In this loop, K is an induction variable because it is computed using the equation K i;j = K i;j1 + 1; j
Reference: [101] <author> Rudolf Eigenmann, Jay Hoeflinger, Zhiyuan Li, and David Padua. </author> <title> Experience in the Automatic Parallelization of Four Perfect-Benchmark Programs. </title> <booktitle> Languages and Compilers for Parallel Computing, Lecture notes in Computer Science, </booktitle> <volume> No. </volume> <pages> 589, </pages> <address> New York: </address> <publisher> Springer-Verlag, </publisher> <address> pp.65-83, </address> <year> 1992. </year>
Reference-contexts: One way to overcome this difficulty is to determine some important properties (such as monotonicity) of the sequence of values assumed by the induction variable by analyzing the original assignments to the induction variables <ref> [101] </ref>. Techniques to recognize induction variables and other forms of recurrences have been presented by Ammar-guellat and Harrison [102], Wolfe [103], and Haghighat and Polychronopoulos [104]. <p> do I = 0; N A (M + K fl I) = B (I) enddo end if Other cases more complex than the previous example may arise, and in some of these it is profitable to apply at run time some of the dependence tests described in Section 2. (See <ref> [101] </ref> for an example.) Multiple-version loops similar to the one used in the previous examples can be controlled by run-time dependence tests but also by other dynamic factors mentioned above. <p> In an early study, Kuck et. al. [56] have determined the parallelism available in a set of algorithms. Their analyzer detects parallelism in do loops and parallelism from tree height reduction. The authors conclude that there is a potential average speedup of about 10. Eigenmann et al. <ref> [101, 154] </ref> conducted "manual compilation" experiments to determine new transformation techniques that significantly improve the performance of real programs. They hand-optimized the Perfect Benchmarks for the Alliant FX/80 and the Cedar machine. The speedups obtained are shown in Table 4. <p> However, there is room for studies that evaluate this technology more comprehensively. There exists evidence for potential improvements of parallelizing compilers. It was given by analyzing real program patterns and deriving new compiler capabilities [159], by optimizing programs manually and discussing the automatability of the transformations applied <ref> [101, 154] </ref>, and by comparing "best" parallelism that one found by compilers [157] and deriving new restructuring capabilities. <p> FTN200, KAP, VAST [145] x x see Table 1 [151] x x NAS 160 KAP, VAST [148] x x x x see Table 2 [153] x x x x x Alliant FX/8 KAP, VAST [152] x x x x Cray Y-MP KAP, fpp [156] x x x Alliant FX/8 VAST <ref> [101, 154] </ref> x x x x FX/8, Cedar KAP [157] x x x simulated KAP test suite: K=Kernels A=Algorithms P=Application programs measures : V=shows rate of successfully vectorized loops N=compares performance numbers of different compilers T=compares transformations of different compilers S=shows speedups due to automatic parallelization I=evaluates individual compiler techniques F=discusses
Reference: [102] <author> Zahira Ammarguellat and Luddy Harrison. </author> <title> Automatic Recognition of Induction & Recurrence Relations by Ab 35 stract Interpretation. </title> <booktitle> Proceedings of Sigplan 1990, </booktitle> <address> York--town Heights N.Y., </address> <booktitle> Sigplan Notices, </booktitle> <volume> 25(6) </volume> <pages> 283-295, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Techniques to recognize induction variables and other forms of recurrences have been presented by Ammar-guellat and Harrison <ref> [102] </ref>, Wolfe [103], and Haghighat and Polychronopoulos [104]. The second type of transformation to be discussed in this section operates on variables or arrays that are rewritten on each loop iteration before they are fetched in the same iteration.
Reference: [103] <author> Michael Wolfe. </author> <title> Beyond Induction Variables. </title> <booktitle> In Proceedings of the ACM SIGPLAN'92 Conf. on Programming Language Design and Implementation, </booktitle> <pages> pages 162-174, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: Techniques to recognize induction variables and other forms of recurrences have been presented by Ammar-guellat and Harrison [102], Wolfe <ref> [103] </ref>, and Haghighat and Polychronopoulos [104]. The second type of transformation to be discussed in this section operates on variables or arrays that are rewritten on each loop iteration before they are fetched in the same iteration.
Reference: [104] <author> Mohammad R. Haghighat and Constantine D. Poly-chronopoulos. </author> <title> Symbolic Program Analysis and Optimization for Parallelizing Compilers. </title> <booktitle> In Conference Record of the 5th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <institution> Yale University, Department of Computer Science, </institution> <year> 1992. </year>
Reference-contexts: Techniques to recognize induction variables and other forms of recurrences have been presented by Ammar-guellat and Harrison [102], Wolfe [103], and Haghighat and Polychronopoulos <ref> [104] </ref>. The second type of transformation to be discussed in this section operates on variables or arrays that are rewritten on each loop iteration before they are fetched in the same iteration.
Reference: [105] <author> P. Feautrier. </author> <title> Array expansion. </title> <booktitle> In Proceedings of the Int'l. Conf. on Supercomputing '88, </booktitle> <pages> pages 429-441, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: In fact, several of the existing parallelizers are only capable of expanding or privatizing scalars, and most of the literature on paral-lelizers only discusses the case of scalars [14, 68]. However, array privatization is very important for the effective parallelization of many real programs. (See the papers by Feautrier <ref> [105] </ref>, Maydan et al. [106], Tu and Padua [107], and Li [108] for array privatization and expansion techniques.) Burke et al. [109] discuss the use of privatization for the parallelization of acyclic code. 3.3 Run-Time Decisions There are decisions that are difficult or impossible to make at compile time.
Reference: [106] <author> D.E. Maydan, S.P. Amarasinghe, and M.S. Lam. </author> <title> Data dependence and data-flow analysis of arrays. </title> <booktitle> In Conference Record of the 5th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 283-292. </pages> <institution> Yale University. Department of Computer Science. YALEU/DCS/RR-915, </institution> <year> 1992. </year>
Reference-contexts: However, array privatization is very important for the effective parallelization of many real programs. (See the papers by Feautrier [105], Maydan et al. <ref> [106] </ref>, Tu and Padua [107], and Li [108] for array privatization and expansion techniques.) Burke et al. [109] discuss the use of privatization for the parallelization of acyclic code. 3.3 Run-Time Decisions There are decisions that are difficult or impossible to make at compile time.
Reference: [107] <author> Peng Tu and David Padua. </author> <title> Automatic Array Privatization. </title> <booktitle> Proc. Sixth Annual Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August 12-14, </month> <year> 1993. </year>
Reference-contexts: However, array privatization is very important for the effective parallelization of many real programs. (See the papers by Feautrier [105], Maydan et al. [106], Tu and Padua <ref> [107] </ref>, and Li [108] for array privatization and expansion techniques.) Burke et al. [109] discuss the use of privatization for the parallelization of acyclic code. 3.3 Run-Time Decisions There are decisions that are difficult or impossible to make at compile time.
Reference: [108] <author> Zhiyuan Li. </author> <title> Array Privatization for Parallel Execution of Loops. </title> <booktitle> In Proceedings of the 1992 Int'l. Conf. on Supercomputing, </booktitle> <pages> pages 313-322, </pages> <address> Washington, D.C., </address> <month> July </month> <year> 1992. </year>
Reference-contexts: However, array privatization is very important for the effective parallelization of many real programs. (See the papers by Feautrier [105], Maydan et al. [106], Tu and Padua [107], and Li <ref> [108] </ref> for array privatization and expansion techniques.) Burke et al. [109] discuss the use of privatization for the parallelization of acyclic code. 3.3 Run-Time Decisions There are decisions that are difficult or impossible to make at compile time.
Reference: [109] <author> M. Burke, R. Cytron, J. Ferrante, and W. Hsieh. </author> <title> Automatic generation of nested fork-join parallelism. </title> <journal> J. of Supercomputing, </journal> <volume> 2(3) </volume> <pages> 71-88, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: However, array privatization is very important for the effective parallelization of many real programs. (See the papers by Feautrier [105], Maydan et al. [106], Tu and Padua [107], and Li [108] for array privatization and expansion techniques.) Burke et al. <ref> [109] </ref> discuss the use of privatization for the parallelization of acyclic code. 3.3 Run-Time Decisions There are decisions that are difficult or impossible to make at compile time. For example, to determine data dependences exactly, the values of certain variables must be known.
Reference: [110] <author> M. Byler, J. Davies, C. Huson, B. Leasure, and M. Wolfe. </author> <title> Multiple version loops. </title> <booktitle> In Proceedings of the 1987 International Conference on Parallel Processing, </booktitle> <address> pp.312-318, </address> <month> August </month> <year> 1987. </year>
Reference-contexts: For example, K could be a function of an input value or the compiler may not be able to determine its value due to limitations of the analysis algorithms. The strategy that is followed in cases like this is to generate conditional parallel code, known as two-version loops <ref> [110] </ref>, that is executed only when K is not zero: if K = 0 then A (M ) = B (N ) else parallel do I = 0; N A (M + K fl I) = B (I) enddo end if Other cases more complex than the previous example may arise,
Reference: [111] <author> Chuan-Qi Zhu and Pen-Chung Yew. </author> <title> A Scheme to Enforce Data Dependence on Large Multiprocessor Systems. </title> <journal> IEEE Trans. on Software Eng., </journal> <volume> SE-13(6):726-739, </volume> <month> June </month> <year> 1987. </year>
Reference-contexts: it is clear that iterations 0; 1; and 4 can execute in parallel in a first step, followed by the parallel execution of iterations 2 and 3. 2 A technique to handle, at run time, situations like the one in the preceding example has been discussed by Zhu and Yew <ref> [111] </ref>. In this technique, the set of iterations that can execute in parallel and their order are computed every time the loop is executed.
Reference: [112] <author> J. Wu, J. Saltz, S. Hiranandani, and H. Berryman. </author> <title> Runtime compilation methods for multicomputers. </title> <editor> In H.D. Schwet-man, editor, </editor> <booktitle> Proceedings of the 1991 Int'l. Conf. on Parallel Processing, </booktitle> <address> St. Charles, Ill., </address> <month> August 12-16, </month> <pages> pages 26-30. </pages> <publisher> CRC Press, Inc., </publisher> <year> 1990. </year> <title> Vol. </title> <booktitle> II Software. </booktitle>
Reference-contexts: In this technique, the set of iterations that can execute in parallel and their order are computed every time the loop is executed. A second technique, proposed by Saltz and his co-workers <ref> [112] </ref>, assumes that the subscripts do not change between loop executions and therefore the subscript analysis is only needed the first time the loop is executed. 3.4 Issues in Non-Fortran languages 3.4.1 Pointer analysis Dependence analysis in the presence of pointers has been found to be a particularly difficult problem.
Reference: [113] <author> N. Jones and S. Muchnick. </author> <title> Flow analysis and optimization of lisp-like structures. In Program Flow Analysis, </title> <journal> Theory and Applications, </journal> <volume> chapter 4, </volume> <pages> pages 102-131. </pages> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1981. </year>
Reference-contexts: For example, a compiler could infer that a pointer refers to a linear linked list (as opposed to a circular linked list), allowing more accurate dependence analysis dur ing a list traversal. This approach has been taken by numerous researchers, with varying degrees of success <ref> [113, 114, 115, 116, 117, 118, 119, 120, 121, 122] </ref>; recursion and cyclic relationships have posed the greatest difficulty (the recent work of Deutsch [123] may prove more powerful).
Reference: [114] <author> V. Guarna. </author> <title> A technique for analyzing pointer and structure references in parallel restructuring compilers. </title> <booktitle> In Proceedings of the Int'l. Conf. on Parallel Processing, </booktitle> <volume> volume 2, </volume> <pages> pages 212-220, </pages> <year> 1988. </year>
Reference-contexts: For example, a compiler could infer that a pointer refers to a linear linked list (as opposed to a circular linked list), allowing more accurate dependence analysis dur ing a list traversal. This approach has been taken by numerous researchers, with varying degrees of success <ref> [113, 114, 115, 116, 117, 118, 119, 120, 121, 122] </ref>; recursion and cyclic relationships have posed the greatest difficulty (the recent work of Deutsch [123] may prove more powerful).
Reference: [115] <author> J. Larus and P. Hilfinger. </author> <title> Detecting conflicts between structure accesses. </title> <booktitle> In Proceedings of the SIGPLAN'88 Conf. on Programming Language Design and Implementation, </booktitle> <pages> pages 21-34, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: For example, a compiler could infer that a pointer refers to a linear linked list (as opposed to a circular linked list), allowing more accurate dependence analysis dur ing a list traversal. This approach has been taken by numerous researchers, with varying degrees of success <ref> [113, 114, 115, 116, 117, 118, 119, 120, 121, 122] </ref>; recursion and cyclic relationships have posed the greatest difficulty (the recent work of Deutsch [123] may prove more powerful).
Reference: [116] <author> S. Horwitz, P. Pfeiffer, and T. Reps. </author> <title> Dependence analysis for pointer variables. </title> <booktitle> In Proceedings of the SIGPLAN'89 Conf. on Programming Language Design and Implementation, </booktitle> <pages> pages 28-40, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: For example, a compiler could infer that a pointer refers to a linear linked list (as opposed to a circular linked list), allowing more accurate dependence analysis dur ing a list traversal. This approach has been taken by numerous researchers, with varying degrees of success <ref> [113, 114, 115, 116, 117, 118, 119, 120, 121, 122] </ref>; recursion and cyclic relationships have posed the greatest difficulty (the recent work of Deutsch [123] may prove more powerful).
Reference: [117] <author> D. Chase, M. Wegman, and F. Zadek. </author> <title> Analysis of pointers and structures. </title> <booktitle> In Proceedings of the SIGPLAN'90 Conf. on Programming Language Design and Implementation, </booktitle> <pages> pages 296-310, </pages> <year> 1990. </year>
Reference-contexts: For example, a compiler could infer that a pointer refers to a linear linked list (as opposed to a circular linked list), allowing more accurate dependence analysis dur ing a list traversal. This approach has been taken by numerous researchers, with varying degrees of success <ref> [113, 114, 115, 116, 117, 118, 119, 120, 121, 122] </ref>; recursion and cyclic relationships have posed the greatest difficulty (the recent work of Deutsch [123] may prove more powerful).
Reference: [118] <author> W. L. Harrison. </author> <title> The interprocedural analysis and automatic parallelization of scheme programs. </title> <journal> Lisp and Symbolic Computations, </journal> 2(3/4):179-396, 1989. 
Reference-contexts: For example, a compiler could infer that a pointer refers to a linear linked list (as opposed to a circular linked list), allowing more accurate dependence analysis dur ing a list traversal. This approach has been taken by numerous researchers, with varying degrees of success <ref> [113, 114, 115, 116, 117, 118, 119, 120, 121, 122] </ref>; recursion and cyclic relationships have posed the greatest difficulty (the recent work of Deutsch [123] may prove more powerful).
Reference: [119] <author> W. L. Harrison. </author> <title> Generalized iteration space and the paral-lelization of symbolic programs. </title> <editor> In I. Foster and E. Tick, editors, </editor> <booktitle> Proceedings of the Workshop on Computation of Symbolic Languages for Parallel Computers, </booktitle> <institution> Argonne National Lab, </institution> <month> October </month> <year> 1991. </year> <month> ANL-91/34. </month>
Reference-contexts: For example, a compiler could infer that a pointer refers to a linear linked list (as opposed to a circular linked list), allowing more accurate dependence analysis dur ing a list traversal. This approach has been taken by numerous researchers, with varying degrees of success <ref> [113, 114, 115, 116, 117, 118, 119, 120, 121, 122] </ref>; recursion and cyclic relationships have posed the greatest difficulty (the recent work of Deutsch [123] may prove more powerful).
Reference: [120] <author> L. Hendren and A. Nicolau. </author> <title> Parallelizing programs with recursive data structures. </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> 1(1), </volume> <year> 1990. </year>
Reference-contexts: For example, a compiler could infer that a pointer refers to a linear linked list (as opposed to a circular linked list), allowing more accurate dependence analysis dur ing a list traversal. This approach has been taken by numerous researchers, with varying degrees of success <ref> [113, 114, 115, 116, 117, 118, 119, 120, 121, 122] </ref>; recursion and cyclic relationships have posed the greatest difficulty (the recent work of Deutsch [123] may prove more powerful).
Reference: [121] <author> J. Loeliger, R. Metzger, M. Seligman, and S. Stroud. </author> <title> Pointer tracking. </title> <booktitle> In Proceedings of Supercomputing'91, </booktitle> <year> 1991. </year>
Reference-contexts: For example, a compiler could infer that a pointer refers to a linear linked list (as opposed to a circular linked list), allowing more accurate dependence analysis dur ing a list traversal. This approach has been taken by numerous researchers, with varying degrees of success <ref> [113, 114, 115, 116, 117, 118, 119, 120, 121, 122] </ref>; recursion and cyclic relationships have posed the greatest difficulty (the recent work of Deutsch [123] may prove more powerful).
Reference: [122] <author> W. Landi and B. Ryder. </author> <title> A safe approximation algorithm for interprocedural pointer aliasing. </title> <booktitle> In Proceedings of the SIGPLAN'92 Conf. on Programming Language Design and Implementation, </booktitle> <pages> pages 235-248, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: For example, a compiler could infer that a pointer refers to a linear linked list (as opposed to a circular linked list), allowing more accurate dependence analysis dur ing a list traversal. This approach has been taken by numerous researchers, with varying degrees of success <ref> [113, 114, 115, 116, 117, 118, 119, 120, 121, 122] </ref>; recursion and cyclic relationships have posed the greatest difficulty (the recent work of Deutsch [123] may prove more powerful).
Reference: [123] <author> A. Deutsch. </author> <title> A storeless model of aliasing and its abstractions using finite representations of right-regular equivalence relations. </title> <booktitle> In Proceedings of the IEEE'92 Int'l. Conf. on Computer Languages, </booktitle> <month> April </month> <year> 1992. </year>
Reference-contexts: This approach has been taken by numerous researchers, with varying degrees of success [113, 114, 115, 116, 117, 118, 119, 120, 121, 122]; recursion and cyclic relationships have posed the greatest difficulty (the recent work of Deutsch <ref> [123] </ref> may prove more powerful). A related approach, originally focused on solving a different problem (automatic type inference or lifetime analysis, for example), can be used to provide alias information as well [124, 125, 126]; the viability of this approach has not been demonstrated.
Reference: [124] <author> C. Ruggieri and T. Murtagh. </author> <title> Lifetime analysis of dynamically allocated objects. </title> <booktitle> In Proceedings of the 15th ACM Symp. on Principles of Programming Languages, </booktitle> <pages> pages 285-293, </pages> <year> 1988. </year>
Reference-contexts: A related approach, originally focused on solving a different problem (automatic type inference or lifetime analysis, for example), can be used to provide alias information as well <ref> [124, 125, 126] </ref>; the viability of this approach has not been demonstrated.
Reference: [125] <author> H. Baker. </author> <title> Unify and conquer (garbage, updating, </title> <booktitle> aliasing,...) in functional languages. In Proceedings of the '90 ACM Conf. on LISP and Functional Programming, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: A related approach, originally focused on solving a different problem (automatic type inference or lifetime analysis, for example), can be used to provide alias information as well <ref> [124, 125, 126] </ref>; the viability of this approach has not been demonstrated.
Reference: [126] <author> E. Wang and P. Hilfinger. </author> <title> Analysis of recursive types in Lisp-like languages. </title> <booktitle> In Proceedings of the '92 ACM Conf. on LISP and Functional Programming, </booktitle> <pages> pages 216-225, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: A related approach, originally focused on solving a different problem (automatic type inference or lifetime analysis, for example), can be used to provide alias information as well <ref> [124, 125, 126] </ref>; the viability of this approach has not been demonstrated.
Reference: [127] <author> H. Dietz and D. Klappholz. </author> <title> Refined C: A sequential language for parallel processing. </title> <booktitle> In Proceedings of Int'l. Conf. on Parallel Processing, </booktitle> <pages> pages 442-449, </pages> <year> 1985. </year>
Reference-contexts: A related approach, originally focused on solving a different problem (automatic type inference or lifetime analysis, for example), can be used to provide alias information as well [124, 125, 126]; the viability of this approach has not been demonstrated. Finally, various language-based approaches <ref> [127, 128, 129, 130] </ref> provide the compiler with additional information on which to base dependence decisions (or in the case of [131], represent a data structure in a more parallel form). 3.4.2 Parallelization of Recursive Constructs Recursion is seldom used today in numerical programs, partly because it is not part of
Reference: [128] <author> D. Klappholz, A. Kallis, and X. Kang. </author> <title> Refined C: An update. </title> <booktitle> Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 331-357, </pages> <year> 1990. </year>
Reference-contexts: A related approach, originally focused on solving a different problem (automatic type inference or lifetime analysis, for example), can be used to provide alias information as well [124, 125, 126]; the viability of this approach has not been demonstrated. Finally, various language-based approaches <ref> [127, 128, 129, 130] </ref> provide the compiler with additional information on which to base dependence decisions (or in the case of [131], represent a data structure in a more parallel form). 3.4.2 Parallelization of Recursive Constructs Recursion is seldom used today in numerical programs, partly because it is not part of
Reference: [129] <author> J. Lucassen and D. Gifford. </author> <title> Polymorphic effect systems. </title> <booktitle> In Proceedings of 15th ACM Symp. on Principles of Programming Languages, </booktitle> <pages> pages 47-57, </pages> <year> 1988. </year>
Reference-contexts: A related approach, originally focused on solving a different problem (automatic type inference or lifetime analysis, for example), can be used to provide alias information as well [124, 125, 126]; the viability of this approach has not been demonstrated. Finally, various language-based approaches <ref> [127, 128, 129, 130] </ref> provide the compiler with additional information on which to base dependence decisions (or in the case of [131], represent a data structure in a more parallel form). 3.4.2 Parallelization of Recursive Constructs Recursion is seldom used today in numerical programs, partly because it is not part of
Reference: [130] <author> L. Hendren, J. Hummel, and A. Nicolau. </author> <title> Abstractions for recursive pointer data structures: Improving the analysis and transformation of imperative programs. </title> <booktitle> In Proceedings of the SIGPLAN'92 Conf. on Programming Language Design and Implementation, </booktitle> <pages> pages 249-269, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: A related approach, originally focused on solving a different problem (automatic type inference or lifetime analysis, for example), can be used to provide alias information as well [124, 125, 126]; the viability of this approach has not been demonstrated. Finally, various language-based approaches <ref> [127, 128, 129, 130] </ref> provide the compiler with additional information on which to base dependence decisions (or in the case of [131], represent a data structure in a more parallel form). 3.4.2 Parallelization of Recursive Constructs Recursion is seldom used today in numerical programs, partly because it is not part of
Reference: [131] <author> J. Solworth. </author> <title> The PARSEQ project: An interim report. Languages and Compilers for Parallel Computing, </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <pages> pages 490-510, </pages> <year> 1990. </year>
Reference-contexts: Finally, various language-based approaches [127, 128, 129, 130] provide the compiler with additional information on which to base dependence decisions (or in the case of <ref> [131] </ref>, represent a data structure in a more parallel form). 3.4.2 Parallelization of Recursive Constructs Recursion is seldom used today in numerical programs, partly because it is not part of the Fortran 77 standard. However, recursion is the most natural way to express some algorithms, especially non-numerical algorithms.
Reference: [132] <author> W. Ludwell Harrison and David Padua. </author> <title> PARCEL: Project for the Automatic Restructuring and Concurrent Evaluation of Lisp. </title> <booktitle> Proceedings of 1988 Int'l. Conf. on Supercomputing, </booktitle> <address> St. Malo, France, </address> <pages> pages 527-538, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: However, recursion is the most natural way to express some algorithms, especially non-numerical algorithms. We describe in this section a technique developed by Harrison <ref> [132] </ref>, called recursion splitting which, although it was originally developed to parallelize Lisp programs, can be applied to programs in other languages including Fortran 90.
Reference: [133] <author> John Cocke and Peter W. Markstein. </author> <title> Measurement of Program Improvement Algorithms. </title> <editor> In S.H. Lavington, editor, </editor> <booktitle> Information Processing. North-Holland,pp. </booktitle> <pages> 221-228, </pages> <year> 1980. </year>
Reference-contexts: This is particularly important in the case of parallelizing compilers, given the large number of possible transformations. The value of effectiveness studies of traditional compiler technology is widely recognized, and there are a number of papers on the subject. For example, Cocke and Markstein <ref> [133] </ref> reported the effectiveness of several traditional techniques such as common subexpression elimination, code motion, and dead code elimination. Unfortunately, published studies on the effectiveness of parallelizing compilers are relatively sparse.
Reference: [134] <author> R. Comerford. </author> <title> How DEC developed Alpha. </title> <journal> IEEE Spectrum, </journal> <month> July </month> <year> 1992. </year>
Reference-contexts: Nevertheless, performance previously associated with supercomputers is becoming commonly available on these new processors. Thus, for example, the PA-RISC HP 730, achieves 75 SPECmarks, while the new DEC Alpha processor is projected to obtain 110 SPECmarks <ref> [134] </ref>. More fundamental studies that attempt to measure the potential of instruction-level transformations have also become available. Many of these studies assume some idealized circumstances, such as unlimited resources or complete compile-time knowledge of dependences and branches.
Reference: [135] <author> A. Nicolau and J. Fisher. </author> <title> Measuring the parallelism available for very long instruction word architectures. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C33:968-976, </volume> <month> November </month> <year> 1984. </year>
Reference-contexts: More fundamental studies that attempt to measure the potential of instruction-level transformations have also become available. Many of these studies assume some idealized circumstances, such as unlimited resources or complete compile-time knowledge of dependences and branches. An early study on numerical kernels by Nicolau and Fisher <ref> [135] </ref> found an average of 90-fold parallelism available at the instruction level, given absolute dependence information and absolute branch prediction. The parallelism found was mainly limited by the problem size, which had to be kept small due to limitations of the experiment implementation.
Reference: [136] <author> D. Wall. </author> <title> Limits of instruction level parallelism. </title> <booktitle> In Proceedings of Architectural Support for Prog. Langs and Operating Systems, </booktitle> <pages> pages 176-189, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: The parallelism found was mainly limited by the problem size, which had to be kept small due to limitations of the experiment implementation. In a more recent study Wall <ref> [136] </ref> evaluated complete numeric and systems benchmarks under various dependence-analysis and branch-prediction conditions, ranging from idealized to realistic.
Reference: [137] <author> G. Tjadan and M. Flynn. </author> <title> Detection and parallel execution of independent statements. </title> <journal> IEEE Transaction on Computers, </journal> 19(10) 889-895, October 1970. 
Reference-contexts: The results showed instruction-level parallelism average factors of 7 for dynamic and 9 for static scheduling under idealized conditions, with factors of about 4-5 estimated to be achievable with state-of-the-art realistic compiler techniques. Early efforts in instruction-level parallelism by Tjaden and Flynn <ref> [137] </ref> and Riseman and Foster [138] investigated the amounts of parallelism available at the machine instruction level for either static (compile-time) or dynamic (run-time) parallelism exploitation. The former study limited itself to finding parallelism within basic blocks, 8 and thus found only factors of 2-3 speedup over sequential execution.
Reference: [138] <author> E. Riseman and C. Foster. </author> <title> The inhibition of potential parallelism by conditional jumps. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-21(12):1405-1411, </volume> <month> December </month> <year> 1972. </year>
Reference-contexts: The results showed instruction-level parallelism average factors of 7 for dynamic and 9 for static scheduling under idealized conditions, with factors of about 4-5 estimated to be achievable with state-of-the-art realistic compiler techniques. Early efforts in instruction-level parallelism by Tjaden and Flynn [137] and Riseman and Foster <ref> [138] </ref> investigated the amounts of parallelism available at the machine instruction level for either static (compile-time) or dynamic (run-time) parallelism exploitation. The former study limited itself to finding parallelism within basic blocks, 8 and thus found only factors of 2-3 speedup over sequential execution.
Reference: [139] <author> K. Ebcioglu. </author> <title> Some Design Ideas for a VLIW Architecture for Sequential Natured Software. </title> <booktitle> In Parallel Processing, Proceedings IFIP WG 10.3 Working Conference on Parallel Processing, </booktitle> <year> 1988. </year>
Reference-contexts: In experiments using a modification of percolation scheduling and a software pipelining scheme to generate code for a VLIW engine under construction at IBM T.J. Watson Labs, speedups of more than 10-fold versus the initial sequential code have been reported by Ebcioglu <ref> [139] </ref>. In a related paper Nakatani and Ebcioglu [140] showed that average speedups of 5.4-fold could still be obtained in systems and AI codes, even when percolation of operations is limited to a relatively small (moving) window in order to reduce code explosion and compilation-time.
Reference: [140] <author> T. Nakatani and K. Ebcioglu. </author> <title> Using a lookahead window in a compaction-based parallelizing compiler. </title> <booktitle> In Proceedings of the 23rd Annual Int'l. Symp. on Microarchitecture, </booktitle> <year> 1990. </year> <month> 36 </month>
Reference-contexts: Watson Labs, speedups of more than 10-fold versus the initial sequential code have been reported by Ebcioglu [139]. In a related paper Nakatani and Ebcioglu <ref> [140] </ref> showed that average speedups of 5.4-fold could still be obtained in systems and AI codes, even when percolation of operations is limited to a relatively small (moving) window in order to reduce code explosion and compilation-time.
Reference: [141] <author> T. Gross and M. Ward. </author> <title> The supression of compensation code. </title> <editor> In D. Gelernter et.al., editor, </editor> <booktitle> Advances in Languages and Compilers for Parallel Processing. </booktitle> <publisher> MIT Press, </publisher> <address> Cam-bridge, Mass., </address> <pages> pp. 260-273, </pages> <year> 1991. </year>
Reference-contexts: Althogh the the Vax 8700 and the Multiflow Trace have different organizations, the basic hardware of the two machines is roughly the same sion in trace scheduling has also been reported by Gross and Ward <ref> [141] </ref>.
Reference: [142] <author> M. Butler, T.-Y. Yeh, Y. Patt, M. Alsup, H. Scales, and M. Shebanow. </author> <title> Single instruction stream parallelism is greater than two. </title> <booktitle> In Proceedings of the 18th Annual Int'l. Symp. on Computer Architecture, </booktitle> <pages> pages 276-286, </pages> <year> 1991. </year>
Reference-contexts: An evaluation of the dynamic exploitation of instruction-level parallelism was done by Butler et. al. <ref> [142] </ref>, who report that with an issue rate of 8 instructions per cycle (and with a window-size limit placed on the total number of instructions currently under evaluation), speedups of 2-5.8 over sequential can be obtained on the SPEC benchmarks. <p> x simulated KAP test suite: K=Kernels A=Algorithms P=Application programs measures : V=shows rate of successfully vectorized loops N=compares performance numbers of different compilers T=compares transformations of different compilers S=shows speedups due to automatic parallelization I=evaluates individual compiler techniques F=discusses future compiler improvements Table 5: Summary of compiler effectiveness studies 32 <ref> [142] </ref>. The measurements have pointed out both success and limitations of available automatic parallelizers. Improvements are necessary to make restructurers consistently useful tools in multiprocessor environments.
Reference: [143] <author> Ulrich Detert. </author> <title> Programmiertechniken fur die Vek-torisierung. </title> <booktitle> In Proceedings of Supercomputer `87, Mannheim, </booktitle> <address> Germany, </address> <month> June </month> <year> 1987. </year>
Reference-contexts: Alliant FX/2800 optimizer). 4.3.1 Recognizing Parallelism One way to evaluate a parallelizing compiler is to count the number of program segments that can be paral-lelized. The two projects discussed here measure the number of do loops that the compilers under evaluation were able to vectorize totally or partially. Detert <ref> [143, 144] </ref> used 101 short Fortran loops to evaluate the compilers of seven parallel machines. Callahan et al. [145] did a similar but more extensive study using 100 short loops. A total of 19 compilers and machines were evaluated. <p> improvements over serial program execution on Alliant FX8 [154] Study Test Suite Measures Machines Compilers K A P V N T S I F [56] x x simulated [149] x x x x x Cyber 203/5 FTN200, KAP, VAST [158] x x simulated Parafrase [155] x x x simulated Parafrase <ref> [144, 143] </ref> x x x see Table 1 [150] x x x x Cyber 205 FTN200, KAP, VAST [145] x x see Table 1 [151] x x NAS 160 KAP, VAST [148] x x x x see Table 2 [153] x x x x x Alliant FX/8 KAP, VAST [152] x
Reference: [144] <author> Ulrich Detert. </author> <title> Untersuchungen an autovektorisierenden compilern. </title> <type> Technical Report ZAM 1/1987, </type> <institution> ZAM, KFA Juelich GmbH, Germany, </institution> <year> 1987. </year>
Reference-contexts: Alliant FX/2800 optimizer). 4.3.1 Recognizing Parallelism One way to evaluate a parallelizing compiler is to count the number of program segments that can be paral-lelized. The two projects discussed here measure the number of do loops that the compilers under evaluation were able to vectorize totally or partially. Detert <ref> [143, 144] </ref> used 101 short Fortran loops to evaluate the compilers of seven parallel machines. Callahan et al. [145] did a similar but more extensive study using 100 short loops. A total of 19 compilers and machines were evaluated. <p> First section: <ref> [144] </ref> (101 loops total). Second section: [145] (100 loops total). <p> improvements over serial program execution on Alliant FX8 [154] Study Test Suite Measures Machines Compilers K A P V N T S I F [56] x x simulated [149] x x x x x Cyber 203/5 FTN200, KAP, VAST [158] x x simulated Parafrase [155] x x x simulated Parafrase <ref> [144, 143] </ref> x x x see Table 1 [150] x x x x Cyber 205 FTN200, KAP, VAST [145] x x see Table 1 [151] x x NAS 160 KAP, VAST [148] x x x x see Table 2 [153] x x x x x Alliant FX/8 KAP, VAST [152] x
Reference: [145] <author> David Callahan, Jack Dongarra, and David Levine. </author> <title> Vector-izing Compilers: A Test Suite and Results. </title> <booktitle> In Proceedings of Supercomputing `88, </booktitle> <pages> pages 98-105, </pages> <year> 1988. </year>
Reference-contexts: The two projects discussed here measure the number of do loops that the compilers under evaluation were able to vectorize totally or partially. Detert [143, 144] used 101 short Fortran loops to evaluate the compilers of seven parallel machines. Callahan et al. <ref> [145] </ref> did a similar but more extensive study using 100 short loops. A total of 19 compilers and machines were evaluated. Both studies show that there is a wide variability in the capabilities of existing compilers. <p> First section: [144] (101 loops total). Second section: <ref> [145] </ref> (100 loops total). Threshold (%) NEC SX Fujitsu VP Cray X-MP CFT77 Cray X-MP CFT 90 14 17 9 11 70 18 26 15 15 Table 2: Number of loops (out of 46) whose automatic/optimal performance ratio is higher than the threshold in [148]. <p> V N T S I F [56] x x simulated [149] x x x x x Cyber 203/5 FTN200, KAP, VAST [158] x x simulated Parafrase [155] x x x simulated Parafrase [144, 143] x x x see Table 1 [150] x x x x Cyber 205 FTN200, KAP, VAST <ref> [145] </ref> x x see Table 1 [151] x x NAS 160 KAP, VAST [148] x x x x see Table 2 [153] x x x x x Alliant FX/8 KAP, VAST [152] x x x x Cray Y-MP KAP, fpp [156] x x x Alliant FX/8 VAST [101, 154] x x
Reference: [146] <author> Z. Shen, Z. Li, and P. Yew. </author> <title> An empirical study on array subscripts and data dependencies. </title> <booktitle> In Proceedings of the 1989 Int'l. Conf. on Parallel Processing, </booktitle> <pages> pages 145-152, </pages> <note> Vol. </note> <institution> II. The Pennsylvania State University Press, University Park, Pennsylvania, </institution> <year> 1989. </year>
Reference-contexts: Table 1 summarizes these results. Data-dependence tests, as described in Section 2, are crucial for the successful recognition of parallel loops. Early evaluation work for these techniques was done by Shen et.al. <ref> [146] </ref> who have analyzed subscript patterns 28 that arise in real programs. Maydan et.al. [19] and Goff et.al. [20] present statistics on the success rates of data-dependence tests.
Reference: [147] <author> Paul M. Petersen and David A. Padua. </author> <title> Dynamic Dependence Analysis: A Novel Method for Data Dependence Evaluation. </title> <booktitle> In Conference Record of the 5th Workshop on Languages and Compilers for Parallel Computing. </booktitle> <institution> Yale University. Department of Computer Science. YALEU/DCS/RR-915, </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: Early evaluation work for these techniques was done by Shen et.al. [146] who have analyzed subscript patterns 28 that arise in real programs. Maydan et.al. [19] and Goff et.al. [20] present statistics on the success rates of data-dependence tests. Recently, Petersen and Padua <ref> [147] </ref> have extended this work by relating these numbers to program performance of a suite of Benchmark programs. 4.3.2 Comparing Performance Measurements Other researchers have focussed on actual timing measurements of automatically parallelized code.
Reference: [148] <author> H. Nobayashi and C. Eoyang. </author> <title> A Comparison Study of Automatically Vectorizing Fortran Compilers. </title> <booktitle> Proceedings of Supercomputing '89, </booktitle> <pages> pp. 820-825, </pages> <year> 1989. </year>
Reference-contexts: Recently, Petersen and Padua [147] have extended this work by relating these numbers to program performance of a suite of Benchmark programs. 4.3.2 Comparing Performance Measurements Other researchers have focussed on actual timing measurements of automatically parallelized code. Thus, Nobayashi and Eoyang <ref> [148] </ref> evaluated several vector-izing compilers by translating a collection of program kernels onto three machines: Cray X-MP, Fujitsu VP, and NEC SX. They found that compilers that vector-ize more loops do not necessarily produce faster code. They also show that kernel measurements can yield very divergent results. <p> Second section: [145] (100 loops total). Threshold (%) NEC SX Fujitsu VP Cray X-MP CFT77 Cray X-MP CFT 90 14 17 9 11 70 18 26 15 15 Table 2: Number of loops (out of 46) whose automatic/optimal performance ratio is higher than the threshold in <ref> [148] </ref>. <p> x x x Cyber 203/5 FTN200, KAP, VAST [158] x x simulated Parafrase [155] x x x simulated Parafrase [144, 143] x x x see Table 1 [150] x x x x Cyber 205 FTN200, KAP, VAST [145] x x see Table 1 [151] x x NAS 160 KAP, VAST <ref> [148] </ref> x x x x see Table 2 [153] x x x x x Alliant FX/8 KAP, VAST [152] x x x x Cray Y-MP KAP, fpp [156] x x x Alliant FX/8 VAST [101, 154] x x x x FX/8, Cedar KAP [157] x x x simulated KAP test suite:
Reference: [149] <author> Clifford N. Arnold. </author> <title> Performance Evaluation of three Automatic Vectorizer Packages. </title> <booktitle> In Proceedings of Int'l. Conf. on Parallel Processing, </booktitle> <pages> pages 235-242, </pages> <year> 1982. </year>
Reference-contexts: Table 2 summarizes one of the measurements, which compared the performance of the automatically restructured loops with that of hand-restructured loops and also shows the number of loops whose automatic/hand-optimized performance ratio is higher than the threshold shown in the Table. Arnold <ref> [149] </ref> reports performance improvements produced by KAP, VAST, and FTN200, the Fortran compiler of the Cyber 200 machines, on 18 Livermore Loops. The measurements were taken on the Cyber 203 and 205 machines. <p> Third and fourth line: Improvements over serial program execution on Alliant FX8 [153]. Fifth line: manual improvements over serial program execution on Alliant FX8 [154] Study Test Suite Measures Machines Compilers K A P V N T S I F [56] x x simulated <ref> [149] </ref> x x x x x Cyber 203/5 FTN200, KAP, VAST [158] x x simulated Parafrase [155] x x x simulated Parafrase [144, 143] x x x see Table 1 [150] x x x x Cyber 205 FTN200, KAP, VAST [145] x x see Table 1 [151] x x NAS 160
Reference: [150] <author> Robert N. Braswell and Malcolm S. Keech. </author> <title> An Evaluation of Vector Fortran 200 Generated by Cyber 205 and ETA-10 Pre-Compilation Tools. </title> <booktitle> In Proceedings of Supercomputing `88, </booktitle> <pages> pages 106-113, </pages> <year> 1988. </year>
Reference-contexts: Arnold [149] reports performance improvements produced by KAP, VAST, and FTN200, the Fortran compiler of the Cyber 200 machines, on 18 Livermore Loops. The measurements were taken on the Cyber 203 and 205 machines. A related study was done by Braswell and Keech <ref> [150] </ref>, who use a set of 90 loops to evaluate KAP, FTN200, and two versions of VAST. The target machine was the Cyber 205. They present timing numbers for 18 of the 90 loops as well as the overall results shown in Table 3. <p> FTN200 VAST-2 KAP/205 ETA VAST-2 No. of loops (partially) vectorized 36 (0) 57 (5) 60 (2) 57 (5) (N = 90 loops) Sum of execution times of 18 test loops 17 15 1.5 1.2 on Cyber 205 Table 3: Vectorization success rate and timing results in <ref> [150] </ref> 30 4.3.4 Evidence for Further Improvements As important as evaluating available compilers, is to look at existing evidence showing potential improvements of the compiler effectiveness. The following reports contribute to this goal. <p> [154] Study Test Suite Measures Machines Compilers K A P V N T S I F [56] x x simulated [149] x x x x x Cyber 203/5 FTN200, KAP, VAST [158] x x simulated Parafrase [155] x x x simulated Parafrase [144, 143] x x x see Table 1 <ref> [150] </ref> x x x x Cyber 205 FTN200, KAP, VAST [145] x x see Table 1 [151] x x NAS 160 KAP, VAST [148] x x x x see Table 2 [153] x x x x x Alliant FX/8 KAP, VAST [152] x x x x Cray Y-MP KAP, fpp [156]
Reference: [151] <author> G.R. Luecke, J. Coyle, W. Haque, J. Hoekstra, H. Jespersen, and R. Schmidt. </author> <title> A comparative study of KAP and VAST: two automatic preprocessors with Fortran 8x Output. Supercomputer 28, </title> <address> V(6):15-25, </address> <year> 1988. </year>
Reference-contexts: Only KAP and ETA VAST-2 were able to vectorize this loop and, in this way, improve the performance by a factor of ten. Another comparative study of KAP and VAST was done by Luecke et al. <ref> [151] </ref>. They discuss a number of transformations applied to a set of loops, including the Livermore Kernels. Differences in transformations applied by KAP and VAST are discussed qualitatively. No performance measurements are reported. <p> [56] x x simulated [149] x x x x x Cyber 203/5 FTN200, KAP, VAST [158] x x simulated Parafrase [155] x x x simulated Parafrase [144, 143] x x x see Table 1 [150] x x x x Cyber 205 FTN200, KAP, VAST [145] x x see Table 1 <ref> [151] </ref> x x NAS 160 KAP, VAST [148] x x x x see Table 2 [153] x x x x x Alliant FX/8 KAP, VAST [152] x x x x Cray Y-MP KAP, fpp [156] x x x Alliant FX/8 VAST [101, 154] x x x x FX/8, Cedar KAP [157]
Reference: [152] <author> Doreen Y. Cheng and Douglas M. Pase. </author> <title> An Evaluation of Automatic and Interactive Parallel Programming Tools. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <pages> pages 412-422, </pages> <year> 1991. </year>
Reference-contexts: Another comparative study of KAP and VAST was done by Luecke et al. [151]. They discuss a number of transformations applied to a set of loops, including the Livermore Kernels. Differences in transformations applied by KAP and VAST are discussed qualitatively. No performance measurements are reported. Cheng and Pase <ref> [152] </ref> measured speed improvements resulting from the automatic parallelization (vector-ization and concurrentization) of 25 programs, including the Perfect Benchmarks R fl . The measurements were taken on a Cray Y-MP machine using KAP and fpp 11 . <p> This does not mean that parallelizers fail all the time, and in fact there are some real programs on which parallelization does a very good job. The two most extensive measurements of the effectiveness of parallelization on real codes are presented in <ref> [152] </ref> and [153]. Both studies report small improvements from automatic parallelization for a majority of the programs studied. However, it should be remembered that these two studies use different types of programs. Cheng and Pase [152] start with hand-optimized codes whereas [153] starts with unmodified programs. <p> two most extensive measurements of the effectiveness of parallelization on real codes are presented in <ref> [152] </ref> and [153]. Both studies report small improvements from automatic parallelization for a majority of the programs studied. However, it should be remembered that these two studies use different types of programs. Cheng and Pase [152] start with hand-optimized codes whereas [153] starts with unmodified programs. This is probably why [153] reports a higher effectiveness in a few programs whereas [152] sometimes shows performance degradations. The (additional) automatic vectorization done in the latter study leads to little or even negative improvement. <p> However, it should be remembered that these two studies use different types of programs. Cheng and Pase <ref> [152] </ref> start with hand-optimized codes whereas [153] starts with unmodified programs. This is probably why [153] reports a higher effectiveness in a few programs whereas [152] sometimes shows performance degradations. The (additional) automatic vectorization done in the latter study leads to little or even negative improvement. Apparently the automatic vectorization could not find more parallelism than the previous manual optimization, but introduced some overhead. <p> First two lines: Improvement over manually vector-optimized programs on Cray Y-MP <ref> [152] </ref>. Third and fourth line: Improvements over serial program execution on Alliant FX8 [153]. <p> Parafrase [144, 143] x x x see Table 1 [150] x x x x Cyber 205 FTN200, KAP, VAST [145] x x see Table 1 [151] x x NAS 160 KAP, VAST [148] x x x x see Table 2 [153] x x x x x Alliant FX/8 KAP, VAST <ref> [152] </ref> x x x x Cray Y-MP KAP, fpp [156] x x x Alliant FX/8 VAST [101, 154] x x x x FX/8, Cedar KAP [157] x x x simulated KAP test suite: K=Kernels A=Algorithms P=Application programs measures : V=shows rate of successfully vectorized loops N=compares performance numbers of different compilers
Reference: [153] <author> William Blume and Rudolf Eigenmann. </author> <title> Performance Analysis of Parallelizing Compilers on the Perfect Benchmarks R fl Programs. </title> <journal> IEEE Trans. of Parallel and Distributed Systems, vol.3, </journal> <volume> no.6, </volume> <pages> pp. 643-656, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: The measurements were obtained on a simulated shared-memory architecture of 32 and 1024 processors, respectively. The effect of disabling the transformations was more important when the number of processors was large. Blume and Eigenmann <ref> [153] </ref> discussed the effectiveness of parallelization on the Perfect Benchmarks suite. The target machine was an eight-processor Al-liant FX/80 machine. <p> This does not mean that parallelizers fail all the time, and in fact there are some real programs on which parallelization does a very good job. The two most extensive measurements of the effectiveness of parallelization on real codes are presented in [152] and <ref> [153] </ref>. Both studies report small improvements from automatic parallelization for a majority of the programs studied. However, it should be remembered that these two studies use different types of programs. Cheng and Pase [152] start with hand-optimized codes whereas [153] starts with unmodified programs. This is probably why [153] reports a <p> effectiveness of parallelization on real codes are presented in [152] and <ref> [153] </ref>. Both studies report small improvements from automatic parallelization for a majority of the programs studied. However, it should be remembered that these two studies use different types of programs. Cheng and Pase [152] start with hand-optimized codes whereas [153] starts with unmodified programs. This is probably why [153] reports a higher effectiveness in a few programs whereas [152] sometimes shows performance degradations. The (additional) automatic vectorization done in the latter study leads to little or even negative improvement. <p> [152] and <ref> [153] </ref>. Both studies report small improvements from automatic parallelization for a majority of the programs studied. However, it should be remembered that these two studies use different types of programs. Cheng and Pase [152] start with hand-optimized codes whereas [153] starts with unmodified programs. This is probably why [153] reports a higher effectiveness in a few programs whereas [152] sometimes shows performance degradations. The (additional) automatic vectorization done in the latter study leads to little or even negative improvement. Apparently the automatic vectorization could not find more parallelism than the previous manual optimization, but introduced some overhead. <p> Apparently the automatic vectorization could not find more parallelism than the previous manual optimization, but introduced some overhead. It is not reported to what extent manual vector optimizations were applied. Another important result is that many restructuring techniques were found ineffective <ref> [153] </ref>, presumably because many of the most time-consuming loops of the programs could not be parallelized. However, it was also shown that these loops can potentially be transformed into parallel code [154] by advanced techniques. <p> First two lines: Improvement over manually vector-optimized programs on Cray Y-MP [152]. Third and fourth line: Improvements over serial program execution on Alliant FX8 <ref> [153] </ref>. <p> [158] x x simulated Parafrase [155] x x x simulated Parafrase [144, 143] x x x see Table 1 [150] x x x x Cyber 205 FTN200, KAP, VAST [145] x x see Table 1 [151] x x NAS 160 KAP, VAST [148] x x x x see Table 2 <ref> [153] </ref> x x x x x Alliant FX/8 KAP, VAST [152] x x x x Cray Y-MP KAP, fpp [156] x x x Alliant FX/8 VAST [101, 154] x x x x FX/8, Cedar KAP [157] x x x simulated KAP test suite: K=Kernels A=Algorithms P=Application programs measures : V=shows rate
Reference: [154] <author> R. Eigenmann, J. Hoeflinger, G. Jaxon, and D. Padua. </author> <title> The Cedar Fortran Project. </title> <type> Technical Report 1262, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomputing Res.& Dev., </institution> <year> 1992. </year>
Reference-contexts: In an early study, Kuck et. al. [56] have determined the parallelism available in a set of algorithms. Their analyzer detects parallelism in do loops and parallelism from tree height reduction. The authors conclude that there is a potential average speedup of about 10. Eigenmann et al. <ref> [101, 154] </ref> conducted "manual compilation" experiments to determine new transformation techniques that significantly improve the performance of real programs. They hand-optimized the Perfect Benchmarks for the Alliant FX/80 and the Cedar machine. The speedups obtained are shown in Table 4. <p> Another important result is that many restructuring techniques were found ineffective [153], presumably because many of the most time-consuming loops of the programs could not be parallelized. However, it was also shown that these loops can potentially be transformed into parallel code <ref> [154] </ref> by advanced techniques. Hence, the existing techniques may become more effective once more powerful complementary compiler technology is developed. The evaluation papers on instruction-level parallelism (Section 4.2) have shown that corresponding compiler technology has been developed that is able to successfully exploit multiple functional units. <p> However, there is room for studies that evaluate this technology more comprehensively. There exists evidence for potential improvements of parallelizing compilers. It was given by analyzing real program patterns and deriving new compiler capabilities [159], by optimizing programs manually and discussing the automatability of the transformations applied <ref> [101, 154] </ref>, and by comparing "best" parallelism that one found by compilers [157] and deriving new restructuring capabilities. <p> First two lines: Improvement over manually vector-optimized programs on Cray Y-MP [152]. Third and fourth line: Improvements over serial program execution on Alliant FX8 [153]. Fifth line: manual improvements over serial program execution on Alliant FX8 <ref> [154] </ref> Study Test Suite Measures Machines Compilers K A P V N T S I F [56] x x simulated [149] x x x x x Cyber 203/5 FTN200, KAP, VAST [158] x x simulated Parafrase [155] x x x simulated Parafrase [144, 143] x x x see Table 1 [150] <p> FTN200, KAP, VAST [145] x x see Table 1 [151] x x NAS 160 KAP, VAST [148] x x x x see Table 2 [153] x x x x x Alliant FX/8 KAP, VAST [152] x x x x Cray Y-MP KAP, fpp [156] x x x Alliant FX/8 VAST <ref> [101, 154] </ref> x x x x FX/8, Cedar KAP [157] x x x simulated KAP test suite: K=Kernels A=Algorithms P=Application programs measures : V=shows rate of successfully vectorized loops N=compares performance numbers of different compilers T=compares transformations of different compilers S=shows speedups due to automatic parallelization I=evaluates individual compiler techniques F=discusses
Reference: [155] <author> Ron Cytron, David J. Kuck, and Alex V. Veidenbaum. </author> <title> The Effect of Restructuring Compilers on Program Performance for High-Speed Computers. </title> <booktitle> Special Issue of Computer Physics Communications devoted to the Proceedings of the Conf. on Vector and Parallel Processors in Computational Science II, </booktitle> <volume> 37 </volume> <pages> 39-48, </pages> <year> 1985. </year>
Reference-contexts: Table 4 shows the improvements by automatic parallelization for the Perfect Benchmarks. 4.3.3 Evaluating Individual Restructuring Techniques The effectiveness studies described so far considered the parallelizing compilers as black boxes. Another approach is to discriminate among individual compiler techniques. Thus, Cytron et al. <ref> [155] </ref> studied the performance degradation of the EISPACK algorithms after disabling various restructuring techniques of Parafrase [4]. <p> Fifth line: manual improvements over serial program execution on Alliant FX8 [154] Study Test Suite Measures Machines Compilers K A P V N T S I F [56] x x simulated [149] x x x x x Cyber 203/5 FTN200, KAP, VAST [158] x x simulated Parafrase <ref> [155] </ref> x x x simulated Parafrase [144, 143] x x x see Table 1 [150] x x x x Cyber 205 FTN200, KAP, VAST [145] x x see Table 1 [151] x x NAS 160 KAP, VAST [148] x x x x see Table 2 [153] x x x x x
Reference: [156] <author> J.P. Singh and J.L. Hennessy. </author> <title> An empirical investigation of the effectiveness and limitations of automatic paralleliza-tion. </title> <booktitle> In Proceedings of the Int'l. Symp. on Shared Memory Multiprocessing, </booktitle> <address> Tokyo, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: It is worth noting that many of the transformations discussed in section 3 were not found necessary to obtain good performance. Most of the loops could be transformed into completely parallel forms (i.e., vector and parallel dos without synchronization) after the transformations just mentioned were applied. Singh and Hennessy <ref> [156] </ref> studied the limitations of automatic parallelization using three scientific applications. They found that the time-consuming loop nests are often complex and require more sophisticated analysis and data restructuring. Recommendations for further development of automatic parallelization technology are given. <p> [150] x x x x Cyber 205 FTN200, KAP, VAST [145] x x see Table 1 [151] x x NAS 160 KAP, VAST [148] x x x x see Table 2 [153] x x x x x Alliant FX/8 KAP, VAST [152] x x x x Cray Y-MP KAP, fpp <ref> [156] </ref> x x x Alliant FX/8 VAST [101, 154] x x x x FX/8, Cedar KAP [157] x x x simulated KAP test suite: K=Kernels A=Algorithms P=Application programs measures : V=shows rate of successfully vectorized loops N=compares performance numbers of different compilers T=compares transformations of different compilers S=shows speedups due to
Reference: [157] <author> Paul Petersen and David Padua. </author> <title> Machine-Independent Evaluation of Parallelizing Compilers. In Advanced Compilation Techniques for Novel Architectures, </title> <month> January </month> <year> 1992. </year>
Reference-contexts: They found that the time-consuming loop nests are often complex and require more sophisticated analysis and data restructuring. Recommendations for further development of automatic parallelization technology are given. These include advances in symbolic data-dependence analysis, dataflow and interprocedural analysis, and privatization/expansion of data structures. Petersen and Padua <ref> [157] </ref> have compared the parallelism found by compilers to an estimated maximum parallelism and derived potential compiler improvements. The compiler used is KAP/Concurrent. The maximum parallelism is measured by instrumenting the program so that the execution can be simulated for an ideal machine, taking into account all essential data dependences. <p> There exists evidence for potential improvements of parallelizing compilers. It was given by analyzing real program patterns and deriving new compiler capabilities [159], by optimizing programs manually and discussing the automatability of the transformations applied [101, 154], and by comparing "best" parallelism that one found by compilers <ref> [157] </ref> and deriving new restructuring capabilities. <p> [151] x x NAS 160 KAP, VAST [148] x x x x see Table 2 [153] x x x x x Alliant FX/8 KAP, VAST [152] x x x x Cray Y-MP KAP, fpp [156] x x x Alliant FX/8 VAST [101, 154] x x x x FX/8, Cedar KAP <ref> [157] </ref> x x x simulated KAP test suite: K=Kernels A=Algorithms P=Application programs measures : V=shows rate of successfully vectorized loops N=compares performance numbers of different compilers T=compares transformations of different compilers S=shows speedups due to automatic parallelization I=evaluates individual compiler techniques F=discusses future compiler improvements Table 5: Summary of compiler effectiveness
Reference: [158] <author> Gyungho Lee, Clyde P. Kruskal, and David J. Kuck. </author> <title> An Empirical Study of Automatic Restructuring of Nonnumerical Programs for Parallel Processors. </title> <journal> Special Issue on Parallel Processing of IEEE Trans. on Computers, </journal> <volume> C-34(10):927-933, </volume> <month> Oct. </month> <year> 1985. </year>
Reference-contexts: Fifth line: manual improvements over serial program execution on Alliant FX8 [154] Study Test Suite Measures Machines Compilers K A P V N T S I F [56] x x simulated [149] x x x x x Cyber 203/5 FTN200, KAP, VAST <ref> [158] </ref> x x simulated Parafrase [155] x x x simulated Parafrase [144, 143] x x x see Table 1 [150] x x x x Cyber 205 FTN200, KAP, VAST [145] x x see Table 1 [151] x x NAS 160 KAP, VAST [148] x x x x see Table 2 [153]
Reference: [159] <author> Z. Shen, Z. Li, and P.-C. Yew. </author> <title> An empirical study of Fortran programs for parallelizing compilers. </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> 1(3) </volume> <pages> 356-364, </pages> <month> July </month> <year> 1990. </year> <month> 37 </month>
Reference-contexts: However, there is room for studies that evaluate this technology more comprehensively. There exists evidence for potential improvements of parallelizing compilers. It was given by analyzing real program patterns and deriving new compiler capabilities <ref> [159] </ref>, by optimizing programs manually and discussing the automatability of the transformations applied [101, 154], and by comparing "best" parallelism that one found by compilers [157] and deriving new restructuring capabilities.
References-found: 159

