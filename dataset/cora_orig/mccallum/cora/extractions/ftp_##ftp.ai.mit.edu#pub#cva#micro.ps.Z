URL: ftp://ftp.ai.mit.edu/pub/cva/micro.ps.Z
Refering-URL: http://www.ai.mit.edu/people/billd/billd.html
Root-URL: 
Title: The Message-Driven Processor: A Multicomputer Processing Node with Efficient Mechanisms  
Author: William J. Dally, Roy Davison J.A. Stuart Fiske, Greg Fyler John S. Keen, Richard A. Lethin, Michael Noakes, Peter R. Nuth 
Address: 545 Technology Square 3065 Bowers Avenue Cambridge, MA 02139 Santa Clara, CA 95052-8131  
Affiliation: Artificial Intelligence Laboratory Platform Architecture Group Massachusetts Institute of Technology Intel Corporation  
Abstract: The Message-Driven Processor (MDP) is an integrated multicomputer node that provides efficient mechanisms for parallel computing. It incorporates a 36-bit integer processor, a memory management unit, a router for a 3-D mesh network, a network interface, a 4K-word fi 36-bit SRAM, and an ECC DRAM controller in a single 1.1M transistor VLSI chip. Rather than being specialized for a single model of computation, the MDP incorporates efficient primitive mechanisms for communication, synchronization and naming. These mechanisms efficiently support most proposed parallel programming models. Each processing node of the MIT J-Machine consists of an MDP with 1 MByte of DRAM. MDPs have been operational since June 1991 and J-Machines built from them have been on-line since July 1991.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Gul Agha. </author> <title> Actors: A Model of Concurrent Computation in Distributed Systems. </title> <type> Technical Report 844, </type> <institution> Massachusetts Institute of Technology Artificial Intelligence Laboratory, </institution> <address> Cambridge, MA 02139, </address> <year> 1985. </year>
Reference-contexts: This cost prohibits the use of fine-grain programming models where processes typically last only a few tens of instructions. The MDP supports a broad range of parallel programming models (including shared-memory [16], data parallel [17], dataflow [26], actors <ref> [1] </ref>, and explicit message-passing [5]) by providing low- overhead primitive mechanisms for communication, synchronization, and naming. Communication mechanisms are provided that permit a user-level task on one node to send a message to any other node in a 4K-node machine in &lt; 2s. <p> To date, two languages have been implemented to the J-machine: The actor language Concurrent Smalltalk (CST) and the dataflow language Id. Concurrent Smalltalk: CST [18] is a parallel object-oriented programming language (based on the Actor model <ref> [1] </ref>) with asynchronous message send and distributed objects. Syntax is similar to that of LISP or SCHEME. Method or function invocation is performed by sending a message to the first argument of the method. The message contains the method selector and the rest of the arguments.
Reference: [2] <institution> Ametek Computer Research Division. Series 2010 Product Description, </institution> <year> 1987. </year>
Reference-contexts: Messages traveling through the network follow a Manhattan shortest path in physical space; they never backtrack. Background: The MDP builds on previous work in multicomputer design. Like the Caltech Cosmic Cube [4], the Intel iPSC [3], the N-CUBE [24], and the Ametek <ref> [2] </ref>, each MDP in the J- Machine has a local memory and communicates with other nodes by passing messages. Because of its low overhead, the MDP can exploit concurrency at a much finer grain than these early message- passing multicomputers.
Reference: [3] <author> Ramune Arlauskas. </author> <title> iPSC/2 System: A Second Generation Hypercube. </title> <booktitle> In Third Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 33-36. </pages> <publisher> ACM, </publisher> <year> 1988. </year>
Reference-contexts: No communication bandwidth is wasted by embedding an esoteric topology into physical space. Messages traveling through the network follow a Manhattan shortest path in physical space; they never backtrack. Background: The MDP builds on previous work in multicomputer design. Like the Caltech Cosmic Cube [4], the Intel iPSC <ref> [3] </ref>, the N-CUBE [24], and the Ametek [2], each MDP in the J- Machine has a local memory and communicates with other nodes by passing messages. Because of its low overhead, the MDP can exploit concurrency at a much finer grain than these early message- passing multicomputers.
Reference: [4] <author> William C. Athas and Charles L. Seitz. </author> <title> Multicomputers. </title> <type> Technical Report 5244:TR:87, </type> <institution> Computer Science Dept., Caltech, Pasadena, </institution> <address> CA 91125, </address> <year> 1987. </year> <month> 24 </month>
Reference-contexts: No communication bandwidth is wasted by embedding an esoteric topology into physical space. Messages traveling through the network follow a Manhattan shortest path in physical space; they never backtrack. Background: The MDP builds on previous work in multicomputer design. Like the Caltech Cosmic Cube <ref> [4] </ref>, the Intel iPSC [3], the N-CUBE [24], and the Ametek [2], each MDP in the J- Machine has a local memory and communicates with other nodes by passing messages.
Reference: [5] <author> William C. Athas and Charles L. Seitz. </author> <title> Multicomputers: Message-Passing Concurrent Computers. </title> <journal> Computer, </journal> <volume> 21(8):9 - 24, </volume> <month> August </month> <year> 1988. </year>
Reference-contexts: This cost prohibits the use of fine-grain programming models where processes typically last only a few tens of instructions. The MDP supports a broad range of parallel programming models (including shared-memory [16], data parallel [17], dataflow [26], actors [1], and explicit message-passing <ref> [5] </ref>) by providing low- overhead primitive mechanisms for communication, synchronization, and naming. Communication mechanisms are provided that permit a user-level task on one node to send a message to any other node in a 4K-node machine in &lt; 2s.
Reference: [6] <author> Iann Barron, Peter Cavill, David May, and Pete Wilson. </author> <title> Transputer Does Five or More MIPS Even When Not Used in Parallel. </title> <publisher> Electronics, </publisher> <pages> pages 109-115, </pages> <month> November </month> <year> 1983. </year>
Reference-contexts: The same IDs (virtual addresses) are used to reference local (on the same node) and remote (on a different node) objects. Like the InMOS transputer <ref> [6] </ref>, the Caltech MOSAIC [20], and the Intel iWARP [7], the MDP is a single chip processing element integrating a processor, memory, and a communication unit. The MDP is unique in that it extends these previous efforts with efficient primitive mechanisms for communication, synchronization and naming [15]. <p> Communication and synchronization performance is competitive with processing nodes specialized to a single model of computation such as iWARP [7] (systolic) or the Transputer <ref> [6] </ref> (CSP). Computers built from fine-grain processing nodes, such as the MDP, consisting of a small but powerful processor and a small amount of memory, are more cost-effective than those built from fewer coarse-grain nodes.
Reference: [7] <author> Shekhar Borkar et al. </author> <title> iWARP: An Integrated Solution to High-Speed Parallel Computing. </title> <booktitle> In Proceedings of the Supercomputing Conference, </booktitle> <pages> pages 330-338. </pages> <publisher> IEEE, </publisher> <month> November </month> <year> 1988. </year>
Reference-contexts: The same IDs (virtual addresses) are used to reference local (on the same node) and remote (on a different node) objects. Like the InMOS transputer [6], the Caltech MOSAIC [20], and the Intel iWARP <ref> [7] </ref>, the MDP is a single chip processing element integrating a processor, memory, and a communication unit. The MDP is unique in that it extends these previous efforts with efficient primitive mechanisms for communication, synchronization and naming [15]. <p> Communication and synchronization performance is competitive with processing nodes specialized to a single model of computation such as iWARP <ref> [7] </ref> (systolic) or the Transputer [6] (CSP). Computers built from fine-grain processing nodes, such as the MDP, consisting of a small but powerful processor and a small amount of memory, are more cost-effective than those built from fewer coarse-grain nodes.
Reference: [8] <author> W.C. Brantley, K.P. McAuliffe, and J. Weiss. </author> <title> RP3 Processor-Memory Element. </title> <journal> IEEE Transactions on Computers, </journal> <pages> pages 782-789, </pages> <year> 1985. </year>
Reference-contexts: Delivering a message and dispatching a task in response to the message's arrival takes &lt; 2s on the J-Machine, as opposed to 5ms on an iPSC-1 or 300s on an iPSC-2. Like the BBN Butterfly [19] and the IBM RP3 <ref> [8] </ref>, the MDP supports a global virtual address 2 and a diagnostic port (3 pins). space. The same IDs (virtual addresses) are used to reference local (on the same node) and remote (on a different node) objects.
Reference: [9] <author> David E. Culler, Anurag Sah, Klaus Erik Shauser, Thorsten von Eicken, and John Wawrzynek. </author> <title> FineGrain Parallelism with Minimal Hardware Support: A Compiler-Controlled Threaded Abstract Machine. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 164-175, </pages> <year> 1991. </year>
Reference-contexts: An Id program can be converted into a dataflow graph, in which operators are represented by nodes and dependencies by arcs. Originally, these dataflow graphs were executed directly on specialized dataflow machines; more recently, researchers have begun compiling dataflow graphs to run on general-purpose parallel machines <ref> [9] </ref>. Dataflow programs are well-suited to large parallel computers, because the abundance of fine-grained tasks, each of which can be as small as a single dataflow operator, makes it easy to mask communication latency with task switches. <p> This makes use of the CFUT tag and fault handler. A more efficient approach is to increase the granularity each task in order to reduce scheduling overhead. We are currently building a system on top of the Berkeley TAM project <ref> [9] </ref> that addresses the inefficiencies of our earlier systems. 7 Concluding Remarks The MDP has been built to demonstrate the utility of general-purpose communication, synchronization and naming mechanisms in a multicomputer building block.
Reference: [10] <author> William J. Dally. </author> <title> Performance Analysis of k-ary n-cube Interconnection Networks. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 39(6), </volume> <month> June </month> <year> 1990. </year>
Reference-contexts: The 3-D network that connects MDPs gives the highest throughput and lowest latency for a given wire density <ref> [10] </ref>. This network makes use of all three dimensions of physical space. It allows the processing nodes to be packed densely and results in uniformly short wires. No communication bandwidth is wasted by embedding an esoteric topology into physical space. <p> The MDP is unique in that it extends these previous efforts with efficient primitive mechanisms for communication, synchronization and naming [15]. The MDP uses a direct communication network based on the work reported in <ref> [12, 10, 14] </ref>. The MDP was built as part of a collaborative project between MIT and Intel Corporation. The project was funded by the Defense Advanced Research Projects Agency. Outline: The next section describes the system architecture of the MDP from hardware and software points of view. <p> The netin module reassembles messages at their destination and buffers them into a message queue. Section 5 contains more implementation details. Engineering: The three-dimensional mesh topology of the J-Machine network was chosen as the most efficient arrangement subject to constraints of wiring density and component pinout <ref> [10] </ref>. These constraints set the width of the six bidirectional channels per MDP node at 9 data bits plus 6 control bits. The J-machine is built as a stack of boards with dense board-to-board interconnections (Section 5) to implement the 3-D network with short wires. 11 or destination tag routing.
Reference: [11] <author> William J. Dally et al. </author> <title> Architecture of a Message-Driven Processor. </title> <booktitle> In Proceedings of the 14th International Symposium on Computer Architecture, </booktitle> <pages> pages 189-205. </pages> <publisher> IEEE, Computer Society Press, </publisher> <month> June </month> <year> 1987. </year>
Reference-contexts: Specifically, the MDP provides efficient hardware mechanisms for communication, synchronization and naming. This section describes the MDP ISA with particular emphasis on these mechanisms. Further details of the MDP ISA are described in <ref> [11] </ref>. Register Set: The MDP provides separate register sets to support rapid switching between three execution levels: background, priority 0 (P0), and priority 1 (P1). The MDP executes at the background level when there are no pending messages.
Reference: [12] <author> William J. Dally and Charles L. Seitz. </author> <title> The Torus Routing Chip. </title> <journal> Distributed Computing, </journal> <volume> 1 </volume> <pages> 187-196, </pages> <year> 1986. </year>
Reference-contexts: The MDP is unique in that it extends these previous efforts with efficient primitive mechanisms for communication, synchronization and naming [15]. The MDP uses a direct communication network based on the work reported in <ref> [12, 10, 14] </ref>. The MDP was built as part of a collaborative project between MIT and Intel Corporation. The project was funded by the Defense Advanced Research Projects Agency. Outline: The next section describes the system architecture of the MDP from hardware and software points of view. <p> If the two indices match, the head flit is stripped off the message, and the rest is routed down to the next dimension. The MDP's 1 This breaks with a tradition of asynchronous network routers <ref> [12, 14] </ref>. 2 A phit is a physical digit, the width of the physical channel. A pair of phits form a flit, or flow-control digit, the granularity of flow-control in the network.
Reference: [13] <author> William J. Dally and Charles L. Seitz. </author> <title> Deadlock Free Message Routing in Multiprocessor Interconnection Networks. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(5):547-53, </volume> <month> May </month> <year> 1987. </year>
Reference-contexts: Since messages are routed in dimension order and messages running in opposite directions along the same dimension do not block, there are no resource cycles, and the network is provably deadlock free <ref> [13] </ref>. Y and Z addresses. At each node along the path, the address in the head flit of the message is compared with the node's index in the current dimension.
Reference: [14] <author> William J. Dally and Paul Song. </author> <title> Design of a Self-Timed VLSI Multicomputer Communication Controller. </title> <booktitle> In Proceedings of the International Conference on Computer Design, </booktitle> <pages> pages 230-234. </pages> <publisher> IEEE, Computer Society Press, </publisher> <month> October </month> <year> 1987. </year>
Reference-contexts: The MDP is unique in that it extends these previous efforts with efficient primitive mechanisms for communication, synchronization and naming [15]. The MDP uses a direct communication network based on the work reported in <ref> [12, 10, 14] </ref>. The MDP was built as part of a collaborative project between MIT and Intel Corporation. The project was funded by the Defense Advanced Research Projects Agency. Outline: The next section describes the system architecture of the MDP from hardware and software points of view. <p> If the two indices match, the head flit is stripped off the message, and the rest is routed down to the next dimension. The MDP's 1 This breaks with a tradition of asynchronous network routers <ref> [12, 14] </ref>. 2 A phit is a physical digit, the width of the physical channel. A pair of phits form a flit, or flow-control digit, the granularity of flow-control in the network.
Reference: [15] <author> William J. Dally, D. Scott Wills, and Richard Lethin. </author> <title> Mechanisms for Parallel Computing. </title> <booktitle> In Proceedings of the NATO Advanced Study Institute on "Parallel Computing on Distributed Memory Mutliprocessors". </booktitle> <publisher> Springer, </publisher> <year> 1991. </year>
Reference-contexts: Efficient Parallel Mechanisms Support Many Programming Models: Mechanisms are the primitive operations provided by a computer's hardware. The abstractions that make up a programming system are built from these mechanisms <ref> [15] </ref>. For example, most sequential machines provide some mechanism for a push-down stack to support the last-in-first-out (LIFO) storage 1 allocation required by many sequential programming models. <p> The MDP is unique in that it extends these previous efforts with efficient primitive mechanisms for communication, synchronization and naming <ref> [15] </ref>. The MDP uses a direct communication network based on the work reported in [12, 10, 14]. The MDP was built as part of a collaborative project between MIT and Intel Corporation. The project was funded by the Defense Advanced Research Projects Agency.
Reference: [16] <author> Allan Gottlieb, Ralph Grishman, Clyde P. Kruskal, Kevin P. McAuliffe, Larry Rudolph, and Marc Snir. </author> <title> The NYU Ultracomputer Designing a MIMD Shared Memory Parallel Computer. </title> <journal> IEEE Transactions on Computers, </journal> <volume> c-32(2):175-189, </volume> <month> February </month> <year> 1983. </year>
Reference-contexts: For example, sequential machines require hundreds of instructions to create a new process. This cost prohibits the use of fine-grain programming models where processes typically last only a few tens of instructions. The MDP supports a broad range of parallel programming models (including shared-memory <ref> [16] </ref>, data parallel [17], dataflow [26], actors [1], and explicit message-passing [5]) by providing low- overhead primitive mechanisms for communication, synchronization, and naming. Communication mechanisms are provided that permit a user-level task on one node to send a message to any other node in a 4K-node machine in &lt; 2s.
Reference: [17] <author> W. Daniel Hillis and Guy L. Steele. </author> <title> Data Parallel Algorithms. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1170-1183, </pages> <year> 1986. </year>
Reference-contexts: For example, sequential machines require hundreds of instructions to create a new process. This cost prohibits the use of fine-grain programming models where processes typically last only a few tens of instructions. The MDP supports a broad range of parallel programming models (including shared-memory [16], data parallel <ref> [17] </ref>, dataflow [26], actors [1], and explicit message-passing [5]) by providing low- overhead primitive mechanisms for communication, synchronization, and naming. Communication mechanisms are provided that permit a user-level task on one node to send a message to any other node in a 4K-node machine in &lt; 2s.
Reference: [18] <author> Waldemar Horwat. </author> <title> Concurrent Smalltalk on the Message-Driven Processor. </title> <type> Master's thesis, </type> <institution> MIT, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: To date, two languages have been implemented to the J-machine: The actor language Concurrent Smalltalk (CST) and the dataflow language Id. Concurrent Smalltalk: CST <ref> [18] </ref> is a parallel object-oriented programming language (based on the Actor model [1]) with asynchronous message send and distributed objects. Syntax is similar to that of LISP or SCHEME. Method or function invocation is performed by sending a message to the first argument of the method. <p> We need to study how we can effectively and automatically throttle the parallelism being created when the machine is saturated. These issues, and others related to the efficiency of programming fine-grained parallel processors, are discussed in more detail in <ref> [18] </ref>. Dataflow Implementation: Id [21] is a functional programming language originally designed for dataflow architectures. An Id program can be converted into a dataflow graph, in which operators are represented by nodes and dependencies by arcs. <p> Its mechanisms have been shown to efficiently support dataflow [25] and object-oriented programming <ref> [18] </ref> models using a shared address space. Our studies have shown that the use of a few simple mechanisms provides orders of magnitude lower communication and synchronization overhead than is possible with multicomput- ers built from off-the-shelf microprocessors.
Reference: [19] <author> BBN Advanced Computers Incorporated. </author> <title> Butterfly Parallel Processor Overview. </title> <type> BBN Report No. 6148, </type> <month> March. </month> <year> 1986. </year>
Reference-contexts: Delivering a message and dispatching a task in response to the message's arrival takes &lt; 2s on the J-Machine, as opposed to 5ms on an iPSC-1 or 300s on an iPSC-2. Like the BBN Butterfly <ref> [19] </ref> and the IBM RP3 [8], the MDP supports a global virtual address 2 and a diagnostic port (3 pins). space. The same IDs (virtual addresses) are used to reference local (on the same node) and remote (on a different node) objects.
Reference: [20] <author> C. Lutz et al. </author> <title> Design of the Mosaic Element. </title> <booktitle> In Proc. MIT Conference on Advanced Research in VLSI, </booktitle> <pages> pages 1-10. </pages> <publisher> Artech Books, </publisher> <year> 1984. </year>
Reference-contexts: The same IDs (virtual addresses) are used to reference local (on the same node) and remote (on a different node) objects. Like the InMOS transputer [6], the Caltech MOSAIC <ref> [20] </ref>, and the Intel iWARP [7], the MDP is a single chip processing element integrating a processor, memory, and a communication unit. The MDP is unique in that it extends these previous efforts with efficient primitive mechanisms for communication, synchronization and naming [15].
Reference: [21] <author> Rishiyur S. Nikhil. </author> <title> ID Version 88.1 Reference Manual. </title> <type> Technical Report 284, </type> <institution> Computation Structure Group, MIT, </institution> <address> Cambridge, MA 02138, </address> <year> 1988. </year>
Reference-contexts: We need to study how we can effectively and automatically throttle the parallelism being created when the machine is saturated. These issues, and others related to the efficiency of programming fine-grained parallel processors, are discussed in more detail in [18]. Dataflow Implementation: Id <ref> [21] </ref> is a functional programming language originally designed for dataflow architectures. An Id program can be converted into a dataflow graph, in which operators are represented by nodes and dependencies by arcs.
Reference: [22] <author> Michael Noakes and William J. Dally. </author> <title> System Design of the J-Machine. </title> <editor> In William J. Dally, editor, </editor> <booktitle> Sixth MIT Conference of Advanced Research in VLSI, </booktitle> <pages> pages 179-194, </pages> <address> Cambridge, MA 02139, 1990. </address> <publisher> The MIT Press. </publisher>
Reference-contexts: In addition to the processor board and chassis, we have also designed a diagnostic interface board, and are in the process of designing a SCSI disk interface, a distributed graphics frame buffer, and an S-bus interface. More details of the J-Machine system design are given in <ref> [22] </ref>. 6 Software The J-machine is intended as a platform for software experiments in fine-grained parallel programming. To this end, we have implemented and are studying software systems for different fine-grain programming models. Fine-grained programs typically execute from 10-100 instructions between communication and synchronization actions.
Reference: [23] <author> Peter R. Nuth. </author> <title> Router Protocol. MIT Concurrent VLSI Architecture Group Internal Memo 23, </title> <year> 1990. </year>
Reference-contexts: Each of the six bidirectional channels can be turned around on alternate cycles with no contention penalty. A novel pad design tolerates clock skew between routers and eliminates the potential for conduction overlap when the channel is turned around <ref> [23] </ref>. Messages are routed through the network with a latency of one 62.5ns processor cycle per hop.
Reference: [24] <author> John F. Palmer. </author> <title> The NCUBE Family of Parallel Supercomputers. </title> <booktitle> In Proc. IEEE International Conference on Computer Design, </booktitle> <pages> page 107. </pages> <publisher> IEEE, </publisher> <year> 1986. </year>
Reference-contexts: Messages traveling through the network follow a Manhattan shortest path in physical space; they never backtrack. Background: The MDP builds on previous work in multicomputer design. Like the Caltech Cosmic Cube [4], the Intel iPSC [3], the N-CUBE <ref> [24] </ref>, and the Ametek [2], each MDP in the J- Machine has a local memory and communicates with other nodes by passing messages. Because of its low overhead, the MDP can exploit concurrency at a much finer grain than these early message- passing multicomputers.
Reference: [25] <author> Ellen Spertus and William J. Dally. </author> <title> Experiments with Dataflow on a General-Purpose Parallel Computer. </title> <booktitle> In Proceedings of International Conference on Parallel Processing, </booktitle> <pages> pages II231-II235, </pages> <month> Aug </month> <year> 1991. </year>
Reference-contexts: Con- versely, the J-machine's fine-grained mechanisms make it an excellent target for dataflow programs. We have experimented with several methods of executing dataflow programs on the J-machine <ref> [25] </ref>. The simplest of the systems translates each node of the dataflow graph into a sequence of MDP instructions. <p> Its mechanisms have been shown to efficiently support dataflow <ref> [25] </ref> and object-oriented programming [18] models using a shared address space. Our studies have shown that the use of a few simple mechanisms provides orders of magnitude lower communication and synchronization overhead than is possible with multicomput- ers built from off-the-shelf microprocessors.
Reference: [26] <author> Arthur H. Veen. </author> <title> Dataflow Machine Architecture. </title> <journal> ACM Computing Surveys, </journal> <volume> 18(4) </volume> <pages> 365-396, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: For example, sequential machines require hundreds of instructions to create a new process. This cost prohibits the use of fine-grain programming models where processes typically last only a few tens of instructions. The MDP supports a broad range of parallel programming models (including shared-memory [16], data parallel [17], dataflow <ref> [26] </ref>, actors [1], and explicit message-passing [5]) by providing low- overhead primitive mechanisms for communication, synchronization, and naming. Communication mechanisms are provided that permit a user-level task on one node to send a message to any other node in a 4K-node machine in &lt; 2s.
Reference: [27] <author> Pen-Chung Yew, Nian-Feng Tzeng, and Duncan H. Lawrie. </author> <title> Distributing Hot-Spot Addressing in Large-Scale Multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(4):388-395, </volume> <month> April </month> <year> 1987. </year> <month> 25 </month>
Reference-contexts: The systems programmer uses these mechanisms to implement a programming model. For example, one can build a shared memory model that gives the application programmer a single, shared address space. The implementation of a combining tree <ref> [27] </ref> illustrates the use of the MDP mechanisms. The combining tree (Figure 3) consists of a number of nodes each containing a value, a count, and a pointer to a parent node. The value is initialized to zero and count is initialized to the number of inputs expected.
References-found: 27

