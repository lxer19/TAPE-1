URL: ftp://ftp.cs.umass.edu/pub/anw/pub/sutton/precup-sutton-98.ps
Refering-URL: http://www-anw.cs.umass.edu/~rich/publications.html
Root-URL: 
Title: Multi-time Models for Temporally  
Abstract: Planning Abstract Planning and learning at multiple levels of temporal abstraction is a key problem for artificial intelligence. In this paper we summarize an approach to this problem based on the mathematical framework of Markov decision processes and reinforcement learning. Current model-based reinforcement learning is based on one-step models that cannot represent common-sense higher-level actions, such as going to lunch, grasping an object, or flying to Denver. This paper generalizes prior work on temporally abstract models (Sutton, 1995b) and extends it from the prediction setting to include actions, control, and planning. We introduce a more general form of temporally abstract model, the multi-time model, and establish its suitability for planning and learning by virtue of its relationship to Bellman equations. This paper summarizes the theoretical framework of multi-time models and illustrates their potential ad The need for hierarchical and abstract planning is a fundamental problem in AI (see, e.g., Sacerdoti, 1977; Laird et al., 1986; Korf, 1985; Kaelbling, 1993; Dayan & Hinton, 1993). Model-based reinforcement learning offers a possible solution to the problem of integrating planning with real-time learning and decision-making (Peng & Williams, 1993, Moore & Atkeson, 1993; Sutton and Barto, in press). However, current model-based reinforcement learning is based on one-step models that cannot represent common-sense, higher-level actions. Modeling such actions requires the ability to handle different, interrelated levels of temporal abstraction. A new approach to modeling at multiple time scales was introduced by Sutton (1995b) based on prior work by Singh (1992), Dayan (1993b), and Sutton and Pinette (1985). This approach enables models of the environment at different temporal scales to be intermixed, producing temporally abstract models. However, that work was concerned only with predicting the environment. This paper summarizes vantages in a gridworld planning task.
Abstract-found: 1
Intro-found: 1
Reference: <author> Dayan, P., & Hinton, G. E. </author> <year> (1993a). </year> <title> Feudal reinforcement learning. </title> <booktitle> Advances in Neural Information Processing Systems 5 (pp. </booktitle> <pages> 271-278). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Dayan, P. </author> <year> (1993b). </year> <title> Improving generalization for temporal difference learning: </title> <booktitle> The successor representation. Neural Computation, </booktitle> <volume> 5, </volume> <pages> 613-624. </pages>
Reference: <author> Kaelbling, L. P. </author> <year> (1993). </year> <title> Hierarchical learning in stochastic domains: Preliminary results. </title> <booktitle> Proceedings of the Tenth International Conference on Machine Learning (pp. </booktitle> <pages> 167-173). </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Korf, R. E. </author> <year> (1985). </year> <title> Learning to solve problems by searching for macro-operators. </title>
Reference: <author> Laird, J. E., Rosenbloom, P. S., & Newell, A. </author> <year> (1986). </year> <title> Chunking in SOAR: The anatomy of a general learning mechanism. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 11-46. </pages>
Reference: <author> Moore, A. W., & Atkeson, C. G. </author> <year> (1993). </year> <title> Prioritized sweeping: Reinforcement learning with less data and less time. </title> <booktitle> Machine Learning, </booktitle> <pages> 103-130. </pages>
Reference: <author> Peng, J., & Williams, J. </author> <year> (1993). </year> <title> Efficient learning and planning within the Dyna framework. Adaptive Behaviour, </title> <booktitle> 4, </booktitle> <pages> 323-334. </pages>
Reference: <author> Precup, D., & Sutton, R. S. </author> <year> (1997). </year> <title> Multi-time models for reinforcement learning. </title> <booktitle> ICML'97 Workshop on Modelling in Reinforcement Learning. </booktitle>
Reference-contexts: This enables models acting at different time scales to be mixed together, and the resulting model can still be used to compute v . We have proven that the set of NOP models is also closed under composition and averaging <ref> (Precup & Sutton, 1997) </ref>. <p> <ref> (Precup & Sutton, 1997) </ref>. These operations permit a richer variety of combinations for NOP models than they do for valid models because the NOP models that are combined need not correspond to a particular policy. 3 Multi-time models The validity and NOP-ness of a model do not imply each other (Precup & Sutton, 1997). Nevertheless, we believe a good model should be both valid and NOP.
Reference: <author> Sacerdoti, E. D. </author> <year> (1974). </year> <title> Planning in a hierarchy of abstraction spaces. </title> <journal> Artificial Intelligence, </journal> <volume> 5, </volume> <pages> 115-135. </pages>
Reference: <author> Singh, S. P. </author> <year> (1992). </year> <title> Scaling reinforcement learning by learning variable temporal resolution models. </title> <booktitle> Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> (pp. 202-207). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Sutton, R. S., & Pinette, B. </author> <year> (1985). </year> <title> The learning of worlds models by connectionist networks. </title> <booktitle> Proceedings of the Seventh Annual Conference of the Cognitive Science Society (pp. </booktitle> <pages> 54-64). </pages>
Reference: <author> Sutton, R. S. </author> <year> (1995a). </year> <title> TD models: Modeling the world as a mixture of time scales, </title> <type> (Technical Report 95-114), </type> <institution> Amherst, MA: University of Massachusetts, Department of Computer Science. </institution>
Reference-contexts: The random weights along each trajectory make this a very general form of model. The only necessary constraint is that the weights depend only on previously visited states. In particular, we can choose weighting sequences that generate the types of multi-step models described in <ref> (Sutton, 1995a) </ref>. If the weighting variables are such that w n =1, and w t = 0 8t 6= n, we obtain n-step models.
Reference: <author> Sutton, R. S. </author> <year> (1995b). </year> <title> TD models: Modeling the world as a mixture of time scales. </title> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning (pp. </booktitle> <pages> 531-539). </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: It is useful to define special terms for consistency with each Bellman equation. Let g; P denote an arbitrary model (an m-vector and an m fi m matrix). Then this model is said to be valid for policy <ref> (Sutton, 1995b) </ref> if and only if lim k!1 P k = 0 and v = g + P v : (3) Any valid model can be used to compute v via the iteration algorithm v k+1 k . <p> If different n-step models of the same policy are averaged, the result is called a mixture model . Mixtures are valid and non-overpromising due to the closure properties established in the previous section. One kind of mixture suggested in <ref> (Sutton, 1995b) </ref> allows an exponential decay of the weights over time, controlled by a parameter fi. <p> However, n-step models, or any linear mixture of n-step models cannot achieve this goal. In order to remediate this problem, models should average differently over all the different trajectories that are possible through the state space. A full fi-model <ref> (Sutton, 1995b) </ref> can distinguish between these two situations. A fi-model is a more general form of mixture model, in which a different fi parameter is associated to each state. <p> We used Q-learning (Watkins, 1989) to learn the optimal state-action value function Q fl U;B associated with each abstract action. The greedy policy with respect to Q fl U;B is the policy associated with the abstract action. At the same time, we used the fi-model learning algorithm presented in <ref> (Sutton, 1995b) </ref> to compute the model corresponding policy. The learning algorithm is completely online and incremental, and its complexity is comparable to that of regular 1-step TD-learning. Models of abstract actions can be built while an agent is acting in the environment without any additional effort.
Reference: <author> Sutton, R. S., & Barto, A. G. </author> <year> (1997). </year> <title> An introduction to reinforcement learning. </title> <publisher> in press. </publisher>
Reference-contexts: This enables models acting at different time scales to be mixed together, and the resulting model can still be used to compute v . We have proven that the set of NOP models is also closed under composition and averaging <ref> (Precup & Sutton, 1997) </ref>. <p> <ref> (Precup & Sutton, 1997) </ref>. These operations permit a richer variety of combinations for NOP models than they do for valid models because the NOP models that are combined need not correspond to a particular policy. 3 Multi-time models The validity and NOP-ness of a model do not imply each other (Precup & Sutton, 1997). Nevertheless, we believe a good model should be both valid and NOP.
Reference: <author> Watkins, C.J.C.H. </author> <year> (1989). </year> <title> Learning with delayed rewards. </title> <type> Doctoral dissertation, </type> <institution> Psychology Department, Cam-bridge University. </institution>
Reference-contexts: The starting point for learning was represented by the outcome states of each abstract action, along with the hypothetical utilities U associated to these states. We used Q-learning <ref> (Watkins, 1989) </ref> to learn the optimal state-action value function Q fl U;B associated with each abstract action. The greedy policy with respect to Q fl U;B is the policy associated with the abstract action.
References-found: 15

