URL: http://www.cis.hut.fi/~aapo/ps/ICASSP97.ps
Refering-URL: http://www.bmc.riken.go.jp/sensor/Allan/ICA/index.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: aapo.hyvarinen@hut.fi  
Title: A FAMILY OF FIXED-POINT ALGORITHMS FOR INDEPENDENT COMPONENT ANALYSIS  
Author: Aapo Hyvrinen 
Address: Rakentajanaukio 2 C, FIN-02150 Espoo, Finland  
Affiliation: Helsinki University of Technology Laboratory of Computer and Information Science  
Abstract: Independent Component Analysis (ICA) is a statistical signal processing technique whose main applications are blind source separation, blind deconvolution, and feature extraction. Estimation of ICA is usually performed by optimizing a 'contrast' function based on higher-order cumulants. In this paper, it is shown how almost any error function can be used to construct a contrast function to perform the ICA estimation. In particular, this means that one can use contrast functions that are robust against outliers. As a practical method for tnding the relevant extrema of such contrast functions, a txed-point iteration scheme is then introduced. The resulting algorithms are quite simple and converge fast and reliably. These algorithms also enable estimation of the independent components one-by-one, using a simple deation scheme. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Jutten and J. Herault, </author> <title> Blind separation of sources, part I: An adaptive algorithm based on neuromimetic architecture, </title> <booktitle> Signal Processing, </booktitle> <volume> vol. 24, </volume> <pages> pp. 110, </pages> <year> 1991. </year>
Reference: [2] <author> P. Comon, </author> <title> Independent component analysis a new concept?, </title> <booktitle> Signal Processing, </booktitle> <volume> vol. 36, </volume> <pages> pp. 287314, </pages> <year> 1994. </year>
Reference-contexts: Using contrast functions that grow slower than the fourth power, we can tnd algorithms that are more robust against outliers. To begin with, note that one intuitive interpretation of contrast functions is that they are measures of non-normality <ref> [2] </ref>. Therefore, to obtain a contrast function based on an arbitrary error function G, it is natural to consider the difference of the expectation of G for actual data from what it would be for a Gaussian variable. <p> The only condition is that the IC s i in question must fultll Efs i g (s i ) g 0 (s i )g 6= 0, which is reminiscent of the classical condition of non-zero kurtosis <ref> [2] </ref>. We also conjecture that for a 'reasonable' choice of G, J G is a global contrast function. <p> This can be considered a generalization of the condition, valid when kurtosis is used as contrast, that kurtosis of the ICs must be non-zero <ref> [2, 8, 4] </ref>. We prove in [10] analytically only the local convergence of the algorithms, i.e. convergence for initial points near a solution.
Reference: [3] <author> A. Bell and T. J. Sejnowski, </author> <title> Edges are the independent components of natural scenes, </title> <booktitle> in NIPS*96, </booktitle> <address> (Denver, Colorado), </address> <year> 1996. </year>
Reference: [4] <author> A. Hyvrinen and E. Oja, </author> <title> One-unit learning rules for independent component analysis, </title> <booktitle> in NIPS*96, </booktitle> <address> (Den-ver, Colorado), </address> <year> 1996. </year>
Reference-contexts: The algorithm presented in this section is especially easy to analyze mathematically. For practical purposes, however, the generalization of this algorithm, to be introduced in the following sections, is much better. 2.1. Kurtosis as a contrast function Kurtosis, or the fourth-order cumulant <ref> [4] </ref>, is detned for a zero-mean random variable v as kurt (v) = Efv 4 g3 (Efv 2 g) 2 . Kurtosis is a contrast function for ICA in the following sense. <p> When w T x = s i for some i, i.e. when the linear combination equals, up to the sign, one of the ICs, the kurtosis of w T x is locally minimized or maximized <ref> [4, 8] </ref>. This property is widely used in ICA algorithms, and forms also the basis of the txed-point algorithm presented in this section. 2.2. Derivation of a txed-point algorihm Now we derive a txed-point algorithm to tnd the relevant extrema of kurtosis. <p> Derivation of a txed-point algorihm Now we derive a txed-point algorithm to tnd the relevant extrema of kurtosis. This is a moditcation for non-whitened data of the algorithm presented in <ref> [4, 9] </ref>. <p> THE GENERALIZED FIXED-POINT ALGORITHMS The actual search for the extrema of a general contrast function J G as in (6) may then be performed in many dierent ways, e.g. by (stochastic) gradient descent, as was done with kurtosis in, e.g. <ref> [4, 8] </ref>. This adaptive, neural-like approach is developed elsewhere. However, as was explained in Section 2, a highly ecient, reliable and simple algorithm for tnding the relevant extrema can be obtained using the a contrast function in a more global and strict sense. Here G (x) = ln cosh (x). <p> Dashed line: Two sub-Gaussian ICs. The maxima of the absolute value of J G were always obtained when the angle was 0 or =2, which were exactly the directions of the ICs. txed-point method. Thus we obtain an important generalization of the txed-point algorithm presented in <ref> [4, 9] </ref>. 4.1. <p> This can be considered a generalization of the condition, valid when kurtosis is used as contrast, that kurtosis of the ICs must be non-zero <ref> [2, 8, 4] </ref>. We prove in [10] analytically only the local convergence of the algorithms, i.e. convergence for initial points near a solution. <p> In fact then the estimations using (10) were practically perfect in spite of the added outliers. For details, see [10]. 6. CONCLUSIONS We introduced a generalized version of the txed-point algorithms presented in <ref> [4, 9] </ref> for ICA estimation. <p> Some applications of ICA using the generalized txed-point algorithm, or the original txed-point algorithm in <ref> [4, 9] </ref>, are described in [11].
Reference: [5] <author> J.-F. Cardoso, </author> <title> Iterative techniques for blind source separation using only fourth-order cumulants, </title> <booktitle> in Proc. EUSIPCO, </booktitle> <pages> pp. 739742, </pages> <year> 1992. </year>
Reference: [6] <author> A. Bell and T. Sejnowski, </author> <title> An information-maximization approach to blind separation and blind deconvolution, </title> <journal> Neural Computation, </journal> <volume> vol. 7, </volume> <pages> pp. 1129 1159, </pages> <year> 1995. </year>
Reference: [7] <author> A. Cichocki, S. I. Amari, and R. Thawonmas, </author> <title> Blind signal extraction using self-adaptive non-linear heb-bian learning rule, </title> <booktitle> in Proc. NOLTA'96, </booktitle> <pages> pp. 377380, </pages> <year> 1996. </year>
Reference: [8] <author> N. Delfosse and P. Loubaton, </author> <title> Adaptive blind separation of independent sources: a deation approach, </title> <booktitle> Signal Processing, </booktitle> <volume> vol. 45, </volume> <pages> pp. 5983, </pages> <year> 1995. </year>
Reference-contexts: When w T x = s i for some i, i.e. when the linear combination equals, up to the sign, one of the ICs, the kurtosis of w T x is locally minimized or maximized <ref> [4, 8] </ref>. This property is widely used in ICA algorithms, and forms also the basis of the txed-point algorithm presented in this section. 2.2. Derivation of a txed-point algorihm Now we derive a txed-point algorithm to tnd the relevant extrema of kurtosis. <p> THE GENERALIZED FIXED-POINT ALGORITHMS The actual search for the extrema of a general contrast function J G as in (6) may then be performed in many dierent ways, e.g. by (stochastic) gradient descent, as was done with kurtosis in, e.g. <ref> [4, 8] </ref>. This adaptive, neural-like approach is developed elsewhere. However, as was explained in Section 2, a highly ecient, reliable and simple algorithm for tnding the relevant extrema can be obtained using the a contrast function in a more global and strict sense. Here G (x) = ln cosh (x). <p> This can be considered a generalization of the condition, valid when kurtosis is used as contrast, that kurtosis of the ICs must be non-zero <ref> [2, 8, 4] </ref>. We prove in [10] analytically only the local convergence of the algorithms, i.e. convergence for initial points near a solution.
Reference: [9] <author> A. Hyvrinen and E. Oja, </author> <title> A fast txed-point algorithm for independent component analysis, </title> <type> Tech. Rep. </type> <institution> A35, Helsinki University of Technology, Laboratory of Computer and Information Science, </institution> <year> 1996. </year> <note> Submitted to a journal. </note>
Reference-contexts: Derivation of a txed-point algorihm Now we derive a txed-point algorithm to tnd the relevant extrema of kurtosis. This is a moditcation for non-whitened data of the algorithm presented in <ref> [4, 9] </ref>. <p> This is proven in <ref> [9, 10] </ref>. Note that in (5) it is implicitly assumed that the co variance matrix C is not singular. If this is not the case, the dimension of the data must be reduced, e.g., with PCA, before running the algorithm. 2.3. <p> Dashed line: Two sub-Gaussian ICs. The maxima of the absolute value of J G were always obtained when the angle was 0 or =2, which were exactly the directions of the ICs. txed-point method. Thus we obtain an important generalization of the txed-point algorithm presented in <ref> [4, 9] </ref>. 4.1. <p> In fact then the estimations using (10) were practically perfect in spite of the added outliers. For details, see [10]. 6. CONCLUSIONS We introduced a generalized version of the txed-point algorithms presented in <ref> [4, 9] </ref> for ICA estimation. <p> Some applications of ICA using the generalized txed-point algorithm, or the original txed-point algorithm in <ref> [4, 9] </ref>, are described in [11].
Reference: [10] <author> A. Hyvrinen, </author> <title> A family of txed-point algorithms for independent component analysis, </title> <type> Tech. Rep. </type> <institution> A40, Helsinki University of Technology, Laboratory of Computer and Information Science, </institution> <year> 1996. </year> <note> Can be found at http://nucleus.hut.t/~aapo. </note>
Reference-contexts: This is proven in <ref> [9, 10] </ref>. Note that in (5) it is implicitly assumed that the co variance matrix C is not singular. If this is not the case, the dimension of the data must be reduced, e.g., with PCA, before running the algorithm. 2.3. <p> Thus, it seems plausible that J G in (6) could be a contrast function, in the sense discussed in subsection 2.1. The fact that J G is indeed a contrast function locally, i.e. near a solution, is proven in <ref> [10] </ref>. The only condition is that the IC s i in question must fultll Efs i g (s i ) g 0 (s i )g 6= 0, which is reminiscent of the classical condition of non-zero kurtosis [2]. <p> Thus we obtain an important generalization of the txed-point algorithm presented in [4, 9]. 4.1. Estimating one of the ICs To derive the generalized txed-point algorithm, trst note that when Ef (w T x) 2 g = wCw = 1, the gradient of J G (w T x) equals <ref> [10] </ref> r w J G (w T x) = E x fxg (w T x)g E - fg 0 (-)Cwg; (7) where g = G 0 is the derivative of G, and g 0 is the derivative of g. Now we can use the same logic as in Section 2. <p> Note that in the latter expectation has been replaced by w T x to enhance the convergence of the algorithm; this mod-itcation does not change the validity of the Kuhn-Tucker conditions. It is proven in <ref> [10] </ref> that using the algorithm in (8), w (k) converges, up to the sign, to one of the rows of the inverse of the mixing matrix A. <p> This can be considered a generalization of the condition, valid when kurtosis is used as contrast, that kurtosis of the ICs must be non-zero [2, 8, 4]. We prove in <ref> [10] </ref> analytically only the local convergence of the algorithms, i.e. convergence for initial points near a solution. <p> Our simulations, however, indicate that if g is a 'nice' function in some intuitive sense (smooth, does not have many local extrema), the algorithms do converge globally, i.e. starting from any (random) initial point w (0). The convergence of (8) is shown in <ref> [10] </ref> to be cubic, and experiments show that usually less than 10 iterations is enough. This means that these algorithms are very fast. They are also very reliable, because no parameters need to be tuned for good convergence. 4.2. <p> Note that the normalization in the txed-point step can be omitted when using (12). The convergence of the orthonormalization method in (12) is proven in <ref> [10] </ref>. 5. SIMULATION RESULTS To demonstrate the convergence of our algorithms, and especially their robustness, we applied our algorithm to blind separation of four artitcially generated source signals in the presence of some disturbing outliers. Two of the signals were super-Gaussian, and two were sub-Gaussian. <p> In fact then the estimations using (10) were practically perfect in spite of the added outliers. For details, see <ref> [10] </ref>. 6. CONCLUSIONS We introduced a generalized version of the txed-point algorithms presented in [4, 9] for ICA estimation.
Reference: [11] <author> J. Karhunen, A. Hyvrinen, R. Vigario, J. Hurri, and E. Oja, </author> <title> Applications of neural blind separation to signal and image processing, </title> <booktitle> in Proc. </booktitle> <address> ICASSP'97, (Mu-nich, Germany), </address> <year> 1997. </year>
Reference-contexts: Let w p+1 (k) = w p+1 (k) j=1 w p+1 (k) T Cw j w j p (11) In certain applications, however, it may be desired to use a symmetric decorrelation, in which no vectors are 'privileged' over others <ref> [11] </ref>. This can be accomplished, e.g., by the classical methods involving matrix square roots, or by the following simple iterative algorithm, where W (k) is the matrix (w 1 (k); :::; w N (k)) of the vectors: 1. <p> Some applications of ICA using the generalized txed-point algorithm, or the original txed-point algorithm in [4, 9], are described in <ref> [11] </ref>.
References-found: 11

