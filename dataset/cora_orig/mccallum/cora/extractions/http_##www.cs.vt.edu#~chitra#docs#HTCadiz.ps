URL: http://www.cs.vt.edu/~chitra/docs/HTCadiz.ps
Refering-URL: http://www.cs.vt.edu/~chitra/expert.html
Root-URL: http://www.cs.vt.edu
Title: The Development of a CHAID-based Model for Chitra93  
Author: by Horacio T. Cadiz 
Degree: submitted to the faculty of the  in partial fulfillment of the requirements for the degree of MASTER OF SCIENCE in Computer Science c flHoracio T. Cadiz and VPI SU 1994 APPROVED: Dr. Marc Abrams, Chairman Dr. Gerald McLaughlin Dr. Osman Balci  
Date: February, 1994  
Affiliation: Project  Virginia Polytechnic Institute and State University  Blacksburg, Virginia  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> M. Abrams and A. K. Agrawala. </author> <title> Performance study of distributed resource sharing algorithms. </title> <journal> IEEE Dist. Processing Technical Committee Newsletter, </journal> <volume> 7(3) </volume> <pages> 18-26, </pages> <month> November </month> <year> 1985. </year>
Reference-contexts: A program state is mapped to a stochastic process model state through aggregation. Through this process, history is embedded in the state. Example 1 Consider a program solving Djikstra's "Dining Philosophers" problem <ref> [1] </ref>. N philosophers sitting around a table spend their lives alternating between eating and thinking. On the table is a bowl of food and N forks arranged in a circle such that each philosopher has one fork to the left and another on the right.
Reference: [2] <author> M. Abrams, N. Doraswamy, and A. Mathur. Chitra: </author> <title> Visual analysis of parallel and distributed programs in the time, event, and frequency domain. </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> 3(6) </volume> <pages> 672-685, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: Chitra has integrated its visualization and modeling modules to make it convenient for an analyst to iterate through the steps of data visualization, data manipulation, and model development. Chitra is built on the premise that fast, simple, convenient, and accurate model construction is crucial for performance analysis <ref> [2] </ref>. Chitra, therefore, offers a methodology for building an empirical model of the behavior of a system under analysis. 1 Chitra is a Sanskrit word for a beautiful or pleasing picture. 2 1.1 Overview of Chitra Chitra91 [2], Chitra92 [14], and Chitra93 are the three generations of the performance analysis tool <p> that fast, simple, convenient, and accurate model construction is crucial for performance analysis <ref> [2] </ref>. Chitra, therefore, offers a methodology for building an empirical model of the behavior of a system under analysis. 1 Chitra is a Sanskrit word for a beautiful or pleasing picture. 2 1.1 Overview of Chitra Chitra91 [2], Chitra92 [14], and Chitra93 are the three generations of the performance analysis tool Chitra. They all describe the behavior of a system by enumerating the sequence of states entered by the system during execution. Each system state is an n-tuple representing n attributes of the system. <p> Thus, in theory, software (i.e., the set of program states in the software) is Markov-dependent and its behavior can be modeled as a homogeneous, continuous time semi-Markov chain <ref> [2] </ref>. We note that the one-to-one correspondence between the set of program states and the set of the model states implies that the transition functions of the model and the finite state machine are equivalent as well.
Reference: [3] <author> M. Abrams, T. Lee, H. Cadiz, and K. Ganugapati. Chitra93: </author> <title> Beyond software visualization. </title> <note> This paper describing Chitra93 is yet to be published., </note> <year> 1994. </year>
Reference-contexts: This results not only in a reduction of the SSS, but a reduction without loss of information. Transition graphs like the one for this dining philosopher program have been observed in 96 CTSM model and the CHAID-based model. other software traces <ref> [3] </ref>. Model Generation for Trace-Driven Simulation Computer architects extensively rely on actual input data which had been generated from the operation of the real system as input to their simulation model. <p> programs and parallel database applications with promising results, the formal validation of the CHAID-based model is still a primary concern. 98 The CHAID-based Model as a Similarity Measure The ability to group together SSS's collected from different runs of the same program is a powerful heuristic for predicting program behavior <ref> [3] </ref>. We had discussed in Chapter 4 how SSS's already grouped together into ensembles could be analyzed. A related case is when the ensemble members are SSS's with similar but not equal parameter sets 1 .
Reference: [4] <author> A. A. Afifi and S. P. Azen. </author> <title> Statistical Analysis: A Computer Oriented Approach. </title> <publisher> Academic Press, Inc., </publisher> <year> 1972. </year>
Reference-contexts: Then, the hypothesis of homogeneity <ref> [4] </ref> states that p 1r = p 2r = = p mr for all r = 1; : : :; n. Two positions I and D are considered to be statistically independent if and only if the hypothesis of homogeneity holds for the two positions. <p> We have 2 0 = 18:04 and v = 5 for Table 2.1 for Example 10. Table 2.2 for Example 11 has 2 0 = 4:80 and v = 2. The reader is directed to <ref> [4] </ref>, [5], and [12] for more comprehensive discussions on the 2 test. The 2 test for H 0 is considered significant if and only if the 2 0 statistic does not exceed a predetermined critical value. Otherwise, the test is considered not significant. <p> The problem is that decreasing the probability of one type of error increases the probability of the other. The usual solution to this problem is to set ff to a certain small value with the hope that fi is also acceptably small <ref> [4] </ref>. For exploratory 29 purposes, a high ff level would be acceptable because it presents relationships which might be worth examining but not really significant. However, when rigor and correctness are the primary aims of the test, a high ff would be unacceptable. <p> Without the p-values, the second method only allows us to say whether the null hypothesis is valid or not. Consider Example 10. With ff = p crit = 0:05 and v = 5, the critical test statistic 2 crit is 11.1, based on the 2 table in <ref> [4] </ref>. Interpolating from the same table, 2 0 = 18.04 has p 0 = 0.003. Because p 0 &lt; p crit , by using the first method of testing, the null hypothesis is rejected.
Reference: [5] <author> A. </author> <title> Agresti. Categorical Data Analysis. </title> <publisher> John Wiley and Sons, </publisher> <year> 1990. </year>
Reference-contexts: Given a contingency table with m rows and n columns, the calculated 27 2 0 statistic for the table is 2 n X m X (O ij F ij ) 2 : (2.4) The 2 test is a simple test for uncovering evidence of associations <ref> [5] </ref> between variables. It is a measure of how the observed frequencies differ from the expected values of the frequencies. It is dependent on the magnitude of the difference between the observed and expected frequencies and on the number of items which contributed to the statistic. <p> We have 2 0 = 18:04 and v = 5 for Table 2.1 for Example 10. Table 2.2 for Example 11 has 2 0 = 4:80 and v = 2. The reader is directed to [4], <ref> [5] </ref>, and [12] for more comprehensive discussions on the 2 test. The 2 test for H 0 is considered significant if and only if the 2 0 statistic does not exceed a predetermined critical value. Otherwise, the test is considered not significant. <p> It does not, for example, give an indication of the direction or the nature of an association between the variables. Rather, it is a simple test for uncovering evidence of associations <ref> [5] </ref>. Its simplicity in performing this role makes the chi-square test ideal for partitioning the data. Together with the chi-square test, Kass introduced a new type of predictor which he called the floating predictor.
Reference: [6] <author> M. S. Aldenderfer and R. K. Blashfield. </author> <title> Cluster Analysis. </title> <publisher> SAGE Publications, </publisher> <year> 1984. </year>
Reference: [7] <author> O. </author> <title> Balci. Guidelines for successful simulation studies. </title> <booktitle> Course Notes, </booktitle> <month> March </month> <year> 1987. </year>
Reference-contexts: The Validation of the CHAID-based Model Extensive validation of the model is necessary to determine the applicability and robustness of the CHAID-based model for performance analysis. Many techniques for model validation are described in <ref> [7] </ref>.
Reference: [8] <author> U. N. Bhat. </author> <title> Elements of Applied Stochastic Processes. </title> <publisher> John Wiley, </publisher> <address> New York, 2nd edition, </address> <year> 1984. </year>
Reference: [9] <author> D. Biggs, B. de Ville, and E. Suen. </author> <title> A method of choosing multiway partitions for classification and decision trees. </title> <journal> J. of App. Stat., </journal> <volume> 18(1) </volume> <pages> 49-62, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: Repeated testing of the same sample, as had been done in the previous subsections, violates the one population, one sample, and one test assumption <ref> [9] </ref>. We increase the probability of finding a relationship (i.e. rejection of the null hypothesis) simply by increasing the number of times we search for such a relationship by chance and not because such a relationship actually exists. <p> As a result, the ff level initially set is no longer reflective of the desired error rate. To counter this increased chance of being in error, ff must be divided by an adjustment factor called the Bonferroni adjustment factor, to maintain the desired error level <ref> [9] </ref>. <p> For a free merge, we have the combination N (m r + 1; m r) = (m r 1)!2! We have N (m r + 1; m r) = m r (2.7) for a monotonic merge. This gives the Bonferroni adjustment factor derived in <ref> [9] </ref> B (m) = 1 + r=1 where r is the possible number of times a merger may be performed (a minimum of two groups must be left after every merger), N (m r + 1; m r) is the number of ways of grouping two categories of the m r <p> Again, we have to adjust the significance level for testing position 1 by using the Bonferroni adjustment to account for this violation. To achieve the desired significance level of ff for testing the optimal position, we have (adapted from <ref> [9] </ref>) an adjusted critical value ff 0 = (N v )B (j) where N v is the number of predictor positions tested and B (m) is the Bonferroni adjustment factor for the chosen predictor position. <p> In addition, as the number of categories of a floating category type became large, the significance levels became too low by up to a factor of 10. This had the effect of discriminating against the newly introduced floating category predictor variables <ref> [9] </ref>.
Reference: [10] <author> Y. M. Bishop, S. E. Fienberg, and P. W. Holland. </author> <title> Discrete Multivariate Analysis. </title> <publisher> MIT Press, </publisher> <year> 1975. </year>
Reference-contexts: It had been criticized as not accounting for the "variability inherent in data" <ref> [10] </ref> and for the bias in its splitting process. Because the possibility of finding the maximum sum of squares as being that of one predictor variable is directly proportional to its number of categories, AID was biased towards predictors with more categories [18].
Reference: [11] <author> L. Brieman, J. Friedman, R. Olshen, and C. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth, </publisher> <year> 1984. </year>
Reference-contexts: It is discarded and the analysis set reverts to the original unmerged set. In our succeeding examples and definitions, we revert to using in Example 2 as our analysis set rather than using 00 in Example 21. A classification rule, based on the definition in <ref> [11] </ref>, is a partition of a population X into n disjoint subsets X 1 ; : : : ; X n , X = S n X n such that for every x 2 X j , the predicted class 55 is j. <p> In medicine, for example, a dependency between a patient's blood pressure and the patient's risk of morbidity (e.g., high-risk, low-risk, no-risk) have been discovered. In meteorology, the levels of some airborne pollutants have been associated with the magnitude of health risks to the the general populace <ref> [11] </ref> and are therefore used to determine the warning levels (e.g, none, first-stage) raised by public health officials. <p> After the two began their collaboration, they were joined by Stone whom they credit for the methodological development of tree classification. They were then joined by Olshen who applied the methods in the field of medicine and who contributed to CART's theoretical development <ref> [11] </ref>. In CART, a binary tree classifier is constructed by a recursive split of a node into two 91 descendant sub-nodes. The root is the entire data set. The sub-nodes are disjoint subsets of the parent node. <p> The root is the entire data set. The sub-nodes are disjoint subsets of the parent node. The terminal nodes (i.e. subsets which are not split), form a partition of the data set. In Figure 5.1, which is adapted from page 21 of <ref> [11] </ref>, we have the data set X split into two disjoint subsets X 1 and X 2 . Both are further split into two disjoint subsets. The former into X 3 and X 4 and the latter into X 5 and X 6 , and so on. <p> The entire construction of a tree, as quoted from <ref> [11] </ref>, is dependent on the following: 92 1. The selection of the splits 2. The decision when to declare a node terminal or to continue splitting it 3.
Reference: [12] <author> E. Caulcott. </author> <title> Significance Tests. </title> <publisher> Routledge and Kegan Paul, </publisher> <year> 1973. </year>
Reference-contexts: We have 2 0 = 18:04 and v = 5 for Table 2.1 for Example 10. Table 2.2 for Example 11 has 2 0 = 4:80 and v = 2. The reader is directed to [4], [5], and <ref> [12] </ref> for more comprehensive discussions on the 2 test. The 2 test for H 0 is considered significant if and only if the 2 0 statistic does not exceed a predetermined critical value. Otherwise, the test is considered not significant.
Reference: [13] <institution> FirstMark Technologies Ltd. </institution> <note> Knowledge Seeker User's Guide, Version 2.1 edition. </note>
Reference-contexts: The result was an algorithm which removed the bias against a particular category type and allowed for the ranking of each variable in order of importance at each split. This algorithm is incorporated in a software called Knowledge Seeker <ref> [13] </ref>. It is available for MS-DOS based PC's and was used by the author to do the preliminary work on this project. 90 5.4 Classification and Regression Trees Recall the two steps in AID.
Reference: [14] <author> K. Ganugapati. </author> <title> The design and implementation of Chitra92, a system to empirically model concurrent software performance. </title> <type> Master's thesis, </type> <institution> Computer Sci. Dept., Virginia Tech, Blacksburg, </institution> <address> VA 24060, </address> <month> April </month> <year> 1993. </year> <month> 102 </month>
Reference-contexts: Introduction Parallel and distributed programs exhibit behavior which is complex and difficult to analyze. The complexity arises from the fact that, in general, different runs of the same parallel or distributed program may execute statements in different orders and consume different amounts of resources <ref> [14] </ref>. This nondeterministic behavior is caused by the competition for system resources between multiple and concurrent threads of execution in such programs. It is a major impediment to making generalizations and, ultimately, gaining knowledge about the characteristics of parallel and distributed programs. <p> Chitra, therefore, offers a methodology for building an empirical model of the behavior of a system under analysis. 1 Chitra is a Sanskrit word for a beautiful or pleasing picture. 2 1.1 Overview of Chitra Chitra91 [2], Chitra92 <ref> [14] </ref>, and Chitra93 are the three generations of the performance analysis tool Chitra. They all describe the behavior of a system by enumerating the sequence of states entered by the system during execution. Each system state is an n-tuple representing n attributes of the system. <p> Occupancy time statistics, a set of histograms, and a transition matrix are calculated on the basis of the states in the model. These modules are integrated together to allow the user to iterate through the process of model building as summarized in Figure 1.1, which is adapted from <ref> [14] </ref>. This integration is the source of one of the main strengths of Chitra the convenient exploration of different alternatives for building a model. 4 5 1.2 The Empirical Model of Program Behavior in Chitra Chitra91 and Chitra92 model program behavior as a homogeneous, continuous time semi-Markov (CTSM) process. <p> Briefly, it contains the frequency of occurrence, the minimum duration, maximum duration, the sample mean of the duration, the sample variance for the duration, and the standard deviation of the duration of each state in the SSS. These statistics and their calculations are fully explained in <ref> [14] </ref>. 3.1.3 The Embedded Chain of Model States The generation of the embedded model chain for the CHAID-based model is relatively straight forward. The states in the chain are the states i and j for every element m ij 6= 0 of the N -step Transition Matrix. <p> In any case, the result is a costly and time-consuming procedure especially when analyzing parallel programs. Program's are executed with a set of data items referred to as the program's parameter set <ref> [14] </ref>. For the Dining Philosophers problem, examples are the number of philosophers or 76 the eat/think policy (e.g., fixed or random eating/thinking time). Multiple SSS's are gener-ated through different runs of a program which, in general, may use different parameter sets for each run.
Reference: [15] <author> P. G. Harrison and N. M. Patel. </author> <title> Performance Modelling of Communication Networks and Computer Architectures. </title> <publisher> Addison-Wesley, </publisher> <year> 1992. </year>
Reference-contexts: The transition from state A to ES in Figure 1.2, for example, considers only state A. There is no possibility of considering the other states that have preceded A to determine that ES or EL is its successor. The following definition, based on <ref> [15] </ref>, formally defines the Markov-dependence property of the CTSM model.
Reference: [16] <author> D. E. Hinkle, J. T. Austin, and G. W. McLaughlin. </author> <title> Using log-linear models in higher education research. </title> <editor> In B. Yancey, editor, </editor> <booktitle> New Directions for Institutional Research, </booktitle> <volume> volume 58, </volume> <pages> pages 23-41. </pages> <publisher> Jossey-Bass, </publisher> <year> 1988. </year>
Reference-contexts: Statistical techniques referred to as log-linear contingency table analysis, logit analysis, or more frequently as log-linear models. could be used to explicitly compare each predictor position. These methods for analyzing the different independent variables such as our predictor positions is presented in <ref> [16] </ref>. Furthermore, a hierarchy of log-linear models which could be used and tested sequentially for determining which independent variable predicts the dependent variable could be incorporated in Chitra. 101
Reference: [17] <author> G. V. Kass. </author> <title> Significance testing in automatic interaction detection (A.I.D.). </title> <journal> Applied Statistics, </journal> <volume> 24(2) </volume> <pages> 178-189, </pages> <year> 1975. </year>
Reference-contexts: We now describe the techniques from which our CHAID-based model is founded. 5.1 Automatic Interaction Detection Automatic Interaction Detection (AID) is a technique which had been developed as a descriptive method for large data sets <ref> [17] </ref>. Kass traces AID's roots to a paper by Belson in 1953. He attributes the current AID method to that which had been described by Morgan and Sonquist in 1963. <p> AID operates by attempting to successively split the data set into 2 subgroups. The sole criterion for splitting is to find the two subgroups which have the maximum "between subgroup sum of squares" <ref> [17] </ref>. In statistical experimental design, this sum of squares is an estimate of the combined effects of treatment levels (i.e. the categories of the predictor variable) and chance on the dependent variable [19].
Reference: [18] <author> G. V. Kass. </author> <title> An exploratory technique for investigating large quantities of categorical data. </title> <journal> Applied Statistics, </journal> <volume> 29(2) </volume> <pages> 119-127, </pages> <year> 1980. </year>
Reference-contexts: In the latter case, the analyst might run experiments and collect more data, perhaps explore new ways of determining how the different categories are related. However, the computational complexity of exhaustively testing all possible permutations of l categories of an independent position is presented by Kass in <ref> [18] </ref> to be O (2 l ). This is extremely prohibitive and would very likely curtail any data exploration. We present a process called a merge which is crucial to building our CHAID-based model. It is a method to augment the statistical tests discussed in the previous sections. <p> The categories for the other positions remain as in . The use of automatic interaction detection (AID) and the 2 (Chi) test gives us the CHAID part of the name of our model. As originally described by Kass <ref> [18] </ref>, CHAID is a technique which "partitions the data into mutually exclusive, exhaustive, subsets that best describe the dependent variable" [18]. <p> The use of automatic interaction detection (AID) and the 2 (Chi) test gives us the CHAID part of the name of our model. As originally described by Kass <ref> [18] </ref>, CHAID is a technique which "partitions the data into mutually exclusive, exhaustive, subsets that best describe the dependent variable" [18]. <p> Thus, after the first merge, the 2 statistics of the succeeding contingency tables must be considered as approximate tests. Our 2 tests are decision rules rather than exact tests of the null hypothesis at the ff level. Kass <ref> [18] </ref> reports successful uses of these approximations in simulation and in practice. 2. The use of (m 1) multiple 2 testing violates the ff level for the experiment. <p> Because the possibility of finding the maximum sum of squares as being that of one predictor variable is directly proportional to its number of categories, AID was biased towards predictors with more categories <ref> [18] </ref>. To remedy both limitations, Kass added a significance testing step during the partitioning process. This allowed for the creation of multi-way splits as opposed to binary splits. It also had the effect of minimizing but not, as we shall see in the next subsection, completely nullifying the bias.
Reference: [19] <author> R. E. Kirk. </author> <title> Experimental Design: Procedures for the Behavioral Sciences. </title> <booktitle> Brooks/Cole, 2nd edition, </booktitle> <year> 1982. </year>
Reference-contexts: In statistical experimental design, this sum of squares is an estimate of the combined effects of treatment levels (i.e. the categories of the predictor variable) and chance on the dependent variable <ref> [19] </ref>. By performing a split only when the effect of chance is below a certain threshold level, we can be assured that the predictor variable defines how a split is performed. The algorithm continues until each of the subgroups 86 formed in the previous stage can no longer be split.
Reference: [20] <author> T. Lehr et al. </author> <title> Visualizing performance debugging. </title> <booktitle> Computer, </booktitle> <pages> pages 38-51, </pages> <month> October </month> <year> 1989. </year>
Reference: [21] <author> M. Maekawa, A. E. Oldehoeft, and R. R. Oldehoeft. </author> <title> Operating Systems Advanced Concepts. </title> <publisher> The Benjamin/Cummings Publishing Company, </publisher> <year> 1987. </year>
Reference: [22] <author> L. Mohr. </author> <title> Understanding Significance Testing. SAGE University Paper series on Quantitative Applications in the Social Sciences no. </title> <address> 07-073. </address> <publisher> SAGE Publications, </publisher> <year> 1990. </year>
Reference-contexts: The units of study, the individuals, are the subsequences in . The variables, properties of individuals that may take two or more values but not at the same time for the same individual <ref> [22] </ref>, are the positions. The dependent variable or dependent position is position 0. A predictor variable or predictor position is any position other than position 0. <p> So far, we have seen how the significance test allows us to reject or accept the null hypothesis. Significance testing can also be used as a strength-of-relationship function <ref> [22] </ref>. Recall that a significant 2 0 means that the dependence between the dependent and predictor 31 variables is strong enough to be believed.
Reference: [23] <author> A. Papoulis. </author> <title> Probability, Random Variables, and Stochastic Processes. </title> <publisher> McGraw-Hill, Inc., 3rd edition, </publisher> <year> 1991. </year>
Reference: [24] <author> M. Simmons and R. Koskela, </author> <title> editors. Performance Instrumentation and Visualization. </title> <publisher> ACM Press, </publisher> <address> New York, </address> <year> 1990. </year> <booktitle> Based on the Workshop on Parallel Computer Systems: Instrumentation and Visualization, </booktitle> <month> May </month> <year> 1989. </year>
Reference-contexts: However, especially for parallel and distributed programs, the details which explain the observed behavior of the system are frequently obscured by the sheer volume and complexity of the collected data. Visualization, a technique which transforms symbolic data into geometric shapes <ref> [24] </ref>, has been incorporated into many modern performance analysis tools with the hope that better insights into the data could be obtained. 1 Chitra 1 is a tool that uses traditional visualization techniques for performance anal-ysis.
Reference: [25] <author> H. S. Stone. </author> <title> High-Performance Computer Architecture. </title> <publisher> Addison-Wesley, </publisher> <address> 2nd edition, </address> <year> 1990. </year>
Reference-contexts: Using the unmodified trace data may be extremely time consuming to process and space consuming to store. In simulating cache references for example, various techniques such as address stripping have been developed for reducing the volume of the trace <ref> [25] </ref>. 97 As usual, the objective is to reduce the length of the trace while maintaining its ability to represent the real input. Developing an empirical model of large trace data is a novel technique for capturing the characteristics of the trace data in a more concise form.
Reference: [26] <author> D. D. Wolff and M. L. Parsons. </author> <title> Pattern Recognition Approach to Data Interpretation. </title> <publisher> Plenu Press, </publisher> <year> 1983. </year> <month> 103 </month>
References-found: 26

