URL: file://ftp.csd.uu.se/pub/papers/theses/0016.ps.gz
Refering-URL: http://www.csd.uu.se/~jb/
Root-URL: 
Title: A Recursion Parallel Prolog Engine  
Author: Johan Bevemyr UPMAIL 
Degree: Thesis for the Degree of Licentiate of Philosophy  
Address: Box 311 S-751 05 UPPSALA SwedenISSN 0283-359X  
Affiliation: Computing Science Department Uppsala University  
Note: UPPSALA THESES IN COMPUTING SCIENCE No. 16/93  
Abstract-found: 0
Intro-found: 1
Reference: 1. <author> At-Kaci, H., </author> <title> Warren's Abstract Machine: A Tutorial Reconstruction, </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1991. </year> <month> f4g </month>
Reference-contexts: There are, by now, numerous variants of WAM; of these we have chosen the variant described by Carlsson [7, 8]. We give a brief overview of this machine in this section; for a more detailed description we refer to At-Kaci's book <ref> [1] </ref>. WAM is a register-oriented machine. Clauses in the database are compiled into sequences of machine instructions. These instructions specialize the head unification for each compiled clause (instead of applying the general unification algorithm). There are also instructions to encode the search rule of Prolog.
Reference: 2. <author> Ali, K. A. M. and Karlsson, R., </author> <title> The Muse Approach to OR-parallel Prolog, </title> <journal> in International Journal of Parallel Programming, </journal> <volume> 19 </volume> <pages> 129-162, </pages> <year> 1990. </year> <note> f53, 57g </note>
Reference-contexts: However, we have compared their result on the nrev benchmark with ours. In addition to comparing Reform Prolog with AND-parallel systems we have also made a limited comparison with JAM [13] (an implementation of Parlog which exploit stream AND-parallelism) and with Muse <ref> [2, 20] </ref> (an OR-parallel implementation of Prolog). When comparing with Muse we executed the map benchmark. This was done using the findall technique for AND-in-OR parallelism described by Carlsson, Danhof and Overbeek [11]. <p> This in turn is an effect of using Reform Compilation for data-parallelism, as a method for parallelizing Prolog. Chapter 7 Discussion 7.1 RELATED WORK There are two sources of parallelism that can be exploited when parallelizing Pro-log: OR-parallelism and AND-parallelism. Aurora [26] and Muse <ref> [2] </ref> are the two outstanding implementations that exploit OR-parallelism. AND-parallel implementations can be divided into several categories: independent AND-parallel and dependent AND-parallel.
Reference: 3. <author> Amdahl, G. M., </author> <title> Validity of the Single Processor Approach to Achieving Large Scale Computing Capabilities, </title> <booktitle> in AFIPS Conference Proceedings, American Federation of Information Processing Societies, </booktitle> <year> 1967. </year> <month> f51g </month>
Reference-contexts: We therefore measured the total suspension time for each worker. The results showed that no worker was suspended more than 0.6% of the total execution time when using 24 processors. Sequential Fraction of Execution Time The sequential part of a program determines its maximal speed-up, according to Amdahl's law <ref> [3] </ref>. We therefore measured the sequential parts of each of our benchmark programs. The sequential part includes everything except the parallel execution|traversing the recursive arguments as well as setting up the global environment for the parallel phase and executing the sequential Prolog code.
Reference: 4. <author> Arro, H., Barklund, J. and Bevemyr, J., </author> <title> Parallel Bounded Quantifications - Preliminary Results, </title> <type> UPMAIL Technical Report No. 74, </type> <institution> Computing Science Department, Uppsala University, </institution> <year> 1993. </year> <month> f57g </month>
Reference-contexts: The implementation of WAM, on which Reform Prolog is based, has also been extended for parallel execution of bounded quantifications on a SIMD (single instruction stream, multiple data stream) computer, Thinking Machines Corp. model CM-2. Arro, Barklund and Bevemyr <ref> [4] </ref> report promising preliminary results from this implementation. 58 Discussion 7.2 FUTURE WORK In the continuation of this work we propose to further investigate scheduling properties, especially for fine grained programs and for programs with nonuniform load balance.
Reference: 5. <author> Barklund, J., </author> <title> Parallel Unification, </title> <type> Ph.D. Thesis, </type> <institution> Computing Science Department, Uppsala University, </institution> <year> 1990. </year> <month> f52g </month>
Reference-contexts: For coarse-grained programs the sequential part of the computation remains insignificant, but for fine-grained programs it starts to dominate. A possible solution is to parallelize this part, e.g., by using methods for parallel unification similar to those presented by Barklund <ref> [5] </ref>. Another potential bottleneck is the use of operating system semaphores. The semaphores supplied by the operation system are slow, activating a suspended worker using semaphores might take as long as 5 milliseconds on a Sun 630 MP.
Reference: 6. <author> Barklund, J. and Millroth, M., </author> <title> Providing Iteration and Concurrency in Logic Programs through Bounded Quantifications, </title> <booktitle> in Proceedings of the International Conference on Fifth Generation Computer Systems 1992 , Ohmsha, </booktitle> <address> Tokyo, </address> <year> 1992. </year> <month> f57g </month>
Reference-contexts: Reform Prolog exploits restricted dependent AND-parallelism using a method which is most closely related to Naish's PNU-Prolog. Bounded quantifications is another way to exploit data-parallelism in Prolog. Barklund and Millroth <ref> [6] </ref> and Voronkov [50], among others, have discussed how bounded quantifications can be used in Prolog. Preliminary results indicate that bounded quantifications can be executed in parallel on shared-memory multiprocessors using the abstract machine proposed in this thesis.
Reference: 7. <author> Bevemyr, J., </author> <title> The Luther WAM Emulator, </title> <type> UPMAIL Technical Report No. 72, </type> <institution> Computing Science Department, Uppsala University, </institution> <year> 1992. </year> <month> f4g </month>
Reference-contexts: It was originally published by Warren in 1983 and has since become the most popular method for implementing Prolog. There are, by now, numerous variants of WAM; of these we have chosen the variant described by Carlsson <ref> [7, 8] </ref>. We give a brief overview of this machine in this section; for a more detailed description we refer to At-Kaci's book [1]. WAM is a register-oriented machine. Clauses in the database are compiled into sequences of machine instructions.
Reference: 8. <author> Carlsson, M., </author> <title> SICStus Prolog Internals Manual, </title> <type> Internal Report, </type> <institution> Swedish Institute of Computer Science, </institution> <year> 1989. </year> <month> f4g </month>
Reference-contexts: It was originally published by Warren in 1983 and has since become the most popular method for implementing Prolog. There are, by now, numerous variants of WAM; of these we have chosen the variant described by Carlsson <ref> [7, 8] </ref>. We give a brief overview of this machine in this section; for a more detailed description we refer to At-Kaci's book [1]. WAM is a register-oriented machine. Clauses in the database are compiled into sequences of machine instructions.
Reference: 9. <author> Carlsson, M., </author> <title> Design and Implementation of an OR-Parallel Prolog Engine, </title> <type> Ph.D. Thesis, </type> <institution> Swedish Institute of Computer Science, </institution> <year> 1990. </year> <month> f23g </month>
Reference-contexts: The reason for this appears to be that Sun 4 machines use directly mapped caches. Chapter 4 Process Management P rocess management and scheduling are critical points in many of the parallel Prolog systems existing today. OR-parallel systems, such as Aurora <ref> [9] </ref> and Muse [20], depend heavily on scheduling algorithms to distribute work among workers. In AND and OR parallel systems it is essential not to exploit all possible parallelism since the scheduling and process spawning overhead might easily grow out of hand.
Reference: 10. <author> Carlsson, M., </author> <title> Freeze, Indexing, and Other Implementation Issues in the WAM, </title> <booktitle> in Logic Programming: Proceedings of the Fourth International Conference, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1987. </year> <note> f24g 61 62 </note>
Reference-contexts: This can be implemented using a technique similar to that used when implementing diff and freeze (as described by Carlsson <ref> [10] </ref>) in combination with a mechanism to signal the appropriate semaphore when a recursion level becomes leftmost. The drawback of this method is that it imposes a comparatively large overhead on active recursion levels.
Reference: 11. <author> Carlsson, M., Danhof, K. and Overbeek, R., </author> <title> A Simplified Approach to the Implementation of AND-Parallelism in an OR-Parallel Environment, </title> <booktitle> in Logic Programming: Proceedings of the Fifth International Conference and Symposium, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1988. </year> <month> f53g </month>
Reference-contexts: When comparing with Muse we executed the map benchmark. This was done using the findall technique for AND-in-OR parallelism described by Carlsson, Danhof and Overbeek <ref> [11] </ref>. Parallelization Overhead The Andorra-I system shows parallelization overheads of an average of 40% on a set of benchmarks [52] compared with the sequential version of Andorra-I. NUA-Prolog 54 Evaluation shows a parallelization overhead of 55% on the nrev benchmark.
Reference: 12. <author> Clocksin, W. F., </author> <title> Design and Simulation of a Sequential Prolog Machine, </title> <journal> in New Generation Computing, </journal> <volume> 3 </volume> <pages> 101-120, </pages> <year> 1985. </year> <month> f20g </month>
Reference-contexts: The consequence is that more space is consumed on the trail, but only for conditionally bound variables; deterministic programs will use no extra space. 20 Data Sharing Using an unbound tag is no novelty, e.g., Clocksin <ref> [12] </ref> has suggested its use. Shen [41] uses an analogous method for timestamping variables in his implementation of dependent AND-parallel Prolog. Mannila and Ukkonen [27] also describe a similar notion of timestamping terms. They use the timestamp information for implementing intelligent backtracking and for fast high-level tracing of Prolog execution.
Reference: 13. <author> Crammond, J., </author> <title> The Abstract Machine and Implementation of Parallel Parlog, </title> <journal> in New Generation Computing, </journal> <volume> 10 </volume> <pages> 385-422, </pages> <year> 1992. </year> <month> f53g </month>
Reference-contexts: However, we have compared their result on the nrev benchmark with ours. In addition to comparing Reform Prolog with AND-parallel systems we have also made a limited comparison with JAM <ref> [13] </ref> (an implementation of Parlog which exploit stream AND-parallelism) and with Muse [2, 20] (an OR-parallel implementation of Prolog). When comparing with Muse we executed the map benchmark. This was done using the findall technique for AND-in-OR parallelism described by Carlsson, Danhof and Overbeek [11].
Reference: 14. <author> Crammond, J., </author> <title> Scheduling and Variable Assignment in the Parallel Parlog Implementation, </title> <booktitle> in Proceedings of the North American Conference on Logic Programming, </booktitle> <address> Austin, </address> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., 1990. f17, 27g </address>
Reference-contexts: The other worker's binding value must then be unified with this worker's, to ensure consistency. A similar method is used in the implementation of Parallel NU-Prolog, as described by Naish [36], and in the implementation of Parallel Parlog, as described by Crammond <ref> [14] </ref>. The operation is illustrated below using C code. BOOL bind (var,term) TAGGED var, term; - TAGGED new_term; if ((new_term = exchange (var,term)) != var) return unify (new_term, term); else return TRUE; - The first method has the disadvantage that it requires an extra tag. <p> This quality makes these implementations unsuitable for the scheduling algorithms described above. Hermenegildo [18] describes a scheduling algorithm for &-Prolog in which each processor keeps a local goal-queue. When a goal-queue becomes empty, goals migrate from other queues. Crammond <ref> [14] </ref> uses a similar scheduling algorithm in his implementation of Par-log, the main difference being that local goal-queues are devided into private and global parts. The advantage of this is that the local parts of the goal-queue can be updated without locking.
Reference: 15. <author> DeGroot, D., </author> <title> Restricted AND-parallelism, </title> <booktitle> in Proceedings of the International Conference on Fifth Generation Computer Systems, Institute for New Generation Computing, </booktitle> <year> 1984. </year> <month> f57g </month>
Reference-contexts: Aurora [26] and Muse [2] are the two outstanding implementations that exploit OR-parallelism. AND-parallel implementations can be divided into several categories: independent AND-parallel and dependent AND-parallel. Several methods have been proposed to exploit independent AND-parallelism, the model proposed by DeGroot <ref> [15] </ref> being the first, Hermenegildo [17] and Lin and Kumar [23] have also developed schemes for independent AND-parallel execution. Restricted dependent AND-parallelism have been exploited by several logic programming implementations, e.g., Naish's [36] parallel NU-Prolog.
Reference: 16. <author> Hatcher, P. J. and Quinn, M. J., </author> <title> Data-parallel Programming on MIMD Computers, </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1991. </year> <month> f30g </month>
Reference-contexts: The sequential worker sets up the arguments for each recursion level in its own argument registers, which are globally accessible. Through the extensions in this chapter, a form of data-parallelism is introduced into WAM. The data-parallelism used is different from that described by, e.g., Hatcher and Quinn <ref> [16] </ref>, in that the same program is executed by all workers, but not necessarily exactly the same instructions|non-determinism and different deterministic choices of clauses in predicates might result in that different parts of the same program are executed by different recursion levels.
Reference: 17. <author> Hermenegildo, M. V., </author> <title> An Abstract Machine for Restricted AND-parallel Execution of Logic Programs, </title> <booktitle> in Third International Conference on Logic Programming, </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1986. </year> <month> f57g </month>
Reference-contexts: Aurora [26] and Muse [2] are the two outstanding implementations that exploit OR-parallelism. AND-parallel implementations can be divided into several categories: independent AND-parallel and dependent AND-parallel. Several methods have been proposed to exploit independent AND-parallelism, the model proposed by DeGroot [15] being the first, Hermenegildo <ref> [17] </ref> and Lin and Kumar [23] have also developed schemes for independent AND-parallel execution. Restricted dependent AND-parallelism have been exploited by several logic programming implementations, e.g., Naish's [36] parallel NU-Prolog. The dependent AND-parallelism is restricted in such a way that only binding deterministic goals are executed in parallel.
Reference: 18. <author> Hermenegildo, M. V., </author> <title> Relating Goal Scheduling, Precedence, and Memory Management in AND-parallel Execution of Logic Programs, </title> <booktitle> in Logic Programming: Proceedings of the Fourth International Conference, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1987. </year> <month> f27g </month>
Reference-contexts: In AND-parallel logic programming implementations the set of parallel processes grows during parallel execution. This quality makes these implementations unsuitable for the scheduling algorithms described above. Hermenegildo <ref> [18] </ref> describes a scheduling algorithm for &-Prolog in which each processor keeps a local goal-queue. When a goal-queue becomes empty, goals migrate from other queues.
Reference: 19. <author> Karlsloot, M. and Tick, E., </author> <title> Sequentializing Parallel Programs, </title> <booktitle> in Proceedings of the Phoenix Seminar and Workshop on Declarative Programming, </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1991. </year> <month> f23g </month>
Reference-contexts: In AND and OR parallel systems it is essential not to exploit all possible parallelism since the scheduling and process spawning overhead might easily grow out of hand. Granularity control is a challenging problem, and a variety of solutions have been proposed <ref> [19, 21] </ref>. Whether to do speculative work or not, is also a delicate problem in these systems. An example of speculative work in an OR-parallel system is to execute a branch of the search tree that might be cut away by another branch to the left of it.
Reference: 20. <author> Karlsson, R., </author> <title> A High Performance OR-parallel Prolog System, </title> <type> Ph.D. Thesis, </type> <institution> Swedish Institute of Computer Science, </institution> <year> 1992. </year> <note> f23, 53g </note>
Reference-contexts: The reason for this appears to be that Sun 4 machines use directly mapped caches. Chapter 4 Process Management P rocess management and scheduling are critical points in many of the parallel Prolog systems existing today. OR-parallel systems, such as Aurora [9] and Muse <ref> [20] </ref>, depend heavily on scheduling algorithms to distribute work among workers. In AND and OR parallel systems it is essential not to exploit all possible parallelism since the scheduling and process spawning overhead might easily grow out of hand. <p> However, we have compared their result on the nrev benchmark with ours. In addition to comparing Reform Prolog with AND-parallel systems we have also made a limited comparison with JAM [13] (an implementation of Parlog which exploit stream AND-parallelism) and with Muse <ref> [2, 20] </ref> (an OR-parallel implementation of Prolog). When comparing with Muse we executed the map benchmark. This was done using the findall technique for AND-in-OR parallelism described by Carlsson, Danhof and Overbeek [11].
Reference: 21. <author> King, A. and Soper, P., </author> <title> Schedule Analysis of Concurrent Logic Programs, </title> <booktitle> in Proceedings of the Joint International Conference and Symposium on Logic Programming, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1992. </year> <month> f23g </month>
Reference-contexts: In AND and OR parallel systems it is essential not to exploit all possible parallelism since the scheduling and process spawning overhead might easily grow out of hand. Granularity control is a challenging problem, and a variety of solutions have been proposed <ref> [19, 21] </ref>. Whether to do speculative work or not, is also a delicate problem in these systems. An example of speculative work in an OR-parallel system is to execute a branch of the search tree that might be cut away by another branch to the left of it.
Reference: 22. <author> Kruskal, C. P. and Weiss, A., </author> <title> Allocating Independent Subtasks on Parallel Processors, </title> <journal> in IEEE Transactions on Software Engineering, </journal> <volume> 11 </volume> <pages> 1001-1016, </pages> <year> 1991. </year> <note> f27g 63 </note>
Reference-contexts: On large systems where many processors must access the queue, such 4.3. Evaluation 27 a device is prone to become a bottleneck. Even on small-scale systems it might become a problem if the time to execute each recursion level is small. Uniform-size chunking <ref> [22] </ref> reduces synchronization overhead by each time allocating a number, K , iterations to each processor. The worst case is that processors finish within K iterations of each other.
Reference: 23. <author> Lin Y.-J. and Kumar, V., </author> <title> AND-parallel Execution of Logic Programs on a Shared Memory Multiprocessor: A Summary of Results, </title> <booktitle> in Logic Programming: Proceedings of the Fifth International Conference and Symposium, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1988. </year> <month> f57g </month>
Reference-contexts: AND-parallel implementations can be divided into several categories: independent AND-parallel and dependent AND-parallel. Several methods have been proposed to exploit independent AND-parallelism, the model proposed by DeGroot [15] being the first, Hermenegildo [17] and Lin and Kumar <ref> [23] </ref> have also developed schemes for independent AND-parallel execution. Restricted dependent AND-parallelism have been exploited by several logic programming implementations, e.g., Naish's [36] parallel NU-Prolog. The dependent AND-parallelism is restricted in such a way that only binding deterministic goals are executed in parallel.
Reference: 24. <author> Lindgren, T., </author> <title> The Compilation and Execution of Recursion Parallel Logic Programs for Shared Memory Multiprocessors, Ph.L. </title> <type> Thesis, </type> <institution> Computing Science Department, Uppsala University, </institution> <year> 1993. </year> <journal> f14, </journal> <volume> 32, 35, </volume> <month> 36g </month>
Reference-contexts: Clearly advanced program analyzing techniques have to be used when compiling these programs. Such techniques are being developed by Lindgren <ref> [24] </ref>. Chapter 3 Data Sharing T he idea underlying recursion parallelism is to execute every recursion level of some recursive predicates in parallel. <p> The compiler analyzes the program and inserts synchronization primitives in the code when needed to ensure binding-determinism. The algorithm for this is described by Lindgren <ref> [24] </ref>. There are four synchronization primitives. They verify the instantiation of a term and suspend until the object complies with the required instantiation, or until the current recursion level is leftmost in the resolvent. <p> Reductions using addition and multiplication are instead compiled as calls to built-in predicates which utilize parallel reduction algorithms to reduce vector lists. The full compilation schema for reductions is described by Lindgren <ref> [24] </ref>. Vector List Instructions These instructions are used to build vector lists that are used for easy access of input arguments, and for communication between recursion levels during the parallel phase. The instructions prepare the environment for the following parallel phase and are as such executed by the sequential worker. <p> for second clause */ Ll: build_rec_poslist X0 X2 X3 X0 % create vector list Xs build_poslist X1 X2 X4 X1 % create vector list Ys /* code to execute rel/2 by each worker */ execute map/2 % execute the base case The compilation schema is described in detail by Lindgren <ref> [24] </ref>. build rec poslist (a, n, v, t) This instruction is used for the main recursive argument when compiling list recursion, e.g, the first argument of map/2 above. It verifies that the term referred to by X a is a list.
Reference: 25. <author> Lindholm, T. and O'Keefe, R. A., </author> <title> Efficient Implementation of a Defensible Semantics for Dynamic Prolog Code, </title> <booktitle> in Logic Programming: Proceedings of the Fourth International Conference, </booktitle> <publisher> MIT Press, </publisher> <year> 1987. </year> <month> f30g </month>
Reference-contexts: They can be allowed to grow and shrink in the same way as a heap in a sequential WAM implementation. The database is also located in shared memory, so all workers have access to it. The common logical database view is used, as described by Lindholm and O'Keefe <ref> [25] </ref>. Only predicates declared as being dynamic may be modified at run time. The logical view of the database, in combination with the restriction on which 5.2.
Reference: 26. <author> Lusk, E., Warren, D. H. D. and Haridi, S., </author> <title> The Aurora OR-parallel Prolog System, </title> <journal> in New Generation Computing, </journal> <volume> 7 </volume> <pages> 243-271, </pages> <year> 1990. </year> <month> f57g </month>
Reference-contexts: This in turn is an effect of using Reform Compilation for data-parallelism, as a method for parallelizing Prolog. Chapter 7 Discussion 7.1 RELATED WORK There are two sources of parallelism that can be exploited when parallelizing Pro-log: OR-parallelism and AND-parallelism. Aurora <ref> [26] </ref> and Muse [2] are the two outstanding implementations that exploit OR-parallelism. AND-parallel implementations can be divided into several categories: independent AND-parallel and dependent AND-parallel.
Reference: 27. <author> Mannila, H. and Ukkonen, E., </author> <title> Timestamped Term Representation for Implementing Prolog, </title> <booktitle> in Proceedings 1986 Symposium on Logic Programmin, </booktitle> <publisher> IEEE Computer Society Press, </publisher> <address> Washington, D.C., </address> <year> 1986. </year> <month> f20g </month>
Reference-contexts: Shen [41] uses an analogous method for timestamping variables in his implementation of dependent AND-parallel Prolog. Mannila and Ukkonen <ref> [27] </ref> also describe a similar notion of timestamping terms. They use the timestamp information for implementing intelligent backtracking and for fast high-level tracing of Prolog execution. A positive side effect of using timestamps, or trailing all bindings, is that it is possible to use a copying garbage collector.
Reference: 28. <author> Marien, A. and Demoen, B., </author> <title> A new Scheme for Unification in WAM, </title> <booktitle> in Logic Programming: Proceedings of the 1991 International Symposium, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1991. </year> <month> f18g </month>
Reference-contexts: This method is also discussed by Naish [36]. It is the method currently used in Reform Prolog. There are schemas for head unification in which the transaction from read to write mode is more explicit than in WAM, e.g., the schema presented by Demoen and Marien <ref> [28] </ref>. Our modifications for creating shared structures should be even simpler when using such schemas. Shen [41] uses a different solution. Instead of delaying unification of an incomplete structure, the top element of the heap is marked.
Reference: 29. <author> Markatos, E. P. and LeBlanc, T. J., </author> <title> Using Processor Affinity in Loop Scheduling on Shared-Memory Multiprocessors, </title> <type> Technical Report 410, </type> <institution> University of Rochester, </institution> <year> 1992. </year> <month> f27g </month>
Reference-contexts: A dynamic scheduling algorithm that preserves many of the good characteristics of static scheduling, but also achieves good load balance for programs that are not well suited for static scheduling, has been proposed by Markatos and LeBlanc <ref> [29] </ref>. The idea behind this algorithm is to use static scheduling as far as possible, but if load imbalance occurs, i.e., a processor is idle while there are iterations to be executed, then iterations migrate from one processor to another.
Reference: 30. <author> Mellor-Crummey, J. M. and Scott, M. L., </author> <title> Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors, </title> <journal> in ACM Transactions on Computer Systems, </journal> <volume> 9 </volume> <pages> 21-65, </pages> <year> 1991. </year> <month> f16g </month>
Reference-contexts: If the exchange operation returns a locked variable then another worker has already managed to lock the variable and the first worker has to wait until the lock is dropped and then try again. Such an arrangement is called a spin lock <ref> [30] </ref>. Only the variable the worker wants to 3.3. Creating Shared Structures 17 bind is locked, and there is no possibility of deadlock. The locking operation is illustrated using C code.
Reference: 31. <author> Millroth, H., </author> <title> Reforming Compilation of Logic Programs, </title> <type> Ph.D. Thesis, </type> <institution> Computing Science Department, Uppsala University, </institution> <year> 1990. </year> <note> f3, 9, 11g </note>
Reference-contexts: Recursion Parallelism The basic idea behind recursion parallelism is to parallelize structure recursion. One might consider different variations on this theme. We have chosen to implement Reform Compilation as described by Millroth <ref> [31] </ref> (see Section 2.2). We have introduced one limitation on the predicates that we attempt to parallelize: bindings of variables shared between recursive calls of the predicate must be deterministic. This is not a severe restriction in practice. Our implementation exploits recursion parallelism. <p> It also makes it possible for the Prolog compiler to handle large parts of the parallelization work; the runtime machinery can remain small and simple. The model is derived from the methods for recursion parallel compilation and execution proposed by Millroth <ref> [31] </ref>, which in its turn is based on Tarnlund's [49] Reform inference system. The execution model for Reform Prolog was designed in cooperation with Thomas Lindgren and H-akan Millroth. 2.1 NOTATION Consider a recursive predicate (below) that recurs over its argument. and are, possibly empty, conjunctions of goals. <p> Predicates satisfying (and predicates that can be rewritten to satisfy) the following conditions are considered for parallel execution. 1. The predicate performs integer-recursion or list-recursion, with some restrictions. Millroth <ref> [31, 34] </ref> describes these restrictions in detail. Essentially the size of the recursion has to be fixed by the time it is initiated, and the recursive argument must be reduced by a fixed amount in each recursive call. 2.
Reference: 32. <author> Millroth, H., </author> <title> Compiling List Processing in Parallelized Prolog, </title> <type> unpublished manuscript, </type> <year> 1992. </year> <month> f34g </month>
Reference-contexts: Our solution is to make the vector appear as a list, or rather, represent the list as a vector (see Figure 5.2). We could also have chosen to implement the more 34 WAM Extensions elaborate cdr-coding schema described by Millroth <ref> [32] </ref>, but our method is simpler and does not require an extra tag. Vector lists are also used for sharing variables between recursion levels. This use corresponds to the use of environments in a sequential WAM. Consider the following program. sum ([],S,S).
Reference: 33. <author> Millroth, H., </author> <title> Reforming Compilation for Nonlinear Recursion, </title> <booktitle> in Proceedings of the International Conference on Logic Programming & Automated Reasoning, </booktitle> <publisher> LNCS 624, Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1992. </year> <month> f12g </month>
Reference-contexts: The predicate is the topmost predicate in the call tree considered for recur sion parallel execution. Nested recursion parallelism is prohibited. Our implementation parallelizes integer and list recursion. It is straightforward to extend our scheme for many other data structures. Millroth <ref> [33] </ref> describes how this can be done for binary trees. Note that non-determinism is allowed within each recursion level, although all bindings of shared variables must be deterministic when the entire program is executed sequentially.
Reference: 34. <author> Millroth, H., SLDR-Resolution: </author> <title> Parallelizing Structural Recursion in Logic Programs, to appear in Massively Parallel Reasoning Systems, </title> <editor> editors Robin-son, J. A. and Sibert, E. E., </editor> <publisher> MIT Press, </publisher> <address> Cambridge, Mass. f11g 64 </address>
Reference-contexts: Predicates satisfying (and predicates that can be rewritten to satisfy) the following conditions are considered for parallel execution. 1. The predicate performs integer-recursion or list-recursion, with some restrictions. Millroth <ref> [31, 34] </ref> describes these restrictions in detail. Essentially the size of the recursion has to be fixed by the time it is initiated, and the recursive argument must be reduced by a fixed amount in each recursive call. 2.
Reference: 35. <author> Moto-oka, T. </author> <title> et al ., Challenge for Knowledge Information Processing Systems, </title> <booktitle> in Proceedings of International Conference on Fifth Generation Computer Systems, </booktitle> <year> 1981. </year> <month> f2g </month>
Reference-contexts: Prolog as a Parallel Language One of the goals of the Fifth Generation Computer Systems project in Japan was to produce a system for parallel programming. In the preliminary report Moto-oka <ref> [35] </ref> defines the requirements of the future computer system. The system should, among other things, be easily programmed in order to reduce the software crisis . They chose the high-level language Prolog as the programming language to parallelize.
Reference: 36. <author> Naish, L., </author> <title> Parallelizing NU-Prolog, </title> <booktitle> in Logic Programming: Proceedings of the Fifth International Conference and Symposium, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1988. </year> <journal> f11, </journal> <volume> 17, 18, 53, </volume> <month> 57g </month>
Reference-contexts: All bindings of variables shared between recursion levels in the predicate are deterministic when executed sequentially. This is similar to Naish's <ref> [36] </ref> definition of binding-determinism. 12 Execution Model 3. The predicate is the topmost predicate in the call tree considered for recur sion parallel execution. Nested recursion parallelism is prohibited. Our implementation parallelizes integer and list recursion. It is straightforward to extend our scheme for many other data structures. <p> The other worker's binding value must then be unified with this worker's, to ensure consistency. A similar method is used in the implementation of Parallel NU-Prolog, as described by Naish <ref> [36] </ref>, and in the implementation of Parallel Parlog, as described by Crammond [14]. The operation is illustrated below using C code. <p> A new instruction has to be introduced after the last unify instruction (when the structure is complete) to bind the variable. This method is also discussed by Naish <ref> [36] </ref>. It is the method currently used in Reform Prolog. There are schemas for head unification in which the transaction from read to write mode is more explicit than in WAM, e.g., the schema presented by Demoen and Marien [28]. <p> All three are compiler-based implementations using abstract machines similar to WAM. Naish's PNU-Prolog <ref> [36] </ref> also supports dependent AND-parallelism but we were not able to find any published performance figures for that system. It should be noted that these systems to some extent exploit different forms of parallelism. Reform Prolog and NUA-Prolog exploit deterministic dependent AND-parallelism (recursion parallelism in the case of Reform Prolog). <p> Several methods have been proposed to exploit independent AND-parallelism, the model proposed by DeGroot [15] being the first, Hermenegildo [17] and Lin and Kumar [23] have also developed schemes for independent AND-parallel execution. Restricted dependent AND-parallelism have been exploited by several logic programming implementations, e.g., Naish's <ref> [36] </ref> parallel NU-Prolog. The dependent AND-parallelism is restricted in such a way that only binding deterministic goals are executed in parallel. Shen [40] recently proposed a scheme for full dependent AND-parallel execution of Prolog.
Reference: 37. <author> Palmer, D. and Naish, L., NUA-Prolog: </author> <title> An Extension to the WAM for Parallel Andorra, </title> <booktitle> in Logic Programming: Proceedings of the 8th International Conference, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., 1991. f53, 54g </address>
Reference-contexts: purpose) proportional to the number of workers instead of proportional to the number of recursion levels to be executed. 6.5 OTHER SYSTEMS We compare Reform Prolog with the other Prolog systems which supports dependent AND-parallelism, that we are aware of: Andorra-I [52], Shen's prototype implementation of DASWAM [41], and NUA-Prolog <ref> [37] </ref>. All three are compiler-based implementations using abstract machines similar to WAM. Naish's PNU-Prolog [36] also supports dependent AND-parallelism but we were not able to find any published performance figures for that system. It should be noted that these systems to some extent exploit different forms of parallelism. <p> This simplifies scheduling and process management. Parallel Efficiency We have calculated the parallel efficiency from published figures. In the case of Andorra-I [52] we have excluded benchmarks that mainly exhibit OR-parallelism. The Andorra-I figures were obtained on a Sequent Symmetry using 10 processors. The NUA-prolog <ref> [37] </ref> figure was obtained from the execution of the nrev benchmark using 11 processors on a Sequent Symmetry. The Muse figure was obtained from executing the map benchmark using 10 processors on a Sequent Symmetry. We were not able to find any figures for JAM and DASWAM.
Reference: 38. <author> Polychronopoulos, C. D. and Kuck, D. J., </author> <title> Guided Self-Scheduling: A Practical Scheduling Scheme for Parallel Supercomputers, </title> <journal> in IEEE Transactions on Computers, </journal> <volume> C-36(12), </volume> <year> 1987. </year> <month> f27g </month>
Reference-contexts: The worst case is that processors finish within K iterations of each other. It is, however, hard to decide which value to use for K , since the optimal value depends on the program being executed. A more sophisticated algorithm is guided self-scheduling <ref> [38] </ref>. This algorithm allocates 1/P times the remaining number of iterations for each processor to execute, where P is the number of processors. The drawback of this method is that towards the end few iterations are allocated per processor.
Reference: 39. <author> Santos Costa, V., Warren, D. H. D. and Yang, R., </author> <title> The Andorra-I Engine: </title>
Reference-contexts: The dependent AND-parallelism is restricted in such a way that only binding deterministic goals are executed in parallel. Shen [40] recently proposed a scheme for full dependent AND-parallel execution of Prolog. There are also several combinations of OR-parallelism and AND-parallelism, e.g., the Andorra-I <ref> [39] </ref> system, developed at Bristol University, which exploits restricted dependent AND-parallelism. Reform Prolog exploits restricted dependent AND-parallelism using a method which is most closely related to Naish's PNU-Prolog. Bounded quantifications is another way to exploit data-parallelism in Prolog.
References-found: 39

