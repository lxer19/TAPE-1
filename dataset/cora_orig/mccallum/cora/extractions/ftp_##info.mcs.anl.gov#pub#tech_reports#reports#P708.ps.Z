URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P708.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/preprints.htm
Root-URL: http://www.mcs.anl.gov
Title: NEOS AND CONDOR: SOLVING OPTIMIZATION PROBLEMS OVER THE INTERNET  
Phone: 60439  
Author: Michael C. Ferris, Michael P. Mesnier, and Jorge J. More 
Date: March 1998  
Address: 9700 South Cass Avenue Argonne, Illinois  Preprint ANL/MCS-P708-0398  
Affiliation: ARGONNE NATIONAL LABORATORY  Mathematics and Computer Science Division  
Abstract: This work was supported by the Mathematical, Information, and Computational Sciences Division subprogram of the Office of Computational and Technology Research, U.S. Department of Energy, under Contract W-31-109-Eng-38, by the National Science Foundation under Grants CDA-9726385 and CCR-9619765, and by the National Science Foundation, through the Center for Research on Parallel Computation, under Cooperative Agreement No. CCR-9120008. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B. M. Averick, J. J. Mor e, C. H. Bischof, A. Carle, and A. Griewank, </author> <title> Computing large sparse Jacobian matrices using automatic differentiation, </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> 15 (1994), </volume> <pages> pp. 285-294. </pages>
Reference-contexts: Moreover, the code produced by ADIFOR for the computation of the sparse Jacobian matrix is typically more efficient than the code produced by differences. On the other hand, the code produced by ADIFOR may not be as efficient as a hand-coded Jacobian matrix. See <ref> [1] </ref> for a full comparison (in terms of memory and speed) of ADIFOR-generated Jacobian matrices with both hand-coded and difference approximations, and [2] and [7] for performance issues related to the automatic computation of gradients.
Reference: [2] <author> C. Bischof, A. Bouaricha, P. Khademi, and J. J. Mor e, </author> <title> Computing gradients in large-scale optimization using automatic differentiation, </title> <journal> INFORMS J. Computing, </journal> <volume> 9 (1997), </volume> <pages> pp. 185-194. </pages>
Reference-contexts: On the other hand, the code produced by ADIFOR may not be as efficient as a hand-coded Jacobian matrix. See [1] for a full comparison (in terms of memory and speed) of ADIFOR-generated Jacobian matrices with both hand-coded and difference approximations, and <ref> [2] </ref> and [7] for performance issues related to the automatic computation of gradients. The automatic generation of the Jacobian matrix and sparsity pattern in the NEOS version of PATH makes the code more accessible and useful than requiring the hand-coding of the Jacobian matrix.
Reference: [3] <author> C. Bischof, A. Carle, G. Corliss, A. Griewank, and P. Hovland, ADIFOR: </author> <title> Generating derivative codes from Fortran programs, </title> <booktitle> Scientific Programming, 1 (1992), </booktitle> <pages> pp. 1-29. </pages>
Reference-contexts: For example, we would like to check that the function provided is indeed differentiable. If the user provides a function that is discontinuous, automatic differentiation tools will generate the Jacobian matrix but will not be able to detect this situation. 4 ADIFOR the automatic differentiator tool ADIFOR/SparsLinc <ref> [3, 5] </ref> is used to produce the Jacobian matrix of F and the sparsity structure of the Jacobian matrix. This information is then fed to PATH. <p> These objects are manipulated and accessed by PATH as described below. Further details on how to invoke ADIFOR can be found in <ref> [3, 5] </ref>. We compute the Jacobian of F by manipulating the gradient object with subroutines provided by ADIFOR in the SparsLinc [4] library.
Reference: [4] <author> C. Bischof, A. Carle, and P. Khademi, </author> <title> Fortran 77 interface specification to the SparsLinC library, </title> <type> Technical Report ANL/MCS-TM-196, </type> <institution> Argonne National Laboratory, Argonne, Illinois, </institution> <year> 1994. </year>
Reference-contexts: Sparse matrix techniques are used for large problems. The process used to solve a nonlinear complementarity problem by this NEOS solver includes the generation of derivative and sparsity information with the ADIFOR <ref> [4, 5] </ref> automatic differentiation tool and the solution of the problem with Condor. The process is governed by a solver script that must check the user data and provide appropriate messages in the case of errors. <p> This process includes the generation of derivative and sparsity information with the ADIFOR <ref> [5, 4] </ref> automatic differentiation tool and the solution of the problem with the Condor [15, 11] distributed resource management system. We discuss ADIFOR further in Section 4, while Condor is discussed in Section 5. <p> The solver (in this case PATH) checks the data and compiles the user's code. If any errors are found at this stage, the compiler error messages are returned to the user, and execution terminates. If the user's code compiles correctly, the automatic differentiation tool ADIFOR <ref> [5, 4] </ref> is used to generate the Jacobian matrix and the sparsity pattern. Additional details on this part of the process are discussed in Section 4. Once the Jacobian matrix and sparsity pattern are obtained, the user's code is linked with the optimization libraries, and execution begins. <p> These objects are manipulated and accessed by PATH as described below. Further details on how to invoke ADIFOR can be found in [3, 5]. We compute the Jacobian of F by manipulating the gradient object with subroutines provided by ADIFOR in the SparsLinc <ref> [4] </ref> library. We first set the gradient object g x for the independent variable x to the identity matrix with the code segment do j = 1, n call dspsd (g_x (j),j,1.d0,1) end do Once g x is defined, we use the ADIFOR-supplied subroutine dspxsq to compute the Jaco-bian matrix.
Reference: [5] <author> C. Bischof, A. Carle, P. Khademi, and A. Mauer, </author> <title> The ADIFOR 2.0 system for the automatic differentiation of Fortran 77 programs, </title> <type> Preprint MCS-P381-1194, </type> <institution> Ar-gonne National Laboratory, Argonne, Illinois, </institution> <year> 1994. </year> <note> Also available as CRPC-TR94491, Center for Research on Parallel Computation, </note> <institution> Rice University. </institution>
Reference-contexts: Sparse matrix techniques are used for large problems. The process used to solve a nonlinear complementarity problem by this NEOS solver includes the generation of derivative and sparsity information with the ADIFOR <ref> [4, 5] </ref> automatic differentiation tool and the solution of the problem with Condor. The process is governed by a solver script that must check the user data and provide appropriate messages in the case of errors. <p> This process includes the generation of derivative and sparsity information with the ADIFOR <ref> [5, 4] </ref> automatic differentiation tool and the solution of the problem with the Condor [15, 11] distributed resource management system. We discuss ADIFOR further in Section 4, while Condor is discussed in Section 5. <p> The solver (in this case PATH) checks the data and compiles the user's code. If any errors are found at this stage, the compiler error messages are returned to the user, and execution terminates. If the user's code compiles correctly, the automatic differentiation tool ADIFOR <ref> [5, 4] </ref> is used to generate the Jacobian matrix and the sparsity pattern. Additional details on this part of the process are discussed in Section 4. Once the Jacobian matrix and sparsity pattern are obtained, the user's code is linked with the optimization libraries, and execution begins. <p> For example, we would like to check that the function provided is indeed differentiable. If the user provides a function that is discontinuous, automatic differentiation tools will generate the Jacobian matrix but will not be able to detect this situation. 4 ADIFOR the automatic differentiator tool ADIFOR/SparsLinc <ref> [3, 5] </ref> is used to produce the Jacobian matrix of F and the sparsity structure of the Jacobian matrix. This information is then fed to PATH. <p> These objects are manipulated and accessed by PATH as described below. Further details on how to invoke ADIFOR can be found in <ref> [3, 5] </ref>. We compute the Jacobian of F by manipulating the gradient object with subroutines provided by ADIFOR in the SparsLinc [4] library.
Reference: [6] <author> C. Bischof, L. Roh, and A. Mauer, ADIC: </author> <title> An extensible automatic differentiation tool for ANSI-C, </title> <journal> Software | Practice and Experience, </journal> <volume> 27 (1997), </volume> <pages> pp. 1427-1456. </pages>
Reference-contexts: Indeed, all nonlinear solvers in NEOS use automatic differentiation tools to compute gradients, Jacobians, and sparsity patterns. We intend to incorporate ADIC <ref> [6] </ref> into most of the nonlinear NEOS solvers to allow problems to be specified in C, as well as Fortran. The final section of the paper describes how the Condor system at the University of Wisconsin is used to process the submitted jobs.
Reference: [7] <author> A. Bouaricha and J. J. Mor e, </author> <title> Impact of partial separability on large-scale optimization, </title> <journal> Comp. Optim. Appl., </journal> <volume> 7 (1997), </volume> <pages> pp. 27-40. </pages>
Reference-contexts: On the other hand, the code produced by ADIFOR may not be as efficient as a hand-coded Jacobian matrix. See [1] for a full comparison (in terms of memory and speed) of ADIFOR-generated Jacobian matrices with both hand-coded and difference approximations, and [2] and <ref> [7] </ref> for performance issues related to the automatic computation of gradients. The automatic generation of the Jacobian matrix and sparsity pattern in the NEOS version of PATH makes the code more accessible and useful than requiring the hand-coding of the Jacobian matrix. <p> This heuristic was also used by Bouaricha and More <ref> [7] </ref> in a similar situation. If the sparsity pattern changes as the iteration proceeds then the heuristic that we are using may fail. However, this situation seems to be rare.
Reference: [8] <author> J. Czyzyk, M. P. Mesnier, and J. J. Mor e, </author> <title> The Network-Enabled Optimization System (NEOS) Server, </title> <type> Preprint MCS-P615-0996, </type> <institution> Argonne National Laboratory, Ar-gonne, Illinois, </institution> <year> 1996. </year> <note> To appear in IEEE Computational Science & Engineering. </note>
Reference-contexts: 1 Introduction The NEOS Server <ref> [8] </ref> is a novel environment for solving optimization problems over the Internet. There is no need to download an optimization solver, write code to call the optimization solver, or compute derivatives for nonlinear problems. <p> The user is provided with a solution and runtime statistics. Each solver in the NEOS optimization library is maintained by a software administrator that is responsible for providing computing resources and for answering questions related to the solver. Registering the solver <ref> [8] </ref> on a few workstations provides adequate resources in most cases, but for large problems, however, we need a different approach. The obvious difficulty is that the owner of a workstation is reluctant to provide large amounts of computing cycles and memory. <p> We do not discuss the design and implementation of the Server because these issues are covered by Czyzyk, Mesnier, and More <ref> [8] </ref>. Extensions to the NEOS Server and the network computing issues that arise from the emerging style of computing used by NEOS are discussed by Gropp and More [14]. <p> We have mentioned nonlinear optimization solvers, but NEOS contains solvers in other areas. A complete listing is available at the NEOS Server homepage: http://www.mcs.anl.gov/otc/Server/ The addition of solvers is not difficult. Indeed, as discussed in <ref> [8] </ref>, NEOS was designed so that solvers in a wide variety of optimization areas can be added easily. We provide Internet users the choice of three interfaces for submitting problems: e-mail, the NEOS Submission Tool, and the NEOS Web interface.
Reference: [9] <author> S. P. Dirkse and M. C. Ferris, MCPLIB: </author> <title> A collection of nonlinear mixed complementarity problems, </title> <booktitle> Optim. Methods Software, 5 (1995), </booktitle> <pages> pp. </pages> <month> 319-345. </month> <title> [10] , The PATH solver: A non-monotone stabilization scheme for mixed complementarity problems, </title> <booktitle> Optim. Methods Software, 5 (1995), </booktitle> <pages> pp. 123-156. </pages>
Reference-contexts: Springfield, Urbana, Illinois, 61801. mesnier@cs.uiuc.edu x Mathematics and Computer Science Division, Argonne National Laboratory, 9700 South Cass Avenue, Argonne, Illinois 60439. more@mcs.anl.gov 1 Many different applications can be formulated as mixed complementarity problems; ex-amples are given in <ref> [9, 13] </ref>.
Reference: [11] <author> D. H. Epema, M. Livny, R. van Dantzig, X. Evers, and J. Pruyne, </author> <title> A worldwide flock of condors: Load sharing among workstation clusters, </title> <journal> Journal on Future Generations of Computer Systems, </journal> <year> (1996). </year>
Reference-contexts: Registering the solver [8] on a few workstations provides adequate resources in most cases, but for large problems, however, we need a different approach. The obvious difficulty is that the owner of a workstation is reluctant to provide large amounts of computing cycles and memory. We use Condor <ref> [15, 11] </ref>, a distributed resource management system, as a provider of computational resources for a NEOS solver. The resources that are managed by Condor are typically large clusters of workstations, many of which would otherwise be idle for long periods of time. <p> This process includes the generation of derivative and sparsity information with the ADIFOR [5, 4] automatic differentiation tool and the solution of the problem with the Condor <ref> [15, 11] </ref> distributed resource management system. We discuss ADIFOR further in Section 4, while Condor is discussed in Section 5. In this section we discuss issues in the solution process that are pertinent to the development of optimization software and problem-solving environments. <p> Note that the resulting column-wise storage is sorted by row indices, even though this is not required by PATH. A final check to determine whether the allocated storage is sufficient is carried out after all rows have been processed. 5 Condor Condor <ref> [11, 15] </ref> is a distributed resource management system, developed at the University of Wisconsin, that manages large heterogeneous clusters of workstations. Due to the ever decreasing cost of low-end workstations, such resources are becoming prevalent in many workplaces.
Reference: [12] <author> M. C. Ferris and T. S. Munson, </author> <title> Interfaces to PATH 3.0: Design, implementation and usage, </title> <type> Mathematical Programming Technical Report 97-12, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, Wisconsin, </institution> <year> 1997. </year>
Reference-contexts: Extensions to the NEOS Server and the network computing issues that arise from the emerging style of computing used by NEOS are discussed by Gropp and More [14]. Mixed complementarity problems submitted to the NEOS Server are currently solved by the PATH <ref> [10, 12] </ref> solver, which implements a Newton-type method for solving systems of non-differentiable equations. Sparse matrix techniques are used for large problems. <p> We discuss ADIFOR further in Section 4, while Condor is discussed in Section 5. In this section we discuss issues in the solution process that are pertinent to the development of optimization software and problem-solving environments. Although the discussion is specific to PATH <ref> [10, 12] </ref>, most of the issues are applicable to all the solvers of nonlinear optimization problems in NEOS. <p> Our interface replaces a single line in the standalone makefile, namely, f77 -o pathsol $(OBJECTS) $(LIBS) with the following line condor compile f77 -o pathsol $(OBJECTS) $(LIBS) Both of these makefiles use precisely the same library of routines that implement PATH as described in <ref> [12] </ref>.
Reference: [13] <author> M. C. Ferris and J.-S. Pang, </author> <title> Engineering and economic applications of complementarity problems, </title> <journal> SIAM Rev., </journal> <volume> 39 (1997), </volume> <pages> pp. 669-713. </pages>
Reference-contexts: Springfield, Urbana, Illinois, 61801. mesnier@cs.uiuc.edu x Mathematics and Computer Science Division, Argonne National Laboratory, 9700 South Cass Avenue, Argonne, Illinois 60439. more@mcs.anl.gov 1 Many different applications can be formulated as mixed complementarity problems; ex-amples are given in <ref> [9, 13] </ref>.
Reference: [14] <author> W. Gropp and J. J. Mor e, </author> <title> Optimization environments and the NEOS server, in Approximation Theory and Optimization, </title> <editor> M. D. Buhmann and A. Iserles, eds., </editor> <publisher> Cam-bridge University Press, </publisher> <year> 1997, </year> <pages> pp. 167-182. 15 </pages>
Reference-contexts: We do not discuss the design and implementation of the Server because these issues are covered by Czyzyk, Mesnier, and More [8]. Extensions to the NEOS Server and the network computing issues that arise from the emerging style of computing used by NEOS are discussed by Gropp and More <ref> [14] </ref>. Mixed complementarity problems submitted to the NEOS Server are currently solved by the PATH [10, 12] solver, which implements a Newton-type method for solving systems of non-differentiable equations. Sparse matrix techniques are used for large problems.
Reference: [15] <author> M. J. Litzkow, M. Livny, and M. W. </author> <title> Mutka, Condor A hunter of idle work-stations, </title> <booktitle> in Proceedings of the 8th International Conference on Distributed Computing Systems, </booktitle> <address> Washington, District of Columbia, 1988, </address> <publisher> IEEE Computer Society Press, </publisher> <pages> pp. 108-111. </pages>
Reference-contexts: Registering the solver [8] on a few workstations provides adequate resources in most cases, but for large problems, however, we need a different approach. The obvious difficulty is that the owner of a workstation is reluctant to provide large amounts of computing cycles and memory. We use Condor <ref> [15, 11] </ref>, a distributed resource management system, as a provider of computational resources for a NEOS solver. The resources that are managed by Condor are typically large clusters of workstations, many of which would otherwise be idle for long periods of time. <p> This process includes the generation of derivative and sparsity information with the ADIFOR [5, 4] automatic differentiation tool and the solution of the problem with the Condor <ref> [15, 11] </ref> distributed resource management system. We discuss ADIFOR further in Section 4, while Condor is discussed in Section 5. In this section we discuss issues in the solution process that are pertinent to the development of optimization software and problem-solving environments. <p> Note that the resulting column-wise storage is sorted by row indices, even though this is not required by PATH. A final check to determine whether the allocated storage is sufficient is carried out after all rows have been processed. 5 Condor Condor <ref> [11, 15] </ref> is a distributed resource management system, developed at the University of Wisconsin, that manages large heterogeneous clusters of workstations. Due to the ever decreasing cost of low-end workstations, such resources are becoming prevalent in many workplaces.
Reference: [16] <author> E. Simantiraki and D. F. Shanno, </author> <title> An infeasible-interior-point algorithm for solving mixed complementarity problems, in Complementarity and Variational Problems: State of the Art, </title> <editor> M. C. Ferris and J. S. Pang, eds., </editor> <address> Philadelphia, Pennsylvania, 1997, </address> <publisher> SIAM Publications, </publisher> <pages> pp. 386-404. </pages>
Reference: [17] <author> L. Wall, T. Christiansen, and R. L. Schwartz, </author> <title> Programming Perl, </title> <publisher> O'Reilly & Associates, Inc., </publisher> <editor> second ed., </editor> <year> 1996. </year> <month> 16 </month>
Reference-contexts: In the remainder of this section we examine the NEOS Submission Tool. The NEOS Submission Tool provides a high-speed link to the NEOS Server via TCP/IP sockets. Once this tool is installed (only Perl <ref> [17] </ref> is required), the user has access to all solvers offered by NEOS. Additional information on the NEOS Submission Tool, including installation instructions, can be obtained from the NEOS Server homepage. Submission of problems via the NEOS Submission Tool is simple.
References-found: 16

