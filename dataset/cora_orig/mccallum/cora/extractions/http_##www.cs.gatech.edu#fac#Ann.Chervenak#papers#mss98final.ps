URL: http://www.cs.gatech.edu/fac/Ann.Chervenak/papers/mss98final.ps
Refering-URL: http://www.cs.gatech.edu/fac/Ann.Chervenak/ptera/backup.html
Root-URL: 
Email: fannc,vivek,kurmaszg@cc.gatech.edu  
Phone: tel +1-404-894-8591  
Title: Protecting File Systems: A Survey of Backup Techniques  
Author: Ann L. Chervenak Vivekanand Vellanki Zachary Kurmas 
Affiliation: Georgia Tech's College of Computing.  
Abstract: This paper presents a survey of backup techniques for protecting file systems. These include such choices as device-based or file-based backup schemes, full vs. incremental backups, and optional data compression. Next, we discuss techniques for on-line backup (backups performed while users continue to access the file system); these techniques include file system locking and creating instantaneous, copy-on-write snapshots of the file system. Last, we discuss protecting data from site disasters and media deterioration. The paper then classifies several research and commercial backup systems according to the parameters already described. The classified systems include the UNIX dump and tar utilities; hierarchical storage managers such as IBM's AD-STAR Distributed Storage Manager, Legato's NetWorker, and UniTree; and several research and academic systems. We conclude with measurements of full and incremental backups over a one-year period in a large networked UNIX environment at 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. A. Anderson and B. H. Kirouac. </author> <title> A Simple and Free System for Automated Network Backups. </title> <booktitle> In The Third Annual System Adminstration, Networking and Security Conference (SANS III), pages 6368. Open Systems Conference Board, </booktitle> <month> April </month> <year> 1994. </year>
Reference: [2] <author> B. Bhushan. </author> <title> Tribology and Mechanics of Magnetic Storage Devices. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: Despite this, tapes occasionally encounter errors that cannot be rectified by the ECC. Tape wear is another reliability issue. Magnetic tapes that are frequently read or written eventually wear out. Tapes last on average several hundred passes <ref> [2] </ref>, [12], [16]. However, they wear out sooner if a particular segment of the tape is accessed repeatedly. Frequent stops and starts on the tape cause excessive wear. Tape heads also wear out, typically after a few hundred or thousand hours of use.
Reference: [3] <author> S. Cariapa, R. Clark, and B. Cox. </author> <note> Origin2000 One-Terabyte Per Hour Backup White Paper. http://www.sgi.com/Technology/teraback/teraback.html. </note>
Reference-contexts: This system achieves a backup rate of over a terabyte of data per hour <ref> [3] </ref> using both Legato NetWorker and Spectra Logic Alexandria hierarchical storage manager software.
Reference: [4] <author> L. Corp. Legato Networker Documentation. </author> <note> http://www.legato.com. </note>
Reference-contexts: needed for recovery, the off-site locations of these volumes, the devices needed to read backups, the amount of space required to restore the file system, system configuration files and shell scripts needed to initiate recovery. 3.4 Legato NetWorker Legato NetWorker is another commercial storage manager that performs automated network backup <ref> [4] </ref>. NetWorker allows up to 64 file systems to send data to up to 32 backup storage devices simultaneously. To achieve optimal transfer rates for backup tape drives, NetWorker interleaves backup data sent simultaneously by multiple clients onto a single tape.
Reference: [5] <author> J. da Silva, O. Gudmundsson, and D. Mosse. </author> <title> Performance of a Parallel Network Backup Manager. </title> <booktitle> In USENIX Conference Proceedings, </booktitle> <pages> pages 17 26, </pages> <year> 1992. </year>
Reference-contexts: Frangipani [25] is a distributed file system built on top of Petal virtual disks that uses the Petal snapshot facility to perform file system backups. 3.7 Amanda The Amanda Network Backup Manager <ref> [5] </ref> [6] was developed at the University of Mary-land at College Park to facilitate network backup of many UNIX workstations in parallel. Amanda uses standard UNIX backup programs like dump and tar.
Reference: [6] <author> J. da Silva and O. Guomundsson. </author> <title> The Amanda Network Backup Manager. </title> <booktitle> In Proceedings of USENIX Systems Administration (LISA VII) Conference, </booktitle> <pages> pages 171182, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Frangipani [25] is a distributed file system built on top of Petal virtual disks that uses the Petal snapshot facility to perform file system backups. 3.7 Amanda The Amanda Network Backup Manager [5] <ref> [6] </ref> was developed at the University of Mary-land at College Park to facilitate network backup of many UNIX workstations in parallel. Amanda uses standard UNIX backup programs like dump and tar.
Reference: [7] <author> R. Green, A. Baird, and C. Davies. </author> <title> Designing a Fast, On-line Backup System for a Log-structured File System. </title> <journal> Digital Technical Journal, </journal> <month> October </month> <year> 1996. </year>
Reference-contexts: One or more machines in the networked file system are designated as Backup Machines. These machines coordinate tape drive operations and maintain a database with information about tapes and dump schedules for individual volumes and sets of volumes. 3.10 Log-Structured File System Backup The Spiralog backup system from DEC <ref> [7] </ref> provides on-line backup of a log-structured file system or LFS [22]. A log-structured file system writes all modifications to disk sequentially in an append-only log [22]. The log is divided into segments, which are composed of disk blocks. <p> LFS can create snapshots, also called time travel, of the file system by creating checkpoints and turning off the segment cleaner so that space is not reclaimed after files are deleted. Spiralog exploits several features of LFS in its backup system <ref> [7] </ref>. It uses LFS's checkpoint capability to create a snapshot of the file system that is then copied to the backup medium.
Reference: [8] <author> S. Hecht. </author> <title> Andrew Backup System. </title> <booktitle> In USENIX Proceedings of the Workshop on Large Installation Systems Administration, </booktitle> <pages> pages pp. 3538, </pages> <month> November </month> <year> 1988. </year>
Reference-contexts: File access permissions were maintained on snapshot copies of files. 3.9 Andrew File System The Andrew File System (AFS) organizes files into volumes, which are the focus of system management, including backup and restore <ref> [8] </ref> [14]. A volume is a collection of files and directories. Typically, a user's home directory resides in a different volume, as do different binary directories. A volume resides entirely on one magnetic disk partition, but a disk partition may hold multiple volumes. Andrew allows cloning or copying of volumes.
Reference: [9] <author> I.B.M. </author> <title> ADSTAR Distributed Storage Manager (ADSM) Distributed Data Recovery White Paper. </title> <address> http://www.storage.ibm.com/storage/software/adsm/adwhddr.htm. </address>
Reference-contexts: Therefore, tar is widely used for transferring files and directories as well as for backups. 3.3 IBM ADSM One commercial storage management system that includes flexible backup functionality is IBM's ADSM, or ADSTAR Distributed Storage Manager <ref> [9] </ref> [10]. ADSM supports automated backup of multiple clients concurrently to separate backup devices, optionally compresses data before backup, and can write multiple copies of a backup file. The storage manager also maintains multiple versions of files for a specified length of time. ADSM allows selective and incremental backups.
Reference: [10] <author> I.B.M. </author> <title> ADSTAR Distributed Storage Manager (ADSM) Frequently Asked Questions. </title> <address> http://www.storage.ibm.com/storage/software/adsm/adfaq.htm. </address>
Reference-contexts: Therefore, tar is widely used for transferring files and directories as well as for backups. 3.3 IBM ADSM One commercial storage management system that includes flexible backup functionality is IBM's ADSM, or ADSTAR Distributed Storage Manager [9] <ref> [10] </ref>. ADSM supports automated backup of multiple clients concurrently to separate backup devices, optionally compresses data before backup, and can write multiple copies of a backup file. The storage manager also maintains multiple versions of files for a specified length of time. ADSM allows selective and incremental backups.
Reference: [11] <author> F. Kim. UniTree: </author> <title> A Closer Look at Solving the Data Storage Problem. </title> <publisher> UniTree Software Inc., </publisher> <address> http://www.unitree.com/newpage/wpaper.htm. </address>
Reference-contexts: Thus, files are protected more quickly than in a daily incremental backup. The user can make up to 15 copies of a file on physically distinct media <ref> [11] </ref>. Future versions of UniTree will provide disaster protection by creating automatic remote copies of files over a wide area network. <p> In addition, UniTree supports a trash can feature, in which directories for each user retain recently discarded files. This feature allows users to quickly restore files they have accidentally deleted without requiring reference to backup tapes <ref> [11] </ref>. UniTree also offers database recovery, including transaction logs. 3.6 Petal and Frangipani The Petal system [15] from the DEC Systems Research Center allows a collection of network-connected servers to cooperatively manage a pool of physical disks.
Reference: [12] <author> T. Kitahara. </author> <title> On the Long Term Storage of Metal Tapes. </title> <institution> Fuji Photo Film Co. Magnetic Recording Lab; FIAT/IFTA Technical Commission in Hilversum (The Netherlands), </institution> <month> June 14th </month> <year> 1988. </year>
Reference-contexts: Despite this, tapes occasionally encounter errors that cannot be rectified by the ECC. Tape wear is another reliability issue. Magnetic tapes that are frequently read or written eventually wear out. Tapes last on average several hundred passes [2], <ref> [12] </ref>, [16]. However, they wear out sooner if a particular segment of the tape is accessed repeatedly. Frequent stops and starts on the tape cause excessive wear. Tape heads also wear out, typically after a few hundred or thousand hours of use. <p> Another set of reliability concerns involves the long-term storage of data on tape. Over time, tapes are subject to corrosion and to mechanical changes including tape shrinkage, creasing of the edges, and peeling of the magnetic layer <ref> [12] </ref>. To avoid losing data due to wear, mechanical problems, or tape deterioration, backup systems must maintain information on tape and drive usage, and must replace old tapes and schedule drive maintenance. Nemeth et al. offer additional practical advice for performing backups in UNIX environments [19].
Reference: [13] <author> R. Kolstad. </author> <title> A Next Step in Backup and Restore Technology. </title> <booktitle> In USENIX Proceedings of the 5th Conference on Large Installation Systems Administration, </booktitle> <pages> pages 7378, </pages> <month> September </month> <year> 1991. </year>
Reference: [14] <author> S. Lammert. </author> <title> The AFS 3.0 Backup System. </title> <booktitle> In USENIX Proceedings of the 4th Con--ference on Large Installation Systems Administration, </booktitle> <pages> pages 143147, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: File access permissions were maintained on snapshot copies of files. 3.9 Andrew File System The Andrew File System (AFS) organizes files into volumes, which are the focus of system management, including backup and restore [8] <ref> [14] </ref>. A volume is a collection of files and directories. Typically, a user's home directory resides in a different volume, as do different binary directories. A volume resides entirely on one magnetic disk partition, but a disk partition may hold multiple volumes. Andrew allows cloning or copying of volumes.
Reference: [15] <author> E. K. Lee and C. A. Thekkath. </author> <title> Petal: Distributed Virtual Disks. </title> <booktitle> In Seventh International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VII), </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: This feature allows users to quickly restore files they have accidentally deleted without requiring reference to backup tapes [11]. UniTree also offers database recovery, including transaction logs. 3.6 Petal and Frangipani The Petal system <ref> [15] </ref> from the DEC Systems Research Center allows a collection of network-connected servers to cooperatively manage a pool of physical disks. The pool of disks appears to the servers as a single large virtual disk. Petal provides a copy-on-write snapshot mechanism.
Reference: [16] <author> J. C. Mallinson. </author> <title> Magnetic Tape Recording: Archival Considerations. </title> <booktitle> In Digest of Papers. Tenth IEEE Symposium on Mass Storage Systems, </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: Despite this, tapes occasionally encounter errors that cannot be rectified by the ECC. Tape wear is another reliability issue. Magnetic tapes that are frequently read or written eventually wear out. Tapes last on average several hundred passes [2], [12], <ref> [16] </ref>. However, they wear out sooner if a particular segment of the tape is accessed repeatedly. Frequent stops and starts on the tape cause excessive wear. Tape heads also wear out, typically after a few hundred or thousand hours of use.
Reference: [17] <author> C. D. Mee and E. D. Daniel, </author> <title> editors. Magnetic Recording, Volume II: Computer Data Storage. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: A tape error is detected when previously written data cannot be successfully read. Most of these errors are caused by debris that become embedded in the tape surface <ref> [17] </ref>. Because the rate of such errors is high, all magnetic tape drives incorporate large amounts of error-correction code. Despite this, tapes occasionally encounter errors that cannot be rectified by the ECC. Tape wear is another reliability issue. Magnetic tapes that are frequently read or written eventually wear out.
Reference: [18] <author> NCSA. </author> <title> UniTree Mass Storage System Frequently Asked Questions. </title> <address> http://consult.ncsa.uiuc.edu/docs/unitree. </address>
Reference-contexts: The save process reads backup data from a client file system or database and forwards it to the media manager, which writes it to the backup device. 3.5 UniTree UniTree <ref> [18] </ref> is a hierarchical mass storage system for UNIX environments originally developed at Lawrence Livermore National Laboratory. UniTree allows automatic migration of files between levels of a storage hierarchy. UniTree does not perform traditional full or incremental backups, but rather provides continuous backup [24].
Reference: [19] <author> E. Nemeth, G. Snyder, S. Seebass, and T. R. Hein. </author> <title> UNIX System Administration Handbook. </title> <publisher> Prentice Hall, Inc., </publisher> <address> Upper Saddle River, New Jersey, </address> <year> 1995. </year>
Reference-contexts: To avoid losing data due to wear, mechanical problems, or tape deterioration, backup systems must maintain information on tape and drive usage, and must replace old tapes and schedule drive maintenance. Nemeth et al. offer additional practical advice for performing backups in UNIX environments <ref> [19] </ref>. The correctness of backup procedures must be monitored; backup software should attempt to re-read tapes after dumps are complete. <p> A scheme developed at Ohio State University maintains multiple redundant backup chains, with independent sets of full and incremental backups [21]. Such a scheme requires twice as many tapes and drives, but offers a high degree of reliability. 2.9 Disaster Recovery Nemeth et al. <ref> [19] </ref> describe techniques that ensure that recovery will be possible after natural disasters. Dump tapes that contain full backups should be stored off-site to prevent data loss in the event of natural disaster or fire. <p> A level N dump backs up all files that have been modified since the last dump of level less than N. Different sites devise schedules for dump based on the activity of the file systems, the capacity and number of tapes, and the amount of redundancy desired <ref> [19] </ref>. A simple but expensive schedule is to do level zero dumps (full backups) every day. A more moderate schedule would perform different levels of backup on a daily, weekly, monthly, and annual basis. <p> For example, daily level three backups could be supplemented with weekly level two backups, monthly level one backups, and level zero backups at least once a year. Backup schemes like the Towers of Hanoi sequence <ref> [19] </ref> alternate backups of many different levels to achieve high redundancy with relatively few tapes; such schemes are complex and may perform poorly on restores, since they access many tapes. The dump program includes options for specifying tape characteristics including capac-ity, tape density, blocking factor, length, and number of tracks.
Reference: [20] <author> R. Pike, D. Presotto, K. Thompson, and H. Trickey. </author> <title> Plan 9 from Bell Labs. </title>
Reference-contexts: After backups, Amanda reports any problems that occurred, including disk errors, backup program problems such as permission errors or software crashes, and client hosts that were down. 3.8 Plan 9 The Plan 9 computing environment <ref> [20] </ref> used a write-once optical disk jukebox for permanent file storage. The system also included a magnetic disk file cache for faster file access. Daily on-line backups were made by creating snapshots of the file system.
Reference: [21] <author> S. M. Romig. </author> <title> Backup at Ohio State, Take 2. </title> <booktitle> In USENIX Proceedings of the 4th Conference on Large Installation Systems Administration, </booktitle> <pages> pages 137 142, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: This allows files to be recovered in the event of a loss of either the current week's or previous week's full backup. A scheme developed at Ohio State University maintains multiple redundant backup chains, with independent sets of full and incremental backups <ref> [21] </ref>. Such a scheme requires twice as many tapes and drives, but offers a high degree of reliability. 2.9 Disaster Recovery Nemeth et al. [19] describe techniques that ensure that recovery will be possible after natural disasters.
Reference: [22] <author> M. Rosenblum and J. Ousterhout. </author> <title> Log-Structured File System. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 115, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: These machines coordinate tape drive operations and maintain a database with information about tapes and dump schedules for individual volumes and sets of volumes. 3.10 Log-Structured File System Backup The Spiralog backup system from DEC [7] provides on-line backup of a log-structured file system or LFS <ref> [22] </ref>. A log-structured file system writes all modifications to disk sequentially in an append-only log [22]. The log is divided into segments, which are composed of disk blocks. Because the log is written sequentially, segments appear in the log in temporal order. <p> information about tapes and dump schedules for individual volumes and sets of volumes. 3.10 Log-Structured File System Backup The Spiralog backup system from DEC [7] provides on-line backup of a log-structured file system or LFS <ref> [22] </ref>. A log-structured file system writes all modifications to disk sequentially in an append-only log [22]. The log is divided into segments, which are composed of disk blocks. Because the log is written sequentially, segments appear in the log in temporal order. Over time, as portions of files are edited and deleted, portions of segments contain obsolete data.
Reference: [23] <author> S. Shumway. </author> <title> Issues in On-line Backup. </title> <booktitle> In USENIX Proceedings of the 5th Conference on Large Installation Systems Administration, </booktitle> <pages> pages 8188, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: The UNIX dump program is sometimes called device-based, although it is more accurately considered a hybrid between a pure device-based and a file-based scheme (Section 3.1). To allow file recovery, device-based backups must include information on how files and directories are organized on disks <ref> [23] </ref> to correlate blocks on the backup medium with particular files. Thus, device-based programs are likely to be specific to a particular file system implementation and not easily portable. <p> On-line backup systems offer higher availability but introduce consistency problems. Shumway <ref> [23] </ref> discusses many of the difficulties with on-line backups when using programs like UNIX tar and dump. The most serious problems occur when directories are moved during backup, changing the file system hierarchy.
Reference: [24] <author> U. </author> <title> Software. </title> <type> Technical Overview. </type> <note> http://www.unitree.com/newpage/techover.htm. </note>
Reference-contexts: UniTree allows automatic migration of files between levels of a storage hierarchy. UniTree does not perform traditional full or incremental backups, but rather provides continuous backup <ref> [24] </ref>. Upon creation, files are stored in a magnetic disk cache. Within a short period, typically 3 to 30 minutes, UniTree copies newly-created files to one or more lower levels of the storage hierarchy. Thus, files are protected more quickly than in a daily incremental backup.
Reference: [25] <author> C. A. Thekkath, T. Mann, and E. K. Lee. Frangipani: </author> <title> A Scalable Distributed File System. </title> <booktitle> In Sixteenth ACM Symposium on Operating System Principles (SOSP-16), </booktitle> <month> October 5-8 </month> <year> 1997. </year>
Reference-contexts: When creating a snapshot, Petal pauses applications briefly (for less than one second). Snapshots may be kept online for quick access to previous versions of data. To create a virtual disk backup, Petal simply copies a snapshot to an archive device using a utility such as tar. Frangipani <ref> [25] </ref> is a distributed file system built on top of Petal virtual disks that uses the Petal snapshot facility to perform file system backups. 3.7 Amanda The Amanda Network Backup Manager [5] [6] was developed at the University of Mary-land at College Park to facilitate network backup of many UNIX workstations
Reference: [26] <author> DUMP(8). </author> <title> Unix System V man page. </title>
Reference-contexts: Blank spaces in the table indicate that information was not available. N/A indicates the category does not apply. 3.1 UNIX dump and rdump The most common UNIX backup program is dump <ref> [26] </ref>, which operates in several passes. dump maintains character arrays for directories and inodes.
Reference: [27] <author> TAR(1). </author> <title> Unix System V man page. </title>
Reference-contexts: The goal is to avoid over-running the end of a tape. Full backups (level 0 dumps) must be performed on a quiescent file system. In addition, dump offers a verification option that checks the correctness of each volume as it is dumped. 3.2 UNIX tar The UNIX tar <ref> [27] </ref> program is a file-based archival program. tar traverses the file system hierarchy in a single pass. It uses standard UNIX file system read calls to read a file sequentially and then copies it sequentially to the backup medium.
Reference: [28] <author> E. D. </author> <title> Zwicky. Torture-testing Backup and Archive Programs: Things You Ought to Know But Probably Would Rather Not. </title> <booktitle> In USENIX Proceedings of the 5th Conference on Large Installation Systems Administration, </booktitle> <pages> pages 181190, </pages> <month> September </month> <year> 1991. </year>
References-found: 28

