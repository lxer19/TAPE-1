URL: ftp://cag.lcs.mit.edu/pub/papers/isca95.ps.Z
Refering-URL: http://www.tns.lcs.mit.edu/~djw/history/alpha.html
Root-URL: 
Title: The MIT Alewife Machine: Architecture and Performance  
Author: Anant Agarwal, Ricardo Bianchini David Chaiken Kirk L. Johnson, David Kranz, John Kubiatowicz, Beng-Hong Lim Kenneth Mackenzie, and Donald Yeung 
Address: Cambridge, Massachusetts 02139  
Affiliation: Laboratory for Computer Science Massachusetts Institute of Technology  
Abstract: Alewife is a multiprocessor architecture that supports up to 512 processing nodes connected over a scalable and cost-effective mesh network at a constant cost per node. The MIT Alewife machine, a prototype implementation of the architecture, demonstrates that a parallel system can be both scalable and programmable. Four mechanisms combine to achieve these goals: software-extended coherent shared memory provides a global, linear address space; integrated message passing allows compiler and operating system designers to provide efficient communication and synchronization; support for fine-grain computation allows many processors to cooperate on small problem sizes; and latency tolerance mechanisms including block multithreading and prefetching mask unavoidable delays due to communication. Microbenchmarks, together with over a dozen complete applications running on the 32-node prototype, help to analyze the behavior of the system. Analysis shows that integrating message passing with shared memory enables a cost-efficient solution to the cache coherence problem and provides a rich set of programming primitives. Block multithreading and prefetching improve performance by up to 25% individually, and 35% together. Finally, language constructs that allow programmers to express fine-grain synchronization can improve performance by over a factor of two. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal, D. Kranz, and V. Natarajan. </author> <title> Automatic Partitioning of Parallel Loops for Cache-Coherent Multiprocessors. </title> <booktitle> In The 22nd International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: For parallel C, Alewife supports the p4 library from Argonne National Laboratory as well as parallel loops and distributed arrays. Automatic partitioning can be used when a program uses parallel loops and arrays <ref> [1] </ref>. Parallelism in Mul-T is specified with the future construct. Low thread creation overhead is achieved using lazy task creation [22], a method for dynamic partitioning and load balancing.
Reference: [2] <author> A. Agarwal, J. Kubiatowicz, D. Kranz, B.H. Lim, D. Yeung, G. D'Souza, and M. Parkin. Sparcle: </author> <title> An Evolutionary Processor Design for Multiprocessors. </title> <journal> IEEE Micro, </journal> <volume> 13(3) </volume> <pages> 48-61, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Coherent shared memory Although Alewife provides the abstraction of globally shared memory to programmers, the system's physical memory is statically distributed over the nodes in the machine. On each node, a Communications and Memory Manage 1 ment Unit (CMMU)[18] fields memory requests from a Sparcle processor <ref> [2] </ref> and determines whether requests access local or remote memory. When necessary, the CMMU synthesizes messages that fetch memory from remote nodes. The memory hardware helps manage locality by caching both private and shared data on each node. <p> Memory is physically distributed over the processing nodes, which use a mesh network for communication. Each Alewife node consists of a Sparcle <ref> [2] </ref> processor, 64K bytes of direct-mapped cache, 4M bytes of data and 2M bytes of directory (to support a 4M byte portion of shared memory), 2M bytes of private (unshared) memory, a floating-point coprocessor, and an Elko-series 2 mesh routing chip (EMRC) from Caltech.
Reference: [3] <author> A. Agarwal, B.H. Lim, D. Kranz, and J. Kubiatowicz. </author> <month> APRIL: </month> <title> A Processor Architecture for Multiprocessing. </title> <booktitle> In The 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 104-114, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Latency tolerance requires support from Alewife's hardware and software components. Prefetching and block multithreading both require lockup-free caches [15]. Prefetching requires support in the compiler and special memory operations. Block multithreading requires a fast context switch <ref> [3] </ref> and a solution to the window of vulnerability problem created by interleaved threads of execution [17]. Although it is helpful to think of Alewife's mechanisms as belonging to four distinct classes, the machine's implementation integrates them tightly.
Reference: [4] <author> G. Alverson, R. Alverson, and D. Callahan. </author> <title> Exploiting Heterogeneous Parallelism on a Multithreaded Multiprocessor. </title> <booktitle> In Workshop on Multi-threaded Computers, Proceedings of Supercomputing, </booktitle> <address> November1991. </address>
Reference-contexts: The *T [23] architecture uses a memory coprocessor model as well. A few architectures incorporate multiple contexts, pioneered by the HEP [29], switching on every instruction. These machines, including Monsoon [25] and Tera <ref> [4] </ref>, do not have caches and rely on a large number of contexts to hide remote memory latency. In contrast, Alewife's block multithreading technique switches only on synchronization faults and cache misses to remote memory, permitting good single-thread performance and requiring less aggressive hardware multithreading support.
Reference: [5] <institution> ANSI/IEEE Std 1596-1992 Scalable Coherent Interface, </institution> <year> 1992. </year>
Reference-contexts: It includes prefetching and a mechanism for depositing data directly in another processor's cache. The KSR1 and DDM [12] provide a shared address space through cache-only memory. These machines also allow prefetching. The Scalable Coherent Interface <ref> [5] </ref> also specifies mechanisms for implementing large, shared address spaces. Both the J-machine [24] and the CM-5 export hardware message-passing interfaces directly to the user. These interfaces differ from the Alewife interface in several ways.
Reference: [6] <author> Arvind, R. Nikhil, and K. Pingali. I-Structures: </author> <title> Data Structures for Parallel Computing. </title> <journal> ACM Transcations on Programming Languages and Systems, </journal> <volume> 11(4) </volume> <pages> 598-632, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: Patterned after I-structures <ref> [6] </ref>, J-structures support producer-consumer style synchronization on vector elements, with full/empty bits associated with each vector element. A J-structure read waits until the element is full before returning its value. A J-structure write updates the element and sets it to full.
Reference: [7] <author> D. Bailey et al. </author> <title> The NAS Parallel Benchmarks. </title> <type> Technical Report RNR-94-007, </type> <institution> NASA Ames Research Center, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: Table 7 summarizes the main characteristics of the applications evaluated on Alewife. The first five applications shown in the table are from the SPLASH suite [28], the three following ones are from the NAS parallel benchmarks <ref> [7] </ref>, the next four are engineering kernels, and the last solves a numerical problem.
Reference: [8] <author> D. Chaiken and A. Agarwal. </author> <title> Software-Extended Coherent Shared Memory: Performance and Cost. </title> <booktitle> In The 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 314-324, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: It presents the cost and the raw performance of each of the mechanisms in isolation. 3.1 Shared memory The Alewife machine provides hardware support for distributed, cache-coherent shared memory. Cache lines in Alewife are 16 bytes in size and are kept coherent through a software-extended scheme called LimitLESS <ref> [8, 9] </ref>. This scheme implements a full-map directory protocol by supporting up to five readers per memory line directly in hardware and by trapping into software for more widely-shared data. Consequently, LimitLESS involves a close interaction between hardware and software.
Reference: [9] <author> D. Chaiken, J. Kubiatowicz, and A. Agarwal. </author> <title> LimitLESS Directories: A Scalable Cache Coherence Scheme. </title> <booktitle> In The 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 224-234. </pages> <publisher> ACM, </publisher> <month> April </month> <year> 1991. </year>
Reference-contexts: When necessary, the CMMU synthesizes messages that fetch memory from remote nodes. The memory hardware helps manage locality by caching both private and shared data on each node. A scalable, software-extended scheme called LimitLESS <ref> [9] </ref> maintains the coherence of cached data. This scheme handles common-case memory accesses in the CMMU hardware, but relies on software traps to enforce coherence for memory blocks that are shared by a large number of processors. <p> It presents the cost and the raw performance of each of the mechanisms in isolation. 3.1 Shared memory The Alewife machine provides hardware support for distributed, cache-coherent shared memory. Cache lines in Alewife are 16 bytes in size and are kept coherent through a software-extended scheme called LimitLESS <ref> [8, 9] </ref>. This scheme implements a full-map directory protocol by supporting up to five readers per memory line directly in hardware and by trapping into software for more widely-shared data. Consequently, LimitLESS involves a close interaction between hardware and software.
Reference: [10] <author> F. Chong, S. Sharma, E. Brewer, and J. Saltz. </author> <title> Multiprocessor Runtime Support for Irregular DAGs. </title> <editor> In R. Kalia and P. Vashishta, editors, </editor> <title> Toward Teraflop Computing and New Grand Challenge Applications. </title> <publisher> Nova Science Publishers, Inc., </publisher> <year> 1995. </year>
Reference-contexts: It shows that for a sparse matrix application with irregular, fine-grain communication requirements, Alewife delivers performance that is comparable to the CM-5 under polling and superior under interrupts. The application is a power grid benchmark from a sparse matrix suite [11] which uses the techniques of <ref> [10] </ref>. of this application on Alewife and the CM-5, under both polling 2 and interrupts. Speedups are computed based on the running time (in cycles) of an optimized sequential code running on a single CM-5 node.
Reference: [11] <author> I. Duff, R. Grimes, and J. Lewis. </author> <title> User's Guide for the Harwell-Boeing Sparse Matrix Collection. </title> <type> Technical Report TR/PA/92/86, </type> <institution> CERFACS, </institution> <month> October </month> <year> 1992. </year>
Reference-contexts: It shows that for a sparse matrix application with irregular, fine-grain communication requirements, Alewife delivers performance that is comparable to the CM-5 under polling and superior under interrupts. The application is a power grid benchmark from a sparse matrix suite <ref> [11] </ref> which uses the techniques of [10]. of this application on Alewife and the CM-5, under both polling 2 and interrupts. Speedups are computed based on the running time (in cycles) of an optimized sequential code running on a single CM-5 node.
Reference: [12] <author> E. Hagersten, A. Landin, and S. Haridi. </author> <title> DDM A Cache-Only Memory Architecture. </title> <journal> IEEE Computer, </journal> <volume> 25(9) </volume> <pages> 44-54, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: DASH [20] is a cache-coherent multiprocessor that uses a full-map directory-based cache coherence protocol. It includes prefetching and a mechanism for depositing data directly in another processor's cache. The KSR1 and DDM <ref> [12] </ref> provide a shared address space through cache-only memory. These machines also allow prefetching. The Scalable Coherent Interface [5] also specifies mechanisms for implementing large, shared address spaces. Both the J-machine [24] and the CM-5 export hardware message-passing interfaces directly to the user.
Reference: [13] <author> D. Kranz, R. Halstead, and E. Mohr. Mul-T: </author> <title> A High-Performance Parallel Lisp. </title> <booktitle> In The Symposium on Programming Languages Design and Implementation, </booktitle> <month> June </month> <year> 1989. </year>
Reference-contexts: Programmers can then fine-tune performance by using the direct message-passing interface integrated with shared memory. Alewife has compilers for a parallel version of ANSI C and a parallel version of LISP called Mul-T <ref> [13] </ref>. For parallel C, Alewife supports the p4 library from Argonne National Laboratory as well as parallel loops and distributed arrays. Automatic partitioning can be used when a program uses parallel loops and arrays [1]. Parallelism in Mul-T is specified with the future construct.
Reference: [14] <author> D. Kranz, K. Johnson, A. Agarwal, J. Kubiatowicz, and B.H. Lim. </author> <title> Integrating Message-Passing and Shared-Memory; Early Experience. </title> <booktitle> In The 4th Annual Symposium on the Principles and Practice of Parallel Programming, </booktitle> <pages> pages 54-63, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Total chip resources: 100K gates and 100K bits of SRAM. as well as the message-passing interfaces share the same network queues, message passing and shared memory are integrated <ref> [14] </ref>. The Transaction Buffer is a 16-entry, fully-associative data store that tracks outstanding cache coherence transactions, holds prefetched data, and stages data in transit between the cache, network and memory. It is integrated closely with the mechanism for removing livelock in the face of block multithreading [17].
Reference: [15] <author> D. Kroft. </author> <title> Lockup-Free Instruction Fetch/Prefetch Cache Organization. </title> <booktitle> In The 8th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 81-87, </pages> <month> June </month> <year> 1981. </year>
Reference-contexts: Block multithreading allows a processor to switch between threads of computation on a cache miss or a failed synchronization attempt. Latency tolerance requires support from Alewife's hardware and software components. Prefetching and block multithreading both require lockup-free caches <ref> [15] </ref>. Prefetching requires support in the compiler and special memory operations. Block multithreading requires a fast context switch [3] and a solution to the window of vulnerability problem created by interleaved threads of execution [17].
Reference: [16] <author> J. Kubiatowicz and A. Agarwal. </author> <title> Anatomy of a Message in the Alewife Multiprocessor. </title> <booktitle> In The International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1993. </year>
Reference-contexts: To do so, the system provides forward progress guarantees to shared memory accesses in the face of message reception interrupts. In addition, the DMA engine maintains the coherence between the data in messages and the data in local caches <ref> [16] </ref>. Fine-grain computation Given a fixed-size data set, the granularity of computation (the time between events that require inter-processor communication) decreases as the number of processors in a system increases. <p> The Cache Management and Invalidation Control and Memory Coherence and DRAM Control blocks comprise, respectively, the processor and memory portions of the cache coherence protocol. In addition, both blocks service requests from the Network Interface and DMA Control block, which provides user-level message passing with locally coherent DMA <ref> [16] </ref>.
Reference: [17] <author> J. Kubiatowicz, D. Chaiken, and A. Agarwal. </author> <title> Closing the Window of Vulnerability in Multiphase Memory Transactions. </title> <booktitle> In The 5th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 274-284. </pages> <publisher> ACM, </publisher> <month> October </month> <year> 1992. </year>
Reference-contexts: Prefetching and block multithreading both require lockup-free caches [15]. Prefetching requires support in the compiler and special memory operations. Block multithreading requires a fast context switch [3] and a solution to the window of vulnerability problem created by interleaved threads of execution <ref> [17] </ref>. Although it is helpful to think of Alewife's mechanisms as belonging to four distinct classes, the machine's implementation integrates them tightly. For example, the CMMU's transaction buffer closes the window of vulnerability opened not only by multithreading, but also by fast message handling and software-extended coherence. <p> The Transaction Buffer is a 16-entry, fully-associative data store that tracks outstanding cache coherence transactions, holds prefetched data, and stages data in transit between the cache, network and memory. It is integrated closely with the mechanism for removing livelock in the face of block multithreading <ref> [17] </ref>. The Registers and Statistics block contains a dedicated cycle counter, a timer, and a number of statistics facilities. The Network Queues and Control block contains asynchronous interfaces for the EMRC network routers. technology from LSI Logic. Shaded blocks are standard-cell memories.
Reference: [18] <author> J. Kubiatowicz, D. Chaiken, A. Agarwal, A. Altman, J. Babb, D. Kranz, B.H. Lim, K. Mackenzie, J. Piscitello, and D. Yeung. </author> <title> The Alewife CMMU: Addressing the Multiprocessor Communications Gap. </title> <booktitle> In HOTCHIPS, </booktitle> <month> August </month> <year> 1994. </year>
Reference-contexts: These changes require an increase of fewer than 2000 gates over the unmodified SPARC design. Together, they yield a processor with support for low-overhead communication, including a 14-cycle context-switch time for a remote data cache miss. 2.2 The Alewife CMMU The Alewife CMMU <ref> [18] </ref> implements most of the unique functionality of Alewife. In an Alewife node, the CMMU is connected directly to the first-level cache bus and serves much the same functionality as a cache-controller/memory-management unit in a uniprocessor.
Reference: [19] <author> J. Kuskin, D. Ofelt, M. Heinrich, J. Heinlein, R. Simoni, K. Ghara-chorloo, J. Chapin, D. Nakahira, J. Baxter, M. Horowitz, A. Gupta, M. Rosenblum, and J. Hennessy. </author> <title> The Stanford FLASH Multiprocessor. </title> <booktitle> In The 21st Annual International Symposium on Computer Architecture 1994, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: Message passing in the T3D is flexible and includes extensive support for DMA. However, the T3D does not provide cache coherence. Several recently proposed architectures are based on the inte gration of shared memory and message passing in some form. FLASH <ref> [19] </ref> includes a microcoded, kernel-level coprocessor for message handling including shared-memory protocol messages. Bulk transfers in FLASH avoid using the receiving processor, but require pre-negotiating memory allocation. FLASH provides a multi-user environment. Typhoon [26] offers user-level message handling and cache coherence, using a second processor dedicated to the network interface.
Reference: [20] <author> D. Lenoski, J. Laudon, T. Joe, D. Nakahira, L. Stevens, A. Gupta, and J. Hennessy. </author> <title> The DASH Prototype: Logic Overhead and Performance. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(1) </volume> <pages> 41-61, </pages> <month> Jan </month> <year> 1993. </year>
Reference-contexts: In general, the degree to which full/empty bits are more important than hardware checking depends on register usage in the application code. 5 Related Work A number of other systems provide a shared address space entirely in hardware. DASH <ref> [20] </ref> is a cache-coherent multiprocessor that uses a full-map directory-based cache coherence protocol. It includes prefetching and a mechanism for depositing data directly in another processor's cache. The KSR1 and DDM [12] provide a shared address space through cache-only memory. These machines also allow prefetching.
Reference: [21] <author> K. Mackenzie, J. Kubiatowicz, A. Agarwal, and M. Frans Kaashoek. FUGU: </author> <title> Implementing Protection and Virtual Memory in a Multiuser, Multimodel Multiprocessor. </title> <note> Technical Memo MIT/LCS/TM-503, Oc-tober 1994. </note>
Reference-contexts: Our future work will investigate mechanisms for protection and virtual memory in multimodel multiprocessors. Implementing a virtual machine model in the face of streamlined user-level communication mechanisms is challenging, and forms the basis of the new FUGU architecture <ref> [21] </ref>. 11 7 Acknowledgments The following members of the Alewife team contributed significantly to the success of the project: Jonathan Babb, Rajeev Barua, Fred Chong, David Hoki, Ed Hurley, Gino Maa, Anne McCarthy, Sramana Mitra, Dan Nussbaum, and John Piscitello.
Reference: [22] <author> E. Mohr, D. Kranz, and R. Halstead. </author> <title> Lazy Task Creation: A Technique for Increasing the Granularity of Parallel Programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 264-280, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Automatic partitioning can be used when a program uses parallel loops and arrays [1]. Parallelism in Mul-T is specified with the future construct. Low thread creation overhead is achieved using lazy task creation <ref> [22] </ref>, a method for dynamic partitioning and load balancing. The Alewife runtime system includes a parallel stop-and-copy garbage collector. 2.4 Alewife debugging and tuning Alewife provides a number of facilities to aid in program debugging and performance tuning.
Reference: [23] <author> R. Nikhil, G. Papadopoulos, and Arvind. </author> <title> *T: A Multithreaded Massively Parallel Architecture. </title> <booktitle> In The 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 156-167, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Bulk transfers in FLASH avoid using the receiving processor, but require pre-negotiating memory allocation. FLASH provides a multi-user environment. Typhoon [26] offers user-level message handling and cache coherence, using a second processor dedicated to the network interface. The *T <ref> [23] </ref> architecture uses a memory coprocessor model as well. A few architectures incorporate multiple contexts, pioneered by the HEP [29], switching on every instruction. These machines, including Monsoon [25] and Tera [4], do not have caches and rely on a large number of contexts to hide remote memory latency.
Reference: [24] <author> M. Noakes, D. Wallach, and W. Dally. </author> <title> The J-Machine Multicomputer: An Architectural Evaluation. </title> <booktitle> In The 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 224-235, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The CMMU overlaps message arrival with interrupt processing by posting the interrupt as soon as it has received the header of a message. Since the operating system reserves one of the four Sparcle hardware contexts for message processing (as in <ref> [24, 27] </ref>), no register saves or restores are necessary. The first 16 words of an incoming message are presented in the memory-mapped input packet array. <p> It includes prefetching and a mechanism for depositing data directly in another processor's cache. The KSR1 and DDM [12] provide a shared address space through cache-only memory. These machines also allow prefetching. The Scalable Coherent Interface [5] also specifies mechanisms for implementing large, shared address spaces. Both the J-machine <ref> [24] </ref> and the CM-5 export hardware message-passing interfaces directly to the user. These interfaces differ from the Alewife interface in several ways. First, in Alewife, messages are normally delivered via an interrupt and dispatched in software, while in the J-machine, messages are queued and dispatched in sequence by the hardware.
Reference: [25] <author> G. Papadopoulosand D. Culler. Monsoon: </author> <title> An Explicit Token-Store Architecture. </title> <booktitle> In The 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 82-91, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Typhoon [26] offers user-level message handling and cache coherence, using a second processor dedicated to the network interface. The *T [23] architecture uses a memory coprocessor model as well. A few architectures incorporate multiple contexts, pioneered by the HEP [29], switching on every instruction. These machines, including Monsoon <ref> [25] </ref> and Tera [4], do not have caches and rely on a large number of contexts to hide remote memory latency. In contrast, Alewife's block multithreading technique switches only on synchronization faults and cache misses to remote memory, permitting good single-thread performance and requiring less aggressive hardware multithreading support.
Reference: [26] <author> S. Reinhardt, J. Larus, and D. Wood. Tempest and Typhoon: </author> <title> User-Level Shared Memory. </title> <booktitle> In The 21st Annual International Symposium on Computer Architecture, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: FLASH [19] includes a microcoded, kernel-level coprocessor for message handling including shared-memory protocol messages. Bulk transfers in FLASH avoid using the receiving processor, but require pre-negotiating memory allocation. FLASH provides a multi-user environment. Typhoon <ref> [26] </ref> offers user-level message handling and cache coherence, using a second processor dedicated to the network interface. The *T [23] architecture uses a memory coprocessor model as well. A few architectures incorporate multiple contexts, pioneered by the HEP [29], switching on every instruction.
Reference: [27] <author> C. Seitz, N. Boden, J. Seizovic, and W.K. Su. </author> <title> The Design of the Caltech Mosaic C Multicomputer. </title> <booktitle> In Research on Integrated Systems Symposium Proceedings, </booktitle> <pages> pages 1-22, </pages> <address> Cambridge, MA, 1993. </address> <publisher> MIT Press. </publisher>
Reference-contexts: The CMMU overlaps message arrival with interrupt processing by posting the interrupt as soon as it has received the header of a message. Since the operating system reserves one of the four Sparcle hardware contexts for message processing (as in <ref> [24, 27] </ref>), no register saves or restores are necessary. The first 16 words of an incoming message are presented in the memory-mapped input packet array.
Reference: [28] <author> J. P. Singh, W.-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Table 7 summarizes the main characteristics of the applications evaluated on Alewife. The first five applications shown in the table are from the SPLASH suite <ref> [28] </ref>, the three following ones are from the NAS parallel benchmarks [7], the next four are engineering kernels, and the last solves a numerical problem.
Reference: [29] <author> B. J. Smith. </author> <title> Architecture and Applications of the HEP Multiprocessor Computer System. </title> <journal> Society of Photo-optical Instrumentation Engineers, </journal> <volume> 298 </volume> <pages> 241-248, </pages> <year> 1981. </year>
Reference-contexts: By synchronizing on exactly the data words to be consumed, fine-grain synchronization eliminates false dependencies and allows a thread to proceed as soon as the data it needs is available. The Alewife machine provides both hardware and software support for fine-grain synchronization. Hardware support consists of a full/empty bit <ref> [29] </ref> for each 32-bit data word. To access these bits, colored load and store instructions are provided that perform full/empty test-and-set operations. Table 5 presents a sample of Alewife data-access instructions. All of these instructions return the original full/empty bit in the coprocessor status word. <p> FLASH provides a multi-user environment. Typhoon [26] offers user-level message handling and cache coherence, using a second processor dedicated to the network interface. The *T [23] architecture uses a memory coprocessor model as well. A few architectures incorporate multiple contexts, pioneered by the HEP <ref> [29] </ref>, switching on every instruction. These machines, including Monsoon [25] and Tera [4], do not have caches and rely on a large number of contexts to hide remote memory latency.
Reference: [30] <author> T. von Eicken, D. Culler, S. Goldstein, and K. Schauser. </author> <title> Active messages: A mechanism for integrated communication and computation. </title> <booktitle> In The 19th Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: Some communication operations, such as file I/O, remote task dispatch, and the inner loops of typical scientific codes, can often be implemented more efficiently with message passing than with shared memory. Further, since Alewife provides a protected user-level message-passing interface, compilation targets such as active messages <ref> [30] </ref> are naturally supported. 5 stio header, $ipiout0 stio dataword, $ipiout1 stio address, $ipiout2 stio length, $ipiout3 ipilaunch 2, 1 a message send. In addition to the required header, this message includes one explicit data word, and one block of data from memory. data types. (b) message passing, random destinations.
Reference: [31] <author> D. Yeung and A. Agarwal. </author> <title> Experience with Fine-Grain Synchronization in MIMD Machines for Preconditioned Conjugate Gradient. </title> <booktitle> In The 4th Annual Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 187-197, </pages> <month> May </month> <year> 1993. </year> <month> 12 </month>
Reference-contexts: The second eliminates the need for the synchronization array by using full/empty bits as provided in Alewife. Checking the bit at each reference is still done in software. The last implementation uses the full capability of Alewife's full/empty bits, checking synchronization state in hardware. (For additional information, see <ref> [31] </ref>). and are not visible on the graph. Synchronization overhead in the coarse-grain implementation is the time spent waiting in barriers; in the fine-grain implementations, it is the time spent both in J-structure operations and in waiting for pending J-structure values.
References-found: 31

