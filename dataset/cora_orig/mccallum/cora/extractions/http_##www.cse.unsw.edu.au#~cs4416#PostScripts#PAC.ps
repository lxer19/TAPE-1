URL: http://www.cse.unsw.edu.au/~cs4416/PostScripts/PAC.ps
Refering-URL: http://www.cse.unsw.edu.au/subjects/cs4416/
Root-URL: http://www.cse.unsw.edu.au
Title: Lecture Notes to COMP9417: Machine Learning  
Author: Dr. Achim Hoffmann 
Note: Contents  
Date: October 9, 1997  
Address: Sydney, Australia  
Affiliation: School of Computer Science and Engineering University of New South Wales,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> D. Angluin and P. Laird. </author> <title> Learning from noisy examples. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 343-370, </pages> <year> 1988. </year>
Reference-contexts: That is the number of false predictions of the classification of a newly presented object throughout the online learning process. The treatment of learning in the presence of noise in the learning data has been analysed in the PAC framework, e.g. by Angluin & Laird <ref> [1] </ref>. They assume that the learning system chooses a hypothesis which is in minimal conflict with the presented data.
Reference: [2] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. Warmuth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the ACM, </journal> <volume> 36(4) </volume> <pages> 929-965, </pages> <year> 1989. </year>
Reference: [3] <author> A. Ehrenfeucht, D. Haussler, M. Kearns, and L. Valiant. </author> <title> A general lower bound on the number of examples needed for learning. </title> <journal> Information and Computation, </journal> <volume> 82 </volume> <pages> 247-261, </pages> <year> 1989. </year>
Reference: [4] <author> E. Gold. </author> <title> Language indentification in the limit. </title> <journal> Information and Control, </journal> <volume> 10 </volume> <pages> 447-474, </pages> <year> 1967. </year> <month> October 9, </month> <year> 1997 </year> <month> 17 </month>
Reference-contexts: Can we think of learning problems for which no algorithm can be designed to learn properly ? To address this kind of questions, E.M. Gold introduced in 1967 <ref> [4] </ref> the notion of Learning in the limit. See e.g. [10] for a survey. 1.2 Quality of Learning Results In the following, we are not that much interested, whether it is possible to learn eventually (in the limit) but we are rather interested to learn within a prespecified frame.
Reference: [5] <author> D. Haussler. </author> <title> Quantifying inductive bias: AI learning algorithms and Valiant's learning framework. </title> <journal> Artificial Intelligence, </journal> <volume> 36 </volume> <pages> 177-221, </pages> <year> 1988. </year> <note> (Reprinted in Readings in Machine Learning, </note> <editor> Shavlik & Dietterich (eds.), </editor> <publisher> Kaufmann Publishers.). </publisher>
Reference-contexts: the almost minimal number of atoms. 3.3 Computational Complexity Results Theorem 6 (<ref> [5] </ref>) Given a sample on f0; 1g n attributes that is consistent with some pure conjuntive concept. Then it is NP-hard to find a consistent pure conjunctive hypothesis with a minimal number of atoms. For a proof, see [5]. The proof uses a polynomial-time reduction from set cover. Fortunately, for the set cover problem a fast approximation algorithm is known. This algorithm can be adopted to be used for learning a pure conjunctive hypothesis with approximating the minimal number of atoms.
Reference: [6] <author> D. S. Johnson. </author> <title> Approximation algorithms for combinatorial problems. </title> <journal> Journal of Computer System Sciences, </journal> <volume> 9 </volume> <pages> 256-278, </pages> <year> 1974. </year>
Reference: [7] <author> N. Littlestone. </author> <title> Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 285-318, </pages> <year> 1988. </year>
Reference-contexts: In general, it is also an analysis of `batch learning' as opposed to `online-learning'. Online-learning reads permanently new examples and adapts its hypothesis upon the new information. N. Littlestone <ref> [7] </ref> proposes a learning model for that setting which measures the quality of the learning performance as the mistake-bound. That is the number of false predictions of the classification of a newly presented object throughout the online learning process.
Reference: [8] <author> R. S. Michalski. </author> <title> A theory and methodology of machine learning. </title> <journal> Artificial Intelligence, </journal> <volume> 20 </volume> <pages> 111-161, </pages> <year> 1983. </year>
Reference-contexts: Considering the concept learning algorithms like AQ (see e.g. <ref> [8] </ref>) or decision tree learners like C4.5 [11] one may wonder, whether the learned concepts can be trusted. <p> Instead objects may be represented by linear or tree-structured attributes. In <ref> [8] </ref> a learning algorithm, AQ, is presented which copes with object descriptions using attributes of the following kinds: * Linear attributes, see figure 7. A linear attribute is an attribute where an example has a particular value within a range of linearly ordered values.
Reference: [9] <author> T. M. Mitchell. </author> <title> Generalization as search. </title> <journal> Artificial Intelligence, </journal> (18):203-226, 1982. 
Reference-contexts: After the formal notions of PAC-learnability have been defined, the relation to practical learning problems will be elaborated in the following. 3 The Version Space The idea of the version space <ref> [9] </ref> is to maintain efficiently a set of hypotheses which are consistent with the provided learning data. To learn a concept properly, the version space October 9, 1997 9 is supposed to be exhausted through eliminating all but one hypothesis from the version space by respectively contradicting learning data.
Reference: [10] <author> D. N. Osherson, M. Stob, and S. Weinstein. </author> <title> Systems that Learn. </title> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: Can we think of learning problems for which no algorithm can be designed to learn properly ? To address this kind of questions, E.M. Gold introduced in 1967 [4] the notion of Learning in the limit. See e.g. <ref> [10] </ref> for a survey. 1.2 Quality of Learning Results In the following, we are not that much interested, whether it is possible to learn eventually (in the limit) but we are rather interested to learn within a prespecified frame.
Reference: [11] <author> J. R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1993. </year>
Reference-contexts: Considering the concept learning algorithms like AQ (see e.g. [8]) or decision tree learners like C4.5 <ref> [11] </ref> one may wonder, whether the learned concepts can be trusted.
Reference: [12] <author> H. Rogers Jr. </author> <title> Theory of Recursive Functions and Effective Computability. </title> <publisher> McGraw-Hill, </publisher> <year> 1967. </year>
Reference-contexts: While the first aspect is probably the simplest way to define approximation, it is probably also the least useful one. On the other hand, the last aspect, which requires 1 Recursion Theory deals, among other issues, with that kind of questions. See e.g. Rogers <ref> [12] </ref> for a comprehensive introduction to recursion theory. October 9, 1997 4 that the most important objects will be correctly classified is probably the most useful one. Unfortunately, it is simultaneously also to most difficult one to be defined formally: No formal definition of importance is known.
Reference: [13] <author> L. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27 </volume> <pages> 1134-1142, </pages> <year> 1984. </year>
Reference-contexts: It is hardly possible to list all conceivable ways of a learner's interaction with its environment. The term learning is too ill-defined to allow an exhaustive definition. 1.4 Valiant's Framework (1984) This subsection introduces the framework proposed by L. Valiant in 1984 <ref> [13] </ref> for investigating the computational complexity of machine learning.
References-found: 13

