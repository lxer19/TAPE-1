URL: http://www.sls.lcs.mit.edu/flammia/flammia97.ps
Refering-URL: http://www.sls.lcs.mit.edu/flammia/publications.html
Root-URL: 
Email: fflammia,zueg@sls.lcs.mit.edu  
Title: LEARNING THE STRUCTURE OF MIXED INITIATIVE DIALOGUES USING A CORPUS OF ANNOTATED CONVERSATIONS 1  
Author: Giovanni Flammia and Victor Zue 
Address: Cambridge, Massachusetts 02139 USA  
Affiliation: Spoken Language Systems Group Laboratory for Computer Science Massachusetts Institute of Technology  
Abstract: This paper reports an ongoing effort to derive linear discourse structures from a corpus of telephone conversations. First, we would like to determine how reliably human annotators can tag discourse segments in dialogues. Second, we begin to investigate how to build machine models for performing this annotation task. To carry out our research, we use a corpus of transcribed and annotated human-human dialogues in a specific information retrieval domain (Movie theater schedules). We conducted an experiment in which 25 different dialogues have each been annotated by at least seven different people. We found that the average precision and recall among annotators in placing segment boundaries is 84.3%, and in assigning segment purpose labels is 80.1%. A simple discourse segment parser based on finite state machines is able to cover 56% of the same dialogues. When the finite state grammar is able to analyse a dialogue, it agrees with human annotators in placing segment boundaries with 59.4% precision and 66.4% recall, and it agrees in segment label accuracy at the 59% level. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Bernsen, N. O., Dybkjaer, L. and Dybkjaer, H. </author> <title> Cooper-ativity in human-machine and human-human spoken dialogue. </title> <booktitle> Discourse Processes Vol. </booktitle> <volume> 21. No. 2. </volume> <year> 1996. </year> <pages> pp. 213-236. </pages>
Reference-contexts: Some of these variabilities satisfy communication needs and may not contribute directly to goal-directed problem solving. However, we believe, as do others, that studying human-human dialogue and comparing it to human-machine dialogue can provide valuable insights <ref> [1] </ref>. To carry out our research, we need to first obtain properly annotated language resources.
Reference: [2] <author> Flammia G. and Zue V. </author> <title> Empirical Evaluation of Human Performance and Agreement in Parsing Discourse Constituents in Spoken Dialogue. </title> <booktitle> Proc. </booktitle> <address> Eurospeech-95. </address> <year> 1995. </year> <pages> pp. 1965-1968. </pages> <address> http://www.sls.lcs.mit.edu/flammia/Nb.html </address>
Reference-contexts: We have previously reported the development of a dialogue annotation tool called Nb which has been used extensively for dialogue annotation in our group and several other institutions. Nb is freely available for Unix and Windows <ref> [2] </ref>. The research reported in this paper is a continuation of our previous work. Specifically, we seek answers to two questions. <p> While they use different measures for computing agreement, they found that different annotators may segment discourse at different level of granularity, and that discourse segmentation is more reliable when performed by listening to the acoustic signal as well as reading the text transcription. Our initial annotation experiment, cited in <ref> [2] </ref>, reported a best case 60% pairwise agreement among annotators in placing segment boundaries.
Reference: [3] <author> Grosz B., Sidner C., </author> <title> Attentions, Intentions and the Structure Of Discourse. </title> <journal> Computational Linguistics, </journal> <volume> Vol. 12. No. 3, </volume> <year> 1986. </year> <pages> pp. 175-204. </pages>
Reference-contexts: In the instructions, we assumed that a conversation can be decomposed sequentially by purpose <ref> [3] </ref>, that is, a conversation can be modeled by a sequence of one or more segments, each segment having the role to fulfill one specific purpose.
Reference: [4] <author> Hearst M. </author> <title> Context and Structure in Automated Full-Text Information Access TR. </title> <institution> UCB/CSD-94/836. University Of California, Berkeley, </institution> <address> CA. </address> <year> 1994. </year>
Reference-contexts: They found that human annotators are reliable when tagging simple discourse segments that refer to the same flight, although they do not report precision/recall results on boundary placements. Hearst reports a study in multi-paragraph discourse segmentation of professionally written narrative text <ref> [4] </ref>. She found that human annotators agree with a reference segmentation with 81% precision and 71% recall. She also reports a machine performance of 66% precision and 61% recall. Litman and Passoneau studied extensively discourse segmentation of spontaneous spoken narrative by nonprofessional speakers [6]. <p> Finally, the model must include a holistic measure of new versus given information. This feature is currently missing from our model, and can be measured using topic identification and information retrieval techniques applied to entire discourse segments rather than to individual dialogue turns <ref> [4, 7] </ref>.
Reference: [5] <author> Hirschberg J., Nakatani C., and Grosz B. </author> <title> Conveying Discourse Structure Through Intonation Variation. </title> <booktitle> ESCA Workshop on Spoken Dialogue. </booktitle> <year> 1995. </year> <pages> pp. 189-192. </pages>
Reference-contexts: Litman and Passoneau studied extensively discourse segmentation of spontaneous spoken narrative by nonprofessional speakers [6]. They report human agreement with a reference segmentation of 72% precision and 63% recall, and machine performance of 63% precision and 46% recall. Hirschberg, Nakatani, and Grosz <ref> [5] </ref> analyzed hierarchical discourse segmentation of spontaneous task-oriented speech monologues by non-professional speakers.
Reference: [6] <author> Litman D.J. and Passonneau R.J. </author> <title> Combining Multiple Knowledge Sources For Discourse Segmentation. </title> <booktitle> ACL Proceedings. </booktitle> <year> 1995. </year> <pages> pp. 108-115. </pages>
Reference-contexts: She found that human annotators agree with a reference segmentation with 81% precision and 71% recall. She also reports a machine performance of 66% precision and 61% recall. Litman and Passoneau studied extensively discourse segmentation of spontaneous spoken narrative by nonprofessional speakers <ref> [6] </ref>. They report human agreement with a reference segmentation of 72% precision and 63% recall, and machine performance of 63% precision and 46% recall. Hirschberg, Nakatani, and Grosz [5] analyzed hierarchical discourse segmentation of spontaneous task-oriented speech monologues by non-professional speakers.
Reference: [7] <author> Swerts M. and Ostendorf M. </author> <title> Discourse prosody in human-machine interactions. </title> <booktitle> ESCA Workshop on Spoken Dialogue. </booktitle> <year> 1995. </year> <pages> pp. 205-208. </pages>
Reference-contexts: The reliability of this task depends on the linguistic variability of the corpus and on the level of detail of the annotation. Swerts and Ostendorf studied highly structured question-and-answer human-machine dialogues from the ATIS airline reservation task <ref> [7] </ref>. They found that human annotators are reliable when tagging simple discourse segments that refer to the same flight, although they do not report precision/recall results on boundary placements. Hearst reports a study in multi-paragraph discourse segmentation of professionally written narrative text [4]. <p> Finally, the model must include a holistic measure of new versus given information. This feature is currently missing from our model, and can be measured using topic identification and information retrieval techniques applied to entire discourse segments rather than to individual dialogue turns <ref> [4, 7] </ref>.
References-found: 7

