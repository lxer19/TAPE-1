URL: http://compiler.lcs.mit.edu/~saman/papers/thesis.ps
Refering-URL: http://cag-www.lcs.mit.edu/~saman/cv.html
Root-URL: 
Title: PARALLELIZING COMPILER TECHNIQUES BASED ON LINEAR INEQUALITIES  
Author: Saman Prabhath Amarasinghe 
Degree: A DISSERTATION SUBMITTED TO THE DEPARTMENT OF ELECTRICAL ENGINEERING AND THE COMMITTEE ON GRADUATE STUDIES OF STANFORD UNIVERSITY IN PARTIAL FULFILLMENT OF THE REQUIREMENTS FOR THE DEGREE OF DOCTOR OF PHILOSOPHY  
Date: January 1997  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> A. Agarwal, D. Chaiken, G. DSouza, K. Johnson, and D. Kranz et. al. </author> <title> The MIT Alewife machine: A large-scale distributed memory multiprocessor. In Scalable Shared Memory Multiprocessors. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year>
Reference-contexts: This is true for bus-based shared address space machines [50,51], and even more so for scalable shared address space machines [26] such as the Stanford DASH [105] and FLASH multiprocessors [101], MIT ALEWIFE <ref> [1] </ref>, Kendall Squares KSR-1 [94], the Convex Exemplar [115], and the Silicon Graphics Origin. The memory on remote processors in these architectures constitutes yet another level in the memory hierarchy. The differences in access times among cache, local, and remote memory can be very large.
Reference: [2] <author> A. Agarwal, D. Kranz, and V. Natarajan. </author> <title> Automatic paritioning of parallel loops for cache-coherent multiprocessors. </title> <booktitle> In Proceedings of the 1993 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1993. </year>
Reference: [3] <author> A. V. Aho, R. Sethi, and J. D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <note> second edition, </note> <year> 1986. </year>
Reference-contexts: We also use simple extensions to standard compiler techniques such as loop invariant removal and induction variable recognition to move some of the division and modulo operators out of inner loops <ref> [3] </ref>. The optimizations, described below, have proved to be important and effective in practice. DIMENSION A (0:(N+nproc-1)/nproc, N, 0:nproc) DO J = 2, 99 A ((I-1)/nproc+1, J, mod (I-1,nproc)+1) = ... ENDDO ENDDO P d - d b d d b 1+ b max 1 155 8.3.1. <p> Value-Centric Approach Instead of location-based data dependence analysis, communication identification can be based on less restrictive value-based data-flow information. Let us use the simple example in Figure 9-2 to illustrate the difference. Data dependence analysis on this program will produce the dependence vectors -[+, 3], <ref> [0, 3] </ref>-, meaning that the read access in iteration may be data dependent on all iterations such that , and .
Reference: [4] <author> J. R. Allen. </author> <title> Unifying vectorization, parallelization, and optimization: The Ardent compiler. </title> <editor> In L. Kartashev and S. Kartashev, editors, </editor> <booktitle> Proceedings of the Third International Conference on Supercomputing, </booktitle> <year> 1988. </year>
Reference: [5] <author> E. R. Altman, R. Govindarajan, and G. R. Gao. </author> <title> Scheduling and mapping: Software pipelining in the presence of structural hazards. </title> <booktitle> In Proceedings of ACM SIGPLAN Conference on Programming Language Design and Implementation 95, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: Related Work Researchers have used integer and linear programming techniques to solve many individual problems in parallelizing compilers. For example, compiler problems such as exact data-dependence analysis [110,119], array analysis based on array summary information [137], instruction scheduling for superscalars <ref> [5] </ref>, automatic data layout for minimizing communication [21], and code generation after loop transformations [10,11] have been solved using linear and integer programming.
Reference: [6] <author> S. P. Amarasinghe, J. M. Anderson, M. S. Lam, and A. W. Lim. </author> <title> An overview of a compiler for scalable parallel machines. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference: [7] <author> S. P. Amarasinghe, J. M. Anderson, M. S Lam, and C.-W. Tseng. </author> <title> An overview of the SUIF compiler for scalable parallel machines. </title> <booktitle> In Proceedings of the Seventh SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 662667, </pages> <address> San Francisco, CA, </address> <month> February </month> <year> 1995. </year>
Reference: [8] <author> S. P. Amarasinghe, J. M. Anderson, C. S. Wilson, S.-W. Liao, R. S. French, M. W. Hall, B. R. Murphy, and M. S. Lam. </author> <title> Multiprocessors from a software perspective. </title> <journal> IEEE Micro, </journal> <volume> 16(3):5261, </volume> <month> June </month> <year> 1996. </year>
Reference-contexts: Thus, the adoption of parallel computing has been much slower than that anticipated 30 years ago [59]. Recent developments in compiler technologies have the potential to deliver the much anticipated breakthrough in parallel computing <ref> [8] </ref>. Compilers have played a critical role in two recent major breakthroughs in performance for general purpose computing: reduced instruction set computers (RISC) and instruction level parallelism (ILP) in microprocessors.
Reference: [9] <author> S. P. Amarasinghe and M. S. Lam. </author> <title> Communication optimization and code generation for distributed memory machines. </title> <booktitle> In Proceedings of the SIGPLAN 93 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 126138, </pages> <address> Albuquerque, NM, </address> <month> June </month> <year> 1993. </year> <month> 208 </month>
Reference-contexts: We present related work in Section 2.5. 2.1. Systems of Linear Inequalities We use a unified framework based on linear inequalities to handle multi-dimensional integer spaces such as iteration, data and processor spaces that are used in analyses and optimization techniques for next-generation compilers <ref> [9] </ref>. We represent all possible values of a set of integer variables as an n-dimensional discrete cartesian space, where the k-th axis corresponds to variable . Coordinate corresponds to the value . <p> Code Generation As a simple example of how linear inequalities framework can be used in compilers, we present a code generation algorithm based on linear inequalities <ref> [9] </ref>. Ancourt and Irigoin presented an algorithm for generation of loop nests after loop transformation by a series of projections of the transformed iteration space [10,11]. In the following, we briefly describe their algorithm, and our heuristics for finding tight and efficient loop bounds. 2.2.1. <p> We have developed a set of heuristics to simplify the system of inequalities, and to pick the order for eliminating the constraints so that the loop bounds generated are simple and efficient <ref> [9] </ref>. The outline of the algorithm is given in Figure 2-8. First, we simplify the system of inequalities. Then we attempt to eliminate the constraints in the given order. To check if a constraint is redundant, we replace the constraint in question with its negation. <p> Otherwise, bound expressions with only outer or no loop index variables receive higher weights since loop invariant bound expressions can be moved out of the inner loops. 2.3. Linear Inequalities with Symbolic Coefficients We have extended Fourier-Motzkin elimination to handle simple non-linear systems <ref> [9] </ref>. The variables of the linear inequalities can have a restricted form of symbolic coefficients. Definition 2-4: A linear inequality with symbolic coefficients is of the form where are integer variables, are symbolic constants and , are integers.
Reference: [10] <author> C. Ancourt and F. Irigoin. </author> <title> Scanning polyhedra with do loops. </title> <booktitle> In Proceedings of the Third ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 3950, </pages> <address> Williamsburg, VA, </address> <month> April </month> <year> 1991. </year>
Reference: [11] <author> M. </author> <type> Ancourt. </type> <institution> Gnration Automatique de Codes de Transfert pour Multiprocesseurs Mmoires Locales. </institution> <type> PhD thesis, </type> <institution> Universit Paris VI, </institution> <month> March </month> <year> 1991. </year>
Reference-contexts: It is essential that we eliminate the redundant messages and amortize the message sending overhead by batching the communication. 9.5.1. Eliminating Redundant Communication Ancourt has also studied the problem of eliminating redundant communication <ref> [11] </ref>. Given a set of iterations and accesses, Ancourts algorithm can construct a set of loop nests that fetches all the data touched without any duplication. This algorithm is adequate for removing redundant traffic if no communication is required within the loop nest.
Reference: [12] <author> J. M. Anderson, S. P. Amarasinghe, and M. S. Lam. </author> <title> Data and computation transformations for multiprocessors. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 166178, </pages> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference: [13] <author> J. M. Anderson and M. S. Lam. </author> <title> Global optimizations for parallelism and locality on scalable parallel machines. </title> <booktitle> In Proceedings of the SIGPLAN 93 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 112125, </pages> <address> Albuquerque, NM, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: We have developed an interprocedural parallelizer with advanced array analyses and optimizations, that is capable of detecting coarse-grain parallelism [75,76,71]. The parallelizer is integrated as a part of the SUIF compiler system [144]. Other advanced optimizations such as loop transformations [146], data and computation co-location <ref> [13] </ref>, data transformations (Chapter 8), synchronization elimination [140], compiler-directed page coloring [30], and compiler-inserted prefetching [116] have also been implemented in the SUIF compiler system. Detection of coarse-grain parallelism, in combination with these other optimizations, can achieve significant performance improvements for sequential scientific applications on multiprocessors. <p> time to expose outermost loop parallelism and to improve data locality among the accesses within the loop [147,148]. b) CompDecomp: We first applied the basic parallelizer to analyze the individual loops, then applied a compiler algorithm to find the computation and the corresponding data decompositions that minimize communication across processors <ref> [13] </ref>. <p> The code generator also takes advantage of this information to minimize synchronization in the parallel program [140]. The data layouts are left unchanged. c) DataTrans: Finally, we include the data transformations described in this chapter. Using the data decompositions calculated by the communication minimization algorithm <ref> [13] </ref>, the compiler reorganizes the arrays in the parallelized code to improve spatial locality. After transforming the array accesses, the access functions are simpli fied using modulo and division optimizations described in Section 8.3. 8.4.2. <p> The computation decompositions can be either generated automatically by an earlier compiler phase <ref> [13] </ref> or manually by the user. Theorem 9-1 shows how to derive computation decompositions from user-specified data decompositions. Theorem 9-1: Assuming that written data are not replicated, the computation decomposition as derived from data decomposition , using the owner-computes rule, is 9.3.
Reference: [14] <author> F. Andre, O. Cheron, and J.-L. Pazat. </author> <title> Compiling sequential programs for distributed memory parallel computers with Pandore II. </title> <booktitle> In Proceedings of the Third Workshop on Compilers for Parallel Computers, </booktitle> <pages> pages 213242, </pages> <address> Vienna, Austria, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Related Work There is a large body of research on language extensions and compiler support for distributed memory machines. Some notable projects are, Al [141], Blaze [100], Crystal [106], FORTRAN-D [85,139], Id Nouveau [123], Kali [114,98], Pandore [15], Pandore II <ref> [14] </ref>, p s i s 1 p r i s k i r a, , , , , , , , , i s k 1 i s n i r i r k 1 i r n i r n p s i s 1 p r a i s
Reference: [15] <author> F. Andre, J. Pazat, and H. Thomas. </author> <title> Pandore: A system to manage data distribution. </title> <editor> In J. Saltz and P. Mehrotra, editors, </editor> <title> Languages, Compilers, and Run-Time Environments for Distributed Memory Machines. </title> <publisher> North-Holland, </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1992. </year>
Reference-contexts: Related Work There is a large body of research on language extensions and compiler support for distributed memory machines. Some notable projects are, Al [141], Blaze [100], Crystal [106], FORTRAN-D [85,139], Id Nouveau [123], Kali [114,98], Pandore <ref> [15] </ref>, Pandore II [14], p s i s 1 p r i s k i r a, , , , , , , , , i s k 1 i s n i r i r k 1 i r n i r n p s i s 1 p r
Reference: [16] <author> B. Appelbe and B. Lakshmanan. </author> <title> Optimizing parallel programs using affinity regions. </title> <booktitle> In Proceedings of the 1993 International Conference on Parallel Processing, </booktitle> <pages> pages 246249, </pages> <address> St. Charles, IL, </address> <month> August </month> <year> 1993. </year>
Reference: [17] <institution> Applied Parallel Research, Placerville, CA. </institution> <note> Forge 90 Distributed Memory Parallelizer: Users Guide, version 8.0 edition, </note> <year> 1992. </year>
Reference: [18] <author> V. Balasundaram and K. Kennedy. </author> <title> A technique for summarizing data access and its use in parallelism enhancing transformations. </title> <booktitle> In Proceedings of the SIGPLAN 89 Conference on Programming Language Design and Implementation, </booktitle> <address> Portland, OR, </address> <month> June </month> <year> 1989. </year>
Reference: [19] <author> U. Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, MA, </address> <year> 1988. </year>
Reference-contexts: Third, we have proposed a value-centric approach to deriving the fine-grain communication for machines with a distributed address space. Previous approaches are location-centric: communication is derived from data decompositions; optimizations are performed using data dependence tests <ref> [19] </ref>, an analysis that determines if accesses may refer to the same location. In this approach, code generation is performed from computation decompositions using a data-flow analysis technique that is based on values instead of locations.
Reference: [20] <author> U. Banerjee, R. Eigenmann, A. Nicolau, and D. Padua. </author> <title> Automatic program parallelization. </title> <booktitle> Proceedings of the IEEE, </booktitle> <address> 81(2):211243, </address> <month> February </month> <year> 1993. </year>
Reference: [21] <author> B. Bixby, K. Kennedy, and U. Kremer. </author> <title> Automatic data layout using 0-1 integer programming. </title> <booktitle> In Proceedings of the International Conference on Parallel Architectures and Compilation Techniques (PACT), </booktitle> <pages> pages 111122, </pages> <address> Montreal, Canada, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: Related Work Researchers have used integer and linear programming techniques to solve many individual problems in parallelizing compilers. For example, compiler problems such as exact data-dependence analysis [110,119], array analysis based on array summary information [137], instruction scheduling for superscalars [5], automatic data layout for minimizing communication <ref> [21] </ref>, and code generation after loop transformations [10,11] have been solved using linear and integer programming.
Reference: [22] <author> W. Blume, R. Doallo, R. Eigenmann, J. Grout, J. Hoeflinger, T. Lawrence, J. Lee, D. Padua, Y. Paek, B. Pottenger, L. Rauchwerger, and P. Tu. </author> <title> Parallel programming with polaris. </title> <journal> IEEE Computer, </journal> <volume> 29(12):7882, </volume> <month> December </month> <year> 1996. </year> <month> 209 </month>
Reference-contexts: For example, optimizations such as unused procedure elimination, which eliminates some loops, and selective procedure inlining, which creates copies of some loops, make the parallel loop counts different. The latest results from the Polaris compiler can be found in <ref> [22] </ref>. 7.6. Chapter Summary We have a fully implemented interprocedural parallelizer with advanced array analyses and we have evaluated its effectiveness by parallelizing more than 115,000 lines of FORTRAN code from 39 programs in four benchmark suites.
Reference: [23] <author> W. Blume and R. Eigenmann. </author> <title> Performance analysis of parallelizing compilers on the Perfect Benchmarks programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(6):643656, </volume> <month> November </month> <year> 1992. </year>
Reference-contexts: Inter-procedural analysis is necessary for obtaining coarse-grain parallelism. If programs are written in a modular style, it is natural that coarse-grain parallel loops will span multiple procedures. For this reason, procedure boundaries must not pose a barrier to analysis <ref> [23] </ref>. This can be accomplished by applying data-flow analysis techniques across procedure boundaries using an interprocedural framework. Although many interprocedural analyses for parallelization have been proposed [70,80,81,86,108], they have rarely been adopted in practice.
Reference: [24] <author> W. Blume, R. Eigenmann, K. Faigin, J. Grout, J. Hoeflinger, D. Padua, P. Petersen, W. Pottenger, L. Rauchwerger, P. Tu, and S. Weatherford. </author> <title> Effective automatic parallelization with Polaris. </title> <journal> International Journal of Parallel Programming, </journal> <month> May </month> <year> 1995. </year>
Reference: [25] <author> W. Blume et al. </author> <title> Polaris: The next generation in parallelizing compilers,. </title> <booktitle> In Proceedings of the Seventh Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Ithaca, NY, </address> <month> August </month> <year> 1994. </year>
Reference: [26] <author> W. J. Bolosky and M. L. Scott. </author> <title> False sharing and its effect on shared memory performance. </title> <booktitle> In Proceedings of the USENIX Symposium on Experiences with Distributed and Multiprocessor Systems (SEDMS IV), </booktitle> <pages> pages 5771, </pages> <address> San Diego, CA, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: Making effective use of the memory hierarchy on multiprocessors is even more important to performance, but also 136 more difficult to achieve. This is true for bus-based shared address space machines [50,51], and even more so for scalable shared address space machines <ref> [26] </ref> such as the Stanford DASH [105] and FLASH multiprocessors [101], MIT ALEWIFE [1], Kendall Squares KSR-1 [94], the Convex Exemplar [115], and the Silicon Graphics Origin. The memory on remote processors in these architectures constitutes yet another level in the memory hierarchy.
Reference: [27] <author> Z. Bozkus, A. Choudhary, G. Fox, T. Haupt, and S. Ranka. </author> <title> A compilation approach for Fortran 90D/HPF compilers on distributed memory MIMD computers. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference: [28] <author> T. Brandes. </author> <title> The importance of direct dependences for automatic parallelism. </title> <booktitle> In Proceedings of the Second International Conference on Supercomputing, </booktitle> <address> St. Malo, France, </address> <month> July </month> <year> 1988. </year>
Reference: [29] <author> M. Bromley, S. Heller, T. McNerney, and G. Steele, Jr. </author> <title> Fortran at ten gigaflops: The Connection Machine convolution compiler. </title> <booktitle> In Proceedings of the SIGPLAN 91 Conference on Programming Language Design and Implementation, </booktitle> <address> Toronto, Canada, </address> <month> June </month> <year> 1991. </year>
Reference: [30] <author> E. Bugnion, J. M. Anderson, T. C. Mowry, M. Rosenblum, and M. S. Lam. </author> <title> Compiler-directed page coloring for multiprocessors. </title> <booktitle> In Proceedings of the Seventh International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VII), </booktitle> <address> Cambridge, MA, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: The parallelizer is integrated as a part of the SUIF compiler system [144]. Other advanced optimizations such as loop transformations [146], data and computation co-location [13], data transformations (Chapter 8), synchronization elimination [140], compiler-directed page coloring <ref> [30] </ref>, and compiler-inserted prefetching [116] have also been implemented in the SUIF compiler system. Detection of coarse-grain parallelism, in combination with these other optimizations, can achieve significant performance improvements for sequential scientific applications on multiprocessors.
Reference: [31] <author> M. Burke and R. Cytron. </author> <title> Interprocedural dependence analysis and parallelization. </title> <booktitle> In Proceedings of the SIGPLAN 86 Symposium on Compiler Construction, </booktitle> <address> Palo Alto, CA, </address> <month> June </month> <year> 1986. </year>
Reference-contexts: Since FORTRAN implements both formal and actual array structures by mapping them to the same linear memory segment, one solution is to perform array data-flow analysis using linearized array accesses <ref> [31] </ref>. Multi-dimensional array accesses are linearized by converting them to linear offsets of the memory locations. All the linearized arrays have the same shape, thus eliminating any reshape problem. However, the regions in multi-dimensional arrays have to be represented as very complex lattice patterns in a one-dimensional linearized space. <p> This approach only shifted the reshapes of parameters and common blocks to the reshapes of equivalences. Another proposed scheme to eliminate the reshape problem is to linearize all the array accesses <ref> [31] </ref>. However, performing array analyses using these linearized accesses is more complex than using multi-dimensional arrays. For example, in array data-flow analysis, many simple regions in multi-dimensional arrays get converted into complex lattice patterns in a one-dimensional linearized space.
Reference: [32] <author> D. Callahan, K. Kennedy, and U. Kremer. </author> <title> A dynamic study of vectorization in PFC. </title> <type> Technical Report TR89-97, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> July </month> <year> 1989. </year>
Reference: [33] <author> S. Carr, K. S. McKinley, and C.-W. Tseng. </author> <title> Compiler optimizations for improving data locality. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VI), </booktitle> <pages> pages 252262, </pages> <address> San Jose, CA, </address> <month> October </month> <year> 1994. </year>
Reference: [34] <author> Z. Chamski. </author> <title> Nested loop sequences: Towards efficient loop structures in automatic parallelization. </title> <type> Technical Report RR-2094, </type> <institution> INRIA Rennes, </institution> <month> October </month> <year> 1993. </year> <month> 210 </month>
Reference: [35] <author> B. Chapman, P. Mehrotra, and H. Zima. </author> <title> Handling distributed data in Vienna Fortran procedures. </title> <booktitle> In Proceedings of the Fifth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> New Haven, CT, </address> <month> August </month> <year> 1992. </year>
Reference: [36] <author> B. Chapman, P. Mehrotra, and H. Zima. </author> <title> Programming in Vienna Fortran. </title> <booktitle> Scientific Programming, </booktitle> <address> 1(1):3150, </address> <month> Fall </month> <year> 1992. </year>
Reference-contexts: Since the combined problem of locating coarse-grain parallelism and determining computation and data decompositions that minimize communication is very complex, many com 178 piler systems for distributed address-space machines rely on users to supply the data decompositions. Languages such as High Performance FORTRAN [83], FORTRAN-D [85] and Vienna FORTRAN <ref> [36] </ref> allow the programmer to annotate the sequential program with data decompositions. Our algorithms can use this decomposition information in lieu of compiler-generated data and computation decomposition information. The organization of this chapter is as follows.
Reference: [37] <author> S. Chatterjee, J. R. Gilbert, F. J. E. Long, R. Schreiber, and S.-H. Teng. </author> <title> Generating local addresses and communication sets for data-parallel programs. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 26(1):7284, </volume> <month> April </month> <year> 1995. </year>
Reference: [38] <author> F. C. Chow. </author> <title> A Portable Machine-Independent Global OptimizerDesign and Measurements. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> December </month> <year> 1983. </year>
Reference-contexts: Compilers have played a critical role in two recent major breakthroughs in performance for general purpose computing: reduced instruction set computers (RISC) and instruction level parallelism (ILP) in microprocessors. Compilers translate complex operations into simple instructions for RISC processors <ref> [38] </ref> and schedule instructions for parallel execution in microprocessors with ILP [102,58]. Since the compilers are able to perform these techniques consistently and reliably without any user intervention, these technologies have been widely used, helping to revolutionize the microprocessor.
Reference: [39] <author> M. Cierniak and W. Li. </author> <title> Unifying data and control transformations for distributed shared memory machines. </title> <booktitle> In Proceedings of ACM SIGPLAN Conference on Programming Language Design and Implementation 95, </booktitle> <month> June </month> <year> 1995. </year>
Reference: [40] <author> Fabien Coelho. </author> <title> Contributions to HPF Compilation. </title> <type> PhD thesis, </type> <institution> Ecole des Mines de Paris, </institution> <month> October </month> <year> 1996. </year>
Reference-contexts: Our approach for communication code generation can handle any iteration and data spaces and communication patterns that can be represented using systems of linear inequalities. A recent compiler for HPF also uses a similar linear algebra framework <ref> [40] </ref>. However, our extension to linear inequalities, to allow symbolic coefficients, further expands this domain such that we can represent distributions with symbolic block sizes. Two algorithms for merging loop nests were proposed contemporaneously by [34,41].
Reference: [41] <author> J.-F. Collard, P. Feautrier, and T. Risset. </author> <title> Construction of do loops from systems of affine constraints. </title> <type> Technical Report 93-15, </type> <institution> Ecole normale suprieure de Lyon, </institution> <month> May </month> <year> 1993. </year>
Reference: [42] <author> R. P. Colwell, R. P. Nix, J. J. ODonnell, D. B. Papworth, and P. K. RodmanRodman. </author> <title> A VLIW architecture for a trace scheduling compiler. </title> <booktitle> In Proceedings of the Second International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-II), </booktitle> <month> October </month> <year> 1987. </year>
Reference: [43] <author> K. Cooper, M. W. Hall, R. T. Hood, K. Kennedy, K. S. McKinley, J. M. Mellor-Crummey, L. Torczon, and S. K. Warren. </author> <title> The ParaScope parallel programming environment. </title> <booktitle> Proceedings of the IEEE, </booktitle> <address> 81(2):244263, </address> <month> February </month> <year> 1993. </year>
Reference: [44] <author> K. Cooper, M. W. Hall, and K. Kennedy. </author> <title> A methodology for procedure cloning. </title> <booktitle> Computer Languages, </booktitle> <address> 19(2):105117, </address> <month> February </month> <year> 1993. </year>
Reference-contexts: Our interprocedural framework utilizes path-specific information only when it can provide opportunities for improved optimization. The system incorporates selective procedure cloning, a program restructuring technique in which the compiler replicates the analysis results in the context of distinct calling environments <ref> [44] </ref>. By applying cloning selectively according to the unique data-flow information it exposes, the interprocedural system can obtain the same precision as full inlining without unnecessary replication. 4.1.1.
Reference: [45] <author> B. Creusillet and F. Irigoin. </author> <title> Interprocedural array region analyses. </title> <booktitle> In Proceedings of the 8th International Workshop on Languages and Compilers for Parallel Computing. </booktitle> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1995. </year>
Reference-contexts: ) true= D A k A k A k , , start S C A k ,( ) start S C A k ,( ) mod e k ( ) A k otherwise 110 Recently, a similar parameter reshape algorithm that uses integer programming was proposed by Creusillet and Irigoin <ref> [45] </ref>. Their algorithm was an extension of our earlier algorithm [75], which did not eliminate lower dimensions, as presented in Theorem 6-2. 6.7. Chapter Summary In this chapter we introduce a systematic approach for analyzing array reshapes.
Reference: [46] <author> B. Creusillet and F. Irigoin. </author> <title> Exact vs. approximate array region analyses. </title> <booktitle> In Proceedings of the 9th International Workshop on Languages and Compilers for Parallel Computing. </booktitle> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1996. </year>
Reference: [47] <author> G. Dantzig. </author> <title> Linear Programming and Extensions. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ, </address> <year> 1963. </year>
Reference-contexts: In our compiler algorithms, we use projection as one of the key transformations in manipulating systems of linear inequalities <ref> [47] </ref>. Suppose we project an n-dimension polyhedron, , onto the ( )-dimensional subspace orthogonal to the axis representing variable . The resulting polyhedron in the ( )-dimensional subspace, , is derived by eliminating the variable from the system of inequalities of .
Reference: [48] <author> G. Dantzig and B. Eaves. </author> <title> Fourier-Motzkin elimination and its dual. </title> <journal> Journal of Combinatorial Theory (A), </journal> <volume> 14:288297, </volume> <year> 1973. </year> <month> 211 </month>
Reference: [49] <author> J. H. Edmondson et al. </author> <title> Internal organization of the Alpha 21164, a 300-MHz 64-bit quad-issue CMOS RISC microprocessor. </title> <journal> Digital Technical Journal, </journal> <volume> 7(1), </volume> <year> 1995. </year> <note> Special Edition. </note>
Reference-contexts: The Digital AlphaServer 8400 used in the experiments is a bus-based shared-memory multiprocessor containing 8 Digital 21164 Alpha processors. The Digital 21164 Alpha is a quad-issue superscalar microprocessor with two 64-bit integer and two 64-bit floating point pipelines <ref> [49] </ref>. There are two levels of caches on-chip: 8 KB instruction/ 8 KB data level 1 cache, and 96 KB of combined level 2 cache. The memory system allows multiple outstanding off-chip memory accesses. Each processor has 4 MB of 10ns external cache.
Reference: [50] <author> S. J. Eggers and T. E. Jeremiassen. </author> <title> Eliminating false sharing. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <pages> pages 377381, </pages> <address> St. Charles, IL, </address> <month> August </month> <year> 1991. </year>
Reference: [51] <author> S. J. Eggers and R. H. Katz. </author> <title> The effect of sharing on the cache and bus performance of parallel programs. </title> <booktitle> In Proceedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-III), </booktitle> <pages> pages 257 270, </pages> <address> Boston, MA, </address> <month> April </month> <year> 1989. </year>
Reference: [52] <author> R. Eigenmann, J. Hoeflinger, Z. Li, and D. Padua. </author> <title> Experience in the automatic parallelization of four Perfect benchmark programs. </title> <editor> In U. Banerjee, D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, Fourth International Workshop, </booktitle> <address> Santa Clara, CA, </address> <month> August </month> <year> 1991. </year> <note> Springer-Verlag. </note>
Reference: [53] <author> P. Feautrier. </author> <title> Array expansion. </title> <booktitle> In Proceedings of the Second International Conference on Supercomputing, </booktitle> <address> St. Malo, France, </address> <month> July </month> <year> 1988. </year>
Reference: [54] <author> P. Feautrier. </author> <title> Parametric integer programming. </title> <journal> Operationnelle/Operations Research, </journal> <volume> 22(3):243268, </volume> <month> September </month> <year> 1988. </year>
Reference: [55] <author> P. Feautrier. </author> <title> Dataflow analysis of scalar and array references. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 20(1):2352, </volume> <month> February </month> <year> 1991. </year>
Reference: [56] <author> P. Feautrier. </author> <title> Towards automatic distribution. </title> <type> Technical Report 92.95, </type> <institution> Institut Blaise Pascal/Laboratoire MASI, </institution> <month> December </month> <year> 1992. </year>
Reference: [57] <author> D. M. Fenwick, D. J. Foley, W. B. Gist, S. R. VanDoren, and D. Wissell. </author> <title> The Alphaserver 8000 series: High-end server platform development. </title> <journal> Digital Technical Journal, </journal> <volume> 7(1), </volume> <year> 1995. </year> <note> Special Edition. </note>
Reference-contexts: The resulting C program is linked to a parallel run-time system that currently runs on several bus-based shared memory architectures (Silicon Graphics Challenge and Power Challenge, and Digital 8400 multiprocessors <ref> [57] </ref>) and scalable shared-memory architectures (Stanford DASH [105] and Kendall Square KSR-1 [94]). We have developed an interprocedural parallelizer with advanced array analyses and optimizations, that is capable of detecting coarse-grain parallelism [75,76,71]. The parallelizer is integrated as a part of the SUIF compiler system [144]. <p> Each processor has 4 MB of 10ns external cache. The architecture provides 32 integer and 32 floating-point registers. The 256-bit data bus, which operates at 75MHz, supports 265ns memory read latencies and 2.1 GB per second of data bandwidth. Banked memory modules are attached to the bus <ref> [57] </ref>. 7.2. Examples of Coarse-Grain Parallelism Not only do some of the SUIF-parallelized loops execute for a long time, they can also be very large.
Reference: [58] <author> J. Fisher. </author> <title> Trace scheduling: A technique for global microcode compaction. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-30(7):478490, </volume> <month> July </month> <year> 1981. </year>
Reference: [59] <author> M. J. Flynn. </author> <title> Parallel processors were the future...and may yet be. </title> <journal> IEEE Computer, </journal> <volume> 29(12):151152, </volume> <month> December </month> <year> 1996. </year>
Reference-contexts: However, so far they have not achieved the predicted performance gains for general purpose computing, mainly because of the inability to create parallel software, either explicitly by a programmer or automatically by a compiler <ref> [59] </ref>. Parallel programs are hard to develop, difficult to debug and expensive to maintain. The current generation of parallelizing compilers cannot extract parallel performance from sequential programs even with extensive user intervention. Thus, the adoption of parallel computing has been much slower than that anticipated 30 years ago [59]. <p> a compiler <ref> [59] </ref>. Parallel programs are hard to develop, difficult to debug and expensive to maintain. The current generation of parallelizing compilers cannot extract parallel performance from sequential programs even with extensive user intervention. Thus, the adoption of parallel computing has been much slower than that anticipated 30 years ago [59]. Recent developments in compiler technologies have the potential to deliver the much anticipated breakthrough in parallel computing [8]. Compilers have played a critical role in two recent major breakthroughs in performance for general purpose computing: reduced instruction set computers (RISC) and instruction level parallelism (ILP) in microprocessors.
Reference: [60] <author> D. Gannon, W. Jalby, and K. Gallivan. </author> <title> Strategies for cache and local memory management by global program transformations. </title> <booktitle> In Proceedings of the First International Conference on Supercomputing. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Athens, Greece, </address> <month> June </month> <year> 1987. </year>
Reference-contexts: Redundant communication due to group reuse Detection of reuse between arbitrary accesses to the same matrix can be expensive. However, one prevalent form of reuse can be incorporated and exploited easily within our model: the set of uniformly generated references <ref> [60] </ref>. Array index functions of uniformly generated references are affine functions of loop indices and symbolic constants, and they differ only in the constant terms.
Reference: [61] <author> D. Gannon, W. Jalby, and K. Gallivan. </author> <title> Strategies for cache and local memory management by global program transformation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5(5):587616, </volume> <month> October </month> <year> 1988. </year>
Reference: [62] <author> M. Gerndt. </author> <title> Automatic Parallelization for Distributed-Memory Multiprocessing Systems. </title> <type> PhD thesis, </type> <institution> University of Bonn, </institution> <month> December </month> <year> 1989. </year> <month> 212 </month>
Reference-contexts: s = 32 p s + 29, MIN (32 p s + 31, N - 3) i r = i s + 3 buffer [index] = X [a] index = index + 1 send the data in buffer to processor p r (b) Send code after aggregated communication 200 SUPERB <ref> [62] </ref> and Vienna Fortran [35,36]. Many of these efforts converged on the development of High Performance Fortran (HPF) as an industry-wide standard language to support distributed memory machines, which extends FORTRAN-90 with data decomposition information [83,99].
Reference: [63] <author> R. L. Graham, D. E. Knuth, and O. Patashnik. </author> <title> Concrete Mathematics. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1989. </year>
Reference-contexts: DIMENSION A (0:(N+nproc-1)/nproc, N, 0:nproc) DO J = 2, 99 A ((I-1)/nproc+1, J, mod (I-1,nproc)+1) = ... ENDDO ENDDO P d - d b d d b 1+ b max 1 155 8.3.1. Modulo and division simplification We exploit fundamental properties of modulo and division operations <ref> [63] </ref> to simplify expressions with these operations. Figure 8-16 is the list of simplifications performed on expressions with modulo and division operations by the compiler. In the list, are expressions and are integers.
Reference: [64] <author> E. D. Granston and A. Veidenbaum. </author> <title> Detecting redundant accesses to array data. </title> <booktitle> In Proceedings of Supercomputing 91, </booktitle> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year>
Reference: [65] <author> T. Gross, S. Hinrichs, G. Lueh, D. OHallaron, J. Stichnoth, and J. Subhlok. </author> <title> Compiling task and data parallel programs for iWarp. </title> <booktitle> In Proceedings of the Workshop on Languages, Compilers, and Run-Time Environments for Distributed Memory Multiprocessors, </booktitle> <address> Boulder, CO, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: The technique to eliminate this redundancy is similar to that of removing redundant communication due to self-reuse. 9.5.2. Communication Aggregation Whether aggregation of small messages into large messages is necessary depends on the machine architecture. For example, machines such as the iWarp <ref> [65] </ref> and CM-5 [118] support fine-grain communication, while machines such as the Intel iPSC have significant overhead in processing every message.
Reference: [66] <author> T. Gross and P. Steenkiste. </author> <title> Structured dataflow analysis for arrays and its use in an optimizing compiler. </title> <journal> SoftwarePractice and Experience, </journal> <volume> 20(2):133155, </volume> <month> February </month> <year> 1990. </year>
Reference: [67] <author> J. Grout. </author> <title> Inline expansion for the polaris research compiler. </title> <type> Masters thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: Related Work Many previous interprocedural analyzers did not address the array reshape problem. One way to avoid array reshape analysis is to perform inline substitution and generate equivalence statements to describe the reshapes that occur in parameter passing and in common blocks <ref> [67] </ref>. This approach only shifted the reshapes of parameters and common blocks to the reshapes of equivalences. Another proposed scheme to eliminate the reshape problem is to linearize all the array accesses [31]. However, performing array analyses using these linearized accesses is more complex than using multi-dimensional arrays.
Reference: [68] <author> M. Gupta and P. Banerjee. </author> <title> Demonstration of automatic data partitioning techniques for parallelizing compilers on multicomputers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(2):179193, </volume> <month> March </month> <year> 1992. </year>
Reference: [69] <author> M. Gupta, S. Midkiff, E. Schonberg, V. Seshadri, K. Wang, D. Shields, W.-M. Ching, and T. Ngo. </author> <title> An hpf compiler for the ibm sp2. </title> <booktitle> In Proceedings of Supercomputing 95, </booktitle> <address> San Diego, CA, </address> <month> December </month> <year> 1995. </year>
Reference: [70] <author> M. Haghighat and C. Polychronopoulos. </author> <title> Symbolic analysis: A basis for parallelization, optimization, and scheduling of programs. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference: [71] <author> M. W. Hall, S. P. Amarasinghe, B. R. Murphy, S.-W. Liao, and M. S. Lam. </author> <title> Detecting coarse-grain parallelism using an interprocedural parallelizing compiler. </title> <booktitle> In Proceedings of Supercomputing 95, </booktitle> <address> San Diego, CA, </address> <month> December </month> <year> 1995. </year>
Reference: [72] <author> M. W. Hall, S. P. Amarasinghe, B. R. Murphy, S.-W. Liao, and M. S. Lam. </author> <title> Interprocedural parallelization analysis: Preliminary results. </title> <type> Technical Report CSL-TR-95-665, </type> <institution> Dept. of Computer Science, Stanford University, </institution> <month> March </month> <year> 1995. </year>
Reference: [73] <author> M. W. Hall, J. M. Anderson, S. P. Amarasinghe, B. R. Murphy, S.-W. Liao, E. Bugnion, and M. S. Lam. </author> <title> Maximizing multiprocessor performance with the suif compiler. </title> <journal> IEEE Computer, </journal> <volume> 29(12):8489, </volume> <month> December </month> <year> 1996. </year>
Reference: [74] <author> M. W. Hall, J. Mellor-Crummey, A. Carle, and R. Rodriguez. FIAT: </author> <title> A framework for interprocedural analysis and transformation. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: The next set of phases includes parallelization analyses on scalar variables. These analyses identify scalar dependences, and perform optimizations, such as scalar privatization and scalar reductions. More information on scalar analyses can be found in <ref> [74] </ref>. The scalar analyses are followed by the array analyses and parallelization, which include the following four phases: The first phase propagates loop context information to the nested loops.
Reference: [75] <author> M. W. Hall, B. R. Murphy, and S. P. Amarasinghe. </author> <title> Interprocedural analysis for parallelization: Design and experience. </title> <booktitle> In Proceedings of the Seventh SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 650655, </pages> <address> San Francisco, CA, </address> <month> February </month> <year> 1995. </year> <month> 213 </month>
Reference-contexts: Their algorithm was an extension of our earlier algorithm <ref> [75] </ref>, which did not eliminate lower dimensions, as presented in Theorem 6-2. 6.7. Chapter Summary In this chapter we introduce a systematic approach for analyzing array reshapes. We present algorithms to handle array reshapes that occur in parameter passing, equivalences and different common block declarations.
Reference: [76] <author> M. W. Hall, B. R. Murphy, S. P. Amarasinghe, S.-W. Liao, and M. S. Lam. </author> <title> Interprocedural analysis for parallelization. </title> <booktitle> In Proceedings of the 8th International Workshop on Languages and Compilers for Parallel Computing. </booktitle> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1995. </year>
Reference-contexts: Top-down traversal for ow-insensitive order The only top-down traversal used in this thesis is flow-insensitive. Thus, we omit the forward-flow and backward-flow orders from the description. The general algorithm for a top-down traversal can be found in <ref> [76] </ref>. A flow-insensitive, top-down pass is used to propagate information into loop bodies and down the call chain. A flow value at a node is the cumulation of the local information of all the enclosing loops and procedure calls. <p> We also provide an empirical evaluation of the compiler system by using it to paral-lelize more than 115,000 lines of FORTRAN code from 39 programs in four benchmark suites. We evaluate the effectiveness of using interprocedural analysis, including two advanced array analysis techniques: array privatization and array reduction <ref> [76] </ref>. We present static counts of the parallelizable loops found using each of these techniques. Static loop counts, though, are not good indicators of whether parallelization is successful. Specifically, parallelizing just one outermost loop can have a profound impact on a programs performance. <p> It performs intraprocedural data dependence, and lacks the capability to privatize arrays or recognize reductions. Note that the baseline system is much more powerful than many existing parallelizing compilers as it contains all the interprocedural scalar analysis 120 <ref> [76] </ref>. Our full system, in addition to the analyses in the baseline system, performs array reduction and privatization analysis and carries out all the analyses interprocedurally. We also separately measure the impact of the three components: interprocedural analysis, array reductions, and array privatization. 7.4.1. <p> When considering only those loops containing calls for the set of 16 programs used in that study, the SUIF system is able to parallelize more than five times as many loops <ref> [76] </ref>. The key difference between the two systems is that SUIF contains full interprocedural array analysis, including array privatiza-tion and reduction recognition. The Polaris compiler system is also a fully implemented parallelizer using advanced analyses [24,142].
Reference: [77] <author> L. Hamey, J. Webb, and I. Wu. </author> <title> An architecture independent programming language for low-level vision. Computer Vision, Graphics, </title> <booktitle> and Image Processing, </booktitle> <address> 48(2):246264, </address> <month> November </month> <year> 1989. </year>
Reference: [78] <author> S. W. Haney. </author> <title> Is c++ fast enough for scientific computing? Computers in Physics, </title> <address> 8(6):690694, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: For example, many computationally intensive algorithms coded using FORTRAN can outperform the same algorithms written using C++ by more than a factor of two <ref> [78] </ref>. Compilers are able to obtain superior performance from FORTRAN programs because many modern language features that hinder compiler optimizations, such as aliasing and dynamic memory allocation, are absent or are severely restricted in the FORTRAN language.
Reference: [79] <author> W.L. Harrison. </author> <title> The interprocedural analysis and automatic parallelization of Scheme programs. Lisp and Symbolic Computation, </title> <address> 2(3/4):179396, </address> <month> October </month> <year> 1989. </year>
Reference: [80] <author> P. Havlak. </author> <title> Interprocedural symbolic analysis. </title> <type> PhD thesis, </type> <institution> Rice University, Dept. of Computer Science, </institution> <month> May </month> <year> 1994. </year>
Reference: [81] <author> P. Havlak and K. Kennedy. </author> <title> An implementation of interprocedural bounded regular section analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3):350360, </volume> <month> July </month> <year> 1991. </year>
Reference-contexts: Accuracy of their analyses is defined by the precision of the summary index set representation. These algorithms use different forms of regular section descriptors as the array index set representation. Each regular section can be used only to precisely represent a limited domain of rectilinear, triangular or diagonal spaces <ref> [81] </ref>. More complex spaces can be represented using multiple regular sections [142]. The scope of data-flow and data-dependence analysis performed using regular section information is much more restricted than using a representation based on linear inequalities. <p> All the data accessed within the interval requiring communication are summarized by a regular section description <ref> [81] </ref>. In this way, the same data used multiple times within the interval are transferred only once. <p> The current HPF compilers [27,69,17], as well as most of the previous compilers for distributed memory machines, use regular section descriptors <ref> [81] </ref> to summarize iteration and data spaces as well as communication. However, regular sections can be used only to precisely represent a limited domain of rectilinear, triangular or diagonal spaces, creating spurious communication.
Reference: [82] <author> J. L. Hennessy and D. A. Patterson. </author> <title> Computer Architecture A Quantitative Approach. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: Locating coarse-grain parallelism is not sufficient to obtain parallel performance. It is critical to make effective use of the memory hierarchy to achieve high performance. Over the last decade, microprocessor speeds have been steadily improving at a rate of 50% to 100% every year <ref> [82] </ref>. Meanwhile, memory access times have been improving at the rate of only 7% per year [82]. A common technique used to bridge this gap between processor and memory speeds is to employ one or more levels of caches. <p> Over the last decade, microprocessor speeds have been steadily improving at a rate of 50% to 100% every year <ref> [82] </ref>. Meanwhile, memory access times have been improving at the rate of only 7% per year [82]. A common technique used to bridge this gap between processor and memory speeds is to employ one or more levels of caches. However, it has been notoriously difficult to use caches effectively for numeric applications. <p> The block size used in the Challenge is 128 bytes. The independent data and address buses provide support for split transactions and the PowerPath-2 can have up to eight outstanding read transactions <ref> [82] </ref>. The Digital AlphaServer 8400 used in the experiments is a bus-based shared-memory multiprocessor containing 8 Digital 21164 Alpha processors. The Digital 21164 Alpha is a quad-issue superscalar microprocessor with two 64-bit integer and two 64-bit floating point pipelines [49]. <p> We introduce a compiler algorithm to eliminate these two classes of problems by transforming data arrays in the programs [7,12]. 8.1.1. False Sharing Misses In a modern computer, data is transferred in fixed-size units known as cache lines, which are typically 4 to 128 bytes long <ref> [82] </ref>. A computation is said to have spatial locality if it uses multiple words in a cache line before the line is displaced from the cache. While spatial locality is common to both uni- and multiprocessors, false sharing is unique to multiprocessors. <p> This approach enables a more general set of data and computation decompositions and allows for more communication optimizations. 202 203 Conclusion From the inception of the first electronic computer, architects have been striving to design the ultimate computer by simply connecting many smaller ones <ref> [82] </ref>. Such multiprocessors, which can bypass many of the physical limitations of uniprocessor performance, were expected to become ubiquitous in computing.
Reference: [83] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran language specification. </title> <booktitle> Scientific Programming, </booktitle> <address> 2(1-2):1170, </address> <year> 1993. </year>
Reference-contexts: In C, pointer arithmetic and type casting can prevent data transformations. 8.2.3. Algorithm Overview The data transformation algorithm uses data decompositions that are either provided by the programmer using a language such as HPF (High Performance FORTRAN) <ref> [83] </ref> or automatically generated by a compiler algorithm [2,13,16,21,68,107,129]. Figure 8-5 is an example of a data decomposition specification using the HPF language. In the example, the array A is mapped to a two-dimensional processor grid by two-dimensional blocks using the template T. <p> Since the combined problem of locating coarse-grain parallelism and determining computation and data decompositions that minimize communication is very complex, many com 178 piler systems for distributed address-space machines rely on users to supply the data decompositions. Languages such as High Performance FORTRAN <ref> [83] </ref>, FORTRAN-D [85] and Vienna FORTRAN [36] allow the programmer to annotate the sequential program with data decompositions. Our algorithms can use this decomposition information in lieu of compiler-generated data and computation decomposition information. The organization of this chapter is as follows.
Reference: [84] <author> M. Hind, M. Burke, P. Carini, and S. Midkiff. </author> <title> An empirical study of precise interprocedural array analysis. </title> <booktitle> Scientific Programming, </booktitle> <address> 3(3):255271, </address> <year> 1994. </year>
Reference-contexts: Related Work Previous evaluations of interprocedural parallelization systems have provided static measurements of the number of additional loops parallelized as a result of interprocedural analysis [81,84,108,138]. We have compared our results with a recent empirical study, which 132 examines the Spec89 and Perfect benchmark suites <ref> [84] </ref>. When considering only those loops containing calls for the set of 16 programs used in that study, the SUIF system is able to parallelize more than five times as many loops [76].
Reference: [85] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8):6680, </volume> <month> August </month> <year> 1992. </year>
Reference-contexts: Since the combined problem of locating coarse-grain parallelism and determining computation and data decompositions that minimize communication is very complex, many com 178 piler systems for distributed address-space machines rely on users to supply the data decompositions. Languages such as High Performance FORTRAN [83], FORTRAN-D <ref> [85] </ref> and Vienna FORTRAN [36] allow the programmer to annotate the sequential program with data decompositions. Our algorithms can use this decomposition information in lieu of compiler-generated data and computation decomposition information. The organization of this chapter is as follows.
Reference: [86] <author> F. Irigoin. </author> <title> Interprocedural analyses for programming environments. </title> <booktitle> In NSF-CNRS Workshop on Evironments and Tools for Parallel Scientific Programming, </booktitle> <month> September </month> <year> 1992. </year>
Reference: [87] <author> F. Irigoin, P. Jouvelot, and R. Triolet. </author> <title> Semantical interprocedural parallelization: An overview of the PIPS project. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Their algorithm did not create exact convex regions in many situations, such as sparse access patterns, but provided approximations using a convex hull of all the indices. In the PIPS project, an index set using an integer-lattice was proposed but not implemented due to practical considerations <ref> [87] </ref>. Our representation for the index sets is most similar to their current representation, which uses a single convex polyhedron as the index set [45,46]. However, there are many access patterns found in practice that cannot be precisely represented by a single convex region.
Reference: [88] <author> T. Jeremiassen and S. Eggers. </author> <title> Reducing false sharing on shared-memory multiprocessors through compile-time data transformations. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 179188, </pages> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: This means, for example, that our compiler can achieve good cache performance by creating cyclic and multi-dimensional blocked distributions. Compile-time data transformations have also been used to eliminate false-sharing in explicitly parallel C code <ref> [88] </ref>. The domain of that work is quite different from ours; we consider both data and computation transformations, and the code is parallelized automatically. Their compiler statically analyzes a parallel program to determine the data accessed by each processor, and then tries to group the data together.
Reference: [89] <author> Y. Ju and H. Dietz. </author> <title> Reduction of cache coherence overhead by compiler data layout and loop transformation. </title> <editor> In U. Banerjee, D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, Fourth International Workshop, </booktitle> <pages> pages 344358, </pages> <address> Santa Clara, CA, </address> <month> August </month> <year> 1991. </year> <note> Springer-Verlag. 214 </note>
Reference: [90] <author> J. Kam and J. Ullman. </author> <title> Global data flow analysis and iterative algorithms. </title> <journal> Journal of the ACM, </journal> <volume> 23(1):159171, </volume> <month> January </month> <year> 1976. </year>
Reference-contexts: One important method used by compiler writers to tackle the complexity of the development process is to take advantage of proven frameworks. Use of tools such as parser generators [96] and data-flow frameworks <ref> [90] </ref> can help create robust and powerful compilers with relative ease. The next generation of compilers, aimed at parallel architectures, such as shared memory multiprocessors, needs to perform even more complex whole program analysis techniques and aggressive optimizations. <p> We compare our approach to related works in Section 4.6, and we summarize in Section 4.7. 4.1. Interprocedural Framework Traditional intraprocedural data-flow analysis frameworks help reduce development time and improve correctness by capturing the common features in a single module <ref> [90] </ref>. In an interprocedural setting, a framework is even more important because of the increased complexity in collecting and managing information about all the procedures in a program. Thus, when building the parallelizer, we rely on an interprocedural framework that encapsulates the common features of interprocedural analysis [74,76].
Reference: [91] <author> P. Keleher and C.-W. Tseng. </author> <title> Improving the compiler/software dsm interface: Preliminary experiences. </title> <booktitle> In Proceedings of the First SUIF Compiler Workshop, </booktitle> <institution> Stanford University, Stanford, </institution> <address> CA, </address> <month> January </month> <year> 1996. </year>
Reference-contexts: This framework is being used at Stanford and elsewhere for developing many other compiler algorithms, such as synchronization optimizations [140], interprocedural propagation in computation and data co-location information, predicated data-ow analysis, and communication analysis for software DSM <ref> [91] </ref>. This framework allowed us to rapidly prototype and test many algorithms and ideas for next-generation compilers. We have implemented an array analysis algorithm using an array summary representation based on lists of systems of linear inequalities.
Reference: [92] <author> W. Kelly and W. Pugh. </author> <title> Minimizing communication while preserving parallelism. </title> <booktitle> In Proceedings of the 1996 ACM International Conference on Supercomputing, </booktitle> <pages> pages 5260, </pages> <month> May </month> <year> 1996. </year>
Reference: [93] <author> W. Kelly, W. Pugh, and E. Rosser. </author> <title> Code generation for multiple mappings. </title> <type> Technical Report CS-TR-3317.1, </type> <institution> Dept. of Computer Science, University of Maryland, </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: Two algorithms for merging loop nests were proposed contemporaneously by [34,41]. These algorithms use linear inequalities to identify the common ranges of iterations and split the iteration space. In addition, they introduce heuristics to limit the exponential growth of the program. A similar algorithm was later introduced by <ref> [93] </ref>. All the compilers for distributed address space machines use a location-centric approach to communication identification. Array privatization present the only opportunity for reducing spurious communication created by this approach. The value-centric approach we introduced creates communication only when there is a producer-consumer relationship.
Reference: [94] <institution> Kendall Square Research, </institution> <address> Waltham, MA. </address> <note> KSR1 Principles of Operation, revision 6.0 edition, </note> <month> October </month> <year> 1992. </year>
Reference-contexts: The resulting C program is linked to a parallel run-time system that currently runs on several bus-based shared memory architectures (Silicon Graphics Challenge and Power Challenge, and Digital 8400 multiprocessors [57]) and scalable shared-memory architectures (Stanford DASH [105] and Kendall Square KSR-1 <ref> [94] </ref>). We have developed an interprocedural parallelizer with advanced array analyses and optimizations, that is capable of detecting coarse-grain parallelism [75,76,71]. The parallelizer is integrated as a part of the SUIF compiler system [144]. <p> This is true for bus-based shared address space machines [50,51], and even more so for scalable shared address space machines [26] such as the Stanford DASH [105] and FLASH multiprocessors [101], MIT ALEWIFE [1], Kendall Squares KSR-1 <ref> [94] </ref>, the Convex Exemplar [115], and the Silicon Graphics Origin. The memory on remote processors in these architectures constitutes yet another level in the memory hierarchy. The differences in access times among cache, local, and remote memory can be very large.
Reference: [95] <author> K. Kennedy, N. Nedeljkovic, and A. Sethi. </author> <title> A linear-time algorithm for computing the memory access sequence in data-parallel programs. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> July </month> <year> 1995. </year>
Reference: [96] <author> B. W. Kernighan and R. Pike. </author> <title> The UNIX Programming Environment. </title> <publisher> Prentice Hall Inc, </publisher> <address> Eaglewood Cliffs, NJ, </address> <year> 1984. </year>
Reference-contexts: Compilers have become very large and complex software systems that require highly skilled compiler writers and many people-years of development. One important method used by compiler writers to tackle the complexity of the development process is to take advantage of proven frameworks. Use of tools such as parser generators <ref> [96] </ref> and data-flow frameworks [90] can help create robust and powerful compilers with relative ease. The next generation of compilers, aimed at parallel architectures, such as shared memory multiprocessors, needs to perform even more complex whole program analysis techniques and aggressive optimizations.
Reference: [97] <author> D. Klappholz, K. Psarris, and X. Kong. </author> <title> On the perfect accuracy of an approximate subscript analysis test. </title> <booktitle> In Proceedings of the 1990 ACM International Conference on Supercomputing, </booktitle> <address> Amsterdam, The Netherlands, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Banked memory modules are attached to the bus [57]. 7.2. Examples of Coarse-Grain Parallelism Not only do some of the SUIF-parallelized loops execute for a long time, they can also be very large. The largest loop SUIF parallelizes is from spec77 of the Perfect benchmark 114 suite <ref> [97] </ref>, consisting of 1002 lines of code from the original loop and its invoked procedures. An outline of the loop is shown in Figure 7-2. The boxes represent procedures and the lines represent procedure invocations.
Reference: [98] <author> C. Koelbel. </author> <title> Compile-time generation of regular communications patterns. </title> <booktitle> In Proceedings of Supercomputing 91, </booktitle> <pages> pages 101110, </pages> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year>
Reference: [99] <author> C. Koelbel, D. Loveman, R. Schreiber, G. Steele, Jr., and M. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference: [100] <author> C. Koelbel, P. Mehrotra, and J. Van Rosendale. </author> <title> Semi-automatic domain decomposition in BLAZE. </title> <editor> In S. Sahni, editor, </editor> <booktitle> Proceedings of the 1987 International Conference on Parallel Processing, </booktitle> <pages> pages 521524, </pages> <address> St. Charles, IL, </address> <month> August </month> <year> 1987. </year>
Reference-contexts: If the bounds of are independent of , the data sent to each processor are identical. 9.6. Related Work There is a large body of research on language extensions and compiler support for distributed memory machines. Some notable projects are, Al [141], Blaze <ref> [100] </ref>, Crystal [106], FORTRAN-D [85,139], Id Nouveau [123], Kali [114,98], Pandore [15], Pandore II [14], p s i s 1 p r i s k i r a, , , , , , , , , i s k 1 i s n i r i r k 1 i r
Reference: [101] <author> J. Kuskin, D. Ofelt, M. Heinrich, , J. Heinlein, R. Simoni, K. Charachorloo, J. Chapin, D. nakahira, J. Baxter, M. Horowitz, A. Gupta, M. Rosenblum, and J. Hennessy. </author> <title> The stanford flash multiprocessor. </title> <booktitle> In Proceedings of the 21th International Symposium on Computer Architecture, </booktitle> <pages> pages 302313, </pages> <address> Chicago, IL, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: This is true for bus-based shared address space machines [50,51], and even more so for scalable shared address space machines [26] such as the Stanford DASH [105] and FLASH multiprocessors <ref> [101] </ref>, MIT ALEWIFE [1], Kendall Squares KSR-1 [94], the Convex Exemplar [115], and the Silicon Graphics Origin. The memory on remote processors in these architectures constitutes yet another level in the memory hierarchy. The differences in access times among cache, local, and remote memory can be very large.
Reference: [102] <author> M. S. Lam. </author> <title> Software pipelining: An effective scheduing technique for VLIW machines. </title> <booktitle> In Proceedings of the SIGPLAN 88 Conference on Programming Language Design and Implementation, </booktitle> <address> Atlanta, GA, </address> <month> June </month> <year> 1988. </year> <month> 215 </month>
Reference: [103] <author> M. S. Lam, E. E. Rothberg, and M. E. Wolf. </author> <title> The cache performance and optimizations of blocked algorithms. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-IV), </booktitle> <pages> pages 6374, </pages> <address> Santa Clara, CA, </address> <month> April </month> <year> 1991. </year>
Reference: [104] <author> W. Landi, B. Ryder, and S. Zhang. </author> <title> Interprocedural modification side effect analysis with pointer aliasing. </title> <booktitle> In Proceedings of the SIGPLAN 93 Conference on Programming Language Design and Implementation, </booktitle> <address> Albuquerque, NM, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: For example, interprocedural analysis using the supergraph [117] program representation, where individual control flow graphs for the procedures in the program are linked together at procedure call and return points, loses context sensitivity by propagating information along unrealizable paths <ref> [104] </ref>. This occurs when the analysis propagates calling context information from one caller through a procedure and returns the side-effect information to a different caller. Furthermore, iterative analysis over this structure is slow because of the large number of control flow paths through which information flows.
Reference: [105] <author> D. Lenoski, J. Laudon, T. Joe, D. Nakahira, L. Stevens, A. Gupta, and J. Hennessy. </author> <title> The DASH prototype: Implementation and performance. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 92105, </pages> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: The resulting C program is linked to a parallel run-time system that currently runs on several bus-based shared memory architectures (Silicon Graphics Challenge and Power Challenge, and Digital 8400 multiprocessors [57]) and scalable shared-memory architectures (Stanford DASH <ref> [105] </ref> and Kendall Square KSR-1 [94]). We have developed an interprocedural parallelizer with advanced array analyses and optimizations, that is capable of detecting coarse-grain parallelism [75,76,71]. The parallelizer is integrated as a part of the SUIF compiler system [144]. <p> Making effective use of the memory hierarchy on multiprocessors is even more important to performance, but also 136 more difficult to achieve. This is true for bus-based shared address space machines [50,51], and even more so for scalable shared address space machines [26] such as the Stanford DASH <ref> [105] </ref> and FLASH multiprocessors [101], MIT ALEWIFE [1], Kendall Squares KSR-1 [94], the Convex Exemplar [115], and the Silicon Graphics Origin. The memory on remote processors in these architectures constitutes yet another level in the memory hierarchy.
Reference: [106] <author> J. Li and M. Chen. </author> <title> Compiling communication-efficient programs for massively parallel machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3):361376, </volume> <month> July </month> <year> 1991. </year>
Reference-contexts: If the bounds of are independent of , the data sent to each processor are identical. 9.6. Related Work There is a large body of research on language extensions and compiler support for distributed memory machines. Some notable projects are, Al [141], Blaze [100], Crystal <ref> [106] </ref>, FORTRAN-D [85,139], Id Nouveau [123], Kali [114,98], Pandore [15], Pandore II [14], p s i s 1 p r i s k i r a, , , , , , , , , i s k 1 i s n i r i r k 1 i r n i
Reference: [107] <author> J. Li and M. Chen. </author> <title> The data alignment phase in compiling programs for distributed-memory machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13(2):213221, </volume> <month> October </month> <year> 1991. </year>
Reference: [108] <author> Z. Li and P. Yew. </author> <title> Efficient interprocedural analysis for program restructuring for parallel programs. </title> <booktitle> In Proceedings of the ACM SIGPLAN Symposium on Parallel Programming: Experience with Applications, Languages, and Systems (PPEALS), </booktitle> <address> New Haven, CT, </address> <month> July </month> <year> 1988. </year>
Reference: [109] <author> A. W. Lim and M. S. Lam. </author> <title> Maximizing parallelism and minimizing synchronization with affine transforms. </title> <booktitle> In Proceedings of the Twenty-forth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <month> January </month> <year> 1997. </year>
Reference: [110] <author> D. E. Maydan. </author> <title> Accurate Analysis of Array References. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Stanford University, </institution> <month> September </month> <year> 1992. </year>
Reference-contexts: The compilers perform a data dependence test on each pair of accesses within the candidate loop for parallelization. Determining the data dependences of loop nests where the loop bounds and array indices are affine functions of the loop indices is equivalent to integer programming <ref> [110] </ref>. Many practical algorithms have been devised to find the data dependence information exactly [19,110,120,149,151]. 3.1.2. Extracting Fine-Grain Parallelism The first-generation parallelizers start the parallelization process by performing a series of symbolic analyses such as constant propagation, induction variable identification and loop-invariant code motion. <p> We have imposed an additional requirement on the precision of the array summary representation. We want the data dependence test based on the summary representation to be at least as precise as the pair-wise array data dependence test that it will replace <ref> [110] </ref>. The pair-wise data dependence test is exact over the affine domain. <p> For example, multiple write accesses, described in Figure 3-4, can only be precisely represented using a set of convex regions. Thus, even for the array data dependence analysis using the summary information to obtain the same precision as the pair-wise data dependence test <ref> [110] </ref>, a summary based on a single convex region is not sufficient. D 1 R 1 93 5.7. Chapter Summary In this chapter we introduce an array summary representation based on lists of systems of linear inequalities. <p> Using this representation, we find data-flow information more accurately than any other previous summary representation. We are also able to perform the data-dependence analysis at the same precision as the exact data dependence test <ref> [110] </ref>. We have defined the set operators used for manipulating array summaries, in this representation. Our intersection, union and projection operators and the empty test are exact. <p> This approach is driven by the need to compute both location-based and value-based dependences. Using this representation, we find data-ow information more accurately than any other previous summary representation. We are also able to perform the data-dependence analysis at the same precision as the exact data dependence test <ref> [110] </ref>. 205 We have designed the first algorithm capable of handling simple reshape patterns that occur in practice. Using integer projections, this algorithm handles array reshapes that occur in parameter passing, equivalences, and different common block declarations.
Reference: [111] <author> D. E. Maydan, S. P. Amarasinghe, and M. S. Lam. </author> <title> Data dependence and data-flow analysis of arrays. </title> <booktitle> In Proceedings of the Fifth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> New Haven, CT, </address> <month> August </month> <year> 1992. </year>
Reference: [112] <author> D. E. Maydan, S. P. Amarasinghe, and M. S. Lam. </author> <title> Array data-flow analysis and its use in array privatization. </title> <booktitle> In Proceedings of the Twentieth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 215, </pages> <address> Charleston, SC, </address> <month> January </month> <year> 1993. </year>
Reference-contexts: We have devised a more efficient algorithm than Feautriers for obtaining data-flow information that is applicable to many common cases found in practice <ref> [112] </ref>. Several other researchers have taken a similar approach to data-flow analysis [28,121,122]. However, none of these algorithms handle general control flow in a direct or efficient manner. Extending the pair-wise data dependence framework is not efficient in handling a large number of array accesses. <p> s p r i i r p r i s p s , a, , , I P I P A i r p r , C a p s , D i r i p s p r 32p 0 1 i 189 problem in calculating last write trees <ref> [112] </ref>. The set of inequalities generated, in conjunction with the final data distribution, defines the communication set for finalization. 9.4. Code Generation for Distributed Address-Space Machines We use the code generation algorithm defined in Section 2.2. for generating SPMD loop nests with communication operations. <p> The second complication arises from the fact that a projected image may contain points that do not correspond to a solution in the original system. In many cases, a simple test can determine that no such degeneracies are present <ref> [112] </ref>. 9.5.1.2. Redundant communication due to group reuse Detection of reuse between arbitrary accesses to the same matrix can be expensive. However, one prevalent form of reuse can be incorporated and exploited easily within our model: the set of uniformly generated references [60].
Reference: [113] <author> D. E. Maydan, J. L. Hennessy, and M. S. Lam. </author> <title> Effectiveness of data dependence analysis. </title> <booktitle> In Proceedings of the NSF-NCRD Workshop on Advanced Compilation Techniques for Novel Architectures, </booktitle> <year> 1992. </year>
Reference: [114] <author> P. Mehrotra and J. Van Rosendale. </author> <title> Programming distributed memory architectures using Kali. </title> <booktitle> In Advances in Languages and Compilers for Parallel Computing, </booktitle> <address> Irvine, CA, August 1990. </address> <publisher> The MIT Press. </publisher> <pages> 216 </pages>
Reference: [115] <author> P. Michielse. </author> <title> Programming the convex exemplar series spp system. </title> <editor> In J. Dongarra and J. Wasniewski, editors, </editor> <booktitle> Proceedings of the first International Workshop in Parallel Scientific Computing, </booktitle> <pages> pages 374382. </pages> <publisher> Springer-Verlag, </publisher> <month> June </month> <year> 1994. </year>
Reference-contexts: This is true for bus-based shared address space machines [50,51], and even more so for scalable shared address space machines [26] such as the Stanford DASH [105] and FLASH multiprocessors [101], MIT ALEWIFE [1], Kendall Squares KSR-1 [94], the Convex Exemplar <ref> [115] </ref>, and the Silicon Graphics Origin. The memory on remote processors in these architectures constitutes yet another level in the memory hierarchy. The differences in access times among cache, local, and remote memory can be very large.
Reference: [116] <author> T. Mowry, M. S. Lam, and A. Gupta. </author> <title> Design and evaluation of a compiler algorithm for prefetching. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-V), </booktitle> <pages> pages 6273, </pages> <address> Boston, MA, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: The parallelizer is integrated as a part of the SUIF compiler system [144]. Other advanced optimizations such as loop transformations [146], data and computation co-location [13], data transformations (Chapter 8), synchronization elimination [140], compiler-directed page coloring [30], and compiler-inserted prefetching <ref> [116] </ref> have also been implemented in the SUIF compiler system. Detection of coarse-grain parallelism, in combination with these other optimizations, can achieve significant performance improvements for sequential scientific applications on multiprocessors. The SUIF compiler system has demonstrated this by obtaining the highest known SPEC92fp and SPEC95fp ratios to date [8,73].
Reference: [117] <author> E. Myers. </author> <title> A precise inter-procedural data flow algorithm. </title> <booktitle> In Conference Record of the Eighth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Williamsburg, VA, </address> <month> January </month> <year> 1981. </year>
Reference-contexts: Traditional intraprocedural data-flow frameworks are ow-sensitive. That is, they derive data-flow results along each possible control flow path through the procedure. Straightforward interprocedural adaptation of flow-sensitive intraprocedural analysis is not sufficient to maintain the same precision over the entire program. For example, interprocedural analysis using the supergraph <ref> [117] </ref> program representation, where individual control flow graphs for the procedures in the program are linked together at procedure call and return points, loses context sensitivity by propagating information along unrealizable paths [104].
Reference: [118] <author> J. Palmer and G. Steele, Jr. </author> <title> Connection Machine model CM-5 system overview. </title> <booktitle> In Frontiers 92: The 4th Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <address> McLean, VA, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: The technique to eliminate this redundancy is similar to that of removing redundant communication due to self-reuse. 9.5.2. Communication Aggregation Whether aggregation of small messages into large messages is necessary depends on the machine architecture. For example, machines such as the iWarp [65] and CM-5 <ref> [118] </ref> support fine-grain communication, while machines such as the Intel iPSC have significant overhead in processing every message.
Reference: [119] <author> W. Pugh. </author> <title> The Omega test: A fast and practical integer programming algorithm for dependence analysis. </title> <booktitle> In Proceedings of Supercomputing 91, </booktitle> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year>
Reference: [120] <author> W. Pugh. </author> <title> A practical algorithm for exact array dependence analysis. </title> <journal> Communications of the ACM, </journal> <volume> 35(8):102114, </volume> <month> August </month> <year> 1992. </year>
Reference: [121] <author> W. Pugh and D. Wonnacott. </author> <title> Eliminating false data dependences using the Omega test. </title> <booktitle> In Proceedings of the SIGPLAN 92 Conference on Programming Language Design and Implementation, </booktitle> <address> San Francisco, CA, </address> <month> June </month> <year> 1992. </year>
Reference: [122] <author> H. Ribas. </author> <title> Obtaining dependence vectors for nested-loop computations. </title> <booktitle> In Proceedings of the 1990 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1990. </year>
Reference: [123] <author> A. Rogers and K. Pingali. </author> <title> Process decomposition through locality of reference. </title> <booktitle> In Proceedings of the SIGPLAN 89 Conference on Programming Language Design and Implementation, </booktitle> <address> Portland, OR, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: Related Work There is a large body of research on language extensions and compiler support for distributed memory machines. Some notable projects are, Al [141], Blaze [100], Crystal [106], FORTRAN-D [85,139], Id Nouveau <ref> [123] </ref>, Kali [114,98], Pandore [15], Pandore II [14], p s i s 1 p r i s k i r a, , , , , , , , , i s k 1 i s n i r i r k 1 i r n i r n p s i <p> Second, we have developed several communication optimizations within the same unified framework. These optimizations include eliminating redundant messages, aggregating messages, and hiding the communication latency by overlapping the communication with computation. These optimizations are essential to achieving an acceptable performance on distributed memory machines <ref> [123] </ref>. Third, we have proposed a value-centric approach to deriving the fine-grain communication for machines with a distributed address space. Previous approaches are location-centric: communication is derived from data decompositions; optimizations are performed using data dependence tests [19], an analysis that determines if accesses may refer to the same location.
Reference: [124] <author> C. Rosene. </author> <title> Incremental Dependence Analysis. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> March </month> <year> 1990. </year>
Reference: [125] <author> R. G. Scarborough and H. G. Kolsky. </author> <title> A vectorizing Fortran compiler. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 30(2):163171, </volume> <month> March </month> <year> 1986. </year>
Reference: [126] <author> M. Schlansker and M. McNamara. </author> <title> The Cydra 5 computer system architecture. </title> <booktitle> In Proceedings of the 1988 IEEE International Conference on Computer Design: VLSI in Computers and Processors (ICCD 88), </booktitle> <month> October </month> <year> 1988. </year>
Reference-contexts: However, it has been notoriously difficult to use caches effectively for numeric applications. In fact, various past machines 2 built for scientific computationssuch as the Cray C90, Cydrome Cydra-5 <ref> [126] </ref> and the Multiflow Trace [42]were all built without caches. However, current multiprocessor systems include complex memory hierarchies and multiple levels of caches. Given that the processor-memory gap continues to widen, exploiting the memory hierarchy is critical to achieving high performance on modern architectures. 1.1. Thesis Overview 1.1.1.
Reference: [127] <author> A. Schrijver. </author> <title> Theory of Linear and Integer Programming. </title> <publisher> John Wiley and Sons, </publisher> <address> Chichester, Great Britain, </address> <year> 1986. </year>
Reference-contexts: To check if a system has an integer solution, we again use Fourier-Motzkin elimination. Since the Fourier-Motzkin elimination algorithm checks if a real solution exists for a system, a branch-and-bound technique is needed to check for the existence of an integer solution <ref> [127] </ref>. Each integer point in the original polyhedron is mapped to an integer point in the polyhedron created by the projection operation. However, the projected polyhedron may contain integer points with no corresponding points in the original polyhedron. If then . <p> But there is only a single integer solution, and , which can be found by integer programming <ref> [127] </ref>. This property of integer systems allows us to precisely extract many of the simple reshape regions that occur in practice. By using this algorithm on the reshape in Figure 6-5, we can determine that the result of the reshape is a simple plane of the array U.
Reference: [128] <author> M. Sharir and A. Pnueli. </author> <title> Two approaches to interprocedural data flow analysis. </title> <editor> In S. Muchnick and N.D. Jones, editors, </editor> <title> Program Flow Analysis: Theory and Applications. </title> <publisher> Prentice Hall Inc, </publisher> <year> 1981. </year> <month> 217 </month>
Reference: [129] <author> T. J. Sheffler, R. Schreiber, J. R. Gilbert, and S. Chatterjee. </author> <title> Aligning parallel arrays to reduce communication. </title> <booktitle> In Frontiers 95: The 5th Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 324331, </pages> <address> McLean, VA, </address> <month> February </month> <year> 1995. </year>
Reference: [130] <author> O. Shivers. </author> <title> Control-Flow Analysis of higher-order languages. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <address> Pittsburgh, PA, </address> <month> May </month> <year> 1991. </year>
Reference: [131] <author> J. P. Singh and J. L. Hennessy. </author> <title> An empirical investigation of the effectiveness of and limitations of automatic parallelization. </title> <booktitle> In Proceedings of the International Symposium on Shared Memory Multiprocessors, </booktitle> <address> Tokyo, Japan, </address> <month> April </month> <year> 1991. </year>
Reference: [132] <author> J.P. Singh, T. Joe, A. Gupta, and J. L. Hennessy. </author> <title> An empirical comparison of the Kendall Square Research KSR-1 and Stanford DASH multiprocessors. </title> <booktitle> In Proceedings of Supercomputing 93, </booktitle> <pages> pages 214225, </pages> <address> Portland, OR, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: Singh et al. explored performance issues on scalable shared address space architectures; they improved cache behavior by transforming two-dimensional arrays into four-dimensional arrays so that each processors local data are contiguous in memory <ref> [132] </ref>. Torrellas et al. [135] and Eggers et al. [50,51] also showed that improving spatial locality and reducing false sharing resulted in significant speedups for a set of programs on shared-memory machines. 8.2.1.
Reference: [133] <author> Stanford SUIF Compiler Group. </author> <title> SUIF: A parallelizing & optimizing research compiler. </title> <type> Technical Report CSL-TR-94-620, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <month> May </month> <year> 1994. </year>
Reference: [134] <author> O. Temam, E. D. Granston, and W. Jalby. </author> <title> To copy or not to copy: A compile-time technique for assessing when data copying should be used to eliminate cache conflicts. </title> <booktitle> In Proceedings of Supercomputing 93, </booktitle> <pages> pages 410419, </pages> <address> Portland, OR, </address> <month> November </month> <year> 1993. </year>
Reference: [135] <author> J. Torrellas, M. S. Lam, and J. L. Hennessy. </author> <title> Shared data placement optimizations to reduce multiprocessor cache miss rates. </title> <booktitle> In Proceedings of the 1990 International Conference on Parallel Processing, </booktitle> <pages> pages 266270, </pages> <address> St. Charles, IL, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: Singh et al. explored performance issues on scalable shared address space architectures; they improved cache behavior by transforming two-dimensional arrays into four-dimensional arrays so that each processors local data are contiguous in memory [132]. Torrellas et al. <ref> [135] </ref> and Eggers et al. [50,51] also showed that improving spatial locality and reducing false sharing resulted in significant speedups for a set of programs on shared-memory machines. 8.2.1.
Reference: [136] <author> E. Torrie, C-W. Tseng, M. Martonosi, and M. W. Hall. </author> <title> Evaluating the impact of advanced memory systems on compiler-parallelized codes. </title> <booktitle> In Proceedings of the International Conference on Parallel Architectures and Compilation Techniques (PACT), </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: To focus on the memory hierarchy issues, our benchmark suite includes only those programs that exhibit a significant amount of parallelism. Several of these programs were identified as having memory performance problems in a simulation study <ref> [136] </ref>. We compiled each program under each of the methods described below. The compiler steps are given in Figure 8-23. We plot the speed up of the parallelized code on the DASH machine.
Reference: [137] <author> R. Triolet, F. Irigoin, and P. Feautrier. </author> <title> Direct parallelization of CALL statements. </title> <booktitle> In Proceedings of the SIGPLAN 86 Symposium on Compiler Construction, </booktitle> <address> Palo Alto, CA, </address> <month> June </month> <year> 1986. </year>
Reference-contexts: In order to perform the aggressive whole program analysis, required to find coarse-grain parallelism, the compiler must analyze the programs in the presence of array reshapes to determine their effect on the rest of the analysis. Previously, array reshapes were handled only within a limited domain <ref> [137] </ref>. We have developed a linear inequalities-based algorithm that can analyze a large class of array reshapes. Using these advanced array analysis techniques we have developed a fully functional inter-procedural parallelizer in the Stanford SUIF compiler system that is capable of detecting coarse-grain parallelism. <p> Related Work Researchers have used integer and linear programming techniques to solve many individual problems in parallelizing compilers. For example, compiler problems such as exact data-dependence analysis [110,119], array analysis based on array summary information <ref> [137] </ref>, instruction scheduling for superscalars [5], automatic data layout for minimizing communication [21], and code generation after loop transformations [10,11] have been solved using linear and integer programming. <p> Triolet et al. first proposed using a system of linear inequalities to represent an array index set <ref> [137] </ref>. This representation was used for data dependences analysis. Their algorithm did not create exact convex regions in many situations, such as sparse access patterns, but provided approximations using a convex hull of all the indices. <p> However, performing array analyses using these linearized accesses is more complex than using multi-dimensional arrays. For example, in array data-flow analysis, many simple regions in multi-dimensional arrays get converted into complex lattice patterns in a one-dimensional linearized space. Simple array reshape analysis is used in a few interprocedural analyzers <ref> [137] </ref>. Their scope was limited to a class of reshapes where the formal array declaration is identical to the lower dimensions of the actual array. These simple reshapes are performed by including the upper dimension information of the actual array with the renamed array section of the formal array.
Reference: [138] <author> R. Triolet, F. Irigoin, and P. Feautrier. </author> <title> Direct parallelization of call statements. </title> <booktitle> In Proceedings of the SIGPLAN 86 Symposium on Compiler Construction, m SIGPLAN Notices 21(7), </booktitle> <pages> pages 176185. </pages> <publisher> ACM, </publisher> <month> July </month> <year> 1986. </year>
Reference: [139] <author> C.-W. Tseng. </author> <title> An Optimizing Fortran D Compiler for MIMD Distributed-Memory Machines. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> January </month> <year> 1993. </year>
Reference: [140] <author> C-W. Tseng. </author> <title> Compiler optimizations for eliminating barrier synchronization. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 144155, </pages> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: The parallelizer is integrated as a part of the SUIF compiler system [144]. Other advanced optimizations such as loop transformations [146], data and computation co-location [13], data transformations (Chapter 8), synchronization elimination <ref> [140] </ref>, compiler-directed page coloring [30], and compiler-inserted prefetching [116] have also been implemented in the SUIF compiler system. Detection of coarse-grain parallelism, in combination with these other optimizations, can achieve significant performance improvements for sequential scientific applications on multiprocessors. <p> Two of these programs, tomcatv and nasa7, have poor memory behavior. We will show that the performance of these programs can be improved significantly via data and loop transformations to improve cache locality (Chapter 8), and by using techniques to minimize synchronization <ref> [140] </ref>. 7.4.3.3. Nas benchmarks We have gathered results for the Nas benchmark on two different multiprocessors using two different datasets as given in Figures 7-9 and 7-10. The results on the different multiprocessors are quite similar. <p> The code generator also takes advantage of this information to minimize synchronization in the parallel program <ref> [140] </ref>. The data layouts are left unchanged. c) DataTrans: Finally, we include the data transformations described in this chapter. Using the data decompositions calculated by the communication minimization algorithm [13], the compiler reorganizes the arrays in the parallelized code to improve spatial locality. <p> This framework is being used at Stanford and elsewhere for developing many other compiler algorithms, such as synchronization optimizations <ref> [140] </ref>, interprocedural propagation in computation and data co-location information, predicated data-ow analysis, and communication analysis for software DSM [91]. This framework allowed us to rapidly prototype and test many algorithms and ideas for next-generation compilers.
Reference: [141] <author> P.-S. Tseng. </author> <title> A parallelizing compiler for distributed memory parallel computers. </title> <booktitle> In Proceedings of the SIGPLAN 90 Conference on Programming Language Design and Implementation, </booktitle> <address> White Plains, NY, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: If the bounds of are independent of , the data sent to each processor are identical. 9.6. Related Work There is a large body of research on language extensions and compiler support for distributed memory machines. Some notable projects are, Al <ref> [141] </ref>, Blaze [100], Crystal [106], FORTRAN-D [85,139], Id Nouveau [123], Kali [114,98], Pandore [15], Pandore II [14], p s i s 1 p r i s k i r a, , , , , , , , , i s k 1 i s n i r i r k 1
Reference: [142] <author> P. Tu. </author> <title> Automatic Array Privatization and Demand-Driven Symbolic Analysis. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <year> 1995. </year> <month> 218 </month>
Reference-contexts: Another implementation of interprocedural array data-flow analysis can be found in the Polaris parallelizing compiler [22,25]. In their algorithm, array privatization is applied only to the cases where all the values used in an iteration are defined before they are used in the same iteration <ref> [142] </ref>. However, array privatization is also applicable to loops in which iterations use values computed outside the loop, where the private copies must be initialized with these values before parallel execution begins. Our algorithm identifies privatizable arrays that require initialization. 4.7. <p> These algorithms use different forms of regular section descriptors as the array index set representation. Each regular section can be used only to precisely represent a limited domain of rectilinear, triangular or diagonal spaces [81]. More complex spaces can be represented using multiple regular sections <ref> [142] </ref>. The scope of data-flow and data-dependence analysis performed using regular section information is much more restricted than using a representation based on linear inequalities.
Reference: [143] <author> J. Uniejewski. </author> <title> SPEC Benchmark Suite: Designed for todays advanced systems. </title> <journal> SPEC Newsletter Volume 1, </journal> <note> Issue 1, SPEC, Fall 1989. </note>
Reference-contexts: The example in Figure 6-3 is extracted from the program hydro2d in the SPEC92fp benchmark suite <ref> [143] </ref>. The common block var1 in the example has two different definitions, one with four two-dimensional arrays of elements and the other with a single large vector. Thus, the array element in procedure INIVAL is the same element accessed by in procedure ASW02.
Reference: [144] <author> R. P. Wilson, R. S. French, C. S. Wilson, S. P. Amarasinghe, J. M. Anderson, S. W. K. Tjiang, S.-W. Liao, C.-W. Tseng, M. W. Hall, M. S. Lam, and J. L. Hennessy. </author> <title> SUIF: An infrastructure for research on parallelizing and optimizing compilers. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 29(12):3137, </volume> <month> December </month> <year> 1994. </year>
Reference-contexts: We have developed an interprocedural parallelizer with advanced array analyses and optimizations, that is capable of detecting coarse-grain parallelism [75,76,71]. The parallelizer is integrated as a part of the SUIF compiler system <ref> [144] </ref>. Other advanced optimizations such as loop transformations [146], data and computation co-location [13], data transformations (Chapter 8), synchronization elimination [140], compiler-directed page coloring [30], and compiler-inserted prefetching [116] have also been implemented in the SUIF compiler system.
Reference: [145] <author> Robert P. Wilson and Monica S. Lam. </author> <title> Efficient context-sensitive pointer analysis for C programs. </title> <booktitle> In Proceedings of the SIGPLAN 95 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 112, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Scientific applications require the development of compilers that identify coarse 204 grain parallelism and perform memory optimizations. Non-scientific applications written in languages such as C and C++, require development of compilers with advanced techniques such as pointer alias analysis <ref> [145] </ref>. This thesis represents a step towards making multiprocessors accepted as general purpose computers. In this thesis, we have developed a set of compiler techniques that extract parallel performance from sequential dense matrix scientific applications.
Reference: [146] <author> M. E. Wolf. </author> <title> Improving Locality and Parallelism in Nested Loops. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Stanford University, </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: We have developed an interprocedural parallelizer with advanced array analyses and optimizations, that is capable of detecting coarse-grain parallelism [75,76,71]. The parallelizer is integrated as a part of the SUIF compiler system [144]. Other advanced optimizations such as loop transformations <ref> [146] </ref>, data and computation co-location [13], data transformations (Chapter 8), synchronization elimination [140], compiler-directed page coloring [30], and compiler-inserted prefetching [116] have also been implemented in the SUIF compiler system.
Reference: [147] <author> M. E. Wolf and M. S. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In Proceedings of the SIGPLAN 91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 3044, </pages> <address> Toronto, Canada, </address> <month> June </month> <year> 1991. </year>
Reference: [148] <author> M. E. Wolf and M. S. Lam. </author> <title> A loop transformation theory and an algorithm to maximize parallelism. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4):452471, </volume> <month> October </month> <year> 1991. </year>
Reference: [149] <author> M. J. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference-contexts: Data dependence analysis is performed on each pair of references to the same array, where two references are said to be dependent if any of the locations accessed by one reference is also accessed by the other <ref> [149] </ref>. A dependence is said to be loop-carried by a loop if the dependence occurs between two iterations for the same instance of the loop. Thus, according to the data dependence test, a loop can be parallelized if there are no loop-carried data dependences. <p> These analyses are performed intraprocedurally; thus a procedure call within the loop will eliminate it as a candidate for parallel execution. Since the presence of any scalar definition within the loop creates a loop-carried dependence, the parallelizer attempts to eliminate this dependence by applying scalar privatization or scalar reduction <ref> [149] </ref>. When the scalar value used in each iteration is created within the same iteration, the loop-carried data dependence can be eliminated by giving each processor a private copy of the variable.
Reference: [150] <author> X3J3 Subcommittee. </author> <title> American National Standard Programming Language Fortran (X3.9-1978). </title> <institution> American National Standards Institute, </institution> <address> New York, NY, </address> <year> 1978. </year>
Reference-contexts: Parallelism in Sequential Scientific Programs Scientific applications are typically computationally intensive, and thus can benefit immensely from parallelization. The domain of applications we are interested is dense matrix scientific applications written in FORTRAN <ref> [150] </ref>. Within this domain, loop nests dominate the computation and multi-dimensional arrays hold most of the data structures. A parallelizing compiler determines loops that can be parallelized by analyzing accesses to scalar and array variables within the loops. <p> In this segment, a three-dimensional array U in the caller is treated as a vector in DCFT. The FORTRAN-77 standard allows array reshapes with equivalence statements, in parameter passing, and with different common block definitions <ref> [150] </ref>. To perform the aggressive whole program anal DIMENSION U (66,64,64) ... CALL DCFT (U (1,1,K),33) ... CALL DCFT (U (1,J,1),33*64) ... <p> Reshapes in FORTRAN A reshape occurs when a data structure defined using one shape is also accessed using a different shape within the program. The FORTRAN-77 definition allows three classes of reshapes: parameter reshapes, equivalences, and different common block declarations <ref> [150] </ref>. Equivalences can affect intraprocedural analysis while the other two affect only interprocedural analysis. 6.1.1. Parameter Reshapes The FORTRAN-77 definition does not restrict the actual parameters of the caller and the formal parameter of the corresponding callee to be of the same type.

References-found: 150

