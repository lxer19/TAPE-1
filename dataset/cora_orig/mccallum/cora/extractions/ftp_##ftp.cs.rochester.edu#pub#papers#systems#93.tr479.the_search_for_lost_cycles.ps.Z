URL: ftp://ftp.cs.rochester.edu/pub/papers/systems/93.tr479.the_search_for_lost_cycles.ps.Z
Refering-URL: http://www.cs.rochester.edu/u/leblanc/pubs.html
Root-URL: 
Email: fcrovella,leblancg@cs.rochester.edu  
Title: The Search for Lost Cycles: A New Approach to Parallel Program Performance Evaluation  
Author: Mark E. Crovella and Thomas J. LeBlanc 
Date: December 1993  
Address: Rochester, New York 14627  
Affiliation: The University of Rochester Computer Science Department  
Pubnum: Technical Report 479  
Abstract: Traditional performance debugging and tuning of parallel programs is based on the "measure-modify" approach, in which detailed measurements of program executions are used to guide incremental changes to the program that result in better performance. Unfortunately, the performance of a parallel algorithm is often related to its implementation, input data, and machine characteristics in surprising ways, and the "measure-modify" approach is unsuited to exploring these relationships fully: it is too heavily dependent on experimentation and measurement, which is impractical for studying the large number of variables that can affect parallel program performance. In this paper we argue that the problem of selecting the best implementation of a parallel algorithm requires a new approach to parallel program performance evaluation, one with a greater balance between measurement and modeling. We first present examples that demonstrate that different parallelizations of a program may be necessary to achieve the best possible performance as one varies the input data, machine architecture, or number of processors used. We then present an approach to performance evaluation based on lost cycles analysis, which involves measurement and modeling of all sources of overhead in a parallel program. We describe a measurement tool for lost cycles analysis that we have incorporated into the runtime environment for Fortran programs on the Kendall Square KSR1, and use this tool to analyze the performance tradeoffs among implementations of 2D FFT and parallel subgraph isomorphism. Using these examples, we show how lost cycles analysis can be used to solve the problems associated with selecting the best implementation in a variable environment. In addition, we show that this approach can capture large amounts of performance data using only a small number of measurements, and that it is flexible enough to allow conclusions to be drawn from empirical data in some cases, and analytic results in other cases. This research was supported under NSF CISE Institutional Infrastructure Program Grant No. CDA-8822724, and ONR Contract No. N00014-92-J-1801 (in conjunction with the DARPA HPCC program, ARPA Order No. 8930). Mark Crovella is supported by an ARPA Research Assistantship in High Performance Computing administered by the Institute for Advanced Computer Studies, University of Maryland. 
Abstract-found: 1
Intro-found: 1
Reference: [Abrams et al., 1992] <author> Marc Abrams, Naganand Doraswamy, and Anup Mather, "Chitra: </author> <title> Visual Analysis of Parallel and Distributed Programs in the Time, Event, and Frequency Domains," </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 3(6) </volume> <pages> 672-685, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: Other methods model overheads in highly empirical ways that are difficult to generalize <ref> [Abrams et al., 1992; Dimpsey and Iyer, 1991] </ref>. These methods develop categories of execution state based solely on program events; as a result it is difficult to predict how time spent in these categories would change with changes in the environment.
Reference: [Alverson and Notkin, 1992] <author> Gail A. Alverson and David Notkin, </author> <title> "Abstracting Data-Representation and Partitioning-Scheduling in Parallel Programs," </title> <editor> In N. Suzuki, editor, </editor> <booktitle> Shared Memory Multiprocessing, </booktitle> <pages> pages 315-338. </pages> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference: [Amdahl, 1967] <author> G. M. </author> <title> Amdahl, "The Validity of the Single Processor Approach to Achieving Large Scale Computing Capabilities," </title> <booktitle> In AFIPS Conference Proceedings, </booktitle> <volume> volume 20, </volume> <pages> pages 483-485. </pages> <publisher> AFIPS Press, </publisher> <address> Reston, Va., </address> <month> April </month> <year> 1967. </year>
Reference-contexts: Most of the models we use are straightforward: pure computation does not vary as we vary processors, insufficient parallelism obeys Amdahl's Law <ref> [Amdahl, 1967] </ref>, and synchronization loss is zero. Load imbalance can arise in two ways: variation in the running time of each loop iteration, and unequal numbers of loop iterations handled by different processors.
Reference: [Anderson and Lazowska, 1990] <author> Thomas E. Anderson and Edward D. Lazowska, </author> <month> "Quartz: </month>
Reference-contexts: In particular the PEM system 6 has developed a taxonomy of parallel overheads similar to ours [Burkhart and Millen, 1989], and Quartz and MemSpy together can measure the overhead categories we use <ref> [Anderson and Lazowska, 1990; Martonosi et al., 1992] </ref>. However, since these (and similar tool sets) are not oriented toward performance prediction of alternative implementations, the specific overhead categories they use are not always amenable to easy analysis.
References-found: 4

