URL: http://aries.www.media.mit.edu/people/aries/ms-thesis.ps.gz
Refering-URL: http://aries.www.media.mit.edu/people/aries/
Root-URL: http://www.media.mit.edu
Title: Evolving Visual Routines  Architecture and Planning,  
Author: by Michael Patrick Johnson Stephen A. Benton 
Degree: in partial fulfillment of the requirements for the degree of MASTER OF SCIENCE IN MEDIA ARTS AND SCIENCES at the  All Rights Reserved Signature of Author Program in Media Arts and Sciences  Certified by Pattie Maes Associate Professor of Media Arts and Sciences Thesis Supervisor Accepted by  Chairperson Departmental Committee on Graduate Students  
Note: Submitted to the Program in  c Massachusetts Institute of Technology,  
Date: June 1993  September 1995  1995  11 August 1995  
Address: Cambridge, MA  
Affiliation: B.S., Computer Science Massachusetts Institute of Technology,  Media Arts and Sciences, School of  Massachusetts Institute of Technology  
Abstract-found: 0
Intro-found: 1
Reference: [ Aho et al., 1986 ] <author> Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. </author> <booktitle> Compilers: Principles, Techniques and Tools. </booktitle> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, Massachusetts, </address> <year> 1986. </year>
Reference-contexts: One way is to apply dead-code removal algorithms to the programs. Examples of these are common in compiler optimizers (see, for example, <ref> [ Aho et al., 1986 ] </ref> ). Unfortunately, dead code removal can be tricky and inefficient for side-effecting code since much more state must be examined.
Reference: [ Aloimonos, 1993 ] <author> Yiannis Aloimonos. </author> <title> Active Perception. </title> <publisher> Lawrence Erlbaum Associates, Inc., </publisher> <address> Hillsdale, </address> <year> 1993. </year>
Reference-contexts: This model, known as general vision, has been shown to be intractible in practice. Active vision is a new paradigm for machine vision that has received significant attention recently <ref> [ Ballard, 1989, Aloimonos, 1993 ] </ref> . <p> Active Vision is a young field which is evolving in many ways. Rather than go into a detailed discussion of the various areas of research, the reader is referred to collections such as <ref> [ Aloimonos, 1993 ] </ref> and [ Blake and Yuille, 1992 ] . In the next section we describe Ullman's visual routines model.
Reference: [ Andre, 1994 ] <author> David Andre. </author> <title> Automatically defined feautures the simultaneous evolution of 2-dimensional feature detectors and an algorithm for using them. </title> <editor> In Kim Kinnear, editor, </editor> <booktitle> Advances in Genetic Programming, chapter 23. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1994. </year>
Reference-contexts: Jeeves by Horswill [ Horswill, 1995 ] and the Meliora system by Whitehead [ Whitehead, 1992 ] actually use the visual routines model, but neither uses GP techniques. Other GP researchers have applied GP to visual recognition tasks, such as Koza [ Koza, 1994 ] and David Andre <ref> [ Andre, 1994 ] </ref> , who both evolved programs for Optical Character Recognition, and Astro Teller [ Teller and Veloso, 1995 ] , who evolved 28 programs for classifying single objects from greyscale images. Finally, Harvery, Cliff and Husbands used GP to perform visual tasks on a real robot. <p> Unfortunately, the perfect solutions were rare in the sense that many runs did not find one and the population size was large, containing 8000 individuals. David Andre performed similar but more in-depth research on the problem of character recognition <ref> [ Andre, 1994 ] </ref> . The most interesting facet of his work is that he performed evolution of feature detectors and decision trees that utilized them by using a string-based GA and GP simultaneously. The GA was used to evolve three by three feature detecting templates.
Reference: [ Ballard and Whitehead, 1990 ] <author> Dana Ballard and Steven Whitehead. </author> <title> Active perception and reinforcement learning. </title> <booktitle> Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <year> 1990. </year>
Reference-contexts: Several researchers have proposed ways of allowing the agent's central control system to actively modulate what is processed by the vision system in a task-dependent manner (e.g. <ref> [ Ballard and Whitehead, 1990 ] </ref> ). For example, if I want to pick up the coffee cup on my desk, I do not need to process and represent all the paper clips, crumbs, every character on every paper, and all the rest of the clutter usually on my desk.
Reference: [ Ballard, 1989 ] <author> Dana Ballard. </author> <title> Reference frames for animate vision. </title> <booktitle> Proceedings of IJCAI-89 Conference, </booktitle> <address> Detroit, </address> <year> 1989. </year>
Reference-contexts: This model, known as general vision, has been shown to be intractible in practice. Active vision is a new paradigm for machine vision that has received significant attention recently <ref> [ Ballard, 1989, Aloimonos, 1993 ] </ref> .
Reference: [ Blake and Yuille, 1992 ] <author> Andrew Blake and Alan Yuille, </author> <title> editors. Active Vision. </title> <booktitle> Artificial Intelligence. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1992. </year>
Reference-contexts: Active Vision is a young field which is evolving in many ways. Rather than go into a detailed discussion of the various areas of research, the reader is referred to collections such as [ Aloimonos, 1993 ] and <ref> [ Blake and Yuille, 1992 ] </ref> . In the next section we describe Ullman's visual routines model.
Reference: [ Chapman, 1992 ] <author> David Chapman. </author> <title> Vision, Instruction and Action. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1992. </year>
Reference-contexts: This person can design visual routines that solve a given task, test them and refine them, hopefully reaching an optimal solution. This may be a reasonable approach in limited cases, such as the Sonja system by David Chapman <ref> [ Chapman, 1992 ] </ref> , which has a set of routines designed by Chapman for solving different visual tasks involved in playing a particular video game. The set of routines is fairly small and fixed, allowing a programmer to work them out by hand. <p> In the next section we describe Ullman's visual routines model. The visual routines model was not originally designed as an active vision theory, but many ideas from it have been incorporated into active vision systems by researchers such as Chapman <ref> [ Chapman, 1992 ] </ref> and Whitehead [ Whitehead, 1992 ] . 2.1.1 Visual Routines Theory One example of a task-dependent theory of vision is Shimon Ullman's Visual Routines theory of intermediate vision [ Ullman, 1984, Ullman, 1987 ] . <p> Following Ullman, David Chapman applied similar operations to the domain of visual attention in his Sonja video game playing system <ref> [ Chapman, 1992 ] </ref> . For example, Sonja can find gaps in a boundary (ostensibly doors), track moving pop-out objects (such as the player's icon) with a marker, color a region and cast rays. In the game, characters move around a two-dimensional world full of impenetrable walls. <p> Return-inhibition maps keep track of which pixels have been visited so that no pixel is visited twice. Chapman, for example, uses them to implement visual search operators <ref> [ Chapman, 1992 ] </ref> . The problem is that evolved programs tend to not use them correctly, leading to infinite loops. Iterators were created to abstract the return-inhibition maps so that they were useful with GP. A short example will help prepare the reader for the discussion of iterators. <p> Each blob will only be indexed once. Also, points are indexed in an arbitrary order. These iterators contain an inhibition map in order to only return each region once. Inhibition maps are described in [ Ullman, 1984 ] and <ref> [ Chapman, 1992 ] </ref> . They are basically bitmaps which store whether a point is inhibited or not. Points are inhibited when they are visited. Thus, an uninhibited point has not been visited. When invoked, the iterator looks for an uninhibited point in the property map specified. <p> This is important if the claim that GP is a good way of solving multiple active vision tasks more efficiently than by programming them by hand is to be demonstrated. We believe the operators are reusable since Chapman successfully used them for many different visual tasks <ref> [ Chapman, 1992 ] </ref> . 62 This was the driving force behind performing the imperative tests even after the functional tests were successful. This will be discussed in depth in Section 5.10. This chapter is divided into three main sections experimental methodology, functional results and imperative results. <p> Ullman argues that the visual routine primitives will have these properties. The fact that these primitives, which Chapman used for many different tasks in Sonja <ref> [ Chapman, 1992 ] </ref> , can be composed to solve our task as well is an important result. This implies that these general, low-levels primitives can be reused for many different visual tasks. Although this is not explicitly shown here, the evidence for it is strong.
Reference: [ Cliff et al., 1993a ] <author> Dave Cliff, Phil Husbands, and Inman Harvey. </author> <booktitle> Explorations in evolutionary robotics. Adaptive Behavior, </booktitle> <volume> 2(1) </volume> <pages> 73-110, </pages> <month> Summer </month> <year> 1993. </year>
Reference-contexts: Harvey, Husbands and Cliff used a genetic algorithm to evolve the morphology and 30 control of a vision system for a real robot [ Harvey et al., 1994 ] . This is an extension of research in [ Cliff et al., 1993b ] and <ref> [ Cliff et al., 1993a ] </ref> . The robot has a CCD camera which digitizes the world in front of it. Control structures in the form of networks are evolved to solve the task of moving the robot towards a set of targets of various sizes using the vision system.
Reference: [ Cliff et al., 1993b ] <author> Dave T. Cliff, Phil Husbands, and Inman Harvey. </author> <title> Evolving visually guided robots. </title> <editor> In J.-A. Meyer, H. Roitblat, and S. Wilson, editors, </editor> <booktitle> Proceedings of the 114 second international conference on Simulation of Adaptive Behavior (SAB 92), </booktitle> <pages> pages 374-383, </pages> <year> 1993. </year>
Reference-contexts: Programs had to be evolved for each digit to be classified. Harvey, Husbands and Cliff used a genetic algorithm to evolve the morphology and 30 control of a vision system for a real robot [ Harvey et al., 1994 ] . This is an extension of research in <ref> [ Cliff et al., 1993b ] </ref> and [ Cliff et al., 1993a ] . The robot has a CCD camera which digitizes the world in front of it.
Reference: [ Goldberg, 1989 ] <author> David E. Goldberg. </author> <title> Genetic Algorithms in Search, Optimization and Machine Learning. </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: In particular, they seem well suited to machine learning and difficult function optimization problems. A full discussion of string-based GAs is beyond the scope of this thesis. For a good discussion of the use of string-based GAs for these types of problems, see <ref> [ Goldberg, 1989 ] </ref> . 21 2.2.2 Basic Genetic Programming A tree genotype Genetic Programming in its current form is attributed to John Koza. Koza decided that using fixed strings of characters as a genotype was too limiting [ Koza, 1992 ] .
Reference: [ Harvey et al., 1994 ] <author> Inman Harvey, Phil Husbands, and Dave Cliff. </author> <title> Seeing the light: Artificial evolution, real vision. </title> <editor> In Dave Cliff, Phil Husbands, Jean-Arcady Meyer, and Stewart W. Wilson, editors, </editor> <booktitle> From Animals to Animats 3: Proceedings of the Third In ternational Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pages 392-401, </pages> <address> Cambridge, Massachusetts, 1994. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Programs had to be evolved for each digit to be classified. Harvey, Husbands and Cliff used a genetic algorithm to evolve the morphology and 30 control of a vision system for a real robot <ref> [ Harvey et al., 1994 ] </ref> . This is an extension of research in [ Cliff et al., 1993b ] and [ Cliff et al., 1993a ] . The robot has a CCD camera which digitizes the world in front of it.
Reference: [ Hogger and Kowalski, 1992 ] <author> C. J. Hogger and R. A. Kowalski. </author> <title> Logic programming. </title> <editor> In Stuart C. Shapiro, editor, </editor> <booktitle> Encyclopedia of Artificial Intelligence, </booktitle> <pages> pages 873-891. </pages> <publisher> John Wiley & Sons, Inc., </publisher> <address> New York, </address> <note> second edition, </note> <year> 1992. </year>
Reference-contexts: Horswill presents an automata-theoretic analysis of the computational power of the visual routines theory. His analysis assumes that visual routines are used to solve Horn clauses, or simple logic statements, which refer to properties and relations in the image (for an introduction to Horn clauses, see for example <ref> [ Hogger and Kowalski, 1992 ] </ref> ). He introduces enumeration oracles and explains how they can be used to solve visual queries expressed as Horn clauses.
Reference: [ Holland, 1975 ] <author> John H. Holland. </author> <title> Adaptation in natural and artificial systems. </title> <publisher> University of Michigan Press, </publisher> <address> Ann Arbor, MI, </address> <year> 1975. </year>
Reference-contexts: Add q 1 and q 2 to P i+1 . (d) Increment i. 2.2 Genetic Programming Genetic Programming (GP) is a powerful new method for automatically inducing computer programs to solve tasks [ Koza, 1992 ] . GP evolved from the original Genetic Algorithm (GA) invented by Holland <ref> [ Holland, 1975 ] </ref> .
Reference: [ Horswill, 1993 ] <author> Ian D. Horswill. </author> <title> Specialization of Perceptual Processes. </title> <type> PhD dissertation, </type> <institution> Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: This is an example of using task-dependent routines only when they are needed. Indeed, such active vision methods have been shown to be successful in mobile robots. For example, Horswill's Polly <ref> [ Horswill, 1993 ] </ref> robot was able to navigate through the corridors of a building, avoid obstacles and give people tours, all in real-time. Active Vision is a young field which is evolving in many ways.
Reference: [ Horswill, 1995 ] <author> Ian Horswill. </author> <title> Visual routines and visual search: a real-time implementation and an automata-theoretic analysis. </title> <booktitle> Accepted for International Joint Conference on Artificial Intelligence (IJCAI-95), </booktitle> <year> 1995. </year>
Reference-contexts: Many GP researchers use the values used by Koza [ Koza, 1992 ] for most of his experiments. 2.3 Related Work Recently, several other researchers have been investigating automatic ways of creating task-specific visual routines. Jeeves by Horswill <ref> [ Horswill, 1995 ] </ref> and the Meliora system by Whitehead [ Whitehead, 1992 ] actually use the visual routines model, but neither uses GP techniques. <p> Each will be described in turn. Jeeves, created by Ian Horswill, is the first demonstration of a real-time visual routines processor which operates on real camera images rather than on hand-drawn bitmaps or internal simulation structures <ref> [ Horswill, 1995 ] </ref> . Jeeves processes the camera image into a set of low-level maps and performs a pre-attentive segmentation step to divide the image into distinct regions. Horswill presents an automata-theoretic analysis of the computational power of the visual routines theory.
Reference: [ Johnson et al., 1995 ] <author> Michael Patrick Johnson, Trevor Darrell, and Pattie Maes. </author> <title> Evolving visual routines. </title> <journal> Artificial Life, </journal> <volume> 1(4) </volume> <pages> 373-389, </pages> <year> 1995. </year>
Reference-contexts: The simple size function used for this research is just the total number of nodes (internal and leaves) in the program. 3.3 Functional Approach This section describes the functional system. The functional system description also appears in the Artificial Life journal <ref> [ Johnson et al., 1995 ] </ref> . 3.3.1 Types There are only three types for the functional system: point An ordered pair of integers, (x; y). point-list An arbitrary length list of points, (P 1 ; P 2 ; : : : ; P n ). percent A floating point value
Reference: [ Koza, 1992 ] <editor> John R. Koza. </editor> <booktitle> Genetic Programming. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1992. </year>
Reference-contexts: An ideal solution would be to find an automatic method for creating appropriate visual routines for a new task. This thesis describes one such method evolutionary computation applied to the aforementioned ALIVE visual task. In particular, the Genetic Programming (GP) paradigm described by John Koza <ref> [ Koza, 1992 ] </ref> was chosen since it meshes extremely well with the visual routines model. GP is a method for automatic induction of computer programs using simulated evolution. Using visual routines as primitives, GP can be used to compose them into programs which will solve a given visual task. <p> Add q 1 and q 2 to P i+1 . (d) Increment i. 2.2 Genetic Programming Genetic Programming (GP) is a powerful new method for automatically inducing computer programs to solve tasks <ref> [ Koza, 1992 ] </ref> . GP evolved from the original Genetic Algorithm (GA) invented by Holland [ Holland, 1975 ] . <p> Koza decided that using fixed strings of characters as a genotype was too limiting <ref> [ Koza, 1992 ] </ref> . Instead, he chose LISP S-expressions to create genotype encodings. S-expressions, also known as forms, describe a function that can be evaluated in the LISP environment. Forms are essentially trees in the mathematical sense. <p> Eventually, one hopes, a good solution will be evolved. GP performs much better than random chance in searching this space. This nonrandomness of GP is discussed in Chapter 9 of <ref> [ Koza, 1992 ] </ref> . The GP algorithm GP is an instance of the abstract GA algorithm described above. Pseudocode for the GP algorithm is given in Figure 2-2. <p> The number of arguments a function takes is fixed and must be specified. Thus, by composing these terminals and functions, one creates trees, with functions being internal nodes and terminals being leaves. According to Koza, these sets should be selected to have two properties sufficiency and closure under type <ref> [ Koza, 1992 ] </ref> . Sufficiency means that there exists some composition of functions and terminals which are able to solve the problem. Sufficiency often cannot be decided a priori and involves an iterative process of primitive selection. <p> Tournament selection promotes more diversity, hopefully guarding against premature convergence to a local maximum. k is often 3 or 5. 26 Genetic operators Koza describes many operators that can be applied to LISP forms <ref> [ Koza, 1992 ] </ref> . Only the ones used in this research are described here. These are: * Creation * Copying (or cloning) * Crossover * Mutation creation In order to create a random program tree, a random element is picked from the union of the function and terminal set. <p> These involve heuristics like picking random depths for each program, choosing only functions until the maximum depth in order to produce full trees, etc. We again refer the reader to his book for more details <ref> [ Koza, 1992 ] </ref> . copying Copying is a straightforward operation. The program is copied as is directly into the next generation. crossover Crossover is a sexual operator it takes two parents and produces two offspring. First, select two parents from the population using the selection criterion. <p> Although this may appear to be a disadvantage, it is often the case that the algorithm is not very sensitive to their values. Many GP researchers use the values used by Koza <ref> [ Koza, 1992 ] </ref> for most of his experiments. 2.3 Related Work Recently, several other researchers have been investigating automatic ways of creating task-specific visual routines. <p> This did not produce a solution. He also implemented a version of the problem which used Automatically Defined Functions (ADFs) which managed to find a good solution (for a description of ADFs, see <ref> [ Koza, 1992 ] </ref> ). Unfortunately, the perfect solutions were rare in the sense that many runs did not find one and the population size was large, containing 8000 individuals. David Andre performed similar but more in-depth research on the problem of character recognition [ Andre, 1994 ] . <p> The programmer must also specify the top level return type, eliminating the problems of programs returning incorrect data types. Only programs consistent with this type specification are allowed in the evolution. Koza discusses syntactic constraints on certain nodes <ref> [ Koza, 1992 ] </ref> , but the method described here is more general. Typed creation In order to create random trees consistent with the type constraints, the system starts by picking a random node with the proper top level return type.
Reference: [ Koza, 1994 ] <author> John R. Koza. </author> <title> Genetic Programming II: Automatic Discovery of Reusable Programs. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1994. </year> <month> 115 </month>
Reference-contexts: Jeeves by Horswill [ Horswill, 1995 ] and the Meliora system by Whitehead [ Whitehead, 1992 ] actually use the visual routines model, but neither uses GP techniques. Other GP researchers have applied GP to visual recognition tasks, such as Koza <ref> [ Koza, 1994 ] </ref> and David Andre [ Andre, 1994 ] , who both evolved programs for Optical Character Recognition, and Astro Teller [ Teller and Veloso, 1995 ] , who evolved 28 programs for classifying single objects from greyscale images. <p> Several researchers have applied GP to computer vision, but mainly for object recognition tasks. For example, Koza describes an experiment in character recognition <ref> [ Koza, 1994 ] </ref> . The goal is to classify four by six bitmaps as the letter I, the letter L, or NIL, meaning neither. The system pretends there is a turtle which can crawl around the bitmap and see the value of the nine bits in its neighborhood.
Reference: [ Maes et al., 1993 ] <author> Pattie Maes, Bruce Blumberg, Trevor Darrell, and Sandy Pentland. </author> <title> Alive: An artificial life interactive video environment. </title> <booktitle> Visual Proceedings of Siggraph '93, </booktitle> <publisher> ACM Press, </publisher> <year> 1993. </year>
Reference-contexts: The set of routines is fairly small and fixed, allowing a programmer to work them out by hand. Another example (and the one on which we will focus) is the simple vision system used by the ALIVE (Artificial Life Interactive Video Environment) virtual environment project <ref> [ Maes et al., 1995, Maes et al., 1993 ] </ref> . In this system, a user can interact in real-time with a computer graphics creature using gestures which are interpreted by a vision system.
Reference: [ Maes et al., 1995 ] <author> Pattie Maes, Trevor Darrell, Bruce Blumberg, and Sandy Pentland. </author> <title> The ALIVE System: Full-body Interaction with Animated Autonomous Agents. </title> <booktitle> In Proceedings of the Computer Animation Conference, </booktitle> <address> Geneva, Switzerland. </address> <publisher> IEEE Press, </publisher> <year> 1995. </year>
Reference-contexts: The set of routines is fairly small and fixed, allowing a programmer to work them out by hand. Another example (and the one on which we will focus) is the simple vision system used by the ALIVE (Artificial Life Interactive Video Environment) virtual environment project <ref> [ Maes et al., 1995, Maes et al., 1993 ] </ref> . In this system, a user can interact in real-time with a computer graphics creature using gestures which are interpreted by a vision system.
Reference: [ Marr, 1982 ] <author> David Marr. </author> <title> Vision. W.H. </title> <publisher> Freeman, </publisher> <address> San Francisco, CA, </address> <year> 1982. </year>
Reference-contexts: Within perception, computer vision is perhaps the most difficult due to the vast quantity of data. Traditional machine vision as pioneered by Marr assumes that the vision system produces a labeled, perfectly resolved model of the world, distinguishing and representing all objects <ref> [ Marr, 1982 ] </ref> . This model, known as general vision, has been shown to be intractible in practice. Active vision is a new paradigm for machine vision that has received significant attention recently [ Ballard, 1989, Aloimonos, 1993 ] . <p> These concepts are different from the traditional machine vision school as described by Marr <ref> [ Marr, 1982 ] </ref> and others. The traditional model, following the Artificial Intelligence research of the time, assumed that the role of the vision system was to produce a labelled, three-dimensional model of the scene.
Reference: [ Nordin et al., 1995 ] <author> Peter Nordin, Frank Francone, and Wolfgang Banzhaf. </author> <title> Explicitly defined introns and destructive crossover in genetic programming. </title> <editor> In Justinian P. Rosca, editor, </editor> <booktitle> Proceedings of the Workshop on Genetic Programming: From Theory to Real-World Applications (ML-95), </booktitle> <pages> pages 6-22. </pages> <institution> National Resource Laboratory for the study of Brain and Behavior, University of Rochester, </institution> <month> June </month> <year> 1995. </year> <note> Technical Report 95.2. </note>
Reference-contexts: Some research has shown that this useless code (called introns after biology, or the unexpressed bits of DNA which comprise some 90% of the genome) serves to protect the useful, important bits of code from being broken apart by unfortunate (destructive) crossover <ref> [ Nordin et al., 1995 ] </ref> . It is an artifact of the evolution and may serve a useful role. Parsimony constraints often keep this bloating to a minimum, but tend to discourage diversity. Intuitively, high diversity is desirable to search a larger space and not waste computation. <p> It is an artifact of the evolution and may serve a useful role. Parsimony constraints often keep this bloating to a minimum, but tend to discourage diversity. Intuitively, high diversity is desirable to search a larger space and not waste computation. The results in <ref> [ Nordin et al., 1995 ] </ref> are the main reason that the decision not to apply a parsimony constraint to the imperative sections was made. <p> Unfortunately, dead code removal can be tricky and inefficient for side-effecting code since much more state must be examined. Another method, suggested as a way of finding implicit introns in code in <ref> [ Nordin et al., 1995 ] </ref> , is to simply try removing each node one by one and retesting the program on the fitness cases each time. If the resulting answer or state is different, then the code is performing calculation and should be left in.
Reference: [ Ramachandran, 1985 ] <author> V. S. Ramachandran. </author> <title> Apparent motion of subjective surfaces. </title> <journal> Perception, </journal> <volume> 14:127 - 134, </volume> <year> 1985. </year>
Reference-contexts: One possible answer is that the central system does some intelligent reasoning in order to determine which routine will solve the task. On the other hand, V. S. Ramachandran, a human vision researcher, suggests a utilitarian theory of perception <ref> [ Ramachandran, 1985 ] </ref> . He argues that since many other systems in the human body are collections of ad hoc pieces that all function in their own way but tend to work together, perception is likely to be the same.
Reference: [ Rosca and Ballard, 1995 ] <author> Justinian Rosca and Dana Ballard. </author> <title> Causality in genetic programming. </title> <booktitle> In Proceedings of the Fifth International Conference on Genetic Algorithms (ICGA95), </booktitle> <year> 1995. </year>
Reference-contexts: Creating an internal node crossover operator attempts to encourage this. Rosca argues that this may not as useful later in a run, however, where smaller changes may be more beneficial <ref> [ Rosca and Ballard, 1995 ] </ref> . mutation First, select a subtree (which again could be a leaf). Replace it with a small, newly created subtree made with the creation operator. Again, mutation is liable to depth constraints and similar solutions as in crossover can be used.
Reference: [ Sims, 1991 ] <author> Karl Sims. </author> <title> Artificial evolution for computer graphics. </title> <journal> Computer Graphics, </journal> <volume> 25 </volume> <pages> 319-328, </pages> <year> 1991. </year>
Reference-contexts: GP experiments often tend toward the former representation, since researchers are often looking for functions that solve problems or optimize parameters. A noteworthy example of the latter representation is Karl Sims's work on genetically created artwork <ref> [ Sims, 1991 ] </ref> , where the genotype is a function that computes the pixel values on a computer image (the phenotype) and where the fitness function is a person's aesthetic taste.
Reference: [ Tackett, 1995 ] <author> Walter A. Tackett. </author> <title> Greedy recombination and genetic search on the space of computer programs. </title> <editor> In D. Whitley and M. Vose, editors, </editor> <booktitle> Foundations of Genetic Algorithms III. </booktitle> <publisher> Morgan Kauffman, </publisher> <address> San Mateo, CA, </address> <year> 1995. </year>
Reference-contexts: such as moving the attention left then immediately right or setting a marker right off the bat when the marker is already initialized to the attention point. 5.8 Why are the final solutions in the imperative case so unwieldy and inefficient? Several GP researchers have noticed the idea of bloating <ref> [ Tackett, 1995 ] </ref> . Bloating is the tendency of useless code to collect toward the end of a GP run.
Reference: [ Teller and Veloso, 1995 ] <author> Astro Teller and Manuela Veloso. </author> <title> PADO: Learning tree-structured algorithms for orchestration into an object recognition system. </title> <type> Technical Report CMU-CS-95-101, </type> <institution> Carnegie-Mellon University, School of Computer Science, </institution> <address> CMU, Pittsburgh, PA, </address> <year> 1995. </year>
Reference-contexts: Other GP researchers have applied GP to visual recognition tasks, such as Koza [ Koza, 1994 ] and David Andre [ Andre, 1994 ] , who both evolved programs for Optical Character Recognition, and Astro Teller <ref> [ Teller and Veloso, 1995 ] </ref> , who evolved 28 programs for classifying single objects from greyscale images. Finally, Harvery, Cliff and Husbands used GP to perform visual tasks on a real robot. Each will be described in turn. <p> Since there was only one robot, the fitness function for each individual in the GP population must be computed serially, a time-intensive undertaking. Finally, Teller and Veloso used Genetic Programming for the PADO (Parallel Algorithm Discovery and Orchestration) system <ref> [ Teller and Veloso, 1995 ] </ref> . PADO performs object recognition on real grayscale camera images. Genetic programming is used to induce programs which operate on pixel values in the image and return a confidence value that the image belongs to the class the program is evolved to recognize. <p> The system was about 50% accurate at distinguishing between seven classes of objects. Due to the complexity of the system, a detailed explanantion is not possible here. The interested reader is referred to <ref> [ Teller and Veloso, 1995 ] </ref> . 31 Chapter 3 Evolving Visual Routines This chapter describes the ALIVE hand-finding problem and the implementation of the GP system and primitives used for this research.
Reference: [ Ullman, 1984 ] <author> Shimon Ullman. </author> <title> Visual routines. </title> <journal> Cognition, </journal> <volume> 18 </volume> <pages> 97-159, </pages> <year> 1984. </year> <month> 116 </month>
Reference-contexts: Furthermore, that information will be different information than I might extract if I were intent on drawing the cup. To explain how task-dependent visual processing could be implemented, Shimon Ull-man proposed the visual routines model <ref> [ Ullman, 1984 ] </ref> . This model describes a set of 12 primitive operations that can be applied to an input image in order to find spatial relations between objects as well as other useful information. The composition of these operations is called a routine. <p> theory, but many ideas from it have been incorporated into active vision systems by researchers such as Chapman [ Chapman, 1992 ] and Whitehead [ Whitehead, 1992 ] . 2.1.1 Visual Routines Theory One example of a task-dependent theory of vision is Shimon Ullman's Visual Routines theory of intermediate vision <ref> [ Ullman, 1984, Ullman, 1987 ] </ref> . The visual routines model breaks the visual system into three main areas: the base representation, the visual routines processor and the higher level components. The base representation is the result of initial, parallel processing of the retinal image. <p> What constitutes a boundary is a function of the task, so must be a parameter to the operation. For example, consider again the inside-outside task. People can still tell if a point is inside a circle with a dashed line for a border (see <ref> [ Ullman, 1984 ] </ref> ). <p> Each blob will only be indexed once. Also, points are indexed in an arbitrary order. These iterators contain an inhibition map in order to only return each region once. Inhibition maps are described in <ref> [ Ullman, 1984 ] </ref> and [ Chapman, 1992 ] . They are basically bitmaps which store whether a point is inhibited or not. Points are inhibited when they are visited. Thus, an uninhibited point has not been visited.
Reference: [ Ullman, 1987 ] <author> Shimon Ullman. </author> <title> Visual routines. </title> <booktitle> Readings in Computer Vision, </booktitle> <pages> pages 298 - 327, </pages> <year> 1987. </year> <editor> Ed. by Martin A. </editor> <publisher> Fischler and Oscar Firschein. </publisher>
Reference-contexts: theory, but many ideas from it have been incorporated into active vision systems by researchers such as Chapman [ Chapman, 1992 ] and Whitehead [ Whitehead, 1992 ] . 2.1.1 Visual Routines Theory One example of a task-dependent theory of vision is Shimon Ullman's Visual Routines theory of intermediate vision <ref> [ Ullman, 1984, Ullman, 1987 ] </ref> . The visual routines model breaks the visual system into three main areas: the base representation, the visual routines processor and the higher level components. The base representation is the result of initial, parallel processing of the retinal image.
Reference: [ Watkins, 1989 ] <author> Chris Watkins. </author> <title> Learning From Delayed Rewards. </title> <type> PhD thesis, </type> <institution> King's College, </institution> <year> 1989. </year>
Reference-contexts: Steven D. Whitehead's dissertation research also tries to find a method for composing visual routine primitives automatically to solve a task [ Whitehead, 1992 ] . Whitehead uses a machine learning algorithm called Q-learning <ref> [ Watkins, 1989 ] </ref> . He applies this system to a simple block-stacking problem in which a robot arm must stack blocks in order to make a duplicate of an example stack.
Reference: [ Weiss and Kulikowski, 1991 ] <author> Sholom M. Weiss and Casimir A. </author> <title> Kulikowski. Computer Systems That Learn. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: Its generalization results can then be compared to the programs evolved to find right hands. Random Subsampling Another common method for testing generalization of solutions in machine learning is called random-subsampling <ref> [ Weiss and Kulikowski, 1991 ] </ref> . Random subsampling involves dividing the full set of training samples into two sets at random the training set and the test set. Only the training set is used during evolution.
Reference: [ Whitehead, 1992 ] <author> Steven D. Whitehead. </author> <title> Reinforcement Learning for the Adaptive Control of Perception and Action. </title> <type> PhD dissertation, </type> <institution> University of Rochester, Department of Computer Science, </institution> <month> February </month> <year> 1992. </year> <month> 117 </month>
Reference-contexts: In the next section we describe Ullman's visual routines model. The visual routines model was not originally designed as an active vision theory, but many ideas from it have been incorporated into active vision systems by researchers such as Chapman [ Chapman, 1992 ] and Whitehead <ref> [ Whitehead, 1992 ] </ref> . 2.1.1 Visual Routines Theory One example of a task-dependent theory of vision is Shimon Ullman's Visual Routines theory of intermediate vision [ Ullman, 1984, Ullman, 1987 ] . <p> Many GP researchers use the values used by Koza [ Koza, 1992 ] for most of his experiments. 2.3 Related Work Recently, several other researchers have been investigating automatic ways of creating task-specific visual routines. Jeeves by Horswill [ Horswill, 1995 ] and the Meliora system by Whitehead <ref> [ Whitehead, 1992 ] </ref> actually use the visual routines model, but neither uses GP techniques. <p> Steven D. Whitehead's dissertation research also tries to find a method for composing visual routine primitives automatically to solve a task <ref> [ Whitehead, 1992 ] </ref> . Whitehead uses a machine learning algorithm called Q-learning [ Watkins, 1989 ] . He applies this system to a simple block-stacking problem in which a robot arm must stack blocks in order to make a duplicate of an example stack. <p> The attention marker is the most important intermediate object it is implicitly effected by most of the functions and is also considered the program's answer when it is finished evaluating. Ullman does not mention an attention marker it was added to this system following Whitehead <ref> [ Whitehead, 1992 ] </ref> . Lines Lines, like markers, closely follow Ullman's proposal. A line is defined by two points and specifies the segment of locations between the two points. Since only one line was allowed in the system at a time, it was not made explicit as a terminal.
References-found: 32

