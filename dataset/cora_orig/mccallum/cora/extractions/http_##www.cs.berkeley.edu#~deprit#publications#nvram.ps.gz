URL: http://www.cs.berkeley.edu/~deprit/publications/nvram.ps.gz
Refering-URL: http://www.cs.berkeley.edu/~deprit/publications.html
Root-URL: 
Title: Non-Volatile Memory for Fast, Reliable File Systems  
Author: Mary Baker, Satoshi Asami, Etienne Deprit, John Ousterhout, Margo Seltzer 
Address: Berkeley, CA 94720  
Affiliation: Computer Science Division Electrical Engineering and Computer Sciences University of California,  
Abstract: Given the decreasing cost of non-volatile RAM (NVRAM), by the late 1990's it will be feasible for most workstations to include a megabyte or more of NVRAM, enabling the design of higher-performance, more reliable systems. We present the trace-driven simulation and analysis of two uses of NVRAM to improve I/O performance in distributed file systems: non-volatile file caches on client workstations to reduce write traffic to file servers, and write buffers for write-optimized file systems to reduce server disk accesses. Our results show that a megabyte of NVRAM on diskless clients reduces the amount of file data written to the server by 40 to 50%. Increasing the amount of NVRAM shows rapidly diminishing returns, and the particular NVRAM block replacement policy makes little difference to write traffic. Closely integrating the NVRAM with the volatile cache provides the best total traffic reduction. At today's prices, volatile memory provides a better performance improvement per dollar than NVRAM for client caching, but as volatile cache sizes increase and NVRAM becomes cheaper, NVRAM will become cost effective. On the server side, providing a one-half megabyte write-buffer per file system reduces disk accesses by about 20% on most of the measured log-structured file systems (LFS), and by 90% on one heavily-used file system that includes transaction-processing work-loads. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Baker, M., Hartman, J., Kupfer, M., Shirriff, K. and Ousterhout, J., </author> <title> ``Measurements of a Distributed File System'', </title> <booktitle> Proceedings of the 13th Symposium on Operating System Principles, </booktitle> <address> Monterey CA, </address> <month> October </month> <year> 1991, </year> <pages> 198-212. </pages> <note> Published as Operating Systems Review 25, </note> <month> 5 (October </month> <year> 1991). </year>
Reference-contexts: A recent study of the Sprite distributed file system [17] shows that for the workload measured, client workstation caches reduce read traffic from applications by 60%, but only reduce the write traffic by 10% <ref> [1] </ref>. As file caches on both clients and servers continue to grow and satisfy even more read traffic, the proportion of write traffic will increase and could potentially become a bottleneck. <p> Another 15% is forced from the client caches by application fsync calls that immediately and synchronously flush a file's dirty data from the cache to the file server's disk <ref> [1] </ref>. Non-volatile memory (NVRAM), such as RAM with battery backup, offers a possibility for reducing write traffic. By storing dirty data in NVRAM, we can guarantee its permanence without the cost of transferring it from client cache to server cache and from server cache to disk. <p> About 30 users do their day-to-day computing on Sprite, and another 40 people use the system occasionally. 2. Non-Volatile Client File Caches The study in <ref> [1] </ref> found that write events currently account for about one-third of the total file bytes transferred from clients to the file server and that most of these writes were a result of Sprite's 30-second delayed write-back and cache consistency policy rather than cache block replacement. <p> The trace data is broken into eight 24-hour trace runs. The traces record similar workloads, except for traces 3 and 4 in which two users performed long-running simulations on large files, resulting in higher file system throughput. More detailed information about the traces can be found in <ref> [1] </ref>, and more detailed information about the trace methodology can be found in [16]. The simulations required several passes over the trace data. We first processed the trace data to convert it into read, write, delete, flush, and invalidate operations on ranges of bytes. <p> The X axis is a log scale. Results are shown for all eight 24-hour traces in <ref> [1] </ref>.
Reference: 2. <author> Baker, M. and Sullivan, M., </author> <title> ``The Recovery Box: Using Fast Recovery to Provide High Availability'', </title> <booktitle> Proceedings of the Summer 1992 USENIX Conference, </booktitle> <address> San Antonio, TX, </address> <month> June 8-12 </month> <year> 1992, </year> <pages> 31-44. </pages>
Reference-contexts: If the clients can recover quickly under most circumstances, then the data will be unavailable for only a short period of time <ref> [2] </ref>. Although fast recovery of client machines may alleviate many of the problems due to client crashes, it is also important to consider that a client may never recover. When servers are down for extended periods, it is not uncommon to move disks between machines.
Reference: 3. <author> Carson, S. and Setia, S., </author> <title> ``Optimal Write Batch Size in Log-structured File Systems'', </title> <booktitle> Proceedings of USENIX Workshop on File Systems, </booktitle> <address> Ann Arbor, MI, </address> <month> May 21-22 </month> <year> 1992, </year> <pages> 79-91. </pages>
Reference-contexts: This paper has only examined minimizing the number disk write accesses, but read latency may also be an important parameter for some systems. Extremely large write I/O's can cause potentially unacceptable latency to any synchronous read requests that queue up behind them. Analytic results in <ref> [3] </ref> show that the optimal write size for an LFS is approximately two disk tracks, typically 50 - 70 kilobytes. The analytic study reports that the increase in mean read response time due to full segment writes is sometimes as much as 37%, but typically about 14%.
Reference: 4. <author> Copeland, G. and Krishnamurthy, R., </author> <title> The Case for Safe RAM, </title> <type> MCC Technical Report Number ACA-ST-080-88, </type> <month> February </month> <year> 1988. </year>
Reference-contexts: NVRAM's combination of high performance relative to disks and permanence relative to volatile RAM will make it worthwhile for small caches and write buffers, even without its price dropping to that of volatile RAM <ref> [4] </ref>. In Section 2, we analyze the effects of the addition of a small amount of NVRAM to the cache on client workstations to reduce write traffic to servers.
Reference: 5. <author> Douglis, F. and Ousterhout, J., </author> <title> ``Transparent Process Migration: Design Alternatives and the Sprite Implementation'', </title> <journal> SoftwarePractice & Experience 21, </journal> <month> 7 (July </month> <year> 1991). </year>
Reference-contexts: All of the workstations in the cluster run the Sprite network operating system, which is largely UNIX-compatible. Most of the applications running on the cluster are standard UNIX applications. In addition, Sprite provides process migration <ref> [5] </ref>, allowing users to offload jobs easily to idle machines in the cluster.
Reference: 6. <author> Elhardt, K. and Bayer, R., </author> <title> ``A Database Cache for High Performance and Fast Restart in Database Systems'', </title> <journal> ACM Transactions on Database Systems 9, </journal> <month> 4 (December </month> <year> 1984), </year> <pages> 503-525. </pages>
Reference: 7. <author> Finlayson, R. and Cheriton, D., </author> <title> ``Log Files: An Extended File Service Exploiting Write-Once Storage'', </title> <booktitle> Proceedings of 11th Symposium on Operating System Principles, </booktitle> <address> Austin, TX, </address> <month> November </month> <year> 1987, </year> <pages> 139-148. </pages> <note> Published as Operating Systems Review 21, </note> <month> 5 (November </month> <year> 1987). </year>
Reference: 8. <author> Gray, J., </author> <title> ``The 5 Minute Rule for Trading Memory for Disc Accesses and the 10 Byte Rule for Trading Memory for CPU Time'', </title> <booktitle> Proceedings of the ACM Special Interest Group on Management of Data, </booktitle> <address> San Francisco, CA, </address> <month> May </month> <year> 1987, </year> <pages> 395-398. </pages>
Reference-contexts: With a write-buffer, these disk accesses could be avoided, because the writes would remain in the NVRAM buffer until a whole segment accumulated <ref> [8] </ref>. For some file systems, no forced synchronous writes occur. /swap1, for instance, saw no partial segments due to fsyncs, because applications never write directly to the swap disk. In addition to the disk bandwidth reductions, there is a disk space cost associated with partial segments.
Reference: 9. <author> Hagmann, R., </author> <title> ``Reimplementing the Cedar File System Using Group Commit'', </title> <booktitle> Proceedings of 11th Symposium on Operating System Principles, </booktitle> <address> Austin, TX, </address> <month> November </month> <year> 1987, </year> <pages> 155-162. </pages> <note> Published as Operating Systems Review 21, </note> <month> 5 (November </month> <year> 1987). </year>
Reference: 10. <institution> IBM 3990 Storage Control Introduction, </institution> <address> IBM's Storage Subsystem Library, #GA32-0098-0, 1st Edition, </address> <month> September </month> <year> 1987. </year>
Reference: 11. <author> Juszczak, C., </author> <title> ``Improving the Performance and Correctness of an NFS Server'', </title> <booktitle> Proceedings of the Winter 1989 USENIX Conference, </booktitle> <address> San Diego, CA, </address> <month> February </month> <year> 1989. </year>
Reference-contexts: Traditional distributed file systems, especially file servers running the UNIX fast file system [13] in the NFS [19] environment, have already used NVRAM to reduce disk traffic. It is particularly beneficial for NFS file systems, since the NFS protocol requires many synchronous write operations <ref> [11] </ref>. The Legato Systems Prestoserve board [15] caches NFS server requests in non-volatile memory to reduce the latency of synchronous writes to the file system, and performance improvements of up to 50% have been reported on systems using this board.
Reference: 12. <author> Kazar, M. L., Leverett, B. W., Anderson, O. T., Apostolides, V., Bottos, B. A., Chutani, S., Everhart, C. F., Mason, W. A., Tu, S. and Zayas, E. R., </author> <title> ``DEcorum File System Architectural Overview'', </title> <booktitle> Proceedings of the Summer 1990 USENIX Conference, </booktitle> <address> Anaheim, CA, </address> <month> June 11-15 </month> <year> 1990, </year> <pages> 151-164. </pages>
Reference-contexts: Given sufficient volatile memory, NVRAM provides better price/performance even at today's prices. Finally, non-volatile caches on client workstations make sense only for systems such as Sprite and AFS 4.0 <ref> [12] </ref> that improve performance by caching dirty data on clients. In NFS file systems, the cache consistency mechanism requires that dirty data be written through almost immediately to the file server, removing any benefit for using non-volatile memory for file caching on clients.
Reference: 13. <author> McKusick, M. K., Joy, W., Leffler, S. and Fabry, R. S., </author> <title> ``Fast File System for UNIX'', </title> <journal> ACM Transactions on Computer Systems 2, </journal> <month> 3 (August </month> <year> 1984), </year> <pages> 181-197. </pages>
Reference-contexts: The use of NVRAM to improve disk performance is not new. Traditional distributed file systems, especially file servers running the UNIX fast file system <ref> [13] </ref> in the NFS [19] environment, have already used NVRAM to reduce disk traffic. It is particularly beneficial for NFS file systems, since the NFS protocol requires many synchronous write operations [11].
Reference: 14. <author> Menon, J. and Hartung, M., </author> <title> ``The IBM 3990 Disk Cache'', </title> <booktitle> Proceedings of COMPCON 1988, </booktitle> <address> San Francisco, CA, </address> <month> June </month> <year> 1988, </year> <pages> 146-151. </pages>
Reference: 15. <author> Moran, J., Sandberg, R., Coleman, D., Kepecs, J. and Lyon, B., </author> <title> ``Breaking Through the NFS Performance Barrier'', </title> <booktitle> Proceedings of EUUG Spring 1990, </booktitle> <address> Munich, Germany, </address> <month> April 23-27 </month> <year> 1990, </year> <pages> 199-206. </pages>
Reference-contexts: It is particularly beneficial for NFS file systems, since the NFS protocol requires many synchronous write operations [11]. The Legato Systems Prestoserve board <ref> [15] </ref> caches NFS server requests in non-volatile memory to reduce the latency of synchronous writes to the file system, and performance improvements of up to 50% have been reported on systems using this board. IBM uses four megabytes of NVRAM on the 3990-3 disk controller in a similar fashion [10][14].
Reference: 16. <author> Ousterhout, J. K., Da Costa, H., Harrison, D., Kunze, J. A., Kupfer, M. and Thompson, J. G., </author> <title> ``A Trace-Driven Analysis of the UNIX 4.2 BSD File System'', </title> <booktitle> Proceedings of the 10th Symposium on Operating System Principles, </booktitle> <address> Orcas Island, WA, </address> <month> December </month> <year> 1985, </year> <pages> 15-24. </pages> <note> Published as Operating Systems Review 19, </note> <month> 5 (October </month> <year> 1985). </year>
Reference-contexts: More detailed information about the traces can be found in [1], and more detailed information about the trace methodology can be found in <ref> [16] </ref>. The simulations required several passes over the trace data. We first processed the trace data to convert it into read, write, delete, flush, and invalidate operations on ranges of bytes.
Reference: 17. <author> Ousterhout, J., Cherenson, A., Douglis, F., Nelson, M. and Welch, B., </author> <title> ``The Sprite Network Operating System'', </title> <booktitle> IEEE Computer 21, </booktitle> <month> 2 (February </month> <year> 1988), </year> <pages> 23-36. </pages>
Reference-contexts: Because large, main-memory file caches more effectively reduce read traffic than write traffic, the write traffic in distributed systems will become increasingly important. A recent study of the Sprite distributed file system <ref> [17] </ref> shows that for the workload measured, client workstation caches reduce read traffic from applications by 60%, but only reduce the write traffic by 10% [1].
Reference: 18. <author> Rosenblum, M. and Ousterhout, J. K., </author> <title> ``The Design and Implementation of a Log-Structured File System'', </title> <booktitle> Proceedings of the 13th Symposium on Operating System Principles, Asilomar, </booktitle> <address> CA, </address> <month> October </month> <year> 1991, </year> <pages> 1-15. </pages> <note> Published as Operating Systems Review 25, 5 (October 1991). Also available as Transactions on Computer Systems 10, </note> <month> 1 (February </month> <year> 1992), </year> <pages> 26-52. </pages>
Reference-contexts: Our measurements show that some new file systems, without NFS's requirement for synchronous operations, can still obtain a significant reduction in the number of writes. Sprite's log-structured file system (LFS) <ref> [18] </ref> is an example of a write-optimized file system that amortizes write cost by collecting a large amount of dirty data and writing it contiguously. Currently, fsync requests from clients often force LFS to write to disk before it has accumulated much data. <p> Several database and file systems use various forms of logging on the file server to improve write I/O performance by minimizing the latency of disk writes [6][7][9][12]<ref> [18] </ref>. Sprite's log-structured file system [18] (LFS) is the most extreme example of a logging file system, because file data and metadata are written to disk only in a log format. Also, file metadata operations in Sprite are performed asynchronously rather than synchronously.
Reference: 19. <author> Sandberg, R., Goldberg, D., Kleiman, S., Walsh, D. and Lyon, B., </author> <title> ``Design and Implementation of the Sun Network Filesystem'', </title> <booktitle> Proceedings of the Summer 1985 USENIX Conference, </booktitle> <address> Portland, OR, </address> <month> June </month> <year> 1985, </year> <pages> 119-130. </pages>
Reference-contexts: The use of NVRAM to improve disk performance is not new. Traditional distributed file systems, especially file servers running the UNIX fast file system [13] in the NFS <ref> [19] </ref> environment, have already used NVRAM to reduce disk traffic. It is particularly beneficial for NFS file systems, since the NFS protocol requires many synchronous write operations [11].
Reference: 20. <author> Seltzer, M., Chen, P. and Ousterhout, J., </author> <title> ``Disk Scheduling Revisited'', </title> <booktitle> Proceedings of the Winter 1990 USENIX Conference, </booktitle> <address> Washington, D.C., </address> <month> January </month> <year> 1990, </year> <pages> 313-324. </pages>
Reference-contexts: Disk writes go to this nonvolatile speed matching buffer to reduce latency. Buffering writes also allows more efficient disk utilization since the system can sort its I/O operations to reduce disk head - 9 - motion. Simulation results in <ref> [20] </ref> show that only 7% of disk bandwidth is used when writing dirty data randomly to a disk. Instead of writing blocks randomly, 1000 I/O's, requiring four megabyte of NVRAM, can be buffered and sorted to utilize 40% of the disk bandwidth.
Reference: 21. <author> Thompson, J. G., </author> <title> Efficient Analysis of Caching Systems, </title> <type> PhD Thesis, </type> <institution> University of California, Berkeley, </institution> <month> October </month> <year> 1987. </year> <note> Also available as Technical Report UCB/CSD 87/374. - 13 </note> - 
Reference-contexts: Reducing write traffic beyond 10 to 17% would require choosing a cache consistency policy more efficient than Sprite's, such as a protocol based on block-by-block invalidation and flushing, rather than whole-file invalidation and flushing <ref> [21] </ref>. 2.4. Small NVRAM Reduces Traffic significantly reduces write traffic from clients to servers.
References-found: 21

