URL: http://www.math.rutgers.edu/~sontag/FTP_DIR/quadratic-vc.ps.gz
Refering-URL: http://www.math.rutgers.edu/~sontag/papers.html
Root-URL: 
Email: koiran@lip.ens-lyon.fr  sontag@hilbert.rutgers.edu  
Title: Neural Networks with Quadratic VC Dimension  
Author: Pascal Koiran Eduardo D. Sontag 
Keyword: feedforward neural networks, learning, Vapnik-Chervonenkis dimension  
Address: 46 allee d'Italie 69364 Lyon Cedex 07, France  New Brunswick, NJ 08903, USA  
Affiliation: LIP, ENS Lyon CNRS  Department of Mathematics Rutgers University  
Abstract: This paper shows that neural networks which use continuous activation functions have VC dimension at least as large as the square of the number of weights w. This result settles a long-standing open question, namely whether the well-known O(w log w) bound, known for hard-threshold nets, also held for more general sigmoidal nets. Implications for the number of samples needed for valid generalization are discussed. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Anthony and N.L. Biggs, </author> <title> Computational Learning Theory: An Introduction, </title> <publisher> Cambridge U. Press, </publisher> <year> 1992. </year>
Reference-contexts: We omit the details of the formalization of PAC learning, since there are excellent references available, both in textbook (e.g. <ref> [1, 11] </ref>) and survey paper (e.g. [10]) form, and the concept is by now very well-known. <p> W n ) = W i . 2 Lemma 2 There exists an architecture of linear and multiplication gates with inputs in R, n output units and O (n) weights such that the following property holds: for every * 2 f0; 1g n , there exists an input w 2 <ref> [0; 1] </ref> such that the output of the network f 02 (w) = (f 02 (w) 1 ; : : :; f 02 (w) n ) of the network satisfies f 02 (w) i 2 [0; 1=2 [ if * i = 0, and f 02 (w) i 2]1=2; 1] if <p> Proof. The construction is based on a simple idea from symbolic dynamics. Consider the logistic map : <ref> [0; 1] </ref> ! [0; 1] such that (x) = 4x (1 x). We claim that for every * 2 f0; 1g n , there exists w 2 [0; 1] such that i1 (w) 2 [0; 1=2 [ if * i = 0, and i1 (w) 2]1=2; 1] if * i = <p> Proof. The construction is based on a simple idea from symbolic dynamics. Consider the logistic map : <ref> [0; 1] </ref> ! [0; 1] such that (x) = 4x (1 x). We claim that for every * 2 f0; 1g n , there exists w 2 [0; 1] such that i1 (w) 2 [0; 1=2 [ if * i = 0, and i1 (w) 2]1=2; 1] if * i = 1. <p> Proof. The construction is based on a simple idea from symbolic dynamics. Consider the logistic map : <ref> [0; 1] </ref> ! [0; 1] such that (x) = 4x (1 x). We claim that for every * 2 f0; 1g n , there exists w 2 [0; 1] such that i1 (w) 2 [0; 1=2 [ if * i = 0, and i1 (w) 2]1=2; 1] if * i = 1. <p> First, note that each element of <ref> [0; 1] </ref> has two distinct preimages by , except 1; and that (1=2) = 1, (1) = 0, and (0) = 0. If * n = 0, choose an element w n in ]0; 1=2 [; otherwise, choose w n in ]1=2; 1 [.
Reference: [2] <author> E.B. Baum and D. Haussler, </author> <title> What size net gives valid generalization?, </title> <booktitle> Neural Computation, 1 (1989), </booktitle> <pages> pp. 151-160. </pages>
Reference-contexts: We conclude with a case-by-case analysis based on the slopes of its linear pieces. The first special case in Theorem 8 follows from the next trivial lemma and from the results of <ref> [2] </ref> for threshold nets. Lemma 4 Let be a piecewise-constant function. An architecture of -gates with n weights can be simulated by an architecture of threshold gates with O (n) weights. Proof. <p> The resulting weights are "shared" among gates. But such a sharing arrangement is not allowed in our definitions, and indeed, cannot be exploited when using standard Cover-like counting arguments, as in <ref> [5, 2] </ref>, which are dependent upon the fan-in of gates. As a simple illustration of this process, consider a way of rewriting the network obtained in Proposition 1 as a network which only employs threshold gates. We sketch next how one eliminates linear gates in that case. <p> Note that without weight sharing, the VC dimension of of a threshold network with n programmable weights remains O (n log n) by the counting argument of <ref> [2] </ref>. A more restrictive type of weight-sharing has been studied in the neural network literature, and proved to be useful in invariant recognition tasks [8]. A formal model is studied in [12], and it is shown that the VC dimension remains O (n log n).
Reference: [3] <author> L. Blum, M. Shub and S. Smale, </author> <title> On the theory of computation and complexity over the real numbers: NP-completeness, recursive functions and universal machines, </title> <journal> Bulletin of the AMS, </journal> <volume> 21 (1989), </volume> <pages> pp. 1-46. </pages>
Reference-contexts: Our construction was originally motivated by a related one, given in [6], which showed that real-number programs (in the Blum-Shub-Smale model of computation) <ref> [3] </ref> with running time T have VC dimension (T 2 ). The desired result on continuous activations is then obtained, approximating Heaviside gates by -nets with large weights and approximating linear gates by -nets with small weights.
Reference: [4] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. Warmuth, </author> <title> Learnability and the Vapnik-Chervonenkis dimension, </title> <journal> J. of the ACM, </journal> <volume> 36 (1989), </volume> <pages> pp. 929-965. </pages>
Reference: [5] <author> T.M. </author> <title> Cover, Capacity problems for linear machines, in: Pattern Recognition, </title> <editor> L. Kanal ed., </editor> <publisher> Thompson Book Co., </publisher> <year> 1988, </year> <pages> pp. 283-289. </pages>
Reference-contexts: In this case, the input-output mapping of the network is affine, so that the VC dimension is bounded by the number of weights (this observation goes back at least to 1965; see <ref> [5] </ref>). If the relation (x) = ax + b (a6=0) holds everywhere except at a nonempty finite number of points, we are in special case 3. <p> The resulting weights are "shared" among gates. But such a sharing arrangement is not allowed in our definitions, and indeed, cannot be exploited when using standard Cover-like counting arguments, as in <ref> [5, 2] </ref>, which are dependent upon the fan-in of gates. As a simple illustration of this process, consider a way of rewriting the network obtained in Proposition 1 as a network which only employs threshold gates. We sketch next how one eliminates linear gates in that case.
Reference: [6] <author> P. Goldberg and M. Jerrum, </author> <title> Bounding the Vapnik-Chervonenkis dimension of concept classes parametrized by real numbers, </title> <booktitle> Machine Learning, 18 (1995), </booktitle> <pages> pp. 131-148. </pages>
Reference-contexts: It could have been the case that the upper bound O (w 2 ) is merely an artifact of the method of proof in <ref> [6] </ref>, and that reliable learning with continuous-activation networks is still possible with far smaller sample sizes, proportional to O (w log w). But this is not the case, and in this paper we answer Maass' open question in the affirmative. <p> This is a somewhat surprising result, since purely linear networks result in VC dimension proportional to w, and purely threshold nets have, as per the results quoted above, VC dimension bounded by w log w. Our construction was originally motivated by a related one, given in <ref> [6] </ref>, which showed that real-number programs (in the Blum-Shub-Smale model of computation) [3] with running time T have VC dimension (T 2 ). The desired result on continuous activations is then obtained, approximating Heaviside gates by -nets with large weights and approximating linear gates by -nets with small weights. <p> If the relation (x) = ax + b (a6=0) holds everywhere except at a nonempty finite number of points, we are in special case 3. The VC dimension of any architecture of n weights is O (n 2 ) by <ref> [6] </ref> (that paper actually deals with arbitrary piecewise-polynomial gate functions). The lower bound is established in Lemmas 6 and 7. The only remaining case is that in which there exist at least two non-trivial pieces, and in at least one is not constant.
Reference: [7] <author> M. Karpinski and A. Macintyre, </author> <title> Polynomial bounds for VC dimension of sigmoidal neural networks, </title> <booktitle> in Proc. 27th ACM Symposium on Theory of Computing, </booktitle> <year> 1995, </year> <pages> pp. 200-208. </pages>
Reference-contexts: This result applies in particular to the standard sigmoid 1=(1 + e x ). (However, in contrast with the piecewise-polynomial case, there is still in that case a large gap between our (w 2 ) lower bound and the O (w 4 ) upper bound which was recently established in <ref> [7] </ref>.) A number of variations, dealing with Boolean inputs, or weakening the assumptions on , are also discussed. The last section includes some brief remarks regarding an interpretation of our results in terms of threshold-only networks with "shared" weights.
Reference: [8] <author> K. Lang and G.E. Hinton, </author> <title> The development of TDNN architecture for speech recognition, </title> <type> Technical Report CMU-CS-88-152, </type> <institution> Carnegie-Mellon University, </institution> <year> 1988. </year>
Reference-contexts: A more restrictive type of weight-sharing has been studied in the neural network literature, and proved to be useful in invariant recognition tasks <ref> [8] </ref>. A formal model is studied in [12], and it is shown that the VC dimension remains O (n log n). In this model one assumes that there is an equivalence relation between weights; this is similar to our weight-sharing mechanism.
Reference: [9] <author> W. Maass, </author> <title> Bounds for the computational power and learning complexity of analog neural nets, </title> <booktitle> in Proc. 25th ACM Symposium on Theory of Computing, </booktitle> <year> 1993, </year> <pages> pp. 335-344. </pages>
Reference-contexts: The work of Cover ([5]) and Baum and Haussler ([2]) dealt with the computation of VC (F) when the class F consists of networks built up from hard-threshold activations and having w weights; they showed that VC (F)= O (w log w). (Conversely, Maass showed in <ref> [9] </ref> that there is also a lower bound of this form.) It would appear that this definitely settled the VC dimension (and hence also the sample size) question. 2 This research was supported in part by US Air Force Grant AFOSR-94-0293. 1 However, the above estimate assumes an architecture based on
Reference: [10] <author> W. Maass, </author> <booktitle> Perspectives of current research about the complexity of learning in neural nets, in Theoretical Advances in Neural Computation and Learning , V.P. </booktitle> <editor> Roychowdhury, K.Y. Siu, and A. Orlitsky, editors, </editor> <publisher> Kluwer, </publisher> <address> Boston, </address> <year> 1994, </year> <pages> pp. 295-336. </pages>
Reference-contexts: We omit the details of the formalization of PAC learning, since there are excellent references available, both in textbook (e.g. [1, 11]) and survey paper (e.g. <ref> [10] </ref>) form, and the concept is by now very well-known. <p> The last reference, in particular, established for that case an upper bound of O (w 2 ), where, as before, w is the number of weights. However it was an open problem (specifically, "open problem number 7" in the recent survey <ref> [10] </ref>) if there is a matching w 2 lower bound for such networks, and more generally for arbitrary continuous-activation nets. <p> The last section includes some brief remarks regarding an interpretation of our results in terms of threshold-only networks with "shared" weights. Basic Terminology and Definitions It is possible to formulate a general definition of "network architecture" that allows for very arbitrary nets; see <ref> [10] </ref>.
Reference: [11] <author> B.K. Natarajan, </author> <title> Machine Learning : A Theoretical Approach, </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: We omit the details of the formalization of PAC learning, since there are excellent references available, both in textbook (e.g. <ref> [1, 11] </ref>) and survey paper (e.g. [10]) form, and the concept is by now very well-known.
Reference: [12] <author> J. Shawe-Taylor, </author> <title> Threshold network learning in the presence of equivalences, </title> <booktitle> in Advances in Neural Information Processing Systems 4, </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1992, </year> <pages> pp. 879-886. </pages>
Reference-contexts: A more restrictive type of weight-sharing has been studied in the neural network literature, and proved to be useful in invariant recognition tasks [8]. A formal model is studied in <ref> [12] </ref>, and it is shown that the VC dimension remains O (n log n). In this model one assumes that there is an equivalence relation between weights; this is similar to our weight-sharing mechanism.
Reference: [13] <author> E.D. Sontag, </author> <title> Sigmoids distinguish better than Heavisides, </title> <booktitle> Neural Computation, 1 (1989), </booktitle> <pages> pp. 470-472. </pages>
Reference-contexts: In contrast, the usually employed gradient descent learning algorithms ("backpropagation" method) rely upon continuous activations, that is, neurons with graded responses. As pointed out in <ref> [13] </ref>, the use of analog activations, which allow the passing of rich (not just binary) information among levels, may result in higher memory capacity as compared with threshold nets.
Reference: [14] <author> E.D. Sontag, </author> <title> Feedforward nets for interpolation and classification, </title> <journal> J. Comp. Syst. Sci, </journal> <volume> 45 (1992), </volume> <pages> pp. 20-48. </pages>
Reference-contexts: This has serious potential implications in learning, essentially because more memory capacity means that a given function f may be able to "memorize" in a "rote" fashion too much data, and less generalization is therefore possible. Indeed, the paper <ref> [14] </ref> showed that there are conceivable (though not very practical) neural architectures with extremely high VC dimensions. Thus the problem of studying VC (F ) for analog networks is an interesting and relevant issue.
Reference: [15] <author> L.G. Valiant, </author> <title> A theory of the learnable, </title> <journal> Comm. of the ACM, </journal> <volume> 27, </volume> <year> 1984, </year> <pages> pp. 1134-1142 </pages>
Reference: [16] <author> V.N. Vapnik, </author> <title> Estimation of Dependencies Based on Empirical Data, </title> <publisher> Springer, </publisher> <address> Berlin, </address> <year> 1982. </year> <month> 11 </month>
References-found: 16

