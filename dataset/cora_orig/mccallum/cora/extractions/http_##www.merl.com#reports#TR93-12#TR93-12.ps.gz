URL: http://www.merl.com/reports/TR93-12/TR93-12.ps.gz
Refering-URL: http://www.merl.com/reports/TR93-12/index.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: e-mail: schabes@merl.com dick@merl.com  
Author: Yves Schabes and Richard C. Waters 
Note: To appear in the 1993 International Workshop on Parsing Technology Copyright c Mitsubishi Electric Research Laboratories, 1993 201 Broadway,  
Address: 201 Broadway; Cambridge, MA 02139  Cambridge, Massachusetts 02139  
Date: 93-12 July, 1993  
Affiliation: MITSUBISHI ELECTRIC RESEARCH LABORATORIES CAMBRIDGE RESEARCH CENTER  Mitsubishi Electric Research Laboratories  
Pubnum: Technical Report  
Abstract: Stochastic lexicalized context-free grammar (SLCFG) is an attractive compromise between the parsing efficiency of stochastic context-free grammar (SCFG) and the lexical sensitivity of stochastic lexicalized tree-adjoining grammar (SLTAG). SLCFG is a restricted form of SLTAG that can only generate context-free languages and can be parsed in cubic time. However, SLCFG retains the lexical sensitivity of SLTAG and is therefore a much better basis for capturing distributional information about words than SCFG. This work may not be copied or reproduced in whole or in part for any commercial purpose. Permission to copy in whole or in part without payment of fee is granted for nonprofit educational and research purposes provided that all such whole or partial copies include the following: a notice that such copying is by permission of Mitsubishi Electric Research Laboratories of Cambridge, Massachusetts; an acknowledgment of the authors and individual contributions to the work; and all applicable portions of the copyright notice. Copying, reproduction, or republishing for any other purpose shall require a license with payment of fee to Mitsubishi Electric Research Laboratories. All rights reserved. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. Booth. </author> <title> Probabilistic representation of formal languages. </title> <booktitle> In Tenth Annual IEEE Symposium on Switching and Automata Theory, </booktitle> <month> October </month> <year> 1969. </year>
Reference-contexts: Early stochastic proposals such as Markov Models, N-gram models [2, 14] and Hidden Markov Models [7] are very effective at capturing simple distributional information about adjacent words. However, they cannot capture long range distributional information nor the hierarchical constraints inherent in natural languages. Stochastic context-free grammar (SCFG) <ref> [1, 3, 5] </ref> extends context-free grammar (CFG) by associating each rule with a probability that controls its use. Each rule is associated with a single probability that is the same for all the sites where the rule can be applied.
Reference: [2] <author> F. Jelinek. </author> <title> Self-organized language modeling for speech recognition. </title> <editor> In Alex Waibel and Kai-Fu Lee, editors, </editor> <booktitle> Readings in speech recognition. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California, </address> <year> 1990. </year> <note> Also in IBM Research Report (1985). </note>
Reference-contexts: The second goal is to capture as many of the hierarchical constraints inherent in natural languages as possible. Unfortunately, these two goals have been more or less incompatible to date. Early stochastic proposals such as Markov Models, N-gram models <ref> [2, 14] </ref> and Hidden Markov Models [7] are very effective at capturing simple distributional information about adjacent words. However, they cannot capture long range distributional information nor the hierarchical constraints inherent in natural languages.
Reference: [3] <author> F. Jelinek, J. D. Lafferty, and R. L. Mercer. </author> <title> Basic methods of probabilistic context free grammars. </title> <type> Technical Report RC 16374 (72684), </type> <institution> IBM, </institution> <address> Yorktown Heights, NY, </address> <year> 1990. </year>
Reference-contexts: Early stochastic proposals such as Markov Models, N-gram models [2, 14] and Hidden Markov Models [7] are very effective at capturing simple distributional information about adjacent words. However, they cannot capture long range distributional information nor the hierarchical constraints inherent in natural languages. Stochastic context-free grammar (SCFG) <ref> [1, 3, 5] </ref> extends context-free grammar (CFG) by associating each rule with a probability that controls its use. Each rule is associated with a single probability that is the same for all the sites where the rule can be applied. <p> For instance, the algorithms for estimating the stochastic parameters and determining the probability of a string require in the worst case O (n 6 )-time for SLTAG [9] but only O (n 3 )-time for SCFG <ref> [3] </ref>. Stochastic lexicalized context-free grammar (SLCFG) is a restricted form of SLTAG that retains most of the advantages of SLTAG without requiring any greater computational resources than SCFG. SLTAG restricts the elementary trees that are possible and the way adjunction can be performed.
Reference: [4] <author> Aravind K. Joshi and Yves Schabes. </author> <title> Tree-adjoining grammars and lexicalized grammars. </title> <editor> In Maurice Nivat and Andreas Podelski, editors, </editor> <title> Tree Automata and Languages. </title> <publisher> Elsevier Science, </publisher> <year> 1992. </year>
Reference-contexts: the probability of a string generated by an SLCFG in O (n 3 )- time, and discuss the algorithms needed to train the parameters of an SLCFG. 2 LCFG Lexicalized context-free grammar (LCFG) [12, 13] is a tree generating system that is a restricted form of lexicalized tree-adjoining grammar (LTAG) <ref> [4] </ref>. The grammar consists of two sets of trees: initial trees, which are combined by substitution and auxiliary trees, which are combined by adjunction. An LCFG is lexicalized because every initial and auxiliary tree is required to contain a terminal symbol on its frontier. <p> Further, a constructive procedure exists for converting any CFG G into an equivalent LCFG G 0 . The fact that LCFG lexicalizes CFG is significant, because every other method for lexi-calizing CFGs without changing the trees derived requires context-sensitive operations <ref> [4] </ref> and therefore dramatically increases worst case processing time. As shown in [12, 13] (and in Section 4) LCFG can be parsed in the worst case just as quickly as CFG.
Reference: [5] <author> K. Lari and S. J. Young. </author> <title> The estimation of stochastic context-free grammars using the Inside-Outside algorithm. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 4 </volume> <pages> 35-56, </pages> <year> 1990. </year>
Reference-contexts: Early stochastic proposals such as Markov Models, N-gram models [2, 14] and Hidden Markov Models [7] are very effective at capturing simple distributional information about adjacent words. However, they cannot capture long range distributional information nor the hierarchical constraints inherent in natural languages. Stochastic context-free grammar (SCFG) <ref> [1, 3, 5] </ref> extends context-free grammar (CFG) by associating each rule with a probability that controls its use. Each rule is associated with a single probability that is the same for all the sites where the rule can be applied. <p> By keeping a record of every attempt to enter a triple into a cell of the array C, one can extend the algorithm so that derivations and therefore the trees they generate can be rapidly recovered. 5 Training an SLCFG In the general case, the training algorithm for SCFG <ref> [5] </ref> requires O (n 3 )-time for each sentence of length n. A training algorithm for SLCFG can be constructed that achieves these same worst case bounds.
Reference: [6] <author> Fernando Pereira and Yves Schabes. </author> <title> Inside-outside reestimation from partially bracketed corpora. </title> <booktitle> In 20 th Meeting of the Association for Computational Linguistics (ACL'92), </booktitle> <address> Newark, Delaware, </address> <year> 1992. </year>
Reference-contexts: As in the last section, the key reason for this is that computations involving SLCFG only require the consideration of contiguous strings. It should be noted that in the special case of a fully bracketed training corpus, the parameters of an SCFG can be estimated in linear time <ref> [6, 10] </ref>. It is an open question whether this can be done for SLCFG. However, it should be straightforward to design an O (n 2 )-time training algorithm for SLCFG given a fully bracketed corpus. 10 Schabes & Waters 6 Conclusion The preceding sections present stochastic lexicalized context-free grammar (SLCFG).
Reference: [7] <author> Lawrence R. Rabiner. </author> <title> A tutorial on hidden Markov models and selected applications in speech recognition. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 77(2) </volume> <pages> 257-285, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: The second goal is to capture as many of the hierarchical constraints inherent in natural languages as possible. Unfortunately, these two goals have been more or less incompatible to date. Early stochastic proposals such as Markov Models, N-gram models [2, 14] and Hidden Markov Models <ref> [7] </ref> are very effective at capturing simple distributional information about adjacent words. However, they cannot capture long range distributional information nor the hierarchical constraints inherent in natural languages. Stochastic context-free grammar (SCFG) [1, 3, 5] extends context-free grammar (CFG) by associating each rule with a probability that controls its use.
Reference: [8] <author> Philip Resnik. </author> <title> Probabilistic tree-adjoining grammars as a framework for statistical natural language processing. </title> <booktitle> In Proceedings of the 14 th International Conference on Computational Linguistics (COLING'92), </booktitle> <year> 1992. </year>
Reference-contexts: In the absence of a formalism that adequately combines this information with other kinds of information, the emphasis in research has been on simple non-hierarchical statistical models of words, such as word N-gram models. Recently, it has been suggested that stochastic lexicalized tree-adjoining grammar (SLTAG) <ref> [8, 9] </ref> may be able to capture both distributional and hierarchical information. An SLTAG grammar consists of a set of trees each of which contains one or more lexical items. These elementary trees can be viewed as the elementary clauses (including their transformational variants) in which the lexical items participate.
Reference: [9] <author> Yves Schabes. </author> <title> Stochastic lexicalized tree-adjoining grammars. </title> <booktitle> In Proceedings of the 14 th International Conference on Computational Linguistics (COLING'92), </booktitle> <year> 1992. </year>
Reference-contexts: In the absence of a formalism that adequately combines this information with other kinds of information, the emphasis in research has been on simple non-hierarchical statistical models of words, such as word N-gram models. Recently, it has been suggested that stochastic lexicalized tree-adjoining grammar (SLTAG) <ref> [8, 9] </ref> may be able to capture both distributional and hierarchical information. An SLTAG grammar consists of a set of trees each of which contains one or more lexical items. These elementary trees can be viewed as the elementary clauses (including their transformational variants) in which the lexical items participate. <p> This makes it possible to represent a great deal of distributional information about words. Unfortunately, the statistical algorithms for SLTAG <ref> [9] </ref> require much more computational resources than the ones for SCFG. For instance, the algorithms for estimating the stochastic parameters and determining the probability of a string require in the worst case O (n 6 )-time for SLTAG [9] but only O (n 3 )-time for SCFG [3]. <p> Unfortunately, the statistical algorithms for SLTAG <ref> [9] </ref> require much more computational resources than the ones for SCFG. For instance, the algorithms for estimating the stochastic parameters and determining the probability of a string require in the worst case O (n 6 )-time for SLTAG [9] but only O (n 3 )-time for SCFG [3]. Stochastic lexicalized context-free grammar (SLCFG) is a restricted form of SLTAG that retains most of the advantages of SLTAG without requiring any greater computational resources than SCFG. <p> for a string is a tree whose probability is as large as any other tree generated for the string. (Note that a most likely derivation need not generate a most likely tree.) 4 Parsing SLCFG Since SLCFG is a restricted case of SLTAG, the O (n 6 )-time SLTAG parser <ref> [9] </ref> can be used for parsing SLCFG. Further, it can be straightforwardly modified to require at most O (n 4 )-time when applied to SLCFG. However, this does not take full advantage of the context-freeness of SLCFG. <p> A training algorithm for SLCFG can be constructed that achieves these same worst case bounds. To start with, since SLCFG is a restricted case of stochastic lexicalized tree-adjoining grammar (SLTAG), the O (n 6 )-time inside-outside reestimation algorithm for SLTAG <ref> [9] </ref> can be used for estimating the parameters of an SLCFG given a training corpus. Straightforward modifications lead to an O (n 4 )-time algorithm for training an SLCFG. However, this alone does not achieve the full potential of SLCFG.
Reference: [10] <author> Yves Schabes, Michael Roth, and Randy Osborne. </author> <title> Parsing the Wall Street Journal with the inside-outside algorithm. </title> <booktitle> In Sixth Conference of the European Chapter of the Association for Computational Linguistics (EACL'93), </booktitle> <address> Utrecht, the Netherlands, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: As in the last section, the key reason for this is that computations involving SLCFG only require the consideration of contiguous strings. It should be noted that in the special case of a fully bracketed training corpus, the parameters of an SCFG can be estimated in linear time <ref> [6, 10] </ref>. It is an open question whether this can be done for SLCFG. However, it should be straightforward to design an O (n 2 )-time training algorithm for SLCFG given a fully bracketed corpus. 10 Schabes & Waters 6 Conclusion The preceding sections present stochastic lexicalized context-free grammar (SLCFG).
Reference: [11] <author> Yves Schabes and Stuart Shieber. </author> <title> An alternative conception of tree-adjoining derivation. </title> <booktitle> In 20 th Meeting of the Association for Computational Linguistics (ACL'92), </booktitle> <year> 1992. </year>
Reference-contexts: The purpose of the codes (fL; Rg etc.) is to insure that left and right adjunction can each be applied at most once on a node. The procedure could easily be modified to account for other constraints on the way derivation should proceed, such as those suggested for LTAGs <ref> [11] </ref>. The procedure Add enters a triple [; code; p] into C [i; k].
Reference: [12] <author> Yves Schabes and Richard C. Waters. </author> <title> Lexicalized context-free grammar: A cubic-time parsable formalism that strongly lexicalizes context-free grammar. </title> <type> Technical Report 93-04, </type> <institution> Mitsubishi Electric Research Labs, </institution> <address> 201 Broadway. Cambridge MA 02139, </address> <year> 1993. </year>
Reference-contexts: However, SLCFG retains most of the key features of SLTAG enumerated above. In particular, the probabilities in SLCFG are 2 Schabes & Waters directly linked to pairs of words. SLCFG is a stochastic extension of lexicalized context-free grammar (LCFG) <ref> [12, 13] </ref>. <p> The following sections, introduce LCFG, define the stochastic extension to SLCFG, present an algorithm that can determine the probability of a string generated by an SLCFG in O (n 3 )- time, and discuss the algorithms needed to train the parameters of an SLCFG. 2 LCFG Lexicalized context-free grammar (LCFG) <ref> [12, 13] </ref> is a tree generating system that is a restricted form of lexicalized tree-adjoining grammar (LTAG) [4]. The grammar consists of two sets of trees: initial trees, which are combined by substitution and auxiliary trees, which are combined by adjunction. <p> LTAG and TAG generate tree adjoining languages and have path sets that are context-free languages. LCFG is intermediate in nature. It can only generate context-free languages, but has path sets that are also context-free languages. 2.2 LCFG Lexicalizes CFG As shown in <ref> [12, 13] </ref> LCFG lexicalizes CFG without changing the trees derived. Further, a constructive procedure exists for converting any CFG G into an equivalent LCFG G 0 . <p> The fact that LCFG lexicalizes CFG is significant, because every other method for lexi-calizing CFGs without changing the trees derived requires context-sensitive operations [4] and therefore dramatically increases worst case processing time. As shown in <ref> [12, 13] </ref> (and in Section 4) LCFG can be parsed in the worst case just as quickly as CFG. <p> This algorithm can be trivially modified to extract a most probable derivation of the given string. More efficient SLCFG processors can be based on the Earley style LCFG recognizer presented in <ref> [12] </ref>. 4.1 Terminology Suppose that G is an SLCFG and that a 1 a n is an input string. Let be a node in an elementary tree (identified by the name of the tree and the position of the node in the tree).
Reference: [13] <author> Yves Schabes and Richard C. Waters. </author> <title> Lexicalized context-free grammars. </title> <booktitle> In 21 st Meeting of the Association for Computational Linguistics (ACL'93), </booktitle> <pages> pages 121-129, </pages> <address> Columbus, Ohio, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: However, SLCFG retains most of the key features of SLTAG enumerated above. In particular, the probabilities in SLCFG are 2 Schabes & Waters directly linked to pairs of words. SLCFG is a stochastic extension of lexicalized context-free grammar (LCFG) <ref> [12, 13] </ref>. <p> The following sections, introduce LCFG, define the stochastic extension to SLCFG, present an algorithm that can determine the probability of a string generated by an SLCFG in O (n 3 )- time, and discuss the algorithms needed to train the parameters of an SLCFG. 2 LCFG Lexicalized context-free grammar (LCFG) <ref> [12, 13] </ref> is a tree generating system that is a restricted form of lexicalized tree-adjoining grammar (LTAG) [4]. The grammar consists of two sets of trees: initial trees, which are combined by substitution and auxiliary trees, which are combined by adjunction. <p> The remainder are auxiliary trees. An LCFG derivation must start with an initial tree rooted in S. After that, the tree can be repeatedly extended using substitution and adjunction. A derivation is complete when every frontier node is labeled with a terminal symbol. 1 In <ref> [13] </ref> these three kinds of auxiliary trees are referred to differently as right recursive, left recursive, and centrally recursive, respectively. <p> LTAG and TAG generate tree adjoining languages and have path sets that are context-free languages. LCFG is intermediate in nature. It can only generate context-free languages, but has path sets that are also context-free languages. 2.2 LCFG Lexicalizes CFG As shown in <ref> [12, 13] </ref> LCFG lexicalizes CFG without changing the trees derived. Further, a constructive procedure exists for converting any CFG G into an equivalent LCFG G 0 . <p> The fact that LCFG lexicalizes CFG is significant, because every other method for lexi-calizing CFGs without changing the trees derived requires context-sensitive operations [4] and therefore dramatically increases worst case processing time. As shown in <ref> [12, 13] </ref> (and in Section 4) LCFG can be parsed in the worst case just as quickly as CFG.
Reference: [14] <author> C. E. Shannon. </author> <title> Prediction and entropy of printed english. </title> <journal> The Bell System Technical Journal, </journal> <volume> 30 </volume> <pages> 50-64, </pages> <year> 1951. </year>
Reference-contexts: The second goal is to capture as many of the hierarchical constraints inherent in natural languages as possible. Unfortunately, these two goals have been more or less incompatible to date. Early stochastic proposals such as Markov Models, N-gram models <ref> [2, 14] </ref> and Hidden Markov Models [7] are very effective at capturing simple distributional information about adjacent words. However, they cannot capture long range distributional information nor the hierarchical constraints inherent in natural languages.
Reference: [15] <author> J. W. Thatcher. </author> <title> Characterizing derivations trees of context free grammars through a generalization of finite automata theory. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 5 </volume> <pages> 365-396, </pages> <year> 1971. </year>
Reference-contexts: The path set is a set of strings over [ N T [ f"g.) The path sets for CFG (and TSG) are regular languages <ref> [15] </ref>. In contrast, just as for LTAG and TAG, the path sets for LCFG are context-free languages.
References-found: 15

