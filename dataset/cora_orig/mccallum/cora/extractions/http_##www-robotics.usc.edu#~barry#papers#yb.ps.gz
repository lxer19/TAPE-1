URL: http://www-robotics.usc.edu/~barry/papers/yb.ps.gz
Refering-URL: http://www-robotics.usc.edu/~agents/people.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Quick 'n' Dirty Generalization For Mobile Robot Learning Content Areas: robotics, reinforcement learning, machine learning,
Abstract: The mobile robot domain challenges policy-iteration reinforcement learning algorithms with difficult problems of structural credit assignment and uncertainty. Structural credit assignment is particularly acute in domains where "real-time" trial length is a limiting factor on the number of learning steps that physical hardware can perform. Noisy sensors and effectors in complex dynamic environments further complicate the learning problem, leading to situations where speed of learning and policy flexibility may be more important than policy optimality. Input generalization addresses these problems but is typically too time consuming for robot domains. We present two algorithms, YB-learning and YB , that perform simple and fast generalization of the input space based on bit-similarity. The algorithms trade off long-term optimality for immediate performance and flexibility. The algorithms were tested in simulation against non-generalized learning across different numbers of discounting steps, and YB was shown to perform better during the earlier stages of learning, particularly in the presence of noise. In trials performed on a sonar-based mobile robot subject to uncertainty of the "real world," YB surpassed the simulation results by a wide margin, strongly supporting the role of such "quick and dirty" generalization strategies in noisy real-time mobile robot domains.
Abstract-found: 1
Intro-found: 1
Reference: <author> Chapman, D. & Kaelbling, L. P. </author> <year> (1991), </year> <title> Input Generalization in Delayed Reinforcement Learning: An Algorithm and Performance Comparisons, </title> <booktitle> in `Proceedings, IJCAI-91', </booktitle> <address> Sydney, Australia. </address>
Reference: <author> Drumheller, M. </author> <year> (1987), </year> <title> `Mobile Robot Localization Using Sonar', </title> <journal> IEEE Transactions on PAMI. </journal>
Reference-contexts: The compensation for sonar uncertainty alleviates to a small extent one of the difficulties of sonar perception. Ultrasound sensors are infamous for their specularity properties; as the sonar beam impacts a surface at an oblique angle, it bounces off and returns a falsely long reading <ref> (Drumheller 1987, Kuc & Di 1986) </ref>. Our strategy consisted of having the robot accumulate readings while vacillating between +15 and -15 degrees from the current heading. This provided repeated readings from slightly different angles which increased accuracy of obstacle detection.
Reference: <author> Kuc, R. & Di, Y. </author> <year> (1986), </year> <title> Intelligent Sensor Approach to Differentiating Sonar Reflections From Corners and Planes, </title> <booktitle> in `International Congress on Intelligent Autonomous Systems', </booktitle> <address> Amsterdam. </address>
Reference: <author> Lin, L.-J. </author> <year> (1993), </year> <title> Scaling Up Reinforcement Learning for Robot Control, </title> <booktitle> in `Proceedings, Tenth International Conference on Machine Learning', </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Mahadevan, S. </author> <year> (1996), </year> <title> Average-Reward Reinforcement Learning: Foundations, Algorithms and Empirical Results, </title> <journal> Machine Learning, </journal> <volume> bf 22, </volume> <pages> 159-196. </pages>
Reference-contexts: 1 Introduction The mobile robot domain remains a challenge for applying reinforcement learning techniques <ref> (Mahadevan & Kaelbling 1996) </ref>. Algorithms based on policy-iteration are faced with physically-grounded problems of structural credit assignment and noise, both with idiosyncratic characteristics. Structural credit assignment is particularly acute in domains where "real-time" trial length is a limiting factor.
Reference: <author> Mahadevan, S. & Connell, J. </author> <year> (1991), </year> <title> Automatic Programming of Behavior-based Robots using Reinforcement Learning, </title> <booktitle> in `Proceedings, AAAI-91', </booktitle> <address> Pittsburgh, PA, </address> <pages> pp. 8-14. </pages>
Reference: <author> Mahadevan, S. & Kaelbling, L. P. </author> <year> (1996), </year> <booktitle> `The National Science Foundation Workshop on Reinforcement Learning', AI Magazine 17(4), </booktitle> <pages> 89-97. </pages>
Reference-contexts: 1 Introduction The mobile robot domain remains a challenge for applying reinforcement learning techniques <ref> (Mahadevan & Kaelbling 1996) </ref>. Algorithms based on policy-iteration are faced with physically-grounded problems of structural credit assignment and noise, both with idiosyncratic characteristics. Structural credit assignment is particularly acute in domains where "real-time" trial length is a limiting factor.
Reference: <author> Mitchell, T. M. & Thrun, S. B. </author> <year> (1993), </year> <title> Explanation Based learning: A Comparison of Symbolic and Neural Network Approaches, </title> <booktitle> in `Tenth International Workshop on Machine Learning', </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 197-204. </pages>
Reference: <author> Ok, D. & Tadepalli, P. </author> <year> (1996), </year> <title> Auto-exploratory Average Reward Reinforcement Learning, </title> <booktitle> in `Proceedings, 14th International Conference on Artificial Intelligence (AAAI-92)', </booktitle> <publisher> AAAI Press/MIT Press, </publisher> <address> Portland, Oregon, </address> <pages> pp. 881-88. </pages>
Reference-contexts: Return to 1. 2.1 The Base learning algorithm Our basic learning algorithm combines the reward discounting and reward averaging techniques that are the most common in reinforcement learning ((Mahadevan 1996) <ref> (Ok & Tadepalli 1996) </ref>). The value of performing action a in state s is R (s;a) where R is the total accumulated reward received for previous performances of action a in state s, and N (s; a) is a normalization factor.
References-found: 9

