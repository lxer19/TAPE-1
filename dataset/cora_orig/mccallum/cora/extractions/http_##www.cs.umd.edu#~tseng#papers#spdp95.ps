URL: http://www.cs.umd.edu/~tseng/papers/spdp95.ps
Refering-URL: http://www.cs.umd.edu/projects/cosmic/papers.html
Root-URL: 
Email: E-mail address tseng@cs.umd.edu  
Title: Communication Analysis for Shared and Distributed Memory Machines  
Author: Chau-Wen Tseng 
Address: College Park, MD 20742  
Affiliation: Department of Computer Science University of Maryland  
Abstract: Advances in programming languages and parallelizing compilers are making parallel computers easier to use by providing a high-level portable programming model that protects software investment. However, experience has shown that simply finding parallelism is not always sufficient for obtaining good performance from today's multiprocessors, largely because the cost of interprocessor communication is much greater than computation or local memory accesses. To overcome this problem, I believe compilers need to perform communication analysis to locate and optimize interprocessor communication. I show how communication analysis has been used to improve performance for both shared and distributed memory machines, and describe a new project to apply these techniques to compilers for software distributed-shared-memory (DSM) systems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal, R. Bianchini, D. Chiaken, K. Johnson, D. Kratz, J. Kubiatowicz, B.-H. Lim, K. Mackenzie, and D. Yeung. </author> <title> The MIT Alewife machine: Architecture and performance. </title> <booktitle> In Proceedings of the 22th International Symposium on Computer Architecture, </booktitle> <address> Santa Margherita Ligure, Italy, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: By taking advantage of software DSMs, shared-memory compilers can quickly compile a wide range of applications for message-passing machines. Reasonable performance should be obtainable for many coarse-grain parallel applications. A recent development that makes compilers for software DSMs more desirable is the development of Flexible-Shared-Memory (FSM) machines (e.g, Alewife <ref> [1] </ref>, Flash [30, 40], Typhoon [51, 53]). These architectures maintain a coherent shared address space on top of phys-ically distributed memories, just like traditional shared-memory machines. In addition, FSM machines also support extensible memory coherence protocols and explicit messages where needed to achieve better performance.
Reference: [2] <author> J. R. Allen and K. Kennedy. </author> <title> Automatic translation of Fortran programs to vector form. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(4) </volume> <pages> 491-542, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: The most important communication optimization is message vectorization, a loop-based optimization fundamental to the Fortran D compilation process [9, 24]. It uses the results of data dependence analysis <ref> [2, 39] </ref> to extract communication identified by communication analysis from within loops, combining element messages per loop iteration with one vectorized message preceding the loop. Because overhead for messages is high, combining messages can significantly reduce communication cost.
Reference: [3] <author> S. Amarasinghe and M. Lam. </author> <title> Communication optimization and code generation for distributed memory machines. </title> <booktitle> In Proceedings of the SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <address> Albuquerque, NM, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: If a solution to the system exists, inter-processor data movement takes place and must be synchronized. The solution calculated by the scan can then be used to calculate the identities of the communication processors, the nonlocal data accessed, and the loop iterations where the data is defined <ref> [3] </ref>. Even after communication analysis identifies inter-processor data movement, it may still be possible to lower synchronization cost by replacing barriers with less expensive forms of point-to-point synchronization such as counters. The compiler tests communication to see whether it fits simple patterns allowing efficient producer/consumer synchronization. <p> The advanced SUIF compiler builds on shared-memory compiler algorithms for identifying parallelism [11, 27, 26, 60] and performing program transformations [62, 63], as well as distributed-memory compilation techniques to select data decompositions [7] and explicitly manage address translation and data movement <ref> [3, 31] </ref>. Many previous researchers have examined performance issues on shared-memory architectures. Singh et al. [54] applied optimizations similar to those in SUIF by hand to representative computations in order to gain good performance.
Reference: [4] <author> C. Ancourt and F. Irigoin. </author> <title> Scanning polyhedra with do loops. </title> <booktitle> In Proceedings of the Third ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Williamsburg, VA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: The SUIF compiler represents communication and computation using symbolic linear inequalities consisting of array subscript expressions and the computation partition. Detecting communication is then equivalent to solving the system of symbolic linear inequalities, scanning the system using Fourier-Motzkin elimination <ref> [4, 5] </ref>. If a solution to the system exists, inter-processor data movement takes place and must be synchronized. The solution calculated by the scan can then be used to calculate the identities of the communication processors, the nonlocal data accessed, and the loop iterations where the data is defined [3].
Reference: [5] <author> C. Ancourt and F. Irigoin. </author> <title> Automatic code distribution. </title> <booktitle> In Proceedings of the Third Workshop on Compilers for Parallel Computers, </booktitle> <address> Vienna, Austria, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: The SUIF compiler represents communication and computation using symbolic linear inequalities consisting of array subscript expressions and the computation partition. Detecting communication is then equivalent to solving the system of symbolic linear inequalities, scanning the system using Fourier-Motzkin elimination <ref> [4, 5] </ref>. If a solution to the system exists, inter-processor data movement takes place and must be synchronized. The solution calculated by the scan can then be used to calculate the identities of the communication processors, the nonlocal data accessed, and the loop iterations where the data is defined [3].
Reference: [6] <author> J. Anderson, S. Amarasinghe, and M. Lam. </author> <title> Data and computation transformation for multiprocessors. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Fortunately, shared-memory compilers can adapt distributed-memory compilation techniques to change data layout and improve performance [59]. The key is to identify data accessed by each processor and change the data layout to make it contiguous, improving spatial locality and reducing memory system effects <ref> [6] </ref>. In many cases spatial locality may be accomplished by adding an additional dimension to distributed arrays and indexing it according to the processor id. The address calculations for the transformed arrays may contain expensive modulo and division operations. <p> The address calculations for the transformed arrays may contain expensive modulo and division operations. Scalar optimizations are thus also needed to improve the performance of these address calculations <ref> [6] </ref>. 4.3 Experimental Results To evaluate their impact, the synchronization and data layout optimizations described were implemented in the SUIF parallelizing compiler [61].
Reference: [7] <author> J. Anderson and M. Lam. </author> <title> Global optimizations for parallelism and locality on scalable parallel machines. </title> <booktitle> In Proceedings of the SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <address> Albuquerque, NM, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: The advanced SUIF compiler builds on shared-memory compiler algorithms for identifying parallelism [11, 27, 26, 60] and performing program transformations [62, 63], as well as distributed-memory compilation techniques to select data decompositions <ref> [7] </ref> and explicitly manage address translation and data movement [3, 31]. Many previous researchers have examined performance issues on shared-memory architectures. Singh et al. [54] applied optimizations similar to those in SUIF by hand to representative computations in order to gain good performance.
Reference: [8] <institution> Applied Parallel Research, Placerville, CA. </institution> <note> Forge 90 Distributed Memory Parallelizer: User's Guide, version 8.0 edition, </note> <year> 1992. </year>
Reference-contexts: Compared with other contemporary systems <ref> [8, 14, 16, 17, 34, 48] </ref>, The Fortran D compiler is less flexible but performs deeper compile-time analysis, many more advanced optimizations, requires fewer language extensions, and relies on less run-time support. Few researchers have published experimental results for large programs.
Reference: [9] <author> V. Balasundaram, G. Fox, K. Kennedy, and U. Kre-mer. </author> <title> An interactive environment for data partitioning and distribution. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: The compiler can then use this information for optimizations, some of which are shown in the figure at the top of the next page. The most important communication optimization is message vectorization, a loop-based optimization fundamental to the Fortran D compilation process <ref> [9, 24] </ref>. It uses the results of data dependence analysis [2, 39] to extract communication identified by communication analysis from within loops, combining element messages per loop iteration with one vectorized message preceding the loop. Because overhead for messages is high, combining messages can significantly reduce communication cost.
Reference: [10] <author> J. Bennett, J. Carter, and W. Zwaenepoel. Munin: </author> <title> Distributed shared memory based on type-specific memory coherence. </title> <booktitle> In Proceedings of the Second ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Seattle, WA, </address> <month> March </month> <year> 1990. </year>
Reference-contexts: One approach for building parallel machines is software distributed-shared-memory (DSM) (e.g., Ivy [43], Treadmarks [20]). These systems use operating systems support to provide a shared address space on top of existing message-passing machines and networks of workstations. The latest generation of software DSMs (e.g, Munin <ref> [10] </ref>, Blizzard/Tempest [22], CVM [36]) also support multiple coherence protocols and explicit messages. Communication in software DSMs is expensive because high software overhead are imposed on top of the hardware costs. Despite this penalty, software DSMs have desirable qualities because they provide a simpler programming interface for users and compilers.
Reference: [11] <author> W. Blume et al. </author> <title> Polaris: </title> <booktitle> The next generation in par-allelizing compilers,. In Proceedings of the Seventh Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Ithaca, NY, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: Re--sults for the first ten Livermore kernels show the compiler is usually within a factor of two of hand-coded programs. The advanced SUIF compiler builds on shared-memory compiler algorithms for identifying parallelism <ref> [11, 27, 26, 60] </ref> and performing program transformations [62, 63], as well as distributed-memory compilation techniques to select data decompositions [7] and explicitly manage address translation and data movement [3, 31]. Many previous researchers have examined performance issues on shared-memory architectures.
Reference: [12] <author> W. Bolosky and M. Scott. </author> <title> False sharing and its effect on shared memory performance. </title> <booktitle> In Proceedings of the USENIX Symposium on Experiences with Distributed and Multiprocessor Systems (SEDMS IV), </booktitle> <address> San Diego, CA, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: Many previous researchers have examined performance issues on shared-memory architectures. Singh et al. [54] applied optimizations similar to those in SUIF by hand to representative computations in order to gain good performance. Others evaluated the benefits of co-locating data and computation [23, 44], as well as false sharing <ref> [12, 21, 25, 41, 55] </ref>. These approaches focused on individual optimizations and were generally applied by hand. Researchers compiling for SIMD languages such as Fortran 90 and Modula-2* have been faced with a programming model that assumed the existence of barriers following each expression evaluation [29, 48, 50].
Reference: [13] <author> Z. Bozkus, A. Choudhary, G. Fox, T. Haupt, S. Ranka, and M. Wu. </author> <title> Compiling Fortran 90D/HPF for dis-tributed memory MIMD computers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 21(1) </volume> <pages> 15-26, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Olander & Schnabel show that DINO programs can be significantly improved through iteration reordering and pipelining [47]. Bozkus et al. describe the a Fortran 90D compiler that translates Fortran 90 array constructs into calls to a portable run-time library <ref> [13] </ref>. Re--sults for the first ten Livermore kernels show the compiler is usually within a factor of two of hand-coded programs.
Reference: [14] <author> T. Brandes. </author> <title> Efficient data parallel programming without explicit message passing for distributed memory multiprocessors. </title> <type> Internal Report AHR-92-4, </type> <institution> High Performance Computing Center, GMD, </institution> <month> September </month> <year> 1992. </year>
Reference-contexts: Compared with other contemporary systems <ref> [8, 14, 16, 17, 34, 48] </ref>, The Fortran D compiler is less flexible but performs deeper compile-time analysis, many more advanced optimizations, requires fewer language extensions, and relies on less run-time support. Few researchers have published experimental results for large programs.
Reference: [15] <author> D. Callahan and K. Kennedy. </author> <title> Compiling programs for distributed-memory multiprocessors. </title> <journal> Journal of Supercomputing, </journal> <volume> 2 </volume> <pages> 151-169, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: The two major compilation steps are partitioning the data and computation across processors, then introducing communication for nonlocal accesses where needed. The compiler partitions computation across processors using the owner computes rulewhere each processor only computes values of data it owns <ref> [15, 24, 49] </ref>. The Fortran D project contributed to the development of High Performance Fortran (HPF), an informal Fortran standard adopted by researchers and vendors for programming massively-parallel processors [37]. The success of HPF hinges on the development of compilers that can provide performance satisfactory to users. <p> I have a preliminary implementation working, and am working on incorporating new optimizations to exploit the multiple coherence protocols and hybrid message-passing capabilities of the CVM system. 6 Related Work The Fortran D compiler is a second-generation distributed-memory compiler that incorporates and extends features from previous compilation systems <ref> [15, 24, 38, 42, 49] </ref>. Compared with other contemporary systems [8, 14, 16, 17, 34, 48], The Fortran D compiler is less flexible but performs deeper compile-time analysis, many more advanced optimizations, requires fewer language extensions, and relies on less run-time support.
Reference: [16] <author> B. Chapman, P. Mehrotra, and H. Zima. </author> <title> Programming in Vienna Fortran. </title> <journal> Scientific Programming, </journal> <volume> 1(1) </volume> <pages> 31-50, </pages> <month> Fall </month> <year> 1992. </year>
Reference-contexts: Compared with other contemporary systems <ref> [8, 14, 16, 17, 34, 48] </ref>, The Fortran D compiler is less flexible but performs deeper compile-time analysis, many more advanced optimizations, requires fewer language extensions, and relies on less run-time support. Few researchers have published experimental results for large programs.
Reference: [17] <author> C. Chase, A. Cheung, A. Reeves, and M. Smith. </author> <title> Paragon: A parallel programming environment for scientific applications using communication structures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 16(2) </volume> <pages> 79-91, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Compared with other contemporary systems <ref> [8, 14, 16, 17, 34, 48] </ref>, The Fortran D compiler is less flexible but performs deeper compile-time analysis, many more advanced optimizations, requires fewer language extensions, and relies on less run-time support. Few researchers have published experimental results for large programs.
Reference: [18] <author> K. Cooper, M. W. Hall, R. T. Hood, K. Kennedy, K. S. McKinley, J. M. Mellor-Crummey, L. Torczon, and S. K. Warren. </author> <title> The ParaScope parallel programming environment. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 81(2) </volume> <pages> 244-263, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: It is implemented as a source-to-source Fortran translator in the context of the ParaScope parallel programming environment <ref> [18] </ref>.
Reference: [19] <author> R. Cytron, J. Lipkis, and E. Schonberg. </author> <title> A compiler-assisted approach to SPMD execution. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <address> New York, NY, </address> <month> November </month> <year> 1990. </year>
Reference-contexts: Cytron et al. were the first to explore the possibilities of exploiting SPMD code for shared-memory multiprocessors, concentrating on safety concerns and the effect on privatization <ref> [19] </ref>. O'Boyle and Bodin apply a classification algorithm to identify data dependences that cross processor boundaries, then apply heuristics based on max-cut to insert barrier synchronization and satisfy dependence [46]. Good results are reported for two programs.
Reference: [20] <author> S. Dwarkadas, P. Keleher, A. Cox, and W. Zwaenepoel. </author> <title> An evaluation of software distributed shared memory for next-generation processors and networks. </title> <booktitle> In Proceedings of the 20th International Symposium on Computer Architecture, </booktitle> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: With enough processors, even these two programs will significantly benefit from optimizations. 5 Compiling for Software DSMs Finally, consider the role of communication analysis for future parallel architectures. One approach for building parallel machines is software distributed-shared-memory (DSM) (e.g., Ivy [43], Treadmarks <ref> [20] </ref>). These systems use operating systems support to provide a shared address space on top of existing message-passing machines and networks of workstations. The latest generation of software DSMs (e.g, Munin [10], Blizzard/Tempest [22], CVM [36]) also support multiple coherence protocols and explicit messages.
Reference: [21] <author> S. J. Eggers and T. E. Jeremiassen. </author> <title> Eliminating false sharing. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: Many previous researchers have examined performance issues on shared-memory architectures. Singh et al. [54] applied optimizations similar to those in SUIF by hand to representative computations in order to gain good performance. Others evaluated the benefits of co-locating data and computation [23, 44], as well as false sharing <ref> [12, 21, 25, 41, 55] </ref>. These approaches focused on individual optimizations and were generally applied by hand. Researchers compiling for SIMD languages such as Fortran 90 and Modula-2* have been faced with a programming model that assumed the existence of barriers following each expression evaluation [29, 48, 50].
Reference: [22] <author> B. Falsafi, A. Lebeck, S. Reinhardt, I. Schoinas, M. Hill, J. Larus, A. Rogers, and D. Wood. </author> <title> Application-specific protocols for user-level shared memory. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <address> Washington, DC, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: One approach for building parallel machines is software distributed-shared-memory (DSM) (e.g., Ivy [43], Treadmarks [20]). These systems use operating systems support to provide a shared address space on top of existing message-passing machines and networks of workstations. The latest generation of software DSMs (e.g, Munin [10], Blizzard/Tempest <ref> [22] </ref>, CVM [36]) also support multiple coherence protocols and explicit messages. Communication in software DSMs is expensive because high software overhead are imposed on top of the hardware costs. Despite this penalty, software DSMs have desirable qualities because they provide a simpler programming interface for users and compilers.
Reference: [23] <author> J. Fang and M. Lu. </author> <title> A solution of cache ping-pong problem in RISC based parallel processing systems. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: Many previous researchers have examined performance issues on shared-memory architectures. Singh et al. [54] applied optimizations similar to those in SUIF by hand to representative computations in order to gain good performance. Others evaluated the benefits of co-locating data and computation <ref> [23, 44] </ref>, as well as false sharing [12, 21, 25, 41, 55]. These approaches focused on individual optimizations and were generally applied by hand.
Reference: [24] <author> M. Gerndt. </author> <title> Updating distributed variables in local computations. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 2(3) </volume> <pages> 171-193, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: The two major compilation steps are partitioning the data and computation across processors, then introducing communication for nonlocal accesses where needed. The compiler partitions computation across processors using the owner computes rulewhere each processor only computes values of data it owns <ref> [15, 24, 49] </ref>. The Fortran D project contributed to the development of High Performance Fortran (HPF), an informal Fortran standard adopted by researchers and vendors for programming massively-parallel processors [37]. The success of HPF hinges on the development of compilers that can provide performance satisfactory to users. <p> The compiler can then use this information for optimizations, some of which are shown in the figure at the top of the next page. The most important communication optimization is message vectorization, a loop-based optimization fundamental to the Fortran D compilation process <ref> [9, 24] </ref>. It uses the results of data dependence analysis [2, 39] to extract communication identified by communication analysis from within loops, combining element messages per loop iteration with one vectorized message preceding the loop. Because overhead for messages is high, combining messages can significantly reduce communication cost. <p> I have a preliminary implementation working, and am working on incorporating new optimizations to exploit the multiple coherence protocols and hybrid message-passing capabilities of the CVM system. 6 Related Work The Fortran D compiler is a second-generation distributed-memory compiler that incorporates and extends features from previous compilation systems <ref> [15, 24, 38, 42, 49] </ref>. Compared with other contemporary systems [8, 14, 16, 17, 34, 48], The Fortran D compiler is less flexible but performs deeper compile-time analysis, many more advanced optimizations, requires fewer language extensions, and relies on less run-time support.
Reference: [25] <author> E. Granston and H. Wishoff. </author> <title> Managing pages in shared virtual memory systems: Getting the compiler into the game. </title> <booktitle> In Proceedings of the 1993 ACM International Conference on Supercomputing, </booktitle> <address> Tokyo, Japan, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: Many previous researchers have examined performance issues on shared-memory architectures. Singh et al. [54] applied optimizations similar to those in SUIF by hand to representative computations in order to gain good performance. Others evaluated the benefits of co-locating data and computation [23, 44], as well as false sharing <ref> [12, 21, 25, 41, 55] </ref>. These approaches focused on individual optimizations and were generally applied by hand. Researchers compiling for SIMD languages such as Fortran 90 and Modula-2* have been faced with a programming model that assumed the existence of barriers following each expression evaluation [29, 48, 50].
Reference: [26] <author> M. Hall, S. Amarasinghe, B. Murphy, S. Liao, and M. Lam. </author> <title> Detecting coarse-grain parallelism using an interprocedural parallelizing compiler. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <address> San Diego, CA, </address> <month> Decem-ber </month> <year> 1995. </year>
Reference-contexts: Results show that even if the compiler is able to detect significant parallelism, performance can be rather poor. The simulation study analyzed the performance of a collection of programs that were successfully (88-100%) par-allelized by the SUIF interprocedural parallelizer <ref> [27, 26] </ref>. Speedups were measured for both an ideal multiprocessor with 1-cycle nonlocal memory access and barrier synchronization latency, as well as a more realistic multiprocessor with 130-cycle access latencies and a 128 KB, 4-way associative cache. <p> Re--sults for the first ten Livermore kernels show the compiler is usually within a factor of two of hand-coded programs. The advanced SUIF compiler builds on shared-memory compiler algorithms for identifying parallelism <ref> [11, 27, 26, 60] </ref> and performing program transformations [62, 63], as well as distributed-memory compilation techniques to select data decompositions [7] and explicitly manage address translation and data movement [3, 31]. Many previous researchers have examined performance issues on shared-memory architectures.
Reference: [27] <author> M. W. Hall, S. Amarasinghe, and B. Murphy. </author> <title> In-terprocedural analysis for parallelization: Design and experience. </title> <booktitle> In Proceedings of the Seventh SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <address> San Francisco, CA, </address> <month> February </month> <year> 1995. </year>
Reference-contexts: Results show that even if the compiler is able to detect significant parallelism, performance can be rather poor. The simulation study analyzed the performance of a collection of programs that were successfully (88-100%) par-allelized by the SUIF interprocedural parallelizer <ref> [27, 26] </ref>. Speedups were measured for both an ideal multiprocessor with 1-cycle nonlocal memory access and barrier synchronization latency, as well as a more realistic multiprocessor with 130-cycle access latencies and a 128 KB, 4-way associative cache. <p> Re--sults for the first ten Livermore kernels show the compiler is usually within a factor of two of hand-coded programs. The advanced SUIF compiler builds on shared-memory compiler algorithms for identifying parallelism <ref> [11, 27, 26, 60] </ref> and performing program transformations [62, 63], as well as distributed-memory compilation techniques to select data decompositions [7] and explicitly manage address translation and data movement [3, 31]. Many previous researchers have examined performance issues on shared-memory architectures.
Reference: [28] <author> M. W. Hall, S. Hiranandani, K. Kennedy, and C.- W. Tseng. </author> <title> Interprocedural compilation of Fortran D for MIMD distributed-memory machines. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <address> Minneapolis, MN, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: Fortran D is a version of Fortran extended to allow users or automatic tools to specify how data may be partitioned onto processors. It is designed to provide a simple yet efficient machine-independent data-parallel programming model, shifting the burden of optimizations to the compiler <ref> [28, 31, 57] </ref>. Given a data decomposition, the Fortran D compiler automatically translates sequential programs into efficient parallel programs, using communication analysis to determine what data needs to be communicated and inserting the necessary messages.
Reference: [29] <author> P. Hatcher and M. Quinn. </author> <title> Data-parallel Programming on MIMD Computers. </title> <publisher> The MIT Press, </publisher> <address> Cam-bridge, MA, </address> <year> 1991. </year>
Reference-contexts: These approaches focused on individual optimizations and were generally applied by hand. Researchers compiling for SIMD languages such as Fortran 90 and Modula-2* have been faced with a programming model that assumed the existence of barriers following each expression evaluation <ref> [29, 48, 50] </ref>. Simple data dependence analysis can be used to reduce barrier synchronization by orders of magnitude, greatly improving performance. In comparison, SUIF begin with parallel loops discovered by parallelizing compilers, where the remaining barriers are significantly harder to remove. <p> In comparison, SUIF begin with parallel loops discovered by parallelizing compilers, where the remaining barriers are significantly harder to remove. For barriers separating statements on the same loop level, Hatcher and Quinn use a two-dimensional radix sort to find the minimal number of barriers <ref> [29] </ref>. Philippsen and Heinz find the minimal number of barriers with an algorithm based on topological sort; they also attempt to minimize the amount of storage needed for intermediate results [48].
Reference: [30] <author> M. Heinrich, J. Kuskin, D. Ofelt, J. Heinlein, J.- P. Singh, R. Simoni, K. Gharachorloo, J. Baxter, D. Nakahira, M. Horowitz, A. Gupta, M. Rosen-blum, and J. Hennessy. </author> <title> The performance impact of flexibility in the Stanford FLASH multiprocessor. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VI), </booktitle> <address> San Jose, CA, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: Reasonable performance should be obtainable for many coarse-grain parallel applications. A recent development that makes compilers for software DSMs more desirable is the development of Flexible-Shared-Memory (FSM) machines (e.g, Alewife [1], Flash <ref> [30, 40] </ref>, Typhoon [51, 53]). These architectures maintain a coherent shared address space on top of phys-ically distributed memories, just like traditional shared-memory machines. In addition, FSM machines also support extensible memory coherence protocols and explicit messages where needed to achieve better performance.
Reference: [31] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Communication analysis is a set of techniques used to track the flow of data between processors. It was first developed in compilers for distributed-memory machines in order to identify where messages must be inserted into a program to access data on other processors <ref> [31] </ref>. However, I will show communication analysis is applicable to all multiprocessor architectures. Communication analysis relies on some knowledge of how computation is partitioned across processors. <p> Fortran D is a version of Fortran extended to allow users or automatic tools to specify how data may be partitioned onto processors. It is designed to provide a simple yet efficient machine-independent data-parallel programming model, shifting the burden of optimizations to the compiler <ref> [28, 31, 57] </ref>. Given a data decomposition, the Fortran D compiler automatically translates sequential programs into efficient parallel programs, using communication analysis to determine what data needs to be communicated and inserting the necessary messages. <p> The advanced SUIF compiler builds on shared-memory compiler algorithms for identifying parallelism [11, 27, 26, 60] and performing program transformations [62, 63], as well as distributed-memory compilation techniques to select data decompositions [7] and explicitly manage address translation and data movement <ref> [3, 31] </ref>. Many previous researchers have examined performance issues on shared-memory architectures. Singh et al. [54] applied optimizations similar to those in SUIF by hand to representative computations in order to gain good performance.
Reference: [32] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Preliminary experiences with the Fortran D compiler. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <address> Portland, OR, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: This information can then be combined with traditional data dependence and data flow analyses to guide compiler optimizations to reduce interprocessor communication. This paper summarizes results from a number of papers <ref> [33, 32, 58, 59] </ref> to demonstrate how communication analysis is useful for both shared and distributed memory machines to improve performance. <p> performing dependence analysis, program transformations, and interproce-dural analysis. 3.2.1 Hand-Optimized Code Comparison To evaluate the status of the prototype Fortran D compiler, the output of the Fortran D compiler was compared with hand-optimized programs on the Intel iPSC/860 and the output of the CM Fortran compiler on the TMC CM-5 <ref> [32] </ref>. The comparisons with hand-optimized programs are presented in the figure on the top half of the next page. Solid and dashed lines correspond to speedups for hand-optimized and Fortran D compiler-generated programs, respectively.
Reference: [33] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Evaluating compiler optimizations for Fortran D. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 21(1) </volume> <pages> 27-45, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: This information can then be combined with traditional data dependence and data flow analyses to guide compiler optimizations to reduce interprocessor communication. This paper summarizes results from a number of papers <ref> [33, 32, 58, 59] </ref> to demonstrate how communication analysis is useful for both shared and distributed memory machines to improve performance. <p> In this section we illustrate how communication analysis can be used to reduce communication costs for distributed-memory machines. 3.1 Communication Optimizations The Fortran D compiler performs a large number of communication and parallelism optimizations based on combining communication analysis with standard data flow and dependence analyses <ref> [33] </ref>. Communication analysis is key because it identifies data that must be communicated, as well as the iterations defining and accessing the data and the processors involved.
Reference: [34] <author> K. Ikudome, G. Fox, A. Kolawa, and J. Flower. </author> <title> An automatic and symbolic parallelization system for distributed memory parallel computers. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: Compared with other contemporary systems <ref> [8, 14, 16, 17, 34, 48] </ref>, The Fortran D compiler is less flexible but performs deeper compile-time analysis, many more advanced optimizations, requires fewer language extensions, and relies on less run-time support. Few researchers have published experimental results for large programs.
Reference: [35] <author> T. Jeremiassen and S. Eggers. </author> <title> Static analysis of barrier synchronization in explicitly parallel systems. </title> <booktitle> In Proceedings of the International Conference on Parallel Architectures and Compilation Techniques (PACT), </booktitle> <address> Montreal, Canada, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: Good results are reported for two programs. Finally, a number of researchers have developed techniques for analyzing synchronization in explicitly parallel programs <ref> [35, 45] </ref>. 7 Conclusion Communication analysis is a set of compilation techniques to identify interprocessor communication in parallel programs. Because of the high expense of interprocessor communication, I believe communication analysis need to be a fundamental part of parallelizing compilers for advanced parallel systems.
Reference: [36] <author> P. Keleher. </author> <title> Multiple writers considered harmful. </title> <type> Technical Report CS-TR-3543, </type> <institution> Dept. of Computer Science, University of Maryland at College Park, </institution> <month> Oc-tober </month> <year> 1995. </year>
Reference-contexts: One approach for building parallel machines is software distributed-shared-memory (DSM) (e.g., Ivy [43], Treadmarks [20]). These systems use operating systems support to provide a shared address space on top of existing message-passing machines and networks of workstations. The latest generation of software DSMs (e.g, Munin [10], Blizzard/Tempest [22], CVM <ref> [36] </ref>) also support multiple coherence protocols and explicit messages. Communication in software DSMs is expensive because high software overhead are imposed on top of the hardware costs. Despite this penalty, software DSMs have desirable qualities because they provide a simpler programming interface for users and compilers. <p> I plan to answer this question by developing a compiler for software DSMs based on the SUIF parallelizing compiler. The compiler is based on the Stanford SUIF compiler, and targets the Coherent Virtual Machine (CVM) software DSM system under development at the University of Maryland <ref> [36] </ref>. By exploiting the operating system support and advanced features of software DSMs such as integrated messaging and multiple coherence protocols, I hope the resulting compiler can approach or exceed the performance of optimized message-passing programs on distributed-memory machines.
Reference: [37] <author> C. Koelbel, D. Loveman, R. Schreiber, G. Steele, Jr., and M. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: The compiler partitions computation across processors using the owner computes rulewhere each processor only computes values of data it owns [15, 24, 49]. The Fortran D project contributed to the development of High Performance Fortran (HPF), an informal Fortran standard adopted by researchers and vendors for programming massively-parallel processors <ref> [37] </ref>. The success of HPF hinges on the development of compilers that can provide performance satisfactory to users.
Reference: [38] <author> C. Koelbel and P. Mehrotra. </author> <title> Programming data parallel algorithms on distributed memory machines using Kali. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: I have a preliminary implementation working, and am working on incorporating new optimizations to exploit the multiple coherence protocols and hybrid message-passing capabilities of the CVM system. 6 Related Work The Fortran D compiler is a second-generation distributed-memory compiler that incorporates and extends features from previous compilation systems <ref> [15, 24, 38, 42, 49] </ref>. Compared with other contemporary systems [8, 14, 16, 17, 34, 48], The Fortran D compiler is less flexible but performs deeper compile-time analysis, many more advanced optimizations, requires fewer language extensions, and relies on less run-time support. <p> Few researchers have published experimental results for large programs. Pingali & Rogers apply message vector-ization, message pipelining, and reduction recognition in ID NOUVEAU to parallelize SIMPLE [52]. Koelbel & Mehrotra are able to parallelize ADI integration in KALI by implicitly applying dynamic data decomposition between computation phases <ref> [38] </ref>. Olander & Schnabel show that DINO programs can be significantly improved through iteration reordering and pipelining [47]. Bozkus et al. describe the a Fortran 90D compiler that translates Fortran 90 array constructs into calls to a portable run-time library [13].
Reference: [39] <author> D. Kuck, R. Kuhn, D. Padua, B. Leasure, and M. J. Wolfe. </author> <title> Dependence graphs and compiler optimizations. </title> <booktitle> In Conference Record of the Eighth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Williamsburg, VA, </address> <month> January </month> <year> 1981. </year>
Reference-contexts: The most important communication optimization is message vectorization, a loop-based optimization fundamental to the Fortran D compilation process [9, 24]. It uses the results of data dependence analysis <ref> [2, 39] </ref> to extract communication identified by communication analysis from within loops, combining element messages per loop iteration with one vectorized message preceding the loop. Because overhead for messages is high, combining messages can significantly reduce communication cost.
Reference: [40] <author> J. Kuskin, D. Ofelt, M. Heinrich, J. Heinlein, R. Si-mon, K. Gharachorloo, J. Chapin, D. Nakahira, J. Baxter, M. Horowitz, A. Gupta, M. Rosenblum, and J. Hennessy. </author> <title> The Stanford FLASH multiprocessor. </title> <booktitle> In Proceedings of the 21th International Symposium on Computer Architecture, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: Reasonable performance should be obtainable for many coarse-grain parallel applications. A recent development that makes compilers for software DSMs more desirable is the development of Flexible-Shared-Memory (FSM) machines (e.g, Alewife [1], Flash <ref> [30, 40] </ref>, Typhoon [51, 53]). These architectures maintain a coherent shared address space on top of phys-ically distributed memories, just like traditional shared-memory machines. In addition, FSM machines also support extensible memory coherence protocols and explicit messages where needed to achieve better performance.
Reference: [41] <author> H. Li and K. Sevcik. NUMACROS: </author> <title> Data parallel programming on NUMA multiprocessors. </title> <booktitle> In Proceedings of the USENIX Symposium on Experiences with Distributed and Multiprocessor Systems (SEDMS IV), </booktitle> <address> San Diego, CA, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: Many previous researchers have examined performance issues on shared-memory architectures. Singh et al. [54] applied optimizations similar to those in SUIF by hand to representative computations in order to gain good performance. Others evaluated the benefits of co-locating data and computation [23, 44], as well as false sharing <ref> [12, 21, 25, 41, 55] </ref>. These approaches focused on individual optimizations and were generally applied by hand. Researchers compiling for SIMD languages such as Fortran 90 and Modula-2* have been faced with a programming model that assumed the existence of barriers following each expression evaluation [29, 48, 50].
Reference: [42] <author> J. Li and M. Chen. </author> <title> Compiling communication-efficient programs for massively parallel machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 361-376, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: I have a preliminary implementation working, and am working on incorporating new optimizations to exploit the multiple coherence protocols and hybrid message-passing capabilities of the CVM system. 6 Related Work The Fortran D compiler is a second-generation distributed-memory compiler that incorporates and extends features from previous compilation systems <ref> [15, 24, 38, 42, 49] </ref>. Compared with other contemporary systems [8, 14, 16, 17, 34, 48], The Fortran D compiler is less flexible but performs deeper compile-time analysis, many more advanced optimizations, requires fewer language extensions, and relies on less run-time support.
Reference: [43] <author> K. Li and P. Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> IEEE Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: With enough processors, even these two programs will significantly benefit from optimizations. 5 Compiling for Software DSMs Finally, consider the role of communication analysis for future parallel architectures. One approach for building parallel machines is software distributed-shared-memory (DSM) (e.g., Ivy <ref> [43] </ref>, Treadmarks [20]). These systems use operating systems support to provide a shared address space on top of existing message-passing machines and networks of workstations. The latest generation of software DSMs (e.g, Munin [10], Blizzard/Tempest [22], CVM [36]) also support multiple coherence protocols and explicit messages.
Reference: [44] <author> E. Markatos and T. LeBlanc. </author> <title> Using processor affinity in loop scheduling on shared-memory multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 5(4) </volume> <pages> 379-400, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Many previous researchers have examined performance issues on shared-memory architectures. Singh et al. [54] applied optimizations similar to those in SUIF by hand to representative computations in order to gain good performance. Others evaluated the benefits of co-locating data and computation <ref> [23, 44] </ref>, as well as false sharing [12, 21, 25, 41, 55]. These approaches focused on individual optimizations and were generally applied by hand.
Reference: [45] <author> S. Midkiff and D. Padua. </author> <title> Compiler generated synchronization for DO loops. </title> <booktitle> In Proceedings of the 1986 International Conference on Parallel Processing, </booktitle> <pages> pages 544-551, </pages> <address> St. Charles, IL, </address> <month> August </month> <year> 1986. </year>
Reference-contexts: Good results are reported for two programs. Finally, a number of researchers have developed techniques for analyzing synchronization in explicitly parallel programs <ref> [35, 45] </ref>. 7 Conclusion Communication analysis is a set of compilation techniques to identify interprocessor communication in parallel programs. Because of the high expense of interprocessor communication, I believe communication analysis need to be a fundamental part of parallelizing compilers for advanced parallel systems.
Reference: [46] <author> M. O'Boyle and F. Bodin. </author> <title> Compiler reduction of synchronization in shared virtual memory systems. </title> <booktitle> In Proceedings of the 1995 ACM International Conference on Supercomputing, </booktitle> <address> Barcelona, Spain, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: O'Boyle and Bodin apply a classification algorithm to identify data dependences that cross processor boundaries, then apply heuristics based on max-cut to insert barrier synchronization and satisfy dependence <ref> [46] </ref>. Good results are reported for two programs. Finally, a number of researchers have developed techniques for analyzing synchronization in explicitly parallel programs [35, 45]. 7 Conclusion Communication analysis is a set of compilation techniques to identify interprocessor communication in parallel programs.
Reference: [47] <author> D. Olander and R. Schnabel. </author> <title> Preliminary experience in developing a parallel thin-layer Navier Stokes code and implications for parallel language design. </title> <booktitle> In Proceedings of the 1992 Scalable High Performance Computing Conference, </booktitle> <address> Williamsburg, VA, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: Koelbel & Mehrotra are able to parallelize ADI integration in KALI by implicitly applying dynamic data decomposition between computation phases [38]. Olander & Schnabel show that DINO programs can be significantly improved through iteration reordering and pipelining <ref> [47] </ref>. Bozkus et al. describe the a Fortran 90D compiler that translates Fortran 90 array constructs into calls to a portable run-time library [13]. Re--sults for the first ten Livermore kernels show the compiler is usually within a factor of two of hand-coded programs.
Reference: [48] <author> M. Philippsen and E. Heinz. </author> <title> Automatic synchronization elimination in synchronous FORALLs. </title> <booktitle> In Frontiers '95: The 5th Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <address> McLean, VA, </address> <month> February </month> <year> 1995. </year>
Reference-contexts: Compared with other contemporary systems <ref> [8, 14, 16, 17, 34, 48] </ref>, The Fortran D compiler is less flexible but performs deeper compile-time analysis, many more advanced optimizations, requires fewer language extensions, and relies on less run-time support. Few researchers have published experimental results for large programs. <p> These approaches focused on individual optimizations and were generally applied by hand. Researchers compiling for SIMD languages such as Fortran 90 and Modula-2* have been faced with a programming model that assumed the existence of barriers following each expression evaluation <ref> [29, 48, 50] </ref>. Simple data dependence analysis can be used to reduce barrier synchronization by orders of magnitude, greatly improving performance. In comparison, SUIF begin with parallel loops discovered by parallelizing compilers, where the remaining barriers are significantly harder to remove. <p> Philippsen and Heinz find the minimal number of barriers with an algorithm based on topological sort; they also attempt to minimize the amount of storage needed for intermediate results <ref> [48] </ref>. Cytron et al. were the first to explore the possibilities of exploiting SPMD code for shared-memory multiprocessors, concentrating on safety concerns and the effect on privatization [19].
Reference: [49] <author> K. Pingali and A. Rogers. </author> <title> Compiling for locality. </title> <booktitle> In Proceedings of the 1990 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: The two major compilation steps are partitioning the data and computation across processors, then introducing communication for nonlocal accesses where needed. The compiler partitions computation across processors using the owner computes rulewhere each processor only computes values of data it owns <ref> [15, 24, 49] </ref>. The Fortran D project contributed to the development of High Performance Fortran (HPF), an informal Fortran standard adopted by researchers and vendors for programming massively-parallel processors [37]. The success of HPF hinges on the development of compilers that can provide performance satisfactory to users. <p> I have a preliminary implementation working, and am working on incorporating new optimizations to exploit the multiple coherence protocols and hybrid message-passing capabilities of the CVM system. 6 Related Work The Fortran D compiler is a second-generation distributed-memory compiler that incorporates and extends features from previous compilation systems <ref> [15, 24, 38, 42, 49] </ref>. Compared with other contemporary systems [8, 14, 16, 17, 34, 48], The Fortran D compiler is less flexible but performs deeper compile-time analysis, many more advanced optimizations, requires fewer language extensions, and relies on less run-time support.
Reference: [50] <author> S. Prakash, M. Dhagat, and R. Bagrodia. </author> <title> Synchronization issues in data-parallel languages. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: These approaches focused on individual optimizations and were generally applied by hand. Researchers compiling for SIMD languages such as Fortran 90 and Modula-2* have been faced with a programming model that assumed the existence of barriers following each expression evaluation <ref> [29, 48, 50] </ref>. Simple data dependence analysis can be used to reduce barrier synchronization by orders of magnitude, greatly improving performance. In comparison, SUIF begin with parallel loops discovered by parallelizing compilers, where the remaining barriers are significantly harder to remove.
Reference: [51] <author> S. K. Reinhardt, J. R. Larus, and D. A. Wood. Tempest and Typhoon: </author> <title> User-level shared memory. </title> <booktitle> In Proceedings of the 21th International Symposium on Computer Architecture, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: Reasonable performance should be obtainable for many coarse-grain parallel applications. A recent development that makes compilers for software DSMs more desirable is the development of Flexible-Shared-Memory (FSM) machines (e.g, Alewife [1], Flash [30, 40], Typhoon <ref> [51, 53] </ref>). These architectures maintain a coherent shared address space on top of phys-ically distributed memories, just like traditional shared-memory machines. In addition, FSM machines also support extensible memory coherence protocols and explicit messages where needed to achieve better performance.
Reference: [52] <author> A. Rogers and K. Pingali. </author> <title> Compiling for distributed memory architectures. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 5(3) </volume> <pages> 281-298, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Few researchers have published experimental results for large programs. Pingali & Rogers apply message vector-ization, message pipelining, and reduction recognition in ID NOUVEAU to parallelize SIMPLE <ref> [52] </ref>. Koelbel & Mehrotra are able to parallelize ADI integration in KALI by implicitly applying dynamic data decomposition between computation phases [38]. Olander & Schnabel show that DINO programs can be significantly improved through iteration reordering and pipelining [47].
Reference: [53] <author> I. Schoinas, B. Falsafi, A. Lebeck, S. Reinhardt, J. Larus, and D. Wood. </author> <title> Fine-grain access control for distributed shared memory. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VI), </booktitle> <address> San Jose, CA, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: Reasonable performance should be obtainable for many coarse-grain parallel applications. A recent development that makes compilers for software DSMs more desirable is the development of Flexible-Shared-Memory (FSM) machines (e.g, Alewife [1], Flash [30, 40], Typhoon <ref> [51, 53] </ref>). These architectures maintain a coherent shared address space on top of phys-ically distributed memories, just like traditional shared-memory machines. In addition, FSM machines also support extensible memory coherence protocols and explicit messages where needed to achieve better performance.
Reference: [54] <author> J.P. Singh, T. Joe, A. Gupta, and J. Hennessy. </author> <title> An empirical comparison of the Kendall Square Research KSR-1 and Stanford DASH multiprocessors. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <address> Portland, OR, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: Many previous researchers have examined performance issues on shared-memory architectures. Singh et al. <ref> [54] </ref> applied optimizations similar to those in SUIF by hand to representative computations in order to gain good performance. Others evaluated the benefits of co-locating data and computation [23, 44], as well as false sharing [12, 21, 25, 41, 55].
Reference: [55] <author> J. Torrellas, M. Lam, and J. Hennessy. </author> <title> False sharing and spatial locality in multiprocessor caches. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 43(6) </volume> <pages> 651-663, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Many previous researchers have examined performance issues on shared-memory architectures. Singh et al. [54] applied optimizations similar to those in SUIF by hand to representative computations in order to gain good performance. Others evaluated the benefits of co-locating data and computation [23, 44], as well as false sharing <ref> [12, 21, 25, 41, 55] </ref>. These approaches focused on individual optimizations and were generally applied by hand. Researchers compiling for SIMD languages such as Fortran 90 and Modula-2* have been faced with a programming model that assumed the existence of barriers following each expression evaluation [29, 48, 50].
Reference: [56] <author> E. Torrie, C.-W. Tseng, M. Martonosi, and M. Hall. </author> <title> Evaluating the impact of advanced memory systems on compiler-parallelized codes. </title> <booktitle> In Proceedings of the International Conference on Parallel Architectures and Compilation Techniques, </booktitle> <address> Limassol, Cyprus, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: However, experience with parallelizing compilers has shown that parallelism detection alone is not sufficient for achieving scalable performance. To discover why speedups are lower than expected, consider a recent simulation study by Torrie et al. that evaluates the impact of advanced memory systems on the behavior of compiler-parallelized programs <ref> [56] </ref>. Results show that even if the compiler is able to detect significant parallelism, performance can be rather poor. The simulation study analyzed the performance of a collection of programs that were successfully (88-100%) par-allelized by the SUIF interprocedural parallelizer [27, 26].
Reference: [57] <author> C.-W. Tseng. </author> <title> An Optimizing Fortran D Compiler for MIMD Distributed-Memory Machines. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: Fortran D is a version of Fortran extended to allow users or automatic tools to specify how data may be partitioned onto processors. It is designed to provide a simple yet efficient machine-independent data-parallel programming model, shifting the burden of optimizations to the compiler <ref> [28, 31, 57] </ref>. Given a data decomposition, the Fortran D compiler automatically translates sequential programs into efficient parallel programs, using communication analysis to determine what data needs to be communicated and inserting the necessary messages.
Reference: [58] <author> C.-W. Tseng. </author> <title> Compiler optimizations for eliminating barrier synchronization. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: This information can then be combined with traditional data dependence and data flow analyses to guide compiler optimizations to reduce interprocessor communication. This paper summarizes results from a number of papers <ref> [33, 32, 58, 59] </ref> to demonstrate how communication analysis is useful for both shared and distributed memory machines to improve performance. <p> The key observation used to eliminate barrier synchronization is that synchronization is needed only if processors are communicating data. If there is no interprocessor communication between two parallel regions separated by a barrier, then there is no need for the barrier <ref> [58] </ref>. Com munication analysis can thus be quite useful for reducing synchronization on shared-memory machines. If it can identify the producers and consumers of all data shared between two regions to be identical (i.e., the same processor), then data movement is local and no synchronization is necessary.
Reference: [59] <author> C.-W. Tseng, J. Anderson, S. Amarasinghe, and M. Lam. </author> <title> Unified compilation techniques for shared and distributed address space machines. </title> <booktitle> In Proceedings of the 1995 ACM International Conference on Supercomputing, </booktitle> <address> Barcelona, Spain, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: This information can then be combined with traditional data dependence and data flow analyses to guide compiler optimizations to reduce interprocessor communication. This paper summarizes results from a number of papers <ref> [33, 32, 58, 59] </ref> to demonstrate how communication analysis is useful for both shared and distributed memory machines to improve performance. <p> In addition, this section shows how the advanced SUIF compiler can apply communication analysis to reduce synchronization costs and apply data layout optimizations to reduce undesirable memory system effects <ref> [59] </ref>. 4.1 Synchronization Optimizations Parallelizing compilers for shared address space machines typically employ a fork-join model, where a single master thread executes the sequential portions of the program, assigning computation to worker threads when parallel loops are encountered. <p> Memory system effects (e.g., conflict misses due to low associativity, replacement misses due to poor spatial locality) can thus significantly degrade cache performance, increasing interprocessor communication. Fortunately, shared-memory compilers can adapt distributed-memory compilation techniques to change data layout and improve performance <ref> [59] </ref>. The key is to identify data accessed by each processor and change the data layout to make it contiguous, improving spatial locality and reducing memory system effects [6].
Reference: [60] <author> P. Tu and D. Padua. </author> <title> Automatic array privatization. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: Re--sults for the first ten Livermore kernels show the compiler is usually within a factor of two of hand-coded programs. The advanced SUIF compiler builds on shared-memory compiler algorithms for identifying parallelism <ref> [11, 27, 26, 60] </ref> and performing program transformations [62, 63], as well as distributed-memory compilation techniques to select data decompositions [7] and explicitly manage address translation and data movement [3, 31]. Many previous researchers have examined performance issues on shared-memory architectures.
Reference: [61] <author> R. Wilson et al. </author> <title> SUIF: An infrastructure for research on parallelizing and optimizing compilers. </title> <journal> ACM SIG-PLAN Notices, </journal> <volume> 29(12) </volume> <pages> 31-37, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: Scalar optimizations are thus also needed to improve the performance of these address calculations [6]. 4.3 Experimental Results To evaluate their impact, the synchronization and data layout optimizations described were implemented in the SUIF parallelizing compiler <ref> [61] </ref>. On thirty programs from the PERFECT, SPEC, and NAS benchmark suites, the synchronization optimizations were able to eliminate about 40% of all parallel loop invocations (by merging SPMD regions).
Reference: [62] <author> M. E. Wolf and M. Lam. </author> <title> A loop transformation theory and an algorithm to maximize parallelism. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 452-471, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Re--sults for the first ten Livermore kernels show the compiler is usually within a factor of two of hand-coded programs. The advanced SUIF compiler builds on shared-memory compiler algorithms for identifying parallelism [11, 27, 26, 60] and performing program transformations <ref> [62, 63] </ref>, as well as distributed-memory compilation techniques to select data decompositions [7] and explicitly manage address translation and data movement [3, 31]. Many previous researchers have examined performance issues on shared-memory architectures.
Reference: [63] <author> M. J. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference-contexts: Re--sults for the first ten Livermore kernels show the compiler is usually within a factor of two of hand-coded programs. The advanced SUIF compiler builds on shared-memory compiler algorithms for identifying parallelism [11, 27, 26, 60] and performing program transformations <ref> [62, 63] </ref>, as well as distributed-memory compilation techniques to select data decompositions [7] and explicitly manage address translation and data movement [3, 31]. Many previous researchers have examined performance issues on shared-memory architectures.
References-found: 63

