URL: ftp://ftp.cc.gatech.edu/pub/coc/tech_reports/1994/GIT-CC-94-35.ps.Z
Refering-URL: http://www.cs.gatech.edu/tech_reports/index.94.html
Root-URL: 
Email: ajay@watson.ibm.com  rama@cc.gatech.edu  
Title: A Comparative Study of Distributed Shared Memory System Design Issues  
Author: Ajay Mohindra P. O. Box Umakishore Ramachandran 
Date: August 1994  
Address: Heights, NY 10598  Atlanta, GA 30332-0280  
Affiliation: IBM T. J. Watson Research Center  Yorktown  College of Computing Georgia Tech  
Pubnum: GIT-CC-94/35  
Abstract: In this research the various issues that arise in the design and implementation of distributed shared memory (DSM) systems are examined. This work has been motivated by two observations: distributed systems will continue to become popular, and will be increasingly used for solving large computational problems; and shared memory paradigm is attractive for programming large distributed systems because it offers a natural transition for a programmer from the world of uniprocessors. The goal of this work is to identify a set of system issues in applying the shared memory paradigm to a distributed system, and evaluate the effects of the ensuing design alternatives on the performance of DSM systems. The design alternatives have been evaluated in two steps. First, we undertake a detailed measurement-based study of a distributed shared memory implementation on the Clouds 1 distributed operating system towards understanding the system issues. Second, a simulation-based approach is used to evaluate the system issues. A new workload model that captures the salient features of parallel and distributed programs is developed and used to drive the simulator. The key results of the research are that the choice of the memory model and coherence protocol does not significantly influence the system performance for applications exhibiting high computation granularity and low state-sharing; weaker memory models become significant for large-scale DSM systems; the unit of coherence maintenance depends on a set of parameters including the overheads for servicing data requests as well as the speed of data transmission on the network; and the design of miscellaneous system services (such as synchronization and data servers) can play an important role in the performance of DSM systems. 
Abstract-found: 1
Intro-found: 1
Reference: [AB86] <author> J. Archibald and Jean-Loup Baer. </author> <title> Cache coherence protocols: Evaluation using a multiprocessor simulation model. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4(4):273 - 298, </volume> <month> Nov </month> <year> 1986. </year>
Reference-contexts: Consistency maintenance of distributed shared memory is similar to cache coherence in shared memory multiprocessors with private caches. In shared memory multiprocessors with private caches, the caches are kept consistent using either a write-invalidate policy or a write-update policy <ref> [AB86] </ref>. In the former a writer acquires exclusive ownership of a cache block by invalidating all peer copies before performing the write, while in the latter concurrent writes to the same cache block are possible from several processors with the updates being sent to keep all the peer copies consistent. <p> Further, we show that by choosing the parameters of the model appropriately it is possible to replicate the results of some real applications which we have implemented on the Clouds DSM platform. Archibald and Baer <ref> [AB86] </ref> have proposed a simple memory reference generator based on a probabilistic approach to evaluate cache coherence schemes in a shared memory multiprocessor. In their model, each processor generates a memory reference stream.
Reference: [AH93] <author> S. Adve and M. D. Hill. </author> <title> A unified formalization of four shared memory models. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(6) </volume> <pages> 613-624, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Essentially, sequential consistency ensures that the view of the memory is consistent at all times from all the processors. There have been several proposals for weakening the consistency requirements for shared accesses. Most of these proposals exploit explicit synchronization in the parallel program <ref> [GLL + 90, LR91, KCZ92, AH93] </ref> to drive the consistency actions. Release Consistency (RC) [GLL + 90] is the most well-known among these models, which distinguishes between two kinds of synchronization accesses, namely, acquire and release, establishing a consistent view of shared memory at the release point.
Reference: [AHJ91] <author> M. Ahamad, P. W. Hutto, and R. John. </author> <title> Implementing and programming causal distributed memory. </title> <booktitle> In 11th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 274-281, </pages> <year> 1991. </year>
Reference-contexts: Release Consistency (RC) [GLL + 90] is the most well-known among these models, which distinguishes between two kinds of synchronization accesses, namely, acquire and release, establishing a consistent view of shared memory at the release point. A few others <ref> [AHJ91] </ref> use causality [Lam79], as a fundamental event ordering mechanism in distributed systems, to drive the consistency actions. A related issue to the memory model supported by the DSM system is the choice of the coherence protocol used for consistency maintenance.
Reference: [AJM + 93] <author> R. Ananthanarayanan, R. John, A Mohindra, M. Ahamad, and U. Ramachandran. </author> <title> An evaluation of state sharing techniques in distributed operating systems. </title> <institution> Git-cc-93-73, Georgia Institute of Technology, </institution> <year> 1993. </year>
Reference-contexts: We did one such study for six different applications <ref> [AJM + 93] </ref>. Unfortunately, it is difficult to glean insight regarding the impact of application characteristics and system parameters on the DSM design from such an experimental study. <p> To this end we do two things: First, the costs are assigned to the different components of the simulator directly from the measurements of the Clouds implementation. Second, we validate the workload model, used to drive the simulator, against our experimental results described in <ref> [AJM + 93] </ref>. 5.1 Simulator The simulator is written in CSIM [Sch86], a process oriented simulation language. The distributed system modeled by the simulator consists of a collection of nodes interconnected by a local area network. <p> Table 4 shows the default values for the other parameters used in the simulator. 5.2.3 Validation of the Workload Model We validate our workload model against the experimental results reported in a companion paper <ref> [AJM + 93] </ref>. For completeness, we describe the two applications used in the validations here. The purpose of validation is to allow us to identify meaningful values for the parameters so that each workload (domain specific) corresponds to some "real" application. <p> As in the case of SC, if an invalidation-based protocol is used for RC then false-sharing will need to be addressed similarly. Our simulation results (which are also corroborated in our experimental studies reported in <ref> [AJM + 93] </ref>) show that there is no clear choice of a memory system that performs well across a variety of applications. <p> Simulation studies (see Section 5) have been performed under the assumption that miscellaneous system services (such as acquiring/releasing locks and barriers) incur negligible cost; therefore the results of the studies do not show significant effect of these services on the performance. 28 In a companion paper <ref> [AJM + 93] </ref> we report on experimental studies of DSM systems that we carried out on Clouds in which we observed that these services play a key role in determining the overall performance of the application.
Reference: [AMMR92] <author> R. Ananthanarayanan, Sathis Menon, Ajay Mohindra, and Umakishore Ramachandran. </author> <title> Experiences in integrating distributed shared memory with virtual memory management. </title> <journal> Operating Systems Review, </journal> <volume> 26(3) </volume> <pages> 4-26, </pages> <year> 1992. </year>
Reference-contexts: The times reported in the next subsection are an average of number of such readings. A page refers to 8 Kbytes. We briefly report the performance results for three categories of experiments. More details about this study can be found in <ref> [AMMR92] </ref>. 4.2 Performance Measurements Table 2 summarizes the basic system times for the three categories of experiments. The network communication times shown in Table 2 are between two compute servers.
Reference: [Ano85] <author> Anonymous. </author> <title> A measure of transaction processing power. </title> <journal> Datamation, </journal> <pages> pages 112-118, </pages> <month> April </month> <year> 1985. </year>
Reference-contexts: of tasks in the parallel program 50 NumberOfNodes Number of processors in the system 8 BlockSize Amount of data transferred upon request for shared memory 8192 MediaSpeed Speed of the network 10 Mbps Table 4: List of parameters for the simulator along with the default values SCAN The SCAN benchmark <ref> [Ano85] </ref> specifies a sequential scan of a file, reading and updating records. Such scans are typical of end-of-day processing in on-line transaction processing systems. The benchmark requires that each record be locked, read, modified, updated, and unlocked.
Reference: [Bau78] <author> G. M. Baudet. </author> <title> Asynchronous iterative methods for multiprocessors. </title> <journal> Journal of the ACM, </journal> <volume> 25(2) </volume> <pages> 226-244, </pages> <month> April </month> <year> 1978. </year>
Reference-contexts: This workload model is similar to the data access patterns of asynchronous algorithms that rely on some other property such as convergence for correctness and termination <ref> [Bau78] </ref>.
Reference: [BBLS91] <author> David Bailey, John Barton, Thomas Lasinski, and Horst Simon. </author> <title> The NAS parallel benchmarks. </title> <type> Technical Report RNR-91-002, </type> <institution> NASA Ames Research Center, </institution> <address> Moffet Field CA 94035, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: This latter approach is attractive since there could be situations where there may be very little sharing of data but independent computation may have to synchronize with one another. For example, in compute-intensive applications, such as the embarrassingly parallel kernel <ref> [BBLS91] </ref> and matrix multiplication, interprocess synchronization is used only to indicate completion of computation. <p> We compare the simulation results obtained from the resulting workload model against the measurements on the real system. We show the results of this validation for Integer sort and SCAN. Integer Sort The integer sort kernel <ref> [BBLS91] </ref> is used in "particle-in-cell" applications. The problem statement for the integer sort benchmark requires that N keys be sorted in parallel. The keys are generated by a prescribed sequential key generation algorithm, and are stored contiguously in shared memory.
Reference: [BF88] <author> Roberto Bisiani and Alessandro Forin. </author> <title> Multilingual parallel programming of heterogeneous machines. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(8) </volume> <pages> 930-945, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: KSR-1, which uses a ring interconnect, provides a sequentially consistent view of the distributed shared memory with each node snooping on the network packets to take appropriate coherence actions. Mach [RTY + 87], Agora <ref> [BF88] </ref>, Memnet [DSF88], Choices [SMC90] and Mether [MF90] are other DSM systems that have been proposed in the literature. Though these systems have not been described here in complete detail, their features with respect to the issues outlined in Section 2 are summarized in Table 1.
Reference: [CBZ91] <author> John B. Carter, John K. Bennett, and Willy Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In The 13th ACM Symposium on Operating Systems Principles, </booktitle> <month> Oct </month> <year> 1991. </year>
Reference-contexts: The duality between the two paradigms for structuring computations is well-known [LN79]. Nevertheless, shared memory has been an appealing paradigm from the point of programming ease even in distributed systems. It is no surprise that several researchers <ref> [LH89, RAK89, FP89, CBZ91] </ref> have proposed system architectures that provide the abstraction of shared memory in a physically non-shared (distributed) architecture. We refer to this abstraction as Distributed Shared Memory (DSM). Figure 1 shows the conceptual representation of a distributed shared memory system. <p> Buffering the writes locally and propagating them to update peer copies prior to a release operation (as proposed in <ref> [LR91, CBZ91] </ref>) is a technique that will solve both of these problems in a DSM implementation. In distributed systems, the number of messages is a measure of protocol performance. <p> Ramachandran, et al. [RAK89] proposed this protocol which also supports a weaker form of read that allows multiple-readers to access shared data (without locking) without any guarantee of consistency. The unique feature of the Munin <ref> [CBZ91] </ref> system is its ability to support multiple coherence protocols. The program variables are annotated with their expected access patterns, and the runtime system chooses the protocol best matched to the access pattern for each variable. <p> A logical question then is whether a memory system that allows multiple coherence protocols to co-exist a la Munin <ref> [CBZ91] </ref> is the right approach to realizing efficient DSM systems. Though from the performance standpoint the answer is `yes', such a system places a substantial burden on the programmer to specify the access patterns for the shared variables so that the correct protocol may be chosen by the system.
Reference: [DLAR91] <author> Partha Dasgupta, Richard LeBlanc, Mustaque Ahamad, and Umakishore Ramachandran. </author> <title> The clouds distributed operating system. </title> <booktitle> IEEE Computer, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: This is done to guarantee forward progress of the computation by reducing thrashing of heavily shared data pages. Clouds <ref> [DLAR91] </ref> is a distributed operating based on passive objects. An object encapsulates data that can be manipulated only from within the object. To allow concurrent execution of more than one computation in the same object, shared-memory style synchronization primitives are provided by the operating system.
Reference: [DM90] <author> Peter B. Danzig and Stephen Melvin. </author> <title> High resolution timing with low resultion clocks and A microsecond resolution timer for Sun workstations. </title> <journal> Operating Systems Review, </journal> <volume> 24(1) </volume> <pages> 23-26, </pages> <year> 1990. </year>
Reference-contexts: It provides reliable transfer of data between nodes. 4.1 Methodology Clouds operating system is implemented on a configuration of Sun 3/60s connected by a 10Mbit/sec Ethernet. The timing measurements are done using a microsecond timer <ref> [DM90] </ref>. Each call to read the timer has an overhead of 20 microseconds. The times reported in the next subsection are an average of number of such readings. A page refers to 8 Kbytes. We briefly report the performance results for three categories of experiments.
Reference: [DSF88] <author> Gary S. Delp, Adarshpal S. Sethi, and David J. Farber. </author> <title> An analysis of Memnet: An experiment in high-speed shared-memory local networking. </title> <journal> In Computer Communication Review, </journal> <volume> volume 18, </volume> <pages> pages 165-174, </pages> <address> Stanford, California, </address> <month> August </month> <year> 1988. </year> <note> ACM SIGCOMM. </note>
Reference-contexts: KSR-1, which uses a ring interconnect, provides a sequentially consistent view of the distributed shared memory with each node snooping on the network packets to take appropriate coherence actions. Mach [RTY + 87], Agora [BF88], Memnet <ref> [DSF88] </ref>, Choices [SMC90] and Mether [MF90] are other DSM systems that have been proposed in the literature. Though these systems have not been described here in complete detail, their features with respect to the issues outlined in Section 2 are summarized in Table 1.
Reference: [FP89] <author> Brett D. Fleisch and Gerald J. Popek. </author> <title> Mirage: A coherent distributed shared memory design. </title> <journal> Operating Systems Review, </journal> <volume> 23(5) </volume> <pages> 211-223, </pages> <month> Dec </month> <year> 1989. </year>
Reference-contexts: The duality between the two paradigms for structuring computations is well-known [LN79]. Nevertheless, shared memory has been an appealing paradigm from the point of programming ease even in distributed systems. It is no surprise that several researchers <ref> [LH89, RAK89, FP89, CBZ91] </ref> have proposed system architectures that provide the abstraction of shared memory in a physically non-shared (distributed) architecture. We refer to this abstraction as Distributed Shared Memory (DSM). Figure 1 shows the conceptual representation of a distributed shared memory system. <p> It provides a shared virtual address space similar in concept to the Domain system with the difference that the granularity of access is a physical page in Ivy as opposed to an object in Domain. Ivy supports a SC memory model using a write-invalidate protocol. Mirage <ref> [FP89] </ref> is an extension of the Ivy memory system which allows a reader or a writer of a page to retain access to the page for a fixed duration of time regardless of pending requests.
Reference: [GLL + 90] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J.L. Hennessey. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In The 17th Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: Essentially, sequential consistency ensures that the view of the memory is consistent at all times from all the processors. There have been several proposals for weakening the consistency requirements for shared accesses. Most of these proposals exploit explicit synchronization in the parallel program <ref> [GLL + 90, LR91, KCZ92, AH93] </ref> to drive the consistency actions. Release Consistency (RC) [GLL + 90] is the most well-known among these models, which distinguishes between two kinds of synchronization accesses, namely, acquire and release, establishing a consistent view of shared memory at the release point. <p> There have been several proposals for weakening the consistency requirements for shared accesses. Most of these proposals exploit explicit synchronization in the parallel program [GLL + 90, LR91, KCZ92, AH93] to drive the consistency actions. Release Consistency (RC) <ref> [GLL + 90] </ref> is the most well-known among these models, which distinguishes between two kinds of synchronization accesses, namely, acquire and release, establishing a consistent view of shared memory at the release point.
Reference: [KCZ92] <author> Pete Keleher, Alan L. Cox, and Willy Zwaenepoel. </author> <title> Lazy release consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Essentially, sequential consistency ensures that the view of the memory is consistent at all times from all the processors. There have been several proposals for weakening the consistency requirements for shared accesses. Most of these proposals exploit explicit synchronization in the parallel program <ref> [GLL + 90, LR91, KCZ92, AH93] </ref> to drive the consistency actions. Release Consistency (RC) [GLL + 90] is the most well-known among these models, which distinguishes between two kinds of synchronization accesses, namely, acquire and release, establishing a consistent view of shared memory at the release point.
Reference: [KL89] <author> R. E. Kessler and Miron Livny. </author> <title> An analysis of distributed shared memory algorithms. </title> <booktitle> In 9th Intl Conference on Distributed Computing Systems, </booktitle> <pages> pages 498-505, </pages> <year> 1989. </year>
Reference-contexts: The interaction between the memory reference streams of the different processors is simulated for different coherence protocols. A synthetic reference generator is used by Kessler and Livny <ref> [KL89] </ref> to evaluate distributed shared memory algorithms, in which the main difference from Archibald and Baer's model is that the memory reference stream of each processor is a sequence of shared and private phases. <p> A compute phase is characterized by the following parameters: the number of memory references, read to write ratio, probability for shared and private data accesses, and the degree of locality within the phase. The compute phase is similar to the shared phase as defined by Kessler and Livny <ref> [KL89] </ref>. A synchronization phase consists of read/write data accesses (both private and shared), with a percentage of the shared data accesses being done under the control of explicit synchronization.
Reference: [Lam79] <author> Leslie Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multiprocessor programs. </title> <journal> IEEE Transaction on Computers, </journal> <volume> C-28(9):690-691, </volume> <month> Sep </month> <year> 1979. </year>
Reference-contexts: The key point to note is that the data granularity has to be sufficiently high to make the DSM paradigm viable. 2.3 Memory Model and Choice of Protocol In a uniprocessor, correctness of execution is ensured by preserving the order of memory references generated by a processor. Lamport <ref> [Lam79] </ref> has proposed sequential consistency (SC) as a memory model for ordering shared memory accesses to ensure correct multiprocessor execution. Essentially, sequential consistency ensures that the view of the memory is consistent at all times from all the processors. <p> Release Consistency (RC) [GLL + 90] is the most well-known among these models, which distinguishes between two kinds of synchronization accesses, namely, acquire and release, establishing a consistent view of shared memory at the release point. A few others [AHJ91] use causality <ref> [Lam79] </ref>, as a fundamental event ordering mechanism in distributed systems, to drive the consistency actions. A related issue to the memory model supported by the DSM system is the choice of the coherence protocol used for consistency maintenance.
Reference: [LH89] <author> Kai Li and Paul Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transaction on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: The duality between the two paradigms for structuring computations is well-known [LN79]. Nevertheless, shared memory has been an appealing paradigm from the point of programming ease even in distributed systems. It is no surprise that several researchers <ref> [LH89, RAK89, FP89, CBZ91] </ref> have proposed system architectures that provide the abstraction of shared memory in a physically non-shared (distributed) architecture. We refer to this abstraction as Distributed Shared Memory (DSM). Figure 1 shows the conceptual representation of a distributed shared memory system. <p> To assure the consistency of the replicated copies of an object a two-level approach is adopted. The lower level detects concurrency violations using a time-stamp based version number scheme for each object. The higher level provides an object locking mechanism. Ivy <ref> [LH89] </ref> is a distributed shared memory system implemented on Apollo workstations interconnected by a token-ring network.
Reference: [LLD + 83] <author> Paul J. Leach, Paul H. Levine, Bryan P. Douros, James A Hamilton, David L Nelson, and Bernard L. Stumpf. </author> <title> The architecture of an integrated local network. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 1(5) </volume> <pages> 842-857, </pages> <month> November </month> <year> 1983. </year>
Reference-contexts: In this section, we have enumerated the issues that need to be addressed efficiently in the design of distributed shared memory systems. These issues form the basis for the comparative study that is presented in the subsequent sections. 3 Related Work Apollo Domain <ref> [LLD + 83] </ref> is one of the earliest systems that implements a single level store (represented as a collection of shared objects) in a local area network of personal workstations and data servers. To assure the consistency of the replicated copies of an object a two-level approach is adopted.
Reference: [LLG + 90] <author> D Lenoski, J. Laudon, K Gharachorloo, A Gupta, and J Hennessy. </author> <title> The directory-based cache coherence protocol for the DASH multiprocessor. </title> <booktitle> Proceedings. The 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <year> 1990. </year> <month> 31 </month>
Reference-contexts: In recent years, several systems have been proposed that implement the distributed shared memory abstraction in hardware. Two examples are the DASH multiprocessor <ref> [LLG + 90] </ref>, and the KSR-1 [Res91]. DASH uses a directory-based write-invalidate protocol to provide a release-consistent view of the shared memory which is physically distributed among the processing nodes.
Reference: [LN79] <author> H. C. Lauer and R. M. Needham. </author> <title> On the duality of operating system structures. </title> <journal> Operating Systems Review, </journal> <volume> 13(2) </volume> <pages> 3-19, </pages> <month> April </month> <year> 1979. </year>
Reference-contexts: To facilitate the programming of distributed systems, two basic paradigms exist: shared memory, and message-passing. These two paradigms have been used for interprocess communication and synchronization in multi-process computations. The duality between the two paradigms for structuring computations is well-known <ref> [LN79] </ref>. Nevertheless, shared memory has been an appealing paradigm from the point of programming ease even in distributed systems. It is no surprise that several researchers [LH89, RAK89, FP89, CBZ91] have proposed system architectures that provide the abstraction of shared memory in a physically non-shared (distributed) architecture.
Reference: [LR90] <author> Joonwon Lee and Umakishore Ramachandran. </author> <title> Synchronization with multiprocessor caches. </title> <booktitle> In Proc. 17th Int'l. Symp. on Computer Architecture, </booktitle> <pages> pages 27-37, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: It is possible to use direct association of locks governing the access to shared cache blocks <ref> [LR90] </ref>. This would allow the data 4 associated with the lock being sent to the requester along with the granting of the lock. Upon release of a lock, the associated data is sent back (if modified) to the memory. <p> However, the granularity of accesses in DSM systems precludes using low-level primitives such as "Test-and-Set" on arbitrary memory locations. One possibility is to combine synchronization with sharing as has been suggested in some multiprocessor cache protocols <ref> [LR90] </ref>. Another possibility is to have an orthogonal set of primitives to achieve synchronization. This latter approach is attractive since there could be situations where there may be very little sharing of data but independent computation may have to synchronize with one another.
Reference: [LR91] <author> Joonwon Lee and Umakishore Ramachandran. </author> <title> Locks, directories, and weak coherence a recipe for scalable shared memory multiprocessors. In Scalable Shared Memory Multiprocessors. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year>
Reference-contexts: Essentially, sequential consistency ensures that the view of the memory is consistent at all times from all the processors. There have been several proposals for weakening the consistency requirements for shared accesses. Most of these proposals exploit explicit synchronization in the parallel program <ref> [GLL + 90, LR91, KCZ92, AH93] </ref> to drive the consistency actions. Release Consistency (RC) [GLL + 90] is the most well-known among these models, which distinguishes between two kinds of synchronization accesses, namely, acquire and release, establishing a consistent view of shared memory at the release point. <p> Buffering the writes locally and propagating them to update peer copies prior to a release operation (as proposed in <ref> [LR91, CBZ91] </ref>) is a technique that will solve both of these problems in a DSM implementation. In distributed systems, the number of messages is a measure of protocol performance.
Reference: [MF90] <author> Ronald G. Minnich and David J. Farber. </author> <title> Reducing host load, network load, and latency in a distributed shared memory. </title> <booktitle> In 10th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 468-475, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: KSR-1, which uses a ring interconnect, provides a sequentially consistent view of the distributed shared memory with each node snooping on the network packets to take appropriate coherence actions. Mach [RTY + 87], Agora [BF88], Memnet [DSF88], Choices [SMC90] and Mether <ref> [MF90] </ref> are other DSM systems that have been proposed in the literature. Though these systems have not been described here in complete detail, their features with respect to the issues outlined in Section 2 are summarized in Table 1. For more details, the reader is referred to [Moh93].
Reference: [Moh93] <author> Ajay Mohindra. </author> <title> Issues in the design of distributed shared memory systems. </title> <type> PhD thesis, </type> <institution> College of Computing, Georgia Tech, </institution> <address> Atlanta, GA 30332-0280, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: Though these systems have not been described here in complete detail, their features with respect to the issues outlined in Section 2 are summarized in Table 1. For more details, the reader is referred to <ref> [Moh93] </ref>. A similar survey can also be found in [NL91]. 4 Distributed Shared Memory in Clouds: A Case Study In this section, we summarize the measurements taken on the Clouds distributed shared memory. <p> The results for the impact of the hardware technology on performance can be found in <ref> [Moh93] </ref>. 5.4.1 Transaction Model One would expect that larger data granularity would reduce the number of messages in the system as fewer data requests are generated, and would increase spatial locality.
Reference: [NL91] <author> Bill Nitzberg and Virginia Lo. </author> <title> Distributed Shared Memory: A Survey of Issues and Algorithms. </title> <booktitle> Computer, </booktitle> <pages> pages 52-60, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Though these systems have not been described here in complete detail, their features with respect to the issues outlined in Section 2 are summarized in Table 1. For more details, the reader is referred to [Moh93]. A similar survey can also be found in <ref> [NL91] </ref>. 4 Distributed Shared Memory in Clouds: A Case Study In this section, we summarize the measurements taken on the Clouds distributed shared memory.
Reference: [RAK89] <author> Umakishore Ramachandran, Mustaque Ahamad, and M. Yousef A. Khalidi. </author> <title> Coherence of distributed shared memory: Unifying synchronization and data transfer. </title> <booktitle> In 18th International Conference on Parallel Processing, </booktitle> <pages> pages 160-169, </pages> <month> Aug </month> <year> 1989. </year>
Reference-contexts: The duality between the two paradigms for structuring computations is well-known [LN79]. Nevertheless, shared memory has been an appealing paradigm from the point of programming ease even in distributed systems. It is no surprise that several researchers <ref> [LH89, RAK89, FP89, CBZ91] </ref> have proposed system architectures that provide the abstraction of shared memory in a physically non-shared (distributed) architecture. We refer to this abstraction as Distributed Shared Memory (DSM). Figure 1 shows the conceptual representation of a distributed shared memory system. <p> To allow concurrent execution of more than one computation in the same object, shared-memory style synchronization primitives are provided by the operating system. The collection of objects in Clouds represents a distributed shared virtual space. Pages are the units of distribution. A lock-based protocol <ref> [RAK89] </ref> that unifies synchronization and data transfer is used for consistency maintenance of the distributed shared memory. In this protocol lock requests (exclusive and shared) result in the page associated with the lock being sent to the requester along with the granting of the lock. <p> Reads or writes to shared data without explicit locking follow single-copy semantics that does not allow multiple-readers or writers. Thus this protocol provides a sequential consistent view of the shared memory at well-defined synchronization points. Ramachandran, et al. <ref> [RAK89] </ref> proposed this protocol which also supports a weaker form of read that allows multiple-readers to access shared data (without locking) without any guarantee of consistency. The unique feature of the Munin [CBZ91] system is its ability to support multiple coherence protocols.
Reference: [Res91] <institution> Kendall Square Research. KSR1 Principles of Operations, </institution> <year> 1991. </year>
Reference-contexts: The latter deals with the amount of shared information processed during this computation phase. While a computation to communication ratio of 100:1 may be reasonable in a tightly coupled shared memory system (such as the KSR-1) <ref> [Res91] </ref>, this ratio is usually in the 1000:1 range for a DSM system. <p> In recent years, several systems have been proposed that implement the distributed shared memory abstraction in hardware. Two examples are the DASH multiprocessor [LLG + 90], and the KSR-1 <ref> [Res91] </ref>. DASH uses a directory-based write-invalidate protocol to provide a release-consistent view of the shared memory which is physically distributed among the processing nodes.
Reference: [RSRM93] <author> Umakishore Ramachandran, Gautam Shah, S. Ravikumar, and Jeyakumar Muthukumarasamy. </author> <title> Scalability study of the KSR-1. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <year> 1993. </year> <note> Also available as a technical report GIT-CC 93/03 from Georgia Institute of Technology. </note>
Reference-contexts: The local buckets are merged into global buckets using a parallel-prefix sum algorithm. The final assignment of ranks is also done in parallel for the partition assigned to each processor. The implementation has been adapted from <ref> [RSRM93] </ref>, has been shown to perform well on KSR-1, a tightly coupled shared memory machine.
Reference: [RTY + 87] <author> Richard Rashid, Avadis Tevanian, Micheal Young, David Golub, Robert Baron, David Black, Willian Bolosky, and Jonathan Chew. </author> <title> Machine-independent virtual memory management for paged uniprocessor and multiprocessor architectures. </title> <booktitle> In Proceedings of the Second International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 31-39, </pages> <year> 1987. </year>
Reference-contexts: KSR-1, which uses a ring interconnect, provides a sequentially consistent view of the distributed shared memory with each node snooping on the network packets to take appropriate coherence actions. Mach <ref> [RTY + 87] </ref>, Agora [BF88], Memnet [DSF88], Choices [SMC90] and Mether [MF90] are other DSM systems that have been proposed in the literature. Though these systems have not been described here in complete detail, their features with respect to the issues outlined in Section 2 are summarized in Table 1.
Reference: [Sch86] <author> H. D. Schwetman. CSIM: </author> <title> A C-based, process-oriented simulation language. </title> <booktitle> In Proceedings of the 1986 Winter Simulation Conference, </booktitle> <pages> pages 387-396, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: Second, we validate the workload model, used to drive the simulator, against our experimental results described in [AJM + 93]. 5.1 Simulator The simulator is written in CSIM <ref> [Sch86] </ref>, a process oriented simulation language. The distributed system modeled by the simulator consists of a collection of nodes interconnected by a local area network. In the simulator, each node is modeled as a set of three CSIM processes: a compute engine, a DSM server, and a media server.
Reference: [SDRC82] <author> John F. Shoch, Yogen K. Dalal, David D. Redell, and Ronald C. Crane. </author> <title> Evolution of the ethernet local computer network. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 1-27, </pages> <month> Aug </month> <year> 1982. </year>
Reference-contexts: The network communication times shown in Table 2 are between two compute servers. A page transfer takes 12.3 milliseconds at the RaTP level as it breaks up an 8 Kbytes message into 6 packets. Note that Ethernet allows a maximum packet size of 1532 bytes <ref> [SDRC82] </ref>. The second category of experiments exercises the DSM subsystem. A get from a data server takes 15.5 milliseconds. Comparing the DSM and RaTP timings for a page transfer (rows 3 and 4), it can be seen that the DSM protocol has an overhead of 3.2 milliseconds.
Reference: [SMC90] <author> A. Sane, K. MacGregor, and Roy H. Campbell. </author> <title> Distributed virtual memory consistency protocols: Design and performance. </title> <booktitle> In Second IEEE Workshop on Experimental Distributed System s, </booktitle> <pages> pages 91-96, </pages> <month> Oct </month> <year> 1990. </year>
Reference-contexts: KSR-1, which uses a ring interconnect, provides a sequentially consistent view of the distributed shared memory with each node snooping on the network packets to take appropriate coherence actions. Mach [RTY + 87], Agora [BF88], Memnet [DSF88], Choices <ref> [SMC90] </ref> and Mether [MF90] are other DSM systems that have been proposed in the literature. Though these systems have not been described here in complete detail, their features with respect to the issues outlined in Section 2 are summarized in Table 1.
Reference: [Wil89] <author> Christopher J. Wilkenloh. RaTP: </author> <title> A transaction support protocol for Ra. </title> <type> Master's thesis, </type> <institution> School of Information and Computer Science, Georgia Institute of Technology, </institution> <year> 1989. </year> <month> 32 </month>
Reference-contexts: The DSMServers implement a First-Come-First-Served queue discipline for processing remote segment requests. The low-level communication protocol used in the Clouds operating system to support DSM is called RaTP <ref> [Wil89] </ref>. It provides reliable transfer of data between nodes. 4.1 Methodology Clouds operating system is implemented on a configuration of Sun 3/60s connected by a 10Mbit/sec Ethernet. The timing measurements are done using a microsecond timer [DM90]. Each call to read the timer has an overhead of 20 microseconds.
References-found: 35

