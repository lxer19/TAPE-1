URL: http://www.cs.cmu.edu/afs/cs/usr/andrewt/papers/sigmetrics97/final/ftp.ps.gz
Refering-URL: http://www.cs.cmu.edu/~andrewt/papers.html
Root-URL: 
Email: fandrewt,rhp,garthg@cs.cmu.edu  
Title: Informed Multi-Process Prefetching and Caching  
Author: Andrew Tomkins, R. Hugo Patterson and Garth Gibson 
Web: http://www.cs.cmu.edu/Groups/PDL  
Address: 5000 Forbes Avenue Pittsburgh, PA 15213-3891  
Affiliation: Carnegie Mellon University  
Date: June 15-18, 1997.  
Note: Appears in Proceedings of the ACM International Conference on Measurement and Modeling of Computer Systems (Sigmetrics '97), Seattle, Washington,  
Abstract: Informed prefetching and caching based on application disclosure of future I/O accesses (hints) can dramatically reduce the execution time of I/O-intensive applications. A recent study showed that, in the context of a single hinting application, prefetching and caching algorithms should adapt to the dynamic load on the disks to obtain the best performance. In this paper, we show how to incorporate adaptivity to disk load into the TIP2 system, which uses cost-benefit analysis to allocate global resources among multiple processes. We compare the resulting system, which we call TIPTOE (TIP with Temporal Overload Estimators) to Cao et al's LRU-SP allocation scheme, also modified to include adaptive prefetching. Using disk-accurate trace-driven simulation we show that, averaged over eleven experiments involving pairs of hinting applications, and with data striped over one to ten disks, TIPTOE delivers 7% lower execution time than LRU-SP. Where the computation and I/O demands of each experiment are closely matched, in a two-disk array, TIPTOE delivers 18% lower execution time. 
Abstract-found: 1
Intro-found: 1
Reference: [Bel66] <author> L.A. Belady. </author> <title> A study of replacement algorithms for virtual storage computers. </title> <journal> IBM Systems Journal, </journal> <volume> 5 </volume> <pages> 78-101, </pages> <year> 1966. </year>
Reference-contexts: TIP2 is that it does not suffer from the shortcomings identified in [KTP + 96]. 1.1 Related Work The bulk of work on prefetching and caching has been based on inferring the future from the past starting with the seminal work that measured the effectiveness of the LRU replacement algorithm <ref> [Bel66] </ref>. Since then, sequential readahead has become a widely-used technique [FO71, MJLF84]. Others have shown how to extract much more complex access patterns from an examination of past accesses [LD97, GA95, CKV93, KE93, PZ91, TD91, Kor90]. Unfortunately, such speculative prefetching risks hurting, rather than helping, performance [Smi85].
Reference: [Cao96] <author> Pei Cao. </author> <title> Application-Controlled File Caching and Prefetching. </title> <type> PhD thesis, </type> <institution> Princeton University, </institution> <year> 1996. </year>
Reference-contexts: This new algorithm called TIPTOE, or TIP with Temporal Overload Estimators, combines the multi-process advantages of cost-benefit allocation with the single-process advantages of dynamic, load-aware prefetching as demonstrated by Forestall. Another strategy for resource allocation in an informed prefetching and caching system is LRU-SP, presented by Cao et al <ref> [CFL94a, Cao96] </ref>, which extends traditional LRU replacement to a mechanism for selecting the buffer supplier forced to evict a cache block. It is straightforward to implement the Forestall algorithm with LRU-SP. With TIPTOE and LRU-SP/Forestall we have comparable prefetching and caching components and dramatically different allocation strategies. <p> At that point, the prefetcher issues the batch of requests and that disk is no longer considered idle. The prefetcher continues to try to fill batches for the remaining idle disks. 3 LRU-SP Resource Allocation Pei Cao's LRU-SP <ref> [CFL94a, CFL94b, Cao96] </ref> algorithm is an alternative to TIP2's cost-benefit strategy for allocating buffers among multiple processes in the presence of hints disclosing future accesses. The goal of the algorithm is to adapt the time-tested LRU algorithm's fairness and performance qualities to this new domain. <p> 1.008 5 1.100 1.018 10 1.096 1.036 Table 6: Average improvement of releasing posthint buffers to the posthint estimator rather than to the head of the LRU queue. prefetching into the two existing systems for informed resource allocation: the TIP2 system of [PGG + 95], and the LRU-SP system of <ref> [CFL94a, Cao96] </ref>. Integrating adaptive prefetching into TIP2 is significantly more challenging because it requires an estimate of the benefit of deep prefetch-ing in terms of reduction in I/O service time per unit of buffer resource.
Reference: [CFKL95] <author> P. Cao, E.W. Felten, A. Karlin, and K. Li. </author> <title> A study of integrated prefetching and caching strategies. </title> <booktitle> In Proceedings of the ACM SIGMETRICS, </booktitle> <month> May, </month> <year> 1995. </year>
Reference-contexts: Fax +1 (1212) 869-0481, or &lt;permissions@acm.org&gt;. array at a time. A powerful mechanism for overcoming this problem is system-managed prefetching and file cache management based on application disclosure 1 of future I/O accesses, which has been shown to reduce the elapsed time of I/O-intensive applications by up to 83% <ref> [PGG + 95, CFL94b, CFKL95, PG94] </ref>. There are a variety of ways to obtain these disclosures. For the programmer, it is both easier and more effective to disclose future accesses than to implement application prefetching by asynchronous I/O [PGG + 95]. <p> On the other hand, the Aggressive algorithm of Cao et al <ref> [CFKL95] </ref>, which prefetches deeply without regard to disk load, may incur substantial computational overhead by performing many unnecessary I/Os when there is ample disk bandwidth. To solve this problem our collaboration developed a new algorithm, Forestall, whose prefetch depth is based on a dynamic estimate of upcoming disk load. <p> With TIPTOE and LRU-SP/Forestall we have comparable prefetching and caching components and dramatically different allocation strategies. This paper analyzes these two informed prefetching and caching systems, contrasting them to each other and to their predecessor systems, TIP2 [PGG + 95] and LRU-SP/Aggressive <ref> [CFKL95] </ref>. For our experiments we collected traces of six I/O-intensive hinting applications (described in [PGG + 95]) that range from databases to scientific computation to speech recognition. Our tracing tools capture and timestamp hints and accesses, allowing accurate modeling of the implications of late-arriving hints and unhinted accesses. <p> This study showed that, for some applications and some disk array sizes, TIP2's assumption of unlimited disk bandwidth results in unnecessary stall. An alternative to TIP2 that achieves less stall in some of these situations is Cao et al's Aggressive algorithm <ref> [CFKL95] </ref>. Aggressive ejects block e to prefetch block p if the disk is currently idle, p is not in memory, e is in memory, and p occurs before e in the hint stream. <p> Lastly, very deep queues allow early prefetches to be reordered behind many other, later prefetches which may lead to unnecessary stall. TIPTOE strikes a balance and issues groups of up to 16 requests, a technique that Cao has called batching <ref> [CFKL95] </ref>. When TIPTOE issues prefetches, it keeps bidding for buffers for the earliest remaining uncached block until either the batch is full, or the benefit of prefetching for that disk is not great enough to win any more buffers. <p> The resulting algorithm is called LRU-SP, LRU with swapping and placeholders. LRU-SP's partitioning of the cache makes it easy to combine different prefetching and caching algorithms with it. In the evaluations that follow, we consider two combinations. LRU-SP/Aggressive was the first combination described in the literature <ref> [CFKL95] </ref>. It uses the Aggressive deep prefetching algorithm described above to make prefetching and caching decisions within an individual process' partition. Because our recent collaboration with the inventors of LRU-SP/Aggressive found that the Forestall algorithm outperforms Aggressive [KTP + 96], we also consider LRU-SP/Forestall. <p> As we mentioned above, a full treatment of this topic requires a theoretical model of nonconstant disk service time so a treatment within TIPTOE is beyond our scope; nonetheless, we observe the phenomenon in simulations. As described in Section 2.4, Cao et al in their presentation of Aggressive <ref> [CFKL95] </ref> describe a mechanism they call "batching" in which the prefetching algorithm waits for the disk to go idle and then submits up to B requests, where the batchsize B is a parameter of the algorithm. LRU-SP, TIPTOE and LRU-SP/Forestall all adopt this scheme in our implementation.
Reference: [CFL94a] <author> P. Cao, E.W. Felten, and K. Li. </author> <title> Application-controlled file caching policies. </title> <booktitle> In 1994 Usenix Summer Technical Conference, </booktitle> <pages> pages 171-182, </pages> <month> June, </month> <year> 1994. </year>
Reference-contexts: This new algorithm called TIPTOE, or TIP with Temporal Overload Estimators, combines the multi-process advantages of cost-benefit allocation with the single-process advantages of dynamic, load-aware prefetching as demonstrated by Forestall. Another strategy for resource allocation in an informed prefetching and caching system is LRU-SP, presented by Cao et al <ref> [CFL94a, Cao96] </ref>, which extends traditional LRU replacement to a mechanism for selecting the buffer supplier forced to evict a cache block. It is straightforward to implement the Forestall algorithm with LRU-SP. With TIPTOE and LRU-SP/Forestall we have comparable prefetching and caching components and dramatically different allocation strategies. <p> At that point, the prefetcher issues the batch of requests and that disk is no longer considered idle. The prefetcher continues to try to fill batches for the remaining idle disks. 3 LRU-SP Resource Allocation Pei Cao's LRU-SP <ref> [CFL94a, CFL94b, Cao96] </ref> algorithm is an alternative to TIP2's cost-benefit strategy for allocating buffers among multiple processes in the presence of hints disclosing future accesses. The goal of the algorithm is to adapt the time-tested LRU algorithm's fairness and performance qualities to this new domain. <p> 1.008 5 1.100 1.018 10 1.096 1.036 Table 6: Average improvement of releasing posthint buffers to the posthint estimator rather than to the head of the LRU queue. prefetching into the two existing systems for informed resource allocation: the TIP2 system of [PGG + 95], and the LRU-SP system of <ref> [CFL94a, Cao96] </ref>. Integrating adaptive prefetching into TIP2 is significantly more challenging because it requires an estimate of the benefit of deep prefetch-ing in terms of reduction in I/O service time per unit of buffer resource.
Reference: [CFL94b] <author> P. Cao, E.W. Felten, and K. Li. </author> <title> Implementation and performance of application-controlled file caching. </title> <booktitle> In Proceedings of the First USENIX Symposium on Operating Systems Design and Implementation, </booktitle> <address> Mon-terey, CA, </address> <pages> pages 165-178, </pages> <month> November, </month> <year> 1994. </year>
Reference-contexts: Fax +1 (1212) 869-0481, or &lt;permissions@acm.org&gt;. array at a time. A powerful mechanism for overcoming this problem is system-managed prefetching and file cache management based on application disclosure 1 of future I/O accesses, which has been shown to reduce the elapsed time of I/O-intensive applications by up to 83% <ref> [PGG + 95, CFL94b, CFKL95, PG94] </ref>. There are a variety of ways to obtain these disclosures. For the programmer, it is both easier and more effective to disclose future accesses than to implement application prefetching by asynchronous I/O [PGG + 95]. <p> As a result, history-based prefetch-ing must be conservative and need not address the resource management issues related to very deep prefetching that are the focus of this paper. Sometimes an application may have advance knowledge of resource needs and advise specific action <ref> [Tri79, SM88, CFL94b] </ref>. In contrast, we ask that applications disclose information and then allow the filesystem to make decisions in the presence of global resource knowledge. Mowry et al [MDK96] show that for some workloads, primarily scientific computing, it is possible for the compiler to generate these disclosures automatically. <p> At that point, the prefetcher issues the batch of requests and that disk is no longer considered idle. The prefetcher continues to try to fill batches for the remaining idle disks. 3 LRU-SP Resource Allocation Pei Cao's LRU-SP <ref> [CFL94a, CFL94b, Cao96] </ref> algorithm is an alternative to TIP2's cost-benefit strategy for allocating buffers among multiple processes in the presence of hints disclosing future accesses. The goal of the algorithm is to adapt the time-tested LRU algorithm's fairness and performance qualities to this new domain.
Reference: [CKV93] <author> K. Curewitz, P. Krishnan, and J.S. Vitter. </author> <title> Practical prefetching via data compression. </title> <booktitle> In Proceedings of the 1993 ACM Conference on Management of Data (SIGMOD), </booktitle> <pages> pages 257-266, </pages> <month> May, </month> <year> 1993. </year>
Reference-contexts: Since then, sequential readahead has become a widely-used technique [FO71, MJLF84]. Others have shown how to extract much more complex access patterns from an examination of past accesses <ref> [LD97, GA95, CKV93, KE93, PZ91, TD91, Kor90] </ref>. Unfortunately, such speculative prefetching risks hurting, rather than helping, performance [Smi85]. As a result, history-based prefetch-ing must be conservative and need not address the resource management issues related to very deep prefetching that are the focus of this paper.
Reference: [CP90] <author> Peter M. Chen and David A. Patterson. </author> <title> Maximizing performance in a striped disk array. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 322-331. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> May </month> <year> 1990. </year> <month> 14 </month>
Reference-contexts: In this section, we describe our simulator, the applications we have traced, and the trace-collection methodology. 4.1 Simulation Environment Our simulator is built on top of the Berkeley RaidSim <ref> [CP90, LK91] </ref> simulator. RaidSim can simulate various flavors of RAID disk arrays using a disk geometry module to determine disk access times. In our simulations, we run with data striped over an array of from 1-10 disks with no parity and a stripe unit of eight 8K blocks.
Reference: [CR93] <author> C. Chen and N. Roussopoulos. </author> <title> Adaptive database buffer allocation using query feedback. </title> <booktitle> In Proc. of the 19th VLDB Conference, </booktitle> <address> Dublin, Ireland, </address> <year> 1993. </year>
Reference-contexts: Others have developed richer languages for expressing and exploiting disclosure [SS95, Kot94, GJ91]. Large integrated applications may implement their own resource management. Some database researchers have shown how to use a query access plan to allocate buffers <ref> [NFS91, CR93] </ref>. Ng, Faloutsos and Sellis's work on marginal gains considered the question of how much benefit a query would derive from an additional buffer.
Reference: [FO71] <author> R. J. Feiertag and E. I. Organisk. </author> <title> The Multics Input/Output system. </title> <booktitle> In Proc. of the 3rd Symp. on Operating System Principles, </booktitle> <pages> pages 35-41, </pages> <year> 1971. </year>
Reference-contexts: Since then, sequential readahead has become a widely-used technique <ref> [FO71, MJLF84] </ref>. Others have shown how to extract much more complex access patterns from an examination of past accesses [LD97, GA95, CKV93, KE93, PZ91, TD91, Kor90]. Unfortunately, such speculative prefetching risks hurting, rather than helping, performance [Smi85].
Reference: [GA95] <author> J. Griffioen and R. Appleton. </author> <title> Performance measurements of automatic prefetching. </title> <booktitle> In Proc. of the ISCA International Conference on Parallel and Distributed Computing Systems, </booktitle> <month> September </month> <year> 1995. </year>
Reference-contexts: Since then, sequential readahead has become a widely-used technique [FO71, MJLF84]. Others have shown how to extract much more complex access patterns from an examination of past accesses <ref> [LD97, GA95, CKV93, KE93, PZ91, TD91, Kor90] </ref>. Unfortunately, such speculative prefetching risks hurting, rather than helping, performance [Smi85]. As a result, history-based prefetch-ing must be conservative and need not address the resource management issues related to very deep prefetching that are the focus of this paper.
Reference: [GJ91] <author> A.S. Grimshaw and E.C. Loyot Jr. </author> <title> ELFS: Object-oriented extensible file systems. </title> <type> Technical Report Computer Science Technical Report No. </type> <institution> TR-91-14, University of Virginia, </institution> <year> 1991. </year>
Reference-contexts: Mowry et al [MDK96] show that for some workloads, primarily scientific computing, it is possible for the compiler to generate these disclosures automatically. Others have developed richer languages for expressing and exploiting disclosure <ref> [SS95, Kot94, GJ91] </ref>. Large integrated applications may implement their own resource management. Some database researchers have shown how to use a query access plan to allocate buffers [NFS91, CR93].
Reference: [KE93] <author> David Kotz and Carla Schlatter Ellis. </author> <title> Practical prefetching techniques for multiprocessor file systems. </title> <journal> Journal of Distributed and Parallel Databases, </journal> <volume> 1(1) </volume> <pages> 33-51, </pages> <month> January, </month> <year> 1993. </year>
Reference-contexts: Since then, sequential readahead has become a widely-used technique [FO71, MJLF84]. Others have shown how to extract much more complex access patterns from an examination of past accesses <ref> [LD97, GA95, CKV93, KE93, PZ91, TD91, Kor90] </ref>. Unfortunately, such speculative prefetching risks hurting, rather than helping, performance [Smi85]. As a result, history-based prefetch-ing must be conservative and need not address the resource management issues related to very deep prefetching that are the focus of this paper.
Reference: [Kor90] <author> Kim Korner. </author> <title> Intelligent caching for remote file service. </title> <booktitle> In Proceedings of the 10th Intl. Conf. on Distributed Computing Systems, </booktitle> <pages> pages 220-226, </pages> <year> 1990. </year>
Reference-contexts: Since then, sequential readahead has become a widely-used technique [FO71, MJLF84]. Others have shown how to extract much more complex access patterns from an examination of past accesses <ref> [LD97, GA95, CKV93, KE93, PZ91, TD91, Kor90] </ref>. Unfortunately, such speculative prefetching risks hurting, rather than helping, performance [Smi85]. As a result, history-based prefetch-ing must be conservative and need not address the resource management issues related to very deep prefetching that are the focus of this paper.
Reference: [Kot94] <author> D. Kotz. </author> <title> Disk-directed I/O for MIMD multiprocessors. </title> <booktitle> In Proc. of the 1st USENIX Symp. on Operating Systems Design and Implementation, </booktitle> <address> Monterey, CA, </address> <pages> pages 61-74, </pages> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: Mowry et al [MDK96] show that for some workloads, primarily scientific computing, it is possible for the compiler to generate these disclosures automatically. Others have developed richer languages for expressing and exploiting disclosure <ref> [SS95, Kot94, GJ91] </ref>. Large integrated applications may implement their own resource management. Some database researchers have shown how to use a query access plan to allocate buffers [NFS91, CR93].
Reference: [KTP + 96] <author> T. Kimbrel, A. Tomkins, R.H. Patterson, B. Bershad, P. Cao, E.W. Felten, G. Gibson, A. Karlin, and K. Li. </author> <title> A trace-driven comparison of algorithms for parallel prefetching and caching. </title> <booktitle> In Proceedings of the Second Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 19-34, </pages> <year> 1996. </year>
Reference-contexts: First, it must manage prefetching and caching within each stream of hinted accesses. Second, it must address the allocation problem of dividing disk and cache resources among multiple hinted and unhinted access streams. In a recent collaboration with Kimbrel, Karlin, Cao et al <ref> [KTP + 96] </ref> we addressed the first of those problems, analyzing prefetching and caching algorithms in the context of a single process disclosing all its accesses at startup. <p> We also consider experiments in which hinting and unhinting processes run together, and three-process experiments. The aggregate improvement over all experiments is 7.6% versus LRU-SP/Aggressive, 5.3% versus LRU-SP/Forestall and 3.6% versus TIP2. TIPTOE's primary advantage over TIP2 is that it does not suffer from the shortcomings identified in <ref> [KTP + 96] </ref>. 1.1 Related Work The bulk of work on prefetching and caching has been based on inferring the future from the past starting with the seminal work that measured the effectiveness of the LRU replacement algorithm [Bel66]. Since then, sequential readahead has become a widely-used technique [FO71, MJLF84]. <p> the future can be issued later with no additional stall time. 2.1.2 Problems with TIP2; the Forestall Algorithm In a recent collaboration with Kimbrel, Karlin, Cao, and others, we undertook a simulation study of prefetching and caching for single processes that hint all future accesses at the start of execution <ref> [KTP + 96] </ref>. This study showed that, for some applications and some disk array sizes, TIP2's assumption of unlimited disk bandwidth results in unnecessary stall. An alternative to TIP2 that achieves less stall in some of these situations is Cao et al's Aggressive algorithm [CFKL95]. <p> We present the estimator for the benefit of prefetching in two distinct pieces. First, Section 2.2.1 describes how we anticipate upcoming hotspots that will lead to application stalls. Our approach follows the model of the Forestall algorithm <ref> [KTP + 96] </ref>. Next, Section 2.2.2 shows how we estimate the system resources necessary to perform a deep prefetch. Finally, we combine these two values to express the benefit of deep prefetching in terms of TIP2's common currency. 2.2.1 Detecting Constrained Disks upcoming hotspots will cause stall. <p> request i if and only if r i is the first reference to an uncached block and X jijdisk (j)=d Incore (j) T disk &gt; i T app : (1) This specification of a constrained disk is very similar to that used by the Forestall algorithm in the earlier study <ref> [KTP + 96] </ref>, but it differs in two details. First, the Incore function used here only counts the first access to a block as a miss instead of assuming all accesses to currently uncached blocks will have to be prefetched. <p> LRU-SP/Aggressive was the first combination described in the literature [CFKL95]. It uses the Aggressive deep prefetching algorithm described above to make prefetching and caching decisions within an individual process' partition. Because our recent collaboration with the inventors of LRU-SP/Aggressive found that the Forestall algorithm outperforms Aggressive <ref> [KTP + 96] </ref>, we also consider LRU-SP/Forestall. In the implementation of LRU-SP/Forestall considered here, we use the same algorithm for detecting constraints as is used in TIPTOE. <p> Overall, they affirm the results of the earlier study <ref> [KTP + 96] </ref>. As before, TIP2 sometimes fails to take advantage of transient disk idleness to perform deep prefetching. This is particularly visible when Davidson is running on a single disk. TIP2 caches a portion of the dataset and lets the disk go idle while it reads the cached data. <p> Experiments 5, 7 and 9 display this effect. In experiment 5, for instance, TIP2's average disk service time is 18% faster than TIP TOE's on a single disk. Lesson 7: Leaving a constrained disk idle leads to additional stall. This effect was documented in <ref> [KTP + 96] </ref> for the single-process case. We discuss it in Figure 2, and mention it here because it arises in the two-process case as well. <p> This effect alone is not responsible for all the difference between the two algorithms in these experiments; Lesson 3 is the primary contributor to the disparity. Lesson 8: Submitting an I/O requires T driver computational overhead. This effect was also documented in the single-process case in <ref> [KTP + 96] </ref>. We discuss it in Figure 3, and it appears in experiments 1, 2, 5, 8 and 9 on larger array sizes. In experiment 1, for instance, LRU-SP/Aggressive on ten disks incurs 52% more driver overhead than TIPTOE. <p> First, how should each hinting process use the limited resources available to it for prefetching and caching; and second, how should system resources be allocated among multiple competing processes. An earlier paper with collaborators <ref> [KTP + 96] </ref> addressed the first problem and showed that prefetching algorithms that are conservative or aggressive without regard to disk load perform worse than algorithms that adapt their prefetching behavior dynamically based on load. <p> However, we found that the resulting algorithm, TIPTOE (TIP with Temporal Overload Estimators), yields lower overall execution times than LRU-SP/Forestall because benefit estimation is a better predictor of caching value than application access rate. We also discovered a number of more specific lessons. Our experiments confirm the conclusions of <ref> [KTP + 96] </ref>, and extend the scope of those results to the multi-process domain. Specifically, we found that leaving a constrained disk idle leads to additional stall and that over-aggressive prefetch-ing from unconstrained disks leads to unnecessary I/Os and higher associated CPU overhead. <p> We also consider experiments in which hinting and unhinting processes run together, and three-process experiments. The aggregate improvement over all experiments is 7.6% versus LRU-SP/Aggressive, 5.3% versus LRU-SP/Forestall and 3.6% versus TIP2. TIPTOE's primary advantage over TIP2 is that it does not suffer from the shortcomings identified in <ref> [KTP + 96] </ref>.
Reference: [LD97] <author> Hui Lei and Dan Duchamp. </author> <title> An analytical approach to file prefetching. </title> <booktitle> In 1997 USENIX Annual Technical Conference, </booktitle> <month> January </month> <year> 1997. </year>
Reference-contexts: Since then, sequential readahead has become a widely-used technique [FO71, MJLF84]. Others have shown how to extract much more complex access patterns from an examination of past accesses <ref> [LD97, GA95, CKV93, KE93, PZ91, TD91, Kor90] </ref>. Unfortunately, such speculative prefetching risks hurting, rather than helping, performance [Smi85]. As a result, history-based prefetch-ing must be conservative and need not address the resource management issues related to very deep prefetching that are the focus of this paper.
Reference: [LHR90] <author> Kai-Fu Lee, Hsiao-Wuen Hon, and Raj Reddy. </author> <title> An overview of the SPHINX speech recognition system. </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, (USA), </journal> <volume> 38(1) </volume> <pages> 35-45, </pages> <month> Jan, </month> <year> 1990. </year>
Reference-contexts: We run two cases. In the first (Postgres1), 20% of the outer relation finds a match in the inner relation. In the second (Postgres2), 80% find a match. One output tuple is written sequentially for every tuple match. Sphinx is a high-quality, speaker-independent, continuous-voice, speech-recognition system <ref> [LHR90] </ref>. In our experiments, Sphinx recognizes an 18-second recording commonly used in Sphinx regression testing. 7 Agrep is a variant of grep written by Wu and Manber at the University of Arizona [WM92]. It is a full-text pattern matching program that performs approximate matches.
Reference: [LK91] <author> Edward K. Lee and Randy H. Katz. </author> <title> Performance consequences of parity placement in disk arrays. </title> <booktitle> In ASPLOS4, </booktitle> <pages> pages 190-199. </pages> <publisher> ACM, </publisher> <year> 1991. </year>
Reference-contexts: In this section, we describe our simulator, the applications we have traced, and the trace-collection methodology. 4.1 Simulation Environment Our simulator is built on top of the Berkeley RaidSim <ref> [CP90, LK91] </ref> simulator. RaidSim can simulate various flavors of RAID disk arrays using a disk geometry module to determine disk access times. In our simulations, we run with data striped over an array of from 1-10 disks with no parity and a stripe unit of eight 8K blocks.
Reference: [MDK96] <author> T. Mowry, A. Demke, and O. Krieger. </author> <title> Automatic compiler-inserted I/O prefetching for out-of-core applications. </title> <booktitle> In Proceedings of the Second Symposium on Operating Systems Design and Implementation, </booktitle> <year> 1996. </year>
Reference-contexts: For the programmer, it is both easier and more effective to disclose future accesses than to implement application prefetching by asynchronous I/O [PGG + 95]. Furthermore, recent work has shown that compilers can induce some programs to disclose their future accesses automatically <ref> [MDK96] </ref>. A system to deliver this functionality must provide solutions to two distinct problems. First, it must manage prefetching and caching within each stream of hinted accesses. Second, it must address the allocation problem of dividing disk and cache resources among multiple hinted and unhinted access streams. <p> Sometimes an application may have advance knowledge of resource needs and advise specific action [Tri79, SM88, CFL94b]. In contrast, we ask that applications disclose information and then allow the filesystem to make decisions in the presence of global resource knowledge. Mowry et al <ref> [MDK96] </ref> show that for some workloads, primarily scientific computing, it is possible for the compiler to generate these disclosures automatically. Others have developed richer languages for expressing and exploiting disclosure [SS95, Kot94, GJ91]. Large integrated applications may implement their own resource management.
Reference: [MJLF84] <author> M. K. McKusick, W. J. Joy, S. J. Le*er, and R. S. Fabry. </author> <title> A fast file system for Unix. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 2(3) </volume> <pages> 181-197, </pages> <month> Aug. </month> <year> 1984. </year>
Reference-contexts: Since then, sequential readahead has become a widely-used technique <ref> [FO71, MJLF84] </ref>. Others have shown how to extract much more complex access patterns from an examination of past accesses [LD97, GA95, CKV93, KE93, PZ91, TD91, Kor90]. Unfortunately, such speculative prefetching risks hurting, rather than helping, performance [Smi85].
Reference: [Nat89] <institution> National Center for Supercomputing Applications. XDataSlice for the X window system. Technical Report http://www.nsca.uiuc.edu/, University of Illi-nois at Urbana-Champaign, </institution> <year> 1989. </year>
Reference-contexts: In our benchmark, Agrep sequentially searches 1349 kernel source files occupying 1922 disk blocks for a simple string that does not occur in any of the files. XDataSlice (Xds) is an interactive scientific visualization tool developed at the National Center for Supercomputer Applications at the University of Illinois <ref> [Nat89] </ref>.
Reference: [NFS91] <author> Raymond Ng, Christos Faloutsos, and Timos Sellis. </author> <title> Flexible buffer allocation based on marginal gains. </title> <booktitle> In Proc. of the 1991 ACM Conf. on Management of Data (SIGMOD), </booktitle> <pages> pages 387-396, </pages> <year> 1991. </year>
Reference-contexts: Others have developed richer languages for expressing and exploiting disclosure [SS95, Kot94, GJ91]. Large integrated applications may implement their own resource management. Some database researchers have shown how to use a query access plan to allocate buffers <ref> [NFS91, CR93] </ref>. Ng, Faloutsos and Sellis's work on marginal gains considered the question of how much benefit a query would derive from an additional buffer.
Reference: [PG94] <author> R. Hugo Patterson and Garth Gibson. </author> <title> Exposing I/O concurrency with informed prefetching. </title> <booktitle> In Proceedings of the Third International Conference on Parallel and Distributed Information Systems, </booktitle> <pages> pages 7-16, </pages> <month> September </month> <year> 1994. </year> <note> Unpublished version in lab. </note>
Reference-contexts: Fax +1 (1212) 869-0481, or &lt;permissions@acm.org&gt;. array at a time. A powerful mechanism for overcoming this problem is system-managed prefetching and file cache management based on application disclosure 1 of future I/O accesses, which has been shown to reduce the elapsed time of I/O-intensive applications by up to 83% <ref> [PGG + 95, CFL94b, CFKL95, PG94] </ref>. There are a variety of ways to obtain these disclosures. For the programmer, it is both easier and more effective to disclose future accesses than to implement application prefetching by asynchronous I/O [PGG + 95]. <p> In this single-process, fully-hinted domain, prefetching and caching decisions are made without reference to global allocation; that is, each process assumes that all resources are dedicated to it. Restricted to this model, the bounded-depth prefetch-ing algorithms of Patterson et al <ref> [PG94, PGG + 95] </ref>, which assume large disk arrays, may fail to prefetch deeply enough into the request stream when disk parallelism is limited or when the I/O workload is highly unbalanced across the disks of an array.
Reference: [PGG + 95] <author> R. Hugo Patterson, Garth A. Gibson, Eka Gint-ing, Daniel Stodolsky, and Jim Zelenka. </author> <title> Informed prefetching and caching. </title> <booktitle> In Proceedings of the 15th Symposium on Operating Systems Principles, </booktitle> <pages> pages 79-95, </pages> <month> December, </month> <year> 1995. </year>
Reference-contexts: Fax +1 (1212) 869-0481, or &lt;permissions@acm.org&gt;. array at a time. A powerful mechanism for overcoming this problem is system-managed prefetching and file cache management based on application disclosure 1 of future I/O accesses, which has been shown to reduce the elapsed time of I/O-intensive applications by up to 83% <ref> [PGG + 95, CFL94b, CFKL95, PG94] </ref>. There are a variety of ways to obtain these disclosures. For the programmer, it is both easier and more effective to disclose future accesses than to implement application prefetching by asynchronous I/O [PGG + 95]. <p> There are a variety of ways to obtain these disclosures. For the programmer, it is both easier and more effective to disclose future accesses than to implement application prefetching by asynchronous I/O <ref> [PGG + 95] </ref>. Furthermore, recent work has shown that compilers can induce some programs to disclose their future accesses automatically [MDK96]. A system to deliver this functionality must provide solutions to two distinct problems. First, it must manage prefetching and caching within each stream of hinted accesses. <p> In this single-process, fully-hinted domain, prefetching and caching decisions are made without reference to global allocation; that is, each process assumes that all resources are dedicated to it. Restricted to this model, the bounded-depth prefetch-ing algorithms of Patterson et al <ref> [PG94, PGG + 95] </ref>, which assume large disk arrays, may fail to prefetch deeply enough into the request stream when disk parallelism is limited or when the I/O workload is highly unbalanced across the disks of an array. <p> The 1 A disclosure is a hint delivered in the language of the existing I/O interface; ie, in terms of filenames, file descriptors, and byte ranges. 1 TIP2 system of Patterson et al <ref> [PGG + 95] </ref> demonstrated that resource allocation decisions can be made by weighing the benefit of providing resources to a consumer against the cost of taking them from a supplier. <p> It is straightforward to implement the Forestall algorithm with LRU-SP. With TIPTOE and LRU-SP/Forestall we have comparable prefetching and caching components and dramatically different allocation strategies. This paper analyzes these two informed prefetching and caching systems, contrasting them to each other and to their predecessor systems, TIP2 <ref> [PGG + 95] </ref> and LRU-SP/Aggressive [CFKL95]. For our experiments we collected traces of six I/O-intensive hinting applications (described in [PGG + 95]) that range from databases to scientific computation to speech recognition. <p> This paper analyzes these two informed prefetching and caching systems, contrasting them to each other and to their predecessor systems, TIP2 <ref> [PGG + 95] </ref> and LRU-SP/Aggressive [CFKL95]. For our experiments we collected traces of six I/O-intensive hinting applications (described in [PGG + 95]) that range from databases to scientific computation to speech recognition. Our tracing tools capture and timestamp hints and accesses, allowing accurate modeling of the implications of late-arriving hints and unhinted accesses. <p> Then, we define the common currency as the magnitude of the change in I/O service time per buffer-access. 2.1.1 TIP2 Estimators The TIP2 cost and benefit estimates given in <ref> [PGG + 95] </ref> are derived from a specific system model. <p> We took advantage of RaidSim's support for multiple threads to allow the concurrent simulation of multiple separately-scripted processes. 4.2 Applications We drive the simulator with traces of a suite of six I/O-intensive applications. These applications were used in the evaluation of the original TIP2 system <ref> [PGG + 95] </ref>, and are carefully documented in that paper. We give a brief description here. The applications have all been modified to provide hints as they run, and our traces capture these hints as they arrive. The overwhelming majority of these hints are accurate. <p> Disks TIP TIPTOE 1 1.086 1.011 3 1.085 1.008 5 1.100 1.018 10 1.096 1.036 Table 6: Average improvement of releasing posthint buffers to the posthint estimator rather than to the head of the LRU queue. prefetching into the two existing systems for informed resource allocation: the TIP2 system of <ref> [PGG + 95] </ref>, and the LRU-SP system of [CFL94a, Cao96]. Integrating adaptive prefetching into TIP2 is significantly more challenging because it requires an estimate of the benefit of deep prefetch-ing in terms of reduction in I/O service time per unit of buffer resource. <p> Higher locality leads to lower disk service times and better performance. Incorporating sensitivity to the layout of ejected blocks into the cost-benefit framework is an area for future research. Our experiments were performed using disk-accurate, trace-driven simulation on traces drawn from six hint-generating I/O-intensive applications described in <ref> [PGG + 95] </ref>. TIPTOE performs the best of the algorithms we studied, improving execution time on average across our hinted two-process experiments by 13% relative to LRU-SP/Aggressive, 7% relative to LRU-SP/Forestall and 3% relative to TIP2.
Reference: [PZ91] <author> Mark Palmer and Stanley B. Zdonik. </author> <title> FIDO: A cache that learns to fetch. </title> <booktitle> In Proceedings of the 17th International Conference on Very Large Data Bases, </booktitle> <pages> pages 255-264, </pages> <month> September, </month> <year> 1991. </year>
Reference-contexts: Since then, sequential readahead has become a widely-used technique [FO71, MJLF84]. Others have shown how to extract much more complex access patterns from an examination of past accesses <ref> [LD97, GA95, CKV93, KE93, PZ91, TD91, Kor90] </ref>. Unfortunately, such speculative prefetching risks hurting, rather than helping, performance [Smi85]. As a result, history-based prefetch-ing must be conservative and need not address the resource management issues related to very deep prefetching that are the focus of this paper.
Reference: [RW94] <author> Chris Ruemmler and John Wilkes. </author> <title> An introduction to disk drive modelling. </title> <journal> IEEE Computer, </journal> <volume> 27(3) </volume> <pages> 17-28, </pages> <month> March, </month> <year> 1994. </year>
Reference-contexts: In our simulations, we run with data striped over an array of from 1-10 disks with no parity and a stripe unit of eight 8K blocks. The geometry module simulates the performance of the HP97560 disk drive <ref> [RW94] </ref>. We use the CSCAN request scheduling discipline. We augmented RaidSim to include a buffer cache module layered on top of the disk array and implemented modules for all four of our algorithms. We also added a module to drive RaidSim from traces instead of relying on randomly generated workloads.
Reference: [SF94] <author> A. Stathopoulos and C. F. Fischer. </author> <title> A Davidson program for finding a few selected extreme eigenpairs of a large, sparse, real, symmetric matrix. </title> <journal> Computer Physics Communications, </journal> <volume> 79 </volume> <pages> 268-290, </pages> <year> 1994. </year>
Reference-contexts: The overwhelming majority of these hints are accurate. We do not explore how to prefetch when many of the hints are inaccurate. Davidson is a computational-physics program that computes, by successive refinement, the extreme eigenvalue-eigenvector pairs of a large, sparse, matrix stored on disk <ref> [SF94] </ref>. In our benchmark, Davidson sequentially reads a 16.3 MByte matrix 61 times. Gnuld is version 2.5.2 of the Free Software Foundation's object code linker. Our benchmark links the 562 object files of a Digital UNIX kernel. These object files comprise approximately 64 MB, and produce an 8.8MB kernel.
Reference: [SM88] <author> Inc. </author> <title> Sun Microsystems. Sun OS reference manual, part number 800-1751-10, revision A, </title> <month> May 9, </month> <year> 1988. </year>
Reference-contexts: As a result, history-based prefetch-ing must be conservative and need not address the resource management issues related to very deep prefetching that are the focus of this paper. Sometimes an application may have advance knowledge of resource needs and advise specific action <ref> [Tri79, SM88, CFL94b] </ref>. In contrast, we ask that applications disclose information and then allow the filesystem to make decisions in the presence of global resource knowledge. Mowry et al [MDK96] show that for some workloads, primarily scientific computing, it is possible for the compiler to generate these disclosures automatically.
Reference: [Smi85] <author> A.J. Smith. </author> <title> Disk cache | miss ratio analysis and design considerations. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 3(3) </volume> <pages> 161-203, </pages> <month> Aug. </month> <year> 1985. </year>
Reference-contexts: Since then, sequential readahead has become a widely-used technique [FO71, MJLF84]. Others have shown how to extract much more complex access patterns from an examination of past accesses [LD97, GA95, CKV93, KE93, PZ91, TD91, Kor90]. Unfortunately, such speculative prefetching risks hurting, rather than helping, performance <ref> [Smi85] </ref>. As a result, history-based prefetch-ing must be conservative and need not address the resource management issues related to very deep prefetching that are the focus of this paper. Sometimes an application may have advance knowledge of resource needs and advise specific action [Tri79, SM88, CFL94b].
Reference: [SR86] <author> M. Stonebraker and L.A. Rowe. </author> <booktitle> The design of POST-GRES. In Proceedings of the ACM SIGMOD 1986 International Conference on Management of Data, </booktitle> <address> Washington, DC, </address> <pages> pages 28-30, </pages> <year> 1986. </year>
Reference-contexts: Our benchmark links the 562 object files of a Digital UNIX kernel. These object files comprise approximately 64 MB, and produce an 8.8MB kernel. Postgres is version 4.2 of an extensible, object-oriented, relational database system from the University of Califor-nia at Berkeley <ref> [SR86, SRH90] </ref>. In our test, Postgres executes a join of two relations. The outer relation contains 20,000 unindexed tuples (3.2 MB) while the inner relation has 200,000 tuples (32MB) and is indexed (5 MB). We run two cases.
Reference: [SRH90] <author> M. Stonebraker, L.A. Rowe, and M. Horohama. </author> <title> The implementation of POSTGRES. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 2(1) </volume> <pages> 125-142, </pages> <month> March, </month> <year> 1990. </year>
Reference-contexts: Our benchmark links the 562 object files of a Digital UNIX kernel. These object files comprise approximately 64 MB, and produce an 8.8MB kernel. Postgres is version 4.2 of an extensible, object-oriented, relational database system from the University of Califor-nia at Berkeley <ref> [SR86, SRH90] </ref>. In our test, Postgres executes a join of two relations. The outer relation contains 20,000 unindexed tuples (3.2 MB) while the inner relation has 200,000 tuples (32MB) and is indexed (5 MB). We run two cases.
Reference: [SS95] <author> D. Steere and M. Satyanarayanan. </author> <title> Using Dynamic Sets to overcome high I/O latencies during search. </title> <booktitle> In Proc. of the 5th Workshop on Hot Topics in Operating Systems, </booktitle> <address> Orcas Island, WA, </address> <pages> pages 136-140, </pages> <month> May 4-5, </month> <year> 1995. </year>
Reference-contexts: Mowry et al [MDK96] show that for some workloads, primarily scientific computing, it is possible for the compiler to generate these disclosures automatically. Others have developed richer languages for expressing and exploiting disclosure <ref> [SS95, Kot94, GJ91] </ref>. Large integrated applications may implement their own resource management. Some database researchers have shown how to use a query access plan to allocate buffers [NFS91, CR93].
Reference: [TD91] <author> C. Tait and D. Duchamp. </author> <title> Detection and exploitation of file working sets. </title> <booktitle> In Proc. Eleventh Intl. Conf. on Distributed Computing Systems, </booktitle> <pages> pages 2-9, </pages> <publisher> IEEE, </publisher> <year> 1991. </year>
Reference-contexts: Since then, sequential readahead has become a widely-used technique [FO71, MJLF84]. Others have shown how to extract much more complex access patterns from an examination of past accesses <ref> [LD97, GA95, CKV93, KE93, PZ91, TD91, Kor90] </ref>. Unfortunately, such speculative prefetching risks hurting, rather than helping, performance [Smi85]. As a result, history-based prefetch-ing must be conservative and need not address the resource management issues related to very deep prefetching that are the focus of this paper.
Reference: [Tri79] <author> K.S. Trivedi. </author> <title> An analysis of prepaging. </title> <journal> Computing, </journal> <volume> 22 </volume> <pages> 191-210, </pages> <year> 1979. </year>
Reference-contexts: As a result, history-based prefetch-ing must be conservative and need not address the resource management issues related to very deep prefetching that are the focus of this paper. Sometimes an application may have advance knowledge of resource needs and advise specific action <ref> [Tri79, SM88, CFL94b] </ref>. In contrast, we ask that applications disclose information and then allow the filesystem to make decisions in the presence of global resource knowledge. Mowry et al [MDK96] show that for some workloads, primarily scientific computing, it is possible for the compiler to generate these disclosures automatically.
Reference: [WM92] <author> S. Wu and U. Manber. </author> <title> Agrep | a fast approximate pattern-matching tool. </title> <booktitle> In Proc. of the 1992 Winter USENIX Conference, </booktitle> <address> San Francisco, CA, </address> <pages> pages 20-24, </pages> <month> Jan, </month> <year> 1992. </year> <month> 15 </month>
Reference-contexts: Sphinx is a high-quality, speaker-independent, continuous-voice, speech-recognition system [LHR90]. In our experiments, Sphinx recognizes an 18-second recording commonly used in Sphinx regression testing. 7 Agrep is a variant of grep written by Wu and Manber at the University of Arizona <ref> [WM92] </ref>. It is a full-text pattern matching program that performs approximate matches. In our benchmark, Agrep sequentially searches 1349 kernel source files occupying 1922 disk blocks for a simple string that does not occur in any of the files.
References-found: 35

