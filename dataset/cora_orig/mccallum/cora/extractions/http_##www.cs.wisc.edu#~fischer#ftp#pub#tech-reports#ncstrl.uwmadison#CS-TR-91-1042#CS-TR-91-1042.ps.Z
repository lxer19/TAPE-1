URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-91-1042/CS-TR-91-1042.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-91-1042/
Root-URL: http://www.cs.wisc.edu
Note: Chapter 1  
Abstract-found: 0
Intro-found: 1
Reference: [Allm91] <author> Allmen, M., </author> <title> Toward spatiotemporal-motion-based recognition, </title> <type> Ph.D. Dissertation, </type> <institution> Computer Science Department, University of Wisconsin-Madison, </institution> <year> 1991. </year>
Reference-contexts: Spatiotemporal representations are formed from images over time that are stacked together into a cube of data. There are characteristic features in the spatiotemporal cube that correspond to occlusion in an image sequence <ref> [Bake88, Allm91] </ref>. <p> A second future direction of this work is a better understanding of the relationship between time and viewpoint. The change in image features over time can be detected 118 with low-level representations such as spatiotemporal surfaces <ref> [Allm91] </ref>. The change in shape with respect to viewpoint that can be computed from a model (for example, the rim appearance representation) and its relationship to the change in shape that is observed in images over time is of great interest.
Reference: [Athe78] <author> Atherton, P., K. Weiler, and D. Greenberg, </author> <title> Polygon shadow generation, </title> <booktitle> Proc. SIGGRAPH, </booktitle> <year> 1978, </year> <pages> 275-281. </pages>
Reference-contexts: With scenes containing only a single point light source, the problem is that of determining the set of shadow regions. Shadow region computation in polyhedral scenes has been incorporated into hidden surface algorithms using shadow volumes [Crow77] and polygon clipping <ref> [Athe78] </ref>. Most hidden-surface removal algorithms can compute shadow regions by performing hidden-surface removal from the position of the light source [Joy88]. Rendering can then be performed by integrating the shading of the shadow regions and the visible surfaces according to some shading model.
Reference: [Bake88] <author> Baker, H. H., </author> <title> Surface reconstruction from image sequences, </title> <booktitle> Proc. 2nd Int. Conf. Computer Vision, </booktitle> <year> 1988, </year> <pages> 334-343. </pages>
Reference-contexts: Spatiotemporal representations are formed from images over time that are stacked together into a cube of data. There are characteristic features in the spatiotemporal cube that correspond to occlusion in an image sequence <ref> [Bake88, Allm91] </ref>.
Reference: [Barr81] <author> Barrow, H. G. and J. M. Tenenbaum, </author> <title> Interpreting line drawings as three-dimensional surfaces, </title> <journal> Artificial Intell. </journal> <volume> 17, </volume> <year> 1981, </year> <pages> 75-116. </pages>
Reference-contexts: Detecting Occlusion Features Edges in an image that are part of the occluding contour are produced at depth discontinuity boundaries in the scene. Standard edge detectors that are based on static image intensities alone cannot reliably distinguish between occluding contours and other types of scene edges <ref> [Barr81] </ref>. With additional information, such as depth data from range-finders or from stereo methods, and dynamic data from motion, occluding contours can be segmented and analyzed. In addition to the segmentation issue is the task of detecting features of the occluding contour such as curvature extrema and T-junctions.
Reference: [Basr88] <author> Basri, R. and S. Ullman, </author> <title> The alignment of objects with smooth surfaces, </title> <booktitle> Proc. 2nd Int. Conf. Computer Vision, </booktitle> <year> 1988, </year> <pages> 482-488. </pages>
Reference-contexts: For example, iterative methods assume that the model-image correspondence is known, and solve for viewpoint by applying numerical techniques to revise an initial estimation of viewpoint [Lowe87, Ponc89, Worr89]. Similarly, the alignment approach <ref> [Basr88, Hutt90] </ref> and parameter space methods [Thom87] assume a model-image correspondence in order to derive a unique viewpoint. Interpretation tree methods solve for correspondence and viewpoint simultaneously by using a constrained search through a tree of model-image correspondences [Grim90a]. <p> The Rim and the Occluding Contour The terminology for describing the sets of points related to the occluding contour varies widely. Marr used the term contour generator [Marr77], and others have used terms such as limbs [Egge89, Nalw88, Mali87] and the rim <ref> [Basr88, Koen87] </ref>. To avoid confusion, the precise meaning is defined here for the sets of 3D points called the rim, the visible rim and the occluded rim, and the sets of 2D points in the image plane called the contour, the occluding contour and the occluded contour. <p> In addition, an explicit model that includes features of self-occlusion for solid shape makes the prediction of the appearance of the model a faster process that can speed up model matching <ref> [Basr88] </ref>. Although the rim appearance representation can be large for worst-case examples, the average case requires much less time and space. Consequently, this representation is much smaller on average than other representations such as aspect graphs and the asp.
Reference: [Bish86] <author> Bishop, G. and D. M. Weimer, </author> <title> Fast phong shading, </title> <booktitle> Proc. SIGGRAPH, </booktitle> <year> 1986, </year> <pages> 103-106. </pages>
Reference-contexts: This approximated normal is the normal used in Equation 3.8 to compute the appropriate intensity value for the point q in the image plane. Although the Phong model is more expensive than the flat shading model, there are fast approximations to the Phong shading model <ref> [Bish86, Duff79] </ref>. 39 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh (a) (c) (d) The scene projected from the light source direction. (c) The scene as observed from the viewer's position without the shadow. (d) The scene observed by the viewer with the shadow. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 3.2.4. <p> Interactive viewing using an interpolated shading model requires that a distinct intensity value be computed for each point inside a given face. Interpolated shading and fast Phong shading can be done relatively quickly <ref> [Swan86, Bish86] </ref>, and a scan line algorithm can again be used to combine information from shadow sets and shaded visible surfaces in the image plane. 3.3. Computational Results A prototype of both the preprocessing and the on-line portions of the hidden line algorithm has been implemented.
Reference: [Boll82] <author> Bolles, R. and R. Cain, </author> <title> Recognizing and locating partially visible objects: The local-feature-focus method, </title> <journal> Int. J. Robotics Research 1(3), </journal> <year> 1982, </year> <pages> 57-82. </pages>
Reference-contexts: The relative position of the occluding contour to the oriented T-junction was used to further constrain potential correspondences. This is similar in style to the local feature focus method <ref> [Boll82] </ref>. A measure of the degree of correspondence was determined by using a least squares distance measure between predicted contour fragments and image contours. An exact solution for viewpoint is not found, but rather a constrained region of viewpoints is found that accounts for the T-junction and occluding contour data.
Reference: [Bowy89] <author> Bowyer, K., D. Eggert, J. Stewman, and L. Stark, </author> <title> Developing the aspect graph representation for use in image understanding, </title> <booktitle> Proc. Image Understanding Workshop, </booktitle> <year> 1989, </year> <pages> 831-849. </pages>
Reference-contexts: Researchers have divided the views of a 3D object 6 in two different ways: (1) the uniform division of the space of all viewpoints [Goad83, Ikeu87, Korn87, Feke84]; and (2) the division of the space of viewpoints into sets based on some definition of equivalence <ref> [Bowy89, Egge89, Plan90, Gigu90, Krie89, Srip89] </ref>. Although the uniformly-divided viewpoint space is often a fair approximation of the appearance of an object, several problems remain. First, the appearance of an object from a single viewpoint is taken to be representative of an entire region of viewpoints. <p> Interpretation tree methods solve for correspondence and viewpoint simultaneously by using a constrained search through a tree of model-image correspondences [Grim90a]. Finally, viewer-centered representations such as characteristic views [Wang90] and the aspect graph <ref> [Plan87, Bowy89, Gigu90] </ref> precompute representative sets of viewpoints and then attempt to solve for the best correspondence between image features and the features in each representative view. <p> The search of an interpretation tree [Grim90] is also made more efficient in space and time when there are global constraints on the possible solutions. Aspect graph methods <ref> [Bowy89, Krie90, Ikeu87] </ref> must address the problem of how to select a few aspects to test from a large number of potential aspects. 9 2.4. Detecting Occlusion Features Edges in an image that are part of the occluding contour are produced at depth discontinuity boundaries in the scene. <p> Recent work in viewer-centered modeling has concentrated on the aspect graph, a graph enumerating all of the topologically-distinct 2D views of a 3D object, as well as the transitions between views. The aspect graph has been constructed for polyhedra <ref> [Plan90, Gigu90, Bowy89] </ref>, solids of revolution [Egge89, Krie89], and piecewise-smooth objects [Srip89, Ponc90]. The rim appearance representation is different from the aspect graph in two significant ways. First, the rim appearance representation stores only the appearance of the rim, that is, the occluding contour. <p> Parameter space methods [Thom87] that can avoid a costly search of the entire space of transformations become much more efficient. The search of an interpretation tree [Grim90] is more efficient in space and time when there are global constraints on the possible solutions. Aspect graph methods <ref> [Bowy89, Krie90, Ikeu87] </ref> must address the problem of how to select a few aspects to test from a large number of potential aspects. There are several important advantages to using the rim appearance representation over the aspect graph.
Reference: [Bowy91] <author> Bowyer, K. W. and C. R. Dyer, </author> <title> Aspect graphs: an introduction and survey of recent results, </title> <journal> Int. J. Imaging Systems and Technologies, </journal> <note> to appear, </note> <year> 1991. </year>
Reference-contexts: The approximation presented here of the exact rim appearance is an approximation in viewpoint space, where the complexity is reduced by approximate visibility criteria. There are other possible approaches to these problems, each with its own various tradeoffs. This is an area of active research <ref> [Ponc90a, Bowy91, Dick91] </ref>. 4.4.3. Complexity Analysis The complexity of constructing the approximate rim appearance representation is bounded by the number of EE-events that affect the appearance of the rim. The number of EE-events determines the construction time and space complexity. Unlike the exact algorithm, EEE-events are not computed.
Reference: [Broo86] <author> Brooks, F., </author> <title> Walkthrough Adynamic graphics system for simulating virtual buildings, </title> <booktitle> Proc. Workshop on Interactive 3D Graphics, </booktitle> <year> 1986, </year> <pages> 9-21. </pages>
Reference-contexts: The animated display of a rotating 3D object gives the user a sense of depth and structure that is useful in design and visualization [Farr85]. The animated presentation of a scene that can be navigated dynamically has application in flight simulation [Yan85] and architectural walk-through <ref> [Broo86] </ref>. There are two essential requirements for this type of animated display: realism in each image and video-rate display. Without an appropriate level of realism, the effect of interaction with an environment is lost. Perceptual continuity is lost when the display rate is too slow. <p> This would be useful, for example, in interactively displaying the appearance of a scene generated by a viewpath moving through a workspace, such as a model of a building <ref> [Broo86] </ref>. However, depending on the problem, aspect space can become very high dimensional and therefore the asp may require more space and time to compute. For related work on extending the asp to perspective projection, see [Plan86, Plan87, Plan88].
Reference: [Burd85] <author> Burde, G. and H. Zieschang, </author> <title> Knots, </title> <publisher> Walter de Gruyter, </publisher> <address> Berlin, </address> <year> 1985. </year>
Reference-contexts: Under projection the closed curves of the rim usually self-intersect. As a more complicated example, Figure 4.3 shows a cylinder that has been stretched and connected into a trefoil knot <ref> [Burd85] </ref>. This surface produces two disjoint rim curves 65 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh of the trefoil, the rim is knotted from all viewpoints. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh that are both also knotted. Although the topology of the rim and the surface are clearly related, the depth of the relationship is not currently known. 4.1.2.
Reference: [Burn90] <author> Burns, J. B., R. Weiss, and E. M. Riseman, </author> <title> View variation of point set and line segment features, </title> <booktitle> Proc. Image Understanding Workshop, </booktitle> <year> 1990, </year> <pages> 650-659. </pages>
Reference-contexts: This is because polyhedra that are "smooth" have rim edges that persist in viewpoint space for only a small range of views. Over this small set of views, the quantitative description of the EE-events that occur is quite stable <ref> [Burn90] </ref>. As the polyhedron becomes a better and better approximation of a smooth object, the cost of maintaining exact information is much higher than the gain in accuracy over this approximate method. 4.4.2. <p> In general, there is a locus of viewpoints within an EE-event patch that can produce a T-junction that exactly matches an image T-junction. Despite this, the variation in the geometry of a T-junction formed by two edges depends on the size of the viewpoint patch where it occurs <ref> [Burn90] </ref>. Consequently, the geometry of a T-junction persisting over a small region of viewpoint space changes very little over that region, and can be closely approximated by the geometry at a single viewpoint within the region. 103 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh (a) (b) (c) the contour terminals. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 5.1.2.
Reference: [Cipo90] <author> Cipolla, R. and A. Blake, </author> <title> The dynamic analysis of apparent contours, </title> <booktitle> Proc. 3rd Int. Conf. Computer Vision, </booktitle> <year> 1990, </year> <pages> 616-623. </pages>
Reference-contexts: Researchers have also begun to integrate stereo data and dynamic information in order to segment and measure the curvature on occlusion boundaries <ref> [Cipo90] </ref>. Contours are tracked throughout a dynamic image sequence using an energy-based contour tracking method [Kass88] in order to model how the contour changes as viewpoint changes. This information is used to reconstruct the properties of the occluding contour and the 3D surface generating it.
Reference: [Cook84] <author> Cook, R. L., T. Porter, and L. Carpenter, </author> <title> Distributed ray tracing, </title> <booktitle> Computer Graphics 18, </booktitle> <month> July </month> <year> 1984, </year> <pages> 137-145. </pages>
Reference-contexts: Interactive Viewing There is a tradeoff in solving the problem of interactive viewing between off-line and on-line solutions. At one extreme is complete off-line image rendering [Denb86]. The on-line phase is a fixed playback of precomputed images that have been rendered with raytracing or other off-line techniques <ref> [Glas88, Cook84] </ref>. The other extreme is to use standard display algorithms such as Z-buffering to render each frame of the sequence independently. The main drawbacks of the total precomputation approach are the size and inflexibility of the resulting animation description. <p> The precomputation of information can be as extreme as complete image rendering off-line. The on-line animation phase is then reduced to a playback of the precomputed images [Denb86]. There are ways to compute realistic images off-line using, for example, ray tracing techniques <ref> [Glas88, Cook84] </ref>, but the high per-frame cost makes interactive animated display using these approaches intractable. The main drawback of the total precomputation approach is the size and inflexibility of the resulting animation description. <p> Scenes of this complexity have been rendered previously using ray tracing techniques that create shadow regions as a by-product of light-source and ray intersections <ref> [Whit80, Cook84] </ref>. The asp represents the appearance of each face in the scene from all viewing directions. By treating each light source as an additional viewing direction, the asp can be used to obtain the appearance of a particular face from the light source position.
Reference: [Crow77] <author> Crow, F. C., </author> <title> Shadow algorithms for computer graphics, </title> <booktitle> Proc. SIGGRAPH, </booktitle> <year> 1977, </year> <pages> 242-248. </pages>
Reference-contexts: With scenes containing only a single point light source, the problem is that of determining the set of shadow regions. Shadow region computation in polyhedral scenes has been incorporated into hidden surface algorithms using shadow volumes <ref> [Crow77] </ref> and polygon clipping [Athe78]. Most hidden-surface removal algorithms can compute shadow regions by performing hidden-surface removal from the position of the light source [Joy88]. Rendering can then be performed by integrating the shading of the shadow regions and the visible surfaces according to some shading model.
Reference: [Denb86] <author> Denber, M. and P. Turner, </author> <title> A differential compiler for computer animation, </title> <booktitle> ACM Computer Graphics 20(4), </booktitle> <year> 1986, </year> <pages> 21-27. </pages>
Reference-contexts: The feature representation also improves on some of the size problems as well as the problem of indexing. 2.2. Interactive Viewing There is a tradeoff in solving the problem of interactive viewing between off-line and on-line solutions. At one extreme is complete off-line image rendering <ref> [Denb86] </ref>. The on-line phase is a fixed playback of precomputed images that have been rendered with raytracing or other off-line techniques [Glas88, Cook84]. The other extreme is to use standard display algorithms such as Z-buffering to render each frame of the sequence independently. <p> In meeting the requirements of realism and video-rate display there is a fundamental trade-off between off-line and on-line solutions. The precomputation of information can be as extreme as complete image rendering off-line. The on-line animation phase is then reduced to a playback of the precomputed images <ref> [Denb86] </ref>. There are ways to compute realistic images off-line using, for example, ray tracing techniques [Glas88, Cook84], but the high per-frame cost makes interactive animated display using these approaches intractable. The main drawback of the total precomputation approach is the size and inflexibility of the resulting animation description.
Reference: [Dick91] <author> Dickinson, S. J., A. P. Pentland, and A. Rosenfeld, </author> <title> From volumes to views: an approach to 3-D object recognition, </title> <booktitle> Proc. IEEE Workshop on Directions 120 in Automated CAD-based Vision, </booktitle> <year> 1991, </year> <pages> 85-96. </pages>
Reference-contexts: The approximation presented here of the exact rim appearance is an approximation in viewpoint space, where the complexity is reduced by approximate visibility criteria. There are other possible approaches to these problems, each with its own various tradeoffs. This is an area of active research <ref> [Ponc90a, Bowy91, Dick91] </ref>. 4.4.3. Complexity Analysis The complexity of constructing the approximate rim appearance representation is bounded by the number of EE-events that affect the appearance of the rim. The number of EE-events determines the construction time and space complexity. Unlike the exact algorithm, EEE-events are not computed.
Reference: [Duff79] <author> Duff, T., </author> <title> Smoothly shaded renderings of polyhedral objects on raster displays, </title> <booktitle> ACM Computer Graphics 13(2), </booktitle> <year> 1979, </year> <pages> 270-275. </pages>
Reference-contexts: This approximated normal is the normal used in Equation 3.8 to compute the appropriate intensity value for the point q in the image plane. Although the Phong model is more expensive than the flat shading model, there are fast approximations to the Phong shading model <ref> [Bish86, Duff79] </ref>. 39 hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh (a) (c) (d) The scene projected from the light source direction. (c) The scene as observed from the viewer's position without the shadow. (d) The scene observed by the viewer with the shadow. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 3.2.4.
Reference: [Egge89] <author> Eggert, D. and K. Bowyer, </author> <title> Computing the orthographic projection aspect graph of solids of revolution, </title> <booktitle> Proc. IEEE Workshop on Interpretation of 3D Scenes, </booktitle> <year> 1989, </year> <pages> 102-108. </pages>
Reference-contexts: Researchers have divided the views of a 3D object 6 in two different ways: (1) the uniform division of the space of all viewpoints [Goad83, Ikeu87, Korn87, Feke84]; and (2) the division of the space of viewpoints into sets based on some definition of equivalence <ref> [Bowy89, Egge89, Plan90, Gigu90, Krie89, Srip89] </ref>. Although the uniformly-divided viewpoint space is often a fair approximation of the appearance of an object, several problems remain. First, the appearance of an object from a single viewpoint is taken to be representative of an entire region of viewpoints. <p> The singularities of the projection mapping have been studied by Whitney [Whit55], and there is work being done toward understanding the form of singularities under projection of specific 3D models such as polyhedra [Plan90, Gigu90, Stew88], surfaces of revolution <ref> [Krie89, Egge89] </ref>, parametric surfaces [Srip89, Ponc90], and generic surfaces [Koen76, Rieg87]. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 19 The singularities (visual events) for polyhedra can be completely characterized as the intersection in the image plane of sets of non-adjacent model edges [Plan90, Gigu90]. <p> Recent work in viewer-centered modeling has concentrated on the aspect graph, a graph enumerating all of the topologically-distinct 2D views of a 3D object, as well as the transitions between views. The aspect graph has been constructed for polyhedra [Plan90, Gigu90, Bowy89], solids of revolution <ref> [Egge89, Krie89] </ref>, and piecewise-smooth objects [Srip89, Ponc90]. The rim appearance representation is different from the aspect graph in two significant ways. First, the rim appearance representation stores only the appearance of the rim, that is, the occluding contour. <p> The Rim and the Occluding Contour The terminology for describing the sets of points related to the occluding contour varies widely. Marr used the term contour generator [Marr77], and others have used terms such as limbs <ref> [Egge89, Nalw88, Mali87] </ref> and the rim [Basr88, Koen87]. <p> The implicit equation for a torus under orthographic projection is an 8th degree 67 polynomial containing over 170 terms. Others have computed the singularities of the rim surface for the purposes of computing the aspect graph for surfaces of revolution <ref> [Egge89, Krie89] </ref> and for parametric surfaces [Ponc90, Srip89]. All of this work with smooth model representations suffers from difficult numerical problems. Numerical difficulties are avoided when polyhedra are used because of the linearity of edges.
Reference: [Farr85] <author> Farrell, E., W. Yang, and R. Zappulla, </author> <title> Animated 3D CT imaging, </title> <journal> IEEE Computer Graphics and Applications 5(12), </journal> <year> 1985, </year> <pages> 26-32. </pages>
Reference-contexts: For example, computer-aided design of 3D objects requires the interactive construction and display of objects from a wide range of viewing directions . The animated display of a rotating 3D object gives the user a sense of depth and structure that is useful in design and visualization <ref> [Farr85] </ref>. The animated presentation of a scene that can be navigated dynamically has application in flight simulation [Yan85] and architectural walk-through [Broo86]. There are two essential requirements for this type of animated display: realism in each image and video-rate display.
Reference: [Faug86] <author> Faugeras, O. and M. Hebert, </author> <title> The representation, recognition, and locating of 3-D objects, </title> <journal> Int. J. Robotics Research 5(3), </journal> <year> 1986, </year> <pages> 27-52. </pages>
Reference-contexts: Aspect graphs have been constructed for piecewise-smooth shapes, although the numerical complexity of those representations makes it difficult to extract and represent the features of interest. Our approach avoids difficult numerical problems by relying on the linear features of polyhedra <ref> [Faug86] </ref>. The second way in which the rim appearance representation differs from the aspect graph is that the rim appearance representation encodes individual features across viewpoint rather than the global topology of the image structure graph. <p> Experience in both computer vision and graphics has shown that the speed and simplicity of linear representations can compensate for size increases and approximation error, provided the appropriate feature information is retained <ref> [Faug86, Lowe89] </ref>. In the present context, it is important to preserve the occluding contour and the interactions between contours as a result of self-occlusion. The local approximation of smooth surfaces with planar patches preserves these contour features while affording linearity.
Reference: [Feke84] <author> Fekete, G. and L. S. Davis, </author> <title> Property spheres: A new representation for 3-D object recognition, </title> <booktitle> Proc. IEEE Workshop on Computer Vision, </booktitle> <year> 1984, </year> <pages> 192-201. </pages>
Reference-contexts: Exactly how to do this is a difficult problem since a 3D object can appear very differently from different views. Researchers have divided the views of a 3D object 6 in two different ways: (1) the uniform division of the space of all viewpoints <ref> [Goad83, Ikeu87, Korn87, Feke84] </ref>; and (2) the division of the space of viewpoints into sets based on some definition of equivalence [Bowy89, Egge89, Plan90, Gigu90, Krie89, Srip89]. Although the uniformly-divided viewpoint space is often a fair approximation of the appearance of an object, several problems remain.
Reference: [Fole82] <author> Foley, J. D. and A. VanDam, </author> <title> Fundamentals of Interactive Computer Graphics, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1982. </year>
Reference-contexts: This is a known problem that all depth-ordering methods must contend with <ref> [Fole82] </ref>. The BSP-tree avoids the problem by subdividing faces that would potentially form occlusion cycles. Our algorithm detects cycles using the topological sort algorithm. Cycles can then be broken by using EE-events to efficiently compute the intersection in the image plane between two faces in the cycle.
Reference: [Fuch83] <author> Fuchs, H., G. D. Abram, and E. D. Grant, </author> <title> Near real-time shaded display of rigid objects, </title> <booktitle> Computer Graphics 17, </booktitle> <month> July </month> <year> 1983, </year> <pages> 65-69. </pages>
Reference-contexts: The Binary Space Partition Tree (BSP-tree) has been used to display polyhedral scenes in near real-time by precomputing a structure that gives relative depth-ordering for faces in the model <ref> [Fuch83, Nayl90] </ref>. The BSP-tree does not take advantage of the frame-to-frame coherence in animation sequences, however. <p> The main drawback of the total precomputation approach is the size and inflexibility of the resulting animation description. At the other end of the spectrum is the synthesis of the frames of the animation sequence on-line. Animation in this case depends on fast dynamic frame display <ref> [Fuch83] </ref>. Without some form of 12 precomputation, however, video-rate display and realistic image synthesis become almost impossible. Wire-frame display is typical for low-cost video-rate display without significant precomputation. This chapter discusses the problem of how to display a polyhedral scene interactively from a moving viewpoint. <p> The approach reported here generates the appearance of the scene on-line as the viewpath is interactively defined. The Binary Space Partition Tree (BSP-tree) has been used to display polyhedral scenes in near real-time by precomputing a structure that gives relative depth-ordering for faces in the model <ref> [Fuch83, Nayl90] </ref>. The BSP-tree simplifies the hidden-surface computation for an arbitrary viewing direction by encoding the relative depth ordering of the model implicitly in a tree structure for all viewing directions. <p> Event-Based Depth Ordering A list of polygons can be displayed with hidden portions removed if the polygons are ordered in depth with respect to the viewer [Newe72]. That is, the surfaces can be scan converted in back-to-front order to cover portions of faces that are hidden [Joy88]. The BSP-tree <ref> [Fuch83] </ref> is based on this premise: the precomputed tree is a more efficient way to sort the faces of a scene in depth. Given a viewpoint, the depth ordering is available from the BSP-tree by an in-order traversal of the tree.
Reference: [Gigu90] <author> Gigus, Z. and J. Malik, </author> <title> Computing the aspect graph for line drawings of polyhedral objects, </title> <journal> IEEE Trans. Pattern Analysis and Machine Intell. </journal> <volume> 12(2), </volume> <year> 1990, </year> <pages> 113-122. </pages>
Reference-contexts: Researchers have divided the views of a 3D object 6 in two different ways: (1) the uniform division of the space of all viewpoints [Goad83, Ikeu87, Korn87, Feke84]; and (2) the division of the space of viewpoints into sets based on some definition of equivalence <ref> [Bowy89, Egge89, Plan90, Gigu90, Krie89, Srip89] </ref>. Although the uniformly-divided viewpoint space is often a fair approximation of the appearance of an object, several problems remain. First, the appearance of an object from a single viewpoint is taken to be representative of an entire region of viewpoints. <p> Interpretation tree methods solve for correspondence and viewpoint simultaneously by using a constrained search through a tree of model-image correspondences [Grim90a]. Finally, viewer-centered representations such as characteristic views [Wang90] and the aspect graph <ref> [Plan87, Bowy89, Gigu90] </ref> precompute representative sets of viewpoints and then attempt to solve for the best correspondence between image features and the features in each representative view. <p> The partial occlusion of an edge by some other edge (the beginning or ending of a T-junction) is another kind of event. More formally, all structural changes in the projection of a polyhedral scene are the result of the overlap in the image plane of three edges <ref> [Plan89, Gigu90] </ref>. In the case of a face that turns away from the viewing direction, the point at which the face is edge-on to the viewer is a 13 degenerate form of the apparent intersection of three edges. <p> The singularities of the projection mapping have been studied by Whitney [Whit55], and there is work being done toward understanding the form of singularities under projection of specific 3D models such as polyhedra <ref> [Plan90, Gigu90, Stew88] </ref>, surfaces of revolution [Krie89, Egge89], parametric surfaces [Srip89, Ponc90], and generic surfaces [Koen76, Rieg87]. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 19 The singularities (visual events) for polyhedra can be completely characterized as the intersection in the image plane of sets of non-adjacent model edges [Plan90, Gigu90]. <p> of specific 3D models such as polyhedra [Plan90, Gigu90, Stew88], surfaces of revolution [Krie89, Egge89], parametric surfaces [Srip89, Ponc90], and generic surfaces [Koen76, Rieg87]. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 19 The singularities (visual events) for polyhedra can be completely characterized as the intersection in the image plane of sets of non-adjacent model edges <ref> [Plan90, Gigu90] </ref>. These events and their inter-relationships form the image structure graph (ISG), a graph where each node is a vertex and each incident edge is a projected model edge. <p> Recent work in viewer-centered modeling has concentrated on the aspect graph, a graph enumerating all of the topologically-distinct 2D views of a 3D object, as well as the transitions between views. The aspect graph has been constructed for polyhedra <ref> [Plan90, Gigu90, Bowy89] </ref>, solids of revolution [Egge89, Krie89], and piecewise-smooth objects [Srip89, Ponc90]. The rim appearance representation is different from the aspect graph in two significant ways. First, the rim appearance representation stores only the appearance of the rim, that is, the occluding contour. <p> A viewpoint that causes a topological change in the line drawing from any infinitesimal change in viewing direction is called a visual event. It has been shown that all such visual events can be found for polyhedra by computing the edge-edge-edge event (EEE-event) <ref> [Plan90, Gigu90] </ref>. The EEE-event is the apparent intersection of three not necessarily adjacent edges. A degenerate case of the EEE-event is the edge-vertex event (EV-event), where two of the edges actually meet at a vertex. See Section 3.1.4 for more details on properties of EEE-events. 4.2.3. <p> The visibility boundaries caused by the grid are specified by the EEE-event [Plan88]. In IR 3 , the locus of points where three lines project to a single point is a ruled quadric surface <ref> [Gigu90] </ref>. The intersection of the ruled surface for three edges with the view sphere forms a polynomial (quadratic) curve with coefficients that are a function of the endpoints of the three segments.
Reference: [Glas88] <author> Glassner, A., </author> <title> Spacetime ray tracing for animation, </title> <journal> IEEE Computer Graphics and Applications 8(2), </journal> <year> 1988, </year> <pages> 60-70. </pages>
Reference-contexts: Interactive Viewing There is a tradeoff in solving the problem of interactive viewing between off-line and on-line solutions. At one extreme is complete off-line image rendering [Denb86]. The on-line phase is a fixed playback of precomputed images that have been rendered with raytracing or other off-line techniques <ref> [Glas88, Cook84] </ref>. The other extreme is to use standard display algorithms such as Z-buffering to render each frame of the sequence independently. The main drawbacks of the total precomputation approach are the size and inflexibility of the resulting animation description. <p> The precomputation of information can be as extreme as complete image rendering off-line. The on-line animation phase is then reduced to a playback of the precomputed images [Denb86]. There are ways to compute realistic images off-line using, for example, ray tracing techniques <ref> [Glas88, Cook84] </ref>, but the high per-frame cost makes interactive animated display using these approaches intractable. The main drawback of the total precomputation approach is the size and inflexibility of the resulting animation description.
Reference: [Goad83] <author> Goad, C., </author> <title> Special purpose automatic programming for 3-D model-based vision, </title> <booktitle> Proc. Image Understanding Workshop, </booktitle> <year> 1983, </year> <pages> 94-104. </pages>
Reference-contexts: Exactly how to do this is a difficult problem since a 3D object can appear very differently from different views. Researchers have divided the views of a 3D object 6 in two different ways: (1) the uniform division of the space of all viewpoints <ref> [Goad83, Ikeu87, Korn87, Feke84] </ref>; and (2) the division of the space of viewpoints into sets based on some definition of equivalence [Bowy89, Egge89, Plan90, Gigu90, Krie89, Srip89]. Although the uniformly-divided viewpoint space is often a fair approximation of the appearance of an object, several problems remain.
Reference: [Grim90] <author> Grimson, W. E. L., </author> <title> Object recognition by computer: The role of geometric constraints, </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1990. </year>
Reference-contexts: In computer vision, the problem of how to solve for the set of viewpoints from which a 3D model will project to an observed set of shape features is considered an important step in certain object recognition paradigms <ref> [Grim90, Lowe87] </ref>. Solutions to the viewpoint-determination problem are based on how features of 3D objects will appear in an image. Our approach to this problem is to use as a model a precomputed representation of the appearance of the occluding contour. <p> Model-Based Object Recognition and Pose Determination Approaches to the viewpoint determination problem must solve two inter-related sub-problems: finding the correct correspondence of model and image features, and recovering the viewpoint that maximizes the match of corresponding features <ref> [Grim90] </ref>. Solutions vary from solving these two problems independently to treating them as a combined process, with the goal of solving for viewpoint. <p> Parameter space methods [Thom87] that can avoid a costly search of the entire space of transformations become much more efficient when they need only consider this reduced set of viewpoints. The search of an interpretation tree <ref> [Grim90] </ref> is also made more efficient in space and time when there are global constraints on the possible solutions. Aspect graph methods [Bowy89, Krie90, Ikeu87] must address the problem of how to select a few aspects to test from a large number of potential aspects. 9 2.4. <p> In the case of iterative methods [Lowe87], a critically necessary starting viewpoint can be obtained from a constrained viewpoint set. Parameter space methods [Thom87] that can avoid a costly search of the entire space of transformations become much more efficient. The search of an interpretation tree <ref> [Grim90] </ref> is more efficient in space and time when there are global constraints on the possible solutions. Aspect graph methods [Bowy89, Krie90, Ikeu87] must address the problem of how to select a few aspects to test from a large number of potential aspects. <p> This study of the rim appearance representation for model-based viewpoint determination has led to related research ideas that integrate rim appearance information with the interpretation tree paradigm. The notion of the interpretation tree with geometric constraints <ref> [Grim90] </ref> can be modified to include the specific geometric appearance information in the rim appearance representation. There has been work using the silhouette that is related to this idea [VanH87]. The following paragraphs briefly summarize the ideas relating the rim appearance to the interpretation tree paradigm for model-based vision. <p> Each level of the tree describes a partial interpretation of the image data. The central idea of the interpretation tree is that correspondences can be pruned by enforcing geometric constraints between assigned parts of the model <ref> [Grim90] </ref>. The primary power of the interpretation tree is derived from the strength of the constraints that prune the tree. The rim appearance representation provides a way to compute exact geometric constraints between parts of the occluding contour. <p> We are interested in how to efficiently integrate the multi-level rim appearance representation with other model features like color, texture and surface markings, and with other known search paradigms. Specifically, the rim appearance representation can provide strong constraints for the interpretation tree paradigm <ref> [Grim90] </ref> for searching the space of model-image correspondences. The rim appearance representation makes occlusion and the appearance of the occluding contour explicit, while the interpretation tree efficiently organizes the search of correspondences and the application of constraints.
Reference: [Grim90a] <author> Grimson, W. E. L., </author> <title> The combinatorics of object recognition in cluttered environments using constrained search, </title> <booktitle> Artificial Intelligence 44, </booktitle> <year> 1990, </year> <pages> 121-165. </pages>
Reference-contexts: Similarly, the alignment approach [Basr88, Hutt90] and parameter space methods [Thom87] assume a model-image correspondence in order to derive a unique viewpoint. Interpretation tree methods solve for correspondence and viewpoint simultaneously by using a constrained search through a tree of model-image correspondences <ref> [Grim90a] </ref>. Finally, viewer-centered representations such as characteristic views [Wang90] and the aspect graph [Plan87, Bowy89, Gigu90] precompute representative sets of viewpoints and then attempt to solve for the best correspondence between image features and the features in each representative view.
Reference: [Horn77] <author> Horn, B. K. P., </author> <title> Understanding image intensities, </title> <journal> Artificial Intell. </journal> <volume> 21, </volume> <year> 1977, </year> <pages> 201-231. </pages>
Reference-contexts: Under the assumptions of diffuse surfaces and point light sources, the shading model described by Equation 3.7 can be used to precompute the shaded intensity of each of the faces in the scene given a fixed set of light source positions. This information is equivalent to the reflectance map <ref> [Horn77] </ref> for the scene, and can be used in conjunction with the asp to compute the shaded appearance of each face in the scene from any viewpoint. 3.2.3.2. Interpolated Shading The shading model defined by Equation 3.7 does not simulate the smoothness of curved surfaces which are approximated by polygons.
Reference: [Hubs81] <author> Hubschman, H. and S. W. Zucker, </author> <title> Frame-to-frame coherence and the hidden surface computation: Constraints for a convex world, </title> <booktitle> Computer Graphics 15, </booktitle> <year> 1981, </year> <pages> 45-54. </pages>
Reference-contexts: Rendering each frame at display-time, however, requires too much computation to be interactive. There are several approaches to this problem that use intermediate representations to reduce the work at display time. Hubschman and Zucker introduced the idea of using frame-to-frame coherence to decrease the time required for hidden-line removal <ref> [Hubs81] </ref>. This work was not extended to any objects other than convex polyhedra. Shelley and Greenberg used frame-to-frame coherence for the generation of an animation sequence corresponding to a smooth viewpath through a 3D environment [Shel82]. <p> At runtime, a viewpath through the space of viewpoints determines those events that are relevant to updating each successive frame in an animation sequence. Hubschman and Zucker introduced the idea of using frame-to-frame coherence to decrease the time required for hidden-line removal <ref> [Hubs81] </ref>. They worked in a world with a small number of stationary convex polyhedra and they found a number of frame-to-frame coherence constraints.
Reference: [Hutt90] <author> Huttenlocher, D. P. and S. Ullman, </author> <title> Recognizing solid objects by alignment with an image, </title> <booktitle> Int. J. Computer Vision 5, </booktitle> <year> 1990, </year> <pages> 195-212. </pages>
Reference-contexts: For example, iterative methods assume that the model-image correspondence is known, and solve for viewpoint by applying numerical techniques to revise an initial estimation of viewpoint [Lowe87, Ponc89, Worr89]. Similarly, the alignment approach <ref> [Basr88, Hutt90] </ref> and parameter space methods [Thom87] assume a model-image correspondence in order to derive a unique viewpoint. Interpretation tree methods solve for correspondence and viewpoint simultaneously by using a constrained search through a tree of model-image correspondences [Grim90a].
Reference: [Ikeu87] <author> Ikeuchi, K., </author> <title> Generating an interpretation tree from a CAD model for 3D object recognition in bin-picking tasks, </title> <journal> Int. J. Computer Vision, </journal> <year> 1987, </year> <pages> 145-165. 121 </pages>
Reference-contexts: Exactly how to do this is a difficult problem since a 3D object can appear very differently from different views. Researchers have divided the views of a 3D object 6 in two different ways: (1) the uniform division of the space of all viewpoints <ref> [Goad83, Ikeu87, Korn87, Feke84] </ref>; and (2) the division of the space of viewpoints into sets based on some definition of equivalence [Bowy89, Egge89, Plan90, Gigu90, Krie89, Srip89]. Although the uniformly-divided viewpoint space is often a fair approximation of the appearance of an object, several problems remain. <p> First, the appearance of an object from a single viewpoint is taken to be representative of an entire region of viewpoints. This is necessary in order to discretize the continuous set of viewpoints. For example, Ikeuchi <ref> [Ikeu87] </ref> divides the view sphere into 240 triangles. The appearance of a 3D object from the center of a single triangle is taken to be representative of the appearance over the entire triangular patch. <p> The search of an interpretation tree [Grim90] is also made more efficient in space and time when there are global constraints on the possible solutions. Aspect graph methods <ref> [Bowy89, Krie90, Ikeu87] </ref> must address the problem of how to select a few aspects to test from a large number of potential aspects. 9 2.4. Detecting Occlusion Features Edges in an image that are part of the occluding contour are produced at depth discontinuity boundaries in the scene. <p> By precomputing and ordering the viewpoints where topological changes occur, the coherency between viewpoints is exploited. This coherence between frames is a result of the fact that for most small changes in viewpoint along a smooth viewpath, only linear changes in the appearance of the scene take place <ref> [Ikeu87] </ref>. Linear changes are those changes in the viewpoint that do not change the structure of the projected line drawing in the image. <p> Parameter space methods [Thom87] that can avoid a costly search of the entire space of transformations become much more efficient. The search of an interpretation tree [Grim90] is more efficient in space and time when there are global constraints on the possible solutions. Aspect graph methods <ref> [Bowy89, Krie90, Ikeu87] </ref> must address the problem of how to select a few aspects to test from a large number of potential aspects. There are several important advantages to using the rim appearance representation over the aspect graph.
Reference: [Joy88] <author> Joy, K., C. Grant, N. Max, and L. </author> <title> Hatfield, Tutorial: Computer Graphics: Image Synthesis, </title> <publisher> IEEE Computer Society Press, </publisher> <address> Washington, D.C., </address> <year> 1988. </year>
Reference-contexts: Shadow region computation in polyhedral scenes has been incorporated into hidden surface algorithms using shadow volumes [Crow77] and polygon clipping [Athe78]. Most hidden-surface removal algorithms can compute shadow regions by performing hidden-surface removal from the position of the light source <ref> [Joy88] </ref>. Rendering can then be performed by integrating the shading of the shadow regions and the visible surfaces according to some shading model. These methods of removing hidden surfaces and finding shadow regions are not designed for interactive viewing. <p> Event-Based Depth Ordering A list of polygons can be displayed with hidden portions removed if the polygons are ordered in depth with respect to the viewer [Newe72]. That is, the surfaces can be scan converted in back-to-front order to cover portions of faces that are hidden <ref> [Joy88] </ref>. The BSP-tree [Fuch83] is based on this premise: the precomputed tree is a more efficient way to sort the faces of a scene in depth. Given a viewpoint, the depth ordering is available from the BSP-tree by an in-order traversal of the tree.
Reference: [Kass88] <author> Kass, M., A. Witkin, and D. Terzopoulos, Snakes: </author> <title> active contour models, </title> <journal> Int. J. Computer Vision, </journal> <year> 1988, </year> <pages> 321-331. </pages>
Reference-contexts: Researchers have also begun to integrate stereo data and dynamic information in order to segment and measure the curvature on occlusion boundaries [Cipo90]. Contours are tracked throughout a dynamic image sequence using an energy-based contour tracking method <ref> [Kass88] </ref> in order to model how the contour changes as viewpoint changes. This information is used to reconstruct the properties of the occluding contour and the 3D surface generating it. Spatiotemporal representations are formed from images over time that are stacked together into a cube of data.
Reference: [Koen76] <author> Koenderink, J. J. and A. J. van Doorn, </author> <title> The singularities of the visual mapping, </title> <booktitle> Biological Cybernetics 24, </booktitle> <year> 1976, </year> <pages> 51-59. </pages>
Reference-contexts: This framework for modeling the appearance of 3D shape relates the two problems of shape display for visualization and shape understanding for recognition. A major theme through this work is the visual event <ref> [Koen76] </ref>. Very generally, an event is an image change resulting from a change in the viewer's vantage point. If the problem is to display a model from a set of viewpoints, as in animation, then the visual event is the change between frames in the animation sequence. <p> The singularities of the projection mapping have been studied by Whitney [Whit55], and there is work being done toward understanding the form of singularities under projection of specific 3D models such as polyhedra [Plan90, Gigu90, Stew88], surfaces of revolution [Krie89, Egge89], parametric surfaces [Srip89, Ponc90], and generic surfaces <ref> [Koen76, Rieg87] </ref>. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 19 The singularities (visual events) for polyhedra can be completely characterized as the intersection in the image plane of sets of non-adjacent model edges [Plan90, Gigu90].
Reference: [Koen84] <author> Koenderink, J. J., </author> <title> What does the occluding contour tell us about solid shape?, </title> <booktitle> Perception 13, </booktitle> <year> 1984, </year> <pages> 321-330. </pages>
Reference-contexts: The projection mapping generates occluding contours, and the opacity of solid shape causes the creation of T-junctions in the image plane. The arrangement of these contours and T-junctions is directly related to the 3D properties of the shape and provides strong information for recognition <ref> [Marr77, Koen84] </ref>. Object-centered object models do not explicitly represent the properties of the occluding contour since the occluding contour is not generated by any specific set of object features. Viewer-centered models of 3D shape are better suited to encode the dynamic, viewpoint-dependent nature of the occluding contour.
Reference: [Koen87] <author> Koenderink, J. J., </author> <title> An internal representation for solid shape based on the topological properties of the apparent contour, in Image Understanding 1985-86, </title> <editor> W. Richards and S. Ullman, ed., </editor> <publisher> Ablex, </publisher> <address> Norwood, N.J., </address> <year> 1987, </year> <pages> 257-285. </pages>
Reference-contexts: The Rim and the Occluding Contour The terminology for describing the sets of points related to the occluding contour varies widely. Marr used the term contour generator [Marr77], and others have used terms such as limbs [Egge89, Nalw88, Mali87] and the rim <ref> [Basr88, Koen87] </ref>. To avoid confusion, the precise meaning is defined here for the sets of 3D points called the rim, the visible rim and the occluded rim, and the sets of 2D points in the image plane called the contour, the occluding contour and the occluded contour. <p> The depth ordering of surfaces relative to the viewer and qualitative pose information can be inferred from T-junctions and their relative orientations. The curvature of the occluding contour is directly related to the 3D surface generating it [Koen90]. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh strongly constrain viewpoint. This figure was taken from <ref> [Koen87] </ref>. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 95 The relationship between sets of occluding contour features generated by arbitrary non-convex 3D shape strongly constrains the possible viewpoints that can generate those features.
Reference: [Koen90] <author> Koenderink, J. J., </author> <title> Solid Shape, </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1990. </year>
Reference-contexts: The rim is the transition, or the boundary, between these patches <ref> [Koen90] </ref>. This definition of the rim is defined by the local visibility condition, and the contour generated by the projection of the rim contains points that are potentially, but not necessarily, visible. The projection of the rim is the contour that would be generated by a transparent shape. <p> In Figure 4.2 (b), the rim edges are exactly those edges on the transition boundary between faces that are potentially visible and faces that are not visible. The rim points for a smooth surface always form a set of closed space curves <ref> [Koen90] </ref>. Likewise, the rim edges for a polyhedron form a set of closed curves made up of connected "chains" of edges in IR 3 . The geometry of the rim is relatively simple, and is related to the topology of the surface itself. <p> There are certain advantages in making use of features of the occluding contour for viewpoint determination. The occluding contour provides strong information for viewpoint constraints and for 3D object recognition <ref> [Koen90, Rich88] </ref>, although until now there has been little work to incorporate this information into a model-based approach. The depth ordering of surfaces relative to the viewer and qualitative pose information can be inferred from T-junctions and their relative orientations. <p> The depth ordering of surfaces relative to the viewer and qualitative pose information can be inferred from T-junctions and their relative orientations. The curvature of the occluding contour is directly related to the 3D surface generating it <ref> [Koen90] </ref>. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh strongly constrain viewpoint. This figure was taken from [Koen87]. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 95 The relationship between sets of occluding contour features generated by arbitrary non-convex 3D shape strongly constrains the possible viewpoints that can generate those features. <p> A "swallowtail" breaks the occluding contour at the T-junction that is created by the self-occlusion of an opaque surface <ref> [Koen90] </ref>. The rim appearance representation computes all of the visual events that affect the visibility of rim edges. These events include T-junctions that cause the occluding contour to split and merge, as well as the EV-events and EEE-events where T-junctions begin and end. <p> This higher-level structure provides a representation of the topological features of the occluding contour that is independent of the constituent edges. The geometry of contour terminals is a key part of the change in the topology of the occluding contour across viewpoint <ref> [Koen90] </ref>. Contours end in polyhedra at concave edges only, i.e., at edges where the measure of the exterior angle between the faces meeting at the edge is less than p. Concave edges can be excluded from the rim since a concave edge can never occlude anything behind it.
Reference: [Korn87] <author> Korn, M. R. and C. R. Dyer, </author> <title> 3-D multiview object representations for model-based object recognition, Pattern Recognition 20, </title> <booktitle> 1987, </booktitle> <pages> 91-103. </pages>
Reference-contexts: Exactly how to do this is a difficult problem since a 3D object can appear very differently from different views. Researchers have divided the views of a 3D object 6 in two different ways: (1) the uniform division of the space of all viewpoints <ref> [Goad83, Ikeu87, Korn87, Feke84] </ref>; and (2) the division of the space of viewpoints into sets based on some definition of equivalence [Bowy89, Egge89, Plan90, Gigu90, Krie89, Srip89]. Although the uniformly-divided viewpoint space is often a fair approximation of the appearance of an object, several problems remain.
Reference: [Krie89] <author> Kriegman, D. J. and J. Ponce, </author> <title> Computing exact aspect graphs of curved objects: Solids of revolution, </title> <booktitle> Proc. IEEE Workshop on Interpretation of 3D Scenes, </booktitle> <year> 1989, </year> <pages> 116-122. </pages>
Reference-contexts: Researchers have divided the views of a 3D object 6 in two different ways: (1) the uniform division of the space of all viewpoints [Goad83, Ikeu87, Korn87, Feke84]; and (2) the division of the space of viewpoints into sets based on some definition of equivalence <ref> [Bowy89, Egge89, Plan90, Gigu90, Krie89, Srip89] </ref>. Although the uniformly-divided viewpoint space is often a fair approximation of the appearance of an object, several problems remain. First, the appearance of an object from a single viewpoint is taken to be representative of an entire region of viewpoints. <p> The singularities of the projection mapping have been studied by Whitney [Whit55], and there is work being done toward understanding the form of singularities under projection of specific 3D models such as polyhedra [Plan90, Gigu90, Stew88], surfaces of revolution <ref> [Krie89, Egge89] </ref>, parametric surfaces [Srip89, Ponc90], and generic surfaces [Koen76, Rieg87]. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 19 The singularities (visual events) for polyhedra can be completely characterized as the intersection in the image plane of sets of non-adjacent model edges [Plan90, Gigu90]. <p> Recent work in viewer-centered modeling has concentrated on the aspect graph, a graph enumerating all of the topologically-distinct 2D views of a 3D object, as well as the transitions between views. The aspect graph has been constructed for polyhedra [Plan90, Gigu90, Bowy89], solids of revolution <ref> [Egge89, Krie89] </ref>, and piecewise-smooth objects [Srip89, Ponc90]. The rim appearance representation is different from the aspect graph in two significant ways. First, the rim appearance representation stores only the appearance of the rim, that is, the occluding contour. <p> The implicit equation for a torus under orthographic projection is an 8th degree 67 polynomial containing over 170 terms. Others have computed the singularities of the rim surface for the purposes of computing the aspect graph for surfaces of revolution <ref> [Egge89, Krie89] </ref> and for parametric surfaces [Ponc90, Srip89]. All of this work with smooth model representations suffers from difficult numerical problems. Numerical difficulties are avoided when polyhedra are used because of the linearity of edges.
Reference: [Krie90] <author> Kriegman, D. J. and J. Ponce, </author> <title> On recognizing and positioning curved 3D objects from image contours, </title> <journal> IEEE Trans. Pattern Analysis and Machine Intell. </journal> <volume> 12(12), </volume> <year> 1990, </year> <pages> 1127-1137. </pages>
Reference-contexts: The search of an interpretation tree [Grim90] is also made more efficient in space and time when there are global constraints on the possible solutions. Aspect graph methods <ref> [Bowy89, Krie90, Ikeu87] </ref> must address the problem of how to select a few aspects to test from a large number of potential aspects. 9 2.4. Detecting Occlusion Features Edges in an image that are part of the occluding contour are produced at depth discontinuity boundaries in the scene. <p> The interesting thing about this surface is that its singularities correspond to self-occlusion in the image plane. There has been recent research in formulating an explicit expression for the locus of points that lie on the folds and creases of the multi-dimensional rim surface. Kriegman and Ponce <ref> [Krie90] </ref> have used parametric patches under a weak perspective viewing model to give an implicit equation for the occluding contour of surfaces of revolution. The implicit equation for a torus under orthographic projection is an 8th degree 67 polynomial containing over 170 terms. <p> Parameter space methods [Thom87] that can avoid a costly search of the entire space of transformations become much more efficient. The search of an interpretation tree [Grim90] is more efficient in space and time when there are global constraints on the possible solutions. Aspect graph methods <ref> [Bowy89, Krie90, Ikeu87] </ref> must address the problem of how to select a few aspects to test from a large number of potential aspects. There are several important advantages to using the rim appearance representation over the aspect graph. <p> The CC-event is the T-junction that is produced by the projection of two smooth surfaces. The difficulty in explicitly representing T-junctions for smooth surfaces is the numerical complexity of the locus of surface points that define the viewing directions where T-junctions occur <ref> [Krie90] </ref>. For polyhedra, the EE-events assembled into piecewise CC-event structures provide an approximation of the smooth T-junction geometry. <p> The rim appearance representation organizes polyhedral contour events. In the smooth case, the locus of viewpoints where visual events occur can be specified as solutions to algebraic equations. The solutions to such equations, however, are numerically 105 complex <ref> [Krie90] </ref>. The polyhedral approximation eliminates the numerical complexity and provides a piecewise organization that preserves many of the interesting and detectable occluding contour features. In summary, the rim appearance representation explicitly represents the visual events that cause a change in the topology of the occluding contour.
Reference: [Kutu91] <author> Kutulakos, K., </author> <type> Personal communication. </type> <year> 1991. </year>
Reference-contexts: This alone represents a substantial savings since there can be as many as O (n 3 ) EEE-events. There are several subtleties to this algorithm that are straightforward but nontrivial <ref> [Kutu91] </ref>. One such subtlety involves cycles in the digraph. Cycles that occur in the digraph under certain conditions cause the topological sort to fail (the topological sort can detect the cycle).
Reference: [Lowe87] <author> Lowe, D. G., </author> <title> Three-dimensional object recognition from single two-dimensional images, </title> <journal> Artificial Intell. </journal> <volume> 31, </volume> <year> 1987, </year> <pages> 355-395. </pages>
Reference-contexts: In computer vision, the problem of how to solve for the set of viewpoints from which a 3D model will project to an observed set of shape features is considered an important step in certain object recognition paradigms <ref> [Grim90, Lowe87] </ref>. Solutions to the viewpoint-determination problem are based on how features of 3D objects will appear in an image. Our approach to this problem is to use as a model a precomputed representation of the appearance of the occluding contour. <p> Solutions vary from solving these two problems independently to treating them as a combined process, with the goal of solving for viewpoint. For example, iterative methods assume that the model-image correspondence is known, and solve for viewpoint by applying numerical techniques to revise an initial estimation of viewpoint <ref> [Lowe87, Ponc89, Worr89] </ref>. Similarly, the alignment approach [Basr88, Hutt90] and parameter space methods [Thom87] assume a model-image correspondence in order to derive a unique viewpoint. Interpretation tree methods solve for correspondence and viewpoint simultaneously by using a constrained search through a tree of model-image correspondences [Grim90a]. <p> Our approach is to use the appearance of the occluding contour to solve for a set of viewpoints that can globally constrain where a single viewpoint solution must lie. In the case of iterative methods <ref> [Lowe87] </ref>, a starting viewpoint can be obtained from this small viewpoint set. Parameter space methods [Thom87] that can avoid a costly search of the entire space of transformations become much more efficient when they need only consider this reduced set of viewpoints. <p> It is assumed that geometric information about the models is known, occluding contours can be detected in the image, and information such as surface normals or texture is not available. The problem of constraining viewpoint for a particular model is often considered a subproblem of model-based 3D object recognition <ref> [Lowe87] </ref>. In general, the model-based approach is to select a model M and the corresponding viewpoint V that will produce a projection that best matches the image data. For each model, the best viewpoint V selected from a space of all possible viewpoints is computed. <p> Initially solving for a constrained viewpoint region using the occluding contour is necessary for many other methods for object recognition because they assume a given, approximate solution. In the case of iterative methods <ref> [Lowe87] </ref>, a critically necessary starting viewpoint can be obtained from a constrained viewpoint set. Parameter space methods [Thom87] that can avoid a costly search of the entire space of transformations become much more efficient. <p> An exact solution for viewpoint is not found, but rather a constrained region of viewpoints is found that accounts for the T-junction and occluding contour data. Given a tightly-constrained set of viewpoints, an exact solution can be found using, for example, an iterative method <ref> [Lowe87] </ref>. Furthermore, a contour correspondence is found, not an exact edge-to-edge correspondence. The two model edges aligned with the T-junction in Because of the symmetry of the torus there are many close solutions that can be found. The solution viewpoint displayed is the center of the final solution viewpoint region.
Reference: [Lowe89] <author> Lowe, D. G., </author> <title> Fitting parameterized 3-D models to images, </title> <type> Technical Report 89-26, </type> <institution> Computer Science Department, University of British Columbia, </institution> <address> Vancouver, </address> <publisher> B. </publisher> <address> C., </address> <year> 1989. </year>
Reference-contexts: Experience in both computer vision and graphics has shown that the speed and simplicity of linear representations can compensate for size increases and approximation error, provided the appropriate feature information is retained <ref> [Faug86, Lowe89] </ref>. In the present context, it is important to preserve the occluding contour and the interactions between contours as a result of self-occlusion. The local approximation of smooth surfaces with planar patches preserves these contour features while affording linearity. <p> The viewer-centered approach to modeling 3D shapes makes the changes in features with respect to viewpoint explicit. These changes can be used in a dynamic context where image features are observed changing over time, or where matching methods must iteratively refine an estimation of viewpoint <ref> [Lowe89] </ref>. In addition, an explicit model that includes features of self-occlusion for solid shape makes the prediction of the appearance of the model a faster process that can speed up model matching [Basr88].
Reference: [Mali87] <author> Malik, J., </author> <title> Interpreting line drawings of curved objects, </title> <booktitle> Int. J. Computer Vision 1, </booktitle> <year> 1987, </year> <pages> 73-103. </pages>
Reference-contexts: The Rim and the Occluding Contour The terminology for describing the sets of points related to the occluding contour varies widely. Marr used the term contour generator [Marr77], and others have used terms such as limbs <ref> [Egge89, Nalw88, Mali87] </ref> and the rim [Basr88, Koen87]. <p> This is based on the organization of contour features as a function of viewpoint. Smooth opaque shapes without surface discontinuities generate only T-junctions, smooth occluding contours, and contour terminals in the image plane <ref> [Mali87] </ref>. It is assumed here that the polyhedral model is an approximation of a smooth shape. It can then be assumed that none of the polyhedral edges are true surface discontinuities.
Reference: [Marr77] <author> Marr, D., </author> <title> Analysis of occluding contour, </title> <journal> Proc. Royal Society London 197, </journal> <year> 1977, </year> <pages> 441-475. </pages>
Reference-contexts: The projection mapping generates occluding contours, and the opacity of solid shape causes the creation of T-junctions in the image plane. The arrangement of these contours and T-junctions is directly related to the 3D properties of the shape and provides strong information for recognition <ref> [Marr77, Koen84] </ref>. Object-centered object models do not explicitly represent the properties of the occluding contour since the occluding contour is not generated by any specific set of object features. Viewer-centered models of 3D shape are better suited to encode the dynamic, viewpoint-dependent nature of the occluding contour. <p> The Rim and the Occluding Contour The terminology for describing the sets of points related to the occluding contour varies widely. Marr used the term contour generator <ref> [Marr77] </ref>, and others have used terms such as limbs [Egge89, Nalw88, Mali87] and the rim [Basr88, Koen87].
Reference: [Murr89] <author> Murray, D., D. Castelow, and B. Buxton, </author> <title> From image sequences to recognized moving polyhedral objects, </title> <booktitle> Int. J. Computer Vision 3, </booktitle> <year> 1989, </year> <pages> 181-208. </pages>
Reference-contexts: Pioneering work in computer vision by L. Roberts, for example, and early work in realistic image synthesis was based almost solely on the geometry of polyhedra. Much of the work in 3D recognition still relies on the simplicity and compactness of the polyhedron (for example, see <ref> [Thom87, Murr89] </ref> ). Experience in both computer vision and graphics has shown that the speed and simplicity of linear representations can compensate for size increases and approximation error, provided the appropriate feature information is retained [Faug86, Lowe89].
Reference: [Nalw88] <author> Nalwa, V. S., </author> <title> Line-drawing interpretation: A mathematical framework, </title> <booktitle> Int. J. Computer Vision 2, </booktitle> <year> 1988, </year> <pages> 103-124. </pages>
Reference-contexts: The Rim and the Occluding Contour The terminology for describing the sets of points related to the occluding contour varies widely. Marr used the term contour generator [Marr77], and others have used terms such as limbs <ref> [Egge89, Nalw88, Mali87] </ref> and the rim [Basr88, Koen87].
Reference: [Nayl90] <author> Naylor, B., SCULPT: </author> <title> An interactive solid modeling tool, </title> <booktitle> Proc. Graphics Interface '90, </booktitle> <year> 1990, </year> <pages> 138-145. </pages>
Reference-contexts: The Binary Space Partition Tree (BSP-tree) has been used to display polyhedral scenes in near real-time by precomputing a structure that gives relative depth-ordering for faces in the model <ref> [Fuch83, Nayl90] </ref>. The BSP-tree does not take advantage of the frame-to-frame coherence in animation sequences, however. <p> The approach reported here generates the appearance of the scene on-line as the viewpath is interactively defined. The Binary Space Partition Tree (BSP-tree) has been used to display polyhedral scenes in near real-time by precomputing a structure that gives relative depth-ordering for faces in the model <ref> [Fuch83, Nayl90] </ref>. The BSP-tree simplifies the hidden-surface computation for an arbitrary viewing direction by encoding the relative depth ordering of the model implicitly in a tree structure for all viewing directions.
Reference: [Newe72] <author> Newell, M. E., R. G. Newell, and T. L. Sancha, </author> <title> A new approach to the shaded picture problem, </title> <booktitle> Proc. ACM Nat. Conf., </booktitle> <year> 1972. </year> <month> 122 </month>
Reference-contexts: Thus there is an asymptotic improvement in the complexity of the algorithm when using visual events to compute depth ordering. 3.4.1. Event-Based Depth Ordering A list of polygons can be displayed with hidden portions removed if the polygons are ordered in depth with respect to the viewer <ref> [Newe72] </ref>. That is, the surfaces can be scan converted in back-to-front order to cover portions of faces that are hidden [Joy88]. The BSP-tree [Fuch83] is based on this premise: the precomputed tree is a more efficient way to sort the faces of a scene in depth.
Reference: [Phon75] <author> Phong, B. T., </author> <title> Illumination for computer generated pictures, </title> <journal> Comm. ACM 18, </journal> <year> 1975, </year> <pages> 311-317. </pages>
Reference-contexts: Interpolated Shading The shading model defined by Equation 3.7 does not simulate the smoothness of curved surfaces which are approximated by polygons. Given a smooth surface and its corresponding polygonal approximation, the Phong shading model <ref> [Phon75] </ref> interpolates geometric information to estimate a normal at each point on the surface. This normal is used with an augmented shading function to shade each point on the surface individually, yielding a smoother shading within and across polygons. <p> Assumption (2) can be handled for a point on a face f contained in IR 3 using a bilinear interpolation scheme that makes use of the normal to the surface at the vertices of f <ref> [Phon75] </ref>. This scheme assumes that the 3D coordinates of the point on the face are known. The information derived from the asp, however, is the appearance in the image plane only, i.e., there is no 3D information.
Reference: [Plan86] <author> Plantinga, W. H. and C. R. Dyer, </author> <title> An algorithm for constructing the aspect graph, </title> <booktitle> Proc. IEEE Symp. Foundations of Computer Science, </booktitle> <year> 1986, </year> <pages> 123-131. </pages>
Reference-contexts: Section 3.5 concludes with a review of the contributions of this work and the promising future directions. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh q x z view sphere. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 16 The asp <ref> [Plan88, Plan87, Plan86] </ref> is a representation that quantifies the visual events in a 3D polyhedral model that arise as a result of occlusion. In general, the visual events that are explicitly represented in the asp are the apparent intersections of edges. <p> However, depending on the problem, aspect space can become very high dimensional and therefore the asp may require more space and time to compute. For related work on extending the asp to perspective projection, see <ref> [Plan86, Plan87, Plan88] </ref>. This chapter has presented an efficient algorithm for interactively viewing a polyhedral scene. The algorithm takes advantage of viewpath coherence, a form of frame-to-frame coherence, which is inherent in the sequence of images generated by a moving viewer along a continuous viewpath.
Reference: [Plan87] <author> Plantinga, W. H. and C. R. Dyer, </author> <title> The asp: A continuous viewer-centered representation for 3D object recognition, </title> <booktitle> Proc. 1st Int. Conf. Computer Vision, </booktitle> <year> 1987, </year> <pages> 626-630. </pages>
Reference-contexts: Interpretation tree methods solve for correspondence and viewpoint simultaneously by using a constrained search through a tree of model-image correspondences [Grim90a]. Finally, viewer-centered representations such as characteristic views [Wang90] and the aspect graph <ref> [Plan87, Bowy89, Gigu90] </ref> precompute representative sets of viewpoints and then attempt to solve for the best correspondence between image features and the features in each representative view. <p> As a result, the storage requirements are Q (n 3 ) even in the case of a single, nondegenerate convex polyhedron. A generalization of this technique to multiple non-convex polyhedra would result in worst-case storage requirements of Q (n 9 ) for a scene with n faces <ref> [Plan87] </ref>. The algorithm presented here places no restrictions on the 14 model polyhedra, allowing arbitrary nonconvexities, and extends to the hidden-surface case. Shelley and Greenberg used frame-to-frame coherence for the generation of an animation sequence corresponding to a smooth viewpath through a 3D environment [Shel82]. <p> Section 3.5 concludes with a review of the contributions of this work and the promising future directions. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh q x z view sphere. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 16 The asp <ref> [Plan88, Plan87, Plan86] </ref> is a representation that quantifies the visual events in a 3D polyhedral model that arise as a result of occlusion. In general, the visual events that are explicitly represented in the asp are the apparent intersections of edges. <p> However, depending on the problem, aspect space can become very high dimensional and therefore the asp may require more space and time to compute. For related work on extending the asp to perspective projection, see <ref> [Plan86, Plan87, Plan88] </ref>. This chapter has presented an efficient algorithm for interactively viewing a polyhedral scene. The algorithm takes advantage of viewpath coherence, a form of frame-to-frame coherence, which is inherent in the sequence of images generated by a moving viewer along a continuous viewpath. <p> The details of this local, piecewise representation of the rim surface are presented in this chapter. A complete discussion of aspect space itself can be found in the work of Plantinga and Dyer <ref> [Plan88, Plan87] </ref>. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh V V 2 V 4 1 V 3 u q q tour surface that is 3D. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 68 4.1.3. An Edge-Based Representation The asp for polyhedra stores visual events affecting every face in the model.
Reference: [Plan88] <author> Plantinga, W. H., </author> <title> The Asp: A Continuous, Viewer-centered Object Representation for Computer Vision, </title> <type> Ph.D. Dissertation, </type> <institution> Computer Science Department, University of Wisconsin-Madison, </institution> <year> 1988. </year>
Reference-contexts: The indexing problem of selecting the appropriate aspect from the graph is difficult because of the size and the lack of organization of the graph. Another problem is the size of the aspect graph for even very simple polyhedra <ref> [Plan88] </ref>. The extension of the method to large databases of models seems infeasible without significant restructuring. Finally, the cost of the computation of the aspect graph is large. 7 The viewer-centered representation presented in this thesis focuses on individual features rather than a constant global topology. <p> A polyhedral scene can be rendered with varying degrees of detail, ranging from wire-frame display to visible-surface display using a shading model. The basic algorithm first introduced by Plantinga <ref> [Plan88] </ref> is presented for animating the display of a polyhedral scene with hidden lines removed; this algorithm is modified to allow display with hidden surfaces removed, including the use of shading models, multiple light sources and shadow computation. <p> The rest of this chapter focuses on how to efficiently apply the computation of visual events in a space of viewpoints to the problem of interactive viewing with either the hidden lines or hidden surfaces removed. The main previous results related to the asp <ref> [Plan88] </ref> are reviewed in Section 3.1. Section 3.1 also defines the orthographic viewing 15 model and the visual event. Section 3.2 presents a detailed discussion of the computation and organization of visual events for display. <p> Section 3.5 concludes with a review of the contributions of this work and the promising future directions. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh q x z view sphere. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 16 The asp <ref> [Plan88, Plan87, Plan86] </ref> is a representation that quantifies the visual events in a 3D polyhedral model that arise as a result of occlusion. In general, the visual events that are explicitly represented in the asp are the apparent intersections of edges. <p> The central characteristic of the asp for a polyhedron is that the asp represents the appearance of the polyhedral faces for all viewpoints rather than a single viewpoint <ref> [Plan88] </ref>. Specifically, the asp for a single face is a volume in the 4D space of image space viewpoint space, i.e., in IR 2 S 2 . The boundaries of this volume are hypersur-faces that correspond to the visibility of the edges bounding the face. <p> For example, the intersection of two volumes in aspect space can be computed exactly in closed form. The asp was initially studied in order to construct the aspect graph. The work by Plantinga <ref> [Plan88] </ref> includes a complete discussion of the definition, construction, and properties of the asp. As Plantinga describes, a fundamental property of aspect space is that occlusion in object space corresponds to set subtraction in aspect space. <p> The equation of the image point where two edges E 1 and E 2 appear to intersect in the image plane (u,v ) is a function of the viewpoint parameter q and is given by <ref> [Plan88] </ref>: u = (x 1 + s a 1 ) cos q - (c 1 + s z 1 ) sin q (3.3) where the parameter s is expressed as (3.4) s = V . ((p 2 - p 1 ) a 2 ) hhhhhhhhhhhhhhhhh in the image plane. <p> 2 p p a p a 2 + 3 3 + EEE-Event E 2 EventE 1 E 3 EventE 2 E 1 E 1 2D surface for E 1 (a) C 1 C 2 The intersection of two 1D curves on a 2D surface in aspect space hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh vector V <ref> [Plan88] </ref>. The value of the viewpoint parameter q for vector V is the value of q where the two 1D curves C 1 and C 2 intersect in aspect space on S 1 . <p> This implies that the construction algorithm can do no better that O (e 2 ) unless other information is given about which edges are "in front of" other edges. For example, if the scene is known to be convex, the construction time of the asp is linear in e <ref> [Plan88] </ref>. 3.2. Event-Based Interactive Display The asp construction phase computes a representation off-line that can be used to interactively generate the image sequences of a polyhedral scene from the position of a moving viewer with hidden-lines removed [Plan88, Plan90a]. <p> Event-Based Interactive Display The asp construction phase computes a representation off-line that can be used to interactively generate the image sequences of a polyhedral scene from the position of a moving viewer with hidden-lines removed <ref> [Plan88, Plan90a] </ref>. The asp encodes the viewpoints within the space of all possible viewpoints where visual events occur. <p> However, depending on the problem, aspect space can become very high dimensional and therefore the asp may require more space and time to compute. For related work on extending the asp to perspective projection, see <ref> [Plan86, Plan87, Plan88] </ref>. This chapter has presented an efficient algorithm for interactively viewing a polyhedral scene. The algorithm takes advantage of viewpath coherence, a form of frame-to-frame coherence, which is inherent in the sequence of images generated by a moving viewer along a continuous viewpath. <p> The details of this local, piecewise representation of the rim surface are presented in this chapter. A complete discussion of aspect space itself can be found in the work of Plantinga and Dyer <ref> [Plan88, Plan87] </ref>. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh V V 2 V 4 1 V 3 u q q tour surface that is 3D. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 68 4.1.3. An Edge-Based Representation The asp for polyhedra stores visual events affecting every face in the model. <p> Constructing the Rim Appearance Representation The rim appearance representation models the occluding contour as a function of viewpoint. Representing the rim as a function of viewpoint is related to the asp <ref> [Plan88] </ref>, a complete representation of appearance as a function of viewpoint. The relationship of the rim appearance representation to the asp and the aspect graph depends on a single key component: aspect space. <p> Aspect space, the cross-product space of the image plane viewpoint space, is the central component in the construction of the rim appearance representation. Aspect space has been shown to be useful for encoding the exact appearance of polyhedra as a function of viewpoint <ref> [Plan88] </ref>, constructing the aspect graph 76 [Plan90, Plan88], and for the interactive animation of polyhedral scenes [Seal90, Plan90a]. 4.3.1. <p> Aspect space, the cross-product space of the image plane viewpoint space, is the central component in the construction of the rim appearance representation. Aspect space has been shown to be useful for encoding the exact appearance of polyhedra as a function of viewpoint [Plan88], constructing the aspect graph 76 <ref> [Plan90, Plan88] </ref>, and for the interactive animation of polyhedral scenes [Seal90, Plan90a]. 4.3.1. Aspect Space and the Rim Appearance Representation Aspect space, the cross-product space of the image plane viewpoint space, makes use of a multi-dimensional space to explicitly encode the appearance of objects for all viewpoints. <p> A review of aspect space and visual events was given in Chapter 3. A complete discussion of 4D aspect space can be found in Plantinga's thesis <ref> [Plan88] </ref>. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh q A appears B appears A disappears B disappears v q A disappearsB appears A appears B disappears (a) (b) x z (u, v) = image space corresponding to the edge in (a). <p> With two degrees of freedom in viewpoint, the dimensionality of the visibility structures for faces, edges and vertices each increase by 1. A fundamental property of aspect space is that occlusion is equivalent to set subtraction in aspect space <ref> [Plan88] </ref>. Consider two faces and their corresponding visibility structures. A point that lies within the visibility structure for both faces is a single image 78 point generated from both faces. <p> e 3 . (c) A linked set of intervals along the viewpoint axis represents the edges that form the T-junction. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 79 The asp is the explicit boundary model of the visibility structures in aspect space that corresponds to the visibility of each vertex, edge and face of a polyhedron <ref> [Plan88, Plan90] </ref>. The asp for a polyhedron quantifies the appearance of each face, taking into account the occlusion relationships between faces. The asp for a single planar face can be described exactly by computing the equations of its boundaries in aspect space. <p> The resulting visibility structure is the appearance of the rim edge including the boundaries in aspect space corresponding to T-junctions formed with other rim edges. The algebraic boundaries generated by this process are specified by the equations for the EV- and EEE-events <ref> [Plan88] </ref>. (b) The final visibility structure for a single edge must be divided into a set of potentially disjoint volumes. This division is based on the global visibility of the edge. The global visibility test must be made to determine if the edge is fully visible or completely occluded. <p> Since the algorithm must compute the intersection of the visibility structure for each rim edge with every other, the construction time is bounded by O (n 5 ). These complexity bounds are the same as those for constructing the asp <ref> [Plan88] </ref>. As with the asp, pathological polyhedra such as picket fences and grids can achieve the worst-case behavior. <p> The image coordinates of a projected T-junction for two edges are represented directly as a function of the viewing direction V and the endpoints of the segments <ref> [Plan88] </ref>. As shown in Figure 5.2 (b), a viewpoint is modeled as a point (q, f) on the unit sphere. The orthographic 99 projective transformation for this sphere of viewpoints is rotation by (q, f) and then orthographic projection in the direction of the z-axis. <p> The set of viewpoints where the two edges project to the T-junction is split into disjoint regions where the T-junction is visible. The visibility boundaries caused by the grid are specified by the EEE-event <ref> [Plan88] </ref>. In IR 3 , the locus of points where three lines project to a single point is a ruled quadric surface [Gigu90]. <p> The following paragraphs give a chapter-by-chapter summary of the primary contributions of this thesis, along with a brief mention of current and future research directions. Chapter 3 reviewed the asp representation <ref> [Plan88, Plan90] </ref>, the basis for our display algorithms using visual events. The visual-event-based algorithm for display, first introduced by Plantinga [Plan88], has been developed, extended, implemented and tested. <p> Chapter 3 reviewed the asp representation [Plan88, Plan90], the basis for our display algorithms using visual events. The visual-event-based algorithm for display, first introduced by Plantinga <ref> [Plan88] </ref>, has been developed, extended, implemented and tested. The problem of how to efficiently display a polyhedral scene over a path of viewpoints is cast as a problem of computing visual events along that path.
Reference: [Plan89] <author> Plantinga, W. H., C. R. Dyer, and W. B. Seales, </author> <title> Real-time hidden-line elimination for a rotating polyhedral scene using the aspect representation, </title> <type> Technical Report 89-3, </type> <institution> University of Pittsburgh, </institution> <address> Pittsburgh, Pa., </address> <year> 1989. </year>
Reference-contexts: The partial occlusion of an edge by some other edge (the beginning or ending of a T-junction) is another kind of event. More formally, all structural changes in the projection of a polyhedral scene are the result of the overlap in the image plane of three edges <ref> [Plan89, Gigu90] </ref>. In the case of a face that turns away from the viewing direction, the point at which the face is edge-on to the viewer is a 13 degenerate form of the apparent intersection of three edges.
Reference: [Plan90] <author> Plantinga, W. H. and C. R. Dyer, </author> <title> Visibility, occlusion, and the aspect graph, </title> <booktitle> Int. J. Computer Vision 5(2), </booktitle> <year> 1990, </year> <pages> 137-160. </pages>
Reference-contexts: Researchers have divided the views of a 3D object 6 in two different ways: (1) the uniform division of the space of all viewpoints [Goad83, Ikeu87, Korn87, Feke84]; and (2) the division of the space of viewpoints into sets based on some definition of equivalence <ref> [Bowy89, Egge89, Plan90, Gigu90, Krie89, Srip89] </ref>. Although the uniformly-divided viewpoint space is often a fair approximation of the appearance of an object, several problems remain. First, the appearance of an object from a single viewpoint is taken to be representative of an entire region of viewpoints. <p> The singularities of the projection mapping have been studied by Whitney [Whit55], and there is work being done toward understanding the form of singularities under projection of specific 3D models such as polyhedra <ref> [Plan90, Gigu90, Stew88] </ref>, surfaces of revolution [Krie89, Egge89], parametric surfaces [Srip89, Ponc90], and generic surfaces [Koen76, Rieg87]. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 19 The singularities (visual events) for polyhedra can be completely characterized as the intersection in the image plane of sets of non-adjacent model edges [Plan90, Gigu90]. <p> of specific 3D models such as polyhedra [Plan90, Gigu90, Stew88], surfaces of revolution [Krie89, Egge89], parametric surfaces [Srip89, Ponc90], and generic surfaces [Koen76, Rieg87]. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 19 The singularities (visual events) for polyhedra can be completely characterized as the intersection in the image plane of sets of non-adjacent model edges <ref> [Plan90, Gigu90] </ref>. These events and their inter-relationships form the image structure graph (ISG), a graph where each node is a vertex and each incident edge is a projected model edge. <p> Recent work in viewer-centered modeling has concentrated on the aspect graph, a graph enumerating all of the topologically-distinct 2D views of a 3D object, as well as the transitions between views. The aspect graph has been constructed for polyhedra <ref> [Plan90, Gigu90, Bowy89] </ref>, solids of revolution [Egge89, Krie89], and piecewise-smooth objects [Srip89, Ponc90]. The rim appearance representation is different from the aspect graph in two significant ways. First, the rim appearance representation stores only the appearance of the rim, that is, the occluding contour. <p> A viewpoint that causes a topological change in the line drawing from any infinitesimal change in viewing direction is called a visual event. It has been shown that all such visual events can be found for polyhedra by computing the edge-edge-edge event (EEE-event) <ref> [Plan90, Gigu90] </ref>. The EEE-event is the apparent intersection of three not necessarily adjacent edges. A degenerate case of the EEE-event is the edge-vertex event (EV-event), where two of the edges actually meet at a vertex. See Section 3.1.4 for more details on properties of EEE-events. 4.2.3. <p> Aspect space, the cross-product space of the image plane viewpoint space, is the central component in the construction of the rim appearance representation. Aspect space has been shown to be useful for encoding the exact appearance of polyhedra as a function of viewpoint [Plan88], constructing the aspect graph 76 <ref> [Plan90, Plan88] </ref>, and for the interactive animation of polyhedral scenes [Seal90, Plan90a]. 4.3.1. Aspect Space and the Rim Appearance Representation Aspect space, the cross-product space of the image plane viewpoint space, makes use of a multi-dimensional space to explicitly encode the appearance of objects for all viewpoints. <p> e 3 . (c) A linked set of intervals along the viewpoint axis represents the edges that form the T-junction. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 79 The asp is the explicit boundary model of the visibility structures in aspect space that corresponds to the visibility of each vertex, edge and face of a polyhedron <ref> [Plan88, Plan90] </ref>. The asp for a polyhedron quantifies the appearance of each face, taking into account the occlusion relationships between faces. The asp for a single planar face can be described exactly by computing the equations of its boundaries in aspect space. <p> Thus S can be a non-connected region on the view sphere where each connected subregion is bounded by a combination of great circles and quadratic curves generated by EEE-events <ref> [Plan90] </ref>. A set of T-junctions (rather than just one T-junction) can be produced by a non-convex shape at a single viewpoint. The geometric relationships between simultaneously visible T-junctions is of interest because of the additional viewpoint constraints that they provide. <p> The following paragraphs give a chapter-by-chapter summary of the primary contributions of this thesis, along with a brief mention of current and future research directions. Chapter 3 reviewed the asp representation <ref> [Plan88, Plan90] </ref>, the basis for our display algorithms using visual events. The visual-event-based algorithm for display, first introduced by Plantinga [Plan88], has been developed, extended, implemented and tested.
Reference: [Plan90a] <author> Plantinga, W. H., W. B. Seales, and C. R. Dyer, </author> <title> Real-time hidden-line elimination for a rotating polyhedral scene using the aspect representation, </title> <booktitle> Proc. Graphics Interface '90, </booktitle> <year> 1990, </year> <pages> 9-16. </pages>
Reference-contexts: Event-Based Interactive Display The asp construction phase computes a representation off-line that can be used to interactively generate the image sequences of a polyhedral scene from the position of a moving viewer with hidden-lines removed <ref> [Plan88, Plan90a] </ref>. The asp encodes the viewpoints within the space of all possible viewpoints where visual events occur. <p> Aspect space has been shown to be useful for encoding the exact appearance of polyhedra as a function of viewpoint [Plan88], constructing the aspect graph 76 [Plan90, Plan88], and for the interactive animation of polyhedral scenes <ref> [Seal90, Plan90a] </ref>. 4.3.1. Aspect Space and the Rim Appearance Representation Aspect space, the cross-product space of the image plane viewpoint space, makes use of a multi-dimensional space to explicitly encode the appearance of objects for all viewpoints.
Reference: [Ponc89] <author> Ponce, J. and D. J. Kriegman, </author> <title> On recognizing and positioning curved 3D objects from image contours, </title> <booktitle> Proc. Image Understanding Workshop, </booktitle> <year> 1989, </year> <pages> 461-470. </pages>
Reference-contexts: Solutions vary from solving these two problems independently to treating them as a combined process, with the goal of solving for viewpoint. For example, iterative methods assume that the model-image correspondence is known, and solve for viewpoint by applying numerical techniques to revise an initial estimation of viewpoint <ref> [Lowe87, Ponc89, Worr89] </ref>. Similarly, the alignment approach [Basr88, Hutt90] and parameter space methods [Thom87] assume a model-image correspondence in order to derive a unique viewpoint. Interpretation tree methods solve for correspondence and viewpoint simultaneously by using a constrained search through a tree of model-image correspondences [Grim90a].
Reference: [Ponc90] <author> Ponce, J. and D. J. Kriegman, </author> <title> Computing exact aspect graphs of curved objects: parametric surfaces, </title> <type> Technical Report UIUCDCS-R-90-1579, </type> <institution> Department of Computer Science, University of Illinois at Urbana-Champaign, </institution> <year> 1990. </year>
Reference-contexts: The singularities of the projection mapping have been studied by Whitney [Whit55], and there is work being done toward understanding the form of singularities under projection of specific 3D models such as polyhedra [Plan90, Gigu90, Stew88], surfaces of revolution [Krie89, Egge89], parametric surfaces <ref> [Srip89, Ponc90] </ref>, and generic surfaces [Koen76, Rieg87]. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 19 The singularities (visual events) for polyhedra can be completely characterized as the intersection in the image plane of sets of non-adjacent model edges [Plan90, Gigu90]. <p> The aspect graph has been constructed for polyhedra [Plan90, Gigu90, Bowy89], solids of revolution [Egge89, Krie89], and piecewise-smooth objects <ref> [Srip89, Ponc90] </ref>. The rim appearance representation is different from the aspect graph in two significant ways. First, the rim appearance representation stores only the appearance of the rim, that is, the occluding contour. <p> The implicit equation for a torus under orthographic projection is an 8th degree 67 polynomial containing over 170 terms. Others have computed the singularities of the rim surface for the purposes of computing the aspect graph for surfaces of revolution [Egge89, Krie89] and for parametric surfaces <ref> [Ponc90, Srip89] </ref>. All of this work with smooth model representations suffers from difficult numerical problems. Numerical difficulties are avoided when polyhedra are used because of the linearity of edges. The rim surface for polyhedra affords a local way of expressing the singulari-ties as piecewise interactions related to individual edges.
Reference: [Ponc90a] <author> Ponce, J. and D. J. Kriegman, </author> <title> Elimination theory and computer vision: recognition and the positioning of curved 3D objects from range, intensity, or contours, </title> <type> Technical Report UIUCDCS-R-90-1612, </type> <institution> Department of Computer Science, University of Illinois at Urbana-Champaign, </institution> <year> 1990. </year>
Reference-contexts: The approximation presented here of the exact rim appearance is an approximation in viewpoint space, where the complexity is reduced by approximate visibility criteria. There are other possible approaches to these problems, each with its own various tradeoffs. This is an area of active research <ref> [Ponc90a, Bowy91, Dick91] </ref>. 4.4.3. Complexity Analysis The complexity of constructing the approximate rim appearance representation is bounded by the number of EE-events that affect the appearance of the rim. The number of EE-events determines the construction time and space complexity. Unlike the exact algorithm, EEE-events are not computed.
Reference: [Rich88] <author> Richards, W., B. Dawson, and D. Whittington, </author> <title> Encoding contour shape by curvature extrema, in Natural Computation, </title> <editor> W. Richards, ed., </editor> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1988, </year> <pages> 83-98. </pages>
Reference-contexts: This viewer-centered representation is essentially the set of visual events that affect the occluding contour. These visual events, such as T-junctions, are strong cues in the projection of 3D shape 3 <ref> [Rich88] </ref>. Features of the occluding contour are stored and organized into a structure making inter-feature relationships and dynamic feature changes explicit. <p> There are certain advantages in making use of features of the occluding contour for viewpoint determination. The occluding contour provides strong information for viewpoint constraints and for 3D object recognition <ref> [Koen90, Rich88] </ref>, although until now there has been little work to incorporate this information into a model-based approach. The depth ordering of surfaces relative to the viewer and qualitative pose information can be inferred from T-junctions and their relative orientations.
Reference: [Rieg87] <author> Rieger, J. H., </author> <title> On the classification of views of piecewise smooth objects, </title> <booktitle> Image and Vision Computing 1, </booktitle> <year> 1987, </year> <pages> 91-97. </pages>
Reference-contexts: The singularities of the projection mapping have been studied by Whitney [Whit55], and there is work being done toward understanding the form of singularities under projection of specific 3D models such as polyhedra [Plan90, Gigu90, Stew88], surfaces of revolution [Krie89, Egge89], parametric surfaces [Srip89, Ponc90], and generic surfaces <ref> [Koen76, Rieg87] </ref>. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 19 The singularities (visual events) for polyhedra can be completely characterized as the intersection in the image plane of sets of non-adjacent model edges [Plan90, Gigu90].
Reference: [Seal90] <author> Seales, W. B. and C. R. Dyer, </author> <title> Shaded rendering and shadow computation for polyhedral animation, </title> <booktitle> Proc. Graphics Interface '90, </booktitle> <year> 1990, </year> <pages> 175-182. </pages>
Reference-contexts: Aspect space has been shown to be useful for encoding the exact appearance of polyhedra as a function of viewpoint [Plan88], constructing the aspect graph 76 [Plan90, Plan88], and for the interactive animation of polyhedral scenes <ref> [Seal90, Plan90a] </ref>. 4.3.1. Aspect Space and the Rim Appearance Representation Aspect space, the cross-product space of the image plane viewpoint space, makes use of a multi-dimensional space to explicitly encode the appearance of objects for all viewpoints.
Reference: [Shel82] <author> Shelley, K. and D. Greenberg, </author> <title> Path specification and path coherence, </title> <booktitle> Computer Graphics 16(3), </booktitle> <year> 1982, </year> <pages> 157-166. </pages>
Reference-contexts: This work was not extended to any objects other than convex polyhedra. Shelley and Greenberg used frame-to-frame coherence for the generation of an animation sequence corresponding to a smooth viewpath through a 3D environment <ref> [Shel82] </ref>. Although the viewpath can be specified interactively, the computation of the appearance of the scene along the viewpath is done off-line and then displayed. <p> The algorithm presented here places no restrictions on the 14 model polyhedra, allowing arbitrary nonconvexities, and extends to the hidden-surface case. Shelley and Greenberg used frame-to-frame coherence for the generation of an animation sequence corresponding to a smooth viewpath through a 3D environment <ref> [Shel82] </ref>. A smooth viewpath is represented as a B-spline, which can be interactively defined. Path coherence (frame-to-frame coherence along a single viewpath) is exploited to reduce the expense of the sorting and culling operations for visible line and visible surface computation.
Reference: [Srip89] <author> Sripradisvarakul, T. and R. Jain, </author> <title> Generating aspect graphs for curved objects, </title> <booktitle> Proc. Workshop on Interpretation of 3D Scenes, </booktitle> <year> 1989, </year> <pages> 109-115. 123 </pages>
Reference-contexts: Researchers have divided the views of a 3D object 6 in two different ways: (1) the uniform division of the space of all viewpoints [Goad83, Ikeu87, Korn87, Feke84]; and (2) the division of the space of viewpoints into sets based on some definition of equivalence <ref> [Bowy89, Egge89, Plan90, Gigu90, Krie89, Srip89] </ref>. Although the uniformly-divided viewpoint space is often a fair approximation of the appearance of an object, several problems remain. First, the appearance of an object from a single viewpoint is taken to be representative of an entire region of viewpoints. <p> The singularities of the projection mapping have been studied by Whitney [Whit55], and there is work being done toward understanding the form of singularities under projection of specific 3D models such as polyhedra [Plan90, Gigu90, Stew88], surfaces of revolution [Krie89, Egge89], parametric surfaces <ref> [Srip89, Ponc90] </ref>, and generic surfaces [Koen76, Rieg87]. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 19 The singularities (visual events) for polyhedra can be completely characterized as the intersection in the image plane of sets of non-adjacent model edges [Plan90, Gigu90]. <p> The aspect graph has been constructed for polyhedra [Plan90, Gigu90, Bowy89], solids of revolution [Egge89, Krie89], and piecewise-smooth objects <ref> [Srip89, Ponc90] </ref>. The rim appearance representation is different from the aspect graph in two significant ways. First, the rim appearance representation stores only the appearance of the rim, that is, the occluding contour. <p> The implicit equation for a torus under orthographic projection is an 8th degree 67 polynomial containing over 170 terms. Others have computed the singularities of the rim surface for the purposes of computing the aspect graph for surfaces of revolution [Egge89, Krie89] and for parametric surfaces <ref> [Ponc90, Srip89] </ref>. All of this work with smooth model representations suffers from difficult numerical problems. Numerical difficulties are avoided when polyhedra are used because of the linearity of edges. The rim surface for polyhedra affords a local way of expressing the singulari-ties as piecewise interactions related to individual edges.
Reference: [Stew88] <author> Stewman, J. and K. Bowyer, </author> <title> Creating the perspective projection aspect graph of polyhedral objects, </title> <booktitle> Proc. 2nd Int. Conf. Computer Vision, </booktitle> <year> 1988, </year> <pages> 494-500. </pages>
Reference-contexts: The singularities of the projection mapping have been studied by Whitney [Whit55], and there is work being done toward understanding the form of singularities under projection of specific 3D models such as polyhedra <ref> [Plan90, Gigu90, Stew88] </ref>, surfaces of revolution [Krie89, Egge89], parametric surfaces [Srip89, Ponc90], and generic surfaces [Koen76, Rieg87]. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 19 The singularities (visual events) for polyhedra can be completely characterized as the intersection in the image plane of sets of non-adjacent model edges [Plan90, Gigu90].
Reference: [Swan86] <author> Swanson, R. W. and L. J. Thayer, </author> <title> A Fast-shaded polygon renderer, </title> <booktitle> Proc. SIGGRAPH, </booktitle> <year> 1986, </year> <pages> 95-101. </pages>
Reference-contexts: Interactive viewing using an interpolated shading model requires that a distinct intensity value be computed for each point inside a given face. Interpolated shading and fast Phong shading can be done relatively quickly <ref> [Swan86, Bish86] </ref>, and a scan line algorithm can again be used to combine information from shadow sets and shaded visible surfaces in the image plane. 3.3. Computational Results A prototype of both the preprocessing and the on-line portions of the hidden line algorithm has been implemented.
Reference: [Thom87] <author> Thompson, D. W. and J. L. Mundy, </author> <title> Three-dimensional model matching from an unconstrained viewpoint, </title> <booktitle> Proc. IEEE Int. Conf. Robotics and Automation, </booktitle> <year> 1987, </year> <pages> 208-220. </pages>
Reference-contexts: For example, iterative methods assume that the model-image correspondence is known, and solve for viewpoint by applying numerical techniques to revise an initial estimation of viewpoint [Lowe87, Ponc89, Worr89]. Similarly, the alignment approach [Basr88, Hutt90] and parameter space methods <ref> [Thom87] </ref> assume a model-image correspondence in order to derive a unique viewpoint. Interpretation tree methods solve for correspondence and viewpoint simultaneously by using a constrained search through a tree of model-image correspondences [Grim90a]. <p> In the case of iterative methods [Lowe87], a starting viewpoint can be obtained from this small viewpoint set. Parameter space methods <ref> [Thom87] </ref> that can avoid a costly search of the entire space of transformations become much more efficient when they need only consider this reduced set of viewpoints. <p> Pioneering work in computer vision by L. Roberts, for example, and early work in realistic image synthesis was based almost solely on the geometry of polyhedra. Much of the work in 3D recognition still relies on the simplicity and compactness of the polyhedron (for example, see <ref> [Thom87, Murr89] </ref> ). Experience in both computer vision and graphics has shown that the speed and simplicity of linear representations can compensate for size increases and approximation error, provided the appropriate feature information is retained [Faug86, Lowe89]. <p> In the case of iterative methods [Lowe87], a critically necessary starting viewpoint can be obtained from a constrained viewpoint set. Parameter space methods <ref> [Thom87] </ref> that can avoid a costly search of the entire space of transformations become much more efficient. The search of an interpretation tree [Grim90] is more efficient in space and time when there are global constraints on the possible solutions.
Reference: [Thom85] <author> Thompson, W. B., K. M. Mutch, and V. A. Berzins, </author> <title> Dynamic occlusion analysis in optical flow fields, </title> <journal> IEEE Trans. Pattern Analysis and Machine Intell. </journal> <volume> 7, </volume> <year> 1985, </year> <pages> 374-383. </pages>
Reference-contexts: In addition to the segmentation issue is the task of detecting features of the occluding contour such as curvature extrema and T-junctions. Work on detecting the occluding contour from optic flow has focused on the occlusion boundary <ref> [Thom85] </ref>, where optic flow constraints are violated. Discontinuities in the flow field are interpreted as depth discontinuities in the scene. These methods depend on the dynamic flow data available from a moving viewer or a dynamic scene. Recent work on detecting occlusion boundaries using stereo shows promising results [Toh90].
Reference: [Toh90] <author> Toh, P. S. and A. K. Forrest, </author> <title> Occlusion detection in early vision, </title> <booktitle> Proc. 3rd Int. Conf. Computer Vision, </booktitle> <year> 1990, </year> <pages> 126-132. </pages>
Reference-contexts: Discontinuities in the flow field are interpreted as depth discontinuities in the scene. These methods depend on the dynamic flow data available from a moving viewer or a dynamic scene. Recent work on detecting occlusion boundaries using stereo shows promising results <ref> [Toh90] </ref>. Stereo methods provide depth data that are used in conjunction with image intensities to segment those image edges that lie on depth discontinuities. The robustness of the stereo algorithm is central to this approach.
Reference: [Vail89] <author> Vaillant, R. and O. Faugeras, </author> <title> Using occluding contours for recovering shape properties of objects, </title> <booktitle> Proc. IEEE Workshop on Interpretation of 3D Scenes, </booktitle> <year> 1989, </year> <pages> 26-32. </pages>
Reference-contexts: The robustness of the stereo algorithm is central to this approach. Unfortunately, most stereo algorithms lack robustness at occlusion boundaries, Other recent work on surface reconstruction has used stereo to detect and use occluding contours <ref> [Vail89] </ref> for surface reconstruction. Researchers have also begun to integrate stereo data and dynamic information in order to segment and measure the curvature on occlusion boundaries [Cipo90].
Reference: [VanH87] <author> VanHove, P., </author> <title> Model-based silhouette recognition, </title> <booktitle> Proc. IEEE Workshop on Computer Vision, </booktitle> <year> 1987, </year> <pages> 88-93. </pages>
Reference-contexts: The notion of the interpretation tree with geometric constraints [Grim90] can be modified to include the specific geometric appearance information in the rim appearance representation. There has been work using the silhouette that is related to this idea <ref> [VanH87] </ref>. The following paragraphs briefly summarize the ideas relating the rim appearance to the interpretation tree paradigm for model-based vision. An interpretation tree is an organized way of searching the possible correspondences between model and image features.
Reference: [Wang90] <author> Wang, R. and H. Freeman, </author> <title> Object recognition based on characteristic view classes, </title> <booktitle> Proc. 10th Int. Conf. Pattern Recognition, </booktitle> <year> 1990, </year> <pages> 17-21. </pages>
Reference-contexts: Interpretation tree methods solve for correspondence and viewpoint simultaneously by using a constrained search through a tree of model-image correspondences [Grim90a]. Finally, viewer-centered representations such as characteristic views <ref> [Wang90] </ref> and the aspect graph [Plan87, Bowy89, Gigu90] precompute representative sets of viewpoints and then attempt to solve for the best correspondence between image features and the features in each representative view.
Reference: [Whit55] <author> Whitney, H., </author> <title> On the singularities of mappings in Euclidean spaces, I: mappings of the plane into the plane, </title> <journal> Ann. of Math. </journal> <volume> 62, </volume> <year> 1955, </year> <pages> 374-410. </pages>
Reference-contexts: Certain singularities can occur under projection from IR 3 into the 2D image plane, and these singularities have been termed visual events. The singularities of the projection mapping have been studied by Whitney <ref> [Whit55] </ref>, and there is work being done toward understanding the form of singularities under projection of specific 3D models such as polyhedra [Plan90, Gigu90, Stew88], surfaces of revolution [Krie89, Egge89], parametric surfaces [Srip89, Ponc90], and generic surfaces [Koen76, Rieg87]. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 19 The singularities (visual events) for polyhedra can be completely
Reference: [Whit80] <author> Whitted, T., </author> <title> An improved illumination model for shaded display, </title> <journal> Comm. ACM 23, </journal> <year> 1980, </year> <pages> 343-349. </pages>
Reference-contexts: Scenes of this complexity have been rendered previously using ray tracing techniques that create shadow regions as a by-product of light-source and ray intersections <ref> [Whit80, Cook84] </ref>. The asp represents the appearance of each face in the scene from all viewing directions. By treating each light source as an additional viewing direction, the asp can be used to obtain the appearance of a particular face from the light source position.
Reference: [Whit88] <author> Whitted, T. and R. Cook, </author> <title> A comprehensive shading model, in Computer Graphics: Image Synthesis, </title> <editor> K. Joy, C. Grant, N. Max and L. Hatfield, ed., </editor> <publisher> IEEE Computer Society Press, </publisher> <address> Washington, D.C., </address> <year> 1988, </year> <pages> 232-243. </pages>
Reference-contexts: N where I A is a constant to account for reflected ambient light, N is the number of light sources, k d is the percentage of incident light reflected, I L i is the radiance of light source i, and L i is the direction of the ith light source <ref> [Whit88] </ref>. Under the assumptions of diffuse surfaces and point light sources, the shading model described by Equation 3.7 can be used to precompute the shaded intensity of each of the faces in the scene given a fixed set of light source positions.
Reference: [Worr89] <author> Worrall, A. D., K. D. Baker, and G. D. Sullivan, </author> <title> Model based perspective inversion, </title> <booktitle> Image and Vision Computing 7(1), </booktitle> <year> 1989, </year> <pages> 17-23. </pages>
Reference-contexts: Solutions vary from solving these two problems independently to treating them as a combined process, with the goal of solving for viewpoint. For example, iterative methods assume that the model-image correspondence is known, and solve for viewpoint by applying numerical techniques to revise an initial estimation of viewpoint <ref> [Lowe87, Ponc89, Worr89] </ref>. Similarly, the alignment approach [Basr88, Hutt90] and parameter space methods [Thom87] assume a model-image correspondence in order to derive a unique viewpoint. Interpretation tree methods solve for correspondence and viewpoint simultaneously by using a constrained search through a tree of model-image correspondences [Grim90a].

References-found: 78

