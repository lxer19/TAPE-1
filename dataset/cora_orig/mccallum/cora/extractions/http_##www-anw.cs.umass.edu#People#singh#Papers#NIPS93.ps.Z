URL: http://www-anw.cs.umass.edu/People/singh/Papers/NIPS93.ps.Z
Refering-URL: http://www-anw.cs.umass.edu/People/singh/Papers/
Root-URL: 
Email: singh@psyche.mit.edu  
Title: Robust Reinforcement Learning in Motion Planning  
Author: Satinder P. Singh Andrew G. Barto, Roderic Grupen, and Christopher Connolly 
Address: Cambridge, MA 02139  Amherst, MA 01003  
Affiliation: Department of Brain and Cognitive Sciences Massachusetts Institute of Technology  Department of Computer Science University of Massachusetts  computer science at the University of Massachusetts, Amherst.  
Note: As appears in Neural Information Processing Systems 6, pp. 655-662, 1994.  This work was done while the first author was finishing his Ph.D in  
Abstract: While exploring to find better solutions, an agent performing online reinforcement learning (RL) can perform worse than is acceptable. In some cases, exploration might have unsafe, or even catastrophic, results, often modeled in terms of reaching `failure' states of the agent's environment. This paper presents a method that uses domain knowledge to reduce the number of failures during exploration. This method formulates the set of actions from which the RL agent composes a control policy to ensure that exploration is conducted in a policy space that excludes most of the unacceptable policies. The resulting action set has a more abstract relationship to the task being solved than is common in many applications of RL. Although the cost of this added safety is that learning may result in a suboptimal solution, we argue that this is an appropriate tradeoff in many problems. We illustrate this method in the domain of motion planning. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Barto, A.G., Bradtke, S.J., & Singh, </author> <title> S.P. (to appear). Learning to act using real-time dynamic programming. </title> <journal> Artificial Intelligence. </journal>
Reference: <author> Connolly, C. </author> <year> (1992). </year> <title> Applications of harmonic functions to robotics. </title> <booktitle> In The 1992 International Symposium on Intelligent Control. IEEE. </booktitle>
Reference: <author> Connolly, C. & Grupen, R. </author> <year> (1993). </year> <title> On the applications of harmonic functions to robotics. </title> <journal> Journal of Robotic Systems, </journal> <volume> 10 (7), </volume> <pages> 931-946. </pages>
Reference-contexts: A safe path in our context is one that avoids all obstacles and terminates in a desired configuration. Connolly (1992) has developed a method that generates safe paths by solving Laplace's equation in configuration space with boundary conditions determined by obstacle and goal configurations <ref> (also see, Connolly & Grupen, 1993) </ref>. Laplace's equation is the partial differential equation r 2 = i=1 @x 2 = 0; (1) whose solution is a harmonic function, , with no interior local minima.
Reference: <author> Mahadevan, S. & Connell, J. </author> <year> (1990). </year> <title> Automatic programming of behavior-based robots using reinforcement learning. </title> <type> Technical report, </type> <institution> IBM Research Division, T.J.Watson Research Center, Yorktown Heights, NY. </institution>
Reference: <author> Moore, A.W. & Atkeson, C.G. </author> <year> (1993). </year> <title> Prioritized sweeping: Reinforcement learning with less data and less real time. </title> <journal> Machine Learning, </journal> <volume> 13 (1). </volume>
Reference: <author> Singh, S.P. </author> <year> (1992). </year> <title> Transfer of learning by composing solutions for elemental sequential tasks. </title> <journal> Machine Learning, </journal> <volume> 8 (3/4), </volume> <pages> 323-339. </pages>
Reference: <author> Singh, S.P. </author> <year> (1993). </year> <title> Learning to Solve Markovian Decision Processes. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Massachusetts. </institution> <note> also, CMPSCI Technical Report 93-77. </note>
Reference: <author> Sutton, </author> <title> R.S., </title> <type> Barto, </type> <institution> A.G., & Williams, </institution> <address> R.J. </address> <year> (1991). </year> <title> Reinforcement learning is direct adaptive optimal control. </title> <booktitle> In Proceedings of the American Control Conference, </booktitle> <pages> pages 2143-2146, </pages> <address> Boston, MA. </address>
Reference: <author> Watkins, C.J.C.H. </author> <year> (1989). </year> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> Cambridge Univ., </institution> <address> Cambridge, England. </address>
Reference-contexts: The learning task is to approximate minimum time paths from every point inside the environment to the goal region without contacting the boundary wall. A reinforcement learning algorithm called Q-learning <ref> (Watkins, 1989) </ref> (see Appendix A) was used to learn the mixing function, k. Figure 1A shows the 2-layer neural network architecture used to store the Q-values.
References-found: 9

