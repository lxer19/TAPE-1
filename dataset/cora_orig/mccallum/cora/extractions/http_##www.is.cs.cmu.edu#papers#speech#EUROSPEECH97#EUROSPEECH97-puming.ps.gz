URL: http://www.is.cs.cmu.edu/papers/speech/EUROSPEECH97/EUROSPEECH97-puming.ps.gz
Refering-URL: http://www.is.cs.cmu.edu/ISL.speech.publications.html
Root-URL: 
Email: Email: fzhan, ahwg@cs.cmu.edu  
Title: SPEAKER NORMALIZATION AND SPEAKER ADAPTATION A COMBINATION FOR CONVERSATIONAL SPEECH RECOGNITION  
Author: Puming Zhan, Martin Westphal, Michael Finke and Alex Waibel 
Address: Germany  
Affiliation: Interactive Systems Laboratories Carnegie Mellon University University of Karlsruhe,  
Abstract: Speaker normalization and speaker adaptation are two strategies to tackle the variations from speaker, channel, and environment. The vocal tract length normalization (VTLN) is an effective speaker normalization approach to compensate for the variations of vocal tract shapes. The Maximum Likelihood Linear Regression(MLLR) is a recent proposed method for speaker-adaptation. In this paper, we propose a speaker-specific Bark scale VTLN method, investigate the combination of the VTLN with MLLR, and present an iterative procedure for decoding the combined system of VTLN and MLLR. The results show that: (1) the new VTLN method is very effective with which the word error rate can be reduced up to 11%; (2) the combination of VTLN and MLLR can provide up to 15% word error reduction; (3) both VTLN and MLLR are more effective for the push-to-talk data than for the cross-talk data. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Christine Tuerk and Tony Robinson. </author> <title> A new frequency shift function for reducing inter-speaker variance. </title> <journal> EuroSpeech-93, </journal> <volume> 1 </volume> <pages> 351-354, </pages> <year> 1993. </year>
Reference-contexts: The speaker-dependent speech recognition system comes from the speaker-dependent speech signal. The reason that the speech signal is speaker-dependent is very complex. It is not only related to the physiological differences of speakers, but also related to the linguistic differences <ref> [1] </ref>. But it is generally agreed that one of the major source of inter-speaker variance is the vocal tract shape, especially the vocal tract length (VTL) [2, 3]. Therefore, some researchers have been devoted to the vocal tract length normalization (VTLN) for speaker normalization [1, 2, 3, 4, 5, 6]. <p> But it is generally agreed that one of the major source of inter-speaker variance is the vocal tract shape, especially the vocal tract length (VTL) [2, 3]. Therefore, some researchers have been devoted to the vocal tract length normalization (VTLN) for speaker normalization <ref> [1, 2, 3, 4, 5, 6] </ref>.
Reference: [2] <author> H. Wakita. </author> <title> Normalization of vowels by vocal-tract length and its application to vowel identification. </title> <journal> IEEE Trans. ASSP,, </journal> <volume> 25 </volume> <pages> 183-192, </pages> <year> 1977. </year>
Reference-contexts: It is not only related to the physiological differences of speakers, but also related to the linguistic differences [1]. But it is generally agreed that one of the major source of inter-speaker variance is the vocal tract shape, especially the vocal tract length (VTL) <ref> [2, 3] </ref>. Therefore, some researchers have been devoted to the vocal tract length normalization (VTLN) for speaker normalization [1, 2, 3, 4, 5, 6]. <p> But it is generally agreed that one of the major source of inter-speaker variance is the vocal tract shape, especially the vocal tract length (VTL) [2, 3]. Therefore, some researchers have been devoted to the vocal tract length normalization (VTLN) for speaker normalization <ref> [1, 2, 3, 4, 5, 6] </ref>. <p> Generally speaking, two issues are involved in VTLN: (1) Given the speech data from a speaker, how to obtain the warping factor for normalization; (2) Given a warping factor, how to do the normalization; The warping factors could be obtained via formant calculation as in <ref> [2, 3, 5] </ref>, or via line search as in [4, 6]. The normalization could be implemented in the Fourier spectrum domain as in [2, 5, 6], or in the Bark domain as in [3, 4]. <p> The normalization could be implemented in the Fourier spectrum domain as in <ref> [2, 5, 6] </ref>, or in the Bark domain as in [3, 4]. We used the VTLN based on Fourier spectrum warping in [7], and estimated the warping factor via formant calculation or line search.
Reference: [3] <author> Yoshio Ono, Hisashi Wakita, and Yunxin Zhao. </author> <title> Speaker normalization using constrained spectra shifts in auditory filter domain. </title> <journal> EuroSpeech-93, </journal> <volume> 1 </volume> <pages> 355-358, </pages> <year> 1993. </year>
Reference-contexts: It is not only related to the physiological differences of speakers, but also related to the linguistic differences [1]. But it is generally agreed that one of the major source of inter-speaker variance is the vocal tract shape, especially the vocal tract length (VTL) <ref> [2, 3] </ref>. Therefore, some researchers have been devoted to the vocal tract length normalization (VTLN) for speaker normalization [1, 2, 3, 4, 5, 6]. <p> But it is generally agreed that one of the major source of inter-speaker variance is the vocal tract shape, especially the vocal tract length (VTL) [2, 3]. Therefore, some researchers have been devoted to the vocal tract length normalization (VTLN) for speaker normalization <ref> [1, 2, 3, 4, 5, 6] </ref>. <p> Generally speaking, two issues are involved in VTLN: (1) Given the speech data from a speaker, how to obtain the warping factor for normalization; (2) Given a warping factor, how to do the normalization; The warping factors could be obtained via formant calculation as in <ref> [2, 3, 5] </ref>, or via line search as in [4, 6]. The normalization could be implemented in the Fourier spectrum domain as in [2, 5, 6], or in the Bark domain as in [3, 4]. <p> The normalization could be implemented in the Fourier spectrum domain as in [2, 5, 6], or in the Bark domain as in <ref> [3, 4] </ref>. We used the VTLN based on Fourier spectrum warping in [7], and estimated the warping factor via formant calculation or line search. We obtained up to 10% word error reduction with the line searching warping factor, and did not get any improvement with the formant method. <p> However, for a specific speaker, the Bark scale should be different in some extent due to the specific vocal tract length/shape. Our approach to do VTLN in Bark domain is not directly to adjust the filter bank space or to shift Bark coefficient as in <ref> [3, 4] </ref>. Instead, we find a specific Bark scale for each speaker, and use this speaker-specific Bark scale to compress the speaker's spectrum. The VTLN is implemented in the process of filter bank integration under the speaker-specific Bark scale. We refer this method as speaker-specific Bark scale warping. <p> The result is also consistent with what we obtained in [7], though the normalization method is different. The major advantage of the speaker-specific Bark scale VTLN is that it is very simple and the performance is also better. Compared to the VTLN in frequency domain and one-Bark-shift method in <ref> [3] </ref>, there is no warping rule need to be specified, no spectrum interpolation need to be handled, and no bandwidth mismatch problem.
Reference: [4] <author> Li Lee and Richard C. Rose. </author> <title> Speaker normalization using efficient frequency warping procedures. </title> <journal> ICASSP-96, </journal> <volume> 1 </volume> <pages> 353-356, </pages> <year> 1996. </year>
Reference-contexts: But it is generally agreed that one of the major source of inter-speaker variance is the vocal tract shape, especially the vocal tract length (VTL) [2, 3]. Therefore, some researchers have been devoted to the vocal tract length normalization (VTLN) for speaker normalization <ref> [1, 2, 3, 4, 5, 6] </ref>. <p> are involved in VTLN: (1) Given the speech data from a speaker, how to obtain the warping factor for normalization; (2) Given a warping factor, how to do the normalization; The warping factors could be obtained via formant calculation as in [2, 3, 5], or via line search as in <ref> [4, 6] </ref>. The normalization could be implemented in the Fourier spectrum domain as in [2, 5, 6], or in the Bark domain as in [3, 4]. We used the VTLN based on Fourier spectrum warping in [7], and estimated the warping factor via formant calculation or line search. <p> The normalization could be implemented in the Fourier spectrum domain as in [2, 5, 6], or in the Bark domain as in <ref> [3, 4] </ref>. We used the VTLN based on Fourier spectrum warping in [7], and estimated the warping factor via formant calculation or line search. We obtained up to 10% word error reduction with the line searching warping factor, and did not get any improvement with the formant method. <p> However, for a specific speaker, the Bark scale should be different in some extent due to the specific vocal tract length/shape. Our approach to do VTLN in Bark domain is not directly to adjust the filter bank space or to shift Bark coefficient as in <ref> [3, 4] </ref>. Instead, we find a specific Bark scale for each speaker, and use this speaker-specific Bark scale to compress the speaker's spectrum. The VTLN is implemented in the process of filter bank integration under the speaker-specific Bark scale. We refer this method as speaker-specific Bark scale warping.
Reference: [5] <author> Ellen Eide and Herbert Gish. </author> <title> A parametric approach to vocal tract length normalization. </title> <journal> ICASSP-96, </journal> <volume> 1 </volume> <pages> 346-348, </pages> <year> 1996. </year>
Reference-contexts: But it is generally agreed that one of the major source of inter-speaker variance is the vocal tract shape, especially the vocal tract length (VTL) [2, 3]. Therefore, some researchers have been devoted to the vocal tract length normalization (VTLN) for speaker normalization <ref> [1, 2, 3, 4, 5, 6] </ref>. <p> Generally speaking, two issues are involved in VTLN: (1) Given the speech data from a speaker, how to obtain the warping factor for normalization; (2) Given a warping factor, how to do the normalization; The warping factors could be obtained via formant calculation as in <ref> [2, 3, 5] </ref>, or via line search as in [4, 6]. The normalization could be implemented in the Fourier spectrum domain as in [2, 5, 6], or in the Bark domain as in [3, 4]. <p> The normalization could be implemented in the Fourier spectrum domain as in <ref> [2, 5, 6] </ref>, or in the Bark domain as in [3, 4]. We used the VTLN based on Fourier spectrum warping in [7], and estimated the warping factor via formant calculation or line search.
Reference: [6] <author> Steven Wegmann, Don McAllaster, Jeremy Orloff, and Barbara Peskin. </author> <title> Speaker normalization on conversational telephone speech. </title> <journal> ICASSP-96, </journal> <volume> 1 </volume> <pages> 339-341, </pages> <year> 1996. </year>
Reference-contexts: But it is generally agreed that one of the major source of inter-speaker variance is the vocal tract shape, especially the vocal tract length (VTL) [2, 3]. Therefore, some researchers have been devoted to the vocal tract length normalization (VTLN) for speaker normalization <ref> [1, 2, 3, 4, 5, 6] </ref>. <p> are involved in VTLN: (1) Given the speech data from a speaker, how to obtain the warping factor for normalization; (2) Given a warping factor, how to do the normalization; The warping factors could be obtained via formant calculation as in [2, 3, 5], or via line search as in <ref> [4, 6] </ref>. The normalization could be implemented in the Fourier spectrum domain as in [2, 5, 6], or in the Bark domain as in [3, 4]. We used the VTLN based on Fourier spectrum warping in [7], and estimated the warping factor via formant calculation or line search. <p> The normalization could be implemented in the Fourier spectrum domain as in <ref> [2, 5, 6] </ref>, or in the Bark domain as in [3, 4]. We used the VTLN based on Fourier spectrum warping in [7], and estimated the warping factor via formant calculation or line search.
Reference: [7] <author> Puming Zhan and Martin Westphal. </author> <title> Speaker normalization based on frequency warping. </title> <booktitle> Proceedings of ICASSP97, </booktitle> <address> Munich, Germany, </address> <year> 1997. </year>
Reference-contexts: The normalization could be implemented in the Fourier spectrum domain as in [2, 5, 6], or in the Bark domain as in [3, 4]. We used the VTLN based on Fourier spectrum warping in <ref> [7] </ref>, and estimated the warping factor via formant calculation or line search. We obtained up to 10% word error reduction with the line searching warping factor, and did not get any improvement with the formant method. <p> We refer this method as speaker-specific Bark scale warping. Figure 1 is the block diagram of the speaker-specific Bark scale front-end. Warping Factor ? Fourier Transform - CriticalBand Integration - x (t) Y ff (n) Compared to the Fourier spectrum warping as in <ref> [7] </ref>, the VTLN is implemented in the Bark domain by the speaker-specific Bark scale filter bank integration. <p> Thus for the normalization purpose, in general, most of the female's spectrum should get more compress towards the standard one, and vice visa for male's spectrum. The result is also consistent with what we obtained in <ref> [7] </ref>, though the normalization method is different. The major advantage of the speaker-specific Bark scale VTLN is that it is very simple and the performance is also better. <p> Compared to the VTLN in frequency domain and one-Bark-shift method in [3], there is no warping rule need to be specified, no spectrum interpolation need to be handled, and no bandwidth mismatch problem. We use the same training procedure as in <ref> [7] </ref> to train the VTLN sys tem. 3 COMPARISON OF VTLN AND MLLR Let z ff (t) be a N -dimension feature vector sequence which is derived from Y ff (n) in equation (3) (usually cepstrum of Y ff (n)), and be used to train the SI system. <p> The SSST database composed of 1/3 push-to-talk dialogs and 2/3 cross-talk dialogs. We use the same database as in <ref> [7] </ref>. Readers can find detail analysis of push-to-talk and cross-talk data in [12]. We use the push-to-talk and cross-talk dialogs together to train the acoustic models, but keep an individual test set for each of them.
Reference: [8] <author> Vassilios V. Digalakis, Dimitry Rtischev, and Leonardo G. Neumeyer. </author> <title> Speaker adaptation using constrained estimation of gaussian mixtures. </title> <journal> IEEE Trans. Speech and Audio Processing, </journal> <volume> 3 </volume> <pages> 357-365, </pages> <year> 1995. </year>
Reference-contexts: The MLLR adaptation linearly transforms a speaker-independent (SI) system towards a speaker-dependent system in the acoustic model space based on adaptation data <ref> [8, 9] </ref>. As we will show in this paper that the VTLN is equivalent to a nonlinear transformation of the speech signal in the feature space. Hence it is interesting to investigate the combination of the two methods in a speech recognition system. <p> In the MLLR adaptation, it is assumed that o ff (t) has linear relationship with z ff (t) as o ff (t) = A i z ff (t) + b i in <ref> [8] </ref>. Where A i is a N fi N matrix and b i is a N fi 1 vector in state i. <p> The adaptation algorithm is to estimate A i and b i to maximum P (o ff (t) j i). A i was assumed to be a di-agonal matrix to avoid the expansive computation in <ref> [8] </ref>. This assumption was eliminated by just linearly transform the mean vector in [9], i.e., replacing the single Gaussian density with N (o ff (t); A i ik + b i ; ik ) in equation (5).
Reference: [9] <author> C. J. Leggetter and P. C. Woodland. </author> <title> Maximum likelihood linear regression for speaker adaptation of continuous density hmms. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 9 </volume> <pages> 171-186, </pages> <year> 1995. </year>
Reference-contexts: The MLLR adaptation linearly transforms a speaker-independent (SI) system towards a speaker-dependent system in the acoustic model space based on adaptation data <ref> [8, 9] </ref>. As we will show in this paper that the VTLN is equivalent to a nonlinear transformation of the speech signal in the feature space. Hence it is interesting to investigate the combination of the two methods in a speech recognition system. <p> The adaptation algorithm is to estimate A i and b i to maximum P (o ff (t) j i). A i was assumed to be a di-agonal matrix to avoid the expansive computation in [8]. This assumption was eliminated by just linearly transform the mean vector in <ref> [9] </ref>, i.e., replacing the single Gaussian density with N (o ff (t); A i ik + b i ; ik ) in equation (5).
Reference: [10] <author> Hynek Hermansky. </author> <title> Perceptual linear predictive (plp) analysis of speech. </title> <journal> J. Acoust. Soc. Am. </journal> <volume> 87(4), </volume> <pages> pages 1738-1752, </pages> <year> 1990. </year>
Reference-contexts: The bandwidth of each T n (!), i.e., h n l n , depends on the Bark scale. 2.2 VTLN based on speaker-specific Bark scale We view the measured Bark scale presented in <ref> [10, 11] </ref> as the average scale which applies to all speakers. However, for a specific speaker, the Bark scale should be different in some extent due to the specific vocal tract length/shape. <p> We define the speaker-specific Bark scale as equation (4): B ff (!) = 6ln (!=(1200ff) + (!=(1200ff)) 2 + 1)) (4) Where ff is the speaker-specific parameter. If we let ff = 1:0 for all speakers, equation (4) becomes equation (3) in <ref> [10] </ref>. Figure 2 is the warping curves of the speaker-specific Bark scale. Three curves are presented in figure 2, which reflect the range of the warping factors obtained during training. <p> The test vocabulary consists of 4606 words. The out of vocabulary word rate is 2.35% for push-to-talk test set, and 0.89% for cross-talk test set. The language model is the class-based trigram language model. We use the same Perceptual Linear Predictive (PLP) cepstral coefficients as in <ref> [10] </ref>, except the bark scale is speaker-specific as equation (4). We calculate 21 filter bank coefficients and use them to derive 13 LPC-Driven cepstral coefficients. Then the cepstral coefficients and power are combined with their first and second derivative to generate a 42-dimensional feature vector.
Reference: [11] <author> E. Zwicker and E. Terhardt. </author> <title> Analytical expressions for critical-band rate and critical bandwidth as a function of frequency. </title> <journal> J. Acoust. Soc. Am. </journal> <volume> 68(5), </volume> <pages> pages 1523-1525, </pages> <year> 1980. </year>
Reference-contexts: The bandwidth of each T n (!), i.e., h n l n , depends on the Bark scale. 2.2 VTLN based on speaker-specific Bark scale We view the measured Bark scale presented in <ref> [10, 11] </ref> as the average scale which applies to all speakers. However, for a specific speaker, the Bark scale should be different in some extent due to the specific vocal tract length/shape.
Reference: [12] <author> Puming Zhan, Klaus Ries, Marsal Gavalda, Donna Gates, Alon Lavie, and Alex Waibe. </author> <title> Janus-ii: Towards spontaneous spanish speech recognition. </title> <booktitle> Proceedings of ICSLP-96, </booktitle> <year> 1996. </year>
Reference-contexts: The SSST database composed of 1/3 push-to-talk dialogs and 2/3 cross-talk dialogs. We use the same database as in [7]. Readers can find detail analysis of push-to-talk and cross-talk data in <ref> [12] </ref>. We use the push-to-talk and cross-talk dialogs together to train the acoustic models, but keep an individual test set for each of them. The push-to-talk test set consists of 86 utterances, the cross-talk test set consists of 117 utterances. The test vocabulary consists of 4606 words.
References-found: 12

