URL: ftp://thales.cs.umd.edu/pub/reports/plasc.ps
Refering-URL: ftp://thales.cs.umd.edu/pub/biographical/xsoft.html
Root-URL: 
Title: Parallel Linear Algebra in Statistical Computations  
Author: G. W. Stewart 
Note: 20742. This work was supported in part by the Air Force Office of Sponsored Research under grant AFOSR-82-0078.  
Address: Mary-land, College Park, MD  
Affiliation: Department of Computer Science and Institute for Advanced Computer Studies, University of  
Date: June 1988  
Pubnum: TR-2045  
Abstract:  
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. S. Almas. </author> <title> Overview of parallel computing. </title> <journal> Parallel Compting, </journal> <volume> 2 </volume> <pages> 191-203, </pages> <year> 1985. </year>
Reference-contexts: Just as there is no single general architecture for parallel computers, there is no general theory of parallel matrix algorithms. The same sequential algorithm will be programmed one way on one system and in a completely different way on another. Since the number of potential architectures is very large <ref> [1, 16] </ref>, I have chosen to restrict this paper to three, for which commercial systems are available. They are simd systems, shared-memory systems, and message-passing systems. We shall treat each of these in the next three sections. Given this paper's title, its focus on computer architectures requires explanation. <p> I chose them because they are the most extensively developed and have been tested in the field. But they are by no means the only possible architectures; nor have I described all possible variants. For more see <ref> [1, 17, 18, 26] </ref>. The statistician contemplating using parallel computers to solve a problem should be prepared to do research in parallel computations. The area is in a state of flux. Machines are proposed and become obsolete even before they are built.
Reference: [2] <author> M. Annoratone, E. Arnould, T. Gross, H. T. Kung, O. Menzicioglu, and J. A. Webb. </author> <title> The warp computer: architecture, implementation, and performance. </title> <journal> IEEE Transaction on Computers, </journal> <volume> C-36:1523-1538, </volume> <year> 1987. </year> <title> 12 Statistics and Parallel Computations </title>
Reference-contexts: Moreover, one cannot afford to build such a big system for a single application; the processors must also be programmable, which increases their complexity. The end result of these considerations is the warp computer, a linear systolic array of high-performance processors <ref> [2] </ref>. By all accounts it is effective, but it is neither simple nor cheap. On the other hand, general purpose simd machines have been built and run on a variety of problems. Their main advantage is that they can bring large numbers of simple processors to bear on single problems.
Reference: [3] <author> J. L. Barlow and I. C. F. Ipsen. </author> <title> Scaled Givens rotations for the solution of linear least squares problems on systolic arrays. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 8 </volume> <pages> 716-734, </pages> <year> 1987. </year>
Reference-contexts: Certainly it is not easy to recognize equation (1) in the above program. Nonetheless, there is a certain satisfaction in designing systolic algorithms for matrix computations to judge from the number that have been published (e.g., see <ref> [3, 5, 21, 29, 30] </ref>). When systolic arrays were first proposed, it was hoped that they would provided inexpensive, special-purpose processing for a variety of applications. Things have not worked out this way. The array above is a toy that solves a 4 fi 4 problem.
Reference: [4] <author> C. Bischof and C. Van Loan. </author> <title> The W Y representation for products of Householder transformations. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 8:s2-s13, </volume> <year> 1987. </year>
Reference-contexts: For more, see <ref> [7, 4] </ref>. 4. Message-Passing Systems A message-passing system is one in which processors communicate by passing packets of data between themselves. The medium may be something as simple as an ethernet, or a slotted ring [27], in which case any processor can talk to any other processor.
Reference: [5] <author> A. Bojanczyk and R. P. Brent. </author> <title> Parallel solution of certain toeplitz least-squares problems. </title> <journal> Linear Algebra and Its Applications, </journal> <volume> 77 </volume> <pages> 43-60, </pages> <year> 1986. </year>
Reference-contexts: Certainly it is not easy to recognize equation (1) in the above program. Nonetheless, there is a certain satisfaction in designing systolic algorithms for matrix computations to judge from the number that have been published (e.g., see <ref> [3, 5, 21, 29, 30] </ref>). When systolic arrays were first proposed, it was hoped that they would provided inexpensive, special-purpose processing for a variety of applications. Things have not worked out this way. The array above is a toy that solves a 4 fi 4 problem.
Reference: [6] <author> A. Brass and G. S. Pawley. </author> <title> Two and three dimensional ffts on highly parallel compters. </title> <journal> Parallel Computing, </journal> <volume> 3 </volume> <pages> 167-184, </pages> <year> 1986. </year>
Reference-contexts: They are very effective with simple algorithms that proceed in short, repetitive bursts of computations. Their main disadvantage is their inflexibility. They are tedious to code, even for highly structured problems like computing the fft <ref> [6] </ref>, and complicated, irregular computations are beyond them. 3. Shared-Memory Systems We now turn to systems of general purpose processors that run asynchronously, the mimd (multiple processor, multiple data) systems. These systems are commonly divided into two categories: shared-memory systems and message-passing systems.
Reference: [7] <author> Jack J. Dongarra, Jeremy Du Croz, , Iain Duff, and Sven Hammarling. </author> <title> A proposal for a set of level 3 basic linear algebra subprograms. </title> <journal> SIGNUM Newsletter, </journal> <volume> 22 </volume> <pages> 2-14, </pages> <year> 1987. </year>
Reference-contexts: For more, see <ref> [7, 4] </ref>. 4. Message-Passing Systems A message-passing system is one in which processors communicate by passing packets of data between themselves. The medium may be something as simple as an ethernet, or a slotted ring [27], in which case any processor can talk to any other processor.
Reference: [8] <author> M. Flynn. </author> <title> Very high speed computing. </title> <booktitle> Proceedings of the IEEE, </booktitle> <pages> 1901-1909, </pages> <year> 1966. </year>
Reference-contexts: Hence the change in emphasis. fl This work was supported in part by the Air Force Office of Sponsored Research under grant AFOSR-82-0078. 2 Statistics and Parallel Computations 2. SIMD Systems and Systolic Arrays A single-instruction, multiple-data (simd <ref> [8] </ref>) system is a group of (usually simple) processors that execute the same sequence of instructions in lockstep under a global control. This results in a nontrivial computation because the instructions are executed with different data, which can pass from processor to processor to be combined with other data.
Reference: [9] <author> G. C. Fox, S. W. Otto, and A. J. G. Hey. </author> <title> Matrix algorithms on a hypercube I: matrix multiplication. </title> <journal> Parallel Computing, </journal> <volume> 4 </volume> <pages> 17-31, </pages> <year> 1987. </year>
Reference-contexts: Since we necessarily have P k, for n k, we have a communication time of T comm ~ = n ( + k t ): Unfortunately, most message-passing systems are unbalanced in that the startup time is large compared to the inverse transmission rate (e.g., see <ref> [9] </ref>). Typically = 10 3 while t can be as small as 10 6 . In this case if k = 100 (say), the transmission time is insignificant compared to the communication time and T comm ~ = n; independent of the number of processors.
Reference: [10] <author> W. M. Gentleman. </author> <title> Using the Harmony Operating System. </title> <type> Technical Report ERB-966, </type> <institution> Natonal Research Council of Canada, </institution> <year> 1985. </year>
Reference-contexts: The variables north, south, east, and west refer to the processors to the north, south, east, and west. The send-request protocol by which data is passed is typical of message passing systems (e.g., see <ref> [10, 25] </ref>). There is no direct link between the program that sends the message and the program that receives it. Instead the sending program calls a routine to send the message, the receiving program calls a routine to request the message, and the operating system does the rest.
Reference: [11] <author> A. George, M. T. Heath, J. Liu, and E. NG. </author> <title> Sparse cholesky factorization on a local-memory multiprocessor. </title> <journal> SIAM Journal on Scientific and Statiscal Computing, </journal> <volume> 9 </volume> <pages> 327-340, </pages> <year> 1988. </year>
Reference-contexts: In more complicated algorithms, say Gaussian elimination with partial pivoting, it is possible to inadvertently arrange the computations so that Statistics and Parallel Computations 11 waves do not form. (Unfortunately, eigenvalue algorithms seem to resist the formation of waves.) For examples of well considered implementation see <ref> [12, 11, 14] </ref> The second is to run large jobs. In many applications, the ratio of arithmetic to communication grows with the size of the problem, so that ultimately communication costs are insignificant. For a highly publicized example of this phenomenon see [15]. The second is to reduce .
Reference: [12] <author> Alan George and Eleanor Chu. </author> <title> Gaussian Elimination with Partial Pivoting and Load Balancing on a Multiprocessor. </title> <type> Technical Report ORNL/TM-10323, </type> <institution> Mathematical Sciences Section, Oak Ridge National Laboratory, </institution> <year> 1987. </year>
Reference-contexts: In more complicated algorithms, say Gaussian elimination with partial pivoting, it is possible to inadvertently arrange the computations so that Statistics and Parallel Computations 11 waves do not form. (Unfortunately, eigenvalue algorithms seem to resist the formation of waves.) For examples of well considered implementation see <ref> [12, 11, 14] </ref> The second is to run large jobs. In many applications, the ratio of arithmetic to communication grows with the size of the problem, so that ultimately communication costs are insignificant. For a highly publicized example of this phenomenon see [15]. The second is to reduce .
Reference: [13] <author> Carlo Ghezzi. </author> <title> Concurrency in programming languages: a survey. </title> <journal> Parallel Computing, </journal> <volume> 2 </volume> <pages> 224-241, </pages> <year> 1985. </year>
Reference-contexts: However, people who write operating systems have long ago developed techniques for doing this [18, Ch. 8]. Actually, the applications programmer may never see this kind of synchronization. Shared-memory systems lend themselves to elegant extensions of existing programming languages that make the implementation of parallel matrix algorithms easy <ref> [13] </ref>.
Reference: [14] <author> L. Guangye and T. F. Coleman. </author> <title> A parallel triangular solver for a distributed memory multiprocessor. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 9 </volume> <pages> 485-502, </pages> <year> 1988. </year>
Reference-contexts: In more complicated algorithms, say Gaussian elimination with partial pivoting, it is possible to inadvertently arrange the computations so that Statistics and Parallel Computations 11 waves do not form. (Unfortunately, eigenvalue algorithms seem to resist the formation of waves.) For examples of well considered implementation see <ref> [12, 11, 14] </ref> The second is to run large jobs. In many applications, the ratio of arithmetic to communication grows with the size of the problem, so that ultimately communication costs are insignificant. For a highly publicized example of this phenomenon see [15]. The second is to reduce .
Reference: [15] <author> J. L. Gustafson, G. R. Montry, and R. E. Benner. </author> <title> Development of parallel methods for a 1024-processor hypercube. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 9:, </volume> <year> 1988. </year> <note> To appear in July. Statistics and Parallel Computations 13 </note>
Reference-contexts: In many applications, the ratio of arithmetic to communication grows with the size of the problem, so that ultimately communication costs are insignificant. For a highly publicized example of this phenomenon see <ref> [15] </ref>. The second is to reduce . Unfortunately, this is not easily done, since the send-receive protocol requires costly calls to the operating system. At the University of Maryland, we are experimenting with a hybrid architecture in which neighboring processors can access one-another's memory [23].
Reference: [16] <author> R. W. Hockney. </author> <title> Mimd computing in the usa|1984. </title> <journal> Parallel Computing, </journal> <volume> 3 </volume> <pages> 119-136, </pages> <year> 1985. </year>
Reference-contexts: Just as there is no single general architecture for parallel computers, there is no general theory of parallel matrix algorithms. The same sequential algorithm will be programmed one way on one system and in a completely different way on another. Since the number of potential architectures is very large <ref> [1, 16] </ref>, I have chosen to restrict this paper to three, for which commercial systems are available. They are simd systems, shared-memory systems, and message-passing systems. We shall treat each of these in the next three sections. Given this paper's title, its focus on computer architectures requires explanation.
Reference: [17] <author> R. W. Hockney and C. R. Jesshope. </author> <title> Parallel Computers. </title> <publisher> Adam Hilger Ltd., </publisher> <address> Bristol, </address> <year> 1981. </year>
Reference-contexts: I chose them because they are the most extensively developed and have been tested in the field. But they are by no means the only possible architectures; nor have I described all possible variants. For more see <ref> [1, 17, 18, 26] </ref>. The statistician contemplating using parallel computers to solve a problem should be prepared to do research in parallel computations. The area is in a state of flux. Machines are proposed and become obsolete even before they are built.
Reference: [18] <author> K. Hwang and F. A. Briggs. </author> <title> Computer Architecture and Parallel Processing. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1984. </year>
Reference-contexts: In practice the processors would be doing other useful work, only spinning on xready when there is nothing else to do. Moreover, we have ignored the tricky question of how to initialize xready. However, people who write operating systems have long ago developed techniques for doing this <ref> [18, Ch. 8] </ref>. Actually, the applications programmer may never see this kind of synchronization. Shared-memory systems lend themselves to elegant extensions of existing programming languages that make the implementation of parallel matrix algorithms easy [13]. <p> I chose them because they are the most extensively developed and have been tested in the field. But they are by no means the only possible architectures; nor have I described all possible variants. For more see <ref> [1, 17, 18, 26] </ref>. The statistician contemplating using parallel computers to solve a problem should be prepared to do research in parallel computations. The area is in a state of flux. Machines are proposed and become obsolete even before they are built.
Reference: [19] <author> R. Karp and R. Miller. </author> <title> Properties of a model for parallel computation: determinacy, termination, queuing. </title> <journal> SIAM Journal on Applied Mathematics, </journal> <volume> 14 </volume> <pages> 1390-1411, </pages> <year> 1966. </year>
Reference-contexts: The send-request protocol described above is sufficient to keep each processor performing the right operations in the right order, no matter how fast or slowly the individual processors run <ref> [19, 24] </ref>. At the present time, message-passing systems offer the greatest hope for connecting very large numbers of general purpose processors in an MIMD system. Unfortunately, communication delays can easily cancel out arithmetic parallelism. To see how this comes about, let us analyze the communication costs for the above algorithm.
Reference: [20] <author> H. T. Kung. </author> <title> Why systolic architectures? IEEE Computer, </title> <booktitle> 15 </booktitle> <pages> 37-46, </pages> <year> 1982. </year>
Reference-contexts: Originally a systolic array meant an array of special processors, acting in lockstep, through which data was pumped like blood through the heart <ref> [20] </ref>. Since the processors are not conceived to be programmable but do have the ability to perform different functions, systolic arrays are not, strictly speaking, simd systems. But neither system exists in a pure form, and the distinction has become blurred.
Reference: [21] <author> F. T. Luk. </author> <title> A triangular processor array for computing singular values. </title> <journal> Linear Algebra and Its Applications, </journal> <volume> 77 </volume> <pages> 259-274, </pages> <year> 1986. </year>
Reference-contexts: Certainly it is not easy to recognize equation (1) in the above program. Nonetheless, there is a certain satisfaction in designing systolic algorithms for matrix computations to judge from the number that have been published (e.g., see <ref> [3, 5, 21, 29, 30] </ref>). When systolic arrays were first proposed, it was hoped that they would provided inexpensive, special-purpose processing for a variety of applications. Things have not worked out this way. The array above is a toy that solves a 4 fi 4 problem.
Reference: [22] <author> W. Oed and O. Lange. </author> <title> Modelling, measurement, and simulation of memory interference in the cray xm-p. </title> <journal> Parallel Computing, </journal> <volume> 3 </volume> <pages> 343-358, </pages> <year> 1986. </year>
Reference-contexts: The formation of the cross-product matrix is an example, since the processes spanned by the parfor are all contending for the same row of X. It is very difficult to model contention in shared-memory systems, since the processors run asynchronously, but it is a serious problem <ref> [22, 31] </ref>. Moreover, as the number of processors grows, it becomes impractical to connect every processor to every bank; the interface in the diagram above becomes a switching network. This has three consequences. First, the complexity, and hence the cost of the network increases with the number of processors.
Reference: [23] <author> D. P. O'Leary, Roger Pierson, G. W. Stewart, and Mark Wieser. </author> <title> The Maryland crab: A Module for Building Parallel Computers. </title> <type> Technical Report TR-1660, </type> <institution> Department of Computer Science, University of Maryland, </institution> <year> 1986. </year>
Reference-contexts: The second is to reduce . Unfortunately, this is not easily done, since the send-receive protocol requires costly calls to the operating system. At the University of Maryland, we are experimenting with a hybrid architecture in which neighboring processors can access one-another's memory <ref> [23] </ref>. The idea is to give local communication the efficiency of shared-variable systems while preserving the limited number of connections of message-passing systems. But this work is in an early phase. 5. Summary We have just completed a survey of three important classes of architectures.
Reference: [24] <author> D. P. O'Leary and G. W. Stewart. </author> <title> From determinacy to systaltic arrays. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36:1355-1359, </volume> <year> 1987. </year>
Reference-contexts: The send-request protocol described above is sufficient to keep each processor performing the right operations in the right order, no matter how fast or slowly the individual processors run <ref> [19, 24] </ref>. At the present time, message-passing systems offer the greatest hope for connecting very large numbers of general purpose processors in an MIMD system. Unfortunately, communication delays can easily cancel out arithmetic parallelism. To see how this comes about, let us analyze the communication costs for the above algorithm.
Reference: [25] <author> D. P. O'Leary, G. W. Stewart, and Robert van de Geijn. </author> <title> domino: A Message Passing Environment for Parallel Computation. </title> <institution> TR-1648, University of Maryland, Computer Science, </institution> <year> 1986. </year>
Reference-contexts: The variables north, south, east, and west refer to the processors to the north, south, east, and west. The send-request protocol by which data is passed is typical of message passing systems (e.g., see <ref> [10, 25] </ref>). There is no direct link between the program that sends the message and the program that receives it. Instead the sending program calls a routine to send the message, the receiving program calls a routine to request the message, and the operating system does the rest.
Reference: [26] <author> J. M. Ortega. </author> <title> Introduction to Parallel and Vector Computing. </title> <publisher> Plenum Press, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: I chose them because they are the most extensively developed and have been tested in the field. But they are by no means the only possible architectures; nor have I described all possible variants. For more see <ref> [1, 17, 18, 26] </ref>. The statistician contemplating using parallel computers to solve a problem should be prepared to do research in parallel computations. The area is in a state of flux. Machines are proposed and become obsolete even before they are built.
Reference: [27] <author> C. Rieger. </author> <title> Zmob: hardware from a user's viewpoint. </title> <booktitle> In Proceedings of the IEEE Computer Society Conference on Pettern Recognition and Image Processing, </booktitle> <pages> pages 399-408, </pages> <year> 1981. </year>
Reference-contexts: For more, see [7, 4]. 4. Message-Passing Systems A message-passing system is one in which processors communicate by passing packets of data between themselves. The medium may be something as simple as an ethernet, or a slotted ring <ref> [27] </ref>, in which case any processor can talk to any other processor. For economy's sake, however, processors are usually connected to one another in a fixed pattern or geometry. In such a system a processor can talk only to its neighbors.
Reference: [28] <author> C. Seitz. </author> <title> The cosmic cube. </title> <journal> Communications of the Association for Computing Machinery, </journal> <volume> 28 </volume> <pages> 22-33, </pages> <year> 1985. </year>
Reference-contexts: Thus systems with small diameter are at a premium. Currently the most popular geometry is the hypercube <ref> [28] </ref>, which is commercially available in the i-psc and the ncube. Geometrically, an n-dimensional hypercube is a system of 2 n processors connected in an n-dimensional cube. Figure 3 shows a 1-cube, a 2-cube, and a 3-cube.
Reference: [29] <author> G. W. Stewart. </author> <title> A Jacobi-like algorithm for computing the Schur decomposition of a non-Hermitian matrix. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 28 </volume> <pages> 853-864, </pages> <year> 1985. </year> <title> 14 Statistics and Parallel Computations </title>
Reference-contexts: Certainly it is not easy to recognize equation (1) in the above program. Nonetheless, there is a certain satisfaction in designing systolic algorithms for matrix computations to judge from the number that have been published (e.g., see <ref> [3, 5, 21, 29, 30] </ref>). When systolic arrays were first proposed, it was hoped that they would provided inexpensive, special-purpose processing for a variety of applications. Things have not worked out this way. The array above is a toy that solves a 4 fi 4 problem.
Reference: [30] <author> Robert A. van de Geijn. </author> <title> Implementing the QR-Algorithm on an Array of Processors. </title> <type> Technical Report 1897, </type> <institution> Department of Computer Science, University of Maryland, </institution> <year> 1987. </year>
Reference-contexts: Certainly it is not easy to recognize equation (1) in the above program. Nonetheless, there is a certain satisfaction in designing systolic algorithms for matrix computations to judge from the number that have been published (e.g., see <ref> [3, 5, 21, 29, 30] </ref>). When systolic arrays were first proposed, it was hoped that they would provided inexpensive, special-purpose processing for a variety of applications. Things have not worked out this way. The array above is a toy that solves a 4 fi 4 problem.
Reference: [31] <author> Pen-Chung Yew, Niau-Feng Tzeng, and D. H. Lawrie. </author> <title> Distributing hot-spot addressing in large scale multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36:338-395, </volume> <year> 1987. </year>
Reference-contexts: The formation of the cross-product matrix is an example, since the processes spanned by the parfor are all contending for the same row of X. It is very difficult to model contention in shared-memory systems, since the processors run asynchronously, but it is a serious problem <ref> [22, 31] </ref>. Moreover, as the number of processors grows, it becomes impractical to connect every processor to every bank; the interface in the diagram above becomes a switching network. This has three consequences. First, the complexity, and hence the cost of the network increases with the number of processors.
References-found: 31

