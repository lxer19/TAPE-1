URL: http://www.cs.gatech.edu/people/home/rosu/papers-uncompressed/hpdc97.ps
Refering-URL: http://www.cs.gatech.edu/people/home/rosu/cv/cv.html
Root-URL: 
Email: fujimotog@cc.gatech.edu  
Title: Supporting Parallel Applications on Clusters of Workstations: The Intelligent Network Interface Approach  
Author: Marcel-Catalin Rosu, Karsten Schwan, and Richard Fujimoto frosu, schwan, 
Note: Partially funded by DARPA grant DABT63-95-C-0125 and by NSF Grants CDA-9501637, CDA-9422033, ECS-9411846, and MIP-94085550  
Address: Atlanta, GA 30332-0280  
Affiliation: Georgia Institute of Technology, College of Computing  
Abstract: This paper presents a novel networking architecture designed for communication intensive parallel applications running on clusters of workstations (COWs) connected by high speed networks. This architecture permits (1) the transfer of selected communication-related functionality from the host machine to the network interface coprocessor, and (2) the exposure of this functionality directly to applications as instructions of a Virtual Communication Machine (VCM) implemented by the coprocessor. The user-level code interacts directly with the network coprocessor as the host kernel only 'connects' the application to the VCM and does not participate in the data transfers. The distinctive feature of our design is its flexibility: the integration of the network with the applicationcan be varied to maximize performance. The resulting communication architecture is characterized by a very low overhead on the host processor, by latency and bandwidth close to the hardware limits, and by an application interface which enables zero-copy messaging and eases the port of some shared-memory parallel applications to COWs. The architecture admits low cost implementations based only on off-the-shelf hardware components. Additionally, its current ATM-based implementation can be used to communicate with any ATM-enabled host. We use three applications to demonstrate the high performance of the architecture's current implementation: (1) a synthetic client/server application, (2) a parallel implementation of the Traveling Salesman Problem, and (3) a parallel engine for discrete event simulation (GTW). The distributed-and shared-memory versions of these applications have comparable performance. Furthermore, the VCM-based approach to creating a clustered, parallel machine is shown to scale well in terms of matching required with offered communication bandwidths. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. Agusleo and N. Soparkar. </author> <title> Employing Logic-Enhanced Memory for High-Performance ATM Network Interfaces. </title> <booktitle> The 5th IEEE Symposium on High Performance Distributed Computing, </booktitle> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: We believe this drawback is offset by the decrease in host overhead. Transferring specialized functionality to the communication coprocessor has been investigated for message-passing parallel machines. For instance, [21] investigates the benefits of running Active Messages on the coprocessor. <ref> [1, 6, 20] </ref> propose new hardware architectures for network interfaces. Their primary objective is to eliminate the I/O bus bottleneck by integrating the network interface with the memory system. We believe that VCM-like abstractions will prove useful for such hardware architectures. 7.
Reference: [2] <author> A. Basu, M. Welsh, and T. von Eicken. </author> <title> Incorporating Memory Management into User-Level Network Interfaces. </title> <address> www2.cs.cornell.edu/U-net/, Dec. </address> <year> 1996. </year>
Reference-contexts: Currently, we use FORE SBA-200E cards in a cluster of Sun UltraSPARCs I. Furthermore, since we use standard ATM interconnect technology with standard AAL5 packets, clusters can be extended across any ATM-based metropolitan area network. We consider that the intelligent network interface approach distinguishes itself from similar proposals <ref> [2, 3, 8, 9, 17, 25] </ref> for several reasons: * The network interface has an active role. <p> Program or instruction completion is signaled using status words also placed in the command area. For each instruction, the VCM checks that an application refers only to the buffers it owns, and uses only its own connections. In contrast to <ref> [2, 9] </ref>, the buffer addresses passed as instruction parameters are references into the VCM address space (i.e., address translation is performed on the host). While less general than a firmware-based TLB, this scheme keeps the overhead on the network coprocessor low. <p> U-Net [25] maps these message queues into the application's address space. Messages are copied between the application and message queue buffers at both endpoints. U-Net/MM <ref> [2] </ref> adds zero-copy messaging to U-Net by incorporating a TLB-like data structure into the firmware. A similar software TLB is used to support VMMC on Myrinet [9]. In contrast to U-Net, the Illinois Fast Messages architecture [17] places the shared queues for outgoing data in the adapter memory. <p> In comparison to [3, 8, 25], in our architecture the message buffers are not allocated by the system; they are part of the application's original address space. This facilitates porting certain shared-memory parallel application to COWs, as shown for GTW. Additionally, different from <ref> [2, 9] </ref>, we do not involve the coprocessor in the buffer management. This characteristic enables our architecture to comply with the operating system abstractions without replicating certain system information, like process priorities, on the adapter. <p> Such replication would be too expensive for both the host and the adapter given the highly dynamic nature of this information in a system with a large number of communication intensive applications (as in <ref> [2] </ref>). Solutions such as prepinning a number of buffer pages per connection [2] may waste host memory and can result in resource allocation contradicting applications' relative priorities. <p> Such replication would be too expensive for both the host and the adapter given the highly dynamic nature of this information in a system with a large number of communication intensive applications (as in <ref> [2] </ref>). Solutions such as prepinning a number of buffer pages per connection [2] may waste host memory and can result in resource allocation contradicting applications' relative priorities. However, in our architecture, the coprocessor performs all the data transfers which is different from the approach in [17, 25] and results in higher latency.
Reference: [3] <author> G. Buzzard, D. Jacobson, M. Mackey, S. Marovich, and J. Wilkes. </author> <title> An implementation of the Hamlyn sender-managed interface architecture. </title> <booktitle> 2nd Symposium on Operating Systems Design and Implementations, </booktitle> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: Currently, we use FORE SBA-200E cards in a cluster of Sun UltraSPARCs I. Furthermore, since we use standard ATM interconnect technology with standard AAL5 packets, clusters can be extended across any ATM-based metropolitan area network. We consider that the intelligent network interface approach distinguishes itself from similar proposals <ref> [2, 3, 8, 9, 17, 25] </ref> for several reasons: * The network interface has an active role. <p> The main interest is in building faster, lighter, and cheaper communication mechanisms for COWs <ref> [3, 9, 11, 17, 25] </ref>. Most of these projects use off-the-shelf components while a few use dedicated hardware [11]. Early user-level communication architectures propose some form of integration between application and network interface, either as message areas [27] or ADCs [8]. <p> A similar software TLB is used to support VMMC on Myrinet [9]. In contrast to U-Net, the Illinois Fast Messages architecture [17] places the shared queues for outgoing data in the adapter memory. The CPU overhead increases, but the coprocessor overhead decreases. In comparison to <ref> [3, 8, 25] </ref>, in our architecture the message buffers are not allocated by the system; they are part of the application's original address space. This facilitates porting certain shared-memory parallel application to COWs, as shown for GTW.
Reference: [4] <author> C. Carothers, R. Fujimoto, Y.-B. Lin, and P. </author> <title> England. Distributed simulations of large-scale pcs networks. </title> <booktitle> The 1994 MASCOTS Conference, </booktitle> <month> Jan. </month> <year> 1994. </year>
Reference-contexts: The results are plotted as Cluster-slowed and they show that a cluster based on our networking architecture and using processors 50% faster than those in the KSR-2 will have good performance even when running this communication-intensive simulation. PCS a wireless network simulation PCS <ref> [4] </ref> is a simulation of a wireless communication network based on a set of radio transmitters structured as a square grid (one transmitter per grid sector).
Reference: [5] <author> S. Das, R. Fujimoto, K. Panesar, D. Allison, and M. Hy-binette. GTW: </author> <title> A Time Warp System for Shared Memory Multiprocessors. </title> <booktitle> Winter Simulation Conference, </booktitle> <month> Dec. </month> <year> 1994. </year>
Reference-contexts: The loop interactions have an important contribution to reducing this overhead. The first large shared-memory application ported on our networking architecture is the Georgia Tech Time Warp (GTW) system <ref> [5] </ref>, a parallel kernel for discrete event simulation used in both research and commercial environments. GTW is designed to support efficient execution of small granularity discrete-event simulation applications that require as little as a few hundred machine instructions for processing an event.
Reference: [6] <author> A. Davis, M. Swanson, and M. Parker. </author> <title> Efficient communication mechanisms for cluster based parallel computing. </title> <address> www.cs.utah.edu/projects/avalanche/index.html, Dec. </address> <year> 1996. </year>
Reference-contexts: We believe this drawback is offset by the decrease in host overhead. Transferring specialized functionality to the communication coprocessor has been investigated for message-passing parallel machines. For instance, [21] investigates the benefits of running Active Messages on the coprocessor. <ref> [1, 6, 20] </ref> propose new hardware architectures for network interfaces. Their primary objective is to eliminate the I/O bus bottleneck by integrating the network interface with the memory system. We believe that VCM-like abstractions will prove useful for such hardware architectures. 7.
Reference: [7] <author> P. Druschel and G. Banga. </author> <title> Lazy Receiver Processing (LRP): </title>
Reference-contexts: Figure 9 shows the performance of the shared-memory and cluster versions: (1) faster CPUs translate in much better performance and (2) our network enables better speedup than the KSR-2 interconnect. 6. Related work Over the past several years, research efforts in operating systems <ref> [7, 8, 23] </ref> and network adapter architecture [10, 22, 24] have focused on improving the communication performance over high-speed interconnects like HIPPI, FDDI, ATM, and Myrinet. The main interest is in building faster, lighter, and cheaper communication mechanisms for COWs [3, 9, 11, 17, 25].
References-found: 7

