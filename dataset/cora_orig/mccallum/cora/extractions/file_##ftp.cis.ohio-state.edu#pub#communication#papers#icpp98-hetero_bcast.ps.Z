URL: file://ftp.cis.ohio-state.edu/pub/communication/papers/icpp98-hetero_bcast.ps.Z
Refering-URL: http://www.cis.ohio-state.edu/~panda/paper.html
Root-URL: 
Email: Email: fbanikaze,moorthy,pandag@cis.ohio-state.edu  
Title: Efficient Collective Communication on Heterogeneous Networks of Workstations  
Author: Mohammad Banikazemi Vijay Moorthy Dhabaleswar K. Panda 
Address: Columbus, OH 43210  
Affiliation: Department of Computer and Information Science The Ohio State University  
Abstract: Networks of Workstations (NOW) have become an attractive alternative platform for high performance computing. Due to the commodity nature of workstations and interconnects and due to the multiplicity of vendors and platforms, the NOW environments are being gradually redefined as Heterogeneous Networks of Workstations (HNOW) environments. This paper presents a new framework for implementing collective communication operations (as defined by the Message Passing Interface (MPI) standard) efficiently for the emerging HNOW environments. We first classify different types of heterogeneity in HNOW and then focus on one important characteristic: communication capabilities of workstations. Taking this characteristic into account, we propose two new approaches (Speed-Partitioned Ordered Chain (SPOC) and Fastest-Node First (FNF)) to implement collective communication operations with reduced latency. We also investigate methods for deriving optimal trees for broadcast and multicast operations. Generating such trees is shown to be computationally intensive. It is shown that the FNF approach, in spite of its simplicity, can deliver performance within 1% of the performance of the optimal trees. Finally, these new approaches are compared with the approach used in the MPICH implementation on experimental as well as on simulated testbeds. On a 24-node existing HNOW environment with SGI workstations and ATM interconnection, our approaches reduce the latency of broadcast and multicast operations by a factor of up to 3:5 compared to the approach used in the existing MPICH implementation. On a 64-node simulated testbed, our approaches can reduce the latency of broadcast and multicast operations by a factor of up to 4:5. Thus, these results demonstrate that there is significant potential for our approaches to be applied towards designing scalable collective communication libraries for current and future generation HNOW environments. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. Anderson, D. Culler, and D. Patterson. </author> <title> A Case for Networks of Workstations (NOW). </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 5464, </pages> <month> Feb </month> <year> 1995. </year>
Reference-contexts: 1 Introduction Networks of Workstations (NOW) are becoming increasingly popular for providing cost-effective and affordable parallel computing for day-to-day computational needs <ref> [1] </ref>. Such environments consist of clusters of workstations connected by Local Area Networks (LANs). Hardware and software LAN technology was not initially developed for parallel processing, and thus the communication overhead between workstations can be quite high.
Reference: [2] <author> M. Banikazemi, V. Moorthy, and D. K. Panda. </author> <title> Efficient Collective Communication on Heterogeneous Networks of Work stations. </title> <type> Technical report OSU-CISRC-03/98-TR07, </type> <institution> Dept. of Computer and Information Science, The Ohio State University, </institution> <month> March </month> <year> 1998. </year>
Reference-contexts: significantly in a HNOW system by considering only one factor communication capabilities of the nodes. 2.1 Major Characteristics A typical HNOW system can be characterized by the following four factors: 1) Communication Capabilities of Workstations (Nodes), 2) Network Architectures, 3) Communication Protocols, and 4) Dedicated Support for Communication and Synchronization <ref> [2] </ref>. These factors are orthogonal to each other. A typical HNOW environment can have one or more of these characteristics. All of the above factors have significant impact on the implementation of collective communication operations on HNOW systems. <p> In the following section, we propose a near-optimal algorithm which runs in polynomial time. Before describing this near-optimal algorithm, let us look at two impor tant properties of optimal trees presented as the following two lemmas (the proofs can be found in <ref> [2] </ref>). Lemma 1 Let W 0 be the source node of a broadcast (or mul-ticast) operation and fW 1 ; W 2 ; ; W N1 g be the set of other participating nodes in the order of the time they have received the message. <p> Due to the space limitations, and since broadcast is a special case of multicast, we only present the multicast results. The simulation results for broadcast can be found in <ref> [2] </ref>. 7.2.2 Multicast Figures 7, 8, and 9 show the impact of speed factor, percentage of faster nodes, and message length on the latency of single multicast with varying sizes of destination sets.
Reference: [3] <author> J. Bruck et al. </author> <title> Efficient Message Passing Interface (MPI) for Parallel Computing on Clusters of Workstations. </title> <journal> JPDC, </journal> <month> pages </month> <year> 1934, </year> <month> Jan </month> <year> 1997. </year>
Reference-contexts: A portable parallel programming environment is key to the success of the NOW/HNOW paradigm. Over the last few years, researchers have developed software packages like PVM [14] and Message Passing Interface standards like MPI <ref> [3, 10] </ref> to provide such portability. Even though these softwares and standards do not force an application developer to understand the intricate details of the hardware, software, and network characteristics, the performance of an application in a NOW/HNOW environment heavily depends on these characteristics.
Reference: [4] <author> W. Gropp, E. Lusk, N. Doss, and A. Skjellum. </author> <title> A High Performance, Portable Implementation of the MPI, Message Passing Interface Standard. </title> <type> Technical report, </type> <institution> Argonne National Laboratory and Mississippi State University. </institution>
Reference-contexts: We measured round trip latency between four different pairs of workstations in a heterogeneous environment. The workstations were connected via Ethernet and used MPICH communication library <ref> [4] </ref> to communicate. Table 1 shows these results. Since the results are symmetric, values are shown for only the upper triangle entries. These values indicate how processor speed affects the time taken to transmit a message from one workstation to another.
Reference: [5] <author> R. Kesavan, K. Bondalapati, and D. K. Panda. </author> <title> Multicast on Irregular Switch-based Networks with Wormhole Routing. </title> <booktitle> In HPCA-3, </booktitle> <pages> pages 4857, </pages> <month> February </month> <year> 1997. </year>
Reference-contexts: Major factors in HNOW systems have been 1 Connectivity is defined as the fraction of ports in a switch which are used for interconnection with other switches <ref> [5] </ref>. single multicast latency with speed factor 4: (a) 10% fast nodes, (b) 25% fast nodes, (c) 50% fast nodes, and (d) factor of improvement for (a) - (c). (a) 64 Bytes, (b) 1 KBytes, (c) 4 KBytes, and d) factor of improvement for (a) - (c). latency for multiple multicast:
Reference: [6] <author> M. Lauria. </author> <title> High Performance MPI Implementation on a Net work of Workstations. </title> <type> Master's thesis, </type> <institution> Department of Computer Science, University of Illinois at Urbana-Champaign , Oct 1996. </institution>
Reference-contexts: The ratio of the communication capabilities of slow and fast nodes is therefore 2:15. 7.1.2 Broadcast For measuring the broadcast latency we followed a method similar to the one used in <ref> [6] </ref>. A broadcast operation starts when the source node initiates it. It is said to be complete when all the other nodes have received the broadcast message. The broadcast latency is defined as the time elapsed between the source node initiating it and the last recipient receiving it. <p> Measurement of broadcast latency was done in the following way. For an N node system, N 1 broadcasts were performed. Each time, after the broadcast, one of the N 1 recipients sent back an acknowledgment (instead of issuing another broadcast as in <ref> [6] </ref>). At the source node the time between initiation of the broadcast and receipt of the acknowledgment was measured. The maximum of these N 1 time readings corresponds to the last broadcast recipient sending back the acknowledgment.
Reference: [7] <author> M. Lin, J. Hsieh, D. H. C. Du, J. P. Thomas, and J. A. Mac Donald. </author> <title> Distributed Network Computing over Local ATM Networks. </title> <journal> IEEE JSAC, </journal> <volume> 13(4), </volume> <month> May </month> <year> 1995. </year>
Reference-contexts: Recently, some projects have emphasized issues related to collective communication in NOW systems. These projects have been centered around the following interconnects: ATM, Ethernet, and Myrinet. Performance of collective communication operations in NOW environments have been evaluated in <ref> [7, 11] </ref>. However, all these studies focus on only one type of interconnect in a NOW system. They also do not consider heterogeneity in workstation speeds, communication protocols, etc. Thus, the solutions derived in these research projects cannot be directly applied to HNOW systems to obtain maximum performance.
Reference: [8] <author> B. Lowekamp and A. Beguelin. </author> <title> ECO: Efficient Collective Operations for Communication on Heterogeneous Networks. </title> <booktitle> In IPPS, </booktitle> <pages> pages 399405, </pages> <year> 1996. </year>
Reference-contexts: They also do not consider heterogeneity in workstation speeds, communication protocols, etc. Thus, the solutions derived in these research projects cannot be directly applied to HNOW systems to obtain maximum performance. To the best of our knowledge, the ECO <ref> [8] </ref> package has been the only effort made to consider the heterogeneity of workstations in NOW environments. ECO is built on top of PVM. It proposes heuristics to partition the participating workstations of a collective communication operation into subnet-works based on pair-wise round-trip latencies.
Reference: [9] <author> P. K. McKinley and D. F. Robinson. </author> <title> Collective Communica tion in Wormhole-Routed Massively Parallel Computers. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 3950, </pages> <month> Dec </month> <year> 1995. </year>
Reference-contexts: The need for collective communication operations such as broadcast, multicast, global reduction, scatter, gather, complete exchange, and barrier synchronization arises frequently in parallel applications <ref> [9] </ref>. Thus, it is critical that the collective communication operations be implemented in the best possible manner (scalable as well as high performance) in a HNOW system. Recently, some projects have emphasized issues related to collective communication in NOW systems.
Reference: [10] <author> Message Passing Interface Forum. </author> <title> MPI: A Message-Passing In terface Standard, </title> <month> Mar </month> <year> 1994. </year>
Reference-contexts: A portable parallel programming environment is key to the success of the NOW/HNOW paradigm. Over the last few years, researchers have developed software packages like PVM [14] and Message Passing Interface standards like MPI <ref> [3, 10] </ref> to provide such portability. Even though these softwares and standards do not force an application developer to understand the intricate details of the hardware, software, and network characteristics, the performance of an application in a NOW/HNOW environment heavily depends on these characteristics.
Reference: [11] <author> N. Nupairoj and L. M. Ni. </author> <title> Performance Evaluation of Some MPI Implementations on Workstation Clusters. </title> <booktitle> In Proceedings of the SPLC Conference, </booktitle> <year> 1994. </year>
Reference-contexts: Recently, some projects have emphasized issues related to collective communication in NOW systems. These projects have been centered around the following interconnects: ATM, Ethernet, and Myrinet. Performance of collective communication operations in NOW environments have been evaluated in <ref> [7, 11] </ref>. However, all these studies focus on only one type of interconnect in a NOW system. They also do not consider heterogeneity in workstation speeds, communication protocols, etc. Thus, the solutions derived in these research projects cannot be directly applied to HNOW systems to obtain maximum performance.
Reference: [12] <author> D. K. Panda. </author> <title> Issues in Designing Efficient and Practical Algorithms for Collective Communication in Wormhole-Routed Sys tems. </title> <booktitle> In ICPP Workshop on Challenges for Parallel Processing, </booktitle> <pages> pages 815, </pages> <year> 1995. </year>
Reference-contexts: 2 ; ; W N1 g is the list of other participating nodes in a nondecreasing order with respect to their message initiation cost, can be expressed as: T broadcast = dlog Ne1 X i=0 0 2 i 1 The same algorithm can be easily used for implementing the multicast <ref> [12] </ref> operation. An algorithm similar to the algorithm used for multicast can also be used to implement the multiple multicast with the only difference being the fact that different trees are used for each multicast.
Reference: [13] <author> D. K. Panda, D. Basak, D. Dai, R. Kesavan, R. Sivaram, M. Banikazemi, and V. Moorthy. </author> <title> Simulation of Modern Parallel Systems: A CSIM-based approach. </title> <booktitle> In Proceedings of the 1997 Winter Simulation Conference (WSC'97), </booktitle> <pages> pages 1013 1020, </pages> <month> December </month> <year> 1997. </year>
Reference-contexts: In the following subsections, first the simulation setup is described in detail. Next, the results for broadcast, single multicast, and multiple multicast are presented. 7.2.1 Simulation Setup We modeled a representative HNOW system where workstations are interconnected with Myrinet switches. A detailed flit-level simulator (built using CSIM <ref> [13] </ref>) was used to model irregular topologies and the wormhole switching technique. A 64-node HNOW system was considered. Based on the experimental results, presented in Section 2.2, we considered the following communication startup times. Two classes of workstations were considered.
Reference: [14] <author> V. S. Sunderam. </author> <title> PVM: A Framework for Parallel and Distributed Computing. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 2(4):315339, </volume> <month> December </month> <year> 1990. </year>
Reference-contexts: This is forcing the NOW environments to be gradually redefined as Heterogeneous Networks of Workstations (HNOW) environments. A portable parallel programming environment is key to the success of the NOW/HNOW paradigm. Over the last few years, researchers have developed software packages like PVM <ref> [14] </ref> and Message Passing Interface standards like MPI [3, 10] to provide such portability.
References-found: 14

