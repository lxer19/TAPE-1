URL: http://www.cs.bham.ac.uk/~anp/dm_docs/tony_xiaohua_hu.ps.gz
Refering-URL: http://www.cs.bham.ac.uk/~anp/bibtex/kdd.bib.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: KNOWLEDGE DISCOVERY IN DATABASES AN ATTRIBUTE-ORIENTED ROUGH SET APPROACH  
Author: By Xiaohua Hu 
Degree: A Thesis Submitted to the Faculty of Graduate Studies and Research In Partial Fulfillment of the Requirements for the Degree of Doctor of Philosophy in Computer Science Faculty of Graduate Studies  
Note: c Copyright 1995: Xiaohua Hu  
Date: June, 1995  
Address: Regina  
Affiliation: University of  Regina, Saskatchewan  
Abstract-found: 0
Intro-found: 1
Reference: [Alt68] <author> E.L. Altman, </author> <year> (1968). </year> <title> Discriminant Analysis and the Prediction of Corporate Bankruptcy, </title> <journal> The Journal of Finance </journal>
Reference-contexts: The data were based on E.L. Altman's <ref> [Alt68] </ref>. The data set contains 66 collected records which represent either bankrupt or non-bankrupt firms. The five numerical attributes correspond to five financial ratios: W: working capital/total assets; R:retained earnings/total assets; E:earning before interest and taxes/total assets; M:market value of equity/book value; and S:sales/total assets. <p> The rules were correct 97.00% of the time using the Leave-One-Out method. The results were then compared to the multiple discriminant analysis (MDA) reported by Altman <ref> [Alt68] </ref>. The performance of each method is depicted in Table 7.3. Example 7.4 (DBMaxi) Experimental Results of Three Test Data Sets: IRIS Data, Appendicitis Data, Thyroid Data. Fisher's [Fis36] IRIS Flower data base is a well-known data set used as a standard benchmark example in today's rule discovery research.
Reference: [ASC95] <author> A. An, N. Shan, C. Chan, N. Cecone, W. Ziarko, </author> <year> (1995). </year> <title> Discovering Rules from Data for Water Demand Prediction, </title> <booktitle> accepted in the IJCAI workshop on Machine Learning and Expert System, </booktitle> <address> Montreal, Canada, </address> <month> Aug. </month> <pages> 21-23, </pages> <year> 1995. </year>
Reference-contexts: The following are some interesting topics for future research. 123 9.2.1 Applications of Knowledge Rules Discovered from Re- lational Databases The knowledge rules learned from relational databases are very useful in many applications, some of which are listed below: (1) Discovery of knowledge rules from knowledge-base systems and expert systems <ref> [ASC95] </ref>. Since rules are derived from a huge number of data stored in a relational database, they represent important knowledge about data in the database.
Reference: [ArM86] <author> B. Arbab and D. Michie, </author> <year> (1985). </year> <title> Generating Rules from Examples, </title> <booktitle> Proc. Ninth Int. Joint Conf. on Artificial Intelligence, </booktitle> <pages> 631-633 </pages>
Reference: [BKM91] <author> C. Baral, S.Kraus, and J. Minker, </author> <year> (1991). </year> <title> Combining Multiple Knowledge Bases, </title> <journal> IEEE Trans. on Knowledge and Data Engineering, </journal> <volume> Vol. 3, </volume> <pages> 208-220 </pages>
Reference: [Boo86] <author> J. Boose, </author> <year> (1986), </year> <title> Rapid Acquisition and Combination of Knowledge from Multiple Experts In The Same Domain. </title> <journal> Future Computing Systems, </journal> <volume> 1/2, </volume> <pages> 191-216 </pages>
Reference-contexts: Only when it is not able to classify one example, it tries to find one rule in the background rule set. The experimental results showed that its classification accuracy is higher than CN2 [ClB91] and AQ-family algorithms [MMHL86] in most situations. Boose <ref> [Boo86] </ref> has proposed an approach for combining the expertise of several individuals by utilizing a common grid via the Expertise Transfer System (ETS). All these methods lack a theoretical formalism about the mechanism of redundant knowledge.
Reference: [BuM78] <author> B.G. Buchanan and T. M. Mitchell, </author> <year> (1978). </year> <title> Model-Directed Learning of Production Rules, Pattern-Directed Inference System, </title> <publisher> Academic Press, </publisher> <editor> Water-man et. al. (eds), </editor> <month> 291-312. </month>
Reference-contexts: In the model-driven methods, an a priori model is used to constrain the search. These methods search a set of possible generalisations in an attempt to find a few "best" hypotheses that satisfy certain requirements. Typical examples of systems which adopt this strategy are AM [Len77], DENDRAL and Meta-DENDRAL <ref> [BuM78] </ref>, and the approach used in the INDUCE system [DiM81]. 10 Data-driven techniques generally have the advantage of supporting incremental learning. The learning process can start not only from the specific training examples, but also from the rules which have already been discovered.
Reference: [CCH91] <author> Y. Cai, N. Cercone and J. Han, </author> <year> (1991). </year> <title> Attribute Oriented Induction in Relational databases, in Knowledge Discovery in Database, </title> <publisher> AAAI/MIT Press, </publisher> <editor> G.Piatetsky-Shapiro and W.J. Frawley (eds), </editor> <volume> 213-228. </volume> <pages> 127 </pages>
Reference-contexts: Hence databases usually contain some attributes that are undesirable, irrelevant, or unimportant to a given discovery task, focussing on a 2 subset of attributes is now common practice. Identifying relevant fields is the most common focussing technique. In previous studies in <ref> [CCH91, HCC92, HCH93, HCH94] </ref>, an attribute-oriented induction method has been developed for knowledge discovery in relational databases. The method integrates a machine learning paradigm, especially learning f rom examples techniques, with database operations. <p> So the important considerations are necessary to determine the most relevant attributes and eliminate the irrelevant or unimportant attributes according to the learning task without losing essential information about the original data in the database (s). These previous studies <ref> [CCH91, HCC92, HCH94] </ref> did not analyze the data dependency relation among the attributes, meaningful information about the data, such as data dependency among the attributes, are not explicitly analyzed by rule-generation algorithms; thus the rules generated in this way are not particularly concise and pertinent but contain some redundant information or <p> Based on this consideration, we present an attribute-oriented rough set based knowledge discovery system for large databases. In this thesis, a framework for knowledge discovery in databases using rough set theory and attribute-oriented induction is proposed. Furthermore, the results from previous studies <ref> [CCH91, HCC92] </ref> are developed in two aspects. First our work [HCH93] expands the function of the previous system [CCH91, HCC92] and overcomes the "overgeneralization" problem of the previous studies. <p> In this thesis, a framework for knowledge discovery in databases using rough set theory and attribute-oriented induction is proposed. Furthermore, the results from previous studies <ref> [CCH91, HCC92] </ref> are developed in two aspects. First our work [HCH93] expands the function of the previous system [CCH91, HCC92] and overcomes the "overgeneralization" problem of the previous studies. The previous method is further developed to find knowledge rules associated with different levels of the concepts in the concept hierarchy [HCH94]. <p> It is conjectured that this problem is computationally intractable to solve optimally for arbitrary K. 25 Chapter 3 Extending DBLEARN DBLEARN is a database learning system developed by Cai, Cercone and Han <ref> [CCH91, HCC92a, HCC92b] </ref>. It implements both LCHR (for Learning Characteristic Rules) and LCLR (for Learning Classification Rules) algorithms. The language of DBLEARN can be viewed as an extension to the relational language SQL for knowledge discovery in databases. The architecture of DBLEARN is presented in Figure 3.1. <p> It implements both LCHR (for Learning Characteristic Rules) and LCLR (for Learning Classification Rules) algorithms. The language of DBLEARN can be viewed as an extension to the relational language SQL for knowledge discovery in databases. The architecture of DBLEARN is presented in Figure 3.1. DBLEARN <ref> [CCH91] </ref> was implemented in an Unix/C/Sybase environment. It can generate many interesting patterns, however, it sometimes tends to discover "over-generalized" patterns. A moderately large threshold may lead to a relatively complex rule with many disjuncts and the results may not be fully generalized. <p> The concepts in a taxonomy can be partially ordered according to general-to-specific ordering. Such a concept tree is specified using an IS-A hierarchy and stored in a relational table, the conceptual hierarchy table. Although data in a relational database are usually well-formatted and modelled by semantic and data models <ref> [CCH91] </ref>, the contents of the data may not be classified. For example, a chemistry database may store a large amount of experimental data in a relational format, but knowledge and effort are needed to classify the data in order to determine the intrinsic regularity of the data. <p> Chapter 7 Implementation and Experiments To test and experiment on the database learning algorithms developed in the previous chapters, an experimental database learning system, DBROUGH [HuC94a, HCH93b, HSCZ94], has been constructed and some interesting experiments have been conducted in the learning system. 7.1 Architecture DBROUGH is a descendant of DBLEARN <ref> [CCH91, HCC92a] </ref>. The architecture of the system is shown in Figure 7.1. The system can discover different kinds of knowledge rules from relational databases, including characteristic rules, discrimination rules, decision rules, maximal generalized rules, data trend regularities and multiple sets of knowledge rules for the discovery task. <p> However most existing algorithms do not take advantage of these database facilities <ref> [CCH91] </ref>. An obvious advantage of our approach over many other learning algorithms is the integration of the learning process with database operations.
Reference: [CeT93] <author> N. Cercone, M. Tsuchiya, (eds), </author> <year> (1993). </year> <title> Special Issue on Learning and Discovery in Knowledge Based Databases, </title> <journal> IEEE Transaction on Knowledge and Data Engineering, </journal> <volume> Vol. </volume> <pages> 5(6). </pages>
Reference-contexts: There are a lot of algorithms developed to find rules from databases directly <ref> [FrP91, CeT93] </ref>, but all these algorithms assume that the data and the data scheme are stable and most of the algorithms focus on discovering the regularities about the current data in the databases.
Reference: [CHH95] <author> N. Cercone, H. Horward, X. Hu and N. Shan, </author> <year> (1995). </year> <title> Data Mining Using Attribute-Oriented generalization and Information Reduction, </title> <booktitle> invited paper in the Second Annual Joint Conf. on Information Sciences Workshop on Rough Set Theory, </booktitle> <address> Wrightville Beach, NC, USA </address>
Reference: [Cen87] <author> J. Cendrowska, </author> <year> (1987). </year> <title> PRISM: An Algorithm for Inducing Modular Rules, </title> <journal> Int. J. Man-Machine Studies, </journal> <volume> Vol. 27, </volume> <pages> 349-370 </pages>
Reference-contexts: The "workbench" design is ideal for exploratory analysis by a user knowledgeable in both data and the operation of the discovery tools. 2.4.3 The ITRULE Algorithm ITRULE is a database learning program based on information theory [SyG92]. Like ID3 [Qui83], CN2 [ClN89] and PRISM <ref> [Cen87] </ref>, it searches for classification rules directly using a measure of rule goodness, J-measure. ITRULE takes sample data in the form of discrete attribute vectors and generate a set of K rules, where K is a user-defined parameter.
Reference: [CeB88] <author> B. Cestnik, I. Bratko, </author> <year> (1988). </year> <title> Learning Redundants Rules in Noisy Domains, </title> <booktitle> Proc. Europe Conf. on Artificial Intelligence, </booktitle> <address> Munich, Italy 348-351 </address>
Reference-contexts: Typically one object is classified with several rules and the decisions are then combined to obtain the final decision. This strategy proved to be very efficient <ref> [CeB88, Gan89, ClB91] </ref>. Many studies showed that such multiple sets of knowledge rules if appropriately combined during classification can improve the classification accuracy [KoK93]. However, the problem of how to combine decisions of multiple knowledge bases remains.
Reference: [Ces90] <author> B. Cestnik, </author> <year> (1990). </year> <title> Estimating Probabilities: A Crucial Task in Machine Learning, </title> <booktitle> Proc. Europe Conf. on Artificial Intelligence. </booktitle>
Reference: [ClN89] <author> P. Clark, T. Niblett, </author> <year> (1989). </year> <title> The CN2 Induction Algorithm, </title> <journal> Machine Learning Journal, </journal> <volume> 3(4): </volume> <pages> 261-283 </pages>
Reference-contexts: The "workbench" design is ideal for exploratory analysis by a user knowledgeable in both data and the operation of the discovery tools. 2.4.3 The ITRULE Algorithm ITRULE is a database learning program based on information theory [SyG92]. Like ID3 [Qui83], CN2 <ref> [ClN89] </ref> and PRISM [Cen87], it searches for classification rules directly using a measure of rule goodness, J-measure. ITRULE takes sample data in the form of discrete attribute vectors and generate a set of K rules, where K is a user-defined parameter.
Reference: [ClB91] <author> P. Clark, R. Boswell, </author> <year> (1989). </year> <title> Rule Induction with CN2: Recent Improvement, </title> <booktitle> Proc. EWSL 91, Porto, </booktitle> <pages> 151-163 </pages>
Reference-contexts: YAILS uses only the foreground set of rules during classification. Only when it is not able to classify one example, it tries to find one rule in the background rule set. The experimental results showed that its classification accuracy is higher than CN2 <ref> [ClB91] </ref> and AQ-family algorithms [MMHL86] in most situations. Boose [Boo86] has proposed an approach for combining the expertise of several individuals by utilizing a common grid via the Expertise Transfer System (ETS). All these methods lack a theoretical formalism about the mechanism of redundant knowledge. <p> Typically one object is classified with several rules and the decisions are then combined to obtain the final decision. This strategy proved to be very efficient <ref> [CeB88, Gan89, ClB91] </ref>. Many studies showed that such multiple sets of knowledge rules if appropriately combined during classification can improve the classification accuracy [KoK93]. However, the problem of how to combine decisions of multiple knowledge bases remains. <p> Currently, there are four strategies for combining multiple sets of knowledge rule: (1) Sum of distribution: Frequencies of covered training instances for all rules that cover a given testing instances are summed up and the instance is classified in the majority class of the resulting distribution <ref> [ClB91, Bun90] </ref>. (2) Voting: Each rule votes for one class.
Reference: [ChF85] <author> Y. Cheng, K.S. Fu, </author> <year> (1985). </year> <title> Conceptual Clustering in Knowledge Organization, </title> <journal> IEEE Transaction on Pattern Analysis and Machine Intelligence, </journal> <pages> 9(5) 592-598. </pages>
Reference: [CKS88] <author> P. Chessman, J. Kelly, M. Self, J. Stutz, W. Taylor, D. Freeman, </author> <year> (1988). </year> <title> AutoClass: A bayesian Classification System, </title> <booktitle> Proc. of the Fifth Internatioal Workshop on Machine Learning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <pages> 230-245. 128 </pages>
Reference: [CoF83] <author> P. Cohen and E. A. Feigenbaum, </author> <year> (1983). </year> <booktitle> The Handbook of Artificial Intelli--gence Vol. </booktitle> <volume> 3, </volume> <publisher> Heuristic Press and William Kaufmann Inc. </publisher>
Reference-contexts: Collectively, these terms encapsulate our view of the fundamental characteristics of discovery in databases. Many machine-learning algorithms are readily applicable for KDD. An important machine learning paradigm, learning f rom examples, that is, learning by generalizing specific facts or observations <ref> [CoF83, DiM83] </ref>, has been adopted in many existing induction learning algorithms. Real-world databases present additional considerations due to the nature of their contents which tend to be large, incomplete, dynamic, noisy and redundant. <p> related to learning from examples, and some recent progress in knowledge discovery in database systems and knowledge base systems which adopt the learning f rom examples paradigm. 2.1 Concepts of Learning From Examples: An AI Approach As a basic method in empirical learning, learning from examples has been studied extensively <ref> [CoF83, DiM83, HaM77, GeN87] </ref>. <p> The domain of structured attributes is defined by the problem background knowledge. 2.1.2 Generalized Rules Learning from examples can be viewed as a reasoning process from specific instances to general concepts. The following generalization rules are particularly useful in learning systems <ref> [CoF83, Mic83] </ref>. (1) Turning constants into variables If the concept F (v) holds for v when v is a constant a, and a constant b, and so on, then these concepts can be generalized into a statement that F (v) holds for every 7 value of v.
Reference: [DiM81] <author> T.G. Dietterich and R.S. Michalski, </author> <year> (1981). </year> <title> Inductive Learning of Structural Descriptions: Evaluation Criteria and Comparative Review of Selected Methods, </title> <journal> Artificial Intelligence, </journal> <volume> Vol. 16, </volume> <pages> 251-294. </pages>
Reference-contexts: The learned concept is an admissible rule if and only if it is both characteristic and discriminant [DiM83,GeN87]. Most learning algorithms are designed for learning admissible rules [DiM83,Mic83]. A few algorithms, such as INDUCE 1.2 <ref> [DiM81] </ref> and SPROUTER [HaM77], are designed for learning characteristic rules. <p> These methods search a set of possible generalisations in an attempt to find a few "best" hypotheses that satisfy certain requirements. Typical examples of systems which adopt this strategy are AM [Len77], DENDRAL and Meta-DENDRAL [BuM78], and the approach used in the INDUCE system <ref> [DiM81] </ref>. 10 Data-driven techniques generally have the advantage of supporting incremental learning. The learning process can start not only from the specific training examples, but also from the rules which have already been discovered. The learning systems are capable of updating the existing hypotheses to account for each new example.
Reference: [DiM83] <author> T.G. Dietterich and R.S. Michalski, </author> <year> (1983). </year> <title> A Comparative Review of Selected Methods for Learning from Examples, </title> <booktitle> Machine Learning: An Artificial Intelligence Approach, </booktitle> <volume> Vol. 1, </volume> <publisher> Morgan Kaufmann 41-82 </publisher>
Reference-contexts: Collectively, these terms encapsulate our view of the fundamental characteristics of discovery in databases. Many machine-learning algorithms are readily applicable for KDD. An important machine learning paradigm, learning f rom examples, that is, learning by generalizing specific facts or observations <ref> [CoF83, DiM83] </ref>, has been adopted in many existing induction learning algorithms. Real-world databases present additional considerations due to the nature of their contents which tend to be large, incomplete, dynamic, noisy and redundant. <p> related to learning from examples, and some recent progress in knowledge discovery in database systems and knowledge base systems which adopt the learning f rom examples paradigm. 2.1 Concepts of Learning From Examples: An AI Approach As a basic method in empirical learning, learning from examples has been studied extensively <ref> [CoF83, DiM83, HaM77, GeN87] </ref>. <p> DBROUGH [HuC94a, HuC94b, HSCZ94, HCH94, HCS94] can discover characteristic rules, discriminant rules and some other knowledge rules. 2.1.4 Control Strategies in Learning from Examples Induction methods can be divided into data-driven (bottom-up), model-driven (top-down), and mixed methods depending on the strategy employed during the search for generalized concepts <ref> [DiM83] </ref>. All of these methods maintain a set, H, of the currently most plausible rules. These methods differ primarily in how they refine the set H so that it eventually includes the desired concepts. In the data-driven methods, the presentation of the training examples drives the search. <p> When new training examples become available, model-driven methods must either backtrack or restart the learning process from the very beginning, because the criteria by which hypotheses were originally tested (or schemas instantiated) have been changed <ref> [DiM83] </ref>. On the other hand, an advantage of model-driven methods is that they tend to have good noise immunity. When a set of hypotheses, H, is tested against noisy training examples, the model-driven methods need not reject a hypothesis on the basis of one or two counterexamples. <p> In the data-driven method, the set of hypotheses, H, is revised each time on the basis of the current training example. Consequently, a single erroneous example can cause a large perturbation in H (from which it may never recover) <ref> [DiM83] </ref>. 2.2 Some Learning From Examples Models Since the 1960's, many algorithms and experimental systems of learning f rom examples have been developed [Mit77], which demonstrated aspects of machine learning in science, industry and business applications [Hau87,Ren86]. <p> Task-relevant data can be viewed as examples for learning processes. Undoubtedly, learning-from-examples should be an important strategy for knowledge discovery in databases. Most learning-from-examples algorithms partition the set of examples into positive and negative sets and perform generalization using the positive data and specialization using the negative ones <ref> [DiM83] </ref>. Unfortunately, a relational database does not explicitly store negative data (even though the negative data can be derived based on the closed world assumption [Rei84]), and thus no explicitly specified negative examples can be used for specialization. <p> Since A 0 is usually much less than N 0 , the worst case in the reduction 112 process is O (N 0 fi N 0 ). Then we examine other learning methods. Most learning algorithms in the literature <ref> [DiM83] </ref> are tuple-oriented algorithms. A tuple-oriented method examines data in the database tuple by tuple and performs generalization based on the comparison of tuple values with the intermediate generalization results. <p> However, the tuple-oriented approach performs generalization tuple by tuple, but the attribute-oriented approach performs generalization attribute by attribute. We compare the search spaces of our algorithms with that of a typical method of learning f rom examples, the candidate elimination algorithm <ref> [DiM83] </ref> In the candidate elimination algorithm, the set of all concepts which are consistent with the training examples is called the version space of the training examples.
Reference: [FaM86] <author> B.C. Falkenhainer and R.S. Michalski, </author> <year> (1986). </year> <title> Integrating Quantitative and Qualitiative Discovery: the ABACUS system, </title> <journal> Machine Learning, </journal> <volume> Vol. 1, No.4, </volume> <pages> 367-401. </pages>
Reference-contexts: The operator VARSEL selects the most relevant attributes and the operator ESEL determines the most representative examples. The operator DISEQ discovers equations governing numerical variables, which is based on the ABACUS-2 system for integrated qualitative and quantitative discovery <ref> [FaM86] </ref>. ABACUS-2 is related to programs such as BACON [LLBS83] and FAHRENHEIT [Zyt87]. Most of these machine learning programs invoked by KGOs are existing learning algorithms which have been well implemented. As in the case of many machine learning systems, the major challenge to the INLEN system is computational inefficiency.
Reference: [FaI92] <author> U. M. Fayyd, K. B. Irani, </author> <year> (1992). </year> <title> The Attribute Selection Problem in Decision Tree Generation, </title> <booktitle> Proc. of 1992 AAAI Conf., </booktitle> <pages> 104-110 </pages>
Reference: [Fi87a] <author> D. Fisher, </author> <year> (1987). </year> <title> Improving Inference Through Conceptual Clustering, </title> <booktitle> Proc. of 1987 AAAI Conf., </booktitle> <address> Seattle, Washington, </address> <pages> 231-239. </pages>
Reference-contexts: However, since the generalization is controlled by the specified value, the generalized relation will eventually shrink in further generalization. 8.5 Discovery of Knowledge by Conceptual Clustering Most conceptual classification algorithms in the literature <ref> [MiS83, Fi87a] </ref> are tuple-oriented algorithms. A tuple-oriented algorithm examines data in the database 116 tuple by tuple and performs generalization and classification based on the compar-ison of tuple values with the intermediate generalization results.
Reference: [Fi87b] <author> D. Fisher, </author> <year> (1987). </year> <title> A Computational Account of Basic Level and Typicality Effects, </title> <booktitle> Proceedings of 1987 AAAI Conf., </booktitle> <address> Seattle, Washington, </address> <pages> 461-465. </pages>
Reference: [Fis36] <author> R. Fisher, </author> <year> (1936). </year> <title> The Use of Multiple Measurements in Taxonomic Problems, </title> <journal> Annals of Eugenics , Vol. </journal> <volume> 7, </volume> <pages> pp 179-188 </pages>
Reference-contexts: The results were then compared to the multiple discriminant analysis (MDA) reported by Altman [Alt68]. The performance of each method is depicted in Table 7.3. Example 7.4 (DBMaxi) Experimental Results of Three Test Data Sets: IRIS Data, Appendicitis Data, Thyroid Data. Fisher's <ref> [Fis36] </ref> IRIS Flower data base is a well-known data set used as a standard benchmark example in today's rule discovery research. Three classes of iris type, i.e. virginica, versicolor and setosa are described by four numerical attributes, i.e., sepal length, sepal width, petal length and patal width.
Reference: [FPM91] <author> W. J. Frawley, G. Piatetsky and C.J. Matheus, </author> <year> (1991). </year> <title> Knowledge Discovery in Database : An Overview, Knowledge Discovery in Database, </title> <publisher> AAAI/MIT Press, </publisher> <editor> G.Piatetsky-Shapiro and W.J. Frawley (eds) 1-27. </editor> <volume> 129 </volume>
Reference-contexts: Data mining has been ranked as one of the most promising topics for research for the 1990s by both database and machine learning researchers [SSU91]. William Frawley and his colleague <ref> [FPM91] </ref> give a definition of knowledge as follows: "Given a set of facts (data) F , a language L, and some measure of certainty C, a pattern is defined as a statement S in L that describes relationships among a subset F s of F with a certainty c, such that
Reference: [Gam89] <author> M. Gams, </author> <year> (1989). </year> <title> New Measurements Highlight the Importance of Redun--dant Knowledge, </title> <booktitle> Proc. 4th Europe Working Session on Learning, </booktitle> <address> Momtpellier 71-80 </address>
Reference-contexts: Typically one object is classified with several rules in the multiple knowledge bases system, and the decisions are then combined to obtain the final conclusion. Many research results illustrated that such multiple rules, if appropriately combined during classification, can improve the classification accuracy <ref> [Kon91, KoK93, Gam89, CB88] </ref>. <p> Since the user can define the number of redundant rules, the preference function and other parameters, this enables a thorough extraction of most valuable rules. The efficiency of the learning algorithms remains practically the same when using redundant knowledge <ref> [Gam89] </ref>. At this point, it seems essential to understand how and why redundant knowledge or multiple knowledge rules help. <p> Indeed this might be a more difficult problem than to determine whether to add another redundant method or not. Similarly, it is very difficult to analyze the cooperation between experts. The phenomenon of importance of redundant knowledge in real life is empirically shown in <ref> [Gam89] </ref>, several strategies for generating multiple knowledge bases or redundant knowledges from a data set and using multiple experts in expert system development have been proposed. Gams [Gam89] developed the inductive learning system GINESYS that generate multiple sets of decision rules. <p> The phenomenon of importance of redundant knowledge in real life is empirically shown in <ref> [Gam89] </ref>, several strategies for generating multiple knowledge bases or redundant knowledges from a data set and using multiple experts in expert system development have been proposed. Gams [Gam89] developed the inductive learning system GINESYS that generate multiple sets of decision rules. One set of rules consists of "main" rule and of several "confirmation" rule. Each instance is classified with one set of rules by combining the probability distribution returned by different rules.
Reference: [GLF81] <author> T. Garvey, J. Lowrance amd M. Fischler, </author> <year> (1981). </year> <title> An Inference Technique for Integrating Knowledge from Disparate Sources, </title> <booktitle> Proc. Seventh Int. Joint Conf. Artificial Intelligence, </booktitle> <volume> 1, </volume> <pages> 319-325. </pages>
Reference: [GeN87] <author> M. Genesereth and N. Nilson, </author> <year> (1987). </year> <booktitle> Logical Foundation of Artificial Intelligence, </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: related to learning from examples, and some recent progress in knowledge discovery in database systems and knowledge base systems which adopt the learning f rom examples paradigm. 2.1 Concepts of Learning From Examples: An AI Approach As a basic method in empirical learning, learning from examples has been studied extensively <ref> [CoF83, DiM83, HaM77, GeN87] </ref>. <p> a set of positive examples of a concept, N is a set of negative examples of a concept, C is the conceptual bias which consists of a set of concepts to be used in defining learning rules and results, and fl is the logical bias which captures particular logic forms <ref> [GeN87] </ref>. In most learning systems, the training examples are classified in advance by the tutor into two disjoint sets, the positive examples set and the negative examples set 6 [Mic83]. The training examples represent low-level, specific information. The learning task is to generalise these low-level concepts to general rules. <p> There could be numerous inductive conclusions derived from a set of training examples. To cope with this multiplicity of possibilities, it is necessary to use some additional information, problem background knowledge, to constrain the space of possible inductive conclusions and locate the most desired one (s) <ref> [Gen87] </ref>. The conceptual bias and the logical bias provide the desired concepts and the logic forms which serve as this kind of background knowledge. These biases restrict the candidates to formulas with a particular vocabulary and logic forms.
Reference: [GoS88] <author> R.M. Goodman, P. Smyth, </author> <year> (1988). </year> <title> Decision Trees design from A communication Theory Standpoint, </title> <journal> IEEE Trans. Infor. Theory, </journal> <volume> Vol. 34, </volume> <pages> 979-994 </pages>
Reference-contexts: This is not to say that decision trees are not useful in problems areas, such as classification where a predetermined "hardwired" solution is sufficient <ref> [GoS88] </ref>.
Reference: [GrS87] <author> B.J. Gragun and H.J. Studel, </author> <year> (1987). </year> <title> A Decision-Table Based Processor for Checking Completeness and Consistency in Rule-Based Expert-Systems, </title> <journal> Int. J. Man-Machine Studies 26(5), </journal> <pages> 633-648 </pages>
Reference: [Grz88] <author> Grzymala-Busse, </author> <year> (1988). </year> <title> Knowledge Discovery Under Uncertainty- A Rough Set Approach, </title> <journal> J. Intell. Rob. Systems, </journal> <volume> vol. 1, </volume> <pages> 3-16 </pages>
Reference-contexts: The basic tools of the theory are possibility measures. There is extensive literature on fuzzy logic which also discusses some of the problems with this theory. The basic problem of fuzzy set theory is the determination of the grade of membership or the value of possibility <ref> [Grz88] </ref>. 48 In the past decade, Z. Pawlak [Paw82] introduced a new tool to deal with vague-ness, called the "rough set model". Fuzzy set theory and rough set theory are independent and offer alternative approaches to uncertainty, as was shown in [Paw85]. <p> Other advantages of the rough set approach include its ease of handling and its simple algorithms [Slo92]. Rough set theory has been successfully implemented in knowledge-based systems in medicine and industry <ref> [Grz88] </ref>. The rough set philosophy is based on the idea of classification. The most important issue addressed in the rough set theory is the idea of imprecise knowledge. In this approach, knowledge is imprecise if it contains imprecise concepts. <p> Objects can be characterized by some selected features represented by attributes. In general, information about objects expressed in this way is not sufficient to characterize objects uniquely, as any two objects are indistinguishable from one another whenever they assume the same values for all the attributes under consideration <ref> [Grz88] </ref>. A relational database may be considered as an information system in which columns are labelled by attributes, rows are labelled by the objects and the entry in column p and row x has the value p (x).
Reference: [HCC92a] <author> J. Han, Y.Cai, N. Cercone, </author> <year> (1992a). </year> <title> Knowledge Discovery in Databases: An Attribute-Oriented Approach, </title> <booktitle> Proceeding of the 18th VLDB Conference, </booktitle> <address> Vancouver , B.C., Canada, </address> <pages> 335-350. </pages>
Reference-contexts: It is conjectured that this problem is computationally intractable to solve optimally for arbitrary K. 25 Chapter 3 Extending DBLEARN DBLEARN is a database learning system developed by Cai, Cercone and Han <ref> [CCH91, HCC92a, HCC92b] </ref>. It implements both LCHR (for Learning Characteristic Rules) and LCLR (for Learning Classification Rules) algorithms. The language of DBLEARN can be viewed as an extension to the relational language SQL for knowledge discovery in databases. The architecture of DBLEARN is presented in Figure 3.1. <p> Chapter 7 Implementation and Experiments To test and experiment on the database learning algorithms developed in the previous chapters, an experimental database learning system, DBROUGH [HuC94a, HCH93b, HSCZ94], has been constructed and some interesting experiments have been conducted in the learning system. 7.1 Architecture DBROUGH is a descendant of DBLEARN <ref> [CCH91, HCC92a] </ref>. The architecture of the system is shown in Figure 7.1. The system can discover different kinds of knowledge rules from relational databases, including characteristic rules, discrimination rules, decision rules, maximal generalized rules, data trend regularities and multiple sets of knowledge rules for the discovery task. <p> It is intended to be used by individuals in " universities, government agencies and industry... to search for grants that are of particular interest" <ref> [HCC92a] </ref>. The NSERC Grants Information System contains a database of information about the grants that are awarded by NSERC. The central table in the database has 10,087 tuples with 11 attributes currently.
Reference: [HCC92b] <author> J. Han, Y.Cai, N. Cercone, </author> <year> (1992). </year> <title> Data Driven Discovery of Quantiative Rules in Relational Databases, </title> <journal> IEEE Trans. Knowledge and Data Engineering, </journal> <volume> 5(2). </volume>
Reference-contexts: Access to large databases is expensive, hence it is necessary to apply the techniques for sampling and other statistical methods. Furthermore, knowledge discovery in databases can benefit from many available tools and techniques in different fields, such as, expert systems, machine learning, intelligent databases, knowledge acquisition, and statistics <ref> [CCH91,HCC92a, HCC92b] </ref>. 20 2.4.1 INLEN System The INLEN system was developed by Kaufman et al in 1989 [KMK91]. <p> It is conjectured that this problem is computationally intractable to solve optimally for arbitrary K. 25 Chapter 3 Extending DBLEARN DBLEARN is a database learning system developed by Cai, Cercone and Han <ref> [CCH91, HCC92a, HCC92b] </ref>. It implements both LCHR (for Learning Characteristic Rules) and LCLR (for Learning Classification Rules) algorithms. The language of DBLEARN can be viewed as an extension to the relational language SQL for knowledge discovery in databases. The architecture of DBLEARN is presented in Figure 3.1.
Reference: [Hau86] <author> D. Haussler, </author> <year> (1986). </year> <title> Quantifying the Inductive Bias in Concept Learning, </title> <booktitle> Proceedings of 1986 AAAI Conference, </booktitle> <address> Philadelphia, PA, </address> <pages> 485-489. 130 </pages>
Reference-contexts: Unfortunately, this testing problem is NP-complete if we allow arbitrarily many examples and arbitrarily many attributes in the hypothesis <ref> [Hau86] </ref>. The second computational problem 12 is that the size of the sets S and G can become unmanageably large. It has been shown that, if the number of attributes is large, the sizes of set S and set G can grow exponentially in the number of examples [Hau86]. <p> the hypothesis <ref> [Hau86] </ref>. The second computational problem 12 is that the size of the sets S and G can become unmanageably large. It has been shown that, if the number of attributes is large, the sizes of set S and set G can grow exponentially in the number of examples [Hau86]. To improve computational efficiency, Haussler proposed a one-sided algorithm in contrast to the two-sided approach of the candidate elimination algorithm. The one-sided algorithm computes only the set S using the positive examples and then checks to see if any negative examples are contained in the set S.
Reference: [Hau87a] <author> D. Haussler, </author> <year> (1987). </year> <title> Bias, Version Spaces and Valient's Learning Frame--work, </title> <booktitle> Proc. 4th Int. Workshop on Machine Learning Workshop, </booktitle> <address> Irvine, CA, </address> <pages> 324-336. </pages>
Reference: [Hau87b] <author> D. Haussler, </author> <year> (1987). </year> <title> Learning Conjuctive Concepts in Structural Domains, </title> <booktitle> Proceedings of 1987 AAAI Conference, </booktitle> <address> Seattle, Washington, </address> <pages> 466-470. </pages>
Reference: [HaM77] <author> F. Hayes-Roth and J. McDermott, </author> <year> (1977). </year> <title> Knowledge Acquisition from Structural Descriptions, </title> <booktitle> Proceedings of 5th International Joint Conference on Artificial Intelligence, </booktitle> <address> Cambridge, MA,356-362. </address>
Reference-contexts: related to learning from examples, and some recent progress in knowledge discovery in database systems and knowledge base systems which adopt the learning f rom examples paradigm. 2.1 Concepts of Learning From Examples: An AI Approach As a basic method in empirical learning, learning from examples has been studied extensively <ref> [CoF83, DiM83, HaM77, GeN87] </ref>. <p> The learned concept is an admissible rule if and only if it is both characteristic and discriminant [DiM83,GeN87]. Most learning algorithms are designed for learning admissible rules [DiM83,Mic83]. A few algorithms, such as INDUCE 1.2 [DiM81] and SPROUTER <ref> [HaM77] </ref>, are designed for learning characteristic rules.
Reference: [HoM91] <author> J. Hong, C. Mao, </author> <year> (1991). </year> <title> Incremental Discovery of Rules and Structure by Hierarchical and Parallel Clustering, Knowledge Discovery in Database, </title> <publisher> AAAI/MIT Press, </publisher> <editor> G.Piatetsky-Shapiro and W.J. Frawley (eds), </editor> <month> 177-194. </month>
Reference-contexts: The clustering algorithms which have been framed as extensions to the numerical taxonomy techniques include CLUSTER/2 [MiS83] and COBWEB [Fis87]; whereas those which can be viewed as an extension of learning-by-observations include HUATAO [ChF83] and Thought/KD1 <ref> [HoM91] </ref>. 3.3.2 An Approach to Concept Clustering Our method is divided into three phases. Phase 1 uses a numerical taxonomy to classify the object set. Phase 2 assigns conceptual descriptions to object classes. Phase 3 finds the hierarchical, inheritance and domain knowledge based on different relationships among classes.
Reference: [HCH93] <author> X. Hu, N. Cercone, J. Han, </author> <year> (1993). </year> <title> Discovery of Konwledge Associated With Conceptual Hierarchies in Databases, </title> <booktitle> Proc. Third International Conference for Young Computer Scientists, </booktitle> <address> Beijing, China. 2.106-2.109 </address>
Reference-contexts: Hence databases usually contain some attributes that are undesirable, irrelevant, or unimportant to a given discovery task, focussing on a 2 subset of attributes is now common practice. Identifying relevant fields is the most common focussing technique. In previous studies in <ref> [CCH91, HCC92, HCH93, HCH94] </ref>, an attribute-oriented induction method has been developed for knowledge discovery in relational databases. The method integrates a machine learning paradigm, especially learning f rom examples techniques, with database operations. <p> In this thesis, a framework for knowledge discovery in databases using rough set theory and attribute-oriented induction is proposed. Furthermore, the results from previous studies [CCH91, HCC92] are developed in two aspects. First our work <ref> [HCH93] </ref> expands the function of the previous system [CCH91, HCC92] and overcomes the "overgeneralization" problem of the previous studies. The previous method is further developed to find knowledge rules associated with different levels of the concepts in the concept hierarchy [HCH94].
Reference: [Hux94] <author> X. Hu, </author> <year> (1994). </year> <title> Object Aggregration and Cluster Identification: A Knowledge Discovery Approach, </title> <journal> Applied Math. Letter. </journal> <volume> 7(4), </volume> <pages> 29-34. </pages>
Reference-contexts: If the concept hierarchy is unavailable, our method can construct a concept hierarchy automatically from the data and infer some knowledge rules based simply on the containment relationship between different clusters in the constructed concept hierarchy. This method combines our conceptual clustering technique <ref> [Hux94] </ref> with machine learning techniques. The rough set technique is incorporated into the learning procedure. Using rough set theory, our method can analyze the attributes globally and identify the most relevant attributes to the learning task. It can handle databases with incomplete information. <p> Notice that for any data set S, we set sim value (S; S) = 0. Algorithm 3.5 Conceptual Data Clustering [CDC] Input. A set of data stored in the relational table. Output. A cluster hierarchy of the data set. Method. 1. Preliminary: Generalize attributes to a "desirable form" <ref> [Hux94] </ref>. For example, for the attribute "age" in an employer database, the substitution of 42 different age values into a small number of distinct higher level concepts, such as "young", "middle-aged", "old", etc. will make the descriptions concise and meaningful. 2. <p> For rule formation, there are three algorithms of knowledge discovery: Hierarchical Knowledge Discovery (HKD), Attribute Knowledge Discovery (AKD) and Inheritance 43 Knowledge Discovery (IKD) <ref> [Hux94] </ref>. For HKD, new rules are discovered by finding all of the possible implications between the descriptions of clusters in a cluster and those in its father cluster, namely D i;j ! D i .
Reference: [HCH94a] <author> X. Hu, N. Cercone, J. Han, </author> <year> (1993). </year> <title> A Rough Set Approach for Knowledge Discovery in Databases, Rough Sets, Fuzzy Sets and Knowledge Discovery, </title> <publisher> Springer Verlag Press, W. Ziarko(ed), </publisher> <pages> 90-99 </pages>
Reference: [HuC94a] <author> X. Hu, N. Cercone, </author> <year> (1994). </year> <title> Learning in Relational Databases: A Rough Set Approach, </title> <journal> Computational Intelligence : An International Journal , special issue on rough set and knowledge discovery, </journal> <volume> 11(2), </volume> <pages> 323-338 </pages>
Reference-contexts: Rough set techniques introduced by Pawlak [Paw82] provide the necessary tools to analyze the set of attributes globally. It is not feasible to apply rough set techniques directly to large database because of the computational complexity, which is NP-hard <ref> [Zir91, HuC94a] </ref>. Although these two approaches are apparently different, in both methods, objects are assumed to be characterized by attributes and attribute values. Our study 3 shows that there is a close connection between attribute-oriented induction and the rough set approach. <p> The learned concept is an admissible rule if and only if it is both characteristic and discriminant [DiM83,GeN87]. Most learning algorithms are designed for learning admissible rules [DiM83,Mic83]. A few algorithms, such as INDUCE 1.2 [DiM81] and SPROUTER [HaM77], are designed for learning characteristic rules. DBROUGH <ref> [HuC94a, HuC94b, HSCZ94, HCH94, HCS94] </ref> can discover characteristic rules, discriminant rules and some other knowledge rules. 2.1.4 Control Strategies in Learning from Examples Induction methods can be divided into data-driven (bottom-up), model-driven (top-down), and mixed methods depending on the strategy employed during the search for generalized concepts [DiM83]. <p> analysis and comparison of these strategies and developing new methods for combining multiple sets of knowledge rules are one of our current research topics. 97 Chapter 7 Implementation and Experiments To test and experiment on the database learning algorithms developed in the previous chapters, an experimental database learning system, DBROUGH <ref> [HuC94a, HCH93b, HSCZ94] </ref>, has been constructed and some interesting experiments have been conducted in the learning system. 7.1 Architecture DBROUGH is a descendant of DBLEARN [CCH91, HCC92a]. The architecture of the system is shown in Figure 7.1.
Reference: [HuS94] <author> X. Hu, N. Shan, </author> <year> (1994). </year> <title> Multiple Knowledge Bases and Rough Set, </title> <booktitle> Proc. of the 7th Florida Research Symposium on AI, </booktitle> <pages> 255-258 131 </pages>
Reference: [HCS94] <author> X. Hu, N. Cercone. N. Shan, </author> <year> (1994). </year> <title> A Rough Set Approach to Compute All Maximal Generalized Rules, </title> <booktitle> Proc. of the 6th International Conference on Computing and Information, </booktitle> <address> Peterborough, Ontario, Canada, </address> <month> May 26-28. </month> <pages> 1078-1089. </pages>
Reference-contexts: The learned concept is an admissible rule if and only if it is both characteristic and discriminant [DiM83,GeN87]. Most learning algorithms are designed for learning admissible rules [DiM83,Mic83]. A few algorithms, such as INDUCE 1.2 [DiM81] and SPROUTER [HaM77], are designed for learning characteristic rules. DBROUGH <ref> [HuC94a, HuC94b, HSCZ94, HCH94, HCS94] </ref> can discover characteristic rules, discriminant rules and some other knowledge rules. 2.1.4 Control Strategies in Learning from Examples Induction methods can be divided into data-driven (bottom-up), model-driven (top-down), and mixed methods depending on the strategy employed during the search for generalized concepts [DiM83].
Reference: [HSCZ94] <author> X. Hu, N. Shan, N. Cercone, W. Ziarko, </author> <year> (1994). </year> <title> DBROUGH: A Rough Set Based Knowledge Discovery System, </title> <booktitle> Proc. of the 8th International Symposium on Methodologies for Intelligent System, Lecture Notes in AI 869 (Methodologies for Intelligent Systems), </booktitle> <publisher> Spring Verlag, </publisher> <pages> 386-395 </pages>
Reference-contexts: The learned concept is an admissible rule if and only if it is both characteristic and discriminant [DiM83,GeN87]. Most learning algorithms are designed for learning admissible rules [DiM83,Mic83]. A few algorithms, such as INDUCE 1.2 [DiM81] and SPROUTER [HaM77], are designed for learning characteristic rules. DBROUGH <ref> [HuC94a, HuC94b, HSCZ94, HCH94, HCS94] </ref> can discover characteristic rules, discriminant rules and some other knowledge rules. 2.1.4 Control Strategies in Learning from Examples Induction methods can be divided into data-driven (bottom-up), model-driven (top-down), and mixed methods depending on the strategy employed during the search for generalized concepts [DiM83]. <p> analysis and comparison of these strategies and developing new methods for combining multiple sets of knowledge rules are one of our current research topics. 97 Chapter 7 Implementation and Experiments To test and experiment on the database learning algorithms developed in the previous chapters, an experimental database learning system, DBROUGH <ref> [HuC94a, HCH93b, HSCZ94] </ref>, has been constructed and some interesting experiments have been conducted in the learning system. 7.1 Architecture DBROUGH is a descendant of DBLEARN [CCH91, HCC92a]. The architecture of the system is shown in Figure 7.1.
Reference: [HCH94b] <author> X. Hu, N. Cercone, J. Han, </author> <year> (1994). </year> <title> A Concept-based Knowledge Discovery Approach in Databases, </title> <booktitle> Proc. of the 10th Canadian Artificial Intelligence Conference, </booktitle> <pages> 47-62, </pages> <address> Banff, Alberta, Canada </address>
Reference: [HCX94] <author> X. Hu, N. Cercone, J. Xie. </author> <year> (1994). </year> <title> Learning Data Trend Regularities From Databases in A Dynamic Environment, </title> <booktitle> Proc. of the AAAI Knowledge Discovery in Databases Workshop, </booktitle> <pages> 323-334 </pages>
Reference-contexts: As a result, we get a set of task-relevant instances objects as shown in Table 8.3. After we get the task-relevant data, the data generalization and data reduction procedure can be applied in the same way as discussed in previous chapters and interesting data trend regularities can be found <ref> [HCX94] </ref>. 121 Chapter 9 Conclusion and Future Directions 9.1 Conclusion The rapid growth of data in the world's databases is one reason for the recent interest in KDD. The vastness of this data also creates one of KDD's greatest challenges.
Reference: [HuC94d] <author> X. Hu, N. Cercone, </author> <year> (1994). </year> <title> Discovery of Decision Rules from Databases: </title>
References-found: 48

