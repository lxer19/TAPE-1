URL: ftp://plucky.cs.yale.edu/GDH/Papers/cdc98_mobile2.ps.gz
Refering-URL: http://www.cs.yale.edu/users/hager/papers.html
Root-URL: http://www.cs.yale.edu
Title: Toward Domain-Independent Navigation: Dynamic Vision and Control  
Author: G.D. Hager D.J. Kriegman A.S. Georghiades O. Ben-Shahar 
Address: New Haven, CT 06520-8285  
Affiliation: Center for Computational Vision and Control Yale University  
Abstract: This paper outlines a set of problems associated with constructing a robust, domain-independent vision-based navigation system suitable for both structured and unstructured environments. The system utilizes visual tracking to monitor a set of automatically selected image features (markers), and employs vision-based control to guide the motion of the robot from the image trajectory of a set of markers. An environment is represented as a graph which may be constructed either under human control (e.g. by giving the system a tour) or autonomously as the system explores. In this paper, we review the system architecture and present two image-based mobile robot controllers for following visually-defined trajectories. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Barett, M. Brill, N. Haag, and P. Payton. </author> <title> Invariant linear methods in photogrammetry and model matching. </title> <editor> In J. Mundy and A. Zisserman, editors, </editor> <booktitle> Geometric Invariance in Computer Vision, </booktitle> <pages> pages 277-292. </pages> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: This problem, which is closely related to the image transfer problem <ref> [1, 11, 27] </ref>, can be solved as follows [8]. For simplicity, suppose that S 1 and S 2 are two "snapshots" (e:g: the first and last images) from a fixed sequence S j which contains four or more points.
Reference: [2] <author> A. Blake and M. Isard. </author> <title> Active Contours. </title> <publisher> Springer-Verlag, </publisher> <year> 1998. </year>
Reference-contexts: Although vision provides a huge amount of data, we quickly focus attention on small portions of the image which are easily distinguished from their local (in the image) surroundings, and track these patches through image sequences. Visual tracking of this type has proven to be simple to perform <ref> [2, 10] </ref>, yet it is robust and it reduces image information to a time history of a small set of feature locations. Consequently, the set of nominal robot paths in our system is represented in terms of the image trajectories of tracked features.
Reference: [3] <author> E. D. Dickmanns and V. Graefe. </author> <title> Dynamic monocular machine vision. Machine Vision and Applications, </title> <address> v:223-240, </address> <year> 1988. </year>
Reference-contexts: Much of the efficiency and robustness of the recent systems can be attributed to the use of special purpose architectures and algorithms that are tailored to exploit domain specific image cues. For example, road followers rely on finding the road boundary and lane markers <ref> [3, 12] </ref> or landmarks [6, 18, 19, 20] whereas mobile robots navigating in hallways have exploited uniform texture of the floor [13], floor/wall features [17, 15], and overhead lights [7].
Reference: [4] <author> B. Espiau, F. Chaumette, and P. Rives. </author> <title> A New Approach to Visual Servoing in Robotics. </title> <journal> IEEE Trans. on Robotics and Automation, </journal> <volume> 8 </volume> <pages> 313-326, </pages> <year> 1992. </year>
Reference-contexts: Methods for holonomic problems of this form have been developed by many authors <ref> [14, 9, 4, 24] </ref>; several recent articles describing adaptations of these ideas to non-holonomic systems can be found in [16].
Reference: [5] <author> O. Faugeras. </author> <title> Three-Dimensional Computer Vision. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1993. </year>
Reference-contexts: 1 i = i ; v 1 i = (u 2 i ; 1), it is well-known that the two measurements must satisfy the following bilinear form known as the epipolar constraint (m 1 i = 0 (2) where F 12 is a 3 fi 3 matrix of rank 2 <ref> [5] </ref>.
Reference: [6] <author> C. Fennema, A. Hanson, E. Riseman, J. Beveridge, and R. Kumar. </author> <title> Model-directed mobile robot navigation. </title> <journal> IEEE Trans. on Robotics and Automation, </journal> <volume> 20(6) </volume> <pages> 1352-69, </pages> <year> 1990. </year>
Reference-contexts: Much of the efficiency and robustness of the recent systems can be attributed to the use of special purpose architectures and algorithms that are tailored to exploit domain specific image cues. For example, road followers rely on finding the road boundary and lane markers [3, 12] or landmarks <ref> [6, 18, 19, 20] </ref> whereas mobile robots navigating in hallways have exploited uniform texture of the floor [13], floor/wall features [17, 15], and overhead lights [7]. However, although these domain specializations lead to impressive performance, they do so by imposing particular sensor cues and representations on low-level navigation.
Reference: [7] <author> T. Fukuda, S. Ito, F. Arai, and Y. Yokoyama. </author> <title> Navigation system based on ceiling landmark recogntion for autonomous mobile robot. </title> <booktitle> In IEEE Int. Workshop on Intelligent Robots and Systems, </booktitle> <pages> pages 150-155, </pages> <year> 1995. </year>
Reference-contexts: For example, road followers rely on finding the road boundary and lane markers [3, 12] or landmarks [6, 18, 19, 20] whereas mobile robots navigating in hallways have exploited uniform texture of the floor [13], floor/wall features [17, 15], and overhead lights <ref> [7] </ref>. However, although these domain specializations lead to impressive performance, they do so by imposing particular sensor cues and representations on low-level navigation. As a result, a system that works in one domain may require substantial redesign before it can be used in another.
Reference: [8] <author> G. Hager, D. Kriegman, E. Yeh, and C. Rasmussen. </author> <title> Image-based prediction of landmark features for mobile robot navigation. </title> <booktitle> In IEEE Conf. on Robotics and Automation, </booktitle> <pages> pages 1040-1046, </pages> <year> 1997. </year>
Reference-contexts: We assume that the robot has already acquired data about a set of nominal paths through the environment. Here we briefly summarize the important aspects of that representation and refer the reader to <ref> [26, 8, 10] </ref> for more details on the techniques used to acquire it. The set of nominal paths is represented as a directed graph (the map) based on the recorded visual trajectories of tracked features which we call markers. <p> This problem, which is closely related to the image transfer problem [1, 11, 27], can be solved as follows <ref> [8] </ref>. For simplicity, suppose that S 1 and S 2 are two "snapshots" (e:g: the first and last images) from a fixed sequence S j which contains four or more points. <p> It is important to note that this system does not always have a unique solution: there are certain geometric conditions which lead to a degenerate linear system. More discussion on this point can be found in <ref> [8] </ref>. 3.2 Motion Control In order to formalize the motion control problem, we now suppose that the robot is traversing an arc in the map corresponding to a stored sequence while tracking a corresponding set of markers.
Reference: [9] <author> G. D. Hager. </author> <title> A modular system for robust hand-eye coordination. </title> <journal> IEEE Trans. Robot. Automat, </journal> <volume> 13(4) </volume> <pages> 582-595, </pages> <year> 1997. </year>
Reference-contexts: Methods for holonomic problems of this form have been developed by many authors <ref> [14, 9, 4, 24] </ref>; several recent articles describing adaptations of these ideas to non-holonomic systems can be found in [16].
Reference: [10] <author> G. D. Hager and K. Toyama. </author> <title> The "XVision" system: A general purpose substrate for real-time vision applications. </title> <booktitle> Comp. Vision, Image Understanding., </booktitle> <volume> 69(1) </volume> <pages> 23-27, </pages> <month> January </month> <year> 1998. </year>
Reference-contexts: Although vision provides a huge amount of data, we quickly focus attention on small portions of the image which are easily distinguished from their local (in the image) surroundings, and track these patches through image sequences. Visual tracking of this type has proven to be simple to perform <ref> [2, 10] </ref>, yet it is robust and it reduces image information to a time history of a small set of feature locations. Consequently, the set of nominal robot paths in our system is represented in terms of the image trajectories of tracked features. <p> We assume that the robot has already acquired data about a set of nominal paths through the environment. Here we briefly summarize the important aspects of that representation and refer the reader to <ref> [26, 8, 10] </ref> for more details on the techniques used to acquire it. The set of nominal paths is represented as a directed graph (the map) based on the recorded visual trajectories of tracked features which we call markers.
Reference: [11] <author> R. </author> <title> Hartley. A linear method for reconstruction from lines and points. </title> <booktitle> In Int. Conf. on Computer Vision, </booktitle> <pages> pages 882-887, </pages> <year> 1995. </year>
Reference-contexts: This problem, which is closely related to the image transfer problem <ref> [1, 11, 27] </ref>, can be solved as follows [8]. For simplicity, suppose that S 1 and S 2 are two "snapshots" (e:g: the first and last images) from a fixed sequence S j which contains four or more points.
Reference: [12] <author> M. Hebert, D. Pomerleau, A. Stentz, and C. Thorpe. </author> <title> Computer vision for navigation; the cmu ugv project. </title> <booktitle> In Proceedings of the Workshop on Vision for Robots, </booktitle> <pages> pages 87-96. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1995. </year>
Reference-contexts: Much of the efficiency and robustness of the recent systems can be attributed to the use of special purpose architectures and algorithms that are tailored to exploit domain specific image cues. For example, road followers rely on finding the road boundary and lane markers <ref> [3, 12] </ref> or landmarks [6, 18, 19, 20] whereas mobile robots navigating in hallways have exploited uniform texture of the floor [13], floor/wall features [17, 15], and overhead lights [7].
Reference: [13] <author> I. Horswill. Polly: </author> <title> A vision-based artificial agent. </title> <booktitle> In AAAI, </booktitle> <pages> pages 824-829, </pages> <year> 1992. </year>
Reference-contexts: For example, road followers rely on finding the road boundary and lane markers [3, 12] or landmarks [6, 18, 19, 20] whereas mobile robots navigating in hallways have exploited uniform texture of the floor <ref> [13] </ref>, floor/wall features [17, 15], and overhead lights [7]. However, although these domain specializations lead to impressive performance, they do so by imposing particular sensor cues and representations on low-level navigation.
Reference: [14] <author> S. Hutchinson, G. D. Hager, and P. Corke. </author> <title> A tutorial introduction to visual servo control. </title> <journal> IEEE Trans. Robot. Automat, </journal> <volume> 12(5) </volume> <pages> 651-670, </pages> <year> 1996. </year>
Reference-contexts: Methods for holonomic problems of this form have been developed by many authors <ref> [14, 9, 4, 24] </ref>; several recent articles describing adaptations of these ideas to non-holonomic systems can be found in [16]. <p> Since the motion of the system is already stabilized by encoder feedback, it is usually possible to model system dynamics as a pure time delay and to choose a control input u = ( _x; _z; _ ) t <ref> [14] </ref>.
Reference: [15] <author> D. Kim and R. Navatia. </author> <title> Symbolic navigation with a generic map. </title> <booktitle> In Proceedings of the Workshop on Vision for Robots, </booktitle> <pages> pages 136-145. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1995. </year>
Reference-contexts: For example, road followers rely on finding the road boundary and lane markers [3, 12] or landmarks [6, 18, 19, 20] whereas mobile robots navigating in hallways have exploited uniform texture of the floor [13], floor/wall features <ref> [17, 15] </ref>, and overhead lights [7]. However, although these domain specializations lead to impressive performance, they do so by imposing particular sensor cues and representations on low-level navigation. As a result, a system that works in one domain may require substantial redesign before it can be used in another.
Reference: [16] <author> D. Kriegman, G. Hager, and A. Morse, </author> <title> editors. The Confluence of Vision and Control. </title> <publisher> Springer-Verlag, </publisher> <year> 1998. </year>
Reference-contexts: Methods for holonomic problems of this form have been developed by many authors [14, 9, 4, 24]; several recent articles describing adaptations of these ideas to non-holonomic systems can be found in <ref> [16] </ref>. Suppose that an observed marker m i has image coordinates m i = (u; v) t 2 IR 2 and external coordinates P i = (X; Y; Z) t 2 IR 3 expressed in the camera frame of reference.
Reference: [17] <author> D. J. Kriegman, E. Triendl, and T. O. Binford. </author> <title> Stereo vision and navigation in buildings for mobile robots. </title> <journal> IEEE Trans. on Robotics and Automation, </journal> <volume> 5(6) </volume> <pages> 792-803, </pages> <month> Decem-ber </month> <year> 1989. </year>
Reference-contexts: For example, road followers rely on finding the road boundary and lane markers [3, 12] or landmarks [6, 18, 19, 20] whereas mobile robots navigating in hallways have exploited uniform texture of the floor [13], floor/wall features <ref> [17, 15] </ref>, and overhead lights [7]. However, although these domain specializations lead to impressive performance, they do so by imposing particular sensor cues and representations on low-level navigation. As a result, a system that works in one domain may require substantial redesign before it can be used in another.
Reference: [18] <author> K.-D. Kuhnert. </author> <title> Fusing dynamic vision and landmark navigation for autonomous driving. </title> <booktitle> In IEEE Int. Workshop on Intelligent Robots and Systems, </booktitle> <pages> pages 113-119, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: Much of the efficiency and robustness of the recent systems can be attributed to the use of special purpose architectures and algorithms that are tailored to exploit domain specific image cues. For example, road followers rely on finding the road boundary and lane markers [3, 12] or landmarks <ref> [6, 18, 19, 20] </ref> whereas mobile robots navigating in hallways have exploited uniform texture of the floor [13], floor/wall features [17, 15], and overhead lights [7]. However, although these domain specializations lead to impressive performance, they do so by imposing particular sensor cues and representations on low-level navigation.
Reference: [19] <author> A. Lazanas and J.-C. Latombe. </author> <title> Landmark-based robot navigation. </title> <booktitle> In Proc. Am. Assoc. Art. Intell., </booktitle> <year> 1992. </year>
Reference-contexts: Much of the efficiency and robustness of the recent systems can be attributed to the use of special purpose architectures and algorithms that are tailored to exploit domain specific image cues. For example, road followers rely on finding the road boundary and lane markers [3, 12] or landmarks <ref> [6, 18, 19, 20] </ref> whereas mobile robots navigating in hallways have exploited uniform texture of the floor [13], floor/wall features [17, 15], and overhead lights [7]. However, although these domain specializations lead to impressive performance, they do so by imposing particular sensor cues and representations on low-level navigation.
Reference: [20] <author> T. S. Levitt, D. T. Lawton, D. M. Chelberg, and P. C. Nelson. </author> <title> Qualitative landmark-based path planning and following. </title> <booktitle> In Proceedings of AAAI-87, </booktitle> <pages> pages 689-694, </pages> <address> Los Altos, July 1987. </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: Much of the efficiency and robustness of the recent systems can be attributed to the use of special purpose architectures and algorithms that are tailored to exploit domain specific image cues. For example, road followers rely on finding the road boundary and lane markers [3, 12] or landmarks <ref> [6, 18, 19, 20] </ref> whereas mobile robots navigating in hallways have exploited uniform texture of the floor [13], floor/wall features [17, 15], and overhead lights [7]. However, although these domain specializations lead to impressive performance, they do so by imposing particular sensor cues and representations on low-level navigation.
Reference: [21] <author> H. Longuet-Higgins. </author> <title> A computer algorithm for reconstructing a scene from two projections. </title> <journal> Nature, </journal> <volume> 293 </volume> <pages> 133-135, </pages> <year> 1981. </year>
Reference-contexts: 12 )R 12 where R 12 2 SO (3) and t 12 2 IR 3 denote the rotation and translation between the camera locations at which the correspond p. 2 ing images were acquired, and skew (t 12 ) is the skew--symmetric matrix whose elements are given by t 12 <ref> [21] </ref>.
Reference: [22] <author> H. P. Moravec. </author> <title> The stanford cart and the cmu rover. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 71(7), </volume> <month> July </month> <year> 1983. </year>
Reference-contexts: 1 Introduction Since early work in the 1970's, such as SRI's Shakey [23] and Moravec's Cart <ref> [22] </ref>, there have been great strides in the development of vision-based navigation methods for mobile robots operating both indoors and outdoors.
Reference: [23] <author> N. J. Nilsson. Shakey, </author> <title> the robot. </title> <type> Technical Note 323, </type> <institution> SRI, </institution> <month> April </month> <year> 1984. </year>
Reference-contexts: 1 Introduction Since early work in the 1970's, such as SRI's Shakey <ref> [23] </ref> and Moravec's Cart [22], there have been great strides in the development of vision-based navigation methods for mobile robots operating both indoors and outdoors.
Reference: [24] <author> N. Papanikolopoulos, P. Khosla, and T. Kanade. </author> <title> Visual tracking of a moving target by a camera mounted on a robot: A combination of control and vision. </title> <journal> IEEE Trans. Robot. Automat, </journal> <volume> 9(1) </volume> <pages> 14-35, </pages> <year> 1993. </year>
Reference-contexts: Methods for holonomic problems of this form have been developed by many authors <ref> [14, 9, 4, 24] </ref>; several recent articles describing adaptations of these ideas to non-holonomic systems can be found in [16]. <p> In our case, since we have the complete tour at our disposal, it is not difficult to compute registered values for Z for every observation for the pre-learned sequence. These values can then be modified online using any of a number of estimation methods <ref> [24] </ref>. A second issue is to map this control vector to the non-holonomic kinematics. There are several possibilities in this case [25].
Reference: [25] <author> D. Popa and J. Wen. </author> <title> Nonholonomic path planning with obstacle avoidance: A path-space approach. </title> <booktitle> In IEEE Conf. on Robotics and Automation, </booktitle> <pages> pages 2662-2667, </pages> <year> 1996. </year>
Reference-contexts: These values can then be modified online using any of a number of estimation methods [24]. A second issue is to map this control vector to the non-holonomic kinematics. There are several possibilities in this case <ref> [25] </ref>. We have chosen the following mapping K = 1 + j _xj= s (t) = s r (t) + _z (16) where and are design parameters chosen to "tune" the system.
Reference: [26] <author> C. Rasmussen and G. D. Hager. </author> <title> Robot navigation using image sequences. </title> <booktitle> In Proc. American Association for Artificial Intelligence, </booktitle> <pages> pages 938-943, </pages> <year> 1996. </year>
Reference-contexts: We assume that the robot has already acquired data about a set of nominal paths through the environment. Here we briefly summarize the important aspects of that representation and refer the reader to <ref> [26, 8, 10] </ref> for more details on the techniques used to acquire it. The set of nominal paths is represented as a directed graph (the map) based on the recorded visual trajectories of tracked features which we call markers.
Reference: [27] <author> A. Shashua and M. Werman. </author> <title> Trilinearity of three perspective views and its associated tensor. </title> <booktitle> In ICCV95, </booktitle> <pages> pages 920-925, </pages> <year> 1995. </year> <note> p. 6 </note>
Reference-contexts: This problem, which is closely related to the image transfer problem <ref> [1, 11, 27] </ref>, can be solved as follows [8]. For simplicity, suppose that S 1 and S 2 are two "snapshots" (e:g: the first and last images) from a fixed sequence S j which contains four or more points.
References-found: 27

