URL: http://www.cs.msu.edu/~sass/Technical/cps-94-20.ps
Refering-URL: http://www.cs.msu.edu/~sass/Technical/technical.html
Root-URL: http://www.cs.msu.edu
Email: sass@cps.msu.edu mutka@cps.msu.edu  
Title: Enabling Unimodular Transformations  
Author: Ron Sass Matt Mutka 
Date: April 7, 1994  
Address: East Lansing, MI 48824-1027  
Affiliation: Department of Computer Science Michigan State University  
Pubnum: Technical Report CPS-94-20  
Abstract: The development of a unimodular transformation theory and associated algorithms has renewed interest in FORTRAN DO loops that are not perfectly (or tightly) nested. In this paper we summarize a number of techniques that convert imperfectly nested loops into perfectly nested loops. We examined over 25,000 lines of scientific FORTRAN code. Statistics on how often imperfect loops occur and how effective two transformations (scalar forward substitution and loop distribution) are at converting imperfectly nested loops into perfectly nested loops are reported. Further, we describe a compiler that integrates scalar forward substitution, loop distribution, and unimodular transformations while maintaining the basic philosophy of unimodular transformation theory. While our data indicate that imperfectly nested loops still present a problem, the compiler we describe is no more limited by perfectly nested loops than other restructuring compilers available today. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> W. Abu-Sufah. </author> <title> Improving the Performance of Virtual Memory Computers. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> November </month> <year> 1978. </year>
Reference-contexts: 2 ; N 2 IF i 3 = n 3 THEN IF i 2 = n 2 THEN P 2 [i 1 ] P 3 [i 1 ; i 2 ] S [i 1 ; i 2 ; i 3 ] END DO Most researchers cite Abu-Sufah's non-basic-to-basic loop transformation <ref> [1] </ref> when referring to this technique, but it is probably much older. (Lamport [12] mentions the technique in 1974.) M.E. Wolf [19] points out an important condition for the legality of this transformation; one that has frequently been omitted. <p> This definition agrees with Abu-Sufah's definition of basic loops <ref> [1] </ref> and with the concept of perfectly nested used by Banerjee [4], Wolfe [21], and others. Tightly-nested has also been used to describe perfect 6 loop nests [12]. The techniques used to handle loop nests that are not perfect are limited to certain loop structures.
Reference: [2] <author> Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, Massachusetts, </address> <year> 1986. </year>
Reference-contexts: SFS will propagate an expression forward within a certain scope (in our case, a DO loop) when there is exactly one def (that does not involve the previous value of the variable) and one or more subsequent uses. The meaning of def and use are given in <ref> [2] </ref>. The SFS method is demonstrated in the example below: DO I=LB,UB TEMP=C*Y (I)-2 DO J=LB,UB ...=TEMP*X (I,J) . . . <p> SFS actually increases the amount of computation, but the effect of the extra computation is much smaller than the effect due to parallel execution so there is a net speed up. (In fact, SFS is the inverse of common subexpression elimination <ref> [2] </ref>, which is used in many optimizing compilers for scalar machines to remove the extra computation.) To summarize, each of the above transformations has made some progress towards solving the problem of dealing with imperfect loop nests. However, each transformation has its own restrictions on when it may be used. <p> If we find that after applying SFS that the unimodular transformation is unable to parallelize the loop, then common 16 subexpression elimination and strength reduction (both are well-known transformations <ref> [2] </ref>) will undo SFS. So either SFS contributes to the parallelization or it is equivalent to not applying any transformations. Similarly, LD is always as good as or better than no transformations.
Reference: [3] <author> Randy Allen and Ken Kennedy. </author> <title> Automatic translation of fortran programs to vector form. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(4), </volume> <month> October </month> <year> 1987. </year>
Reference-contexts: The number of times each of these cases occurred is shown in Table 2. Note that the total exceeds 31 because some nests exhibited more than one of the characteristics. These characteristics are not new discoveries. Recurrence statements are discussed in <ref> [3] </ref> and [9]. These statements tend to be difficult to parallelize. Usually, the most effective technique is to replace the statement with a machine-optimized library call, i.e., change the algorithm. <p> Usually, the most effective technique is to replace the statement with a machine-optimized library call, i.e., change the algorithm. In [9] the authors reference a study [13] that showed this library-call solution yielded a 50% increase in performance for a Conjugate Gradient algorithm on the Cedar architecture. In <ref> [3] </ref>, Allen and Kennedy include an example that has an induction variable and mention a transformation induction 12 do j = 1, ip do i = 1, ip if (iflag .eq. 0) then acc = acc+a (j,i)*b (ibstar+i) else acc = acc+a (i,j)*b (ibstar+i) endif end do c (icstar+j) = acc
Reference: [4] <author> Utpal Banerjee. </author> <title> Unimodular transformations of double loops. </title> <editor> In Alexandru Nicolau, David Gelernter, Thomas Gross, and David Padua, editors, </editor> <booktitle> Advances in Languages and Compilers for Parallel Processing, </booktitle> <pages> pages 192-219. </pages> <publisher> Pitman Publishing, </publisher> <year> 1991. </year>
Reference-contexts: This definition agrees with Abu-Sufah's definition of basic loops [1] and with the concept of perfectly nested used by Banerjee <ref> [4] </ref>, Wolfe [21], and others. Tightly-nested has also been used to describe perfect 6 loop nests [12]. The techniques used to handle loop nests that are not perfect are limited to certain loop structures.
Reference: [5] <author> Utpal Banerjee, </author> <month> July </month> <year> 1992. </year> <title> Personal communication to R. Sass during a short course on Restructuring Compilers in Trento, </title> <address> Italy. </address>
Reference-contexts: If restructuring compilers are based on the application of a single unimodular transformation, then imperfect loop nests are not converted. In [6], Banerjee discusses perfectly nested loops and suggests that general loops will be considered in a future volume of [6]. Nevertheless, general loops remain a problem <ref> [5] </ref>. This paper analyzes how often perfect loop nests occur in scientific FORTRAN codes, analyzes the effects of known transformations on the codes, and describes a compiler that is fundamentally based on unimodular transformations but is not, with respect to perfectly nested loops, as restrictive.
Reference: [6] <author> Utpal Banerjee. </author> <title> Loop Transformations for Restructuring Compilers: The Foundations. </title> <publisher> Kluwer Academic Publishers, Norwell, </publisher> <address> Massachusetts, </address> <year> 1993. </year>
Reference-contexts: A few of the transformations would convert some portion of the imperfect loop nests into perfect loop nests. If restructuring compilers are based on the application of a single unimodular transformation, then imperfect loop nests are not converted. In <ref> [6] </ref>, Banerjee discusses perfectly nested loops and suggests that general loops will be considered in a future volume of [6]. Nevertheless, general loops remain a problem [5]. <p> If restructuring compilers are based on the application of a single unimodular transformation, then imperfect loop nests are not converted. In <ref> [6] </ref>, Banerjee discusses perfectly nested loops and suggests that general loops will be considered in a future volume of [6]. Nevertheless, general loops remain a problem [5].
Reference: [7] <author> F. Bodin, S. Lelait, and D. Windheiser. </author> <title> Sigma toolbox. </title> <type> Technical Report Sigma Toolbox Manual, version 0.2alpha, </type> <institution> Irisa and Indiana University, </institution> <month> July </month> <year> 1993. </year>
Reference-contexts: In order to process the sample data, we developed a program to parse FORTRAN, perform the necessary transformations, and classify the loop nests. We used the parser and database library from the Sigma Toolbox 0.2a (which, in turn, was based on the SIGMACS project) <ref> [7, 10, 17] </ref>.
Reference: [8] <author> J.J. Dongarra and E. Grosse. </author> <title> Distribution of mathematical software via electronic mail. </title> <journal> Communications of the ACM, </journal> <volume> 30, </volume> <year> 1987. </year>
Reference-contexts: Finally, it tests for perfect, apperfect, and general, in that order. The summarized results are listed package-by-package in Table 1. Discussion The packages we have selected are well-known FORTRAN codes that are available to the public on the Internet. (We retrieved these via Netlib <ref> [8] </ref>.) Linpack is a collection of linear algebra subroutines. The Misc package includes subroutines that were collected from various sources e.g., samples used in research papers to demonstrate the techniques described in x2. The Nascodes is a collection of five NAS benchmarks.
Reference: [9] <author> R. Eigenmann, J. Hoeflinger, G. Jaxon, and D. Padua. </author> <title> Cedar fortran and its restructuring compiler. </title> <editor> In Alexandru Nicolau, David Gelernter, Thomas Gross, and David Padua, editors, </editor> <booktitle> Advances in Languages and Compilers for Parallel Processing, </booktitle> <pages> pages 1-23. </pages> <publisher> Pitman Publishing, </publisher> <year> 1991. </year>
Reference-contexts: The number of times each of these cases occurred is shown in Table 2. Note that the total exceeds 31 because some nests exhibited more than one of the characteristics. These characteristics are not new discoveries. Recurrence statements are discussed in [3] and <ref> [9] </ref>. These statements tend to be difficult to parallelize. Usually, the most effective technique is to replace the statement with a machine-optimized library call, i.e., change the algorithm. In [9] the authors reference a study [13] that showed this library-call solution yielded a 50% increase in performance for a Conjugate Gradient <p> These characteristics are not new discoveries. Recurrence statements are discussed in [3] and <ref> [9] </ref>. These statements tend to be difficult to parallelize. Usually, the most effective technique is to replace the statement with a machine-optimized library call, i.e., change the algorithm. In [9] the authors reference a study [13] that showed this library-call solution yielded a 50% increase in performance for a Conjugate Gradient algorithm on the Cedar architecture.
Reference: [10] <author> Dennis Gannon, Jenq Kuen Lee, Bruce Shei, Sekhar Sarukaiand Srivinas Narayana, Nee-lakantan Sundaresan, Daya Atapattu, and Francois Bodin. </author> <title> Sigma II: A toolkit for building parallelizing compilers and performance analysis systems. </title> <booktitle> In Proceedings of the Programm-ming Environments for Parallel Computing, </booktitle> <address> Edinburgh, Scotland, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: In order to process the sample data, we developed a program to parse FORTRAN, perform the necessary transformations, and classify the loop nests. We used the parser and database library from the Sigma Toolbox 0.2a (which, in turn, was based on the SIGMACS project) <ref> [7, 10, 17] </ref>.
Reference: [11] <author> Wayne Kelly and William Pugh. </author> <title> A framework for unifying reordering transformations. </title> <type> Technical Report CS-TR-2995.1, </type> <institution> University of Maryland, College Park, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: 1 Introduction The automatic restructuring of FORTRAN code for parallel execution has focused mainly on the exploitation of DO loops. A majority of the performance-increasing techniques developed over the last twenty years assume loops are perfectly nested. Researchers have begun to question this assumption <ref> [11] </ref> as new developments, such as unimodular transformation (UT) theory, have continued to be limited to perfectly nested loops. This was not a concern with the older techniques because restructuring compilers performed a large number of transformations.
Reference: [12] <author> Leslie Lamport. </author> <title> The parallel execution of do loops. </title> <journal> Communications of the ACM, </journal> <volume> 17(2), </volume> <month> February </month> <year> 1974. </year>
Reference-contexts: 2 = n 2 THEN P 2 [i 1 ] P 3 [i 1 ; i 2 ] S [i 1 ; i 2 ; i 3 ] END DO Most researchers cite Abu-Sufah's non-basic-to-basic loop transformation [1] when referring to this technique, but it is probably much older. (Lamport <ref> [12] </ref> mentions the technique in 1974.) M.E. Wolf [19] points out an important condition for the legality of this transformation; one that has frequently been omitted. <p> This definition agrees with Abu-Sufah's definition of basic loops [1] and with the concept of perfectly nested used by Banerjee [4], Wolfe [21], and others. Tightly-nested has also been used to describe perfect 6 loop nests <ref> [12] </ref>. The techniques used to handle loop nests that are not perfect are limited to certain loop structures. We establish a class based on this characteristic, that is a superset of the perfect loop nests but still does not encompass all loop nests.
Reference: [13] <author> U. Meier and R. Eigenmann. </author> <title> Parallelization and performance of conjugate gradient algorithm on the cedar hierarchical-memory multiprocessor. </title> <type> Technical Report 1035, </type> <institution> University of Illinois at Urbana-Champaign, Center for Supercomputing R&D, </institution> <year> 1990. </year>
Reference-contexts: These characteristics are not new discoveries. Recurrence statements are discussed in [3] and [9]. These statements tend to be difficult to parallelize. Usually, the most effective technique is to replace the statement with a machine-optimized library call, i.e., change the algorithm. In [9] the authors reference a study <ref> [13] </ref> that showed this library-call solution yielded a 50% increase in performance for a Conjugate Gradient algorithm on the Cedar architecture.
Reference: [14] <author> David Padua, David Kuck, and Duncan Lawrie. </author> <title> High-speed multiprocessors and compilation techniques. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-29(9), </volume> <month> September </month> <year> 1980. </year> <month> 20 </month>
Reference-contexts: Because the assumption of perfectly nested loops is so common, many researchers do not explicitly state the assumption. In this section we discuss the advantages and the disadvantages of a number of existing transformations that are useful for converting loops. Loop Distribution <ref> [14] </ref> (LD) is an old and popular technique based on data dependences. This method makes loops suitable for automatic conversion to vector or parallel form. Nevertheless, a major problem with loop distribution is that it cannot break a strongly connected component in the 2 graph of data dependences.
Reference: [15] <author> V. Sarkar. </author> <title> Automatic partitioning of a program dependence graph into parallel tasks. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 35(5/6), </volume> <month> September </month> <year> 1991. </year>
Reference-contexts: Another minor problem is that loop distribution can make a single loop nest into multiple nests. If each nest needs a barrier synchronization, then loop distribution can be costly for some architectures. Compilers frequently use loop fusion to counteract this <ref> [15] </ref>, but loop fusion is difficult if a unimodular transformation is applied to one of the nests because the compiler may not be able to match the loop bounds. There is a very simple method for converting imperfectly nested loops into perfectly nested loops by introducing conditional statements. <p> Similarly, LD is always as good as or better than no transformations. If LD has been applied and does not contribute to the parallel execution of the DO loop, it is easy to undo LD with loop fusion. 2 This, too, is common practice <ref> [15] </ref>. Therefore, if SFS followed LD is applied to a loop nest, the composition will either contribute to the parallel execution of the loop or it is equivalent to no transformations. SFS Alone.
Reference: [16] <author> Ron Sass and Matt Mutka. </author> <title> Transformations on doubly nested loops. </title> <booktitle> In Proceedings of the International Conference on Parallel Architectures and Compilation Techniques (to appear), </booktitle> <month> August </month> <year> 1994. </year>
Reference-contexts: This makes performing the transformation on imperfect loop nests for loop skewing easy. Nevertheless, it remains complex for general unimodular transformations and it is not clear how often it may improve upon the non-basic-to-basic transformation. The phase method, described in <ref> [16] </ref>, is an attempt to extend UT theory to include loops that are not perfectly nested. It has the advantage that it is only limited by the structure of the loop and not by the desired transformation or the presence of strongly connected components. The phase method in [16], though, can <p> described in <ref> [16] </ref>, is an attempt to extend UT theory to include loops that are not perfectly nested. It has the advantage that it is only limited by the structure of the loop and not by the desired transformation or the presence of strongly connected components. The phase method in [16], though, can only be applied to doubly nested loops, which is a serious practical limitation. It also suffers from the minor problem that a single loop nest may introduce one or two extra barrier synchronizations. Another technique useful in our research is Scalar Forward Substitution (SFS).
Reference: [17] <author> Bruce Shei and Dennis Gannon. Sigmacs: </author> <title> a programmable programming environment. </title> <editor> In Alexandru Nicolau, David Gelernter, Thomas Gross, and David Padua, editors, </editor> <booktitle> Advances in Languages and Compilers for Parallel Processing, </booktitle> <pages> pages 88-108. </pages> <publisher> Pitman Publishing, </publisher> <year> 1991. </year>
Reference-contexts: In order to process the sample data, we developed a program to parse FORTRAN, perform the necessary transformations, and classify the loop nests. We used the parser and database library from the Sigma Toolbox 0.2a (which, in turn, was based on the SIGMACS project) <ref> [7, 10, 17] </ref>.
Reference: [18] <author> Debbie Whitfield and Mary Lou Soffa. </author> <title> An approach to ordering optimizing transformations. </title> <booktitle> In Proceedings of the Second ACM SIGPLAN Symposium on Principles and Practices of Parallel Programming, </booktitle> <pages> pages 137-146, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: As researchers advanced the state of restructuring compilers, it was realized that a single, static ordering decided a priori will not be good in all cases certainly not optimal. Whitfield and Soffa <ref> [18] </ref> discuss this in terms of transformations enabling (or disabling) other transformations. In practice, over a large collection of loops, one might observe that transformation X will frequently enable transformation Y and at the same time observe that transformation Y will enable transformation X.
Reference: [19] <author> Michael E. Wolf. </author> <title> Improving Locality and Parallelism in Nested Loops. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: Wolf <ref> [19] </ref> points out an important condition for the legality of this transformation; one that has frequently been omitted. Informally, the condition ensures that we do not move a statement into the body of a loop if there exists the possibility that the loop body will not be executed. <p> The legality conditions make this method difficult to perform and, because it was developed before UT theory became well understood, it is not clear how the conditions can be merged with the theory. In his Ph.D. thesis, M.E. Wolf <ref> [19] </ref> discusses imperfect loop nests. Similar to M.J. Wolfe, he starts with Abu-Sufah's non-basic-to-basic method but he states that under certain conditions, after the unimodular transformations, some of the if statements can be moved out of the innermost loop (and part of the condition removed). <p> That is, i 0 3 = n 3 (i 0 1 ) is only true at the beginning of the execution of the i 0 3 loop. (See Wolf <ref> [19] </ref> for more details.) 4 Wolf claims ([19], p. 51) that if the unimodular transformation is a skew then the non-perfectly [sic] nested portions would be just moved in and moved out again [to their original positions]. This makes performing the transformation on imperfect loop nests for loop skewing easy.
Reference: [20] <author> Michael Wolfe. </author> <title> Scalar vs. parallel optimizations. </title> <type> Technical Report CS/E 90-010, </type> <institution> Oregon Graduate Institute of Science and Technology, 19600 NW von Neumann Drive, Beaverton, </institution> <address> OR 97006, </address> <year> 1990. </year>
Reference-contexts: In practice, over a large collection of loops, one might observe that transformation X will frequently enable transformation Y and at the same time observe that transformation Y will enable transformation X. Thus, we should not give a fixed order to these two transformations. M.J. Wolfe <ref> [20] </ref> gives examples involving scalar and parallel transformations where a particular ordering is good in one case and bad for another. Likewise there are two examples where the same is true for the reverse ordering.
Reference: [21] <author> Michael J. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> Pitman Publishing, </publisher> <address> 128 Long Acre, London WC2E 9AN, </address> <year> 1989. </year>
Reference-contexts: For some architectures (e.g., superscalar) these conditionals can have a large effect on the execution speed. M.J. Wolfe briefly discusses imperfect loop nests in <ref> [21] </ref>. He begins with the non-basic-to-basic 3 approach but points out that the same dependences that prevent loop distribution will frequently prevent loop interchange. <p> This definition agrees with Abu-Sufah's definition of basic loops [1] and with the concept of perfectly nested used by Banerjee [4], Wolfe <ref> [21] </ref>, and others. Tightly-nested has also been used to describe perfect 6 loop nests [12]. The techniques used to handle loop nests that are not perfect are limited to certain loop structures.
Reference: [22] <author> M.J. Wolfe. </author> <title> Techniques for improving the inherent parallelism in programs. </title> <type> Technical Report 78-929, </type> <institution> Department of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> July </month> <year> 1990. </year>
Reference-contexts: a (i, j)+a (i, k)*a (k, j) end do a recurrence statement, the nest (b) has an induction variable (and a recurrence statement), the nest (c) has multiple DO's, and the dependences in the nest (d) made it impossible to apply any of our transformations. 13 variable substitution (described in <ref> [22] </ref>).
References-found: 22

