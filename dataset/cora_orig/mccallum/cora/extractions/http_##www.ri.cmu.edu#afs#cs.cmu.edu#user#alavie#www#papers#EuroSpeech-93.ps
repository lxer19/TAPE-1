URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/alavie/www/papers/EuroSpeech-93.ps
Refering-URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/alavie/www/publications.html
Root-URL: 
Title: RECENT ADVANCES IN JANUS: A SPEECH TRANSLATION SYSTEM  
Author: M.Woszczyna, N.Coccaro, A.Eisele, A.Lavie, A.McNair, T.Polzin, I.Rogina, C.P.Rose,T.Sloboda, M.Tomita, J.Tsutsumi, N.Aoki-Waibel, A.Waibel, W. Ward 
Affiliation: Carnegie Mellon University University of Karlsruhe  
Abstract: We present recent advances from our efforts in increasing coverage, robustness, generality and speed of JANUS, CMU's speech-to-speech translation system. JANUS is a speaker-independent system which translates spoken utterances in English and also in German into one of German, English or Japanese. The system has been designed around the task of conference registration (CR). It has initially been built based on a speech database of 12 read dialogs, encompassing a vocabulary of around 500 words. We have since been expanding the system along several dimensions to improve speed, robustness and coverage and to move toward spontaneous input. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> L. Osterholtz, A. McNair, I. Rogina, H. Saito, T. Slo-boda, J. Tebelskis, A. Waibel, and M. Woszczyna. </author> <title> Testing Generality in JANUS: A Multi-Lingual Speech to Speech Translation System, </title> <booktitle> volume 1, </booktitle> <pages> pp 209-212. </pages> <month> ICASSP </month> <year> 1992. </year>
Reference-contexts: On the machine translation side, 5.) a cleaner interlingua was designed and syntactic and domain-specific analysis were separated for greater reusability of components and greater quality of trans lation, 6.) a semantic parser was developed to achieve semantic analysis, should more careful analysis fail. The JANUS <ref> [1] </ref> framework as it is presented here also allows us to experiment with components of a speech translation system, in an effort to achieve both robustness and high-quality translation. In the following we describe these efforts and system components that have been developed to date.
Reference: 2. <author> Austin S., Schwartz R. </author> <title> A Comparison of Several Approximate Algorithms for Finding N-best Hypotheses, </title> <booktitle> ICASSP 1991, </booktitle> <volume> volume 1, </volume> <pages> pp 701-704. </pages>
Reference-contexts: Speed and memory requirements have been dramatically improved: Though the amount of hypotheses computed for each utterance was increased from 6 to 100 hypotheses, the time required for their computation was reduced from typically 3 minutes to 3 seconds. This was achieved by implementing the word dependent N-best algorithm <ref> [2] </ref> as a backward pass in the forward backward algorithm: First a fast firstbest search is per formed, saving the scores at each possible word ending. In a second pass, this information is used for aggressive pruning to reduce the search effort for the N-best search.
Reference: 3. <author> O. Schmidbauer and J. Tebelskis. </author> <title> An LVQ based Reference Model for Speaker-Adaptive Speech Recognition. </title> <booktitle> ICASSP 1992, </booktitle> <volume> volume 1, </volume> <pages> pages 441-444. </pages>
Reference-contexts: Based on this list, more computationally expensive language models are then applied to achieve further improvement of recognition accuracy. 2.1. Acoustic modeling For acoustic modeling, several alternative algorithms are being evaluated including TDNN, MS-TDNN, MLP and LVQ <ref> [4, 3] </ref>. In the main JANUS system, an LVQ algorithm with context-dependent phonemes is now used for speaker independent recognition. For each phoneme, there is a context independent set of prototypical vectors.
Reference: 4. <author> J. Tebelskis and A. Waibel. </author> <title> Performance through consistency: MS-TDNNs for large vocabulary continuous speech recognition, </title> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Based on this list, more computationally expensive language models are then applied to achieve further improvement of recognition accuracy. 2.1. Acoustic modeling For acoustic modeling, several alternative algorithms are being evaluated including TDNN, MS-TDNN, MLP and LVQ <ref> [4, 3] </ref>. In the main JANUS system, an LVQ algorithm with context-dependent phonemes is now used for speaker independent recognition. For each phoneme, there is a context independent set of prototypical vectors.
Reference: 5. <author> W. Ward, </author> <title> Understanding Spontaneous Speech, </title> <booktitle> DARPA Speech and Natural Language Workshop 1989, </booktitle> <pages> pp 137-141. </pages>
Reference: 6. <author> J.G. Carbonell and P.J. Hayes, </author> <title> Recovery Strategies for Parsing Extragrammatical Language, </title> <institution> Carnegie-Mellon University Computer Science Technical Report 1984, (CMU-CS-84-107) </institution>
Reference-contexts: Software called MORPHE is also used for morphlogical generation of German. 3.4. Semantic Pattern Based Parsing Our robust semantic parser combines frame based semantics with semantic phrase grammars. We use a frame based parser similar to the DYPAR parser used by Carbonell, et al. to process ill-formed text <ref> [6] </ref>, and the MINDS system previously developed at CMU. Semantic information is represented in a set of frames. Each frame contains a set of slots representing pieces of information. In order to fill the slots in the frames, we use semantic fragment grammars.
Reference: 7. <author> A.J. Jain, A. Waibel, D. Touretzky, </author> <title> PARSEC: A Structured Connectionist Parsing System for Spoken Language, </title> <booktitle> ICASSP 1992, </booktitle> <volume> volume 1, </volume> <pages> pp 205-208. </pages>
Reference-contexts: Subnets correspond to more specific semantic classes of constituents. In this way, the interpretation returned by the parser can be easily mapped onto the interlingua and missing information can be filled by meaningful default values with minimal effort. 3.5. Connectionist Parsing The connectionist parsing system PARSEC <ref> [7] </ref> is used as a fall-back module if the symbolic high precision one fails to analyze the input. The important aspect of the PARSEC system is that it learns to parse sentences from a corpus of training examples.
Reference: 8. <author> T.S. Polzin, </author> <title> Pronoun Resolution. Interaction of Syntactic and Semantic Information in Connectionist Parsing, </title> <type> Thesis, </type> <institution> Carnegie Mellon University, Department of Philosophy, </institution> <note> Computational Linguistics, in preparation. </note>
Reference: 9. <author> Tomita, M. (ed.), </author> <title> Generalized LR Parsing, </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston MA, </address> <year> 1991. </year>
Reference-contexts: Generalized LR Parser The first step of the translation process is syntactic parsing with the Generalized LR Parser/Compiler. The Generalized LR parsing algorithm is an extension of LR parsing with the special device called "Graph-Structured Stack" <ref> [9] </ref>, and it can handle arbitrary context-free grammars while most of the LR efficiency is preserved. A grammar with about 455 rules for general colloquial English is written in a Pseudo Unification formalism, that is similar to Unification Grammar and LFG formalisms.
Reference: 10. <author> Tomita, M. and Nyberg, E.; </author> <title> The Generation Kit and The Transformation Kit: User's Guide Technical Memo, Center for Machine Translation, </title> <institution> Carnegie Mellon University, CMU-CMT-88-MEMO, </institution> <year> 1988 </year>
Reference-contexts: The Interlingua The output of the parser, known as "syntactic f-structure", is then fed into a mapper to produce an Interlingua representation. For the mapper, we use a software tool known as Transformation Kit <ref> [10] </ref>. A mapping grammar with about 300 rules is written for the Conference Registration domain of English. ((PREV-UTTERANCES ((SPEECH-ACT *ACKNOWL) (VALUE *HELLO))) (TIME *PRESENT) (PARTY ((DEFINITE +) (NUMBER *SG) (ANIM -) (TYPE *CONFERENCE) (CONCEPT *OFFICE))) (SPEECH-ACT *IDENTIFY-OTHER)) 3.3. <p> There are about 300 rules in the generation mapping grammar for German, and 230 rules for Japanese. The f-structure is then fed into sentence generation software called "GENKIT" <ref> [10] </ref> to produce a sentence in the target language. A grammar for GENKIT is written in the same formalism as the Generalized LR Parser: phrase structure rules augmented with pseudo unification equations. The GENKIT grammar for general colloquial German has about 90 rules, and Japanese about 60 rules.
Reference: 11. <author> Lavie, A and Tomita, M.; </author> <title> An Efficient Word-Skipping Parsing Algorithm for Context-Free Grammars submitted to 3rd International Workshop on Parsing Technologies (IWPT93) Belguim, </title> <year> 1993. </year>
References-found: 11

