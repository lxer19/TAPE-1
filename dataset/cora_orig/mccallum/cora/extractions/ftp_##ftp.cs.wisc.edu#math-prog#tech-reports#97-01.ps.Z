URL: ftp://ftp.cs.wisc.edu/math-prog/tech-reports/97-01.ps.Z
Refering-URL: 
Root-URL: 
Title: R u t c o r Research  Smooth Methods of Multipliers for Complementarity Problems  
Author: R e p o r t Jonathan Eckstein a Michael C. Ferris b 
Address: 640 Bartholomew Road Piscataway, New Jersey 08854-8003  640 Bartholomew Road, Piscataway, NJ 08854.  Wisconsin, 1210 West Dayton Street Madison, WI, 53706.  
Affiliation: RUTCOR Rutgers Center for Operations Research Rutgers University  of Management and RUTCOR, Rutgers University,  b Department of Computer Sciences, University of  
Email: Email: rrr@rutcor.rutgers.edu  E-mail: jeckstei@rutcor.rutgers.edu  E-mail: ferris@cs.wisc.edu  
Phone: Telephone: 732-445-3804 Telefax: 732-445-5472  
Degree: a Faculty  
Web: http://rutcor.rutgers.edu/ rrr  
Date: RRR 27-96, February 1998  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> H. Attouch and M. Thera. </author> <title> A general duality principle for the sum of two operators. </title> <journal> Journal of Convex Analysis. </journal> <volume> 3 </volume> <pages> 1-24, </pages> <year> 1996. </year>
Reference-contexts: 1 Introduction This paper concerns the solution of the nonlinear complementarity problem (NCP). Let l 2 <ref> [1; 1) and u 2 (1; 1] </ref> , with l u. Define W (l; u) = fx 2 &lt; n j l x ug, suppose W (l; u) D &lt; n , and let F : D ! &lt; n be continuous. <p> In the course of our derivation, Section 2 develops a simple duality framework for pairs of set-valued operators. The framework resembles <ref> [1] </ref>, but allows the two mappings in the pair to operate on different spaces. A similar duality structure for pairs of monotone operators appears in [20]. <p> The framework resembles [1], but allows the two mappings in the pair to operate on different spaces. A similar duality structure for pairs of monotone operators appears in [20]. The main distinction of our approach, as opposed to <ref> [1, 20] </ref>, is to introduce a primal-dual, "saddle-point" formulation, in addition to the standard primal and dual formulations. Towards the end of Section 2, we show how to apply the duality framework to variational inequalities and complementarity problems, refining the framework for variational inequalities that appears in [21, 27]. <p> The duality scheme of <ref> [1] </ref> is similar, with the restrictions X = Y and M = I. <p> Furthermore, the sub-differential of the generalized Lagrangian L : X fi Y ! <ref> [1; +1] </ref> defined by L (x; y) = f (x) + hy; Mxi g fl (y) is precisely K [A; B; M ] = K [@f ; @g; M ]. <p> For i = 1; : : : ; n, let h i be a Bregman function with zone &lt;, and let fc k g 1 k=0 (0; 1) be bounded away from zero. Suppose that the sequences fy k g 1 <ref> [1] </ref> g k=1 [2] g k=1 &lt; n and fffi k g 1 k=0 [0; 1) meet the conditions 1 X c k ffi k max 1; fl fl fl &lt; 1 (38) fl [1] x k+1 fl fl ffi k 8 k 0 (39) [1] ) = y k+1 = <p> Suppose that the sequences fy k g 1 <ref> [1] </ref> g k=1 [2] g k=1 &lt; n and fffi k g 1 k=0 [0; 1) meet the conditions 1 X c k ffi k max 1; fl fl fl &lt; 1 (38) fl [1] x k+1 fl fl ffi k 8 k 0 (39) [1] ) = y k+1 = P k (x k+1 where P k is defined as in (32). Then y k ! y fl = F (x fl ), where x fl is some solution to (1). <p> sequences fy k g 1 <ref> [1] </ref> g k=1 [2] g k=1 &lt; n and fffi k g 1 k=0 [0; 1) meet the conditions 1 X c k ffi k max 1; fl fl fl &lt; 1 (38) fl [1] x k+1 fl fl ffi k 8 k 0 (39) [1] ) = y k+1 = P k (x k+1 where P k is defined as in (32). Then y k ! y fl = F (x fl ), where x fl is some solution to (1). All limit points x 1 of fx k [1] g and fx k [2] <p> k 8 k 0 (39) <ref> [1] </ref> ) = y k+1 = P k (x k+1 where P k is defined as in (32). Then y k ! y fl = F (x fl ), where x fl is some solution to (1). All limit points x 1 of fx k [1] g and fx k [2] g are also solutions of (1), with F (x 1 ) = y fl = F (x fl ). If imrh i = &lt; for all i, then such sequences are guaranteed to exist. Proof. <p> The recursion can be rewritten A k (y k+1 ) + B k (y k+1 ) 3 e k , where A k and B k are defined by (28)-(29). From (40), we have (x k+1 <ref> [1] </ref> ; y k+1 ) 2 F and (x k+1 [2] ; y k+1 ) 2 P k , which yield (y k+1 ; x k+1 [1] ) 2 A k [2] ) 2 B k , courtesy of (28) and P k = B k 1 , as established above. <p> From (40), we have (x k+1 <ref> [1] </ref> ; y k+1 ) 2 F and (x k+1 [2] ; y k+1 ) 2 P k , which yield (y k+1 ; x k+1 [1] ) 2 A k [2] ) 2 B k , courtesy of (28) and P k = B k 1 , as established above. Setting e k : [1] x k+1 [2] for all k 1, whence ke k k ffi k by (39), we have A k (y k+1 <p> 2 F and (x k+1 [2] ; y k+1 ) 2 P k , which yield (y k+1 ; x k+1 <ref> [1] </ref> ) 2 A k [2] ) 2 B k , courtesy of (28) and P k = B k 1 , as established above. Setting e k : [1] x k+1 [2] for all k 1, whence ke k k ffi k by (39), we have A k (y k+1 ) + B k (y k+1 ) 3 e k , and the claim is established. <p> The final statement follows from Proposition 7, even if we were to require ffi k 0, so it only remains to show that all limit points of fx k <ref> [1] </ref> g and fx k [2] g are primal solutions. From (38) and fc k g being bounded away from zero, ffi k ! 0 and e k ! 0. Therefore, fx k [1] g [2] g have the same limit points. <p> require ffi k 0, so it only remains to show that all limit points of fx k <ref> [1] </ref> g and fx k [2] g are primal solutions. From (38) and fc k g being bounded away from zero, ffi k ! 0 and e k ! 0. Therefore, fx k [1] g [2] g have the same limit points. Let x 1 be such that x k [2] ! x 1 Page 18 RRR 27-96 for some infinite set K f0; 1; 2; : : :g. Since F is continuous and y k = F (x k [1] ) for all <p> Therefore, fx k <ref> [1] </ref> g [2] g have the same limit points. Let x 1 be such that x k [2] ! x 1 Page 18 RRR 27-96 for some infinite set K f0; 1; 2; : : :g. Since F is continuous and y k = F (x k [1] ) for all k 1, taking limits over k 2 K yields y fl = F (x 1 ). <p> Proof. First consider the exact iteration (41)-(42). Then we can set x k+1 <ref> [1] </ref> = x k+1 and (39)-(40) will hold for any ffi k 0. The continuous differentiability of F + P k follows from the discussion above. Now consider (43)-(44). In this case, we let x k+1 [2] = x k+1 . <p> Since F and hence F 1 constitute maximal monotone operators, the set F 1 (y) must be closed and convex for every y 2 &lt; n (see e.g. [3]). Thus, (43) guarantees the existence of some x k+1 <ref> [1] </ref> 2 F 1 (P k (x k+1 )) such that kx k+1 [1] x k+1 [2] k ffi k . Thus, (39)-(40) can be satisfied. The analysis of (45)-(46) is similar, except that we have x k+1 [1] = x k+1 , and (45) guarantees the existence of x k+1 <p> Thus, (43) guarantees the existence of some x k+1 <ref> [1] </ref> 2 F 1 (P k (x k+1 )) such that kx k+1 [1] x k+1 [2] k ffi k . Thus, (39)-(40) can be satisfied. The analysis of (45)-(46) is similar, except that we have x k+1 [1] = x k+1 , and (45) guarantees the existence of x k+1 [2] . Since either x k = x k [1] or x k <p> Thus, (43) guarantees the existence of some x k+1 <ref> [1] </ref> 2 F 1 (P k (x k+1 )) such that kx k+1 [1] x k+1 [2] k ffi k . Thus, (39)-(40) can be satisfied. The analysis of (45)-(46) is similar, except that we have x k+1 [1] = x k+1 , and (45) guarantees the existence of x k+1 [2] . Since either x k = x k [1] or x k = x k [2] for every k, the assertion about limit points of fx k g follows from the limit point properties of fx k <p> that kx k+1 <ref> [1] </ref> x k+1 [2] k ffi k . Thus, (39)-(40) can be satisfied. The analysis of (45)-(46) is similar, except that we have x k+1 [1] = x k+1 , and (45) guarantees the existence of x k+1 [2] . Since either x k = x k [1] or x k = x k [2] for every k, the assertion about limit points of fx k g follows from the limit point properties of fx k [1] g and fx k RRR 27-96 Page 19 (41)-(42) constitute a generalized method of multipliers iteration for the complementarity problem (1), <p> Since either x k = x k <ref> [1] </ref> or x k = x k [2] for every k, the assertion about limit points of fx k g follows from the limit point properties of fx k [1] g and fx k RRR 27-96 Page 19 (41)-(42) constitute a generalized method of multipliers iteration for the complementarity problem (1), and by appropriate choice of h, the subproblem function F + P k of (41) can be made differentiable, if F is differentiable. <p> Depending on the problem, these conditions might be difficult to verify. Finally, the dual method's theory does not guarantee convergence of the primal iterates fx k g, fx k <ref> [1] </ref> g, or [2] g, but only makes assertions about limit points. 3.3 Primal-Dual Application to Complementarity The primal-dual method obtained by applying Proposition 6 to T = T PD = K [F; N C ; I] combines and improves upon the best theoretical features of the primal and dual methods.
Reference: [2] <author> S. C. Billups, S. P. Dirkse, and M. C. Ferris. </author> <title> A comparison of large scale mixed complementarity problem solvers. </title> <journal> Computational Optimization and Applications, </journal> <volume> 7 </volume> <pages> 3-25, </pages> <year> 1997. </year>
Reference-contexts: For i = 1; : : : ; n, let h i be a Bregman function with zone &lt;, and let fc k g 1 k=0 (0; 1) be bounded away from zero. Suppose that the sequences fy k g 1 [1] g k=1 <ref> [2] </ref> g k=1 &lt; n and fffi k g 1 k=0 [0; 1) meet the conditions 1 X c k ffi k max 1; fl fl fl &lt; 1 (38) fl [1] x k+1 fl fl ffi k 8 k 0 (39) [1] ) = y k+1 = P k (x <p> Then y k ! y fl = F (x fl ), where x fl is some solution to (1). All limit points x 1 of fx k [1] g and fx k <ref> [2] </ref> g are also solutions of (1), with F (x 1 ) = y fl = F (x fl ). If imrh i = &lt; for all i, then such sequences are guaranteed to exist. Proof. <p> The recursion can be rewritten A k (y k+1 ) + B k (y k+1 ) 3 e k , where A k and B k are defined by (28)-(29). From (40), we have (x k+1 [1] ; y k+1 ) 2 F and (x k+1 <ref> [2] </ref> ; y k+1 ) 2 P k , which yield (y k+1 ; x k+1 [1] ) 2 A k [2] ) 2 B k , courtesy of (28) and P k = B k 1 , as established above. Setting e k : [1] x k+1 [2] for all <p> From (40), we have (x k+1 [1] ; y k+1 ) 2 F and (x k+1 <ref> [2] </ref> ; y k+1 ) 2 P k , which yield (y k+1 ; x k+1 [1] ) 2 A k [2] ) 2 B k , courtesy of (28) and P k = B k 1 , as established above. Setting e k : [1] x k+1 [2] for all k 1, whence ke k k ffi k by (39), we have A k (y k+1 ) + B k (y <p> (x k+1 <ref> [2] </ref> ; y k+1 ) 2 P k , which yield (y k+1 ; x k+1 [1] ) 2 A k [2] ) 2 B k , courtesy of (28) and P k = B k 1 , as established above. Setting e k : [1] x k+1 [2] for all k 1, whence ke k k ffi k by (39), we have A k (y k+1 ) + B k (y k+1 ) 3 e k , and the claim is established. <p> The final statement follows from Proposition 7, even if we were to require ffi k 0, so it only remains to show that all limit points of fx k [1] g and fx k <ref> [2] </ref> g are primal solutions. From (38) and fc k g being bounded away from zero, ffi k ! 0 and e k ! 0. Therefore, fx k [1] g [2] g have the same limit points. Let x 1 be such that x k [2] ! x 1 Page 18 <p> k 0, so it only remains to show that all limit points of fx k [1] g and fx k <ref> [2] </ref> g are primal solutions. From (38) and fc k g being bounded away from zero, ffi k ! 0 and e k ! 0. Therefore, fx k [1] g [2] g have the same limit points. Let x 1 be such that x k [2] ! x 1 Page 18 RRR 27-96 for some infinite set K f0; 1; 2; : : :g. <p> [1] g and fx k <ref> [2] </ref> g are primal solutions. From (38) and fc k g being bounded away from zero, ffi k ! 0 and e k ! 0. Therefore, fx k [1] g [2] g have the same limit points. Let x 1 be such that x k [2] ! x 1 Page 18 RRR 27-96 for some infinite set K f0; 1; 2; : : :g. Since F is continuous and y k = F (x k [1] ) for all k 1, taking limits over k 2 K yields y fl = F (x 1 ). <p> Since F is continuous and y k = F (x k [1] ) for all k 1, taking limits over k 2 K yields y fl = F (x 1 ). From y k+1 = P k (x k+1 <ref> [2] </ref> ), we also have x k+1 [2] 2 B k (y k+1 ), and hence [2] + c k rh (y k ) rh (y k+1 ) ; y k+1 2 N C for all k 0. <p> Since F is continuous and y k = F (x k [1] ) for all k 1, taking limits over k 2 K yields y fl = F (x 1 ). From y k+1 = P k (x k+1 <ref> [2] </ref> ), we also have x k+1 [2] 2 B k (y k+1 ), and hence [2] + c k rh (y k ) rh (y k+1 ) ; y k+1 2 N C for all k 0. <p> From y k+1 = P k (x k+1 <ref> [2] </ref> ), we also have x k+1 [2] 2 B k (y k+1 ), and hence [2] + c k rh (y k ) rh (y k+1 ) ; y k+1 2 N C for all k 0. <p> Proof. First consider the exact iteration (41)-(42). Then we can set x k+1 [1] = x k+1 and (39)-(40) will hold for any ffi k 0. The continuous differentiability of F + P k follows from the discussion above. Now consider (43)-(44). In this case, we let x k+1 <ref> [2] </ref> = x k+1 . Since F and hence F 1 constitute maximal monotone operators, the set F 1 (y) must be closed and convex for every y 2 &lt; n (see e.g. [3]). <p> Thus, (43) guarantees the existence of some x k+1 [1] 2 F 1 (P k (x k+1 )) such that kx k+1 [1] x k+1 <ref> [2] </ref> k ffi k . Thus, (39)-(40) can be satisfied. The analysis of (45)-(46) is similar, except that we have x k+1 [1] = x k+1 , and (45) guarantees the existence of x k+1 [2] . <p> 2 F 1 (P k (x k+1 )) such that kx k+1 [1] x k+1 <ref> [2] </ref> k ffi k . Thus, (39)-(40) can be satisfied. The analysis of (45)-(46) is similar, except that we have x k+1 [1] = x k+1 , and (45) guarantees the existence of x k+1 [2] . Since either x k = x k [1] or x k = x k [2] for every k, the assertion about limit points of fx k g follows from the limit point properties of fx k [1] g and fx k RRR 27-96 Page 19 (41)-(42) constitute a generalized <p> Thus, (39)-(40) can be satisfied. The analysis of (45)-(46) is similar, except that we have x k+1 [1] = x k+1 , and (45) guarantees the existence of x k+1 <ref> [2] </ref> . Since either x k = x k [1] or x k = x k [2] for every k, the assertion about limit points of fx k g follows from the limit point properties of fx k [1] g and fx k RRR 27-96 Page 19 (41)-(42) constitute a generalized method of multipliers iteration for the complementarity problem (1), and by appropriate choice of h, the <p> Depending on the problem, these conditions might be difficult to verify. Finally, the dual method's theory does not guarantee convergence of the primal iterates fx k g, fx k [1] g, or <ref> [2] </ref> g, but only makes assertions about limit points. 3.3 Primal-Dual Application to Complementarity The primal-dual method obtained by applying Proposition 6 to T = T PD = K [F; N C ; I] combines and improves upon the best theoretical features of the primal and dual methods. <p> We partition the error vector e k of (20), which in this case lies in &lt; n fi &lt; n , into subvectors e k <ref> [2] </ref> 2 &lt; n . <p> = (x k ; y k ), Bregman function ^ h, and operator T PD , takes the form F (x k+1 ) + y k+1 + c k r ~ h (x k+1 ) r ~ h (x k ) = e k x k+1 + N C 1 <ref> [2] </ref> ; (49) Page 20 RRR 27-96 where h (x) = P n i=1 h i (x i ), as before. If we set e k [2] 0, then (49) is equivalent to B k (y k+1 ) 3 x k+1 , where B k is defined as in (29) for <p> + c k r ~ h (x k+1 ) r ~ h (x k ) = e k x k+1 + N C 1 <ref> [2] </ref> ; (49) Page 20 RRR 27-96 where h (x) = P n i=1 h i (x i ), as before. If we set e k [2] 0, then (49) is equivalent to B k (y k+1 ) 3 x k+1 , where B k is defined as in (29) for the dual method. <p> Since our implementation is preliminary and MATLAB is an interpreted language, we do not list run times. The "primal residual" column gives the final value of kx k mid As can be seen from the tables, and by comparison with the results in <ref> [2] </ref>, the algorithm is fairly robust. For all but 3 of the 79 instance/starting point combinations attempted, it terminates within 100 iterations with a primal residual of 10 6 or less, indicating convergence to a solution.
Reference: [3] <editor> H. Brezis. Operateurs Maximaux Monotones et Semi-Groupes de Contractions dans les Espaces de Hilbert. </editor> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1973. </year>
Reference-contexts: restrict our attention to the monotone case in which F satisfies hF (x) F (y); x yi 0 8 x; y 2 &lt; n : (4) This assumption will allow us to model (1) as the problem of finding a root of the sum of two monotone operators (see e.g. <ref> [3] </ref>), as will be explained in Section 2. To find such a root, we then apply generalized proximal algorithms based on Bregman functions [6, 7, 9, 12, 17, 18, 33]. <p> The fundamental problem customarily associated with a monotone operator T is that of finding a zero or root, that is, some x 2 X such that 0 2 T (x) (see e.g. <ref> [3, 31] </ref>). 2.1 The Duality Framework Suppose we are given an operator A on a Hilbert space X, an operator B on a Hilbert space Y , and a linear mapping M : X ! Y . We will denote such a triple by P (A; B; M ). <p> Now consider (43)-(44). In this case, we let x k+1 [2] = x k+1 . Since F and hence F 1 constitute maximal monotone operators, the set F 1 (y) must be closed and convex for every y 2 &lt; n (see e.g. <ref> [3] </ref>). Thus, (43) guarantees the existence of some x k+1 [1] 2 F 1 (P k (x k+1 )) such that kx k+1 [1] x k+1 [2] k ffi k . Thus, (39)-(40) can be satisfied.
Reference: [4] <author> R. E. Bruck. </author> <title> An iterative solution of a variational inequality for certain monotone operators in Hilbert space. </title> <journal> Bulletin of the American Mathematical Society, </journal> <volume> 81 </volume> <pages> 890-892, </pages> <year> 1975. </year>
Reference-contexts: A3. T has the following two properties (see, e.g. [6, 7, 8]): (i) If f (x k ; y k )g T , fx k g S, and fx k g is convergent, then fy k g has a limit point; (ii) T is paramonotone <ref> [4, 8] </ref>, that is, (x; y); (x 0 ; y 0 ) 2 T and hx x 0 ; y y 0 i = 0 collectively imply that (x; y 0 ) 2 T .
Reference: [5] <author> R. S. Burachik and A. N. Iusem. </author> <title> A generalized proximal point algorithm for the nonlinear complementarity problem. </title> <type> Working paper, </type> <institution> Instituto de Matematica Pura e Aplicada, Rio de Janeiro, </institution> <year> 1995. </year>
Reference-contexts: To find such a root, we then apply generalized proximal algorithms based on Bregman functions [6, 7, 9, 12, 17, 18, 33]. A number of recent papers <ref> [5, 6, 8] </ref> have stressed the ability of proximal terms arising from appropriately-formulated Bregman functions to act like barrier functions, giving rise to "interior point" proximal methods for variational inequality problems. Such methods are derived by applying Bregman proximal methods to a primal formulation of (1) or (2). <p> The primal-dual formulation yields a new proximal method of multipliers for (1), along the lines of the proximal method of multipliers for convex programming (e.g. [30]). This primal-dual method combines the best theoretical features of primal methods in the spirit of <ref> [5, 6, 8] </ref> with the best features of the new dual method. Some preliminary computational results on the MCPLIB [14] suite of test problems are given in Section 4. <p> Since rh approaches infinity on the boundary of C, it acts as a barrier function that simplifies the subproblems by removing boundary effects. This phenomenon has already been noted in numerous prior works, including <ref> [5, 8] </ref>. However, setting S = int C also has drawbacks. First, in attempting to apply Proposition 6, S = int C rules out invoking assumption A1, forcing one to appeal to assumptions A2 or A3, each of which places restrictions on the maximal monotone operator T . <p> The approximation condition (25) is much more practical to check than the corresponding condition in [7]. We cannot apply Proposition 7 to show existence of fx k g in this setting, because S 6 dom T . However, suitable existence results may be found in <ref> [5, 6, 7, 8] </ref>. Note that in the case l &gt; 1 and u &lt; +1, the condition on P 1 immediate consequence of P 1 k=0 c k ke k k &lt; 1, and becomes redundant.
Reference: [6] <author> R. S. Burachik and A. N. Iusem. </author> <title> A generalized proximal point algorithm for the variational inequality problem in a Hilbert space. </title> <type> Working paper, </type> <institution> Instituto de Matematica Pura e Aplicada, Rio de Janeiro, </institution> <year> 1995. </year>
Reference-contexts: To find such a root, we then apply generalized proximal algorithms based on Bregman functions <ref> [6, 7, 9, 12, 17, 18, 33] </ref>. A number of recent papers [5, 6, 8] have stressed the ability of proximal terms arising from appropriately-formulated Bregman functions to act like barrier functions, giving rise to "interior point" proximal methods for variational inequality problems. <p> To find such a root, we then apply generalized proximal algorithms based on Bregman functions [6, 7, 9, 12, 17, 18, 33]. A number of recent papers <ref> [5, 6, 8] </ref> have stressed the ability of proximal terms arising from appropriately-formulated Bregman functions to act like barrier functions, giving rise to "interior point" proximal methods for variational inequality problems. Such methods are derived by applying Bregman proximal methods to a primal formulation of (1) or (2). <p> The primal-dual formulation yields a new proximal method of multipliers for (1), along the lines of the proximal method of multipliers for convex programming (e.g. [30]). This primal-dual method combines the best theoretical features of primal methods in the spirit of <ref> [5, 6, 8] </ref> with the best features of the new dual method. Some preliminary computational results on the MCPLIB [14] suite of test problems are given in Section 4. <p> We now describe the algorithm of [18] for solving the inclusion 0 2 T (x), where T is a maximal monotone operator on &lt; n . Earlier treatments of closely related algorithms may be found in <ref> [6, 7, 9, 12, 17, 23, 33] </ref> The algorithm in [18] requires two auxiliary constructs, a function h and a set S. <p> Let any one of the following assumptions A1-A3 hold: A1. S domT . A2. T = @f , the subdifferential mapping of some closed proper convex function f : &lt; n ! &lt; [ f+1g. A3. T has the following two properties (see, e.g. <ref> [6, 7, 8] </ref>): (i) If f (x k ; y k )g T , fx k g S, and fx k g is convergent, then fy k g has a limit point; (ii) T is paramonotone [4, 8], that is, (x; y); (x 0 ; y 0 ) 2 T and <p> The approximation condition (25) is much more practical to check than the corresponding condition in [7]. We cannot apply Proposition 7 to show existence of fx k g in this setting, because S 6 dom T . However, suitable existence results may be found in <ref> [5, 6, 7, 8] </ref>. Note that in the case l &gt; 1 and u &lt; +1, the condition on P 1 immediate consequence of P 1 k=0 c k ke k k &lt; 1, and becomes redundant.

Reference: [8] <author> Y. Censor, A. N. Iusem, and S. A. Zenios. </author> <title> An interior-point method with Bregman functions for the variational inequality problem with paramonotone operators. </title> <type> Working paper, </type> <institution> University of Haifa, </institution> <year> 1994. </year>
Reference-contexts: To find such a root, we then apply generalized proximal algorithms based on Bregman functions [6, 7, 9, 12, 17, 18, 33]. A number of recent papers <ref> [5, 6, 8] </ref> have stressed the ability of proximal terms arising from appropriately-formulated Bregman functions to act like barrier functions, giving rise to "interior point" proximal methods for variational inequality problems. Such methods are derived by applying Bregman proximal methods to a primal formulation of (1) or (2). <p> The primal-dual formulation yields a new proximal method of multipliers for (1), along the lines of the proximal method of multipliers for convex programming (e.g. [30]). This primal-dual method combines the best theoretical features of primal methods in the spirit of <ref> [5, 6, 8] </ref> with the best features of the new dual method. Some preliminary computational results on the MCPLIB [14] suite of test problems are given in Section 4. <p> Let any one of the following assumptions A1-A3 hold: A1. S domT . A2. T = @f , the subdifferential mapping of some closed proper convex function f : &lt; n ! &lt; [ f+1g. A3. T has the following two properties (see, e.g. <ref> [6, 7, 8] </ref>): (i) If f (x k ; y k )g T , fx k g S, and fx k g is convergent, then fy k g has a limit point; (ii) T is paramonotone [4, 8], that is, (x; y); (x 0 ; y 0 ) 2 T and <p> A3. T has the following two properties (see, e.g. [6, 7, 8]): (i) If f (x k ; y k )g T , fx k g S, and fx k g is convergent, then fy k g has a limit point; (ii) T is paramonotone <ref> [4, 8] </ref>, that is, (x; y); (x 0 ; y 0 ) 2 T and hx x 0 ; y y 0 i = 0 collectively imply that (x; y 0 ) 2 T . <p> Since rh approaches infinity on the boundary of C, it acts as a barrier function that simplifies the subproblems by removing boundary effects. This phenomenon has already been noted in numerous prior works, including <ref> [5, 8] </ref>. However, setting S = int C also has drawbacks. First, in attempting to apply Proposition 6, S = int C rules out invoking assumption A1, forcing one to appeal to assumptions A2 or A3, each of which places restrictions on the maximal monotone operator T . <p> The approximation condition (25) is much more practical to check than the corresponding condition in [7]. We cannot apply Proposition 7 to show existence of fx k g in this setting, because S 6 dom T . However, suitable existence results may be found in <ref> [5, 6, 7, 8] </ref>. Note that in the case l &gt; 1 and u &lt; +1, the condition on P 1 immediate consequence of P 1 k=0 c k ke k k &lt; 1, and becomes redundant.
Reference: [9] <author> Y. Censor and S. A. Zenios. </author> <title> The proximal minimization algorithm with D-functions. </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 73 </volume> <pages> 451-464, </pages> <year> 1992. </year>
Reference-contexts: To find such a root, we then apply generalized proximal algorithms based on Bregman functions <ref> [6, 7, 9, 12, 17, 18, 33] </ref>. A number of recent papers [5, 6, 8] have stressed the ability of proximal terms arising from appropriately-formulated Bregman functions to act like barrier functions, giving rise to "interior point" proximal methods for variational inequality problems. <p> We now describe the algorithm of [18] for solving the inclusion 0 2 T (x), where T is a maximal monotone operator on &lt; n . Earlier treatments of closely related algorithms may be found in <ref> [6, 7, 9, 12, 17, 23, 33] </ref> The algorithm in [18] requires two auxiliary constructs, a function h and a set S. <p> Examples of pairs (h; S) meeting these conditions may be found in <ref> [9, 13, 17, 33] </ref>, and many references therein. In particular, [13] gives some general sufficient conditions for (h; S) to satisfy B1-B7. We now state the main result of [18].
Reference: [10] <author> C. Chen and O. L. Mangasarian. </author> <title> Smoothing methods for convex inequalities and linear complementarity problems. </title> <journal> Mathematical Programming, </journal> <volume> 78 </volume> <pages> 51-70, </pages> <year> 1995. </year>
Reference-contexts: In producing sequences of subproblems consisting of differentiable nonlinear equations, our algorithms bear some resemblance to recently proposed smoothing methods for the LCP and NCP <ref> [10, 11, 22] </ref>. However, such methods are akin to pure penalty methods in constrained optimization | they have a penalty parameter that must be driven to infinity to obtain convergence. <p> chose h as in (37) with q = 3=2, and set ~ h (x) = (1=2)x &gt; Dx, D being a diagonal matrix determined via D ii = max (0:1 krF ii (x 0 )k ; 10:0) This choice corresponds to standard problem scaling mechanisms that have proven successful in <ref> [10, 15] </ref>. In the interest of further improving scaling, we also define the function P k slightly differently from (32). Instead, we use P k (x) = P (x; y k ) where P (x; y) = (rh) D being the diagonal matrix defined above.
Reference: [11] <author> C. Chen and O. L. Mangasarian. </author> <title> A class of smoothing functions for nonlinear and mixed complementarity problems. </title> <journal> Computational Optimization and Applications, </journal> <volume> 5 </volume> <pages> 97-138, </pages> <year> 1996. </year>
Reference-contexts: In producing sequences of subproblems consisting of differentiable nonlinear equations, our algorithms bear some resemblance to recently proposed smoothing methods for the LCP and NCP <ref> [10, 11, 22] </ref>. However, such methods are akin to pure penalty methods in constrained optimization | they have a penalty parameter that must be driven to infinity to obtain convergence.
Reference: [12] <author> G. Chen and M. Teboulle. </author> <title> A convergence analysis of proximal-like minimization algorithms using Bregman functions. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 3 </volume> <pages> 538-543, </pages> <year> 1993. </year>
Reference-contexts: To find such a root, we then apply generalized proximal algorithms based on Bregman functions <ref> [6, 7, 9, 12, 17, 18, 33] </ref>. A number of recent papers [5, 6, 8] have stressed the ability of proximal terms arising from appropriately-formulated Bregman functions to act like barrier functions, giving rise to "interior point" proximal methods for variational inequality problems. <p> We now describe the algorithm of [18] for solving the inclusion 0 2 T (x), where T is a maximal monotone operator on &lt; n . Earlier treatments of closely related algorithms may be found in <ref> [6, 7, 9, 12, 17, 23, 33] </ref> The algorithm in [18] requires two auxiliary constructs, a function h and a set S.
Reference: [13] <author> A. R. De Pierro and A. N. Iusem. </author> <title> A relaxed version of Bregman's method for convex programming. </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 51 </volume> <pages> 421-440, </pages> <year> 1986. </year>
Reference-contexts: Examples of pairs (h; S) meeting these conditions may be found in <ref> [9, 13, 17, 33] </ref>, and many references therein. In particular, [13] gives some general sufficient conditions for (h; S) to satisfy B1-B7. We now state the main result of [18]. <p> Examples of pairs (h; S) meeting these conditions may be found in [9, 13, 17, 33], and many references therein. In particular, <ref> [13] </ref> gives some general sufficient conditions for (h; S) to satisfy B1-B7. We now state the main result of [18].
Reference: [14] <author> S. P. Dirkse and M. C. Ferris. MCPLIB: </author> <title> A collection of nonlinear mixed complementarity problems. </title> <journal> Optimization Methods and Software, </journal> <volume> 5 </volume> <pages> 319-345, </pages> <year> 1995. </year>
Reference-contexts: This primal-dual method combines the best theoretical features of primal methods in the spirit of [5, 6, 8] with the best features of the new dual method. Some preliminary computational results on the MCPLIB <ref> [14] </ref> suite of test problems are given in Section 4. <p> We coded a version of the algorithm (50)-(51) in MATLAB, and used it to solve the problems in the MCPLIB collection <ref> [14] </ref>, exploiting the interface developed in [19]. We note that most of the problems in the collection do not satisfy the monotonicity condition (5) postulated in our theory. In fact, only the problems cycle and optcont31 are definitely known to be monotone. <p> k ) &gt; D (x x k ) + j=1 0 1 q j + c k D jj ; 0 A = y k+1 q j + c k j ; 0 : The initial values x 0 of the primal variables are specified in the MCPLIB test suite <ref> [14] </ref>. For the initial multipliers, we used the formula y 0 = P (x 0 ; F (x 0 )); kP (x 0 ; F (x 0 ))k 10 6 F (x 0 ); otherwise, where P is defined by (52).
Reference: [15] <author> S. P. Dirkse and M. C. Ferris. </author> <title> The PATH solver: A non-monotone stabilization scheme for mixed complementarity problems. </title> <journal> Optimization Methods and Software, </journal> <volume> 5 </volume> <pages> 123-156, </pages> <year> 1995. </year>
Reference-contexts: chose h as in (37) with q = 3=2, and set ~ h (x) = (1=2)x &gt; Dx, D being a diagonal matrix determined via D ii = max (0:1 krF ii (x 0 )k ; 10:0) This choice corresponds to standard problem scaling mechanisms that have proven successful in <ref> [10, 15] </ref>. In the interest of further improving scaling, we also define the function P k slightly differently from (32). Instead, we use P k (x) = P (x; y k ) where P (x; y) = (rh) D being the diagonal matrix defined above.
Reference: [16] <author> J. Eckstein. </author> <title> Splitting Methods for Monotone Operators, with Applications to Parallel Optimization. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <address> Cambridge, MA, </address> <year> 1989. </year> <title> Report LIDS-TH-1877, Laboratory for Information and Decision Systems, </title> <publisher> M.I.T. </publisher>
Reference-contexts: For the development in Section 3, we will require only the special case X = Y = &lt; n and M = I, but we consider the general P (A; B; M ) in order to make connections to <ref> [16, 20] </ref> and other previous work.
Reference: [17] <author> J. Eckstein. </author> <title> Nonlinear proximal point algorithms using Bregman functions, with applications to convex programming. </title> <journal> Mathematics of Operations Research, </journal> <volume> 18 </volume> <pages> 202-226, </pages> <year> 1993. </year>
Reference-contexts: To find such a root, we then apply generalized proximal algorithms based on Bregman functions <ref> [6, 7, 9, 12, 17, 18, 33] </ref>. A number of recent papers [5, 6, 8] have stressed the ability of proximal terms arising from appropriately-formulated Bregman functions to act like barrier functions, giving rise to "interior point" proximal methods for variational inequality problems. <p> Therefore, we can use a standard algorithm such as Newton's method to solve these subproblems. A similar phenomenon has already been pointed out for smooth convex programming problems in [24]. That paper notes that Page 2 RRR 27-96 one of the augmented Lagrangian methods proposed in <ref> [17] </ref> yields a twice-differentiable augmented Lagrangian, as opposed to the classical once-differentiable augmented Lagrangian for inequality constraints (e.g. [30]). In producing sequences of subproblems consisting of differentiable nonlinear equations, our algorithms bear some resemblance to recently proposed smoothing methods for the LCP and NCP [10, 11, 22]. <p> We now describe the algorithm of [18] for solving the inclusion 0 2 T (x), where T is a maximal monotone operator on &lt; n . Earlier treatments of closely related algorithms may be found in <ref> [6, 7, 9, 12, 17, 23, 33] </ref> The algorithm in [18] requires two auxiliary constructs, a function h and a set S. <p> Examples of pairs (h; S) meeting these conditions may be found in <ref> [9, 13, 17, 33] </ref>, and many references therein. In particular, [13] gives some general sufficient conditions for (h; S) to satisfy B1-B7. We now state the main result of [18]. <p> Then if im rh = &lt; n , sequences fz k g 1 1 k=0 &lt; n jointly conforming to (20) exist. Proof. Set e k = 0 for all k, and consult case (i) of <ref> [17, Theorem 4] </ref>. 2 We now consider applying Proposition 6 with either T = T P , T = T D , or T = T PD . <p> can restate the requirements that (rh) 1 be differentiable and that (35) hold as r 2 h i (y i ) &gt; 0 8 y i 6= 0 i = 1; : : : ; n y i !0 (36) One possible choice of a Bregman function meeting these conditions <ref> [17, Example 2] </ref> is h (y) = q i=1 q Page 16 RRR 27-96 then inverting once again produces the smoothed exterior function P k . In this case, rh i (y i ) = (sgn y i )jy i j q1 q2 has the desired properties. <p> The three methods bear much the same relationship as the proximal minimization algorithms, methods of multipliers, and proximal methods of multipliers presented for convex optimization in [30] (for the special case h (x) = (1=2)kxk 2 ) and later in <ref> [17] </ref> (for general h). We therefore refer to the dual method as a "method of multipliers," and the primal-dual method as a "proximal method of multipliers." 4 Computational Results on the MCPLIB Test Suite We conclude with some preliminary computational results for the proximal method of multipliers.
Reference: [18] <author> J. Eckstein. </author> <title> Approximate iterations in Bregman-function-based proximal algorithms. </title> <type> Research Report 12-96, </type> <institution> Rutgers Center for Operations Research, Rutgers University, </institution> <address> New Brunswick, NJ, </address> <year> 1996. </year>
Reference-contexts: To find such a root, we then apply generalized proximal algorithms based on Bregman functions <ref> [6, 7, 9, 12, 17, 18, 33] </ref>. A number of recent papers [5, 6, 8] have stressed the ability of proximal terms arising from appropriately-formulated Bregman functions to act like barrier functions, giving rise to "interior point" proximal methods for variational inequality problems. <p> We can attempt to solve (1) by applying any method for finding the root of a monotone operator to either T P , T D , or T PD . In this paper, we employ only the Bregman-function-based proximal algorithm of <ref> [18] </ref>, and study the algorithms for (1) that result when it is applied to T P , T D , and T PD . We now describe the algorithm of [18] for solving the inclusion 0 2 T (x), where T is a maximal monotone operator on &lt; n . <p> In this paper, we employ only the Bregman-function-based proximal algorithm of <ref> [18] </ref>, and study the algorithms for (1) that result when it is applied to T P , T D , and T PD . We now describe the algorithm of [18] for solving the inclusion 0 2 T (x), where T is a maximal monotone operator on &lt; n . Earlier treatments of closely related algorithms may be found in [6, 7, 9, 12, 17, 23, 33] The algorithm in [18] requires two auxiliary constructs, a function h and a set <p> We now describe the algorithm of <ref> [18] </ref> for solving the inclusion 0 2 T (x), where T is a maximal monotone operator on &lt; n . Earlier treatments of closely related algorithms may be found in [6, 7, 9, 12, 17, 23, 33] The algorithm in [18] requires two auxiliary constructs, a function h and a set S. <p> Given two points x, y 2 &lt; n and a function h differentiable at y, we define D h (x; y) = h (x) h (y) hrh (y); x yi : (19) We then say that h is a Bregman function with zone S if the following conditions hold <ref> [18] </ref>: RRR 27-96 Page 9 B1. S &lt; n is a convex open set. B2. h : &lt; n ! &lt; [ f+1g is finite and continuous on S. B3. h is strictly convex on S. B4. h is continuously differentiable on S. B5. <p> Examples of pairs (h; S) meeting these conditions may be found in [9, 13, 17, 33], and many references therein. In particular, [13] gives some general sufficient conditions for (h; S) to satisfy B1-B7. We now state the main result of <ref> [18] </ref>. Proposition 6 Let T be a maximal monotone operator on &lt; n , and let h be a Bregman function with zone S, where S " ri dom T 6= ;. Let any one of the following assumptions A1-A3 hold: A1. S domT . A2. <p> Proof. By minor reformulation of <ref> [18, Theorem 1] </ref>. 2 Similar forms for the error sequence can be found for example in [25].
Reference: [19] <author> M. C. Ferris and T. F. Rutherford. </author> <title> Accessing realistic complementarity problems within Matlab. </title> <editor> In G. Di Pillo and F. Giannessi, editors, </editor> <booktitle> Proceedings of Nonlinear Optimization and Applications Workshop, </booktitle> <address> Erice June 1995, </address> <publisher> Plenum Press, </publisher> <address> New York, </address> <year> 1996. </year>
Reference-contexts: We coded a version of the algorithm (50)-(51) in MATLAB, and used it to solve the problems in the MCPLIB collection [14], exploiting the interface developed in <ref> [19] </ref>. We note that most of the problems in the collection do not satisfy the monotonicity condition (5) postulated in our theory. In fact, only the problems cycle and optcont31 are definitely known to be monotone.
Reference: [20] <author> M. Fukushima. </author> <title> The primal Douglas-Rachford splitting algorithm for a class of monotone mappings with application to the traffic equilibrium problem. </title> <journal> Mathematical Programming, </journal> <volume> 72 </volume> <pages> 1-15, </pages> <year> 1996. </year> <pages> RRR 27-96 Page 27 </pages>
Reference-contexts: In the course of our derivation, Section 2 develops a simple duality framework for pairs of set-valued operators. The framework resembles [1], but allows the two mappings in the pair to operate on different spaces. A similar duality structure for pairs of monotone operators appears in <ref> [20] </ref>. The main distinction of our approach, as opposed to [1, 20], is to introduce a primal-dual, "saddle-point" formulation, in addition to the standard primal and dual formulations. <p> The framework resembles [1], but allows the two mappings in the pair to operate on different spaces. A similar duality structure for pairs of monotone operators appears in [20]. The main distinction of our approach, as opposed to <ref> [1, 20] </ref>, is to introduce a primal-dual, "saddle-point" formulation, in addition to the standard primal and dual formulations. Towards the end of Section 2, we show how to apply the duality framework to variational inequalities and complementarity problems, refining the framework for variational inequalities that appears in [21, 27]. <p> For the development in Section 3, we will require only the special case X = Y = &lt; n and M = I, but we consider the general P (A; B; M ) in order to make connections to <ref> [16, 20] </ref> and other previous work. <p> Some typical sufficient conditions for T P to be maximal are that A and B be maximal, that MM &gt; be an isomorphism of Y , thus guaranteeing maximality of M &gt; BM (see [21, Proposition 4.1] or <ref> [20, Proposition 3.2] </ref>), and a condition such as dom A " int dom (BM ) 6= ;, in order to ensure maximality of the sum T P = A + M &gt; BM [29]. This last condition can be weakened somewhat if X is finite-dimensional.
Reference: [21] <author> D. Gabay. </author> <title> Applications of the method of multipliers to variational inequalities. </title> <editor> In M. Fortin and R. Glowinski, editors, </editor> <title> Augmented Lagrangian Methods: Applications to the Solution of Boundary Value Problems. </title> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1983. </year>
Reference-contexts: In contrast, we emphasize dual and primal-dual formulations. Applying Bregman proximal methods to such formulations yields augmented-Lagrangian-like algorithms, or "methods of multipliers." In the dual case, we obtain a class of methods generalizing <ref> [21, "ALG1"] </ref>. By careful choice of Bregman function, we generate methods which involve solving (provided that F is differentiable) a once-differentiable system of equations at each iteration, as opposed to a nonsmooth system, as in [21]. <p> By careful choice of Bregman function, we generate methods which involve solving (provided that F is differentiable) a once-differentiable system of equations at each iteration, as opposed to a nonsmooth system, as in <ref> [21] </ref>. Therefore, we can use a standard algorithm such as Newton's method to solve these subproblems. A similar phenomenon has already been pointed out for smooth convex programming problems in [24]. <p> Towards the end of Section 2, we show how to apply the duality framework to variational inequalities and complementarity problems, refining the framework for variational inequalities that appears in <ref> [21, 27] </ref>. Section 3 combines the duality framework of Section 2 with Bregman-function-based proximal theory and shows how to produce new, smooth methods of multipliers for (1). <p> Some typical sufficient conditions for T P to be maximal are that A and B be maximal, that MM &gt; be an isomorphism of Y , thus guaranteeing maximality of M &gt; BM (see <ref> [21, Proposition 4.1] </ref> or [20, Proposition 3.2]), and a condition such as dom A " int dom (BM ) 6= ;, in order to ensure maximality of the sum T P = A + M &gt; BM [29]. This last condition can be weakened somewhat if X is finite-dimensional. <p> F 1 and N C 1 may both be general set-valued operators on &lt; n , in the sense of Section 2. Although the notation is different, this dual problem is essentially the same dual proposed in <ref> [21, 27] </ref>. The formulation (15) may appear somewhat awkward, but we will not have to work with it directly in a computational setting. It will, however, prove very useful in deriving algorithms.
Reference: [22] <author> S. A. Gabriel and J. J. </author> <title> More. Smoothing of mixed complementarity problems. </title> <editor> In M. C. Ferris and J. S. Pang, editors, </editor> <title> Complementarity and Variational Problems: State of the Art, </title> <publisher> SIAM Publications, </publisher> <address> Philadelphia, </address> <year> 1997. </year>
Reference-contexts: In producing sequences of subproblems consisting of differentiable nonlinear equations, our algorithms bear some resemblance to recently proposed smoothing methods for the LCP and NCP <ref> [10, 11, 22] </ref>. However, such methods are akin to pure penalty methods in constrained optimization | they have a penalty parameter that must be driven to infinity to obtain convergence.
Reference: [23] <author> S. Kabbadj. Methodes Proximales Entropiques. </author> <type> Doctoral thesis, </type> <institution> Universite de Montpel-lier II Sciences et Techniques du Languedoc, </institution> <year> 1994. </year>
Reference-contexts: We now describe the algorithm of [18] for solving the inclusion 0 2 T (x), where T is a maximal monotone operator on &lt; n . Earlier treatments of closely related algorithms may be found in <ref> [6, 7, 9, 12, 17, 23, 33] </ref> The algorithm in [18] requires two auxiliary constructs, a function h and a set S.
Reference: [24] <author> K. C. Kiwiel. </author> <title> On the twice differentiable cubic augmented Lagrangian. </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 88 </volume> <pages> 233-236, </pages> <year> 1996. </year>
Reference-contexts: Therefore, we can use a standard algorithm such as Newton's method to solve these subproblems. A similar phenomenon has already been pointed out for smooth convex programming problems in <ref> [24] </ref>. That paper notes that Page 2 RRR 27-96 one of the augmented Lagrangian methods proposed in [17] yields a twice-differentiable augmented Lagrangian, as opposed to the classical once-differentiable augmented Lagrangian for inequality constraints (e.g. [30]). <p> The case q = 3=2 leads to an expression resembling the convex programming cubic augmented Lagrangian discussed in <ref> [24] </ref>. Bregman proximal method smoothes the set-valued, nonsmooth N C term in the original problem F (x) + N C (x) 3 0 into the differentiable term P k of the subproblem computation. First, we take N C , and "dualize" it to obtain its inverse N C 1 .
Reference: [25] <author> Z.-Q. Luo and P. Tseng. </author> <title> Error bounds and convergence analysis of feasible descent methods: A general approach. </title> <journal> Annals of Operations Research, </journal> <volume> 46 </volume> <pages> 157-178, </pages> <year> 1993. </year>
Reference-contexts: Proof. By minor reformulation of [18, Theorem 1]. 2 Similar forms for the error sequence can be found for example in <ref> [25] </ref>. Note that the condition (22) is implied by the more easily-verified condition 1 X c k ke k kkz k k &lt; 1: (23) Furthermore, when S or dom T is bounded, fz k g is necessarily bounded, and (21) implies (23) and (22).
Reference: [26] <author> G. J. Minty. </author> <title> Monotone (nonlinear) operators in Hilbert space. </title> <journal> Duke Mathematics Journal, </journal> <volume> 29 </volume> <pages> 341-346, </pages> <year> 1962. </year>
Reference-contexts: If A and B are maximal, T 1 is also maximal. The linear map T 2 is also maximal <ref> [26] </ref>, and maximality of T 1 + T 2 then follows from [29, Theorem 1 (a)]. 2 Note that it is also straightforward to prove the theorem from first principles, without invoking the deep analytical machinery of [26, 29]. <p> The linear map T 2 is also maximal [26], and maximality of T 1 + T 2 then follows from [29, Theorem 1 (a)]. 2 Note that it is also straightforward to prove the theorem from first principles, without invoking the deep analytical machinery of <ref> [26, 29] </ref>. <p> From [29] we have that b F + N C must be maximal. Now, the openness of D and the analysis of <ref> [26, Theorem 4] </ref> imply that b F agrees in value with F on D C = dom N C = dom T P , so it follows that b F + N C = T P . 2 Page 8 RRR 27-96 Proposition 4 Suppose F is a continuous monotone function
Reference: [27] <author> U. Mosco. </author> <title> Dual variational inequalities. </title> <journal> Journal of Mathematical Analysis and its Applications, </journal> <volume> 40 </volume> <pages> 202-206, </pages> <year> 1972. </year>
Reference-contexts: Towards the end of Section 2, we show how to apply the duality framework to variational inequalities and complementarity problems, refining the framework for variational inequalities that appears in <ref> [21, 27] </ref>. Section 3 combines the duality framework of Section 2 with Bregman-function-based proximal theory and shows how to produce new, smooth methods of multipliers for (1). <p> F 1 and N C 1 may both be general set-valued operators on &lt; n , in the sense of Section 2. Although the notation is different, this dual problem is essentially the same dual proposed in <ref> [21, 27] </ref>. The formulation (15) may appear somewhat awkward, but we will not have to work with it directly in a computational setting. It will, however, prove very useful in deriving algorithms.
Reference: [28] <author> R. T. Rockafellar. </author> <title> Convex Analysis. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, New Jersey, </address> <year> 1970. </year>
Reference-contexts: Then the primal formulation is equivalent to the optimization problem min f (x) + g (M x): (10) Similarly, the dual formulation is equivalent to min f fl (M &gt; y) + g fl (y); (11) where " fl " denotes the convex conjugacy operation <ref> [28, Section 12] </ref>. Furthermore, the sub-differential of the generalized Lagrangian L : X fi Y ! [1; +1] defined by L (x; y) = f (x) + hy; Mxi g fl (y) is precisely K [A; B; M ] = K [@f ; @g; M ]. <p> Proof. Let b F be some maximal extension of F into a monotone operator [32, Proposition 12.6]. Then we have dom b F D C = dom N C 6= ;, and therefore ri dom b F " ri dom N C 6= ;, where "ri" denotes relative interior <ref> [28, Section 6] </ref>. From [29] we have that b F + N C must be maximal. <p> Now consider assumption P1. In this case, we have T P = rf + N C = rf + @ffi ( jC) = @(f + ffi ( jC)), where the last equality follows from <ref> [28, Theorem 23.8] </ref> and dom f C = dom ffi ( jC) 6= ;. Therefore, assumption A2 of Proposition 6 is satisfied. Page 12 RRR 27-96 Alternatively, assume that P2 holds.
Reference: [29] <author> R. T. Rockafellar. </author> <title> On the maximality of sums of nonlinear monotone operators. </title> <journal> Transactions of the American Mathematical Society, </journal> <volume> 149 </volume> <pages> 75-88, </pages> <year> 1970. </year>
Reference-contexts: an isomorphism of Y , thus guaranteeing maximality of M &gt; BM (see [21, Proposition 4.1] or [20, Proposition 3.2]), and a condition such as dom A " int dom (BM ) 6= ;, in order to ensure maximality of the sum T P = A + M &gt; BM <ref> [29] </ref>. This last condition can be weakened somewhat if X is finite-dimensional. The analysis of the dual formulation is similar. <p> If A and B are maximal, T 1 is also maximal. The linear map T 2 is also maximal [26], and maximality of T 1 + T 2 then follows from <ref> [29, Theorem 1 (a)] </ref>. 2 Note that it is also straightforward to prove the theorem from first principles, without invoking the deep analytical machinery of [26, 29]. <p> The linear map T 2 is also maximal [26], and maximality of T 1 + T 2 then follows from [29, Theorem 1 (a)]. 2 Note that it is also straightforward to prove the theorem from first principles, without invoking the deep analytical machinery of <ref> [26, 29] </ref>. <p> Then we have dom b F D C = dom N C 6= ;, and therefore ri dom b F " ri dom N C 6= ;, where "ri" denotes relative interior [28, Section 6]. From <ref> [29] </ref> we have that b F + N C must be maximal. <p> By appealing to (17), it is clear that the conditions (18) on y are equivalent to y 2 ri dom (N C 1 ). Therefore, we have ri dom (F 1 (I)) " ri dom (N C 1 ) 6= ;. The maximality of N C and <ref> [29] </ref> then imply the maximality of T D = F 1 (I) + N C 1 . 2 Note that if l &gt; 1 and u &lt; +1, the conditions (18) are void, and Proposition 4 requires only maximality of F . <p> The operations of subtracting a constant rh (y k ) and scaling by 1=c k preserve this maximality. Finally, since dom rh = &lt; n , we also have maximality of B = B k from <ref> [29] </ref>. Invoking Proposition 1, the problem dual to (27), or equivalently A k (y) + B k (y) 3 0, is of the form A k 1 (x) + B k 1 (x) 3 0, where we are interchanging the notational roles of "x" and "y".
Reference: [30] <author> R. T. Rockafellar. </author> <title> Augmented Lagrangians and applications of the proximal point algorithm in convex programming. </title> <journal> Mathematics of Operations Research, </journal> <volume> 1 </volume> <pages> 97-116, </pages> <year> 1976. </year>
Reference-contexts: A similar phenomenon has already been pointed out for smooth convex programming problems in [24]. That paper notes that Page 2 RRR 27-96 one of the augmented Lagrangian methods proposed in [17] yields a twice-differentiable augmented Lagrangian, as opposed to the classical once-differentiable augmented Lagrangian for inequality constraints (e.g. <ref> [30] </ref>). In producing sequences of subproblems consisting of differentiable nonlinear equations, our algorithms bear some resemblance to recently proposed smoothing methods for the LCP and NCP [10, 11, 22]. <p> The primal-dual formulation yields a new proximal method of multipliers for (1), along the lines of the proximal method of multipliers for convex programming (e.g. <ref> [30] </ref>). This primal-dual method combines the best theoretical features of primal methods in the spirit of [5, 6, 8] with the best features of the new dual method. Some preliminary computational results on the MCPLIB [14] suite of test problems are given in Section 4. <p> Thus, the iteration (50)-(51) has all the theoretical advantages of the primal and dual approaches, and the disadvantages of neither. The three methods bear much the same relationship as the proximal minimization algorithms, methods of multipliers, and proximal methods of multipliers presented for convex optimization in <ref> [30] </ref> (for the special case h (x) = (1=2)kxk 2 ) and later in [17] (for general h).
Reference: [31] <author> R. T. Rockafellar. </author> <title> Monotone operators and the proximal point algorithm. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 14 </volume> <pages> 877-898, </pages> <year> 1976. </year>
Reference-contexts: The fundamental problem customarily associated with a monotone operator T is that of finding a zero or root, that is, some x 2 X such that 0 2 T (x) (see e.g. <ref> [3, 31] </ref>). 2.1 The Duality Framework Suppose we are given an operator A on a Hilbert space X, an operator B on a Hilbert space Y , and a linear mapping M : X ! Y . We will denote such a triple by P (A; B; M ).
Reference: [32] <author> R. T. Rockafellar and R. J.-B. Wets. </author> <title> Variational Analysis. </title> <note> Springer-Verlag, to appear, </note> <year> 1997. </year>
Reference-contexts: Then T P = F + N C is maximal monotone. Proof. Let b F be some maximal extension of F into a monotone operator <ref> [32, Proposition 12.6] </ref>. Then we have dom b F D C = dom N C 6= ;, and therefore ri dom b F " ri dom N C 6= ;, where "ri" denotes relative interior [28, Section 6].
Reference: [33] <author> M. Teboulle. </author> <title> Entropic proximal mappings with applications to nonlinear programming. </title> <journal> Mathematics of Operations Research, </journal> <volume> 17 </volume> <pages> 670-690, </pages> <year> 1992. </year>
Reference-contexts: To find such a root, we then apply generalized proximal algorithms based on Bregman functions <ref> [6, 7, 9, 12, 17, 18, 33] </ref>. A number of recent papers [5, 6, 8] have stressed the ability of proximal terms arising from appropriately-formulated Bregman functions to act like barrier functions, giving rise to "interior point" proximal methods for variational inequality problems. <p> We now describe the algorithm of [18] for solving the inclusion 0 2 T (x), where T is a maximal monotone operator on &lt; n . Earlier treatments of closely related algorithms may be found in <ref> [6, 7, 9, 12, 17, 23, 33] </ref> The algorithm in [18] requires two auxiliary constructs, a function h and a set S. <p> Examples of pairs (h; S) meeting these conditions may be found in <ref> [9, 13, 17, 33] </ref>, and many references therein. In particular, [13] gives some general sufficient conditions for (h; S) to satisfy B1-B7. We now state the main result of [18].
References-found: 32

