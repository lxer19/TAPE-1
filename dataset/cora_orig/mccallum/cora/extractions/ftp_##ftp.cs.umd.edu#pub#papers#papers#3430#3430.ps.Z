URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3430/3430.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Email: fwak,pughg@cs.umd.edu  
Title: A Unifying Framework for Iteration Reordering Transformations  
Author: Wayne Kelly and William Pugh 
Date: February 27, 1995  
Address: College Park, MD 20742  
Affiliation: Department of Computer Science University of Maryland,  
Abstract: We present a framework for unifying iteration reordering transformations such as loop interchange, loop distribution, skewing, tiling, index set splitting and statement reordering. The framework is based on the idea that a transformation can be represented as a mapping from the original iteration space to a new iteration space. The framework is designed to provide a uniform way to represent and reason about transformations. We also provide algorithms to test the legality of mappings, and to generate optimized code for mappings. 
Abstract-found: 1
Intro-found: 1
Reference: [AK87] <author> J. R. Allen and K. Kennedy. </author> <title> Automatic translation of Fortran programs to vector form. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(4) </volume> <pages> 491-542, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: 1 Introduction Optimizing compilers reorder iterations of statements to improve instruction scheduling, register use, and cache utilization, and to expose parallelism. Many different reordering transformations have been developed and studied, such as loop interchange, loop distribution, skewing, tiling, index set splitting and statement reordering <ref> [AK87, Wol89b, Wol90, CK92] </ref>. Each of these transformations has its own special legality checks and transformation rules. These checks and rules make it hard to analyze or predict the effects of compositions of these transformations, without performing the transformations and analyzing the resulting code. <p> By generalizing in these ways, we can represent a much broader set of reordering transformations, including any transformation that can be obtained by some combination of loop interchange, loop reversal, loop skewing, statement reordering, loop distribution, loop fusion, loop blocking 1 (or tiling) <ref> [AK87] </ref> and index set splitting 1 [Ban79]. pings. 1.2 Overview Our framework is designed to provide a uniform way to represent and reason about reordering transformations. The framework itself is not designed to decide which transformation should be applied.
Reference: [Ban79] <author> U. Banerjee. </author> <title> Speedup of Ordinary Programs. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, U. of Illinois at Urbana-Champaign, </institution> <month> October </month> <year> 1979. </year>
Reference-contexts: By generalizing in these ways, we can represent a much broader set of reordering transformations, including any transformation that can be obtained by some combination of loop interchange, loop reversal, loop skewing, statement reordering, loop distribution, loop fusion, loop blocking 1 (or tiling) [AK87] and index set splitting 1 <ref> [Ban79] </ref>. pings. 1.2 Overview Our framework is designed to provide a uniform way to represent and reason about reordering transformations. The framework itself is not designed to decide which transformation should be applied.
Reference: [Ban90] <author> U. Banerjee. </author> <title> Unimodular transformations of double loops. </title> <booktitle> In Proc. of the 3rd Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 192-219, </pages> <address> Irvine, CA, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: Each of these transformations has its own special legality checks and transformation rules. These checks and rules make it hard to analyze or predict the effects of compositions of these transformations, without performing the transformations and analyzing the resulting code. Unimodular transformations <ref> [Ban90, WL91] </ref> go some way towards solving this problem. Unimodular transformations is a unified framework that is able to describe any transformation that can be obtained by composing loop interchange, loop skewing, and loop reversal. <p> Note: Unimodular transformations include loop interchange, skewing and reversal <ref> [Ban90, WL91] </ref>. <p> For example, if the mapping is [i 1 ; i 2 ] ! [i 1 + i 2 ; i 1 ] then i 1 is replaced by j 2 and i 2 is replaced by j 1 j 2 . 6 Related Work The framework of Unimodular transformations <ref> [Ban90, WL91] </ref> has the same goal as our work, in that it attempts to provide a unified framework for describing loop transformations.
Reference: [CK92] <author> Steve Carr and Ken Kennedy. </author> <title> Compiler blockability of numerical algorithms. </title> <booktitle> In Proceedings Supercomputing'92, </booktitle> <pages> pages 114-125, </pages> <address> Minneapolis, Minnesota, </address> <month> Nov </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Optimizing compilers reorder iterations of statements to improve instruction scheduling, register use, and cache utilization, and to expose parallelism. Many different reordering transformations have been developed and studied, such as loop interchange, loop distribution, skewing, tiling, index set splitting and statement reordering <ref> [AK87, Wol89b, Wol90, CK92] </ref>. Each of these transformations has its own special legality checks and transformation rules. These checks and rules make it hard to analyze or predict the effects of compositions of these transformations, without performing the transformations and analyzing the resulting code.
Reference: [Fea92] <author> Paul Feautrier. </author> <title> Some efficient solutions to the affine scheduling problem, Part II, Multidimensional time. </title> <journal> Int. J. of Parallel Programming, </journal> <volume> 21(6), </volume> <month> Dec </month> <year> 1992. </year>
Reference-contexts: Because it uses only single level affine schedules and requires that all dependences be carried by the outer loop, it can only be applied to programs that can be executed in linear time on a parallel machine. Methods to generate efficient code were not given. Paul Feautrier <ref> [Fea92] </ref> generates schedules that are similar in form to our mappings but have slightly different semantics. His methods are designed to generate a schedule that produces code with a "maximal" amount of parallelism.
Reference: [KP94a] <author> Wayne Kelly and William Pugh. </author> <title> Determining schedules based on performance estimation. </title> <journal> Parallel Processing Letters, </journal> <volume> 4(3) </volume> <pages> 205-219, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: It is this surrounding system that is finally responsible for deciding which transformation should be applied. The framework does however provide some algorithms that would aid the surrounding system in its task. <ref> [KP94a] </ref> is an exam ple of a surrounding system that uses our framework. 1 Our current implementation cannot handle all cases of these transformations. The rest of this paper is organized as follows. In Section 2 we describe how dependences and mappings are represented.
Reference: [KP94b] <author> Wayne Kelly and William Pugh. </author> <title> Finding legal reordering transformations using mappings. </title> <booktitle> In Lecture Notes in Computer Science 892: Seventh International Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Ithaca, NY, </address> <month> August </month> <year> 1994. </year> <note> Springer-Verlag. </note>
Reference-contexts: Transformation: 8s p 2 D, replace T p by (T p j C) [ (T p j :C) so that the new program performs exactly the same set of computations as the original program. This can be similarly verified using the Omega library. We have also developed techniques <ref> [KP94b] </ref> to give a closed form characterization of the set of legal map pings. 5 Optimized Code Generation In this section we describe an algorithm to generate efficient source code for a mapping.
Reference: [LP92] <author> Wei Li and Keshav Pingali. </author> <title> A singular loop transformation framework based on non-singular matrices. </title> <booktitle> In 5th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 249-260, </pages> <institution> Yale University, </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: It therefore cannot represent some important transformations such as loop fusion, loop distribution and statement reordering. Unimodular transformations are generalized in <ref> [LP92, Ram92] </ref> to include mappings that are invertible but not unimodular. This allows the resulting programs to have steps in their loops, which can be useful for optimizing locality. Uni-modular transformations are combined with blocking in [WL91, ST92].
Reference: [Lu91] <author> Lee-Chung Lu. </author> <title> A unified framework for systematic loop transformations. </title> <booktitle> In Proc. of the 3rd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 28-38, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: division and modulo operations (provided the denominator is a known integer constant). * We require the mapping to be invertable, but not necessarily unimodular. * We allow the dimensionality of the old and new iteration spaces to be different. * We allow the mapping to be piecewise (as suggested by <ref> [Lu91] </ref>): we can specify a mapping T p as i T pi jC pi where the C pi 's are disjoint sets which together contain all points in the iteration space of statement s p .
Reference: [Pug91] <author> William Pugh. </author> <title> Uniform techniques for loop optimization. </title> <booktitle> In 1991 International Conference on Supercomputing, </booktitle> <pages> pages 341-352, </pages> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: See <ref> [Pug91] </ref> for a more thorough description. 2.2 Control dependence We require that any if-then-else constructs be converted into guarded assignment statements or that they be treated as atomic statements. We also require that all loop bounds be affine functions of surrounding loop variables and symbolic constants. <p> This allows the resulting programs to have steps in their loops, which can be useful for optimizing locality. Uni-modular transformations are combined with blocking in [WL91, ST92]. A similar approach, although not using a unimodular framework, is described in [Wol89a]. Pugh <ref> [Pug91] </ref> gives techniques to represent loop fusion, loop distribution and statement reordering in addition to the transformations representable by uni-modular transformations.
Reference: [Pug92] <author> William Pugh. </author> <title> The Omega test: a fast and practical integer programming algorithm for dependence analysis. </title> <journal> Communications of the ACM, </journal> <volume> 8 </volume> <pages> 102-114, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Since our framework includes loop fusion, they are not sufficient for our purposes either. We evaluate and represent dependences exactly using affine constraints over integer variables. We use the Omega Library <ref> [Pug92, PW92] </ref> to manipulate and simplify these constraints. The following is a brief description of integer tu ple relations and dependence relations. 2.1 Integer tuple relations and sets An integer k-tuple is simply a point in Z k . A tuple relation is a mapping from tuples to tuples.
Reference: [PW92] <author> William Pugh and David Wonnacott. </author> <title> Going beyond integer programming with the Omega test to eliminate false data dependences. </title> <type> Technical Report CS-TR-3191, </type> <institution> Dept. of Computer Science, University of Maryland, College Park, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: In Section 4 we describe an algorithm that tests whether a mapping is legal. In Section 5 we describe our code generation algorithm. This algorithm takes a mapping and produces optimized code corresponding to the transformation represented by that mapping. By making use of the gist operation <ref> [PW92] </ref> we are able to produce code with a smaller number of conditionals and loop bounds than would otherwise be necessary. <p> Since our framework includes loop fusion, they are not sufficient for our purposes either. We evaluate and represent dependences exactly using affine constraints over integer variables. We use the Omega Library <ref> [Pug92, PW92] </ref> to manipulate and simplify these constraints. The following is a brief description of integer tu ple relations and dependence relations. 2.1 Integer tuple relations and sets An integer k-tuple is simply a point in Z k . A tuple relation is a mapping from tuples to tuples. <p> An alternative approach is to annotate the dependence information in such a way that certain dependences are ignored under the presumption that they can be removed if necessary. 2.4 gist, approx and hull We make use of the gist operation that was originally developed in <ref> [PW92] </ref>. The gist operation is designed to operate on two relations, each of which is represented by a single conjunction of constraints. Intuitively, (gist p given q) is defined as the new information contained in p, given that we already know q.
Reference: [Ram92] <author> J. Ramanujam. </author> <title> Non-unimodular transformations of nested loops. </title> <booktitle> In Supercomputing `92, </booktitle> <pages> pages 214-223, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: It therefore cannot represent some important transformations such as loop fusion, loop distribution and statement reordering. Unimodular transformations are generalized in <ref> [LP92, Ram92] </ref> to include mappings that are invertible but not unimodular. This allows the resulting programs to have steps in their loops, which can be useful for optimizing locality. Uni-modular transformations are combined with blocking in [WL91, ST92].
Reference: [ST92] <author> Vivek Sarkar and Radhika Thekkath. </author> <title> A general framework for iteration-reordering loop transformations. </title> <booktitle> In ACM SIGPLAN'92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 175-187, </pages> <address> San Francisco, California, </address> <month> Jun </month> <year> 1992. </year>
Reference-contexts: Unimodular transformations are generalized in [LP92, Ram92] to include mappings that are invertible but not unimodular. This allows the resulting programs to have steps in their loops, which can be useful for optimizing locality. Uni-modular transformations are combined with blocking in <ref> [WL91, ST92] </ref>. A similar approach, although not using a unimodular framework, is described in [Wol89a]. Pugh [Pug91] gives techniques to represent loop fusion, loop distribution and statement reordering in addition to the transformations representable by uni-modular transformations.
Reference: [WL91] <author> Michael E. Wolf and Monica S. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In ACM SIGPLAN'91 Conference on Programming Language Design and Implementation, </booktitle> <year> 1991. </year>
Reference-contexts: Each of these transformations has its own special legality checks and transformation rules. These checks and rules make it hard to analyze or predict the effects of compositions of these transformations, without performing the transformations and analyzing the resulting code. Unimodular transformations <ref> [Ban90, WL91] </ref> go some way towards solving this problem. Unimodular transformations is a unified framework that is able to describe any transformation that can be obtained by composing loop interchange, loop skewing, and loop reversal. <p> Note: Unimodular transformations include loop interchange, skewing and reversal <ref> [Ban90, WL91] </ref>. <p> For example, if the mapping is [i 1 ; i 2 ] ! [i 1 + i 2 ; i 1 ] then i 1 is replaced by j 2 and i 2 is replaced by j 1 j 2 . 6 Related Work The framework of Unimodular transformations <ref> [Ban90, WL91] </ref> has the same goal as our work, in that it attempts to provide a unified framework for describing loop transformations. <p> Unimodular transformations are generalized in [LP92, Ram92] to include mappings that are invertible but not unimodular. This allows the resulting programs to have steps in their loops, which can be useful for optimizing locality. Uni-modular transformations are combined with blocking in <ref> [WL91, ST92] </ref>. A similar approach, although not using a unimodular framework, is described in [Wol89a]. Pugh [Pug91] gives techniques to represent loop fusion, loop distribution and statement reordering in addition to the transformations representable by uni-modular transformations.
Reference: [Wol89a] <author> Michael Wolfe. </author> <title> More iteration space tiling. </title> <booktitle> In Proc. Supercomputing 89, </booktitle> <pages> pages 655-664, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: This allows the resulting programs to have steps in their loops, which can be useful for optimizing locality. Uni-modular transformations are combined with blocking in [WL91, ST92]. A similar approach, although not using a unimodular framework, is described in <ref> [Wol89a] </ref>. Pugh [Pug91] gives techniques to represent loop fusion, loop distribution and statement reordering in addition to the transformations representable by uni-modular transformations.
Reference: [Wol89b] <author> Michael Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> Pitman Publishing, </publisher> <address> London, </address> <year> 1989. </year>
Reference-contexts: 1 Introduction Optimizing compilers reorder iterations of statements to improve instruction scheduling, register use, and cache utilization, and to expose parallelism. Many different reordering transformations have been developed and studied, such as loop interchange, loop distribution, skewing, tiling, index set splitting and statement reordering <ref> [AK87, Wol89b, Wol90, CK92] </ref>. Each of these transformations has its own special legality checks and transformation rules. These checks and rules make it hard to analyze or predict the effects of compositions of these transformations, without performing the transformations and analyzing the resulting code.
Reference: [Wol90] <author> Michael Wolfe. </author> <title> Massive parallelism through program restructuring. </title> <booktitle> In Symposium on Frontiers on Massively Parallel Computation, </booktitle> <pages> pages 407-415, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Optimizing compilers reorder iterations of statements to improve instruction scheduling, register use, and cache utilization, and to expose parallelism. Many different reordering transformations have been developed and studied, such as loop interchange, loop distribution, skewing, tiling, index set splitting and statement reordering <ref> [AK87, Wol89b, Wol90, CK92] </ref>. Each of these transformations has its own special legality checks and transformation rules. These checks and rules make it hard to analyze or predict the effects of compositions of these transformations, without performing the transformations and analyzing the resulting code.
Reference: [Wol91] <author> Michael Wolfe. </author> <title> The tiny loop restructuring research tool. </title> <booktitle> In Proc of 1991 International Conference on Parallel Processing, </booktitle> <address> pages II-46 II-53, </address> <year> 1991. </year>
Reference-contexts: version refers to the nesting of the loops around the inner most statement). do 20 k = 1, n 10 a (i,k) = a (i,k)/a (k,k) 20 a (i,j) = a (i,j)-a (k,j)*a (i,k) Michael Wolfe notes that this transformation requires imperfect triangular loop interchange, distribution, and index set splitting <ref> [Wol91] </ref>.
References-found: 19

