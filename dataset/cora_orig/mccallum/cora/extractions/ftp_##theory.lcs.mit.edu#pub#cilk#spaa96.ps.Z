URL: ftp://theory.lcs.mit.edu/pub/cilk/spaa96.ps.Z
Refering-URL: http://theory.lcs.mit.edu/~cilk/abstracts/spaa96.html
Root-URL: 
Email: rdb@cs.utexas.edu  fathena,cfj,cel,randallg@lcs.mit.edu  
Title: An Analysis of Dag-Consistent Distributed Shared-Memory Algorithms  
Author: Robert D. Blumofe Matteo Frigo Christopher F. Joerg Charles E. Leiserson Keith H. Randall 
Address: Austin, Texas 78712  545 Technology Square Cambridge, Massachusetts 02139  
Affiliation: Department of Computer Sciences The University of Texas at Austin  MIT Laboratory for Computer Science  
Abstract: In this paper, we analyze the performance of parallel multi-threaded algorithms that use dag-consistent distributed shared memory. Specifically, we analyze execution time, page faults, and space requirements for multithreaded algorithms executed by a work-stealing thread scheduler and the BACKER algorithm for maintaining dag consistency. We prove that if the accesses to the backing store are random and independent (the BACKER algorithm actually uses hashing), the expected execution time T P (C) of a fully strict multithreaded computation on P processors, each with a LRU cache of C pages, is O(T 1 (C)=P + mCT ), where T 1 (C) is the total work of the computation including page faults, T is its critical-path length excluding page faults, and m is the minimum page transfer time. As a corollary to this theorem, we show that the expected number F P (C) of page faults incurred by a computation executed on P processors can be related to the number F 1 (C) of serial page faults by the formula F P (C) F 1 (C) + O(CPT ). Finally, we give simple bounds on the number of page faults and the space requirements for regular divide-and-conquer algorithms. We use these bounds to analyze parallel multithreaded algorithms for matrix multiplication and LU-decomposition. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Sarita V. Adve and Mark D. Hill. </author> <title> Weak ordering anew definition. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-14, </pages> <address> Seattle, Washington, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: Relaxed shared-memory consistency models are motivated by the fact that sequential consistency [22] and various forms of processor consistency [16] are too expensive to implement in a distributed setting. (Even modern SMP's do not implement sequential consistency.) Relaxed models, such as location consistency [14] and various forms of release consistency <ref> [1, 13, 15] </ref>, ensure consistency (to varying degrees) only when explicit synchronization operations occur, such as the acquisition or release of a lock.
Reference: [2] <author> Mustaque Ahamad, Phillip W. Hutto, and Ranjit John. </author> <title> Implementing and programming causal distributed shared memory. </title> <booktitle> In Proceedings of the 11th International Conference on Distributed Computing systems, </booktitle> <pages> pages 274-281, </pages> <address> Arlington, Texas, </address> <month> May </month> <year> 1991. </year> <month> 11 </month>
Reference-contexts: Causal memory <ref> [2] </ref> ensures consistency only to the extent that if a process A reads a value written by another process B, then all subsequent operations by A must appear to occur after the write by B.
Reference: [3] <author> Monica Beltrametti, Kenneth Bobey, and John R. Zorbas. </author> <title> The control mechanism for the Myrias parallel computer system. </title> <journal> Computer Architecture News, </journal> <volume> 16(4) </volume> <pages> 21-30, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: By leveraging this high-level knowledge, the BACKER algorithm in con junction with the work-stealing scheduler is able to execute multi-threaded algorithms with the performance bounds shown here. The BLAZE parallel language [25] and the Myrias parallel computer <ref> [3] </ref> define a high-level relaxed consistency model much like dag consistency, but we do not know of any efficient implementation of either of these systems. After an extensive literature search, we are aware of no other distributed shared memory with analytical performance bounds for any nontrivial algorithms.
Reference: [4] <author> Brian N. Bershad, Matthew J. Zekauskas, and Wayne A. Sawdon. </author> <title> The Midway distributed shared memory system. </title> <booktitle> In Digest of Papers from the Thirty-Eighth IEEE Computer Society International Conference (Spring COMPCON), </booktitle> <pages> pages 528-537, </pages> <address> San Francisco, California, </address> <month> February </month> <year> 1993. </year>
Reference-contexts: Most DSM's implement one of these relaxed consistency models [11, 18, 21, 27], though some implement a fixed collection of consistency models <ref> [4] </ref>, while others merely implement a collection of mechanisms on top of which users write their own DSM consistency policies [23, 26]. All of these consistency models and the DSM's that implement these models take a low-level view of a parallel program as a collection of cooperating processes.
Reference: [5] <author> Guy E. Blelloch. </author> <title> Programming parallel algorithms. </title> <journal> Communications of the ACM, </journal> <volume> 39(3), </volume> <month> March </month> <year> 1996. </year>
Reference-contexts: Any mul-tithreaded algorithm can be measured in terms of its work and critical-path length <ref> [5, 9, 10, 20] </ref>. Consider the multithreaded computation that results when a given multithreaded algorithm is used to solve a given input problem.
Reference: [6] <author> Guy E. Blelloch, Phillip B. Gibbons, and Yossi Matias. </author> <title> Provably efficient scheduling for languages with fine-grained parallelism. </title> <booktitle> In Proceedings of the Seventh Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 1-12, </pages> <address> Santa Barbara, California, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: It recursively factors A 00 into L 00 U 00 . Then, it uses back substitution to solve 4 In recent work, Blelloch, Gibbons, and Matias <ref> [6] </ref> have shown that series-parallel dag computations can be scheduled to achieve substantially better space bounds than we report here. For example, they give a bound of S P (n) = O (n 2 + Plg 2 n) for matrix multiplication.
Reference: [7] <author> Robert D. Blumofe. </author> <title> Executing Multithreaded Programs Efficiently. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <month> September </month> <year> 1995. </year>
Reference-contexts: 1 Introduction In recent work [8, 17], we have proposed dag-consistent distributed shared memory as a virtual-memory model for multi-threaded parallel-programming systems such as Cilk, a C-based multithreaded language and runtime system <ref> [7, 9, 17] </ref>. A multi-threaded program defines a partial execution order on its instructions, and we view this partial order as a directed acyclic graph or dag. <p> In this paper, we analyze the execution time, page faults, and space requirements of multithreaded algorithms written with this consistency model when the execution is scheduled by the randomized work-stealing scheduler from <ref> [7, 10] </ref> and dag consistency is maintained by the BACKER coherence algorithm from [8]. A multithreaded algorithm is a collection of thread definitions. Analogous to a procedure definition, a thread definition is a block of serial code, possibly with conditional and looping constructions. <p> The resource requirements of a multithreaded algorithm when used to solve a given input problem are modeled, in graph-theoretic terms, by a multithreaded computation <ref> [7] </ref>. A multithreaded computation is composed of two structures: a spawn tree of threads and a dag of instructions. The spawn tree of threads is the parallel analogue of a call tree of procedures. <p> Specifically, for any such algorithm and any number P of processors, the randomized work-stealing scheduler executes the algorithm in expected time O (T 1 (n)=P + T (n)) <ref> [7, 10] </ref>. The randomized work-stealing sched-uler operates as follows. Each processor maintains a ready deque (doubly-ended queue) of threads from which work is obtained. When a thread is spawned, the parent thread is suspended and put on the bottom of the deque and execution commences on the spawned child thread. <p> We anticipate that it should be possible to memory-map files and use our existing dag-consistency mechanisms to provide a parallel, asynchronous I/O capability for Cilk. We are also currently working on supporting dag-consistent shared memory in our Cilk-NOW runtime system <ref> [7] </ref> which executes Cilk programs in an adaptively parallel and fault-tolerant manner on networks of workstations. We expect that the well-structured nature of Cilk computations will allow the runtime system to maintain dag consistency efficiently, even in the presence of processor faults.
Reference: [8] <author> Robert D. Blumofe, Matteo Frigo, Christopher F. Joerg, Charles E. Leiserson, and Keith H. Randall. </author> <title> Dag-consistent distributed shared memory. </title> <booktitle> In Proceedings of the 10th International Parallel Processing Symposium, </booktitle> <address> Honolulu, Hawaii, </address> <month> April </month> <year> 1996. </year>
Reference-contexts: 1 Introduction In recent work <ref> [8, 17] </ref>, we have proposed dag-consistent distributed shared memory as a virtual-memory model for multi-threaded parallel-programming systems such as Cilk, a C-based multithreaded language and runtime system [7, 9, 17]. <p> In this paper, we analyze the execution time, page faults, and space requirements of multithreaded algorithms written with this consistency model when the execution is scheduled by the randomized work-stealing scheduler from [7, 10] and dag consistency is maintained by the BACKER coherence algorithm from <ref> [8] </ref>. A multithreaded algorithm is a collection of thread definitions. Analogous to a procedure definition, a thread definition is a block of serial code, possibly with conditional and looping constructions. <p> The BACKER coherence algorithm and the work-stealing scheduler have been implemented in the Cilk runtime system with encouraging empirical results <ref> [8] </ref>. In order to model performance for multithreaded algorithms that use dag-consistent shared memory, we observe that running times will vary as a function of the cache size C, so we must introduce measures that account for this dependence. <p> Instruction k can still have a different viewpoint on the memory than j, for instance, it can see writes performed by other instructions (such as l in the figure) incomparable with j. In previous work <ref> [8, 17] </ref>, we presented a definition of dag consis tency different from Definition 1. Our Definition 1 is stronger than 3 a path from i to k through j, then j masks i. <p> All of j's successors will then see that same value. With the old definition, two of j's successors could see different values. A full justification of Definition 1 and explanation of its properties is beyond the scope of this paper. We now describe the BACKER coherence algorithm from <ref> [8] </ref>, in which the global backing store holds shared-memory objects that are fetched into local processor caches. We state without proof a theorem that BACKER maintains dag consistency (Definition 1). <p> Similarly, each secondary transfer in an execution can be associated with a currently running secondary subcomputa-tion such that each secondary subcomputation has at most 3C secondary transfers associated with it. Proof: For this proof, we use a fact shown in <ref> [8] </ref> that executing a subcomputation starting with an arbitrary cache can only incur C more page faults than the same block of code incurred in the serial execution. <p> Consequently, the number of serial page faults is F 1 (C; n) = (n= p m) 3 = n 3 =m 3=2 , even if we assume that A and C never fault. The divide-and-conquer matrixmul algorithm from <ref> [8] </ref> uses the processor cache much more effectively.
Reference: [9] <author> Robert D. Blumofe, Christopher F. Joerg, Bradley C. Kuszmaul, Charles E. Leiserson, Keith H. Randall, and Yuli Zhou. Cilk: </author> <title> An efficient multithreaded runtime system. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP), </booktitle> <pages> pages 207-216, </pages> <address> Santa Barbara, California, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: 1 Introduction In recent work [8, 17], we have proposed dag-consistent distributed shared memory as a virtual-memory model for multi-threaded parallel-programming systems such as Cilk, a C-based multithreaded language and runtime system <ref> [7, 9, 17] </ref>. A multi-threaded program defines a partial execution order on its instructions, and we view this partial order as a directed acyclic graph or dag. <p> Any mul-tithreaded algorithm can be measured in terms of its work and critical-path length <ref> [5, 9, 10, 20] </ref>. Consider the multithreaded computation that results when a given multithreaded algorithm is used to solve a given input problem.
Reference: [10] <author> Robert D. Blumofe and Charles E. Leiserson. </author> <title> Scheduling multi-threaded computations by work stealing. </title> <booktitle> In Proceedings of the 35th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 356-368, </pages> <address> Santa Fe, New Mexico, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: In this paper, we analyze the execution time, page faults, and space requirements of multithreaded algorithms written with this consistency model when the execution is scheduled by the randomized work-stealing scheduler from <ref> [7, 10] </ref> and dag consistency is maintained by the BACKER coherence algorithm from [8]. A multithreaded algorithm is a collection of thread definitions. Analogous to a procedure definition, a thread definition is a block of serial code, possibly with conditional and looping constructions. <p> Any mul-tithreaded algorithm can be measured in terms of its work and critical-path length <ref> [5, 9, 10, 20] </ref>. Consider the multithreaded computation that results when a given multithreaded algorithm is used to solve a given input problem. <p> Specifically, for any such algorithm and any number P of processors, the randomized work-stealing scheduler executes the algorithm in expected time O (T 1 (n)=P + T (n)) <ref> [7, 10] </ref>. The randomized work-stealing sched-uler operates as follows. Each processor maintains a ready deque (doubly-ended queue) of threads from which work is obtained. When a thread is spawned, the parent thread is suspended and put on the bottom of the deque and execution commences on the spawned child thread. <p> Finally, in this paper, we analyze the space requirements of simple multithreaded algorithms that use dag-consistent shared memory. We assume that the computation is scheduled by a sched-uler, such as the work-stealing algorithm, that maintains the busy-leaves property <ref> [10] </ref>. For a given simple multithreaded algorithm, let S 1 (n) denote the space required by the standard, depth-first serial execution of the algorithm to solve a problem of size n. <p> In previous work, we have shown that the space used by a P-processor execution is at most S 1 (n)P in the worst case <ref> [10] </ref>. We improve this characterization of the space requirements, and we provide a much stronger upper bound on the space requirements of regular divide-and-conquer multithreaded algorithms. <p> In addition, we bound the number of page faults. The exposition of the proofs in this section makes heavy use of results and techniques from <ref> [10] </ref>. Our analysis of execution time is organized as follows. First, we prove a lemma describing how the BACKER algorithm adds page faults to a parallel execution. Then, we obtain a bound on the number of rounds that a parallel execution contains. <p> We shall show that with probability at least 1 e, an execution contains only O (T + lg (1=e)) rounds. To bound the number of rounds, we shall use a delay-sequence argument. We define a modified dag D 0 exactly as in <ref> [10] </ref>. (The dag D 0 is for the purposes of analysis only and has no effect on the computation.) The critical-path length of D 0 is at most 2T . <p> Since there are at least 4P steal requests during the first 22CP events, the probability is at least 1=2 that any task that is critical at the beginning of a round is the target of a steal request <ref> [10, Lemma 10] </ref>, if it is not executed locally by the processor on which it resides. Any task takes at most 3mC + 1 4mC time to execute, since we are ignoring the effects of congestion for the moment. <p> We now want to show that with probability at least 1 e, the total number of rounds is O (T + lg (1=e)). Consider a possible delay sequence. Recall from <ref> [10] </ref> that a delay sequence of size R is a maximal path U in the augmented dag D 0 of length at most 2T , along with a partition P of R which represents the number of rounds during which each task of the path in D 0 is critical. <p> Then for any e &gt; 0, the execution time is O (T 1 (C)=P + mCT + m lg P + mC lg (1=e)) with probability at least 1 e. Moreover, the expected execution time is O (T 1 (C)=P + mCT ). Proof: As in <ref> [10] </ref>, we shall use an accounting argument to bound the running time. During the execution, at each time step, each processor puts a dollar into one of 5 buckets according to its activity at that time step. <p> A busy-leaves scheduler is a scheduler with the property that at all times during the execution, if a thread has no living children, then that thread has a processor working on it. The work-stealing scheduler is a busy-leaves scheduler <ref> [10] </ref>. We shall proceed through a series of lemmas that provide an exact characterization of the space used by simple mul-tithreaded algorithms when executed by a busy-leaves scheduler. <p> We shall then specialize this characterization to provide space bounds for regular divide-and-conquer algorithms. Previous work <ref> [10] </ref> has shown that a busy-leaves scheduler can efficiently execute a fully strict multithreaded algorithm on P processors using no more space than P times the space required to execute the algorithm on a single processor.
Reference: [11] <author> John B. Carter, John K. Bennett, and Willy Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <address> Pacific Grove, California, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: Causal memory [2] ensures consistency only to the extent that if a process A reads a value written by another process B, then all subsequent operations by A must appear to occur after the write by B. Most DSM's implement one of these relaxed consistency models <ref> [11, 18, 21, 27] </ref>, though some implement a fixed collection of consistency models [4], while others merely implement a collection of mechanisms on top of which users write their own DSM consistency policies [23, 26].
Reference: [12] <author> Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1990. </year>
Reference-contexts: Consequently, we obtain the recurrence F 1 (C; n) aF 1 (C; n=b) + p (n) if n &gt; n C ; (1) 2 Other cases exist besides the three given here. 7 We can solve this recurrence using standard techniques <ref> [12, Sec--tion 4.4] </ref>. We iterate the recurrence, stopping as soon as we reach the first value of the iteration count k such that n=b k n C holds, or equivalently when k = dlog b (n=n C )e holds. <p> The solution to this recurrence <ref> [12, Section 4.4] </ref> is * S 1 (n) = Q (lg k+1 n), if s (n) = Q (lg k n) for some constant k 0, and 3 Other cases exist besides those given here. 9 * S 1 (n) = Q (s (n)), if s (n) = W (n e
Reference: [13] <author> Michel Dubois, Christoph Scheurich, and Faye Briggs. </author> <title> Memory access buffering in multiprocessors. </title> <booktitle> In Proceedings of the 13th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 434-442, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: Relaxed shared-memory consistency models are motivated by the fact that sequential consistency [22] and various forms of processor consistency [16] are too expensive to implement in a distributed setting. (Even modern SMP's do not implement sequential consistency.) Relaxed models, such as location consistency [14] and various forms of release consistency <ref> [1, 13, 15] </ref>, ensure consistency (to varying degrees) only when explicit synchronization operations occur, such as the acquisition or release of a lock.
Reference: [14] <author> Guang R. Gao and Vivek Sarkar. </author> <title> Location consistency: Stepping beyond the barriers of memory coherence and serializability. </title> <type> Technical Report 78, </type> <institution> McGill University, School of Computer Science, Advanced Compilers, Architectures, and Parallel Systems (ACAPS) Laboratory, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: Relaxed shared-memory consistency models are motivated by the fact that sequential consistency [22] and various forms of processor consistency [16] are too expensive to implement in a distributed setting. (Even modern SMP's do not implement sequential consistency.) Relaxed models, such as location consistency <ref> [14] </ref> and various forms of release consistency [1, 13, 15], ensure consistency (to varying degrees) only when explicit synchronization operations occur, such as the acquisition or release of a lock.
Reference: [15] <author> Kourosh Gharachorloo, Daniel Lenoski, James Laudon, Phillip Gibbons, Anoop Gupta, and John Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <address> Seattle, Washington, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Relaxed shared-memory consistency models are motivated by the fact that sequential consistency [22] and various forms of processor consistency [16] are too expensive to implement in a distributed setting. (Even modern SMP's do not implement sequential consistency.) Relaxed models, such as location consistency [14] and various forms of release consistency <ref> [1, 13, 15] </ref>, ensure consistency (to varying degrees) only when explicit synchronization operations occur, such as the acquisition or release of a lock.
Reference: [16] <author> James R. Goodman. </author> <title> Cache consistency and sequential consistency. </title> <type> Technical Report 61, </type> <institution> IEEE Scalable Coherent Interface (SCI) Working Group, </institution> <month> March </month> <year> 1989. </year>
Reference-contexts: Relaxed shared-memory consistency models are motivated by the fact that sequential consistency [22] and various forms of processor consistency <ref> [16] </ref> are too expensive to implement in a distributed setting. (Even modern SMP's do not implement sequential consistency.) Relaxed models, such as location consistency [14] and various forms of release consistency [1, 13, 15], ensure consistency (to varying degrees) only when explicit synchronization operations occur, such as the acquisition or release
Reference: [17] <author> Christopher F. Joerg. </author> <title> The Cilk System for Parallel Multithreaded Computing. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <month> January </month> <year> 1996. </year>
Reference-contexts: 1 Introduction In recent work <ref> [8, 17] </ref>, we have proposed dag-consistent distributed shared memory as a virtual-memory model for multi-threaded parallel-programming systems such as Cilk, a C-based multithreaded language and runtime system [7, 9, 17]. <p> 1 Introduction In recent work [8, 17], we have proposed dag-consistent distributed shared memory as a virtual-memory model for multi-threaded parallel-programming systems such as Cilk, a C-based multithreaded language and runtime system <ref> [7, 9, 17] </ref>. A multi-threaded program defines a partial execution order on its instructions, and we view this partial order as a directed acyclic graph or dag. <p> Instruction k can still have a different viewpoint on the memory than j, for instance, it can see writes performed by other instructions (such as l in the figure) incomparable with j. In previous work <ref> [8, 17] </ref>, we presented a definition of dag consis tency different from Definition 1. Our Definition 1 is stronger than 3 a path from i to k through j, then j masks i.
Reference: [18] <author> Kirk L. Johnson, M. Frans Kaashoek, and Deborah A. Wallach. </author> <title> CRL: High-performance all-software distributed shared memory. </title> <booktitle> In Proceedings of the Fifteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 213-228, </pages> <address> Copper Mountain Resort, Colorado, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: Causal memory [2] ensures consistency only to the extent that if a process A reads a value written by another process B, then all subsequent operations by A must appear to occur after the write by B. Most DSM's implement one of these relaxed consistency models <ref> [11, 18, 21, 27] </ref>, though some implement a fixed collection of consistency models [4], while others merely implement a collection of mechanisms on top of which users write their own DSM consistency policies [23, 26].
Reference: [19] <author> Edward G. Coffman Jr. and Peter J. Denning. </author> <title> Operating Systems Theory. </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1973. </year>
Reference-contexts: We shall further assume that pages in the cache are maintained using the popular LRU (least-recently-used) <ref> [19] </ref> heuristic. In addition to servicing page faults, BACKER must reconcile pages between the processor page caches and the backing store so that the semantics of the execution obey the assumptions of dag consistency.
Reference: [20] <author> Richard M. Karp and Vijaya Ramachandran. </author> <title> Parallel algorithms for shared-memory machines. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer ScienceVolume A: Algorithms and Complexity, chapter 17, </booktitle> <pages> pages 869-941. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1990. </year>
Reference-contexts: Any mul-tithreaded algorithm can be measured in terms of its work and critical-path length <ref> [5, 9, 10, 20] </ref>. Consider the multithreaded computation that results when a given multithreaded algorithm is used to solve a given input problem.
Reference: [21] <author> Pete Keleher, Alan L. Cox, Sandhya Dwarkadas, and Willy Zwaenepoel. TreadMarks: </author> <title> Distributed shared memory on standard workstations and operating systems. </title> <booktitle> In USENIX Winter 1994 Conference Proceedings, </booktitle> <pages> pages 115-132, </pages> <address> San Francisco, California, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: Causal memory [2] ensures consistency only to the extent that if a process A reads a value written by another process B, then all subsequent operations by A must appear to occur after the write by B. Most DSM's implement one of these relaxed consistency models <ref> [11, 18, 21, 27] </ref>, though some implement a fixed collection of consistency models [4], while others merely implement a collection of mechanisms on top of which users write their own DSM consistency policies [23, 26].
Reference: [22] <author> Leslie Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multiprocess programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690-691, </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: Relaxed shared-memory consistency models are motivated by the fact that sequential consistency <ref> [22] </ref> and various forms of processor consistency [16] are too expensive to implement in a distributed setting. (Even modern SMP's do not implement sequential consistency.) Relaxed models, such as location consistency [14] and various forms of release consistency [1, 13, 15], ensure consistency (to varying degrees) only when explicit synchronization operations
Reference: [23] <author> James R. Larus, Brad Richards, and Guhan Viswanathan. </author> <title> LCM: Memory system support for parallel language implementation. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 208-218, </pages> <address> San Jose, California, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: Most DSM's implement one of these relaxed consistency models [11, 18, 21, 27], though some implement a fixed collection of consistency models [4], while others merely implement a collection of mechanisms on top of which users write their own DSM consistency policies <ref> [23, 26] </ref>. All of these consistency models and the DSM's that implement these models take a low-level view of a parallel program as a collection of cooperating processes.
Reference: [24] <author> F. Thomson Leighton. </author> <title> Introduction to Parallel Algorithms and Architectures: Arrays Trees Hypercubes. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, California, </address> <year> 1992. </year>
Reference-contexts: Multiplying two n fin matrices (using the ordinary algorithm, and not a variant of Strassen's algorithm [28]) can be performed using Q (n 3 ) work and can be done in Q (lg n) time <ref> [24] </ref>. Thus, for this problem, we have T 1 (n) = Q (n 3 ) and T (n) = Q (lg n). If there were no page faults, we would see T P (n) = O (n 3 =P + lg n). We must also account for page faults, however.
Reference: [25] <author> Piyush Mehrotra and John Van Rosendale. </author> <title> The BLAZE language: A parallel language for scientific programming. </title> <journal> Parallel Computing, </journal> <volume> 5 </volume> <pages> 339-361, </pages> <year> 1987. </year>
Reference-contexts: By leveraging this high-level knowledge, the BACKER algorithm in con junction with the work-stealing scheduler is able to execute multi-threaded algorithms with the performance bounds shown here. The BLAZE parallel language <ref> [25] </ref> and the Myrias parallel computer [3] define a high-level relaxed consistency model much like dag consistency, but we do not know of any efficient implementation of either of these systems.
Reference: [26] <author> Steven K. Reinhardt, James R. Larus, and David A. Wood. Tempest and Typhoon: </author> <title> User-level shared memory. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 325-336, </pages> <address> Chicago, Illinois, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: Most DSM's implement one of these relaxed consistency models [11, 18, 21, 27], though some implement a fixed collection of consistency models [4], while others merely implement a collection of mechanisms on top of which users write their own DSM consistency policies <ref> [23, 26] </ref>. All of these consistency models and the DSM's that implement these models take a low-level view of a parallel program as a collection of cooperating processes.
Reference: [27] <author> Daniel J. Scales and Monica S. Lam. </author> <title> The design and evaluation of a shared object system for distributed memory machines. </title> <booktitle> In Proceedings of the First Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 101-114, </pages> <address> Monterey, California, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: Causal memory [2] ensures consistency only to the extent that if a process A reads a value written by another process B, then all subsequent operations by A must appear to occur after the write by B. Most DSM's implement one of these relaxed consistency models <ref> [11, 18, 21, 27] </ref>, though some implement a fixed collection of consistency models [4], while others merely implement a collection of mechanisms on top of which users write their own DSM consistency policies [23, 26].
Reference: [28] <author> Volker Strassen. </author> <title> Gaussian elimination is not optimal. </title> <journal> Numerische Mathematik, </journal> <volume> 14(3) </volume> <pages> 354-356, </pages> <year> 1969. </year> <month> 12 </month>
Reference-contexts: Multiplying two n fin matrices (using the ordinary algorithm, and not a variant of Strassen's algorithm <ref> [28] </ref>) can be performed using Q (n 3 ) work and can be done in Q (lg n) time [24]. Thus, for this problem, we have T 1 (n) = Q (n 3 ) and T (n) = Q (lg n).
References-found: 28

