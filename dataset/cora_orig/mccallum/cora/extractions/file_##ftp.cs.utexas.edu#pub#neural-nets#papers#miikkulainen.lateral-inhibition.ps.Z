URL: file://ftp.cs.utexas.edu/pub/neural-nets/papers/miikkulainen.lateral-inhibition.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/nn/pages/publications/abstracts.html
Root-URL: 
Email: risto@cs.utexas.edu  
Title: SELF-ORGANIZING PROCESS BASED ON LATERAL INHIBITION AND SYNAPTIC RESOURCE REDISTRIBUTION  
Author: Risto Miikkulainen 
Address: Austin, TX 78712-1188, U.S.A.  
Affiliation: Department of Computer Sciences The University of Texas at Austin  
Abstract: Self-organizing feature maps are usually implemented by abstracting the low-level neural and parallel distributed processes. An external supervisor finds the unit whose weight vector is closest in Euclidian distance to the input vector and determines the neighborhood for weight adaptation. The weights are changed proportional to the Euclidian distance. In a biologically more plausible implementation, similarity is measured by a scalar product, neighborhood is selected through lateral inhibition and weights are changed by redistributing synaptic resources. The resulting self-organizing process is quite similar to the abstract case. However, the process is somewhat hampered by boundary effects and the parameters need to be carefully evolved. It is also necessary to add a redundant dimension to the input vectors.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. K. Hartline. </author> <title> Inhibition of activity of visual receptors by illuminating nearby retinal areas in the limulus eye. </title> <booktitle> Federation Proceedings, </booktitle> <address> 8(1):69-, </address> <year> 1949. </year>
Reference-contexts: = X ij;h ~ h (2) where the function is the familiar sigmoid activation function, or its piecewise linear approximation (x) = 0 x ffi 1 x fi The sigmoid introduces a nonlinearity (a soft threshold between ffi and fi) into the reponse, and limits its output within the range <ref> [0; 1] </ref>. The neighborhood is selected by focusing the initial response of the map through lateral inhibition. <p> Lateral inhibition is a biologically plausible form of lateral connectivity, well documented in e.g. low-level vision <ref> [1; 14] </ref>. The lateral weights are assumed to be preassigned, and their task is to support self-organization of the weights on the external input connections. The self-organizing process does not directly modify the lateral connections.
Reference: [2] <author> Donald O. Hebb. </author> <title> The Organization of Behaviour. </title> <publisher> John Wiley, </publisher> <address> New York, </address> <year> 1949. </year>
Reference-contexts: A mask larger than the network (d = 4; fl E = 0:0075; = 6:0) sees the whole network as one large activity cluster and concentrates the response around the center. 4.2 Weight adaptation According to a well-known hypothesis due to Hebb <ref> [2] </ref>, permanent synaptic efficacy changes require both presynaptic and postsynaptic activity. The connection is significant and deserves to be enforced if the activity causes further activity.
Reference: [3] <author> Geoffrey E. Hinton, James L. McClelland, and David E. Rumelhart. </author> <title> Distributed representations. </title> <editor> In David E. Rumelhart and James L. Mc-Clelland, editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition. Volume I: Foundations. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: How could the transformation of the n-dimensional input space onto a redundant n + 1-dimensional hypersphere surface be implemented in biological systems? If the data is represented by coarse coding <ref> [3] </ref>, the input activation to the neuron may be highly redundant. In addition, the input components should be coupled in such a way that an increase in the activity of one component is accompanied by an increasingly larger decrease in the activity of other components.
Reference: [4] <author> Jari Kangas, Teuvo Kohonen, Jorma Laaksonen, Olli Simula, and Olli Vent-a. </author> <title> Variants of self-organizing maps. </title> <type> Technical report, </type> <institution> Laboratory of Computer and Information Science, Helsinki University of Technology, </institution> <year> 1989. </year>
Reference-contexts: The process is both a method for organizing complex empirical knowledge and a model of learning in biological neural networks. The theory of self-organizing feature maps is fairly well understood and demonstrated <ref> [7; 8; 4; 17] </ref> and a number of applications and extensions of feature maps have also been developed [9; 10; 11; 12; 13; 15; 16]. These implementations are based on an abstraction of the theory, which relies on global supervision and strong computational capabilities on the processing units.
Reference: [5] <author> Teuvo Kohonen. </author> <title> Automatic formation of topological maps of patterns in a self-organizing system. </title> <booktitle> In Proceedings of the 2nd Scandinavian Conference on Image Analysis. Pattern Recognition Society of Finland, </booktitle> <year> 1981. </year>
Reference-contexts: 1 Introduction The self-organizing feature mapping <ref> [5; 7] </ref> is an unsupervised learning process where the processing units of e.g. 2-D laminar network become sensitive to specific items of the input space in a topological order which corresponds to the topological order of the input items.
Reference: [6] <author> Teuvo Kohonen. </author> <title> Self-organized formation of topologically correct feature maps. </title> <journal> Biological Cybernetics, </journal> <volume> 43 </volume> <pages> 59-69, </pages> <year> 1982. </year>
Reference-contexts: A plausible implementation must be based on local computations, simple enough to be carried out by neurons. Kohonen has suggested that lateral inhibition and redistribution of synaptic resources could be responsible for self-organization in biological systems <ref> [6] </ref>. An implementation of this idea is discussed and simulation results are presented in this paper. 2 Self-organizing feature maps A 2-D topological feature map consists of an array of processing units, each with n weight parameters. <p> The subtle differences between them are enough to force the network to organize. One can say that order is brought about by boundary effects: the boundaries mold the mapping in shape, as suggested by Kohonen <ref> [6] </ref>. Note that the network needs to be almost fully connected to produce order. As in the abstract implementation, the larger the mask, the stronger is its self-organizing capability, i.e. the faster the convergence to the proper order.
Reference: [7] <author> Teuvo Kohonen. </author> <title> Self-Organization and Associative Memory. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin; New York, </address> <year> 1984. </year>
Reference-contexts: 1 Introduction The self-organizing feature mapping <ref> [5; 7] </ref> is an unsupervised learning process where the processing units of e.g. 2-D laminar network become sensitive to specific items of the input space in a topological order which corresponds to the topological order of the input items. <p> The process is both a method for organizing complex empirical knowledge and a model of learning in biological neural networks. The theory of self-organizing feature maps is fairly well understood and demonstrated <ref> [7; 8; 4; 17] </ref> and a number of applications and extensions of feature maps have also been developed [9; 10; 11; 12; 13; 15; 16]. These implementations are based on an abstraction of the theory, which relies on global supervision and strong computational capabilities on the processing units. <p> Since the dimensions are optimally chosen in the self-organizing process <ref> [7] </ref> and the third input dimension is redundant, the mapping directly represents the original 2-dimensional input space. Note that the coordinate transformation is unique only when =2 &lt; x 2 &lt; =2, and the rotation direction is correct only when =2 x 1 =2.
Reference: [8] <author> Teuvo Kohonen, Gyorgy Barna, and Ronald Chrisley. </author> <title> Statistical pattern recognition with neural networks: Benchmarking studies. </title> <booktitle> In Proceedings of the IEEE Second Annual International Conference on Neural Networks. IEEE, </booktitle> <year> 1988. </year>
Reference-contexts: The process is both a method for organizing complex empirical knowledge and a model of learning in biological neural networks. The theory of self-organizing feature maps is fairly well understood and demonstrated <ref> [7; 8; 4; 17] </ref> and a number of applications and extensions of feature maps have also been developed [9; 10; 11; 12; 13; 15; 16]. These implementations are based on an abstraction of the theory, which relies on global supervision and strong computational capabilities on the processing units.
Reference: [9] <author> Teuvo Kohonen, Kai Makisara, and Tapio Sara-maki. </author> <title> Phonotopic maps insightful representation of phonological features for speech recognition. </title> <booktitle> In Proceedings of the 6th International Conference on Pattern Recognition, </booktitle> <pages> pages 182-185. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1984. </year>
Reference-contexts: The theory of self-organizing feature maps is fairly well understood and demonstrated [7; 8; 4; 17] and a number of applications and extensions of feature maps have also been developed <ref> [9; 10; 11; 12; 13; 15; 16] </ref>. These implementations are based on an abstraction of the theory, which relies on global supervision and strong computational capabilities on the processing units.
Reference: [10] <author> Teuvo Kohonen, Kimmo Raivio, Olli Simula, Olli Vent-a, and Jukka Henriksson. </author> <title> An adaptive discrete-signal detector based on self-organizing maps. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, </booktitle> <address> Washington, DC, </address> <year> 1990. </year>
Reference-contexts: The theory of self-organizing feature maps is fairly well understood and demonstrated [7; 8; 4; 17] and a number of applications and extensions of feature maps have also been developed <ref> [9; 10; 11; 12; 13; 15; 16] </ref>. These implementations are based on an abstraction of the theory, which relies on global supervision and strong computational capabilities on the processing units.
Reference: [11] <author> Risto Miikkulainen. DISCERN: </author> <title> A Distributed Artificial Neural Network Model of Script Processing and Memory. </title> <type> PhD thesis, </type> <institution> Computer Science Department, University of California, </institution> <address> Los Angeles, </address> <year> 1990. </year>
Reference-contexts: The theory of self-organizing feature maps is fairly well understood and demonstrated [7; 8; 4; 17] and a number of applications and extensions of feature maps have also been developed <ref> [9; 10; 11; 12; 13; 15; 16] </ref>. These implementations are based on an abstraction of the theory, which relies on global supervision and strong computational capabilities on the processing units.
Reference: [12] <author> Risto Miikkulainen. </author> <title> A distributed feature map model of the lexicon. </title> <booktitle> In Proceedings of the Twelfth Annual Cognitive Science Society Conference, </booktitle> <address> Hillsdale, NJ, 1990. </address> <publisher> Lawrence Erlbaum. </publisher>
Reference-contexts: The theory of self-organizing feature maps is fairly well understood and demonstrated [7; 8; 4; 17] and a number of applications and extensions of feature maps have also been developed <ref> [9; 10; 11; 12; 13; 15; 16] </ref>. These implementations are based on an abstraction of the theory, which relies on global supervision and strong computational capabilities on the processing units.
Reference: [13] <author> Risto Miikkulainen. </author> <title> Script recognition with hierarchical feature maps. </title> <journal> Connection Science, </journal> 2(1&2):83-101, 1990. 
Reference-contexts: The theory of self-organizing feature maps is fairly well understood and demonstrated [7; 8; 4; 17] and a number of applications and extensions of feature maps have also been developed <ref> [9; 10; 11; 12; 13; 15; 16] </ref>. These implementations are based on an abstraction of the theory, which relies on global supervision and strong computational capabilities on the processing units.
Reference: [14] <author> F. Ratliff, H. K. Hartline, and D. Lange. </author> <title> The dynamics of lateral inhibition in the compound eye of limulus. </title> <booktitle> In Proceedings of the International Symposium on the Functional Organization of the Compound Eye, </booktitle> <year> 1966. </year>
Reference-contexts: Lateral inhibition is a biologically plausible form of lateral connectivity, well documented in e.g. low-level vision <ref> [1; 14] </ref>. The lateral weights are assumed to be preassigned, and their task is to support self-organization of the weights on the external input connections. The self-organizing process does not directly modify the lateral connections.
Reference: [15] <author> Helge Ritter and Teuvo Kohonen. </author> <title> Self-organizing semantic maps. </title> <journal> Biological Cybernetics, </journal> <volume> 61 </volume> <pages> 241-254, </pages> <year> 1989. </year>
Reference-contexts: The theory of self-organizing feature maps is fairly well understood and demonstrated [7; 8; 4; 17] and a number of applications and extensions of feature maps have also been developed <ref> [9; 10; 11; 12; 13; 15; 16] </ref>. These implementations are based on an abstraction of the theory, which relies on global supervision and strong computational capabilities on the processing units.
Reference: [16] <author> Helge J. Ritter, Thomas M. Martinetz, and Klaus J. Schulten. </author> <title> Topology-conserving maps for learning visuomotor-coordination. Neural Networks, </title> <type> 2(3), </type> <year> 1989. </year>
Reference-contexts: The theory of self-organizing feature maps is fairly well understood and demonstrated [7; 8; 4; 17] and a number of applications and extensions of feature maps have also been developed <ref> [9; 10; 11; 12; 13; 15; 16] </ref>. These implementations are based on an abstraction of the theory, which relies on global supervision and strong computational capabilities on the processing units.
Reference: [17] <author> Helge J. Ritter and Klaus J. Schulten. </author> <title> Convergency properties of kohonen's topology conserving maps: Fluctuations, stability and dimension selection. </title> <journal> Biological Cybernetics, </journal> <volume> 60 </volume> <pages> 59-71, </pages> <year> 1988. </year>
Reference-contexts: The process is both a method for organizing complex empirical knowledge and a model of learning in biological neural networks. The theory of self-organizing feature maps is fairly well understood and demonstrated <ref> [7; 8; 4; 17] </ref> and a number of applications and extensions of feature maps have also been developed [9; 10; 11; 12; 13; 15; 16]. These implementations are based on an abstraction of the theory, which relies on global supervision and strong computational capabilities on the processing units.
References-found: 17

