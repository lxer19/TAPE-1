URL: ftp://ftp.cis.ufl.edu/cis/tech-reports/tr95/tr95-025.ps
Refering-URL: http://www.cis.ufl.edu/tech-reports/tech-reports/tr95-abstracts.html
Root-URL: http://www.cis.ufl.edu
Title: MULTIFRONTAL ALGORITHMS FOR SPARSE INVERSE SUBSETS AND INCOMPLETE LU FACTORIZATION  
Author: BY YOGIN EON CAMPBELL 
Degree: A DISSERTATION PRESENTED TO THE GRADUATE SCHOOL OF THE UNIVERSITY OF FLORIDA IN PARTIAL FULFILLMENT OF THE REQUIREMENTS FOR THE DEGREE OF DOCTOR OF PHILOSOPHY  
Affiliation: UNIVERSITY OF FLORIDA  
Abstract: 1995 Technical Report TR-95-025, Computer and Information Sciences Department, University of Florida, Gainesville, FL, 32611 USA. October, 1995. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. Amestoy, T. A. Davis, and I. S. Duff. </author> <title> An approximate minimum degree ordering algorithm. </title> <note> SIAM Journal on Matrix Analysis and Applications (to appear). </note>
Reference-contexts: First, it is among the fastest LU factorization algorithms on a wide range of problems, especially those involving very unsymmetric matrices [14,16]. Second, it uses an excellent fill-control ordering algorithm | the unsymmetric analogue of the approximate minimum degree ordering developed by Amestoy, Davis, and Duff <ref> [1] </ref>. Finally, it can efficiently factorize matrices of arbitrary symmetry. An outline of the dissertation follows. In Chapter 2 we review prior research on the sparse subset problem and incomplete factorization. <p> In the analysis phase a fill-reducing pivot ordering algorithm (such as the approximate minimum degree algorithm <ref> [1] </ref> or the minimum degree algorithm [27]) is used to establish the pivot order and data structures. In addition, the precedence relationships among the frontal matrices that are used in the numerical phase are established and given by the assembly or elimination tree [19,39]. <p> This relaxed amalgamation allows the use of dense matrix kernels in the innermost loops (level 3 BLAS [17]). Fifth, pivot selection and degree update are based on an unsymmetric analogue of the (symmetric) approximate minimum degree ordering algorithm <ref> [1] </ref>. Finally, aggressive assembly is done: as many rows and columns of entries as possible from the original matrix and unassembled contribution blocks are assembled into the current frontal matrix. <p> This choice of seed pivot is based on two criteria: fill-in control and numerical stability. To help in selecting a pivot with low fill-in, the upper bound degrees of the remaining rows and columns in the active submatrix are maintained (see <ref> [1] </ref> for a discussion of how these upper bound degrees are efficiently computed). Let d x and d x be the true and approximate degree of the row or column x, respectively, where d x d x . <p> Finding a tight upper bound for the degrees of the rows and columns in the contribution matrix involves the use of an efficient scheme described in [15]. As shown by Davis et al. <ref> [1] </ref>, the tight upper bounds on the degrees usually result in low fill-in. 10.2.5 Step 2c: Numerical Update Using the Level 3 BLAS Forming the Schur complement to update the contribution block is done in step 2c. <p> completed, C is then put onto a heap for later assembly, the LU factors computed within the current frontal matrix are stored, and, finally, information on the row and column structure of C are saved to allow the easy assembly of C into subsequent frontal matrices (the quotient graph information <ref> [1] </ref>). Data Structures The real memory used by the unsymmetric-pattern multifrontal algorithm is partitioned as shown in Figure 10-3. (This memory layout is actually for an early version of the algorithm embodied in MA38/UMFPACK2.0.) A similar layout is used for the integer memory used by the algorithm.
Reference: [2] <author> P. R. Amestoy and I. S. Duff. </author> <title> Memory management issues in sparse multifrontal methods on multiprocessors. </title> <journal> The International Journal of Supercomputer Applications, </journal> <volume> 7(1) </volume> <pages> 64-82, </pages> <month> Spring </month> <year> 1993. </year>
Reference-contexts: This is essentially the same partitioning scheme used by Amestoy and Duff in <ref> [2] </ref>. on the proportion of memory to allocate to the two areas. In a preprocessing step we compute the amount of memory required for the sequential algorithm and used this amount plus 5-10%. <p> For the state of a tree node to change from waiting to ready enough memory needs to be available to accommodate the staircase upper triangular data structure of the node. As in <ref> [2] </ref>, the active memory is subdivided into a buddy area and a fixed-block area. The motivation behind this subdivision is to more efficiently use the available memory and, at the same time, lessen memory bottlenecks that would hamper concurrency.
Reference: [3] <author> E. Anderson and Y. Saad. </author> <title> Solving sparse triangular linear systems on parallel computers. </title> <journal> International Journal of High Speed Computing, </journal> <volume> 1 </volume> <pages> 73-95, </pages> <year> 1989. </year>
Reference: [4] <author> O. Axelsson. </author> <title> A survey of vectorizable preconditioning methods for large scale finite element matrix problems. CNA-190: Center for Numerical Analysis, </title> <institution> The University of Texas at Austin, Texas, </institution> <month> February </month> <year> 1984. </year>
Reference: [5] <author> O. Axelsson. </author> <title> A survey of preconditioned iterative methods for linear systems of algebraic equations. </title> <journal> BIT, </journal> <volume> 25 </volume> <pages> 166-187, </pages> <year> 1985. </year>
Reference-contexts: be compensated for by a sufficient decrease in the number of iterations required to solve the system. 1 2 Since no definitive theory exists that can specify the best set of entries to drop in forming the the incomplete factors, several dropping heuristics have been reported in the research literature <ref> [5, 42, 44, 47] </ref>. We discuss these in x 2.3.1. Our incomplete LU algorithm uses the concept of fill-level to establish a dropping criterion. (The fill-level dropping heuristic was used in a different way by Watts [52].) Each entry has an associated fill-level.
Reference: [6] <author> O. Axelsson and N. Munksgaard. </author> <title> A class of preconditioned conjugate gardi-ents methods for the solution of a mixed finite-element discretization of the biharmonic operator. </title> <journal> International Journal of Numerical Mathematics and Engineering, </journal> <volume> 14 </volume> <pages> 1001-1019, </pages> <year> 1978. </year>
Reference-contexts: The ILU factors are used as preconditioners to accelerate the convergence rate of the conjugate gradient squared iterative method [49]. The convergence rate of the conjugate gradient type iterative method on the system Ax = b strongly depends on the distribution of eigenvalues of A <ref> [6, 34] </ref>. A preconditioner matrix, M , is used to transform the coefficient matrix A into M 1 A, in the hope that the preconditioned matrix M 1 A has a better (denser) distribution of eigenvalues than A.
Reference: [7] <author> O. Axelsson and V.A. Barker. </author> <title> Finite Element Solutions of Boundary Value Problems. Theory and Computation. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: There is no fill-in with these methods, so the memory usage is generally not much greater than the number of nonzeros in the coefficient matrix. Also, the influence of round-off errors can be controlled by careful evaluation of the residuals <ref> [7] </ref>. Getting an iterative method to converge quickly enough is often the main stumbling block. The convergence rate depends primarily on the distribution of eigenvalues of the coefficient matrix [8,34]. A system with a coefficient matrix having eigenvalues grouped around one or a few values typically converges rapidly.
Reference: [8] <author> O. Axelsson and N. Munksgaard. </author> <title> Analysis of incomplete factorizations with fixed storage allocation. </title> <editor> In D. J. Evans, editor, </editor> <title> Preconditioning Methods: </title> <journal> Analysis and Applications, </journal> <pages> pages 219-241. </pages> <publisher> Gordon and Breach, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: Finding a good drop tolerance value becomes the major problem for these algorithms. To partially alleviate this problem, algorithms that use a hybrid dropping strategy have been proposed <ref> [8, 36] </ref>. The hybrid dropping strategy involves discarding entries based on some combination of their absolute numerical values, their locations, and the amount of fill-in or storage allowed for the factors.
Reference: [9] <author> R. Barret, M. Berry, T. Chan, J. Dimmel, J. Donato, J. Dongarra, V. Eijkhout, R. Pozo, C. Romine, and H. van der Vorst. </author> <title> Templates for the Solution of Linear Systems. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1994. </year>
Reference-contexts: In our research on preconditioners reported in Chapter 10 we used the preconditioned conjugate gradient squared method. An excellent introduction to both the stationary and nonstationary iterative methods is given in <ref> [9] </ref>. The nonstationary method most effective for a given problem or problem class depends on the properties of the matrices involved (for example symmetry and positive-definiteness), the availability of the eigenvalues of the matrices, and the amount of storage available. <p> Making the best selection for a given problem domain usually depends on experience gained via trial and error using different methods. A useful guide to begin with (Figure 2-1) is given in Barret et al. <ref> [9] </ref>. An iterative method starts with some initial guess of the solution and, via iterative refinement, obtains the desired solution (if the method converges). There is no fill-in with these methods, so the memory usage is generally not much greater than the number of nonzeros in the coefficient matrix.
Reference: [10] <author> R. Betancourt and F. L. Alvarado. </author> <title> Parallel inversion of sparse matrices. </title> <journal> IEEE Transactions on Power Systems, PWRS-1(1):74-81, 1986. </journal> <volume> 106 107 </volume>
Reference-contexts: Broussolle [11], for example, shows how the performance improvements gained by the use of the Takahashi equations to compute Zsparse makes it possible to perform statistical test over hundreds of measurements, in real time, for a network of 300 nodes on a small IBM 370-168 computer. Betancourt and Alvarado <ref> [10] </ref>, developed a parallel algorithm to efficiently compute Zsparse and the full inverse for symmetric matrices using the Takahashi equations. However, there is little else published on computing sparse subsets other than those mentioned for the diagonal set and Zsparse.
Reference: [11] <author> F. Broussolle. </author> <title> State estimation in power system: Detecting bad data through the sparse inverse matrix method. </title> <journal> IEEE Transactions on Power Apparatus and Systems, </journal> <volume> PAS-97(3):678-682, </volume> <month> May </month> <year> 1978. </year>
Reference-contexts: Both Zsparse and the diagonal set are important in several practical applications as mentioned in the introductory chapter. This result thus allowed researchers to compute them more efficiently . Broussolle <ref> [11] </ref>, for example, shows how the performance improvements gained by the use of the Takahashi equations to compute Zsparse makes it possible to perform statistical test over hundreds of measurements, in real time, for a network of 300 nodes on a small IBM 370-168 computer.
Reference: [12] <author> J. D. F. Cosgrove, J. C. Diaz, and A. Griewank. </author> <title> Approximate inverse precon-ditionings for sparse linear systems. </title> <journal> International Journal on Computer Mathematics, </journal> <pages> pages 91-110, </pages> <year> 1992. </year>
Reference: [13] <author> CRAY C90 Series. </author> <title> Functional Description Manual. </title> <institution> HR-04028, Cray Research, Inc., Chippewa Falls, WI, </institution> <year> 1994. </year>
Reference-contexts: Three of the major components are the mainframe, the solid-state storage (SSD) and the input-output section (IOS) <ref> [13] </ref>. The major components of the mainframe or computational section of the CRAY-C98 is shown in Figure (5-2). This section houses 8 CPUs which are connected to an input/output section, a central memory, the interprocessor communication section, and the real-time clock.
Reference: [14] <author> T. A. Davis. </author> <title> A combined unifrontal/multifrontal method for unsymmetric sparse matrices. </title> <type> Technical Report TR-94-005, </type> <institution> University of Florida, </institution> <address> Gainesville, FL, </address> <year> 1994. </year>
Reference: [15] <author> T. A. Davis and I. S. Duff. </author> <title> An unsymmetric-pattern multifrontal method for sparse LU factorization. </title> <note> SIAM Journal on Matrix Analysis and Applications (to appear). </note>
Reference-contexts: In the multifrontal formulation, single entries are not dropped but, rather, entire rows and columns of entries in the contribution blocks of the frontal matrices. Our ILU multifontal algorithm has its roots in the unsymmetric-pattern mul-tifrontal algorithm of Davis and Duff <ref> [15] </ref>. We chose this particular multifrontal algorithm as the basis for our ILU algorithm for a number of reasons. First, it is among the fastest LU factorization algorithms on a wide range of problems, especially those involving very unsymmetric matrices [14,16]. <p> When the pattern of A is highly unsymmetric the use of the symmetrized pattern in the classical method can result in poor performance due to excessive amounts of fill-in. This difficulty is addressed, and solved, in the unsymmetric-pattern multifrontal method of Davis and Duff <ref> [15] </ref>. In this method the algorithm operates on the unsymmetric pattern of the matrix A, rather than a symmetrized form of A. We briefly discuss some of the main differences between the symmetric (classical) and unsymmetric multifrontal algorithms in the second section of this chapter. <p> These matrices are part of the Harwell-Boeing sparse matrix collection [20]. matrices range from small (the 100-by-100 nos4 matrix) to fairly large (74652-by-74752 for the finan512 matrix). The matrices were first factored using a 39 modified form, with strick diagonal pivoting, of UMFPACK <ref> [15] </ref>. The supernodal tree information was also generated by UMFPACK. Table 6-2 gives the execution times for the scalar and block-partitioned Zsparse implementations. For the block-partitioned algorithm we give CPU times for block sizes 8, 32, 64, 128, and 256. <p> be used in the inverse multifrontal context to compute the nonsymmetric Zsparse. 8.1 Mapping a Frontal Matrix to Two Inverse Frontal Matrices As pointed out in Chapter 3, the main difference between the symmetric multi-frontal method of Duff and Reid [22,23] and the unsymmetric-pattern multifrontal method of Davis and Duff <ref> [15] </ref> is that the column and row index patterns of the frontal matrices are not identical in the unsymmetric multifrontal method as they are in the symmetric multifrontal method. As a result, the frontal matrix in the unsymmetric multifrontal method is rectangular rather than square. <p> Fortunately, the EVTEST library call is relatively cheap, about 200 clock cycles. 72 We present the results for runs done on of three matrices from the Harwell-Boeing collection [20]: plat1919, bcsstk25 and finan512 (Table 9.3). The factors and assembly tree information were obtained using UMFPACK <ref> [15] </ref>. Table 9-1. Statistics on matrices matrix n jLj discipline comments fi10 3 plat1919 1919 83 oceanography Atlantic and Indian oceans bcsstk25 15439 2426 structural eng. 76 story skyscraper finan512 74752 9565 economics portfolio optimization Tables 9-2 through 9-10 gives the results for these runs. <p> Following this, we give a fairly detailed, step by step review of the (complete) unsymmetric-pattern multifrontal algorithm from which our incomplete factorization algorithm is derived. For more details on the unsymmetric-pattern multifrontal algorithm we refer the reader to <ref> [15] </ref>. Next, we discuss the incomplete unsymmetric multifrontal algorithm. We then present the results of our numerical experiments, focusing on the quality of the incomplete factors used as preconditioners in the conjugate gradient squared iterative method. <p> this fill-in entry can eventually go all the way down to 1, but never higher than 7. 10.2 The Unsymmetric Multifrontal (Complete) LU Algorithm Our unsymmetric multifrontal incomplete LU factorization algorithm is derived from the unsymmetric-pattern multifrontal factorization algorithm of Davis and Duff (Harwell subroutine MA38 or UMFPACK Version 2.0) <ref> [15] </ref>. Some of the main features of the Davis/Duff algorithm are summarized below. First, this algorithm has no separate analyze and factorize phases. Pivot selection and elimination are both done in a single pass since it is assumed from the outset that pivoting for numerical stability is necessary. <p> Finding a tight upper bound for the degrees of the rows and columns in the contribution matrix involves the use of an efficient scheme described in <ref> [15] </ref>. As shown by Davis et al. [1], the tight upper bounds on the degrees usually result in low fill-in. 10.2.5 Step 2c: Numerical Update Using the Level 3 BLAS Forming the Schur complement to update the contribution block is done in step 2c.
Reference: [16] <author> T. A. Davis and I. S. Duff. </author> <title> A combined unifrontal/multifrontal method for un-symmetric sparse matrices. </title> <type> Technical Report TR-95-020, </type> <institution> University of Florida, </institution> <address> Gainesville, FL, </address> <year> 1995. </year>
Reference: [17] <author> J. Dongarra, J. Du Croz, S. Hammarling, and I. Duff. </author> <title> A set of Level 3 Basic Linear Algebra Subprograms. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 16(1) </volume> <pages> 1-17, </pages> <year> 1990. </year>
Reference-contexts: To take advantage of these capabilities it is essential that the algorithm process data in dense blocks (rather than as single entries), as much as possible. An algorithm structured on the use of the Basic Linear Algebra Subroutines (BLAS) <ref> [17] </ref> (especially the level 2 and level 3 BLAS) is designed to do exactly this. We design all algorithms in this dissertation with the use of dense matrix kernels in mind. <p> Fourth, advantage is taken of repeated structures in the matrix by factorizing more than one pivot in each frontal matrix: relaxed amalgamation. This relaxed amalgamation allows the use of dense matrix kernels in the innermost loops (level 3 BLAS <ref> [17] </ref>). Fifth, pivot selection and degree update are based on an unsymmetric analogue of the (symmetric) approximate minimum degree ordering algorithm [1].
Reference: [18] <author> P. F. Dubois, A. Greenbaum, and G. H. Rodrigue. </author> <title> Approximating the inverse of a matrix for use in iterative algorithms on vector processors. </title> <journal> Computing, </journal> <volume> 22 </volume> <pages> 257-268, </pages> <year> 1979. </year>
Reference: [19] <author> I. S. Duff, A. M. Erisman, and J. K. Reid. </author> <title> Direct Methods for Sparse Matrices. </title> <publisher> Oxford Science Publications, </publisher> <address> New York, NY, </address> <year> 1991. </year>
Reference-contexts: To begin with, we note that the inverse of any matrix is usually full, even if the matrix is sparse <ref> [19] </ref>. If the dimension of the matrix in question is n, it is, therefore, computationally very expensive (O (n 3 )) (and inefficient) to compute the full inverse when only a sparse subset is required. <p> For a similar reasons the k indices in the summation in Equation (4.5) are greater than i. Note that the diagonal entries can be just as easily computed using either Equation (4.4) or Equation 4.5. 18 The elements of Z can be computed in reverse Crout order <ref> [19] </ref>. That is, evaluate in order the elements in rows and columns n, n 1, : : : 1. For each column, only the entries in the lower triangular part of Z are evaluated using Equation 4.4.
Reference: [20] <author> I. S. Duff, R. G. Grimes, and J. G. Lewis. </author> <title> Users' guide for the Harwell-Boeing sparse matrix collection (release 1). </title> <type> Technical Report RAL-92-086, </type> <institution> Rutherford Appleton Laboratory, </institution> <address> Didcot, Oxon, England, </address> <month> Dec. </month> <year> 1992. </year>
Reference-contexts: Step 3a involves a saxpy operation, while lines 3b and 6a involve dot products. 6.3.3 Numerical Results Statistics for the test matrices used in our numerical experiments are shown in Table 6-1. These matrices are part of the Harwell-Boeing sparse matrix collection <ref> [20] </ref>. matrices range from small (the 100-by-100 nos4 matrix) to fairly large (74652-by-74752 for the finan512 matrix). The matrices were first factored using a 39 modified form, with strick diagonal pivoting, of UMFPACK [15]. The supernodal tree information was also generated by UMFPACK. <p> Fortunately, the EVTEST library call is relatively cheap, about 200 clock cycles. 72 We present the results for runs done on of three matrices from the Harwell-Boeing collection <ref> [20] </ref>: plat1919, bcsstk25 and finan512 (Table 9.3). The factors and assembly tree information were obtained using UMFPACK [15]. Table 9-1.
Reference: [21] <author> I.S. Duff and G. A. Meurant. </author> <title> The effect of ordering on preconditioned conjugate gradients. </title> <journal> BIT, </journal> <volume> 29 </volume> <pages> 635-657, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: Note that for the three smaller matrices (Tables 10-7 10-12), both the condition number and Frobenius norm of the remainder matrix (jjRjj F ) also decrease rather rapidly with increasing level tolerance. The decrease of jjRjj F with increasing level tolerance supports the observation made in <ref> [21] </ref> that entries with higher levels tend to have smaller numerical values. 97 Table 10-8. Numerical Results: B for pores3 level ftime stime ttime (sec) (sec) (sec) 1 3 .55 .4 .95 5 .53 .47 1.0 Table 10-9.
Reference: [22] <author> I.S. Duff and J.K. Reid. </author> <title> The multifrontal solution of indefinite sparse symmetric linear equations. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 9 </volume> <pages> 302-325, </pages> <year> 1983. </year>
Reference: [23] <author> I. S. Duff and J. K. Reid. </author> <title> The multifrontal solution of unsymmetric sets of linear equations. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 5(3) </volume> <pages> 633-641, </pages> <year> 1984. </year> <month> 108 </month>
Reference: [24] <author> S. C. Eisenstat and J. W. H. Liu. </author> <title> Exploiting structural symmetry in unsymmetric sparse symbolic factorization. </title> <type> Technical Report CS-90-12, </type> <institution> Dept. of Computer Science, York Univ., </institution> <address> North York, Ontario, </address> <month> Nov. </month> <year> 1990. </year>
Reference: [25] <author> A. M. Erisman and W. F.Tinney. </author> <title> On computing certain elements of the inverse of a sparse matrix. </title> <journal> Communications of the ACM, </journal> <volume> 18 </volume> <pages> 177-179, </pages> <month> March </month> <year> 1975. </year>
Reference-contexts: We can do much better than this using the Takahashi equations. Erisman and Tinney in a 1975 paper <ref> [25] </ref>, for example, showed that for symmetric matrices the sparse inverse subset (Zsparse) and the set of entries on the diagonal of Z can both be computed (using the Takahashi equations) without having to compute any inverse entry outside of these sets. <p> The fifth and final result concerns the direct z-dependency set of all entries in Zsparse. Erisman and Tinney <ref> [25] </ref>, showed that the only inverse entries that an entry in Zsparse dependent on also belong to Zsparse. In our terminology, this is equivalent to saying that the direct z-dependency set of Zsparse is equal to Zsparse. We prove this result next.
Reference: [26] <author> A. George, M. Heath, J. W. H. Liu, and E. G. Y. Ng. </author> <title> Solution of sparse positive-definite systems on a hypercube. </title> <journal> Journal of Computational and Applied Mathematics, </journal> <volume> 27 </volume> <pages> 129-156, </pages> <year> 1989. </year>
Reference: [27] <author> A. George and J. W. H. Liu. </author> <title> Computer Solution of Large Sparse Positive-Definite Systems. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1981. </year>
Reference-contexts: In the analysis phase a fill-reducing pivot ordering algorithm (such as the approximate minimum degree algorithm [1] or the minimum degree algorithm <ref> [27] </ref>) is used to establish the pivot order and data structures. In addition, the precedence relationships among the frontal matrices that are used in the numerical phase are established and given by the assembly or elimination tree [19,39]. In this phase only the pattern of A is used.
Reference: [28] <author> J. R. Gilbert and J. W. H. Liu. </author> <title> Elimination structures for unsymmetric sparse LU factors. </title> <type> Technical Report CS-90-11, </type> <institution> Dept. of Computer Science, York Univ., </institution> <address> North York, Ontario, </address> <month> Feb. </month> <year> 1990. </year>
Reference: [29] <author> J. R. Gilbert, C. Moler, and R. Schreiber. </author> <title> Sparse matrices in MATLAB: Design and implementation. </title> <journal> SIAM Journal of Matrix Analysis and Applications, </journal> <volume> 13(1) </volume> <pages> 333-356, </pages> <year> 1992. </year>
Reference-contexts: Oftentimes, solving problems involving very large sparse matrices is only feasible when the sparsity of the matrices involved is taken into account. Gilbert et al. <ref> [29] </ref>, gives the following example that clearly illustrates the necessity of taking advantage of sparsity. Let D be the matrix representation of the discrete 5-point Laplacian on a square 64-by-64 grid. The matrix D has dimensions 4096-by-4096 and 20224 nonzeros.
Reference: [30] <author> G. H. Golub and C. F. van Loan. </author> <title> Matrix Computations. </title> <publisher> The Johns Hopkins University Press, </publisher> <address> Baltimore, MD and London, UK, </address> <note> second edition, </note> <year> 1990. </year>
Reference-contexts: Equation (6.9) gives the extended form to compute entries in the inverse row of F i . We used the colon notation found in <ref> [30] </ref> where X [p:q;r:s] represents the matrix with row indices ranging from p to q and column indices from r to s. If p = q then X [p;r:s] is a row vector with column indices r through s. <p> m 2 , m 1 and m 2 are defined by Equation 9.6, 1 i m 1 , 1 j m, and T ij = U ij . m 1 = djU j=be 00 (9.6) In addition, we use the notation found in the book by Golub and Van Loan <ref> [30] </ref>, where X [s:t;u:v] refers to a matrix X with row indices ranging from s to t, and column 63 indices from u to v.
Reference: [31] <author> A. Greenbaum. </author> <title> Solving sparse triangular linear systems using Fortran with extensions on the NYU Ultracomputer prototype. </title> <type> Technical Report Tech. Report 99, </type> <institution> NYU Ultracomputer Note, </institution> <address> New York University, New York, U.S.A, </address> <month> Apr. </month> <year> 1986. </year>
Reference: [32] <author> M. J. Grote and T. Huckle. </author> <title> Parallel preconditioning with sparse approximate inverses. </title> <editor> In D. Bailey, P. Bjorstad, J. Gilbert, M. Mascagni, R. Schreiber, H. Si-mon, V. Torczon, and L. Watson editors, </editor> <booktitle> Proceedings of the seventh SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 466-471. </pages> <publisher> SIAM, </publisher> <year> 1995. </year>
Reference: [33] <author> I. Gustafson. </author> <title> Modified incomplete Cholesky (MIC) methods. </title> <editor> In D. J. Evans, editor, </editor> <title> Preconditioning Methods: </title> <journal> Analysis and Applications, </journal> <pages> pages 265-293. </pages> <publisher> Gordon and Breach, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: This prevents the algorithm from failing or producing factors which are not positive-definite. Kershaw showed that the preconditioners are still accurate if pivots with values less than or equal to zero occur infrequently. In the modified incomplete Cholesky algorithm presented by Gustafson <ref> [33] </ref>, the entries that would otherwise be dropped from a given row are added to the diagonal entry, preserving rowsums.
Reference: [34] <author> A. Jennings and G. A. Malik. </author> <title> Partial elimination. </title> <journal> Journal of the Institute of Mathematics and its Applications, </journal> <volume> 20 </volume> <pages> 307-316, </pages> <year> 1977. </year>
Reference-contexts: The ILU factors are used as preconditioners to accelerate the convergence rate of the conjugate gradient squared iterative method [49]. The convergence rate of the conjugate gradient type iterative method on the system Ax = b strongly depends on the distribution of eigenvalues of A <ref> [6, 34] </ref>. A preconditioner matrix, M , is used to transform the coefficient matrix A into M 1 A, in the hope that the preconditioned matrix M 1 A has a better (denser) distribution of eigenvalues than A.
Reference: [35] <author> T. Johnson and T. A. Davis. </author> <title> Parallel buddy memory management. </title> <journal> Parallel Processing Letters, </journal> <volume> 2(4) </volume> <pages> 391-398, </pages> <year> 1992. </year>
Reference-contexts: Note that it is necessary for the size of the active area to be at least as large as the amount of memory required by the largest inverse frontal matrix. The buddy area is managed using a buddy memory manager <ref> [35] </ref>, while a very simple link-list manager is used for the fixed-block area. Memory can be allocated and/or deallocated concurrently from the buddy and fixed-block areas. 9.2.6 The Parallel Algorithm An outline of the parallel block-partitioned Zsparse algorithm is given in Figure 9-10.
Reference: [36] <author> M. T. Jones and P. E. Plassman. </author> <title> An improved Cholesky factorization. </title> <type> Technical Report Preprint MCS-P206-0191, </type> <institution> Argonne National Laboratory, Argonne, Illinois, </institution> <year> 1992. </year> <month> 109 </month>
Reference-contexts: Finding a good drop tolerance value becomes the major problem for these algorithms. To partially alleviate this problem, algorithms that use a hybrid dropping strategy have been proposed <ref> [8, 36] </ref>. The hybrid dropping strategy involves discarding entries based on some combination of their absolute numerical values, their locations, and the amount of fill-in or storage allowed for the factors.
Reference: [37] <author> D. S. Kershaw. </author> <title> The incomplete Cholesky-conjugate gradient method for the iterative solution of systems of linear equations. </title> <journal> Journal of Computational Physics, </journal> <volume> 26 </volume> <pages> 43-65, </pages> <year> 1978. </year>
Reference-contexts: For example, Ker-shaw <ref> [37] </ref>, generalized the algorithm to apply to symmetric positive-definite matrices and to nonsingular matrices arising from partial differential equations by setting pivots with values less than or equal to zero to some positive value. This prevents the algorithm from failing or producing factors which are not positive-definite. <p> That is, rather than allowing a local pivot failure due to the size of the pivot value being too small, we replace the pivot value by some specified value (typically 1.0). Kershaw <ref> [37] </ref>, used this strategy to avoid pivot failures and/or the loss of positive-definiteness of the incomplete Cholesky factors.
Reference: [38] <author> J. L. Larson. </author> <title> Multitasking on the Cray X-MP-2 multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 17 </volume> <pages> 62-69, </pages> <month> July </month> <year> 1984. </year>
Reference-contexts: A call to EV-CLEAR (ievent) changes the status of the event variable, ievent, to "cleared." The approximate timings for these Macrotasking library calls are shown below <ref> [38] </ref>. 24 Table 5-1.
Reference: [39] <author> J. W. H. Liu. </author> <title> The role of elimination trees in sparse factorization. </title> <journal> SIAM Journal on Matrix Analysis and Applications, </journal> <volume> 11(1) </volume> <pages> 134-172, </pages> <year> 1990. </year>
Reference-contexts: Figure 3-1 illustrates this partitioning scheme. The assembly tree can be constructed using the parent-child relationship given by parent (i) = min f j j l ji 6= 0 ; j &gt; ig = min f j j j 2 L 00 i g <ref> [39] </ref>. Figure 3-2 shows the assembly tree for the filled matrix in Equation 3.2. The node numbers correspond to labels for frontal matrices and the arrows specify the dependency relationships among frontal matrices.
Reference: [40] <author> J. W. H. Liu. </author> <title> The multifrontal method for sparse matrix solution: </title> <journal> Theory and practice. Siam Review, </journal> <volume> 34(1) </volume> <pages> 82-109, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: The multifrontal algorithms can, therefore, better take advantage of locality of reference and use standard dense kernels. 11 12 The classical multifrontal method of Duff and Reid [22,23] (see also Liu's article <ref> [40] </ref>, for a review), assumes structural symmetry of the coefficient matrix A. We discuss this method in some detail in first section of this chapter. For nonsymmetric matrices the classical multifrontal method operates on the symmetrized A + A T pattern, storing explicit zeros wherever necessary.
Reference: [41] <author> J. W. H. Liu, E. G. Ng, and B. W. Peyton. </author> <title> On finding supernodes for sparse matrix computations. </title> <journal> SIAM Journal on Matrix Analysis and Applications, </journal> <volume> 14(1) </volume> <pages> 242-252, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: We show in this section how to extend the simple algorithm for computing Zsparse to take advantage of the supernodes formed during the LDU factorization. 6.2.1 The Supernodal Inverse Constructs Our definition of a supernode comes from a theorem given by Liu et al. <ref> [41] </ref>: Theorem 6.2.1 (41) The column set S = fi,i+1, : : : ;i+mg is a supernode of the matrix L if and only if S is a maximal set of contiguous columns such that s + j 1 is a child of s + j in the elimination (assembly tree <p> All we really need is the pattern of nonzeros in the factorized matrix; even the supernodal structures can be reconstructed from the filled-matrix pattern information <ref> [41] </ref>. The essential theoretical results, stated in Equations 6.4 and 6.5, and the ensuing observations, give a precise description of the locations of the direct dependency sets when inverting an inverse frontal matrix. This allows the use of level 2 and level 3 BLAS in the implementation.
Reference: [42] <author> J. Meijerink and A. Van Der Vorst. </author> <title> An iterative solution method for linear systems of which the coefficient matrix is a symmetric M-matrix. </title> <journal> Mathematics of Computation, </journal> <volume> 31 </volume> <pages> 134-155, </pages> <year> 1977. </year>
Reference-contexts: be compensated for by a sufficient decrease in the number of iterations required to solve the system. 1 2 Since no definitive theory exists that can specify the best set of entries to drop in forming the the incomplete factors, several dropping heuristics have been reported in the research literature <ref> [5, 42, 44, 47] </ref>. We discuss these in x 2.3.1. Our incomplete LU algorithm uses the concept of fill-level to establish a dropping criterion. (The fill-level dropping heuristic was used in a different way by Watts [52].) Each entry has an associated fill-level. <p> Consequently, several dropping heuristics have been reported in the literature, all attempting to produce preconditioners that reduce the convergence rate while remaining relatively cheap to construct and use. Meijerink and van Der Vorst <ref> [42] </ref>, showed that for symmetric positive-definite M-matrices 1 the incomplete Cholesky factorization exists (no zero pivots) as long as the set of entries dropped do not include the entries on the diagonal.
Reference: [43] <author> R. Melhem. </author> <title> Parallel solution of linear systems with striped sparse matrices. </title> <journal> Parallel Computing, </journal> <volume> 6 </volume> <pages> 165-184, </pages> <year> 1988. </year>
Reference: [44] <author> N. Munksgaard. </author> <title> Solving sparse symmetric sets of linear equations by preconditioned conjugate gardients. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 6 </volume> <pages> 206-219, </pages> <year> 1980. </year>
Reference-contexts: be compensated for by a sufficient decrease in the number of iterations required to solve the system. 1 2 Since no definitive theory exists that can specify the best set of entries to drop in forming the the incomplete factors, several dropping heuristics have been reported in the research literature <ref> [5, 42, 44, 47] </ref>. We discuss these in x 2.3.1. Our incomplete LU algorithm uses the concept of fill-level to establish a dropping criterion. (The fill-level dropping heuristic was used in a different way by Watts [52].) Each entry has an associated fill-level.
Reference: [45] <author> C. D. Polychronopoulos and D. J. Kuck. </author> <title> Guided self-scheduling: A practical scheduling scheme for parallel supercomputers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(12):1425-1439, </volume> <month> December </month> <year> 1987. </year>
Reference-contexts: We use a two-dimensional scheme to partition the tree nodes (tree tasks) to obtain the finer grain node tasks. The unprocessed tree and node tasks are maintained in a single queue. From this queue, node tasks are scheduled to processors using the guided self-scheduling scheme discussed in <ref> [45] </ref>. The memory used is partitioned into two segments; this allows us to maintain a high level of concurrency and, at the same time, keep the total amount of memory usage low. <p> For Z ij 2 S 1 and Z pq 2 S 2 , Z ij depends on Z pq if j=p. Figures 9-5 and 9-6 illustrate some of these dependencies. 9.2.3 Task Scheduling We use the guided self-scheduling technique (GSS) <ref> [45] </ref> to assign node tasks to processors. The basic idea behind the GSS scheduling scheme is that a variable-sized group of tasks is assigned to a processor each time it becomes available. <p> As shown Polychronopolous and Kuck <ref> [45] </ref>, by "guiding" (or adjusting) the amount of work given to a processor on a given scheduling pass, guided self-scheduling can result in both good load balancing and low scheduling overhead. A purely self-scheduling scheme would ordinarily assign a fixed number of tasks to every processor on each assignment.
Reference: [46] <author> Y. Saad. </author> <title> Practical use of polynomial preconditionings for the conjugate gradient method. </title> <type> Technical Report Research Report RR 283, </type> <institution> Yale University, Department of Computer Science, </institution> <year> 1983. </year>
Reference: [47] <author> Y. Saad. </author> <title> Preconditioning techniques for nonsymmetric and indefinite systems. </title> <journal> Journal of Computational and Applied Mathematics, </journal> <volume> 24 </volume> <pages> 89-105, </pages> <year> 1988. </year>
Reference-contexts: be compensated for by a sufficient decrease in the number of iterations required to solve the system. 1 2 Since no definitive theory exists that can specify the best set of entries to drop in forming the the incomplete factors, several dropping heuristics have been reported in the research literature <ref> [5, 42, 44, 47] </ref>. We discuss these in x 2.3.1. Our incomplete LU algorithm uses the concept of fill-level to establish a dropping criterion. (The fill-level dropping heuristic was used in a different way by Watts [52].) Each entry has an associated fill-level.
Reference: [48] <author> J. Saltz. </author> <title> Aggregation methods for solving sparse triangular systems on multiprocessors. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 11 </volume> <pages> 123-144, </pages> <year> 1990. </year>
Reference: [49] <author> P. Sonnveld. </author> <title> CGS, a fast Lanczos-type solver for nonsymmetric linear systems. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 10 </volume> <pages> 36-52, </pages> <year> 1989. </year>
Reference-contexts: The ILU factors are used as preconditioners to accelerate the convergence rate of the conjugate gradient squared iterative method <ref> [49] </ref>. The convergence rate of the conjugate gradient type iterative method on the system Ax = b strongly depends on the distribution of eigenvalues of A [6, 34]. <p> The frontal growth-factor, g, was set to two. We used the preconditioned conjugate gradient square iterative method (CGS) <ref> [49] </ref> with a maximum of 250 iterations allowed for each run with a preconditioner. Convergence is achieved if the relative 2-norm of the residual is less than 10 5 and the relative error in the solution is less than 10 3 . Each of the runs used 95 Table 10-5.
Reference: [50] <author> K. Takahashi, J. Fagan, and M. Chen. </author> <title> Formation of a sparse bus impedance matrix and its application to short circuit study. </title> <booktitle> 8th PICA Conference Proceedings, </booktitle> <address> Minneapolis, </address> <publisher> Minnesota, </publisher> <pages> pages 177-179, </pages> <month> June, 4-6 </month> <year> 1973. </year> <month> 110 </month>
Reference-contexts: Two equations relating the LDU factors of a matrix to the inverse of the matrix, presented by Takahashi et al. <ref> [50] </ref>, are used in evaluating the inverse entries. This problem is of significant theoretical and practical importance because it arises in several application areas. <p> The first equations using the LDU factors to compute inverse entries were published by Takahashi, Fagan, and Chen <ref> [50] </ref> in 1973. <p> We discuss the details of this algorithm in Chapter 10. 15 CHAPTER 4 THE TAKAHASHI EQUATIONS 4.1 The Two Equations Relating the L, D, and U Factors to Z Takahashi et al. <ref> [50] </ref>, presented two equations to compute the inverse of a general matrix, A, using its LDU factorization.
Reference: [51] <author> R. S. Varga. </author> <title> Factorizations and normalized iterative methods, in Boundary Problems in Differential Equations (edited by R.E. </title> <type> Langer). </type> <institution> The University of Wisconsin Press, Madison, Wisconsin, </institution> <year> 1960. </year>
Reference: [52] <author> J. W. Watts III. </author> <title> A conjugate gradient-truncated direct method for the iterative solution of the reservoir simulation pressure equation. </title> <journal> Society of Petroleum Engineers, </journal> <volume> 21 </volume> <pages> 345-353, </pages> <year> 1981. </year>
Reference-contexts: We discuss these in x 2.3.1. Our incomplete LU algorithm uses the concept of fill-level to establish a dropping criterion. (The fill-level dropping heuristic was used in a different way by Watts <ref> [52] </ref>.) Each entry has an associated fill-level.
Reference: [53] <author> M. Yannakakis. </author> <title> Computing the minimum fill-in is NP-complete. </title> <journal> SIAM Journal on Algebraic and Discrete Methods, </journal> (2):77-79, 1981. 
Reference-contexts: Although much research has gone into the development of fill-reducing strategies, and some with significant success, the fill-in problem is still of major concern for 6 two reasons. First, finding a pivot ordering that will result in the minimum amount of fill-in is an NP-complete problem <ref> [53] </ref>. The fill-reducing strategies are, therefore, based on heuristics that cannot guarantee a small amount of fill-in. Second, even if it were possible to get the minimum amount of fill-in (or close to it), this amount can still be significant.

References-found: 53

