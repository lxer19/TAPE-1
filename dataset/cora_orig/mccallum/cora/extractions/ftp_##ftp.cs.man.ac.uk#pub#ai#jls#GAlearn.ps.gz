URL: ftp://ftp.cs.man.ac.uk/pub/ai/jls/GAlearn.ps.gz
Refering-URL: http://www.cs.man.ac.uk/~magnus/magnus.html
Root-URL: http://www.cs.man.ac.uk
Title: The dynamics of a Genetic Algorithm for a simple learning problem  
Author: Magnus Rattray and Jonathan L Shapiro 
Date: 1996)  
Note: To appear in J. Phys A (accepted August 15th,  
Address: Manchester, Oxford Road, Manchester M13 9PL, UK  
Affiliation: Computer Science Department, University of  
Abstract: A formalism for describing the dynamics of Genetic Algorithms (GAs) using methods from statistical mechanics is applied to the problem of generalization in a perceptron with binary weights. The dynamics are solved for the case where a new batch of training patterns is presented to each population member each generation, which considerably simplifies the calculation. The theory is shown to agree closely to simulations of a real GA averaged over many runs, accurately predicting the mean best solution found. For weak selection and large problem size the difference equations describing the dynamics can be expressed analytically and we find that the effects of noise due to the finite size of each training batch can be removed by increasing the population size appropriately. If this population resizing is used, one can deduce the most computationally efficient size of training batch each generation. For independent patterns this choice also gives the minimum total number of training patterns used. Although using independent patterns is a very inefficient use of training patterns in general, this work may also prove useful for determining the optimum batch size in the case where patterns are recycled. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Baum E B, </author> <title> Boneh D and Garret C 1995 COLT '95: </title> <booktitle> Proc. of the 8th Annual Conf. on Computational Learning Theory (New York) p 230-239 </booktitle>
Reference-contexts: This method uses a microscopic description and is difficult to apply to specific problems of realistic size due to high-dimensionality of the equations of motion. More recently, a number of results have been derived for the performance of a GA on a class of simple additive problems <ref> [1, 12, 20] </ref>. These approaches use a macroscopic description, but assume a particular form for the distribution of macroscopics which is only applicable in large populations and for a specific class of problem. <p> Baum et al have shown that this problem is similar to a paramagnet whose energy is corrupted by noise and they suggest that the GA may perform well in this case, since it is relatively robust towards noise when compared to local search methods <ref> [1] </ref>. The noise in the training energy is due to the finite size of the training set and is a feature of many machine learning problems [7]. <p> Baum et al have shown that a closely related GA scales as O (N log 2 2 N ) on this problem if the population size is sufficiently large so that weights can be assumed to come from a binomial distribution <ref> [1] </ref>. This is effectively a maximum entropy assumption with a constraint on the mean overlap alone. They use culling selection, where the best half of the population survives each generation leading to a change in the mean overlap proportional to the population's standard deviation.
Reference: [2] <institution> Davis L 1991 Handbook Of Genetic Algorithms (Van Nostrand Reinhold, </institution> <address> New York) </address>
Reference-contexts: 1. Introduction Genetic Algorithms (GAs) are adaptive search techniques, which can be used to find low energy states in poorly characterized, high-dimensional energy landscapes [8, 11]. They have already been successfully applied in a large range of domains <ref> [2] </ref> and a review of the literature shows that they are becoming increasingly popular. In particular, GAs have been used in a number of machine learning applications, including the design and training of artificial neural networks [7, 19, 28].
Reference: [3] <editor> De la Maza M and Tidor B 1991 Proc. </editor> <booktitle> of the ORSA CSTS Conf. Computer Science and Operations Research: New Developments in their Interfaces p 425-440 27 </booktitle>
Reference-contexts: Under selection, new population members are chosen from the present population with replacement, with a probability proportional to their Boltzmann weight. The selection strength fi is analogous to the inverse temperature and determines the intensity of selection, with larger fi leading to a higher variance of selection probabilities <ref> [3, 15] </ref>. Under standard uniform crossover, the population is divided into pairs at random and the new population is produced by swapping weights at each site within a pair with some fixed probability.
Reference: [4] <institution> Derrida B 1981 Phys. </institution> <address> Rev. B 24 2613-25 </address>
Reference-contexts: Following Prugel-Bennett and Shapiro one can use Derrida's trick and express the logarithm as an integral in order to decouple the average <ref> [4, 15] </ref>. hlog Z s i = Z 1 dt t Z 1 dt t 11 where, f (t; fi; fl) = dR dE p (R) p (EjR) exp te fiE+flR (24) The distribution of overlaps within an infinite population is approximated by a cumulant expansion around a Gaussian distribution [16],
Reference: [5] <institution> Falconer D S 1989 Introduction to Quantitative Genetics (Longman Scientific and Technical, </institution> <address> Burnt Mill, England) </address>
Reference-contexts: Other attempts to describe GAs in terms of population moments (or schema moments or average Walsh coefficients) suffer from this problem. Macroscopic descriptions of population dynamics are also widely used in quantitative genetics (see, for example, reference <ref> [5] </ref>). In this field the importance of finite-population fluctuations is more widely appreciated; the infinite population limit is usually taken explicitly. Using the statistical mechanics approach, equations for fitness moments which include finite-population fluctuations can be derived by averaging the cumulants, which are more 4 robust statistics.
Reference: [6] <editor> Forrest S and Mitchell M 1993 Foundations of Genetic Algorithms 2 ed Whitley L D (Morgan Kaufmann, </editor> <address> San Mateo, Calif) p 109-129 </address>
Reference-contexts: This approach has led to false predictions of problem difficulty, especially when the dynamic nature of the search is ignored <ref> [6, 9] </ref>. A rigorous approach introduced by Vose et al describes the population dynamics as a dynamical system in a high-dimensional Euclidean space, with each genetic operator incorporated as a transition tensor [25, 26].
Reference: [7] <institution> Fitzpatrick J M and Grefenstette J J 1988 Machine Learning 3 101-120 </institution>
Reference-contexts: They have already been successfully applied in a large range of domains [2] and a review of the literature shows that they are becoming increasingly popular. In particular, GAs have been used in a number of machine learning applications, including the design and training of artificial neural networks <ref> [7, 19, 28] </ref>. In the simple GA considered here, each population member is represented by a genotype, in this case a binary string, and an objective function assigns an energy x email: rattraym@cs.man.ac.uk 2 to each such genotype. <p> The noise in the training energy is due to the finite size of the training set and is a feature of many machine learning problems <ref> [7] </ref>. We show that the noise in the training energy is well approximated by a Gaussian distribution for large problem size, whose mean and variance can be exactly determined and are simple functions of the overlap between pupil and teacher.
Reference: [8] <editor> Goldberg D E 1989 Genetic Algorithms in Search, </editor> <booktitle> Optimization and Machine Learning (Addison-Wesley, </booktitle> <address> Reading, MA) </address>
Reference-contexts: 1. Introduction Genetic Algorithms (GAs) are adaptive search techniques, which can be used to find low energy states in poorly characterized, high-dimensional energy landscapes <ref> [8, 11] </ref>. They have already been successfully applied in a large range of domains [2] and a review of the literature shows that they are becoming increasingly popular. <p> Many other theoretical approaches are based on the intuitive idea that above average fitness building blocks are preferentially sampled by the GA, which, if they can be usefully recombined, results in highly fit individuals being produced <ref> [8, 11] </ref>.
Reference: [9] <editor> Grefenstette J J 1993 Foundations of Genetic Algorithms 2 ed Whitley L D (Morgan Kaufmann, </editor> <address> San Mateo, Calif) p 75 </address>
Reference-contexts: This approach has led to false predictions of problem difficulty, especially when the dynamic nature of the search is ignored <ref> [6, 9] </ref>. A rigorous approach introduced by Vose et al describes the population dynamics as a dynamical system in a high-dimensional Euclidean space, with each genetic operator incorporated as a transition tensor [25, 26].
Reference: [10] <institution> Gyorgyi G 1990 Phys. </institution> <note> Rev. A 41 7097-7100 </note>
Reference-contexts: For large N it is possible to calculate the entropy of solutions compatible with the total training set and there is a first-order transition to perfect generalization as the size of training set is increased <ref> [10, 22] </ref>. This transition occurs for O (N ) patterns and beyond the transition the weights of the teacher are the only weights compatible with the training set.
Reference: [11] <institution> Holland J H 1975 Adaptation in Natural and Artificial Systems (The University of Michigan Press, </institution> <address> Ann Arbor) </address>
Reference-contexts: 1. Introduction Genetic Algorithms (GAs) are adaptive search techniques, which can be used to find low energy states in poorly characterized, high-dimensional energy landscapes <ref> [8, 11] </ref>. They have already been successfully applied in a large range of domains [2] and a review of the literature shows that they are becoming increasingly popular. <p> Many other theoretical approaches are based on the intuitive idea that above average fitness building blocks are preferentially sampled by the GA, which, if they can be usefully recombined, results in highly fit individuals being produced <ref> [8, 11] </ref>.
Reference: [12] <institution> Muhlenbein H and Schlierkamp-Voosen D 1995 Lecture Notes in Computer Science 899 142-168 </institution>
Reference-contexts: This method uses a microscopic description and is difficult to apply to specific problems of realistic size due to high-dimensionality of the equations of motion. More recently, a number of results have been derived for the performance of a GA on a class of simple additive problems <ref> [1, 12, 20] </ref>. These approaches use a macroscopic description, but assume a particular form for the distribution of macroscopics which is only applicable in large populations and for a specific class of problem.
Reference: [13] <institution> Peck C C and Dhawan A P 1995 Evolutionary Computation 3 1 39-80 </institution>
Reference-contexts: Macroscopic descriptions can result in low-dimensional equations which can be more easily studied. Another formalism based on the evolution of parent distributions was developed by Peck and Dhawan <ref> [13] </ref>, but they did not use the formalism to develop equations describing finite population dynamics. The importance of choosing appropriate quantities to average is well-known in statistical physics, but does not seem to be widely appreciated in genetic algorithm theory. <p> Thus, many results are only accurate in the infinite population limit, even though this limit is not taken explicitly. For example, Srinivas and Patnaik [23] and Peck and Dhawan <ref> [13] </ref> both produce equations for the moments of the fitness distribution in terms of the moments of the initial distribution. These are moments of the average distribution. Consequently, the equations do not correctly describe a finite population and results presented in these papers reflect that.
Reference: [14] <institution> Prugel-Bennett A 1996 Modelling Evolving Populations, NORDITA, Blegdamsvej 17, DK-2100 Copenhagen, Denmark, </institution> <note> (submitted for publication) </note>
Reference-contexts: A formalism has been developed by Prugel-Bennett, Shapiro and Rattray which describes the dynamics of a simple GA using methods from statistical mechanics <ref> [14, 15, 16, 17] </ref>. This formalism has been successfully applied to a number of simple Ising systems and has been used to determine optimal settings for some of the GA search parameters [21]. <p> In the following sections, difference equations will be derived for the average change of a small set of these macroscopics, due to each operator. A more exact approach considers fluctuations from mean behaviour by modelling the evolution of an ensemble of populations described by a set of order parameters <ref> [14] </ref>. Here, it is assumed that the dynamics average sufficiently well so that we can describe the dynamics in terms of deterministic equations for the average behaviour of each macroscopic. <p> mean correlation is q and is defined by, q = P (P 1) ff=1 fi&gt;ff q fffi (14) In order to model a finite population we consider that P population members are randomly sampled from an infinite population, which is described by a set of infinite population cumulants, K n <ref> [14] </ref>. The expectation values for the mean correlation and the first cumulant of a finite population are equal to the infinite population values. <p> reference [16] for a derivation), P 2 = 1 P 3 + P 2 P 4 = 1 P 12 6 Although we model the evolution of a finite population, it is more natural to follow the macroscopics associated with the infinite population from which the finite population is sampled <ref> [14] </ref>. The expected cumulants of a finite population can be retrieved through equations (15a) to (15d). 4. Crossover and mutation The mean effects of standard crossover and mutation on the distribution of overlaps within the population are equivalent to the paramagnet results given in [16]. <p> The expected cumulants of a finite population after crossover are determined from equations (15a) to (15d). 5. The cumulants after selection Under selection, P new population members are chosen from the present population with replacement. Following Prugel-Bennett we split this operation into two stages <ref> [14] </ref>. First we randomly sample P population members from an infinite population in order to create a finite population. Then an infinite population is generated from this finite population by selection.
Reference: [15] <author> Prugel-Bennett A and Shapiro J L 1994 Phys. </author> <title> Rev. Lett. 72 1305 [16] ||1995 The Dynamics of a Genetic Algorithm for Simple Random Ising Systems, </title> <institution> Computer Science Dept., University of Manchester, </institution> <address> Oxford Road, Manchester M13 9PL, </address> <note> UK (to appear in Physica D) </note>
Reference-contexts: A formalism has been developed by Prugel-Bennett, Shapiro and Rattray which describes the dynamics of a simple GA using methods from statistical mechanics <ref> [14, 15, 16, 17] </ref>. This formalism has been successfully applied to a number of simple Ising systems and has been used to determine optimal settings for some of the GA search parameters [21]. <p> Under selection, new population members are chosen from the present population with replacement, with a probability proportional to their Boltzmann weight. The selection strength fi is analogous to the inverse temperature and determines the intensity of selection, with larger fi leading to a higher variance of selection probabilities <ref> [3, 15] </ref>. Under standard uniform crossover, the population is divided into pairs at random and the new population is produced by swapping weights at each site within a pair with some fixed probability. <p> Two improvements are required to model selection accurately; the population should be finite and the distribution from which it is drawn should be modelled in terms of more than two cumulants, going beyond a Gaussian approximation <ref> [15] </ref>. The higher cumulants play a particularly important role in selection which will be described in section 5.1 [16]. <p> Following Prugel-Bennett and Shapiro one can use Derrida's trick and express the logarithm as an integral in order to decouple the average <ref> [4, 15] </ref>. hlog Z s i = Z 1 dt t Z 1 dt t 11 where, f (t; fi; fl) = dR dE p (R) p (EjR) exp te fiE+flR (24) The distribution of overlaps within an infinite population is approximated by a cumulant expansion around a Gaussian distribution [16], <p> Under these simplifying assumptions one finds, E g (R) ' @ cos 1 (K 1 ) q 1 A (26) N cos 1 (K 1 ) 1 Following Prugel-Bennett and Shapiro <ref> [15] </ref>, one can expand the integrand in equation (23) for small fi (as long as is at least O (1) so that the variance of p (EjR) is O (N )), P t 2 i 1 (fi; fl) ! where, ^ae n (fi; fl) = dR dE p (R) p (EjR) <p> For zero noise the selection strength should be scaled so that the effective selection strength kfi is inversely proportional to the standard deviation of the population <ref> [15] </ref>, fi = k 2 Here, k is defined in equation (32) and fi s is the scaled selection strength and remains fixed throughout the searchy.
Reference: [17] <institution> Rattray L M 1995 Complex Systems 9(3) 213-234 </institution>
Reference-contexts: A formalism has been developed by Prugel-Bennett, Shapiro and Rattray which describes the dynamics of a simple GA using methods from statistical mechanics <ref> [14, 15, 16, 17] </ref>. This formalism has been successfully applied to a number of simple Ising systems and has been used to determine optimal settings for some of the GA search parameters [21]. <p> All other relevant properties of the population after crossover can be found from the maximum entropy ansatz. A more general method is to follow the evolution of a number of cumulants explicitly, as in references <ref> [16, 17] </ref>, but this is unnecessary here because of the special form of crossover used, which is not appropriate in problems with stronger spatial interactions. 8 3.3. <p> To make the problem tractable, it is assumed that before selection the population is at maximum entropy with constraints on the mean overlap and correlation within the population, as discussed in Appendix A. The calculation presented here is similar to that presented elsewhere <ref> [17] </ref>, except for a minor refinement which seems to be important when considering problems with noise under selection.
Reference: [18] <institution> Saad D and Solla S A 1995 Phys. </institution> <address> Rev. E 52 4225 </address>
Reference-contexts: It would be interesting to see how the dynamics of the GA compares to gradient methods in networks with continuous weights, for which the dynamics of generalization for a class of multi-layer architectures have recently been solved analytically in the case of on-line learning <ref> [18] </ref>.
Reference: [19] <editor> Schaffer J D, Whitley D and Eshelman L J 1992 Proc. </editor> <booktitle> of the Int. Conf. on Combinations of Genetic Algorithms and Neural Networks (IEEE Computer Society Press, </booktitle> <address> Los Alamitos, CA) p 1-37 </address>
Reference-contexts: They have already been successfully applied in a large range of domains [2] and a review of the literature shows that they are becoming increasingly popular. In particular, GAs have been used in a number of machine learning applications, including the design and training of artificial neural networks <ref> [7, 19, 28] </ref>. In the simple GA considered here, each population member is represented by a genotype, in this case a binary string, and an objective function assigns an energy x email: rattraym@cs.man.ac.uk 2 to each such genotype.
Reference: [20] <institution> Thierens D and Goldberg D 1995 Lecture Notes in Computer Science 866 119-129 </institution>
Reference-contexts: This method uses a microscopic description and is difficult to apply to specific problems of realistic size due to high-dimensionality of the equations of motion. More recently, a number of results have been derived for the performance of a GA on a class of simple additive problems <ref> [1, 12, 20] </ref>. These approaches use a macroscopic description, but assume a particular form for the distribution of macroscopics which is only applicable in large populations and for a specific class of problem.
Reference: [21] <author> Shapiro J L, </author> <booktitle> Prugel-Bennett A and Rattray L M 1994 Lecture Notes in Computer Science 865 17 </booktitle>
Reference-contexts: This formalism has been successfully applied to a number of simple Ising systems and has been used to determine optimal settings for some of the GA search parameters <ref> [21] </ref>. It describes problems of realistic size and includes finite population effects, which have been shown to be crucial to understanding how the GA searches.
Reference: [22] <author> Sompolinsky H and Tishby N 1990 Phys. </author> <title> Rev. </title> <journal> Lett. </journal> <pages> 65 1683-86 </pages>
Reference-contexts: For large N it is possible to calculate the entropy of solutions compatible with the total training set and there is a first-order transition to perfect generalization as the size of training set is increased <ref> [10, 22] </ref>. This transition occurs for O (N ) patterns and beyond the transition the weights of the teacher are the only weights compatible with the training set.
Reference: [23] <editor> Srinivas M and Patnaik L M 1996 IEEE Trans. </editor> <booktitle> Knowledge Data Eng. </booktitle> <pages> 8 1 120-133 </pages>
Reference-contexts: Thus, many results are only accurate in the infinite population limit, even though this limit is not taken explicitly. For example, Srinivas and Patnaik <ref> [23] </ref> and Peck and Dhawan [13] both produce equations for the moments of the fitness distribution in terms of the moments of the initial distribution. These are moments of the average distribution. Consequently, the equations do not correctly describe a finite population and results presented in these papers reflect that.
Reference: [24] <institution> Syswerda G 1993 Foundations of Genetic Algorithms 2 ed Whitley L D (Morgan Kaufmann, </institution> <address> San Mateo, CA) </address>
Reference-contexts: Here, bit-simulated crossover is used, with new population members created by selecting weights at each site from any population member in the original population with equal probability <ref> [24] </ref>. In practice, the weights at every site are completely shu*ed within the population and this brings the population straight to the fixed point of standard crossover.
Reference: [25] <institution> Vose M D and Liepins G E 1991 Complex Systems 5 31 </institution>
Reference-contexts: A rigorous approach introduced by Vose et al describes the population dynamics as a dynamical system in a high-dimensional Euclidean space, with each genetic operator incorporated as a transition tensor <ref> [25, 26] </ref>. This method uses a microscopic description and is difficult to apply to specific problems of realistic size due to high-dimensionality of the equations of motion.
Reference: [26] <editor> Vose M D 1993 Foundations of Genetic Algorithms 2 ed Whitley L D (Morgan Kaufmann, </editor> <address> San Mateo, Calif.) p 63 </address>
Reference-contexts: A rigorous approach introduced by Vose et al describes the population dynamics as a dynamical system in a high-dimensional Euclidean space, with each genetic operator incorporated as a transition tensor <ref> [25, 26] </ref>. This method uses a microscopic description and is difficult to apply to specific problems of realistic size due to high-dimensionality of the equations of motion.
Reference: [27] <institution> Vose M D and Wright A H 1995 Evolutionary Computation 2 4 347-368 </institution>
Reference-contexts: Other researchers have introduced theories based on averages. A description of GA dynamics in terms of the evolution of the parent distribution from which finite populations are sampled was produced by Vose and Wright <ref> [27] </ref>. This microscopic approach provides a description of the finite population effects which is elegant and correct. However, like other microscopic descriptions it is difficult to apply to specific realistic problems due to the enormous dimensionality of the system.
Reference: [28] <editor> Yao X 1993 Int. J. </editor> <booktitle> of Neural Systems 4(3) 203-222 </booktitle>
Reference-contexts: They have already been successfully applied in a large range of domains [2] and a review of the literature shows that they are becoming increasingly popular. In particular, GAs have been used in a number of machine learning applications, including the design and training of artificial neural networks <ref> [7, 19, 28] </ref>. In the simple GA considered here, each population member is represented by a genotype, in this case a binary string, and an objective function assigns an energy x email: rattraym@cs.man.ac.uk 2 to each such genotype.
References-found: 27

