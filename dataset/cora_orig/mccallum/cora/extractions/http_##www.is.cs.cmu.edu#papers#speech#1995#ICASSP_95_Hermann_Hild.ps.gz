URL: http://www.is.cs.cmu.edu/papers/speech/1995/ICASSP_95_Hermann_Hild.ps.gz
Refering-URL: http://www.is.cs.cmu.edu/ISL.speech.publications.html
Root-URL: 
Title: LANGUAGE MODELS FOR A SPELLED LETTER RECOGNIZER  
Author: Martin Betz and Hermann Hild 
Date: May 1995.  
Note: To appear in: Proc. IEEE International Conf. on Acoustics, Speech, and Signal Processing, Detroit, USA,  
Address: 76128 Karlsruhe, Germany  Pittsburgh, USA  
Affiliation: Interactive Systems Laboratories University of Karlsruhe  Carnegie Mellon University  
Abstract: In some speech recognition applications, it is reasonable to constrain the search space of a speech rec-ognizer to a large but finite set of sentences. We demonstrate the problem on a spelling task, where the recognition of continuously spelled last names is constrained to 110,000 entries (= 43,000 unique names) of a telephone book. Several techniques to address this problem are compared: recognition without any language model, bigrams, functions to map a hypothesis onto a legal string, n-best lists, and finally a newly developed method which integrates all constraints directly into the search process within reasonable memory and time bounds. The baseline result of 56% string accuracy is improved to 62, 85, 88, and 92%, respectively. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Austin, R. Schwartz, and P. Placeway. </author> <title> The Forward-Backward Search Algorithm. </title> <booktitle> In Proc. of the International Conference on Acoustics, Speech and Signal Processing. IEEE, </booktitle> <year> 1991. </year>
Reference-contexts: Good results are achieved with k &gt;= 40 active hypotheses. Significant loss in performance was observed with k &lt; 20 hypotheses. With beam search, the "Forward-Backward Algorithm" <ref> [1] </ref> and some other pruning techniques, the search time could be reduced from 12 sec/string to about 3 secs/string (on a HP 735 workstation), still achieving 92.7% string and 97.7% word accuracy. However, this is still almost one order of magnitude slower than the recognition without any language model.
Reference: [2] <author> R. A. Cole, M. Fanty, Gopalakrishnan, and R. D. Janssen. </author> <title> Speaker-Independent Name Retrival from Spellings using a Database of 50,000 Names. </title> <booktitle> In Proc. of the International Conference on Acoustics, Speech and Signal Processing, </booktitle> <address> Toronto, </address> <publisher> IEEE, </publisher> <month> May </month> <year> 1991. </year>
Reference: [3] <author> P. Haffner, M. Franzini, and A. Waibel. </author> <title> Integrating Time Alignment and Neural Networks for High Performance Continuous Speech Recognition. </title> <booktitle> In Proc. International Conference on Acoustics, Speech, and Signal Processing. IEEE, </booktitle> <month> May </month> <year> 1991. </year>
Reference-contexts: In this paper, we will demonstrate our letter recognizer and the effects of various language models and search techniques on the task of spelled name recognition. Related work on isolated letters was reported by Cole et. al.[2]. 2. THE LETTER RECOGNIZER The Multi-State Time Delay Neural Network (MS-TDNN) <ref> [3, 5] </ref> integrates the time-shift invariant architecture of a TDNN and a nonlinear time alignment procedure (DTW) into a high accuracy word-level Input Layer 16 melscale FFT coefficients at a 10 msec frame rate Hidden Layer 15 hidden units Phoneme L. 59 phoneme units (only 9 shown) DTW Layer 27 word
Reference: [4] <author> H. Hild and A. Waibel. </author> <title> Multi-Speaker/Speaker-Independent Architectures for the Multi-State Time Delay Neural Network. </title> <booktitle> In Proc. International Conf. on Acoustics, Speech, and Signal Processing. IEEE, </booktitle> <year> 1993. </year>
Reference: [5] <author> H. Hild and A. Waibel. </author> <title> Speaker-Independent Connected Letter Recognition With a Multi-Sate Time Delay Neural Network. </title> <booktitle> In 3rd European Conf. on Speech, Communication and Technology, </booktitle> <month> Sept. </month> <year> 1993. </year>
Reference-contexts: In this paper, we will demonstrate our letter recognizer and the effects of various language models and search techniques on the task of spelled name recognition. Related work on isolated letters was reported by Cole et. al.[2]. 2. THE LETTER RECOGNIZER The Multi-State Time Delay Neural Network (MS-TDNN) <ref> [3, 5] </ref> integrates the time-shift invariant architecture of a TDNN and a nonlinear time alignment procedure (DTW) into a high accuracy word-level Input Layer 16 melscale FFT coefficients at a 10 msec frame rate Hidden Layer 15 hidden units Phoneme L. 59 phoneme units (only 9 shown) DTW Layer 27 word
Reference: [6] <author> H. Ney. </author> <title> The Use of a One-Stage Dynamic Programming Algorithm for Connected Word Recognition. </title> <booktitle> In Transactions on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 263-271. </pages> <publisher> IEEE, </publisher> <month> April </month> <year> 1984. </year>
Reference-contexts: For example, in the minFSG graph, the letter E occurs in over 5,800 transitions. Since the full left context of a string is considered during the search, each transition may have a different individual accumulated search score. Therefore, if the conventional one-stage dynamic time warping (DTW) search <ref> [6] </ref> is employed, one individual word model is needed in the DTW search matrix for every transition in the minFSG graph. With a total of 57,713 transitions, this results in a prohibitively time and memory consuming search process.
Reference: [7] <author> H. Sakoe. </author> <title> Two Level DP-Matching ADynamic Programming-Based Pattern Matching Algorithm for Connected Word Recognition. In Transactions on Acoustic, Speech, </title> <booktitle> Signal Processing, </booktitle> <volume> volume ASSP-27, </volume> <pages> pages 588-595, </pages> <month> Dec. </month> <year> 1979. </year>
Reference-contexts: With a total of 57,713 transitions, this results in a prohibitively time and memory consuming search process. To remedy the problem, we use a technique similar to the "Two-Level DP-Matching" <ref> [7] </ref>: The letter E occurs in over 5,800 dif ferent contexts, but the partial optimal score through its word model is independent of the search context.
Reference: [8] <author> M.Woszczyna, N.Aoki-Waibel, F.D.But, N.Coccaro, K.Horiguchi, T.Kemp, A.Lavie, A.McNair, T.Polzin, I.Rogina, C.P.Rose, T.Schultz, B.Suhm M.Tomita, </author> <title> A.Waibel Janus 93: Towards Spontaneous Speech Translation. </title> <booktitle> In Proc. International Conf. on Acoustics, Speech, and Signal Processing. IEEE, </booktitle> <month> May </month> <year> 1994. </year>
Reference-contexts: Labs string acc. (%) No language model 56.4 Bigrams 62.1 Closest match 85.0 N-best 88.1 Full constraints 92.7 Table 4: Summary of results for different language models and search techniques. who contributed with discussions and support, especially Monika Woszczyna for helping out with the n-best search module from the JANUS <ref> [8] </ref> speech recog nizer, and Alex Waibel.
References-found: 8

