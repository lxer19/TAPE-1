URL: ftp://ftp.cs.umd.edu/pub/papers/TRs/3250.ps.Z
Refering-URL: http://www.cs.umd.edu/users/davew/pubs.html
Root-URL: 
Email: pugh@cs.umd.edu davew@cs.umd.edu  
Title: Static Analysis of Upper and Lower Bounds on Dependences and Parallelism  
Author: William Pugh David Wonnacott 
Note: This work is supported by an NSF PYI grant CCR-9157384 and by a Packard Fellowship.  
Address: College Park, MD 20742  
Affiliation: Institute for Advanced Computer Studies Dept. of Computer Science Dept. of Computer Science Univ. of Maryland,  
Date: December, 1992  Revised March, 1994  
Pubnum: UMIACS-TR-94-40  CS-TR-3250  
Abstract: Existing compilers often fail to parallelize sequential code, even when a program can be manually transformed into parallel form by a sequence of well-understood transformations (as is the case for many of the Perfect Club Benchmark programs). These failures can occur for several reasons: the code transformations implemented in the compiler may not be sufficient to produce parallel code, the compiler may not find the proper sequence of transformations, or the compiler may not be able to prove that one of the necessary transformations is legal. When a compiler extract sufficient parallelism from a program, the programmer extract additional parallelism. Unfortunately, the programmer is typically left to search for parallelism without significant assistance. The compiler generally does not give feedback about which parts of the program might contain additional parallelism, or about the types of transformations that might be needed to realize this parallelism. Standard program transformations and dependence abstractions cannot be used to provide this feedback. In this paper, we propose a two step approach for the search for parallelism in sequential programs: We first construct several sets of constraints that describe, for each statement, which iterations of that statement can be executed concurrently. By constructing constraints that correspond to different assumptions about which dependences might be eliminated through additional analysis, transformations and user assertions, we can determine whether we can expose parallelism by eliminating dependences. In the second step of our search for parallelism, we examine these constraint sets to identify the kinds of transformations that are needed to exploit scalable parallelism. Our tests will identify conditional parallelism and parallelism that can be exposed by combinations of transformations that reorder the iteration space (such as loop interchange and loop peeling). This approach lets us distinguish inherently sequential code from code that contains unexploited parallelism. It also produces information about the kinds of transformations that will be needed to parallelize the code, without worrying about the order of application of the transformations. Furthermore, when our dependence test is inexact, we can identify which unresolved dependences inhibit parallelism by comparing the effects of assuming dependence or independence. We are currently exploring the use of this information in programmer-assisted parallelization. 
Abstract-found: 1
Intro-found: 1
Reference: [AI91] <author> Corinne Ancourt and Fran~cois Irigoin. </author> <title> Scanning polyhedra with DO loops. </title> <booktitle> In Proc. of the 3rd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 39-50, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: When standard Fourier-Motzkin variable elimination cannot eliminate an integer variable exactly, we can sometimes eliminate the variable exactly by introducing quasi-linear constraints (constraints containing floor and ceiling operators) <ref> [AI91] </ref>. For example, (9ff s:t: x = 5ff) dx=5e bx=5c. In these cases, negation is easy to apply (e.g., :(dx=5e bx=5c) (dx=5e &gt; bx=5c)).
Reference: [AK87] <author> J.R. Allen and K. Kennedy. </author> <title> Automatic translation of Fortran programs to vector form. </title> <journal> ACM Trans. on Programming Languages and Systems, </journal> <volume> 9(4) </volume> <pages> 491-542, </pages> <month> October </month> <year> 1987. </year>
Reference: [AL93] <author> Saman P. Amarasinghe and Monica S. Lam. </author> <title> Communication optimization and code generation for distributed memory machines. </title> <booktitle> In ACM '93 Conf. on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: Both Feautrier [Fea88, Fea91] and Maydan, Amarasinghe and Lam [MAL93] describe how to use value-based dependence information for array expansion or privatization. Amarasinghe and Lam <ref> [AL93] </ref> describe ways to use value-based dependence analysis in analyzing and generating code for distributed memory multi-computers.
Reference: [B + 89] <author> M. Berry et al. </author> <title> The PERFECT Club benchmarks: Effective performance evaluation of supercomputers. </title> <journal> International Journal of Supercomputing Applications, </journal> <volume> 3(3) </volume> <pages> 5-40, </pages> <month> March </month> <year> 1989. </year>
Reference: [Ban88] <author> Utpal Banerjee. </author> <title> An introduction to a formal theory of dependence analysis. </title> <journal> Journal of Supercomputing, </journal> <volume> 2(2) </volume> <pages> 133-149, </pages> <year> 1988. </year>
Reference: [Ban90] <author> U. Banerjee. </author> <title> Unimodular transformations of double loops. </title> <booktitle> In Proc. of the 3rd Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 192-219, </pages> <address> Irvine, CA, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: If desired, directional peeling can be applied (Section 5.3.1). 5.1 Using Unimodular Loop Transformation Techniques We can hand the self-dependences off to a module that looks for a unimodular loop transformation that will expose the most parallelism <ref> [Ban90, WL90, KKB92] </ref>. This module can report whether the self-dependences allow for coarse-grain parallelism, and if not, whether the self-dependences allow for fine-grain parallelism. Standard unimodular techniques will find any parallelism that can be exposed using loop interchange, loop skewing and loop reversal.
Reference: [Bra88] <author> Thomas Brandes. </author> <title> The importance of direct dependences for automatic parallelism. </title> <booktitle> In Proc of 1988 International Conference on Supercomputing, </booktitle> <pages> pages 407-417, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: In addition to the blow-up problem, the methods described by [Fea91] and [MAL93] fail when forced to negate certain pathological cases of non-convex constraints. Our methods can handle these (although some of the pathological cases will create performance problems). Our methods for array data-flow analysis, like those of <ref> [Bra88, Rib90, Fea91, MAL92, Voe92a, Voe92b, MAL93] </ref>, are based on extending standard array dependence analysis methods to analysis the flow of values rather than the reuse of memory. Array data-flow dependence analysis methods such as [GS90, Ros90, Li92, DGS93] are based on extending scalar data-flow analysis methods to arrays.
Reference: [Coo72] <author> D. C. Cooper. </author> <title> Theorem proving in arithmetic with multiplication. </title> <editor> In B. Meltzer and D. Michie, editors, </editor> <booktitle> Machine Intelligence 7, </booktitle> <pages> pages 91-99. </pages> <publisher> American Elsevier, </publisher> <address> New York, </address> <year> 1972. </year>
Reference: [DGS93] <author> Evelyn Duesterwald, Rajiv Gupta, and Mary Lou Soffa. </author> <title> A practical data flow framework for array reference analysis and its use in optimizations. </title> <booktitle> In ACM '93 Conf. on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: Our methods for array data-flow analysis, like those of [Bra88, Rib90, Fea91, MAL92, Voe92a, Voe92b, MAL93], are based on extending standard array dependence analysis methods to analysis the flow of values rather than the reuse of memory. Array data-flow dependence analysis methods such as <ref> [GS90, Ros90, Li92, DGS93] </ref> are based on extending scalar data-flow analysis methods to arrays. In general, the later approach deals better with control flow but the former approach gives more information about which iteration is dependent on which iteration (as opposed to simply summarizing which loops carry the dependence).
Reference: [EHLP91] <author> R. Eigenmann, J. Hoeflinger, Z. Li, and D. Padua. </author> <title> Experience in the automatic paral-lelization of 4 Perfect benchmark programs. </title> <booktitle> In Proc. of the 4th Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1991. </year>
Reference-contexts: Example dialog generated for INTERF extract 6.1 Extended Example: INTERF We now describe an application of our techniques to the routine INTERF from the Perfect Club benchmark MDG. This routine contains substantial parallelism that is not exploited by current compilers <ref> [EHLP91] </ref>. Figure 4 shows a condensed version of this code after structuring by VAST-90 (from Pacific-Sierra Research) and automatic induction variable recognition. We have omitted statements that are easily parallelizable or equivalent to other statements that were left in.
Reference: [Eig92] <author> Rudolf Eigenmann. </author> <title> Toward a methodology of optimizing programs for high-performance computers. CSRD Rpt. </title> <type> 1178, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> August </month> <year> 1992. </year>
Reference: [Eig93] <author> R. Eigenmann. </author> <title> Towards a methodology of optimizing programs for high-performance computers. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <pages> pages 27-36, </pages> <month> July </month> <year> 1993. </year>
Reference: [Fea88] <author> Paul Feautrier. </author> <title> Array expansion. </title> <booktitle> In ACM Int. Conf. on Supercomputing, St Malo, </booktitle> <pages> pages 429-441, </pages> <year> 1988. </year>
Reference-contexts: there is no loop-carried value-based flow dependence between the two array references. 3.7 Interactions If a program contains only linear terms and we do not recognize reduction dependences, we can eliminate all dependences other than value-base flow dependences by a sequence of storage-dependence breaking transformations (e.g., expansion, priva-tization, and renaming) <ref> [Fea88, Fea91] </ref>. That is, we can completely separate value flow and memory usage issues for these programs. However, parallel reductions and/or non-linear terms may prevent us eliminating all storage-based dependences. 3.7.1 Non-linear terms. <p> There are a number of papers on techniques to analyze array kills and value-based array data dependence [Bra88, Fea88, GS90, Ros90, Rib90, Fea91, Li92, MAL92, May92, MAL93, DGS93, Mas94]. Of these, only the technique we describe here and that of Feautrier <ref> [Fea88, Fea91] </ref> and of Maslov [Mas94] are complete over the domain of affine expressions and no control flow other than loops. <p> In general, the later approach deals better with control flow but the former approach gives more information about which iteration is dependent on which iteration (as opposed to simply summarizing which loops carry the dependence). Both Feautrier <ref> [Fea88, Fea91] </ref> and Maydan, Amarasinghe and Lam [MAL93] describe how to use value-based dependence information for array expansion or privatization. Amarasinghe and Lam [AL93] describe ways to use value-based dependence analysis in analyzing and generating code for distributed memory multi-computers.
Reference: [Fea91] <author> Paul Feautrier. </author> <title> Dataflow analysis of array and scalar references. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 20(1), </volume> <month> February </month> <year> 1991. </year>
Reference-contexts: there is no loop-carried value-based flow dependence between the two array references. 3.7 Interactions If a program contains only linear terms and we do not recognize reduction dependences, we can eliminate all dependences other than value-base flow dependences by a sequence of storage-dependence breaking transformations (e.g., expansion, priva-tization, and renaming) <ref> [Fea88, Fea91] </ref>. That is, we can completely separate value flow and memory usage issues for these programs. However, parallel reductions and/or non-linear terms may prevent us eliminating all storage-based dependences. 3.7.1 Non-linear terms. <p> Excerpts from SUBROUTINE INTERF of MDG benchmark Static Analysis of Upper and Lower Bounds on Dependences and Parallelism 23 Sun SPARCstation 10-51 Execution time (millisecs) Dependence Analysis Memory-based Value-based From Code f77 -c -O2 All dependences In Cycles <ref> [Fea91] </ref> across 300 1.5 2.7 2.1 burg 400 5.5 31 26 relax 200 1.6 8.7 5.2 gosser 400 2.3 23 14 choles 300 2.7 11 9.3 lanczos 1000 11 41 35 jacobi 900 150 300 250 [MAL93] extract from ocean 200 4.5 6.8 5.1 Perfect TFRD: olda; simplified 1700 26 320 <p> In [PW93], we give full descriptions of our techniques for simplifying formulas containing negation (as mentioned in Section 3.5.1). We also compare the performance of the technique described in this paper for analysis of array kills with that of <ref> [Fea91, MAL93] </ref>. In other papers [Pug91, KP93], we describe a unified framework for reordering transformations. Within this framework, we discuss methods for checking the legality of a transformation, generating transformations, and producing transformed code corresponding to a reordering transformation. <p> There are a number of papers on techniques to analyze array kills and value-based array data dependence [Bra88, Fea88, GS90, Ros90, Rib90, Fea91, Li92, MAL92, May92, MAL93, DGS93, Mas94]. Of these, only the technique we describe here and that of Feautrier <ref> [Fea88, Fea91] </ref> and of Maslov [Mas94] are complete over the domain of affine expressions and no control flow other than loops. <p> Maslov's techniques achieve speed comparable to ours, and utilize our algorithms for manipulating and analyzing Presburger formulas. Both our methods and Maslov's methods handle more complicated control flow than Feautrier, such as if's with non-affine guards, although inexactly. The methods described by [MAL93] improve on the efficiency of <ref> [Fea91] </ref> but do not work for certain cases handled by [Fea91] and this paper. The quast's (Quasi-Affine Search Trees)/Last-Write-Trees constructed by [Fea91, MAL93] may contain infeasible paths. To enable compile-time transformations such as privatization, it is necessary to determine which of these paths are feasible. <p> Both our methods and Maslov's methods handle more complicated control flow than Feautrier, such as if's with non-affine guards, although inexactly. The methods described by [MAL93] improve on the efficiency of <ref> [Fea91] </ref> but do not work for certain cases handled by [Fea91] and this paper. The quast's (Quasi-Affine Search Trees)/Last-Write-Trees constructed by [Fea91, MAL93] may contain infeasible paths. To enable compile-time transformations such as privatization, it is necessary to determine which of these paths are feasible. <p> The methods described by [MAL93] improve on the efficiency of [Fea91] but do not work for certain cases handled by [Fea91] and this paper. The quast's (Quasi-Affine Search Trees)/Last-Write-Trees constructed by <ref> [Fea91, MAL93] </ref> may contain infeasible paths. To enable compile-time transformations such as privatization, it is necessary to determine which of these paths are feasible. <p> Converting these expressions into disjunctive normal form would be infeasible for many real problems. The methods we describe in [PW93] should control this blow-up. In addition to the blow-up problem, the methods described by <ref> [Fea91] </ref> and [MAL93] fail when forced to negate certain pathological cases of non-convex constraints. Our methods can handle these (although some of the pathological cases will create performance problems). <p> In addition to the blow-up problem, the methods described by [Fea91] and [MAL93] fail when forced to negate certain pathological cases of non-convex constraints. Our methods can handle these (although some of the pathological cases will create performance problems). Our methods for array data-flow analysis, like those of <ref> [Bra88, Rib90, Fea91, MAL92, Voe92a, Voe92b, MAL93] </ref>, are based on extending standard array dependence analysis methods to analysis the flow of values rather than the reuse of memory. Array data-flow dependence analysis methods such as [GS90, Ros90, Li92, DGS93] are based on extending scalar data-flow analysis methods to arrays. <p> In general, the later approach deals better with control flow but the former approach gives more information about which iteration is dependent on which iteration (as opposed to simply summarizing which loops carry the dependence). Both Feautrier <ref> [Fea88, Fea91] </ref> and Maydan, Amarasinghe and Lam [MAL93] describe how to use value-based dependence information for array expansion or privatization. Amarasinghe and Lam [AL93] describe ways to use value-based dependence analysis in analyzing and generating code for distributed memory multi-computers.
Reference: [GKT91] <author> G. Goff, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Practical dependence testing. </title> <booktitle> In ACM SIGPLAN'91 Conference on Programming Language Design and Implementation, </booktitle> <year> 1991. </year>
Reference-contexts: As part of this work [Pug91], we describe dependence relations and techniques for computing their transitive closure. There have been a number of papers on improving the accuracy of memory-based array data dependence analysis <ref> [KKP90, LC90, WT92, GKT91, MHL91b] </ref>. Some of these methods provide ways of recognizing when their results are exact, but do not describe any methods for computing lower bounds on dependences when they are not exact.
Reference: [GS90] <author> Thomas Gross and Peter Steenkiste. </author> <title> Structured dataflow analysis for arrays and its use in an optimizing compiler. </title> <journal> Software Practice and Experience, </journal> <volume> 20 </volume> <pages> 133-155, </pages> <month> February </month> <year> 1990. </year> <note> 28 William Pugh and Dave Wonnacott </note>
Reference-contexts: Our methods for array data-flow analysis, like those of [Bra88, Rib90, Fea91, MAL92, Voe92a, Voe92b, MAL93], are based on extending standard array dependence analysis methods to analysis the flow of values rather than the reuse of memory. Array data-flow dependence analysis methods such as <ref> [GS90, Ros90, Li92, DGS93] </ref> are based on extending scalar data-flow analysis methods to arrays. In general, the later approach deals better with control flow but the former approach gives more information about which iteration is dependent on which iteration (as opposed to simply summarizing which loops carry the dependence).
Reference: [IJT91] <author> Fran~cois Irigoin, Pierre Jouvelot, and Remi Triolet. </author> <title> Semantical interprocedural paral-lelization: An overview of the pips project. </title> <booktitle> In Proc. of the 1991 International Conference on Supercomputing, </booktitle> <pages> pages 244-253, </pages> <month> June </month> <year> 1991. </year>
Reference: [KK67] <author> G. Kreisel and J. L. Krevine. </author> <title> Elements of Mathematical Logic. </title> <publisher> North-Holland Pub. Co., </publisher> <year> 1967. </year>
Reference: [KKB92] <author> K. G. Kumar, D. Kulkarni, and A. Basu. </author> <title> Deriving good transformations for mapping nested loops on hieracical parallel machines in polynomial time. </title> <booktitle> In Proc. of the 1992 International Conference on Supercomputing, </booktitle> <pages> pages 82-92, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: If desired, directional peeling can be applied (Section 5.3.1). 5.1 Using Unimodular Loop Transformation Techniques We can hand the self-dependences off to a module that looks for a unimodular loop transformation that will expose the most parallelism <ref> [Ban90, WL90, KKB92] </ref>. This module can report whether the self-dependences allow for coarse-grain parallelism, and if not, whether the self-dependences allow for fine-grain parallelism. Standard unimodular techniques will find any parallelism that can be exposed using loop interchange, loop skewing and loop reversal.
Reference: [KKP90] <author> X. Kong, D. Klappholz, and K. Psarris. </author> <title> The I test: A new test for subscript data dependence. </title> <booktitle> In Proceedings of the 1990 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1990. </year>
Reference-contexts: As part of this work [Pug91], we describe dependence relations and techniques for computing their transitive closure. There have been a number of papers on improving the accuracy of memory-based array data dependence analysis <ref> [KKP90, LC90, WT92, GKT91, MHL91b] </ref>. Some of these methods provide ways of recognizing when their results are exact, but do not describe any methods for computing lower bounds on dependences when they are not exact.
Reference: [KP93] <author> Wayne Kelly and William Pugh. </author> <title> A framework for unifying reordering transformations. </title> <type> Technical Report CS-TR-3193, </type> <institution> Dept. of Computer Science, University of Maryland, College Park, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: This seems surprising, as it is difficult to see how this parallelism can be achieved, but Figure 2 shows code that achieves it. This transformation can be identified and generated automatically by the system described in <ref> [KP93] </ref>. Static Analysis of Upper and Lower Bounds on Dependences and Parallelism 21 forall i := 1 to n do for j := 1 to i-1 do endfor x (i) := x (i)+val (i,i)*v (i) /* s1 */ for j := i+1 to n do endfor endfor Fig. 2. <p> In [PW93], we give full descriptions of our techniques for simplifying formulas containing negation (as mentioned in Section 3.5.1). We also compare the performance of the technique described in this paper for analysis of array kills with that of [Fea91, MAL93]. In other papers <ref> [Pug91, KP93] </ref>, we describe a unified framework for reordering transformations. Within this framework, we discuss methods for checking the legality of a transformation, generating transformations, and producing transformed code corresponding to a reordering transformation.
Reference: [Lar93] <author> James R. Larus. </author> <title> Loop-level parallelism in numeric and symbolic programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(7) </volume> <pages> 812-826, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: Amarasinghe and Lam [AL93] describe ways to use value-based dependence analysis in analyzing and generating code for distributed memory multi-computers. There have been a number of recent papers <ref> [Lar93, PP93, MHL91a] </ref> describing the results of instrumenting programs so that during a run on sample data, information about the actual dependences and available parallelism are collected. However, we do not know of any other work on static analysis of upper bounds on parallelism. 9.
Reference: [LC90] <author> L. Lu and M. Chen. </author> <title> Subdomain dependence test for massive parallelism. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <address> New York, NY, </address> <month> November </month> <year> 1990. </year>
Reference-contexts: As part of this work [Pug91], we describe dependence relations and techniques for computing their transitive closure. There have been a number of papers on improving the accuracy of memory-based array data dependence analysis <ref> [KKP90, LC90, WT92, GKT91, MHL91b] </ref>. Some of these methods provide ways of recognizing when their results are exact, but do not describe any methods for computing lower bounds on dependences when they are not exact.
Reference: [Li92] <author> Zhiyuan Li. </author> <title> Array privatization for parallel execution of loops. </title> <booktitle> In Proc. of the 1992 International Conference on Supercomputing, </booktitle> <pages> pages 313-322, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Our methods for array data-flow analysis, like those of [Bra88, Rib90, Fea91, MAL92, Voe92a, Voe92b, MAL93], are based on extending standard array dependence analysis methods to analysis the flow of values rather than the reuse of memory. Array data-flow dependence analysis methods such as <ref> [GS90, Ros90, Li92, DGS93] </ref> are based on extending scalar data-flow analysis methods to arrays. In general, the later approach deals better with control flow but the former approach gives more information about which iteration is dependent on which iteration (as opposed to simply summarizing which loops carry the dependence).
Reference: [MAL92] <author> Dror E. Maydan, Saman P. Amarasinghe, and Monica S. Lam. </author> <title> Data dependence and data-flow analysis of arrays. </title> <booktitle> In 5th Workshop on Languages and Compilers for Parallel Computing (Yale University tech. report YALEU/DCS/RR-915), </booktitle> <pages> pages 283-292, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: In addition to the blow-up problem, the methods described by [Fea91] and [MAL93] fail when forced to negate certain pathological cases of non-convex constraints. Our methods can handle these (although some of the pathological cases will create performance problems). Our methods for array data-flow analysis, like those of <ref> [Bra88, Rib90, Fea91, MAL92, Voe92a, Voe92b, MAL93] </ref>, are based on extending standard array dependence analysis methods to analysis the flow of values rather than the reuse of memory. Array data-flow dependence analysis methods such as [GS90, Ros90, Li92, DGS93] are based on extending scalar data-flow analysis methods to arrays.
Reference: [MAL93] <author> Dror E. Maydan, Saman P. Amarasinghe, and Monica S. Lam. </author> <title> Array data-flow analysis and its use in array privatization. </title> <booktitle> In ACM '93 Conf. on Principles of Programming Languages, </booktitle> <month> January </month> <year> 1993. </year>
Reference-contexts: (millisecs) Dependence Analysis Memory-based Value-based From Code f77 -c -O2 All dependences In Cycles [Fea91] across 300 1.5 2.7 2.1 burg 400 5.5 31 26 relax 200 1.6 8.7 5.2 gosser 400 2.3 23 14 choles 300 2.7 11 9.3 lanczos 1000 11 41 35 jacobi 900 150 300 250 <ref> [MAL93] </ref> extract from ocean 200 4.5 6.8 5.1 Perfect TFRD: olda; simplified 1700 26 320 180 NASA btrix 4100 170 310 240 NAS cfft2d1 800 16 150 130 Kernels cholsky 1400 31 73 40 emit 1500 17 79 38 gmtry 1900 13 58 38 vpenta 1800 96 98 64 Table I. <p> In [PW93], we give full descriptions of our techniques for simplifying formulas containing negation (as mentioned in Section 3.5.1). We also compare the performance of the technique described in this paper for analysis of array kills with that of <ref> [Fea91, MAL93] </ref>. In other papers [Pug91, KP93], we describe a unified framework for reordering transformations. Within this framework, we discuss methods for checking the legality of a transformation, generating transformations, and producing transformed code corresponding to a reordering transformation. <p> Maslov's techniques achieve speed comparable to ours, and utilize our algorithms for manipulating and analyzing Presburger formulas. Both our methods and Maslov's methods handle more complicated control flow than Feautrier, such as if's with non-affine guards, although inexactly. The methods described by <ref> [MAL93] </ref> improve on the efficiency of [Fea91] but do not work for certain cases handled by [Fea91] and this paper. The quast's (Quasi-Affine Search Trees)/Last-Write-Trees constructed by [Fea91, MAL93] may contain infeasible paths. <p> The methods described by [MAL93] improve on the efficiency of [Fea91] but do not work for certain cases handled by [Fea91] and this paper. The quast's (Quasi-Affine Search Trees)/Last-Write-Trees constructed by <ref> [Fea91, MAL93] </ref> may contain infeasible paths. To enable compile-time transformations such as privatization, it is necessary to determine which of these paths are feasible. <p> Converting these expressions into disjunctive normal form would be infeasible for many real problems. The methods we describe in [PW93] should control this blow-up. In addition to the blow-up problem, the methods described by [Fea91] and <ref> [MAL93] </ref> fail when forced to negate certain pathological cases of non-convex constraints. Our methods can handle these (although some of the pathological cases will create performance problems). <p> In addition to the blow-up problem, the methods described by [Fea91] and [MAL93] fail when forced to negate certain pathological cases of non-convex constraints. Our methods can handle these (although some of the pathological cases will create performance problems). Our methods for array data-flow analysis, like those of <ref> [Bra88, Rib90, Fea91, MAL92, Voe92a, Voe92b, MAL93] </ref>, are based on extending standard array dependence analysis methods to analysis the flow of values rather than the reuse of memory. Array data-flow dependence analysis methods such as [GS90, Ros90, Li92, DGS93] are based on extending scalar data-flow analysis methods to arrays. <p> In general, the later approach deals better with control flow but the former approach gives more information about which iteration is dependent on which iteration (as opposed to simply summarizing which loops carry the dependence). Both Feautrier [Fea88, Fea91] and Maydan, Amarasinghe and Lam <ref> [MAL93] </ref> describe how to use value-based dependence information for array expansion or privatization. Amarasinghe and Lam [AL93] describe ways to use value-based dependence analysis in analyzing and generating code for distributed memory multi-computers.
Reference: [Mas92] <author> Vadim Maslov. Delinearization: </author> <title> an efficient way to break multiloop dependence equations. </title> <booktitle> In ACM SIGPLAN '92 Conf. on Programming Language Design and Implementation, </booktitle> <address> San Francisco, California, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: The dependence may involve subscripts or loop bounds containing non-linear expressions such as i*c+j, p (i), or a variable that cannot be expressed as an affine function of the loop indices and constants. It may be possible to eliminate some of these false dependences by using methods such as <ref> [McK90, Mas92] </ref>. However, these methods generally require user interaction and assertions. We describe methods for computing upper and lower bounds for dependences involving non-linear references in Section 3.6. Reduction. We normally recognize a flow dependence between two successive updates to an array.
Reference: [Mas94] <author> Vadim Maslov. </author> <title> Lazy array data-flow dependence analysis. </title> <booktitle> In ACM '94 Conf. on Principles of Programming Languages, </booktitle> <month> January </month> <year> 1994. </year>
Reference-contexts: There are a number of papers on techniques to analyze array kills and value-based array data dependence [Bra88, Fea88, GS90, Ros90, Rib90, Fea91, Li92, MAL92, May92, MAL93, DGS93, Mas94]. Of these, only the technique we describe here and that of Feautrier [Fea88, Fea91] and of Maslov <ref> [Mas94] </ref> are complete over the domain of affine expressions and no control flow other than loops.
Reference: [May92] <author> Dror Eliezer Maydan. </author> <title> Accurate Analysis of Array References. </title> <type> PhD thesis, </type> <institution> Computer Systems Laboratory, Stanford U., </institution> <month> September </month> <year> 1992. </year>
Reference-contexts: We discuss related work in Section 8, comment on needed future work in Section 9, describe how to obtain source code for our implementation in Section 10, and offer concluding comments in Section 11. 2. CAUSES OF FALSE DEPENDENCES In a study <ref> [MHL91a, May92] </ref> of Perfect Club Benchmark programs QCD, MDG, ARC2D and TRFD, Dror Maydan et. al. found that a total of 170 loop carried value-based flow dependences were expressed during execution of the programs on the standard input data.
Reference: [McK90] <author> Kathryn S. McKinley. </author> <title> Dependence analysis of arrays subscripted by index arrays. </title> <institution> Technical Report RICE COMP TR91-162, Dept. of Computer Science, Rice University, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: The dependence may involve subscripts or loop bounds containing non-linear expressions such as i*c+j, p (i), or a variable that cannot be expressed as an affine function of the loop indices and constants. It may be possible to eliminate some of these false dependences by using methods such as <ref> [McK90, Mas92] </ref>. However, these methods generally require user interaction and assertions. We describe methods for computing upper and lower bounds for dependences involving non-linear references in Section 3.6. Reduction. We normally recognize a flow dependence between two successive updates to an array.
Reference: [MHL91a] <author> D. E. Maydan, J. L. Hennessy, and M. S. Lam. </author> <title> Effectiveness of data dependence analysis. </title> <booktitle> In Proceedings of the NSF-NCRD Workshop on Advanced Compilation Techniques for Novel Architectures, </booktitle> <year> 1991. </year>
Reference-contexts: We discuss related work in Section 8, comment on needed future work in Section 9, describe how to obtain source code for our implementation in Section 10, and offer concluding comments in Section 11. 2. CAUSES OF FALSE DEPENDENCES In a study <ref> [MHL91a, May92] </ref> of Perfect Club Benchmark programs QCD, MDG, ARC2D and TRFD, Dror Maydan et. al. found that a total of 170 loop carried value-based flow dependences were expressed during execution of the programs on the standard input data. <p> We address more precise dependence abstractions in Section 3.2. Reductions and dependence abstractions were not cited as a cause of false dependences in <ref> [MHL91a] </ref>, since that study only examined whether or not dependences occurred, not whether they actually prevented parallelism. 3. DEPENDENCE ANALYSIS For the methods we describe to apply, we must be able to determine which loops and conditionals control the execution of each statement. <p> Amarasinghe and Lam [AL93] describe ways to use value-based dependence analysis in analyzing and generating code for distributed memory multi-computers. There have been a number of recent papers <ref> [Lar93, PP93, MHL91a] </ref> describing the results of instrumenting programs so that during a run on sample data, information about the actual dependences and available parallelism are collected. However, we do not know of any other work on static analysis of upper bounds on parallelism. 9.
Reference: [MHL91b] <author> D. E. Maydan, J. L. Hennessy, and M. S. Lam. </author> <title> Efficient and exact data dependence analysis. </title> <booktitle> In ACM SIGPLAN'91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 1-14, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: As part of this work [Pug91], we describe dependence relations and techniques for computing their transitive closure. There have been a number of papers on improving the accuracy of memory-based array data dependence analysis <ref> [KKP90, LC90, WT92, GKT91, MHL91b] </ref>. Some of these methods provide ways of recognizing when their results are exact, but do not describe any methods for computing lower bounds on dependences when they are not exact.
Reference: [PP93] <author> Paul M. Petersen and David A. Padua. </author> <title> Static and dynamic evaluation of data dependence analysis. </title> <booktitle> In International Conference on Supercomputers, </booktitle> <month> July </month> <year> 1993. </year>
Reference-contexts: Amarasinghe and Lam [AL93] describe ways to use value-based dependence analysis in analyzing and generating code for distributed memory multi-computers. There have been a number of recent papers <ref> [Lar93, PP93, MHL91a] </ref> describing the results of instrumenting programs so that during a run on sample data, information about the actual dependences and available parallelism are collected. However, we do not know of any other work on static analysis of upper bounds on parallelism. 9.
Reference: [Pug91] <author> William Pugh. </author> <title> Uniform techniques for loop optimization. </title> <booktitle> In 1991 International Conference on Supercomputing, </booktitle> <pages> pages 341-352, </pages> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Dependence difference summaries do not describe which iterations of the statement are involved in the dependence, and do not describe the effect of the values of symbolic constants. We therefore represent dependences with dependence relations <ref> [Pug91] </ref>. <p> of the statement, as it gives the conditions under which any pair of iterations I and I 0 can be executed concurrently: ([I] ! [I 0 ]) 62 D. 4.1 Computing the Self-Dependences of a Statement In previous work, we described methods for computing the transitive self-dependences of a statement <ref> [Pug91] </ref>. Unfortunately, no complete method is possible for computing an exact, closed form for the transitive closure of an arbitrary dependence relation (it is equivalent to adding multiplication to Presburger, which makes the theory undecidable). As described in [Pug91], it is possible to complete an exact transitive closure for dependences in <p> work, we described methods for computing the transitive self-dependences of a statement <ref> [Pug91] </ref>. Unfortunately, no complete method is possible for computing an exact, closed form for the transitive closure of an arbitrary dependence relation (it is equivalent to adding multiplication to Presburger, which makes the theory undecidable). As described in [Pug91], it is possible to complete an exact transitive closure for dependences in certain special forms. For example, we can compute the exact transitive closure of any dependence relation that can be represented exactly by a direction/distance vector. In [Pug91], we were only concerned with computing a lower bound on the <p> As described in <ref> [Pug91] </ref>, it is possible to complete an exact transitive closure for dependences in certain special forms. For example, we can compute the exact transitive closure of any dependence relation that can be represented exactly by a direction/distance vector. In [Pug91], we were only concerned with computing a lower bound on the transitive closure. Here, we discuss how to compute both an upper and lower bound on the transitive closure. <p> Standard unimodular techniques will find any parallelism that can be exposed using loop interchange, loop skewing and loop reversal. Since we consider each statement separately (using transitive self-dependences), we will also find parallelism that can be only exposed using loop distribution, statement reordering, and loop alignment <ref> [Pug91] </ref>. This check will find the parallelism when applied to D ff for the statement in Example 3. <p> In [PW93], we give full descriptions of our techniques for simplifying formulas containing negation (as mentioned in Section 3.5.1). We also compare the performance of the technique described in this paper for analysis of array kills with that of [Fea91, MAL93]. In other papers <ref> [Pug91, KP93] </ref>, we describe a unified framework for reordering transformations. Within this framework, we discuss methods for checking the legality of a transformation, generating transformations, and producing transformed code corresponding to a reordering transformation. <p> In other papers [Pug91, KP93], we describe a unified framework for reordering transformations. Within this framework, we discuss methods for checking the legality of a transformation, generating transformations, and producing transformed code corresponding to a reordering transformation. As part of this work <ref> [Pug91] </ref>, we describe dependence relations and techniques for computing their transitive closure. There have been a number of papers on improving the accuracy of memory-based array data dependence analysis [KKP90, LC90, WT92, GKT91, MHL91b].
Reference: [Pug92] <author> William Pugh. </author> <title> The Omega test: a fast and practical integer programming algorithm for dependence analysis. </title> <journal> Communications of the ACM, </journal> <volume> 8 </volume> <pages> 102-114, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Techniques described in our previous papers <ref> [Pug92, PW92b] </ref> allow us to eliminate existentially quantified variables, check for the feasibility of a conjunction of constraints, and perform other simplifications, but these techniques do not address negation. <p> Dependences that are not part of a cycle can be ignored, and we need not use expensive techniques to analyze them. Given our initial, upper bound on dependences (calculated using standard techniques such as <ref> [Pug92] </ref>), we calculate the strongly connected components (scc's) of the statement graph. <p> We have not yet fully implemented in techniques in [PW93], so we have been unable to evaluate our performance on codes that stress our methods for handling negation. 8. RELATED WORK In our first paper on array data dependence analysis <ref> [Pug92] </ref>, we describe a set of algorithms (the Omega test) that can be used to check for the existence of integer solutions to sets of linear constraints and calculate the "shadow" of a set of linear constraints (i.e., eliminate existential quantified variables).
Reference: [Pug93] <author> William Pugh. </author> <title> Definitions of dependence distance. </title> <journal> Letters on Programming Languages and Systems, </journal> <month> September </month> <year> 1993. </year>
Reference-contexts: The term "dependence distance" is not uniquely defined for unnormalized loops see <ref> [Pug93] </ref> for details. 8 William Pugh and Dave Wonnacott 3.2 Dependence Relations The first step we take to improving the accuracy of our data dependence information is to represent dependences with an abstraction that is more accurate than a dependence difference summary.
Reference: [PW92a] <author> William Pugh and David Wonnacott. </author> <title> Eliminating false data dependences using the Omega test. </title> <booktitle> In SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 140-151, </pages> <address> San Francisco, California, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: With our current implementation, this simplification requires 5 milliseconds on a Sun SparcStation 10-51. The above equation for value-based dependence analysis is best thought of as a denotational specification of what we compute. Using the techniques for value-base dependence analysis described in <ref> [PW92a, PW94] </ref> as a prepass, followed by the techniques described here, decreases the total cost of value-based dependence analysis by a factor of 0:9 1:75 while not changing what is computed. <p> In [PW93], we describe a number of techniques that decrease the cost of value-based dependence analysis by a factor of 1:1 2:9 and eliminate the need to perform a prepass using the techniques described in <ref> [PW92a, PW94] </ref>. 3.5.1 Simplifying formulas containing negation.
Reference: [PW92b] <author> William Pugh and David Wonnacott. </author> <title> Going beyond integer programming with the Omega test to eliminate false data dependences. </title> <type> Technical Report CS-TR-3191, </type> <institution> Dept. of Computer Science, University of Maryland, College Park, </institution> <month> December </month> <year> 1992. </year> <title> An earlier version of this paper appeared at the SIGPLAN PLDI'92 conference. Static Analysis of Upper and Lower Bounds on Dependences and Parallelism 29 </title>
Reference-contexts: Techniques described in our previous papers <ref> [Pug92, PW92b] </ref> allow us to eliminate existentially quantified variables, check for the feasibility of a conjunction of constraints, and perform other simplifications, but these techniques do not address negation. <p> If we use a method such as <ref> [PW92b] </ref> that eliminates some dependences that are not value-based, we can base the scc on only the flow dependences that might be value-based. <p> We then show how these techniques can be used to perform efficient analysis of memory-based array data dependences. In our second paper on dependence analysis <ref> [PW92b] </ref>, we extend the Omega test with efficient techniques for removing redundant constraints and checking when one set of constraints implies another. We give techniques that can identify some dependences as being not value based.
Reference: [PW93] <author> William Pugh and David Wonnacott. </author> <title> An evaluation of exact methods for analysis of value-based array data dependences. </title> <booktitle> In Sixth Annual Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: Some of the problems are described in Section 3.5.1. By using techniques described in <ref> [PW93] </ref>, we simplify the above expression to: f [i] ! [i 00 ] j (1 = i = i 00 n) _ (1 i = i 00 = 2n) _ (1 i = i 00 2n ^ n = 1) g Thus, we have discovered that there is a dependence from <p> Using the techniques for value-base dependence analysis described in [PW92a, PW94] as a prepass, followed by the techniques described here, decreases the total cost of value-based dependence analysis by a factor of 0:9 1:75 while not changing what is computed. In <ref> [PW93] </ref>, we describe a number of techniques that decrease the cost of value-based dependence analysis by a factor of 1:1 2:9 and eliminate the need to perform a prepass using the techniques described in [PW92a, PW94]. 3.5.1 Simplifying formulas containing negation. <p> Although we can always eliminate one variable this way, we may not be able to eliminate multiple variables this way, since we may not be able to apply Fourier-Motzkin variable elimination to a set of constraints containing floor and ceiling operators. In <ref> [PW93] </ref>, we give a partial (but effective) solution to the first problem and a complete and exact solution to the second. 3.6 Non-linear Dependences If a structured routine does not meet the conditions under which we can produce an exact dependence relation, we can create relations that give lower and upper <p> The time shown for value-based dependence anal 24 William Pugh and Dave Wonnacott ysis does not include the memory-based analysis time, though the memory-based dependences were used during value-based analysis. The value-based dependence analysis was done using the optimizations described in <ref> [PW93] </ref>. The times shown in this table include analysis of both arrays and scalars, because we need to have a dependence relation for each possible dependence when we construct the transitive self-dependence relations (as described in Section 4.1). <p> For value-based dependences, we also show the time required to analyze just the dependences that might be in cycles. These results reassure us that it will be practical to apply these techniques to many significant, real world problems. We have not yet fully implemented in techniques in <ref> [PW93] </ref>, so we have been unable to evaluate our performance on codes that stress our methods for handling negation. 8. <p> We give techniques that can identify some dependences as being not value based. However, these techniques do not identify all non-value based dependences and work on dependence differences, not dependence relations. In <ref> [PW93] </ref>, we give full descriptions of our techniques for simplifying formulas containing negation (as mentioned in Section 3.5.1). We also compare the performance of the technique described in this paper for analysis of array kills with that of [Fea91, MAL93]. <p> Each of these conditions is a conjunction of linear constraints, and may include non-convex constraints (e.g., constraints such as "i is even" specified using wildcards or quasi-linear constraints). Converting these expressions into disjunctive normal form would be infeasible for many real problems. The methods we describe in <ref> [PW93] </ref> should control this blow-up. In addition to the blow-up problem, the methods described by [Fea91] and [MAL93] fail when forced to negate certain pathological cases of non-convex constraints. Our methods can handle these (although some of the pathological cases will create performance problems).
Reference: [PW94] <author> William Pugh and David Wonnacott. </author> <title> Going beyond integer programming with the Omega test to eliminate false data dependences. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <note> 1994. To appear. </note>
Reference-contexts: With our current implementation, this simplification requires 5 milliseconds on a Sun SparcStation 10-51. The above equation for value-based dependence analysis is best thought of as a denotational specification of what we compute. Using the techniques for value-base dependence analysis described in <ref> [PW92a, PW94] </ref> as a prepass, followed by the techniques described here, decreases the total cost of value-based dependence analysis by a factor of 0:9 1:75 while not changing what is computed. <p> In [PW93], we describe a number of techniques that decrease the cost of value-based dependence analysis by a factor of 1:1 2:9 and eliminate the need to perform a prepass using the techniques described in <ref> [PW92a, PW94] </ref>. 3.5.1 Simplifying formulas containing negation.
Reference: [Rib90] <author> Hudson Ribas. </author> <title> Obtaining dependence vectors for nested-loop computations. </title> <booktitle> In Proc of 1990 International Conference on Parallel Processing, </booktitle> <address> pages II-212 II-219, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: In addition to the blow-up problem, the methods described by [Fea91] and [MAL93] fail when forced to negate certain pathological cases of non-convex constraints. Our methods can handle these (although some of the pathological cases will create performance problems). Our methods for array data-flow analysis, like those of <ref> [Bra88, Rib90, Fea91, MAL92, Voe92a, Voe92b, MAL93] </ref>, are based on extending standard array dependence analysis methods to analysis the flow of values rather than the reuse of memory. Array data-flow dependence analysis methods such as [GS90, Ros90, Li92, DGS93] are based on extending scalar data-flow analysis methods to arrays.
Reference: [Ros90] <author> Carl Rosene. </author> <title> Incremental Dependence Analysis. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> March </month> <year> 1990. </year>
Reference-contexts: Our methods for array data-flow analysis, like those of [Bra88, Rib90, Fea91, MAL92, Voe92a, Voe92b, MAL93], are based on extending standard array dependence analysis methods to analysis the flow of values rather than the reuse of memory. Array data-flow dependence analysis methods such as <ref> [GS90, Ros90, Li92, DGS93] </ref> are based on extending scalar data-flow analysis methods to arrays. In general, the later approach deals better with control flow but the former approach gives more information about which iteration is dependent on which iteration (as opposed to simply summarizing which loops carry the dependence).
Reference: [Voe92a] <author> Valentin V. Voevodin. </author> <booktitle> Mathematical Foundations of Parallel Computing. </booktitle> <publisher> World Scientific Publishers, </publisher> <year> 1992. </year> <booktitle> World Scientific Series in Computer Science, </booktitle> <volume> vol. </volume> <pages> 33. </pages>
Reference-contexts: In addition to the blow-up problem, the methods described by [Fea91] and [MAL93] fail when forced to negate certain pathological cases of non-convex constraints. Our methods can handle these (although some of the pathological cases will create performance problems). Our methods for array data-flow analysis, like those of <ref> [Bra88, Rib90, Fea91, MAL92, Voe92a, Voe92b, MAL93] </ref>, are based on extending standard array dependence analysis methods to analysis the flow of values rather than the reuse of memory. Array data-flow dependence analysis methods such as [GS90, Ros90, Li92, DGS93] are based on extending scalar data-flow analysis methods to arrays.
Reference: [Voe92b] <author> Vladimir V. Voevodin. </author> <title> Theory and practice of parallelism detection in sequential programs. </title> <journal> Programming and Computer Software (Programmirovaniye), </journal> <volume> 18(3), </volume> <month> May </month> <year> 1992. </year>
Reference-contexts: In addition to the blow-up problem, the methods described by [Fea91] and [MAL93] fail when forced to negate certain pathological cases of non-convex constraints. Our methods can handle these (although some of the pathological cases will create performance problems). Our methods for array data-flow analysis, like those of <ref> [Bra88, Rib90, Fea91, MAL92, Voe92a, Voe92b, MAL93] </ref>, are based on extending standard array dependence analysis methods to analysis the flow of values rather than the reuse of memory. Array data-flow dependence analysis methods such as [GS90, Ros90, Li92, DGS93] are based on extending scalar data-flow analysis methods to arrays.
Reference: [WL90] <author> M. E. Wolf and M. Lam. </author> <title> Maximizing parallelism via loop transformations. </title> <booktitle> In Proceedings of the Third Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Irvine, CA, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: If desired, directional peeling can be applied (Section 5.3.1). 5.1 Using Unimodular Loop Transformation Techniques We can hand the self-dependences off to a module that looks for a unimodular loop transformation that will expose the most parallelism <ref> [Ban90, WL90, KKB92] </ref>. This module can report whether the self-dependences allow for coarse-grain parallelism, and if not, whether the self-dependences allow for fine-grain parallelism. Standard unimodular techniques will find any parallelism that can be exposed using loop interchange, loop skewing and loop reversal.
Reference: [Wol89] <author> Michael Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> Pitman Publishing, </publisher> <address> London, </address> <year> 1989. </year>
Reference: [Wol91] <author> Michael Wolfe. </author> <title> The tiny loop restructuring research tool. </title> <booktitle> In Proc of 1991 International Conference on Parallel Processing, </booktitle> <address> pages II-46 II-53, </address> <year> 1991. </year>
Reference-contexts: IMPLEMENTATION STATUS AND BENCHMARK AVAILABILITY The techniques described here are being implemented in our extended version of Michael Wolfe's tiny tool <ref> [Wol91] </ref>, which is available for anonymous ftp from ftp.cs.umd.edu:pub/omega. The programs analyzed in Section 7 come from a set of benchmark programs for comparing the performance and coverage of algorithms for analyzing value-based flow dependences between array references.
Reference: [WT92] <author> M. J. Wolfe and C. Tseng. </author> <title> The Power test for data dependence. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(5) </volume> <pages> 591-601, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: As part of this work [Pug91], we describe dependence relations and techniques for computing their transitive closure. There have been a number of papers on improving the accuracy of memory-based array data dependence analysis <ref> [KKP90, LC90, WT92, GKT91, MHL91b] </ref>. Some of these methods provide ways of recognizing when their results are exact, but do not describe any methods for computing lower bounds on dependences when they are not exact.
Reference: [ZC91] <author> Hans Zima and Barbara Chapman. </author> <title> Supercompilers for Parallel and Vector Computers. </title> <publisher> ACM Press, </publisher> <year> 1991. </year>
Reference-contexts: The notation we use in the constraints is adapted from <ref> [ZC91] </ref>: A; B; : : : Refers to a specific array reference in a program I; I 0 ; I 00 ; : : : An iteration vector that represents a specific set of values of the loop variables for a loop nest. [A] The set of iteration vectors for which
References-found: 49

