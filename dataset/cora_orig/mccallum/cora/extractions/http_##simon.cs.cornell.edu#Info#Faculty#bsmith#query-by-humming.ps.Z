URL: http://simon.cs.cornell.edu/Info/Faculty/bsmith/query-by-humming.ps.Z
Refering-URL: 
Root-URL: 
Email: fghias,bsmithg@cs.cornell.edu, logan@ghs.com, chamber@engr.sgi.com  
Title: Query By Humming Musical Information Retrieval in An Audio Database  
Author: Asif Ghias Jonathan Logan David Chamberlin Brian C. Smith 
Keyword: Musical information retrieval, multimedia databases, pitch tracking  
Affiliation: Cornell University  
Abstract: The emergence of audio and video data types in databases will require new information retrieval methods adapted to the specific characteristics and needs of these data types. An effective and natural way of querying a musical audio database is by humming the tune of a song. In this paper, a system for querying an audio database by humming is described along with a scheme for representing the melodic information in a song as relative pitch changes. Relevant difficulties involved with tracking pitch are enumerated, along with the approach we followed, and the performance results of system indicating its effectiveness are presented. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Ricardo A. Baesa-Yates and Chris H. Perleberg. </author> <title> Fast and practical approximate string matching. Combinatorial Pattern Matching, </title> <booktitle> Third Annual Symposium, </booktitle> <pages> pages 185-192, </pages> <year> 1992. </year>
Reference-contexts: Hummed queries may be recorded in a variety of formats, depending upon the platform-specific audio input capabilities of Matlab. We have experimented with 16-bit, 44Khz WAV format on a Pentium system, and 8-bit, 8Khz AU format on a Sun Sparcstation. The query engine uses an approximate pattern matching algorithm <ref> [1] </ref>, described in below, in order to tolerate humming errors. Tracking Pitch in Hummed Queries This section describes how user input to the system (humming) is converted into a sequence of relative pitch transitions. <p> By approximate we mean that the algorithm should be able to take into account various forms of errors. Figure summarizes the various forms of errors anticipated in a typical pattern matching scheme. The algorithm that mismatch we adopted for this purpose is described by Baesa-Yates and Perleberg <ref> [1] </ref>. This algorithm addresses the problem of string matching with k mismatches.
Reference: 2. <author> Ricardo Baeza-Yates and G.H. Gonnet. </author> <title> Fast string matching with mismatches. </title> <booktitle> Information and Computation, </booktitle> <year> 1992. </year>
Reference-contexts: It is worth mentioning that several algorithms have been developed that address the problem of approximate string matching. Running times have ranged from O (mn) for the brute force algorithm to O (kn) [9] or O (n log (m)) <ref> [2] </ref>. The algorithm that we adopted offers better performance for average cases than most other algorithms. The worst case for this algorithm occurs when P (the key) consists of m occurrences of a single distinct character, and T (contour representation of song) consists of n instances of that character.
Reference: 3. <author> Stephen Handel. </author> <title> Listening: An Introduction to the Perception of Auditory Events. </title> <publisher> The MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: Our approach hinges upon the observation that melodic contour, defined as the sequence of relative differences in pitch between successive notes, can be used to discriminate between melodies. Handel <ref> [3] </ref> indicates that melodic contour is one of the most important methods that listeners use to determine similarities between melodies.
Reference: 4. <author> Michael Jerome Hawley. </author> <title> Structure out of Sound. </title> <type> PhD thesis, </type> <institution> MIT, </institution> <month> September </month> <year> 1993. </year>
Reference-contexts: From a research standpoint, an interesting question is how to extract melodies from complex audio signals <ref> [4] </ref>. Finally, we would like to characterize the improvement gained by increasing the resolution of the relative pitch differences by considering query alphabets of three, five and more possible relationships between adjacent pitches. <p> An important issue is precisely where to draw the line between notes that are a little higher from the previous note and those that are much higher. Previous work on efficiently searching a database of melodies by humming seems to be limited. Mike Hawley <ref> [4] </ref> briefly discusses a method of querying a collection of melodic themes by searching for exact matches of sequences of relative pitches input by a MIDI keyboard. We have incorporated approximate pattern matching, implementing an actual audio database (of MIDI songs) and most significantly by allowing queries to be hummed.
Reference: 5. <author> Wolfgang Hess. </author> <title> Pitch Determination of Speech Signals. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin Heidelberg, </address> <year> 1983. </year>
Reference-contexts: The vibrations of the vocal cords in voiced sounds are caused as a consequence of forces that are exerted on the laryngeal walls when air flows through the glottis (the gap between the vocal cords 1 ). Hess <ref> [5] </ref> describes a model of the vocal cords as proposed by Hirano [6]. For the purposes of this paper though, it is sufficient to know that the glottis repeatedly opens and closes thus providing bursts of air through the vocal tract.
Reference: 6. <author> M. Hirano. </author> <title> Structure and vibratory behavior of the vocal folds. </title> <editor> In M. Sawashima and F.S. Cooper, editors, </editor> <booktitle> Dynamic aspects of speech production, </booktitle> <pages> pages 13-27. </pages> <institution> University of Tokyo Press, </institution> <year> 1976. </year>
Reference-contexts: Hess [5] describes a model of the vocal cords as proposed by Hirano <ref> [6] </ref>. For the purposes of this paper though, it is sufficient to know that the glottis repeatedly opens and closes thus providing bursts of air through the vocal tract. The vocal tract can be modeled as a linear passive transmission system with a transfer function H (z).
Reference: 7. <author> L.R. Rabiner J.J. Dubnowski and R.W. Schafer. </author> <title> Real-time digital hardware pitch detector. </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> ASSP-24(1):2-8, </volume> <month> Feb </month> <year> 1976. </year>
Reference-contexts: So if we can track those bursts of air we can find the pitch of the segment. Tracking pitch We tried three methods for tracking pitch: Autocorrelation, Maximum Likelihood, Cepstrum Analysis. * Autocorrelation Autocorrelation is one of the oldest of the classical pitch trackers <ref> [7] </ref>. Autocorrelation isolates and tracks the peak energy levels of the signal which is a measure of the pitch. Referring back to figure 3, we see that the signal s (n) peaks where the impulses occur.
Reference: 8. <author> T. Kageyama and Y. Takashima. </author> <title> A melody retrieval method with hummed melody (language: Japanese). </title> <journal> Transactions of the Institute of Electronics, Information and Communication Engineers D-II, </journal> <volume> J77D-II(8):1543-1551, </volume> <month> August </month> <year> 1994. </year>
Reference-contexts: We have incorporated approximate pattern matching, implementing an actual audio database (of MIDI songs) and most significantly by allowing queries to be hummed. Kageyama and Takashima <ref> [8] </ref> published a paper on retrieving melodies using a hummed melody in a Japanese journal, but we were unable to locate a translated version.
Reference: 9. <author> G. Landau and U. Vishkin. </author> <title> Efficient string matching with k mismatches. </title> <journal> Theoretical Computer Science, </journal> <volume> 43 </volume> <pages> 239-249, </pages> <year> 1986. </year>
Reference-contexts: Each of the errors in the figure above corresponds to k=1. It is worth mentioning that several algorithms have been developed that address the problem of approximate string matching. Running times have ranged from O (mn) for the brute force algorithm to O (kn) <ref> [9] </ref> or O (n log (m)) [2]. The algorithm that we adopted offers better performance for average cases than most other algorithms.
Reference: 10. <author> A. V. Oppenheim. </author> <title> A speech analysis-synthesis system based on homomorphic filtering. </title> <journal> J. Acoustical Society of America, </journal> <volume> 45 </volume> <pages> 458-465, </pages> <month> February </month> <year> 1969. </year>
Reference-contexts: Therefore, we discarded this method. For a detailed explanation of this method, the reader may refer to [14]. * Cepstrum Analysis Cepstrum analysis is the definitive classical method of pitch extraction. For an explanation, the reader is directed to Op-penheim and Schafer's original work in <ref> [10] </ref> or in a more compact form in [11]. We found that this method did not give very accurate results for humming. The output of these methods can be construed as a sequence of frequency estimations for successive pitches in the input.
Reference: 11. <author> Alan V. Oppenheim and Ronald W. Schafer. </author> <title> Discrete-time Signal Processing. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year>
Reference-contexts: For a detailed explanation of this method, the reader may refer to [14]. * Cepstrum Analysis Cepstrum analysis is the definitive classical method of pitch extraction. For an explanation, the reader is directed to Op-penheim and Schafer's original work in [10] or in a more compact form in <ref> [11] </ref>. We found that this method did not give very accurate results for humming. The output of these methods can be construed as a sequence of frequency estimations for successive pitches in the input.
Reference: 12. <author> R. Plomp. </author> <title> Aspects of tone sensation. </title> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1976. </year>
Reference-contexts: Schouten studied the pitch of periodic sound waves produced by an optical siren in which the fundamental of 200Hz was canceled completely. The pitch of the complex tone, however, was the same as that prior to the elimination of the fundamental. <ref> [12] </ref> Since we were interested in tracking pitch in humming, we examined methods for automatically tracking pitch in a human voice.
Reference: 13. <author> M. M. Sondhi. </author> <title> New methods of pitch extraction. </title> <journal> IEEE Trans. Audio Electroacoust. (Special Issue on Speech Communication and ProcessingPartII, </journal> <volume> AU-16:262-266, </volume> <month> June </month> <year> 1968. </year>
Reference-contexts: The melodic contours of the source songs are currently generated automatically from MIDI data, which is convenient but not optimal. More accuracy and less redundant information could be obtained by entering the melodic themes for 2 The modifications include low-pass filtering and center-clipping (as described in Sondhi's paper <ref> [13] </ref>) which help eliminate the formant structure that generally causes difficulty for autocorrelation based pitch detectors. particular songs by hand. From a research standpoint, an interesting question is how to extract melodies from complex audio signals [4].
Reference: 14. <author> James D. Wise, James R. Caprio, and Thomas W. Parks. </author> <title> Maximum likelihood pitch estimation. </title> <journal> IEEE Trans. Acoust., Speech, Signal Processing, </journal> <volume> 24(5) </volume> <pages> 418-423, </pages> <month> October </month> <year> 1976. </year>
Reference-contexts: We found our implementation of autocorrelation to require approximately 45 seconds for 10 seconds of 44KHz, 16-bit audio on a 90MHz pentium workstation. * Maximum Likelihood Maximum Likelihood <ref> [14] </ref> is a modification of Autocorrelation that increases the accuracy of the pitch and decreases the chances of aliasing. Unfortunately, the computational complexity of this method makes autocorrelation look blindingly fast. <p> With some optimizations, we improved the performance to approximately 15 minutes per 10 seconds of audio, but this is still far too slow for our purposes. Therefore, we discarded this method. For a detailed explanation of this method, the reader may refer to <ref> [14] </ref>. * Cepstrum Analysis Cepstrum analysis is the definitive classical method of pitch extraction. For an explanation, the reader is directed to Op-penheim and Schafer's original work in [10] or in a more compact form in [11]. We found that this method did not give very accurate results for humming.
References-found: 14

