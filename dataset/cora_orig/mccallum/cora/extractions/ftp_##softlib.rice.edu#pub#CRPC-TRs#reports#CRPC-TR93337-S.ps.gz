URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR93337-S.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: A Code Placement Framework and its Application to Communication Generation  
Author: Reinhard v. Hanxleden Ken Kennedy 
Address: P.O. Box 1892 Houston, TX 77251-1892  
Affiliation: Rice University  
Note: Center for Research on Parallel Computation  
Date: October, 1993  
Pubnum: CRPC-TR93337-S  
Abstract-found: 0
Intro-found: 1
Reference: [AL93] <author> S. P. Amarasinghe and M. S. Lam. </author> <title> Communication optimization and code generation for distributed memory machines. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 28(6) </volume> <pages> 126-138, </pages> <month> June </month> <year> 1993. </year> <booktitle> Proceedings of the ACM SIGPLAN '93 Conference on Programming Language Design and Implementation. </booktitle>
Reference-contexts: Duesterwald et al. incorporate iteration distance vectors into an array reference data flow framework which is then applied to memory optimizations and controlled loop unrolling [DGS93]. Amarasinghe and Lam optimize communication generation using Last Write Trees <ref> [AL93] </ref>. They assume regular array access patterns and distributions. Gupta and Schonberg use Available Section Descriptors, computed by interval based data flow analysis, to determine the availability of data on a virtual processor grid [GS93].
Reference: [All70] <author> F. E. Allen. </author> <title> Control flow analysis. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 5(7) </volume> <pages> 1-19, </pages> <year> 1970. </year>
Reference-contexts: This definition often depends on the values of variables at nodes that are connected to n through edges in E. A general data flow analysis algorithm that considers loop nesting hierarchies is interval analysis. It can be used for forward problems (like available expressions) <ref> [All70, Coc70] </ref> and backward problems (like live variables) [Ken71], and it has also been used for code motion [DP93]. We are using a variant of interval analysis that is based on Tarjan intervals [Tar74].
Reference: [CK92] <author> S. Carr and K. Kennedy. </author> <title> Scalar replacement in the presence of conditional control flow. </title> <type> Technical Report TR92283, </type> <institution> Rice University, CRPC, </institution> <month> November </month> <year> 1992. </year> <note> To appear in Software Practice& Experience. </note>
Reference-contexts: He uses an iterative approach, with separate frameworks for loads and stores. Carr and Kennedy combine partial redundancy elimination with dependence analysis to perform scalar replacement <ref> [CK92] </ref>. Granston and Veidenbaum combine dependence analysis and partial redundancy elimination to detect redundant global memory accesses in parallelized and vectorized codes [GV91].
Reference: [CM69] <author> J. Cocke and R. Miller. </author> <title> Some analysis techniques for optimizing computer programs. </title> <booktitle> In Proceedings of the 2nd Annual Hawaii International Conference on System Sciences, </booktitle> <pages> pages 143-146, </pages> <year> 1969. </year>
Reference-contexts: Note that forward edges are the only edges not crossing nesting level boundaries. We extend G to have the following properties: * G is reducible; i.e., each loop has a unique header node. This can be achieved, for example, by node splitting <ref> [CM69] </ref>. 6 * For each non-empty interval T (h), there exists a unique g 2 T (h) such that (g; h) 2 E; i.e., there is only one back edge out of T (h). This can be achieved by adding a post body node to T (h).
Reference: [Coc70] <author> J. Cocke. </author> <title> Global common subexpression elimination. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 5(7) </volume> <pages> 20-24, </pages> <year> 1970. </year>
Reference-contexts: This definition often depends on the values of variables at nodes that are connected to n through edges in E. A general data flow analysis algorithm that considers loop nesting hierarchies is interval analysis. It can be used for forward problems (like available expressions) <ref> [All70, Coc70] </ref> and backward problems (like live variables) [Ken71], and it has also been used for code motion [DP93]. We are using a variant of interval analysis that is based on Tarjan intervals [Tar74].
Reference: [DGS93] <author> E. Duesterwald, R. Gupta, and M. L. Soffa. </author> <title> A practical data flow framework for array reference analysis and its use in optimizations. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 28(6) </volume> <pages> 68-77, </pages> <month> June </month> <year> 1993. </year> <booktitle> Proceedings of the ACM SIGPLAN '93 Conference on Programming Language Design and Implementation. </booktitle>
Reference-contexts: Duesterwald et al. incorporate iteration distance vectors into an array reference data flow framework which is then applied to memory optimizations and controlled loop unrolling <ref> [DGS93] </ref>. Amarasinghe and Lam optimize communication generation using Last Write Trees [AL93]. They assume regular array access patterns and distributions. Gupta and Schonberg use Available Section Descriptors, computed by interval based data flow analysis, to determine the availability of data on a virtual processor grid [GS93].
Reference: [Dha88a] <author> D.M. Dhamdhere. </author> <title> A fast algorithm for code movement optimization. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 23(10) </volume> <pages> 172-180, </pages> <year> 1988. </year>
Reference-contexts: The original dataflow framework for performing partial redundancy elimination was developed by Morel and Ren-voise [MR79] and has since then experienced various refinements <ref> [JD82, DS88, Dha88a, Dha91, DRZ92, KRS92] </ref>. fl Available via anonymous ftp from softlib.rice.edu as pub/CRPC-TRs/reports/CRPC-TR93337-S. y This work is supported by an IBM fellowship. 1 Dhamdhere uses partial redundancy elimination techniques for placing register loads and stores [Dha88b]. He uses an iterative approach, with separate frameworks for loads and stores.
Reference: [Dha88b] <author> D.M. Dhamdhere. </author> <title> Register assignment using code placement techniques. </title> <journal> Computer Languages, </journal> <volume> 13(2) </volume> <pages> 75-93, </pages> <year> 1988. </year>
Reference-contexts: developed by Morel and Ren-voise [MR79] and has since then experienced various refinements [JD82, DS88, Dha88a, Dha91, DRZ92, KRS92]. fl Available via anonymous ftp from softlib.rice.edu as pub/CRPC-TRs/reports/CRPC-TR93337-S. y This work is supported by an IBM fellowship. 1 Dhamdhere uses partial redundancy elimination techniques for placing register loads and stores <ref> [Dha88b] </ref>. He uses an iterative approach, with separate frameworks for loads and stores. Carr and Kennedy combine partial redundancy elimination with dependence analysis to perform scalar replacement [CK92]. Granston and Veidenbaum combine dependence analysis and partial redundancy elimination to detect redundant global memory accesses in parallelized and vectorized codes [GV91].
Reference: [Dha91] <author> D.M. Dhamdhere. </author> <title> Practical adaptation of the global optimization algorithm of Morel and Renvoise. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(2) </volume> <pages> 291-294, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: The original dataflow framework for performing partial redundancy elimination was developed by Morel and Ren-voise [MR79] and has since then experienced various refinements <ref> [JD82, DS88, Dha88a, Dha91, DRZ92, KRS92] </ref>. fl Available via anonymous ftp from softlib.rice.edu as pub/CRPC-TRs/reports/CRPC-TR93337-S. y This work is supported by an IBM fellowship. 1 Dhamdhere uses partial redundancy elimination techniques for placing register loads and stores [Dha88b]. He uses an iterative approach, with separate frameworks for loads and stores.
Reference: [DK83] <author> D.M. Dhamdhere and J.S. Keith. </author> <title> Characterization of program loops in code optimization. </title> <journal> Computer Languages, </journal> <volume> 8 </volume> <pages> 69-76, </pages> <year> 1983. </year>
Reference-contexts: Several techniques exist to circumvent this difficulty, like for example adding an extra guard and a preheader node to each loop [Sor89], explicitly introducing zero-trip paths <ref> [DK83] </ref>, or collapsing innermost loops [HKK + 92]. These strategies, however, result in some loss of information, and they do not fully apply to nested loops. Therefore, the Give-N-Take framework generally treats a loop as if it will be executed at least once.
Reference: [DK93] <author> D. M. Dhamdhere and U. P. Khedker. </author> <title> Complexity of bidirectional data flow analysis. </title> <booktitle> In Conference Record of the Twentieth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 397-408, </pages> <address> Charleston, South Carolina, </address> <month> January </month> <year> 1993. </year>
Reference: [DP93] <author> D.M. Dhamdhere and H. Patil. </author> <title> An elimination algorithm for bidirectional data flow problems using edge placement. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 15(2) </volume> <pages> 312-336, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: A general data flow analysis algorithm that considers loop nesting hierarchies is interval analysis. It can be used for forward problems (like available expressions) [All70, Coc70] and backward problems (like live variables) [Ken71], and it has also been used for code motion <ref> [DP93] </ref>. We are using a variant of interval analysis that is based on Tarjan intervals [Tar74].
Reference: [DRZ92] <author> D.M. Dhamdhere, B.K. Rosen, and F.K. Zadeck. </author> <title> How to analyze large programs efficiently and informatively. </title> <booktitle> In Proceedings of the ACM SIGPLAN '92 Conference on Program Language Design and Implementation, </booktitle> <pages> pages 212-223, </pages> <address> San Francisco, CA, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: The original dataflow framework for performing partial redundancy elimination was developed by Morel and Ren-voise [MR79] and has since then experienced various refinements <ref> [JD82, DS88, Dha88a, Dha91, DRZ92, KRS92] </ref>. fl Available via anonymous ftp from softlib.rice.edu as pub/CRPC-TRs/reports/CRPC-TR93337-S. y This work is supported by an IBM fellowship. 1 Dhamdhere uses partial redundancy elimination techniques for placing register loads and stores [Dha88b]. He uses an iterative approach, with separate frameworks for loads and stores.
Reference: [DS88] <author> K. Drechsler and M. Stadel. </author> <title> A solution to a problem with Morel and Renvoise's "Global optimization by suppression of partial redundancies". </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 10(4) </volume> <pages> 635-640, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: The original dataflow framework for performing partial redundancy elimination was developed by Morel and Ren-voise [MR79] and has since then experienced various refinements <ref> [JD82, DS88, Dha88a, Dha91, DRZ92, KRS92] </ref>. fl Available via anonymous ftp from softlib.rice.edu as pub/CRPC-TRs/reports/CRPC-TR93337-S. y This work is supported by an IBM fellowship. 1 Dhamdhere uses partial redundancy elimination techniques for placing register loads and stores [Dha88b]. He uses an iterative approach, with separate frameworks for loads and stores.
Reference: [GS90] <author> T. Gross and P. Steenkiste. </author> <title> Structured dataflow analysis for arrays and its use in an optimizing compiler. </title> <journal> Software|Practice and Experience, </journal> <volume> 20(2) </volume> <pages> 133-155, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: Gross and Steenkiste combine dependence analysis with data flow analysis and regular sections to analyze, for example, reaching definitions <ref> [GS90] </ref>. It turns out that efficient communication generation is closely related to partial redundancy elimination techniques, which were originally developed for transformations like loop invariant code motion, common subexpression elimination, or strength reduction.
Reference: [GS93] <author> M. Gupta and E. Schonberg. </author> <title> A framework for exploiting data availability to optimize communication. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: Amarasinghe and Lam optimize communication generation using Last Write Trees [AL93]. They assume regular array access patterns and distributions. Gupta and Schonberg use Available Section Descriptors, computed by interval based data flow analysis, to determine the availability of data on a virtual processor grid <ref> [GS93] </ref>. They apply (regular) mapping functions to map this information to individual processors and list redundant communication elimination and communication generation as possible applications. An iterative data flow framework for generating communication statements in the presence of indirection arrays has been developed by von Hanxleden et al. [HKK + 92].
Reference: [GV91] <author> E. Granston and A. Veidenbaum. </author> <title> Detecting redundant accesses to array data. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: He uses an iterative approach, with separate frameworks for loads and stores. Carr and Kennedy combine partial redundancy elimination with dependence analysis to perform scalar replacement [CK92]. Granston and Veidenbaum combine dependence analysis and partial redundancy elimination to detect redundant global memory accesses in parallelized and vectorized codes <ref> [GV91] </ref>. Their technique tries to eliminate these operations where possible, also across loop nests and in the presence of conditionals, and they eliminate reads of not-owned variables if these variables have already been read or written locally. <p> This information is already contained in GIVEN in (for RES in ) and GIVEN out (for RES out ). An incremental schedule can take advantage of these situations [HKK + 92]. A similar effect can be achieved with mode vectors <ref> [GV91] </ref>; however, they do so not by compile time analysis but by keeping track of available data at run time.
Reference: [HHKT92] <author> M. W. Hall, S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Interprocedural compilation of Fortran D for MIMD distributed-memory machines. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <address> Minneapolis, MN, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: We assume that there is only one distribution statement reaching each occurrence; this can be ensured, for example, by cloning occurrences that are reached by multiple distributions <ref> [HHKT92] </ref>. 11 * Computation distribution; i.e., how the computation of the statement containing the occurrence is dis- tributed. Two occurrences are assigned to the same portion iff they agree on all four fields.
Reference: [Hig93] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran language specification, version 1.0. </title> <type> Technical Report CRPC-TR92225, </type> <institution> Center for Research on Parallel Computation, Rice University, Houston, TX, </institution> <year> 1992 </year> <month> (revised Jan. </month> <year> 1993). </year> <note> To appear in Scientific Programming, </note> <month> July </month> <year> 1993. </year>
Reference-contexts: 1 Introduction An important step in compiling data parallel languages, like Fortran D [HKT92a] or High Performance Fortran <ref> [Hig93] </ref>, onto distributed memory machines is the generation of communication statements that allow each processor to reference and define data that they do not own by default.
Reference: [HKK + 92] <author> R. v. Hanxleden, K. Kennedy, C. Koelbel, R. Das, and J. Saltz. </author> <title> Compiler analysis for irregular problems in Fortran D. </title> <booktitle> In Proceedings of the Fifth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> New Haven, CT, </address> <month> August </month> <year> 1992. </year> <note> Available via anonymous ftp from softlib.rice.edu as pub/CRPC-TRs/reports/CRPC-TR92287-S. </note>
Reference-contexts: They apply (regular) mapping functions to map this information to individual processors and list redundant communication elimination and communication generation as possible applications. An iterative data flow framework for generating communication statements in the presence of indirection arrays has been developed by von Hanxleden et al. <ref> [HKK + 92] </ref>. It uses a loop flow graph designed to ease the hoisting of communication out of innermost loops. However, it handles nested loops and jumps out of loops only to a limited degree, and communication is treated monolithically. <p> Several techniques exist to circumvent this difficulty, like for example adding an extra guard and a preheader node to each loop [Sor89], explicitly introducing zero-trip paths [DK83], or collapsing innermost loops <ref> [HKK + 92] </ref>. These strategies, however, result in some loss of information, and they do not fully apply to nested loops. Therefore, the Give-N-Take framework generally treats a loop as if it will be executed at least once. <p> The corresponding flow graph is in Figure 11. Data declarations, initializations, distribution statements, etc., have been omitted. Communication schedule generation, which is a non-trivial problem in itself <ref> [HKK + 92] </ref>, is also excluded. For simplicity, we assume that all arrays are distributed BLOCK-wise and that we use the owner computes rule. In case of an indirect definition (like the x (b (m))), we assume that computation is distributed based on the owner of the indirection array. <p> We can also take advantage of overlapping portions by taking into account what data are already available when generating communication. This information is already contained in GIVEN in (for RES in ) and GIVEN out (for RES out ). An incremental schedule can take advantage of these situations <ref> [HKK + 92] </ref>. A similar effect can be achieved with mode vectors [GV91]; however, they do so not by compile time analysis but by keeping track of available data at run time.
Reference: [HKT92a] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiler support for machine-independent parallel programming in Fortran D. </title> <editor> In J. Saltz and P. Mehrotra, editors, </editor> <title> Languages, Compilers, and Run-Time Environments for Distributed Memory Machines. </title> <publisher> North-Holland, </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1992. </year>
Reference-contexts: 1 Introduction An important step in compiling data parallel languages, like Fortran D <ref> [HKT92a] </ref> or High Performance Fortran [Hig93], onto distributed memory machines is the generation of communication statements that allow each processor to reference and define data that they do not own by default.
Reference: [HKT92b] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Evaluation of compiler optimizations for Fortran D on MIMD distributed-memory machines. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <address> Washington, DC, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Since generating an individual message for each not-owned datum would be prohibitively expensive on most architectures, optimizations like message vectorization, latency hiding, and avoiding redundant communication are crucial for achieving acceptable performance <ref> [HKT92b] </ref>. Dependence analysis can guide such optimizations, for example by guaranteeing the safety of hoisting communication out of a loop nest.
Reference: [JD82] <author> S.M. Joshi and D.M. Dhamdhere. </author> <title> A composite hoisting-strength reduction transformation for global program optimization, parts I & II. </title> <journal> International Journal of Computer Mathematics, </journal> <volume> 11 </volume> <pages> 21-41, 111-126, </pages> <year> 1982. </year>
Reference-contexts: The original dataflow framework for performing partial redundancy elimination was developed by Morel and Ren-voise [MR79] and has since then experienced various refinements <ref> [JD82, DS88, Dha88a, Dha91, DRZ92, KRS92] </ref>. fl Available via anonymous ftp from softlib.rice.edu as pub/CRPC-TRs/reports/CRPC-TR93337-S. y This work is supported by an IBM fellowship. 1 Dhamdhere uses partial redundancy elimination techniques for placing register loads and stores [Dha88b]. He uses an iterative approach, with separate frameworks for loads and stores.
Reference: [Ken71] <author> K. Kennedy. </author> <title> A global flow analysis algorithm. </title> <journal> International Journal of Computer Mathematics, </journal> <volume> 3 </volume> <pages> 5-15, </pages> <year> 1971. </year>
Reference-contexts: A general data flow analysis algorithm that considers loop nesting hierarchies is interval analysis. It can be used for forward problems (like available expressions) [All70, Coc70] and backward problems (like live variables) <ref> [Ken71] </ref>, and it has also been used for code motion [DP93]. We are using a variant of interval analysis that is based on Tarjan intervals [Tar74].
Reference: [KRS92] <author> J. Knoop, O. Ruthing, and B. Steffen. </author> <title> Lazy code motion. </title> <booktitle> In Proceedings of the ACM SIGPLAN '92 Conference on Program Language Design and Implementation, </booktitle> <address> San Francisco, CA, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: The original dataflow framework for performing partial redundancy elimination was developed by Morel and Ren-voise [MR79] and has since then experienced various refinements <ref> [JD82, DS88, Dha88a, Dha91, DRZ92, KRS92] </ref>. fl Available via anonymous ftp from softlib.rice.edu as pub/CRPC-TRs/reports/CRPC-TR93337-S. y This work is supported by an IBM fellowship. 1 Dhamdhere uses partial redundancy elimination techniques for placing register loads and stores [Dha88b]. He uses an iterative approach, with separate frameworks for loads and stores. <p> with multiple outgoing edges to a node with multiple incoming edges. (Intuitively, a critical edge might indicate a location in the program where we cannot place production without affecting paths that are not supposed to be affected by the production.) This can be achieved, for example, by inserting synthetic nodes <ref> [KRS92] </ref>. Code generated for synthetic nodes would reside in newly created basic blocks, like for example a new ELSE branch or a landing pad for a jump out of a loop. It can easily be seen that the graph shown in Figure 11 fulfills these properties.
Reference: [MR79] <author> E. Morel and C. </author> <title> Renvoise. Global optimization by suppression of partial redundancies. </title> <journal> Communications of the ACM, </journal> <volume> 22(2) </volume> <pages> 96-103, </pages> <month> February </month> <year> 1979. </year>
Reference-contexts: It turns out that efficient communication generation is closely related to partial redundancy elimination techniques, which were originally developed for transformations like loop invariant code motion, common subexpression elimination, or strength reduction. The original dataflow framework for performing partial redundancy elimination was developed by Morel and Ren-voise <ref> [MR79] </ref> and has since then experienced various refinements [JD82, DS88, Dha88a, Dha91, DRZ92, KRS92]. fl Available via anonymous ftp from softlib.rice.edu as pub/CRPC-TRs/reports/CRPC-TR93337-S. y This work is supported by an IBM fellowship. 1 Dhamdhere uses partial redundancy elimination techniques for placing register loads and stores [Dha88b].
Reference: [SBW90] <author> J. Saltz, H. Berryman, and J. Wu. </author> <title> Multiprocessors and runtime compilation. </title> <type> ICASE Report 90-59, </type> <institution> Institute for Computer Application in Science and Engineering, Hampton, VA, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: In general, we assume that aggregate communication primitives capable of handling irregular array accesses are used for communication, like for example the Parti communications library <ref> [SBW90] </ref>. However, the compiler recognizes regular subscripts (for example, subscripts linear in loop variables or auxiliary induction variables) and communicates these using simpler, faster communication routines.
Reference: [Sor89] <author> A. Sorkin. </author> <title> Some comments on "A solution to a problem with Morel and Renvoise's `Global optimization by suppression of partial redundancies' ". ACM Transactions on Programming Languages and Systems, </title> <booktitle> 11(4) </booktitle> <pages> 666-668, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: Several techniques exist to circumvent this difficulty, like for example adding an extra guard and a preheader node to each loop <ref> [Sor89] </ref>, explicitly introducing zero-trip paths [DK83], or collapsing innermost loops [HKK + 92]. These strategies, however, result in some loss of information, and they do not fully apply to nested loops. Therefore, the Give-N-Take framework generally treats a loop as if it will be executed at least once.
Reference: [Tar74] <author> R. E. Tarjan. </author> <title> Testing flow graph reducibility. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 9 </volume> <pages> 355-365, </pages> <year> 1974. </year>
Reference-contexts: It can be used for forward problems (like available expressions) [All70, Coc70] and backward problems (like live variables) [Ken71], and it has also been used for code motion [DP93]. We are using a variant of interval analysis that is based on Tarjan intervals <ref> [Tar74] </ref>. Like Allen-Cocke intervals, a Tarjan interval T (h) is a set of control flow nodes that corresponds to a loop in the program text, entered through a unique header node h, where h 62 T (h).
References-found: 29

