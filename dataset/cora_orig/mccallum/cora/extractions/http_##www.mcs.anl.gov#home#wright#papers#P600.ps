URL: http://www.mcs.anl.gov/home/wright/papers/P600.ps
Refering-URL: http://www.cs.cornell.edu/Info/People/mwagner/pPCx/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: MODIFIED CHOLESKY FACTORIZATIONS IN INTERIOR-POINT ALGORITHMS FOR LINEAR PROGRAMMING  
Author: STEPHEN WRIGHT 
Keyword: Key words. Interior-point algorithms and software, Cholesky factorization, Matrix perturbations, Error analysis.  
Address: ARGONNE, IL 60439, USA.  
Affiliation: MATHEMATICS AND COMPUTER SCIENCE DIVISION, AR-GONNE NATIONAL LABORATORY,  
Date: MAY, 1996,  REVISED JUNE, 1998 AND JULY, 1998.  
Note: PREPRINT ANL/MCS-P600-0596,  
Abstract: We investigate a modified Cholesky algorithm typical of those used in most interior-point codes for linear programming. Cholesky-based interior-point codes are popular for three reasons: their implementation requires only minimal changes to standard sparse Cholesky algorithms (allowing us to take full advantage of software written by specialists in that area); they tend to be more efficient than competing approaches that use alternative factorizations; and they perform robustly on most practical problems, yielding good interior-point steps even when the coefficient matrix of the main linear system to be solved for the step components is ill-conditioned. We investigate this surprisingly robust performance by using analytical tools from matrix perturbation theory and error analysis, illustrating our results with computational experiments. Finally, we point out the potential limitations of this approach. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. D. Andersen and K. D. Andersen, </author> <title> The apos linear programming solver: An implementation of the homogeneous algorithm, </title> <type> CORE Discussion Paper 9337, CORE, </type> <institution> Catholic University of Louvain, </institution> <year> 1997. </year>
Reference-contexts: The net effects of these approaches, and the approaches used in other Cholesky-based codes such as OB1 [9], HOPDM [6] and the APOS code of XPRESS-MP <ref> [1] </ref>, are all quite similar to those of the algorithm modchol that we analyze in this paper: Each small or negative pivot causes the Cholesky procedure to skip one stage, and the solution component corresponding to this pivot is set to zero (or to a very small number). <p> In a path following method, we have r xs = XS1 1;(10) where is the duality gap defined by = x T s=n;(11) and 2 <ref> [0; 1] </ref> is a centering parameter. <p> The second assumption is that t * 1=2 = ffi 1 :(73) We can expect this estimate to hold in all but pathological cases, since the elements of ~ L J J are bounded by 1, and its diagonal elements lie in the range <ref> [* 1=2 ; 1] </ref>. 16 In the following result, we bound the difference L T (^z z) in terms of k^zk, kzk and the norms kf k and kek of the perturbation vectors. <p> Hence, for all sufficiently small and all ff 2 <ref> [0; 1] </ref>, we have x i + ffx i &gt; 0. Similar logic can be applied to the remaining indices i 2 N , thereby proving (91). 5.3. Scaling the System (16a). We can use (84) to analyze the eigenstructure of the coefficient matrix AD 2 A T . <p> Our test problems have the form (2), with m = 6 and n = 12. The matrix A is fully dense, with elements (~ 1 :5)10 6 (~ 2 :5) , where ~ 1 and ~ 2 are random variables drawn from a uniform distribution on the interval <ref> [0; 1] </ref>. (Of course, the values of ~ 1 and ~ 2 are different for each element of the matrix.) After fixing the number of indices to appear in B, we set jN j = n jBj; N = f1; 2; ; jN jg; B = fjN j + 1; ; <p> = fjN j + 1; ; ng: (Note that the problem is degenerate whenever jBj 6= 6.) A primal solution x fl is constructed with x fl i = 10 3~1 (i = jN j + 1; ; n); where ~ is again randomly drawn from the uniform distribution on <ref> [0; 1] </ref>. We choose the dual solution fl to be the vector (1; 1; ; 1) T , and fix an optimal dual slack vector s fl to be s fl i = 0 (i = jN j + 1; ; n); where ~ is random as above.
Reference: [2] <author> A. R. Curtis and J. K. Reid, </author> <title> On the automatic scaling of matrices for Gaussian elimination, </title> <journal> J. Inst. Maths Applics, </journal> <volume> 10 (1972), </volume> <pages> pp. 118-124. </pages>
Reference-contexts: Most interior-point codes try to avoid these potential difficulties by prescaling the matrix A by some heuristic procedures, for example the one proposed by Curtis and Reid <ref> [2] </ref>. A second point concerns the matrix A B , the basic part of the constraint matrix A. Our analysis is quite general in that it allows A B to be rank deficient.
Reference: [3] <author> J. Czyzyk, S. Mehrotra, and S. J. Wright, </author> <title> PCx User Guide, </title> <type> Technical Report OTC 96/01, </type> <institution> Optimization Technology Center, Argonne National Laboratory and Northwestern University, </institution> <month> October </month> <year> 1996. </year> <title> Modified March, </title> <year> 1997. </year>
Reference-contexts: Instead of crashing, most codes modify the Cholesky procedure so that it skips the unacceptable pivots or replaces them with workable values. For instance, the offending pivot element is sometimes replaced by a huge number, as in LIPSOL [20] and PCx <ref> [3] </ref>. In other codes such as IPMOS [19], the pivot is replaced by a moderate number, but the corresponding right-hand side element is set to zero, as are the off-diagonal elements in the corresponding column of the Cholesky factor. <p> have the advantage that they can be implemented by changing just a few lines of a general sparse Cholesky codes, so it is possible to take advantage of the long-term development effort that has gone into designing such codes and their underlying algorithms. (The recent codes LIPSOL [20] and PCx <ref> [3] </ref> make explicit use of the sparse Cholesky code of Ng and Peyton [10].) Moreover, the good practical performance of pivot-skipping strategies made the search for alternatives less urgent. In this paper, we investigate the good performance of pivot-skipping strategies on the majority of practical problems. <p> The pivot skipping itself can be performed explicitly (by inserting a column of zeros in the Cholesky factor and maintaining a record of the set J ), or it can be "simulated," as in LIPSOL [20] and PCx <ref> [3] </ref>, by inserting a huge element in the pivot position prior to the computation of the column of the Cholesky factor and updating of the remainder of the matrix. In PCx [3], we needed to change fewer than 20 lines of the sparse Cholesky code of Ng and Peyton [10]. <p> maintaining a record of the set J ), or it can be "simulated," as in LIPSOL [20] and PCx <ref> [3] </ref>, by inserting a huge element in the pivot position prior to the computation of the column of the Cholesky factor and updating of the remainder of the matrix. In PCx [3], we needed to change fewer than 20 lines of the sparse Cholesky code of Ng and Peyton [10]. To test that the analysis of this paper was reflected in computations, we coded a simple primal-dual interior-point algorithm and applied it to test problems with controlled degeneracy properties.
Reference: [4] <author> I. I. Dikin, </author> <title> On the speed of an iterative process, </title> <type> Upravlyaemye Sistemi, </type> <year> (1974). </year>
Reference-contexts: By performing block elimination on (87), we have that AD 2 A T w = AD 2 (X 1 w): A well-known result (see Stewart [11], Todd [13], Dikin <ref> [4] </ref>, and Vanderbei and La-garias [14]) states that the norm k (AD 2 A T ) 1 AD 2 k is bounded over the set of all positive definite diagonal matrices D.
Reference: [5] <author> A. Forsgren, P. Gill, and J. Shinnerl, </author> <title> Stability of symmetric ill-conditioned systems arising in interior methods for constrained optimization, </title> <journal> SIAM Journal on Matrix Analysis and Applications, </journal> <volume> 17 (1996), </volume> <pages> pp. 187-211. </pages>
Reference-contexts: Ill conditioning of the coefficient matrix is the key issue in this formulation as well, but we showed that, in general, the calculated steps are good search directions for the interior-point method. Forsgren, Gill, and Shinnerl <ref> [5] </ref> perform a similar analysis in the context of logarithmic barrier methods for nonlinear problems, but they assume a certain ordering of the rows and columns of the coefficient matrix. 2 Notation. We summarize here the notation used in the remainder of the paper.
Reference: [6] <author> J. Gondzio, </author> <title> HOPDM (version 2.12): A fast lp solver based on a primal-dual interior point method, </title> <journal> European Journal of Operations Research, </journal> <volume> 85 (1995), </volume> <pages> pp. 221-225. </pages>
Reference-contexts: The net effects of these approaches, and the approaches used in other Cholesky-based codes such as OB1 [9], HOPDM <ref> [6] </ref> and the APOS code of XPRESS-MP [1], are all quite similar to those of the algorithm modchol that we analyze in this paper: Each small or negative pivot causes the Cholesky procedure to skip one stage, and the solution component corresponding to this pivot is set to zero (or to <p> Mehrotra's method requires the solution of two linear systems at each iteration|the affine scaling system (7), (8), (9), and the search direction system (7), (8), (12)|though the coefficient matrix is the same for both systems. Gondzio's <ref> [6] </ref> higher-order corrector method refines the step by solving additional linear systems, all with the same coefficient matrix as in (7).
Reference: [7] <author> N. J. Higham, </author> <title> Accuracy and Stability of Numerical Algorithms, </title> <publisher> SIAM Publications, </publisher> <address> Philadel-phia, </address> <year> 1996. </year>
Reference-contexts: The procedure terminates when none of the remaining diagonal elements is large enough to qualify as a pivot, and an approximate solution is computed with the partial factors. Higham <ref> [7, Chapter 10] </ref> presents an error analysis of this approach, and M. Wright [15] has considered its use in factoring the Hessian matrices that arise in the Newton/logarithmic-barrier method for nonlinear programming. <p> The submatrix consisting of rows and columns i through j is denoted by M i:j;i:j . Unit roundoff error is denoted by u. Higham <ref> [7, Chapter 1] </ref> defines u implicitly by the statement that when ff and are any two floating-point numbers, op denotes +, , fi, and =, and fl () denotes the floating-point representation of a real number, we have fl (ff op ) = (ff op )(1 + ffi) for some ffi <p> Cholesky factorization/triangular-solve pro-cedure on the matrix M J J , roundoff error in modchol and errors arising during the triangular substitutions can all be accounted for by adding a term E u J J to the coefficient matrix M J J in (65), where kE u see, for example, Higham <ref> [7, Theorem 10.4] </ref>. (Recall from Section 1 that ffi u denotes a modest multiple of u and that kM J J k = ffi 1 because of (26).) We assume that the error in evaluating M can also be incorporated into E u J J ; this is certainly true in
Reference: [8] <author> C. L. Lawson and R. J. Hanson, </author> <title> Solving Least Squares Problems, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1974. </year> <note> Reprinted by SIAM Publications, </note> <year> 1995. </year>
Reference-contexts: This work was supported by the Mathematical, Information, and Computational Sciences Division subprogram of the Office of Computational and Technology Research, U.S. Department of Energy, under Contract W-31-109-Eng-38. 1 In the context of Cholesky factorization of general symmetric positive semidefi--nite matrices, Lawson and Hanson <ref> [8, p. 125] </ref> advocate the use of pivot skipping when negative pivot are encountered. <p> While the resulting estimate appears to hold for the vast majority of practical problems of the type in question, there are cases in which it underestimates the value of k ~ L 1 J J k. See Lawson and Hanson <ref> [8, p. 31] </ref> for a classic example. Finally, we note that when all the skipped pivots occur in the lower right corner of the matrix M (as happens on most of the smaller problems we tested), we can replace the bound kEk * 1=2 by the tighter bound kEk *.
Reference: [9] <author> I. J. Lustig, R. E. Marsten, and D. F. Shanno, </author> <title> Computational experience with a primal-dual interior point method for linear programming, Linear Algebra and Its Applications, </title> <booktitle> 152 (1991), </booktitle> <pages> pp. 191-222. </pages>
Reference-contexts: The net effects of these approaches, and the approaches used in other Cholesky-based codes such as OB1 <ref> [9] </ref>, HOPDM [6] and the APOS code of XPRESS-MP [1], are all quite similar to those of the algorithm modchol that we analyze in this paper: Each small or negative pivot causes the Cholesky procedure to skip one stage, and the solution component corresponding to this pivot is set to zero
Reference: [10] <author> E. Ng and B. W. Peyton, </author> <title> Block sparse Cholesky algorithms on advanced uniprocessor computers, </title> <journal> SIAM Journal on Scientific Computing, </journal> <volume> 14 (1993), </volume> <pages> pp. 1034-1056. </pages>
Reference-contexts: lines of a general sparse Cholesky codes, so it is possible to take advantage of the long-term development effort that has gone into designing such codes and their underlying algorithms. (The recent codes LIPSOL [20] and PCx [3] make explicit use of the sparse Cholesky code of Ng and Peyton <ref> [10] </ref>.) Moreover, the good practical performance of pivot-skipping strategies made the search for alternatives less urgent. In this paper, we investigate the good performance of pivot-skipping strategies on the majority of practical problems. <p> In PCx [3], we needed to change fewer than 20 lines of the sparse Cholesky code of Ng and Peyton <ref> [10] </ref>. To test that the analysis of this paper was reflected in computations, we coded a simple primal-dual interior-point algorithm and applied it to test problems with controlled degeneracy properties.
Reference: [11] <author> G. W. Stewart, </author> <title> On scaled projections and pseudoinverses, Linear Algebra and Its Applications, </title> <booktitle> 112 (1989), </booktitle> <pages> pp. 189-193. </pages>
Reference-contexts: By performing block elimination on (87), we have that AD 2 A T w = AD 2 (X 1 w): A well-known result (see Stewart <ref> [11] </ref>, Todd [13], Dikin [4], and Vanderbei and La-garias [14]) states that the norm k (AD 2 A T ) 1 AD 2 k is bounded over the set of all positive definite diagonal matrices D.
Reference: [12] <author> G. W. Stewart and J. Sun, </author> <title> Matrix Perturbation Theory, </title> <booktitle> Computer Science and Scientific Computing, </booktitle> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: It is an immediate consequence of an eigenvalue perturbation result of Stewart and Sun <ref> [12, Corollary IV.4.13] </ref> and Lemma 3.2 that m X [ 2 i ] 2 kEk 2 The following result shows that if * is sufficiently small relative to the pth eigen value of M , them at least p pivots are accepted during modchol. Lemma 3.5. <p> Proof. Note first that p j J j by (47) and Lemma 3.5. The result is a straightforward consequence of Theorem V.2.8 of Stewart and Sun <ref> [12, p. 238] </ref>.
Reference: [13] <author> M. J. Todd, </author> <title> A Dantzig-Wolfe-like variant of Karmarkar's interior-point linear programming algorithm, </title> <journal> Operations Research, </journal> <volume> 38 (1990), </volume> <pages> pp. 1006-1018. </pages>
Reference-contexts: By performing block elimination on (87), we have that AD 2 A T w = AD 2 (X 1 w): A well-known result (see Stewart [11], Todd <ref> [13] </ref>, Dikin [4], and Vanderbei and La-garias [14]) states that the norm k (AD 2 A T ) 1 AD 2 k is bounded over the set of all positive definite diagonal matrices D.
Reference: [14] <author> R. J. Vanderbei and J. C. Lagarias, </author> <title> Dikin's convergence result for the affine-scaling algorithm, </title> <booktitle> Contemporary Mathematics, </booktitle> <year> (1990). </year>
Reference-contexts: By performing block elimination on (87), we have that AD 2 A T w = AD 2 (X 1 w): A well-known result (see Stewart [11], Todd [13], Dikin [4], and Vanderbei and La-garias <ref> [14] </ref>) states that the norm k (AD 2 A T ) 1 AD 2 k is bounded over the set of all positive definite diagonal matrices D.
Reference: [15] <author> M. H. Wright, </author> <title> Some properties of the Hessian of the logarithmic barrier function, </title> <journal> Mathematical Programming, </journal> <volume> 67 (1994), </volume> <pages> pp. 265-295. </pages>
Reference-contexts: The procedure terminates when none of the remaining diagonal elements is large enough to qualify as a pivot, and an approximate solution is computed with the partial factors. Higham [7, Chapter 10] presents an error analysis of this approach, and M. Wright <ref> [15] </ref> has considered its use in factoring the Hessian matrices that arise in the Newton/logarithmic-barrier method for nonlinear programming. This strategy is not practical in the context of interior-point linear programming codes because the matrices in question are too large to allow row and column exchanges to be performed efficiently. <p> Our analysis in this section applies to primal- and dual-degenerate linear programs. We conclude with some computational results in Section 6. A number of other theoretical papers on linear algebra operations in barrier and interior-point methods have appeared in recent years. We mentioned above the paper of M. Wright <ref> [15] </ref>, in which a Cholesky procedure with diagonal pivoting is used as the basis of an algorithm to construct steps that are accurate both in the subspace spanned by the active constraint Jacobian and its complement.
Reference: [16] <author> S. J. Wright, </author> <title> A path-following interior-point algorithm for linear and quadratic optimization problems, </title> <journal> Annals of Operations Research, </journal> <volume> 62 (1996), </volume> <pages> pp. </pages> <month> 103-130. </month> <title> [17] , Primal-Dual Interior-Point Methods, </title> <publisher> SIAM Publications, </publisher> <address> Philadelphia, </address> <year> 1997. </year> <title> [18] , Stability of augmented system factorizations in interior-point methods, </title> <journal> SIAM Journal on Matrix Analysis and Applications, </journal> <volume> 18 (1997), </volume> <pages> pp. 191-222. </pages>
Reference-contexts: Note than by our choice of B, A B consists of the last jBj columns of A. We modified A in some of the problems to introduce various types of rank deficiency. The code was an implementation of the infeasible-interior-point algorithm described by Wright <ref> [16] </ref>. The details of this algorithm are unimportant; we need note only that its iterates satisfy the estimates (83) in exact arithmetic and that the algorithm takes steps along the affine scaling direction during its later iterations, provided that these steps make reasonable progress.
Reference: [19] <author> X. Xu, P. Hung, and Y. Ye, </author> <title> A simplified homogeneous and self-dual linear programming algorithm and its implementation, </title> <journal> Annals of Operations Research, </journal> <volume> 62 (1996), </volume> <pages> pp. 151-172. </pages>
Reference-contexts: Instead of crashing, most codes modify the Cholesky procedure so that it skips the unacceptable pivots or replaces them with workable values. For instance, the offending pivot element is sometimes replaced by a huge number, as in LIPSOL [20] and PCx [3]. In other codes such as IPMOS <ref> [19] </ref>, the pivot is replaced by a moderate number, but the corresponding right-hand side element is set to zero, as are the off-diagonal elements in the corresponding column of the Cholesky factor.
Reference: [20] <author> Y. Zhang, </author> <title> Solving large-scale linear programs by interior-point methods under the MATLAB enviroment, </title> <type> Technical Report TR96-01, </type> <institution> Department of Mathematics and Statistics, University of Maryland Baltimore County, Baltimore, Md, </institution> <year> 1996. </year> <month> 33 </month>
Reference-contexts: Instead of crashing, most codes modify the Cholesky procedure so that it skips the unacceptable pivots or replaces them with workable values. For instance, the offending pivot element is sometimes replaced by a huge number, as in LIPSOL <ref> [20] </ref> and PCx [3]. In other codes such as IPMOS [19], the pivot is replaced by a moderate number, but the corresponding right-hand side element is set to zero, as are the off-diagonal elements in the corresponding column of the Cholesky factor. <p> hand, pivot-skipping strategies have the advantage that they can be implemented by changing just a few lines of a general sparse Cholesky codes, so it is possible to take advantage of the long-term development effort that has gone into designing such codes and their underlying algorithms. (The recent codes LIPSOL <ref> [20] </ref> and PCx [3] make explicit use of the sparse Cholesky code of Ng and Peyton [10].) Moreover, the good practical performance of pivot-skipping strategies made the search for alternatives less urgent. In this paper, we investigate the good performance of pivot-skipping strategies on the majority of practical problems. <p> The pivot skipping itself can be performed explicitly (by inserting a column of zeros in the Cholesky factor and maintaining a record of the set J ), or it can be "simulated," as in LIPSOL <ref> [20] </ref> and PCx [3], by inserting a huge element in the pivot position prior to the computation of the column of the Cholesky factor and updating of the remainder of the matrix.
References-found: 18

