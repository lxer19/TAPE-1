URL: http://www.cs.umd.edu/~tseng/papers/ipps98.ps
Refering-URL: http://www.cs.umd.edu/projects/cosmic/papers.html
Root-URL: 
Title: Compile-time Synchronization Optimizations for Software DSMs  
Author: Hwansoo Han, Chau-Wen Tseng 
Address: College Park, MD 20742  
Affiliation: Department of Computer Science University of Maryland  
Abstract: Software distributed-shared-memory (DSM) systems provide a desirable target for parallelizing compilers due to their flexibility. However, studies show synchronization and load imbalance are significant sources of overhead. In this paper, we investigate the impact of compilation techniques for eliminating synchronization overhead in software DSMs, developing new algorithms to handle situations found in practice. We evaluate the contributions of synchronization elimination algorithms based on 1) dependence analysis, 2) communication analysis, 3) exploiting coherence protocols in software DSMs, and 4) aggressive expansion of parallel SPMD regions. We also found suppressing expensive parallelism to be useful for one application. Experiments indicate these techniques eliminate almost all parallel task invocations, and reduce the number of barriers executed by 66% on average. On a 16 processor IBM SP-2, speedups are improved on average by 35%, and are tripled for some applications. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Carter, J. Bennett, and W. Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: Stoher and O'Boyle [25] extend this work, presenting an optimal algorithm for eliminating barrier synchronization in perfectly nested loops. There has been a large amount of research on software DSMs <ref> [1, 6, 19, 24, 30] </ref>. More recently, groups have examined combining compilers and software DSMs. Viswanathan and Larus developed a two-part predictive protocol for iterative computations for use in the data-parallel language C** [29]. Chandra and Larus evaluated combining the PGI HPF compiler and the Tempest software DSM system [2].
Reference: [2] <author> S. Chandra and J. Larus. </author> <title> Optimizing communication in HPF programs for fine-grain distributed shared memory. </title> <booktitle> In Proceedings of the Sixth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Las Vegas, NV, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: More recently, groups have examined combining compilers and software DSMs. Viswanathan and Larus developed a two-part predictive protocol for iterative computations for use in the data-parallel language C** [29]. Chandra and Larus evaluated combining the PGI HPF compiler and the Tempest software DSM system <ref> [2] </ref>. Results on a network of workstations connected by Myrinet indicates shared-memory versions of dense matrix programs achieve performance close to the message-passing codes generated. Granston and Wishoff suggest a number of compiler optimizations for software DSMs [7].
Reference: [3] <author> A. Cox, S. Dwarkadas, H. Lu, and W. Zwaenepoel. </author> <title> Evaluating the performance of software distributed shared memory as a target for parallelizing compilers. </title> <booktitle> In Proceedings of the 11th International Parallel Processing Symposium, </booktitle> <address> Geneva, Switzerland, </address> <month> Apr. </month> <year> 1997. </year>
Reference-contexts: The IBM SP-2 and DEC Alpha Cluster were provided by NSF CISE Institutional Infrastructure Award #CDA9401151 and grants from IBM and DEC. (16 Processor SP-2) Shared-memory parallelizing compilers are easy to use, flexible, and can accept a wide range of applications. Results from recent studies <ref> [3, 14] </ref> indicate they can approach the performance of current message-passing compilers or explicitly-parallel message-passing programs on distributed-memory machines. <p> Cox et al. conducted an experimental study to evaluate the performance of TreadMarks as a target for the Forge SPF shared-memory compiler from APR <ref> [3] </ref>. Results show that SPF/TreadMarks is slightly less efficient for dense-matrix programs, but outperforms compiler-generated message-passing code for irregular programs. They also identify opportunities for the compiler to eliminate unneeded barrier synchronization and aggregating messages in the shared-memory programs.
Reference: [4] <author> R. Cytron, J. Lipkis, and E. Schonberg. </author> <title> A compiler-assisted approach to SPMD execution. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <address> New York, NY, </address> <month> Nov. </month> <year> 1990. </year>
Reference-contexts: Eliminating barriers in compiler-parallelized codes is more difficult. Cytron et al. were the first to explore the possibilities of exploiting SPMD code for shared-memory multiprocessors <ref> [4] </ref>. They concentrated on safety concerns and the effect on privatiza-tion. O'Boyle and Bodin [20] present techniques similar to local communication analysis. They apply a classification algorithm to identify data dependences that cross processor boundaries, then apply heuristics based on max-cut to insert barrier synchronization and satisfy dependence.
Reference: [5] <author> S. Dwarkadas, A. Cox, and W. Zwaenepoel. </author> <title> An integrated compile-time/run-time software distributed shared memory system. </title> <booktitle> In Proceedings of the Eighth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VIII), </booktitle> <address> Boston, MA, </address> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: Mirchandaney et al. propose using section locks and broadcast barriers to guide eager updates of data and reductions based on multiple-writer protocols [18]. Dwarkadas et al. applied compiler analysis to explicitly par allel programs to improve their performance on a software DSM <ref> [5] </ref>. By combining analysis in the ParaScope programming environment with TreadMarks, they were able to compute data access patterns at compile time and use it to help the runtime system aggregate communication and synchronization.
Reference: [6] <author> S. Dwarkadas, P. Keleher, A. Cox, and W. Zwaenepoel. </author> <title> Evaluation of release consistent software distributed shared memory on emerging network technology. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 244-255, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Its primary coherence protocol implements a multiple-writer version of lazy release consistency, which allows processors to delay making modifications to shared data visible to other processors until special synchronization [13]. Experiments show that lazy-release-consistency protocols generally cause less communication than release consistency <ref> [6] </ref>. Consistency information in CVM is piggybacked on synchronization messages. Multiple updates are also aggregated in a single message where possible. 2.3 SUIF/CVM Interface SUIF was retargeted to generate code for CVM by providing a run-time interface based on CVM thread creation and synchronization primitives. <p> Stoher and O'Boyle [25] extend this work, presenting an optimal algorithm for eliminating barrier synchronization in perfectly nested loops. There has been a large amount of research on software DSMs <ref> [1, 6, 19, 24, 30] </ref>. More recently, groups have examined combining compilers and software DSMs. Viswanathan and Larus developed a two-part predictive protocol for iterative computations for use in the data-parallel language C** [29]. Chandra and Larus evaluated combining the PGI HPF compiler and the Tempest software DSM system [2].
Reference: [7] <author> E. Granston and H. Wishoff. </author> <title> Managing pages in shared virtual memory systems: Getting the compiler into the game. </title> <booktitle> In Proceedings of the 1993 ACM International Conference on Supercomputing, </booktitle> <address> Tokyo, Japan, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: Results on a network of workstations connected by Myrinet indicates shared-memory versions of dense matrix programs achieve performance close to the message-passing codes generated. Granston and Wishoff suggest a number of compiler optimizations for software DSMs <ref> [7] </ref>. These include tiling loop iterations so computation is on partitioned matching page boundaries, aligning arrays to pages, and inserting hints to use weak coherence. Mirchandaney et al. propose using section locks and broadcast barriers to guide eager updates of data and reductions based on multiple-writer protocols [18].
Reference: [8] <author> M. Hall, S. Amarasinghe, B. Murphy, S. Liao, and M. Lam. </author> <title> Detecting coarse-grain parallelism using an interprocedural parallelizing compiler. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <address> San Diego, CA, </address> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: In this paper we investigate a number of compiler techniques for reducing synchronization overhead and load imbalance for compiler-parallelized applications on software DSMs. Our techniques are evaluated in a prototype system [14] using the CVM [12] software distributed-shared-memory (DSM) as a compilation tar get for the SUIF <ref> [8] </ref> shared-memory compiler. <p> We present our prototype system, followed by experimental results. We conclude with a discussion of related work. 2 Background 2.1 SUIF Shared-Memory Compiler SUIF is a optimizing and parallelizing compiler developed at Stan-ford <ref> [8] </ref>. It has been successful in finding parallelism in many standard scientific applications. <p> Simple RPCs on the SP-2 require 160 secs. Nonlocal accesses causing page misses require 2-3 messages and take from 1 to 1.3 secs to fetch remote data. In our experiments, CVM [12] applications written in Fortran 77 were automatically parallelized by the Stanford SUIF parallelizing compiler version 1.1.2 <ref> [8] </ref>, with close to 100% of the computation in parallel regions. A simple chunk scheduling policy assigns contiguous iterations of equal or near-equal size to each processor, resulting in a consistent computation partition that encourages good locality.
Reference: [9] <author> H. Han, C.-W. Tseng, and P. Keleher. </author> <title> Reducing synchronization overhead for compiler-parallelized codes on software DSMs. </title> <booktitle> In Proceedings of the Tenth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Minneapolis, MN, </address> <month> Aug. </month> <year> 1997. </year>
Reference-contexts: If the subscripts differ by a sufficiently small constant, then each processor will access remote data only from neighboring processors. We find local communication analysis is sufficient to detect interprocessor communication in our applications <ref> [9] </ref>. In addition, we discover taking advantage of nearest-neighbor synchronization requires implementing a customized synchronization routine which sends a single message to each neighboring processor upon arrival, and continues as soon as messages are received from all neighboring processors [9]. <p> communication analysis is sufficient to detect interprocessor communication in our applications <ref> [9] </ref>. In addition, we discover taking advantage of nearest-neighbor synchronization requires implementing a customized synchronization routine which sends a single message to each neighboring processor upon arrival, and continues as soon as messages are received from all neighboring processors [9]. <p> Run-time enhancements such as the flush update protocol and support for reductions were helpful, but much synchronization overhead remained. More recently, we also presented techniques for reducing synchronization in software DSMs, including local communication analysis, customized nearest-neighbor synchronization <ref> [9] </ref>. Experiments found significant idle time remained, so in this paper we focused on additional optimizations, particularly techniques for aggressive expansion of SPMD regions. As a result, performance has improved. 6 Conclusions In this paper we investigate ways to improve the performance of shared-memory parallelizing compilers targeting software DSMs.
Reference: [10] <author> P. Hatcher and M. Quinn. </author> <title> Data-parallel Programming on MIMD Computers. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1991. </year>
Reference-contexts: Researchers compiling for fine-grain data-parallel languages sought to eliminate barriers following each expression evaluation <ref> [10, 21, 22] </ref>. Simple data dependence analysis can be used to reduce barrier synchronization by orders of magnitude, greatly improving performance. For barriers separating statements on the same loop level, Hatcher and Quinn use a two-dimensional radix sort to find the minimal number of barriers [10]. <p> Simple data dependence analysis can be used to reduce barrier synchronization by orders of magnitude, greatly improving performance. For barriers separating statements on the same loop level, Hatcher and Quinn use a two-dimensional radix sort to find the minimal number of barriers <ref> [10] </ref>. Philippsen and Heinz find the minimal number of barriers with an algorithm based on topological sort; they also attempt to minimize the amount of storage needed for intermediate results [21]. Eliminating barriers in compiler-parallelized codes is more difficult.
Reference: [11] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> Aug. </month> <year> 1992. </year>
Reference-contexts: The compiler can thus replace the barrier with a special execute-once barrier. Since anti-dependences may be ignored, the algorithm for inserting barrier synchronization becomes similar to the algorithm for message vectorization <ref> [11] </ref>. The level of the deepest true/flow cross-processor dependence becomes the point where synchronization must be inserted to prevent data races. <p> This paper extends our previous work on synchronization optimizations. We earlier presented communication analysis based on compile-time computation partition as a means of eliminating or replacing barrier synchronization [27]. Communication analysis techniques were similar to those used by distributed-memory compilers to calculate explicit communication <ref> [11] </ref>. Results were presented on a shared-memory multiprocessor. Here we use communication analysis for a simple static chunk partitioning of loop iterations, rather than more sophisticated partitioning based on global automatic data decompositions. We appear to eliminate approximately the same barriers despite the decrease in precision.
Reference: [12] <author> P. Keleher. </author> <title> The relative importance of concurrent writers and weak consistency models. </title> <booktitle> In 16th International Conference on Distributed Computing Systems, </booktitle> <address> Hong Kong, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: In this paper we investigate a number of compiler techniques for reducing synchronization overhead and load imbalance for compiler-parallelized applications on software DSMs. Our techniques are evaluated in a prototype system [14] using the CVM <ref> [12] </ref> software distributed-shared-memory (DSM) as a compilation tar get for the SUIF [8] shared-memory compiler. <p> After each parallel computation worker threads spin or go to sleep, waiting for additional work from the master thread. 2.2 CVM Software DSM CVM is a software DSM that supports coherent shared memory for multiple protocols and consistency models <ref> [12] </ref>. It is written entirely as a user-level library and runs on most UNIX-like systems. Its primary coherence protocol implements a multiple-writer version of lazy release consistency, which allows processors to delay making modifications to shared data visible to other processors until special synchronization [13]. <p> Simple RPCs on the SP-2 require 160 secs. Nonlocal accesses causing page misses require 2-3 messages and take from 1 to 1.3 secs to fetch remote data. In our experiments, CVM <ref> [12] </ref> applications written in Fortran 77 were automatically parallelized by the Stanford SUIF parallelizing compiler version 1.1.2 [8], with close to 100% of the computation in parallel regions.
Reference: [13] <author> P. Keleher, A. L. Cox, and W. Zwaenepoel. </author> <title> Lazy release consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: It is written entirely as a user-level library and runs on most UNIX-like systems. Its primary coherence protocol implements a multiple-writer version of lazy release consistency, which allows processors to delay making modifications to shared data visible to other processors until special synchronization <ref> [13] </ref>. Experiments show that lazy-release-consistency protocols generally cause less communication than release consistency [6]. Consistency information in CVM is piggybacked on synchronization messages.
Reference: [14] <author> P. Keleher and C.-W. Tseng. </author> <title> Enhancing software DSM for compiler-parallelized applications. </title> <booktitle> In Proceedings of the 11th International Parallel Processing Symposium, </booktitle> <address> Geneva, Switzerland, </address> <month> Apr. </month> <year> 1997. </year>
Reference-contexts: The IBM SP-2 and DEC Alpha Cluster were provided by NSF CISE Institutional Infrastructure Award #CDA9401151 and grants from IBM and DEC. (16 Processor SP-2) Shared-memory parallelizing compilers are easy to use, flexible, and can accept a wide range of applications. Results from recent studies <ref> [3, 14] </ref> indicate they can approach the performance of current message-passing compilers or explicitly-parallel message-passing programs on distributed-memory machines. <p> In this paper we investigate a number of compiler techniques for reducing synchronization overhead and load imbalance for compiler-parallelized applications on software DSMs. Our techniques are evaluated in a prototype system <ref> [14] </ref> using the CVM [12] software distributed-shared-memory (DSM) as a compilation tar get for the SUIF [8] shared-memory compiler. <p> Performance was improved by adding customized support for reductions, as well as a flush update protocol that at barriers automatically sends updates to processors possessing copies of recently modified shared data <ref> [14] </ref>. Compiler analysis needed to use the flush update protocol is much simpler than communication analysis needed in HPF compilers. <p> The resulting C output code was compiled by g++ version 2.7.2 with the -O2 flag, then linked with the SUIF run-time system and the CVM libraries to produce executable code on the IBM SP-2. Customized support for reductions and the flush update protocol were used to improve overall performance <ref> [14] </ref>. All synchronization optimizations were implemented in the SUIF compiler, except for expanding SPMD regions across procedure boundaries (due to lack of interprocedural analysis). For our experiments, procedures in time-step loops were inlined by hand. <p> We find synchronization overhead is significantly higher for software DSMs, making optimizations more important. We also earlier described compiler and run-time techniques for obtaining improved performance for compiler-parallelized programs on software DSMs <ref> [14] </ref>. Run-time enhancements such as the flush update protocol and support for reductions were helpful, but much synchronization overhead remained. More recently, we also presented techniques for reducing synchronization in software DSMs, including local communication analysis, customized nearest-neighbor synchronization [9].
Reference: [15] <author> C. Koelbel, D. Loveman, R. Schreiber, G. Steele, Jr., and M. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> The MIT Press, </publisher> <address> Cam-bridge, MA, </address> <year> 1994. </year>
Reference-contexts: Expert users are willing to expend time and effort to write message-passing programs for important applications. However, easier programming methods must be found for general users to make parallel computing truly successful. Compilers for languages such as High Performance Fortran (HPF) <ref> [15] </ref> provide a partial solution because they allow users to avoid writing explicit message-passing code, but HPF compilers currently only support a limited class of data-parallel applications.
Reference: [16] <author> Z. Li. </author> <title> Compiler algorithms for event variable synchronization. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: for software DSMs than previously measured for shared-memory systems, even when fewer barriers are eliminated. 5 Related Work Before studying methods for eliminating barrier synchronization, researchers investigated efficient use of data and event synchronization, where post and wait statements are used to synchronize between data items [26] or loop iterations <ref> [16] </ref>. Researchers compiling for fine-grain data-parallel languages sought to eliminate barriers following each expression evaluation [10, 21, 22]. Simple data dependence analysis can be used to reduce barrier synchronization by orders of magnitude, greatly improving performance.
Reference: [17] <author> H. Lu, A. Cox, S. Dwarkadas, R. Rajamony, and W. Zwaenepoel. </author> <title> Compiler and software distributed shared memory support for irregular applications. </title> <booktitle> In Proceedings of the Sixth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Las Vegas, NV, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: In comparison, SUIF at compile time eliminates many barriers guarding only anti-dependences. Lu et al. found that software DSMs can also efficiently support irregular applications when using compile-time analysis to prefetch index arrays at run time <ref> [17] </ref>. Tzeng and Kongmunvattana improve the efficiency of barriers for software DSMs [28]. Our measurements show actual time spent in barrier routines is small compared to load imbalance caused by barriers, so improving barrier efficiency is likely to have only minor impact on application performance.
Reference: [18] <author> R. Mirchandaney, S. Hiranandani, and A. Sethi. </author> <title> Improving the performance of DSM systems via compiler involvement. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <address> Washington, DC, </address> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: These include tiling loop iterations so computation is on partitioned matching page boundaries, aligning arrays to pages, and inserting hints to use weak coherence. Mirchandaney et al. propose using section locks and broadcast barriers to guide eager updates of data and reductions based on multiple-writer protocols <ref> [18] </ref>. Dwarkadas et al. applied compiler analysis to explicitly par allel programs to improve their performance on a software DSM [5].
Reference: [19] <author> S. Mukherjee, S. Sharma, M. Hill, J. Larus, A. Rogers, and J. Saltz. </author> <title> Efficient support for irregular applications on distributed-memory machines. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Stoher and O'Boyle [25] extend this work, presenting an optimal algorithm for eliminating barrier synchronization in perfectly nested loops. There has been a large amount of research on software DSMs <ref> [1, 6, 19, 24, 30] </ref>. More recently, groups have examined combining compilers and software DSMs. Viswanathan and Larus developed a two-part predictive protocol for iterative computations for use in the data-parallel language C** [29]. Chandra and Larus evaluated combining the PGI HPF compiler and the Tempest software DSM system [2].
Reference: [20] <author> M. O'Boyle and F. Bodin. </author> <title> Compiler reduction of synchronization in shared virtual memory systems. </title> <booktitle> In Proceedings of the 1995 ACM International Conference on Supercomputing, </booktitle> <address> Barcelona, Spain, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Eliminating barriers in compiler-parallelized codes is more difficult. Cytron et al. were the first to explore the possibilities of exploiting SPMD code for shared-memory multiprocessors [4]. They concentrated on safety concerns and the effect on privatiza-tion. O'Boyle and Bodin <ref> [20] </ref> present techniques similar to local communication analysis. They apply a classification algorithm to identify data dependences that cross processor boundaries, then apply heuristics based on max-cut to insert barrier synchronization and satisfy dependence.
Reference: [21] <author> M. Philippsen and E. Heinz. </author> <title> Automatic synchronization elimination in synchronous FORALLs. </title> <booktitle> In Frontiers '95: The 5th Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <address> McLean, VA, </address> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: Researchers compiling for fine-grain data-parallel languages sought to eliminate barriers following each expression evaluation <ref> [10, 21, 22] </ref>. Simple data dependence analysis can be used to reduce barrier synchronization by orders of magnitude, greatly improving performance. For barriers separating statements on the same loop level, Hatcher and Quinn use a two-dimensional radix sort to find the minimal number of barriers [10]. <p> Philippsen and Heinz find the minimal number of barriers with an algorithm based on topological sort; they also attempt to minimize the amount of storage needed for intermediate results <ref> [21] </ref>. Eliminating barriers in compiler-parallelized codes is more difficult. Cytron et al. were the first to explore the possibilities of exploiting SPMD code for shared-memory multiprocessors [4]. They concentrated on safety concerns and the effect on privatiza-tion. O'Boyle and Bodin [20] present techniques similar to local communication analysis.
Reference: [22] <author> S. Prakash, M. Dhagat, and R. Bagrodia. </author> <title> Synchronization issues in data-parallel languages. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> Aug. </month> <year> 1993. </year>
Reference-contexts: Researchers compiling for fine-grain data-parallel languages sought to eliminate barriers following each expression evaluation <ref> [10, 21, 22] </ref>. Simple data dependence analysis can be used to reduce barrier synchronization by orders of magnitude, greatly improving performance. For barriers separating statements on the same loop level, Hatcher and Quinn use a two-dimensional radix sort to find the minimal number of barriers [10].
Reference: [23] <author> R. Rajamony and A. Cox. </author> <title> A performance debugger for eliminating excess synchronization in shared-memory parallel programs. </title> <booktitle> In Proceedings of the Fourth International Workshop on Modeling, Analysis, </booktitle> <institution> and Simulation of Computer and Telecommunication Systems (MASCOTS), </institution> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: Many of their suggestions are implemented in the SUIF/CVM system and are evaluated in this paper. Rajamony and Cox developed a performance debugger for detecting unnecessary synchronization at run-time by instrumenting all loads and stores <ref> [23] </ref>. In the SPLASH application Water, it was able to detect barriers guarding only anti and output dependences that may be eliminated by applying odd-even renaming. In comparison, SUIF at compile time eliminates many barriers guarding only anti-dependences.
Reference: [24] <author> D. Scales and K. Gharachorloo. </author> <title> Design and performance of the Shasta distributed shared memory protocol. </title> <booktitle> In Proceedings of the 1997 ACM International Conference on Supercomputing, </booktitle> <address> Vienna, Austria, </address> <month> July </month> <year> 1997. </year>
Reference-contexts: Stoher and O'Boyle [25] extend this work, presenting an optimal algorithm for eliminating barrier synchronization in perfectly nested loops. There has been a large amount of research on software DSMs <ref> [1, 6, 19, 24, 30] </ref>. More recently, groups have examined combining compilers and software DSMs. Viswanathan and Larus developed a two-part predictive protocol for iterative computations for use in the data-parallel language C** [29]. Chandra and Larus evaluated combining the PGI HPF compiler and the Tempest software DSM system [2].
Reference: [25] <author> E. Stohr and M. O'Boyle. </author> <title> A graph based approachto barrier synchro-nisation minimisation. </title> <booktitle> In Proceedings of the 1997 ACM International Conference on Supercomputing, </booktitle> <address> Vienna, Austria, </address> <month> July </month> <year> 1997. </year>
Reference-contexts: O'Boyle and Bodin [20] present techniques similar to local communication analysis. They apply a classification algorithm to identify data dependences that cross processor boundaries, then apply heuristics based on max-cut to insert barrier synchronization and satisfy dependence. Stoher and O'Boyle <ref> [25] </ref> extend this work, presenting an optimal algorithm for eliminating barrier synchronization in perfectly nested loops. There has been a large amount of research on software DSMs [1, 6, 19, 24, 30]. More recently, groups have examined combining compilers and software DSMs.
Reference: [26] <author> P. Tang, P. Yew, and C. Zhu. </author> <title> Compiler techniques for data synchronization in nested parallel loops. </title> <booktitle> In Proceedings of the 1990 ACM International Conference on Supercomputing,Amsterdam, </booktitle> <address> The Netherlands, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: thus achieve better results for software DSMs than previously measured for shared-memory systems, even when fewer barriers are eliminated. 5 Related Work Before studying methods for eliminating barrier synchronization, researchers investigated efficient use of data and event synchronization, where post and wait statements are used to synchronize between data items <ref> [26] </ref> or loop iterations [16]. Researchers compiling for fine-grain data-parallel languages sought to eliminate barriers following each expression evaluation [10, 21, 22]. Simple data dependence analysis can be used to reduce barrier synchronization by orders of magnitude, greatly improving performance.
Reference: [27] <author> C.-W. Tseng. </author> <title> Compiler optimizations for eliminating barrier synchronization. </title> <booktitle> In Proceedingsof the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Barriers are particularly expensive in software DSMs, since synchronization triggers communication of coherence data for lazy-release-consistency systems. The added cost of communication and OS support also increase the potential for load imbalance in software DSMs. In previous work, we developed compiler algorithms for barrier elimination <ref> [27] </ref>. We first generate code for parallel loops using the single-program, multiple-data (SPMD) programming model found in message-passing programs, where all threads execute the entire program. Sequential computation is either replicated or explicitly guarded to limit execution to a single thread, while parallel computation is partitioned and executed across processors. <p> J = 1,N ... broadcast DO TIME CALL P () CALL Q () barrier PROCEDURE P () DO I = LB 1 ,UB 1 PROCEDURE Q () DO J = LB 1 ,UB 1 (H) 3.1 Communication Analysis Our implementation of communication analysis in SUIF differs somewhat from previous work <ref> [27] </ref>. Instead of relying on actual data/computation partitions provided in a language such as HPF, we rely on knowledge of the simple computation partitioning strategy applied by the SUIF compiler to efficiently detect most cases of interprocessor communication. <p> This paper extends our previous work on synchronization optimizations. We earlier presented communication analysis based on compile-time computation partition as a means of eliminating or replacing barrier synchronization <ref> [27] </ref>. Communication analysis techniques were similar to those used by distributed-memory compilers to calculate explicit communication [11]. Results were presented on a shared-memory multiprocessor. Here we use communication analysis for a simple static chunk partitioning of loop iterations, rather than more sophisticated partitioning based on global automatic data decompositions.
Reference: [28] <author> N.-F. Tzeng and A. Kongmunvattana. </author> <title> Distributed shared memory systems with improved barrier synchronization and data transfer. </title> <booktitle> In Proceedings of the 1997 ACM International Conference on Supercomputing, </booktitle> <address> Vienna, Austria, </address> <month> July </month> <year> 1997. </year>
Reference-contexts: In comparison, SUIF at compile time eliminates many barriers guarding only anti-dependences. Lu et al. found that software DSMs can also efficiently support irregular applications when using compile-time analysis to prefetch index arrays at run time [17]. Tzeng and Kongmunvattana improve the efficiency of barriers for software DSMs <ref> [28] </ref>. Our measurements show actual time spent in barrier routines is small compared to load imbalance caused by barriers, so improving barrier efficiency is likely to have only minor impact on application performance. This paper extends our previous work on synchronization optimizations.
Reference: [29] <author> G. Viswanathan and J. Larus. </author> <title> Compiler-directed shared-memory communication for iterative parallel computations. </title> <booktitle> In Proceedings of Supercomputing '96, </booktitle> <address> Pittsburgh, PA, </address> <month> Nov. </month> <year> 1996. </year>
Reference-contexts: There has been a large amount of research on software DSMs [1, 6, 19, 24, 30]. More recently, groups have examined combining compilers and software DSMs. Viswanathan and Larus developed a two-part predictive protocol for iterative computations for use in the data-parallel language C** <ref> [29] </ref>. Chandra and Larus evaluated combining the PGI HPF compiler and the Tempest software DSM system [2]. Results on a network of workstations connected by Myrinet indicates shared-memory versions of dense matrix programs achieve performance close to the message-passing codes generated.
Reference: [30] <author> Y. Zhou, L. Iftode, J. Singh, K. Li, B. Toonen, I. Schoinas, M. Hill, and D. Wood. </author> <title> Relaxed consistency and coherence granularity ind DSM systems: A performance evaluation. </title> <booktitle> In Proceedings of the Sixth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Las Vegas, NV, </address> <month> June </month> <year> 1997. </year> <month> 8 </month>
Reference-contexts: Stoher and O'Boyle [25] extend this work, presenting an optimal algorithm for eliminating barrier synchronization in perfectly nested loops. There has been a large amount of research on software DSMs <ref> [1, 6, 19, 24, 30] </ref>. More recently, groups have examined combining compilers and software DSMs. Viswanathan and Larus developed a two-part predictive protocol for iterative computations for use in the data-parallel language C** [29]. Chandra and Larus evaluated combining the PGI HPF compiler and the Tempest software DSM system [2].
References-found: 30

