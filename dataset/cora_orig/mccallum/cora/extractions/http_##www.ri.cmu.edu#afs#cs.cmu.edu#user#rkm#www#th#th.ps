URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/rkm/www/th/th.ps
Refering-URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/rkm/www/speechbib.html
Root-URL: 
Title: Efficient Algorithms for Speech Recognition  
Author: Mosur K. Ravishankar Roberto Bisiani, co-chair Raj Reddy, co-chair Alexander Rudnicky Richard Stern Wayne Ward c Mosur K. Ravishankar 
Degree: Submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy. Thesis Committee:  
Note: This research was supported by the Department of the Navy, Naval Research Laboratory under Grant No. N00014-93-1-2005. The views and conclusions contained in this document are those of the author and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. government.  
Address: Pittsburgh, PA 15213  
Affiliation: School of Computer Science Computer Science Division Carnegie Mellon University  (University of Milan)  
Date: May 15, 1996  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Alleva, F., Hon, H., Hwang, M., Rosenfeld, R. and Weide, R. </author> <title> Applying SPHINX-II to the DARPA Wall Street Journal CSR Task. </title> <booktitle> In Proceedings of Speech and Natural Language Workshop, </booktitle> <month> Feb. </month> <year> 1992, </year> <pages> pp 393-398. </pages>
Reference-contexts: For this purpose, we have chosen the Sphinx-II speech recognition system 1 at Carnegie Mellon that has been used extensively in speech research and the yearly ARPA evaluations. Various aspects of this baseline system and its precursors have been reported in the literature, notably in <ref> [32, 33, 35, 28, 1, 2] </ref>. Most of these concentrate on the modelling aspects of the system|acoustic, grammatical or lexical|and their effect on recognition accuracy. In this chapter we focus on obtaining a comprehensive set of performance characteristics for this system.
Reference: [2] <author> Alleva, F., Huang, X., and Hwang, M. </author> <title> An Improved Search Algorithm for Continuous Speech Recognition. </title> <booktitle> In IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <year> 1993. </year>
Reference-contexts: For this purpose, we have chosen the Sphinx-II speech recognition system 1 at Carnegie Mellon that has been used extensively in speech research and the yearly ARPA evaluations. Various aspects of this baseline system and its precursors have been reported in the literature, notably in <ref> [32, 33, 35, 28, 1, 2] </ref>. Most of these concentrate on the modelling aspects of the system|acoustic, grammatical or lexical|and their effect on recognition accuracy. In this chapter we focus on obtaining a comprehensive set of performance characteristics for this system. <p> Stack decoding, a variant of the A* search algorithm 2 [42], is more appropriate for use with such grammars which lead to greater recognition accuracy. This algorithm maintains a stack of several possible partial decodings (i.e, word sequence hypotheses) which are expanded in a best-first manner <ref> [9, 2, 50] </ref>. Since each partial hypothesis is a linear word sequence, any arbitrary language model can be applied to it. Stack decoding also allows the decoder to output several most likely N-best hypotheses rather than just the single best one. <p> above candidate list) - extend H by appending W to it, giving new partial hypothesis H'; evaluate new score for H' using forward and backward lattices; insert H' into the stack in accordance with its new score; - - The specific details relevant to the Sphinx-II implementation are covered in <ref> [2] </ref>. Most of the additional details pertain to two steps: identifying candidate word extensions for a partial hypothesis H, and computing the score for each newly created partial hypothesis H 0 . Candidate words are located by looking for lattice entries that begin where the partial hypothesis ends.
Reference: [3] <author> Antoniol, G., Brugnara, F., Cettolo, M. and Federico, M. </author> <title> Language Model Representation for Beam-Search Decoding. </title> <booktitle> In IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <month> May </month> <year> 1995, </year> <pages> pp 588-591. </pages>
Reference-contexts: Since many words share common pronunciation prefixes, they can also share models and avoid duplication. Trees were initially used in fast match algorithms for producing candidate word lists for further search. Recently, they have been introduced in the main search component of several systems <ref> [44, 39, 43, 3] </ref>. The main problem faced by them is in using a language model. Normally, transitions between words are accompanied by a prior language model probability. <p> Hence, the use of a flat lexical structure for bigram transitions must continue to incur this cost. Replicated Bigram Trees Ney and others <ref> [40, 3] </ref> have suggested creating copies of the lexical tree to handle bigram transitions. The leaf nodes at the first level (unigram) lexical tree have secondary (bigram) trees hanging off them for bigram transitions. <p> SEARCH SPEED OPTIMIZATION * The number of unigram transitions is reduced significantly because of the tree structure. However, the number of bigram transitions is similar to that of the baseline system (Section 3.4.3, Table 3.6), which still constitutes a significant computational load. Alternative solutions are proposed in <ref> [40, 3] </ref> that construct separate secondary trees for the bigram section, instead of the flat lexical structure of Figure 4.4. Both of them report results on 10,000 word vocabulary experiments. <p> The compact nature of the word lattice, combined with its low error rate, makes it an ideal input for further postprocessing using more detailed acoustic models and search algorithms. The lexical tree described in this section can be contrasted to those described in <ref> [40, 3, 39, 43] </ref> in their treatment of the language model. By deferring the application of language model probabilities to the leaves of the tree, we gain a significant reduction in computation. 68 CHAPTER 4.
Reference: [4] <author> Bahl, L.R., Bakis, R., Cohen, P.S., Cole, A.G., Jelinek, F., Lewis, B.L. and Mercer, </author> <title> R.L. Further Results on the Recognition of a Continuously Read Natural Corpus. </title> <booktitle> In IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <month> Apr. </month> <year> 1980, </year> <pages> pp 872-876. </pages>
Reference-contexts: INTRODUCTION Most systems employ triphones as one form of context-dependent HMM models <ref> [4, 33] </ref> to deal with this problem. Triphones are basically phones observed in the context of given preceding and succeeding phones. There are approximately 50 phones in spoken English language. <p> The production of sound corresponding to a phone is influenced by neighbouring phones. For example, the AE phone in the word "man" sounds different from that in 11 12 CHAPTER 2. BACKGROUND "lack"; the former is more nasal. IBM <ref> [4] </ref> proposed the use of triphone or context-dependent phone models to deal with such variations. With 50 phones, there can be up to 50 3 triphones, but only a fraction of them are actually observed in practice.
Reference: [5] <author> Bahl, L.R., Balakrishnan-Aiyer, S., Franz, M., Gopalakrishnan, P.S., Gopinath, R., Novak, M., Padmanabhan, M. and Roukos, S. </author> <title> The IBM Large Vocabulary Continuous Speech Recognition System for the ARPA NAB News Task. </title> <booktitle> In Proceedings of ARPA Spoken Language System Technology Workshop, </booktitle> <month> Jan. </month> <year> 1995, </year> <pages> pp 121-126. </pages>
Reference-contexts: The use of HMMs in speech has been described, for example, by Rabiner [52]. Currently, almost all systems use HMMs for modelling triphones and context-independent phones (also referred to as monophones or basephones). These include BBN [41], CMU [35, 27], the Cambridge HTK system [65], IBM <ref> [5] </ref>, and LIMSI [18], among others. We will give a brief description of HMMs as used in speech. First of all, the sampled speech input is usually preprocessed, through various signal-processing steps, into a cepstrum or other feature stream that contains one feature vector every frame. <p> At the time that this work was begun, the Sphinx-II semi-continuous acoustic models were the best available to us. Over the last two years fully continuous acoustic models <ref> [66, 5, 18] </ref> have become much more widely used in the speech community. They reduce the word error rate of recognition systems by a relative amount of about 20-30% compared to semi-continuous acoustic models 1 [46, 47]. The use of fully continuous models does not eliminate the search problem. <p> The lattice can be searched using more detailed and sophisticated models and search algorithms efficiently. The use of multi-pass systems is not new. Most current speech recognition systems are of that nature <ref> [41, 65, 5, 15, 19, 38] </ref>. Many reports cite the savings to be had by postprocessing a word lattice [65, 38] instead of the entire vocabulary. However, the task of actually producing such lattices efficiently has been relatively unexplored.
Reference: [6] <author> Bahl, L.R., Brown, P.F., DeSouza, P.V. and Mercer, </author> <title> R.L. Obtaining Candidate Words by Polling in a Large Vocabulary Speech Recognition System. </title> <booktitle> In IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <year> 1988. </year>
Reference-contexts: To avoid an exponential growth in the set of possible word sequences in medium and large vocabulary systems, partial hypotheses are expanded only by a limited set of candidate words at each step. These candidates are identified by a fast match step <ref> [6, 7, 8, 20] </ref>. Since our experiments have been mostly confined to Viterbi decoding, we do not explore stack decoding in any greater detail. Tree Structured Lexicons Even with the beam search heuristic, straightforward Viterbi decoding is expensive.
Reference: [7] <author> Bahl, L.R., De Gennaro, V., Gopalakrishnan, P.S. and Mercer, </author> <title> R.L. A Fast Approximate Acoustic Match for Large Vocabulary Speech Recognition. </title> <journal> IEEE Transactions on Speech and Audio Processing, </journal> <volume> Vol. 1, No. 1, </volume> <month> Jan </month> <year> 1993, </year> <pages> pp 59-67. 126 Bibliography </pages>
Reference-contexts: To avoid an exponential growth in the set of possible word sequences in medium and large vocabulary systems, partial hypotheses are expanded only by a limited set of candidate words at each step. These candidates are identified by a fast match step <ref> [6, 7, 8, 20] </ref>. Since our experiments have been mostly confined to Viterbi decoding, we do not explore stack decoding in any greater detail. Tree Structured Lexicons Even with the beam search heuristic, straightforward Viterbi decoding is expensive.
Reference: [8] <author> Bahl, L.R., DeSouza, P.V., Gopalakrishnan, P.S., Nahamoo, D. and Pich-eney, M. </author> <title> A Fast Match for Continuous Speech Recognition Using Allophonic Models. </title> <booktitle> In IEEE International Conference on Acoustics, Speech, and Signal Processing, 1992, vol.I, </booktitle> <pages> pp. </pages> <address> I-17 - I-21. </address>
Reference-contexts: To avoid an exponential growth in the set of possible word sequences in medium and large vocabulary systems, partial hypotheses are expanded only by a limited set of candidate words at each step. These candidates are identified by a fast match step <ref> [6, 7, 8, 20] </ref>. Since our experiments have been mostly confined to Viterbi decoding, we do not explore stack decoding in any greater detail. Tree Structured Lexicons Even with the beam search heuristic, straightforward Viterbi decoding is expensive.
Reference: [9] <author> Bahl, L.R., Jelinek, F. and Mercer, R. </author> <title> A Maximum Likelihood Approach to Continuous Speech Recognition. </title> <journal> In IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> Vol. PAMI-5, No. 2, </volume> <month> Mar. </month> <year> 1983, </year> <pages> pp. 179-190. </pages>
Reference-contexts: Speech recognition|searching for the most likely sequence of words given the input speech|gives rise to an exponential search space if all possible sequences of words are considered. The problem has generally been tackled in two ways: Viterbi decoding [62, 52] using beam search [37], or stack decoding <ref> [9, 50] </ref> which is a variant of the A* algorithm [42]. Some hybrid versions that combine Viterbi decoding with the A* algorithm also exist [21]. <p> Stack decoding, a variant of the A* search algorithm 2 [42], is more appropriate for use with such grammars which lead to greater recognition accuracy. This algorithm maintains a stack of several possible partial decodings (i.e, word sequence hypotheses) which are expanded in a best-first manner <ref> [9, 2, 50] </ref>. Since each partial hypothesis is a linear word sequence, any arbitrary language model can be applied to it. Stack decoding also allows the decoder to output several most likely N-best hypotheses rather than just the single best one.
Reference: [10] <author> Baker, J.K. </author> <title> The DRAGON System-An Overview. </title> <booktitle> In IEEE Transactions on Acoustics, Speech, and Signal Processing, </booktitle> <address> ASSP-23(1), </address> <month> Feb. </month> <year> 1975, </year> <pages> pp. 24-29. </pages>
Reference-contexts: The usage and training of HMMs has been covered widely in the literature. Initially described by Baum in [11], it was first used in speech recognition systems by CMU <ref> [10] </ref> and IBM [29]. The use of HMMs in speech has been described, for example, by Rabiner [52]. Currently, almost all systems use HMMs for modelling triphones and context-independent phones (also referred to as monophones or basephones).
Reference: [11] <author> Baum, L.E. </author> <title> An Inequality and Associated Maximization Technique in Statistical Estimation of Probabilistic Functions of Markov Processes. </title> <booktitle> Inequalities 3 </booktitle> <pages> 1-8, </pages> <year> 1972. </year>
Reference-contexts: The usage and training of HMMs has been covered widely in the literature. Initially described by Baum in <ref> [11] </ref>, it was first used in speech recognition systems by CMU [10] and IBM [29]. The use of HMMs in speech has been described, for example, by Rabiner [52]. Currently, almost all systems use HMMs for modelling triphones and context-independent phones (also referred to as monophones or basephones).
Reference: [12] <author> Bellegarda, J. and Nahamoo, D. </author> <title> Tied Mixture Continuous Parameter Modeling for Speech Recognition. </title> <booktitle> In IEEE Transactions on Acoustics, Speech, and Signal Processing, </booktitle> <month> Dec. </month> <year> 1990, </year> <pages> pp 2033-2045. </pages>
Reference-contexts: In this chapter we focus on obtaining a comprehensive set of performance characteristics for this system. The baseline Sphinx-II recognition system uses semi-continuous or tied-mixture hidden Markov models (HMMs) for the acoustic models <ref> [52, 27, 12] </ref> and word bigram or trigram backoff language models (see Sections 2.1 and 2.2). It is a 3-pass decoder structured as follows: 1. Time synchronous Viterbi beam search [52, 62, 37] in the forward direction.
Reference: [13] <author> Bisiani, R. and Ravishankar, </author> <title> M PLUS: A Distributed Shared-Memory System. </title> <booktitle> 17th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1990, </year> <pages> pp. 115-124. </pages>
Reference-contexts: It is possible to take good advantage of this facility. One of the early attempts at speeding up the Sphinx-II baseline system exploited the large degree of concurrency within its algorithmic steps [54]. In a parallel implementation on the PLUS multiprocessor designed at CMU <ref> [13] </ref>, a speed up of 3.9 was 90 CHAPTER 4. SEARCH SPEED OPTIMIZATION obtained on a 5 node configuration. The parallelization involved static partitioning of data and computation among multiple threads using the Mach C-threads facility [16]. <p> For example, the evaluation of acoustic models and the HMM network search can be performed in parallel, with very simple communication between them. One of the early work in this area was in parallelizing the forward Viterbi pass of the baseline Sphinx-II system on a 5-node shared-memory multiprocessor <ref> [14, 13] </ref> on the 1000-word Resource Management task [51], which yielded a speedup of about 3.8 [54]. Parallelizing the lexical tree search is a little different, but the potential exists, nevertheless.
Reference: [14] <author> Bisiani, R. and Ravishankar, </author> <title> M Design and Implementation of the PLUS Multiprocessor Internal report, </title> <institution> School of Computer Science, Carnegie Mellon University, Pittsburgh, </institution> <month> Jan. </month> <year> 1991. </year>
Reference-contexts: For example, the evaluation of acoustic models and the HMM network search can be performed in parallel, with very simple communication between them. One of the early work in this area was in parallelizing the forward Viterbi pass of the baseline Sphinx-II system on a 5-node shared-memory multiprocessor <ref> [14, 13] </ref> on the 1000-word Resource Management task [51], which yielded a speedup of about 3.8 [54]. Parallelizing the lexical tree search is a little different, but the potential exists, nevertheless.
Reference: [15] <author> Chase, L., Rosenfeld, R., Hauptmann, A., Ravishankar, M., Thayer, E., Place-way, P., Weide, R. and Lu, C. </author> <title> Improvements in Language, Lexical, and Phonetic Modeling in Sphinx-II. </title> <booktitle> In Proceedings of ARPA Spoken Language Systems Technology Workshop, </booktitle> <month> Jan. </month> <year> 1995, </year> <pages> pp. 60-65. </pages>
Reference-contexts: The lattice can be searched using more detailed and sophisticated models and search algorithms efficiently. The use of multi-pass systems is not new. Most current speech recognition systems are of that nature <ref> [41, 65, 5, 15, 19, 38] </ref>. Many reports cite the savings to be had by postprocessing a word lattice [65, 38] instead of the entire vocabulary. However, the task of actually producing such lattices efficiently has been relatively unexplored.
Reference: [16] <author> Cooper, </author> <title> E.C. and Draves, R.P. C Threads. </title> <type> Technical Report, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> Nov. </month> <year> 1990. </year>
Reference-contexts: In a parallel implementation on the PLUS multiprocessor designed at CMU [13], a speed up of 3.9 was 90 CHAPTER 4. SEARCH SPEED OPTIMIZATION obtained on a 5 node configuration. The parallelization involved static partitioning of data and computation among multiple threads using the Mach C-threads facility <ref> [16] </ref>. Though the lexical tree decoder has significant structural differences compared to the baseline system, some of the parallelization techniques can still be applied to it. We explore this question in some detail in this section.
Reference: [17] <author> Coremen, T.H., Leiserson, C.E. and Rivest, </author> <title> R.L. Introduction to Algorithms, </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1992. </year>
Reference-contexts: The word sequence making up this path has to be the globally optimum one. Given the above formulation of the problem, any of the textbook algorithms for finding the least-cost path can be applied <ref> [17] </ref>. Given a graph with N nodes and E edges, the least-cost path can be found in time proportional to N + E.
Reference: [18] <author> Gauvain, J.L., Lamel, L.F., Adda, G., and Adda-Decker, M. </author> <title> The LIMSI Nov93 WSJ System. </title> <booktitle> In Proceedings of ARPA Speech and Natural Language Workshop, </booktitle> <month> Mar. </month> <year> 1994, </year> <pages> pp 125-128. Bibliography 127 </pages>
Reference-contexts: The use of HMMs in speech has been described, for example, by Rabiner [52]. Currently, almost all systems use HMMs for modelling triphones and context-independent phones (also referred to as monophones or basephones). These include BBN [41], CMU [35, 27], the Cambridge HTK system [65], IBM [5], and LIMSI <ref> [18] </ref>, among others. We will give a brief description of HMMs as used in speech. First of all, the sampled speech input is usually preprocessed, through various signal-processing steps, into a cepstrum or other feature stream that contains one feature vector every frame. Frames are typically spaced at 10msec intervals. <p> At the time that this work was begun, the Sphinx-II semi-continuous acoustic models were the best available to us. Over the last two years fully continuous acoustic models <ref> [66, 5, 18] </ref> have become much more widely used in the speech community. They reduce the word error rate of recognition systems by a relative amount of about 20-30% compared to semi-continuous acoustic models 1 [46, 47]. The use of fully continuous models does not eliminate the search problem.
Reference: [19] <author> Gauvain, J.L., Lamel, L. and Adda-Decker, M. </author> <title> Developments in Large Vocabulary Dictation: The LIMSI Nov94 NAB System. </title> <booktitle> In Proceedings of ARPA Spoken Language Systems Technology Workshop, </booktitle> <month> Jan. </month> <year> 1995, </year> <pages> pp. 131-138. </pages>
Reference-contexts: The lattice can be searched using more detailed and sophisticated models and search algorithms efficiently. The use of multi-pass systems is not new. Most current speech recognition systems are of that nature <ref> [41, 65, 5, 15, 19, 38] </ref>. Many reports cite the savings to be had by postprocessing a word lattice [65, 38] instead of the entire vocabulary. However, the task of actually producing such lattices efficiently has been relatively unexplored.
Reference: [20] <author> Gillick, L.S. and Roth, R. </author> <title> A Rapid Match Algorithm for Continuous Speech Recognition. </title> <booktitle> In Proceedings of DARPA Speech and Natural Language Workshop, </booktitle> <month> Jun. </month> <year> 1990, </year> <pages> pp. 170-172. </pages>
Reference-contexts: To avoid an exponential growth in the set of possible word sequences in medium and large vocabulary systems, partial hypotheses are expanded only by a limited set of candidate words at each step. These candidates are identified by a fast match step <ref> [6, 7, 8, 20] </ref>. Since our experiments have been mostly confined to Viterbi decoding, we do not explore stack decoding in any greater detail. Tree Structured Lexicons Even with the beam search heuristic, straightforward Viterbi decoding is expensive.
Reference: [21] <author> Gopalakrishnan, </author> <title> P.S., Bahl, L.R., and Mercer, R.L. A Tree Search Strategy for Large-Vocabulary Continuous Speech Recognition. </title> <booktitle> In IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <month> May </month> <year> 1995, </year> <pages> pp 572-575. </pages>
Reference-contexts: The problem has generally been tackled in two ways: Viterbi decoding [62, 52] using beam search [37], or stack decoding [9, 50] which is a variant of the A* algorithm [42]. Some hybrid versions that combine Viterbi decoding with the A* algorithm also exist <ref> [21] </ref>. Viterbi Decoding Viterbi decoding is a dynamic programming algorithm that searches the state space for the most likely state sequence that accounts for the input speech.
Reference: [22] <author> Gopalakrishnan, P.S., Nahamoo, D., Padmanabhan, M. and Picheny, M.A. </author> <title> A Channel-Bank-Based Phone Detection Strategy. </title> <booktitle> In IEEE International Conference on Acoustics, Speech, and Signal Processing, April 1994, </booktitle> <volume> Vol II, </volume> <pages> pp 161-164. </pages>
Reference-contexts: If a region of frames has many phones active in it, on the other hand, it indicates a high degree of acoustic confusion in that region. The acoustic modelling in that segment is less reliable. The technique of identifying active phones has been discussed in <ref> [22] </ref>, however, only in the context of applying it to a fast match. They have reported a reduction in fast-match computation of about 50% with a slightly under 10% increase in error rate. A similar technique using phone posterior probabilities has also been reported in [56].
Reference: [23] <author> Gopinath, R. et al. </author> <title> The IBM continuous speech recognition system on demonstration. </title> <booktitle> ARPA Spoken Language Systems Technology Workshop, </booktitle> <month> Jan. </month> <year> 1995. </year>
Reference-contexts: In order to be practically useful, speech recognition systems have to be efficient in their usage of computational resources as well. There clearly are several real-time recognition systems around in the ARPA speech research community <ref> [23, 60, 55, 24] </ref>. However, the published literature is relatively bare regarding them. Their performance has never been formally evaluated with respect to the research systems or with respect to one another, in the way that the accuracy of research systems has been.
Reference: [24] <editor> Hauptmann, A. et al. </editor> <booktitle> The News-on-Demand demonstration. In ARPA Speech Recognition Workshop, </booktitle> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: In order to be practically useful, speech recognition systems have to be efficient in their usage of computational resources as well. There clearly are several real-time recognition systems around in the ARPA speech research community <ref> [23, 60, 55, 24] </ref>. However, the published literature is relatively bare regarding them. Their performance has never been formally evaluated with respect to the research systems or with respect to one another, in the way that the accuracy of research systems has been.
Reference: [25] <author> Huang, X., Acero, A., Alleva, F., Beeferman, D., Hwang, M. and Mahajan, M. </author> <title> From CMU Sphinx-II to Microsoft Whisper-Making Speech Recognition Usable. In Automatic Speech and Speaker Recognition-Advanced Topics, </title> <editor> Lee, Paliwal, and Soong, editors, </editor> <publisher> Kluwer Publishers, </publisher> <year> 1994. </year>
Reference-contexts: The size of the acoustic models is trivially reduced by a factor of 4, simply by reducing the precision of their representation from 32 bits to 8 bits, with no difference in accuracy. This has, in fact, been done in many other systems as in <ref> [25] </ref>. The new observation is that in addition to memory size reduction, the smaller precision also allows us to speed up the computation of acoustic output probabilities of senones every frame. The computation involves the summation of probabilities|in log-domain, which is cumbersome.
Reference: [26] <author> Huang, X., Acero, A., Alleva, F., Hwang, M., Jiang, L. and Mahajan, M. </author> <title> Mi-crosoft Windows Highly Intelligent Speech Recognizer: Whisper. </title> <booktitle> In IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <month> May </month> <year> 1995, </year> <journal> Vol. </journal> <volume> 1, </volume> <pages> pp. 93-96. </pages>
Reference-contexts: This is actually fairly high since the time to decode a sentence on an HP735 platform is reported to be about 15 minutes on average. 2.4.2 Memory Size and Speed Improvements in Whisper The CMU Sphinx-II system has been improved in many ways by Microsoft in producing the Whisper system <ref> [26] </ref>. They report that memory size has been reduced by a factor of 20 and speed improved by a factor of 5, compared to Sphinx-II under the same accuracy constraints.
Reference: [27] <author> Hwang, Mei-Yuh. </author> <title> Subphonetic Acoustic Modeling for Speaker-Independent Continuous Speech Recognition. </title> <type> Ph.D. thesis, Tech Report No. </type> <institution> CMU-CS-93-230, Computer Science Department, Carnegie Mellon University, </institution> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: Acoustic Models One of the key issues in acoustic modelling has been the choice of a good unit of speech <ref> [32, 27] </ref>. In small vocabulary systems of a few tens of words, it is possible to build separate models for entire words, but this approach quickly becomes infeasible as the vocabulary size grows. For one thing, it is hard to obtain sufficient training data to build all individual word models. <p> For example, in Sphinx-II, a 20,000 word vocabulary has about 75,000 distinct triphones, each of which is modelled by a 5-state HMM, for a total of about 375,000 states. Since there isn't sufficient training data to build models for each state, they are clustered into equivalence classes called senones <ref> [27] </ref>. The introduction of context-dependent acoustic models, even after clustering into equivalence classes, creates an explosion in the memory requirements to store such models. For example, the Sphinx-II system with 10,000 senones occupies tens of megabytes of memory. <p> The results from this analysis show that the search component is several tens of times slower than real time on the reported tasks. (The acoustic output probability computation is relatively smaller since these tests have been conducted using semi-continuous acoustic models <ref> [28, 27] </ref>.) Furthermore, the search time itself can be further decomposed into two main components: the evaluation of HMM models, and carrying out cross-word transitions at word boundaries. The former is simply a measure of the task complexity. <p> The use of HMMs in speech has been described, for example, by Rabiner [52]. Currently, almost all systems use HMMs for modelling triphones and context-independent phones (also referred to as monophones or basephones). These include BBN [41], CMU <ref> [35, 27] </ref>, the Cambridge HTK system [65], IBM [5], and LIMSI [18], among others. We will give a brief description of HMMs as used in speech. <p> The sharing can be of different degrees. In semi-continuous systems, all states share a single mixture Gaussian codebook, but the mixture coefficients are distinct for individual states. In Sphinx-II, states are grouped into clusters called senones <ref> [27] </ref>, with a single codebook (per feature stream) shared among all senones, but distinct mixture weights for each. Thus, Sphinx-II uses semi-continuous modelling with state clustering. Even simpler discrete HMM models can be derived by replacing the mean and variance vectors representing Gaussian densities with a single centroid. <p> In this chapter we focus on obtaining a comprehensive set of performance characteristics for this system. The baseline Sphinx-II recognition system uses semi-continuous or tied-mixture hidden Markov models (HMMs) for the acoustic models <ref> [52, 27, 12] </ref> and word bigram or trigram backoff language models (see Sections 2.1 and 2.2). It is a 3-pass decoder structured as follows: 1. Time synchronous Viterbi beam search [52, 62, 37] in the forward direction. <p> In addition to the acoustic models and pronunciation lexicon described below, Sphinx-II uses word bigram and trigram grammars. These have been discussed in Section 2.2. 3.1.1 Acoustic Model Signal Processing A detailed description of the signal processing front end in Sphinx-II is contained in Section 4.2.1 Signal Processing of <ref> [27] </ref>. The block diagram in Figure 3.1 depicts the overall processing. Briefly, the stream of 16-bit samples of speech data, sampled at 16KHz, is converted into 12-element mel scale frequency cepstrum vectors and a power coefficient in each 10msec frame. <p> All HMMs in Sphinx-II have the same 5-state Bakis topology shown in the Figure 3.2. (The background on HMMs has been covered briefly in Section 2.1.2.) As mentioned in Section 2.1.2, Sphinx-II uses semi-continuous acoustic modelling with 256 component densities in each feature codebook. States are clustered into senones <ref> [27] </ref>, where each senone has its own set of 256 mixture coefficients weighting the codebook for each feature stream. <p> Mei-Yuh Hwang in her dissertation <ref> [27] </ref> has pointed out that ": : : each senone describes a very short distinct acoustic event (shorter than a phoneme): : : ," and ": : : it can be used to construct models of all kinds of acoustic phenomena." One of the phenomena modelled by senones is the relative
Reference: [28] <author> Hwang, M. and Huang X. </author> <title> Shared-Distribution Hidden Markov Models for Speech Recognition. </title> <journal> In IEEE Transactions on Speech and Audio Processing, </journal> <month> Oct. </month> <year> 1993, </year> <pages> pp 414-420. 128 Bibliography </pages>
Reference-contexts: The results from this analysis show that the search component is several tens of times slower than real time on the reported tasks. (The acoustic output probability computation is relatively smaller since these tests have been conducted using semi-continuous acoustic models <ref> [28, 27] </ref>.) Furthermore, the search time itself can be further decomposed into two main components: the evaluation of HMM models, and carrying out cross-word transitions at word boundaries. The former is simply a measure of the task complexity. <p> For this purpose, we have chosen the Sphinx-II speech recognition system 1 at Carnegie Mellon that has been used extensively in speech research and the yearly ARPA evaluations. Various aspects of this baseline system and its precursors have been reported in the literature, notably in <ref> [32, 33, 35, 28, 1, 2] </ref>. Most of these concentrate on the modelling aspects of the system|acoustic, grammatical or lexical|and their effect on recognition accuracy. In this chapter we focus on obtaining a comprehensive set of performance characteristics for this system.
Reference: [29] <author> Jelinek, F. </author> <title> Continuous Speech Recognition by Statistical Methods. </title> <booktitle> In Proceedings of the IEEE, </booktitle> <volume> Vol. 64, No. 4, </volume> <month> Apr. </month> <year> 1976, </year> <pages> pp. 532-556. </pages>
Reference-contexts: The usage and training of HMMs has been covered widely in the literature. Initially described by Baum in [11], it was first used in speech recognition systems by CMU [10] and IBM <ref> [29] </ref>. The use of HMMs in speech has been described, for example, by Rabiner [52]. Currently, almost all systems use HMMs for modelling triphones and context-independent phones (also referred to as monophones or basephones).
Reference: [30] <author> Katz, </author> <title> S.M. Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer. </title> <journal> In IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> vol. ASSP-35, </volume> <month> Mar. </month> <pages> 87, pp. 400-401. </pages>
Reference-contexts: But a backoff weight is applied to account for the fact that w j is known to be not one of the bigram successors of w i <ref> [30] </ref>. Other higher-order backoff n-gram grammars can be defined similarly. * Class n-gram grammars. These are similar to word n-gram grammars, except that the tokens are entire word classes, such as digit, number, month, proper name, etc.
Reference: [31] <author> Kershaw, D.J., Robinson, A.J., and Renals, S.J. </author> <title> The 1995 ABBOT Hybrid Connectionist-HMM Large-Vocabulary Recognition System. </title> <booktitle> In ARPA Speech Recognition Workshop, </booktitle> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: We explore the trade-offs presented by these alternatives. A somewhat similar approach to search pruning has also been suggested in <ref> [56, 31] </ref>. In their work, phones are pruned from the search process based on their posterior probabilities estimated using neural network models. It is also different in that the pruning mechanism is embedded in a hybrid Viterbi-stack decoding algorithm.
Reference: [32] <author> Lee, K. </author> <title> Large Vocabulary Speaker-Independent Continuous Speech Recognition: The SPHINX System. </title> <type> Ph.D. thesis, </type> <institution> Computer Science Department, Carnegie Mellon University, </institution> <month> April </month> <year> 1988. </year>
Reference-contexts: Acoustic Models One of the key issues in acoustic modelling has been the choice of a good unit of speech <ref> [32, 27] </ref>. In small vocabulary systems of a few tens of words, it is possible to build separate models for entire words, but this approach quickly becomes infeasible as the vocabulary size grows. For one thing, it is hard to obtain sufficient training data to build all individual word models. <p> For this purpose, we have chosen the Sphinx-II speech recognition system 1 at Carnegie Mellon that has been used extensively in speech research and the yearly ARPA evaluations. Various aspects of this baseline system and its precursors have been reported in the literature, notably in <ref> [32, 33, 35, 28, 1, 2] </ref>. Most of these concentrate on the modelling aspects of the system|acoustic, grammatical or lexical|and their effect on recognition accuracy. In this chapter we focus on obtaining a comprehensive set of performance characteristics for this system.
Reference: [33] <author> Lee, K. </author> <title> Context-Dependent Phonetic Hidden Markov Models for Continuous Speech Recognition. </title> <booktitle> In IEEE Transactions on Acoustics, Speech, and Signal Processing, </booktitle> <month> Apr. </month> <year> 1990, </year> <pages> pp 599-609. </pages>
Reference-contexts: INTRODUCTION Most systems employ triphones as one form of context-dependent HMM models <ref> [4, 33] </ref> to deal with this problem. Triphones are basically phones observed in the context of given preceding and succeeding phones. There are approximately 50 phones in spoken English language. <p> For this purpose, we have chosen the Sphinx-II speech recognition system 1 at Carnegie Mellon that has been used extensively in speech research and the yearly ARPA evaluations. Various aspects of this baseline system and its precursors have been reported in the literature, notably in <ref> [32, 33, 35, 28, 1, 2] </ref>. Most of these concentrate on the modelling aspects of the system|acoustic, grammatical or lexical|and their effect on recognition accuracy. In this chapter we focus on obtaining a comprehensive set of performance characteristics for this system.
Reference: [34] <author> Lee, K. </author> <title> Context-Dependent Phonetic Hidden Markov Models for Speaker-Independent Continuous Speech Recognition. In Readings in Speech Recognition, </title> <editor> ed. Waibel, A. and Lee, K. </editor> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1990, </year> <pages> pp. 347-365. </pages>
Reference-contexts: Since Sphinx-II uses triphone acoustic models <ref> [34] </ref>, these base phone sequences are converted into triphone sequences by simply taking each base phone together with its left and right context base phones. (Note that the phonetic left context at the beginning of a word is the last base phone from the previous word.
Reference: [35] <author> Lee, K., Hon, H., and Reddy, R. </author> <title> An Overview of the SPHINX Speech Recognition System. </title> <booktitle> In IEEE Transactions on Acoustics, Speech, and Signal Processing, </booktitle> <month> Jan </month> <year> 1990, </year> <pages> pp 35-45. </pages>
Reference-contexts: The use of HMMs in speech has been described, for example, by Rabiner [52]. Currently, almost all systems use HMMs for modelling triphones and context-independent phones (also referred to as monophones or basephones). These include BBN [41], CMU <ref> [35, 27] </ref>, the Cambridge HTK system [65], IBM [5], and LIMSI [18], among others. We will give a brief description of HMMs as used in speech. <p> For this purpose, we have chosen the Sphinx-II speech recognition system 1 at Carnegie Mellon that has been used extensively in speech research and the yearly ARPA evaluations. Various aspects of this baseline system and its precursors have been reported in the literature, notably in <ref> [32, 33, 35, 28, 1, 2] </ref>. Most of these concentrate on the modelling aspects of the system|acoustic, grammatical or lexical|and their effect on recognition accuracy. In this chapter we focus on obtaining a comprehensive set of performance characteristics for this system.
Reference: [36] <author> Ljolje, A., Riley, M., Hindle, D. and Pereira, F. </author> <title> The AT&T 60,000 Word Speech-To-Text System. </title> <booktitle> In Proceedings of ARPA Spoken Language System Technology Workshop, </booktitle> <month> Jan. </month> <year> 1995, </year> <pages> pp. 162-165. </pages>
Reference-contexts: This is not a major concern in small vocabulary systems in which words are not easily confusable, but becomes an issue as the vocabulary size and the degree of confusability increase. 1 Some systems define word pronunciations as networks of phones instead of simple linear sequences <ref> [36] </ref>. 4 CHAPTER 1. INTRODUCTION Most systems employ triphones as one form of context-dependent HMM models [4, 33] to deal with this problem. Triphones are basically phones observed in the context of given preceding and succeeding phones. There are approximately 50 phones in spoken English language.
Reference: [37] <author> Lowerre, B. </author> <title> The Harpy Speech Understanding System. </title> <type> Ph.D. thesis, </type> <institution> Computer Science Department, Carnegie Mellon University, </institution> <month> Apr </month> <year> 1976. </year>
Reference-contexts: Speech recognition|searching for the most likely sequence of words given the input speech|gives rise to an exponential search space if all possible sequences of words are considered. The problem has generally been tackled in two ways: Viterbi decoding [62, 52] using beam search <ref> [37] </ref>, or stack decoding [9, 50] which is a variant of the A* algorithm [42]. Some hybrid versions that combine Viterbi decoding with the A* algorithm also exist [21]. <p> To keep the computation within manageable limits, only the most likely states are evaluated in each frame, according to the beam search heuristic <ref> [37] </ref>. At the end of time t, the state with the highest path probability P max (t) is found. <p> The baseline Sphinx-II recognition system uses semi-continuous or tied-mixture hidden Markov models (HMMs) for the acoustic models [52, 27, 12] and word bigram or trigram backoff language models (see Sections 2.1 and 2.2). It is a 3-pass decoder structured as follows: 1. Time synchronous Viterbi beam search <ref> [52, 62, 37] </ref> in the forward direction. It is a complete search of the full vocabulary, using semi-continuous acoustic models, a bigram or trigram language model, and cross-word triphone modelling during the search.
Reference: [38] <author> Murveit, H., Butzberger, J., Digalakis, V. and Weintraub, M. </author> <title> Large-Vocabulary Dictation Using SRI's Decipher Speech Recognition System: Progressive Search Techniques. </title> <booktitle> In IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <month> Apr. </month> <year> 1993, </year> <month> vol.II, </month> <pages> pp. </pages> <address> II-319 II-322. </address>
Reference-contexts: In our work, a 10sec long sentence typically produces a word lattice containing about 1000 word instances. Given such compact lattices with low error rates, one can search them using sophisticated models and search algorithms very efficiently and obtain results with a lower word error rate, as described in <ref> [38, 65, 41] </ref>. Most systems use such multipass techniques. However, there has been relatively little work reported in actually creating such lattices efficiently. This is important for the practical applicability of such techniques. <p> The lattice can be searched using more detailed and sophisticated models and search algorithms efficiently. The use of multi-pass systems is not new. Most current speech recognition systems are of that nature <ref> [41, 65, 5, 15, 19, 38] </ref>. Many reports cite the savings to be had by postprocessing a word lattice [65, 38] instead of the entire vocabulary. However, the task of actually producing such lattices efficiently has been relatively unexplored. <p> The use of multi-pass systems is not new. Most current speech recognition systems are of that nature [41, 65, 5, 15, 19, 38]. Many reports cite the savings to be had by postprocessing a word lattice <ref> [65, 38] </ref> instead of the entire vocabulary. However, the task of actually producing such lattices efficiently has been relatively unexplored. This is necessary for the practical application of accurate, large vocabulary speech recognition and it is addressed in this thesis.
Reference: [39] <author> Murveit, H., Monaco, P., Digalakis, V. and Butzberger, J. </author> <title> Techniques to Achieve an Accurate Real-Time Large-Vocabulary Speech Recognition System. </title> <booktitle> In Proceedings of ARPA Human Language Technology Workshop, </booktitle> <month> Mar. </month> <year> 1994, </year> <pages> pp 368-373. Bibliography 129 </pages>
Reference-contexts: Since many words share common pronunciation prefixes, they can also share models and avoid duplication. Trees were initially used in fast match algorithms for producing candidate word lists for further search. Recently, they have been introduced in the main search component of several systems <ref> [44, 39, 43, 3] </ref>. The main problem faced by them is in using a language model. Normally, transitions between words are accompanied by a prior language model probability. <p> There can be a conflict between the best incoming cross-word transition 18 CHAPTER 2. BACKGROUND for different words that share the same root node. This problem has been usually solved by making copies of the lexical tree to resolve such conflicts. Approximate Bigram Trees SRI <ref> [39] </ref> and CRIM [43] augment their lexical tree structure with a flat copy of the lexicon that is activated for bigram transitions. All bigram transitions enter the flat lexicon copy, while the backed off unigram transitions enter the roots of the lexical tree. <p> This scheme creates additional copies of words that did not exist in the original flat structure. For example, in the conventional flat lexicon (or in the auxiliary flat lexicon copy of <ref> [39] </ref>), there is only one instance of each word. However, in this proposed scheme the same word can appear in multiple secondary trees. Since the short function words are recognized often (though spuriously), their bigram copies are frequently active. They are also among the larger ones, as noted above. <p> Moreover, the active HMMs to be evaluated during search are concentrated near the beginning of words. Specifically, over 60-70% of the active HMMs are word-initial models. This is not a new result. It has also been pointed out before, for example in <ref> [39, 43] </ref>, although it has not been quantified as systematically. * The other half of the search cost is attributable to the evaluation of crossword transitions, along with the need to perform several thousands of language model accesses in each frame. <p> Tree-structured lexicons have often been used in the past, especially in fast-match algorithms as a precursor step to a stack-decoding algorithm. More recently, tree search has come into widespread use in the main decoding process <ref> [43, 39] </ref>. Figure 4.1 shows a simple base-phone lexical tree 2 for a small example lexicon. <p> One solution to this problem has been to augment the lexical tree with a separate flat bigram section. The latter is used for all bigram transitions and the lexical tree only for unigram transitions <ref> [39] </ref>. The scheme is shown in Figure 4.4. <p> The compact nature of the word lattice, combined with its low error rate, makes it an ideal input for further postprocessing using more detailed acoustic models and search algorithms. The lexical tree described in this section can be contrasted to those described in <ref> [40, 3, 39, 43] </ref> in their treatment of the language model. By deferring the application of language model probabilities to the leaves of the tree, we gain a significant reduction in computation. 68 CHAPTER 4. <p> It analyzes the technique of deferring the application of language model probabilities until the leaves of the lexical tree in an efficient way, and reduces the cost of computing these probabilities by an order of magnitude. Other tree-based searches <ref> [39, 43, 65, 66, 40] </ref> attempt to overcome the problem by creating bigram copies of the search tree. This has three problems: the search space is increased, the cost of language model access is still substantial, and the physical memory size requirements also increase. <p> Furthermore, it operates in a small fraction of real time and its cost is negligible. Word lattice searches have been proposed in <ref> [65, 39] </ref>, for example, but they are directed more towards using the lattice for driving later search passes with more detailed models. * Use of HMM state output probabilities as a fast match to produce candidate 7.3.
Reference: [40] <author> Ney, H., Haeb-Umbach, R. and Tran, B.-H. </author> <title> Improvements in Beam Search for 10000-Word Continuous Speech Recognition. </title> <booktitle> In IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <month> Mar. </month> <year> 1992, </year> <month> vol.I, </month> <pages> pp. </pages> <address> I-9 - I-12. </address>
Reference-contexts: Hence, the use of a flat lexical structure for bigram transitions must continue to incur this cost. Replicated Bigram Trees Ney and others <ref> [40, 3] </ref> have suggested creating copies of the lexical tree to handle bigram transitions. The leaf nodes at the first level (unigram) lexical tree have secondary (bigram) trees hanging off them for bigram transitions. <p> SEARCH SPEED OPTIMIZATION * The number of unigram transitions is reduced significantly because of the tree structure. However, the number of bigram transitions is similar to that of the baseline system (Section 3.4.3, Table 3.6), which still constitutes a significant computational load. Alternative solutions are proposed in <ref> [40, 3] </ref> that construct separate secondary trees for the bigram section, instead of the flat lexical structure of Figure 4.4. Both of them report results on 10,000 word vocabulary experiments. <p> The compact nature of the word lattice, combined with its low error rate, makes it an ideal input for further postprocessing using more detailed acoustic models and search algorithms. The lexical tree described in this section can be contrasted to those described in <ref> [40, 3, 39, 43] </ref> in their treatment of the language model. By deferring the application of language model probabilities to the leaves of the tree, we gain a significant reduction in computation. 68 CHAPTER 4. <p> It analyzes the technique of deferring the application of language model probabilities until the leaves of the lexical tree in an efficient way, and reduces the cost of computing these probabilities by an order of magnitude. Other tree-based searches <ref> [39, 43, 65, 66, 40] </ref> attempt to overcome the problem by creating bigram copies of the search tree. This has three problems: the search space is increased, the cost of language model access is still substantial, and the physical memory size requirements also increase.
Reference: [41] <author> Nguyen, L., Anastasakos, T., Kubala, F., LaPre, C., Makhoul, J., Schwartz, R., Yuan, N., Zavaliagkos, G. and Zhao, Y. </author> <title> The 1994 BBN/BYBLOS Speech Recognition System. </title> <booktitle> In Proceedings of ARPA Spoken Language Systems Technology Workshop, </booktitle> <month> Jan. </month> <year> 1995, </year> <pages> pp. 77-81. </pages>
Reference-contexts: In our work, a 10sec long sentence typically produces a word lattice containing about 1000 word instances. Given such compact lattices with low error rates, one can search them using sophisticated models and search algorithms very efficiently and obtain results with a lower word error rate, as described in <ref> [38, 65, 41] </ref>. Most systems use such multipass techniques. However, there has been relatively little work reported in actually creating such lattices efficiently. This is important for the practical applicability of such techniques. <p> The use of HMMs in speech has been described, for example, by Rabiner [52]. Currently, almost all systems use HMMs for modelling triphones and context-independent phones (also referred to as monophones or basephones). These include BBN <ref> [41] </ref>, CMU [35, 27], the Cambridge HTK system [65], IBM [5], and LIMSI [18], among others. We will give a brief description of HMMs as used in speech. <p> The lattice can be searched using more detailed and sophisticated models and search algorithms efficiently. The use of multi-pass systems is not new. Most current speech recognition systems are of that nature <ref> [41, 65, 5, 15, 19, 38] </ref>. Many reports cite the savings to be had by postprocessing a word lattice [65, 38] instead of the entire vocabulary. However, the task of actually producing such lattices efficiently has been relatively unexplored.
Reference: [42] <author> Nilsson, </author> <title> N.J. </title> <booktitle> Problem Solving Methods in Artificial Intelligence. </booktitle> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1971. </year>
Reference-contexts: The problem has generally been tackled in two ways: Viterbi decoding [62, 52] using beam search [37], or stack decoding [9, 50] which is a variant of the A* algorithm <ref> [42] </ref>. Some hybrid versions that combine Viterbi decoding with the A* algorithm also exist [21]. Viterbi Decoding Viterbi decoding is a dynamic programming algorithm that searches the state space for the most likely state sequence that accounts for the input speech. <p> Although a trigram grammar is used in the forward pass, it is not a complete trigram search (see Section 3.2.2). Stack decoding, a variant of the A* search algorithm 2 <ref> [42] </ref>, is more appropriate for use with such grammars which lead to greater recognition accuracy. This algorithm maintains a stack of several possible partial decodings (i.e, word sequence hypotheses) which are expanded in a best-first manner [9, 2, 50]. <p> Specifically, each partial hypotheses in the sorted stack can account for a different initial segment of the input speech. This makes it hard to compare the path probabilities of the entries in the stack. It has been shown in <ref> [42] </ref> that the second issue can be solved by attaching a heuristic score with every partial hypothesis H that accounts for the remaining portion of the speech not included in H. <p> The backward pass identifies several beginning times for a word, but typically only one ending time. Acoustic scores for each word segmentation are available in the backward pass word lattice. 3.3.2 A* Search The A* search algorithm is described in <ref> [42] </ref>. It works by maintaining an ordered stack or list of partial hypotheses, sorted in descending order of likelihood. Hypotheses are word sequences and may be of different lengths, accounting for different lengths of input speed.
Reference: [43] <author> Normandin, Y., Bowness, D., Cardin, R., Drouin, C., Lacouture, R. and Lazarides, A. </author> <title> CRIM's November 94 Continuous Speech Recognition System. </title> <booktitle> In Proceedings of ARPA Speech and Natural Language Workshop, </booktitle> <month> Jan. </month> <year> 1995, </year> <pages> pp 153-155. </pages>
Reference-contexts: Since many words share common pronunciation prefixes, they can also share models and avoid duplication. Trees were initially used in fast match algorithms for producing candidate word lists for further search. Recently, they have been introduced in the main search component of several systems <ref> [44, 39, 43, 3] </ref>. The main problem faced by them is in using a language model. Normally, transitions between words are accompanied by a prior language model probability. <p> There can be a conflict between the best incoming cross-word transition 18 CHAPTER 2. BACKGROUND for different words that share the same root node. This problem has been usually solved by making copies of the lexical tree to resolve such conflicts. Approximate Bigram Trees SRI [39] and CRIM <ref> [43] </ref> augment their lexical tree structure with a flat copy of the lexicon that is activated for bigram transitions. All bigram transitions enter the flat lexicon copy, while the backed off unigram transitions enter the roots of the lexical tree. <p> Moreover, the active HMMs to be evaluated during search are concentrated near the beginning of words. Specifically, over 60-70% of the active HMMs are word-initial models. This is not a new result. It has also been pointed out before, for example in <ref> [39, 43] </ref>, although it has not been quantified as systematically. * The other half of the search cost is attributable to the evaluation of crossword transitions, along with the need to perform several thousands of language model accesses in each frame. <p> Tree-structured lexicons have often been used in the past, especially in fast-match algorithms as a precursor step to a stack-decoding algorithm. More recently, tree search has come into widespread use in the main decoding process <ref> [43, 39] </ref>. Figure 4.1 shows a simple base-phone lexical tree 2 for a small example lexicon. <p> The compact nature of the word lattice, combined with its low error rate, makes it an ideal input for further postprocessing using more detailed acoustic models and search algorithms. The lexical tree described in this section can be contrasted to those described in <ref> [40, 3, 39, 43] </ref> in their treatment of the language model. By deferring the application of language model probabilities to the leaves of the tree, we gain a significant reduction in computation. 68 CHAPTER 4. <p> It analyzes the technique of deferring the application of language model probabilities until the leaves of the lexical tree in an efficient way, and reduces the cost of computing these probabilities by an order of magnitude. Other tree-based searches <ref> [39, 43, 65, 66, 40] </ref> attempt to overcome the problem by creating bigram copies of the search tree. This has three problems: the search space is increased, the cost of language model access is still substantial, and the physical memory size requirements also increase.
Reference: [44] <author> Odell, J.J., Valtchev, V., Woodland, </author> <title> P.C. and Young, S.J. A One Pass Decoder Design for Large Vocabulary Recognition. </title> <booktitle> In Proceedings of ARPA Human Language Technology Workshop, </booktitle> <address> Princeton, </address> <year> 1994. </year>
Reference-contexts: Since many words share common pronunciation prefixes, they can also share models and avoid duplication. Trees were initially used in fast match algorithms for producing candidate word lists for further search. Recently, they have been introduced in the main search component of several systems <ref> [44, 39, 43, 3] </ref>. The main problem faced by them is in using a language model. Normally, transitions between words are accompanied by a prior language model probability. <p> Since the short function words are recognized often (though spuriously), their bigram copies are frequently active. They are also among the larger ones, as noted above. It is unclear how much overhead this adds to the system. 2.4. RELATED WORK 19 Dynamic Network Decoding Cambridge University <ref> [44] </ref> designed a one-pass decoder that uses the lexical tree structure, with copies for cross-word transitions, but instantiates new copies at every transition, as necessary. Basically, the traditional re-entrant lexical structure is replaced with a non-re-entrant structure.
Reference: [45] <author> Pallett, D.S., Fiscus, J.G., Fisher, W.M., Garofolo, J.S., Lund, B.A., and Przybocki, M.A. </author> <title> 1993 Benchmark Tests for the ARPA Spoken Language Program. </title> <booktitle> In Proceedings of ARPA Speech and Natural Language Workshop, </booktitle> <month> Mar. </month> <year> 1994, </year> <pages> pp 15-40. </pages>
Reference-contexts: All tests are carried out using two vocabulary sizes of about 20,000 (20K) and 58,000 (58K) words, respectively. The test sentences are taken from the ARPA evaluations in 1993 and 1994 <ref> [45, 46] </ref>. <p> It is clear that the largest single factor that determines the word error rate is the test set itself. In fact, if the input speech were broken down by individual speakers, a much greater variation would be observed <ref> [45, 46] </ref>. Part of this might be attributable to different out-of-vocabulary (OOV) rates for the sets of sentences uttered by individual speakers. However, a detailed examination of a speaker-by-speaker OOV rate and error rate does not show any strong correlation between the two. <p> SMALL VOCABULARY SYSTEMS speedup of the search algorithm produces an overall speedup of only a small factor. In this chapter we compare the baseline Sphinx-II system and the lexical tree search system on the speech recognition component of the ATIS (Airline Travel Information Service) task 1 <ref> [45, 46] </ref>, which has a vocabulary of about 3K words.
Reference: [46] <author> Pallett, D.S., Fiscus, J.G., Fisher, W.M., Garofolo, J.S., Lund, B.A., Martin, A. and Przybocki, M.A. </author> <title> 1994 Benchmark Tests for the ARPA Spoken Language Program. </title> <booktitle> In Proceedings of ARPA Spoken Language Systems Technology Workshop, </booktitle> <month> Jan. </month> <year> 1995, </year> <pages> pp 5-38. </pages>
Reference-contexts: All tests are carried out using two vocabulary sizes of about 20,000 (20K) and 58,000 (58K) words, respectively. The test sentences are taken from the ARPA evaluations in 1993 and 1994 <ref> [45, 46] </ref>. <p> It is clear that the largest single factor that determines the word error rate is the test set itself. In fact, if the input speech were broken down by individual speakers, a much greater variation would be observed <ref> [45, 46] </ref>. Part of this might be attributable to different out-of-vocabulary (OOV) rates for the sets of sentences uttered by individual speakers. However, a detailed examination of a speaker-by-speaker OOV rate and error rate does not show any strong correlation between the two. <p> Over the last two years fully continuous acoustic models [66, 5, 18] have become much more widely used in the speech community. They reduce the word error rate of recognition systems by a relative amount of about 20-30% compared to semi-continuous acoustic models 1 <ref> [46, 47] </ref>. The use of fully continuous models does not eliminate the search problem. On the other hand, the cost of computing output probabilities for each state in each frame becomes much more significant that in the semi-continuous system. Hence, improving the speed of search alone is not sufficient. <p> SMALL VOCABULARY SYSTEMS speedup of the search algorithm produces an overall speedup of only a small factor. In this chapter we compare the baseline Sphinx-II system and the lexical tree search system on the speech recognition component of the ATIS (Airline Travel Information Service) task 1 <ref> [45, 46] </ref>, which has a vocabulary of about 3K words.
Reference: [47] <author> Pallett, D.S., Fiscus, J.G., Fisher, W.M., Garofolo, J.S., Martin, A. and Przy-bocki, M.A. </author> <title> 1995 HUB-3 NIST Multiple Microphone Corpus Benchmark Tests. </title> <booktitle> In ARPA Speech Recognition Workshop, </booktitle> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: Over the last two years fully continuous acoustic models [66, 5, 18] have become much more widely used in the speech community. They reduce the word error rate of recognition systems by a relative amount of about 20-30% compared to semi-continuous acoustic models 1 <ref> [46, 47] </ref>. The use of fully continuous models does not eliminate the search problem. On the other hand, the cost of computing output probabilities for each state in each frame becomes much more significant that in the semi-continuous system. Hence, improving the speed of search alone is not sufficient.
Reference: [48] <author> Pallett, D.S., Fiscus, J.G., Garofolo, J.S., and Przybocki, M.A. </author> <title> 1995 Hub-4 "Dry Run" Broadcast Materials Benchmark Tests. </title> <booktitle> In ARPA Speech Recognition Workshop, </booktitle> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: Speech research has been concentrated heavily on acoustic and language modelling issues. Since the late 1980s, the complexity of tasks undertaken by speech researchers has grown from the 1000-word Resource Management (RM ) task [51] to essentially unlimited vocabulary tasks such as transcription of radio news broadcast in 1995 <ref> [48] </ref>. While the word recognition accuracy has remained impressive, considering the increase in task complexity, the resource requirements have grown as well. The RM task ran about an order of magnitude slower than real time on processors of that day. <p> Since the late 1980s, the complexity of tasks undertaken by speech researchers has grown from the 1000-word Resource Management (RM ) task [51] to essentially unlimited vocabulary tasks such as transcription of radio news broadcast in 1995 <ref> [48] </ref>. The RM task ran about an order of magnitude slower than real time on processors of that day. The unlimited vocabulary tasks run about two orders of magnitude slower than real time on modern workstations.
Reference: [49] <author> Patel, S. </author> <title> A Lower-Complexity Viterbi Algorithm. </title> <booktitle> In IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <month> May </month> <year> 1995, </year> <journal> Vol. </journal> <volume> 1, </volume> <pages> pp. 592-595. 130 Bibliography </pages>
Reference-contexts: They do not report the reduction in the number of active HMMs as a result of this pruning. 2.4.4 Lower Complexity Viterbi Algorithm A new approach to the Viterbi algorithm, specifically applicable to speech recognition, is described by Patel in <ref> [49] </ref>. It is aimed at reducing the cost of the large number of cross-word transitions and has an expected complexity of N p N T , instead of N 2 T (Section 2.3.1). The algorithm depends on ordering the exit path probabilities and 2.5.
Reference: [50] <author> Paul, Douglas B. </author> <title> An Efficient A* Stack Decoder Algorithm for Continuous Speech Recognition with a Stochastic Language Model. </title> <booktitle> In Proceedings of DARPA Speech and Natural Language Workshop, </booktitle> <month> Feb. </month> <year> 1992, </year> <pages> pp 405-409. </pages>
Reference-contexts: Speech recognition|searching for the most likely sequence of words given the input speech|gives rise to an exponential search space if all possible sequences of words are considered. The problem has generally been tackled in two ways: Viterbi decoding [62, 52] using beam search [37], or stack decoding <ref> [9, 50] </ref> which is a variant of the A* algorithm [42]. Some hybrid versions that combine Viterbi decoding with the A* algorithm also exist [21]. <p> Stack decoding, a variant of the A* search algorithm 2 [42], is more appropriate for use with such grammars which lead to greater recognition accuracy. This algorithm maintains a stack of several possible partial decodings (i.e, word sequence hypotheses) which are expanded in a best-first manner <ref> [9, 2, 50] </ref>. Since each partial hypothesis is a linear word sequence, any arbitrary language model can be applied to it. Stack decoding also allows the decoder to output several most likely N-best hypotheses rather than just the single best one.
Reference: [51] <author> Price, P., Fisher, W.M., Bernstein, J. </author> <title> and Pallet, D.S. The DARPA 1000-Word Resource Management Database for Continuous Speech Recognition. </title> <booktitle> In IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <year> 1988. </year>
Reference-contexts: Speech research has been concentrated heavily on acoustic and language modelling issues. Since the late 1980s, the complexity of tasks undertaken by speech researchers has grown from the 1000-word Resource Management (RM ) task <ref> [51] </ref> to essentially unlimited vocabulary tasks such as transcription of radio news broadcast in 1995 [48]. While the word recognition accuracy has remained impressive, considering the increase in task complexity, the resource requirements have grown as well. <p> The complexity of speech tasks is constantly growing, outpacing the growth in the power of commonly available workstations. Since the late 1980s, the complexity of tasks undertaken by speech researchers has grown from the 1000-word Resource Management (RM ) task <ref> [51] </ref> to essentially unlimited vocabulary tasks such as transcription of radio news broadcast in 1995 [48]. The RM task ran about an order of magnitude slower than real time on processors of that day. <p> One of the early work in this area was in parallelizing the forward Viterbi pass of the baseline Sphinx-II system on a 5-node shared-memory multiprocessor [14, 13] on the 1000-word Resource Management task <ref> [51] </ref>, which yielded a speedup of about 3.8 [54]. Parallelizing the lexical tree search is a little different, but the potential exists, nevertheless. Chapter 5 Memory Size Reduction The second important computational resource needed by modern speech recognition systems, after CPU power, is main memory size.
Reference: [52] <author> Rabiner, </author> <title> L.R. A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition. In Readings in Speech Recognition, </title> <editor> ed. Waibel, A. and Lee, K. </editor> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1990, </year> <pages> pp. 267-296. </pages>
Reference-contexts: Speech recognition|searching for the most likely sequence of words given the input speech|gives rise to an exponential search space if all possible sequences of words are considered. The problem has generally been tackled in two ways: Viterbi decoding <ref> [62, 52] </ref> using beam search [37], or stack decoding [9, 50] which is a variant of the A* algorithm [42]. Some hybrid versions that combine Viterbi decoding with the A* algorithm also exist [21]. <p> The usage and training of HMMs has been covered widely in the literature. Initially described by Baum in [11], it was first used in speech recognition systems by CMU [10] and IBM [29]. The use of HMMs in speech has been described, for example, by Rabiner <ref> [52] </ref>. Currently, almost all systems use HMMs for modelling triphones and context-independent phones (also referred to as monophones or basephones). These include BBN [41], CMU [35, 27], the Cambridge HTK system [65], IBM [5], and LIMSI [18], among others. <p> In this chapter we focus on obtaining a comprehensive set of performance characteristics for this system. The baseline Sphinx-II recognition system uses semi-continuous or tied-mixture hidden Markov models (HMMs) for the acoustic models <ref> [52, 27, 12] </ref> and word bigram or trigram backoff language models (see Sections 2.1 and 2.2). It is a 3-pass decoder structured as follows: 1. Time synchronous Viterbi beam search [52, 62, 37] in the forward direction. <p> The baseline Sphinx-II recognition system uses semi-continuous or tied-mixture hidden Markov models (HMMs) for the acoustic models [52, 27, 12] and word bigram or trigram backoff language models (see Sections 2.1 and 2.2). It is a 3-pass decoder structured as follows: 1. Time synchronous Viterbi beam search <ref> [52, 62, 37] </ref> in the forward direction. It is a complete search of the full vocabulary, using semi-continuous acoustic models, a bigram or trigram language model, and cross-word triphone modelling during the search. <p> But we first need to know what the correct phone is in a given frame. For that we use the Viterbi alignment <ref> [52] </ref> of the correct sequence of phones to the input speech 10 . Specifically, the experiment consists of the following steps: 1. Obtain the Viterbi alignment for an entire test set.
Reference: [53] <author> Rabiner, </author> <title> L.R. Applications of Voice Processing to Telecommunications. </title> <booktitle> In Proceedings of the IEEE, </booktitle> <volume> Vol. 82, No. 2, </volume> <month> Feb. </month> <year> 1994, </year> <pages> pp. 199-228. </pages>
Reference-contexts: Our ability to communicate with machines and computers, through keyboards, mice and other devices, is an order of magnitude slower and more cumbersome. In order to make this communication more user-friendly, speech input is an essential component. There are broadly three classes of speech recognition applications, as described in <ref> [53] </ref>. In isolated word recognition systems each word is spoken with pauses before and after it, so that end-pointing techniques can be used to identify word boundaries reliably. Second, highly constrained command-and-control applications use small vocabularies, limited to specific phrases, but use connected word or continuous speech.
Reference: [54] <author> Ravishankar, M. </author> <title> Parallel Implementation of Fast Beam Search for Speaker-Independent Continuous Speech Recognition. </title> <note> Technical report submitted to Computer Science and Automation, </note> <institution> Indian Institute of Science, </institution> <address> Bangalore, India, </address> <month> Apr. </month> <year> 1993. </year>
Reference-contexts: It is possible to take good advantage of this facility. One of the early attempts at speeding up the Sphinx-II baseline system exploited the large degree of concurrency within its algorithmic steps <ref> [54] </ref>. In a parallel implementation on the PLUS multiprocessor designed at CMU [13], a speed up of 3.9 was 90 CHAPTER 4. SEARCH SPEED OPTIMIZATION obtained on a 5 node configuration. The parallelization involved static partitioning of data and computation among multiple threads using the Mach C-threads facility [16]. <p> One of the early work in this area was in parallelizing the forward Viterbi pass of the baseline Sphinx-II system on a 5-node shared-memory multiprocessor [14, 13] on the 1000-word Resource Management task [51], which yielded a speedup of about 3.8 <ref> [54] </ref>. Parallelizing the lexical tree search is a little different, but the potential exists, nevertheless. Chapter 5 Memory Size Reduction The second important computational resource needed by modern speech recognition systems, after CPU power, is main memory size.
Reference: [55] <author> Ravishankar, M. et al. </author> <title> The CMU continuous speech recognition system demonstration. </title> <booktitle> ARPA Spoken Language Technology Workshop, </booktitle> <month> Mar. </month> <year> 1994. </year>
Reference-contexts: In order to be practically useful, speech recognition systems have to be efficient in their usage of computational resources as well. There clearly are several real-time recognition systems around in the ARPA speech research community <ref> [23, 60, 55, 24] </ref>. However, the published literature is relatively bare regarding them. Their performance has never been formally evaluated with respect to the research systems or with respect to one another, in the way that the accuracy of research systems has been.
Reference: [56] <author> Renals, S. and Hochberg, M. </author> <title> Efficient Search Using Posterior Phone Probability Estimates. </title> <booktitle> In IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <month> May </month> <year> 1995, </year> <pages> pp 596-599. </pages>
Reference-contexts: If the parameters a and K are tightened to reduce the number of context-dependent states evaluated by 95%, there is a 15% relative loss of accuracy. (The baseline test conditions have not be specified for these experiments.) 2.4.3 Search Pruning Using Posterior Phone Probabilities In <ref> [56] </ref>, Renals and Hochberg describe a method of deactivating certain phones during search to achieve higher recognition speed. The method is incorporated into a fast match pass that produces words and posterior probabilities for their noway stack decoder. <p> We explore the trade-offs presented by these alternatives. A somewhat similar approach to search pruning has also been suggested in <ref> [56, 31] </ref>. In their work, phones are pruned from the search process based on their posterior probabilities estimated using neural network models. It is also different in that the pruning mechanism is embedded in a hybrid Viterbi-stack decoding algorithm. <p> They have reported a reduction in fast-match computation of about 50% with a slightly under 10% increase in error rate. A similar technique using phone posterior probabilities has also been reported in <ref> [56] </ref>. It is also in the phase of the fastmatch step that generates word candidates and posterior probabilities to a stack-decoding algorithm.
Reference: [57] <author> Rosenfeld, R. </author> <title> Adaptive Statistical Language Modeling: A Maximum Entropy Approach. </title> <type> Ph.D. thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <year> 1994. </year>
Reference-contexts: There is also a fair amount of handcrafting involved. * Long distance grammars. Unlike n-gram LMs, these are capable of relating words separated by some distance (i.e., with some intervening words). For example, the trigger-pair mechanism discussed in <ref> [57] </ref> is of this variety. Long distance grammars are primarily used to rescore n-best hypothesis lists from previous decodings. 2.3.
Reference: [58] <author> Rosenfeld, R. and Seymore, K. </author> <type> Personal communication. </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> Mar. </month> <year> 1996. </year>
Reference-contexts: We do not know of any work dealing with the language model in the way described here. Other approaches for reducing the size of language models include various forms of clustering, for example into class-based language models, and eliminating potentially useless bigrams and trigrams from the model <ref> [58] </ref>. These approaches generally suffer from some loss of accuracy. The advantage of simple word bigram and trigram grammars is that they are easy to generate from large volumes of training data.
Reference: [59] <author> Schwartz, R. and Chow, Y.L. </author> <title> The Optimal N-Best Algorithm: An Efficient Procedure for Finding Multiple Sentence Hypotheses. </title> <booktitle> In IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <month> Apr. </month> <year> 1990. </year>
Reference-contexts: A complete hypothesis, or simply hypothesis, accounts for the entire input speech. 6 CHAPTER 1. INTRODUCTION possible word extensions, evaluates the resulting (partial) hypotheses with respect to the input speech and re-inserts them in the sorted stack. Any number of N -best hypotheses <ref> [59] </ref> can be generated in this manner. To avoid an exponential growth in the set of possible word sequences in medium and large vocabulary systems, partial hypotheses are expanded only by a limited set of candidate words at each step. <p> An A* or stack search using the word segmentations and scores produced by the forward and backward Viterbi passes above. It produces an N-best list <ref> [59] </ref> of alternative hypotheses as its output, as described briefly in Section 1.2. There is no acoustic rescoring in this pass. However, any arbitrary language model can be applied in creating the N-best list. In this thesis, we will restrict our discussion to word trigram language models.
Reference: [60] <author> Schwartz, R.M. et al. </author> <title> The BBN continuous speech recognition system demonstration. </title> <booktitle> ARPA Spoken Language Technology Workshop, </booktitle> <month> Mar. </month> <year> 1994. </year> <note> Bibliography 131 </note>
Reference-contexts: In order to be practically useful, speech recognition systems have to be efficient in their usage of computational resources as well. There clearly are several real-time recognition systems around in the ARPA speech research community <ref> [23, 60, 55, 24] </ref>. However, the published literature is relatively bare regarding them. Their performance has never been formally evaluated with respect to the research systems or with respect to one another, in the way that the accuracy of research systems has been.
Reference: [61] <author> Sites, </author> <title> R.L., editor. Alpha Architecture Reference Manual. </title> <publisher> Digital Press, </publisher> <year> 1992. </year>
Reference-contexts: It is important to keep these caveats in mind in interpreting the timing results. Having said that, we note that all experiments were carried out on one particular model of Digital Equipment Corporation's Alpha workstations. The Alpha architecture <ref> [61] </ref> includes a special RP CC instruction that allows an application to time very short 40 CHAPTER 3. THE SPHINX-II BASELINE SYSTEM events of as little as a few hundred machine cycles with negligible overhead. All timing measurements are normalized to an Alpha processor running at 175MHz.
Reference: [62] <author> Viterbi, A.J. </author> <title> Error Bounds for Convolutional Codes and an Asymptotically Optimum Decoding Algorithm. </title> <journal> In IEEE Transactions on Information Theory, </journal> <volume> vol. IT-13, </volume> <month> Apr. </month> <year> 1967, </year> <pages> pp. 260-269. </pages>
Reference-contexts: Speech recognition|searching for the most likely sequence of words given the input speech|gives rise to an exponential search space if all possible sequences of words are considered. The problem has generally been tackled in two ways: Viterbi decoding <ref> [62, 52] </ref> using beam search [37], or stack decoding [9, 50] which is a variant of the A* algorithm [42]. Some hybrid versions that combine Viterbi decoding with the A* algorithm also exist [21]. <p> Since the work reported in this thesis is based on the former, we briefly review its basic principles here. 2.3.1 Viterbi Beam Search Viterbi search <ref> [62] </ref> is essentially a dynamic programming algorithm, consisting of traversing a network of HMM states and maintaining the best possible path score at each state in each frame. <p> The baseline Sphinx-II recognition system uses semi-continuous or tied-mixture hidden Markov models (HMMs) for the acoustic models [52, 27, 12] and word bigram or trigram backoff language models (see Sections 2.1 and 2.2). It is a 3-pass decoder structured as follows: 1. Time synchronous Viterbi beam search <ref> [52, 62, 37] </ref> in the forward direction. It is a complete search of the full vocabulary, using semi-continuous acoustic models, a bigram or trigram language model, and cross-word triphone modelling during the search.
Reference: [63] <author> Weide, R. </author> <type> Personal communication. </type> <institution> School of Computer Science, Carnegie Mellon University. </institution>
Reference: [64] <author> Woodland, P.C., Gales, M.J.F., Pye, D., and Valtchev, V. </author> <title> The HTK Large Vocabulary Recognition System for the 1995 ARPA H3 Task. </title> <booktitle> ARPA Speech Recognition Workshop, </booktitle> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: is as follows: * In Section 4.2 we discuss lexical tree Viterbi search and all its design ramifica 1 The contribution of acoustic modelling in different systems to recognition accuracy is hard to estimate since some systems use not one but several sets of acoustic models, particularly for speaker adaptation <ref> [64] </ref>. The overall accuracy resulting from the use of continuous HMM models plus several cycles of mean and variance adaptation was about 50% better than semi-continuous HMM modelling with little or no adaptation. 4.2. LEXICAL TREE SEARCH 51 tions.
Reference: [65] <author> Woodland, P.C. Leggetter, C.J., Odell, J.J., Valtchev, V. and Young, S.J. </author> <title> The Development of the 1994 HTK Large Vocabulary Speech Recognition System. </title> <booktitle> In Proceedings of ARPA Spoken Language System Technology Workshop, </booktitle> <month> Jan. </month> <year> 1995, </year> <pages> pp 104-109. </pages>
Reference-contexts: In our work, a 10sec long sentence typically produces a word lattice containing about 1000 word instances. Given such compact lattices with low error rates, one can search them using sophisticated models and search algorithms very efficiently and obtain results with a lower word error rate, as described in <ref> [38, 65, 41] </ref>. Most systems use such multipass techniques. However, there has been relatively little work reported in actually creating such lattices efficiently. This is important for the practical applicability of such techniques. <p> The use of HMMs in speech has been described, for example, by Rabiner [52]. Currently, almost all systems use HMMs for modelling triphones and context-independent phones (also referred to as monophones or basephones). These include BBN [41], CMU [35, 27], the Cambridge HTK system <ref> [65] </ref>, IBM [5], and LIMSI [18], among others. We will give a brief description of HMMs as used in speech. First of all, the sampled speech input is usually preprocessed, through various signal-processing steps, into a cepstrum or other feature stream that contains one feature vector every frame. <p> Furthermore, the lattice error rate|the fraction of correct words not found in the lattice around the expected time|is extremely small. It is about 2%, excluding out-of-vocabulary words. This is substantially the same as the lattice error rate of the baseline Sphinx-II system, and similar to the results reported in <ref> [65] </ref>. The compact nature of the word lattice, combined with its low error rate, makes it an ideal input for further postprocessing using more detailed acoustic models and search algorithms. <p> The creation of word graphs and some of their uses as described here, has also been reported in <ref> [65] </ref>. However, they do not report on the performance issues or on the use of the shortest path algorithm to find a global optimum that overcomes the suboptimality of the Viterbi search algorithm. <p> The lattice can be searched using more detailed and sophisticated models and search algorithms efficiently. The use of multi-pass systems is not new. Most current speech recognition systems are of that nature <ref> [41, 65, 5, 15, 19, 38] </ref>. Many reports cite the savings to be had by postprocessing a word lattice [65, 38] instead of the entire vocabulary. However, the task of actually producing such lattices efficiently has been relatively unexplored. <p> The use of multi-pass systems is not new. Most current speech recognition systems are of that nature [41, 65, 5, 15, 19, 38]. Many reports cite the savings to be had by postprocessing a word lattice <ref> [65, 38] </ref> instead of the entire vocabulary. However, the task of actually producing such lattices efficiently has been relatively unexplored. This is necessary for the practical application of accurate, large vocabulary speech recognition and it is addressed in this thesis. <p> It analyzes the technique of deferring the application of language model probabilities until the leaves of the lexical tree in an efficient way, and reduces the cost of computing these probabilities by an order of magnitude. Other tree-based searches <ref> [39, 43, 65, 66, 40] </ref> attempt to overcome the problem by creating bigram copies of the search tree. This has three problems: the search space is increased, the cost of language model access is still substantial, and the physical memory size requirements also increase. <p> Furthermore, it operates in a small fraction of real time and its cost is negligible. Word lattice searches have been proposed in <ref> [65, 39] </ref>, for example, but they are directed more towards using the lattice for driving later search passes with more detailed models. * Use of HMM state output probabilities as a fast match to produce candidate 7.3.
Reference: [66] <author> Woodland, P.C., Odell, J.J., Valtchev, V. and Young, S.J. </author> <title> The HTK Large Vocabulary Continuous Speech Recognition System: An Overview. </title> <booktitle> In Proceedings of ARPA Speech and Natural Language Workshop, </booktitle> <month> Mar. </month> <year> 1994, </year> <pages> pp 98-101. 132 Bibliography </pages>
Reference-contexts: At the time that this work was begun, the Sphinx-II semi-continuous acoustic models were the best available to us. Over the last two years fully continuous acoustic models <ref> [66, 5, 18] </ref> have become much more widely used in the speech community. They reduce the word error rate of recognition systems by a relative amount of about 20-30% compared to semi-continuous acoustic models 1 [46, 47]. The use of fully continuous models does not eliminate the search problem. <p> It analyzes the technique of deferring the application of language model probabilities until the leaves of the lexical tree in an efficient way, and reduces the cost of computing these probabilities by an order of magnitude. Other tree-based searches <ref> [39, 43, 65, 66, 40] </ref> attempt to overcome the problem by creating bigram copies of the search tree. This has three problems: the search space is increased, the cost of language model access is still substantial, and the physical memory size requirements also increase.
References-found: 66

