URL: ftp://ftp.cs.bris.ac.uk/pub/users/cgc/aa1-proof-95.ps.Z
Refering-URL: http://www.cs.bris.ac.uk/~cgc/papers.ml.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Analysis of the Convergence and Generalization of AA1  
Author: Christophe Giraud-Carrier Tony Martinez 
Affiliation: Department of Computer Science Brigham Young University  
Date: May 1993  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Giraud-Carrier, C. </author> <title> A Precept-Driven Learning Algorithm. </title> <type> Master's Thesis, </type> <institution> Brigham Young University, Department of Computer Science, </institution> <year> 1993. </year>
Reference-contexts: Also, it is not necessarily best to try to maintain the instance set consistent (nature is replete with inconsistencies). Current research seeks to expand the learning scheme by removing AA1's rigid precedence given to newer rules and by allowing inconsistencies to subsist (see for example <ref> [1] </ref>). Complexity The hierarchical architecture of AA1 guarantees that execution is O (logn) where n is the number of nodes in the network. During learning, note that only the training instance requires full execution of the learning algorithm.
Reference: [2] <author> Martinez, T.R. </author> <title> Adaptive Self-Organizing Networks. </title> <type> Ph.D. Thesis, UCLA Tech. Rep. </type> - <address> CSD 860093, </address> <year> 1986. </year>
Reference-contexts: Section 5 presents results of simulations of AA1 on real-world data. Section 6 summarizes the paper. 2. AA1 Learning Algorithm This section gives a short overview of AA1. Only those details essential to the proof of the following section are presented. For more details on AA1, see <ref> [2, 3] </ref>. An instance set is a set of instances, where each instance is a conjunction of Boolean input values together with the output value they imply. We use V and V' to mean V is positive (i.e., true) and V is negative (i.e., false), respectively. <p> The network finally undergoes a phase of self-deletion in which nodes that are no longer needed (such as non-discriminant nodes) are removed from the network. Self-deletion increases parsimony. Details on the various kinds of self-deletions may be found in <ref> [2, 3] </ref>. The construction of OSDN is effected by a process of node selection and node combination. Node selection consists of a greedy search through the network for a set of nodes that, when combined, give rise to OSDN. <p> Since PN NTN, it does not matter whether PN outputs 0 or 1 for NI. Hence, there are eight possible functions for PN, summarized in <ref> [2, Table 6.2] </ref>. We give the complete proof for the case where both G 1 2 output 1 for NI. Then, the function of PN is the AND of the outputs of G 1 2 . <p> Theorem 2: NTN is a complete discriminant node such that AA1 fulfills the new instance set. Proof: There are only four ways, depending on the polarity of NI and the output of OSDN for NI, that OSDN and CTN may combine to produce NTN, as summarized in <ref> [2, Table 6.3] </ref>. We give the complete proof for the case where OSDN outputs a 1 for NI and NI is positive. Then NTNs function is the OR of the outputs of CTN and OSDN.
Reference: [3] <author> Martinez, T.R., and Vidal, J.J. </author> <title> Adaptive Parallel Logic Networks. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5, </volume> <month> 1 (Feb. </month> <year> 1988), </year> <pages> 26-58. </pages>
Reference-contexts: As each instance is presented, the network adapts both its topology and its nodes functions to learn the new instance, while preserving consistency with previously acquired knowledge. Several learning algorithms have been developed for ASOCS. They include Adaptive Algorithm (AA) 1 <ref> [3] </ref>, AA2 [4] and AA3 [5]. AA1 is the object of this paper. AA1 learns by discrimination and implements knowledge in a distributed fashion. This paper analyzes the convergence and generalization properties of AA1. Convergence refers to the system's ability to learn the instance set. <p> Section 5 presents results of simulations of AA1 on real-world data. Section 6 summarizes the paper. 2. AA1 Learning Algorithm This section gives a short overview of AA1. Only those details essential to the proof of the following section are presented. For more details on AA1, see <ref> [2, 3] </ref>. An instance set is a set of instances, where each instance is a conjunction of Boolean input values together with the output value they imply. We use V and V' to mean V is positive (i.e., true) and V is negative (i.e., false), respectively. <p> The network finally undergoes a phase of self-deletion in which nodes that are no longer needed (such as non-discriminant nodes) are removed from the network. Self-deletion increases parsimony. Details on the various kinds of self-deletions may be found in <ref> [2, 3] </ref>. The construction of OSDN is effected by a process of node selection and node combination. Node selection consists of a greedy search through the network for a set of nodes that, when combined, give rise to OSDN. <p> Due to space, we can only give a high-level example of how AA1 updates the network upon receipt of a new instance. A detailed example is found in <ref> [3] </ref>. Consider the network of Figure 2. Assume the new instance is positive and causes the top node to output ?, node n3 to output 1 and node n2 to output ?. The result of processing the new instance is in Figure 3.
Reference: [4] <author> Martinez, T.R., and Campbell, </author> <title> D.M. A Self-Adjusting Dynamic Logic Module. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 11, 4 (1991), </volume> <pages> 303-313. </pages>
Reference-contexts: As each instance is presented, the network adapts both its topology and its nodes functions to learn the new instance, while preserving consistency with previously acquired knowledge. Several learning algorithms have been developed for ASOCS. They include Adaptive Algorithm (AA) 1 [3], AA2 <ref> [4] </ref> and AA3 [5]. AA1 is the object of this paper. AA1 learns by discrimination and implements knowledge in a distributed fashion. This paper analyzes the convergence and generalization properties of AA1. Convergence refers to the system's ability to learn the instance set.
Reference: [5] <author> Martinez, T.R., and Campbell, </author> <title> D.M. A Self-Organizing Binary Decision Tree for Incrementally Defined Rule Based Systems. </title> <journal> In IEEE Transactions on System, Man, and Cybernetics, </journal> <volume> 21, 5 (1991), </volume> <pages> 1231-1238. </pages>
Reference-contexts: As each instance is presented, the network adapts both its topology and its nodes functions to learn the new instance, while preserving consistency with previously acquired knowledge. Several learning algorithms have been developed for ASOCS. They include Adaptive Algorithm (AA) 1 [3], AA2 [4] and AA3 <ref> [5] </ref>. AA1 is the object of this paper. AA1 learns by discrimination and implements knowledge in a distributed fashion. This paper analyzes the convergence and generalization properties of AA1. Convergence refers to the system's ability to learn the instance set.
Reference: [6] <author> Martinez, T.R., Hughes, B., and Campbell, </author> <title> D.M. Priority ASOCS. </title> <note> To appear in Journal of Artificial Neural Networks, </note> <year> 1994. </year>
Reference-contexts: For example, instances AB fi Z and BC fi Z' produce an inconsistency since for A, B and C positive, the first one implies that Z is positive while the second one implies that Z is negative. Inconsistency is solved by giving precedence to the newer instances (see <ref> [6] </ref> for details). Complete minimization is not reasonable, but AA1 attempts partial minimization through pairwise comparison of the training instance and instances in the current instance set. The correctness of this aspect of the algorithm has been proved elsewhere [10]. The correctness of the actual learning algorithm has not.
Reference: [7] <author> Minsky, M., and Papert, S. </author> <title> Perceptrons. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA (1967) </address> . 
Reference-contexts: Convergence refers to the system's ability to learn the instance set. Convergence has played a significant historical role in the development of neural networks. Rosenblatt's perceptron [9], the first neural network, was quickly proven to be limited to linearly separable functions <ref> [7] </ref>. However, it was also shown that a (at least) three-layer network could be used to represent any arbitrary function. The challenge was thus to design a learning algorithm for multilayer networks. Researchers struggled until the first multilayer learning algorithm, backpropagation [11, 12], was proposed.
Reference: [8] <author> Murphy, P.M., and Aha, D.W. </author> <title> UCI Repository of machine learning databases. </title> <address> Irvine, CA: </address> <institution> University of California, Department of Information and Computer Science, </institution> <year> 1992. </year>
Reference-contexts: A global D column would be sufficient to meet the goal of Node Selection. However, it is still unclear how to do away with (or effectively replace) the P and N columns. 5. Simulation Results AA1 was tested on several real-world applications, drawn from the Irvine machine learning database <ref> [8] </ref>. No minimization was attempted, and self-deletion was limited to complete discriminant deletion. The results are summarized in Table 1. For Congressional Voting Data and Hepatitis, the results are averages (on the test set) over 10 runs of AA1.
Reference: [9] <author> Rosenblatt, E. </author> <title> The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain. </title> <journal> Psychological Review, </journal> <volume> 65, </volume> <year> (1958), </year> <pages> 386-408. </pages>
Reference-contexts: AA1 learns by discrimination and implements knowledge in a distributed fashion. This paper analyzes the convergence and generalization properties of AA1. Convergence refers to the system's ability to learn the instance set. Convergence has played a significant historical role in the development of neural networks. Rosenblatt's perceptron <ref> [9] </ref>, the first neural network, was quickly proven to be limited to linearly separable functions [7]. However, it was also shown that a (at least) three-layer network could be used to represent any arbitrary function. The challenge was thus to design a learning algorithm for multilayer networks.
Reference: [10] <author> Rudolph, G. </author> <title> A Location-Independent ASOCS Model. </title> <type> Master's Thesis, </type> <institution> Brigham Young University, Department of Computer Science, </institution> <year> 1991. </year>
Reference-contexts: Complete minimization is not reasonable, but AA1 attempts partial minimization through pairwise comparison of the training instance and instances in the current instance set. The correctness of this aspect of the algorithm has been proved elsewhere <ref> [10] </ref>. The correctness of the actual learning algorithm has not. Section 3 fills this gap. The result of the above preprocessing phase is a delete-list and an add-list containing the instances to be removed and added, respectively, from the current instance set to keep it consistent and somewhat minimal.
Reference: [11] <author> Rumelhart, D. and McClelland, J. </author> <title> Parallel Distributed Processing. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA (1986), </address> <note> Vol. 1. </note>
Reference-contexts: Some of the reasons for this interest can be traced to the discovery of the wide range of neural network applications, the increasing interest in self-organizing systems, and the design of improved learning algorithms. A good overview of the current state of the art is found in <ref> [11] </ref>. Most of the current models are based on static networks of computing nodes, where learning is effected by changing the weights of the connections between nodes and/or by modifying the nodes functions. <p> However, it was also shown that a (at least) three-layer network could be used to represent any arbitrary function. The challenge was thus to design a learning algorithm for multilayer networks. Researchers struggled until the first multilayer learning algorithm, backpropagation <ref> [11, 12] </ref>, was proposed. However, because it uses a gradient-descent technique and is thus subject to local minima, there is no guarantee, in practice, that backpropagation will find a plausible solution to a training set.
Reference: [12] <author> Werbos, P.J. </author> <title> Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences. </title> <type> Ph.D. Thesis, </type> <institution> Harvard University, </institution> <address> Cambridge, MA, </address> <year> 1974. </year> <note> AND 1 0 P N D 1 0 1 AND OR 1 0 ? 0 0 AND 0 n1 1 0 1 OR 1 1 1 ? 0 0 AND 0 ? 0 AND 0 1 1 0 1 1 ? 1 n1 n4 0 Table 1 - AA1 Simulation Results Congressional Voting Data 92.4% Monk1 98.6% Monk2 82.6% Monk3 (with noise) 91.2% Monk3 (without noise) 99.1% Hepatitis 70.3% </note>
Reference-contexts: However, it was also shown that a (at least) three-layer network could be used to represent any arbitrary function. The challenge was thus to design a learning algorithm for multilayer networks. Researchers struggled until the first multilayer learning algorithm, backpropagation <ref> [11, 12] </ref>, was proposed. However, because it uses a gradient-descent technique and is thus subject to local minima, there is no guarantee, in practice, that backpropagation will find a plausible solution to a training set.
References-found: 12

