URL: ftp://ftp.cnl.salk.edu/pub/wiskott/publications/WisSej97b-NeuralMapFormation-JSNC.ps.gz
Refering-URL: http://www.cnl.salk.edu/~wiskott/Abstracts/WisSej97b.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: fwiskott,terryg@salk.edu,  
Phone: 3  
Title: Objective Functions for Neural Map Formation  
Author: Laurenz Wiskott and Terrence Sejnowski 
Web: http://www.cnl.salk.edu/CNL  
Address: San Diego, CA 92186-5800  La Jolla, CA 92093  
Affiliation: 1 Computational Neurobiology Laboratory 2 Howard Hughes Medical Institute The Salk Institute for Biological Studies,  Department of Biology University of California, San Diego  
Date: 242-248 (1997)  
Note: Proc. of the 4th Joint Symposium on Neural Computation, Los Angeles, CA, May, pp.  
Abstract: A unifying framework for analyzing models of neural map formation is presented based on growth rules derived from objective functions and normalization rules derived from constraint functions. Coordinate transformations play an important role in deriving various rules from the same function. Ten different models from the literature are classified within the objective function framework presented here. Though models may look different, they may actually be equivalent in terms of their stable solutions. The techniques used in this analysis may also be useful in investigating other types of neural dynamics.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Amari, S. </author> <title> Topographic organization of nerve fields. </title> <journal> Bulletin of Mathematical Biology 42 (1980), </journal> <pages> 339-364. </pages>
Reference-contexts: The models [2, 5, 6, 7, 10] can be directly classified under one coordinate transformation. The models [3, 8, 11, 12] can probably 5 be made consistent with minor modifications. The applicability of our objective function framework to model [13] is unclear. Another model <ref> [1] </ref> is not listed because it can clearly not be described within our objective function framework.
Reference: [2] <author> Bienenstock, E., and von der Malsburg, C. </author> <title> A neural network for invariant pattern recognition. </title> <journal> Europhysics Letters 4, </journal> <volume> 1 (1987), </volume> <pages> 121-126. </pages>
Reference-contexts: For the orthogonal normalization rule the dynamics increases the value of the objective function, while for the non-orthogonal normalization the value decreases and the objective function that generates the growth rule is not even a Lyapunov function for the combined system. 4 Reference Classification Bienenstock & von der Malsburg <ref> [2] </ref> Q 1 I 1 N 1 Goodhill [3] Q 1 I 1 N 1 = Haussler & von der Malsburg [5] L w Q w I w &gt; N w Konen & von der Malsburg [6] Q ffw N ffw = Linsker [7] L 1 Q 1 I 1 Miller <p> Since the dynamics needs to be curl-free and the normalization rules orthogonal in the same coordinate system, only entries in the same column may be combined to obtain a consistent objective function framework for a system. Classifications of ten different models are shown in Table 2. The models <ref> [2, 5, 6, 7, 10] </ref> can be directly classified under one coordinate transformation. The models [3, 8, 11, 12] can probably 5 be made consistent with minor modifications. The applicability of our objective function framework to model [13] is unclear. <p> However, the concept of equivalent models shows that normalization rules have to be judged in combination with growth rules, e.g. N w + I w + Q w (multiplicative normalization) is equivalent to N 1 + I 1 + Q 1 (subtractive normalization). * Models of dynamic link matching <ref> [2, 6] </ref> introduced similarity values rather implicitly. A more direct formulation of dynamic link matching can be derived from the objective function L + Q. * Objective functions provide a link between neural dynamics and algorithmic systems.
Reference: [3] <author> Goodhill, G. J. </author> <title> Topography and ocular dominance: A model exploring positive correlations. </title> <journal> Biol. Cybern. </journal> <volume> 69 (1993), </volume> <pages> 109-118. </pages>
Reference-contexts: increases the value of the objective function, while for the non-orthogonal normalization the value decreases and the objective function that generates the growth rule is not even a Lyapunov function for the combined system. 4 Reference Classification Bienenstock & von der Malsburg [2] Q 1 I 1 N 1 Goodhill <ref> [3] </ref> Q 1 I 1 N 1 = Haussler & von der Malsburg [5] L w Q w I w &gt; N w Konen & von der Malsburg [6] Q ffw N ffw = Linsker [7] L 1 Q 1 I 1 Miller et al. [8] Q ff I ff I <p> Classifications of ten different models are shown in Table 2. The models [2, 5, 6, 7, 10] can be directly classified under one coordinate transformation. The models <ref> [3, 8, 11, 12] </ref> can probably 5 be made consistent with minor modifications. The applicability of our objective function framework to model [13] is unclear. Another model [1] is not listed because it can clearly not be described within our objective function framework.
Reference: [4] <author> Goodhill, G. J., Finch, S., and Sejnowski, T. J. </author> <title> Optimizing cortical mappings. </title> <booktitle> In Advances in Neural Information Processing Systems (Cambridge, </booktitle> <address> MA, </address> <year> 1996), </year> <editor> D. Touretzky, M. Mozer, and M. Hasselmo, Eds., </editor> <volume> vol. 8, </volume> <publisher> MIT Press, </publisher> <pages> pp. 330-336. </pages>
Reference-contexts: A more direct formulation of dynamic link matching can be derived from the objective function L + Q. * Objective functions provide a link between neural dynamics and algorithmic systems. For instance, the C-measure proposed in <ref> [4] </ref> as a unifying objective function for many different map formation algorithms is a one-to-one mapping version of the quadratic term Q. The objective function framework provides a basis on which many models of neural map formation can be analyzed and understood in a unified fashion.
Reference: [5] <author> H aussler, A. F., and von der Malsburg, C. </author> <title> Development of retinotopic projections | An analytical treatment. </title> <journal> J. Theor. Neurobiol. </journal> <volume> 2 (1983), </volume> <pages> 47-73. </pages>
Reference-contexts: This correlation model is accurate for linear models <ref> [e.g. 2, 5, 7, 8] </ref> and is an approximation for non-linear models [e.g. 3, 6, 10, 11, 12, 13]. 3 Objective Functions With Equation (2) a linear Hebbian growth rule can be written as _w i = P j D ij w j . <p> value decreases and the objective function that generates the growth rule is not even a Lyapunov function for the combined system. 4 Reference Classification Bienenstock & von der Malsburg [2] Q 1 I 1 N 1 Goodhill [3] Q 1 I 1 N 1 = Haussler & von der Malsburg <ref> [5] </ref> L w Q w I w &gt; N w Konen & von der Malsburg [6] Q ffw N ffw = Linsker [7] L 1 Q 1 I 1 Miller et al. [8] Q ff I ff I ff = (N w Obermayer et al. [10] Q 1 Z 1 = <p> Since the dynamics needs to be curl-free and the normalization rules orthogonal in the same coordinate system, only entries in the same column may be combined to obtain a consistent objective function framework for a system. Classifications of ten different models are shown in Table 2. The models <ref> [2, 5, 6, 7, 10] </ref> can be directly classified under one coordinate transformation. The models [3, 8, 11, 12] can probably 5 be made consistent with minor modifications. The applicability of our objective function framework to model [13] is unclear.
Reference: [6] <author> Konen, W., and von der Malsburg, C. </author> <title> Learning to generalize from single examples in the dynamic link architecture. </title> <booktitle> Neural Computation 5, 5 (1993), </booktitle> <pages> 719-735. </pages>
Reference-contexts: This correlation model is accurate for linear models [e.g. 2, 5, 7, 8] and is an approximation for non-linear models <ref> [e.g. 3, 6, 10, 11, 12, 13] </ref>. 3 Objective Functions With Equation (2) a linear Hebbian growth rule can be written as _w i = P j D ij w j . <p> Lyapunov function for the combined system. 4 Reference Classification Bienenstock & von der Malsburg [2] Q 1 I 1 N 1 Goodhill [3] Q 1 I 1 N 1 = Haussler & von der Malsburg [5] L w Q w I w &gt; N w Konen & von der Malsburg <ref> [6] </ref> Q ffw N ffw = Linsker [7] L 1 Q 1 I 1 Miller et al. [8] Q ff I ff I ff = (N w Obermayer et al. [10] Q 1 Z 1 = Tanaka [11] Q w I w &gt; N ffw ) von der Malsburg [12] Q <p> Since the dynamics needs to be curl-free and the normalization rules orthogonal in the same coordinate system, only entries in the same column may be combined to obtain a consistent objective function framework for a system. Classifications of ten different models are shown in Table 2. The models <ref> [2, 5, 6, 7, 10] </ref> can be directly classified under one coordinate transformation. The models [3, 8, 11, 12] can probably 5 be made consistent with minor modifications. The applicability of our objective function framework to model [13] is unclear. <p> The limitation constraint I can be waived for systems with positive weights and multiplicative normalization rules <ref> [6, 10, 12] </ref>. Since the model in [7] uses negative and positive weights and weights have a lower and an upper bound, no normalization rule is necessary. <p> However, the concept of equivalent models shows that normalization rules have to be judged in combination with growth rules, e.g. N w + I w + Q w (multiplicative normalization) is equivalent to N 1 + I 1 + Q 1 (subtractive normalization). * Models of dynamic link matching <ref> [2, 6] </ref> introduced similarity values rather implicitly. A more direct formulation of dynamic link matching can be derived from the objective function L + Q. * Objective functions provide a link between neural dynamics and algorithmic systems.
Reference: [7] <author> Linsker, R. </author> <title> From basic network principles to neural architecture: Emergence of orientation columns. </title> <institution> Ntl. Acad. Sci. </institution> <address> USA 83 (1986), </address> <pages> 8779-8783. </pages>
Reference-contexts: This correlation model is accurate for linear models <ref> [e.g. 2, 5, 7, 8] </ref> and is an approximation for non-linear models [e.g. 3, 6, 10, 11, 12, 13]. 3 Objective Functions With Equation (2) a linear Hebbian growth rule can be written as _w i = P j D ij w j . <p> Reference Classification Bienenstock & von der Malsburg [2] Q 1 I 1 N 1 Goodhill [3] Q 1 I 1 N 1 = Haussler & von der Malsburg [5] L w Q w I w &gt; N w Konen & von der Malsburg [6] Q ffw N ffw = Linsker <ref> [7] </ref> L 1 Q 1 I 1 Miller et al. [8] Q ff I ff I ff = (N w Obermayer et al. [10] Q 1 Z 1 = Tanaka [11] Q w I w &gt; N ffw ) von der Malsburg [12] Q 1 N w = Whitelaw & Cowan <p> Since the dynamics needs to be curl-free and the normalization rules orthogonal in the same coordinate system, only entries in the same column may be combined to obtain a consistent objective function framework for a system. Classifications of ten different models are shown in Table 2. The models <ref> [2, 5, 6, 7, 10] </ref> can be directly classified under one coordinate transformation. The models [3, 8, 11, 12] can probably 5 be made consistent with minor modifications. The applicability of our objective function framework to model [13] is unclear. <p> The limitation constraint I can be waived for systems with positive weights and multiplicative normalization rules [6, 10, 12]. Since the model in <ref> [7] </ref> uses negative and positive weights and weights have a lower and an upper bound, no normalization rule is necessary. The weights converge to their upper or lower limit. 6 Discussion A unifying framework for analyzing models of neural map formation has been presented.
Reference: [8] <author> Miller, K. D., Keller, J. B., and Stryker, M. P. </author> <title> Ocular dominance column development: Analysis and simulation. </title> <booktitle> Science 245 (1989), </booktitle> <pages> 605-245. </pages>
Reference-contexts: This correlation model is accurate for linear models <ref> [e.g. 2, 5, 7, 8] </ref> and is an approximation for non-linear models [e.g. 3, 6, 10, 11, 12, 13]. 3 Objective Functions With Equation (2) a linear Hebbian growth rule can be written as _w i = P j D ij w j . <p> I 1 N 1 Goodhill [3] Q 1 I 1 N 1 = Haussler & von der Malsburg [5] L w Q w I w &gt; N w Konen & von der Malsburg [6] Q ffw N ffw = Linsker [7] L 1 Q 1 I 1 Miller et al. <ref> [8] </ref> Q ff I ff I ff = (N w Obermayer et al. [10] Q 1 Z 1 = Tanaka [11] Q w I w &gt; N ffw ) von der Malsburg [12] Q 1 N w = Whitelaw & Cowan [13] Q 1 Q ff N ? = Table 2: <p> Classifications of ten different models are shown in Table 2. The models [2, 5, 6, 7, 10] can be directly classified under one coordinate transformation. The models <ref> [3, 8, 11, 12] </ref> can probably 5 be made consistent with minor modifications. The applicability of our objective function framework to model [13] is unclear. Another model [1] is not listed because it can clearly not be described within our objective function framework.
Reference: [9] <author> Miller, K. D., and MacKay, D. J. C. </author> <title> The role of constraints in Hebbian learning. </title> <booktitle> Neural Computation 6 (1994), </booktitle> <pages> 100-126. </pages>
Reference-contexts: This is because stable fixed points are preserved under coordinate transformations with finite derivatives. * In <ref> [9] </ref> a clear distinction between multiplicative and subtractive normalization was made. However, the concept of equivalent models shows that normalization rules have to be judged in combination with growth rules, e.g.
Reference: [10] <author> Obermayer, K., Ritter, H., and Schulten, K. </author> <title> Large-scale simulations of self-organizing neural networks on parallel computers: Application to biological modelling. </title> <booktitle> Parallel Computing 14 (1990), </booktitle> <pages> 381-404. </pages>
Reference-contexts: This correlation model is accurate for linear models [e.g. 2, 5, 7, 8] and is an approximation for non-linear models <ref> [e.g. 3, 6, 10, 11, 12, 13] </ref>. 3 Objective Functions With Equation (2) a linear Hebbian growth rule can be written as _w i = P j D ij w j . <p> Haussler & von der Malsburg [5] L w Q w I w &gt; N w Konen & von der Malsburg [6] Q ffw N ffw = Linsker [7] L 1 Q 1 I 1 Miller et al. [8] Q ff I ff I ff = (N w Obermayer et al. <ref> [10] </ref> Q 1 Z 1 = Tanaka [11] Q w I w &gt; N ffw ) von der Malsburg [12] Q 1 N w = Whitelaw & Cowan [13] Q 1 Q ff N ? = Table 2: Classification of weight dynamics in previous models. derived by other methods, e.g. penalty <p> Since the dynamics needs to be curl-free and the normalization rules orthogonal in the same coordinate system, only entries in the same column may be combined to obtain a consistent objective function framework for a system. Classifications of ten different models are shown in Table 2. The models <ref> [2, 5, 6, 7, 10] </ref> can be directly classified under one coordinate transformation. The models [3, 8, 11, 12] can probably 5 be made consistent with minor modifications. The applicability of our objective function framework to model [13] is unclear. <p> The limitation constraint I can be waived for systems with positive weights and multiplicative normalization rules <ref> [6, 10, 12] </ref>. Since the model in [7] uses negative and positive weights and weights have a lower and an upper bound, no normalization rule is necessary.
Reference: [11] <author> Tanaka, S. </author> <title> Theory of self-organization of cortical maps: Mathematical framework. Neural Networks 3 (1990), 625-640. [12] von der Malsburg, C. Self-organization of orientation sensitive cells in the striate cortex. </title> <booktitle> Kybernetik 14 (1973), </booktitle> <pages> 85-100. </pages>
Reference-contexts: This correlation model is accurate for linear models [e.g. 2, 5, 7, 8] and is an approximation for non-linear models <ref> [e.g. 3, 6, 10, 11, 12, 13] </ref>. 3 Objective Functions With Equation (2) a linear Hebbian growth rule can be written as _w i = P j D ij w j . <p> w Q w I w &gt; N w Konen & von der Malsburg [6] Q ffw N ffw = Linsker [7] L 1 Q 1 I 1 Miller et al. [8] Q ff I ff I ff = (N w Obermayer et al. [10] Q 1 Z 1 = Tanaka <ref> [11] </ref> Q w I w &gt; N ffw ) von der Malsburg [12] Q 1 N w = Whitelaw & Cowan [13] Q 1 Q ff N ? = Table 2: Classification of weight dynamics in previous models. derived by other methods, e.g. penalty functions, indicated by subscripts and &gt;, or <p> Classifications of ten different models are shown in Table 2. The models [2, 5, 6, 7, 10] can be directly classified under one coordinate transformation. The models <ref> [3, 8, 11, 12] </ref> can probably 5 be made consistent with minor modifications. The applicability of our objective function framework to model [13] is unclear. Another model [1] is not listed because it can clearly not be described within our objective function framework.
Reference: [13] <author> Whitelaw, D. J., and Cowan, J. D. </author> <title> Specificity and plasticity of retinotectal connections: A computational model. </title> <journal> J. Neuroscience 1, </journal> <volume> 12 (1981), </volume> <pages> 1369-1387. </pages>
Reference-contexts: This correlation model is accurate for linear models [e.g. 2, 5, 7, 8] and is an approximation for non-linear models <ref> [e.g. 3, 6, 10, 11, 12, 13] </ref>. 3 Objective Functions With Equation (2) a linear Hebbian growth rule can be written as _w i = P j D ij w j . <p> L 1 Q 1 I 1 Miller et al. [8] Q ff I ff I ff = (N w Obermayer et al. [10] Q 1 Z 1 = Tanaka [11] Q w I w &gt; N ffw ) von der Malsburg [12] Q 1 N w = Whitelaw & Cowan <ref> [13] </ref> Q 1 Q ff N ? = Table 2: Classification of weight dynamics in previous models. derived by other methods, e.g. penalty functions, indicated by subscripts and &gt;, or integrated normalization, indicated by subscript '. <p> Classifications of ten different models are shown in Table 2. The models [2, 5, 6, 7, 10] can be directly classified under one coordinate transformation. The models [3, 8, 11, 12] can probably 5 be made consistent with minor modifications. The applicability of our objective function framework to model <ref> [13] </ref> is unclear. Another model [1] is not listed because it can clearly not be described within our objective function framework.
Reference: [14] <author> Wiskott, L., and Sejnowski, T. </author> <title> Objective functions for neural map formation. </title> <type> Tech. Rep. </type> <institution> INC-9701, Institute for Neural Computation, University of California, </institution> <address> San Diego, La Jolla, CA 92093, </address> <month> Jan. </month> <year> 1997. </year> <pages> 27 pages. 7 </pages>
Reference-contexts: The goal here is to provide a unifying objective function framework for a wide variety of models and to provide means by which analysis becomes easier. A more detailed description of this work is given in <ref> [14] </ref>. 2 Correlations The architecture considered here consists of an input layer all-to-all connected to an output layer without feed-back connections. Input neurons are indicated by (retina), and output neurons by t (tectum).
References-found: 13

