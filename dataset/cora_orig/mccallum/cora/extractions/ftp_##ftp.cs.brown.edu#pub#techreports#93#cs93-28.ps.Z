URL: ftp://ftp.cs.brown.edu/pub/techreports/93/cs93-28.ps.Z
Refering-URL: http://www.cs.brown.edu/publications/techreports/reports/CS-93-28.html
Root-URL: http://www.cs.brown.edu/
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> R. B. Arps, T. K. Truong, D. J. Lu, R. C. Pasco, and T. D. Friedman, </author> <title> "A MultiPurpose VLSI Chip for Adaptive Data Compression of Bilevel Images," </title> <journal> IBM J. Res. Develop. </journal> <volume> 32 (Nov. </volume> <year> 1988), </year> <pages> 775-795. </pages>
Reference: [2] <author> T. Bell, </author> <title> "A Unifying Theory and Improvements for Existing Approaches to Text Compression," </title> <institution> Univ. of Canterbury, </institution> <type> Ph.D. Thesis, </type> <year> 1986. </year>
Reference: [3] <author> T. Bell and A. M. Moffat, </author> <title> "A Note on the DMC Data Compression Scheme," </title> <journal> Computer Journal 32 (1989), </journal> <pages> 16-20. </pages>
Reference: [4] <author> T. C. Bell, J. G. Cleary, and I. H. Witten, </author> <title> Text Compression, </title> <publisher> Prentice-Hall, </publisher> <address> En-glewood Cliffs, NJ, </address> <year> 1990. </year>
Reference-contexts: Non-adaptive models are not very interesting, since their effectiveness depends only on how well their probabilities happen to match 1.3. THESIS 3 the statistics of the file being encoded; Bell, Cleary, and Witten show that the match can be arbitrarily bad <ref> [4] </ref>. In Section 4.1.2 we prove that one simple adaptive model gives exactly the same code length as a corresponding semi-adaptive decrementing model. For both image and text compression, it is possible to use simple models, based solely on global event counts. <p> M = log 2 (number of possible distributions) = log 2 t + n 1 ! ~ n log 2 (et=n): Example: For our short sample file, t = 8 and n = 3, so L M = log 2 2 = log 2 45 A similar result appears in <ref> [4] </ref> and [8]. For a typical file L M is only about 2560 bits, or 320 bytes. <p> We can encode a symbol using arithmetic coding only if its probability is nonzero, but in an adaptive code we have no way of estimating the probability of a symbol before it has occurred for the first time. This is the zero-frequency problem, discussed in detail in <ref> [4] </ref> and [76]. For large files with small alphabets and simple models, all solutions to this problem give roughly the same compression. In this section we adopt 4.1. <p> In other words, L A = L SD + L M . Proof : Combine Equations (4.1), (4.2), and (4.3), noting that n1 = n t =t!. 2 Cleary and Witten [8] and Bell, Cleary, and Witten <ref> [4] </ref> present a similar result in a more general setting, showing approximate equality between enumerative codes (which are similar to arithmetic codes) and adaptive codes. Our result applies to a specific method of dealing with the first occurrence of each symbol; we show exact equality in that important special case. <p> For text files, the increased sophistication invariably takes the form of conditioning the symbol probabilities on contexts consisting of one or more symbols of preceding text. (Langdon [38] and Bell, Witten, Cleary, and Moffat <ref> [2,3, 4] </ref> have proven that both Ziv-Lempel coding [79,80] and the dynamic Markov coding method of Cormack and Horspool [9] can be reduced to finite context models, despite superficial indications to the contrary.) One significant difficulty with using high-order models is that many contexts do not occur often enough to provide <p> We have compared PPMC and PPMD on the Bell-Cleary-Witten corpus <ref> [4] </ref>. Table 4.2 shows that for text files PPMD compresses consistently about 1 ffi ffi better than PPMC. The compression results for PPMC differ from those reported in [4] because of implementation differences; we used versions of PPMC and PPMD that were identical except for the escape probability calculations. <p> We have compared PPMC and PPMD on the Bell-Cleary-Witten corpus <ref> [4] </ref>. Table 4.2 shows that for text files PPMD compresses consistently about 1 ffi ffi better than PPMC. The compression results for PPMC differ from those reported in [4] because of implementation differences; we used versions of PPMC and PPMD that were identical except for the escape probability calculations. Moffat [43] has applied the PPMD modification to his implementation and reports similar results. <p> Data structure for high order models We use a multiply-linked list structure similar to the vine pointers of Bell et al. <ref> [4] </ref>. In the versions of the Fast PPM system that use Rice coding, we keep the context lists sorted according to frequency count, while in the version that uses only quasi-arithmetic coding we do not reorganize the lists at all. <p> FAST PPM TEXT COMPRESSION 89 the symbol was found. (If it was found at the highest order, we do not create any new nodes.) This procedure runs somewhat counter to a recommendation of Bell et al. <ref> [4, pages 149-150] </ref>, but compression does not appear to suffer greatly. We also use a lazy update rule as in [4], updating statistics only for contexts actually searched. In our implementation we allow the model to grow without bound, never deleting nodes or restarting the model. <p> We also use a lazy update rule as in <ref> [4] </ref>, updating statistics only for contexts actually searched. In our implementation we allow the model to grow without bound, never deleting nodes or restarting the model.
Reference: [5] <author> J. L. Bentley, D. D. Sleator, R. E. Tarjan, and V. K. Wei, </author> <title> "A Locally Adaptive Data Compression Scheme," </title> <journal> Comm. ACM 29 (Apr. </journal> <year> 1986), </year> <pages> 320-330. </pages>
Reference: [6] <author> D. Chevion, E. D. Karnin, and E. Walach, </author> <title> "High Efficiency, Multiplication Free Approximation of Arithmetic Coding," </title> <booktitle> in Proc. Data Compression Conference, </booktitle> <editor> J. A. Storer and J. H. Reif, eds., </editor> <address> Snowbird, Utah, </address> <month> Apr. </month> <pages> 8-11, </pages> <year> 1991, </year> <pages> 43-52. </pages>
Reference-contexts: Historically, much of the arithmetic coding research by Rissanen, Langdon, and others at IBM has focused on bilevel images [39]. The 16 CHAPTER 2. STATISTICAL CODING Q-Coder [1,37,41,50,51,52] is a binary arithmetic coder; work by Rissanen and Mohi-uddin [62,63], Chevion et al. <ref> [6] </ref> , and Feygin et al. [16] extends some of the Q-Coder ideas to multi-symbol alphabets. The quasi-arithmetic coder discussed in Section 2.2 is formulated as a binary coder, though in Section 2.2.6 we show that it can be extended to a multi-symbol alphabet.
Reference: [7] <author> J. G. Cleary and I. H. Witten, </author> <title> "Data Compression Using Adaptive Coding and Partial String Matching," </title> <journal> IEEE Trans. Comm. </journal> <month> COM-32 (Apr. </month> <year> 1984), </year> <pages> 396-402. </pages>
Reference-contexts: We present a method similar in spirit to the prediction by partial matching method (PPM) of Cleary and Witten <ref> [7] </ref>, but with a significant difference. PPM encodes each input symbol using the longest context in which the symbol has already occurred, up to a specified maximum length. <p> In Section 4.2.1 we give a detailed analysis showing that scaling can improve compression considerably, and can never hurt compression by very much. In Section 4.3 we discuss the Prediction by Partial Matching (PPM) method of Cleary and Witten <ref> [7] </ref>, and in Section 4.3.2 we present a practical improvement to PPMC, the preferred text compression program in the literature.
Reference: [8] <author> J. G. Cleary and I. H. Witten, </author> <title> "A Comparison of Enumerative and Adaptive Codes," </title> <journal> IEEE Trans. Inform. Theory IT-30 (Mar. </journal> <year> 1984), </year> <pages> 306-315. </pages>
Reference-contexts: The decrementing count idea appears in the analysis of enumerative codes by Cleary and Witten <ref> [8] </ref> and also in [42]. The code length for a semi-adaptive 4.1. ADAPTIVE AND SEMI-ADAPTIVE MODELS FOR TEXT COMPRESSION73 decrementing code is L SD = log 2 i=1 ! ! Example 15 : We illustrate a decrementing code with the same file as in Example 14. <p> log 2 (number of possible distributions) = log 2 t + n 1 ! ~ n log 2 (et=n): Example: For our short sample file, t = 8 and n = 3, so L M = log 2 2 = log 2 45 A similar result appears in [4] and <ref> [8] </ref>. For a typical file L M is only about 2560 bits, or 320 bytes. <p> In other words, L A = L SD + L M . Proof : Combine Equations (4.1), (4.2), and (4.3), noting that n1 = n t =t!. 2 Cleary and Witten <ref> [8] </ref> and Bell, Cleary, and Witten [4] present a similar result in a more general setting, showing approximate equality between enumerative codes (which are similar to arithmetic codes) and adaptive codes.
Reference: [9] <author> G. V. Cormack and R. N. Horspool, </author> <title> "Data Compression Using Dynamic Markov Modelling," </title> <note> Computer Journal 30 (Dec. </note> <year> 1987), </year> <pages> 541-550. </pages>
Reference-contexts: In practice there are several ways to do this: * Periodically restarting the model. This often discards too much information to be effective, although Cormack and Horspool find that it gives good results when growing large dynamic Markov models <ref> [9] </ref>. * Using a sliding window on the text [36]. This requires excessive computational resources. * Recency rank coding [5,12,66]. This is simple but corresponds to a rather coarse model of recency. * Exponential aging (giving exponentially increasing weights to successive symbols) [10,46]. <p> the increased sophistication invariably takes the form of conditioning the symbol probabilities on contexts consisting of one or more symbols of preceding text. (Langdon [38] and Bell, Witten, Cleary, and Moffat [2,3, 4] have proven that both Ziv-Lempel coding [79,80] and the dynamic Markov coding method of Cormack and Horspool <ref> [9] </ref> can be reduced to finite context models, despite superficial indications to the contrary.) One significant difficulty with using high-order models is that many contexts do not occur often enough to provide reliable symbol probability estimates.
Reference: [10] <author> G. V. Cormack and R. N. Horspool, </author> <title> "Algorithms for Adaptive Huffman Codes," </title> <journal> Inform. Process. Lett. </journal> <month> 18 (Mar. </month> <year> 1984), </year> <pages> 159-165. </pages>
Reference-contexts: The smallest average number of events coded per input symbol occurs when the tree is a Huffman tree, since such trees have minimum average weighted path length; however, maintaining such trees dynamically is complicated <ref> [10, 36,73,74] </ref>. 2.2 Quasi-arithmetic coding The primary disadvantage of arithmetic coding is its slowness.
Reference: [11] <author> S. De Agostino and J. A. Storer, </author> <title> "Parallel Algorithms for Optimal Compression using Dictionaries with the Prefix Property," </title> <booktitle> in Proc. Data Compression Conference, </booktitle> <editor> J. A. Storer and M. Cohn, eds., </editor> <address> Snowbird, Utah, </address> <month> Mar. </month> <pages> 24-26, </pages> <year> 1992, </year> <pages> 52-61. </pages>
Reference-contexts: Parallel decoding is difficult, since the decoding processors cannot easily determine the lengths or the starting locations of the output codewords, although in fact, De Agostino and Storer <ref> [11] </ref> show that this can be done by assigning processors to encoded bits. Bit-transpose coding We can achieve decodability by rearranging (transposing) the output bits.
Reference: [12] <author> P. Elias, </author> <title> "Interval and Recency Rank Source Coding: Two On-line Adaptive Variable Length Schemes," </title> <journal> IEEE Trans. Inform. Theory IT-33 (Jan. </journal> <year> 1987), </year> <pages> 3-10. </pages>
Reference: [13] <author> P. Elias, </author> <title> "Universal Codeword Sets and Representations of Integers," </title> <journal> IEEE Trans. Inform. Theory IT-21 (Mar. </journal> <year> 1975), </year> <pages> 194-203. 93 94 REFERENCES </pages>
Reference-contexts: Example 11 : We can derive the Elias code for the positive integers <ref> [13] </ref> by using the maximally unbalanced subdivision technique of Example 10 and by doubling the full integer range whenever we see enough 1s to output a bit and expand the current interval so that it coincides with the full range.
Reference: [14] <author> T. Endoh and Y. Yamakazi, </author> <title> "Progressive Coding Scheme for Multilevel Images," Picture Coding Symp. </title> <booktitle> (1986), </booktitle> <pages> 21-22. </pages>
Reference: [15] <author> N. Faller, </author> <title> "An Adaptive System for Data Compression," </title> <booktitle> Record of the 7th Asilo-mar Conference on Circuits, Systems, and Computers, </booktitle> <year> 1973. </year>
Reference: [16] <author> G. Feygin, P. G. Gulak, and P. Chow, </author> <title> "Minimizing Error and VLSI Complexity in the Multiplication Free Approximation of Arithmetic Coding," </title> <booktitle> in Proc. Data Compression Conference, </booktitle> <editor> J. A. Storer and M. Cohn, eds., </editor> <address> Snowbird, Utah, </address> <month> Mar. </month> <journal> 30-Apr. </journal> <volume> 1, </volume> <year> 1993, </year> <pages> 118-127. </pages>
Reference-contexts: Historically, much of the arithmetic coding research by Rissanen, Langdon, and others at IBM has focused on bilevel images [39]. The 16 CHAPTER 2. STATISTICAL CODING Q-Coder [1,37,41,50,51,52] is a binary arithmetic coder; work by Rissanen and Mohi-uddin [62,63], Chevion et al. [6] , and Feygin et al. <ref> [16] </ref> extends some of the Q-Coder ideas to multi-symbol alphabets. The quasi-arithmetic coder discussed in Section 2.2 is formulated as a binary coder, though in Section 2.2.6 we show that it can be extended to a multi-symbol alphabet.
Reference: [17] <author> A. S. Fraenkel and S. T. Klein, </author> <title> "Robust Universal Complete Codes as Alternatives to Huffman Codes," </title> <institution> Dept. of Applied Mathematics, The Weizmann Institute of Science, </institution> <type> Technical Report, </type> <institution> Rehovot, Israel, </institution> <year> 1985. </year>
Reference-contexts: The assumption of equally-likely distributions is not very good for text files; in practice we can reduce the cost of encoding the model by 50 percent or more by encoding each of the counts using a suitable encoding of the integers, such as Fibonacci coding <ref> [17] </ref>.
Reference: [18] <author> P. </author> <title> Franti, </title> <type> Personal communication, </type> <year> 1993. </year>
Reference-contexts: The probability of out-of-range values falls off sharply, so when P is out of range it is reasonable to use exponential prefix codes, i.e., Golomb codes or the simpler Rice codes. This distribution clearly differs from the Laplace distribution commonly assumed in predictive image coding. Franti <ref> [18] </ref> has applied the FELICS technique to the problem of block truncation coding for lossy image compression; he finds that the distributions of quantization levels of the sub-sample images are similar to the distributions described here, and that the FELICS method is generally effective in encoding the quantization levels.
Reference: [19] <author> R. G. Gallager, </author> <title> "Variations on a Theme by Huffman," </title> <journal> IEEE Trans. Inform. Theory IT-24 (Nov. </journal> <year> 1978), </year> <pages> 668-674. </pages>
Reference: [20] <author> R. G. Gallager and D. C. Van Voorhis, </author> <title> "Optimal Source Codes for Geometrically Distributed Integer Alphabets," </title> <journal> IEEE Trans. Inform. Theory IT-21 (Mar. </journal> <year> 1975), </year> <pages> 228-230. </pages>
Reference-contexts: When the distribution of values to be encoded is exponential, it can be shown that a Golomb code with the correct choice of the parameter m produces an optimal prefix code for the distribution <ref> [20] </ref>. Rice coding has been used as the basis for a lossless hardware compressor [72].
Reference: [21] <author> N. Garcia, C. Munoz, and A. Sanz, </author> <title> "Image Compression Based on Hierarchical Coding," </title> <booktitle> SPIE Image Coding 594 (1985), </booktitle> <pages> 150-157. </pages>
Reference: [22] <author> S. W. Golomb, </author> <title> "Run-Length Encodings," </title> <journal> IEEE Trans. Inform. Theory IT-12 (July 1966), </journal> <pages> 399-401. </pages>
Reference-contexts: Although Huffman codes are optimal among prefix codes, the expected code length of a Huffman code is the same as the entropy only if all the probabilities are powers of 1 = 2 . 2.3.2 Golomb and Rice codes A simpler procedure, due to Golomb <ref> [22] </ref>, gives prefix codes that are suboptimal but very easy to implement. Golomb codes are distinguished from each other by a single parameter, so dynamic updating is accomplished by estimating the value of the parameter. In Section 2.3.3 we show how this can be done effectively.
Reference: [23] <author> M. Guazzo, </author> <title> "A General Minimum-Redundancy Source-Coding Algorithm," </title> <journal> IEEE Trans. Inform. Theory IT-26 (Jan. </journal> <year> 1980), </year> <pages> 15-25. </pages>
Reference-contexts: This follow-on procedure may be repeated any number of times, so the current interval size is always longer than 1 = 4 . Mechanisms for incremental transmission and fixed precision arithmetic have been developed through the years by Pasco [49], Rissanen [60], Rubin [65], Rissanen and Langdon [61], Guazzo <ref> [23] </ref>, and Witten, Neal, and Cleary [77]. The bit-stuffing idea of Langdon and others at IBM that limits the propagation of carries in the additions is roughly equivalent to the follow-on procedure described above. We now describe in detail how the coding and interval expansion work.
Reference: [24] <author> A. Habibi, </author> <title> "Comparison of nth-Order DPCM Encoder With Linear Transformations and Block Quantization Techniques," </title> <journal> IEEE Trans. Comm. </journal> <note> Tech. COM-19 (Dec. </note> <year> 1971), </year> <pages> 948-956. </pages>
Reference: [25] <author> R. W. </author> <title> Hamming, Coding and Information Theory, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1980. </year>
Reference-contexts: STATISTICAL CODING Instantaneous codes are just those with the prefix property: no code word is a prefix of another code word <ref> [25, pages 53-55] </ref>. Coding is simple for an instantaneous code the code bits for all events are disjoint and independent. In Section 2.3 we discuss three methods of coding that produce instantaneous codes, namely Huffman coding, Golomb coding, and Rice coding.
Reference: [26] <author> P. G. Howard and J. S. Vitter, </author> <title> "New Methods for Lossless Image Compression Using Arithmetic Coding," </title> <booktitle> Information Processing and Management 28 (1992), </booktitle> <pages> 765-779. </pages>
Reference-contexts: Second, none of the measures are cumulative: we cannot simply add together modeling and coding effects, or the effects of multiple cascaded compressors. In <ref> [26] </ref> we introduce a new measure, which we call compression gain, that is additive and that can be used to measure compression with respect to any standard. <p> These methods can be proven asymptotically optimal for images generated by finite state sources, but their practical usefulness appears to be limited. Our work on lossless data compression revolves around a paradigm we have developed <ref> [26] </ref> that separates the process into the following four components: * Pixel sequence. Careful selection of the pixel processing order can permit progressive compression, parallel computation, and improved compression efficiency. * Prediction.
Reference: [27] <author> P. G. Howard and J. S. Vitter, </author> <title> "Practical Implementations of Arithmetic Coding," in Image and Text Compression, </title> <editor> J. A. Storer, ed., </editor> <publisher> Kluwer Academic Publishers, Norwell, </publisher> <address> MA, </address> <year> 1992, </year> <pages> 85-112. </pages>
Reference-contexts: In the Q-Coder work at IBM, the time-consuming multiplications are replaced by additions and shifts, and low-order bits are ignored. In <ref> [27] </ref> we describe a different approach to approximate arithmetic coding. Recalling that the fractional bits characteristic of arithmetic coding are stored as state information in the coder, we reduce the number of possible states, and replace arithmetic operations by table lookups. <p> When either count overflows, we scale both counts downward; the new scaled count pair is the closest to the (unavailable) new count pair, closeness being measured by average excess code length. The use of scaling does not hurt compression by much, as shown in Section 4.2.1. In <ref> [27] </ref> we present an alternative deterministic probability estimation method. In the implementation we denote each possible pair of counts by an index number, and we precompute all the transitions to new count states, including those requiring scaling.
Reference: [28] <author> P. G. Howard and J. S. Vitter, </author> <title> "Analysis of Arithmetic Coding for Data Compression," </title> <booktitle> Information Processing and Management 28 (1992), </booktitle> <pages> 749-763. </pages>
Reference-contexts: The theoretical code length is log 2 0:000044092 = 14:469 bits, slightly shorter than in Example 16. The actual code length is 15 bits, since the final subinterval can be determined from the output 00110 01010 11101. 2 4.2.1 Analysis of scaling In <ref> [28] </ref> we rigorously analyze arithmetic coding from both the modeling and coding points of view. Coding effects are negligible, as shown in Section 2.1.4.
Reference: [29] <author> P. G. Howard and J. S. Vitter, </author> <title> "Error Modeling for Hierarchical Lossless Image Compression," </title> <booktitle> in Proc. Data Compression Conference, </booktitle> <editor> J. A. Storer and M. Cohn, eds., </editor> <address> Snowbird, Utah, </address> <month> Mar. </month> <pages> 24-26, </pages> <year> 1992, </year> <pages> 269-278. 95 </pages>
Reference-contexts: In [26] we introduce a new measure, which we call compression gain, that is additive and that can be used to measure compression with respect to any standard. In <ref> [29] </ref> we give an improved formulation that uses a more convenient scale; we define the compression gain by compression gain = 100 log e reference size compressed size ; (1:1) where the reference size and compressed size are expressed in the same units, usually either total bytes in the file or <p> The overhead increases with the number of blocks, but for medium-sized values of k (say k = 16) the net code length is often reduced because of the more realistic error model. Modeling by variability index In <ref> [29] </ref> we introduce the notion of error modeling by variability index, presenting and motivating the algorithm. In this section we use the variability index both for error modeling and as an experimental tool to show that prediction errors are not always Laplace distributed, as previously assumed.
Reference: [30] <author> P. G. Howard and J. S. Vitter, </author> <title> "Parallel Lossless Image Compression Using Huff-man and Arithmetic Coding," </title> <booktitle> in Proc. Data Compression Conference, </booktitle> <editor> J. A. Storer and M. Cohn, eds., </editor> <address> Snowbird, Utah, </address> <month> Mar. </month> <pages> 24-26, </pages> <year> 1992, </year> <pages> 299-308. </pages>
Reference-contexts: In <ref> [30] </ref> we present general-purpose algorithms for encoding and decoding using both Huffman and quasi-arithmetic coding, and apply them to the problem of lossless compression of high-resolution grayscale images. Our system for lossless image compression described in Chapter 3 has four components: pixel sequence, prediction, error modeling, and coding.
Reference: [31] <author> P. G. Howard and J. S. Vitter, </author> <title> "Design and Analysis of Fast Text Compression Based on Quasi-Arithmetic Coding," </title> <booktitle> in Proc. Data Compression Conference, </booktitle> <editor> J. A. Storer and M. Cohn, eds., </editor> <address> Snowbird, Utah, </address> <month> Mar. </month> <journal> 30-Apr. </journal> <volume> 1, </volume> <year> 1993, </year> <pages> 98-107. </pages>
Reference-contexts: Compression figures are in bits per input symbol. 4.4 Fast PPM text compression In <ref> [31] </ref> we show that we can obtain significantly faster compression with only a small loss of compression efficiency by modifying both the modeling and coding aspects of PPM.
Reference: [32] <author> P. G. Howard and J. S. Vitter, </author> <title> "Fast and Efficient Lossless Image Compression," </title> <booktitle> in Proc. Data Compression Conference, </booktitle> <editor> J. A. Storer and M. Cohn, eds., </editor> <address> Snowbird, Utah, </address> <month> Mar. </month> <journal> 30-Apr. </journal> <volume> 1, </volume> <year> 1993, </year> <pages> 351-360. </pages>
Reference-contexts: The simulation results described in Section 2.4.1 confirm the usefulness of this technique. 3.5 FELICS: a fast, efficient, lossless image compression system In <ref> [32] </ref> we present a simple system for lossless image compression that runs very fast with only minimal loss of compression efficiency. We call this technique FELICS, for Fast, Efficient, Lossless Image Compression System.
Reference: [33] <author> D. A. Huffman, </author> <title> "A Method for the Construction of Minimum Redundancy Codes," </title> <booktitle> Proceedings of the Institute of Radio Engineers 40 (1952), </booktitle> <pages> 1098-1101. </pages>
Reference-contexts: Adjusted binary codes are used as one of the two parts of each codeword in Golomb coding, and we use them in the image compression algorithm of Section 3.5. 2.3.1 Huffman codes If we are given the probabilities of n events, we can use Huffman's procedure <ref> [33] </ref> to construct an optimal prefix coding tree for those events. We briefly describe the procedure. We begin with a forest of n weighted trees, each consisting of a leaf node corresponding to an event; the weights of the leaves are proportional to the probabilities of the corresponding events.
Reference: [34] <author> A. K. Jain, </author> <title> "Image Data Compression: A Review," </title> <booktitle> Proc. of the IEEE 69 (Mar. </booktitle> <year> 1981), </year> <pages> 349-389. </pages>
Reference: [35] <author> K. Knowlton, </author> <title> "Progressive Transmission of Gray-Scale and Binary Pictures by Simple, Efficient, and Lossless Encoding Schemes," </title> <booktitle> Proc. of the IEEE 68 (July 1980), </booktitle> <pages> 885-896. </pages>
Reference: [36] <author> D. E. Knuth, </author> <title> "Dynamic Huffman Coding," </title> <journal> J. </journal> <note> Algorithms 6 (June 1985), 163-180. </note>
Reference-contexts: In practice there are several ways to do this: * Periodically restarting the model. This often discards too much information to be effective, although Cormack and Horspool find that it gives good results when growing large dynamic Markov models [9]. * Using a sliding window on the text <ref> [36] </ref>. This requires excessive computational resources. * Recency rank coding [5,12,66]. This is simple but corresponds to a rather coarse model of recency. * Exponential aging (giving exponentially increasing weights to successive symbols) [10,46]. This is moderately difficult to implement because of the changing weight increments. * Periodic scaling [77].
Reference: [37] <author> G. G. Langdon, </author> <title> "Probabilistic and Q-Coder Algorithms for Binary Source Adaptation," </title> <booktitle> in Proc. Data Compression Conference, </booktitle> <editor> J. A. Storer and J. H. Reif, eds., </editor> <address> Snowbird, Utah, </address> <month> Apr. </month> <pages> 8-11, </pages> <year> 1991, </year> <pages> 13-22. </pages>
Reference: [38] <author> G. G. Langdon, </author> <title> "A Note on the Ziv-Lempel Model for Compressing Individual Sequences," </title> <journal> IEEE Trans. Inform. Theory IT-29 (Mar. </journal> <year> 1983), </year> <pages> 284-287. </pages>
Reference-contexts: For text files, the increased sophistication invariably takes the form of conditioning the symbol probabilities on contexts consisting of one or more symbols of preceding text. (Langdon <ref> [38] </ref> and Bell, Witten, Cleary, and Moffat [2,3, 4] have proven that both Ziv-Lempel coding [79,80] and the dynamic Markov coding method of Cormack and Horspool [9] can be reduced to finite context models, despite superficial indications to the contrary.) One significant difficulty with using high-order models is that many contexts
Reference: [39] <author> G. G. Langdon and J. Rissanen, </author> <title> "Compression of Black-White Images with Arithmetic Coding," </title> <journal> IEEE Trans. Comm. COM-29 (1981), </journal> <pages> 858-867. </pages>
Reference-contexts: The coding of bilevel images, an important problem with a natural two-symbol alphabet, often produces probabilities close to 1, suggesting the use of arithmetic coding to obtain good compression. Historically, much of the arithmetic coding research by Rissanen, Langdon, and others at IBM has focused on bilevel images <ref> [39] </ref>. The 16 CHAPTER 2. STATISTICAL CODING Q-Coder [1,37,41,50,51,52] is a binary arithmetic coder; work by Rissanen and Mohi-uddin [62,63], Chevion et al. [6] , and Feygin et al. [16] extends some of the Q-Coder ideas to multi-symbol alphabets. <p> In Section 2.2.4 we give a practical implementation of the same coder. 2 2.2.2 Simplifications and applications of quasi-arithmetic coding Langdon and Rissanen <ref> [39] </ref> suggest identifying the input symbols as the less probable symbol (LPS) and more probable symbol (MPS) rather than as 0 and 1. By doing this we can often combine transitions and eliminate states. Example 8 : We modify Example 6 to use the MPS/LPS idea.
Reference: [40] <author> A. Lempel and J. Ziv, </author> <title> "Compression of Two-Dimensional Images," in Combinatorial Algorithms on Words, </title> <editor> A. Apostolico and Z. Galil, eds., </editor> <booktitle> NATO ASI Series #F12, </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1984, </year> <pages> 141-154. </pages>
Reference-contexts: Images, however, are two-dimensional, so the contexts are more complicated than for one-dimensional strings. In addition, images are typically quantized analog data, and the exact matches needed for high-order modeling are only rarely found in the data. Lempel and Ziv <ref> [40] </ref> have presented a method for two-dimensional dictionary based coding that uses a space-filling curve for its pixel sequence, and Sheinwald, Lempel, and Ziv [68] give another dictionary method that covers the image with repeated rectangles of various sizes.
Reference: [41] <author> J. L. Mitchell and W. B. Pennebaker, </author> <title> "Optimal Hardware and Software Arithmetic Coding Procedures for the Q-Coder," </title> <journal> IBM J. Res. Develop. </journal> <volume> 32 (Nov. </volume> <year> 1988), </year> <pages> 727-736. </pages>
Reference: [42] <author> A. M. Moffat, </author> <title> "Predictive Text Compression Based upon the Future Rather than the Past," </title> <booktitle> Australian Computer Science Communications 9 (1987), </booktitle> <pages> 254-261. </pages>
Reference-contexts: The decrementing count idea appears in the analysis of enumerative codes by Cleary and Witten [8] and also in <ref> [42] </ref>. The code length for a semi-adaptive 4.1. ADAPTIVE AND SEMI-ADAPTIVE MODELS FOR TEXT COMPRESSION73 decrementing code is L SD = log 2 i=1 ! ! Example 15 : We illustrate a decrementing code with the same file as in Example 14.
Reference: [43] <author> A. M. Moffat, </author> <type> Personal communication, </type> <year> 1992. </year>
Reference-contexts: Table 4.2 shows that for text files PPMD compresses consistently about 1 ffi ffi better than PPMC. The compression results for PPMC differ from those reported in [4] because of implementation differences; we used versions of PPMC and PPMD that were identical except for the escape probability calculations. Moffat <ref> [43] </ref> has applied the PPMD modification to his implementation and reports similar results. PPMD has the added advantage of making analysis more tractable by making the code length independent of the appearance order of symbols in the context. 4.4.
Reference: [44] <author> A. M. Moffat, </author> <title> "Word-Based Text Compression," </title> <journal> Software-Practice and Experience 19 (Feb. </journal> <year> 1989), </year> <pages> 185-198. </pages>
Reference-contexts: The new subinterval is [L + P C (H L); L + P N (H L)). The need to maintain and supply cumulative probabilities requires the model to have a complicated data structure; Moffat <ref> [44] </ref> investigates this problem, and concludes for a multi-symbol alphabet that binary search trees are about twice as fast as move-to-front lists.
Reference: [45] <author> A. M. Moffat, </author> <title> "Implementing the PPM Data Compression Scheme," </title> <journal> IEEE Trans. Comm. </journal> <month> COM-38 (Nov. </month> <year> 1990), </year> <pages> 1917-1921. </pages>
Reference-contexts: If we are maintaining only cumulative symbol counts we can truncate fractional counts, then add the index of each symbol to its cumulative 2.1. ARITHMETIC CODING 13 count; this is useful in a hardware-based solution. Moffat <ref> [45] </ref> rounds fractional counts upwards, but retains more precision by using fixed-point counts with a few fractional bits; this allows counts to become almost 0. With three fractional bits, the counts can become as small as 1 = 8 .) The scaling process introduces a modeling effect. <p> The probabilities passed to the coder are based on symbol frequency counts, periodically scaled down to exploit locality of reference. Cleary and Witten specify two ad hoc methods, called PPMA and PPMB, for computing the probability of the escape symbol. Moffat <ref> [45] </ref> implements the algorithm and proposes a third method, PPMC, for computing the escape probability: he treats the escape event as a separate symbol; when a symbol occurs for the first time he adds 1 to both the escape count and the new symbol's count. <p> In this section we present a slightly improved method for estimating the escape probability, which we call PPMD. Moffat's PPMC method <ref> [45] </ref> is widely considered to be the best method of estimating escape probabilities. In PPMC, each symbol's weight in a context is taken to be number of times it has occurred so far in the context.
Reference: [46] <author> K. Mohiuddin, J. J. Rissanen, and M. Wax, </author> <title> "Adaptive Model for Nonstationary Sources," </title> <journal> IBM Technical Disclosure Bulletin 28 (Apr. </journal> <year> 1986), </year> <pages> 4798-4800. 96 REFERENCES </pages>
Reference: [47] <author> A. N. Netravali and J. O. </author> <title> Limb, "Picture Coding: A Review," </title> <booktitle> Proc. of the IEEE 68 (Mar. </booktitle> <year> 1980), </year> <pages> 366-406. </pages>
Reference: [48] <author> J. B. O'Neal, </author> <title> "Predictive Quantizing Differential Pulse Code Modulation for the Transmission of Television Signals," </title> <institution> Bell Syst. Tech. J. </institution> <month> 45 (May-June </month> <year> 1966), </year> <pages> 689-721. </pages>
Reference: [49] <author> R. </author> <title> Pasco, "Source Coding Algorithms for Fast Data Compression," </title> <institution> Stanford Univ., </institution> <type> Ph.D. Thesis, </type> <year> 1976. </year>
Reference-contexts: This follow-on procedure may be repeated any number of times, so the current interval size is always longer than 1 = 4 . Mechanisms for incremental transmission and fixed precision arithmetic have been developed through the years by Pasco <ref> [49] </ref>, Rissanen [60], Rubin [65], Rissanen and Langdon [61], Guazzo [23], and Witten, Neal, and Cleary [77]. The bit-stuffing idea of Langdon and others at IBM that limits the propagation of carries in the additions is roughly equivalent to the follow-on procedure described above.
Reference: [50] <author> W. B. Pennebaker and J. L. Mitchell, </author> <title> "Probability Estimation for the Q-Coder," </title> <journal> IBM J. Res. Develop. </journal> <volume> 32 (Nov. </volume> <year> 1988), </year> <pages> 737-752. </pages>
Reference: [51] <author> W. B. Pennebaker and J. L. Mitchell, </author> <title> "Software Implementations of the Q-Coder," </title> <journal> IBM J. Res. Develop. </journal> <volume> 32 (Nov. </volume> <year> 1988), </year> <pages> 753-774. </pages>
Reference: [52] <author> W. B. Pennebaker, J. L. Mitchell, G. G. Langdon, and R. B. </author> <title> Arps, "An Overview of the Basic Principles of the Q-Coder Adaptive Binary Arithmetic Coder," </title> <journal> IBM J. Res. Develop. </journal> <volume> 32 (Nov. </volume> <year> 1988), </year> <pages> 717-726. </pages>
Reference: [53] <author> R. F. Rice, </author> <title> "Some Practical Universal Noiseless Coding Techniques," </title> <institution> Jet Propulsion Laboratory, JPL Publication 79-22, Pasadena, California, </institution> <month> Mar. </month> <year> 1979. </year>
Reference: [54] <author> R. F. Rice, </author> <title> "Some Practical Universal Noiseless Coding Techniques|Part II," </title> <institution> Jet Propulsion Laboratory, JPL Publication 83-17, Pasadena, California, </institution> <month> Mar. </month> <year> 1983. </year>
Reference: [55] <author> R. F. Rice, </author> <title> "Some Practical Universal Noiseless Coding Techniques|Part III, Module PSI14,K+," </title> <institution> Jet Propulsion Laboratory, JPL Publication 91-3, Pasadena, California, </institution> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: A common method of obtaining and transmitting coding parameters in algorithms that use Rice coding <ref> [55] </ref> is to divide the data into blocks, and for each block to compute the code lengths that would be obtained using each of a set of reasonable parameter values and output the best parameter value as side information.
Reference: [56] <author> J. Rissanen, </author> <title> "Modeling by Shortest Data Description," </title> <booktitle> Automatica 14 (1978), </booktitle> <pages> 465-571. </pages>
Reference: [57] <author> J. Rissanen, </author> <title> "A Universal Prior for Integers and Estimation by Minimum Description Length," </title> <journal> Ann. Statist. </journal> <volume> 11 (1983), </volume> <pages> 416-432. </pages>
Reference: [58] <author> J. Rissanen, </author> <title> "Universal Coding, Information, Prediction, and Estimation," </title> <journal> IEEE Trans. Inform. Theory IT-30 (July 1984), </journal> <pages> 629-636. </pages>
Reference: [59] <author> J. Rissanen and G. G. Langdon, </author> <title> "Universal Modeling and Coding," </title> <journal> IEEE Trans. Inform. Theory IT-27 (Jan. </journal> <year> 1981), </year> <pages> 12-23. </pages>
Reference-contexts: It is desirable to use a good model of the source of the data so that the statistical coder can work with accurate probabilities. The separation of the compression process into coding and modeling is due to Rissanen and Langdon <ref> [59] </ref>. In this chapter we give a brief overview of both modeling and coding. We introduce arithmetic codes and distinguish them from prefix codes; we describe and mathematically analyze arithmetic codes in more detail in Chapter 2, where we also present quasi-arithmetic coding, our practical implementation of arithmetic coding.
Reference: [60] <author> J. J. Rissanen, </author> <title> "Generalized Kraft Inequality and Arithmetic Coding," </title> <institution> IBM J. Res. Develop. </institution> <month> 20 (May </month> <year> 1976), </year> <pages> 198-203. </pages>
Reference-contexts: This follow-on procedure may be repeated any number of times, so the current interval size is always longer than 1 = 4 . Mechanisms for incremental transmission and fixed precision arithmetic have been developed through the years by Pasco [49], Rissanen <ref> [60] </ref>, Rubin [65], Rissanen and Langdon [61], Guazzo [23], and Witten, Neal, and Cleary [77]. The bit-stuffing idea of Langdon and others at IBM that limits the propagation of carries in the additions is roughly equivalent to the follow-on procedure described above.
Reference: [61] <author> J. J. Rissanen and G. G. Langdon, </author> <title> "Arithmetic Coding," </title> <institution> IBM J. Res. Develop. </institution> <month> 23 (Mar. </month> <year> 1979), </year> <pages> 146-162. </pages>
Reference-contexts: This follow-on procedure may be repeated any number of times, so the current interval size is always longer than 1 = 4 . Mechanisms for incremental transmission and fixed precision arithmetic have been developed through the years by Pasco [49], Rissanen [60], Rubin [65], Rissanen and Langdon <ref> [61] </ref>, Guazzo [23], and Witten, Neal, and Cleary [77]. The bit-stuffing idea of Langdon and others at IBM that limits the propagation of carries in the additions is roughly equivalent to the follow-on procedure described above. We now describe in detail how the coding and interval expansion work.
Reference: [62] <author> J. J. Rissanen and K. M. Mohiuddin, </author> <year> 1987, </year> <editor> U. S. Patent 4,652,856, </editor> <publisher> IBM. </publisher>
Reference: [63] <author> J. J. Rissanen and K. M. Mohiuddin, </author> <title> "A Multiplication-Free Multialphabet Arithmetic Code," </title> <journal> IEEE Trans. Comm. </journal> <month> 37 (Feb. </month> <year> 1989), </year> <pages> 93-98. </pages>
Reference: [64] <author> P. Roos, M. A. Viergever, M. C. A. van Dijke, and J. H. Peters, </author> <title> "Reversible Intraframe Compression of Medical Images," </title> <journal> IEEE Trans. </journal> <note> Medical Imaging 7 (Dec. </note> <year> 1988), </year> <pages> 328-336. 97 </pages>
Reference: [65] <author> F. Rubin, </author> <title> "Arithmetic Stream Coding Using Fixed Precision Registers," </title> <journal> IEEE Trans. Inform. Theory IT-25 (Nov. </journal> <year> 1979), </year> <pages> 672-675. </pages>
Reference-contexts: This follow-on procedure may be repeated any number of times, so the current interval size is always longer than 1 = 4 . Mechanisms for incremental transmission and fixed precision arithmetic have been developed through the years by Pasco [49], Rissanen [60], Rubin <ref> [65] </ref>, Rissanen and Langdon [61], Guazzo [23], and Witten, Neal, and Cleary [77]. The bit-stuffing idea of Langdon and others at IBM that limits the propagation of carries in the additions is roughly equivalent to the follow-on procedure described above.
Reference: [66] <author> B. Y. Ryabko, </author> <title> "Data Compression by Means of a Book Stack," </title> <note> Problemy Peredachi Informatsii 16 (1980). </note>
Reference: [67] <author> C. E. Shannon, </author> <title> "A Mathematical Theory of Communication," </title> <institution> Bell Syst. Tech. J. </institution> <month> 27 (July </month> <year> 1948), </year> <pages> 398-403. </pages>
Reference-contexts: INTRODUCTION TO DATA COMPRESSION Data can be compressed whenever some patterns of data symbols are more likely to occur than others. Shannon <ref> [67] </ref> showed that for the best possible compression code (in the sense of minimum average code length), the output length contains a contribution of log 2 p bits from the encoding of each symbol whose probability of occurrence is p. <p> Arithmetic codes almost always give better compression than prefix codes, but they lack the direct correspondence between the events in the input data set and bits or groups of bits in the coded output file. Shannon <ref> [67] </ref> showed that for a given set of probabilities P = fp 1 ; . . . ; p n g, the optimal expected number of code bits is n X p i log 2 p i ; a quantity which he called the entropy of P , usually denoted by <p> They are appropriate in this case: in Huffman coding we wish to avoid changing weights since changing weights often requires changing the structure of the coding tree. Our theorem does not contradict Shannon's theorem <ref> [67] </ref>; he discusses only the best static code.
Reference: [68] <author> D. Sheinwald, A. Lempel, and J. Ziv, </author> <title> "Two-Dimensional Encoding by Finite State Encoders," </title> <journal> IEEE Transactions on Communications COM-38 (1990), </journal> <pages> 341-347. </pages>
Reference-contexts: Lempel and Ziv [40] have presented a method for two-dimensional dictionary based coding that uses a space-filling curve for its pixel sequence, and Sheinwald, Lempel, and Ziv <ref> [68] </ref> give another dictionary method that covers the image with repeated rectangles of various sizes. These methods can be proven asymptotically optimal for images generated by finite state sources, but their practical usefulness appears to be limited.
Reference: [69] <author> R. G. Stone, </author> <title> "On Encoding of Commas Between Strings," </title> <journal> Comm. </journal> <note> ACM 22 (May 1979), 310-311. </note>
Reference: [70] <author> H. H. Torbey and H. E. Meadows, </author> <title> "System for Lossless Digital Image Compression," </title> <booktitle> Proc. of SPIE Visual Communication and Image Processing IV 1199 (Nov. </booktitle> <pages> 8-10, </pages> <year> 1989), </year> <pages> 989-1002. </pages>
Reference: [71] <author> K. H. Tzou, </author> <title> "Progressive Image Transmission: a Review and Comparsion of Techniques," </title> <booktitle> Optical Engineering 26 (July 1987), </booktitle> <pages> 581-589. </pages>
Reference-contexts: A progressive method allows an end user to browse an image without having to decode much of the encoded data; if more detail is desired, the image can be successively refined as more of the encoded data is decoded. An excellent survey of progressive techniques appears in <ref> [71] </ref>. 3.4.
Reference: [72] <author> J. Venbrux, N. Liu, K. Liu, P. Vincent, and R. Merrell, </author> <title> "A Very High Speed Lossless Compression/Decompression Chip Set," </title> <institution> Jet Propulsion Laboratory, JPL Publication 91-13, Pasadena, California, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: PREFIX CODES 37 Then we compute n mod 2 k and output this value using a k bit binary code. The resulting codes give less compression efficiency than Golomb codes, but they are even easier to implement, especially in hardware <ref> [72] </ref>, since we can compute bn=2 k c by shifting and n mod 2 k by masking out all but the k low order bits. Our parameter estimation method for Golomb codes applies to Rice codes too. <p> When the distribution of values to be encoded is exponential, it can be shown that a Golomb code with the correct choice of the parameter m produces an optimal prefix code for the distribution [20]. Rice coding has been used as the basis for a lossless hardware compressor <ref> [72] </ref>. Its compression effectiveness is analyzed in [78]. 2.3.3 Selection of Golomb or Rice coding parameter We now describe an on-line algorithm for estimating the coding parameter for Golomb and Rice codes, and prove a bound on its effectiveness.
Reference: [73] <author> J. S. Vitter, </author> <title> "Dynamic Huffman Coding," </title> <journal> ACM Trans. Math. </journal> <note> Software 15 (June 1989), 158-167, also appears as Algorithm 673, Collected Algorithms of ACM, </note> <year> 1989. </year>
Reference: [74] <author> J. S. Vitter, </author> <title> "Design and Analysis of Dynamic Huffman Codes," </title> <journal> Journal of the ACM 34 (Oct. </journal> <year> 1987), </year> <pages> 825-845. </pages>
Reference: [75] <author> M. Wang, </author> <title> "Almost Asymptotically Optimal Flag Encoding of the Integers," </title> <journal> IEEE Trans. Inform. Theory IT-34 (Mar. </journal> <year> 1988), </year> <pages> 324-326. </pages>
Reference: [76] <author> I. H. Witten and T. C. Bell, </author> <title> "The Zero Frequency Problem: Estimating the Probabilities of Novel Events in Adaptive Text Compression," </title> <journal> IEEE Trans. Inform. Theory IT-37 (July 1991), </journal> <pages> 1085-1094. </pages>
Reference-contexts: We can encode a symbol using arithmetic coding only if its probability is nonzero, but in an adaptive code we have no way of estimating the probability of a symbol before it has occurred for the first time. This is the zero-frequency problem, discussed in detail in [4] and <ref> [76] </ref>. For large files with small alphabets and simple models, all solutions to this problem give roughly the same compression. In this section we adopt 4.1. ADAPTIVE AND SEMI-ADAPTIVE MODELS FOR TEXT COMPRESSION75 the solution used in [77], simply assigning an initial weight of 1 to all alphabet symbols. <p> In practice, PPMC compresses better than PPMA and PPMB. PPMP and PPMX appear in <ref> [76] </ref>; they are based on the assumption that the appearance of symbols for the first time in a file is approximately a Poisson process. In Section 4.3.2 we indicate a method, called PPMD, that provides improved estimation of the escape probability.
Reference: [77] <author> I. H. Witten, R. M. Neal, and J. G. Cleary, </author> <title> "Arithmetic Coding for Data Compression," </title> <journal> Comm. </journal> <note> ACM 30 (June 1987), 520-540. </note>
Reference-contexts: Parallel compression and decompression is possible both with prefix codes and with quasi-arithmetic codes. 2.1 Arithmetic coding In this section we explain how arithmetic coding works and give implementation details; our treatment is based on that of Witten, Neal, and Cleary <ref> [77] </ref>. We point out the usefulness of binary arithmetic coding, that is, coding with a two-symbol alphabet. Our focus is on encoding, but the decoding process is similar. 2.1.1 Basic algorithm for arithmetic coding The algorithm for encoding a file using arithmetic coding works conceptually as follows: 1. <p> STATISTICAL CODING Interval in [0; 1 = 2 ). (c) Interval in [ 1 = 2 ; 1). (d) Interval in [ 1 = 4 ; 3 = 4 ) (follow-on case). part of the final interval. Witten, Neal, and Cleary <ref> [77] </ref> add a clever mechanism for preventing the current interval from shrinking too much when the endpoints are close to 1 = 2 but straddle 1 = 2 . <p> Mechanisms for incremental transmission and fixed precision arithmetic have been developed through the years by Pasco [49], Rissanen [60], Rubin [65], Rissanen and Langdon [61], Guazzo [23], and Witten, Neal, and Cleary <ref> [77] </ref>. The bit-stuffing idea of Langdon and others at IBM that limits the propagation of carries in the additions is roughly equivalent to the follow-on procedure described above. We now describe in detail how the coding and interval expansion work. <p> The new subinterval is [L + b C (HL) N (HL) T c). (In this discussion we continue to use half-open intervals as in the real arithmetic case. In implementa tions <ref> [77] </ref> it is more convenient to subtract 1 from the right endpoints and use closed intervals.) Example 3 : Suppose that at a certain point in the encoding we have symbol counts c a = 4, c b = 5, and c EOF = 1 and current interval [25; 89) from <p> This is the zero-frequency problem, discussed in detail in [4] and [76]. For large files with small alphabets and simple models, all solutions to this problem give roughly the same compression. In this section we adopt 4.1. ADAPTIVE AND SEMI-ADAPTIVE MODELS FOR TEXT COMPRESSION75 the solution used in <ref> [77] </ref>, simply assigning an initial weight of 1 to all alphabet symbols. <p> This requires excessive computational resources. * Recency rank coding [5,12,66]. This is simple but corresponds to a rather coarse model of recency. * Exponential aging (giving exponentially increasing weights to successive symbols) [10,46]. This is moderately difficult to implement because of the changing weight increments. * Periodic scaling <ref> [77] </ref>. This is simple to implement, fast and effective in operation, and amenable to analysis. It also has the computationally desirable property of keeping the symbol weights small. In effect, scaling is a practical version of exponential aging.
Reference: [78] <author> P.-S. Yeh, R. F. Rice, and W. Miller, </author> <title> "On the Optimality of Code Options for a Universal Noiseless Coder," </title> <institution> Jet Propulsion Laboratory, JPL Publication 91-2, Pasadena, California, </institution> <month> Feb. </month> <year> 1991. </year>
Reference-contexts: Rice coding has been used as the basis for a lossless hardware compressor [72]. Its compression effectiveness is analyzed in <ref> [78] </ref>. 2.3.3 Selection of Golomb or Rice coding parameter We now describe an on-line algorithm for estimating the coding parameter for Golomb and Rice codes, and prove a bound on its effectiveness.
Reference: [79] <author> J. Ziv and A. Lempel, </author> <title> "A Universal Algorithm for Sequential Data Compression," </title> <journal> IEEE Trans. Inform. Theory IT-23 (May 1977), </journal> <pages> 337-343. </pages>
Reference: [80] <author> J. Ziv and A. Lempel, </author> <title> "Compression of Individual Sequences via Variable Rate Coding," </title> <journal> IEEE Trans. Inform. </journal> <note> Theory IT-24 (Sept. </note> <year> 1978), </year> <month> 530-536. </month> <title> The text of this report is set in Computer Modern 12/14. The chapter titles are 14-point Computer Modern Dunhill, and the chapter numbers are 20-point Computer Modern Dunhill. The versals at the start of each chapter are 36-point Computer Duerer Roman. </title>
References-found: 80

