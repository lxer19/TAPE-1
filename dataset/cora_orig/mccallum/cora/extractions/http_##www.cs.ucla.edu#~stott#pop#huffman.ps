URL: http://www.cs.ucla.edu/~stott/pop/huffman.ps
Refering-URL: http://www.cs.ucla.edu/~stott/pop/
Root-URL: http://www.cs.ucla.edu
Title: THE CONSTRUCTION OF HUFFMAN CODES IS A SUBMODULAR (`CONVEX') OPTIMIZATION PROBLEM OVER A LATTICE OF
Author: D. STOTT PARKER AND PRASAD RAM 
Keyword: Key words. Huffman coding, adaptive coding, prefix codes, enumeration of trees, lattices, combinatorial optimization, convexity, submodular functions, entropy, tree imbalance, Schur convex functions, majorization, Moebius inversion, combinatorial inequalities, FKG inequality, quadrangle inequality, Monge matrices, dynamic programming, greedy algorithms.  
Affiliation: 94A15,  
Note: AMS subject classifications.  
Pubnum: 94A24, 94A29, 94A45, 90C25, 90C27, 90C39, 90C48, 52A41, 68Q20, 68R05, 05A05, 05A20, 05C05, 05C30, 06A07, 26B25, 26D15.  
Abstract: We show that the space of all binary Huffman codes for a finite alphabet defines a lattice, ordered by the imbalance of the code trees. Representing code trees as path-length sequences, we show that the imbalance ordering is closely related to a majorization ordering on real-valued sequences that correspond to discrete probability density functions. Furthermore, this tree imbalance is a partial ordering that is consistent with the total orderings given by either the external path length (sum of tree path lengths), or the entropy determined by the tree structure. On the imbalance lattice, we show the weighted path-length of a tree (the usual objective function for Huffman coding) is a submodular function, as is the corresponding function on the majorization lattice. Submodular functions are discrete analogues of convex functions. These results give perspective on Huffman coding, and suggest new approaches to coding as optimization over a lattice. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Abrahams, </author> <title> Code and Parse Trees for Lossless Source Encoding, Proc. Compression & Complexity of Sequences (SEQUENCES'97), </title> <address> Positano, Italy, </address> <year> 1997, </year> <note> IEEE Press, to appear. </note>
Reference-contexts: 1. Introduction. The Huffman algorithm has been used heavily to produce efficient binary codes for almost half a century now. It has inspired a large literature with diverse theoretical and practical contributions. A comprehensive, very recent survey is <ref> [1] </ref>. Although the algorithm is quite elegant, it is tricky to prove correct and to reason about. <p> The central inequality is due to submodularity. Putting 0 &lt; &lt; 1 and = (1 ) shows that b f is convex. Lovasz goes on [25, p.250-251] to point out that min f f (X) j X S g = min f b f (x) j x 2 <ref> [0; 1] </ref> n g and that as a consequence there is a polynomial-time algorithm to minimize f . The vector lattice h&lt; + n ; vec ; min vec ; max vec i; is exactly the extension of the set lattice to nonnegative vectors.
Reference: [2] <author> A. Aggarwal, A. Bar-Noy, S. Khuller, D. Kravets, B. Schieber, </author> <title> Efficient Minimum Cost Matching and Transportation using the Quadrangle Inequality, </title> <editor> J. </editor> <booktitle> Algorithms, </booktitle> <month> 19:1 (July </month> <year> 1995), </year> <pages> pp. 116-143. </pages>
Reference-contexts: Results from exploiting the quadrangle inequality in dynamic programming appear in <ref> [2, 7, 35] </ref> for problems ranging from DNA sequencing to minimum cost matching.
Reference: [3] <author> A. Berman, R.J. Plemmons, </author> <title> Nonnegative matrices in the mathematical sciences, </title> <publisher> SIAM, </publisher> <year> 1994. </year>
Reference-contexts: Note: the derivatives @ 2 f @x 2 can still be positive. In fact, the Hessian r 2 f = ( @ 2 f @x i @x j ) still can even be positive semidefinite (hence f can be convex), or be an M-matrix <ref> [3, ch.6] </ref>. Theorem 4.7. When twice differentiable, F is submodular on the majorization lattice h&lt; + n ; ; u; ti iff for all i 6= j between 1 and n 1, @ 2 F (z) @z i+1 @z j @ 2 F (z) + @z i+1 @z j+1 Proof.
Reference: [4] <author> R.E. Burkard, B. Klinz, R. Rudolf, </author> <title> Perspectives of Monge properties in optimization, </title> <journal> Discrete Applied Math., </journal> <volume> 70 (1996), </volume> <pages> pp. 95-161. </pages>
Reference-contexts: Burkard et al. have also compiled a comprehensive survey of many incarnations of the Monge condition in <ref> [4] </ref>. Recently Klein explored the connection between dynamic programming and submodu-larity [20]. <p> Acknowledgements. We are very grateful to Pierre Hasenfratz for insightful comments that improved this paper. A conversation with Mordecai Golin, who provided us with an expanded version of [14], inspired us to discuss dynamic programming explicitly in this paper. He also pointed out the survey <ref> [4] </ref> to us. Also, we are indebted to two anonymous referees for clarifications of the exposition, especially of the significance of submodularity and of Shannon's work [38].
Reference: [5] <author> B.A. Davey, H.A. Priestley, </author> <title> Introduction to Lattices and Order, </title> <publisher> Cambridge U. Press, </publisher> <year> 1990. </year>
Reference-contexts: The proof of s + fi t + is also similar, but easier, since then it is never the case that (` i ) + = (` i+1 ) + . 3.4. The Vector Lattice and Distribution Lattice. Recall <ref> [5] </ref> that a lattice is an algebra hS; v; u; ti in which S is a set, v is a partial ordering on S, and for all a; b 2 S, there is a unique greatest lower bound (glb) a u b and least upper bound (lub) a t b.
Reference: [6] <author> J. Edmonds, </author> <title> Submodular Functions, Matroids and Certain Polyhedra, in Combinatorial Structures and their Applications, </title> <editor> R. Guy et al., eds., </editor> <publisher> Gordon & Breach, </publisher> <year> 1970, </year> <pages> pp. 69-87. </pages>
Reference-contexts: For example, the objective functions commonly used in evaluating codes are submodular on this lattice. Submodular functions are closely related to convex functions (as we explain later; see Theorem 4.5), and are often easy to optimize <ref> [6, 9, 23, 24, 25] </ref>. Huffman coding gives a significant example of the importance of submodularity in algorithms. 2. Ordered Sequences, Rooted Binary Trees, and Huffman Codes. 2.1. Ordered Sequences. <p> Submodularity. Most work on submodular functions assumes the lattice is the lattice of subsets of a given set, the case originally emphasized by Edmonds <ref> [6] </ref>. However, the definition applies to any lattice: Definition 4.1. A real-valued function f : L ! &lt; defined on a lattice hL; v; u; ti is submodular if f (x u y) + f (x t y) f (x) + f (y) 16 D.S. PARKER AND P. <p> Submodularity has a long history in dynamic programming. By 1781, Monge had found a form of submodularity to be important in simplifying the transportation problem [17]. In 1970, Edmonds <ref> [6] </ref> related submodularity to matroids and greedily-solvable optimization problems.
Reference: [7] <author> D. Eppstein, Z. Galil, R. Giancarlo, </author> <title> G.F. Italiano, Sparse dynamic Programming. II. Convex and concave cost functions, </title> <journal> Journal of the ACM, </journal> <month> 39:3 (July </month> <year> 1992), </year> <pages> pp. 546-567. </pages>
Reference-contexts: Results from exploiting the quadrangle inequality in dynamic programming appear in <ref> [2, 7, 35] </ref> for problems ranging from DNA sequencing to minimum cost matching.
Reference: [8] <author> N. Faller, </author> <title> An adaptive system for data compression, </title> <booktitle> Record of the 7th Asilomar Conference on Circuits, Systems, and Computers, </booktitle> <year> 1973, </year> <pages> pp. 593-597. </pages>
Reference-contexts: Practical Applications in Adaptive Coding. In many practical situations it is difficult or impossible to know a priori the weights w used in Huffman coding. A natural idea, which occurred independently to Faller <ref> [8] </ref> and Gallager [11], is to allow the weights to be determined dynamically, and have the Huffman code `evolve' over time. Dynamic Huffman coding is the strategy of repeatedly constructing the Huffman code for the input so far, and using it in transmitting the next input symbol.
Reference: [9] <author> S. Fujishige, </author> <title> Submodular functions and optimization, </title> <publisher> North-Holland Elsevier, </publisher> <year> 1991. </year>
Reference-contexts: For example, the objective functions commonly used in evaluating codes are submodular on this lattice. Submodular functions are closely related to convex functions (as we explain later; see Theorem 4.5), and are often easy to optimize <ref> [6, 9, 23, 24, 25] </ref>. Huffman coding gives a significant example of the importance of submodularity in algorithms. 2. Ordered Sequences, Rooted Binary Trees, and Huffman Codes. 2.1. Ordered Sequences. <p> A fine survey of results with FKG-like inequalities is [15]. Second, submodularity is closely related to convexity. Book-length surveys by Fujishige <ref> [9] </ref> and Narayanan [28] review connections between submodularity and optimization (and even electrical network theory). The relationship between convexity and submodularity was neatly summarized by Lovasz with the following memorable definition and result. Definition 4.4.
Reference: [10] <author> R.G. Gallager, </author> <title> Information Theory and Reliable Communications, </title> <editor> J. </editor> <publisher> Wiley, </publisher> <year> 1968. </year> <title> HUFFMAN CODING AS SUBMODULAR OPTIMIZATION OVER A LATTICE 27 </title>
Reference-contexts: A binary tree having path-length sequence h 1 3 3 4 4 4 4 i Path-length sequences obey what we call the Kraft equality, a special case of the Kraft inequality of noiseless coding theory (see e.g., <ref> [10] </ref>, p.45). Theorem 2.1. For all n 1, h ` 1 ` n i is the sequence of path-lengths in a rooted binary tree iff n X 2 ` i = 1: Thus ` is a path-length sequence iff 2 ` is a density sequence. Proof.
Reference: [11] <author> R.G. Gallager, </author> <title> Variations on a theme by Huffman, </title> <journal> IEEE Trans. Information Theory, </journal> <month> IT-24:6 (November </month> <year> 1978), </year> <pages> pp. 668-674. </pages>
Reference-contexts: Practical Applications in Adaptive Coding. In many practical situations it is difficult or impossible to know a priori the weights w used in Huffman coding. A natural idea, which occurred independently to Faller [8] and Gallager <ref> [11] </ref>, is to allow the weights to be determined dynamically, and have the Huffman code `evolve' over time. Dynamic Huffman coding is the strategy of repeatedly constructing the Huffman code for the input so far, and using it in transmitting the next input symbol.
Reference: [12] <author> E.N. Gilbert, </author> <title> Codes Based on Inaccurate Source Probabilities, </title> <journal> IEEE Trans. Information Theory, </journal> <month> IT-17:3 (May </month> <year> 1971), </year> <pages> pp. 304-314. </pages> <note> g(N ) is analyzed on p. 309. </note>
Reference-contexts: T n is enumerated as sequence M0710 (A002572) in [39]. An upper bound on T n can be obtained from the Catalan number C n , which computes the number of unordered binary trees: for n 3, T n 1 2 C n 2 n3 . Gilbert <ref> [12] </ref>, using the notation g (N ) for T N , points out that T n is well approximated for n 30 by 2.3. Huffman Codes are Optimal Path-Length Sequences.
Reference: [13] <author> C.R. Glassey and R.M. Karp, </author> <title> On the optimality of Huffman trees, </title> <journal> SIAM J. Appl. Math., </journal> <volume> 31 (1976), </volume> <pages> pp. 368-378. </pages>
Reference-contexts: Majorization is an important ordering on sequences that has many applications in pure and applied mathematics [27]. We have related it to greedy algorithms directly [33]. Earlier majorization was recognized as an important property of the internal node weights produced by the Huffman algorithm <ref> [13, 32] </ref>, and in this work we go further to clarify its pervasive role. By viewing the space of trees as a lattice, a variety of new theorems and algorithms become possible. For example, the objective functions commonly used in evaluating codes are submodular on this lattice.
Reference: [14] <author> M.J. Golin, G. </author> <title> Rote, A dynamic programming algorithm for constructing optimal prefix-free codes for unequal letter costs, </title> <booktitle> Proc. ICALP 95, </booktitle> <editor> Z. Fulop, F. Gecseg, eds., </editor> <publisher> Springer-Verlag, </publisher> <year> 1995, </year> <pages> pp. 256-267. </pages>
Reference-contexts: Burkard et al. have also compiled a comprehensive survey of many incarnations of the Monge condition in [4]. Recently Klein explored the connection between dynamic programming and submodu-larity [20]. Also Golin and Rote <ref> [14] </ref> developed dynamic programming algorithms for prefix codes when the codeword letters have differing costs, a useful case not handled by Huffman's algorithm; and they recently extended this work to exploit the Monge property. 6. Other Applications. <p> Acknowledgements. We are very grateful to Pierre Hasenfratz for insightful comments that improved this paper. A conversation with Mordecai Golin, who provided us with an expanded version of <ref> [14] </ref>, inspired us to discuss dynamic programming explicitly in this paper. He also pointed out the survey [4] to us. Also, we are indebted to two anonymous referees for clarifications of the exposition, especially of the significance of submodularity and of Shannon's work [38].
Reference: [15] <author> R.L. Graham, </author> <title> Applications of the FKG Inequality and its Relatives, in Mathematical Programming: The State of the Art, </title> <editor> B. Korte, A. Bachem, M. Grotschel, eds., </editor> <publisher> Springer-Verlag, </publisher> <year> 1983, </year> <pages> pp. 115-131. </pages>
Reference-contexts: First, submodularity is directly related to the Fortuin-Kasteleyn-Ginibre (FKG) "correlation" inequalities, which generalize a basic inequality of Tchebycheff on mean values of functions (hence expected values of random variables). A fine survey of results with FKG-like inequalities is <ref> [15] </ref>. Second, submodularity is closely related to convexity. Book-length surveys by Fujishige [9] and Narayanan [28] review connections between submodularity and optimization (and even electrical network theory). The relationship between convexity and submodularity was neatly summarized by Lovasz with the following memorable definition and result. Definition 4.4.
Reference: [16] <author> G.H. Hardy, J.E. Littlewood, G. Polya, </author> <title> Inequalities, </title> <publisher> Cambridge University Press, </publisher> <year> 1934. </year>
Reference-contexts: The vector properties required here follow from this. 3.5. The Majorization Lattice and Density Lattice. We reproduce basic majorization concepts developed in the paper [34]. Majorization as defined here is an extension of the classical majorization of Muirhead and Hardy-Littlewood-Polya <ref> [16] </ref>, useful in the study of inequalities. The book by Marshall and Olkin [27] provides a very good account of the classical theory and its applications. The classical theory defines a majorization ordering on descendingly-ordered (or sometimes ascendingly-ordered) multisets, and although quite beautiful is also quite complex.
Reference: [17] <author> A.J. Hoffman. </author> <title> On Simple Linear Programming Problems, in Convexity, </title> <booktitle> Proc. Seventh Symposium in Pure Mathematics, </booktitle> <volume> Vol. VII, </volume> <editor> V. Klee, ed., </editor> <publisher> AMS, </publisher> <year> 1961, </year> <pages> pp. 317-327. </pages>
Reference-contexts: Submodularity has a long history in dynamic programming. By 1781, Monge had found a form of submodularity to be important in simplifying the transportation problem <ref> [17] </ref>. In 1970, Edmonds [6] related submodularity to matroids and greedily-solvable optimization problems.
Reference: [18] <author> D.A. Huffman, </author> <title> A method for the construction of minimum redundancy codes, </title> <booktitle> Proc. IRE, 40 (1951), </booktitle> <pages> pp. 1098-1101. </pages>
Reference-contexts: 5 5 i 1238 h 2 2 2 3 4 5 5 i 1259 h 2 2 3 3 3 4 4 i 1260 Table 2.2 Path-length sequences ` and their weighted path-length g w (`) for w = h 189 95 73 71 28 23 21 i Huffman's breakthrough <ref> [18] </ref> was to identify an efficient algorithm that finds an optimal tree, avoiding a search over the exponentially large space of trees. The algorithm repeatedly combines the two tree leaves with least weight, whose sum becomes the weight of a new leaf.
Reference: [19] <author> F.K. Hwang, </author> <title> Generalized Huffman Trees, </title> <journal> SIAM J. Appl. Math., </journal> <volume> 37 (1979), </volume> <pages> pp. 124-127. </pages>
Reference-contexts: For example, a direct corollary of Theorem 4.3 (using w = h 1 1 i) is that the function mapping a path-length sequence to its level of balance is submodular on the imbalance lattice. It would be interesting to extend the work here for the t-ary codes discussed in <ref> [19] </ref>. Majorization, we believe, can be exploited further in characterizing optimal codes. We have established that the imbalance ordering on tree path-length sequences ` is isomorphic to the majorization ordering on exponentiated tree path-length sequences x = 2 ` .
Reference: [20] <author> C.M. Klein, </author> <title> A submodular approach to discrete dynamic programming, </title> <journal> European J. Operational Research, </journal> <month> 80:1 (Jan. </month> <year> 1995), </year> <pages> pp. 145-155. </pages>
Reference-contexts: Burkard et al. have also compiled a comprehensive survey of many incarnations of the Monge condition in [4]. Recently Klein explored the connection between dynamic programming and submodu-larity <ref> [20] </ref>. Also Golin and Rote [14] developed dynamic programming algorithms for prefix codes when the codeword letters have differing costs, a useful case not handled by Huffman's algorithm; and they recently extended this work to exploit the Monge property. 6. Other Applications.
Reference: [21] <author> D.E. Knuth, </author> <title> Optimum Binary Search Trees, </title> <journal> Acta Informatica, </journal> <volume> 1 (1971), </volume> <pages> pp. 14-25. </pages>
Reference-contexts: By 1781, Monge had found a form of submodularity to be important in simplifying the transportation problem [17]. In 1970, Edmonds [6] related submodularity to matroids and greedily-solvable optimization problems. In 1980, Yao [42] generalized upon Knuth's famous O (n 2 ) algorithm for optimum binary search trees <ref> [21] </ref> by giving an O (n 2 ) algorithm for the dynamic programming problem c (i; i) = 0 w (i; j) w (i 0 ; j 0 ) (i 0 i j j 0 ) Yao called the final constraint the quadrangle inequality, noting that it implies the (inverse) triangle
Reference: [22] <author> D.E. Knuth, </author> <title> Dynamic Huffman Coding, </title> <journal> J. Algorithms, </journal> <volume> 6 (1985), </volume> <pages> pp. 163-180. </pages>
Reference-contexts: Dynamic Huffman coding is the strategy of repeatedly constructing the Huffman code for the input so far, and using it in transmitting the next input symbol. Knuth presented an efficient algorithm for dynamic Huffman coding in <ref> [22] </ref>, and his performance results for the algorithm show it consistently producing compression very near (though not surpassing) the compression attained with static Huffman code for the entire input. 26 D.S. PARKER AND P. <p> For example, adaptive coding algorithms can start at any point in the lattice, as long as both ends of the communication know which one. Rather than rely on the dynamic Huffman algorithm to derive reasonable operating points for the code, or rely on Knuth's `windowed' algorithm <ref> [22] </ref>, one can immediately begin with a mutually-agreed-upon, `reasonable' initial code (depending on the type of information being transmitted), and then adapt this code using some mutually-agreed-upon greedy algorithm for moving in the imbalance lattice. Acknowledgements. We are very grateful to Pierre Hasenfratz for insightful comments that improved this paper.
Reference: [23] <author> E. Lawler, </author> <title> Combinatorial Optimization: Networks & Matroids, </title> <address> Holt-Rinehart-Winston, </address> <year> 1976. </year>
Reference-contexts: For example, the objective functions commonly used in evaluating codes are submodular on this lattice. Submodular functions are closely related to convex functions (as we explain later; see Theorem 4.5), and are often easy to optimize <ref> [6, 9, 23, 24, 25] </ref>. Huffman coding gives a significant example of the importance of submodularity in algorithms. 2. Ordered Sequences, Rooted Binary Trees, and Huffman Codes. 2.1. Ordered Sequences.
Reference: [24] <author> E.L. Lawler, </author> <title> Submodular Functions & Polymatroid Optimization, in Combinatorial Optimization: Annotated Bibliographies, </title> <editor> A.H.G. Rinnooy Kan, M. O'hEigeartaigh, J.K. Lenstra, eds., J. </editor> <publisher> Wiley & Sons, </publisher> <year> 1985, </year> <pages> pp. 32-38. </pages>
Reference-contexts: For example, the objective functions commonly used in evaluating codes are submodular on this lattice. Submodular functions are closely related to convex functions (as we explain later; see Theorem 4.5), and are often easy to optimize <ref> [6, 9, 23, 24, 25] </ref>. Huffman coding gives a significant example of the importance of submodularity in algorithms. 2. Ordered Sequences, Rooted Binary Trees, and Huffman Codes. 2.1. Ordered Sequences. <p> Huffman coding gives another example of a dynamic programming problem that can be sped up considerably because the objective function is submodular over the solution space. Lawler <ref> [24] </ref> remarked: If a discrete optimization problem can be solved efficiently, it is quite likely that submodularity is responsible. In recent years there has been a growing appreciation of the fact that submodularity plays a pivotal role in discrete optimization, not unlike that of convexity in continuous optimization.
Reference: [25] <author> L. Lov asz, </author> <title> Submodular functions and convexity, in Mathematical Programming: The State of the Art, </title> <editor> B. Korte, A. Bachem, M. Grotschel, eds., </editor> <publisher> Springer-Verlag, </publisher> <year> 1983, </year> <pages> pp. 235-257. </pages>
Reference-contexts: For example, the objective functions commonly used in evaluating codes are submodular on this lattice. Submodular functions are closely related to convex functions (as we explain later; see Theorem 4.5), and are often easy to optimize <ref> [6, 9, 23, 24, 25] </ref>. Huffman coding gives a significant example of the importance of submodularity in algorithms. 2. Ordered Sequences, Rooted Binary Trees, and Huffman Codes. 2.1. Ordered Sequences. <p> Theorem 4.5. (Lovasz <ref> [25, p.249] </ref>) f : S ! &lt; + is submodular iff its greedy extension b f : &lt; + n ! &lt; + is convex. Proof. <p> The central inequality is due to submodularity. Putting 0 &lt; &lt; 1 and = (1 ) shows that b f is convex. Lovasz goes on <ref> [25, p.250-251] </ref> to point out that min f f (X) j X S g = min f b f (x) j x 2 [0; 1] n g and that as a consequence there is a polynomial-time algorithm to minimize f . <p> HUFFMAN CODING AS SUBMODULAR OPTIMIZATION OVER A LATTICE 21 Since monotonicity and convexity are related, Theorems 4.5-4.9 connect convexity, sub-modularity, Schur convexity, and majorization. There are actually many connections. See the survey [27, ch.6], in which submodular functions are called L-subadditive functions. Just as Lovasz showed for submodular functions <ref> [25] </ref>, Schur convex functions are closed under various operations: min, max, convolution, composition with convex functions, etc. [27, ch.3]. Theorem 4.5 is also reminiscent of symmetric gauge functions, which are Schur convex; see [27, p.96]. 5. Huffman Coding as Submodular Dynamic Programming. <p> Although weighted path length g w is not monotone on the imbalance lattice of trees, a monotone summary of weighted path length g w mon has the properties we need. In <ref> [25, p.241] </ref>, Lovasz stated the following definition and theorem for set lattices (easily proved for general lattices) about the `monotonization' of a function f : Definition 5.1.
Reference: [26] <author> U. Manber, </author> <title> Introduction to Algorithms, </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: CA 90245 (prasad@cp10.es.xerox.com). 1 The algorithm is closely related to sorting, in the sense that the sorted sequence of a sequence of integer values h x 1 x n i is obtainable directly from the optimal code tree for the values h 2 x 1 2 x n i (e.g., <ref> [26, p.335] </ref>). 2 D.S. PARKER AND P. RAM mathematical (and not purely procedural) understanding of coding will ultimately pay off in improved algorithms. The imbalance lattice and its imbalance ordering on trees depend on majorization in an essential way.
Reference: [27] <author> A.W. Marshall, I. Olkin, </author> <title> Inequalities: Theory of Majorization and Its Applications, </title> <publisher> Academic Press, </publisher> <year> 1979. </year>
Reference-contexts: RAM mathematical (and not purely procedural) understanding of coding will ultimately pay off in improved algorithms. The imbalance lattice and its imbalance ordering on trees depend on majorization in an essential way. Majorization is an important ordering on sequences that has many applications in pure and applied mathematics <ref> [27] </ref>. We have related it to greedy algorithms directly [33]. Earlier majorization was recognized as an important property of the internal node weights produced by the Huffman algorithm [13, 32], and in this work we go further to clarify its pervasive role. <p> The Majorization Lattice and Density Lattice. We reproduce basic majorization concepts developed in the paper [34]. Majorization as defined here is an extension of the classical majorization of Muirhead and Hardy-Littlewood-Polya [16], useful in the study of inequalities. The book by Marshall and Olkin <ref> [27] </ref> provides a very good account of the classical theory and its applications. The classical theory defines a majorization ordering on descendingly-ordered (or sometimes ascendingly-ordered) multisets, and although quite beautiful is also quite complex. We have transplanted the theory to rely only on linear algebra and convexity. <p> The classical theory defines a majorization ordering on descendingly-ordered (or sometimes ascendingly-ordered) multisets, and although quite beautiful is also quite complex. We have transplanted the theory to rely only on linear algebra and convexity. Thus the definitions in this section are ours, and the results vary from those in <ref> [27] </ref>. HUFFMAN CODING AS SUBMODULAR OPTIMIZATION OVER A LATTICE 11 Definition 3.13. <p> (log 2 (x)) = i Notice that G w is convex on &lt; + n , as its Hessian r 2 G w = ( @ 2 G w (x) @x i @x j ) = 1 ln (2) diag ( w i x 2 ) is positive semidefinite there <ref> [27, p.448] </ref>. (recall we are assuming all weights are positive). G w is actually also submodular on the majorization lattice. We prove this directly now, and show later how submodularity can be established using only vector calculus. Theorem 4.2. <p> Vector lattices, also called Riesz spaces, can be more `natural' than set lattices in some ways. For example, submodularity has a natural characterization: Theorem 4.6. (Lorentz <ref> [27, p.150] </ref>) When twice differentiable, f is submodular on the vector lattice h&lt; + n ; vec ; min vec ; max vec i @ 2 f 0 (i 6= j; 1 i; j n): 20 D.S. PARKER AND P. RAM Proof. Essentially definition. <p> HUFFMAN CODING AS SUBMODULAR OPTIMIZATION OVER A LATTICE 21 Since monotonicity and convexity are related, Theorems 4.5-4.9 connect convexity, sub-modularity, Schur convexity, and majorization. There are actually many connections. See the survey <ref> [27, ch.6] </ref>, in which submodular functions are called L-subadditive functions. Just as Lovasz showed for submodular functions [25], Schur convex functions are closed under various operations: min, max, convolution, composition with convex functions, etc. [27, ch.3]. <p> There are actually many connections. See the survey [27, ch.6], in which submodular functions are called L-subadditive functions. Just as Lovasz showed for submodular functions [25], Schur convex functions are closed under various operations: min, max, convolution, composition with convex functions, etc. <ref> [27, ch.3] </ref>. Theorem 4.5 is also reminiscent of symmetric gauge functions, which are Schur convex; see [27, p.96]. 5. Huffman Coding as Submodular Dynamic Programming. The results of the previous sections can now be applied to Huffman coding. 5.1. Non-monotonicity of Weighted Path-Length over the Lattices. <p> Just as Lovasz showed for submodular functions [25], Schur convex functions are closed under various operations: min, max, convolution, composition with convex functions, etc. [27, ch.3]. Theorem 4.5 is also reminiscent of symmetric gauge functions, which are Schur convex; see <ref> [27, p.96] </ref>. 5. Huffman Coding as Submodular Dynamic Programming. The results of the previous sections can now be applied to Huffman coding. 5.1. Non-monotonicity of Weighted Path-Length over the Lattices.
Reference: [28] <author> H. Narayanan, </author> <title> Submodular functions and electrical networks, </title> <publisher> North-Holland Elsevier, </publisher> <year> 1997. </year>
Reference-contexts: A fine survey of results with FKG-like inequalities is [15]. Second, submodularity is closely related to convexity. Book-length surveys by Fujishige [9] and Narayanan <ref> [28] </ref> review connections between submodularity and optimization (and even electrical network theory). The relationship between convexity and submodularity was neatly summarized by Lovasz with the following memorable definition and result. Definition 4.4.
Reference: [29] <author> A. Ostrowski, Sur quelques applications des fonctions convexes et concaves au sens de I. Schur (offert en homage a P. Montel), J. </author> <note> Math. Pures Appl., </note> <month> 31 </month> <year> (1952), </year> <pages> pp. 253-292. </pages>
Reference: [30] <author> J.M. Pallo, </author> <title> Enumerating, ranking and unranking binary trees, </title> <journal> Computer Journal, </journal> <month> 29:2 (April </month> <year> 1986), </year> <pages> pp. 171-175. </pages>
Reference-contexts: The idea of using using lattices in coding dates back at least to Shannon in 1950 [38]. However, we have not found the lattice characterization of tree imbalance elsewhere. Following considerable work in the early 1980's on enumeration of trees, Pallo classified trees by their rotational structure (e.g., <ref> [30, 31] </ref>), and showed that they then form a lattice. Our work differs from Pallo's in that we classify trees by their path-length (imbalance) structure. 3.1. Important Properties of Tree Path-Length Sequences. Theorem 3.1.
Reference: [31] <author> J.M. Pallo, </author> <title> Some properties of the rotation lattice of binary trees, </title> <journal> Computer Journal, </journal> <month> 31:6 (Dec. </month> <year> 1988), </year> <pages> pp. 564-565. </pages>
Reference-contexts: The idea of using using lattices in coding dates back at least to Shannon in 1950 [38]. However, we have not found the lattice characterization of tree imbalance elsewhere. Following considerable work in the early 1980's on enumeration of trees, Pallo classified trees by their rotational structure (e.g., <ref> [30, 31] </ref>), and showed that they then form a lattice. Our work differs from Pallo's in that we classify trees by their path-length (imbalance) structure. 3.1. Important Properties of Tree Path-Length Sequences. Theorem 3.1.
Reference: [32] <author> D.S. Parker, </author> <title> Conditions for Optimality of the Huffman Algorithm, </title> <journal> SIAM J. Comput., </journal> <month> 9:3 (August </month> <year> 1980), </year> <pages> pp. 470-489. </pages>
Reference-contexts: Majorization is an important ordering on sequences that has many applications in pure and applied mathematics [27]. We have related it to greedy algorithms directly [33]. Earlier majorization was recognized as an important property of the internal node weights produced by the Huffman algorithm <ref> [13, 32] </ref>, and in this work we go further to clarify its pervasive role. By viewing the space of trees as a lattice, a variety of new theorems and algorithms become possible. For example, the objective functions commonly used in evaluating codes are submodular on this lattice. <p> Negative entropy is an important example of such a function; related functions are discussed in <ref> [32] </ref>. Furthermore, the methods developed above hold out hope for entirely new approaches to Huffman coding. We sketch two possibilities. HUFFMAN CODING AS SUBMODULAR OPTIMIZATION OVER A LATTICE 25 6.1. Continuous Approximation of Huffman Codes.
Reference: [33] <author> D.S. Parker, P. Ram, Greed and Majorization, </author> <month> November </month> <year> 1994. </year> <note> Issued as Technical Report CSD-960003, </note> <institution> UCLA Computer Science Dept., </institution> <month> March </month> <year> 1996. </year>
Reference-contexts: The imbalance lattice and its imbalance ordering on trees depend on majorization in an essential way. Majorization is an important ordering on sequences that has many applications in pure and applied mathematics [27]. We have related it to greedy algorithms directly <ref> [33] </ref>. Earlier majorization was recognized as an important property of the internal node weights produced by the Huffman algorithm [13, 32], and in this work we go further to clarify its pervasive role.
Reference: [34] <author> D.S. Parker, P. Ram, </author> <title> A Linear Algebraic Reconstruction of Majorization, </title> <type> Technical Report CSD-970036, </type> <institution> UCLA Computer Science Dept., </institution> <month> September </month> <year> 1997. </year>
Reference-contexts: The one-dimensional algebra h&lt; + ; ; min; maxi is a distributive lattice. The vector properties required here follow from this. 3.5. The Majorization Lattice and Density Lattice. We reproduce basic majorization concepts developed in the paper <ref> [34] </ref>. Majorization as defined here is an extension of the classical majorization of Muirhead and Hardy-Littlewood-Polya [16], useful in the study of inequalities. The book by Marshall and Olkin [27] provides a very good account of the classical theory and its applications. <p> f is differentiable, f is monotone on the vector lattice iff rf (x) = h @f=@x 1 @f=@x n i vec 0; which implies @F (@ x) @x i+1 @ (F (@ x)) = @x i This rederives the result that @F=@z i @F=@z i+1 when F is Schur convex <ref> [34] </ref>. HUFFMAN CODING AS SUBMODULAR OPTIMIZATION OVER A LATTICE 21 Since monotonicity and convexity are related, Theorems 4.5-4.9 connect convexity, sub-modularity, Schur convexity, and majorization. There are actually many connections. See the survey [27, ch.6], in which submodular functions are called L-subadditive functions.
Reference: [35] <author> U. Pferschy, R. Rudolf, G.J. Woeginger, </author> <title> Monge matrices make maximization manageable, </title> <journal> Operations Research Letters, </journal> <volume> 16 (1994), </volume> <pages> pp. 245-254. </pages>
Reference-contexts: Results from exploiting the quadrangle inequality in dynamic programming appear in <ref> [2, 7, 35] </ref> for problems ranging from DNA sequencing to minimum cost matching.
Reference: [36] <author> G.-C. Rota, </author> <title> On the Foundations of Combinatory Theory I. Theory of Mobius Functions, </title> <journal> Z. Wahrscheinlichkeitstheorie, </journal> <volume> 2 (1964), </volume> <pages> pp. 340-368. </pages>
Reference-contexts: The theory of Mobius inversion <ref> [36] </ref> gives a generalized notion of differential on partially-ordered domains (although here we consider only totally-ordered sequences). We can think of R as an integral operator (which transforms a sequence to its left-to-right "integral"), with @ as its inverse differential operator. Theorem 3.14.
Reference: [37] <author> I. </author> <title> Schur, Uber eine Klasse von Mittelbildungen mit Anwendungen auf die Determinantenthe-orie, </title> <journal> Sitzungsber. Berl. Math. Ges., </journal> <volume> 22 (1923), </volume> <pages> pp. 9-20. </pages>
Reference: [38] <author> C.E. Shannon, </author> <title> The Lattice Theory of Information, </title> <journal> Proc. IRE Trans. Information Theory, </journal> <note> 1 (1950). Reprinted in Claude Elwood Shannon: Collected Papers, IEEE Press, </note> <year> 1993. </year>
Reference-contexts: In this section we show ternary balancing exchanges give an imbalance ordering on binary trees that defines a lattice. The idea of using using lattices in coding dates back at least to Shannon in 1950 <ref> [38] </ref>. However, we have not found the lattice characterization of tree imbalance elsewhere. Following considerable work in the early 1980's on enumeration of trees, Pallo classified trees by their rotational structure (e.g., [30, 31]), and showed that they then form a lattice. <p> He also pointed out the survey [4] to us. Also, we are indebted to two anonymous referees for clarifications of the exposition, especially of the significance of submodularity and of Shannon's work <ref> [38] </ref>.
Reference: [39] <author> N.J.A. Sloane, S. Plouffe, </author> <title> The Encyclopedia of Integer Sequences, </title> <publisher> Academic Press, </publisher> <year> 1995. </year>
Reference-contexts: Table 2.1 a lexicographic tabulation of all possible sequences for 1 n 7, along with T n , the total number of inequivalent sequences of length n. T n is enumerated as sequence M0710 (A002572) in <ref> [39] </ref>. An upper bound on T n can be obtained from the Catalan number C n , which computes the number of unordered binary trees: for n 3, T n 1 2 C n 2 n3 .
Reference: [40] <author> J.S. Vitter, </author> <title> Design and Analysis of Dynamic Huffman Codes, </title> <journal> J. ACM, </journal> <month> 34:4 (October </month> <year> 1987), </year> <pages> pp. 825-845. </pages>
Reference-contexts: Knuth presented an efficient algorithm for dynamic Huffman coding in [22], and his performance results for the algorithm show it consistently producing compression very near (though not surpassing) the compression attained with static Huffman code for the entire input. 26 D.S. PARKER AND P. RAM Vitter <ref> [40, 41] </ref> then developed a dynamic Huffman algorithm that improves on Knuth's in the following way: rather than simply revise the Huffman tree after each input symbol, Vitter also finds a new Huffman tree of minimal external path length P i ` i and height max i ` i .
Reference: [41] <author> J.S. Vitter, </author> <title> Algorithm 673: Dynamic Huffman Coding, </title> <journal> ACM TOMS, </journal> <month> 15:2 (June </month> <year> 1989), </year> <pages> pp. 158-167. </pages>
Reference-contexts: Knuth presented an efficient algorithm for dynamic Huffman coding in [22], and his performance results for the algorithm show it consistently producing compression very near (though not surpassing) the compression attained with static Huffman code for the entire input. 26 D.S. PARKER AND P. RAM Vitter <ref> [40, 41] </ref> then developed a dynamic Huffman algorithm that improves on Knuth's in the following way: rather than simply revise the Huffman tree after each input symbol, Vitter also finds a new Huffman tree of minimal external path length P i ` i and height max i ` i .
Reference: [42] <author> F.F. Yao, </author> <title> Efficient dynamic programming using quadrangle inequalities, </title> <booktitle> Proc. 12th Annual ACM Symp. on Theory of Computing, </booktitle> <year> 1980, </year> <pages> pp. 429-435. </pages>
Reference-contexts: Submodularity has a long history in dynamic programming. By 1781, Monge had found a form of submodularity to be important in simplifying the transportation problem [17]. In 1970, Edmonds [6] related submodularity to matroids and greedily-solvable optimization problems. In 1980, Yao <ref> [42] </ref> generalized upon Knuth's famous O (n 2 ) algorithm for optimum binary search trees [21] by giving an O (n 2 ) algorithm for the dynamic programming problem c (i; i) = 0 w (i; j) w (i 0 ; j 0 ) (i 0 i j j 0 )
References-found: 42

