URL: http://www.cs.princeton.edu/~jps/papers/scope-consistency.ps
Refering-URL: http://www.cs.princeton.edu/~jps/papers/appls-arch.html
Root-URL: http://www.cs.princeton.edu
Title: Scope Consistency A Bridge between Release Consistency and Entry Consistency  
Author: Liviu Iftode, Jaswinder Pal Singh and Kai Li 
Address: Princeton, NJ 08544  
Affiliation: Department of Computer Science, Princeton University,  
Date: June 1996  
Note: To appear in Proceedings of the 8th Anual ACM Symposium on Parallel Algorithms a nd Architectures,  
Abstract: This paper proposes a new consistency model for shared virtual memory, called Scope Consistency (ScC), which offers most of the potential performance advantages of the EC model without requiring explicit bindings between data and synchronization variables. Instead, ScC dynamically detects the bindings implied by the programmer allowing a programming interface as simple as that of RC or LRC in most cases. We compare ScC with Automatic Update Release Consistency (AURC), a modified LRC protocol which takes advantage of new network interfaces that provide automatic update support. AURC already improves performance substantially over the all-software LRC. For three of the five applications we used, ScC further improves the speedups achieved by AURC by about 10%. We also show how ScC may be used without any hardware support. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. V. Adve, A. L. Cox, S. Dwarkadas, R. Rajamony, and W. Zwaenepoel. </author> <title> A Comparison of Entry Consistency and Lazy Release Consistency Implementation. </title> <booktitle> In The 2nd IEEE Symposium on High-Performance Computer Architecture, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: The program will behave correctly since the new value for X 0 is not needed in computing the interaction between X 2 and X 1 . Hardware support LRC is a consistency model which can be implemented completely in software using different all-software write collection schemes <ref> [1] </ref>. AURC is a variant of LRC which takes advantage of automatic update support provided in the new network interfaces to merge the updates from multiple writers [10]. ScC needs the same kind of hardware support as AURC in order to guarantee its performance benefit. <p> We can say that EC binds data explicitly to locks while the ScC binds dynamic memory references implicitly to locks. Global Synchronization. EC has difficulty including global synchronization in the model <ref> [1] </ref>. The alternatives are: (1) not to bind data to barriers at all; (2) to have explicit binding as for locks; (3) to bind the entire address space. The first approach substantially limits the intuition of shared memory. <p> On the other hand, although the communication volume may be higher in ScC than in EC due to page fragmentation, the page size transfer in ScC allows prefetching to occur <ref> [1] </ref>. Let us now examine how ScC might be implemented using the automatic update support exploited by the AURC protocol. <p> The Memory Channel [8] allows remote memory to be mapped into the local virtual address space and have writes propagated automatically, but without a corresponding local memory mapping. LRC and EC have been compared in recent studies <ref> [23, 1] </ref>. Although their results cannot be directly compared with ours, since they compared LRC with EC while we compared AURC with ScC, a few points are worth noting.
Reference: [2] <author> B.N. Bershad, M.J. Zekauskas, </author> <title> and W.A. </title> <booktitle> Sawdon. The Midway Distributed Shared Memory System. In Proceedings of the IEEE COMPCON '93 Conference, </booktitle> <month> February </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Several relaxed consistency models <ref> [7, 6, 14, 2] </ref> have been proposed to improve the performance of shared virtual memory systems. These relaxed consistency models alleviate the "false sharing" and extra communication problems caused by using a page as the granularity of communication and coherence. However, models with increasing effectiveness also increase programming complexity. <p> The consistency model can be further relaxed by exploiting the association of shared data objects with synchronization variables (locks or barriers). For example, Entry consistency (EC) <ref> [2] </ref> lets the acquire operation obtain updates only to the data that have been explicitly associated with the acquired synchronization variable | not all the shared data that have been modified by other processors | and thus reduces unnecessary update propagations compared to LRC or AURC. <p> Adding more locks is, however, an alternative solution in ScC, and its usage would be very similar to EC except for binding. 3 Comparisons with LRC and EC Scope Consistency is a relaxed page-based consistency model situated between RC [7] and EC <ref> [2] </ref> (see also Figure 5). 3.1 ScC versus LRC Both ScC and LRC are page-based shared virtual memory systems which don't require explicit binding of data to synchronization. However, ScC assumes an implicit binding determined from memory accesses. <p> Both schemes relax the memory consistency model by taking advantage of the relationship between data and synchronization. Both schemes are particularly effective for applications based on extensive use of point-to-point (lock) synchronization. However the solutions proposed by the two schemes are quite different. Binding. In EC <ref> [2] </ref>, the binding between data objects and synchronization objects is specified explicitly by the programmer. In fact, the solution proposed by EC goes beyond page-based shared memory systems, approaching object-based or region-based shared memory models. <p> In fact, the solution proposed by EC goes beyond page-based shared memory systems, approaching object-based or region-based shared memory models. EC requires explicit associations of data with synchronizations because the protocol relies on this information to identify which updates must be propagated and when. In a system like Midway <ref> [2] </ref> which supports both the EC and RC models, the default model is RC. Only the data objects explicitly associated with synchronization variables can be treated using the EC. <p> Small non-contiguous bindings are more efficient because they require less communication and/or less processing. But they are also more difficult to program since the programmer must establish many more bindings for each lock. The communication granularity depends on how write collection is implemented. Midway <ref> [2] </ref> uses software dirty bits, so the amount of communication is proportional to the size of the updated data (not like diffs in LRC). In ScC the binding granularity is determined by the write detection granularity, which is of word size for both automatic update and all-software write detection schemes. <p> Relaxed consistency models [7] allow shared virtual memory to reduce the cost of false sharing. New coherence protocols that improve the performance of release consistency in this context include lazy release consistency [14], and entry consistency <ref> [2] </ref>, in which shared data are explicitly associated with some synchronization variable. The TreadMarks library [13] is an example of state-of-the-art implementations of shared virtual memory on stock hardware. It uses LRC and allows for multiple writers to a page.
Reference: [3] <author> R. Bisiani and M. Ravishankar. </author> <title> PLUS: A Distributed Shared-Memory System. </title> <booktitle> In Proceedings of the 17th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 115-124, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Blizzard-S [19] rewrites an existing executable file to insert a state table lookup before every shared-memory reference. This technique works well in the presence of fine-grain sharing. Alternative directory-based protocols for memory mapped network interfaces were proposed for Cashmere [15]. The Plus <ref> [3] </ref>, Galactica Net [12], Merlin [18] and its successor SESAME [21], systems implement hardware-based shared memory using a sort of write-through mechanism which is similar in some ways to automatic update.
Reference: [4] <author> M. Blumrich, K. Li, R. Alpert, C. Dubnicki, E. Felten, and J. Sandberg. </author> <title> A Virtual Memory Mapped Network Interface for the SHRIMP Multicomputer. </title> <booktitle> In Proceedings of the 21st Annual Symposium on Computer Architecture, </booktitle> <pages> pages 142-153, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: LRC (or RC) can be modified to take advantage of new network interfaces that provide hardware support for word-level automatic updates of remote copies of shared data upon writes <ref> [8, 4] </ref>. Automatic Update Release Consistency (AURC) [10] is one such protocol that improves performance substantially over all-software LRC, by using the automatic update mechanism provided in the SHRIMP network interface [4]. <p> Automatic Update Release Consistency (AURC) [10] is one such protocol that improves performance substantially over all-software LRC, by using the automatic update mechanism provided in the SHRIMP network interface <ref> [4] </ref>. The additional programming requirements imposed by these RC-based protocols are small: Application programs simply need to mark all their synchronizations as acquire or release operations as appropriate. The consistency model can be further relaxed by exploiting the association of shared data objects with synchronization variables (locks or barriers). <p> Such support is provided in several recent network interfaces to implement memory-mapped communication (e.g. SHRIMP <ref> [4] </ref>, DEC Memory Channel [8]), and has been found to be very useful for efficiently implementing shared virtual memory [10, 15]. To understand the performance implications of this approach, we implemented ScC and AURC protocols within the TangoLite simulation framework [9].
Reference: [5] <author> J. B. Carter, J. K. Bennett, and Willy Zwaenepoel. </author> <title> Techniques for Reducing Consistency-Related Communication in Distributed Shared-Memory Systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 13(3) </volume> <pages> 205-244, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: The challenge is to define models that maximize the performance benefits of relaxed consistency while minimizing the additional programming requirements. Protocols based on Release Consistency (RC) [7] are accepted to offer a reasonable tradeoff between performance and programming complexity for shared virtual memory <ref> [5] </ref>. RC guarantees memory consistency only at synchronization points, which are marked as acquire or release operations. Updates to shared data are globally performed at each release operation.
Reference: [6] <author> J.B. Carter, J.K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and Performance of Munin. </title> <booktitle> In Proceedings of the Thirteenth Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Several relaxed consistency models <ref> [7, 6, 14, 2] </ref> have been proposed to improve the performance of shared virtual memory systems. These relaxed consistency models alleviate the "false sharing" and extra communication problems caused by using a page as the granularity of communication and coherence. However, models with increasing effectiveness also increase programming complexity.
Reference: [7] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Several relaxed consistency models <ref> [7, 6, 14, 2] </ref> have been proposed to improve the performance of shared virtual memory systems. These relaxed consistency models alleviate the "false sharing" and extra communication problems caused by using a page as the granularity of communication and coherence. However, models with increasing effectiveness also increase programming complexity. <p> However, models with increasing effectiveness also increase programming complexity. The challenge is to define models that maximize the performance benefits of relaxed consistency while minimizing the additional programming requirements. Protocols based on Release Consistency (RC) <ref> [7] </ref> are accepted to offer a reasonable tradeoff between performance and programming complexity for shared virtual memory [5]. RC guarantees memory consistency only at synchronization points, which are marked as acquire or release operations. Updates to shared data are globally performed at each release operation. <p> Adding more locks is, however, an alternative solution in ScC, and its usage would be very similar to EC except for binding. 3 Comparisons with LRC and EC Scope Consistency is a relaxed page-based consistency model situated between RC <ref> [7] </ref> and EC [2] (see also Figure 5). 3.1 ScC versus LRC Both ScC and LRC are page-based shared virtual memory systems which don't require explicit binding of data to synchronization. However, ScC assumes an implicit binding determined from memory accesses. <p> It is difficult to speak of the performance improvement due to ScC in this context except by comparing with the original LRC. 7 Related Work The concept of shared virtual memory was proposed in Kai Li's Ph.D. thesis in 1986 [16, 17]. Relaxed consistency models <ref> [7] </ref> allow shared virtual memory to reduce the cost of false sharing. New coherence protocols that improve the performance of release consistency in this context include lazy release consistency [14], and entry consistency [2], in which shared data are explicitly associated with some synchronization variable.
Reference: [8] <author> Richard Gillett. </author> <title> Memory Channel Network for PCI. </title> <booktitle> In Proceedings of Hot Interconnects '95 Symposium, </booktitle> <month> August </month> <year> 1995. </year>
Reference-contexts: LRC (or RC) can be modified to take advantage of new network interfaces that provide hardware support for word-level automatic updates of remote copies of shared data upon writes <ref> [8, 4] </ref>. Automatic Update Release Consistency (AURC) [10] is one such protocol that improves performance substantially over all-software LRC, by using the automatic update mechanism provided in the SHRIMP network interface [4]. <p> Such support is provided in several recent network interfaces to implement memory-mapped communication (e.g. SHRIMP [4], DEC Memory Channel <ref> [8] </ref>), and has been found to be very useful for efficiently implementing shared virtual memory [10, 15]. To understand the performance implications of this approach, we implemented ScC and AURC protocols within the TangoLite simulation framework [9]. <p> Alternative directory-based protocols for memory mapped network interfaces were proposed for Cashmere [15]. The Plus [3], Galactica Net [12], Merlin [18] and its successor SESAME [21], systems implement hardware-based shared memory using a sort of write-through mechanism which is similar in some ways to automatic update. The Memory Channel <ref> [8] </ref> allows remote memory to be mapped into the local virtual address space and have writes propagated automatically, but without a corresponding local memory mapping. LRC and EC have been compared in recent studies [23, 1].
Reference: [9] <author> S.A. Herrod. </author> <title> TangoLite; A Multiprocessor Simulation Environment. </title> <institution> Computer Systems Laboratory, Stanford University, </institution> <year> 1994. </year>
Reference-contexts: SHRIMP [4], DEC Memory Channel [8]), and has been found to be very useful for efficiently implementing shared virtual memory [10, 15]. To understand the performance implications of this approach, we implemented ScC and AURC protocols within the TangoLite simulation framework <ref> [9] </ref>. We conducted detailed simulation studies with five real applications, as well as a synthetic benchmark designed to emphasize a communication pattern that illustrates the benefits of ScC. ScC reduces the number of page faults in the synthetic benchmark by 45%. <p> The simulator interfaces with the TangoLite execution-driven reference generator <ref> [9] </ref>. The architectural parameters (table 1) we use are essentially those of the SHRIMP multicomputer, which has a network interface that supports automatic update. The simulator handles contention in detail both at the memory bus level as well as in the network interface between the incoming and outgoing traffic.
Reference: [10] <author> L. Iftode, C. Dubnicki, E. W. Felten, and Kai Li. </author> <title> Improving Release-Consistent Shared Virtual Memory using Automatic Update. </title> <booktitle> In The 2nd IEEE Symposium on High-Performance Computer Architecture, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: LRC (or RC) can be modified to take advantage of new network interfaces that provide hardware support for word-level automatic updates of remote copies of shared data upon writes [8, 4]. Automatic Update Release Consistency (AURC) <ref> [10] </ref> is one such protocol that improves performance substantially over all-software LRC, by using the automatic update mechanism provided in the SHRIMP network interface [4]. <p> Such support is provided in several recent network interfaces to implement memory-mapped communication (e.g. SHRIMP [4], DEC Memory Channel [8]), and has been found to be very useful for efficiently implementing shared virtual memory <ref> [10, 15] </ref>. To understand the performance implications of this approach, we implemented ScC and AURC protocols within the TangoLite simulation framework [9]. We conducted detailed simulation studies with five real applications, as well as a synthetic benchmark designed to emphasize a communication pattern that illustrates the benefits of ScC. <p> Hardware support LRC is a consistency model which can be implemented completely in software using different all-software write collection schemes [1]. AURC is a variant of LRC which takes advantage of automatic update support provided in the new network interfaces to merge the updates from multiple writers <ref> [10] </ref>. ScC needs the same kind of hardware support as AURC in order to guarantee its performance benefit. The reason is that while ScC postpones some page invalidations, all updates must be eventually performed at the next barrier (global scope) if not earlier. <p> This AU feature provides adequate support for a shared virtual memory implementation. AURC is such a protocol designed to support LRC in this environment. We now briefly describe the AURC protocol. For details see <ref> [10] </ref>. The basic approach in AURC is to use automatic update mappings to merge updates from multiple writers on the same page into the home's copy of that page (see Figure 8). <p> Contention in the network itself is not simulated. We compare the performance of ScC against AURC as implemented on a SHRIMP-like machine. Results from a previous studies <ref> [10, 11] </ref> show that AURC substantially outperforms the all-software LRC protocol running on the same hardware (the latter does not exploit the automatic update feature). <p> The effects on overall speedup are diminished to about 12% because of the increase in protocol overhead and communication. 6 All-software Implementation for ScC The hardware support for automatic update is very valuable for shared virtual memory independent of scope consistency <ref> [10] </ref>, and several shared virtual memory systems have been built using this feature (see [10, 15, 11]). We have seen that scope consistency can be easily built on top of this in software, and has performance benefits. <p> are diminished to about 12% because of the increase in protocol overhead and communication. 6 All-software Implementation for ScC The hardware support for automatic update is very valuable for shared virtual memory independent of scope consistency [10], and several shared virtual memory systems have been built using this feature (see <ref> [10, 15, 11] </ref>). We have seen that scope consistency can be easily built on top of this in software, and has performance benefits. However, it is also interesting to see if ScC can be built on top of an all-software protocol, without automatic update support.
Reference: [11] <author> L. Iftode, J. P. Singh, and Kai Li. </author> <title> Understanding Application Performance on Shared Virtual Memory. </title> <booktitle> In Proceedings of the 23rd Annual Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: Contention in the network itself is not simulated. We compare the performance of ScC against AURC as implemented on a SHRIMP-like machine. Results from a previous studies <ref> [10, 11] </ref> show that AURC substantially outperforms the all-software LRC protocol running on the same hardware (the latter does not exploit the automatic update feature). <p> are diminished to about 12% because of the increase in protocol overhead and communication. 6 All-software Implementation for ScC The hardware support for automatic update is very valuable for shared virtual memory independent of scope consistency [10], and several shared virtual memory systems have been built using this feature (see <ref> [10, 15, 11] </ref>). We have seen that scope consistency can be easily built on top of this in software, and has performance benefits. However, it is also interesting to see if ScC can be built on top of an all-software protocol, without automatic update support.
Reference: [12] <author> Andrew W. Wilson Jr. Richard P. LaRowe Jr. and Marc J. Teller. </author> <title> Hardware Assist for Distributed Shared Memory. </title> <booktitle> In Proceedings of 13th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 246-255, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Blizzard-S [19] rewrites an existing executable file to insert a state table lookup before every shared-memory reference. This technique works well in the presence of fine-grain sharing. Alternative directory-based protocols for memory mapped network interfaces were proposed for Cashmere [15]. The Plus [3], Galactica Net <ref> [12] </ref>, Merlin [18] and its successor SESAME [21], systems implement hardware-based shared memory using a sort of write-through mechanism which is similar in some ways to automatic update.
Reference: [13] <author> P. Keleher, A.L. Cox, S. Dwarkadas, and W. Zwaenepoel. TreadMarks: </author> <title> Distributed Shared Memory on Standard Workstations and Operating Systems. </title> <booktitle> In Proceedings of the Winter USENIX Conference, </booktitle> <pages> pages 115-132, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: Therefore, by reducing the number of page invalidations the update cost is also proportionally reduced. A diff-based all software LRC scheme like the one used in TreadMarks <ref> [13] </ref> doesn't have the same property of keeping a home copy up-to-date. Updates are embedded in diffs which have to be transferred eventually and applied on every local copy. Therefore, postponing some invalidations and saving some page faults doesn't ultimately save communication cost. <p> That is, all copies of a page map to a fixed home, and the home copy is always kept up-to-date by automatic update. This replaces more expensive all-software solutions for write detection used in LRC implementations, like diff computation <ref> [13] </ref> or software dirty bit schemes [23]. This mechanism only keeps the home copy up to date. To keep the other copies coherent according to the consistency model, a software protocol must supplement the hardware support with an invalidation-based scheme. <p> Then, when a processor which is not the home has a page miss due to the invalidation, the local copy is updated by transferring the entire page from the home on demand. AURC uses vector timestamps <ref> [13] </ref> to describe synchronization intervals and the page version. As in LRC, timestamps are built from local counters incremented on each interval, where an interval for a processor is the period between two local synchronization release events. Timestamping allows selective page invalidation and implicit prefetch detection. <p> New coherence protocols that improve the performance of release consistency in this context include lazy release consistency [14], and entry consistency [2], in which shared data are explicitly associated with some synchronization variable. The TreadMarks library <ref> [13] </ref> is an example of state-of-the-art implementations of shared virtual memory on stock hardware. It uses LRC and allows for multiple writers to a page. This implementation provides respectable performance in the absence of fine-grained sharing.
Reference: [14] <author> P. Keleher, A.L. Cox, and W. Zwaenepoel. </author> <title> Lazy Consistency for Software Distributed Shared Memory. </title> <booktitle> In Proceedings of the 19th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Several relaxed consistency models <ref> [7, 6, 14, 2] </ref> have been proposed to improve the performance of shared virtual memory systems. These relaxed consistency models alleviate the "false sharing" and extra communication problems caused by using a page as the granularity of communication and coherence. However, models with increasing effectiveness also increase programming complexity. <p> RC guarantees memory consistency only at synchronization points, which are marked as acquire or release operations. Updates to shared data are globally performed at each release operation. Lazy Release Consistency (LRC) <ref> [14] </ref> is a relaxed implementation of RC in which coherence actions are postponed from the release to the next acquire operation. <p> Example II LRC to ScC. P 0 updates X first and then Y ; Y is updated in the critical section guarded by L 1 lock, while X is updated outside it. P 1 acquires L 1 after P 0 has released it. In LRC model <ref> [14] </ref>, after acquiring L 1 , P 1 is guaranteed to see all writes which occurred at P 0 before the L 1 release, so the new value of both X and Y are guaranteed to be visible. <p> Relaxed consistency models [7] allow shared virtual memory to reduce the cost of false sharing. New coherence protocols that improve the performance of release consistency in this context include lazy release consistency <ref> [14] </ref>, and entry consistency [2], in which shared data are explicitly associated with some synchronization variable. The TreadMarks library [13] is an example of state-of-the-art implementations of shared virtual memory on stock hardware. It uses LRC and allows for multiple writers to a page.
Reference: [15] <author> L. I. Kontothanassis and M. L. Scott. </author> <title> Using Memory-Mapped Network Interfaces to Improve t he Performance of Distributed Shared Memory. </title> <booktitle> In The 2nd IEEE Symposium on High-Performance Computer Architecture, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: Such support is provided in several recent network interfaces to implement memory-mapped communication (e.g. SHRIMP [4], DEC Memory Channel [8]), and has been found to be very useful for efficiently implementing shared virtual memory <ref> [10, 15] </ref>. To understand the performance implications of this approach, we implemented ScC and AURC protocols within the TangoLite simulation framework [9]. We conducted detailed simulation studies with five real applications, as well as a synthetic benchmark designed to emphasize a communication pattern that illustrates the benefits of ScC. <p> are diminished to about 12% because of the increase in protocol overhead and communication. 6 All-software Implementation for ScC The hardware support for automatic update is very valuable for shared virtual memory independent of scope consistency [10], and several shared virtual memory systems have been built using this feature (see <ref> [10, 15, 11] </ref>). We have seen that scope consistency can be easily built on top of this in software, and has performance benefits. However, it is also interesting to see if ScC can be built on top of an all-software protocol, without automatic update support. <p> Blizzard-S [19] rewrites an existing executable file to insert a state table lookup before every shared-memory reference. This technique works well in the presence of fine-grain sharing. Alternative directory-based protocols for memory mapped network interfaces were proposed for Cashmere <ref> [15] </ref>. The Plus [3], Galactica Net [12], Merlin [18] and its successor SESAME [21], systems implement hardware-based shared memory using a sort of write-through mechanism which is similar in some ways to automatic update.
Reference: [16] <author> K. Li. </author> <title> Shared Virtual Memory on Loosely-coupled Multiprocessors. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <month> October </month> <year> 1986. </year> <note> Tech Report YALEU-RR-492. </note>
Reference-contexts: It is difficult to speak of the performance improvement due to ScC in this context except by comparing with the original LRC. 7 Related Work The concept of shared virtual memory was proposed in Kai Li's Ph.D. thesis in 1986 <ref> [16, 17] </ref>. Relaxed consistency models [7] allow shared virtual memory to reduce the cost of false sharing. New coherence protocols that improve the performance of release consistency in this context include lazy release consistency [14], and entry consistency [2], in which shared data are explicitly associated with some synchronization variable.
Reference: [17] <author> K. Li and P. Hudak. </author> <title> Memory Coherence in Shared Virtual Memory Systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: It is difficult to speak of the performance improvement due to ScC in this context except by comparing with the original LRC. 7 Related Work The concept of shared virtual memory was proposed in Kai Li's Ph.D. thesis in 1986 <ref> [16, 17] </ref>. Relaxed consistency models [7] allow shared virtual memory to reduce the cost of false sharing. New coherence protocols that improve the performance of release consistency in this context include lazy release consistency [14], and entry consistency [2], in which shared data are explicitly associated with some synchronization variable.
Reference: [18] <author> Creve Maples. </author> <title> A High-Performance, Memory-Based Interconnection System For Multicomputer Environments. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <pages> pages 295-304, </pages> <month> November </month> <year> 1990. </year>
Reference-contexts: Blizzard-S [19] rewrites an existing executable file to insert a state table lookup before every shared-memory reference. This technique works well in the presence of fine-grain sharing. Alternative directory-based protocols for memory mapped network interfaces were proposed for Cashmere [15]. The Plus [3], Galactica Net [12], Merlin <ref> [18] </ref> and its successor SESAME [21], systems implement hardware-based shared memory using a sort of write-through mechanism which is similar in some ways to automatic update.
Reference: [19] <author> I. Schoinas, B. Falsafi, A.R. Lebeck, S.K. Reinhardt, J.R. Larus, and D.A. Wood. </author> <title> Fine-grain Access for Distributed Shared Memory. </title> <booktitle> In The 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 297-306, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: It uses LRC and allows for multiple writers to a page. This implementation provides respectable performance in the absence of fine-grained sharing. Software-only techniques have been proposed to reduce false sharing by controlling fine-grain accesses to shared memory. Blizzard-S <ref> [19] </ref> rewrites an existing executable file to insert a state table lookup before every shared-memory reference. This technique works well in the presence of fine-grain sharing. Alternative directory-based protocols for memory mapped network interfaces were proposed for Cashmere [15].
Reference: [20] <author> J.P. Singh, J.L. Hennessy, and A. Gupta. </author> <title> Implications of Hierarchical N-Body Methods for Multiprocessor Architecture. </title> <type> Technical Report CSL-TR-92-506, </type> <institution> Stanford University, </institution> <year> 1992. </year>
Reference-contexts: The programs for this evaluation were appropriately selected from the Splash-2 suite [22] to cover distinct cases of interest. The problem sizes used are the default sizes indicated in Splash2. We also developed a simplified N-body kernel representing a simple pairwise force calculation phase <ref> [20] </ref>, for which we arranged data distribution to cause a lot of false sharing. This is intended to to illustrate a situation which showcases ScC. We measured the benefits obtained from ScC by two means: reductions in the number of page misses, and increases in speedup (hence performance) obtained.
Reference: [21] <author> Larry D. Wittie, Gudjon Hermannsson, and Ai Li. </author> <title> Eager Sharing for Efficient Massive Parallelism. </title> <booktitle> In Proceedings of the 1992 International Conference on Parall el Processing, </booktitle> <pages> pages 251-255, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: This technique works well in the presence of fine-grain sharing. Alternative directory-based protocols for memory mapped network interfaces were proposed for Cashmere [15]. The Plus [3], Galactica Net [12], Merlin [18] and its successor SESAME <ref> [21] </ref>, systems implement hardware-based shared memory using a sort of write-through mechanism which is similar in some ways to automatic update. The Memory Channel [8] allows remote memory to be mapped into the local virtual address space and have writes propagated automatically, but without a corresponding local memory mapping.
Reference: [22] <author> S.C. Woo, M. Ohara, E. Torrie, J.P. Singh, and A. Gupta. </author> <title> Methodological Considerations and Characterization of the SPLASH-2 Parallel Application Suite. </title> <booktitle> In Proceedings of the 23rd Annual Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1995. </year>
Reference-contexts: We evaluate the performance gained from ScC for different classes of sharing patterns in real applications. The programs for this evaluation were appropriately selected from the Splash-2 suite <ref> [22] </ref> to cover distinct cases of interest. The problem sizes used are the default sizes indicated in Splash2. We also developed a simplified N-body kernel representing a simple pairwise force calculation phase [20], for which we arranged data distribution to cause a lot of false sharing. <p> Our N-body kernel, a simplified force calculation from an O (n) N-body algorithm using particles and space cells, displays such false sharing. It is based loosely on the Water-spatial application in Splash-2 <ref> [22] </ref>. The computational domain is divided into cells which are distributed among processors. Each processor computes the interactions of the particles belonging to its cell with particles from nearby cells.
Reference: [23] <author> M.J. Zekauskas, W.A. Sawdon, , and B.N.Bershad. </author> <title> Software Write Detection for a Distributed Shared Memory. </title> <booktitle> In Proceedings of the Operating Systems Design and Implementation Symposium, </booktitle> <pages> pages 87-100, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: That is, all copies of a page map to a fixed home, and the home copy is always kept up-to-date by automatic update. This replaces more expensive all-software solutions for write detection used in LRC implementations, like diff computation [13] or software dirty bit schemes <ref> [23] </ref>. This mechanism only keeps the home copy up to date. To keep the other copies coherent according to the consistency model, a software protocol must supplement the hardware support with an invalidation-based scheme. As in LRC, invalidations are sent to a processor when it does an acquire operation. <p> The Memory Channel [8] allows remote memory to be mapped into the local virtual address space and have writes propagated automatically, but without a corresponding local memory mapping. LRC and EC have been compared in recent studies <ref> [23, 1] </ref>. Although their results cannot be directly compared with ours, since they compared LRC with EC while we compared AURC with ScC, a few points are worth noting.
References-found: 23

