URL: http://www.cs.utah.edu/~cs686/Previous/s97/povey.ps
Refering-URL: http://www.cs.utah.edu/~cs686/Previous/s97/
Root-URL: 
Email: fpovey,harrisong@cs.uq.edu.au  
Title: A Distributed Internet Cache  
Author: Dean Povey, John Harrison 
Keyword: Internet, World Wide Web, Caching, Harvest, Resource Discovery.  
Address: 4072  
Affiliation: School of Information Technology University of Queensland Brisbane, QLD,  
Abstract: This paper outlines the need for replication of resources on the Internet to combat the huge growth in user population and bandwidth requirements. It describes deficiencies in an existing hierarchical caching scheme for document replication and presents an alternative approach that more widely distributes the load across multiple servers, and provides additional resource discovery features. A simulation experiment which compares the distributed and hierarchical approaches is described and its results presented. These results indicate the applicability of the distributed caching scheme in the majority of situations considered. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Paul Albitz and Cricket Liu. </author> <title> DNS and BIND in a nutshell. </title> <publisher> O'Reilly and Associates, </publisher> <year> 1992. </year>
Reference-contexts: As the Internet grew these host files quickly became inconsistent due to the frequency of updates required. Thus, the DNS was created to manage the naming of hosts by using a hierarchical structure <ref> [1] </ref>. Like the host file, sibling information is stored statically in the hierarchical cache, resulting in consistency problems as the frequency of updates grows. Although a system could be conceived to propagate this information between nodes, this problem was not addressed by the designers of hierarchical cache.
Reference: [2] <author> Azer Bestavros, Robert L. Carter, Carlos R. Cunha Mark E. Crovella, Abdelsalam Hed-daya and Sulaiman A. Mirdad. </author> <title> Application-level document caching in the Internet. </title> <booktitle> In Proceedings of the Second IEEE International Workshop on Services in Distributed and Net-worked Environments, </booktitle> <pages> pages 166-173, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: A study of document caching indicated that the performance gain which is achievable by caching at the LAN level using a proxy server, is not that much higher than that obtained by using a client cache <ref> [2] </ref>. <p> Disk space required at upper level nodes In a study of application-level document caching for the Internet at Boston University, the increase in cache size as a function of the number of users was examined for both local and remote documents in a cache <ref> [2] </ref>. A Cache Expansion Index (CEI) was computed which indicates the ratio of the cache size required to maintain a given byte hit rate as the number of users increases. <p> it is unlikely that it will improve at the same rate by which the demand 1 The constant for local documents was quite small at around 0.03 (Meaning a 3% increase in cache size is needed for one additional user) The constant for remote documents was much higher at 0.12 <ref> [2] </ref>. for it is growing. As the requirements for storage space increases, the proportional size of caches at nodes will be forced to decrease, resulting in lower hit rates. Thus, as the number of users and file sizes increase, the efficiency of the hierarchical approach will degrade.
Reference: [3] <author> Anawat Chankhunthod, Peter B. Danzig, Chuck Neerdaels, Michael F. Schwartz and Kurt J. Worrell. </author> <title> A Hierachical Internet Object Cache. </title> <type> Technical report, </type> <institution> Department of Computer Science, University of Southern Cali-fornia and Department of Computer Science, Universityi of Colorado, Boulder, </institution> <year> 1995. </year>
Reference-contexts: Following these findings, researchers at the University of Colorado and the University of Southern California developed a hierarchical caching strategy <ref> [3] </ref> as part of the Harvest resource discovery project [11].
Reference: [4] <author> Peter B. Danzig, Richard S. Hall and Michael F. Schwartz. </author> <title> A case for caching file objects inside Internetworks. </title> <type> Technical report, </type> <institution> Department of Computer Science, University of Colorado, Boulder, </institution> <year> 1993. </year>
Reference-contexts: A study which examined traces of FTP transfers on the NSFNET, concluded that by placing multiple file caches at strategic points the volume of NSFNET traffic could be reduced by as much as 21% <ref> [4] </ref>. In section 2, existing schemes for document replication will be outlined. In section 3, a hierarchical caching strategy will be discussed. An alternative approach will be introduced in section 4, and compared with the hierarchical scheme. <p> share the cache, this performance can be achieved using less resources. 3 The Hierarchical Cache In their 1993 study of file transfer traffic on the NSFNET, Danzig, Hall and Schwartz showed that by providing caches in a hierarchical arrangement, the network bandwidth consumed for file transfer could be significantly reduced <ref> [4] </ref>. Following these findings, researchers at the University of Colorado and the University of Southern California developed a hierarchical caching strategy [3] as part of the Harvest resource discovery project [11].
Reference: [5] <author> Peter B. Danzig, Sugih Jamin, Ramon Caceres and Danny Mitzel. </author> <title> Characteristics of wide-area TCP/IP conversations, September 1991. </title> <booktitle> ACM SIGCOMM 91 Conference. </booktitle>
Reference-contexts: In addition, the size of files is also growing at a high rate. A study of the characteristics of wide area TCP/IP communications found that size of FTP files increased an order of magnitude from 1989 to 1991 <ref> [5] </ref>. The increasing use of multimedia applications which have large storage requirements indicates that file sizes are likely to continue grow at a high rate, thus further decreasing the available storage capacity of nodes.
Reference: [6] <author> Jeffrey K. MacKie-Mason and Hal R. </author> <title> Varian. Some economics of the Internet. </title> <institution> In 10th Michigan Public Utility Conference at Western Michigan University, page 37, </institution> <month> November </month> <year> 1992. </year> <note> revised version: February 17, </note> <year> 1994. </year>
Reference-contexts: 1 Introduction The Internet is already large (approximately 12.8 million hosts at the last count [15]) and is growing at an exponential rate. The number of packets on the National Science Federation Network (NSFNET) alone grew by over 400% between 1988 and 1993 <ref> [6] </ref>. A dramatic increase in the number of users, coupled with multimedia based information services, real-time audio/video transmissions and the emergence of network commerce are contributing to an incredible demand for bandwidth. Unfortunately, the resources available to service this demand are not expanding at a comparative rate. <p> Unfortunately, the resources available to service this demand are not expanding at a comparative rate. The amount of traffic on the network doubles every year, while the communications and router costs decrease at only 30% annually <ref> [6] </ref>. Subsequently, there is growing concern in the Internet community about the ability of the network to sustain future volumes of traffic. All these factors motivate the need for strategies to reduce network usage.
Reference: [7] <institution> National Laboratory for Applied Network Research. A distributed testbed for national information provisioning. </institution> <note> Available from http://www.nlanr.net/Cache/, 1996. </note>
Reference-contexts: The Harvest cache (and a later version named "Squid") is currently being used to provide caching for the World Wide Web in New Zealand [8], and as the infrastructure for a distributed information provision testbed developed by the National Laboratory for Applied Network Research (NLANR) <ref> [7] </ref>. The goals of the hierarchical cache are to distribute load away from server hot spots and to reduce access latency. It aims to do this by reducing redundant document transfers using a mechanism which enables the caches of multiple organisations to be shared in a hierarchical arrangement of servers. <p> A level of security must also be maintained to prevent malicious users from using the cache to circumvent the charging scheme. Differences in levels of the Hierarchy Experience with the hierarchical cache has shown that the hit rates in root caches are less than those at lower levels <ref> [7] </ref>. This might be expected due to the reduced commonality in the sharing of documents by a broader range of users. Researchers on the NLANR project suggest that there may be a need to provide different functionality at different levels of the cache.
Reference: [8] <author> Donald Neal. </author> <title> The Harvest object cache in New Zealand. </title> <note> Available from http://- www.waikato.ac.nz/harvest/www5/- Overview.html, </note> <year> 1996. </year>
Reference-contexts: The Harvest cache (and a later version named "Squid") is currently being used to provide caching for the World Wide Web in New Zealand <ref> [8] </ref>, and as the infrastructure for a distributed information provision testbed developed by the National Laboratory for Applied Network Research (NLANR) [7]. The goals of the hierarchical cache are to distribute load away from server hot spots and to reduce access latency.
Reference: [9] <author> M. T. Ozsu and P. Valduriez. </author> <title> Principles of Distributed Database Systems. </title> <publisher> Prentice-Hall, </publisher> <year> 1991. </year>
Reference-contexts: An alternative approach will be introduced in section 4, and compared with the hierarchical scheme. Finally, a simulation experiment will be outlined and its results presented. 2 Replication and Caching on the In ternet Replication is often used in distributed file systems [12] and databases <ref> [9] </ref> to improve the reliability and performance of data access. However, although there are similarities between these systems and information retrieval protocols such as the Hypertext transfer protocol (HTTP) which are used on the Internet, there are several differences which require us to develop different strategies. 1.
Reference: [10] <author> Dean Povey. </author> <title> Internet Cache Protocol-NG (proposed specification). </title> <note> Available from http://www.psy.uq.edu.au/~dean/- project/icp.html, </note> <year> 1996. </year>
Reference-contexts: In the majority of situations examined there is no significant performance difference between the caching strategies used. A prototype implementation of the distributed cache has been completed as well as a draft protocol document outlining the caching protocol used <ref> [10] </ref>. 6.1 Future Work There are still some outstanding issues which need to be resolved before the implementation of the distributed cache can be completed. These include: 1.
Reference: [11] <author> Michael F. Schwartz. </author> <title> Internet resource discovery at the University of Colorado. </title> <journal> IEEE Computer, </journal> <volume> Volume 26, Number 9, </volume> <pages> pages 25-35, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: Following these findings, researchers at the University of Colorado and the University of Southern California developed a hierarchical caching strategy [3] as part of the Harvest resource discovery project <ref> [11] </ref>. The Harvest cache (and a later version named "Squid") is currently being used to provide caching for the World Wide Web in New Zealand [8], and as the infrastructure for a distributed information provision testbed developed by the National Laboratory for Applied Network Research (NLANR) [7].
Reference: [12] <author> Andrew S. Tanenbaum. </author> <title> Modern Operating Systems. </title> <publisher> Prentice-Hall, </publisher> <year> 1992. </year>
Reference-contexts: An alternative approach will be introduced in section 4, and compared with the hierarchical scheme. Finally, a simulation experiment will be outlined and its results presented. 2 Replication and Caching on the In ternet Replication is often used in distributed file systems <ref> [12] </ref> and databases [9] to improve the reliability and performance of data access. However, although there are similarities between these systems and information retrieval protocols such as the Hypertext transfer protocol (HTTP) which are used on the Internet, there are several differences which require us to develop different strategies. 1.
Reference: [13] <author> D. Wessels and K. Kaffy. </author> <title> Internet Cache Protocol (ICP), </title> <type> version 2, </type> <month> November </month> <year> 1996. </year> <type> IETF Internet-Draft. </type>
Reference: [14] <author> Duane Wessels. </author> <title> Intelligent caching for WorldWide Web objects. </title> <type> Master's thesis, </type> <institution> Washing-ton State University, </institution> <year> 1995. </year>
Reference-contexts: This form of replication is referred to as Internet caching. Internet caching is particularly suited to the World Wide Web where many of the objects are small and are modified frequently <ref> [14] </ref>. Caching does not have the problems associated with mirroring because: 1. It is transparent to the user. Because most schemes require no user intervention there is no need for the user to know the physical lo cation of the replicated document. 2. It is demand based.
Reference: [15] <author> Network Wizards. </author> <title> Internet Domain Survey. </title> <note> Available from http://www.nw.com/, 1996. </note>
Reference-contexts: 1 Introduction The Internet is already large (approximately 12.8 million hosts at the last count <ref> [15] </ref>) and is growing at an exponential rate. The number of packets on the National Science Federation Network (NSFNET) alone grew by over 400% between 1988 and 1993 [6].
References-found: 15

