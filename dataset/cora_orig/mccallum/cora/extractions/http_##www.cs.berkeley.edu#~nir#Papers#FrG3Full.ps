URL: http://www.cs.berkeley.edu/~nir/Papers/FrG3Full.ps
Refering-URL: http://http.cs.berkeley.edu/~nir/publications.html
Root-URL: 
Email: nir@cs.berkeley.edu  moises@erg.sri.com  
Title: LEARNING BAYESIAN NETWORKS WITH LOCAL STRUCTURE  
Author: NIR FRIEDMAN AND MOISES GOLDSZMIDT 
Address: 387 Soda Hall,  Berkeley, CA 94720.  333 Ravenswood Avenue, EK329, Menlo Park, CA 94025.  
Affiliation: Computer Science Division,  University of California,  SRI International,  
Abstract: We examine a novel addition to the known methods for learning Bayesian networks from data that improves the quality of the learned networks. Our approach explicitly represents and learns the local structure in the conditional probability distributions (CPDs) that quantify these networks. This increases the space of possible models, enabling the representation of CPDs with a variable number of parameters. The resulting learning procedure induces models that better emulate the interactions present in the data. We describe the theoretical foundations and practical aspects of learning local structures and provide an empirical evaluation of the proposed learning procedure. This evaluation indicates that learning curves characterizing this procedure converge faster, in the number of training instances, than those of the standard procedure, which ignores the local structure of the CPDs. Our results also show that networks learned with local structures tend to be more complex (in terms of arcs), yet require fewer parameters. 
Abstract-found: 1
Intro-found: 1
Reference: <author> I. Beinlich, G. Suermondt, R. Chavez, and G. Cooper. </author> <title> The ALARM monitoring system: A case study with two probabilistic inference techniques for belief networks. </title> <booktitle> In Proc. 2'nd European Conf. on AI and Medicine. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1989. </year>
Reference-contexts: These experiments involved learning approximately 15,000 networks. The results are summarized below. 24 NIR FRIEDMAN AND MOISES GOLDSZMIDT TABLE 1. Description of the networks used in the experiments. Name Description n jjUjj jfij Alarm A network by medical experts for monitoring patients in intensive care <ref> (Beinlich et al., 1989) </ref>. 37 2 53:95 509 Hailfinder A network for modeling summer hail in northeastern Col orado (http://www.lis.pitt.edu/~dsl/hailfinder). 56 2 106:56 2656 Insurance A network for classifying insurance applications (Russell et al., 1995). 5.1.
Reference: <author> R. R. Bouckaert. </author> <title> Properties of Bayesian network learning algorithms. </title> <editor> In R. Lopez de Mantaras and D. Poole, editors, </editor> <booktitle> Proc. Tenth Conference on Uncertainty in Artificial Intelligence (UAI '94), </booktitle> <pages> pages 102-109. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA, </address> <year> 1994. </year>
Reference: <author> C. Boutilier, N. Friedman, M. Goldszmidt, and D. Koller. </author> <title> Context-specific independence in Bayesian networks. </title> <editor> In E. Horvitz and F. Jensen, editors, </editor> <booktitle> Proc. Twelfth Conference on Uncertainty in Artificial Intelligence (UAI '96), </booktitle> <pages> pages 115-123. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA, </address> <year> 1996. </year>
Reference-contexts: There are different types of local structures for CPDs, a prominent example is the noisy-or gate and its generalizations (Heckerman and Breese, 1994; Pearl, 1988; Srinivas, 1993). In this article, we focus on learning local structures that are motivated by properties of context-specific independence (CSI) <ref> (Boutilier et al., 1996) </ref>. These independence statements imply that in some contexts, defined by an assignment to variables in the network, the conditional probability of variable X is independent of some of its parents. <p> A formal foundation for representing and reasoning with such regularities is provided in the notion of context-specific independence (CSI) <ref> (Boutilier et al., 1996) </ref>.
Reference: <author> W. Buntine. </author> <title> A theory of learning classification rules. </title> <type> PhD thesis, </type> <institution> University of Technology, </institution> <address> Sydney, Australia, </address> <year> 1991. </year>
Reference: <author> W. Buntine. </author> <title> Theory refinement on Bayesian networks. </title> <editor> In B. D. D'Ambrosio, P. Smets, and P. P. Bonissone, editors, </editor> <booktitle> Proc. Seventh Annual Conference on Uncertainty Artificial Intelligence (UAI '92), </booktitle> <pages> pages 52-60. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA, </address> <year> 1991. </year>
Reference: <author> W. Buntine. </author> <title> Learning classification trees. </title> <editor> In D. J. Hand, editor, </editor> <booktitle> Artificial Intelligence Frontiers in Statistics, number III in AI and Statistics. </booktitle> <publisher> Chapman & Hall, </publisher> <address> London, </address> <year> 1993. </year>
Reference: <author> D. M. Chickering. </author> <title> Learning Bayesian networks is NP-complete. </title> <editor> In D. Fisher and H.-J. Lenz, editors, </editor> <booktitle> Learning from Data: Artificial Intelligence and Statistics V. </booktitle> <publisher> Springer Verlag, </publisher> <year> 1996. </year>
Reference: <author> G. F. Cooper and E. Herskovits. </author> <title> A Bayesian method for the induction of probabilistic networks from data. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 309-347, </pages> <year> 1992. </year>
Reference: <author> T. M. Cover and J. A. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: In this code, the exact length of each codeword depends on the probability assigned to that particular instance. There is no closed-form description of this length. However, it is known <ref> (Cover and Thomas, 1991) </ref> that we can approximate the optimal encoding length using log P B (u) as the encod ing length of each instance u. <p> Thus, once again we derive an information-theoretic interpretation of DL data (fi L ; D). This interpretation shows that the encoding of X depends only on the values of L . From the data processing inequality <ref> (Cover and Thomas, 1991) </ref> it follows that H (X i j L ) H (X i j Pa i ). This implies that a local structure cannot fit the data better than a tabular CPD. <p> It well known that I P (X; Y j Z) 0, and that I P (X; Y j Z) = 0, if and only if X is independent of Y, given Z <ref> (Cover and Thomas, 1991) </ref>. 26 NIR FRIEDMAN AND MOISES GOLDSZMIDT Using the mutual information as a quantitative measure of strength of dependencies, we can measure the extent to which the independence assumptions represented in G are violated in the real distribution.
Reference: <author> M. H. </author> <title> DeGroot. Optimal Statistical Decisions. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1970. </year>
Reference: <author> F. J. Diez. </author> <title> Parameter adjustment in Bayes networks: The generalized noisy or-gate. </title> <editor> In D. Heckerman and A. Mamdani, editors, </editor> <booktitle> Proc. Ninth Conference on Uncertainty in Artificial Intelligence (UAI '93), </booktitle> <pages> pages 99-105. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA, </address> <year> 1993. </year>
Reference: <author> N. Friedman and Z. Yakhini. </author> <title> On the sample complexity of learning Bayesian networks. </title>
Reference: <editor> In E. Horvitz and F. Jensen, editors, </editor> <title> Proc. Twelfth Conference on Uncertainty in 4 All products and company names mentioned in this article are the trademarks of their respective holders. LEARNING BAYESIAN NETWORKS WITH LOCAL STRUCTURE 35 Artificial Intelligence (UAI '96). </title> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA, </address> <year> 1996. </year>
Reference: <author> D. Heckerman and J. S. Breese. </author> <title> A new look at causal independence. </title> <editor> In R. Lopez de Mantaras and D. Poole, editors, </editor> <booktitle> Proc. Tenth Conference on Uncertainty in Artificial Intelligence (UAI '94), </booktitle> <pages> pages 286-292. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA, </address> <year> 1994. </year>
Reference: <author> D. Heckerman, D. Geiger, and D. M. Chickering. </author> <title> Learning Bayesian networks: The combination of knowledge and statistical data. </title> <journal> Machine Learning, </journal> <volume> 20 </volume> <pages> 197-243, </pages> <year> 1995. </year>
Reference-contexts: B = hG; Li that best matches D. 1 To formalize the notion of goodness of fit of a network with respect to the data, we normally introduce a scoring function, and to solve the optimization problem we usually rely on heuristic search techniques over the space of possible networks <ref> (Heckerman, 1995) </ref>. Several different scoring functions have been proposed in the literature. In this article we focus our attention on the ones that are most frequently used: the Minimal Description Length (MDL) score (Lam and Bacchus, 1994) and the BDe score (Heckerman et al., 1995a). 2.1. <p> X i jpa i j G h )dfi X i jpa i : (6) (This decomposition is analogous to the decomposition in Equation 2.) When the prior on each multinomial distribution fi X i jpa i is a Dirichlet prior , the integrals in Equation 6 have a closed-form solution <ref> (Heckerman, 1995) </ref>. We briefly review the properties of Dirichlet priors. For more detailed description, we refer the reader to DeGroot (1970). A Dirichlet prior for a multinomial distribution of a variable X is specified by a set of hyperpa rameters fN 0 x : x 2 Val (X)g.
Reference: <author> D. Heckerman. </author> <title> A tutorial on learning Bayesian networks. </title> <type> Technical Report MSR-TR-95-06, </type> <institution> Microsoft Research, </institution> <year> 1995. </year>
Reference-contexts: B = hG; Li that best matches D. 1 To formalize the notion of goodness of fit of a network with respect to the data, we normally introduce a scoring function, and to solve the optimization problem we usually rely on heuristic search techniques over the space of possible networks <ref> (Heckerman, 1995) </ref>. Several different scoring functions have been proposed in the literature. In this article we focus our attention on the ones that are most frequently used: the Minimal Description Length (MDL) score (Lam and Bacchus, 1994) and the BDe score (Heckerman et al., 1995a). 2.1. <p> X i jpa i j G h )dfi X i jpa i : (6) (This decomposition is analogous to the decomposition in Equation 2.) When the prior on each multinomial distribution fi X i jpa i is a Dirichlet prior , the integrals in Equation 6 have a closed-form solution <ref> (Heckerman, 1995) </ref>. We briefly review the properties of Dirichlet priors. For more detailed description, we refer the reader to DeGroot (1970). A Dirichlet prior for a multinomial distribution of a variable X is specified by a set of hyperpa rameters fN 0 x : x 2 Val (X)g.
Reference: <author> W. Lam and F. Bacchus. </author> <title> Learning Bayesian belief networks: An approach based on the MDL principle. </title> <journal> Computational Intelligence, </journal> <volume> 10 </volume> <pages> 269-293, </pages> <year> 1994. </year>
Reference-contexts: Several different scoring functions have been proposed in the literature. In this article we focus our attention on the ones that are most frequently used: the Minimal Description Length (MDL) score <ref> (Lam and Bacchus, 1994) </ref> and the BDe score (Heckerman et al., 1995a). 2.1. THE MDL SCORE The MDL principle (Rissanen, 1989) is motivated by universal coding. Suppose that we are given a set D of instances, which we would like to store in our records.
Reference: <author> R. Musick. </author> <title> Belief Network Induction. </title> <type> PhD thesis, </type> <institution> University of California, Berkeley, </institution> <address> CA, </address> <year> 1994. </year>
Reference: <author> R. M. Neal. </author> <title> Connectionist learning of belief networks. </title> <journal> Artificial Intelligence, </journal> <volume> 56 </volume> <pages> 71-113, </pages> <year> 1992. </year>
Reference: <author> J. Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Fran-cisco, CA, </address> <year> 1988. </year>
Reference-contexts: The second component is a collection of conditional probability distributions (CPDs) that describe the conditional probability of each variable given its parents in the graph. Together, these two components represent a unique probability distribution <ref> (Pearl, 1988) </ref>. In recent years there has been growing interest in learning Bayesian networks from data; see, for example, Cooper and Herskovits (1992); Buntine (1991b); Heckerman (1995); and Lam and Bacchus (1994). <p> The graph structure G encodes the following set of independence statements: each variable X i is independent of its nondescendants, given its parents in G. The set composed of a variable and its parents is usually referred to as a family. Standard arguments <ref> (Pearl, 1988) </ref> show that any distribution P that satisfies the independence statements encoded in the graph G can be factored as n Y P (X i j Pa i ); (1) where Pa i denote the parents of X i in G.
Reference: <author> J. R. Quinlan and R. Rivest. </author> <title> Inferring decision trees using the minimum description length principle. </title> <journal> Information and Computation, </journal> <volume> 80 </volume> <pages> 227-248, </pages> <year> 1989. </year>
Reference-contexts: In this example, the default table requires five parameters instead of the eight parameters required by the tabular representation. Figure 2 (b) describes another possible representation based on decision trees <ref> (Quinlan and Rivest, 1989) </ref>. Each leaf in the decision tree describes a probability for S, and the internal nodes and arcs encode the necessary information to decide how to choose among leaves, based on the values of S's parents.
Reference: <author> J. R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA, </address> <year> 1993. </year>
Reference: <author> J. Rissanen. </author> <title> Stochastic Complexity in Statistical Inquiry. </title> <publisher> World Scientific, </publisher> <address> River Edge, NJ, </address> <year> 1989. </year>
Reference-contexts: In this article we focus our attention on the ones that are most frequently used: the Minimal Description Length (MDL) score (Lam and Bacchus, 1994) and the BDe score (Heckerman et al., 1995a). 2.1. THE MDL SCORE The MDL principle <ref> (Rissanen, 1989) </ref> is motivated by universal coding. Suppose that we are given a set D of instances, which we would like to store in our records. Naturally, we would like to conserve space and save a compressed version of D.
Reference: <author> S. Russell, J. Binder, D. Koller, and K. </author> <title> Kanazawa. Local learning in probabilistic networks with hidden variables. </title> <booktitle> In Proc. Fourteenth International Joint Conference on Artificial Intelligence (IJCAI '95), </booktitle> <pages> pages 1146-1152. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Fran-cisco, CA, </address> <year> 1995. </year>
Reference-contexts: Name Description n jjUjj jfij Alarm A network by medical experts for monitoring patients in intensive care (Beinlich et al., 1989). 37 2 53:95 509 Hailfinder A network for modeling summer hail in northeastern Col orado (http://www.lis.pitt.edu/~dsl/hailfinder). 56 2 106:56 2656 Insurance A network for classifying insurance applications <ref> (Russell et al., 1995) </ref>. 5.1. METHODOLOGY The data sets used in the experiments were sampled from three Bayesian networks whose main characteristics are described in Table 1.
Reference: <author> G. Schwarz. </author> <title> Estimating the dimension of a model. </title> <journal> Annals of Statistics, </journal> <volume> 6 </volume> <pages> 461-464, </pages> <year> 1978. </year>
Reference: <author> J. E. Shore and R. W. Johnson. </author> <title> Axiomatic derivation of the principle of maximum entropy and the principle of minimum cross-entropy. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-26(1):26-37, </volume> <year> 1980. </year>
Reference: <author> D. J. Spiegelhalter and S. L. Lauritzen. </author> <title> Sequential updating of conditional probabilities on directed graphical structures. </title> <journal> Networks, </journal> <volume> 20 </volume> <pages> 579-605, </pages> <year> 1990. </year>
Reference: <author> S. Srinivas. </author> <title> A generalization of the noisy-or model. </title> <editor> In D. Heckerman and A. Mamdani, editors, </editor> <booktitle> Proc. Ninth Conference on Uncertainty in Artificial Intelligence (UAI '93), </booktitle> <pages> pages 208-215. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA, </address> <year> 1993. </year>

References-found: 28

