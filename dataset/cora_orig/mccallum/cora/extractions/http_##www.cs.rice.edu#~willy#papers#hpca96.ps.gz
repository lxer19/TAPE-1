URL: http://www.cs.rice.edu/~willy/papers/hpca96.ps.gz
Refering-URL: http://www.cs.rice.edu/~willy/TreadMarks/papers.html
Root-URL: 
Email: fsarita,alc,sandhya,rrk,willyg@rice.edu  
Title: A Comparison of Entry Consistency and Lazy Release Consistency Implementations  
Author: Sarita V. Adve, Alan L. Cox, Sandhya Dwarkadas, Ramakrishnan Rajamony, Willy Zwaenepoel 
Note: This work is supported in part by the National Science Foundation under Grants CCR-91163343, CCR-9211004, CCR-9410457, CCR-9502500, and CDA-9502791, by the Texas Advanced Technology Program under Grants 0036404013 and 003604016, and by a grant from Tech-Sym, Inc.  
Address: Houston, TX 77005-1892  
Affiliation: Departments of Computer Science and Electrical and Computer Engineering Rice University  
Abstract: This paper compares several implementations of entry consistency (EC) and lazy release consistency (LRC), two relaxed memory models in use with software distributed shared memory (DSM) systems. We use six applications in our study: SOR, Quicksort, Water, Barnes-Hut, IS, and 3D-FFT. For these applications, EC's requirement that all shared data be associated with a synchronization object leads to a fair amount of additional programming effort. We identify, in particular, extra synchronization, lock rebinding, and object granularity as sources of extra complexity. In terms of performance, for the set of applications and for the computing environment utilized neither model is consistently better than the other. For SOR and IS, execution times are about the same, but LRC is faster for Water (33%) and Barnes-Hut (41%) and EC is faster for Quicksort (14%) and 3D-FFT (10%). Among the implementations of EC and LRC, we independently vary the method for write trapping and the method for write collection. Our goal is to separate implementation issues from any particular model. We consider write trapping by compiler instrumentation of the code and by twinning (comparing the current version of shared data with an older version). Write collection is done either by scanning timestamps or by building diffs, records of the changes to shared data. For write trapping in EC, twinning is faster if data is shared at the granularity of a single word. For larger granularities than a word, compiler instrumentation is faster. For write trapping in LRC, twinning gives the best performance for all applications. For write collection in EC, timestamping works best in applications dominated by migratory data, while for other data diffing works best. For LRC, increased communication overhead in transmitting timestamps becomes an additional factor working in favor of diffing for applications with fine-grain sharing. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. V. Adve and M. D. Hill. </author> <title> A unified formalization of four shared-memory models. </title> <journal> IEEE TPDS, </journal> <month> June </month> <year> 1993. </year>
Reference-contexts: For each block for which a change is detected at the end of an interval, the corresponding entry in the timestamp array is set to the tuple (local processor id, current interval index). Intervals of different processes are partially ordered <ref> [1] </ref>: (i) intervals on a single processor are totally ordered by program order, and (ii) an interval on processor p precedes an interval on processor q if the interval of q begins with the acquire corresponding to the release that concluded the interval of p.
Reference: [2] <author> D. Bailey et al. </author> <title> The NAS parallel benchmarks. </title> <type> Technical Report TR RNR-91-002, </type> <institution> NASA Ames, </institution> <month> August </month> <year> 1991. </year>
Reference-contexts: The applications used in the comparison are: Red-Black Successive Over-Relaxation (SOR), Quicksort, Water, Barnes-Hut, Integer Sort (IS), and three-dimensional FFT (3D-FFT). SOR and Quicksort are small test programs. Water and Barnes-Hut are from the Splash suite [12]. IS and 3D-FFT are from the NAS benchmark suite <ref> [2] </ref>. The outline of the rest of this paper is as follows. Section 2 presents the applications used in this study. Section 3 discusses EC and LRC, contrasts programming in EC and LRC, and illustrates the differences with examples from the applications. Section 4 discusses write trapping.
Reference: [3] <author> B.N. Bershad, M.J. Zekauskas, </author> <title> and W.A. </title> <booktitle> Sawdon. The Midway distributed shared memory system. In Proceedings of the '93 CompCon Conference, </booktitle> <month> February </month> <year> 1993. </year>
Reference-contexts: Early such systems suffered from high communication overhead. To combat these problems, software DSM implementations have turned to relaxed memory models [6]. Two popular models in use with current DSM systems are lazy release consistency (LRC) [8], used in TreadMarks [10], and entry consistency (EC) <ref> [3] </ref>, used in Midway [13]. Both LRC and EC allow delaying the propagation of modifications to shared data until a synchronization operation occurs. To do so, both models require that the programmer use only system-provided synchronization primitives. EC, in addition, requires shared data to be associated with a synchronization object. <p> In contrast, the papers on EC have argued that by restricting the regions of memory to be made consistent to those associated with a synchronization object, EC is best served by an update protocol <ref> [3] </ref>. Midway uses an update protocol, and so do the implementations of EC in this study. We will discuss the influence of these implementation decisions on the performance results in Section 7. We could not directly compare Tread-Marks and Midway, because TreadMarks runs on Unix and Midway runs on Mach. <p> Lazy Release Consistency Sections 3.1 and 3.2 summarize the two consistency models under discussion. For more extensive discussions we refer the reader to the papers introducing release consistency [7], lazy release consistency [8], and entry consistency <ref> [3] </ref>. Section 3.3 discusses the differences in programming under the two models. 3.1 Entry Consistency (EC) In EC, all shared data must be explicitly declared as such in the program text, and associated with a synchronization object that protects access to that shared data. Processes must synchronize via system-supplied primitives. <p> For twinning a block is always a single word. The notion of a timestamp is different for EC and LRC, because of differences in the two models. Timestamping for EC. We use the notion of an incarnation number associated with each lock, as introduced in Midway <ref> [3] </ref>. Every time a lock is transferred, its incarnation number is incremented. A timestamp is associated with each block in the shared address space (i.e., each word or double-word).
Reference: [4] <author> J.B. Carter, J.K. Bennett, and W. Zwaenepoel. </author> <title> Techniques for reducing consistency-related information in distributed shared memory systems. </title> <journal> ACM TOCS, </journal> <month> August </month> <year> 1995. </year>
Reference-contexts: With EC, no communication takes place. With LRC, at the barrier before the second phase, the page must be invalidated at both processors. Our implementations of LRC are, however, not subject to the "ping-pong" effect, because they allow multiple concurrent writers per page <ref> [4] </ref>. Rebinding. The rebinding effect is an artifact of the extra synchronization required in EC (see Section 3.3). In the EC implementations, the acquire message carries the acquirer's value of the incarnation number for the lock.
Reference: [5] <author> M. Castro et al. </author> <title> Efficient and flexible object sharing. </title> <type> Technical report, </type> <institution> INESC, </institution> <month> July </month> <year> 1995. </year>
Reference-contexts: The execution times in this paper should not be compared directly to their results, because of differences in processor speed (40Mhz vs. 25Mhz) and differences in communication speed and page fault overhead in the underlying operating systems (Ultrix vs. Mach 3.0). Castro et al. <ref> [5] </ref> compare DiSOM, an object-oriented parallel programming system using EC, to TreadMarks, a DSM system using LRC. They conclude that DiSOM provides better performance than TreadMarks for a number of applications (matrix multiply, SOR, TSP, and Water). Superficially, these results appear to contradict the results in this paper.
Reference: [6] <author> M. Dubois and C. Scheurich. </author> <title> Memory access dependencies in shared-memory multiprocessors. </title> <journal> IEEE TSE, </journal> <month> June </month> <year> 1990. </year>
Reference-contexts: Here, we focus on software implementations of DSM. Early such systems suffered from high communication overhead. To combat these problems, software DSM implementations have turned to relaxed memory models <ref> [6] </ref>. Two popular models in use with current DSM systems are lazy release consistency (LRC) [8], used in TreadMarks [10], and entry consistency (EC) [3], used in Midway [13]. Both LRC and EC allow delaying the propagation of modifications to shared data until a synchronization operation occurs.
Reference: [7] <author> K. Gharachorloo et al. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th ISCA, </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: Lazy Release Consistency Sections 3.1 and 3.2 summarize the two consistency models under discussion. For more extensive discussions we refer the reader to the papers introducing release consistency <ref> [7] </ref>, lazy release consistency [8], and entry consistency [3].
Reference: [8] <author> P. Keleher, A. L. Cox, and W. Zwaenepoel. </author> <title> Lazy release consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th ISCA, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: Here, we focus on software implementations of DSM. Early such systems suffered from high communication overhead. To combat these problems, software DSM implementations have turned to relaxed memory models [6]. Two popular models in use with current DSM systems are lazy release consistency (LRC) <ref> [8] </ref>, used in TreadMarks [10], and entry consistency (EC) [3], used in Midway [13]. Both LRC and EC allow delaying the propagation of modifications to shared data until a synchronization operation occurs. To do so, both models require that the programmer use only system-provided synchronization primitives. <p> Lazy Release Consistency Sections 3.1 and 3.2 summarize the two consistency models under discussion. For more extensive discussions we refer the reader to the papers introducing release consistency [7], lazy release consistency <ref> [8] </ref>, and entry consistency [3]. Section 3.3 discusses the differences in programming under the two models. 3.1 Entry Consistency (EC) In EC, all shared data must be explicitly declared as such in the program text, and associated with a synchronization object that protects access to that shared data.
Reference: [9] <author> P. Keleher et al. </author> <title> An evaluation of software-based release consistent protocols. </title> <journal> JPDC, </journal> <month> October </month> <year> 1995. </year>
Reference-contexts: Table 1 summarizes the various implementations. An implementation of EC or LRC must also decide whether to use an invalidate or an update protocol. Previous work <ref> [9] </ref> has shown that an invalidate protocol results in the best performance for LRC. An in Trapping Model Comp.Ins.
Reference: [10] <author> P. Keleher et al. Treadmarks: </author> <title> Distributed shared memory on standard workstations and operating systems. </title> <booktitle> In Proceedings of the Winter Usenix Conference, </booktitle> <month> January </month> <year> 1994. </year>
Reference-contexts: Here, we focus on software implementations of DSM. Early such systems suffered from high communication overhead. To combat these problems, software DSM implementations have turned to relaxed memory models [6]. Two popular models in use with current DSM systems are lazy release consistency (LRC) [8], used in TreadMarks <ref> [10] </ref>, and entry consistency (EC) [3], used in Midway [13]. Both LRC and EC allow delaying the propagation of modifications to shared data until a synchronization operation occurs. To do so, both models require that the programmer use only system-provided synchronization primitives. <p> The combination of compiler instrumentation and diffing is not considered in any detail because its memory requirements appear prohibitive. The combination of compiler instrumentation and timestamps is used to implement EC in Midway [13]. Twinning and diffing is used to implement LRC in TreadMarks <ref> [10] </ref>. This paper attempts a more methodical exploration of the various combinations, and considers in addition to the above: compiler instrumentation and timestamps for LRC, twinning and timestamps both for LRC and EC, and a twinning and diffing algorithm for EC that improves over earlier published methods [13]. <p> Section 9 presents related work. Section 10 summarizes our conclusions. 2 Applications We used six programs in this study: Red-Black SOR, Quicksort, Water, Barnes-Hut, Integer Sort, and 3D-FFT. SOR, Quicksort, and Water were used in earlier studies of Midway [13] and TreadMarks <ref> [10] </ref>. We describe the applications as written for a sequentially consistent system. SOR uses Red-Black Successive Over-Relaxation to solve partial differential equations. The program determines the steady state values in a system where the boundary elements are kept constant.
Reference: [11] <author> K. Li and P. Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM TOCS, </journal> <month> November </month> <year> 1989. </year>
Reference-contexts: 1 Introduction Distributed shared memory (DSM) enables processes on different machines to share memory, even though the machines physically do not share memory <ref> [11] </ref>. This approach is attractive since most programmers find shared memory easier to use than message passing, which requires them to explicitly partition data and manage communication. Here, we focus on software implementations of DSM. Early such systems suffered from high communication overhead.
Reference: [12] <author> J.P. Singh, W.-D. Weber, and A. Gupta. </author> <title> SPLASH: Stan-ford parallel applications for shared-memory. </title> <type> Technical Report CSL-TR-91-469, </type> <institution> Stanford University, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: The applications used in the comparison are: Red-Black Successive Over-Relaxation (SOR), Quicksort, Water, Barnes-Hut, Integer Sort (IS), and three-dimensional FFT (3D-FFT). SOR and Quicksort are small test programs. Water and Barnes-Hut are from the Splash suite <ref> [12] </ref>. IS and 3D-FFT are from the NAS benchmark suite [2]. The outline of the rest of this paper is as follows. Section 2 presents the applications used in this study. <p> The smaller partition is enqueued in the task queue and the processor continues to work on the larger partition. When the partition size reaches a threshold of 1024 integers, the partition is sorted locally using a bubblesort algorithm. Water, from the SPLASH suite <ref> [12] </ref>, is a molecular dynamics simulation. The molecules are distributed equally among processors. There are two key phases in each timestep. In the first phase, called the force computation phase, a processor updates the forces due to the interaction of its molecules with those of half of the other processors. <p> A lock protects access to each molecule record during the force computation phase, because each force value is updated by several processors. No lock is required during the displacement computation phase, because each processor only updates the displacements of the molecules it owns. As suggested in the SPLASH report <ref> [12] </ref>, in the force computation phase, each processor uses a local variable to accumulate its updates to a molecule's force record. At the end of the phase, the processor acquires a lock on each molecule that it needs to update and applies the accumulated updates at once.
Reference: [13] <author> M.J. Zekauskas, </author> <title> W.A. Sawdon, and B.N. Bershad. Software write detection for distributed shared memory. </title> <booktitle> In Proceedings of the 1st OSDI Symposium, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: Early such systems suffered from high communication overhead. To combat these problems, software DSM implementations have turned to relaxed memory models [6]. Two popular models in use with current DSM systems are lazy release consistency (LRC) [8], used in TreadMarks [10], and entry consistency (EC) [3], used in Midway <ref> [13] </ref>. Both LRC and EC allow delaying the propagation of modifications to shared data until a synchronization operation occurs. To do so, both models require that the programmer use only system-provided synchronization primitives. EC, in addition, requires shared data to be associated with a synchronization object. <p> The combination of compiler instrumentation and diffing is not considered in any detail because its memory requirements appear prohibitive. The combination of compiler instrumentation and timestamps is used to implement EC in Midway <ref> [13] </ref>. Twinning and diffing is used to implement LRC in TreadMarks [10]. <p> This paper attempts a more methodical exploration of the various combinations, and considers in addition to the above: compiler instrumentation and timestamps for LRC, twinning and timestamps both for LRC and EC, and a twinning and diffing algorithm for EC that improves over earlier published methods <ref> [13] </ref>. Table 1 summarizes the various implementations. An implementation of EC or LRC must also decide whether to use an invalidate or an update protocol. Previous work [9] has shown that an invalidate protocol results in the best performance for LRC. An in Trapping Model Comp.Ins. <p> Previous work [9] has shown that an invalidate protocol results in the best performance for LRC. An in Trapping Model Comp.Ins. Twinning Collection Timestamping EC ~Midway new LRC new new Diffing EC <ref> [13] </ref> improved LRC TreadMarks Table 1 Combinations of Write Trapping and Write Collection Explored in This Paper validate protocol is therefore used in TreadMarks, and also in the implementations of LRC in this study. <p> Section 9 presents related work. Section 10 summarizes our conclusions. 2 Applications We used six programs in this study: Red-Black SOR, Quicksort, Water, Barnes-Hut, Integer Sort, and 3D-FFT. SOR, Quicksort, and Water were used in earlier studies of Midway <ref> [13] </ref> and TreadMarks [10]. We describe the applications as written for a sequentially consistent system. SOR uses Red-Black Successive Over-Relaxation to solve partial differential equations. The program determines the steady state values in a system where the boundary elements are kept constant. <p> As the overhead to set the dirty bit is incurred on every shared write, we have to carry out this operation as fast as possible. The approach we follow is identical to the one presented by Zekauskas et al. <ref> [13] </ref>. Shared data is allocated from large, fixed size regions. A region is made up of three parts. At the head is a code template, which consists of a set of routines that set the dirty bits for stores of different granularity (word, double-word, etc.). <p> The inserted code computes the beginning of the template from the store address, and branches to the code within it. For EC, the extra code inserted after a shared write by our compiler, and the template code itself, are identical to the Midway codes <ref> [13] </ref>. Differences between EC and LRC. When shared writes are instrumented, write collection requires scanning the dirty bits to determine which ones are set. In EC, when a lock is acquired, we only need to scan the dirty bits of the shared data object associated with the lock. <p> When the write lock is acquired, we write-protect the pages corresponding to the object using the virtual memory hardware. When the page is written, we make a physical copy (the twin), and unprotect the page in user space. Our implementation differs from the Midway VM implementation of EC <ref> [13] </ref> in that they do not distinguish between large and small objects, thereby taking a protection fault on each first write to a shared object. <p> Also, Water's performance with diffing has become better than with timestamping. We attribute this result to the need to communicate a large number of timestamps for these applications which exhibit relatively fine-grain sharing within a page (see Section 5.3). 9 Related Work The paper by Zekauskas et al. <ref> [13] </ref> studies the difference between a compiler instrumentation based and a virtual memory based implementation of EC. There are a number of differences between their study and ours. First, we consider both EC and LRC, and offer a comparison between the two.
References-found: 13

