URL: ftp://hpsl.cs.umd.edu/pub/papers/chaos++-2.ps.Z
Refering-URL: http://www.cs.umd.edu/projects/hpsl/papers.brandnew/LocalResources/tech-10-23.htm
Root-URL: 
Email: fchialin, als, saltzg@cs.umd.edu  
Title: Object-Oriented Runtime Support for Complex Distributed Data Structures  
Author: Chialin Chang Alan Sussman Joel Saltz 
Address: College Park, MD 20742  
Affiliation: Institute for Advanced Computer Studies and Department of Computer Science University of Maryland,  
Abstract: Object-oriented applications utilize language constructs such as pointers to synthesize dynamic complex data structures, such as linked lists, trees and graphs, with elements consisting of complex composite data types. Traditionally, however, applications executed on distributed memory parallel architectures in single-program multiple-data (SPMD) mode use distributed (multi-dimensional) data arrays. Good performance has been achieved by applying runtime techniques to such applications executing in a loosely synchronous manner. Existing runtime systems that rely solely on global indices are not always applicable to object-oriented applications, since no global names or indices are imposed upon dynamic complex data structures linked by pointers. We describe a portable object-oriented runtime library that has been designed to support applications that use dynamic distributed data structures, including both arrays and pointer-based data structures. In particular, CHAOS++ deals with complex data types and pointer-based data structures by providing two abstractions, mobile objects and globally addressable objects. CHAOS++ uses preprocessing techniques to analyze communication patterns, and provides data exchange routines to perform efficient data transfers between processors. Results for applications taken from three distinct classes are also presented to demonstrate the wide applicability and good performance characteristics of the runtime library.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Gagan Agrawal, Alan Sussman, and Joel Saltz. </author> <title> Compiler and runtime support for structured and block structured applications. </title> <booktitle> In Proceedings Supercomputing '93, </booktitle> <pages> pages 578-587. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1993. </year>
Reference-contexts: The CHAOS++ runtime library assumes no special compiler support, and does not rely on any architecture-dependent parallel system features. Integration with the CHAOS runtime library, for array-based adaptive irregular applications has already been accomplished, and integration with the Multiblock PARTI runtime library <ref> [1, 30] </ref>, for multiple structured grid applications, is currently in progress. CHAOS++ is targeted as a prototype library that will be used to provide part of the runtime support needed for High Performance Fortran and High Performance C/C++ compilers.
Reference: [2] <author> Scott B. Baden, Scott R. Kohn, and Stephen J. Fink. </author> <title> Programming with LPARX. </title> <type> Technical Report CS94-377, </type> <institution> Dept. of Computer Science and Engineering, University of California, </institution> <address> San Diego, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: However, accesses to remote elements in pC++ are implemented through asynchronous communication. The second type of system is a user-level class library that assumes no special support from the compiler, just like CHAOS++. Examples include P++ [22] and LPARX <ref> [2, 15] </ref>. P++ is a 16 parallel array class library that permits array objects to be distributed across arbitrary collections of processors. Parallel array operations are carried out by operators that are overloaded to work on all the elements in an array in parallel.
Reference: [3] <author> Robert Bennett, Kelvin Bryant, Alan Sussman, Raja Das, and Joel Saltz. Jovian: </author> <title> A framework for optimizing parallel I/O. </title> <booktitle> In Proceedings of the 1994 Scalable Parallel Libraries Conference. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1994. </year>
Reference-contexts: For every specified sensor image, the algorithm first computes a sub-image that spatially contains the whole query region, and partitions the sub-image across the processors to obtain good load balance. Jovian I/O library <ref> [3] </ref> routines are then invoked to read the computed sub-image from the disks and distribute the data vectors as specified. The Jovian library has been designed to optimize the I/O performance of multiprocessor architectures with multiple disks or disk arrays. <p> We are in the process of integrating CHAOS++ into the runtime software being developed by the Parallel Compiler Runtime Consortium. Finally, we have also linked CHAOS++ to the Jovian I/O library <ref> [3] </ref>. The resulting software is intended to support disk-based data parallel applications with datasets that do not fit in the processor memory of even very large parallel machines, such as the VIM application from the UMd Grand Challenge project in Land Cover Dynamics.
Reference: [4] <author> Robert Bennett, Kelvin Bryant, Alan Sussman, Raja Das, and Joel Saltz. </author> <title> Collective I/O: Models and implementation. </title> <institution> Technical Report CS-TR-3429 and UMIACS-TR-95-29, University of Maryland, Department of Computer Science and UMIACS, </institution> <month> February </month> <year> 1995. </year> <note> Submitted to Third Workshop on I/O in Parallel and Distributed Systems (IOPADS). </note>
Reference-contexts: A detailed description of the I/O performance of the VIM application using the Jovian library is given in <ref> [4] </ref>. 14 Number of processors Intel iPSC/860 IBM SP-1 1 130.61 11.18 4 20.52 3.10 16 4.11 1.63 Table 3: Performance results for image segmentation. 4.3 Image Processing Image Segmentation The third application we have implemented is image segmentation.
Reference: [5] <author> K. Mani Chandy and Carl Kesselman. </author> <title> CC++: A declarative concurrent object oriented programming notation. </title> <type> Technical Report CS-TR-92-01, </type> <institution> Department of Computer Science, California Institute of Technology, </institution> <year> 1992. </year>
Reference-contexts: Parallelism is exploited by assigning objects to different processors so that com putation is carried out on different objects concurrently. A number of concurrent object-oriented systems (e.g., <ref> [5, 13, 11, 19] </ref>) have been proposed in the literature to provide support for a global object name space and remote method invocation. <p> It effectively consists of a processor identifier and a local pointer (that is only valid on the named processor), and has been implemented as part of several language extensions, including Split-C [16], CC++ <ref> [5] </ref>, and pC++ [19]. 3 The CHAOS++ Runtime Library CHAOS++ is designed to effectively support applications that contain complex data types and pointer-based data structures. <p> Such systems provide a global object name space, implement the communication necessary for remote method invocation, and schedule method invocations on objects. Parallelism is exploited by both a compiler and an associated runtime system. Examples of this type of systems include Mentat [10, 11], CC++ <ref> [5] </ref>, Charm++ [13], pC++ [19], C** [17] and Concert [6]. These systems differ in several ways, including the granularity of objects, the way parallelism is expressed, the implementation of synchronization, the policy of scheduling method invocations, and the means through which data is shared.
Reference: [6] <author> Andrew A. Chien, Vijay Karamcheti, and John Plevyak. </author> <title> The Concert system compiler and run-time support for efficient, fine-grained concurrent object-oriented programs. </title> <type> Technical Report 1815, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: Parallelism is exploited by both a compiler and an associated runtime system. Examples of this type of systems include Mentat [10, 11], CC++ [5], Charm++ [13], pC++ [19], C** [17] and Concert <ref> [6] </ref>. These systems differ in several ways, including the granularity of objects, the way parallelism is expressed, the implementation of synchronization, the policy of scheduling method invocations, and the means through which data is shared.
Reference: [7] <author> R. Das, D. J. Mavriplis, J. Saltz, S. Gupta, and R. Ponnusamy. </author> <title> The design and implementation of a parallel unstructured Euler solver using software primitives. </title> <journal> AIAA Journal, </journal> <volume> 32(3) </volume> <pages> 489-496, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Good performance on distributed memory architectures has been achieved by applying such runtime techniques to various problems with irregular data access patterns, such as molecular dynamics for computational chemistry [12], particle-in-cell (PIC) codes for computational aerodynamics [20], and computational fluid dynamics <ref> [7] </ref>. However, many existing runtime systems for distributed memory parallel machines fail to handle pointers, which are often used in object-oriented applications to construct complex objects and data structures. <p> This makes it possible to utilize various preprocessing strategies to optimize the computation and communication. The CHAOS runtime library <ref> [7] </ref> has been developed to efficiently handle such adaptive and irregular problems. It employs various runtime techniques to optimize data movement between processor memories. CHAOS carries out its optimization through two phases, the inspector phase and the executor phase [27].
Reference: [8] <author> Raja Das, Mustafa Uysal, Joel Saltz, and Yuan-Shin Hwang. </author> <title> Communication optimizations for irregular scientific computations on distributed memory architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 22(3) </volume> <pages> 462-479, </pages> <month> September </month> <year> 1994. </year> <note> Also available as University of Maryland Technical Report CS-TR-3163 and UMIACS-TR-93-109. </note>
Reference-contexts: The runtime library we have designed and implemented to solve these problems is called CHAOS++. CHAOS++ is a runtime library targeted at parallel object-oriented applications with dynamic communication patterns. It subsumes CHAOS <ref> [8] </ref>, which is a runtime library developed to efficiently support applications with irregular patterns of access to distributed arrays.
Reference: [9] <author> Geoffrey C. Fox, Roy D. Williams, and Paul C. Messina. </author> <title> Parallel Computing Works. </title> <publisher> Morgan Kaufman, </publisher> <year> 1994. </year>
Reference-contexts: On the other hand, a large class of applications have been developed to execute on distributed memory parallel computers using the single-program multiple-data (SPMD) mode, in a loosely synchronous manner <ref> [9] </ref>. That is, collections of data objects are partitioned among the local memories of processors, often in an irregular fashion for better locality or load balance, and the program executes a sequence of parallel computational phases.
Reference: [10] <author> A. S. Grimshaw. </author> <title> Easy to use object-oriented parallel programming with Mentat. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 39-51, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Such systems provide a global object name space, implement the communication necessary for remote method invocation, and schedule method invocations on objects. Parallelism is exploited by both a compiler and an associated runtime system. Examples of this type of systems include Mentat <ref> [10, 11] </ref>, CC++ [5], Charm++ [13], pC++ [19], C** [17] and Concert [6]. These systems differ in several ways, including the granularity of objects, the way parallelism is expressed, the implementation of synchronization, the policy of scheduling method invocations, and the means through which data is shared.
Reference: [11] <author> Andrew S. Grimshaw, Jon B. Weissman, and W. Timothy Stayer. </author> <title> Portable run-time support for dynamic object-oriented parallel processing. </title> <type> Technical Report CS-93-40, </type> <institution> Dept. of Computer Science, University of Virginia, </institution> <address> Charlottesville, Virginia 22903, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Parallelism is exploited by assigning objects to different processors so that com putation is carried out on different objects concurrently. A number of concurrent object-oriented systems (e.g., <ref> [5, 13, 11, 19] </ref>) have been proposed in the literature to provide support for a global object name space and remote method invocation. <p> Such systems provide a global object name space, implement the communication necessary for remote method invocation, and schedule method invocations on objects. Parallelism is exploited by both a compiler and an associated runtime system. Examples of this type of systems include Mentat <ref> [10, 11] </ref>, CC++ [5], Charm++ [13], pC++ [19], C** [17] and Concert [6]. These systems differ in several ways, including the granularity of objects, the way parallelism is expressed, the implementation of synchronization, the policy of scheduling method invocations, and the means through which data is shared.
Reference: [12] <author> Yuan-Shin Hwang, Raja Das, Joel Saltz, Bernard Brooks, and Milan Hodoscek. </author> <title> Parallelizing molecular dynamics programs for distributed memory machines: An application of the CHAOS runtime support library. </title> <institution> Technical Report CS-TR-3374 and UMIACS-TR-94-125, University of Maryland, Department of Computer Science and UMIACS, </institution> <month> November </month> <year> 1994. </year> <note> To appear in IEEE Computational Science and Engineering. </note>
Reference-contexts: Optimizations that can be carried out by compilers are therefore limited, and runtime analysis is often required [27]. Good performance on distributed memory architectures has been achieved by applying such runtime techniques to various problems with irregular data access patterns, such as molecular dynamics for computational chemistry <ref> [12] </ref>, particle-in-cell (PIC) codes for computational aerodynamics [20], and computational fluid dynamics [7]. However, many existing runtime systems for distributed memory parallel machines fail to handle pointers, which are often used in object-oriented applications to construct complex objects and data structures.
Reference: [13] <author> L.V. Kale and Sanjeev Krishnan. </author> <title> CHARM++ : A portable concurrent object oriented system based on C++. </title> <booktitle> In Proceedings of the 1993 Conference on Object-Oriented Programming Systems, Languages, and Applications, </booktitle> <pages> pages 91-108, </pages> <address> Washington, DC, </address> <month> October </month> <year> 1993. </year> <note> ACM. </note>
Reference-contexts: Parallelism is exploited by assigning objects to different processors so that com putation is carried out on different objects concurrently. A number of concurrent object-oriented systems (e.g., <ref> [5, 13, 11, 19] </ref>) have been proposed in the literature to provide support for a global object name space and remote method invocation. <p> Such systems provide a global object name space, implement the communication necessary for remote method invocation, and schedule method invocations on objects. Parallelism is exploited by both a compiler and an associated runtime system. Examples of this type of systems include Mentat [10, 11], CC++ [5], Charm++ <ref> [13] </ref>, pC++ [19], C** [17] and Concert [6]. These systems differ in several ways, including the granularity of objects, the way parallelism is expressed, the implementation of synchronization, the policy of scheduling method invocations, and the means through which data is shared.
Reference: [14] <author> C. Koelbel, D. Loveman, R. Schreiber, G. Steele, Jr., and M. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: structures to express associations between data objects, and data movement is usually closely related to the semantics of the data structures. pC++ provides an aggregate called a collection, whose elements are mapped to a set of processors using templates that are similar to those provided by High Performance Fortran (HPF) <ref> [14] </ref>. In pC++, concurrent method invocations on objects (elements) in a collection are effectively carried out in parallel, assuming there are no data dependencies between method invocations. However, accesses to remote elements in pC++ are implemented through asynchronous communication.
Reference: [15] <author> S.R. Kohn and S.B. Baden. </author> <title> A robust parallel programming model for dynamic non-uniform scientific computations. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference (SHPCC-94), </booktitle> <pages> pages 509-517. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> May </month> <year> 1994. </year> <month> 18 </month>
Reference-contexts: However, accesses to remote elements in pC++ are implemented through asynchronous communication. The second type of system is a user-level class library that assumes no special support from the compiler, just like CHAOS++. Examples include P++ [22] and LPARX <ref> [2, 15] </ref>. P++ is a 16 parallel array class library that permits array objects to be distributed across arbitrary collections of processors. Parallel array operations are carried out by operators that are overloaded to work on all the elements in an array in parallel.
Reference: [16] <author> A. Krishnamurthy, D.E. Culler, A. Dusseau, S.C. Goldstein, S. Lumetta, T. von Eicken, and K. Yelick. </author> <title> Parallel programming in Split-C. </title> <booktitle> In Proceedings Supercomputing '93, </booktitle> <pages> pages 262-273. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1993. </year>
Reference-contexts: It effectively consists of a processor identifier and a local pointer (that is only valid on the named processor), and has been implemented as part of several language extensions, including Split-C <ref> [16] </ref>, CC++ [5], and pC++ [19]. 3 The CHAOS++ Runtime Library CHAOS++ is designed to effectively support applications that contain complex data types and pointer-based data structures.
Reference: [17] <author> James R. Larus, Brad Richards, and Guhan Viswanathan. </author> <title> C**: A large-grain, object-oriented, data-parallel programming language. </title> <type> Technical Report 1126, </type> <institution> University of Wisconsin-Madison, </institution> <month> November </month> <year> 1992. </year>
Reference-contexts: Parallelism is exploited by both a compiler and an associated runtime system. Examples of this type of systems include Mentat [10, 11], CC++ [5], Charm++ [13], pC++ [19], C** <ref> [17] </ref> and Concert [6]. These systems differ in several ways, including the granularity of objects, the way parallelism is expressed, the implementation of synchronization, the policy of scheduling method invocations, and the means through which data is shared.
Reference: [18] <author> Kai Li and Paul Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: However, the techniques used in the library can also be applied to other parallel environments that provide a standard C++ compiler and a mechanism for off-processor data accesses, including various distributed shared memory architectures <ref> [18, 21, 29] </ref>. The remainder of the paper is structured as follows. In Section 2, we give a brief overview of the CHAOS runtime library and discuss issues related to the additional runtime support required for parallel object-oriented applications.
Reference: [19] <author> A. Malony, B. Mohr, P. Beckman, D. Gannon, S. Yang, F. Bodin, and S. Kesavan. </author> <title> Implementing a parallel C++ runtime system for scalable parallel systems. </title> <booktitle> In Proceedings Supercomputing '93, </booktitle> <pages> pages 588-597. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1993. </year>
Reference-contexts: Parallelism is exploited by assigning objects to different processors so that com putation is carried out on different objects concurrently. A number of concurrent object-oriented systems (e.g., <ref> [5, 13, 11, 19] </ref>) have been proposed in the literature to provide support for a global object name space and remote method invocation. <p> It effectively consists of a processor identifier and a local pointer (that is only valid on the named processor), and has been implemented as part of several language extensions, including Split-C [16], CC++ [5], and pC++ <ref> [19] </ref>. 3 The CHAOS++ Runtime Library CHAOS++ is designed to effectively support applications that contain complex data types and pointer-based data structures. <p> Such systems provide a global object name space, implement the communication necessary for remote method invocation, and schedule method invocations on objects. Parallelism is exploited by both a compiler and an associated runtime system. Examples of this type of systems include Mentat [10, 11], CC++ [5], Charm++ [13], pC++ <ref> [19] </ref>, C** [17] and Concert [6]. These systems differ in several ways, including the granularity of objects, the way parallelism is expressed, the implementation of synchronization, the policy of scheduling method invocations, and the means through which data is shared.
Reference: [20] <author> B. Moon and J. Saltz. </author> <title> Adaptive runtime support for direct simulation Monte Carlo methods on distributed memory architectures. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference (SHPCC-94), </booktitle> <pages> pages 176-183. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> May </month> <year> 1994. </year>
Reference-contexts: Good performance on distributed memory architectures has been achieved by applying such runtime techniques to various problems with irregular data access patterns, such as molecular dynamics for computational chemistry [12], particle-in-cell (PIC) codes for computational aerodynamics <ref> [20] </ref>, and computational fluid dynamics [7]. However, many existing runtime systems for distributed memory parallel machines fail to handle pointers, which are often used in object-oriented applications to construct complex objects and data structures. <p> Mesh cells are distributed among the processors to achieve a good load balance. Since particles move between mesh cells as the program executes, the cells are redistributed across the processors occasionally (once every few time steps) to maintain good load balance. Moon <ref> [20] </ref> describes a parallel implementation of the DSMC application that uses the CHAOS runtime library. <p> Table 1 shows the performance of both the C++/CHAOS++ code and the Fortran/CHAOS code. The simulated space consists of 9720 cells, and initially contains about 48600 particles. 400 time steps are performed, and a chain partitioner <ref> [20] </ref> is used to dynamically partition the mesh cells at runtime when load imbalance is detected. The Fortran code has been shown to be a good implementation, and the C++ version is at most 15% slower.
Reference: [21] <author> Bill Nitzberg and Virginia Lo. </author> <title> Distributed shared memory: A survey of issues and algorithms. </title> <journal> IEEE Computer, </journal> <volume> 24(8) </volume> <pages> 52-60, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: However, the techniques used in the library can also be applied to other parallel environments that provide a standard C++ compiler and a mechanism for off-processor data accesses, including various distributed shared memory architectures <ref> [18, 21, 29] </ref>. The remainder of the paper is structured as follows. In Section 2, we give a brief overview of the CHAOS runtime library and discuss issues related to the additional runtime support required for parallel object-oriented applications.
Reference: [22] <author> R. Parsons and D. Quinlan. </author> <title> Run-time recognition of task parallelism within the P++ class library. </title> <booktitle> In Proceedings of the 1993 Scalable Parallel Libraries Conference, </booktitle> <pages> pages 77-86. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1993. </year>
Reference-contexts: However, accesses to remote elements in pC++ are implemented through asynchronous communication. The second type of system is a user-level class library that assumes no special support from the compiler, just like CHAOS++. Examples include P++ <ref> [22] </ref> and LPARX [2, 15]. P++ is a 16 parallel array class library that permits array objects to be distributed across arbitrary collections of processors. Parallel array operations are carried out by operators that are overloaded to work on all the elements in an array in parallel.
Reference: [23] <author> Rahul Parulekar, Larry Davis, Rama Chellappa, Joel Saltz, Alan Sussman, and John Towhshend. </author> <title> High performance computing for land cover dynamics. </title> <booktitle> In Proceedings of the International Joint Conference on Pattern Recognition, </booktitle> <month> September </month> <year> 1994. </year>
Reference-contexts: It has been developed as part of the on-going Grand Challenge project in Land Cover Dynamics at the University of Maryland <ref> [23] </ref>. The overall project involves developing scalable and portable parallel programs for a variety of image and map data processing applications, eventually integrated with new models for parallel I/O of large-scale images and maps.
Reference: [24] <author> D.F.G. Rault and M.S. Woronowicz. </author> <title> Spacecraft contamination investigation by direct simulation Monte Carlo contamination on UARS/HALOE. </title> <booktitle> In Proceedings AIAA 31th Aerospace Sciences Meeting and Exhibit, </booktitle> <address> Reno, Nevada, </address> <month> January </month> <year> 1993. </year>
Reference-contexts: The method includes movement and collision handling of simulated particles on a spatial flow field domain overlaid by a Cartesian mesh <ref> [24] </ref>. Depending upon its current spatial location, each particle is associated with a mesh cell, which typically contains a few particles, and moves from cell to cell as it participates in collisions and various boundary interactions in the simulated physical space.
Reference: [25] <author> Patrick J. Roache. </author> <title> Computational Fluid Dynamics, volume 1. </title> <publisher> Hermosa Publishers, </publisher> <address> Albuquerque, N.M., </address> <year> 1972. </year>
Reference-contexts: Physical quantities such as velocity components, rotational energy and position coordinates are associated with each particle, and are modified as the simulation evolves. What distinguishes the DSMC method from other Particle-in-Cell (PIC) methods is that the movement and collision processes of particles are completely uncoupled over a time step <ref> [25] </ref>. The parallel implementation of the method is relatively straightforward. Mesh cells are distributed among the processors to achieve a good load balance.
Reference: [26] <author> Claudia Rodrguez. </author> <title> An appearance-based approach to object recognition in aerial images. </title> <type> Master's thesis, </type> <institution> University of Maryland, College Park, MD 20742, </institution> <year> 1994. </year>
Reference-contexts: This application segments a given image into a hierarchy of components, based on the border contrast between the components, and serves as a preprocessing phase for an appearance-based object recognition system developed at the University of Maryland <ref> [26] </ref>. The hierarchy generated by the preprocessing is used by a high-level vision phase to heuristically combine components from various levels of the hierarchy into possible instances of objects. Further analysis by shape delineation processes would select the combinations that correspond to the locally best instances of objects.
Reference: [27] <author> Joel H. Saltz, Ravi Mirchandaney, and Kay Crowley. </author> <title> Run-time parallelization and scheduling of loops. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 40(5) </volume> <pages> 603-612, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Optimizations that can be carried out by compilers are therefore limited, and runtime analysis is often required <ref> [27] </ref>. Good performance on distributed memory architectures has been achieved by applying such runtime techniques to various problems with irregular data access patterns, such as molecular dynamics for computational chemistry [12], particle-in-cell (PIC) codes for computational aerodynamics [20], and computational fluid dynamics [7]. <p> The CHAOS runtime library [7] has been developed to efficiently handle such adaptive and irregular problems. It employs various runtime techniques to optimize data movement between processor memories. CHAOS carries out its optimization through two phases, the inspector phase and the executor phase <ref> [27] </ref>. During program execution, the CHAOS inspector routines examine the data references, given in global indices, and precompute the locations of the data each processor needs to send and receive.
Reference: [28] <author> Shamik D. Sharma, Ravi Ponnusamy, Bongki Moon, Yuan-Shin Hwang, Raja Das, and Joel Saltz. </author> <title> Run-time and compile-time support for adaptive irregular problems. </title> <booktitle> In Proceedings Supercomputing '94, </booktitle> <pages> pages 97-106. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1994. </year>
Reference-contexts: CHAOS also provides primitives to redistribute data arrays efficiently at runtime to improve data locality and load balance. Special attention has been devoted to optimizing the inspector routines for adaptive applications, where data access patterns change occasionally at runtime, so that communication patterns cannot be reused many times <ref> [28] </ref>. 2.2 Issues in Runtime Support for Pointer-Based Data Structures The CHAOS runtime library has been successfully applied to irregular and adaptive problems that use distributed arrays of primitive data types (integers, double precisions, etc.).
Reference: [29] <author> J.P. Singh, T. Joe, J.L. Hennessy, and A. Gupta. </author> <title> An empirical comparison of the Kendall Square Research KSR-1 and Stanford DASH multiprocessors. </title> <booktitle> In Proceedings Supercomputing '93, </booktitle> <pages> pages 214-225. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1993. </year>
Reference-contexts: However, the techniques used in the library can also be applied to other parallel environments that provide a standard C++ compiler and a mechanism for off-processor data accesses, including various distributed shared memory architectures <ref> [18, 21, 29] </ref>. The remainder of the paper is structured as follows. In Section 2, we give a brief overview of the CHAOS runtime library and discuss issues related to the additional runtime support required for parallel object-oriented applications.
Reference: [30] <author> Alan Sussman, Gagan Agrawal, and Joel Saltz. </author> <title> A manual for the multiblock PARTI runtime primitives, revision 4.1. </title> <institution> Technical Report CS-TR-3070.1 and UMIACS-TR-93-36.1, University of Maryland, Department of Computer Science and UMIACS, </institution> <month> December </month> <year> 1993. </year> <month> 19 </month>
Reference-contexts: The CHAOS++ runtime library assumes no special compiler support, and does not rely on any architecture-dependent parallel system features. Integration with the CHAOS runtime library, for array-based adaptive irregular applications has already been accomplished, and integration with the Multiblock PARTI runtime library <ref> [1, 30] </ref>, for multiple structured grid applications, is currently in progress. CHAOS++ is targeted as a prototype library that will be used to provide part of the runtime support needed for High Performance Fortran and High Performance C/C++ compilers.
References-found: 30

