URL: http://www.demo.cs.brandeis.edu/papers/adr97.ps
Refering-URL: http://www.demo.cs.brandeis.edu/papers/long.html
Root-URL: http://www.cs.brandeis.edu
Email: blair@cs.brandeis.edu pollack@cs.brandeis.edu  
Title: Analysis of Dynamical Recognizers  
Author: Alan D. Blair Jordan B. Pollack 
Date: September 7, 1995 (revised June 14, 1996)  
Address: Waltham, MA 02254-9110  
Affiliation: Dept. of Computer Science Volen Center for Complex Systems Brandeis University  
Abstract: Pollack (1991) demonstrated that second-order recurrent neural networks can act as dynamical recognizers for formal languages when trained on positive and negative examples, and observed both phase transitions in learning and IFS-like fractal state sets. Follow-on work focused mainly on the extraction and minimization of a finite state automaton (FSA) from the trained network. However, such networks are capable of inducing languages which are not regular, and therefore not equivalent to any FSA. Indeed, it may be simpler for a small network to fit its training data by inducing such a non-regular language. But when is the network's language not regular? In this paper, using a low dimensional network capable of learning all the Tomita data sets, we present an empirical method for testing whether the language induced by the network is regular or not. We also provide a detailed "-machine analysis of trained networks for both regular and non-regular languages. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Barnsley, M.F. </author> <year> 1988. </year> <title> Fractals Everywhere, </title> <publisher> (Academic Press, </publisher> <address> San Diego, CA). </address>
Reference-contexts: Finally, we remark that the functions w 0 and w 1 map X continuously into itself and as such define an Iterated Function System or IFS <ref> (Barnsley, 1988) </ref> as noted in (Kolen, 1994). <p> For this reason we call A 0 a subattractor of the IFS. If w 0 and w 1 were contractive maps, A 0 would contain the whole of A <ref> (Barnsley, 1988) </ref>, but in our case they are generally not contractive so ^ A 0 may contain only part of ^ A.
Reference: <author> Casey, M. </author> <year> 1996. </year> <title> The Dynamics of Discrete-Time Computation, with Application to Recurrent Neural Networks and Finite State Machine Extraction, </title> <booktitle> Neural Computation 8(6). </booktitle>
Reference-contexts: below, this architecture was adequate to the task of learning any of the data sets from (Tomita, 1982) within a few hundred training epochs. 3 Analysis Our purpose here is not only to train the network but also to gain some insight into how it accomplishes its assigned task [see <ref> (Casey, 1996) </ref> for another, related approach to this problem]. If the recurrent layer has d nodes, the state space of the system is the d-dimensional hypercube X = [1; 1] d .
Reference: <author> Casey, M. </author> <year> 1993. </year> <title> Computation Dynamics in Discrete-Time Recurrent Neural Networks, </title> <booktitle> Proceedings of the Annual Research Symposium of UCSD Institute for Neural Computation, </booktitle> <pages> 78-95. </pages>
Reference: <author> Cleeremans, A., D. Servan-Schreiber & J. McClelland, </author> <year> 1989. </year> <title> Finite State Automata and Simple Recurrent Networks, </title> <journal> Neural Computation, </journal> <volume> 1(3), </volume> <pages> 372-381. </pages>
Reference: <author> Crutchfield, J.P. </author> <year> 1994. </year> <title> The Calculi of Emergence: Computation, Dynamics and Induction, </title> <journal> Physica D, </journal> <volume> 75, </volume> <pages> 11-54. </pages>
Reference: <author> Crutchfield, J.P. & K. Young, </author> <year> 1990. </year> <title> Computation at the Onset of Chaos. </title> <editor> In Zurek, W.H., ed. </editor> <title> Complexity, Entropy and the Physics of Information (Addison-Wesley, </title> <address> Reading, MA). </address> <note> 13 Das, </note> <author> S. & M.C. Mozer, </author> <year> 1994. </year> <title> A Unified Gradient-Descent/Clustering Architecture for Finite State Machine Induction, </title> <booktitle> Neural Information Processing Systems 6, </booktitle> <pages> 19-26. </pages>
Reference-contexts: In discrete form, the above procedure may be equated with the Hopcroft Minimization Algorithm (Hopcroft & Ullman, 1979), or the method of "-machines <ref> (Crutchfield & Young, 1990, Crutchfield, 1994) </ref>, and was first used in the present context by Giles et al. (1992).
Reference: <author> Fahlman, S.E. </author> <year> 1989. </year> <title> Fast-learning variations on back-propagation: an empirical study. </title> <editor> In D. Touretzky, G. Hinton & T. Sejnowski, eds. </editor> <booktitle> Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <address> Pittsburgh, PA, </address> <publisher> 38-51 (Morgan Kaufman, </publisher> <address> San Mateo). </address>
Reference-contexts: by the network as x 0 varies, though we do not pursue this line of enquiry here. 4 Results Networks with the architecture described in x2 were trained to recognize formal languages using backpropagation through time (Williams & Zipser, 1989, Rumelhart et al., 1986), with a modification similar to Quickprop <ref> (Fahlman, 1989) </ref> 1 and a learning rate of 0:03.
Reference: <author> Forcada, </author> <title> M.L. & R.C. </title> <address> Carrasco, </address> <year> 1995. </year> <title> Learning the initial state of a second-order recurrent neural network during regular-language inference, </title> <journal> Neural Computation, </journal> <volume> 7(5), </volume> <pages> 923-930. </pages>
Reference: <author> Frasconi, P., M. Gori & G. Soda, </author> <year> 1995. </year> <title> Recurrent Neural Networks and Prior Knowledge for Sequence Processing: A Constrained Nondeterministic Approach, </title> <journal> Knowledge Based Systems, </journal> <volume> 8(6), </volume> <pages> 313-332. </pages>
Reference: <author> Giles, C.L., C.B. Miller, D. Chen, H.H. Chen, G.Z. Sun & Y.C. Lee, </author> <year> 1992. </year> <title> Learning and Extracting Finite State Automata with Second-Order Recurrent Neural Networks, </title> <booktitle> Neural Computation 4(3), </booktitle> <pages> 393-405. </pages>
Reference-contexts: Casey (1996) showed that if the network robustly models an FSA, the method proposed in <ref> (Giles et al., 1992) </ref> will successfully extract the FSA given a fine enough resolution. However in many cases the language induced by the network is not regular, and therefore cannot be modeled exactly by any FSA. <p> through this subnetwork W t to produce the next state x t = w t (x t1 ) given by x t = tanh (W j0 t + k=1 t x k This part of the architecture is equivalent to the second order recurrent networks used in (Pollack, 1991) and <ref> (Giles et al., 1992) </ref>, with a slightly different notation. <p> Of course in practice we cannot implement the above procedure exactly for our trained networks, but must instead use a discrete approximation. Following the analysis of <ref> (Giles et al., 1992) </ref> we can approximate A 0 computationally along the lines of (3.1) as follows: first `discretize' the system at resolution r by dividing the state space into r d points in a regular lattice and approximating w 0 & w 1 with functions ^w 0 & ^w 1 <p> The details of the successive subdivisions are outlined in 10 The above examples would also be amenable to previous, clustering-based, ap-proaches because of the way they `partition [their] state space into fairly well-separated, distinct regions or clusters' as hypothesized in <ref> (Giles et al., 1992) </ref>. Those shown in of scattered points with no obvious clustering, yet our fine-grained analysis was able to extract from it a 7-node FSA a little larger than the minimal FSA of 4 nodes found by Tomita. Network N2 (left) seems to have induced a non-regular language.
Reference: <author> Hopcroft, J.E. & J.D. Ullman, </author> <year> 1979. </year> <title> Introduction to Automata Theory, Languages, </title> <publisher> and Computation (Addison-Wesley, </publisher> <address> Reading, MA). </address>
Reference-contexts: In discrete form, the above procedure may be equated with the Hopcroft Minimization Algorithm <ref> (Hopcroft & Ullman, 1979) </ref>, or the method of "-machines (Crutchfield & Young, 1990, Crutchfield, 1994), and was first used in the present context by Giles et al. (1992).
Reference: <author> Jordan, M.I. </author> <year> 1986. </year> <title> Attractor dynamics and parallelism in a connectionist sequential machine, </title> <booktitle> Proceedings of the Eighth Conference of the Cognitive Science Society, </booktitle> <address> Amherst, MA, </address> <pages> 531-546. </pages>
Reference-contexts: 1 Introduction Pollack (1991) showed one way a recurrent network may be trained to recognize formal languages from examples. The resulting networks often displayed complex limit dynamics which were fractal in nature (Kolen, 1993). Alternative architectures had been employed earlier for related tasks <ref> (Jordan, 1986, Pollack, 1987, Cleeremans et al., 1989) </ref>. Others have been proposed since (Watrous & Kuhn, 1992, Frasconi et al., 1992, Zeng et al., 1994, Das & Mozer, 1994, Forcada & Carrasco, 1995) and a number of approaches to analysing recurrent networks have been developed.
Reference: <author> Kolen, J.F. </author> <year> 1993. </year> <title> Fool's Gold: Extracting Finite State Machines from Recurrent Network Dynamics, </title> <booktitle> Neural Information Processing Systems 6, </booktitle> <pages> 501-508. </pages>
Reference-contexts: 1 Introduction Pollack (1991) showed one way a recurrent network may be trained to recognize formal languages from examples. The resulting networks often displayed complex limit dynamics which were fractal in nature <ref> (Kolen, 1993) </ref>. Alternative architectures had been employed earlier for related tasks (Jordan, 1986, Pollack, 1987, Cleeremans et al., 1989).
Reference: <author> Kolen, J.F. </author> <year> 1994. </year> <title> Exploring the Computational Capabilities of Recurrent Neural Networks, </title> <type> Ph.D. Thesis, </type> <institution> Ohio State University. </institution>
Reference-contexts: Finally, we remark that the functions w 0 and w 1 map X continuously into itself and as such define an Iterated Function System or IFS (Barnsley, 1988) as noted in <ref> (Kolen, 1994) </ref>.
Reference: <author> Lang, K.J. </author> <year> 1992. </year> <title> Random DFA's can be Approximately Learned from Sparse Uniform Examples, </title> <booktitle> Proc. Fifth ACM Workshop on Computational Learning Theory, </booktitle> <pages> 45. </pages>
Reference: <author> Manolios, P. & R. Fanelli, </author> <year> 1994. </year> <title> First order recurrent neural networks and deterministic finite state automata, </title> <booktitle> Neural Computation 6(6), </booktitle> <pages> 1155-1173. </pages>
Reference: <author> Omlin, C.W. & C.L. Giles, </author> <year> 1996. </year> <title> Extraction of Rules from Discrete-Time Recurrent Neural Networks, </title> <booktitle> Neural Networks, </booktitle> <volume> 9(1), </volume> <pages> 41. </pages>
Reference-contexts: Many researchers implicitly regard an extracted FSA as superior to the trained network from which it was extracted <ref> (Omlin & Giles, 1996) </ref> with regard to predictability, compactness of description, and to the particular way each of them `generalizes' to classify new, unseen input strings. For this reason, earlier work in the field had focused on extracting an FSA which approximates the behavior of the network.
Reference: <author> Pollack, J.B. </author> <year> 1991. </year> <title> The Induction of Dynamical Recognizers, </title> <booktitle> Machine Learning 7, </booktitle> <pages> 227-252. </pages>
Reference-contexts: is then fed through this subnetwork W t to produce the next state x t = w t (x t1 ) given by x t = tanh (W j0 t + k=1 t x k This part of the architecture is equivalent to the second order recurrent networks used in <ref> (Pollack, 1991) </ref> and (Giles et al., 1992), with a slightly different notation.
Reference: <author> Pollack, J.B. </author> <year> 1987. </year> <title> Cascaded back propagation on dynamic connectionist networks, </title> <booktitle> Proceedings of the Ninth Annual Conference of the Cognitive Science Society, </booktitle> <address> Seattle, WA, </address> <pages> 391-404. </pages>
Reference: <author> Rumelhart, D.E., G.E. Hinton & R.J. Williams, </author> <year> 1986. </year> <title> Learning representations by back-propagating errors, </title> <booktitle> Nature 323, </booktitle> <pages> 533-536. </pages>
Reference: <editor> Siegelmann, H.T. & E.D. Sontag, </editor> <booktitle> 1992. On the computational power of neural networks, Proceedings of the Fifth ACM Workshop on Computational Learning Theory, </booktitle> <address> Pittsburgh, PA. </address>
Reference-contexts: The class of languages recognizable by such networks lies somewhere between the regular and recursive language families, and is known to contain some languages that are not context-free <ref> (Siegelmann & Sontag, 1992) </ref>. In the work reported here the state-space dimension d was always equal to 2, which gives the model a total of 17 free parameters - 3 for the perceptron, 6 for each subnetwork and 2 for the initial point.
Reference: <author> Tino, P., Bill G. Horne, C.L. Giles & P.C. Collingwood, </author> <year> 1995. </year> <title> Finite State Machines and Recurrent Neural Networks Automata and Dynamical Systems Approaches, </title> <type> Tech. </type> <institution> Rept. UMIACS-TR-95-1, Institute for Advanced Computer Studies, University of Maryland. </institution> <note> 14 Tino, </note> <author> P. & J. Sajda, </author> <year> 1995. </year> <title> Learning and Extracting Initial Mealy Automata with a Modular Neural Network Model, </title> <booktitle> Neural Computation 7(4), </booktitle> <pages> 822. </pages>
Reference: <author> Tomita, M. </author> <year> 1982. </year> <title> Dynamic construction of finite-state automata from examples using hill-climbing, </title> <booktitle> Proceedings of the Fourth Annual Cognitive Science Conference, </booktitle> <address> Ann Arbor, MI, </address> <pages> 105-108. </pages>
Reference-contexts: As outlined below, this architecture was adequate to the task of learning any of the data sets from <ref> (Tomita, 1982) </ref> within a few hundred training epochs. 3 Analysis Our purpose here is not only to train the network but also to gain some insight into how it accomplishes its assigned task [see (Casey, 1996) for another, related approach to this problem]. <p> In addition, we allowed the initial point x 0 to vary as part of the backpropagation a modification that was also developed in independent work by Forcada & Carrasco (1995). Our seven groups of training strings (see Appendix) were copied exactly from <ref> (Tomita, 1982) </ref> except that we did not include the empty string in our training sets. Rows 4 and 5 of Table 1 show the size (i.e. number of states) of the largest FSA generated from the trained networks at two different resolutions by the methods described in x3. <p> 3 that it accepts all nonempty strings of the form (10) fl , but rejects any string ending in 110. 5 Conclusion By allowing the decision boundary and the initial point to vary, our networks with two hidden nodes were able to induce languages from all the data sets of <ref> (Tomita, 1982) </ref> within a few hundred epochs.
Reference: <author> Trakhtenbrot, B.A. & Ya.M. </author> <month> Barzdin' </month> <year> 1973. </year> <title> Finite Automata; Behavior and Synthesis (North-Holland, </title> <publisher> Amsterdam). </publisher>
Reference: <author> Watrous, R.L. & G.M. Kuhn, </author> <year> 1992. </year> <title> Induction of Finite State Languages Using Second-Order Recurrent Networks, </title> <booktitle> Neural Computation 4(3), </booktitle> <pages> 406-414. </pages>
Reference: <author> Williams, R.J. & D. Zipser, </author> <year> 1989. </year> <title> A learning algorithm for continually running fully recurrent neural networks, </title> <booktitle> Neural Computation 1(2), </booktitle> <pages> 270. </pages>
Reference-contexts: ^ A should make it possible to analyse the general family of languages accepted by the network as x 0 varies, though we do not pursue this line of enquiry here. 4 Results Networks with the architecture described in x2 were trained to recognize formal languages using backpropagation through time <ref> (Williams & Zipser, 1989, Rumelhart et al., 1986) </ref>, with a modification similar to Quickprop (Fahlman, 1989) 1 and a learning rate of 0:03. <p> In contrast to (Pol-lack, 1991) where the backpropagation was truncated, we backpropagated through all levels of recurrence as in <ref> (Williams & Zipser, 1989) </ref>. In addition, we allowed the initial point x 0 to vary as part of the backpropagation a modification that was also developed in independent work by Forcada & Carrasco (1995).
Reference: <author> Zeng, Z., R.M. Goodman & P. Smyth, </author> <year> 1994. </year> <title> Learning Finite State Machines with Self-Clustering Recurrent Networks, </title> <booktitle> Neural Computation 5(6), </booktitle> <pages> 976-990. 15 </pages>
References-found: 27

