URL: http://polaris.cs.uiuc.edu/reports/1424.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: SOLVING LARGE NONSYMMETRIC SPARSE LINEAR SYSTEMS USING MCSPARSE  
Author: K. A. GALLIVAN B. A. MARSOLF AND H. A. G. WIJSHOFF 
Abstract: In this paper, the methods and implementation techniques used for the nonsymmetric sparse linear system solver, MCSPARSE, on the Cedar system are described. A novel reordering scheme (H*) upon which the solver is based is presented. The tradeoffs discussed include stability and fill-in control, hierarchical parallelism, and load balancing. Experimental results demonstrating the effectiveness of the solver with respect to each of these issues are presented. We also address the implications of this work for other parallel processing systems. 1. Introduction. Several techniques have been proposed to solve large nonsymmetric sparse systems of linear equations on parallel processors. In this paper, we present a new method for solving 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. ALAGHBAND, </author> <title> Parallel pivoting combined with parallel reduction and fill-in control, </title> <booktitle> Parallel Computing, 11 (1989), </booktitle> <pages> pp. 201-221. </pages>
Reference-contexts: However, the method tends to work well on matrices with a near-symmetric structure and the pivot sequence is constrained which can cause stability concerns. Another approach to parallel sparse solvers exploits the dynamic identification and application of parallel pivots <ref> [1, 8, 18, 43] </ref>. At each stage, a set of pivots that can be applied in parallel is constructed and the appropriate updates performed. These codes typically concentrate on medium and fine grain parallelism and tend to be most efficient on a moderate number of processors with fairly tight coupling.
Reference: [2] <author> M. ARIOLI AND I. S. DUFF, </author> <title> Experiments in tearing large sparse systems, in Reliable Numerical Computation, </title> <editor> M. G. Cox and S. Hammarling, eds., </editor> <publisher> Oxford University Press, </publisher> <address> New York, </address> <year> 1990, </year> <pages> pp. 207-226. </pages>
Reference-contexts: Several different methods for finding the bordered block triangular form have been proposed. Partitioning and tearing methods [41] can be used, and algorithms such as P 4 [31], P 5 [13], the Hierarchical Partition by Lin and Mah [35], and the level set algorithm by Arioli and Duff <ref> [2] </ref> were introduced for ordering the matrix into the desired form. <p> In the factorization phase, precautions have to be taken to guarantee a reasonably stable solution method. P 5 , the Hierarchical Method, and the level set algorithm guarantee structurally nonsingular blocks. However, these methods are still potentially unstable. Iterative refinement can be used to improve the instability, see <ref> [2] </ref>. In our method, stability is guaranteed by allowing pivots to be taken within the diagonal blocks as well as the border. This was also attempted with the P 4 ordering [3]; however, the overhead incurred prevented this approach from being competitive with other direct solvers. <p> Methods for dealing with these small blocks efficiently will be discussed in Section 7. The results of H* can be compared with the related orderings produced by the P 4 algorithm [31], the P 5 algorithm [13], and the level set algorithm by Arioli and Duff <ref> [2] </ref> on a subset of the Harwell-Boeing matrices, the results of which are available in the literature [2, 3]. This subset comprises the Grenoble matrices and the Westerberg's matrices. The matrices range in order from 67 to 2021. <p> The results of H* can be compared with the related orderings produced by the P 4 algorithm [31], the P 5 algorithm [13], and the level set algorithm by Arioli and Duff [2] on a subset of the Harwell-Boeing matrices, the results of which are available in the literature <ref> [2, 3] </ref>. This subset comprises the Grenoble matrices and the Westerberg's matrices. The matrices range in order from 67 to 2021. <p> 62 65 4 102 west0989 989 69 77 84 106 85 4 48 west1505 1505 79 116 127 112 145 4 79 west2021 2021 93 160 175 156 188 4 455 TABLE 2 Number of Rows in Border & Largest Diagonal Block Size from this table since, as indicated in <ref> [2] </ref>, P 5 usually generates blocks of size 1 or 2, with an occasional block of size 3. <p> This is not surprising given that they use nonsymmetric permutations, which are more flexible. Unfortunately, the small diagonal blocks and border have less than satisfactory properties when coupled with a factorization algorithm. As Arioli and Duff point out in <ref> [2] </ref>, the small diagonal block sizes can cause difficulties with both parallelism and the ability to choose stable pivots when the pivot searches are constrained to the diagonal blocks. <p> Finally, the resulting factorization cannot be characterized as simply as those that result from MCSPARSE's strategy. Other strategies discussed in the literature have resulted in solvers with either unacceptable cost or stability control, e.g., <ref> [2, 3] </ref>. The strategy we prefer is one which preserves, up to a point, the overall structure of the matrix while allowing the implementation of a global pivoting strategy that yields a factorization with stability similar to more conventional nonsymmetric solvers.
Reference: [3] <author> M. ARIOLI, I. S. DUFF, N. I. M. GOULD, AND J. K. REID, </author> <title> Use of the P 4 and P 5 algorithms for in-core factorization of sparse matrices, </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 11 (1990), </volume> <pages> pp. 913-927. </pages>
Reference-contexts: However, these methods are still potentially unstable. Iterative refinement can be used to improve the instability, see [2]. In our method, stability is guaranteed by allowing pivots to be taken within the diagonal blocks as well as the border. This was also attempted with the P 4 ordering <ref> [3] </ref>; however, the overhead incurred prevented this approach from being competitive with other direct solvers. Within MCSPARSE border pivoting relies upon a symmetric permutation, referred to below as casting, which minimizes the associated overhead. <p> The results of H* can be compared with the related orderings produced by the P 4 algorithm [31], the P 5 algorithm [13], and the level set algorithm by Arioli and Duff [2] on a subset of the Harwell-Boeing matrices, the results of which are available in the literature <ref> [2, 3] </ref>. This subset comprises the Grenoble matrices and the Westerberg's matrices. The matrices range in order from 67 to 2021. <p> Further attempts to improve stability via pivoting produced a prohibitive cost and the use of simple iterative refinement did not result in satisfactory recovery of accuracy <ref> [3] </ref>. Since MCSPARSE would suffer similar stability problems if the pivot selection was constrained to just the diagonal blocks, we will present an alternative method for maintaining stability. <p> Finally, the resulting factorization cannot be characterized as simply as those that result from MCSPARSE's strategy. Other strategies discussed in the literature have resulted in solvers with either unacceptable cost or stability control, e.g., <ref> [2, 3] </ref>. The strategy we prefer is one which preserves, up to a point, the overall structure of the matrix while allowing the implementation of a global pivoting strategy that yields a factorization with stability similar to more conventional nonsymmetric solvers.
Reference: [4] <author> C. C. ASHCRAFT, R. G. GRIMES, J. G. LEWIS, B. W. PEYTON, AND H. D. SIMON, </author> <title> Progress in sparse matrix methods for large linear systems on vector supercomputers, </title> <journal> Intl. J. Supercomputing Appl., </journal> <volume> 1 (1987), </volume> <pages> pp. 10-30. </pages>
Reference-contexts: In addition, careful consideration of data layout is necessary to effectively use the hybrid memory system comprising private cluster memories and a shared global memory. Many approaches for solving nonsymmetric sparse systems in parallel have been investigated. One of these is the multi-frontal scheme <ref> [4, 11, 12, 37] </ref>. A multi-frontal scheme constructs a directed acyclic graph (called an assembly DAG) to organize the parallel work. A node in the assembly DAG represents a certain computation, which may include handling the information from the node's successors and performing some pivot eliminations. <p> These codes typically concentrate on medium and fine grain parallelism and tend to be most efficient on a moderate number of processors with fairly tight coupling. There is also previous work on performance improvements of direct sparse solvers on vector supercomputers <ref> [4] </ref>. The results indicate that vectorization can sometimes be used to improve fl Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, Urbana, Illinois 61801. Supported by the National Science Foundation under Grant No. US NSF CCR-9120105, by the Department of Energy under Grant No.
Reference: [5] <author> J. R. BUNCH, </author> <title> Analysis of sparse elimination, </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 11 (1974), </volume> <pages> pp. 847-873. 35 </pages>
Reference-contexts: The extra factor of 2 is due to the two pivots per column. In the case of sparse matrices, the growth factor bound can be reduced significantly and depends upon the sparsity of the matrix <ref> [5, 22] </ref>. When the partial pivoting conditions are relaxed, or another pivoting strategy is combined with casting, the bound on the growth factor is easily deduced from modifications to the standard analysis.
Reference: [6] <author> L. K. CHEUNG AND E. S. KUH, </author> <title> The bordered triangular matrix and minimum essential sets of a digraph, </title> <journal> I.E.E.E. Transactions on Circuits and Systems, </journal> <month> CAS-21 </month> <year> (1974), </year> <pages> pp. 633-639. </pages>
Reference-contexts: Research into orderings for transforming matrices into the bordered triangular form has been done using graph theory methods for finding the minimal essential set <ref> [6, 39] </ref>. These methods rely on the fact that the sparse system is positive definite, so that pivots can be chosen from the elements of the diagonal without losing stability. In the case of nonsymmetric systems, which are not necessarily positive definite, these methods are not always successful.
Reference: [7] <author> T. A. DAVIS AND E. S. DAVIDSON, </author> <title> Pairwise reduction for the direct, parallel solution of sparse unsymmetric sets of linear equations, </title> <journal> IEEE Trans. Comput., </journal> <volume> 37 (1988), </volume> <pages> pp. 1648-1654. </pages>
Reference-contexts: Permutations which do not destroy this structure include those which are done within a diagonal block and those which exchange a row in a diagonal block with a row in the border. For example, pairwise pivoting could be used to eliminate the rows of the border in parallel <ref> [7] </ref>. This preserves not only the general structure but also the number of rows in each of the diagonal blocks and the border. There are some drawbacks, however. If the matrix was, in fact, symmetric in pattern, then the symmetry has been destroyed.
Reference: [8] <author> T. A. DAVIS AND P. C. YEW, </author> <title> A nondeterministic parallel algorithm for general unsymmetric sparse LU factorization, </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 11 (1990), </volume> <pages> pp. 383-402. </pages>
Reference-contexts: However, the method tends to work well on matrices with a near-symmetric structure and the pivot sequence is constrained which can cause stability concerns. Another approach to parallel sparse solvers exploits the dynamic identification and application of parallel pivots <ref> [1, 8, 18, 43] </ref>. At each stage, a set of pivots that can be applied in parallel is constructed and the appropriate updates performed. These codes typically concentrate on medium and fine grain parallelism and tend to be most efficient on a moderate number of processors with fairly tight coupling.
Reference: [9] <author> I. S. DUFF, </author> <title> Algorithm 575. permutations for a zero-free diagonal, </title> <journal> ACM Trans. Math. Software, </journal> <volume> 7 (1981), </volume> <pages> pp. </pages> <month> 387-390. </month> <title> [10] , On algorithms for obtaining a maximum transversal, </title> <journal> ACM Trans. Math. Software, </journal> <volume> 7 (1981), </volume> <pages> pp. </pages> <month> 315-330. </month> <title> [11] , Parallel implementation of multifrontal schemes, </title> <booktitle> Parallel Computing, 3 (1986), </booktitle> <pages> pp. 193-204. </pages>
Reference-contexts: Algorithms for finding set representation [30] or solutions to the assignment problem [33] could be used. An alternative algorithm involves finding maximal matchings in bipartite graphs [32]. H0 is based on work by Duff and Gustavson <ref> [9, 10, 29] </ref>. The algorithm uses a depth first search of the matrix to determine a series of column interchanges. The algorithm creates a transversal by assigning a unique diagonal position to each column of the matrix. These assignments determine a column permutation which places nonzero elements on the diagonal.
Reference: [12] <author> I. S. DUFF AND J. K. REID, </author> <title> The multifrontal solution of indefinite sparse symmetric linear equations, </title> <journal> ACM Trans. Math. Software, </journal> <volume> 9 (1983), </volume> <pages> pp. 302-325. </pages>
Reference-contexts: In addition, careful consideration of data layout is necessary to effectively use the hybrid memory system comprising private cluster memories and a shared global memory. Many approaches for solving nonsymmetric sparse systems in parallel have been investigated. One of these is the multi-frontal scheme <ref> [4, 11, 12, 37] </ref>. A multi-frontal scheme constructs a directed acyclic graph (called an assembly DAG) to organize the parallel work. A node in the assembly DAG represents a certain computation, which may include handling the information from the node's successors and performing some pivot eliminations.
Reference: [13] <author> A. M. ERISMAN, R. G. GRIMES, J. G. LEWIS, AND W. G. POOLE, </author> <title> A structurally stable modification of Hellerman-Rarick's P 4 algorithm for reordering unsymmetric sparse matrices, </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 22 (1985), </volume> <pages> pp. 369-385. </pages>
Reference-contexts: Several different methods for finding the bordered block triangular form have been proposed. Partitioning and tearing methods [41] can be used, and algorithms such as P 4 [31], P 5 <ref> [13] </ref>, the Hierarchical Partition by Lin and Mah [35], and the level set algorithm by Arioli and Duff [2] were introduced for ordering the matrix into the desired form. <p> Methods for dealing with these small blocks efficiently will be discussed in Section 7. The results of H* can be compared with the related orderings produced by the P 4 algorithm [31], the P 5 algorithm <ref> [13] </ref>, and the level set algorithm by Arioli and Duff [2] on a subset of the Harwell-Boeing matrices, the results of which are available in the literature [2, 3]. This subset comprises the Grenoble matrices and the Westerberg's matrices. The matrices range in order from 67 to 2021.
Reference: [14] <author> A. M. ERISMAN, R. G. GRIMES, J. G. LEWIS, W. G. POOLE, AND H. D. SIMON, </author> <title> Evaluation of orderings for unsymmetric sparse matrices, </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 8 (1987), </volume> <pages> pp. 600-624. </pages>
Reference-contexts: Both of these approaches can be used as part of an algorithm which exploits multiple levels of parallelism. Tearing techniques have been proposed to expose large grain structure and parallelism by reordering the matrix into a bordered block triangular matrix <ref> [14, 20, 31] </ref>. This effectively partitions the problem into small subproblems (the diagonal blocks) and then eliminates all connections between the subproblems (the border blocks). Unfortunately, the associated factorization routines are often unable to preserve stability and sparsity without destroying this structure.
Reference: [15] <author> K. GALLIVAN, B. MARSOLF, AND H. WIJSHOFF, </author> <title> A large-grain parallel sparse system solver, </title> <booktitle> in Proc. Fourth SIAM Conf. on Parallel Proc. for Scient. Comp., </booktitle> <address> Chicago, IL, </address> <year> 1989, </year> <pages> pp. 23-28. </pages>
Reference-contexts: The strategy used in MCSPARSE, called casting, is described in detail for both arbitrary and bordered block upper triangular matrices in <ref> [15, 20, 38] </ref>. We review it briefly here. Casting is a symmetric permutation that decreases the size of the diagonal block containing the cast pivot by moving a row and column into the border, thereby increasing its size by one. More formally: DEFINITION 5.1.
Reference: [16] <author> K. GALLIVAN, A. SAMEH, AND Z. ZLATEV, </author> <title> Parallel hybrid sparse linear system solver, </title> <booktitle> Computing systems in engineering, 1 (1990), </booktitle> <pages> pp. </pages> <month> 183-195. </month> <title> [17] , Solving general sparse linear systems using conjugate gradient-type methods, </title> <booktitle> in Proceedings of the 1990 International Conference on Supercomputing, </booktitle> <address> New York, 1990, </address> <publisher> ACM Press, </publisher> <pages> pp. 132-139. </pages> <month> June 11-15, </month> <year> 1990, </year> <title> Amsterdam, The Netherlands. [18] , Parallel direct method codes for general sparse matrices, </title> <booktitle> in Proceedings of NATO ASI on Linear Systems, </booktitle> <editor> S. Bertocchi and Vespucci, eds., </editor> <year> 1991. </year>
Reference-contexts: This produces a modular algorithm which lends itself to parallelism and facilitates the modification of the solver to function as a parallel preconditioner in a direct/iterative hybrid solver <ref> [16, 46] </ref>. For a discussion of another large grain approach which does not isolate the border, diagonal, and off-diagonal blocks during the factorization, see the discussion of LORA in [19]. <p> At some point, the performance improvement due to the multilevel parallelism will decline and a hybrid strategy of direct and iterative methods similar to that used in a code based on the parallel Y12M code used in the experiments here should be considered <ref> [16, 17] </ref>.
Reference: [19] <author> K. A. GALLIVAN, P. C. HANSEN, T. OSTROMSKY, AND Z. ZLATEV, </author> <title> A locally optimized reordering algorithm and its application to a parallel sparse linear system solver, </title> <journal> Computing, </journal> <volume> 54, </volume> <pages> pp. 39-67. </pages>
Reference-contexts: For a discussion of another large grain approach which does not isolate the border, diagonal, and off-diagonal blocks during the factorization, see the discussion of LORA in <ref> [19] </ref>. It should be noted that though the H* ordering placed the rows into specific diagonal blocks, H* did not determine the final pivot sequence. Pivoting can be done within the diagonal block to improve stability or fill-in without affecting the overall structure of the matrix. <p> MCSPARSE also makes use of this technique. However, when the tradeoffs between stability and sparsity become too large, MCSPARSE can be joined with more sophisticated iterative methods. In addition, the MCSPARSE structure can also be used with incomplete factorization methods to form an iterative solver <ref> [19] </ref>. Stability Results. In order to investigate the efficacy of the casting-based pivoting strategy, MC-SPARSE was used to solve systems defined by the large matrices ( 1000 rows) from the RUA portion of the Harwell-Boeing test set. <p> It is also possible to use a faster, but usually less successful, reordering strategy which results in a matrix that can be factored by a different parallel algorithm, or whose form can be made consistent with the bordered block triangular form assumed by MCSPARSE <ref> [19] </ref>. 9. Conclusions. A new ordering technique H* and an associated parallel factorization algorithm, MCSPARSE, for solving sparse nonsymmetric linear systems on a multicluster architecture have been presented.
Reference: [20] <author> K. A. GALLIVAN, B. A. MARSOLF, AND H. A. G. WIJSHOFF, </author> <title> MCSPARSE: A parallel sparse unsymmetric linear system solver, </title> <type> Tech. Report CSRD Report No. 1142, </type> <institution> Center for Supercomputing Research and Development, University of Illinois, Urbana, IL, </institution> <year> 1991. </year> <title> [21] , The parallel solution of nonsymmetric sparse linear systems using the H* reordering and an associated factorization., </title> <booktitle> in Proceedings of the 8th ACM International Conference on Supercomputing, </booktitle> <address> Manchester, England, </address> <month> July 11-15 </month> <year> 1994, </year> <pages> pp. 419-430. </pages>
Reference-contexts: Both of these approaches can be used as part of an algorithm which exploits multiple levels of parallelism. Tearing techniques have been proposed to expose large grain structure and parallelism by reordering the matrix into a bordered block triangular matrix <ref> [14, 20, 31] </ref>. This effectively partitions the problem into small subproblems (the diagonal blocks) and then eliminates all connections between the subproblems (the border blocks). Unfortunately, the associated factorization routines are often unable to preserve stability and sparsity without destroying this structure. <p> After enhancement i 1. Before reduction h f d b B S C h f d b B S C 3. After reduction FIG. 5. Enhanced Separator Set Reduction performance results which include the ordering time. The interested reader should consult <ref> [20] </ref> for details concerning the tuning of the heuristics that produces the data presented below. In this paper, we restrict ourselves to the description of the following parameter settings which were used for each of the phases of H*. <p> The strategy used in MCSPARSE, called casting, is described in detail for both arbitrary and bordered block upper triangular matrices in <ref> [15, 20, 38] </ref>. We review it briefly here. Casting is a symmetric permutation that decreases the size of the diagonal block containing the cast pivot by moving a row and column into the border, thereby increasing its size by one. More formally: DEFINITION 5.1. <p> However, since there are at most two pivots per column, the factorization can be characterized algebraically in a manner very similar to the standard LU factorization [21]. This simplifies the way in which the factorization can be saved and used to solve systems with right-hand sides supplied later <ref> [20] </ref>.
Reference: [22] <author> C. W. GEAR, </author> <title> Numerical errors in sparse linear equations, </title> <type> Tech. Report UIUDCS-F-75-885, </type> <institution> Department of Computer Science, University of Illinois, Urbana, IL, </institution> <year> 1975. </year>
Reference-contexts: The extra factor of 2 is due to the two pivots per column. In the case of sparse matrices, the growth factor bound can be reduced significantly and depends upon the sparsity of the matrix <ref> [5, 22] </ref>. When the partial pivoting conditions are relaxed, or another pivoting strategy is combined with casting, the bound on the growth factor is easily deduced from modifications to the standard analysis.
Reference: [23] <author> A. GEORGE, </author> <title> Nested dissection of a regular finite element mesh, </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 10 (1973), </volume> <pages> pp. </pages> <month> 345-363. </month> <title> [24] , An automatic one-way dissection algorithm for irregular finite element problems, </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 17 (1980), </volume> <pages> pp. 740-751. </pages>
Reference-contexts: It is also possible to approach the problem starting from the standard techniques used to produce separator sets for structurally symmetric matrices, e.g., nested dissection <ref> [23, 26] </ref>. As in the standard approaches, the ordering H2 starts with the construction of separator sets for the adjacency matrix of A + A T . In our implementation of H2, we used a straight-forward implementation of automatic nested dissection [25].
Reference: [25] <author> A. GEORGE AND J. LIU, </author> <title> Computer Solution of Large Sparse Positive Definite Systems, </title> <publisher> Prentice Hall, </publisher> <year> 1981. </year>
Reference-contexts: As in the standard approaches, the ordering H2 starts with the construction of separator sets for the adjacency matrix of A + A T . In our implementation of H2, we used a straight-forward implementation of automatic nested dissection <ref> [25] </ref>. However, other initial orderings could have been used, such as one-way dissection [24], more sophisticated implementations of automatic nested dissection [36], or the graph bisection heuristics proposed in [34].
Reference: [26] <author> A. GEORGE AND J. W. H. LIU, </author> <title> An automatic nested dissection algorithm for irregular finite-element problems, </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 15 (1978), </volume> <pages> pp. 1053-1069. </pages>
Reference-contexts: It is also possible to approach the problem starting from the standard techniques used to produce separator sets for structurally symmetric matrices, e.g., nested dissection <ref> [23, 26] </ref>. As in the standard approaches, the ordering H2 starts with the construction of separator sets for the adjacency matrix of A + A T . In our implementation of H2, we used a straight-forward implementation of automatic nested dissection [25].
Reference: [27] <author> J. P. GESCHIERE, </author> <title> Research on parallelizing the reordering phase of MCSPARSE, a large grain parallel sparse unsym-metric linear system solver, </title> <type> Tech. Report INF/SCR-92-23, </type> <institution> Department of Computer Science, Utrecht University, </institution> <month> August </month> <year> 1992. </year> <type> Master Thesis. </type>
Reference-contexts: Wijshoff and Geschiere have considered more aggressively parallel forms of the phases of H*, but a discussion of their results is beyond the scope of this paper; the interested reader is referred to <ref> [27, 28] </ref>. 33 2 4 6 8 10 12 14 16 18 20 1 3 Matrices Ordered by Sequential Factorization Time Cluster Speed Up = T (mcsp) 8 T (mcsp) 32 T (mcsp) 8 T (flat) 32 FIG. 8. Comparison of hierarchical and flat parallelism.
Reference: [28] <author> J. P. GESCHIERE AND H. A. G. WIJSHOFF, </author> <title> Exploiting large grain parallelism in a sparse direct linear system solver, </title> <type> Tech. Report 93-18, </type> <institution> High Performance Computing Division, Leiden University, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: Wijshoff and Geschiere have considered more aggressively parallel forms of the phases of H*, but a discussion of their results is beyond the scope of this paper; the interested reader is referred to <ref> [27, 28] </ref>. 33 2 4 6 8 10 12 14 16 18 20 1 3 Matrices Ordered by Sequential Factorization Time Cluster Speed Up = T (mcsp) 8 T (mcsp) 32 T (mcsp) 8 T (flat) 32 FIG. 8. Comparison of hierarchical and flat parallelism.
Reference: [29] <author> F. GUSTAVSON, </author> <title> Finding the block lower triangular form of a matrix, in Sparse Matrix Computations, </title> <editor> J. Bunch and D. Rose, eds., </editor> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1976. </year>
Reference-contexts: Algorithms for finding set representation [30] or solutions to the assignment problem [33] could be used. An alternative algorithm involves finding maximal matchings in bipartite graphs [32]. H0 is based on work by Duff and Gustavson <ref> [9, 10, 29] </ref>. The algorithm uses a depth first search of the matrix to determine a series of column interchanges. The algorithm creates a transversal by assigning a unique diagonal position to each column of the matrix. These assignments determine a column permutation which places nonzero elements on the diagonal.
Reference: [30] <author> M. HALL, JR., </author> <title> An algorithm for distinct representatives, </title> <journal> The American Mathematical Monthly, </journal> <volume> 63 (1956), </volume> <pages> pp. 716-717. </pages>
Reference-contexts: The transversal ordering is a matching between the columns and the diagonal positions of the matrix and can be found using many different algorithms. Algorithms for finding set representation <ref> [30] </ref> or solutions to the assignment problem [33] could be used. An alternative algorithm involves finding maximal matchings in bipartite graphs [32]. H0 is based on work by Duff and Gustavson [9, 10, 29].
Reference: [31] <author> E. HELLERMAN AND D. C. RARICK, </author> <title> The partitioned preassigned pivot procedure (P 4 ), in Sparse Matrices and their Applications, </title> <editor> D. J. Rose and R. A. Willoughby, eds., </editor> <publisher> Plenum, </publisher> <address> New York, </address> <year> 1972. </year>
Reference-contexts: Both of these approaches can be used as part of an algorithm which exploits multiple levels of parallelism. Tearing techniques have been proposed to expose large grain structure and parallelism by reordering the matrix into a bordered block triangular matrix <ref> [14, 20, 31] </ref>. This effectively partitions the problem into small subproblems (the diagonal blocks) and then eliminates all connections between the subproblems (the border blocks). Unfortunately, the associated factorization routines are often unable to preserve stability and sparsity without destroying this structure. <p> Several different methods for finding the bordered block triangular form have been proposed. Partitioning and tearing methods [41] can be used, and algorithms such as P 4 <ref> [31] </ref>, P 5 [13], the Hierarchical Partition by Lin and Mah [35], and the level set algorithm by Arioli and Duff [2] were introduced for ordering the matrix into the desired form. <p> Methods for dealing with these small blocks efficiently will be discussed in Section 7. The results of H* can be compared with the related orderings produced by the P 4 algorithm <ref> [31] </ref>, the P 5 algorithm [13], and the level set algorithm by Arioli and Duff [2] on a subset of the Harwell-Boeing matrices, the results of which are available in the literature [2, 3]. This subset comprises the Grenoble matrices and the Westerberg's matrices.
Reference: [32] <author> J. E. HOPCROFT AND R. M. KARP, </author> <title> An n 5 2 algorithm for maximum matchings in bipartite graphs, </title> <journal> SIAM J. Comput., </journal> <volume> 2 (1973), </volume> <pages> pp. 225-231. </pages>
Reference-contexts: Algorithms for finding set representation [30] or solutions to the assignment problem [33] could be used. An alternative algorithm involves finding maximal matchings in bipartite graphs <ref> [32] </ref>. H0 is based on work by Duff and Gustavson [9, 10, 29]. The algorithm uses a depth first search of the matrix to determine a series of column interchanges. The algorithm creates a transversal by assigning a unique diagonal position to each column of the matrix.
Reference: [33] <author> H. W. KUHN, </author> <title> The Hungarian method for the assignment problem, </title> <journal> Naval Research Logistics Quarterly, </journal> <volume> 2 (1955), </volume> <pages> pp. 83-97. </pages>
Reference-contexts: The transversal ordering is a matching between the columns and the diagonal positions of the matrix and can be found using many different algorithms. Algorithms for finding set representation [30] or solutions to the assignment problem <ref> [33] </ref> could be used. An alternative algorithm involves finding maximal matchings in bipartite graphs [32]. H0 is based on work by Duff and Gustavson [9, 10, 29]. The algorithm uses a depth first search of the matrix to determine a series of column interchanges.
Reference: [34] <author> C. LEISERSON AND J. LEWIS, </author> <title> Orderings for parallel sparse symmetric factorization, </title> <booktitle> in Proc. Third SIAM Conf. on 36 Parallel Proc. for Scient. Comp., </booktitle> <address> Los Angeles, CA., </address> <year> 1987, </year> <pages> pp. 27-31. </pages>
Reference-contexts: In our implementation of H2, we used a straight-forward implementation of automatic nested dissection [25]. However, other initial orderings could have been used, such as one-way dissection [24], more sophisticated implementations of automatic nested dissection [36], or the graph bisection heuristics proposed in <ref> [34] </ref>. H2 is applied only to the diagonal blocks that H1 failed to reduce to a size less than a user-specified threshold, T done .
Reference: [35] <author> T. D. LIN AND R. S. H. MAH, </author> <title> Hierarchical partition anew optimal pivoting algorithm, </title> <journal> Mathematical Programming, </journal> <volume> 12 (1977), </volume> <pages> pp. 260-278. </pages>
Reference-contexts: Several different methods for finding the bordered block triangular form have been proposed. Partitioning and tearing methods [41] can be used, and algorithms such as P 4 [31], P 5 [13], the Hierarchical Partition by Lin and Mah <ref> [35] </ref>, and the level set algorithm by Arioli and Duff [2] were introduced for ordering the matrix into the desired form.
Reference: [36] <author> R. LIPTON, D. ROSE, AND R. TARJAN, </author> <title> Generalized nested dissection, </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 16 (1979), </volume> <pages> pp. 346-358. </pages>
Reference-contexts: In our implementation of H2, we used a straight-forward implementation of automatic nested dissection [25]. However, other initial orderings could have been used, such as one-way dissection [24], more sophisticated implementations of automatic nested dissection <ref> [36] </ref>, or the graph bisection heuristics proposed in [34]. H2 is applied only to the diagonal blocks that H1 failed to reduce to a size less than a user-specified threshold, T done .
Reference: [37] <author> J. W. H. LIU, </author> <title> The multifrontal method for sparse matrix solution: </title> <journal> Theory and practice, SIAM Review, </journal> <volume> 34 (1992), </volume> <pages> pp. 82-109. </pages>
Reference-contexts: In addition, careful consideration of data layout is necessary to effectively use the hybrid memory system comprising private cluster memories and a shared global memory. Many approaches for solving nonsymmetric sparse systems in parallel have been investigated. One of these is the multi-frontal scheme <ref> [4, 11, 12, 37] </ref>. A multi-frontal scheme constructs a directed acyclic graph (called an assembly DAG) to organize the parallel work. A node in the assembly DAG represents a certain computation, which may include handling the information from the node's successors and performing some pivot eliminations.
Reference: [38] <author> B. MARSOLF, </author> <title> Large grain parallel sparse system solver, </title> <type> Tech. Report CSRD Report No. 1125, </type> <institution> Center for Supercomputing Research and Development, University of Illinois, Urbana, IL, </institution> <year> 1991. </year> <type> Master Thesis. </type>
Reference-contexts: The strategy used in MCSPARSE, called casting, is described in detail for both arbitrary and bordered block upper triangular matrices in <ref> [15, 20, 38] </ref>. We review it briefly here. Casting is a symmetric permutation that decreases the size of the diagonal block containing the cast pivot by moving a row and column into the border, thereby increasing its size by one. More formally: DEFINITION 5.1. <p> When implementing the solver, several assumptions about the mapping must be examined. The mapping assumes there are enough rows in the diagonal block for the processors of the cluster to be effective. The current H* ordering <ref> [38] </ref>, however, usually generates a large number of small diagonal blocks containing between one and four rows. Since a full Cedar cluster contains eight processors, such diagonal blocks would be unable to use more than half of the processors. A method for overcoming this problem was described in Section 7.
Reference: [39] <author> A. SANGIOVANNI-VINCENTELLI, </author> <title> An optimization problem arising from tearing methods, in Sparse Matrix Computations, </title> <editor> J. R. Bunch and D. J. Rose, eds., </editor> <publisher> Academic Press Inc., </publisher> <address> New York, </address> <year> 1976, </year> <pages> pp. 97-110. </pages>
Reference-contexts: Research into orderings for transforming matrices into the bordered triangular form has been done using graph theory methods for finding the minimal essential set <ref> [6, 39] </ref>. These methods rely on the fact that the sparse system is positive definite, so that pivots can be chosen from the elements of the diagonal without losing stability. In the case of nonsymmetric systems, which are not necessarily positive definite, these methods are not always successful.
Reference: [40] <author> STAFF, </author> <title> The Cedar Project, </title> <type> Tech. Report CSRD Report No. 1122, </type> <institution> Center for Supercomputing Research and Development, University of Illinois, Urbana, IL, </institution> <year> 1991. </year>
Reference-contexts: In this paper, we present a new method for solving such linear systems using novel reordering and pivoting schemes; we also discuss the issues involved in mapping this method onto a parallel processor. The discussion focuses on the tradeoffs involved with an implementation on the Cedar system <ref> [40] </ref>, a multicluster architecture (four clusters, each with eight processors), but will also include comments on the applicability of the techniques to other systems.
Reference: [41] <author> D. V. STEWARD, </author> <title> Partitioning and tearing systems of equations, </title> <journal> SIAM Journal Numerical Analysis, </journal> <volume> 2 (1965), </volume> <pages> pp. 345-365. </pages>
Reference-contexts: For nonsymmetric systems, the bordered block triangular form is preferable when pivot selection is restricted to within the diagonal blocks since the overall structure of the system is not destroyed. Several different methods for finding the bordered block triangular form have been proposed. Partitioning and tearing methods <ref> [41] </ref> can be used, and algorithms such as P 4 [31], P 5 [13], the Hierarchical Partition by Lin and Mah [35], and the level set algorithm by Arioli and Duff [2] were introduced for ordering the matrix into the desired form.
Reference: [42] <author> R. E. TARJAN, </author> <title> Depth-first search and linear graph algorithms, </title> <journal> SIAM J. Computing, </journal> <volume> 1 (1972), </volume> <pages> pp. 146-160. </pages>
Reference-contexts: However, even with the bound removed, the algorithm still tries the elements with the largest absolute value first. The performance of the H0 algorithm relies upon the ability to quickly find an adequate bound for the transversal. 3.3. Tarjan's Algorithm. Tarjan's algorithm <ref> [42] </ref> finds the strongly connected components of the digraph associated with the matrix with time complexity linear in the number of nodes and edges. 1 A renumbering of the nodes of the digraph corresponding to the decomposition of the graph into strongly connected components yields a symmetric ordering which transforms the
Reference: [43] <author> A. F. VAN DER STAPPEN, R. H. BISSELING, AND J. G. G. VAN DE VORST, </author> <title> Parallel sparse LU decomposition on a mesh network of transputers, </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 14 (1993), </volume> <pages> pp. 853-879. </pages>
Reference-contexts: However, the method tends to work well on matrices with a near-symmetric structure and the pivot sequence is constrained which can cause stability concerns. Another approach to parallel sparse solvers exploits the dynamic identification and application of parallel pivots <ref> [1, 8, 18, 43] </ref>. At each stage, a set of pivots that can be applied in parallel is constructed and the appropriate updates performed. These codes typically concentrate on medium and fine grain parallelism and tend to be most efficient on a moderate number of processors with fairly tight coupling.
Reference: [44] <author> X. WANG, </author> <title> private communication. </title>
Reference-contexts: At some point, the performance improvement due to the multilevel parallelism will decline and a hybrid strategy of direct and iterative methods similar to that used in a code based on the parallel Y12M code used in the experiments here should be considered [16, 17]. Initial results, <ref> [44] </ref>, indicate that MCSPARSE can be adapted to use a combination of positional dropping (i.e., ignoring a fill-in element due to its position in the matrix) and numerical dropping (i.e., ignoring a fill-in element because of its relative magnitude) to produce a preconditioner for iterative methods for nonsymmetric systems.
Reference: [45] <author> H. A. G. WIJSHOFF, </author> <title> Symmetric orderings for unsymmetric sparse matrices, </title> <type> Tech. Report CSRD Report No. 901, </type> <institution> Center for Supercomputing Research and Development, University of Illinois, Urbana, IL, </institution> <year> 1989. </year>
Reference-contexts: This approach enables MCSPARSE to be competitive with other direct solvers, see Section 8. Descriptions of the algorithms used within H* are presented in the next section. A preliminary algorithmic description of the H* ordering is in <ref> [45] </ref>. 3. The H* Ordering. 3.1. Background. The interpretation of the actions of H* depends upon the notion of a graph associated with a sparse matrix. DEFINITION 3.1. Given a nonsymmetric (N fi N ) sparse matrix A .
Reference: [46] <author> U. M. YANG, </author> <title> Preconditioned Iterative Solver for Nonsymmetric Linear Systems, </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1994. </year> <note> In preparation. 37 </note>
Reference-contexts: This produces a modular algorithm which lends itself to parallelism and facilitates the modification of the solver to function as a parallel preconditioner in a direct/iterative hybrid solver <ref> [16, 46] </ref>. For a discussion of another large grain approach which does not isolate the border, diagonal, and off-diagonal blocks during the factorization, see the discussion of LORA in [19].
References-found: 40

