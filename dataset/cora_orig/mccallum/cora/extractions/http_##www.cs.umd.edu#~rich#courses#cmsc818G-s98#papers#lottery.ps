URL: http://www.cs.umd.edu/~rich/courses/cmsc818G-s98/papers/lottery.ps
Refering-URL: http://www.cs.umd.edu/~rich/courses/cmsc818G-s98/resources.html
Root-URL: 
Title: Lottery Scheduling: Flexible Proportional-Share Resource Management  
Author: Carl A. Waldspurger William E. Weihl 
Address: Cambridge, MA 02139 USA  
Affiliation: MIT Laboratory for Computer Science  
Date: November 1994  
Note: First published in Proc. of the First Symposium on Operating Systems Design and Implementation, Usenix Association,  
Abstract: This paper presents lottery scheduling, a novel randomized resource allocation mechanism. Lottery scheduling provides efficient, responsive control over the relative execution rates of computations. Such control is beyond the capabilities of conventional schedulers, and is desirable in systems that service requests of varying importance, such as databases, media-based applications, and networks. Lottery scheduling also supports modular resource management by enabling concurrent modules to insulate their resource allocation policies from one another. A currency abstraction is introduced to flexibly name, share, and protect resource rights. We also show that lottery scheduling can be generalized to manage many diverse resources, such as I/O bandwidth, memory, and access to locks. We have implemented a prototype lottery scheduler for the Mach 3.0 microkernel, and found that it provides flexible and responsive control over the relative execution rates of a wide range of applications. The overhead imposed by our unoptimized prototype is comparable to that of the standard Mach timesharing policy. 
Abstract-found: 1
Intro-found: 1
Reference: [Acc86] <author> M. Accetta, R. Baron, D. Golub, R. Rashid, A. Teva-nian, and M. Young. </author> <title> Mach: A New Kernel Foundation for UNIX Development, </title> <booktitle> Proceedings of the Summer 1986 USENIX Conference, </booktitle> <month> June </month> <year> 1986. </year>
Reference-contexts: Without compensation tickets, a client that does not consume its entire allocated quantum would receive less than its entitled share of the processor. 4 Implementation We have implemented a prototype lottery scheduler by modifying the Mach 3.0 microkernel (MK82) <ref> [Acc86, Loe92] </ref> on a 25MHz MIPS-based DECStation 5000/125. Full support is provided for ticket transfers, ticket inflation, ticket currencies, and compensation tickets. 2 The scheduling quantum on this platform is 100 milliseconds. 4.1 Random Numbers An efficient lottery scheduler requires a fast way to generate uniformly-distributed random numbers.
Reference: [And93] <author> T. E. Anderson, S. S. Owicki, J. B. Saxe, and C. P. Thacker. </author> <title> High-Speed Switch Scheduling for Local-Area Networks, </title> <journal> ACM Transactions on Computer Systems, </journal> <month> November </month> <year> 1993. </year>
Reference-contexts: However, unlike a true market, prices are not permitted to vary with demand, and ancillary parameters are introduced to restrict resource consumption [Che93]. The statistical matching technique for fair switching in the AN2 network exploits randomness to support frequent changes of bandwidth allocation <ref> [And93] </ref>. This work is similar to our proposed application of lottery scheduling to communication channels. 8 Conclusions We have presented lottery scheduling, a novel mechanism that provides efficient and responsive control over the relative execution rates of computations.
Reference: [Car90] <author> D. G. Carta. </author> <title> Two Fast Implementations of the `Minimal Standard' Random Number Generator, </title> <journal> Communications of the ACM, </journal> <month> January </month> <year> 1990. </year>
Reference-contexts: The fifteenth ticket is randomly selected, and the client list is searched for the winner. A running ticket sum is accumulated until the winning ticket value is reached. In this example, the third client is the winner. Park-Miller algorithm <ref> [Par88, Car90] </ref> that executes in approximately 10 RISC instructions. Our assembly-language implementation is listed in Appendix A. 4.2 Lotteries A straightforward way to implement a centralized lottery scheduler is to randomly select a winning ticket, and then search a list of clients to locate the client holding that ticket.
Reference: [Che93] <author> D. R. Cheriton and K. Harty. </author> <title> A Market Approach to Operating System Memory Allocation, </title> <type> Working Paper, </type> <institution> Computer Science Department, Stanford University, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: This scheme charges applications for both memory leases and I/O capacity, allowing application-specific tradeoffs to be made. However, unlike a true market, prices are not permitted to vary with demand, and ancillary parameters are introduced to restrict resource consumption <ref> [Che93] </ref>. The statistical matching technique for fair switching in the AN2 network exploits randomness to support frequent changes of bandwidth allocation [And93].
Reference: [Com94] <author> C. L. Compton and D. L. Tennenhouse. </author> <title> Collaborative Load Shedding for Media-based Applications, </title> <booktitle> Proceedings of the International Conference on Multimedia Computing and Systems, </booktitle> <month> May </month> <year> 1994. </year>
Reference-contexts: Compton and Tennen-house described the need to control the quality of service when two or more video viewers are displayed a level of control not offered by current operating systems <ref> [Com94] </ref>. They attempted, with mixed success, to control video display rates at the application level among a group of mutually trusting viewers. Cooperating viewers employed feedback mechanisms to adjust their relative frame rates.
Reference: [Dei90] <author> H. M. Deitel. </author> <title> Operating Systems, </title> <publisher> Addison-Wesley, </publisher> <year> 1990. </year>
Reference-contexts: In fact, with the exception of hard real-time systems, it has been observed that the assignment of priorities and dynamic priority adjustment schemes are often ad-hoc <ref> [Dei90] </ref>. Even popular priority-based schemes for CPU allocation such as decay-usage scheduling are poorly understood, despite the fact that they are employed by numerous operating systems, including Unix [Hel93]. Existing fair share schedulers [Hen84, Kay88] and mi-croeconomic schedulers [Fer88, Wal92] successfully address some of the problems with absolute priority schemes. <p> A task with higher priority is given absolute precedence over a task with lower priority. Priorities may be static, or they may be allowed to vary dynamically. Many sophisticated priority schemes are somewhat arbitrary, since priorities themselves are rarely meaningfully assigned <ref> [Dei90] </ref>. The ability to express priorities provides absolute, but extremely crude, control over scheduling, since resource rights do not vary smoothly with priorities. Conventional priority mechanisms are also inadequate for insulating the resource allocation policies of separate modules.
Reference: [Dre88] <author> K. E. Drexler and M. S. Miller. </author> <title> Incentive Engineering for Computational Resource Management in The Ecology of Computation, </title> <editor> B. Huberman (ed.), </editor> <publisher> North-Holland, </publisher> <year> 1988. </year>
Reference-contexts: While this technique avoids the addition of feedback loops introduced by other fair share schedulers, it still assumes a fixed workload consisting of long-running compute-bound processes to ensure steady-state fairness at a time scale of minutes. Microeconomic schedulers <ref> [Dre88, Fer88, Wal92] </ref> use auctions to allocate resources among clients that bid monetary funds. Funds encapsulate resource rights and serve as a form of priority. <p> Microeconomic schedulers [Dre88, Fer88, Wal92] use auctions to allocate resources among clients that bid monetary funds. Funds encapsulate resource rights and serve as a form of priority. Both the escalator algorithm proposed for uniprocessor scheduling <ref> [Dre88] </ref> and the distributed Spawn system [Wal89, Wal92] rely upon auctions in which bidders increase their bids linearly over time. The Spawn system successfully allocated resources proportional to client funding in a network of heterogeneous workstations. However, experience with Spawn revealed that auction dynamics can be unexpectedly volatile.
Reference: [Dui90] <author> D. Duis and J. Johnson. </author> <title> Improving User-Interface Responsiveness Despite Performance Limitations, </title> <booktitle> Proceedings of the Thirty-Fifth IEEE Computer Society International Conference (COMPCON), </booktitle> <month> March </month> <year> 1990. </year>
Reference-contexts: The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. government. to rapidly focus available resources on tasks that are currently important <ref> [Dui90] </ref>. Few general-purpose schemes even come close to supporting flexible, responsive control over service rates. Those that do exist generally rely upon a simple notion of priority that does not provide the encapsulation and modularity properties required for the engineering of large software systems.
Reference: [Fer88] <author> D. Ferguson, Y. Yemini, and C. Nikolaou. </author> <title> Microeco-nomic Algorithms for Load-Balancing in Distributed Computer Systems, </title> <booktitle> International Conference on Distributed Computer Systems, </booktitle> <year> 1988. </year>
Reference-contexts: Even popular priority-based schemes for CPU allocation such as decay-usage scheduling are poorly understood, despite the fact that they are employed by numerous operating systems, including Unix [Hel93]. Existing fair share schedulers [Hen84, Kay88] and mi-croeconomic schedulers <ref> [Fer88, Wal92] </ref> successfully address some of the problems with absolute priority schemes. However, the assumptions and overheads associated with these systems limit them to relatively coarse control over long-running computations. Interactive systems require rapid, dynamic control over scheduling at a time scale of milliseconds to seconds. <p> While this technique avoids the addition of feedback loops introduced by other fair share schedulers, it still assumes a fixed workload consisting of long-running compute-bound processes to ensure steady-state fairness at a time scale of minutes. Microeconomic schedulers <ref> [Dre88, Fer88, Wal92] </ref> use auctions to allocate resources among clients that bid monetary funds. Funds encapsulate resource rights and serve as a form of priority.
Reference: [Har92] <author> K. Harty and D. R. Cheriton. </author> <title> Application-Controlled Physical Memory using External Page-Cache Management, </title> <booktitle> Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1992. </year>
Reference-contexts: However, experience with Spawn revealed that auction dynamics can be unexpectedly volatile. The overhead of bidding also limits the applicability of auctions to relatively coarse-grain tasks. A market-based approach for memory allocation has also been developed to allow memory-intensive applications to optimize their memory consumption in a decentralized manner <ref> [Har92] </ref>. This scheme charges applications for both memory leases and I/O capacity, allowing application-specific tradeoffs to be made. However, unlike a true market, prices are not permitted to vary with demand, and ancillary parameters are introduced to restrict resource consumption [Che93].
Reference: [Hel93] <author> J. L. Hellerstein. </author> <title> Achieving Service Rate Objectives with Decay Usage Scheduling, </title> <journal> IEEE Transactions on Software Engineering, </journal> <month> August </month> <year> 1993. </year>
Reference-contexts: Such control is desirable across a wide spectrum of systems. For long-running computations such as scientific applications and simulations, the consumption of computing resources that are shared among users and applications of varying importance must be regulated <ref> [Hel93] </ref>. For interactive computations such as databases and media-based applications, programmers and users need the ability fl E-mail: fcarl, weihlg@lcs.mit.edu. World Wide Web: http://www.psg.lcs.mit.edu/. The first author was supported in part by an AT&T USL Fellowship and by a grant from the MIT X Consortium. Prof. <p> Even popular priority-based schemes for CPU allocation such as decay-usage scheduling are poorly understood, despite the fact that they are employed by numerous operating systems, including Unix <ref> [Hel93] </ref>. Existing fair share schedulers [Hen84, Kay88] and mi-croeconomic schedulers [Fer88, Wal92] successfully address some of the problems with absolute priority schemes. However, the assumptions and overheads associated with these systems limit them to relatively coarse control over long-running computations. <p> A technique also exists for achieving service rate objectives in systems that employ decay usage scheduling by manipulating base priorities and various scheduler parameters <ref> [Hel93] </ref>. While this technique avoids the addition of feedback loops introduced by other fair share schedulers, it still assumes a fixed workload consisting of long-running compute-bound processes to ensure steady-state fairness at a time scale of minutes.
Reference: [Hen84] <author> G. J. Henry. </author> <title> The Fair Share Scheduler, </title> <journal> AT&T Bell Laboratories Technical Journal, </journal> <month> October </month> <year> 1984. </year>
Reference-contexts: Even popular priority-based schemes for CPU allocation such as decay-usage scheduling are poorly understood, despite the fact that they are employed by numerous operating systems, including Unix [Hel93]. Existing fair share schedulers <ref> [Hen84, Kay88] </ref> and mi-croeconomic schedulers [Fer88, Wal92] successfully address some of the problems with absolute priority schemes. However, the assumptions and overheads associated with these systems limit them to relatively coarse control over long-running computations. <p> Conventional priority mechanisms are also inadequate for insulating the resource allocation policies of separate modules. Since priorities are absolute, it is difficult to compose or abstract inter-module priority relationships. Fair share schedulers allocate resources so that users get fair machine shares over long periods of time <ref> [Hen84, Kay88] </ref>. These schedulers monitor CPU usage and dynamically adjust conventional priorities to push actual usage closer to entitled shares. However, the algorithms used by these systems are complex, requiring periodic usage updates, complicated dynamic priority adjustments, and administrative parameter setting to ensure fairness on a time scale of minutes.
Reference: [Hog88] <author> T. Hogg. </author> <title> Private communication (during Spawn system development), </title> <year> 1988. </year>
Reference-contexts: Scientists frequently execute several separate Monte-Carlo experiments to explore various hypotheses. It is often desirable to obtain approximate results quickly whenever a new experiment is started, while allowing older experiments to continue reducing their error at a slower rate <ref> [Hog88] </ref>. 5 entire run, the two tasks executed 25378 and 12619 iterations/sec., for an actual ratio of 2.01 : 1.
Reference: [Kan89] <author> G. Kane. </author> <title> Mips RISC Architecture, </title> <publisher> Prentice-Hall, </publisher> <year> 1989. </year>
Reference: [Kay88] <author> J. Kay and P. Lauder. </author> <title> A Fair Share Scheduler, </title> <journal> Communications of the ACM, </journal> <month> January </month> <year> 1988. </year>
Reference-contexts: Even popular priority-based schemes for CPU allocation such as decay-usage scheduling are poorly understood, despite the fact that they are employed by numerous operating systems, including Unix [Hel93]. Existing fair share schedulers <ref> [Hen84, Kay88] </ref> and mi-croeconomic schedulers [Fer88, Wal92] successfully address some of the problems with absolute priority schemes. However, the assumptions and overheads associated with these systems limit them to relatively coarse control over long-running computations. <p> Conventional priority mechanisms are also inadequate for insulating the resource allocation policies of separate modules. Since priorities are absolute, it is difficult to compose or abstract inter-module priority relationships. Fair share schedulers allocate resources so that users get fair machine shares over long periods of time <ref> [Hen84, Kay88] </ref>. These schedulers monitor CPU usage and dynamically adjust conventional priorities to push actual usage closer to entitled shares. However, the algorithms used by these systems are complex, requiring periodic usage updates, complicated dynamic priority adjustments, and administrative parameter setting to ensure fairness on a time scale of minutes.
Reference: [Loe92] <author> K. Loepere. </author> <title> Mach 3 Kernel Principles. </title> <institution> Open Software Foundation and Carnegie Mellon University, </institution> <year> 1992. </year>
Reference-contexts: Without compensation tickets, a client that does not consume its entire allocated quantum would receive less than its entitled share of the processor. 4 Implementation We have implemented a prototype lottery scheduler by modifying the Mach 3.0 microkernel (MK82) <ref> [Acc86, Loe92] </ref> on a 25MHz MIPS-based DECStation 5000/125. Full support is provided for ticket transfers, ticket inflation, ticket currencies, and compensation tickets. 2 The scheduling quantum on this platform is 100 milliseconds. 4.1 Random Numbers An efficient lottery scheduler requires a fast way to generate uniformly-distributed random numbers.
Reference: [Par88] <author> S. K. Park and K. W. Miller. </author> <title> Random Number Generators: Good Ones Are Hard to Find, </title> <journal> Communications of the ACM, </journal> <month> October </month> <year> 1988. </year>
Reference-contexts: The fifteenth ticket is randomly selected, and the client list is searched for the winner. A running ticket sum is accumulated until the winning ticket value is reached. In this example, the third client is the winner. Park-Miller algorithm <ref> [Par88, Car90] </ref> that executes in approximately 10 RISC instructions. Our assembly-language implementation is listed in Appendix A. 4.2 Lotteries A straightforward way to implement a centralized lottery scheduler is to randomly select a winning ticket, and then search a list of clients to locate the client holding that ticket.
Reference: [Pre88] <author> W. H. Press, B. P. Flannery, S. A. Teukolsky, and W. T. Vetterling. </author> <title> Numerical Recipes in C: </title> <booktitle> The Art of Scientific Computing. </booktitle> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1988. </year>
Reference-contexts: A practical application that benefits from such control is the Monte-Carlo algorithm <ref> [Pre88] </ref>. Monte-Carlo is a probabilistic algorithm that is widely used in the physical sciences for computing average properties of systems. Since errors in the computed average are proportional to 1= p n, where n is the number of trials, accurate results require a large number of trials. <p> Each task is based on the sample code presented in <ref> [Pre88] </ref>, and is allocated a share of time that is proportional to the square of its relative error. 6 When a new task is started, it initially receives a large share of the processor.
Reference: [Sha90] <author> L. Sha, R. Rajkumar, and J. P. Lehoczky. </author> <title> Priority Inheritance Protocols: An Approach to Real-Time Synchronization, </title> <journal> IEEE Transactions on Computers, </journal> <month> Septem-ber </month> <year> 1990. </year>
Reference-contexts: For example, when a client needs to block pending a reply from an RPC, it can temporarily transfer its tickets to the server on which it is waiting. This idea also conveniently solves the conventional priority inversion problem in a manner similar to priority inheritance <ref> [Sha90] </ref>. Clients also have the ability to divide ticket transfers across multiple servers on which they may be waiting. 3.2 Ticket Inflation Ticket inflation is an alternative to explicit ticket transfers in which a client can escalate its resource rights by creating more lottery tickets. <p> The net effect of these transfers is that a thread which acquires the mutex executes with its own funding plus the funding of all waiting threads, as depicted in Figure 10. This solves the priority inversion problem <ref> [Sha90] </ref>, in which a mutex owner with little funding could execute very slowly due to competition with other threads 9 Under unmodified Mach, threads with equal priority are run round-robin; with lottery scheduling, it is possible for a thread to win several lotteries in a row.
Reference: [Tri82] <author> K. S. Trivedi. </author> <title> Probability and Statistics with Reliability, Queuing, </title> <booktitle> and Computer Science Applications. </booktitle> <publisher> Prentice-Hall, </publisher> <year> 1982. </year>
Reference-contexts: Thus, a client's average response time is inversely proportional to its ticket allocation. The properties of both binomial and geometric distributions are well-understood <ref> [Tri82] </ref>. With a scheduling quantum of 10 milliseconds (100 lotteries per second), reasonable fairness can be achieved over subsecond time intervals. As computation speeds continue to increase, shorter time quanta can be used to further improve accuracy while maintaining a fixed proportion of scheduler overhead.
Reference: [Wal89] <author> C. A. Waldspurger. </author> <title> A Distributed Computational Economy for Utilizing Idle Resources, </title> <type> Master's thesis, </type> <institution> MIT, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: Microeconomic schedulers [Dre88, Fer88, Wal92] use auctions to allocate resources among clients that bid monetary funds. Funds encapsulate resource rights and serve as a form of priority. Both the escalator algorithm proposed for uniprocessor scheduling [Dre88] and the distributed Spawn system <ref> [Wal89, Wal92] </ref> rely upon auctions in which bidders increase their bids linearly over time. The Spawn system successfully allocated resources proportional to client funding in a network of heterogeneous workstations. However, experience with Spawn revealed that auction dynamics can be unexpectedly volatile.
Reference: [Wal92] <author> C. A. Waldspurger, T. Hogg, B. A. Huberman, J. O. Kephart, and W. S. Stornetta. Spawn: </author> <title> A Distributed Computational Economy, </title> <journal> IEEE Transactions on Software Engineering, </journal> <month> February </month> <year> 1992. </year>
Reference-contexts: Even popular priority-based schemes for CPU allocation such as decay-usage scheduling are poorly understood, despite the fact that they are employed by numerous operating systems, including Unix [Hel93]. Existing fair share schedulers [Hen84, Kay88] and mi-croeconomic schedulers <ref> [Fer88, Wal92] </ref> successfully address some of the problems with absolute priority schemes. However, the assumptions and overheads associated with these systems limit them to relatively coarse control over long-running computations. Interactive systems require rapid, dynamic control over scheduling at a time scale of milliseconds to seconds. <p> Finally, tickets are uniform because rights for heterogeneous resources can be homogeneously represented as tickets. These properties of lottery tickets are similar to those of money in computational economies <ref> [Wal92] </ref>. 2.2 Lotteries Scheduling by lottery is probabilistically fair. The expected allocation of resources to clients is proportional to the number of tickets that they hold. Since the scheduling algorithm is randomized, the actual allocated proportions are not guaranteed to match the expected proportions exactly. <p> While this technique avoids the addition of feedback loops introduced by other fair share schedulers, it still assumes a fixed workload consisting of long-running compute-bound processes to ensure steady-state fairness at a time scale of minutes. Microeconomic schedulers <ref> [Dre88, Fer88, Wal92] </ref> use auctions to allocate resources among clients that bid monetary funds. Funds encapsulate resource rights and serve as a form of priority. <p> Microeconomic schedulers [Dre88, Fer88, Wal92] use auctions to allocate resources among clients that bid monetary funds. Funds encapsulate resource rights and serve as a form of priority. Both the escalator algorithm proposed for uniprocessor scheduling [Dre88] and the distributed Spawn system <ref> [Wal89, Wal92] </ref> rely upon auctions in which bidders increase their bids linearly over time. The Spawn system successfully allocated resources proportional to client funding in a network of heterogeneous workstations. However, experience with Spawn revealed that auction dynamics can be unexpectedly volatile.
Reference: [Wei84] <author> R. P. Weicker. Dhrystone: </author> <title> A Synthetic Systems Programming Benchmark, </title> <journal> Communications of the ACM, </journal> <month> October </month> <year> 1984. </year>
Reference-contexts: Each point plotted in Figure 4 indicates the relative execution rate that was observed for two tasks executing the Dhrystone benchmark <ref> [Wei84] </ref> for sixty seconds with a given relative ticket allocation. Three runs were executed for each integral ratio between one and ten. 4 In this case, it would be preferable to instead fund all threads capable of receiving the message.
Reference: [Wei91] <author> W. Weihl, E. Brewer, A. Colbrook, C. Dellarocas, W. Hsieh, A. Joseph, C. Waldspurger, and P. Wang. </author> <title> Prelude: A System for Portable Parallel Software, </title> <type> Technical Report MIT/LCS/TR-519, </type> <institution> MIT Lab for Computer Science, </institution> <month> October </month> <year> 1991. </year>
Reference-contexts: We have implemented a pseudo-random number generator based on the 2 Our first lottery scheduler implementation, developed for the Prelude <ref> [Wei91] </ref> runtime system, lacked support for ticket transfers and currencies. lottery with a total of 20 tickets. The fifteenth ticket is randomly selected, and the client list is searched for the winner. A running ticket sum is accumulated until the winning ticket value is reached.
References-found: 24

