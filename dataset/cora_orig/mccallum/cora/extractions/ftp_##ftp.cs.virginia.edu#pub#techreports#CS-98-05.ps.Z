URL: ftp://ftp.cs.virginia.edu/pub/techreports/CS-98-05.ps.Z
Refering-URL: ftp://ftp.cs.virginia.edu/pub/techreports/README.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Abstract: The Counterflow Pipeline (CFP) organization may be a good target for synthesis of applicationspecific microprocessors for embedded systems because it has a regular and simple structure. This paper describes a design environment for tailoring CFPs to an embedded application to improve performance. Our system allows exploring the design space of all possible CFPs for a given embedded application to understand the impact of different design decisions on performance. We have used the environment to derive heuristics that help to find the best CFP for an application. Preliminary results using our heuristics indicate that speedup for several small graphs range from 1.3 to 2.0 over a general-purpose CFP and that the heuristics find designs that are within 10% of optimal. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Benitez M. E. and Davidson, J. W., </author> <title> A Portable Global Optimizer and Linker, </title> <booktitle> Proc. of the SIGPLAN Notices 1988 Symp. on Programming Language Design and Implementation , pp. 329338, </booktitle> <address> Atlanta, Georgia, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: The loop is compiled by the optimizer vpo <ref> [1] </ref> into instructions for the SPARC architecture. During optimization, vpo builds the data dependency graph for the kernel loop. The graph serves as an input to the rest of the synthesis system, which consists of modules for hardware allocation, processor topology determination, instruction scheduling, simulation, and performance analysis. 2.1.
Reference: [2] <author> Binh N. N., Imai M, Shiomi A, et al., </author> <title> A Hardware/software partitioning algorithm for designing pipelined ASIPs with least gate counts, </title> <booktitle> Proc. of 33rd Design Automation Conf. </booktitle> , <pages> pp. 527532, </pages> <address> Las Vegas, Nevada, </address> <month> June </month> <year> 1996. </year>
Reference: [3] <author> Corporaal, H. and Hoogerbrugge J., </author> <title> Cosynthesis with the MOVE framework, </title> <booktitle> Symp. on Modelling, Analysis, and Simulation, </booktitle> <volume> CESA 96, </volume> <pages> pp. 184189, </pages> <address> Lille, France, </address> <month> July </month> <year> 1996. </year>
Reference: [4] <author> Ebeling C., Cronquiest D. C., Franklin P., et al., </author> <title> Mapping applications to the RaPiD configurable architecture, </title> <booktitle> IEEE 5th Annual Symp. on Field-Programmable Custom Computing Machines , pp. </booktitle> <pages> 106-115, </pages> <address> Napa Valley, California, </address> <month> April 16-18, </month> <year> 1997. </year>
Reference: [5] <author> Fisher J. A., Faraboschi P., and Desoli G., </author> <title> Custom-fit processors: Letting applications define architectures, </title> <type> Technical Report HPL96144, </type> <institution> Hewlett-Packard Laboratories, Palo Alto, California, </institution> <year> 1996. </year>
Reference: [6] <author> Goering R., </author> <title> Codesign turns workplace on its head, EE Times, </title> <journal> Monday, Jan. </journal> <volume> 12, </volume> <year> 1998. </year>
Reference-contexts: Related Work Recently there has been much interest in automated design of applicationspecific integrated processors (ASIPs) because of the increasing importance of high-performance and quick turnaround in the embedded systems market <ref> [6] </ref>. ASIP techniques typically address two broad problems: instruction set and micro-architecture synthesis. Instruction set synthesis attempts to discover micro-operations in a program (or set of programs) that can be combined to create instructions [10,11].
Reference: [7] <author> Gupta R. K., and Micheli G., </author> <title> Hardwaresoftware cosynthesis for digital systems, </title> <journal> IEEE Design and Test of Computers, </journal> <volume> Vol. 10, No. 3, </volume> <pages> pp. 2941, </pages> <month> Sept. </month> <year> 1993. </year>
Reference: [8] <author> Hauser J. R., and Wawrzynek J., Garp: </author> <title> A MIPS processor with a reconfigurable coprocessor, </title> <booktitle> IEEE 5th Annual Symp. </booktitle> <volume> on Field-Programmable Custom Comput Graph 0 1 2 1 2 3 4 5 6 7 8 9 10 p e -u Opt H 1 0 10 20 1 2 3 4 5 6 7 8 9 10 Graph % f D s n p c H1 11 ing Machines, </volume> <pages> pp. 1221, </pages> <address> Napa Valley, California, </address> <month> April 16-18, </month> <year> 1997. </year>
Reference: [9] <author> Hennessy J. L. and Patterson D. A., </author> <title> Computer Architecture: </title> <publisher> A Quantitative Approach , 2nd edition , Morgan Kaufmann Publishers, Inc., </publisher> <address> San Francisco, Cal-ifornia, </address> <year> 1996. </year>
Reference-contexts: Synthesis Strategy Most high-performance embedded applications have two parts: a control and a computation-intensive part. The computation part is typically a kernel loop that accounts for the majority of execution time. According to Amdahls Law <ref> [9] </ref>, increasing the performance of the most frequently executed portion of an application increases overall performance. Thus, synthesizing custom hardware for the computation-intensive portion may be an effective technique to increase performance. The type of applications we are considering need only a modest kernel speedup to effectively improve overall performance.
Reference: [10] <author> Holmer B., </author> <title> Automatic Design of Instruction Sets, </title> <type> Ph.D. thesis, </type> <institution> University of California, Berkeley, </institution> <year> 1993. </year>
Reference: [11] <author> Huang I-J and Despain A. M., </author> <title> Synthesis of application specific instruction sets, </title> <journal> IEEE Trans. on Computer-Aided Design of Integrated Circuits and Systems, </journal> <volume> Vol 14, No. 6, </volume> <pages> pp. 663675, </pages> <month> June </month> <year> 1995. </year>
Reference: [12] <author> Rau B. R. and Fisher J. A., </author> <title> Instruction-level parallel processing: History, overview, and perspective, </title> <journal> J. of Supercomputing, </journal> <volume> Vol 7, </volume> <pages> pp. 950, </pages> <month> May </month> <year> 1993. </year>
Reference: [13] <author> Razdan R. and Smith M. D., </author> <title> A High-Performance Microarchitecture with Hardware-Programmable Functional Units, </title> <booktitle> Proc. of 27th Annual Intl Symp. on Microarchitecture, </booktitle> <pages> pp. 172180, </pages> <address> Dec.1994, San Jose, California. </address>
Reference: [14] <author> SPARC International, Inc., </author> <title> The SPARC Architecture Manual, </title> <publisher> Version 8 , Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1992. </year>
Reference-contexts: A clock cycle for the counterflow pipeline is a very small time unit such as a few gate delays. The instruction set we use is a subset of the SPARC V8 <ref> [14] </ref> instruction set architecture. The instruction subset does not support register windows or tagged operations, but is representative of modern instruction sets for embedded RISC cores. 2.
Reference: [15] <author> Sproull R. F., Sutherland I. E., and Molnar C. E., </author> <title> The Counterflow Pipeline Processor Architecture, </title> <journal> IEEE Design and Test of Computers, pp. </journal> <volume> 4859, Vol. 11, No. 3, </volume> <month> Fall </month> <year> 1994. </year>
Reference-contexts: This is especially useful for embedded systems (e.g., automobile control systems, avionics, cellular phones, etc.) where a small increase in performance and decrease in cost can have a large impact on a products viability. A new computer organization called the Counterflow Pipeline (CFP), proposed by Sproull, Suther-land, and Molnar <ref> [15] </ref>, has several characteristics that make it a possible target organization for the synthesis of applicationspecific microprocessors. The CFP has a simple and regular structure, local control, high degree of modularity, asynchronous implementations, and inherent handling of complex structures such as result forwarding and speculative execution.
References-found: 15

