URL: ftp://ftp.cs.dartmouth.edu/pub/kotz/papers/cormen:integrate.ps.Z
Refering-URL: http://www.cs.dartmouth.edu/~dfk/papers/dags.html
Root-URL: http://www.cs.dartmouth.edu
Title: Integrating Theory and Practice in Parallel File Systems  
Author: Thomas H. Cormen David Kotz 
Web: URL ftp://ftp.cs.dartmouth.edu/pub/CS-papers/Kotz/cormen:integrate.ps.Z  
Address: College.  Hanover, NH 03755-3551  
Affiliation: of Dartmouth  Department of Mathematics and Computer Science Dartmouth College  
Note: Copyright 1993 by Trustees  Appeared in DAGS/PC Symposium on Parallel I/O and Databases, pages 64-74. Available at  
Abstract: Several algorithms for parallel disk systems have appeared in the literature recently, and they are asymptotically optimal in terms of the number of disk accesses. Scalable systems with parallel disks must be able to run these algorithms. We present for the first time a list of capabilities that must be provided by the system to support these optimal algorithms: control over declustering, querying about the configuration, independent I/O, and turning off parity, file caching, and prefetching. We summarize recent theoretical and empirical work that justifies the need for these capabilities. In addition, we sketch an organization for a parallel file interface with low-level primitives and higher-level operations.
Abstract-found: 1
Intro-found: 1
Reference: [BCR92] <author> Rajesh Bordawekar, Alok Choudhary, and Juan Miguel Del Rosario. </author> <title> An experimental performance evaluation of Touchstone Delta Concurrent File System. </title> <type> Technical Report SCCS-420, </type> <institution> NPAC, Syracuse University, </institution> <year> 1992. </year> <title> To appear, </title> <booktitle> 1993 International Conference on Supercomputing. </booktitle>
Reference-contexts: The performance of Intel's CFS when reading or writing a two-dimensional matrix, for example, depends heavily on the layout of the matrix across disks and across memories of the multiprocessor, and also on the order of requests <ref> [dBC93, BCR92, Nit92, GP91, GL91] </ref>. del Rosario et al. [dBC93] find that the nCUBE exhibits similar inefficiencies: when reading columns from a two-dimensional matrix stored in row-major order, read times increase by factors of 30-50.
Reference: [CBF93] <author> Peter F. Corbett, Sandra Johnson Baylor, and Dror G. Feitelson. </author> <title> Overview of the Vesta parallel file system. </title> <booktitle> In IPPS '93 Workshop on Input/Output in Parallel Computer Systems, </booktitle> <pages> pages 1-16, </pages> <year> 1993. </year>
Reference-contexts: Writes must be fully striped, however, thus limiting some algorithms. Neither the file system for the newer Scalable Disk Array [TMC92, LIN + 93] nor the file system for the MasPar MP-1 and MP-2 [Mas91, Mas92] support independent I/O as we have defined it. 2 IBM's Vesta file system <ref> [CBF93] </ref> for its Vulcan prototype multiprocessor supports many of the capabilities we require. Users can control the declustering of a file when it is created, specifying the number of disks, record size, and stripe-unit size.
Reference: [Cor92] <author> Thomas H. Cormen. </author> <title> Virtual Memory for Data-Parallel Computing. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <year> 1992. </year> <note> Available as Technical Report MIT/LCS/TR-559. </note>
Reference-contexts: General permutations: Vitter and Shriver [VS90, VS92] use their sorting algorithm to perform general permutations by sorting on target addresses. Bit-defined permutations: Cormen <ref> [Cor92, Cor93] </ref> presents algorithms to perform bit-defined permutations often with fewer parallel I/O operations than general permutations. <p> This class includes all BPC permutations, Gray code permutations, and inverse Gray code permutations. General matrix transpose: Cormen <ref> [Cor92] </ref> gives an asymptotically optimal algorithm for ma trix transpose with arbitrary dimensions, not just those that are powers of 2. Fast Fourier Transform: Vitter and Shriver [VS90, VS92] give an asymptotically optimal algo rithm to compute an FFT.
Reference: [Cor93] <author> Thomas H. Cormen. </author> <title> Fast permuting in disk arrays. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> 17(1-2):41-57, January and February 1993. </note>
Reference-contexts: General permutations: Vitter and Shriver [VS90, VS92] use their sorting algorithm to perform general permutations by sorting on target addresses. Bit-defined permutations: Cormen <ref> [Cor92, Cor93] </ref> presents algorithms to perform bit-defined permutations often with fewer parallel I/O operations than general permutations.
Reference: [CW93] <author> Thomas H. Cormen and Leonard F. Wisniewski. </author> <title> Asymptotically tight bounds for performing BMMC permutations on parallel disk systems. </title> <note> To appear in SPAA '93, </note> <month> January </month> <year> 1993. </year>
Reference-contexts: Among the useful BPC permutations are matrix transpose 3 with dimensions that are powers of 2, bit-reversal permutations, vector-reversal permutations, hypercube permutations, and matrix reblocking. Cormen and Wisniewski <ref> [CW93] </ref> present an asymptotically optimal algorithm for BMMC (bit-matrix-multiply/complement) permutations, in which each target address is formed by multiplying a source address by a matrix that is nonsingular over GF (2) and then complementing a fixed subset of the resulting bits.
Reference: [dBC93] <author> Juan Miguel del Rosario, Rajesh Borawekar, and Alok Choudhary. </author> <title> Improved parallel I/O via a two-phase run-time access strategy. </title> <booktitle> In IPPS '93 Workshop on Input/Output in Parallel Computer Systems, </booktitle> <pages> pages 56-70, </pages> <year> 1993. </year>
Reference-contexts: In fact, the operating system treats each disk as a separate file system and does not decluster individual files across disks. Thus, the nCUBE provides the low-level access one needs, but no higher-level access. The current nCUBE file system <ref> [dBC93] </ref> supports declustering and does allow applications to manipulate the striping unit size and distribution pattern. The file system for the Kendall Square Research KSR-1 [KSR92] shared-memory multiprocessor declusters file data across disk arrays attached to different processors. <p> The performance of Intel's CFS when reading or writing a two-dimensional matrix, for example, depends heavily on the layout of the matrix across disks and across memories of the multiprocessor, and also on the order of requests <ref> [dBC93, BCR92, Nit92, GP91, GL91] </ref>. del Rosario et al. [dBC93] find that the nCUBE exhibits similar inefficiencies: when reading columns from a two-dimensional matrix stored in row-major order, read times increase by factors of 30-50. <p> The performance of Intel's CFS when reading or writing a two-dimensional matrix, for example, depends heavily on the layout of the matrix across disks and across memories of the multiprocessor, and also on the order of requests [dBC93, BCR92, Nit92, GP91, GL91]. del Rosario et al. <ref> [dBC93] </ref> find that the nCUBE exhibits similar inefficiencies: when reading columns from a two-dimensional matrix stored in row-major order, read times increase by factors of 30-50. One solution is to transfer data from disk into memory and then permute it within memory to its final destination [dBC93]. <p> del Rosario et al. <ref> [dBC93] </ref> find that the nCUBE exhibits similar inefficiencies: when reading columns from a two-dimensional matrix stored in row-major order, read times increase by factors of 30-50. One solution is to transfer data from disk into memory and then permute it within memory to its final destination [dBC93]. Nitzberg [Nit92] shows that some layouts experience poor performance on CFS because of thrashing in the file system cache. His solution to this problem carefully schedules the processors' accesses to the disks by reducing concurrency [Nit92].
Reference: [DBMS79] <author> J. Dongarra, J. Bunch, C. Moler, and G. W. Stewart. </author> <title> LINPACK User's Guide. </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1979. </year>
Reference-contexts: The library can be invoked either directly by the user or by a smart compiler, much like the LINPACK suite of numerical algorithms <ref> [DBMS79] </ref>. This library depends on the existence of the above primitive operations for detailed control of I/O. 5 Parity We claimed in Section 2 that parallel file systems should be able to turn off parity or other redundancy information on a per-file basis.
Reference: [FPD93] <author> James C. French, Terrence W. Pratt, and Mriganka Das. </author> <title> Performance measurement of the Concurrent File System of the Intel iPSC/2 hypercube. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> 17(1-2):115-121, January and February 1993. </note>
Reference-contexts: Existing systems Here we survey some existing systems and their support for the above capabilities. Table 1 summarizes these systems. One of the first commercial multiprocessor file systems is the Concurrent File System (CFS) <ref> [Pie89, FPD93, PFDJ89] </ref> for the Intel iPSC and Touchstone Delta multiprocessors [Int88]. CFS declusters files across several I/O processors, each with one or more disks. It provides the user with several different access modes, allowing different ways of sharing a common file pointer.
Reference: [GL91] <author> Andrew S. Grimshaw and Edmond C. Loyot, Jr. </author> <title> ELFS: object-oriented extensible file systems. </title> <type> Technical Report TR-91-14, </type> <institution> Univ. of Virginia Computer Science Department, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: The performance of Intel's CFS when reading or writing a two-dimensional matrix, for example, depends heavily on the layout of the matrix across disks and across memories of the multiprocessor, and also on the order of requests <ref> [dBC93, BCR92, Nit92, GP91, GL91] </ref>. del Rosario et al. [dBC93] find that the nCUBE exhibits similar inefficiencies: when reading columns from a two-dimensional matrix stored in row-major order, read times increase by factors of 30-50. <p> Each of these examples highlights the need for programs to organize their I/O carefully. To do so, we must be able to discover and control the I/O system configuration. Grimshaw et al. make many of the same arguments for their ELFS file system <ref> [GP91, GL91] </ref>. ELFS is an extensible file system, building object-oriented, operation-specific classes on top of a simple set of file access primitives. ELFS leaves decisions about declustering, caching, and prefetching to the higher-level functions, which have a broader understanding of the operation.
Reference: [GP91] <author> Andrew S. Grimshaw and Jeff Prem. </author> <title> High performance parallel file objects. </title> <booktitle> In Sixth Annual Distributed-Memory Computer Conference, </booktitle> <pages> pages 720-723, </pages> <year> 1991. </year> <title> [Int88] iPSC/2 I/O facilities. </title> <publisher> Intel Corporation, </publisher> <year> 1988. </year> <title> Order number 280120-001. </title>
Reference-contexts: The performance of Intel's CFS when reading or writing a two-dimensional matrix, for example, depends heavily on the layout of the matrix across disks and across memories of the multiprocessor, and also on the order of requests <ref> [dBC93, BCR92, Nit92, GP91, GL91] </ref>. del Rosario et al. [dBC93] find that the nCUBE exhibits similar inefficiencies: when reading columns from a two-dimensional matrix stored in row-major order, read times increase by factors of 30-50. <p> Each of these examples highlights the need for programs to organize their I/O carefully. To do so, we must be able to discover and control the I/O system configuration. Grimshaw et al. make many of the same arguments for their ELFS file system <ref> [GP91, GL91] </ref>. ELFS is an extensible file system, building object-oriented, operation-specific classes on top of a simple set of file access primitives. ELFS leaves decisions about declustering, caching, and prefetching to the higher-level functions, which have a broader understanding of the operation.
Reference: [Kot93] <author> David Kotz. </author> <title> Multiprocessor file system interfaces. </title> <booktitle> In Proceedings of the Second International Conference on Parallel and Distributed Information Systems, </booktitle> <pages> pages 194-201, </pages> <year> 1993. </year>
Reference-contexts: Without detailing a specific file system interface (although some of our ideas are given in <ref> [Kot93] </ref>), we propose an interface with two personalities. Low-level primitive operations The primitive operations provide the "traditional" file system interface, such as basic read, write, and seek operations. The file system provides default declustering, caching, prefetching, and parity, making this interface sufficient for many simple applications.
Reference: [KSR92] <institution> KSR1 technology background. Kendall Square Research, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: Thus, the nCUBE provides the low-level access one needs, but no higher-level access. The current nCUBE file system [dBC93] supports declustering and does allow applications to manipulate the striping unit size and distribution pattern. The file system for the Kendall Square Research KSR-1 <ref> [KSR92] </ref> shared-memory multiprocessor declusters file data across disk arrays attached to different processors. The memory-mapped interface uses virtual memory techniques to page data to and from the file, which does not provide sufficient control to an application trying to optimize disk I/O.
Reference: [LIN + 93] <author> Susan J. LoVerso, Marshall Isman, Andy Nanopoulos, William Nesheim, Ewan D. Milne, and Richard Wheeler. sfs: </author> <title> A parallel file system for the CM-5. </title> <booktitle> In Proceedings of the 1993 Summer Usenix Conference, </booktitle> <year> 1993. </year> <note> To appear. </note>
Reference-contexts: Reads and writes in the Thinking Machines Corporation's DataVault [TMC91] are controlled directly by the user. Writes must be fully striped, however, thus limiting some algorithms. Neither the file system for the newer Scalable Disk Array <ref> [TMC92, LIN + 93] </ref> nor the file system for the MasPar MP-1 and MP-2 [Mas91, Mas92] support independent I/O as we have defined it. 2 IBM's Vesta file system [CBF93] for its Vulcan prototype multiprocessor supports many of the capabilities we require.
Reference: [Mas91] <author> MP-1 family: </author> <title> Massively parallel computers. </title> <institution> MasPar Computer Corporation brochure number PL014.0691, 1990,1991. </institution>
Reference-contexts: Writes must be fully striped, however, thus limiting some algorithms. Neither the file system for the newer Scalable Disk Array [TMC92, LIN + 93] nor the file system for the MasPar MP-1 and MP-2 <ref> [Mas91, Mas92] </ref> support independent I/O as we have defined it. 2 IBM's Vesta file system [CBF93] for its Vulcan prototype multiprocessor supports many of the capabilities we require. Users can control the declustering of a file when it is created, specifying the number of disks, record size, and stripe-unit size.
Reference: [Mas92] <institution> Parallel file I/O routines. MasPar Computer Corporation, </institution> <year> 1992. </year>
Reference-contexts: Writes must be fully striped, however, thus limiting some algorithms. Neither the file system for the newer Scalable Disk Array [TMC92, LIN + 93] nor the file system for the MasPar MP-1 and MP-2 <ref> [Mas91, Mas92] </ref> support independent I/O as we have defined it. 2 IBM's Vesta file system [CBF93] for its Vulcan prototype multiprocessor supports many of the capabilities we require. Users can control the declustering of a file when it is created, specifying the number of disks, record size, and stripe-unit size.
Reference: [Nit92] <author> Bill Nitzberg. </author> <title> Performance of the iPSC/860 concurrent file system. </title> <type> Technical Report RND-92-020, </type> <institution> NAS Systems Division, NASA Ames, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: The performance of Intel's CFS when reading or writing a two-dimensional matrix, for example, depends heavily on the layout of the matrix across disks and across memories of the multiprocessor, and also on the order of requests <ref> [dBC93, BCR92, Nit92, GP91, GL91] </ref>. del Rosario et al. [dBC93] find that the nCUBE exhibits similar inefficiencies: when reading columns from a two-dimensional matrix stored in row-major order, read times increase by factors of 30-50. <p> One solution is to transfer data from disk into memory and then permute it within memory to its final destination [dBC93]. Nitzberg <ref> [Nit92] </ref> shows that some layouts experience poor performance on CFS because of thrashing in the file system cache. His solution to this problem carefully schedules the processors' accesses to the disks by reducing concurrency [Nit92]. Each of these examples highlights the need for programs to organize their I/O carefully. <p> Nitzberg <ref> [Nit92] </ref> shows that some layouts experience poor performance on CFS because of thrashing in the file system cache. His solution to this problem carefully schedules the processors' accesses to the disks by reducing concurrency [Nit92]. Each of these examples highlights the need for programs to organize their I/O carefully. To do so, we must be able to discover and control the I/O system configuration. Grimshaw et al. make many of the same arguments for their ELFS file system [GP91, GL91].
Reference: [NV91] <author> Mark H. Nodine and Jeffrey Scott Vitter. </author> <title> Large-scale sorting in parallel memories. </title> <booktitle> In Proceedings of the 3rd Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 29-39, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: These algorithms, which are oriented toward out-of-core situations, are asymptotically optimal in terms of the number of parallel disk accesses. They solve the following problems: Sorting: Vitter and Shriver [VS90, VS92] give a randomized sorting algorithm, and Nodine and Vitter <ref> [NV91, NV92] </ref> present a deterministic sorting algorithm. General permutations: Vitter and Shriver [VS90, VS92] use their sorting algorithm to perform general permutations by sorting on target addresses. Bit-defined permutations: Cormen [Cor92, Cor93] presents algorithms to perform bit-defined permutations often with fewer parallel I/O operations than general permutations.
Reference: [NV92] <author> Mark H. Nodine and Jeffrey Scott Vitter. </author> <title> Optimal deterministic sorting on parallel disks. </title> <type> Technical Report CS-92-08, </type> <institution> Department of Computer Science, Brown University, </institution> <year> 1992. </year>
Reference-contexts: These algorithms, which are oriented toward out-of-core situations, are asymptotically optimal in terms of the number of parallel disk accesses. They solve the following problems: Sorting: Vitter and Shriver [VS90, VS92] give a randomized sorting algorithm, and Nodine and Vitter <ref> [NV91, NV92] </ref> present a deterministic sorting algorithm. General permutations: Vitter and Shriver [VS90, VS92] use their sorting algorithm to perform general permutations by sorting on target addresses. Bit-defined permutations: Cormen [Cor92, Cor93] presents algorithms to perform bit-defined permutations often with fewer parallel I/O operations than general permutations.
Reference: [PFDJ89] <author> Terrence W. Pratt, James C. French, Phillip M. Dickens, and Stanley A. Janet, Jr. </author> <title> A comparison of the architecture and performance of two parallel file systems. </title> <booktitle> In Fourth Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 161-166, </pages> <year> 1989. </year>
Reference-contexts: Existing systems Here we survey some existing systems and their support for the above capabilities. Table 1 summarizes these systems. One of the first commercial multiprocessor file systems is the Concurrent File System (CFS) <ref> [Pie89, FPD93, PFDJ89] </ref> for the Intel iPSC and Touchstone Delta multiprocessors [Int88]. CFS declusters files across several I/O processors, each with one or more disks. It provides the user with several different access modes, allowing different ways of sharing a common file pointer. <p> RAID level three organization limits the disk array at each I/O node to only fully striped I/O. The apparent number of independent disks, therefore, is only the number of I/O nodes, rather than the larger number of physical disks. The first file system for the nCUBE multiprocessor <ref> [PFDJ89] </ref> gives plenty of control to the user. In fact, the operating system treats each disk as a separate file system and does not decluster individual files across disks. Thus, the nCUBE provides the low-level access one needs, but no higher-level access.
Reference: [PGK88] <author> David A. Patterson, Garth Gibson, and Randy H. Katz. </author> <title> A case for redundant arrays of inexpensive disks (RAID). </title> <booktitle> In ACM International Conference on Management of Data (SIGMOD), </booktitle> <pages> pages 109-116, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: A common distribution pattern is striping, in which striping units are distributed in round-robin order among the disks; a stripe consists of the data distributed in one round. Striping unit sizes are often either one bit (as in RAID level three <ref> [PGK88] </ref>) or equal to the block size (as in RAID levels four and five). The optimal algorithms assume striping with a block-sized striping unit. The programmer, therefore, should be able to redefine the striping unit size and distribution pattern of individual files. <p> This section shows why we want to do so. Because we maintain parity to improve data reliability, this section also describes typical situations in which we can turn off parity without compromising data reliability. The cost of maintaining parity Patterson, Gibson, and Katz <ref> [PGK88] </ref> outline various RAID (Redundant Arrays of Inexpensive Disks) organizations. RAID levels four and five support independent I/Os. Both use check disks to store parity information. In level four, the parity information is stored on a single dedicated check disk.
Reference: [Pie89] <author> Paul Pierce. </author> <title> A concurrent file system for a highly parallel mass storage system. </title> <booktitle> In Fourth Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 155-160, </pages> <year> 1989. </year>
Reference-contexts: Existing systems Here we survey some existing systems and their support for the above capabilities. Table 1 summarizes these systems. One of the first commercial multiprocessor file systems is the Concurrent File System (CFS) <ref> [Pie89, FPD93, PFDJ89] </ref> for the Intel iPSC and Touchstone Delta multiprocessors [Int88]. CFS declusters files across several I/O processors, each with one or more disks. It provides the user with several different access modes, allowing different ways of sharing a common file pointer.
Reference: [Rul93] <author> Brad Rullman, </author> <month> April </month> <year> 1993. </year> <title> Private communication. </title>
Reference-contexts: Its designers claim that the Parallel File System (PFS) for the Intel Paragon supports our list of capabilities <ref> [Rul93] </ref>, but we have not had the opportunity to verify this claim. We note, however, that the Paragon does not maintain parity across I/O nodes. Instead, each I/O node controls a separate RAID-level-three disk array, which maintains its own parity information independent of all other I/O nodes.
Reference: [TMC91] <institution> Thinking Machines Corporation, Cambridge, Massachusetts. </institution> <note> Connection Machine I/O System Programming Guide, </note> <month> October </month> <year> 1991. </year>
Reference-contexts: The memory-mapped interface uses virtual memory techniques to page data to and from the file, which does not provide sufficient control to an application trying to optimize disk I/O. Reads and writes in the Thinking Machines Corporation's DataVault <ref> [TMC91] </ref> are controlled directly by the user. Writes must be fully striped, however, thus limiting some algorithms.
Reference: [TMC92] <institution> CM-5 scalable disk array. Thinking Machines Corporation glossy, </institution> <month> November </month> <year> 1992. </year>
Reference-contexts: Reads and writes in the Thinking Machines Corporation's DataVault [TMC91] are controlled directly by the user. Writes must be fully striped, however, thus limiting some algorithms. Neither the file system for the newer Scalable Disk Array <ref> [TMC92, LIN + 93] </ref> nor the file system for the MasPar MP-1 and MP-2 [Mas91, Mas92] support independent I/O as we have defined it. 2 IBM's Vesta file system [CBF93] for its Vulcan prototype multiprocessor supports many of the capabilities we require.
Reference: [VS90] <author> Jeffrey Scott Vitter and Elizabeth A. M. Shriver. </author> <title> Optimal disk I/O with parallel block transfer. </title> <booktitle> In Proceedings of the Twenty Second Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 159-169, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Theoretical grounds Several algorithms for parallel disk systems have been developed recently. These algorithms, which are oriented toward out-of-core situations, are asymptotically optimal in terms of the number of parallel disk accesses. They solve the following problems: Sorting: Vitter and Shriver <ref> [VS90, VS92] </ref> give a randomized sorting algorithm, and Nodine and Vitter [NV91, NV92] present a deterministic sorting algorithm. General permutations: Vitter and Shriver [VS90, VS92] use their sorting algorithm to perform general permutations by sorting on target addresses. <p> They solve the following problems: Sorting: Vitter and Shriver <ref> [VS90, VS92] </ref> give a randomized sorting algorithm, and Nodine and Vitter [NV91, NV92] present a deterministic sorting algorithm. General permutations: Vitter and Shriver [VS90, VS92] use their sorting algorithm to perform general permutations by sorting on target addresses. Bit-defined permutations: Cormen [Cor92, Cor93] presents algorithms to perform bit-defined permutations often with fewer parallel I/O operations than general permutations. <p> This class includes all BPC permutations, Gray code permutations, and inverse Gray code permutations. General matrix transpose: Cormen [Cor92] gives an asymptotically optimal algorithm for ma trix transpose with arbitrary dimensions, not just those that are powers of 2. Fast Fourier Transform: Vitter and Shriver <ref> [VS90, VS92] </ref> give an asymptotically optimal algo rithm to compute an FFT. Matrix multiplication: Vitter and Shriver [VS90, VS92] cover matrix multiplication as well. LU decomposition: Womble et al. [WGWR93] sketch an LU-decomposition algorithm. <p> General matrix transpose: Cormen [Cor92] gives an asymptotically optimal algorithm for ma trix transpose with arbitrary dimensions, not just those that are powers of 2. Fast Fourier Transform: Vitter and Shriver <ref> [VS90, VS92] </ref> give an asymptotically optimal algo rithm to compute an FFT. Matrix multiplication: Vitter and Shriver [VS90, VS92] cover matrix multiplication as well. LU decomposition: Womble et al. [WGWR93] sketch an LU-decomposition algorithm. <p> The parallel disk model used by these algorithms was originally proposed by Vitter and Shriver <ref> [VS90, VS92] </ref>. The cost measure is the number of parallel I/O operations performed over the course of a computation. The model does not specify the memory's organization, connection to the disks, or relation to the processors, and so it is independent of any particular machine architecture. <p> It turns out that the constraint of fully striped I/O increases the number of disk accesses by more than a constant factor compared to independent I/O <ref> [VS90, VS92] </ref>. Disk accesses are expensive enough; to increase their number by more than a constant factor for large amounts of data can be prohibitively expensive. The algorithms treat all physical memory uniformly; there is no distinct file cache.
Reference: [VS92] <author> Jeffrey Scott Vitter and Elizabeth A. M. Shriver. </author> <title> Algorithms for parallel memory I: Two-level memories. </title> <type> Technical Report CS-92-04, </type> <institution> Department of Computer Science, Brown University, </institution> <month> August </month> <year> 1992. </year> <note> Revised version of Technical Report CS-90-21. </note>
Reference-contexts: Theoretical grounds Several algorithms for parallel disk systems have been developed recently. These algorithms, which are oriented toward out-of-core situations, are asymptotically optimal in terms of the number of parallel disk accesses. They solve the following problems: Sorting: Vitter and Shriver <ref> [VS90, VS92] </ref> give a randomized sorting algorithm, and Nodine and Vitter [NV91, NV92] present a deterministic sorting algorithm. General permutations: Vitter and Shriver [VS90, VS92] use their sorting algorithm to perform general permutations by sorting on target addresses. <p> They solve the following problems: Sorting: Vitter and Shriver <ref> [VS90, VS92] </ref> give a randomized sorting algorithm, and Nodine and Vitter [NV91, NV92] present a deterministic sorting algorithm. General permutations: Vitter and Shriver [VS90, VS92] use their sorting algorithm to perform general permutations by sorting on target addresses. Bit-defined permutations: Cormen [Cor92, Cor93] presents algorithms to perform bit-defined permutations often with fewer parallel I/O operations than general permutations. <p> This class includes all BPC permutations, Gray code permutations, and inverse Gray code permutations. General matrix transpose: Cormen [Cor92] gives an asymptotically optimal algorithm for ma trix transpose with arbitrary dimensions, not just those that are powers of 2. Fast Fourier Transform: Vitter and Shriver <ref> [VS90, VS92] </ref> give an asymptotically optimal algo rithm to compute an FFT. Matrix multiplication: Vitter and Shriver [VS90, VS92] cover matrix multiplication as well. LU decomposition: Womble et al. [WGWR93] sketch an LU-decomposition algorithm. <p> General matrix transpose: Cormen [Cor92] gives an asymptotically optimal algorithm for ma trix transpose with arbitrary dimensions, not just those that are powers of 2. Fast Fourier Transform: Vitter and Shriver <ref> [VS90, VS92] </ref> give an asymptotically optimal algo rithm to compute an FFT. Matrix multiplication: Vitter and Shriver [VS90, VS92] cover matrix multiplication as well. LU decomposition: Womble et al. [WGWR93] sketch an LU-decomposition algorithm. <p> The parallel disk model used by these algorithms was originally proposed by Vitter and Shriver <ref> [VS90, VS92] </ref>. The cost measure is the number of parallel I/O operations performed over the course of a computation. The model does not specify the memory's organization, connection to the disks, or relation to the processors, and so it is independent of any particular machine architecture. <p> It turns out that the constraint of fully striped I/O increases the number of disk accesses by more than a constant factor compared to independent I/O <ref> [VS90, VS92] </ref>. Disk accesses are expensive enough; to increase their number by more than a constant factor for large amounts of data can be prohibitively expensive. The algorithms treat all physical memory uniformly; there is no distinct file cache.
Reference: [WGWR93] <author> David Womble, David Greenberg, Stephen Wheat, and Rolf Riesen. </author> <title> Beyond core: Making parallel computer I/O practical. </title> <booktitle> In DAGS '93, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: Fast Fourier Transform: Vitter and Shriver [VS90, VS92] give an asymptotically optimal algo rithm to compute an FFT. Matrix multiplication: Vitter and Shriver [VS90, VS92] cover matrix multiplication as well. LU decomposition: Womble et al. <ref> [WGWR93] </ref> sketch an LU-decomposition algorithm.
References-found: 27

