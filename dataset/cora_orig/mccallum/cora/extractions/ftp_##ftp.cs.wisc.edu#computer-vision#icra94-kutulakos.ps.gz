URL: ftp://ftp.cs.wisc.edu/computer-vision/icra94-kutulakos.ps.gz
Refering-URL: http://www.cs.wisc.edu/computer-vision/pubs.html
Root-URL: 
Title: Provable Strategies for Vision-Guided Exploration in Three Dimensions  
Author: Kiriakos N. Kutulakos Charles R. Dyer Vladimir J. Lumelsky 
Address: Madison, Wisconsin 53706 USA  
Affiliation: Computer Sciences Department University of Wisconsin  
Abstract: An approach is presented for exploring an unknown, arbitrary surface in three-dimensional (3D) space by a mobile robot. The main contributions are (1) an analysis of the capabilities a robot must possess and the trade-offs involved in the design of an exploration strategy, and (2) two provably-correct exploration strategies that exploit these trade-offs and use visual sensors (e.g., cameras and range sensors) to plan the robot's motion. No such analysis existed previously for the case of a robot moving freely in 3D space. The approach exploits the notion of the occlusion boundary, i.e., the points separating the visible from the occluded parts of an object. The occlusion boundary is a collection of curves that slide over the surface when the robot's position is continuously controlled, inducing the visibility of surface points over which they slide. The paths generated by our strategies force the occlusion boundary to slide over the entire surface. The strategies provide a basis for integrating motion planning and visual sensing under a common computational framework. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. J. Koenderink, </author> <title> Solid Shape. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: These strategies build on computer vision research that has studied the appearance of smooth and piecewise-smooth surfaces <ref> [1, 2] </ref>. Our results provide a basis for integrating motion planning and visual sensing under a common computational framework. <p> A convenient way to visualize and study the occlusion boundary and the visible rim is to look at their spherical projections onto an image centered at the robot's position (Figure 3). Their projections are identical; this projected contour is known as the occluding contour <ref> [1] </ref>. In general, the occlusion boundary is collection of closed, piecewise-smooth curves, ff 1 ; : : : ; ff n , whose number and shape depends on the robot's position.
Reference: [2] <author> J. H. Rieger, </author> <title> On the classification of views of piecewise smooth objects, </title> <journal> Image and Vision Computing, </journal> <volume> vol. 5, no. 2, </volume> <pages> pp. 91-97, </pages> <year> 1987. </year>
Reference-contexts: These strategies build on computer vision research that has studied the appearance of smooth and piecewise-smooth surfaces <ref> [1, 2] </ref>. Our results provide a basis for integrating motion planning and visual sensing under a common computational framework.
Reference: [3] <author> J. H. Reif, </author> <title> Complexity of the mover's problem and generalizations, </title> <booktitle> in Proc. Twentieth Symposium on Foundations of Computer Science, </booktitle> <pages> pp. 421-427, </pages> <year> 1979. </year>
Reference-contexts: Our results provide a basis for integrating motion planning and visual sensing under a common computational framework. Our work has been inspired by recent approaches which, instead of assuming that complete information about the environment is available <ref> [3] </ref>, use only the sensor information that is necessary for planning the motion of the robot [4]. These approaches follow a purposive [5], act-while-thinking strategy and consider robotic motion planning as a continuous process where sensing and action are tightly coupled [6-8].
Reference: [4] <author> R. A. Brooks, </author> <title> A robust layered control system for a mobile robot, </title> <journal> IEEE J. Robotics Automat., </journal> <volume> vol. 2, no. 1, </volume> <pages> pp. 14-23, </pages> <year> 1986. </year>
Reference-contexts: Our work has been inspired by recent approaches which, instead of assuming that complete information about the environment is available [3], use only the sensor information that is necessary for planning the motion of the robot <ref> [4] </ref>. These approaches follow a purposive [5], act-while-thinking strategy and consider robotic motion planning as a continuous process where sensing and action are tightly coupled [6-8]. In our previous work we focused on the path planning problem, i.e., the task of reaching a location within an unknown three-dimensional environment [7].
Reference: [5] <author> Y. Aloimonos, </author> <title> Purposive and qualitative active vision, </title> <booktitle> in Proc. Int. Conf. on Pattern Recognition, </booktitle> <pages> pp. 346-360, </pages> <year> 1990. </year>
Reference-contexts: Our work has been inspired by recent approaches which, instead of assuming that complete information about the environment is available [3], use only the sensor information that is necessary for planning the motion of the robot [4]. These approaches follow a purposive <ref> [5] </ref>, act-while-thinking strategy and consider robotic motion planning as a continuous process where sensing and action are tightly coupled [6-8]. In our previous work we focused on the path planning problem, i.e., the task of reaching a location within an unknown three-dimensional environment [7].
Reference: [6] <author> V. J. Lumelsky, S. Mukhopadhyay, and K. Sun, </author> <title> Dynamic path planning in sensor-based terrain acquisition, </title> <journal> IEEE Trans. Robotics Automat., </journal> <volume> vol. 6, no. 4, </volume> <pages> pp. 462-472, </pages> <year> 1990. </year>
Reference: [7] <author> K. N. Kutulakos, V. J. Lumelsky, and C. R. Dyer, </author> <title> Vision-guided exploration: A step toward general motion planning in three dimensions, </title> <booktitle> in Proc. IEEE Robotics Automat. Conf., </booktitle> <pages> pp. 289-296, </pages> <year> 1993. </year>
Reference-contexts: These approaches follow a purposive [5], act-while-thinking strategy and consider robotic motion planning as a continuous process where sensing and action are tightly coupled [6-8]. In our previous work we focused on the path planning problem, i.e., the task of reaching a location within an unknown three-dimensional environment <ref> [7] </ref>. That work provides the basis for the work we present here. More specifically, in [7] we obtained three main results: * In order to solve the path planning problem a robot must, in general, be capable of exploring the surface of an arbitrary object. <p> In our previous work we focused on the path planning problem, i.e., the task of reaching a location within an unknown three-dimensional environment <ref> [7] </ref>. That work provides the basis for the work we present here. More specifically, in [7] we obtained three main results: * In order to solve the path planning problem a robot must, in general, be capable of exploring the surface of an arbitrary object.
Reference: [8] <author> A. Blake, A. Zisserman, and R. Cipolla, </author> <title> Visual exploration of free-space, in Active Vision (A. </title> <editor> Blake and A. Yuille, </editor> <booktitle> eds.), </booktitle> <pages> pp. 175-188, </pages> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: A number of recent approaches in computer vision have been suggested for exploring the surfaces of objects <ref> [8, 10-12] </ref>, but their correctness in arbitrary environments has not been investigated. 2 Overview of our Approach As a robot moves in its environment, the set of visible points changes. In our approach we capture and control these visibility transitions by analyzing the occlusion boundary of objects.
Reference: [9] <author> N. S. Rao, S. S. Iyengar, B. J. Oommen, and R. Kashyap, </author> <title> Terrain acquisition by point robot amidst polyhedral obstacles, </title> <booktitle> in Proc. Third Conf. on Artificial Intelligence Applications, </booktitle> <pages> pp. 170-175, </pages> <year> 1987. </year>
Reference-contexts: In the case of polyhedra, the strategies developed (e.g., <ref> [9] </ref>) rely on the fact that the objects consist of a finite collection of planar faces, and produce paths whose lengths diverge in the limit.
Reference: [10] <author> C. I. Connoly, </author> <title> The determination of next best views, </title> <booktitle> in Proc. IEEE Robotics Automat. Conf., </booktitle> <pages> pp. 432-435, </pages> <year> 1985. </year>
Reference: [11] <author> J. Maver and R. </author> <title> Bajcsy, Occlusions as a guide for planning the next view, </title> <journal> IEEE Trans. Pattern Anal. Machine Intell., </journal> <volume> vol. 15, no. 5, </volume> <pages> pp. 417-433, </pages> <year> 1993. </year>
Reference: [12] <author> P. Whaite and F. P. Ferrie, </author> <title> From uncertainty to visual exploration, </title> <journal> IEEE Trans. Pattern Anal. Machine Intell., </journal> <volume> vol. 13, no. 10, </volume> <pages> pp. 1038-1049, </pages> <year> 1991. </year>
Reference: [13] <author> J. J. Koenderink and A. J. van Doorn, </author> <title> The singularities of the visual mapping, </title> <journal> Biological Cybernetics, </journal> <volume> vol. 24, </volume> <pages> pp. 51-59, </pages> <year> 1976. </year>
Reference: [14] <author> J. Ponce, S. Petitjean, and D. Kriegman, </author> <title> Computing exact aspect graphs of curved objects: Algebraic surfaces, </title> <booktitle> in Proc. Second European Conference on Computer Vision, </booktitle> <year> 1992. </year>
Reference: [15] <author> J. H. Rieger, </author> <title> The geometry of view space of opaque objects bounded by smooth surfaces, </title> <journal> Artificial Intelligence, </journal> <volume> vol. 44, </volume> <pages> pp. 1-40, </pages> <year> 1990. </year>
Reference: [16] <author> R. Cipolla and A. Blake, </author> <title> Surface shape from the deformation of apparent contours, </title> <journal> Int. J. Computer Vision, </journal> <volume> vol. 9, no. 2, </volume> <pages> pp. 83-112, </pages> <year> 1992. </year>
Reference-contexts: In particular, given an instantaneous direction of motion, fl 0 (t), we can express the motion of the occlusion boundary using the epipolar parameterization. The details of this parameterization are not important here and the reader is referred to <ref> [16] </ref>. The important point is that given a smooth segment fi (t) of image. For simplicity, the image of the occlusion boundary on a plane tangent to the spherical image is shown. <p> With such a sensor the robot can detect the occluding contour. Furthermore, when the robot moves along a path fl and knows its speed and acceleration, it can determine the coordinates of all points on the visible rim from the deformation of the occluding contour <ref> [16] </ref>.
Reference: [17] <author> K. N. Kutulakos and C. R. Dyer, </author> <title> Global surface reconstruction by purposive control of observer motion, </title> <type> Tech. Rep. 1141, </type> <institution> Computer Sciences Department, University of Wisconsin Madison, </institution> <month> April </month> <year> 1993. </year> <note> Available via ftp from ftp.cs.wisc.edu. </note>
Reference-contexts: When the occlusion boundary at time T slides over a surface point that was occluded from positions fl (t); 0 t &lt; T , that point becomes visible. The following theorem (see <ref> [17] </ref> for a proof) gives a qualitative characterization of these visibility transitions (Figure 4): Theorem 3.1 Let p 2 O fl (t) and let q p be a point of tangency of the segment pfl (t) with S. <p> These motions cause the visible rim to slide over a neighborhood of that point, according to Theorem 3.1. In general, this motion corresponds to motion on the plane defined by the selected point p n , the surface normal n (p n ), and the robot's position <ref> [17] </ref>. Strategy A c uses this observation to explore a neighborhood W n of p n while at the same time guaranteeing that W n is large enough to ensure completeness. To achieve this, consider why W n can be arbitrarily small. <p> This essentially reduces the determination of n to the determination of the bitangents and asymptotes at p n . The following provides an intuitive explanation for the selection process through an example; for formal proofs and more details see <ref> [17, 18] </ref>. Suppose p n is on the visible rim when the robot is positioned at c. The angle between segment cp n and the asymptotes and bitangents affects the distance between p n and the endpoints of the visible rim curve containing it. <p> This can be provably achieved by circumnavigating p n on T p n (S), until the robot's initial position, fl (t 0 ) is reached, while maintaining the visibility of p n <ref> [17, 18] </ref>. <p> This is because the convergence properties of A c change only when the robot attempts to explore the surface near the boundary of such a region <ref> [17] </ref>: When the robot uses A c to explore the surface near a parabolic curve bounding a surface concavity, the set of points explored near the parabolic curve diminishes as the parabolic curve is approached.
Reference: [18] <author> K. N. Kutulakos, V. J. Lumelsky, and C. R. Dyer, </author> <title> Provable strategies for vision-guided exploration in three dimensions, </title> <type> tech. rep., </type> <institution> Computer Sciences Department, University of Wisconsin Madison, </institution> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: The following lemma shows that after p n is reached, the robot can guarantee that a neighborhood of p n is explored by performing an arbitrarily small position adjustment along n (p n ). See <ref> [18] </ref> for the proof. Lemma 5.1 Let E (d) be the set of visible points when the robot is at distance d from p n along n (p n ). <p> Unfortunately, an arbitrarily small motion away from p n can leave E (d) arbitrarily small. The robot must therefore move sufficiently far from p n so that E (d) contains a neighborhood of p n that is large enough to guarantee completeness. In <ref> [18] </ref> we show that the robot can guarantee completeness by simply moving away from p n along n (p n ) until either a collision with the surface does not allow further motion, or an a priori defined distance &gt; 0 is reached. <p> An a priori defined distance from p n is reached. c. d &lt; d + , where d + ; d are the distances of B + and B to p n , respectively. The following theorem establishes the correctness of the strategy. The interested reader is referred to <ref> [18] </ref> for the proof. It is based on the fact that the surface is smooth and does not have infinite curvature at p n . <p> Because the robot can collide with the surface, this motion also involves moving closer to p n along a curve in the intersection S " , if such a collision occurs. Due to lack of space a description of this strategy is omitted. The interested reader is referred to <ref> [18] </ref> for the details. <p> This essentially reduces the determination of n to the determination of the bitangents and asymptotes at p n . The following provides an intuitive explanation for the selection process through an example; for formal proofs and more details see <ref> [17, 18] </ref>. Suppose p n is on the visible rim when the robot is positioned at c. The angle between segment cp n and the asymptotes and bitangents affects the distance between p n and the endpoints of the visible rim curve containing it. <p> This can be provably achieved by circumnavigating p n on T p n (S), until the robot's initial position, fl (t 0 ) is reached, while maintaining the visibility of p n <ref> [17, 18] </ref>.
Reference: [19] <author> Y. L. Kergosien, </author> <title> La famille des projections orthogonales d'une surface et ses singularit es, </title> <editor> C. R. </editor> <booktitle> Acad. Sc. Paris, </booktitle> <volume> vol. 292, </volume> <pages> pp. 929-932, </pages> <year> 1981. </year>
Reference-contexts: a topological change in the visible rim when c is infinitesimally perturbed on . * The size of W n is constrained by the amount of robot motion on . 4 A precise definition of generic surfaces is beyond the scope of this paper, and the reader is referred to <ref> [19] </ref>. Intuitively, generic surfaces are surfaces whose geometrical characteristics do not change if they are infinitesimally perturbed. All smooth surfaces (except for a zero-measure set) are generic.

Reference: [1] <author> J. J. Koenderink, </author> <title> Solid Shape. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: These strategies build on computer vision research that has studied the appearance of smooth and piecewise-smooth surfaces <ref> [1, 2] </ref>. Our results provide a basis for integrating motion planning and visual sensing under a common computational framework. <p> A convenient way to visualize and study the occlusion boundary and the visible rim is to look at their spherical projections onto an image centered at the robot's position (Figure 3). Their projections are identical; this projected contour is known as the occluding contour <ref> [1] </ref>. In general, the occlusion boundary is collection of closed, piecewise-smooth curves, ff 1 ; : : : ; ff n , whose number and shape depends on the robot's position.
Reference: [2] <author> J. H. Rieger, </author> <title> On the classification of views of piecewise smooth objects, </title> <journal> Image and Vision Computing, </journal> <volume> vol. 5, no. 2, </volume> <pages> pp. 91-97, </pages> <year> 1987. </year>
Reference-contexts: These strategies build on computer vision research that has studied the appearance of smooth and piecewise-smooth surfaces <ref> [1, 2] </ref>. Our results provide a basis for integrating motion planning and visual sensing under a common computational framework.
Reference: [3] <author> J. H. Reif, </author> <title> Complexity of the mover's problem and generalizations, </title> <booktitle> in Proc. Twentieth Symposium on Foundations of Computer Science, </booktitle> <pages> pp. 421-427, </pages> <year> 1979. </year>
Reference-contexts: Our results provide a basis for integrating motion planning and visual sensing under a common computational framework. Our work has been inspired by recent approaches which, instead of assuming that complete information about the environment is available <ref> [3] </ref>, use only the sensor information that is necessary for planning the motion of the robot [4]. These approaches follow a purposive [5], act-while-thinking strategy and consider robotic motion planning as a continuous process where sensing and action are tightly coupled [6-8].
Reference: [4] <author> R. A. Brooks, </author> <title> A robust layered control system for a mobile robot, </title> <journal> IEEE J. Robotics Automat., </journal> <volume> vol. 2, no. 1, </volume> <pages> pp. 14-23, </pages> <year> 1986. </year>
Reference-contexts: Our work has been inspired by recent approaches which, instead of assuming that complete information about the environment is available [3], use only the sensor information that is necessary for planning the motion of the robot <ref> [4] </ref>. These approaches follow a purposive [5], act-while-thinking strategy and consider robotic motion planning as a continuous process where sensing and action are tightly coupled [6-8]. In our previous work we focused on the path planning problem, i.e., the task of reaching a location within an unknown three-dimensional environment [7].
Reference: [5] <author> Y. Aloimonos, </author> <title> Purposive and qualitative active vision, </title> <booktitle> in Proc. Int. Conf. on Pattern Recognition, </booktitle> <pages> pp. 346-360, </pages> <year> 1990. </year>
Reference-contexts: Our work has been inspired by recent approaches which, instead of assuming that complete information about the environment is available [3], use only the sensor information that is necessary for planning the motion of the robot [4]. These approaches follow a purposive <ref> [5] </ref>, act-while-thinking strategy and consider robotic motion planning as a continuous process where sensing and action are tightly coupled [6-8]. In our previous work we focused on the path planning problem, i.e., the task of reaching a location within an unknown three-dimensional environment [7].
Reference: [6] <author> V. J. Lumelsky, S. Mukhopadhyay, and K. Sun, </author> <title> Dynamic path planning in sensor-based terrain acquisition, </title> <journal> IEEE Trans. Robotics Automat., </journal> <volume> vol. 6, no. 4, </volume> <pages> pp. 462-472, </pages> <year> 1990. </year>
Reference: [7] <author> K. N. Kutulakos, V. J. Lumelsky, and C. R. Dyer, </author> <title> Vision guided exploration: A step toward general motion planning in three dimensions, </title> <booktitle> in Proc. IEEE Robotics Automat. Conf., </booktitle> <pages> pp. 289-296, </pages> <year> 1993. </year>
Reference-contexts: These approaches follow a purposive [5], act-while-thinking strategy and consider robotic motion planning as a continuous process where sensing and action are tightly coupled [6-8]. In our previous work we focused on the path planning problem, i.e., the task of reaching a location within an unknown three-dimensional environment <ref> [7] </ref>. That work provides the basis for the work we present here. More specifically, in [7] we obtained three main results: * In order to solve the path planning problem a robot must, in general, be capable of exploring the surface of an arbitrary object. <p> In our previous work we focused on the path planning problem, i.e., the task of reaching a location within an unknown three-dimensional environment <ref> [7] </ref>. That work provides the basis for the work we present here. More specifically, in [7] we obtained three main results: * In order to solve the path planning problem a robot must, in general, be capable of exploring the surface of an arbitrary object.
Reference: [8] <author> A. Blake, A. Zisserman, and R. Cipolla, </author> <title> Visual exploration of free-space, in Active Vision (A. </title> <editor> Blake and A. Yuille, </editor> <booktitle> eds.), </booktitle> <pages> pp. 175-188, </pages> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: A number of recent approaches in computer vision have been suggested for exploring the surfaces of objects <ref> [8, 10-12] </ref>, but their correctness in arbitrary environments has not been investigated. 2 Overview of our Approach As a robot moves in its environment, the set of visible points changes. In our approach we capture and control these visibility transitions by analyzing the occlusion boundary of objects.
Reference: [9] <author> N. S. Rao, S. S. Iyengar, B. J. Oommen, and R. Kashyap, </author> <title> Terrain acquisition by point robot amidst polyhedral obstacles, </title> <booktitle> in Proc. Third Conf. on Artificial Intelligence Applications, </booktitle> <pages> pp. 170-175, </pages> <year> 1987. </year>
Reference-contexts: In the case of polyhedra, the strategies developed (e.g., <ref> [9] </ref>) rely on the fact that the objects consist of a finite collection of planar faces, and produce paths whose lengths diverge in the limit.
Reference: [10] <author> C. I. Connoly, </author> <title> The determination of next best views, </title> <booktitle> in Proc. IEEE Robotics Automat. Conf., </booktitle> <pages> pp. 432-435, </pages> <year> 1985. </year>
Reference: [11] <author> J. Maver and R. </author> <title> Bajcsy, Occlusions as a guide for planning the next view, </title> <journal> IEEE Trans. Pattern Anal. Machine Intell., </journal> <volume> vol. 15, no. 5, </volume> <pages> pp. 417-433, </pages> <year> 1993. </year>
Reference: [12] <author> P. Whaite and F. P. Ferrie, </author> <title> From uncertainty to visual exploration, </title> <journal> IEEE Trans. Pattern Anal. Machine Intell., </journal> <volume> vol. 13, no. 10, </volume> <pages> pp. 1038-1049, </pages> <year> 1991. </year>
Reference: [13] <author> J. J. Koenderink and A. J. van Doorn, </author> <title> The singularities of the visual mapping, </title> <journal> Biological Cybernetics, </journal> <volume> vol. 24, </volume> <pages> pp. 51-59, </pages> <year> 1976. </year>
Reference: [14] <author> J. Ponce, S. Petitjean, and D. Kriegman, </author> <title> Computing exact aspect graphs of curved objects: Algebraic surfaces, </title> <booktitle> in Proc. Second European Conference on Computer Vision, </booktitle> <year> 1992. </year>
Reference: [15] <author> J. H. Rieger, </author> <title> The geometry of view space of opaque objects bounded by smooth surfaces, </title> <journal> Artificial Intelligence, </journal> <volume> vol. 44, </volume> <pages> pp. 1-40, </pages> <year> 1990. </year>
Reference: [16] <author> R. Cipolla and A. Blake, </author> <title> Surface shape from the deformation of apparent contours, </title> <journal> Int. J. Computer Vision, </journal> <volume> vol. 9, no. 2, </volume> <pages> pp. 83-112, </pages> <year> 1992. </year>
Reference-contexts: In particular, given an instantaneous direction of motion, fl 0 (t), we can express the motion of the occlusion boundary using the epipolar parameterization. The details of this parameterization are not important here and the reader is referred to <ref> [16] </ref>. The important point is that given a smooth segment fi (t) of image. For simplicity, the image of the occlusion boundary on a plane tangent to the spherical image is shown. <p> With such a sensor the robot can detect the occluding contour. Furthermore, when the robot moves along a path fl and knows its speed and acceleration, it can determine the coordinates of all points on the visible rim from the deformation of the occluding contour <ref> [16] </ref>.
Reference: [17] <author> K. N. Kutulakos and C. R. Dyer, </author> <title> Global surface reconstruction by purposive control of observer motion, </title> <type> Tech. Rep. 1141, </type> <institution> Computer Sciences Department, University of Wisconsin Madison, </institution> <month> April </month> <year> 1993. </year> <note> Available via ftp from ftp.cs.wisc.edu. </note>
Reference-contexts: When the occlusion boundary at time T slides over a surface point that was occluded from positions fl (t); 0 t &lt; T , that point becomes visible. The following theorem (see <ref> [17] </ref> for a proof) gives a qualitative characterization of these visibility transitions (Figure 4): Theorem 3.1 Let p 2 O fl (t) and let q p be a point of tangency of the segment pfl (t) with S. <p> These motions cause the visible rim to slide over a neighborhood of that point, according to Theorem 3.1. In general, this motion corresponds to motion on the plane defined by the selected point p n , the surface normal n (p n ), and the robot's position <ref> [17] </ref>. Strategy A c uses this observation to explore a neighborhood W n of p n while at the same time guaranteeing that W n is large enough to ensure completeness. To achieve this, consider why W n can be arbitrarily small. <p> This essentially reduces the determination of n to the determination of the bitangents and asymptotes at p n . The following provides an intuitive explanation for the selection process through an example; for formal proofs and more details see <ref> [17, 18] </ref>. Suppose p n is on the visible rim when the robot is positioned at c. The angle between segment cp n and the asymptotes and bitangents affects the distance between p n and the endpoints of the visible rim curve containing it. <p> This can be provably achieved by circumnavigating p n on T p n (S), until the robot's initial position, fl (t 0 ) is reached, while maintaining the visibility of p n <ref> [17, 18] </ref>. <p> This is because the convergence properties of A c change only when the robot attempts to explore the surface near the boundary of such a region <ref> [17] </ref>: When the robot uses A c to explore the surface near a parabolic curve bounding a surface concavity, the set of points explored near the parabolic curve diminishes as the parabolic curve is approached.
Reference: [18] <author> K. N. Kutulakos, V. J. Lumelsky, and C. R. Dyer, </author> <title> Prov able strategies for vision-guided exploration in three dimensions, </title> <type> tech. rep., </type> <institution> Computer Sciences Department, University of Wisconsin Madison, </institution> <year> 1993. </year> <note> To appear. </note>
Reference-contexts: The following lemma shows that after p n is reached, the robot can guarantee that a neighborhood of p n is explored by performing an arbitrarily small position adjustment along n (p n ). See <ref> [18] </ref> for the proof. Lemma 5.1 Let E (d) be the set of visible points when the robot is at distance d from p n along n (p n ). <p> Unfortunately, an arbitrarily small motion away from p n can leave E (d) arbitrarily small. The robot must therefore move sufficiently far from p n so that E (d) contains a neighborhood of p n that is large enough to guarantee completeness. In <ref> [18] </ref> we show that the robot can guarantee completeness by simply moving away from p n along n (p n ) until either a collision with the surface does not allow further motion, or an a priori defined distance &gt; 0 is reached. <p> An a priori defined distance from p n is reached. c. d &lt; d + , where d + ; d are the distances of B + and B to p n , respectively. The following theorem establishes the correctness of the strategy. The interested reader is referred to <ref> [18] </ref> for the proof. It is based on the fact that the surface is smooth and does not have infinite curvature at p n . <p> Because the robot can collide with the surface, this motion also involves moving closer to p n along a curve in the intersection S " , if such a collision occurs. Due to lack of space a description of this strategy is omitted. The interested reader is referred to <ref> [18] </ref> for the details. <p> This essentially reduces the determination of n to the determination of the bitangents and asymptotes at p n . The following provides an intuitive explanation for the selection process through an example; for formal proofs and more details see <ref> [17, 18] </ref>. Suppose p n is on the visible rim when the robot is positioned at c. The angle between segment cp n and the asymptotes and bitangents affects the distance between p n and the endpoints of the visible rim curve containing it. <p> This can be provably achieved by circumnavigating p n on T p n (S), until the robot's initial position, fl (t 0 ) is reached, while maintaining the visibility of p n <ref> [17, 18] </ref>.
Reference: [19] <author> Y. L. Kergosien, </author> <title> La famille des projections orthogonales d'une surface et ses singularit es, </title> <editor> C. R. </editor> <booktitle> Acad. Sc. Paris, </booktitle> <volume> vol. 292, </volume> <pages> pp. 929-932, </pages> <year> 1981. </year>
Reference-contexts: a topological change in the visible rim when c is infinitesimally perturbed on . * The size of W n is constrained by the amount of robot motion on . 4 A precise definition of generic surfaces is beyond the scope of this paper, and the reader is referred to <ref> [19] </ref>. Intuitively, generic surfaces are surfaces whose geometrical characteristics do not change if they are infinitesimally perturbed. All smooth surfaces (except for a zero-measure set) are generic.
Reference: [20] <author> M. P. D. Carmo, </author> <title> Differential Geometry of Curves and Surfaces. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall Inc., </publisher> <year> 1976. </year>
Reference-contexts: The main idea in this process is that every position, ^c i n 2 n , can be characterized by two geometric relationships: (1) The angle formed by segment ^c i n p n and the asymptotes <ref> [20] </ref> and bitangents 5 at p n , and (2) the distance between ^c i n and p n . In the following we show how these two relationships can be used to determine n .
References-found: 39

