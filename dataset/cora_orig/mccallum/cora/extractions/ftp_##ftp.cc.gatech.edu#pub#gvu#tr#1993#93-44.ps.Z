URL: ftp://ftp.cc.gatech.edu/pub/gvu/tr/1993/93-44.ps.Z
Refering-URL: http://www.cs.gatech.edu/gvu/reports/1993/
Root-URL: 
Email: email: guzdial@cc.gatech.edu  email: soloway@eecs.umich.edu  
Phone: (404) 853-9387,  
Title: Page 1 Characterizing process change using log file data  
Author: Mark Guzdial Chris Walton Michael Konemann Elliot Soloway 
Keyword: Evaluation methodology, educational software, learning, usability evaluation, log file data, event trace records, automated analyses  
Address: Atlanta, GA 30332-0280,  1405 North Fifth Avenue, St. Charles, IL 60174  Waukesha, WI 53186  1101 Beal Avenue, Ann Arbor, MI 48109-2110,  
Affiliation: GVU Center, College of Computing, Georgia Institute of Technology  Advanced Technology Group, Professional Education Division, Arther Andersen Co.  Computer Science Department, Carroll College  HiCE Research Group, Artificial Intelligence Laboratory, The University of Michigan  
Abstract: Students learning a new task with an unfamiliar interface must learn the task, the interface, and a task-to-device mapping which enables them to develop an efficient process for achieving goals with that software. To characterize student process and learning, we have used two methods which rely on log file data rather than the more typical interview-based data. The first method creates a graphic snapshot of process, and the second method creates a transition diagram of process. Both techniques are presented with examples of their use. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Aho, A.V., Brian W. Kernighan, and Peter J. Weinberger. </author> <title> The AWK Programming Language. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley, </publisher> <year> 1988. </year>
Reference-contexts: Debugging Final Review p=0.17 p=0.64 p=0.24 p=0.65 p=0.01 p=0.29 p=0.17 p=0.02 p=0.33 p=0.12 p=0.12 We analyzed Emile's log files using Hawk (Hypercard AWK), a text analysis tool based on an Awk-like programming language and implemented in a HyperCard environment (for more on Hawk, see [7]; for more on Awk, see <ref> [1] </ref>). We used Hawk to recode the Emile log files in terms of these five states, then to compute a transition diagram for each student's process on each program.
Reference: 2. <author> Badre, Albert N., Scott E. Hudson, and Paulo J. Santos. </author> <title> An environment to support user interface evaluation using synchronized video and event trace rec ording. </title> <institution> Georgia Institute of Technology, GVU Center, </institution> <year> 1993. </year> <note> Technical Report GIT-GVU-93-16. </note>
Reference-contexts: A method of characterizing process that would be more amenable to the realities of educational software would be to log user actions in the interface and analyze these data, which we call log file data (sometimes also called event trace records <ref> [2] </ref>). Log file data do not provide insight into task and interface representations, but they can provide valuable clues in terms of the observable performance. Specifically, we can address two kinds of questions about individual users and about classes of users: 1.
Reference: 3. <author> Card, Stuart K., Thomas P. Moran, and Allen Newell. </author> <title> The Pyschology of Human-Computer Interaction. </title> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum and Associates, </publisher> <year> 1983. </year>
Reference-contexts: Typically, an evaluation of user process would involve think-aloud protocols or interviews some method Page 2 which involves querying the user to determine task understanding, interface understanding, and the task-to-device mapping <ref> [3] </ref>. A comparison over time should show a change in the user's process in terms of mapping and order of actions. <p> A relatively simple strategy for analyzing log file data is to simply count key variables in the log data. Several of the speakers at the INTERCHI'93 panel on software tools for usability mentioned using spreadsheets for implementing this strategy [14]. Card, Moran, and Newell also use this strategy <ref> [3] </ref> to test their predictions of usability. A detailed example is provided in Shute and Glaser [12] where they gathered 30 variables on various uses of their educational simulation software (e.g., the number of experiments run, the number of notebook entries made) to measure the usefulness of their software.
Reference: 4. <author> Dershimer, Charles and Carl Berger. </author> <title> Characterizing student interactions with a hypermedia learning environment. </title> <booktitle> Paper presented at the American Educational Research Association annual meeting, </booktitle> <address> San Francisco, CA., </address> <year> 1992. </year>
Reference: 5. <author> Dershimer, Charles, Carl Berger, and David Jackson. </author> <title> Designing hyper-media for concept development: Formative evaluation through analysis of log files. </title> <booktitle> Paper presented at the National Association for Research in Science Teaching annual meeting, Fontana, </booktitle> <address> WI., </address> <year> 1991. </year>
Reference: 6. <author> Guzdial, Mark, Elliot Soloway, Phyllis Blumenfeld, Luke Hohmann, Ken Ewing, Iris Tabak, Kathy Brade, and Yasmin Kafai. </author> <title> The future of CAD: Technological support for kids building artifacts. In Learning to design, designing to learn: Using technology to transform the curriculum, </title> <editor> eds. D. Balestri, S. Ehrmann, and D.L. Ferguson. </editor> <address> Norwood, NJ: </address> <publisher> Ablex Publishing Company, </publisher> <year> 1992. </year>
Reference: 7. <author> Guzdial, Mark J. </author> <title> Deriving software usage patterns from log files. </title> <note> Submitted to Behavior Research Methods, Instruments, and Computers (1993) </note>
Reference-contexts: p=0.25 p=0.01 Decomposition Initial Review Composition Debugging Final Review p=0.17 p=0.64 p=0.24 p=0.65 p=0.01 p=0.29 p=0.17 p=0.02 p=0.33 p=0.12 p=0.12 We analyzed Emile's log files using Hawk (Hypercard AWK), a text analysis tool based on an Awk-like programming language and implemented in a HyperCard environment (for more on Hawk, see <ref> [7] </ref>; for more on Awk, see [1]). We used Hawk to recode the Emile log files in terms of these five states, then to compute a transition diagram for each student's process on each program.
Reference: 8. <author> Guzdial, Mark J. Emile: </author> <title> Software-realized scaffolding for science learners programming in mixed media. </title> <type> Unpublished Ph.D. dissertation, </type> <institution> University of Michigan, </institution> <year> 1993. </year>
Reference-contexts: EMILE AND PROCESS TRANSITION DIAGRAMS Emile is a programming environment used by high sc hool stud ents to cr ea te simu lations and demonstrations of physics concepts using graphical elements (e.g., buttons and text fields) as well as text program elements <ref> [8] </ref>. Like GPCeditor, Emile provides goals and program components which can be arranged in a hierarchy using decomposition actions. These components can be assembled into a complete program using composition actions, then tested using debugging actions.
Reference: 9. <author> Hammer, J.M. and W.B. </author> <title> Rouse. Analysis and modeling of freeform text editing behavior. </title> <booktitle> In Proceedings of the 1979 International Conference on Cybernetics and Society, </booktitle> <pages> 659-664. </pages> <address> Denver: </address> <year> 1979. </year>
Reference-contexts: A more complex strategy is to use the log file data to characterize user process. The key example is Hammer and Rouse's <ref> [9] </ref> use of Markov analysis (a common analysis technique found in most finite mathematics texts, e.g. [10]) to characterize keystroke data in use of an editor. They expected to find differences in process for different classes of users working on different tasks. <p> Hammer and Rouse used yet another method for quantifying transition diagram methods which involved computing a statistic indicating whether different chains of actions might indeed be from the same process <ref> [9] </ref>. We chose the count of transition diagram arcs as a simpler method that provided insight into our data. Table 1 summarizes the number of transition diagram arcs for five students at the same level of experience (the fourth program in a study of Emile).
Reference: 10. <author> Kemeny, J.G., J.L. Snell, and G. L. Thompson. </author> <title> Introduction to Finite Mathematics. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall, </publisher> <year> 1974. </year>
Reference-contexts: A more complex strategy is to use the log file data to characterize user process. The key example is Hammer and Rouse's [9] use of Markov analysis (a common analysis technique found in most finite mathematics texts, e.g. <ref> [10] </ref>) to characterize keystroke data in use of an editor. They expected to find differences in process for different classes of users working on different tasks. Unfortunately, they instead found that the individual differences swamped all group differences.
Reference: 11. <author> Kieras, David and Peter G. Polson. </author> <title> An approach to the formal analysis of user complexity. </title> <journal> International Journal of Man-Machine Studies 22 (1985): </journal> <pages> 365-394. </pages>
Reference-contexts: We can characterize the student's problem in terms of a general user's development of process. A user's process with software (i.e., the actions taken and the order in which they are taken) is dependent on three factors (based on Kieras and Polson <ref> [11] </ref>): The user's task representation, The user's device (interface) representation, and The correspondence between the user's goals and the methods available in the software, called the task-to-device mapping. When users begin to use software with an unfamiliar interface, they must construct a device representation and a task-to-device mapping.
Reference: 12. <author> Shute, Valerie J. and Robert Glaser. </author> <title> A large-scale evaluation of an intelligent discovery world: Smithtown. Interactive Learning Environments 1 (1, </title> <booktitle> 1990): </booktitle> <pages> 51-77. </pages>
Reference-contexts: Several of the speakers at the INTERCHI'93 panel on software tools for usability mentioned using spreadsheets for implementing this strategy [14]. Card, Moran, and Newell also use this strategy [3] to test their predictions of usability. A detailed example is provided in Shute and Glaser <ref> [12] </ref> where they gathered 30 variables on various uses of their educational simulation software (e.g., the number of experiments run, the number of notebook entries made) to measure the usefulness of their software.
Reference: 13. <author> Soloway, Eliot, Mark Guzdial, Kathy Brade, Luke Hohmann, Iris Tabak, Peri Weingrad, and Phyllis Blumenfeld. </author> <title> Technological support for the learning and doing of design. In Foundations and frontiers of adaptive learning environments, </title> <editor> eds. Marlene Jones and P.H. Winne. </editor> <address> New York: </address> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: The GPCeditor automatically collects log file data as the user works through the program. In our analysis of the GPCeditor log file data, we defined three high-level states and three low-level states. The high-level states are defined in our model of process for the GPCeditor (see <ref> [13] </ref> for more on this model), and the low-level states were seen as key stepping stones in this process. Decomposition is a high-level state corresponding to creation or deletion actions of either goals or plans. Decomposition actions are all menu-based actions.
Reference: 14. <author> Weiler, Paul, Richard Cordes, Monty Hammontree, Derek Hoiem, and Michael Thompson. </author> <title> Software for the usability lab: A sampling of current tools (Panel session). </title> <booktitle> In INTERCHI'93 Conference Proceedings: Conference on Human Factors in Computing Systems, </booktitle> <editor> eds. Stacey Ashlund, Kevin Mullet, Austin Henderson, Erik Hollnagel, and Ted White. </editor> <address> New York: </address> <publisher> ACM, </publisher> <year> 1993. </year>
Reference-contexts: METHODS OF LOG FILE ANALYSES Collecting log file data to address usability concerns is a data collection method that is growing in popularity (e.g., as noted in the INTERCHI'93 panel on the subject <ref> [14] </ref>). There are two main approaches to automated analyses of log file data: counting of key variables and characterization of process. A relatively simple strategy for analyzing log file data is to simply count key variables in the log data. <p> A relatively simple strategy for analyzing log file data is to simply count key variables in the log data. Several of the speakers at the INTERCHI'93 panel on software tools for usability mentioned using spreadsheets for implementing this strategy <ref> [14] </ref>. Card, Moran, and Newell also use this strategy [3] to test their predictions of usability.
References-found: 14

