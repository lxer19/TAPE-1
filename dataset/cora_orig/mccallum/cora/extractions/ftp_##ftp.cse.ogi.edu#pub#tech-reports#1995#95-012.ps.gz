URL: ftp://ftp.cse.ogi.edu/pub/tech-reports/1995/95-012.ps.gz
Refering-URL: http://www.cse.ogi.edu/~brianh/me.html
Root-URL: http://www.cse.ogi.edu
Email: Email: sutton@cse.ogi.edu  
Title: Evaluating the Effectiveness of Dialogue for an Automated Spoken Questionnaire  for Spoken Language Understanding  
Author: Stephen Sutton, Brian Hansen, Terri Lander, David G. Novick, Ronald Cole 
Address: 20000 NW Walker Rd. PO Box 91000 Portland, OR 97291-1000  
Affiliation: Center  Oregon Graduate Institute of Science Technology  
Abstract: We present and apply an empirical methodology for evaluating the effectiveness of dialogues in spoken language systems. This methodology is suitable in particular for evaluation of dialogue-based systems that collect information from the user, such as an automated spoken questionnaire. Our method for assessing effectiveness involves coding answers from users for responsiveness. For this effort, we developed a behavioral coding scheme tailored to the requirements of automated spoken questionnaires interacting via the telephone. The codes cover a range of behavior from Concise to No response. We have used this evaluation methodology in the development of an automated spoken questionnaire. In connection with this project, we collected over 4,000 telephone calls responding to the questionnaire. A sample of the calls was transcribed and coded using our behavioral coding scheme. We then used the data from the codes to choose among alternative protocols for the dialogue and to evaluate differences in system voice, such as natural versus synthetic and male versus female. In particular, we illustrate the utility of our methodology by testing the hypothesis that a synthesized system voice would elicit more constrained user responses than a human voice and report the evaluation results. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Cole, R.A., Novick, D.G., Fanty, M., Vermeulen, P., Sutton, S., Burnett, D., Schalkwyk, J. </author> <year> 1994. </year> <title> A prototype voice-response questionnaire for the U.S. Census. </title> <booktitle> Proceedings of International Conference on Spoken Language Processing, </booktitle> <pages> 683-686. </pages> <address> Yokohama, Japan. </address>
Reference-contexts: Introduction In this paper, we present an evaluation methodology that was used in the development of a prototype automated spoken questionnaire (ASQ) for the Year 2000 Census <ref> (Cole et al., 1994) </ref>. Our motivation arises from the need for an evaluation metric and a systematic procedure with which to measure the effectiveness of a dialogue.
Reference: <author> Esposito, J., and Rothgeb, J. </author> <year> 1992. </year> <title> Behavior coding manual: CATI/CAPI overlap. </title> <type> Draft report. </type> <institution> Bureau of Labor Statistics/Bureau of the Census. </institution>
Reference-contexts: The behavioral coding scheme provides a set of features that characterize aspects of the response deemed useful for ASQ evaluation purposes. Particular emphasis was placed on capturing the responsiveness of utterances. The behavioral coding scheme is an adaptation of a coding scheme used by the Bureau of the Census <ref> (Esposito & Rothgeb, 1992) </ref> for the purposes of pretesting a questionnaire. It was intended to identify particular interviewer and respondent behavior indicative of problems with a question or sections of the questionnaire. We made a number of key modifications to improve its suitability for use with an ASQ.
Reference: <author> Lander, T. </author> <year> 1994. </year> <title> Behavior Code Guide. Internal report. Center for Spoken Language Understanding, </title> <institution> Oregon Graduate Institute. </institution>
Reference: <author> Lloyd, P. </author> <year> 1992. </year> <title> The role of clarification requests in childrens communication of route directions by telephone. </title> <booktitle> Discourse Processes, </booktitle> <volume> 15: </volume> <pages> 357-374. </pages>
Reference-contexts: Request for clarification (RC) responses are where the user, instead of providing an answer, seeks to clarify the question. There are various kinds of clarification requests possible, including general request for repetition, specific request for repetition, specific request for confirmation, and specific request for specification <ref> (Lloyd, 1992) </ref>. Dont know (DK) responses are cases where the user indicates that the information is not available. Refusal (RF) responses are where the user explicitly declines to provide the requested information. Multiple codes Three codes are intended for use in certain restricted combinations with codes already described.
References-found: 4

