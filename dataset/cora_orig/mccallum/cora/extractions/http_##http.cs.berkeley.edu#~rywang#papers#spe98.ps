URL: http://http.cs.berkeley.edu/~rywang/papers/spe98.ps
Refering-URL: http://http.cs.berkeley.edu/~rywang/papers/spe98.html
Root-URL: 
Title: Experience with a Distributed File System Implementation  
Author: Randolph Y. Wang and Thomas E. Anderson Michael D. Dahlin 
Address: Austin  
Affiliation: University of California at Berkeley  University of Texas at  
Abstract: This paper highlights some of the lessons learned during the course of implementing xFS, a fully distributed file system. xFS is an interesting case study for two reasons. First, xFS's serverless architecture leads to more complex distributed programming issues than are faced by traditional client-server operating system services. Second, xFS implements a complex, multithreaded service that is tightly coupled with the underlying operating system. This combination turned out to be quite challenging. On one hand, the complexity of the system forced us to turn to distributed programming tools based on formal methods to verify the correctness of our distributed algorithms; on the other hand the complex interactions with the operating system on individual nodes violated some of the tools' assumptions, making it difficult to use them in this environment. Furthermore, the xFS system tested the limits of abstractions such as threads, RPC, and vnodes that have traditionally been used in building distributed file systems. Based on our experience, we suggest several strategies that should be followed by those wishing to build distributed operating systems services, and we also indicate several areas where programming tools and operating system abstractions might be improved.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Abrosimov, V., Armand, F., and Ortega, M. I. </author> <title> A Distributed Consistency Server for the CHORUS System. </title> <booktitle> In Proc. of the 3rd USENIX Symposium on Experiences with Distributed and Multiprocessor Systems (March 1992). </booktitle>
Reference-contexts: Despite many years of research in extensible file systems <ref> [39, 5, 40, 38, 42, 22, 1, 28, 52] </ref>, the construction of portable file systems has remained a difficult undertaking, especially for the commercial operating systems. We are currently investigating portable and efficient interposition agents as a means of distributing our code [17].
Reference: [2] <author> Anderson, T., Culler, D., Patterson, D., </author> <title> and the NOW team. A Case for NOW (Networks of Workstations). </title> <journal> IEEE Micro (Feb. </journal> <year> 1995), </year> <pages> 54-64. </pages>
Reference-contexts: 1 Introduction The recent emergence of high-performance local area networks [4, 7] and cluster technology <ref> [2, 50] </ref> has resulted in a renewed interest in distributed operating systems services. Relative to the client-server programs of the previous generation, the new peer-to-peer distributed systems enabled by low latency, high bandwidth communication are more complex due to their performance, scalability, and 1 availability requirements. <p> The original Active Message system was suitable only for parallel applications, not for distributed operating systems. The requirements of xFS and other similar softwares being written as part of the Berkeley NOW project <ref> [2] </ref> led to many improvements to the Active Message interface to provide better support for cluster computing.
Reference: [3] <author> Anderson, T., Dahlin, M., Neefe, J., Patterson, D., Roselli, D., and Wang, R. </author> <title> Serverless Network File Systems. </title> <journal> ACM Transactions on Computer Systems 14, </journal> <month> 1 (Feb. </month> <year> 1996), </year> <pages> 41-79. </pages>
Reference-contexts: This growing complexity has outpaced our understanding of how to engineer these systems. xFS, a network file system described in a previous paper <ref> [3] </ref>, is an example of such a serverless distributed system. It distributes its cache, secondary storage, and metadata management over closely coupled workstations. The decentralized nature of the system, while offering superior performance, scalability, and availability compared to traditional client-server file systems, also increases its complexity.
Reference: [4] <author> Anderson, T., Owicki, S., Saxe, J., and Thacker, C. </author> <title> High-Speed Switch Scheduling for Local-Area Networks. </title> <journal> ACM Transactions on Computer Systems 11, </journal> <volume> 4 (Nov. </volume> <year> 1993), </year> <pages> 319-52. 27 </pages>
Reference-contexts: 1 Introduction The recent emergence of high-performance local area networks <ref> [4, 7] </ref> and cluster technology [2, 50] has resulted in a renewed interest in distributed operating systems services.
Reference: [5] <author> Bershad, B. N., and Pinkerton, C. B. Watchdogs: </author> <title> Extending the UNIX File System. </title> <booktitle> In Proc. of the 1988 Winter USENIX (Februrary 1988). </booktitle>
Reference-contexts: Despite many years of research in extensible file systems <ref> [39, 5, 40, 38, 42, 22, 1, 28, 52] </ref>, the construction of portable file systems has remained a difficult undertaking, especially for the commercial operating systems. We are currently investigating portable and efficient interposition agents as a means of distributing our code [17].
Reference: [6] <author> Birrell, A. D., and Nelson, B. J. </author> <title> Implementing Remote Procedure Calls. </title> <journal> ACM Transactions on Computer Systems 2, </journal> <month> 1 (February </month> <year> 1984), </year> <pages> 39-59. </pages>
Reference-contexts: At the end of this section, we discuss the aspects of RPC and Active Messages that might be combined to provide good support for high performance peer-to-peer distributed systems. 5.1 RPC RPC communication <ref> [6] </ref> provides easy-to-understand semantics and has been the tool of choice for many distributed operating system builders.
Reference: [7] <author> Boden, N., Cohen, D., Felderman, R., Kulawik, A., Seitz, C., Seizovic, J., and Su, W. </author> <title> Myrinet A Gigabit-per-Second Local-Area Network. </title> <journal> IEEE MICRO (Feb. </journal> <year> 1995), </year> <pages> 29-36. </pages>
Reference-contexts: 1 Introduction The recent emergence of high-performance local area networks <ref> [4, 7] </ref> and cluster technology [2, 50] has resulted in a renewed interest in distributed operating systems services. <p> They receive striped writes from clients. They also react to requests from managers by supplying data to the clients which have initiated the I/O operations. Our prototype runs on Sun SPARC and UltraSPARC workstations connected by the Myrinet <ref> [7] </ref> network. In order to ease development and take advantage of pre-existing user level software modules, the bulk of the client code executes at user level. We use a loadable kernel module to implement the Solaris kernel vnode interface and redirect I/O requests to the user level daemon.
Reference: [8] <author> Brewer, E., and Kuszmaul, B. </author> <title> How to Get Good Performance from the CM5 Data Network. </title> <booktitle> In Proc. of the 1994 International Parallel Processing Symposium (April 1994). </booktitle>
Reference-contexts: For example, when gathering data from different senders, it is important to coordinate the communicating parties to prevent overrunning the bottleneck machine <ref> [8] </ref>. In xFS, we have seen the importance of this "rate-matching" technique when a client is assembling multiple data fragments from different storage servers. Another example is the choice of the different interfaces for different message sizes. The current Active Message interface provides three flavors of message interfaces.
Reference: [9] <author> Chandra, S., Richards, B., and Larus, J. R. Teapot: </author> <title> Language Support for Writing Memory Coherence Protocols. </title> <booktitle> In Proc. of SIGPLAN Conference on Programming Language Design and Implementation (May 1996). </booktitle>
Reference-contexts: The cache coherence protocol in xFS is similar to those seen in hardware DSM systems such as DASH [32] and Alewife [30]. But even minor modifications to these protocols can lead to subtle bugs <ref> [9] </ref>. Also, aspects of the cluster file system require protocol modifications that do not apply to DSM systems. For example, xFS must maintain reliable data storage in the face of node failures. <p> The next subsection provides an example of such a case. 3.3 Implementing Cache Coherence with Formal Methods After several unsuccessful attempts of completing the cache coherence protocol using traditional development methods, we decided to rewrite the system using Teapot <ref> [9] </ref>, a tool for writing memory coherence protocols. Our experience with this more formal approach has been positive.
Reference: [10] <author> Corbett, P., Baylor, S., and Feitelson, D. </author> <title> Overview of the Vesta Parallel File System. Computer Architecture News 21, </title> <address> 5 (Dec. </address> <year> 1993), </year> <pages> 7-14. </pages>
Reference-contexts: Moreover, this style of interaction is becoming 3 cache, a distributed metadata manager, and a striped network disk respectively. more common; many experimental systems including Vesta <ref> [10] </ref>, Zebra [20], Treadmarks [27], SAM [45], GMS [15], CRL [25], Inktomi [53], and Petal [31] have elements similar to those found in xFS. Thus, we believe these lessons are widely applicable to this new generation of peer-to-peer distributed systems.
Reference: [11] <author> Culler, D., Keeton, K., Liu, L., Mainwaring, A., Martin, R., Rodrigues, S., Wright, K., and Yoshikawa, C. </author> <title> The Generic Active Message Interface Specification. </title> <address> http://now.cs.berkeley.edu/Papers/Papers/gam spec.ps, </address> <year> 1994. </year>
Reference-contexts: To cope with these issues, we reimplemented our 17 communications layer using Active Messages, a communication layer originally designed for supercom-puting applications <ref> [11] </ref> but which has been enhanced to support cluster applications [33]. We found that Active Messages provided excellent performance, and by clearing away RPC's inappropriate abstractions it provided better semantics for supporting our communications patterns.
Reference: [12] <author> Culler, D. E., Dusseau, A., Goldstein, S. C., Krishnamurthy, A., Lumetta, S., von Eicken, T., and Yelick, K. </author> <title> Parallel Programming in Split-C. </title> <booktitle> In Proc. of Supercomputing '93 (November 1993). </booktitle>
Reference-contexts: Empirical evidence suggests that it is difficult to make layered protocols fast. On the other hand, Active Messages have proven to be an excellent "assembly language" for constructing high performance network abstractions including Split-C <ref> [12] </ref>, Thinking Machines message passing library [49], MPI [16], and Fast Sockets [41]. The Fast Sockets example is particularly relevant since it demonstrated a high-performance implementation of the Unix socket interface that, like RPC, has been widely used by distributed operating systems.
Reference: [13] <author> Dill, D. L., Drexler, A. J., Hu, A. J., and Yang, C. H. </author> <title> Protocol Verification as a Hardware Design Aid. </title> <booktitle> In IEEE International Conference on Computer Design: VLSI in Computers and Processors (1992), </booktitle> <pages> pp. 522-525. </pages>
Reference-contexts: The use of such continuations allows us to avoid having to manually decompose a handler into atomically executable pieces and then sequence them with state transitions. The protocol verifier is based on the Murfi system <ref> [13] </ref>. It systematically checks for protocol bugs such as invariant violations and deadlocks by performing an exhaustive state space exploration. Effectively, Murfi generates an exhaustive test vector for the distributed protocol; it reduces the length of this test vector by exploiting symmetries in the protocol.
Reference: [14] <author> Draves, R. P., Bershad, B. N., Rashid, R. F., and Dean, R. W. </author> <title> Using Continuations to Implement Thread Management and Communication in Operating Systems. </title> <booktitle> In Proc. of the 13th ACM Symposium on Operating Systems Principles (October 1991), </booktitle> <pages> pp. 122-136. </pages>
Reference-contexts: One solution to this problem is to use a continuation <ref> [14] </ref> which captures the execution context before sending the request event and restores the execution context when the reply event arrives. Another useful extension 16 is to allow multiple threads to interact with an event loop. It is not possible or even desirable to eliminate all uses of multithreading.
Reference: [15] <author> Feeley, M. J., Morgan, W. E., Pighin, F. P., Karlin, A. R., Levy, H. M., and Thekkath, C. A. </author> <title> Implementing Global Memory Management in a Workstation Cluster. </title> <booktitle> In Proc. of the 15th ACM Symposium on Operating Systems Principles (December 1995), </booktitle> <pages> pp. 201-212. </pages>
Reference-contexts: Moreover, this style of interaction is becoming 3 cache, a distributed metadata manager, and a striped network disk respectively. more common; many experimental systems including Vesta [10], Zebra [20], Treadmarks [27], SAM [45], GMS <ref> [15] </ref>, CRL [25], Inktomi [53], and Petal [31] have elements similar to those found in xFS. Thus, we believe these lessons are widely applicable to this new generation of peer-to-peer distributed systems. <p> A closer examination revealed that the bulk of the overhead can be attributed to the inefficiency in the virtual memory system. Similar problems were noted in the OSF/1 kernel, whose high kernel miss overhead impacted the design of GMS <ref> [15] </ref>. 6.3 Portability The vnode layers of different operating systems are considerably different [52]. One needs arcane knowledge of the kernel internals in order to port a file system to a different vnode layer. This is one of the major obstacles to deploying xFS on a variety of different platforms.
Reference: [16] <author> Forum, M. P. I. </author> <title> MPI: A Message-Passing Interface Standard. </title> <type> Tech. Rep. </type> <institution> CS-94-320, University of Tennessee Computer Science Department, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: Empirical evidence suggests that it is difficult to make layered protocols fast. On the other hand, Active Messages have proven to be an excellent "assembly language" for constructing high performance network abstractions including Split-C [12], Thinking Machines message passing library [49], MPI <ref> [16] </ref>, and Fast Sockets [41]. The Fast Sockets example is particularly relevant since it demonstrated a high-performance implementation of the Unix socket interface that, like RPC, has been widely used by distributed operating systems.
Reference: [17] <author> Ghormley, D. P., Petrou, D., and Anderson, T. E. SLIC: </author> <title> Secure Loadable Interposition Code. </title> <type> Tech. Rep. </type> <institution> UCB/CSD 96/920, University of California at Berkeley, </institution> <month> November </month> <year> 1996. </year>
Reference-contexts: We are currently investigating portable and efficient interposition agents as a means of distributing our code <ref> [17] </ref>. In this section, we have seen that the vnode layer has achieved the goal of making xFS available to unmodified applications, but at a considerable cost. Some of the disadvantages are the complexity due to the lack of coherence support, interface overhead on cache misses, and poor portability.
Reference: [18] <author> Gosling, J., and McGilton, H. </author> <title> The Java(tm) Language Environment: A White Paper. </title> <address> http://- java.dimensionx.com/whitePaper/java-whitepaper-1.html, </address> <year> 1995. </year>
Reference-contexts: Using events as the communication mechanism among different modules that are single-threaded event loops can eliminate the source of many synchronization bugs. As thread programming is entering the mainstream <ref> [18] </ref>, and support for reusable software components matures [48], we believe our experience is applicable for a larger audience. 4.1 Client Software Architecture cache, interacts with the existing kernel file cache, and initiates upcalls to the user space for cache misses.
Reference: [19] <author> Greenwald, M., and Cheriton, D. </author> <title> The Synergy Between Non-blocking Synchronization and Operating System Structure. </title> <booktitle> In Proc. of the Second Symposium on Operating Systems Design and Implementation (October 1996), </booktitle> <pages> pp. 123-136. </pages>
Reference-contexts: The judicious use of single-threaded event loops, when applicable, can reduce the complexity. A different approach to the thread deadlock problem is to use wait-free synchronization <ref> [19] </ref>. While event loops reduce unnecessary concurrency, wait-free synchronization preserves concurrency and retries the operations if conflicts are detected.
Reference: [20] <author> Hartman, J., and Ousterhout, J. </author> <title> The Zebra Striped Network File System. </title> <journal> ACM Transactions on Computer Systems (Aug. </journal> <year> 1995). </year> <month> 28 </month>
Reference-contexts: Moreover, this style of interaction is becoming 3 cache, a distributed metadata manager, and a striped network disk respectively. more common; many experimental systems including Vesta [10], Zebra <ref> [20] </ref>, Treadmarks [27], SAM [45], GMS [15], CRL [25], Inktomi [53], and Petal [31] have elements similar to those found in xFS. Thus, we believe these lessons are widely applicable to this new generation of peer-to-peer distributed systems.
Reference: [21] <author> Heidemann, J., and Popek, G. </author> <title> File-system Development with Stackable Layers. </title> <journal> ACM Transactions on Computer Systems 12, </journal> <month> 1 (Feb. </month> <year> 1994), </year> <pages> 58-89. </pages>
Reference-contexts: Some of the main issues are coherence, efficiency, and portability. Some of these problems have been noted in previous efforts to extend the vnode interface <ref> [40, 42, 46, 28, 21] </ref>.
Reference: [22] <author> Heidemann, J. S., and Popek, G. J. </author> <title> A Layered Approached to File System Development. </title> <type> Tech. Rep. </type> <institution> CSD-91007, UCLA Computer Science Department, </institution> <month> March </month> <year> 1991. </year>
Reference-contexts: Despite many years of research in extensible file systems <ref> [39, 5, 40, 38, 42, 22, 1, 28, 52] </ref>, the construction of portable file systems has remained a difficult undertaking, especially for the commercial operating systems. We are currently investigating portable and efficient interposition agents as a means of distributing our code [17].
Reference: [23] <author> Howard, J., Kazar, M., Menees, S., Nichols, D., Satyanarayanan, M., Sidebotham, R., and West, M. </author> <title> Scale and Performance in a Distributed File System. </title> <journal> ACM Transactions on Computer Systems 6, </journal> <month> 1 (Feb. </month> <year> 1988), </year> <pages> 51-81. </pages>
Reference-contexts: The weak consistency models provided by traditional distributed file systems such as NFS [43] and Andrew <ref> [23] </ref> do not satisfy the more strict consistency models assumed by many distributed applications. The vnode layer should provide a better coherence interface to allow file systems to support the strong consistency needed by these applications. <p> The local file systems obviously need not consider coherence, and NFS [43] only provides an ad hoc style of weak consistency. Some later research file systems, such as Andrew <ref> [23] </ref>, had more sophisticated consistency semantics but consistency actions were only triggered on file open events. Once a file is open, its content and attributes cannot be affected by remote machines. This simplifies vnode coherence management.
Reference: [24] <author> Hutchinson, N. C., and Peterson, L. L. </author> <title> The x-Kernel: An Architecture for Implementing Network Protocols. </title> <journal> IEEE Transactions on Software Engineering 17, </journal> <month> 1 (January </month> <year> 1991). </year>
Reference-contexts: In this section, we discuss some practical engineering heuristics. The key observation is that the xFS client software architecture is analogous to that of a network protocol stack <ref> [24] </ref> or a graphical user interface application [37]. Using events as the communication mechanism among different modules that are single-threaded event loops can eliminate the source of many synchronization bugs. <p> inconsistency in certain situations to avoid deadlocks.) 15 4.3 Use of Event Loops The client architecture shown in Figure 4 is analogous to a network protocol stack in which each protocol component has a message interface that allows it to send packets to and receive packets from an adjacent protocol <ref> [24] </ref>. In one possible implementation strategy of this architecture, the absence of multiple threads executing concurrently in a protocol component eliminates the need for concurrency control. Similarly, in [37], Ousterhout observes that single-threaded event loops have many advantages over threads for most graphical user interface applications.
Reference: [25] <author> Johnson, K. L., Kaashoek, M. F., and Wallach, D. A. </author> <title> CRL: High Performance All-Software Distributed Shared Memory. </title> <booktitle> In Proc. of the 15th ACM Symposium on Operating Systems Principles (December 1995), </booktitle> <pages> pp. 213-228. </pages>
Reference-contexts: Moreover, this style of interaction is becoming 3 cache, a distributed metadata manager, and a striped network disk respectively. more common; many experimental systems including Vesta [10], Zebra [20], Treadmarks [27], SAM [45], GMS [15], CRL <ref> [25] </ref>, Inktomi [53], and Petal [31] have elements similar to those found in xFS. Thus, we believe these lessons are widely applicable to this new generation of peer-to-peer distributed systems.
Reference: [26] <author> Keeton, K. K., Anderson, T. E., and Patterson, D. A. </author> <title> LogP Quantified: The Case for Low-Overhead Local Area Networks. </title> <booktitle> In Proc. of 1995 Hot Interconnects III (August 1995). </booktitle>
Reference-contexts: First, the serverless architecture requires the use of many small control messages, whose efficient transmission is crucial to the performance of xFS. Traditional RPC implementations layered on top of heavy weight protocols (such as TCP and UDP) <ref> [26] </ref> cannot satisfy this performance requirement. Second, instead of using simple two-party request/reply exchanges, xFS transactions can require the cooperation of several machines.
Reference: [27] <author> Keleher, P., Cox, A. L., Dwarkadas, S., and Zwaenepoel, W. TreadMarks: </author> <title> Distributed Shared Memory on Standard Workstations and Operating Systems. </title> <booktitle> In Proc. of the 1994 Winter Usenix Conference (January 1994), </booktitle> <pages> pp. 115-132. </pages>
Reference-contexts: Moreover, this style of interaction is becoming 3 cache, a distributed metadata manager, and a striped network disk respectively. more common; many experimental systems including Vesta [10], Zebra [20], Treadmarks <ref> [27] </ref>, SAM [45], GMS [15], CRL [25], Inktomi [53], and Petal [31] have elements similar to those found in xFS. Thus, we believe these lessons are widely applicable to this new generation of peer-to-peer distributed systems.
Reference: [28] <author> Khalidi, Y. A., and Nelson, M. N. </author> <title> Extensible File Systems in Spring. </title> <booktitle> In Proc. of the 14th ACM Symposium on Operating Systems Principles (December 1993), </booktitle> <pages> pp. 1-14. </pages>
Reference-contexts: Some of the main issues are coherence, efficiency, and portability. Some of these problems have been noted in previous efforts to extend the vnode interface <ref> [40, 42, 46, 28, 21] </ref>. <p> Despite many years of research in extensible file systems <ref> [39, 5, 40, 38, 42, 22, 1, 28, 52] </ref>, the construction of portable file systems has remained a difficult undertaking, especially for the commercial operating systems. We are currently investigating portable and efficient interposition agents as a means of distributing our code [17].
Reference: [29] <author> Kleiman, S. R. Vnodes: </author> <title> An Architecture for Multiple File System Types in Sun UNIX. </title> <booktitle> In Proc. of the 1986 Summer Usenix Conference (June 1986), </booktitle> <pages> pp. 238-247. </pages>
Reference-contexts: Unfortunately, few improvements have been made to the commercial operating systems; and the performance and interface problems are exacerbated by new serverless distributed systems. 6.1 Cache Coherence The vnode interface was originally designed to integrate NFS into Unix based operating systems that only supported local file systems <ref> [29] </ref>. The local file systems obviously need not consider coherence, and NFS [43] only provides an ad hoc style of weak consistency. Some later research file systems, such as Andrew [23], had more sophisticated consistency semantics but consistency actions were only triggered on file open events.
Reference: [30] <author> Kubiatowicz, J., and Agarwal, A. </author> <title> Anatomy of a Message in the Alewife Multiprocessor. </title> <booktitle> In Proc. of the 7th International Conf. on Supercomputing (July 1993). </booktitle>
Reference-contexts: The job of the metadata manager is tracking locations of file data blocks, and forwarding requests from clients to the appropriate destinations. Its functionality is similar to the directory in traditional DSM systems such as DASH [32] and Alewife <ref> [30] </ref>. Finally, the storage servers collectively provide the illusion of a striped network disk. They receive striped writes from clients. They also react to requests from managers by supplying data to the clients which have initiated the I/O operations. <p> Finally, client-to-client data transfers in xFS, while more efficient than passing all data through the server, introduce potential circular dependencies. The cache coherence protocol in xFS is similar to those seen in hardware DSM systems such as DASH [32] and Alewife <ref> [30] </ref>. But even minor modifications to these protocols can lead to subtle bugs [9]. Also, aspects of the cluster file system require protocol modifications that do not apply to DSM systems. For example, xFS must maintain reliable data storage in the face of node failures.
Reference: [31] <author> Lee, E. K., and Thekkath, C. E. </author> <title> Petal: Distributed Virtual Disks. </title> <booktitle> In Seventh International Conference on Architectural Support for Programming Languages and Operating Systems (October 1996), </booktitle> <pages> pp. 84-92. </pages>
Reference-contexts: Moreover, this style of interaction is becoming 3 cache, a distributed metadata manager, and a striped network disk respectively. more common; many experimental systems including Vesta [10], Zebra [20], Treadmarks [27], SAM [45], GMS [15], CRL [25], Inktomi [53], and Petal <ref> [31] </ref> have elements similar to those found in xFS. Thus, we believe these lessons are widely applicable to this new generation of peer-to-peer distributed systems.
Reference: [32] <author> Lenoski, D., Laudon, J., Gharachorloo, K., Gupta, A., and Hennessy, J. </author> <title> The Directory-Based Cache Coherence Protocol for the DASH Multiprocessor. </title> <booktitle> In Proc. of the 17th International Symposium on Computer Architecture (May 1990), </booktitle> <pages> pp. 148-159. </pages>
Reference-contexts: The job of the metadata manager is tracking locations of file data blocks, and forwarding requests from clients to the appropriate destinations. Its functionality is similar to the directory in traditional DSM systems such as DASH <ref> [32] </ref> and Alewife [30]. Finally, the storage servers collectively provide the illusion of a striped network disk. They receive striped writes from clients. They also react to requests from managers by supplying data to the clients which have initiated the I/O operations. <p> Finally, client-to-client data transfers in xFS, while more efficient than passing all data through the server, introduce potential circular dependencies. The cache coherence protocol in xFS is similar to those seen in hardware DSM systems such as DASH <ref> [32] </ref> and Alewife [30]. But even minor modifications to these protocols can lead to subtle bugs [9]. Also, aspects of the cluster file system require protocol modifications that do not apply to DSM systems. For example, xFS must maintain reliable data storage in the face of node failures.
Reference: [33] <author> Mainwaring, A., and Culler, D. </author> <title> Active Message Application Programming Interface and Communication Subsystem Organization. </title> <type> Tech. Rep. </type> <institution> UCB/CSD 96/918, University of California, Berkeley, </institution> <month> October </month> <year> 1996. </year>
Reference-contexts: Furthermore, the overhead imposed by current RPC implementations' layered architectures result in overheads that are unacceptable on new, low-latency networks. We found that programming directly on top of Active Message <ref> [51, 33] </ref> provided acceptable semantics and excellent performance, but a high-performance, continuation-passing RPC system would be simpler to program than raw Active Messages. * Kernel vnode layers should provide more support for cache coherence, and they should be reim-plemented to reduce cache miss overhead. <p> We use a loadable kernel module to implement the Solaris kernel vnode interface and redirect I/O requests to the user level daemon. All network communications occur at user level, using low overhead Active Messages <ref> [51, 33] </ref>. Although the architecture is fundamentally sound and has demonstrated excellent scalability, the peer-to-peer serverless architecture entails many implementation challenges. For example, one I/O request by a user can potentially involve five different machines, demanding more formal approaches when reasoning about the correctness of interactions in such a system. <p> To cope with these issues, we reimplemented our 17 communications layer using Active Messages, a communication layer originally designed for supercom-puting applications [11] but which has been enhanced to support cluster applications <ref> [33] </ref>. We found that Active Messages provided excellent performance, and by clearing away RPC's inappropriate abstractions it provided better semantics for supporting our communications patterns. Nonetheless, the Active Message interface exposes low-level details that we would prefer not to manage at the application level. <p> More details can be found in <ref> [33] </ref>. * Naming. The original Active Message interface assumes a single program image (SPMD) model, where the communicating parties are simply referenced by contiguous node numbers, and the message handlers are referenced by absolute program counter addresses.
Reference: [34] <author> Nelson, G., Leino, K., Saxe, J., and Stata, R. </author> <title> Extended Static Checker Home Page. </title> <note> http://- www.research.digital.com/SRC/esc/Esc.html, 1996. </note>
Reference-contexts: This is analogous to the thread-per-packet solution in a network protocol stack. One can use static checkers to prove the correctness of these local synchronizations <ref> [34] </ref>. To summarize, we have found that managing concurrency with threads has been a difficult problem in the xFS implementation, especially when threads enter off-the-shelf software modules over which we do not have full control of the implementation.
Reference: [35] <author> Nelson, M., Welch, B., and Ousterhout, J. </author> <title> Caching in the Sprite Network File System. </title> <journal> ACM Transactions on Computer Systems 6, </journal> <month> 1 (Feb. </month> <year> 1988). </year>
Reference-contexts: These differences are the result of balancing performance, functionality, and complexity. To better understand the design tradeoffs involved when we move from a traditional client-server architecture to a peer-to-peer architecture, consider the difference between the Sprite file system cache coherence protocol <ref> [35] </ref> and that of xFS. In Sprite, the server acts as both the token manager and the home of data and clients only exchange network messages with the server. <p> Some later research file systems, such as Andrew [23], had more sophisticated consistency semantics but consistency actions were only triggered on file open events. Once a file is open, its content and attributes cannot be affected by remote machines. This simplifies vnode coherence management. Other systems such as Sprite <ref> [35] </ref> and Spritely NFS [47] disable caching and fall back to a central server when a file is opened for concurrent writing sharing. This also simplifies coherence management.
Reference: [36] <author> Ousterhout, J. K. </author> <title> The Role of Distributed State. </title> <booktitle> In CMU Computer Science: a 25th Anniversary Commemorative (1991), </booktitle> <editor> R. F. Rashid, Ed., </editor> <publisher> Addison-Wesley, </publisher> <pages> pp. 199-217. </pages>
Reference-contexts: In an early version of that system, granting of a read token could be overtaken by a subsequent revoke of the same token and this led to a subtle race condition <ref> [36] </ref>. Similarly, the Active Message layer used by xFS does not enforce ordering between pairs of hosts, and the xFS protocol allows multiple outstanding requests.
Reference: [37] <author> Ousterhout, J. K. </author> <title> Why Threads Are a Bad Idea. </title> <note> http://www.sunlabs.com/~ouster/, 1995. 29 </note>
Reference-contexts: As heavily multithreaded software becomes prevalent, designers of services that interact with multiple multithreaded subsystems will find it increasingly difficult to reason about the behavior of their systems. Our experience bears out the hypothesis that the judicious use of single-threaded event loops can be effective in managing this complexity <ref> [37] </ref>. Many of the difficulties we encountered were rooted in mismatches between the service we were constructing and the tools and interfaces on which we built. Programmers should be aware of these mismatches so that they can structure their applications to minimize this dissonance or avoid using unsuitable interfaces. <p> In this section, we discuss some practical engineering heuristics. The key observation is that the xFS client software architecture is analogous to that of a network protocol stack [24] or a graphical user interface application <ref> [37] </ref>. Using events as the communication mechanism among different modules that are single-threaded event loops can eliminate the source of many synchronization bugs. <p> In one possible implementation strategy of this architecture, the absence of multiple threads executing concurrently in a protocol component eliminates the need for concurrency control. Similarly, in <ref> [37] </ref>, Ousterhout observes that single-threaded event loops have many advantages over threads for most graphical user interface applications. The same arguments can apply to several of the components in xFS.
Reference: [38] <author> Peterson, L., Hutchinson, N., O'Malley, S., and Rao, H. </author> <title> The x-kernel: A Platform for Accessing Internet Resources. </title> <booktitle> IEEE Computer 23, </booktitle> <month> 5 (September </month> <year> 1990), </year> <pages> 23-33. </pages>
Reference-contexts: Despite many years of research in extensible file systems <ref> [39, 5, 40, 38, 42, 22, 1, 28, 52] </ref>, the construction of portable file systems has remained a difficult undertaking, especially for the commercial operating systems. We are currently investigating portable and efficient interposition agents as a means of distributing our code [17].
Reference: [39] <author> Rees, J., Levine, P. H., Mishkin, N., and Leach, P. J. </author> <title> An Extensible I/O System. </title> <booktitle> In Proc. of the 1986 Summer USENIX (June 1986). </booktitle>
Reference-contexts: Despite many years of research in extensible file systems <ref> [39, 5, 40, 38, 42, 22, 1, 28, 52] </ref>, the construction of portable file systems has remained a difficult undertaking, especially for the commercial operating systems. We are currently investigating portable and efficient interposition agents as a means of distributing our code [17].
Reference: [40] <author> Richard G. Guy, e. a. </author> <title> Implementation of the Ficus Replicated File System. </title> <booktitle> In Proc. of the 1990 Summer USENIX (June 1990). </booktitle>
Reference-contexts: Some of the main issues are coherence, efficiency, and portability. Some of these problems have been noted in previous efforts to extend the vnode interface <ref> [40, 42, 46, 28, 21] </ref>. <p> Despite many years of research in extensible file systems <ref> [39, 5, 40, 38, 42, 22, 1, 28, 52] </ref>, the construction of portable file systems has remained a difficult undertaking, especially for the commercial operating systems. We are currently investigating portable and efficient interposition agents as a means of distributing our code [17].
Reference: [41] <author> Rodrigues, S. H., Anderson, T. E., and Culler, D. E. </author> <title> High-Performance Local-Area Communication Using Fast Sockets. </title> <booktitle> In Proc. of the Winter 1997 USENIX (Jan. </booktitle> <year> 1997). </year>
Reference-contexts: Empirical evidence suggests that it is difficult to make layered protocols fast. On the other hand, Active Messages have proven to be an excellent "assembly language" for constructing high performance network abstractions including Split-C [12], Thinking Machines message passing library [49], MPI [16], and Fast Sockets <ref> [41] </ref>. The Fast Sockets example is particularly relevant since it demonstrated a high-performance implementation of the Unix socket interface that, like RPC, has been widely used by distributed operating systems.
Reference: [42] <author> Rosenthal, D. S. H. </author> <title> Evolving the Vnode Interface. </title> <booktitle> In Proc. of the 1990 Summer USENIX (June 1990). </booktitle>
Reference-contexts: Some of the main issues are coherence, efficiency, and portability. Some of these problems have been noted in previous efforts to extend the vnode interface <ref> [40, 42, 46, 28, 21] </ref>. <p> Despite many years of research in extensible file systems <ref> [39, 5, 40, 38, 42, 22, 1, 28, 52] </ref>, the construction of portable file systems has remained a difficult undertaking, especially for the commercial operating systems. We are currently investigating portable and efficient interposition agents as a means of distributing our code [17].
Reference: [43] <author> Sandberg, R., Goldberg, D., Kleiman, S., Walsh, D., and Lyon, B. </author> <title> Design and Implementation of the Sun Network Filesystem. </title> <booktitle> In Proc. of the Summer 1985 USENIX (June 1985), </booktitle> <pages> pp. 119-130. </pages>
Reference-contexts: The weak consistency models provided by traditional distributed file systems such as NFS <ref> [43] </ref> and Andrew [23] do not satisfy the more strict consistency models assumed by many distributed applications. The vnode layer should provide a better coherence interface to allow file systems to support the strong consistency needed by these applications. <p> The local file systems obviously need not consider coherence, and NFS <ref> [43] </ref> only provides an ad hoc style of weak consistency. Some later research file systems, such as Andrew [23], had more sophisticated consistency semantics but consistency actions were only triggered on file open events. Once a file is open, its content and attributes cannot be affected by remote machines.
Reference: [44] <author> Savage, S., Anderson, T. E., Burrows, M., and Nelson, G. Eraser: </author> <title> A Dynamic Data Race Detector for Multi-Threaded Programs. </title> <booktitle> In Proceedings of the ACM Sixteenth Symposium on Operating Systems Principles (Oct. </booktitle> <year> 1997). </year>
Reference-contexts: Our experience indicates that composing complex multithreaded subsystems in this way leads to many concurrency control bugs, and that the "natural" ordering of events frequently leads to races and deadlocks. Ideally, we would like to use formal methods (such as the ones reported in <ref> [44] </ref>) to verify the correctness of the synchronization behavior of our system.
Reference: [45] <author> Scales, D. J., and Lam, M. S. </author> <title> The Design and Evaluation of a Shared Object System for Distributed Memory Machines. </title> <booktitle> In Proc. of the First Symposium on Operating Systems Design and Implementation (November 1994), </booktitle> <pages> pp. 101-114. </pages>
Reference-contexts: Moreover, this style of interaction is becoming 3 cache, a distributed metadata manager, and a striped network disk respectively. more common; many experimental systems including Vesta [10], Zebra [20], Treadmarks [27], SAM <ref> [45] </ref>, GMS [15], CRL [25], Inktomi [53], and Petal [31] have elements similar to those found in xFS. Thus, we believe these lessons are widely applicable to this new generation of peer-to-peer distributed systems.
Reference: [46] <author> Skinner, G. C., and Wong, T. K. </author> <title> Stacking Vnodes: A Progress Report. </title> <booktitle> In Proc. of the 1993 Summer USENIX (June 1993). </booktitle>
Reference-contexts: Some of the main issues are coherence, efficiency, and portability. Some of these problems have been noted in previous efforts to extend the vnode interface <ref> [40, 42, 46, 28, 21] </ref>.
Reference: [47] <author> Srinivasan, V., and Mogul, J. Spritely NFS: </author> <title> Experiments with Cache Consistency Protocols. </title> <booktitle> In Proc. of the 12th Symposium on Operating Systems Principles (Dec. </booktitle> <year> 1989), </year> <pages> pp. 45-57. </pages>
Reference-contexts: Once a file is open, its content and attributes cannot be affected by remote machines. This simplifies vnode coherence management. Other systems such as Sprite [35] and Spritely NFS <ref> [47] </ref> disable caching and fall back to a central server when a file is opened for concurrent writing sharing. This also simplifies coherence management.
Reference: [48] <author> Sun Microsystems. </author> <title> JavaBeans API Specification, </title> <note> 1996. http://www.javasoft.com/beans/. </note>
Reference-contexts: Using events as the communication mechanism among different modules that are single-threaded event loops can eliminate the source of many synchronization bugs. As thread programming is entering the mainstream [18], and support for reusable software components matures <ref> [48] </ref>, we believe our experience is applicable for a larger audience. 4.1 Client Software Architecture cache, interacts with the existing kernel file cache, and initiates upcalls to the user space for cache misses. <p> Deadlocks in a hierarchy of components that must support events flowing in opposite directions are by no means unique to xFS. In the JavaBeans Development Kit (BDK) <ref> [48] </ref>, for example, events normally flow bottom-up towards the enclosing beans, while normal program flow travels top-down towards the enclosed beans. If the bean methods in question are protected by monitor locks, one runs the risk of deadlocks when multiple threads execute concurrently in opposite directions.
Reference: [49] <author> Tucker, L., and Mainwaring, A. </author> <title> CMMD: Active Messages on the CM-5. </title> <booktitle> Parallel Computing 20, </booktitle> <month> 4 (August </month> <year> 1994), </year> <pages> 481-496. </pages> <note> [50] von Eicken, </note> <author> T., Basu, A., Buch, V., and Vogels, W. U-Net: </author> <title> A User-Level Network Interface for Parallel and Distributed Computing. </title> <booktitle> In Proc. of the 15th ACM Symposium on Operating Systems Principles (December 1995), </booktitle> <pages> pp. 40-53. </pages> <note> [51] von Eicken, </note> <author> T., Culler, D., Goldstein, S., and Schauser, K. E. </author> <title> Active Messages: A Mechanism for Integrated Communication and Computation. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-V) (May 1992), </booktitle> <pages> pp. 256-266. </pages>
Reference-contexts: Empirical evidence suggests that it is difficult to make layered protocols fast. On the other hand, Active Messages have proven to be an excellent "assembly language" for constructing high performance network abstractions including Split-C [12], Thinking Machines message passing library <ref> [49] </ref>, MPI [16], and Fast Sockets [41]. The Fast Sockets example is particularly relevant since it demonstrated a high-performance implementation of the Unix socket interface that, like RPC, has been widely used by distributed operating systems.
Reference: [52] <author> Webber, N. </author> <title> Operating System Support for Portable Filesystem Extensions. </title> <booktitle> In Proceedings of the 1993 USENIX Winter Conference (January 1993), </booktitle> <pages> pp. 219-228. </pages>
Reference-contexts: Similar problems were noted in the OSF/1 kernel, whose high kernel miss overhead impacted the design of GMS [15]. 6.3 Portability The vnode layers of different operating systems are considerably different <ref> [52] </ref>. One needs arcane knowledge of the kernel internals in order to port a file system to a different vnode layer. This is one of the major obstacles to deploying xFS on a variety of different platforms. <p> Despite many years of research in extensible file systems <ref> [39, 5, 40, 38, 42, 22, 1, 28, 52] </ref>, the construction of portable file systems has remained a difficult undertaking, especially for the commercial operating systems. We are currently investigating portable and efficient interposition agents as a means of distributing our code [17].
Reference: [53] <author> Woodruff, A., Aoki, P. M., Brewer, E., Gauthier, P., and Rowe, L. A. </author> <title> An Investigation of Documents from the World Wide Web. </title> <note> http://www.bmrc.berkeley.edu/papers/inktomi/, 1996. 30 </note>
Reference-contexts: Moreover, this style of interaction is becoming 3 cache, a distributed metadata manager, and a striped network disk respectively. more common; many experimental systems including Vesta [10], Zebra [20], Treadmarks [27], SAM [45], GMS [15], CRL [25], Inktomi <ref> [53] </ref>, and Petal [31] have elements similar to those found in xFS. Thus, we believe these lessons are widely applicable to this new generation of peer-to-peer distributed systems.
References-found: 51

