URL: http://www.cs.rice.edu/~adve/Papers/hpca97remote-write.ps.gz
Refering-URL: http://www.cs.rice.edu/~adve/Papers/
Root-URL: 
Title: An Evaluation of Fine-Grain Producer-Initiated Communication in Cache-Coherent Multiprocessors  
Author: Hazim Abdel-Shafi Jonathan Hall Sarita V. Adve Vikram S. Adve 
Address: Houston, Texas 77005  2111 NE 25th Ave, MS JF1-19 Hillsboro, Oregon 97124  
Affiliation: Electrical and Computer Engineering Computer Science Rice University  Intel Corporation  
Date: (February, 1997)  
Note: To appear in Proceedings of HPCA-3  
Abstract: Prefetching is a widely used consumer-initiated mechanism to hide communication latency in shared-memory multiprocessors. However, prefetching is inapplicable or insufficient for some communication patterns such as irregular communication, pipelined loops, and synchronization. For these cases, a combination of two fine-grain, producer-initiated primitives (referred to as remote-writes) is better able to reduce the latency of communication. This paper demonstrates experimentally that remote writes provide significant performance benefits in cache-coherent shared-memory multiprocessors with and without prefetch-ing. Further, the combination of remote writes and prefetching is able to eliminate most of the memory system overhead in the applications, except misses due to cache conflicts. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. Abdel-Shafi. </author> <title> Fine-Grain Producer-Initiated Communication in Cache-Coherent Multiprocessors. </title> <type> Master's thesis, </type> <institution> Rice University, </institution> <year> 1997. </year>
Reference-contexts: accessed line, a WriteThroughInv or WriteSendInv invalidates the copy, while a WriteThrough or WriteSend updates it 1 We compared performance with and without the coalescing buffer and found that the buffer does reduce overall network traffic, sometimes by a large amount, and improves execution time for some of our applications <ref> [1] </ref>. 3 To appear in Proceedings of HPCA-3 (February, 1997) and sets it to shared state. With WriteSend and WriteSendInv, we chose to move the cache line into the destination's L2 cache, rather than the L1 cache, to minimize the possibility of cache conflicts. <p> Deadlock is avoided by sending the write-back on the reply network which is guaranteed to be deadlock free. Thus, the deadlock avoidance mechanisms of our base system prevent deadlocks due to WriteSends as well <ref> [1] </ref>. 3 Experimental Methodology We evaluate the performance impact of remote writes by comparing four systems: a base cache 2 A race occurs if the directory's write-back message reaches the processor after the processor has itself issued a write-back for the line. <p> To confirm this, we ran Base and PF with larger write buffers and found a negligible improvement in performance <ref> [1] </ref>. arrays are distributed by assigning blocks of consecutive X-Y planes to each processor. Communication occurs only in one phase, which consists of one loop with coarse-grain communication followed by two fine-grain pipelined loop nests with an intervening short reduction. In the pipelines, flags signal completion of each pipeline stage.
Reference: [2] <author> R. G. Covington et al. </author> <title> The Efficient Simulation of Parallel Computer Systems. </title> <journal> International Journal in Computer Simulation, </journal> <volume> 1, </volume> <year> 1991. </year>
Reference-contexts: We refer to these systems as Base, RW, PF, and PF+RW. For our evaluation, we use an execution-driven simulator based on direct execution, derived from the Rice Parallel Processing Testbed (RPPT) <ref> [2, 18] </ref>. 3.1 Architectures Modeled The Base system is similar to the one described in Section 2.2 (but without support for remote writes). For the RW system, we augment Base with support for remote writes as described in Section 2.2.
Reference: [3] <author> F. Dahlgren and P. Stenstrom. </author> <title> Using Write Caches to Improve Performance of Cache Coherence Protocols in Shared-Memory Multiprocessors. </title> <journal> JPDC, </journal> <volume> 26, </volume> <year> 1995. </year>
Reference-contexts: Perhaps most importantly, all but one of the basic mechanisms (described next) would already be present in a typical implementation of the underlying cache-coherence protocol. Processor-Cache Sub-system. The one additional mechanism we use to improve the performance of remote writes is a coalescing (or merging) line write buffer <ref> [3, 4] </ref>, located between the L1 and L2 caches. <p> The competitive-update protocol is a hardware-controlled hybrid update-invalidate protocol [7]. Dahlgren and Sten-strom have shown that the addition of a write-cache (similar to a coalescing buffer) can reduce coherence traffic, allowing hybrid update-invalidate protocols to significantly outperform write-invalidate protocols <ref> [3] </ref>. All of the above schemes use an update operation to transfer a written value directly into the caches of all the processors that currently contain a copy of the line; caches periodically get invalidated based on various runtime heuristics.
Reference: [4] <author> J. Edmondson et al. </author> <title> Internal Organization of the Alpha 21164. </title> <journal> Digital Technical Journal, </journal> <volume> 7(1), </volume> <year> 1995. </year>
Reference-contexts: Perhaps most importantly, all but one of the basic mechanisms (described next) would already be present in a typical implementation of the underlying cache-coherence protocol. Processor-Cache Sub-system. The one additional mechanism we use to improve the performance of remote writes is a coalescing (or merging) line write buffer <ref> [3, 4] </ref>, located between the L1 and L2 caches.
Reference: [5] <author> J. R. Goodman et al. </author> <title> Efficient Synchronization Primitives for Large-Scale Cache-Coherent Multiprocessors. </title> <booktitle> In ASPLOS-III, </booktitle> <year> 1989. </year>
Reference-contexts: In fact, the combination of WriteSend and MCS locks achieves essentially the same efficient communication pattern as hardware queue-based locks such as QOLB <ref> [5] </ref> 4 , but with much simpler and more general-purpose hardware support. Tree barriers: Our tree barrier implements a full barrier using a binary tree. <p> Write-update and hybrid update-invalidate protocols (under hardware or software control) are another mechanism for producer-initiated communication. Software primitives for updates include the KSR1 poststore [21], the DASH update [11], the SEL WRITE [19], and notify <ref> [5] </ref>. The competitive-update protocol is a hardware-controlled hybrid update-invalidate protocol [7]. Dahlgren and Sten-strom have shown that the addition of a write-cache (similar to a coalescing buffer) can reduce coherence traffic, allowing hybrid update-invalidate protocols to significantly outperform write-invalidate protocols [3].
Reference: [6] <author> M. D. Hill et al. </author> <title> Cooperative Shared Memory: Software and Hardware Support for Scalable Multiprocessors. </title> <journal> ACM TOCS, </journal> <volume> 11(4) </volume> <pages> 300-318, </pages> <year> 1993. </year>
Reference-contexts: Such an operation is well suited to migratory sharing where data or synchronization variables are repeatedly read and modified by different processors in unknown order. Primitives like Check-In (in the CICO model) and others <ref> [6, 23] </ref> have been proposed to support such an operation. While the above two types of producer-initiated primitives have been previously proposed and studied, few studies have evaluated the additional benefits of these primitives when prefetching is already available. <p> Finally, update protocols by themselves are not effective in handling unpredictable and irregular accesses, but these are partially optimized using WriteThrough. Our WriteThrough primitive is similar to the check-in primitive of the CICO model <ref> [6] </ref>. Unlike writes with CICO, however, WriteThrough does not require ownership. The CICO model does not provide any operation similar to WriteSend. Dynamic self-invalidation is a hardware-controlled mechanism analogous to check-in [10].
Reference: [7] <author> A. R. Karlin et al. </author> <title> Competitive Snoopy Caching. </title> <journal> Al-gorithmica, </journal> <volume> (3), </volume> <year> 1988. </year>
Reference-contexts: Write-update and hybrid update-invalidate protocols (under hardware or software control) are another mechanism for producer-initiated communication. Software primitives for updates include the KSR1 poststore [21], the DASH update [11], the SEL WRITE [19], and notify [5]. The competitive-update protocol is a hardware-controlled hybrid update-invalidate protocol <ref> [7] </ref>. Dahlgren and Sten-strom have shown that the addition of a write-cache (similar to a coalescing buffer) can reduce coherence traffic, allowing hybrid update-invalidate protocols to significantly outperform write-invalidate protocols [3].
Reference: [8] <author> D. Koufaty et al. </author> <title> Data Forwarding in Scalable Shared-Memory Multiprocessors. </title> <booktitle> In ICS, </booktitle> <year> 1995. </year>
Reference-contexts: Second, that study focuses on applications with coarse-grain doall parallelism, whereas we also examine fine-grain parallelism (doacross loops and two important synchronization kernels) and irregular sharing patterns. Koufaty et al. proposed a detailed compiler algorithm for forwarding, and showed significant improvements due to forwarding <ref> [8] </ref>. Although this work discusses how forwarding compares with prefetching, it does not include prefetching in the performance evaluation.
Reference: [9] <author> D. Kranz et al. </author> <title> Integrating Message-Passing and Shared-Memory: Early Experience. </title> <booktitle> In PPoPP, </booktitle> <year> 1993. </year>
Reference-contexts: Kranz et al. show similar results for a tree-barrier using fine-grain processor-to-processor messages in Alewife, although those messages are not cache-coherent and user-level handlers are required to handle them at the receiver <ref> [9] </ref>. All our applications use our tree barrier implementation, with WriteSends in the RW and PF+RW versions. 4.3 MP3D MP3D is a simulation of particles in rarefied flow. The two primary data structures are particles and an array of cells representing physical space.
Reference: [10] <author> A. Lebeck and D. Wood. </author> <title> Dynamic Self-Invalidation: Reducing Coherence Overhead in Shared-Memory Multiprocessors. </title> <booktitle> In 22nd ISCA, </booktitle> <year> 1995. </year>
Reference-contexts: Our WriteThrough primitive is similar to the check-in primitive of the CICO model [6]. Unlike writes with CICO, however, WriteThrough does not require ownership. The CICO model does not provide any operation similar to WriteSend. Dynamic self-invalidation is a hardware-controlled mechanism analogous to check-in <ref> [10] </ref>. Skeppstedt and Stenstrom developed compiler algorithms to insert instructions similar to WriteThrough to update memory and reduce coherence overhead [23].
Reference: [11] <author> D. Lenoski et al. </author> <title> The Stanford DASH Multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3), </volume> <year> 1992. </year>
Reference-contexts: Such an operation is well-suited to a regular producer-consumer style of data sharing, or to certain synchronization algorithms based on predictable communication patterns. Three primitives have been proposed to support such an operation: the forwarding write [16, 17], the DASH deliver <ref> [11] </ref>, and the PSET WRITE [19]. 1 To appear in Proceedings of HPCA-3 (February, 1997) In the second case described above, the writer cannot usually predict who will be the next reader. <p> Since WriteSend specifies only one destination processor, in cases where multiple consumers exist, separate WriteSends to different copies of the data at the different consumers would be needed. Previously proposed primitives similar to WriteSend use a bit vector to specify multiple destination processors <ref> [11, 16, 19] </ref>. We chose a single destination primarily because multiple sends from a single instruction are more difficult to implement within a coherence protocol, and a bit vector is not scalable. All our applications, except for a single broadcast operation in one application, needed only one destination processor. <p> Overall, however, remote writes are able to combine with prefetching to address virtually all the sources of memory system overhead, except for conflict misses, in the applications and kernels we studied. 5 Related Work Three primitives have been proposed that provide functionality similar to WriteSend: the DASH deliver <ref> [11] </ref>, the forwarding write [16, 17], and the PSET WRITE [19]. The most extensively studied is the forwarding write primitive, but it was evaluated as the sole producer-initiated primitive. A performance study of applications from the Perfect Club Benchmarks examined codes containing doall loop parallelism [16]. <p> For five applications from SPLASH, including Water and MP3D, the study concluded that forwarding provides little additional benefit over prefetching. Our experiments, however, show that using WriteThrough operations for such irregular sharing patterns provides significant additional benefits when combined with prefetching. The DASH deliver <ref> [11] </ref> and the PSET WRITE [19] primitives, are both similar to the forwarding write. No performance results were presented for the former. The latter was studied for one application for which it provided insignificant gains, and was not compared with fine-grain prefetching. <p> Write-update and hybrid update-invalidate protocols (under hardware or software control) are another mechanism for producer-initiated communication. Software primitives for updates include the KSR1 poststore [21], the DASH update <ref> [11] </ref>, the SEL WRITE [19], and notify [5]. The competitive-update protocol is a hardware-controlled hybrid update-invalidate protocol [7]. Dahlgren and Sten-strom have shown that the addition of a write-cache (similar to a coalescing buffer) can reduce coherence traffic, allowing hybrid update-invalidate protocols to significantly outperform write-invalidate protocols [3].
Reference: [12] <author> B.-H. Lim and A. Agarwal. </author> <title> Reactive Synchronization Algorithms for Multiprocessors. </title> <booktitle> In ASPLOS-VI, </booktitle> <year> 1994. </year>
Reference-contexts: If no processors are waiting, then a global lock variable is reset, using WriteThrough in the RW version. We compare the overhead of an MCS lock on Base and RW, using a method similar to that used by Lim and Agarwal <ref> [12] </ref>. We measure the total time for processors to execute 1000 acquire-release pairs each, with a 200-cycle holding time for the lock to model a small critical section. <p> There is a uniformly distributed delay between 0 and 1000 cycles after a release, before the same processor tries to reacquire the lock. We also measure the same execution time when using a zero-latency spin lock <ref> [12] </ref>. The difference between the two times, divided by the number of locks acquired per processor, is the average overhead for the MCS lock. (less than 4 processors accessing the lock).
Reference: [13] <author> J. M. Mellor-Crummey and M. L. Scott. </author> <title> Synchronization Without Contention. </title> <booktitle> In ASPLOS-IV, </booktitle> <year> 1991. </year>
Reference-contexts: The two kernels are MCS locks <ref> [13] </ref> and tree barriers. We chose these kernels because they are of fundamental importance to shared-memory applications, and are well-suited to producer-initiated communication. <p> There is also remaining overhead due to load imbalance, but such overhead is usually not directly targeted by memory system optimizations like remote writes or prefetches. 4.2 Synchronization Kernels MCS locks: An MCS lock is efficient under moderate to high lock contention <ref> [13] </ref>. Processors needing a lock form a queue and spin locally on different variables. The processor holding the lock releases it by writing directly to the variable of the next processor in the queue, using WriteSend in the RW version.
Reference: [14] <author> T. C. Mowry et al. </author> <title> Design and Evaluation of a Compiler Algorithm for Prefetching. </title> <booktitle> In ASPLOS-V, </booktitle> <year> 1992. </year>
Reference-contexts: Although this information is more than is required for prefetch-ing, advanced parallelizing compilers should be able to determine this information. We generally followed Mowry's algorithm <ref> [14] </ref> to insert prefetches in the applications, except that in some cases, we used extensive experimentation to schedule prefetches so as to maximize performance. We do not prefetch synchronization variables, although there may be benefit in prefetching low-contention locks.
Reference: [15] <author> V. S. Pai et al. </author> <title> The Impact of Instruction-Level Parallelism on Multiprocessor Performance and Simulation Methodolgy. </title> <booktitle> In HPCA-3, </booktitle> <year> 1997. </year>
Reference-contexts: In our experiments, we model a fairly conservative processor. Current processors aggressively exploit instruction-level-parallelism; e.g., using multiple issue, dynamic scheduling, and non-blocking reads. Recent work has shown that such techniques make memory stall time more dominant in shared-memory systems <ref> [15] </ref>. This would make techniques such as remote writes even more important for future systems. Finally, our suggested implementation of remote writes adds little complexity to the base system.
Reference: [16] <author> D. Poulsen. </author> <title> Memory Latency Reduction via Data Prefetching and Data Forwarding in Shared-Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1994. </year>
Reference-contexts: Such an operation is well-suited to a regular producer-consumer style of data sharing, or to certain synchronization algorithms based on predictable communication patterns. Three primitives have been proposed to support such an operation: the forwarding write <ref> [16, 17] </ref>, the DASH deliver [11], and the PSET WRITE [19]. 1 To appear in Proceedings of HPCA-3 (February, 1997) In the second case described above, the writer cannot usually predict who will be the next reader. <p> While the above two types of producer-initiated primitives have been previously proposed and studied, few studies have evaluated the additional benefits of these primitives when prefetching is already available. The few such studies <ref> [16, 24] </ref> have used a single producer-initiated primitive (data forwarding), and have been unable to combine the benefits of forwarding and prefetching, for reasons discussed below. The goal of this paper is to explore whether fine-grain producer-initiated communication primitives can provide complementary benefits to software prefetching in a shared-memory multiprocessor. <p> Nevertheless, WriteSend was effective in reducing most of the latency it targeted; these latencies are difficult to address with prefetching. There are three principal reasons why the results of our study differ from those of previous studies that evaluated data forwarding in combination with prefetching <ref> [16, 24] </ref>: (i) the use of a combination of WriteSend and WriteThrough primitives; (ii) more realistic architectural assumptions compared to some previous studies, and (iii) a broader application domain, including applications with pipelined loops and irregular sharing patterns, and important synchronization kernels. <p> Since WriteSend specifies only one destination processor, in cases where multiple consumers exist, separate WriteSends to different copies of the data at the different consumers would be needed. Previously proposed primitives similar to WriteSend use a bit vector to specify multiple destination processors <ref> [11, 16, 19] </ref>. We chose a single destination primarily because multiple sends from a single instruction are more difficult to implement within a coherence protocol, and a bit vector is not scalable. All our applications, except for a single broadcast operation in one application, needed only one destination processor. <p> remote writes are able to combine with prefetching to address virtually all the sources of memory system overhead, except for conflict misses, in the applications and kernels we studied. 5 Related Work Three primitives have been proposed that provide functionality similar to WriteSend: the DASH deliver [11], the forwarding write <ref> [16, 17] </ref>, and the PSET WRITE [19]. The most extensively studied is the forwarding write primitive, but it was evaluated as the sole producer-initiated primitive. A performance study of applications from the Perfect Club Benchmarks examined codes containing doall loop parallelism [16]. <p> The most extensively studied is the forwarding write primitive, but it was evaluated as the sole producer-initiated primitive. A performance study of applications from the Perfect Club Benchmarks examined codes containing doall loop parallelism <ref> [16] </ref>. Surprisingly, the study found that when forwarding and prefetching were combined, the performance of each application fell in between that of forwarding alone or prefetching alone.
Reference: [17] <author> D. Poulsen and P.-C. Yew. </author> <title> Data Prefetching and Data Forwarding in Shared-Memory Multiprocessors. </title> <booktitle> In ICPP, </booktitle> <year> 1994. </year>
Reference-contexts: Such an operation is well-suited to a regular producer-consumer style of data sharing, or to certain synchronization algorithms based on predictable communication patterns. Three primitives have been proposed to support such an operation: the forwarding write <ref> [16, 17] </ref>, the DASH deliver [11], and the PSET WRITE [19]. 1 To appear in Proceedings of HPCA-3 (February, 1997) In the second case described above, the writer cannot usually predict who will be the next reader. <p> remote writes are able to combine with prefetching to address virtually all the sources of memory system overhead, except for conflict misses, in the applications and kernels we studied. 5 Related Work Three primitives have been proposed that provide functionality similar to WriteSend: the DASH deliver [11], the forwarding write <ref> [16, 17] </ref>, and the PSET WRITE [19]. The most extensively studied is the forwarding write primitive, but it was evaluated as the sole producer-initiated primitive. A performance study of applications from the Perfect Club Benchmarks examined codes containing doall loop parallelism [16].
Reference: [18] <author> U. Rajagopalan. </author> <title> The Effects of Interconnection Networks on the Performance of Shared-Memory Multiprocessors. </title> <type> Master's thesis, </type> <institution> Rice University, </institution> <year> 1994. </year>
Reference-contexts: We refer to these systems as Base, RW, PF, and PF+RW. For our evaluation, we use an execution-driven simulator based on direct execution, derived from the Rice Parallel Processing Testbed (RPPT) <ref> [2, 18] </ref>. 3.1 Architectures Modeled The Base system is similar to the one described in Section 2.2 (but without support for remote writes). For the RW system, we augment Base with support for remote writes as described in Section 2.2.
Reference: [19] <author> U. Ramachandran et al. </author> <title> Architectural Mechanisms for Explicit Communication in Shared Memory Multiprocessors. </title> <booktitle> In Supercomputing'95, </booktitle> <year> 1995. </year>
Reference-contexts: Such an operation is well-suited to a regular producer-consumer style of data sharing, or to certain synchronization algorithms based on predictable communication patterns. Three primitives have been proposed to support such an operation: the forwarding write [16, 17], the DASH deliver [11], and the PSET WRITE <ref> [19] </ref>. 1 To appear in Proceedings of HPCA-3 (February, 1997) In the second case described above, the writer cannot usually predict who will be the next reader. Here, a "write-through" operation could transfer the data to memory so that the latency of a subsequent prefetch or read is reduced. <p> Since WriteSend specifies only one destination processor, in cases where multiple consumers exist, separate WriteSends to different copies of the data at the different consumers would be needed. Previously proposed primitives similar to WriteSend use a bit vector to specify multiple destination processors <ref> [11, 16, 19] </ref>. We chose a single destination primarily because multiple sends from a single instruction are more difficult to implement within a coherence protocol, and a bit vector is not scalable. All our applications, except for a single broadcast operation in one application, needed only one destination processor. <p> with prefetching to address virtually all the sources of memory system overhead, except for conflict misses, in the applications and kernels we studied. 5 Related Work Three primitives have been proposed that provide functionality similar to WriteSend: the DASH deliver [11], the forwarding write [16, 17], and the PSET WRITE <ref> [19] </ref>. The most extensively studied is the forwarding write primitive, but it was evaluated as the sole producer-initiated primitive. A performance study of applications from the Perfect Club Benchmarks examined codes containing doall loop parallelism [16]. <p> For five applications from SPLASH, including Water and MP3D, the study concluded that forwarding provides little additional benefit over prefetching. Our experiments, however, show that using WriteThrough operations for such irregular sharing patterns provides significant additional benefits when combined with prefetching. The DASH deliver [11] and the PSET WRITE <ref> [19] </ref> primitives, are both similar to the forwarding write. No performance results were presented for the former. The latter was studied for one application for which it provided insignificant gains, and was not compared with fine-grain prefetching. <p> Write-update and hybrid update-invalidate protocols (under hardware or software control) are another mechanism for producer-initiated communication. Software primitives for updates include the KSR1 poststore [21], the DASH update [11], the SEL WRITE <ref> [19] </ref>, and notify [5]. The competitive-update protocol is a hardware-controlled hybrid update-invalidate protocol [7]. Dahlgren and Sten-strom have shown that the addition of a write-cache (similar to a coalescing buffer) can reduce coherence traffic, allowing hybrid update-invalidate protocols to significantly outperform write-invalidate protocols [3].
Reference: [20] <author> A. Raynaud et al. </author> <title> Distance-Adaptive Update Protocols for Scalable Shared-Memory Multiprocessors. </title> <booktitle> In HPCA-2, </booktitle> <year> 1996. </year>
Reference-contexts: Raynaud et al. have proposed a different variation, called distance adaptive protocols, where hardware keeps track of a superset of current caches holding the line; on a write, a fairly complex runtime heuristic determines whether to send updates or invalidates to the recorded set of caches <ref> [20] </ref>. The primary difference between the above update schemes and remote writes is that most update schemes do not give the programmer or compiler direct control to update a specific processor cache for selectively exploiting static communication and synchronization patterns.
Reference: [21] <author> E. Rosti et al. </author> <title> The KSR1: Experimentation and Modeling of Poststore. </title> <booktitle> In SIGMETRICS, </booktitle> <year> 1993. </year>
Reference-contexts: Write-update and hybrid update-invalidate protocols (under hardware or software control) are another mechanism for producer-initiated communication. Software primitives for updates include the KSR1 poststore <ref> [21] </ref>, the DASH update [11], the SEL WRITE [19], and notify [5]. The competitive-update protocol is a hardware-controlled hybrid update-invalidate protocol [7].
Reference: [22] <author> J. P. Singh et al. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1), </volume> <year> 1992. </year>
Reference-contexts: We chose these kernels because they are of fundamental importance to shared-memory applications, and are well-suited to producer-initiated communication. The five applications are Radix from the SPLASH-2 suite [26], Water and MP3D (without locking for cells) from the SPLASH suite <ref> [22] </ref>, MICCG3D from the MIT Alewife group, and Erlebacher from the Rice Parallel Compiler group. The last two applications have static access patterns, and contain tightly synchronized pipelined loop nests. MP3D and Water have dynamic access patterns, and Radix exhibits both types of patterns.
Reference: [23] <author> J. Skeppstedt and P. Stenstrom. </author> <title> A Compiler Algorithm that Reduces Read Latency in Ownership-Based Cache Coherence Protocols. </title> <booktitle> In PACT, </booktitle> <year> 1995. </year>
Reference-contexts: Such an operation is well suited to migratory sharing where data or synchronization variables are repeatedly read and modified by different processors in unknown order. Primitives like Check-In (in the CICO model) and others <ref> [6, 23] </ref> have been proposed to support such an operation. While the above two types of producer-initiated primitives have been previously proposed and studied, few studies have evaluated the additional benefits of these primitives when prefetching is already available. <p> The CICO model does not provide any operation similar to WriteSend. Dynamic self-invalidation is a hardware-controlled mechanism analogous to check-in [10]. Skeppstedt and Stenstrom developed compiler algorithms to insert instructions similar to WriteThrough to update memory and reduce coherence overhead <ref> [23] </ref>. Performance studies of these primitives have not evaluated the additional benefits of the primitives over prefetching. 6 Conclusions This study evaluates the performance of fine-grain producer-initiated communication, both with and without fine-grain software prefetching in a base cache-coherent shared-memory system.
Reference: [24] <author> P. Trancoso and J. Torrellas. </author> <title> The Impact of Speeding up Critical Sections with Data Prefetching and Forwarding. </title> <booktitle> In Supercomputing '96, </booktitle> <year> 1996. </year>
Reference-contexts: While the above two types of producer-initiated primitives have been previously proposed and studied, few studies have evaluated the additional benefits of these primitives when prefetching is already available. The few such studies <ref> [16, 24] </ref> have used a single producer-initiated primitive (data forwarding), and have been unable to combine the benefits of forwarding and prefetching, for reasons discussed below. The goal of this paper is to explore whether fine-grain producer-initiated communication primitives can provide complementary benefits to software prefetching in a shared-memory multiprocessor. <p> Nevertheless, WriteSend was effective in reducing most of the latency it targeted; these latencies are difficult to address with prefetching. There are three principal reasons why the results of our study differ from those of previous studies that evaluated data forwarding in combination with prefetching <ref> [16, 24] </ref>: (i) the use of a combination of WriteSend and WriteThrough primitives; (ii) more realistic architectural assumptions compared to some previous studies, and (iii) a broader application domain, including applications with pipelined loops and irregular sharing patterns, and important synchronization kernels. <p> That study also mentions, but does not evaluate, several optimizations used in this paper, namely coalescing forwarded writes, forwarding only to the L2 cache, and forwarding only to memory for dynamically scheduled loops. Another study examined the performance impact of forwarding and prefetching for critical sections alone <ref> [24] </ref>. For five applications from SPLASH, including Water and MP3D, the study concluded that forwarding provides little additional benefit over prefetching. Our experiments, however, show that using WriteThrough operations for such irregular sharing patterns provides significant additional benefits when combined with prefetching.
Reference: [25] <author> S. C. Woo et al. </author> <title> The Performance Advantages of Integrating Block Data Transfer in Cache-Coherent Multiprocessors. </title> <booktitle> In ASPLOS-VI, </booktitle> <year> 1994. </year>
Reference-contexts: Coarse-grain producer-initiated (or bulk transfer) primitives are primarily useful for regular, coarse-grain data sharing patterns. In such cases, however, software prefetching is also highly effective and bulk transfer primitives have been shown to provide little additional performance benefit over prefetching for scientific codes <ref> [25] </ref>. In contrast, fine-grain producer-initiated primitives appear to be useful in certain cases where prefetching is inapplicable or insufficient.
Reference: [26] <author> S. C. Woo et al. </author> <title> The SPLASH-2 programs: Characterization and Methodological Considerations. </title> <booktitle> In 22nd ISCA, </booktitle> <year> 1995. </year> <month> 12 </month>
Reference-contexts: The two kernels are MCS locks [13] and tree barriers. We chose these kernels because they are of fundamental importance to shared-memory applications, and are well-suited to producer-initiated communication. The five applications are Radix from the SPLASH-2 suite <ref> [26] </ref>, Water and MP3D (without locking for cells) from the SPLASH suite [22], MICCG3D from the MIT Alewife group, and Erlebacher from the Rice Parallel Compiler group. The last two applications have static access patterns, and contain tightly synchronized pipelined loop nests. <p> MP3D and Water have dynamic access patterns, and Radix exhibits both types of patterns. Input sizes, cache sizes, and the number of processors used for each application are listed in Figure 2. The input and cache sizes are based on the methodology described by Woo et al. <ref> [26] </ref>. We use 32 processors for all applications except MP3D, for which we used 16 processors because of its poor scalability.
References-found: 26

