URL: ftp://ftp.cs.wisc.edu/sohi/papers/1996/toc.arb.ps.gz
Refering-URL: http://www.cs.wisc.edu/~sohi/sohi.html
Root-URL: 
Email: mfrankl@blessing.eng.clemson.edu sohi@cs.wisc.edu  
Title: ARB: A Hardware Mechanism for Dynamic Reordering of Memory References*  
Author: Manoj Franklin Gurindar S. Sohi 
Note: This work was supported by National Science Foundation grants CCR-8919635 and CCR-9410706 and by an IBM Graduate Fel lowship.  
Address: 221-C Riggs Hall 1210 West Dayton Street Clemson, SC 29634-0915, USA Madison, WI 53706, USA  
Affiliation: Department of Electrical and Computer Engineering Computer Sciences Department Clemson University University of Wisconsin-Madison  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> D. W. Anderson, F. J. Sparacio, and R. M. Tomasulo, </author> <title> ``The IBM System/360 Model 91: </title> <journal> Machine Philosophy and Instruction-Handling,'' IBM Journal of Research and Development, </journal> <pages> pp. 8-24, </pages> <month> January </month> <year> 1967. </year>
Reference-contexts: The store queue of the IBM 360/91 and its variants <ref> [1, 3, 12, 16] </ref> are examples of the above basic implementation. A tacit assumption in these techniques is that the ability to reorder store instructions to execute before preceding loads is not important 2 .
Reference: [2] <author> T. M. Austin and G. S. Sohi, </author> <title> ``Dynamic Dependency Analysis of Ordinary Programs,'' </title> <booktitle> Proc. 19th Annual International Symposium on Computer Architecture, </booktitle> <year> 1992. </year>
Reference-contexts: Memory renaming is analogous to register renaming; providing more physical storage allows more parallelism to be exploited <ref> [2] </ref>. However, if not used with caution, the memory renaming feature of the ARB could lead to untoward recovery actions because of loads inadvertently fetching incorrect values from the ARB when multiple stores to the same address are present in the active instruction window.
Reference: [3] <author> L. J. Boland, G. D. Granito, A. U. Marcotte, B. U. Messina, and J. W. Smith, </author> <title> ``The IBM System/360 Model 91: Storage System,'' </title> <journal> IBM Journal, </journal> <pages> pp. 54-68, </pages> <month> January </month> <year> 1967. </year>
Reference-contexts: The store queue of the IBM 360/91 and its variants <ref> [1, 3, 12, 16] </ref> are examples of the above basic implementation. A tacit assumption in these techniques is that the ability to reorder store instructions to execute before preceding loads is not important 2 .
Reference: [4] <author> J. R. Ellis, Bulldog: </author> <title> A Compiler for VLIW Architectures. </title> <publisher> The MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: The instruction scheduler must ensure that the reordering does not violate dependencies; this is done by determining if the reordered pair of references access the same memory location. The process of determining if two memory referencing instructions access the same memory location is called memory disambiguation or memory antialiasing <ref> [4] </ref>, and is a fundamental step in any scheme to reorder memory operations. 1.1. Need for Good Dynamic Disambiguation Developing an execution schedule, and therefore reordering of memory references, can be done statically by the compiler or dynamically by the hardware.
Reference: [5] <author> M. Franklin and G. S. Sohi, </author> <title> ``The Expandable Split Window Paradigm for Exploiting Fine-Grain Parallelism,'' </title> <booktitle> Proc. 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 58-67, </pages> <year> 1992. </year>
Reference-contexts: Section 5 presents the results of a simulation study that evaluates the performance of the ARB in a superscalar processor. Section 6 describes application of the two-level hierarchical ARB in the multiscalar processor [6, 18], the erstwhile Expandable Split Window (ESW) processor <ref> [5] </ref>. Section 7 provides a summary and draws the conclusions of this research. 2. BACKGROUND AND PREVIOUS WORK The first step in the process of dynamically reordering memory operations is the disambiguation step. Techniques for dynamic disambiguation use the following basic principle. <p> APPLICATION TO MULTISCALAR PROCESSOR The discussion of ARB in section 3 was based primarily on the superscalar processor as the underlying execution model. In this section, we demonstrate how the ARB can be used in a different execution model, namely the multiscalar model <ref> [5, 6, 18] </ref>. 6.1. Multiscalar Processor The multiscalar processor was earlier known as the ESW (Expandable Split Window) processor [5]. True to its name at inception, the multiscalar processor splits a large window of instructions into multiple tasks, and exploits parallelism by overlapping the execution of these tasks. <p> In this section, we demonstrate how the ARB can be used in a different execution model, namely the multiscalar model [5, 6, 18]. 6.1. Multiscalar Processor The multiscalar processor was earlier known as the ESW (Expandable Split Window) processor <ref> [5] </ref>. True to its name at inception, the multiscalar processor splits a large window of instructions into multiple tasks, and exploits parallelism by overlapping the execution of these tasks. <p> When all the instructions in the unit at the head have completed execution, the unit is committed, and the head pointer is moved forward to the next unit. Further details of the multiscalar processor can be had from <ref> [5, 6, 18] </ref>. 6.2. The Problem of Memory Reference Reordering in the Multiscalar Processor In a multiscalar processor, at any given time, many sequential tasks may have been initiated and executed in parallel, but not all instructions of these tasks may have been fetched at that time.
Reference: [6] <author> M. Franklin, </author> <title> ``The Multiscalar Architecture,'' </title> <type> Ph.D. Thesis, </type> <institution> Computer Sciences Department, University of Wisconsin-Madison, </institution> <year> 1993. </year> <type> Also Technical Report TR 1196, </type> <institution> Computer Sciences Department, University of Wisconsin-Madison, </institution> <year> 1993. </year>
Reference-contexts: Section 5 presents the results of a simulation study that evaluates the performance of the ARB in a superscalar processor. Section 6 describes application of the two-level hierarchical ARB in the multiscalar processor <ref> [6, 18] </ref>, the erstwhile Expandable Split Window (ESW) processor [5]. Section 7 provides a summary and draws the conclusions of this research. 2. BACKGROUND AND PREVIOUS WORK The first step in the process of dynamically reordering memory operations is the disambiguation step. <p> APPLICATION TO MULTISCALAR PROCESSOR The discussion of ARB in section 3 was based primarily on the superscalar processor as the underlying execution model. In this section, we demonstrate how the ARB can be used in a different execution model, namely the multiscalar model <ref> [5, 6, 18] </ref>. 6.1. Multiscalar Processor The multiscalar processor was earlier known as the ESW (Expandable Split Window) processor [5]. True to its name at inception, the multiscalar processor splits a large window of instructions into multiple tasks, and exploits parallelism by overlapping the execution of these tasks. <p> When all the instructions in the unit at the head have completed execution, the unit is committed, and the head pointer is moved forward to the next unit. Further details of the multiscalar processor can be had from <ref> [5, 6, 18] </ref>. 6.2. The Problem of Memory Reference Reordering in the Multiscalar Processor In a multiscalar processor, at any given time, many sequential tasks may have been initiated and executed in parallel, but not all instructions of these tasks may have been fetched at that time.
Reference: [7] <author> D. M. Gallagher, W. Y. Chen, S. A. Mahlke, J. C. Gyllenhaal, and W. W. Hwu, B. Heggy, and M. L. Soffa, </author> <title> ``Architectural Support for Register Allocation in the Presence of Aliasing,'' </title> <booktitle> Proc. Supercomputing '90, </booktitle> <pages> pp. 730-739, </pages> <month> November </month> <year> 1990. </year>
Reference-contexts: Memory disambiguation can also be performed statically, dynamically, or at both times, in an orthogonal manner. Static disambiguation techniques, however, have limitations which makes dynamic disambiguation attractive, either to complement static disambiguation or to work all by itself <ref> [7, 11, 14] </ref>. In statically scheduled processors, dynamic disambiguation is used to complement static disambiguation, and involves disambiguating only those ambiguous references that have been reordered at compile time. <p> Similarly, it should allow the execution of stores before disambiguating them against preceding loads and stores. (Note that compilers for statically scheduled processors that rely on run-time disambiguation allow statically unresolved memory references by reordering ambiguous memory references <ref> [7, 11, 14] </ref>. ) 1.4. Paper Objective and Organization The objective of this paper is to propose a new hardware mechanism for supporting memory operation reordering in an aggressive ILP processor. The proposed mechanism, called an Address Resolution Buffer (ARB), is very general, and is applicable to different execution models.
Reference: [8] <author> W. W. Hwu, </author> <title> ``Exploiting Concurrency to Achieve High Performance in a Single-chip Microarchitec-ture,'' </title> <type> Ph.D. Thesis, Report No. </type> <institution> UCB/CSD 88/398, Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, </institution> <year> 1988. </year>
Reference-contexts: For speculatively executed instructions, however, there is an additional phase, the commit phase. When it is known that an instruction that was executed speculatively was indeed meant to be executed, its effects can be committed, and the state of the machine updated <ref> [8, 9, 15, 17] </ref>. With speculative execution, memory operations need special treatment. A store operation can be allowed to proceed to the memory system only when it is guaranteed to commit, otherwise the old memory value will be lost, complicating the recovery procedures in case of an incorrect speculation.
Reference: [9] <author> M. Johnson, </author> <title> Superscalar Design. </title> <address> Englewood Cliffs, New Jersey: </address> <publisher> Prentice Hall, </publisher> <year> 1990. </year>
Reference-contexts: For speculatively executed instructions, however, there is an additional phase, the commit phase. When it is known that an instruction that was executed speculatively was indeed meant to be executed, its effects can be committed, and the state of the machine updated <ref> [8, 9, 15, 17] </ref>. With speculative execution, memory operations need special treatment. A store operation can be allowed to proceed to the memory system only when it is guaranteed to commit, otherwise the old memory value will be lost, complicating the recovery procedures in case of an incorrect speculation.
Reference: [10] <author> G. Kane, </author> <title> MIPS R2000 RISC Architecture. </title> <address> Englewood Cliffs, New Jersey: </address> <publisher> Prentice Hall, </publisher> <year> 1987. </year>
Reference-contexts: Accordingly, our evaluation is limited to a comparison of one ARB organization with equivalent organizations of existing solutions. 5.1. Experimental Framework Our methodology of experimentation is simulation. We have developed a superscalar simulator that uses the MIPS R2000 - R2010 instruction set and functional unit latencies <ref> [10] </ref>. This simulator accepts executable images of sequential programs (compiled for MIPS R2000-based machines), and simulates their execution, keeping track of relevant information on a cycle-by-cycle basis. It models speculative execution, and is not trace driven.
Reference: [11] <author> A. Nicolau, </author> <title> ``Run-Time Disambiguation: Coping With Statically Unpredictable Dependencies,'' </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 38, </volume> <pages> pp. 663-678, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: Memory disambiguation can also be performed statically, dynamically, or at both times, in an orthogonal manner. Static disambiguation techniques, however, have limitations which makes dynamic disambiguation attractive, either to complement static disambiguation or to work all by itself <ref> [7, 11, 14] </ref>. In statically scheduled processors, dynamic disambiguation is used to complement static disambiguation, and involves disambiguating only those ambiguous references that have been reordered at compile time. <p> Similarly, it should allow the execution of stores before disambiguating them against preceding loads and stores. (Note that compilers for statically scheduled processors that rely on run-time disambiguation allow statically unresolved memory references by reordering ambiguous memory references <ref> [7, 11, 14] </ref>. ) 1.4. Paper Objective and Organization The objective of this paper is to propose a new hardware mechanism for supporting memory operation reordering in an aggressive ILP processor. The proposed mechanism, called an Address Resolution Buffer (ARB), is very general, and is applicable to different execution models.
Reference: [12] <author> Y. N. Patt, S. W. Melvin, W. W. Hwu, and M. Shebanow, </author> <title> ``Critical Issues Regarding HPS, A High Performance Microarchitecture,'' </title> <booktitle> in Proc. 18th Annual Workshop on Microprogramming, </booktitle> <address> Pacific Grove, CA, </address> <pages> pp. 109-116, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: The store queue of the IBM 360/91 and its variants <ref> [1, 3, 12, 16] </ref> are examples of the above basic implementation. A tacit assumption in these techniques is that the ability to reorder store instructions to execute before preceding loads is not important 2 . <p> This reduces the amount of ILP that can be exploited. The dependency matrix of HPS partially addresses the unresolved references problem <ref> [12] </ref>. Here, an unknown address store operation does not stall issue; rather it is made to step aside (into the memory-dependency handling mechanism), allowing succeeding instructions to enter the active instruction window.
Reference: [13] <author> D. N. Pnevmatikatos, M. Franklin, and G. S. Sohi, </author> <title> ``Control Flow Prediction for Dynamic ILP Processors,'' </title> <booktitle> Proc. The 26th Annual International Symposium on Microarchitecture (MICRO-26), </booktitle> <pages> pp. 153-163, </pages> <year> 1993. </year>
Reference-contexts: These pointers are managed by a control unit, which also performs the function of assigning tasks to the execution units. Every cycle, the control unit assigns a new task to the tail unit (using control flow prediction <ref> [13] </ref> if required) unless the circular unit queue is full. The active units, the ones from the head to the tail, together constitute the large dynamic window of instructions, and they contain tasks in the sequential order in which the tasks appear in the dynamic instruction stream.
Reference: [14] <author> G. M. Silberman and K. Ebcioglu, </author> <title> ``An Architectural Framework for Migration from CISC to Higher Performance Platforms,'' </title> <booktitle> Proc. International Conference on Supercomputing, </booktitle> <pages> pp. 198-215, </pages> <year> 1992. </year>
Reference-contexts: Memory disambiguation can also be performed statically, dynamically, or at both times, in an orthogonal manner. Static disambiguation techniques, however, have limitations which makes dynamic disambiguation attractive, either to complement static disambiguation or to work all by itself <ref> [7, 11, 14] </ref>. In statically scheduled processors, dynamic disambiguation is used to complement static disambiguation, and involves disambiguating only those ambiguous references that have been reordered at compile time. <p> Similarly, it should allow the execution of stores before disambiguating them against preceding loads and stores. (Note that compilers for statically scheduled processors that rely on run-time disambiguation allow statically unresolved memory references by reordering ambiguous memory references <ref> [7, 11, 14] </ref>. ) 1.4. Paper Objective and Organization The objective of this paper is to propose a new hardware mechanism for supporting memory operation reordering in an aggressive ILP processor. The proposed mechanism, called an Address Resolution Buffer (ARB), is very general, and is applicable to different execution models.
Reference: [15] <author> J. E. Smith and A. R. Pleszkun, </author> <title> ``Implementing Precise Interrupts in Pipelined Processors,'' </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 37, </volume> <pages> pp. 562-573, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: For speculatively executed instructions, however, there is an additional phase, the commit phase. When it is known that an instruction that was executed speculatively was indeed meant to be executed, its effects can be committed, and the state of the machine updated <ref> [8, 9, 15, 17] </ref>. With speculative execution, memory operations need special treatment. A store operation can be allowed to proceed to the memory system only when it is guaranteed to commit, otherwise the old memory value will be lost, complicating the recovery procedures in case of an incorrect speculation.
Reference: [16] <author> J. E. Smith, </author> <title> ``Dynamic Instruction Scheduling and the Astronautics ZS-1,'' </title> <booktitle> IEEE Computer, </booktitle> <pages> pp. 21-35, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: The store queue of the IBM 360/91 and its variants <ref> [1, 3, 12, 16] </ref> are examples of the above basic implementation. A tacit assumption in these techniques is that the ability to reorder store instructions to execute before preceding loads is not important 2 .
Reference: [17] <author> G. S. Sohi, </author> <title> ``Instruction Issue Logic for High-Performance, Interruptible, Multiple Functional Unit, Pipelined Computers,'' </title> <journal> IEEE Trans. on Computers, </journal> <volume> vol. 39, </volume> <pages> pp. 349-359, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: For speculatively executed instructions, however, there is an additional phase, the commit phase. When it is known that an instruction that was executed speculatively was indeed meant to be executed, its effects can be committed, and the state of the machine updated <ref> [8, 9, 15, 17] </ref>. With speculative execution, memory operations need special treatment. A store operation can be allowed to proceed to the memory system only when it is guaranteed to commit, otherwise the old memory value will be lost, complicating the recovery procedures in case of an incorrect speculation.
Reference: [18] <author> G. S. Sohi, S. E. Breach, and T. N. Vijaykumar, </author> <title> ``Multiscalar Processors,'' </title> <booktitle> Proceedings of 22nd Annual International Symposium on Computer Architecture, </booktitle> <year> 1995. </year>
Reference-contexts: Section 5 presents the results of a simulation study that evaluates the performance of the ARB in a superscalar processor. Section 6 describes application of the two-level hierarchical ARB in the multiscalar processor <ref> [6, 18] </ref>, the erstwhile Expandable Split Window (ESW) processor [5]. Section 7 provides a summary and draws the conclusions of this research. 2. BACKGROUND AND PREVIOUS WORK The first step in the process of dynamically reordering memory operations is the disambiguation step. <p> APPLICATION TO MULTISCALAR PROCESSOR The discussion of ARB in section 3 was based primarily on the superscalar processor as the underlying execution model. In this section, we demonstrate how the ARB can be used in a different execution model, namely the multiscalar model <ref> [5, 6, 18] </ref>. 6.1. Multiscalar Processor The multiscalar processor was earlier known as the ESW (Expandable Split Window) processor [5]. True to its name at inception, the multiscalar processor splits a large window of instructions into multiple tasks, and exploits parallelism by overlapping the execution of these tasks. <p> When all the instructions in the unit at the head have completed execution, the unit is committed, and the head pointer is moved forward to the next unit. Further details of the multiscalar processor can be had from <ref> [5, 6, 18] </ref>. 6.2. The Problem of Memory Reference Reordering in the Multiscalar Processor In a multiscalar processor, at any given time, many sequential tasks may have been initiated and executed in parallel, but not all instructions of these tasks may have been fetched at that time.

References-found: 18

