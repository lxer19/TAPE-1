URL: ftp://ftp.cs.dartmouth.edu/pub/kotz/papers/kotz:flexibility2.ps.Z
Refering-URL: http://www.cs.dartmouth.edu/~dfk/papers/kotz:flexibility2.html
Root-URL: http://www.cs.dartmouth.edu
Email: fdfk,nilsg@cs.dartmouth.edu  
Title: Flexibility and Performance of Parallel File Systems  
Author: David Kotz and Nils Nieuwejaar 
Address: Hanover, NH 03755 USA  
Affiliation: Department of Computer Science Dartmouth College  
Web: URL ftp://ftp.cs.dartmouth.edu/pub/kotz/papers/kotz:flexibility2.ps.Z.  
Note: Copyright 1996 by Springer-Verlag. Appeared in the Third International Conference of the Austrian Center for Parallel Computation (ACPC), which was published in the Springer-Verlag Lecture Notes in Computer Science series. Available at  
Abstract: As we gain experience with parallel file systems, it becomes increasingly clear that a single solution does not suit all applications. For example, it appears to be impossible to find a single appropriate interface, caching policy, file structure, or disk-management strategy. Furthermore, the proliferation of file-system interfaces and abstractions make applications difficult to port. We propose that the traditional functionality of parallel file systems be separated into two components: a fixed core that is standard on all platforms, encapsulating only primitive abstractions and interfaces, and a set of high-level libraries to provide a variety of abstractions and application-programmer interfaces (APIs). We present our current and next-generation file systems as examples of this structure. Their features, such as a three-dimensional file structure, strided read and write interfaces, and I/O-node programs, are specifically designed with the flexibility and performance necessary to support a wide range of applications.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> B. Bershad, S. Savage, P. Pardyak, E. Gun Sirer, M. E. Fiuczynski, D. Becker, C. Chambers, and S. Eggers. </author> <title> Extensibility, safety and performance in the SPIN operating system. </title> <booktitle> In Proc. of the 15th ACM SOSP, </booktitle> <pages> pages 267-284, </pages> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: All three of these systems provide the application programmer some control over the parallel file system, primarily by selecting existing policies from the built-in alternatives. Galley2 promotes the use of application-selected code on the I/O nodes. Several operating systems can download user code into the kernel <ref> [14, 26, 1] </ref>. Other researchers have noted that it is useful to move the function to the data rather than to move the data to the function [3, 42, 17].
Reference: 2. <author> A. J. Borr and F. Putzolu. </author> <title> High performance SQL through low-level system integration. </title> <booktitle> In Proc. of the ACM SIGMOD Conf., </booktitle> <pages> pages 342-349, </pages> <year> 1988. </year>
Reference-contexts: Many more techniques for improving the performance of parallel file systems have been described, including caching and prefetching [24, 23, 34], two-phase I/O [10], disk-directed I/O [20], compute-node caching [37], chunking [40], compression [41], filtering <ref> [21, 2] </ref>, and so forth. The diversity of current systems and techniques indicates that there is clearly no consensus about the structure of, interface to, or even functionality of parallel file systems. <p> Incoming data can be filtered in a data-dependent way, passing only the necessary data on to the compute node, saving network bandwidth and compute-node memory <ref> [21, 2] </ref>. Blocks can be moved directly between I/O nodes, for example, to rearrange blocks between disks during a copy or permutation operation, without passing through compute nodes. Format conversion, compression, and decompression are also possible. <p> Other researchers have noted that it is useful to move the function to the data rather than to move the data to the function [3, 42, 17]. Some distributed database systems execute part of the SQL query in the server rather than the client, to reduce client-server traffic <ref> [2] </ref>. Hatcher and Quinn hint that allowing user code to run on nCUBE I/O nodes would be a good idea [18]. 7 Status Galley runs on the IBM SP-2 and on workstation clusters [31], and has so far been extremely successful [32].
Reference: 3. <author> J. B. Carter, J. K. Bennett, and W. Zwaenepoel. </author> <title> Techniques for reducing consistency-related communication in distributed shared-memory systems. </title> <journal> ACM TOCS, </journal> <volume> 13(3) </volume> <pages> 205-243, </pages> <month> Aug. </month> <year> 1995. </year>
Reference-contexts: Galley2 promotes the use of application-selected code on the I/O nodes. Several operating systems can download user code into the kernel [14, 26, 1]. Other researchers have noted that it is useful to move the function to the data rather than to move the data to the function <ref> [3, 42, 17] </ref>. Some distributed database systems execute part of the SQL query in the server rather than the client, to reduce client-server traffic [2].
Reference: 4. <author> A. Choudhary, R. Bordawekar, M. Harry, R. Krishnaiyer, R. Ponnusamy, T. Singh, and R. Thakur. </author> <title> PASSION: parallel and scalable software for input-output. </title> <type> Technical Report SCCS-636, </type> <institution> ECE Dept., NPAC and CASE Center, Syracuse University, </institution> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: Some languages or libraries would provide a traditional read-write abstraction; others (probably with compiler support) would provide transparent out-of-core data structures; still others may provide persistent objects. Some libraries may be designed for particular application classes like computational chemistry [13] or to support a particular language <ref> [7, 4] </ref>. Finally, some compilers and programmers may choose to generate application-specific code using the core interface directly. The concept of I/O libraries is not new; the C stdio library and the C++ iostreams library are common examples, both layered above the "core" kernel interface.
Reference: 5. <author> P. Corbett, D. Feitelson, Y. Hsu, J.-P. Prost, M. Snir, S. Fineberg, B. Nitzberg, B. Traversat, and P. Wong. </author> <title> MPI-IO: a parallel file I/O interface for MPI. </title> <type> Technical Report NAS-95-002, </type> <institution> NASA Ames Research Center, </institution> <month> Jan. </month> <year> 1995. </year> <note> Version 0.3. </note>
Reference-contexts: This diversity of current systems, particularly of the application-programmer's interface (API), also makes it difficult to write portable applications. Nearly every file system mentioned above has its own API. A standard interface is being developed, MPI-IO <ref> [5] </ref>, but even that interface is appropriate only for a certain class of applications. 2 Solution We believe that flexibility is needed for performance. An application programmer should be able to choose the interfaces and abstractions that work best for that application.
Reference: 6. <author> P. F. Corbett, D. G. Feitelson, J.-P. Prost, G. S. Almasi, S. J. Baylor, A. S. Bol-marcich, Y. Hsu, J. Satran, M. Snir, R. Colao, B. Herr, J. Kavaky, T. R. Morgan, and A. Zlotek. </author> <title> Parallel file systems for the IBM SP computers. </title> <journal> IBM Sys. Journal, </journal> <volume> 34(2) </volume> <pages> 222-248, </pages> <month> Jan. </month> <year> 1995. </year>
Reference-contexts: Nodes with attached disks are usually reserved as I/O nodes, while applications run on some cluster of the remaining compute nodes. In the past few years, many parallel file systems have been described in the literature, including Bridge/PFS [12], CFS [35], nCUBE [9], OSF/PFS [38], sfs [27], Vesta/PIOFS <ref> [6] </ref>, HFS [25], PIOUS [30], RAMA [29], PPFS [19], Scotch [15], and Galley [31, 32]. <p> We have ported several application libraries on top of Galley, including a traditional striped-file library, Panda [39, 43], Vesta <ref> [6] </ref>, and SOLAR [44]. We are also using Galley to investigate policies for managing multi-application workloads. We are building a simulator for Galley2, to evaluate some of the key ideas, and a full implementation, to experiment with real applications.
Reference: 7. <author> T. H. Cormen and A. Colvin. </author> <title> ViC*: A preprocessor for virtual-memory C*. </title> <type> Technical Report PCS-TR94-243, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: Some languages or libraries would provide a traditional read-write abstraction; others (probably with compiler support) would provide transparent out-of-core data structures; still others may provide persistent objects. Some libraries may be designed for particular application classes like computational chemistry [13] or to support a particular language <ref> [7, 4] </ref>. Finally, some compilers and programmers may choose to generate application-specific code using the core interface directly. The concept of I/O libraries is not new; the C stdio library and the C++ iostreams library are common examples, both layered above the "core" kernel interface.
Reference: 8. <author> T. H. Cormen and D. Kotz. </author> <title> Integrating theory and practice in parallel file systems. </title> <booktitle> In Proc. of the 1993 DAGS/PC Symposium, </booktitle> <pages> pages 64-74, </pages> <address> Hanover, NH, </address> <month> June </month> <year> 1993. </year> <institution> Dartmouth Inst. for Adv. Graduate Studies. </institution> <note> Revised as Dartmouth PCS-TR93-188 on 9/20/94. </note>
Reference-contexts: It is important that applications be able to choose the interface and policies that work best for them, and for application programmers to have control over I/O <ref> [46, 8] </ref>. This diversity of current systems, particularly of the application-programmer's interface (API), also makes it difficult to write portable applications. Nearly every file system mentioned above has its own API.
Reference: 9. <author> E. DeBenedictis and J. M. del Rosario. </author> <title> nCUBE parallel I/O software. </title> <booktitle> In Proc. of the 11th IPCCC, </booktitle> <pages> pages 0117-0124, </pages> <month> Apr. </month> <year> 1992. </year>
Reference-contexts: Nodes with attached disks are usually reserved as I/O nodes, while applications run on some cluster of the remaining compute nodes. In the past few years, many parallel file systems have been described in the literature, including Bridge/PFS [12], CFS [35], nCUBE <ref> [9] </ref>, OSF/PFS [38], sfs [27], Vesta/PIOFS [6], HFS [25], PIOUS [30], RAMA [29], PPFS [19], Scotch [15], and Galley [31, 32].
Reference: 10. <author> J. M. del Rosario, R. Bordawekar, and A. Choudhary. </author> <title> Improved parallel I/O via a two-phase run-time access strategy. </title> <booktitle> In IPPS '93 Workshop on I/O in Par. Comp. Sys., </booktitle> <pages> pages 56-70, </pages> <year> 1993. </year> <note> Also published in Computer Architecture News 21(5), </note> <month> December </month> <year> 1993, </year> <pages> pages 31-38. </pages>
Reference-contexts: Many more techniques for improving the performance of parallel file systems have been described, including caching and prefetching [24, 23, 34], two-phase I/O <ref> [10] </ref>, disk-directed I/O [20], compute-node caching [37], chunking [40], compression [41], filtering [21, 2], and so forth. The diversity of current systems and techniques indicates that there is clearly no consensus about the structure of, interface to, or even functionality of parallel file systems.
Reference: 11. <author> J. M. del Rosario and A. Choudhary. </author> <title> High performance I/O for parallel computers: Problems and prospects. </title> <journal> IEEE Computer, </journal> <volume> 27(3) </volume> <pages> 59-68, </pages> <month> Mar. </month> <year> 1994. </year>
Reference-contexts: 1 Introduction Scientific applications are increasingly dependent on multiprocessor computers to satisfy their computational needs. Many scientific applications, however, also use tremendous amounts of data <ref> [11] </ref>: input data collected from satellites or seismic experiments, checkpointing output, and visualization output. Worse, some applications manipulate data sets too large to fit in main memory, requiring either explicit or implicit virtual memory support.
Reference: 12. <author> P. C. Dibble. </author> <title> A Parallel Interleaved File System. </title> <type> PhD thesis, </type> <institution> University of Rochester, </institution> <month> Mar. </month> <year> 1990. </year>
Reference-contexts: Nodes with attached disks are usually reserved as I/O nodes, while applications run on some cluster of the remaining compute nodes. In the past few years, many parallel file systems have been described in the literature, including Bridge/PFS <ref> [12] </ref>, CFS [35], nCUBE [9], OSF/PFS [38], sfs [27], Vesta/PIOFS [6], HFS [25], PIOUS [30], RAMA [29], PPFS [19], Scotch [15], and Galley [31, 32].
Reference: 13. <author> I. Foster and J. Nieplocha. </author> <title> ChemIO: High-performance I/O for computational chemistry applications. </title> <address> WWW http://www.mcs.anl.gov/chemio/, Feb. </address> <year> 1996. </year>
Reference-contexts: Some languages or libraries would provide a traditional read-write abstraction; others (probably with compiler support) would provide transparent out-of-core data structures; still others may provide persistent objects. Some libraries may be designed for particular application classes like computational chemistry <ref> [13] </ref> or to support a particular language [7, 4]. Finally, some compilers and programmers may choose to generate application-specific code using the core interface directly.
Reference: 14. <author> R. S. Gaines. </author> <title> An operating system based on the concept of a supervisory computer. </title> <journal> Comm. of the ACM, </journal> <volume> 15(3) </volume> <pages> 150-156, </pages> <month> Mar. </month> <year> 1972. </year>
Reference-contexts: All three of these systems provide the application programmer some control over the parallel file system, primarily by selecting existing policies from the built-in alternatives. Galley2 promotes the use of application-selected code on the I/O nodes. Several operating systems can download user code into the kernel <ref> [14, 26, 1] </ref>. Other researchers have noted that it is useful to move the function to the data rather than to move the data to the function [3, 42, 17].
Reference: 15. <author> G. A. Gibson, D. Stodolsky, P. W. Chang, W. V. Courtright II, C. G. Demetriou, E. Ginting, M. Holland, Q. Ma, L. Neal, R. H. Patterson, J. Su, R. Youssef, and J. Zelenka. </author> <title> The Scotch parallel storage systems. </title> <booktitle> In Proc. of 40th IEEE Computer Society International Conference (COMPCON 95), </booktitle> <pages> pages 403-410, </pages> <address> San Francisco, </address> <month> Spring </month> <year> 1995. </year>
Reference-contexts: In the past few years, many parallel file systems have been described in the literature, including Bridge/PFS [12], CFS [35], nCUBE [9], OSF/PFS [38], sfs [27], Vesta/PIOFS [6], HFS [25], PIOUS [30], RAMA [29], PPFS [19], Scotch <ref> [15] </ref>, and Galley [31, 32]. Many more techniques for improving the performance of parallel file systems have been described, including caching and prefetching [24, 23, 34], two-phase I/O [10], disk-directed I/O [20], compute-node caching [37], chunking [40], compression [41], filtering [21, 2], and so forth.
Reference: 16. <author> J. Gosling and H. McGilton. </author> <title> The Java language: A white paper. Sun Microsys-tems, </title> <year> 1994. </year>
Reference-contexts: for requesting I/O to and from buffers? - message-passing: what is the best interface for I/O-node programs to com municate with the compute nodes, and with each other? What is the appropriate mechanism to support I/O-node programs? We are considering three alternatives: processes, threads within a safe language like Java <ref> [16] </ref> or Python 3 , and threads running sandboxed code [45].
Reference: 17. <author> R. S. Gray. </author> <title> Agent Tcl: A transportable agent system. </title> <booktitle> In Proceedings of the CIKM Workshop on Intelligent Information Agents, Fourth International Conference on Information and Knowledge Management (CIKM 95), </booktitle> <address> Baltimore, Maryland, </address> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: Galley2 promotes the use of application-selected code on the I/O nodes. Several operating systems can download user code into the kernel [14, 26, 1]. Other researchers have noted that it is useful to move the function to the data rather than to move the data to the function <ref> [3, 42, 17] </ref>. Some distributed database systems execute part of the SQL query in the server rather than the client, to reduce client-server traffic [2].
Reference: 18. <author> P. J. Hatcher and M. J. Quinn. C*-Linda: </author> <title> A programming environment with multiple data-parallel modules and parallel I/O. </title> <booktitle> In Proc. of the 24th HICSS, </booktitle> <pages> pages 382-389, </pages> <year> 1991. </year>
Reference-contexts: Some distributed database systems execute part of the SQL query in the server rather than the client, to reduce client-server traffic [2]. Hatcher and Quinn hint that allowing user code to run on nCUBE I/O nodes would be a good idea <ref> [18] </ref>. 7 Status Galley runs on the IBM SP-2 and on workstation clusters [31], and has so far been extremely successful [32]. We have ported several application libraries on top of Galley, including a traditional striped-file library, Panda [39, 43], Vesta [6], and SOLAR [44].
Reference: 19. <author> J. Huber, C. L. Elford, D. A. Reed, A. A. Chien, and D. S. Blumenthal. </author> <title> PPFS: A high performance portable parallel file system. </title> <booktitle> In Proc. of the 9th ACM Int'l Conf. on Supercomp., </booktitle> <pages> pages 385-394, </pages> <address> Barcelona, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: In the past few years, many parallel file systems have been described in the literature, including Bridge/PFS [12], CFS [35], nCUBE [9], OSF/PFS [38], sfs [27], Vesta/PIOFS [6], HFS [25], PIOUS [30], RAMA [29], PPFS <ref> [19] </ref>, Scotch [15], and Galley [31, 32]. Many more techniques for improving the performance of parallel file systems have been described, including caching and prefetching [24, 23, 34], two-phase I/O [10], disk-directed I/O [20], compute-node caching [37], chunking [40], compression [41], filtering [21, 2], and so forth. <p> Galley permits, but does not enforce, a building-block approach to library design; other approaches are possible. Finally, the Hurricane operating system does not dedicate nodes to I/O, so it is not unusual for application code to run on "I/O" nodes. The Portable Parallel File System (PPFS) <ref> [19] </ref> is a testbed for experimenting with parallel file-system issues. It includes many alternative policies for declus-tering, caching, prefetching, and consistency control, and allows application programmers to select appropriate policies for their needs. It also supports user-defined declustering patterns through an upcall function.
Reference: 20. <author> D. Kotz. </author> <title> Disk-directed I/O for MIMD multiprocessors. </title> <booktitle> In Proc. of the 1994 Symp. on OS Design and Impl., </booktitle> <pages> pages 61-74, </pages> <month> Nov. </month> <year> 1994. </year> <note> Updated as Dartmouth TR PCS-TR94-226 on November 8, </note> <year> 1994. </year>
Reference-contexts: Many more techniques for improving the performance of parallel file systems have been described, including caching and prefetching [24, 23, 34], two-phase I/O [10], disk-directed I/O <ref> [20] </ref>, compute-node caching [37], chunking [40], compression [41], filtering [21, 2], and so forth. The diversity of current systems and techniques indicates that there is clearly no consensus about the structure of, interface to, or even functionality of parallel file systems. <p> The I/O-node servers shrink down to simple I/O managers that arbitrate resources among the local user-selected library modules. There are many reasons to allow application-selected code on the I/O node. Application-specific optimizations can be applied to I/O-node caching and prefetch-ing. Mechanisms like disk-directed I/O <ref> [20] </ref> can be implemented, using application-specific data-distribution information. File data can be distributed among memories according to a data-dependent mapping function, for example, in applications with a data-dependent decomposition of unstructured data [21].
Reference: 21. <author> D. Kotz. </author> <title> Expanding the potential for disk-directed I/O. </title> <booktitle> In Proc. of the 1995 IEEE SPDP, </booktitle> <pages> pages 490-495, </pages> <month> Oct. </month> <year> 1995. </year>
Reference-contexts: Many more techniques for improving the performance of parallel file systems have been described, including caching and prefetching [24, 23, 34], two-phase I/O [10], disk-directed I/O [20], compute-node caching [37], chunking [40], compression [41], filtering <ref> [21, 2] </ref>, and so forth. The diversity of current systems and techniques indicates that there is clearly no consensus about the structure of, interface to, or even functionality of parallel file systems. <p> Application-specific optimizations can be applied to I/O-node caching and prefetch-ing. Mechanisms like disk-directed I/O [20] can be implemented, using application-specific data-distribution information. File data can be distributed among memories according to a data-dependent mapping function, for example, in applications with a data-dependent decomposition of unstructured data <ref> [21] </ref>. Incoming data can be filtered in a data-dependent way, passing only the necessary data on to the compute node, saving network bandwidth and compute-node memory [21, 2]. <p> Incoming data can be filtered in a data-dependent way, passing only the necessary data on to the compute node, saving network bandwidth and compute-node memory <ref> [21, 2] </ref>. Blocks can be moved directly between I/O nodes, for example, to rearrange blocks between disks during a copy or permutation operation, without passing through compute nodes. Format conversion, compression, and decompression are also possible.
Reference: 22. <author> D. Kotz. </author> <title> Introduction to multiprocessor I/O architecture. </title> <editor> In R. Jain, J. Werth, and J. C. Browne, editors, </editor> <booktitle> Input/Output in Parallel and Distributed Computer Systems, chapter 4, </booktitle> <pages> pages 97-123. </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1996. </year>
Reference-contexts: The I/O system becomes the bottleneck in all of these applications, a bottleneck that is worsening as processor speeds continue to improve more rapidly than disk speeds. Fortunately, it is now possible to configure most parallel systems with sufficient I/O hardware <ref> [22] </ref>. Most of today's parallel computers interconnect tens This research was funded by NSF under grant number CCR-9404919 and by NASA Ames under agreement numbers NCC 2-849 and NAG 2-936. This paper appeared previously in ACM Operating Systems Review 30 (2), April 1996, pp. 63-73.
Reference: 23. <author> D. Kotz and C. S. Ellis. </author> <title> Caching and writeback policies in parallel file systems. </title> <journal> J. of Par. and Dist. Comp., </journal> <note> 17(1-2):140-145, January and February 1993. </note>
Reference-contexts: Many more techniques for improving the performance of parallel file systems have been described, including caching and prefetching <ref> [24, 23, 34] </ref>, two-phase I/O [10], disk-directed I/O [20], compute-node caching [37], chunking [40], compression [41], filtering [21, 2], and so forth. The diversity of current systems and techniques indicates that there is clearly no consensus about the structure of, interface to, or even functionality of parallel file systems.
Reference: 24. <author> D. Kotz and C. S. Ellis. </author> <title> Practical prefetching techniques for multiprocessor file systems. </title> <journal> J. of Dist. and Par. Databases, </journal> <volume> 1(1) </volume> <pages> 33-51, </pages> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: Many more techniques for improving the performance of parallel file systems have been described, including caching and prefetching <ref> [24, 23, 34] </ref>, two-phase I/O [10], disk-directed I/O [20], compute-node caching [37], chunking [40], compression [41], filtering [21, 2], and so forth. The diversity of current systems and techniques indicates that there is clearly no consensus about the structure of, interface to, or even functionality of parallel file systems.
Reference: 25. <author> O. Krieger and M. Stumm. </author> <title> HFS: A performance-oriented flexible file system based on building-block compositions. </title> <booktitle> In 4th Workshop on I/O in Par. and Dist. Sys., </booktitle> <pages> pages 95-108, </pages> <address> Philadelphia, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: In the past few years, many parallel file systems have been described in the literature, including Bridge/PFS [12], CFS [35], nCUBE [9], OSF/PFS [38], sfs [27], Vesta/PIOFS [6], HFS <ref> [25] </ref>, PIOUS [30], RAMA [29], PPFS [19], Scotch [15], and Galley [31, 32]. <p> Indeed, it seems that no one interface or structure will be appropriate for all parallel applications; for maximum performance, flexibility of the underlying system is critical <ref> [25] </ref>. It is important that applications be able to choose the interface and policies that work best for them, and for application programmers to have control over I/O [46, 8]. This diversity of current systems, particularly of the application-programmer's interface (API), also makes it difficult to write portable applications. <p> The tricky part might be dynamic linking of sandboxed code. 3. what is the overhead? 6 Related work The Hurricane File System (HFS) <ref> [25] </ref>, a parallel file system for the Hector multiprocessor, is also designed with the philosophy that flexibility is critical for performance. Indeed, their results clearly demonstrate the tremendous performance impact of choosing the right file structure and management policies for the application's access pattern.
Reference: 26. <author> C. H. Lee, M. C. Chen, and R. C. Chang. </author> <title> HiPEC: High performance external virtual memory caching. </title> <booktitle> In Proc. of the 1994 Symp. on OS Design and Impl., </booktitle> <pages> pages 153-164, </pages> <year> 1994. </year>
Reference-contexts: All three of these systems provide the application programmer some control over the parallel file system, primarily by selecting existing policies from the built-in alternatives. Galley2 promotes the use of application-selected code on the I/O nodes. Several operating systems can download user code into the kernel <ref> [14, 26, 1] </ref>. Other researchers have noted that it is useful to move the function to the data rather than to move the data to the function [3, 42, 17].
Reference: 27. <author> S. J. LoVerso, M. Isman, A. Nanopoulos, W. Nesheim, E. D. Milne, and R. Wheeler. sfs: </author> <title> A parallel file system for the CM-5. </title> <booktitle> In Proc. of the 1993 Summer USENIX Conf., </booktitle> <pages> pages 291-305, </pages> <year> 1993. </year>
Reference-contexts: Nodes with attached disks are usually reserved as I/O nodes, while applications run on some cluster of the remaining compute nodes. In the past few years, many parallel file systems have been described in the literature, including Bridge/PFS [12], CFS [35], nCUBE [9], OSF/PFS [38], sfs <ref> [27] </ref>, Vesta/PIOFS [6], HFS [25], PIOUS [30], RAMA [29], PPFS [19], Scotch [15], and Galley [31, 32].
Reference: 28. <author> Message Passing Interface Forum. </author> <title> MPI: A Message-Passing Interface Standard, </title> <address> 1.0 edition, </address> <month> May 5 </month> <year> 1994. </year> <note> http://www.mcs.anl.gov/Projects/mpi/standard.html. </note>
Reference-contexts: While the implementation of the core is platform dependent, and provided by the platform vendor, its interface is standard across all platforms. This approach has proven successful with the MPI message-passing standard <ref> [28] </ref>. 1 We avoid the term "kernel," as the core may be comprised of user-level libraries, server daemons, and kernel code. Fig. 1. Our proposed evolution of parallel file-system structure. Traditional systems depend on a fixed "core" file system that attempts to serve all applications.
Reference: 29. <author> E. L. Miller and R. H. Katz. </author> <title> RAMA: Easy access to a high-bandwidth massively parallel file system. </title> <booktitle> In Proc. of the 1995 Winter USENIX Conf., </booktitle> <pages> pages 59-70, </pages> <month> Jan. </month> <year> 1995. </year>
Reference-contexts: In the past few years, many parallel file systems have been described in the literature, including Bridge/PFS [12], CFS [35], nCUBE [9], OSF/PFS [38], sfs [27], Vesta/PIOFS [6], HFS [25], PIOUS [30], RAMA <ref> [29] </ref>, PPFS [19], Scotch [15], and Galley [31, 32]. Many more techniques for improving the performance of parallel file systems have been described, including caching and prefetching [24, 23, 34], two-phase I/O [10], disk-directed I/O [20], compute-node caching [37], chunking [40], compression [41], filtering [21, 2], and so forth.
Reference: 30. <author> S. A. Moyer and V. S. Sunderam. </author> <title> PIOUS: a scalable parallel I/O system for distributed computing environments. </title> <booktitle> In Proc. of the Scalable High-Perf. Comp. Conf., </booktitle> <pages> pages 71-78, </pages> <year> 1994. </year>
Reference-contexts: In the past few years, many parallel file systems have been described in the literature, including Bridge/PFS [12], CFS [35], nCUBE [9], OSF/PFS [38], sfs [27], Vesta/PIOFS [6], HFS [25], PIOUS <ref> [30] </ref>, RAMA [29], PPFS [19], Scotch [15], and Galley [31, 32].
Reference: 31. <author> N. Nieuwejaar and D. Kotz. </author> <title> The Galley parallel file system. </title> <booktitle> In Proc. of the 10th ACM Int'l Conf. on Supercomp., </booktitle> <pages> pages 374-381, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: In the past few years, many parallel file systems have been described in the literature, including Bridge/PFS [12], CFS [35], nCUBE [9], OSF/PFS [38], sfs [27], Vesta/PIOFS [6], HFS [25], PIOUS [30], RAMA [29], PPFS [19], Scotch [15], and Galley <ref> [31, 32] </ref>. Many more techniques for improving the performance of parallel file systems have been described, including caching and prefetching [24, 23, 34], two-phase I/O [10], disk-directed I/O [20], compute-node caching [37], chunking [40], compression [41], filtering [21, 2], and so forth. <p> In the second, with the tentative name Galley2, we go a step further and allow user code to run on the I/O nodes. The next two sections discuss each file system in more detail. 3 The Galley Parallel File System Our current parallel file system, Galley <ref> [31, 32] </ref>, looks like Figure 1b. A more detailed picture is shown in Figure 2. The core file system includes servers that run on the I/O nodes and a tiny interface library that runs on the compute nodes. The I/O-node servers manage file-system metadata, I/O-node caching, and disk scheduling. <p> More information about Galley is available on the WWW 2 and in forthcoming papers <ref> [31, 32] </ref>. 4 The Galley2 Parallel File System Our next-generation file system, which we so far call "Galley2" for lack of a better name, goes beyond Galley to allow application control over I/O-node activities. <p> Hatcher and Quinn hint that allowing user code to run on nCUBE I/O nodes would be a good idea [18]. 7 Status Galley runs on the IBM SP-2 and on workstation clusters <ref> [31] </ref>, and has so far been extremely successful [32]. We have ported several application libraries on top of Galley, including a traditional striped-file library, Panda [39, 43], Vesta [6], and SOLAR [44]. We are also using Galley to investigate policies for managing multi-application workloads.
Reference: 32. <author> N. Nieuwejaar and D. Kotz. </author> <title> Performance of the Galley parallel file system. </title> <booktitle> In 4th Workshop on I/O in Par. and Dist. Sys., </booktitle> <pages> pages 83-94, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: In the past few years, many parallel file systems have been described in the literature, including Bridge/PFS [12], CFS [35], nCUBE [9], OSF/PFS [38], sfs [27], Vesta/PIOFS [6], HFS [25], PIOUS [30], RAMA [29], PPFS [19], Scotch [15], and Galley <ref> [31, 32] </ref>. Many more techniques for improving the performance of parallel file systems have been described, including caching and prefetching [24, 23, 34], two-phase I/O [10], disk-directed I/O [20], compute-node caching [37], chunking [40], compression [41], filtering [21, 2], and so forth. <p> In the second, with the tentative name Galley2, we go a step further and allow user code to run on the I/O nodes. The next two sections discuss each file system in more detail. 3 The Galley Parallel File System Our current parallel file system, Galley <ref> [31, 32] </ref>, looks like Figure 1b. A more detailed picture is shown in Figure 2. The core file system includes servers that run on the I/O nodes and a tiny interface library that runs on the compute nodes. The I/O-node servers manage file-system metadata, I/O-node caching, and disk scheduling. <p> To allow application libraries to support these patterns efficiently, the Galley interface supports both structured (e.g., strided and nested strided) and unstructured read and write requests. This interface leads to dramatically better performance <ref> [32] </ref>. Galley's features, including the global name space, three-dimensional file structure, and structured read and write requests, make it a suitable and efficient base for constructing parallel file systems, much more so than building directly on distributed Unix systems. <p> More information about Galley is available on the WWW 2 and in forthcoming papers <ref> [31, 32] </ref>. 4 The Galley2 Parallel File System Our next-generation file system, which we so far call "Galley2" for lack of a better name, goes beyond Galley to allow application control over I/O-node activities. <p> Hatcher and Quinn hint that allowing user code to run on nCUBE I/O nodes would be a good idea [18]. 7 Status Galley runs on the IBM SP-2 and on workstation clusters [31], and has so far been extremely successful <ref> [32] </ref>. We have ported several application libraries on top of Galley, including a traditional striped-file library, Panda [39, 43], Vesta [6], and SOLAR [44]. We are also using Galley to investigate policies for managing multi-application workloads.
Reference: 33. <author> N. Nieuwejaar, D. Kotz, A. Purakayastha, C. S. Ellis, and M. </author> <title> Best. File-access characteristics of parallel scientific workloads. </title> <type> Technical Report PCS-TR95-263, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> Aug. </month> <year> 1995. </year> <note> To appear in IEEE TPDS. </note>
Reference-contexts: The structure of parallel files, beyond the fact that they are collections of local files, is completely determined by library code. Multiple applications wishing to use the same parallel files must maintain a mutually agreed structure, by convention. In an extensive characterization of parallel scientific applications <ref> [33] </ref>, we found that many applications access files in small pieces, typically in a regular "strided" pattern. To allow application libraries to support these patterns efficiently, the Galley interface supports both structured (e.g., strided and nested strided) and unstructured read and write requests.
Reference: 34. <author> R. H. Patterson, G. A. Gibson, E. Ginting, D. Stodolsky, and J. Zelenka. </author> <title> Informed prefetching and caching. </title> <booktitle> In Proc. of the 15th ACM SOSP, </booktitle> <pages> pages 79-95, </pages> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: Many more techniques for improving the performance of parallel file systems have been described, including caching and prefetching <ref> [24, 23, 34] </ref>, two-phase I/O [10], disk-directed I/O [20], compute-node caching [37], chunking [40], compression [41], filtering [21, 2], and so forth. The diversity of current systems and techniques indicates that there is clearly no consensus about the structure of, interface to, or even functionality of parallel file systems. <p> Unlike Galley, however, there is no clearly defined lower-level interface to which programmers may write new high-level libraries. Unlike Galley2, it does not allow application-selected code (beyond that already included in PPFS) to execute on the I/O nodes. In the Transparent Informed Prefetching (TIP) system <ref> [34] </ref> an application provides a set of hints about its future accesses to the file system. The file system uses these hints to make intelligent caching and prefetching decisions. While this technique can lead to better performance through better prefetching, it only affects prefetching and caching behavior.
Reference: 35. <author> P. Pierce. </author> <title> A concurrent file system for a highly parallel mass storage system. </title> <booktitle> In Proc. of the Fourth Conf. on Hypercube Concurrent Comp. and Appl., </booktitle> <pages> pages 155-160. </pages> <publisher> Golden Gate Enterprises, </publisher> <address> Los Altos, CA, </address> <month> Mar. </month> <year> 1989. </year>
Reference-contexts: Nodes with attached disks are usually reserved as I/O nodes, while applications run on some cluster of the remaining compute nodes. In the past few years, many parallel file systems have been described in the literature, including Bridge/PFS [12], CFS <ref> [35] </ref>, nCUBE [9], OSF/PFS [38], sfs [27], Vesta/PIOFS [6], HFS [25], PIOUS [30], RAMA [29], PPFS [19], Scotch [15], and Galley [31, 32].
Reference: 36. <author> C. Pu, T. Autrey, A. Black, C. Consel, C. Cowan, J. Inouye, L. Kethana, J. Walpole, and K. Zhang. </author> <title> Optimistic incremental specialization: Streamlining a commercial operating system. </title> <booktitle> In Proc. of the 15th ACM SOSP, </booktitle> <pages> pages 314-324, </pages> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: Again, we expect most applications to choose from pre-defined libraries, but we also encourage use of application-specific code written by application programmers, generated automatically by compilers, or generated at run time <ref> [36] </ref>. We refer to all of these choices as "application-selected code." 2 http://www.cs.dartmouth.edu/~nils/galley.html Fig. 3. The structure of the Galley2 parallel file system depends on application I/O libraries that have components on both the compute and I/O nodes.
Reference: 37. <author> A. Purakayastha, C. S. Ellis, and D. Kotz. </author> <title> ENWRICH: a compute-processor write caching scheme for parallel file systems. </title> <booktitle> In 4th Workshop on I/O in Par. and Dist. Sys., </booktitle> <pages> pages 55-68, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Many more techniques for improving the performance of parallel file systems have been described, including caching and prefetching [24, 23, 34], two-phase I/O [10], disk-directed I/O [20], compute-node caching <ref> [37] </ref>, chunking [40], compression [41], filtering [21, 2], and so forth. The diversity of current systems and techniques indicates that there is clearly no consensus about the structure of, interface to, or even functionality of parallel file systems.
Reference: 38. <author> P. J. Roy. </author> <title> Unix file access and caching in a multicomputer environment. </title> <booktitle> In Proc. of the Usenix Mach III Symposium, </booktitle> <pages> pages 21-37, </pages> <year> 1993. </year>
Reference-contexts: Nodes with attached disks are usually reserved as I/O nodes, while applications run on some cluster of the remaining compute nodes. In the past few years, many parallel file systems have been described in the literature, including Bridge/PFS [12], CFS [35], nCUBE [9], OSF/PFS <ref> [38] </ref>, sfs [27], Vesta/PIOFS [6], HFS [25], PIOUS [30], RAMA [29], PPFS [19], Scotch [15], and Galley [31, 32].
Reference: 39. <author> K. E. Seamons, Y. Chen, P. Jones, J. Jozwiak, and M. Winslett. </author> <title> Server-directed collective I/O in Panda. </title> <booktitle> In Proc. of Supercomp. </booktitle> <volume> '95, </volume> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: We have ported several application libraries on top of Galley, including a traditional striped-file library, Panda <ref> [39, 43] </ref>, Vesta [6], and SOLAR [44]. We are also using Galley to investigate policies for managing multi-application workloads. We are building a simulator for Galley2, to evaluate some of the key ideas, and a full implementation, to experiment with real applications.
Reference: 40. <author> K. E. Seamons and M. Winslett. </author> <title> An efficient abstract interface for multidimensional array I/O. </title> <booktitle> In Proc. of Supercomp. </booktitle> <volume> '94, </volume> <pages> pages 650-659, </pages> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: Many more techniques for improving the performance of parallel file systems have been described, including caching and prefetching [24, 23, 34], two-phase I/O [10], disk-directed I/O [20], compute-node caching [37], chunking <ref> [40] </ref>, compression [41], filtering [21, 2], and so forth. The diversity of current systems and techniques indicates that there is clearly no consensus about the structure of, interface to, or even functionality of parallel file systems.
Reference: 41. <author> K. E. Seamons and M. Winslett. </author> <title> A data management approach for handling large compressed arrays in high performance computing. </title> <booktitle> In Proc. of the 5th Symp. on the Frontiers of Massively Par. Comp., </booktitle> <pages> pages 119-128, </pages> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: Many more techniques for improving the performance of parallel file systems have been described, including caching and prefetching [24, 23, 34], two-phase I/O [10], disk-directed I/O [20], compute-node caching [37], chunking [40], compression <ref> [41] </ref>, filtering [21, 2], and so forth. The diversity of current systems and techniques indicates that there is clearly no consensus about the structure of, interface to, or even functionality of parallel file systems.
Reference: 42. <author> J. W. Stamos and D. K. Gifford. </author> <title> Remote execution. </title> <journal> ACM TOPLAS, </journal> <volume> 12(4) </volume> <pages> 537-565, </pages> <month> Oct. </month> <year> 1990. </year>
Reference-contexts: Galley2 promotes the use of application-selected code on the I/O nodes. Several operating systems can download user code into the kernel [14, 26, 1]. Other researchers have noted that it is useful to move the function to the data rather than to move the data to the function <ref> [3, 42, 17] </ref>. Some distributed database systems execute part of the SQL query in the server rather than the client, to reduce client-server traffic [2].
Reference: 43. <author> J. T. Thomas. </author> <title> The Panda array I/O library on the Galley parallel file system. </title> <type> Technical Report PCS-TR96-288, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> June </month> <year> 1996. </year> <note> Senior Honors Thesis. </note>
Reference-contexts: We have ported several application libraries on top of Galley, including a traditional striped-file library, Panda <ref> [39, 43] </ref>, Vesta [6], and SOLAR [44]. We are also using Galley to investigate policies for managing multi-application workloads. We are building a simulator for Galley2, to evaluate some of the key ideas, and a full implementation, to experiment with real applications.
Reference: 44. <author> S. Toledo and F. G. Gustavson. </author> <title> The design and implementation of SOLAR, a portable library for scalable out-of-core linear algebra computations. </title> <booktitle> In 4th Workshop on I/O in Par. and Dist. Sys., </booktitle> <pages> pages 28-40, </pages> <address> Philadelphia, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: We have ported several application libraries on top of Galley, including a traditional striped-file library, Panda [39, 43], Vesta [6], and SOLAR <ref> [44] </ref>. We are also using Galley to investigate policies for managing multi-application workloads. We are building a simulator for Galley2, to evaluate some of the key ideas, and a full implementation, to experiment with real applications.
Reference: 45. <author> R. Wahbe, S. Lucco, T. E. Anderson, and S. L. Graham. </author> <title> Efficient software-based fault isolation. </title> <booktitle> In Proc. of 14th ACM SOSP, </booktitle> <pages> pages 203-216, </pages> <year> 1993. </year>
Reference-contexts: is the best interface for I/O-node programs to com municate with the compute nodes, and with each other? What is the appropriate mechanism to support I/O-node programs? We are considering three alternatives: processes, threads within a safe language like Java [16] or Python 3 , and threads running sandboxed code <ref> [45] </ref>.
Reference: 46. <author> D. Womble, D. Greenberg, R. Riesen, and S. Wheat. </author> <title> Out of core, out of mind: Practical parallel I/O. </title> <booktitle> In Proc. of the Scalable Par. Libraries Conf., </booktitle> <pages> pages 10-16, </pages> <institution> Mississippi State University, </institution> <month> Oct. </month> <year> 1993. </year> <title> This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: It is important that applications be able to choose the interface and policies that work best for them, and for application programmers to have control over I/O <ref> [46, 8] </ref>. This diversity of current systems, particularly of the application-programmer's interface (API), also makes it difficult to write portable applications. Nearly every file system mentioned above has its own API.
References-found: 46

