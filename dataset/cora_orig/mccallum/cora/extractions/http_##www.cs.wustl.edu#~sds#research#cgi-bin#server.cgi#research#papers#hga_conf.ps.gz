URL: http://www.cs.wustl.edu/~sds/research/cgi-bin/server.cgi/research/papers/hga_conf.ps.gz
Refering-URL: http://www.cs.wustl.edu/~sds/research/type.html
Root-URL: 
Email: sds@cs.wustl.edu  samal@cse.unl.edu and seth@cse.unl.edu  
Title: HGA: A Hardware-Based Genetic Algorithm  
Author: Stephen D. Scott Ashok Samal and Sharad Seth 
Keyword: Parallel Genetic Algorithms, Function Opti mization, Field Programmable Gate Arrays (FPGAs), Per formance Acceleration, Performance Evaluation.  
Address: St. Louis, MO 63130-4899  Lincoln, NE 68588-0115  
Affiliation: Dept. of Computer Science Washington University  Dept. of Computer Science and Engineering University of Nebraska-Lincoln  
Note: In Proc. of the 1995 ACM/SIGDA Third Int. Symposium on Field-Programmable Gate Arrays, pp. 53-59 53  
Abstract: A genetic algorithm (GA) is a robust problem-solving method based on natural selection. Hardware's speed advantage and its ability to parallelize offer great rewards to genetic al gorithms. Speedups of 1-3 orders of magnitude have been observed when frequently used software routines were im plemented in hardware by way of reprogrammable field-pro grammable gate arrays (FPGAs). Reprogrammability is es sential in a general-purpose GA engine because certain GA modules require changeability (e.g. the function to be opti mized by the GA). Thus a hardware-based GA is both feasi ble and desirable. A fully functional hardware-based genetic algorithm (the HGA) is presented here as a proof-of-concept system. It was designed using VHDL to allow for easy scala bility. It is designed to act as a coprocessor with the CPU of a PC. The user programs the FPGAs which implement the function to be optimized. Other GA parameters may also be specified by the user. Simulation results and performance analyses of the HGA are presented. A prototype HGA is de scribed and compared to a similar GA implemented in soft ware. In the simple tests, the prototype took about 6% as many clock cycles to run as the software-based GA. Further suggested improvements could realistically make the HGA 2-3 orders of magnitude faster than the software-based GA. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Darwin on a chip. </author> <title> The Economist, </title> <address> 326(7798):85, Febru ary 1993. </address>
Reference-contexts: Alberta has implemented a suite of proprietary GAs in a text compression chip [14], and Tetsuya Higuchi et. al. at the Electrotechnical Laboratory in Tsukuba are developing 54 self-adapting hardware which uses a GA to modify hard ware configuration bit strings that control the connections in programmable logic devices (PLDs) <ref> [1] </ref>. Unfortunately, the amount of current work in hardware GAs is small and the application areas differ greatly from the intention of this work. For brevity's sake, many details of this work are omitted.
Reference: [2] <institution> The Programmable Logic Data Book. </institution> <address> Xilinx, Incorpo rated, San Jose, California, </address> <year> 1994. </year>
Reference-contexts: Based on our VHDL syntheses and mappings to Xilinx FPGA technology, we predict that the HGA is feasible with these parameters. Most of the HGA modules should be im plementable on Xilinx XC4006s or XC4008s <ref> [2] </ref> since much of their logic is independent of the parameters (i.e. state machine logic). The logic requirements of the modules FM, CMM, RNG, MIM and PS would not be affected much by the parameters.
Reference: [3] <author> P. M. Athanas and H. F. Silverman. </author> <title> Processor reconfig uration through instruction-set metamorphosis. </title> <journal> IEEE Computer, </journal> <volume> 26(3) </volume> <pages> 11-18, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: Some of this work includes Gokhale et. al.'s Splash project [9], Bertin et. al.'s program mable active memory (PAM) architecture [4], Athanas and Silverman <ref> [3] </ref> with their development of the PRISM-I system, the FPGA-based neural network by Eldredge and Hutch ings [8] which utilizes run-time reconfiguration, and Wirthlin et. al.'s Nano Processor (nP) [15]. <p> C or VHDL). Then soft ware translates the specification into a hardware image and programs the FPGA (s) which implement the fitness func tion. This software-to-hardware translator could be similar in function to the PRISM-I system <ref> [3] </ref> or to an HDL synthe sizer. Then the front end sends a "Go" signal to the back end. When the HGA back end detects the signal, it runs the GA based on the parameters already in the shared mem ory.
Reference: [4] <author> P. Bertin, D. Roncin, and J. Vuillemin. </author> <title> Programmable active memories: A performance assessment. </title> <type> Technical report, </type> <institution> Digital Equipment Corporation Paris Research Laboratory, </institution> <address> Cedex France, </address> <year> 1993. </year>
Reference-contexts: This work builds upon other research in reconfigurable hardware systems which improved system performance by mapping some or all software components to hardware us ing reprogrammable FPGAs. Some of this work includes Gokhale et. al.'s Splash project [9], Bertin et. al.'s program mable active memory (PAM) architecture <ref> [4] </ref>, Athanas and Silverman [3] with their development of the PRISM-I system, the FPGA-based neural network by Eldredge and Hutch ings [8] which utilizes run-time reconfiguration, and Wirthlin et. al.'s Nano Processor (nP) [15].
Reference: [5] <author> S. Casselman. </author> <title> Virtual computing and the virtual com puter. </title> <editor> In Robert Werner and Regina Spencer Sipple, editors, </editor> <booktitle> IEEE Workshop on FPGAs for Custom Com puting Machines, </booktitle> <pages> pages 43-48. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> April </month> <year> 1993. </year>
Reference-contexts: This type of research has inspired the manufacture of commercial products, includ ing Virtual Computer Corporation's line of Virtual Com puters <ref> [5] </ref> and the X-12 system from National Technologies Incorporated [12]. Both of these product lines are intended for use in reconfigurable hardware systems. So far little work has been done in implementing a hard ware-based GA.
Reference: [6] <author> P. K. Chan. </author> <title> A Field-Programmable Prototyping Board: XC4000 BORG User's Guide. </title> <institution> Board of Studies in Com puter Engineering, University of California, Santa Cruz, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: The HGA was designed to operate in a coprocessor capacity, waiting for the CPU to supply a "Go" signal to start HGA execution. For this to be feasible, the HGA was implemented on a prototyping board called the BORG board <ref> [6] </ref> which was connected to the bus of a PC. This allowed the HGA and the CPU to share memory, thus relieving the need for large amounts of I/O be tween the CPU and HGA. The BORG board consists of five Xilinx FPGAs.
Reference: [7] <author> K. A. De Jong and W. M. Spears. </author> <title> Using genetic algo rithms to solve NP-complete problems. </title> <editor> In J. David Schaffer, editor, </editor> <booktitle> Proceedings of the Third Interna tional Conference on Genetic Algorithms, </booktitle> <pages> pages 124 132. </pages> <publisher> Morgan Kaufmann Publishers, Incorporated, </publisher> <month> June </month> <year> 1989. </year>
Reference-contexts: For complex problems, large val ues of m and g are required, so it is imperative to make the operations as efficient as possible. Work by Spears and De Jong <ref> [7] </ref> indicates that for NP-complete problems, m = 100 and values of g on the order of 10 4 -10 5 may be necessary to obtain a good result and avoid premature convergence to a local optimum.
Reference: [8] <author> J. G. Eldredge and B. L. Hutchings. </author> <title> FPGA density en hancement of a neural network through run-time recon figuration. </title> <booktitle> In IEEE Workshop on FPGAs for Custom Computing Machines, </booktitle> <pages> pages 180-188, </pages> <address> Napa, CA, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: Some of this work includes Gokhale et. al.'s Splash project [9], Bertin et. al.'s program mable active memory (PAM) architecture [4], Athanas and Silverman [3] with their development of the PRISM-I system, the FPGA-based neural network by Eldredge and Hutch ings <ref> [8] </ref> which utilizes run-time reconfiguration, and Wirthlin et. al.'s Nano Processor (nP) [15]. This type of research has inspired the manufacture of commercial products, includ ing Virtual Computer Corporation's line of Virtual Com puters [5] and the X-12 system from National Technologies Incorporated [12].
Reference: [9] <author> M. Gokhale, W. Holmes, A. Kosper, S. Lucas, R. Min nich, D. Sweely, and D. Lopresti. </author> <title> Building and using a highly parallel programmable logic array. </title> <journal> IEEE Com puter, </journal> <pages> pages 81-89, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: This work builds upon other research in reconfigurable hardware systems which improved system performance by mapping some or all software components to hardware us ing reprogrammable FPGAs. Some of this work includes Gokhale et. al.'s Splash project <ref> [9] </ref>, Bertin et. al.'s program mable active memory (PAM) architecture [4], Athanas and Silverman [3] with their development of the PRISM-I system, the FPGA-based neural network by Eldredge and Hutch ings [8] which utilizes run-time reconfiguration, and Wirthlin et. al.'s Nano Processor (nP) [15].
Reference: [10] <author> D. E. Goldberg. </author> <title> Genetic Algorithms in Search, Op timization, and Machine Learning. </title> <publisher> Addison-Wesley Publishing Company, Incorporated, </publisher> <address> Reading, Mas sachusetts, </address> <year> 1989. </year>
Reference-contexts: 1 Introduction A genetic algorithm (GA) is an optimization method based on natural selection that is simple to implement <ref> [10] </ref>. Ge netic algorithms have been applied to many hard optimiza tion problems including VLSI layout optimization, boolean satisfiability and the Hamiltonian circuit problem. They have been recognized as a robust general-purpose optimiza tion technique. <p> These four properties make GAs robust, powerful, and data-independent <ref> [10] </ref>. A GA is a stochastic technique with simple operations based on the theory of natural selection. <p> Values for these parameters would be selected by the user in software which would send the appropriate signals to ini tialize and start the HGA. 3.2 The Modules and Their Functions The modules in Figure 1 are patterned after the GA oper ators defined in Goldberg's simple genetic algorithm (SGA) <ref> [10] </ref>. The HGA modules operate concurrently with each other and together form a coarse-grained pipeline. All mod ules are written in VHDL and are independent of the operat ing environment and implementation technology (e.g. Xilinx FPGAs or fabricated chips) except for the memory interface module. <p> This allows the implementation of more complex fitness functions in the HGA. Comparison with a Software-Based GA We tested the HGA with the prototype and a VHDL simula tor against the software-based SGA <ref> [10] </ref> running on a Silicon Graphics 4D/440 with four MIPS R3000 CPUs, each running at 33 MHz. We chose the SGA for comparison because the HGA was patterned after it. Thus the SGA is functionally similar to the HGA. <p> The external fitness evaluator is the only module which requires reprogramma bility, thus it is the only module which truly needs an FPGA implementation. 5.2 Genetic Algorithm Extensions The genetic algorithm side of this work could be extended by implementing other genetic algorithm operators and en codings <ref> [10] </ref>. Additionally, other selection methods could be implemented and made available to the user as an HGA pa rameter via an improved user interface (software front end). Acknowledgements Assistance for this work was received from Mentor Graphics Corporation and Xilinx, Incorporated through their dona tions of software and hardware, respectively.
Reference: [11] <author> P. Kenyon, S. Seth, P. Agrawal, A. Clematis, G. Do dero, and V. Gianuzzi. </author> <title> Programming pipelined CAD applications on message passing architectures. </title> <journal> Concur rency Practice and Experience, </journal> <note> 1995. To appear. </note>
Reference-contexts: The performance analysis in cluded analyzing the pipelines to identify bottlenecks using techniques described in <ref> [11] </ref>. 56 4.1 Verification of Correct Functionality To verify the design's correctness, the modules were con nected and simulated. During simulation each module was scrutinized to ensure correct functionality. During these simulations the HGA ran on different fitness functions to see how well the functions were optimized. <p> First the modules in the pipe lines pictured in Figures 1 and 2 were analyzed to determine the parameters which impact asynchronous pipeline perfor mance. These parameters are defined in <ref> [11] </ref> as follows. 1. The actual service time s i of pipeline stage (module) i is the amount of time stage i takes to receive a message at its inputs, process it and send the output to the next stage. 2. <p> Both the equation evaluations and simulation re sults appear in Table 3. The heading "nsel = 1" implies one selection module was used (Figure 1), and "nsel = 2" implies two parallel selection modules were used (Figure 2). To remove a bottleneck, Kenyon et. al. <ref> [11] </ref> suggest either parallelizing the bottleneck stage or breaking it into smaller stages. Due to the functional simplicity of the population se quencer and its tight coupling with the MIM, neither of these options is possible.
Reference: [12] <author> J. McLeod. </author> <title> Reconfigurable computer changes architec ture. </title> <publisher> Electronics, </publisher> <pages> page 5, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: This type of research has inspired the manufacture of commercial products, includ ing Virtual Computer Corporation's line of Virtual Com puters [5] and the X-12 system from National Technologies Incorporated <ref> [12] </ref>. Both of these product lines are intended for use in reconfigurable hardware systems. So far little work has been done in implementing a hard ware-based GA.
Reference: [13] <author> S. D. Scott. HGA: </author> <title> A Hardware-Based Genetic Al gorithm. </title> <type> Master's thesis, </type> <institution> University of Nebraska Lincoln, </institution> <month> August </month> <year> 1994. </year> <note> Available via anonymous ftp at ftp.cs.unl.edu (129.93.33.24) in /pub/TechReps/UNL CSE-94-020.ps.gz. </note>
Reference-contexts: For brevity's sake, many details of this work are omitted. For more information on any of the following sections (in cluding VHDL source code for the design itself), the reader may refer to <ref> [13] </ref>. 2 Background on Genetic Algorithms A genetic algorithm (GA) is a natural selection-based op timization technique.
Reference: [14] <author> L. Wirbel. </author> <title> Compression chip is first to use genetic al gorithms. Electronic Engineering Times, </title> <type> page 17, </type> <institution> De cember 1992. </institution>
Reference-contexts: Both of these product lines are intended for use in reconfigurable hardware systems. So far little work has been done in implementing a hard ware-based GA. DCP Research Corporation in Edmonton, Alberta has implemented a suite of proprietary GAs in a text compression chip <ref> [14] </ref>, and Tetsuya Higuchi et. al. at the Electrotechnical Laboratory in Tsukuba are developing 54 self-adapting hardware which uses a GA to modify hard ware configuration bit strings that control the connections in programmable logic devices (PLDs) [1].
Reference: [15] <author> M. K. Wirthlin, K. Gilson, and B. L. Hutchings. </author> <title> The nanoprocessor: A low resource reconfigurable processor. </title> <booktitle> In IEEE Workshop on FPGAs for Custom Computing Machines, </booktitle> <pages> pages 23-30, </pages> <address> Napa, CA, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: work includes Gokhale et. al.'s Splash project [9], Bertin et. al.'s program mable active memory (PAM) architecture [4], Athanas and Silverman [3] with their development of the PRISM-I system, the FPGA-based neural network by Eldredge and Hutch ings [8] which utilizes run-time reconfiguration, and Wirthlin et. al.'s Nano Processor (nP) <ref> [15] </ref>. This type of research has inspired the manufacture of commercial products, includ ing Virtual Computer Corporation's line of Virtual Com puters [5] and the X-12 system from National Technologies Incorporated [12]. Both of these product lines are intended for use in reconfigurable hardware systems.
References-found: 15

