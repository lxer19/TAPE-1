URL: http://www.cs.jhu.edu/~brill/TAGGING_ADVANCES.ps
Refering-URL: http://www.cs.jhu.edu/~brill/acadpubs.html
Root-URL: 
Keyword: Error-Driven Learning Eric Brill Spoken Language Systems Group  
Address: Cambridge, Massachusetts 02139  
Affiliation: Laboratory for Computer Science Massachusetts Institute of Technology  
Abstract: A Report of Recent Progress in Transformation-Based ABSTRACT Most recent research in trainable part of speech taggers has explored stochastic tagging. While these taggers obtain high accuracy, linguistic information is captured indirectly, typically in tens of thousands of lexical and contextual probabilities. In [Brill 92], a trainable rule-based tagger was described that obtained performance comparable to that of stochastic taggers, but captured relevant linguistic information in a small number of simple non-stochastic rules. In this paper, we describe a number of extensions to this rule-based tagger. First, we describe a method for expressing lexical relations in tagging that stochastic taggers are currently unable to express. Next, we show a rule-based approach to tagging unknown words. Finally, we show how the tagger can be extended into a k-best tagger, where multiple tags can be assigned to words in some cases of uncertainty. 
Abstract-found: 1
Intro-found: 1
Reference: [Brill 92] <author> E. </author> <title> Brill 1992. A simple rule-based part of speech tagger. </title> <booktitle> In Proceedings of the Third Conference on Applied Natural Language Processing, </booktitle> <address> Trento, Italy. </address>
Reference-contexts: Almost all recent work in developing automatically trained part of speech taggers has been on further exploring Markov-model based tagging [Jelinek 85, Church 88, DeRose 88, DeMarcken 90, Merialdo 91, Cutting et al. 92, Kupiec 92, Charniak et al. 93, Weischedel et al. 93]. 1 In <ref> [Brill 92] </ref>, a trainable rule-based tagger is described fl This research was supported by ARPA under contract N00014-89-J-1332, monitored through the Office of Naval Research. 1 Markov-model based taggers assign a sentence the tag sequence that maximizes P rob (wordjtag) fl P rob (tagjprevious n tags). that achieves performance comparable to <p> TRANSFORMATION-BASED ERROR-DRIVEN LEARNING Transformation-based error-driven learning has been applied to a number of natural language problems, including part of speech tagging, prepositional phrase attachment disambiguation, and syntactic parsing <ref> [Brill 92, Brill 93, Brill 93a] </ref>. A similar approach is being explored for machine translation [Su et al. 92]. Figure 1 illustrates the learning process. First, unannotated text is passed through the initial-state annotator. <p> Once an ordered list of transformations is learned, new text can be annotated by first applying the initial state annotator to it and then applying each of the learned transformations, in order. 3. AN EARLIER ATTEMPT The original tranformation-based tagger <ref> [Brill 92] </ref> works as follows. The start state annotator assigns each word its most likely tag as indicated in the training corpus.
Reference: [Brill 93] <author> E. </author> <title> Brill 1993. Automatic grammar induction and parsing free text: a transformation-based approach. </title> <booktitle> In Proceedings of the 31st Meeting of the Association of Computational Linguistics, </booktitle> <address> Columbus, Ohio. </address>
Reference-contexts: TRANSFORMATION-BASED ERROR-DRIVEN LEARNING Transformation-based error-driven learning has been applied to a number of natural language problems, including part of speech tagging, prepositional phrase attachment disambiguation, and syntactic parsing <ref> [Brill 92, Brill 93, Brill 93a] </ref>. A similar approach is being explored for machine translation [Su et al. 92]. Figure 1 illustrates the learning process. First, unannotated text is passed through the initial-state annotator.
Reference: [Brill 93a] <author> E. </author> <title> Brill 1993. A corpus-based approach to language learning. </title> <type> Ph.D. Dissertation, </type> <institution> Department of Computer and Information Science, University of Penn-sylvania. </institution>
Reference-contexts: TRANSFORMATION-BASED ERROR-DRIVEN LEARNING Transformation-based error-driven learning has been applied to a number of natural language problems, including part of speech tagging, prepositional phrase attachment disambiguation, and syntactic parsing <ref> [Brill 92, Brill 93, Brill 93a] </ref>. A similar approach is being explored for machine translation [Su et al. 92]. Figure 1 illustrates the learning process. First, unannotated text is passed through the initial-state annotator. <p> This approach has already been successfully applied to a system for prepositional phrase disambiguation <ref> [Brill 93a] </ref>. 5. UNKNOWN WORDS In addition to not being lexicalized, another problem with the original transformation-based tagger was its relatively low accuracy at tagging unknown words. 10 In the start state annotator for tagging, words are assigned their most likely tag, estimated from a training corpus. <p> We can modify the transformation-based tagger to return multiple tags for a word by making a simple mod 13 This learner has also been applied to tagging Old English. See <ref> [Brill 93a] </ref>. # of Rules Accuracy Avg. # of tags per word 0 96.5 1.00 50 96.9 1.03 100 97.2 1.06 Table 2: Results from k-best tagging. ification to the contextual transformations described above. The initial-state annotator is the tagging output of the transformation-based tagger described above.
Reference: [Charniak et al. 93] <author> E. Charniak, C. Hendrickson, N. Jacob-son, and M. Perkowitz. </author> <year> 1993. </year> <title> Equations for part-of-speech tagging. </title> <booktitle> In Proceedings of Conference of the American Association for Artificial Intelligence (AAAI), </booktitle> <address> Washington, D.C. </address>
Reference: [Church 88] <author> K. Church. </author> <year> 1988. </year> <title> A stochastic parts program and noun phrase parser for unrestricted text. </title> <booktitle> In Proceedings of the Second Conference on Applied Natural Language Processing, </booktitle> <address> Austin, Texas. </address>
Reference: [Cutting et al. 92] <author> D. Cutting, J. Kupiec, J. Pedersen, and P. Sibun. </author> <year> 1992. </year> <booktitle> A practical part-of-speech tagger In Proceedings of the Third Conference on Applied Natural Language Processing, </booktitle> <address> Trento, Italy. </address>
Reference: [DeRose 88] <author> S. </author> <title> DeRose 1988. Grammatical category disambiguation by statistical optimization. </title> <journal> Computational Linguistics, </journal> <volume> Volume 14. </volume>
Reference: [DeMarcken 90] <author> C. DeMarcken. </author> <year> 1990. </year> <title> Parsing the LOB corpus. </title> <booktitle> In Proceedings of the 1990 Conference of the Association for Computational Linguistics. </booktitle>
Reference-contexts: K-BEST TAGS There are certain circumstances where one is willing to relax the one tag per word requirement in order to increase the probability that the correct tag will be assigned to each word. In <ref> [DeMarcken 90, Weischedel et al. 93] </ref>, k-best tags are assigned within a stochastic tagger by returning all tags within some threshold of probability of being correct for a particular word. <p> In <ref> [DeMarcken 90] </ref>, the test set is included in the training set, and so it is difficult to know how this system would do on fresh text. In [Weischedel et al. 93], a k-best tag experiment was run on the Wall Street Journal corpus.
Reference: [Harris 62] <author> Z. Harris. </author> <year> 1962. </year> <title> String Analysis of Language Structure, </title> <publisher> Mouton and Co., </publisher> <address> The Hague. </address>
Reference-contexts: 1. INTRODUCTION When automated part of speech tagging was initially explored <ref> [Klein and Simmons 63, Harris 62] </ref>, people manually engineered rules for tagging, sometimes with the aid of a corpus. As large corpora became available, it became clear that simple Markov-model based stochastic taggers that were automatically trained could achieve high rates of tagging accuracy [Jelinek 85].
Reference: [Klein and Simmons 63] <author> S. Klein and R. Simmons. </author> <year> 1963. </year> <title> A computational approach to grammatical coding of English words. </title> <journal> JACM, </journal> <volume> Volume 10. </volume>
Reference-contexts: 1. INTRODUCTION When automated part of speech tagging was initially explored <ref> [Klein and Simmons 63, Harris 62] </ref>, people manually engineered rules for tagging, sometimes with the aid of a corpus. As large corpora became available, it became clear that simple Markov-model based stochastic taggers that were automatically trained could achieve high rates of tagging accuracy [Jelinek 85].
Reference: [Jelinek 85] <author> F. Jelinek. </author> <year> 1985. </year> <title> Markov source modeling of text generation. In Impact of Processing Techniques on Communication. </title> <editor> J. Skwirzinski, ed., </editor> <publisher> Dordrecht. </publisher>
Reference-contexts: As large corpora became available, it became clear that simple Markov-model based stochastic taggers that were automatically trained could achieve high rates of tagging accuracy <ref> [Jelinek 85] </ref>. These stochastic taggers have a number of advantages over the manually built taggers, including obviating the need for laborious manual rule construction, and possibly capturing useful information that may not have been noticed by the human engineer.
Reference: [Kupiec 92] <author> J. Kupiec. </author> <year> 1992. </year> <title> Robust part-of-speech tagging using a hidden Markov model. </title> <booktitle> Computer Speech and Language. </booktitle>
Reference: [Marcus et al. 93] <author> M. Marcus, B. Santorini, and M. Marcinkiewicz. </author> <year> 1993. </year> <title> Building a large annotated corpus of English: the Penn Treebank. </title> <journal> Computational Linguistics, </journal> <volume> Volume 19. </volume>
Reference-contexts: A stochastic trigram tagger would have to capture this linguistic information indirectly from frequency counts of all trigrams of the form: 8 * ADVERB PRESENT VERB * ADVERB BASE VERB 6 All experiments were run on the Penn Treebank tagged Wall Street Journal corpus, version 0.5 <ref> [Marcus et al. 93] </ref>. 7 In the Penn Treebank, n't is treated as a separate token, so don't becomes do/VB-NON3rd-SING n't/ADVERB. 8 Where a star can match any part of speech tag. Training # of Rules Corpus or Context. Acc.
Reference: [Merialdo 91] <author> B. Merialdo. </author> <year> 1991. </year> <title> Tagging text with a probabilistic model. </title> <booktitle> In IEEE International Conference on Acoustics, Speech and Signal Processing. </booktitle>
Reference: [Miller 90] <author> G. Miller. </author> <year> 1990. </year> <title> WordNet: an on-line lexical database. </title> <journal> International Journal of Lexicography. </journal>
Reference-contexts: We are currently exploring the possibility of incorporating word classes into the rule-based learner in hopes of overcoming this problem. The idea is quite simple. Given a source of word class information, such as WordNet <ref> [Miller 90] </ref>, the learner is extended such that a rule is allowed to make reference to parts of speech, words, and word classes, allowing for rules such as Change the tag from X to Y if the following word belongs to word class Z.
Reference: [Su et al. 92] <author> K. Su, M. Wu, and J. Chang. </author> <year> 1992. </year> <title> A new quantitative quality measure for machine translation Systems. </title> <booktitle> In Proceedings of COLING-92, </booktitle> <address> Nantes, France. </address>
Reference-contexts: TRANSFORMATION-BASED ERROR-DRIVEN LEARNING Transformation-based error-driven learning has been applied to a number of natural language problems, including part of speech tagging, prepositional phrase attachment disambiguation, and syntactic parsing [Brill 92, Brill 93, Brill 93a]. A similar approach is being explored for machine translation <ref> [Su et al. 92] </ref>. Figure 1 illustrates the learning process. First, unannotated text is passed through the initial-state annotator. The initial-state annotator can range in complexity from assigning random structure to assigning the output of a sophisticated manually created annotator.
Reference: [Weischedel et al. 93] <author> R. Weischedel, M. Meteer, R. Schwartz, L. Ramshaw, and J. Palmucci. </author> <year> 1993. </year> <title> Coping with ambiguity and unknown words through probabilistic models. </title> <journal> Computational Linguistics, </journal> <volume> Volume 19. </volume>
Reference-contexts: Rules 600 K 219 96.9 Rule-Based With Lex. Rules 600 K 267 97.2 Table 1: Comparison of Tagging Accuracy With No Unknown Words ADVERB * PRESENT VERB ADVERB * BASE VERB and from the fact that P (n 0 tjADV ERB) is fairly high. In <ref> [Weischedel et al. 93] </ref>, results are given when training and testing a Markov-model based tagger on the Penn Treebank Tagged Wall Street Journal Corpus. They cite results making the closed vocabulary assumption that all possible tags for all words in the test set are known. <p> Accuracy of that tagger was 96.9%. Disallowing lexicalized transformations resulted in an 11% increase in the error rate. These results are summarized in table 1. 9 In both <ref> [Weischedel et al. 93] </ref> and here, the test set was incorporated into the lexicon, but was not used in learning contextual information. Testing with no unknown words might seem like an unrealistic test. <p> Unknown word accuracy on the test corpus was 85.0%, and overall tagging accuracy on the test corpus was 96.5%. To our knowledge, this is the highest overall tagging accuracy ever quoted on the Penn Treebank Corpus when making the open vocabulary assumption. In <ref> [Weischedel et al. 93] </ref>, a statistical approach to tagging unknown words is shown. In this approach, a number of suffixes and important features are prespecified. <p> K-BEST TAGS There are certain circumstances where one is willing to relax the one tag per word requirement in order to increase the probability that the correct tag will be assigned to each word. In <ref> [DeMarcken 90, Weischedel et al. 93] </ref>, k-best tags are assigned within a stochastic tagger by returning all tags within some threshold of probability of being correct for a particular word. <p> In [DeMarcken 90], the test set is included in the training set, and so it is difficult to know how this system would do on fresh text. In <ref> [Weischedel et al. 93] </ref>, a k-best tag experiment was run on the Wall Street Journal corpus. They quote the average number of tags per word for various threshold settings, but do not provide accuracy results. large numbers of lexical and contextual probabilities.
References-found: 17

