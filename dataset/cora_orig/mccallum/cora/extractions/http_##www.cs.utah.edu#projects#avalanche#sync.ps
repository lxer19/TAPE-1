URL: http://www.cs.utah.edu/projects/avalanche/sync.ps
Refering-URL: http://www.cs.utah.edu/projects/avalanche/avalanche-publications.html
Root-URL: 
Email: kuramkotg@cs.utah.edu  
Title: A Comparison of Software and Hardware Synchronization Mechanisms for Distributed Shared Memory Multiprocessors overall performance
Author: John B. Carter, Chen-Chi Kuo, Ravindra Kuramkote fretrac, chenchi, 
Note: Although the  nization operations should be provided. This work was supported by the Space and Naval Warfare Systems Command (SPAWAR) and Advanced Research Projects Agency (ARPA), Communication and Memory Architectures for Scalable Parallel Computing, ARPA order #B990 under SPAWAR contract #N00039-95-C-0018  
Date: September 24, 1996  
Address: Salt Lake City, UT 84112  
Affiliation: Department of Computer Science University of Utah,  
Web: WWW: http://www.cs.utah.edu/projects/avalanche UUCS-96-011  
Abstract: Efficient synchronization is an essential component of parallel computing. The designers of traditional multiprocessors have included hardware support only for simple operations such as compare-and-swap and load-linked/store-conditional, while high level synchronization primitives such as locks, barriers, and condition variables have been implemented in software [9, 14, 15]. With the advent of directory-based distributed shared memory (DSM) multiprocessors with significant flexibility in their cache controllers [7, 12, 17], it is worthwhile considering whether this flexibility should be used to support higher level synchronization primitives in hardware. In particular, as part of maintaining data consistency, these architectures maintain lists of processors with a copy of a given cache line, which is most of the hardware needed to implement distributed locks. We studied two software and four hardware implementations of locks and found that hardware implementation can reduce lock acquire and release times by 25-94% compared to well tuned software locks. In terms of macrobenchmark performance, hardware locks reduce application running times by up to 75% on a synthetic benchmark with heavy lock contention and by 3%-6% on a suite of SPLASH-2 benchmarks. In addition, emerging cache coherence protocols promise to increase the time spent synchronizing relative to the time spent accessing shared data, and our study shows that hardware locks can reduce SPLASH-2 execution times by up to 10-13% if the time spent accessing shared data is small. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal and D. Chaiken et al. </author> <title> The MIT Alewife Machine: A large-scale distributed-memory multiprocessor. </title> <type> Technical Report Technical Memp 454, </type> <institution> MIT/LCS, </institution> <year> 1991. </year>
Reference-contexts: This implementation is extremely simple, but starvation is possible and FIFO ordering is not maintained. 3.2.2 Ordered Centralized Hardware Lock Mechanism Architectures, such as Alewife's <ref> [1] </ref>, that manage their directories using linked lists can easily implement a centralized locking strategy that maintains FIFO ordering. For example, in Alewife each directory entry can directly store up to four sharing processors or nodes awaiting a lock. <p> For each directory entry in the directory controller, a list of the first four nodes waiting on the lock are stored in the lock's directory entry, as in Alewife <ref> [1] </ref>. As long as no more than four nodes are waiting on the lock at a time, it remains in low contention (centralized) mode releases go back to the home node and the lock cannot be cached.
Reference: [2] <author> J. Archibald and J.-L. Baer. </author> <title> Cache coherence protocols: Evaluation using a multiprocessor simulation model. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4(4) </volume> <pages> 273-298, </pages> <month> November </month> <year> 1986. </year>
Reference: [3] <author> J.K. Bennett, J.B. Carter, and W. Zwaenepoel. </author> <title> Adaptive software cache management for distributed shared memory architectures. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 125-134, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Both hardware and software locks must communicate with the directory controllers managing the lock's cache line, but since conventional data consistency protocols are not well suited to synchronization access patterns <ref> [3] </ref>, the software lock implementations will require extra communication traffic. These observations led us to investigate four directory-based hardware lock mechanisms. The two simplest hardware mechanisms communicate only with the directory controller on the lock's home node to acquire and release locks.
Reference: [4] <author> N.J. Boden, D. Cohen, R.E. Felderman, A.E. Kulawik, C.L. Seitz, J.N. Seizovic, and W.-K. Su. </author> <title> Myrinet A gigabit-per-second local-area network. </title> <journal> IEEE MICRO, </journal> <volume> 15(February):29-36, </volume> <month> February </month> <year> 1995. </year>
Reference-contexts: Our experience and that of others [5] is that accurate performance measurements of a multiprocessor's backplane is critical for effectively evaluating the true performance of DSM architectures. Therefore, we have developed a very detailed, flit-by-flit model of the Myrinet network fabric <ref> [4] </ref> to accurately model network delays and contention. The Myrinet fabric is mesh-connected, with one crossbar at the core of each switching node.
Reference: [5] <author> D.C. Burger and D.A. Wood. </author> <title> Accuracy vs performance in parallel simulation of interconnection networks. </title> <booktitle> In Proceedings of the ACM/IEEE International Parallel Processing Symposium (IPPS), </booktitle> <month> April </month> <year> 1995. </year>
Reference-contexts: It generates multiple streams of memory reference events, which we used to drive our detailed simulation model of the Widget multiprocessor memory and communication hierarchy described in Section 2. We replaced the built-in spinlocks with our synchronization protocols as described in Section 3. Our experience and that of others <ref> [5] </ref> is that accurate performance measurements of a multiprocessor's backplane is critical for effectively evaluating the true performance of DSM architectures. Therefore, we have developed a very detailed, flit-by-flit model of the Myrinet network fabric [4] to accurately model network delays and contention.
Reference: [6] <author> J.B. Carter, J.K. Bennett, and W. Zwaenepoel. </author> <title> Techniques for reducing consistency-related communication in distributed shared memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 13(3) </volume> <pages> 205-243, </pages> <month> August </month> <year> 1995. </year>
Reference: [7] <author> J.B. Carter and R. Kuramkote. </author> <title> Avalanche: Cache and DSM protocol design. </title> <type> Technical report, </type> <institution> University of Utah, </institution> <month> April </month> <year> 1995. </year>
Reference-contexts: Data was kept coherent using a release consistent invalidation protocol. Space constraints make it impossible to discuss all of the details of the simulation environment herein amore detailed description of the Widget architecture can be found elsewhere <ref> [7] </ref>.
Reference: [8] <author> D. Chaiken and A. Agarwal. </author> <title> Software-extended coherent shared memory: Performance and cost. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 314-324, </pages> <month> April </month> <year> 1994. </year>
Reference: [9] <author> A. Gottlieb, R. Grishman, C.P. Kruskal, K.P. McAuliffe, L. Rudolph, and M. Snir. </author> <title> The NYU multi-computer designing a MIMD shared-memory parallel machine. </title> <journal> IEEE Transactions on Programming Languages and Systems, </journal> <volume> 5(2) </volume> <pages> 164-189, </pages> <month> April </month> <year> 1983. </year>
Reference: [10] <author> A. Gupta and W.-D. Weber. </author> <title> Cache invalidation patterns in shared-memory multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 41(7) </volume> <pages> 794-810, </pages> <month> July </month> <year> 1992. </year> <month> 23 </month>
Reference: [11] <author> M. Heinrich and J. Kuskin et al. </author> <title> The performance impact of flexibility in the Stanford FLASH multi-processor. </title> <booktitle> In Proceedings of the 6th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 274-285, </pages> <month> October </month> <year> 1994. </year>
Reference: [12] <author> J. Kuskin and D. Ofelt et al. </author> <title> The Stanford FLASH multiprocessor. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 302-313, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: This architecture can implement the centralized scheme described above with FIFO access guarantees. The only disadvantage of this scheme is the software cost for handling copyset overflows. In the FLASH multiprocessor <ref> [12] </ref>, the linked list could easily be managed by the MAGIC chip and stored in the data cache. 3.2.3 Distributed Hardware Lock Mechanism The ordered centralized lock mechanism guarantees FIFO access ordering, but like all centralized protocols, it can result in serious network and controller hot spots that degrade other transactions
Reference: [13] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W.-D. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. S. Lam. </author> <title> The Stanford DASH multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: in IO space so that all reads and writes can be snooped off of the Runway by the Widget node controller. 3.2.1 Simple Centralized Hardware Lock Mechanism The DASH multiprocessor's directory controllers maintain a 64-bit copyset for each cache line, where each bit represents one of the 64 DASH processors <ref> [13] </ref>. This full map directory design can be adapted to support locking by converting acquire requests for remote locks into lock requests to the relevant directory controller.
Reference: [14] <author> Beng-Hong Lim and Anant Agarwal. </author> <title> Reactive synchronization algorithms for multiprocessors. </title> <booktitle> In Proceedings of the Sixth Symposium on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VI), </booktitle> <pages> pages 25-35, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: However, MCS locks had the best overall performance in a previous 8 study of synchronization mechanisms [15], which led Lim and Agarwal to adopt it as their high contention mechanism in their reactive synchronization implementation <ref> [14] </ref>. MCS locks perform best on machines that support both the compare-and-swap and fetch-and-store operations, but it can be implemented using only fetch-and-store, albeit with higher overhead and without FIFO order guaranteed. Our implementation assumes that the underlying hardware provides both primitives. <p> The final adaptive protocol attempts to switch between centralized and distributed modes based on dynamic access behavior, in a manner analogous to Lim and Agarwal's software reactive locks <ref> [14] </ref>. All four mechanisms exploit Widget's ability to support multiple consistency protocols by extending the hardware state engine in each directory controller to exchange the required synchronization messages, described below. <p> Figure 6 contains the parameters and a brief description for each of these test program. The global counter program has been used in couple of previous studies <ref> [14, 16] </ref>. The program has one global counter protected by a lock. All participating processes compete for the lock, update the counter, wait for a period of time for the shared lock, and compete for the next run.
Reference: [15] <author> J. M. Mellor-Crummey and M. L. Scott. </author> <title> Algorithms for scalable synchronization on shared-memory multiprocessors. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 9(1) </volume> <pages> 21-65, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: During periods of low contention, the extra work required to implement the distributed queue is unnecessary and can significantly increase the synchronization overhead compared to ticket locks. However, MCS locks had the best overall performance in a previous 8 study of synchronization mechanisms <ref> [15] </ref>, which led Lim and Agarwal to adopt it as their high contention mechanism in their reactive synchronization implementation [14].
Reference: [16] <author> Maged M Michael and Michael L. Scott. </author> <title> Scalability of atomic primitives on distributed shared memory multiprocessors. </title> <type> Technical Report 528, </type> <institution> University of Rochester Computer Science Department, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: Figure 6 contains the parameters and a brief description for each of these test program. The global counter program has been used in couple of previous studies <ref> [14, 16] </ref>. The program has one global counter protected by a lock. All participating processes compete for the lock, update the counter, wait for a period of time for the shared lock, and compete for the next run.
Reference: [17] <author> S.K. Reinhardt, J.R. Larus, and D.A. Wood. Tempest and Typhoon: </author> <title> User-level shared memory. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 325-336, </pages> <month> April </month> <year> 1994. </year>
Reference: [18] <author> A. Saulsbury, T. Wilkinson, J. Carter, and A. Landin. </author> <title> An argument for simple coma. </title> <booktitle> In Proceedings of the First Annual Symposium on High Performance Computer Architecture, </booktitle> <pages> pages 276-285, </pages> <month> January </month> <year> 1995. </year>
Reference: [19] <author> J.E. Veenstra and R.J. Fowler. </author> <title> A performance evaluation of optimal hybrid cache coherency protocols. </title> <booktitle> In Proceedings of the 5th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 149-160, </pages> <month> September </month> <year> 1992. </year>
Reference: [20] <author> J.E. Veenstra and R.J. Fowler. Mint: </author> <title> A front end for efficient simulation of shared-memory multiprocessors. </title> <booktitle> In MASCOTS 1994, </booktitle> <month> January </month> <year> 1994. </year>
Reference-contexts: to acquire the lock will continue to respond to lock requests to avoid race conditions. 4 Experimental Methodology 4.1 Simulation Environment To evaluate a wide variety of possible synchronization implementations, we used the PAINT simulation environment, a locally ported HP PA-RISC version of the MINT multiprocessor memory hierarchy simulation environment <ref> [20] </ref>. PAINT simulates a collection of processors and provides support for spinlocks, semaphores, barriers, shared memory, and most Unix system calls. It generates multiple streams of memory reference events, which we used to drive our detailed simulation model of the Widget multiprocessor memory and communication hierarchy described in Section 2.
Reference: [21] <author> A. Wilson and R. LaRowe. </author> <title> Hiding shared memory reference latency on the GalacticaNet distributed shared memory architecture. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 15(4) </volume> <pages> 351-367, </pages> <month> August </month> <year> 1992. </year>
Reference: [22] <author> S.C. Woo, M. Ohara, E. Torrie, J.P. Singh, and A. Gupta. </author> <title> The SPLASH-2 programs: Characterization and methodological considerations. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 24-36, </pages> <month> June </month> <year> 1995. </year> <month> 24 </month>
Reference-contexts: In addition, the number of participating processes adds another dimension to the analysis. In the ensuing discussion, counter-n represents the global 14 counter program with 500 cycles in each critical section and n cycles between critical sections. barnes, fmm, and radiosity are from the SPLASH-2 benchmark suite <ref> [22] </ref>. Table 2 contains the average duration of critical sections and the average time between acquire attempts for each of these programs.
References-found: 22

