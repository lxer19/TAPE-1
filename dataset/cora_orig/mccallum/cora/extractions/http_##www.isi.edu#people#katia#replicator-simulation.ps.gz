URL: http://www.isi.edu/people/katia/replicator-simulation.ps.gz
Refering-URL: http://www.isi.edu/people/katia/
Root-URL: http://www.isi.edu
Email: katia@isi.edu  danzig@usc.edu  
Phone: (310)822-1511 (voice) (310)823-6714 (fax)  (213)740-6578 (voice) (213)740-7285 (fax)  
Title: Evaluating the Performance of Flood-d: A Tool for Efficiently Replicating Internet Information Services  
Author: Katia Obraczka Peter B. Danzig 
Address: 4676 Admiralty Way suite 1001 Marina del Rey, CA 90292  Los Angeles, CA 90089-0781  
Affiliation: Information Science Institute University of Southern California  Computer Science Department University of Southern California  
Abstract: Traditional replication algorithms do not address the scale and administrative decentralization of today's internetworks. They manage a single group of replicas and rely on system administrators to hand configure the topologies over which updates travel. While this is appropriate for applications with a small number of replicas that operate within single administrative boundaries, it does not scale in wide-area, highly replicated services whose replicas spread throughout the Internet's thousands of autonomously administered domains. We have proposed and implemented a scalable and efficient tool to replicate wide-area, autonomously managed services. We target replication degrees of thousands of weakly consistent replicas. The main goal of our replication tool is to make traditional replication services scale in today's exponentially growing, autonomously managed in-ternetworks. Our tool, which we call flood-d, allows servers to be organized in multiple replication groups. For each replication group, flood-d builds a logical update topology that is resilient to server failure, and tries to minimize the communication cost and propagation time needed to transmit updates. Flood-d's logical topologies are computed based on communication latency and available network bandwidth. This paper describes flood-d, and presents simulation results obtained when using flood-d to extend existing replication algorithms. Our results show the gains of organizing service replicas into multiple, smaller replication groups, and using network-cognizant logical topologies to propagate updates. We argue that existing as well as emerging Internet information services can benefit from flood-d's services. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. Berners-Lee, R. Cailliau, A. Luotonen, H.F. Nielsen, and A. </author> <title> Secret. The World-Wide Web. </title> <journal> Communications of the ACM, </journal> <volume> 37 </volume> <pages> 76-82, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: A separate paper [18] reports the design and implementation of flood-d, as well as preliminary experimental performance results. Flood-d was originally targeted at improving the performance of replicated information services including replicated databases, and distributed information dissemination services, such as the Web <ref> [1] </ref> and Internet archives. However, more recently developed applications, most of which sparked by the Internet's exponential growth, can also benefit from flood-d's services. For instance, network and server load information are critical for distributed computation management tools like Globus [7] and PRM [17]. <p> They try to keep up with the dynamics of the underlying physical topology, which becomes increasingly harder as the Internet's scale and complexity increases. Other Internet information services such as the Web <ref> [1] </ref>, archie [6], Gopher [13], and FTP archives also replicate their data for better performance.
Reference: [2] <author> M. Bowman, P. B. Danzig, U. Manber, and M. Schwartz. </author> <title> Scalable internet resource discovery: Research problems and approaches. </title> <journal> Communications of the ACM, </journal> <volume> 37(8), </volume> <month> August </month> <year> 1994. </year>
Reference-contexts: They also limit network traffic associated with group membership. 2.4 Implementation Flood-d provides a logical update topology computation service that other applications can use. For instance, to replicate Harvest <ref> [2] </ref> servers, whose data is organized as a directory tree, mirror-d [19], our weak consistent file archiver, propagates updates according to flood-d's current topology.
Reference: [3] <author> A. Chankhunthod, P. Danzig, C. Neerdaels, M. Schwartz, and K. Worrell. </author> <title> A hierarchical internet object cache. </title> <booktitle> Proceedings of the USENIX 1996 Conference, </booktitle> <pages> pages 153-163, </pages> <month> January </month> <year> 1996. </year> <note> http://catarina.usc.edu/danzig/cache.ps. </note>
Reference-contexts: However, more recently developed applications, most of which sparked by the Internet's exponential growth, can also benefit from flood-d's services. For instance, network and server load information are critical for distributed computation management tools like Globus [7] and PRM [17]. Internet cache hierarchies such as Harvest <ref> [3] </ref> and Squid [25] can also use flood-d's network and server load information to configure their cache servers. Flood-d's logical topologies can be tailored to fit the requirements of different distributed applications.
Reference: [4] <author> P.B. Danzig, K. Obraczka, and A. Kumar. </author> <title> An analysis of wide-area name server traffic: A study of the domain name system. </title> <booktitle> Proc. of the ACM SIGCOMM '92, </booktitle> <address> Baltimore, Maryland, </address> <pages> pages 281-292, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: They also use these administrative boundaries to partition their name space into several domains, which only need to be replicated in a handful of servers to meet adequate performance. In fact, in <ref> [4] </ref>, we show that over 85% of second level domains in the DNS hierarchy are replicated at most three times, while 100% of these domains use at most 7 replicas. In the same study, we also show that more than 90% of DNS's second-level domains store less than 1,000 entries.
Reference: [5] <author> S. Deering and D. Cheriton. </author> <title> Multicast routing in datagram internetworks and extended lans. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 8(2) </volume> <pages> 85-111, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: For consistency most services mirror their replicas off of a primary site or use a manually configured mirroring topology. 1.3 Multicast IP multicast and reliable multicast transport protocols are often considered a good foundation on which to build data replication protocols. IP multicasting <ref> [5] </ref> delivers best-effort datagrams to a group of hosts sharing a single multicast address. It relies on transport-level protocols for reliability and sequencing.
Reference: [6] <author> A. Emtage and P. Deutsch. archie: </author> <title> An electronic directory service for the Internet. </title> <booktitle> Proceedings of the Winter 1992 Usenix Conference, </booktitle> <pages> pages 93-110, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: They try to keep up with the dynamics of the underlying physical topology, which becomes increasingly harder as the Internet's scale and complexity increases. Other Internet information services such as the Web [1], archie <ref> [6] </ref>, Gopher [13], and FTP archives also replicate their data for better performance.
Reference: [7] <author> I. Foster and C. Kesselman. </author> <title> The globus project. </title> <note> Globus web page http://www.globus.org/, June 1997. </note>
Reference-contexts: However, more recently developed applications, most of which sparked by the Internet's exponential growth, can also benefit from flood-d's services. For instance, network and server load information are critical for distributed computation management tools like Globus <ref> [7] </ref> and PRM [17]. Internet cache hierarchies such as Harvest [3] and Squid [25] can also use flood-d's network and server load information to configure their cache servers. Flood-d's logical topologies can be tailored to fit the requirements of different distributed applications.
Reference: [8] <author> R. A. Golding. </author> <title> Weak-Consistency Group Communication and Membership. </title> <type> PhD thesis, </type> <institution> University of California, Santa Cruz, </institution> <month> December </month> <year> 1992. </year> <note> Computer and Information Sciences Technical Report UCSC-CRL-92-52. </note>
Reference-contexts: We start by briefly reviewing information service consistency requirements and examining existing replication algorithms. 1.1 Information Service Consistency Group communication mechanisms for wide-area, replicated information services should not trade availability and response time for globally ordered delivery <ref> [8] </ref>. On one hand, these services need to guarantee that replicas eventually converge to a consistent, updated state during both normal operation and when recovering from network partition and server or link failures. <p> Golding modified Grapevine's consistency maintenance protocol to eliminate its garbage collection problems. He named the modified algorithm Timestamped Anti-Entropy (TSAE) Protocol and used it to build a replicated, distributed bibliographic database system <ref> [8] </ref>. Like other replication algorithms, TSAE floods updates. Periodically, a replica starts an anti-entropy session, in which it selects a peer to exchange updates.
Reference: [9] <author> S. Hotz and R. Nagamati. </author> <title> Network topology generator (NTG): A tool for generating network topology and policy for protocol simulation purposes. </title> <type> Technical Report, </type> <institution> Computer Science Department, University of Southern California, </institution> <month> Spring </month> <year> 1992. </year>
Reference-contexts: We compare two different partner selection policies: random, in which a replica randomly chooses another replica to exchange consistency state, and cost-based, in which the peer 11 12 with the minimum cost link is selected. To assign communication costs to links, we use NTG <ref> [9] </ref>, a random topology generator, to generate logical topologies for replication groups of different sizes. We feed the topology generator with the group size, average node degree, and link bandwidths.
Reference: [10] <author> J. Howard, M. Kazar, S. Menees, D. Nichols, M. Satyanarayanan, R. Sidebotham, and M. West. </author> <title> Scale and performance in a distributed file system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 51-81, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: Because of the limited domain sizes and small number of replicas, DNS's primary-copy replication scheme performs quite adequately. Similarly, distributed file systems organize their file space hierarchically, where intermediate nodes are directories and leaf nodes are files. Like LOCUS [21], Andrew-AFS 2 <ref> [16, 10] </ref>, and Coda [23], distributed file systems use locality of reference to partition their file space into directory subtrees. File servers replicate a subset of files in a directory subtree.
Reference: [11] <author> B. Kantor and P. Lapsley. </author> <title> Network news transfer protocol a proposed standard for the stream-based transmission of news. Internet Request for Comments RFC 977, </title> <month> February </month> <year> 1986. </year>
Reference-contexts: On the other hand, they need not compromise their availability and response time and incur the extra overhead of strong consistency protocols. Grapevine [24], the Global Name Service [12], and Usenet News (or Netnews for short) <ref> [11] </ref> use weak consistency replication mechanisms.
Reference: [12] <author> B. Lampson. </author> <title> Designing a global name service. </title> <booktitle> Proceedings of the 5th. ACM Symposium on the Principles of Distributed Computing, </booktitle> <pages> pages 1-10, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: On the other hand, they need not compromise their availability and response time and incur the extra overhead of strong consistency protocols. Grapevine [24], the Global Name Service <ref> [12] </ref>, and Usenet News (or Netnews for short) [11] use weak consistency replication mechanisms.
Reference: [13] <author> M. McCahill. </author> <title> The Internet Gopher protocol: a distributed server information system. </title> <journal> ConneXions The Interoperability Report, </journal> <volume> 6(7) </volume> <pages> 10-14, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: They try to keep up with the dynamics of the underlying physical topology, which becomes increasingly harder as the Internet's scale and complexity increases. Other Internet information services such as the Web [1], archie [6], Gopher <ref> [13] </ref>, and FTP archives also replicate their data for better performance.
Reference: [14] <author> P. Mockapetris. </author> <title> Domain names concepts and facilities. Internet Request for Comments RFC 1034, </title> <month> November </month> <year> 1987. </year>
Reference-contexts: Naming services such as the Domain Name Service (DNS) <ref> [14, 15] </ref> organize their name space hierarchically according to well-defined administrative boundaries. They also use these administrative boundaries to partition their name space into several domains, which only need to be replicated in a handful of servers to meet adequate performance.
Reference: [15] <author> P. Mockapetris. </author> <title> Domain names implementation and specification. Internet Request for Comments RFC 1035, </title> <month> November </month> <year> 1987. </year>
Reference-contexts: Naming services such as the Domain Name Service (DNS) <ref> [14, 15] </ref> organize their name space hierarchically according to well-defined administrative boundaries. They also use these administrative boundaries to partition their name space into several domains, which only need to be replicated in a handful of servers to meet adequate performance.
Reference: [16] <author> J. Morris, M. Satyanarayanan, M. Conner, J. Howard, D. Rosenthal, and F. Smith. Andrew: </author> <title> A distributed personal computing environment. </title> <journal> Communications of the ACM, </journal> <volume> 29(3) </volume> <pages> 184-201, </pages> <month> March </month> <year> 1986. </year> <month> 20 </month>
Reference-contexts: Because of the limited domain sizes and small number of replicas, DNS's primary-copy replication scheme performs quite adequately. Similarly, distributed file systems organize their file space hierarchically, where intermediate nodes are directories and leaf nodes are files. Like LOCUS [21], Andrew-AFS 2 <ref> [16, 10] </ref>, and Coda [23], distributed file systems use locality of reference to partition their file space into directory subtrees. File servers replicate a subset of files in a directory subtree.
Reference: [17] <author> Clifford Neuman and Santosh Rao. </author> <title> The prospero resource manager (prm). </title> <note> PRM web page http:nii.isi.edu/gost-group/products/prm, June 1997. </note>
Reference-contexts: However, more recently developed applications, most of which sparked by the Internet's exponential growth, can also benefit from flood-d's services. For instance, network and server load information are critical for distributed computation management tools like Globus [7] and PRM <ref> [17] </ref>. Internet cache hierarchies such as Harvest [3] and Squid [25] can also use flood-d's network and server load information to configure their cache servers. Flood-d's logical topologies can be tailored to fit the requirements of different distributed applications.
Reference: [18] <author> K. Obraczka, P.B. Danzig, D. DeLucia, and E. Tsai. </author> <title> A tool for massively replicating internet archives: Design, implementation, and experience. </title> <booktitle> Proceedings of the 16th. IEEE ICDCS, </booktitle> <pages> pages 657-664, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: In our implementation, flood-d periodically measures available bandwidth and propagation delay among participating sites. Estimates collected at each site are gathered into a cost matrix which flood-d uses to compute the logical topology connecting these sites. A separate paper <ref> [18] </ref> reports the design and implementation of flood-d, as well as preliminary experimental performance results. Flood-d was originally targeted at improving the performance of replicated information services including replicated databases, and distributed information dissemination services, such as the Web [1] and Internet archives.
Reference: [19] <author> Katia Obraczka. </author> <title> Massively Replicating Services in Wide-Area Internetworks. </title> <type> PhD thesis, </type> <institution> University of Southern California, </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: They also limit network traffic associated with group membership. 2.4 Implementation Flood-d provides a logical update topology computation service that other applications can use. For instance, to replicate Harvest [2] servers, whose data is organized as a directory tree, mirror-d <ref> [19] </ref>, our weak consistent file archiver, propagates updates according to flood-d's current topology. <p> We are currently using k = 2. A simulated annealing algorithm to construct k-connected, low edge-cost, low diameter graphs is described and evaluated in <ref> [19] </ref>. The master then floods the new topology to all group members. Topology update messages also contain the current group membership.
Reference: [20] <author> D. Oppen and Y. Dalal. </author> <title> The Clearinghouse: A decentralized agent for locating named objects in a distributed environment. </title> <journal> ACM Transactions on Office Information Systems, </journal> <volume> 1(3) </volume> <pages> 230-253, </pages> <month> July </month> <year> 1983. </year>
Reference-contexts: Because layered network protocols hide the network topology from application programs, replicas themselves cannot select their flooding peers to optimize use of the network. Both Grapevine and its commercial successor, the Clearinghouse <ref> [20] </ref> ignore network and update topology. The Global Name Service assumes the existence of a single administrator who hand-configures the topology over which updates travel. The Global Name Service administrator places replicas in a Hamiltonian cycle, and reconfigures the ring when replicas are added or removed.
Reference: [21] <author> G. Popek, B. Walker, J. Chow, D. Edwards, C. Kline, and G. Rudisin ang G. Thiel. </author> <title> LOCUS: A network transparent, high reliability distributed system. </title> <booktitle> Proc. of the 8th. Symposium on Operating Systems Principles, </booktitle> <pages> pages 169-177, </pages> <month> December </month> <year> 1981. </year>
Reference-contexts: Because of the limited domain sizes and small number of replicas, DNS's primary-copy replication scheme performs quite adequately. Similarly, distributed file systems organize their file space hierarchically, where intermediate nodes are directories and leaf nodes are files. Like LOCUS <ref> [21] </ref>, Andrew-AFS 2 [16, 10], and Coda [23], distributed file systems use locality of reference to partition their file space into directory subtrees. File servers replicate a subset of files in a directory subtree.
Reference: [22] <author> J.H. Saltzer, D.P. Reed, and D.D. Clark. </author> <title> End-To-End arguments in system design. </title> <booktitle> Proceedings of the 2nd International Conference on Distributed Systems, </booktitle> <pages> pages 509-512, </pages> <month> April </month> <year> 1981. </year>
Reference-contexts: In particular, it can not solve the consistency problem raised when replicas temporarily crash or when IP routers crash and lose state crucial to reliable, multipoint delivery. In such cases, the application itself must re-establish consistency. Recall the end-to-end argument in layered design <ref> [22] </ref>; functions that can only be completely and correctly implemented by the application should be moved into the application.
Reference: [23] <author> M. Satyanarayanan. </author> <title> Scalable, secure, and highly available distributed file access. </title> <journal> Computer Magazine, </journal> <volume> 23(5) </volume> <pages> 9-21, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Because of the limited domain sizes and small number of replicas, DNS's primary-copy replication scheme performs quite adequately. Similarly, distributed file systems organize their file space hierarchically, where intermediate nodes are directories and leaf nodes are files. Like LOCUS [21], Andrew-AFS 2 [16, 10], and Coda <ref> [23] </ref>, distributed file systems use locality of reference to partition their file space into directory subtrees. File servers replicate a subset of files in a directory subtree. Both LOCUS and Andrew provide read-only file replication, while Coda uses distributed updates to keep its read-write file replicas weakly consistent.
Reference: [24] <author> M. Schroeder, A. Birrell, and R. Needham. </author> <title> Experience with Grapevine: The growth of a distributed system. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 2(1) </volume> <pages> 3-23, </pages> <month> February </month> <year> 1984. </year>
Reference-contexts: On the other hand, they need not compromise their availability and response time and incur the extra overhead of strong consistency protocols. Grapevine <ref> [24] </ref>, the Global Name Service [12], and Usenet News (or Netnews for short) [11] use weak consistency replication mechanisms.
Reference: [25] <author> D. Wessels. </author> <title> Squid internet object cache. </title> <note> Squid web page http://squid.nlanr.net/, July 1997. </note>
Reference-contexts: However, more recently developed applications, most of which sparked by the Internet's exponential growth, can also benefit from flood-d's services. For instance, network and server load information are critical for distributed computation management tools like Globus [7] and PRM [17]. Internet cache hierarchies such as Harvest [3] and Squid <ref> [25] </ref> can also use flood-d's network and server load information to configure their cache servers. Flood-d's logical topologies can be tailored to fit the requirements of different distributed applications. In the case of replicated information services, flood-d generates fault-tolerant topologies that try to minimize both update propagation cost and time.
References-found: 25

