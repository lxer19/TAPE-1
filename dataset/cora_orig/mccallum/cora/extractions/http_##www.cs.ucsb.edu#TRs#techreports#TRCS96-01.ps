URL: http://www.cs.ucsb.edu/TRs/techreports/TRCS96-01.ps
Refering-URL: http://www.cs.ucsb.edu/TRs/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Run-time Compilation for Parallel Sparse Matrix Computations  
Author: Cong Fu and Tao Yang 
Web: http://www.cs.ucsb.edu/f~cfu,~tyangg  
Address: Santa Barbara, CA 93106.  
Affiliation: Department of Computer Science University of California,  
Abstract: Run-time compilation techniques have been shown effective for automating the parallelization of loops with unstructured indirect data accessing patterns. However, it is still an open problem to efficiently parallelize sparse matrix factorizations commonly used in iterative numerical problems. The difficulty is that a factorization process contains irregularly-interleaved communication and computation with varying granularities and it is hard to obtain scalable performance on distributed memory machines. In this paper, we present an inspector/executor approach for parallelizing such applications by embodying automatic graph scheduling techniques to optimize interleaved communication and computation. We describe a run-time system called RAPID that provides a set of library functions for specifying irregular data objects and tasks that access these objects. The system extracts a task dependence graph from data access patterns, and executes tasks efficiently on a distributed memory machine. We discuss a set of optimization strategies used in this system and demonstrate the application of this system in parallelizing sparse Cholesky and LU factorizations. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> F. T. Chong, S. D. Sharma, E. A. Brewer, and J. Saltz. </author> <title> Multiprocessor Runtime Support for Fine-Grained Irregular DAGs. </title> <editor> In Rajiv K. Kalia and Priya Vashishta, editors, </editor> <title> Toward Teraflop Computing and New Grand Challenge Applications., </title> <address> New York, 1995. </address> <publisher> Nova Science Publishers. </publisher>
Reference-contexts: Since sparse matrix factorization dominates the computation at each iteration, an effective run-time optimization at the inspector stage could improve the code performance at the executor stage substantially. Previous results <ref> [1, 8, 17, 20] </ref> have demonstrated that graph scheduling can effectively exploit irregular task parallelism if task dependencies are given explicitly. We generalize the previous work and discuss a run-time library system called RAPID for exploiting general irregular parallelism embedded in unstructured task graphs.
Reference: [2] <author> M. Cosnard and M. Loi. </author> <title> Automatic Task Graph Generation Techniques. </title> <journal> Parallel Processing Letters, </journal> <volume> 5(4) </volume> <pages> 527-538, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: Such codes are hard to write using existing parallel languages or libraries. The focus of this paper is to demonstrate how we are able to deliver good performance for sparse codes and our future work will address the automatic generation of inspector specification code and automatic task partitioning <ref> [2, 11, 15] </ref>. Cholesky factorization is performed on a symmetric positive definite matrix A of size n fi n. In a block sparse Cholesky algorithm as shown in Figure 2, matrix A is partitioned into N fi N submatrices. A partitioning example is shown in Figure 4.
Reference: [3] <author> R. Cytron and J. Ferrante. </author> <title> What's in a name? The Value of Renaming for Parallelism Detection and Storage Allocation. </title> <booktitle> In Proc. of International Conf. on Parallel Processing, </booktitle> <pages> pages 19-27, </pages> <month> February </month> <year> 1987. </year>
Reference-contexts: The true dependence can be enforced easily since one task cannot start to execute until the required data objects produced by its predecessors arrive at the local processor. With the presence of anti and output dependencies, run-time synchronization becomes more complicated. We can use renaming techniques <ref> [3] </ref> to remove these dependencies, however it needs additional memory optimization. We use the following simple strategy to remove output and anti dependence. 1) First we delete all the redundant edges for output and anti dependencies if they are subsumed by a dependence path in the graph.
Reference: [4] <author> R. Das, M. Uysal, J. Saltz, and Y.-S. Hwang. </author> <title> Communication Optimizations for Irregular Scientific Computations on Distributed Memory Architectures . Journal of Parallel and Distributed Computing, </title> <booktitle> 22(3) </booktitle> <pages> 462-479, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: 1 Introduction Program transformation and parallelization techniques for structured codes have been shown successful in many application domains. However it is still difficult to parallelize unstructured codes, which can be found in many scientific applications [18]. In <ref> [4] </ref> an important class of unstructured and sparse problems which involve iterative computations is identified and has been successfully parallelized using the inspector/executor approach. The cost of optimizations conducted at the inspector stage is amortized over many computation iterations at the executor stage.
Reference: [5] <author> J. Demmel, S. Eisenstat, J. Gilbert, X. Li, and J. Liu. </author> <title> A Supernodal Approach to Sparse Partial Pivoting. </title> <type> Technical Report CSD-95-883, </type> <institution> UC Berkeley, </institution> <month> September </month> <year> 1995. </year>
Reference-contexts: The single node performance is reasonable since it has reached 25 45% of LIN-PACK performance and the multi-processor speedups are good considering the current status of parallel sparse LU factorization research. This work is still preliminary and we are comparing it with other sparse LU code <ref> [5] </ref>.
Reference: [6] <author> C. Fu and T. Yang. </author> <title> Efficient Run-time Support for Irregular Task Computations with Mixed Granularities. </title> <booktitle> In Proceedings of IEEE International Parallel Processing Symposium, Hawaii, </booktitle> <month> April </month> <year> 1996. </year>
Reference-contexts: It should be noted that a full optimization at a preprocessing stage does not suffice to produce efficient code. A careful design in the task communication model is further required to execute the pre-optimized task computation and communication schedule. In <ref> [6] </ref> we have developed an efficient run-time task communication protocol for executing general irregular task computations with mixed granulari-ties. The distinguishing feature of the protocol is that we tightly and correctly incorporate several communication optimizations together in one execution framework. <p> Another presentation is called left-looking. Our task model can capture parallelism arising from both styles. We have reported good results based on 2-D block partitioning of a sparse matrix in <ref> [6] </ref> using our run-time support. Here we further incorporate the advantage of commuting tasks. The choice of block size depends on the cache size of the target machine. But a block size that is too large will reduce available parallelism. <p> Only in one case the performance actually degrades and this could be caused by changing the clustering strategy. 5.2 Sparse LU factorization with partial pivoting LU factorization without pivoting can be used for positive definite or strictly diagonally dominant matrices. We have reported good performance for it in <ref> [6] </ref>. However in many cases pivoting is needed to maintain the numerical stability.
Reference: [7] <author> C. Fu and T. Yang. </author> <title> Run-time Compilation for Parallel Sparse Matrix Computations. </title> <type> Technical Report TRCS96-01, </type> <institution> UCSB, </institution> <year> 1996. </year>
Reference-contexts: Section 5 presents experimental results on sparse Cholesky factorization and sparse LU factorization with partial pivoting. It also analyzes the system overhead and presents a performance improvement after increasing task granular-ities. Section 6 gives the conclusions. A longer version of this paper is <ref> [7] </ref>. 2 System Overview At the inspector stage, the RAPID system provides a set of C library functions for users to specify shared data objects, and tasks that access these objects. Then it derives the dependence structure from the specification, performs dependence transformation and maps tasks to multi-processors. <p> Illustration of sender site inconsistency. Based on the properties of a scheduled task graph discussed in Section 3.3, we can show the above inconsistency situations will never happen in RAPID. The detailed proofs can be found in <ref> [7] </ref>. Therefore, the execution of a scheduled task graph is correct in the RAPID system. Theorem 1 Given a scheduled task graph, the executor stage will have no receiver site inconsistency and no sender site inconsistency.
Reference: [8] <author> A. Gerasoulis, J. Jiao, and T. Yang. </author> <title> Scheduling of Structured and Unstructured Computation . In Dominique Sot-teau D. </title> <editor> Frank Hsu, Arnold Rosenberg, editor, </editor> <title> Interconnections Networks and Mappings and Scheduling Parallel Computation. </title> <journal> American Math. Society, </journal> <year> 1995. </year>
Reference-contexts: Since sparse matrix factorization dominates the computation at each iteration, an effective run-time optimization at the inspector stage could improve the code performance at the executor stage substantially. Previous results <ref> [1, 8, 17, 20] </ref> have demonstrated that graph scheduling can effectively exploit irregular task parallelism if task dependencies are given explicitly. We generalize the previous work and discuss a run-time library system called RAPID for exploiting general irregular parallelism embedded in unstructured task graphs. <p> However, another important reason is that pre-determined schedule does not match actual run-time situation very well. This is mainly caused by run-time computation weights variation. When the variation is within a small range, the static schedule can still deliver good performance <ref> [8, 19] </ref>. But when the task granularities are small, the performance will become very sensitive to the weights variation. We have examined distribution of supernode sizes after splitting large supern-odes for matrices BCSSTK15 and BCSSTK24.
Reference: [9] <author> M. Heath, E. Ng, and B. Peyton. </author> <title> Parallel Algorithms for Sparse Linear Systems . SIAM Review, </title> <booktitle> 33(3) </booktitle> <pages> 420-460, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: In a block sparse Cholesky algorithm as shown in Figure 2, matrix A is partitioned into N fi N submatrices. A partitioning example is shown in Figure 4. Notice that this submatrix partitioning is not uniform due to supernode partitioning <ref> [9, 14] </ref>. We assume that the nonzero structure information is available after symbolic factorization and supernode partitioning. These operations are performed before task specification. Each data object is defined as a non-zero sub-matrix of A.
Reference: [10] <author> S.J. Kim and J.C. Browne. </author> <title> A General Approach to Mapping of Parallel Computation upon Multiprocessor Architectures, </title> . <booktitle> In International Conf. on Parallel Processing, </booktitle> <pages> pages 318-328, </pages> <year> 1988. </year>
Reference-contexts: We use a graph scheduling algorithm to exploit task parallelism and also determine the execution order of commuting operations so as to minimize parallel time. Algorithms for static scheduling of DAGs have been extensively studied in the literatures, e.g. <ref> [10, 15, 20] </ref>. The main optimizations are eliminating unnecessary communication to exploit data locality, overlapping communication with computation to hide communication latency, and exploiting task concurrency to balance loads among processors. A global performance monitoring for minimizing the overall execution time is needed to guide these optimizations.
Reference: [11] <author> C. D. </author> <title> Polychronopoulos. </title> <publisher> Parallel Programming and Compilers . Kluwer Academic Publishers, </publisher> <year> 1988. </year>
Reference-contexts: Such codes are hard to write using existing parallel languages or libraries. The focus of this paper is to demonstrate how we are able to deliver good performance for sparse codes and our future work will address the automatic generation of inspector specification code and automatic task partitioning <ref> [2, 11, 15] </ref>. Cholesky factorization is performed on a symmetric positive definite matrix A of size n fi n. In a block sparse Cholesky algorithm as shown in Figure 2, matrix A is partitioned into N fi N submatrices. A partitioning example is shown in Figure 4.
Reference: [12] <author> M. Rinard. </author> <title> Communication Optimizations for Parallel Computing Using Data Access Information . In Proceedings of Supercomputing, </title> <address> San Diego, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: The system makes use of graph transformation and scheduling techniques, and an efficient task communication protocol. The design of library functions is based on three concepts: distributed shared data objects, tasks and access specifications. Similar concepts have been proposed in JADE <ref> [12] </ref> which extracts task dependence and schedules tasks dynamically. Such an approach has a flexibility to handle problems with adaptive structures; however, it is still an open problem to balance the benefits of such flexibility and run-time control overhead in parallelizing applications such as sparse matrix factorization [12]. <p> proposed in JADE <ref> [12] </ref> which extracts task dependence and schedules tasks dynamically. Such an approach has a flexibility to handle problems with adaptive structures; however, it is still an open problem to balance the benefits of such flexibility and run-time control overhead in parallelizing applications such as sparse matrix factorization [12]. Our approach extracts dependence and schedules tasks at the inspector stage to trade flexibility for performance. It should be noted that a full optimization at a preprocessing stage does not suffice to produce efficient code.
Reference: [13] <author> E. Rothberg. </author> <title> Exploiting the Memory Hierarchy in Sequential and Parallel Sparse Cholesky Factorization . PhD thesis, </title> <institution> Dept. of Computer Science, Stanford, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: We have reported good performance for it in [6]. However in many cases pivoting is needed to maintain the numerical stability. Because partial pivoting operations make the data struc tures change dynamically, it has been an open problem to 1 In comparison, Rothberg <ref> [13] </ref> reported that the sequential performance of his code achieved 77% of LINPACK for BCSSTK15 and 71% in average for other matrices on IBM RS/6000 Model 320. <p> In order to improve the performance further, we need to increase the average task grain size. One way is to use the supernode amalgamation <ref> [13] </ref> technique. The idea behind the supernode amalgamation is that we can merge several small supernodes together to form a big one so that the corresponding task becomes coarser. This merging process is done by relaxing the restriction that all the columns in the same supernode have identical nonzero pattern.
Reference: [14] <author> E. Rothberg and R. Schreiber. </author> <title> Efficient Parallel Sparse Cholesky Factorization . In Proc. </title> <booktitle> of Seventh SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 407-412, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: In a block sparse Cholesky algorithm as shown in Figure 2, matrix A is partitioned into N fi N submatrices. A partitioning example is shown in Figure 4. Notice that this submatrix partitioning is not uniform due to supernode partitioning <ref> [9, 14] </ref>. We assume that the nonzero structure information is available after symbolic factorization and supernode partitioning. These operations are performed before task specification. Each data object is defined as a non-zero sub-matrix of A.
Reference: [15] <author> V. Sarkar. </author> <title> Partitioning and Scheduling Parallel Programs for Execution on Multiprocessors . MIT Press, </title> <year> 1989. </year>
Reference-contexts: Such codes are hard to write using existing parallel languages or libraries. The focus of this paper is to demonstrate how we are able to deliver good performance for sparse codes and our future work will address the automatic generation of inspector specification code and automatic task partitioning <ref> [2, 11, 15] </ref>. Cholesky factorization is performed on a symmetric positive definite matrix A of size n fi n. In a block sparse Cholesky algorithm as shown in Figure 2, matrix A is partitioned into N fi N submatrices. A partitioning example is shown in Figure 4. <p> We use a graph scheduling algorithm to exploit task parallelism and also determine the execution order of commuting operations so as to minimize parallel time. Algorithms for static scheduling of DAGs have been extensively studied in the literatures, e.g. <ref> [10, 15, 20] </ref>. The main optimizations are eliminating unnecessary communication to exploit data locality, overlapping communication with computation to hide communication latency, and exploiting task concurrency to balance loads among processors. A global performance monitoring for minimizing the overall execution time is needed to guide these optimizations.
Reference: [16] <author> K. E. Schauser and C. J. Scheiman. </author> <title> Experience with Active Messages on the Meiko CS-2. </title> <booktitle> In International Parallel Processing Symposium, </booktitle> <pages> pages 140-149, </pages> <year> 1995. </year>
Reference-contexts: The only requirement is that a processor needs to allocate a dedicated space for each data object it needs. RMA can be implemented in modern multi-processor architectures such as Cray-T3D and Meiko CS-2 <ref> [16] </ref>. We have implemented our system on Meiko CS-2 which provides Direct Memory Access (DMA) as the major way to access non-local memory. Eliminating redundant communication and synchronization. A task may send the same message to several successors and some of these successors could be assigned to the same processor. <p> The effective communication bandwidth of Meiko CS-2 is about 1 5MBytes/sec for a double-precision matrix of size less than 6 fi 6 under the assumption that the message is sent only once at a time. The bandwidth would be higher if a ping-pong type of test is conducted <ref> [16] </ref>. Also it takes the main processor 9 microseconds [16] to dispatch a remote memory access descriptor to the communication co-processor. 5.1 Sparse Cholesky factorization The Cholesky program in Figure 2 is presented in a right-looking style. Another presentation is called left-looking. <p> The bandwidth would be higher if a ping-pong type of test is conducted <ref> [16] </ref>. Also it takes the main processor 9 microseconds [16] to dispatch a remote memory access descriptor to the communication co-processor. 5.1 Sparse Cholesky factorization The Cholesky program in Figure 2 is presented in a right-looking style. Another presentation is called left-looking. Our task model can capture parallelism arising from both styles.
Reference: [17] <author> S. Venugopal, V. Naik, and J. Saltz. </author> <title> Performance of Distributed Sparse Cholesky Factorization with Pre-scheduling. </title> <booktitle> In Proc. of Supercomputing'92, </booktitle> <pages> pages 52-61, </pages> <address> Minneapolis, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: Since sparse matrix factorization dominates the computation at each iteration, an effective run-time optimization at the inspector stage could improve the code performance at the executor stage substantially. Previous results <ref> [1, 8, 17, 20] </ref> have demonstrated that graph scheduling can effectively exploit irregular task parallelism if task dependencies are given explicitly. We generalize the previous work and discuss a run-time library system called RAPID for exploiting general irregular parallelism embedded in unstructured task graphs.
Reference: [18] <author> C.-P. Wen, S. Chakrabarti, E. Deprit, A. Krishnamurthy, and K. Yelick. </author> <title> Runtime Support for Portable Distributed Data Structures, chapter 9. Languages, Compilers, and Runtime Systems for Scalable Computers. </title> <publisher> Kluwer Academic Publishers, </publisher> <month> May </month> <year> 1995. </year>
Reference-contexts: 1 Introduction Program transformation and parallelization techniques for structured codes have been shown successful in many application domains. However it is still difficult to parallelize unstructured codes, which can be found in many scientific applications <ref> [18] </ref>. In [4] an important class of unstructured and sparse problems which involve iterative computations is identified and has been successfully parallelized using the inspector/executor approach. The cost of optimizations conducted at the inspector stage is amortized over many computation iterations at the executor stage.
Reference: [19] <author> T. Yang, C. Fu, A. Gerasoulis, and V. Sarkar. </author> <title> Mapping Iterative Task Graphs on Distributed-memory Machines . In International Conf. </title> <booktitle> on Parallel Processing, </booktitle> <pages> pages 151-158, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: However, another important reason is that pre-determined schedule does not match actual run-time situation very well. This is mainly caused by run-time computation weights variation. When the variation is within a small range, the static schedule can still deliver good performance <ref> [8, 19] </ref>. But when the task granularities are small, the performance will become very sensitive to the weights variation. We have examined distribution of supernode sizes after splitting large supern-odes for matrices BCSSTK15 and BCSSTK24.
Reference: [20] <author> T. Yang and A. Gerasoulis. </author> <title> PYRROS: Static Task Scheduling and Code Generation for Message-Passing Multiprocessors . In Proc. </title> <booktitle> of 6th ACM Inter. Confer. on Supercomputing, </booktitle> <pages> pages 428-437, </pages> <year> 1992. </year>
Reference-contexts: Since sparse matrix factorization dominates the computation at each iteration, an effective run-time optimization at the inspector stage could improve the code performance at the executor stage substantially. Previous results <ref> [1, 8, 17, 20] </ref> have demonstrated that graph scheduling can effectively exploit irregular task parallelism if task dependencies are given explicitly. We generalize the previous work and discuss a run-time library system called RAPID for exploiting general irregular parallelism embedded in unstructured task graphs. <p> We use a graph scheduling algorithm to exploit task parallelism and also determine the execution order of commuting operations so as to minimize parallel time. Algorithms for static scheduling of DAGs have been extensively studied in the literatures, e.g. <ref> [10, 15, 20] </ref>. The main optimizations are eliminating unnecessary communication to exploit data locality, overlapping communication with computation to hide communication latency, and exploiting task concurrency to balance loads among processors. A global performance monitoring for minimizing the overall execution time is needed to guide these optimizations. <p> At the second stage, clusters are mapped to a fixed number of physical processors available at the run-time by using the PYRROS algorithm <ref> [20] </ref> to balance loads and overlap computation with communication. chart. (b) The scheduled task graph. Notice that task T 3 has to wait for the arrival of data object x sent from T 1 at processor 0. Computation of T 2 can be overlapped with this sending. <p> Eliminating redundant communication and synchronization. A task may send the same message to several successors and some of these successors could be assigned to the same processor. In this case, it is enough to send this message once to the processor on which those successors reside <ref> [20] </ref>. Another optimization is to eliminate unnecessary synchronization. Since the RMA directly writes data to a remote address, it is possible that the copy at the remote address is still being used by other tasks and then the execution at the remote processor is incorrect.
Reference: [21] <author> T. Yang and A. Gerasoulis. </author> <title> DSC: Scheduling Parallel Tasks on An Unbounded Number of Processors . IEEE Transactions on Parallel and Distributed Systems, </title> <booktitle> 5(9) </booktitle> <pages> 951-967, </pages> <year> 1994. </year> <note> A short version is Proc. of Supercomputing '91. </note>
Reference-contexts: Our scheduling algorithm using weight and dependence information has two stages. At the first stage, we cluster tasks to a set of threads (or directly call them clusters) to reduce communication and exploit the data locality. Two clustering strategies are used: 1) Use the DSC algorithm <ref> [21] </ref>. 2) Form clusters based on the data accessing patterns. If tasks write or modify the same data object, they will be assigned into one cluster. This data-driven approach is essentially following the owner-computes rule. Currently we use this strategy when a task graph contains commuting operations.
References-found: 21

