URL: http://bruce.www.media.mit.edu/people/bruce/siggraph.ps
Refering-URL: http://bruce.www.media.mit.edu/people/bruce/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: MultiLevel Direction of Autonomous Creatures for Real-Time Virtual Environments  
Author: Bruce M. Blumberg and Tinsley A. Galyean 
Affiliation: MIT Media Lab  
Abstract: There have been several recent efforts to build behavior-based autonomous creatures. While competent autonomous action is highly desirable, there is an important need to integrate autonomy with directability. In this paper we discuss the problem of building autonomous animated creatures for interactive virtual environments which are also capable of being directed at multiple levels. We present an approach to control which allows an external entity to direct an autonomous creature at the motivational level, the task level, and the direct motor level. We also detail a layered architecture and a general behavioral model for perception and actionselection which incorporates explicit support for multilevel direction. These ideas have been implemented and used to develop several autonomous animated creatures. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Arkin, </author> <title> R.C., Integrating Behavioral, Perceptual, and World Knowledge in Reactive Navigation, in Designing Autonomous Agents, </title> <editor> P. Maes, Editor. </editor> <booktitle> 1990, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, pp.105-122. </address>
Reference-contexts: Subsequently, a gradient field is calculated, and this is used to derive a bearing away from areas of high potential. Following Arkin <ref> [1] </ref>, some behaviors within the Behavior System represent their pattern of activity as a potential fields as well (for example, moveto). These potential fields are combined with the field generated by the vision sensor to arrive at a compromise trajectory.
Reference: 2. <author> Badler, N.I., C. Phillips, and B.L. Webber, </author> <title> Simulating Humans: Computer Graphics, Animation, and Control . 1993, </title> <publisher> Oxford University Press, </publisher> <address> New York. </address>
Reference-contexts: Such creatures are, in effect, autonomous agents with their own perceptional, behavioral, and motor systems. Typically, authors have focused on behavioral models for a specific kind of creature in a given environment, and implemented a limited set of behaviors. There are examples of locomotion <ref> [2, 5, 7, 14, 16] </ref> , flocking [1 8], grasping [9], and lifting [2]. Tu and Terzopoulos's Fish [20] represent one of the most impressive examples of this approach. Advances in behavioral animation are critically important to the development of creatures for use in interactive virtual environments. <p> Typically, authors have focused on behavioral models for a specific kind of creature in a given environment, and implemented a limited set of behaviors. There are examples of locomotion [2, 5, 7, 14, 16] , flocking [1 8], grasping [9], and lifting <ref> [2] </ref>. Tu and Terzopoulos's Fish [20] represent one of the most impressive examples of this approach. Advances in behavioral animation are critically important to the development of creatures for use in interactive virtual environments. <p> These levels are detailed in Figure 1. By providing the ability to direct the creature at multiple levels the animator or developer can choose the appropriate level of control for a given situation. Both Badler and Zeltzer have proposed similar decomposition of control <ref> [2, 23] </ref>. 2.2 A General Behavior Model We propose a distributed behavioral model, inspired by work in Ethology and autonomous robot research, for perception and actionselection in autonomous animated creatures but which also supports external control.
Reference: 3. <editor> Blumberg, B. ActionSelection in Hamsterdam: </editor> <title> Lessons from Ethology. </title> <booktitle> in Third International Conference on the Simulation of Adaptive Behavior . Brighton, England,1 994, </booktitle> <publisher> MIT Press. </publisher> <address> pp.108 117. </address>
Reference-contexts: Indeed, it is this ability to perform competently in the absence of external control which makes high level motivational or behavioral control possible. Actionselection has been a topic of some interest among Ethol-ogists and Computer Scientists alike, and a number of algorithms have been proposed <ref> [3, 4, 12, 19-22] </ref>. Earlier work [3], presented a computational model of actionselection which draws heavily on ideas from Ethology. The algorithm presented below is derived from this work but incorporates a number of important new features. The interested reader may consult [3] for the ethological justification for the algorithm. <p> Actionselection has been a topic of some interest among Ethol-ogists and Computer Scientists alike, and a number of algorithms have been proposed [3, 4, 12, 19-22]. Earlier work <ref> [3] </ref>, presented a computational model of actionselection which draws heavily on ideas from Ethology. The algorithm presented below is derived from this work but incorporates a number of important new features. The interested reader may consult [3] for the ethological justification for the algorithm. <p> Earlier work <ref> [3] </ref>, presented a computational model of actionselection which draws heavily on ideas from Ethology. The algorithm presented below is derived from this work but incorporates a number of important new features. The interested reader may consult [3] for the ethological justification for the algorithm.
Reference: 4. <author> Brooks, R., </author> <title> A Robust Layered Control System for a Mobile Robot. 1986. </title> <journal> IEEE Journal of Robotics and Automation RA-2.pp. </journal> <pages> 14-23. </pages>
Reference-contexts: Tu and Terzopoulos's Fish [20] represent one of the most impressive examples of this approach. Advances in behavioral animation are critically important to the development of creatures for use in interactive virtual environments. Research in autonomous robots <ref> [4, 8, 12] </ref> supports the need to couple real-time action with dynamic and unpredictable environments. Their insights only serve to strengthen the argument for autonomous animated creatures. Pure autonomy, perhaps, should not be the ultimate goal. Imagine making an interactive virtual Lassie experience for children. <p> Indeed, it is this ability to perform competently in the absence of external control which makes high level motivational or behavioral control possible. Actionselection has been a topic of some interest among Ethol-ogists and Computer Scientists alike, and a number of algorithms have been proposed <ref> [3, 4, 12, 19-22] </ref>. Earlier work [3], presented a computational model of actionselection which draws heavily on ideas from Ethology. The algorithm presented below is derived from this work but incorporates a number of important new features. The interested reader may consult [3] for the ethological justification for the algorithm.
Reference: 5. <editor> Bruderlin, A rmin a nd T homas W. Calvert. </editor> <booktitle> Dynamic Anima tion of Human Walking . Proceedings of SIGGRAPH 89 (Boston, </booktitle> <address> MA, July 3 1-August 4, </address> <year> 1989). </year> <booktitle> In Computer Graphics 23, </booktitle> <month> 3 (July </month> <year> 1989), </year> <pages> 233-242. </pages>
Reference-contexts: Such creatures are, in effect, autonomous agents with their own perceptional, behavioral, and motor systems. Typically, authors have focused on behavioral models for a specific kind of creature in a given environment, and implemented a limited set of behaviors. There are examples of locomotion <ref> [2, 5, 7, 14, 16] </ref> , flocking [1 8], grasping [9], and lifting [2]. Tu and Terzopoulos's Fish [20] represent one of the most impressive examples of this approach. Advances in behavioral animation are critically important to the development of creatures for use in interactive virtual environments.
Reference: 6. <author> Galyean, T. A. </author> <title> Narrative Guidance of Interactivity , Ph.D. </title> <type> Dissertation, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1995. </year>
Reference-contexts: The evaluation of the Behavior System itself typically takes less than 6-8 milliseconds. We have also developed a number of creatures which are used in the context of an interactive story system <ref> [ 6] </ref>. This system features a computational director which provides direction to the creatures so as to meet the requirements of the story. For example, at the beginning of the story, a dog hops out of a car and wanders around.
Reference: 7. <author> Girard, Michael and A. A. Maciejewski. </author> <title> Computational Mod eling for the Computer Animation of Legged Figures. </title> <booktitle> Proceedings of SIGGRAPH 85 (San Francisco, </booktitle> <address> CA, </address> <month> July 22-26, </month> <year> 1985). </year> <booktitle> In Com puter Graphics 19, </booktitle> <pages> 263-270. </pages>
Reference-contexts: Such creatures are, in effect, autonomous agents with their own perceptional, behavioral, and motor systems. Typically, authors have focused on behavioral models for a specific kind of creature in a given environment, and implemented a limited set of behaviors. There are examples of locomotion <ref> [2, 5, 7, 14, 16] </ref> , flocking [1 8], grasping [9], and lifting [2]. Tu and Terzopoulos's Fish [20] represent one of the most impressive examples of this approach. Advances in behavioral animation are critically important to the development of creatures for use in interactive virtual environments.
Reference: 8. <author> Horswill, I. </author> <title> A Simple, Cheap, and Robust Visual Navigation System. </title> <booktitle> in Second International Conference on the Simulation of Adaptive Behavior. </booktitle> <address> Honolulu, HI, 1993. </address> <publisher> MIT Press, pp.129-137. </publisher>
Reference-contexts: Tu and Terzopoulos's Fish [20] represent one of the most impressive examples of this approach. Advances in behavioral animation are critically important to the development of creatures for use in interactive virtual environments. Research in autonomous robots <ref> [4, 8, 12] </ref> supports the need to couple real-time action with dynamic and unpredictable environments. Their insights only serve to strengthen the argument for autonomous animated creatures. Pure autonomy, perhaps, should not be the ultimate goal. Imagine making an interactive virtual Lassie experience for children. <p> While it is important to support all three types of sensing, we have found synthetic vision to be particularly useful for low-level navigation and obstacle avoidance. Several researchers, including Renault [17], Reynolds [18], and Latombe [10] have suggested similar approaches. 5.1 Synthetic Vision For Navigation Horswill <ref> [8] </ref> points out that while vision in general is a very hard problem, there are many tasks for which it is possible to use what he calls lightweight vision.
Reference: 9. <author> Koga, Yoshihito, Koichi Kondo, James Kuffner, and Jean Claude Latombe. </author> <title> Planning Motion with Intentions. </title> <booktitle> Proceedings of SIGGRAPH 94 (Orlando, </booktitle> <address> FL, </address> <month> July 24-29, </month> <year> 1994). </year> <booktitle> In Computer Graphics Proceedings, Annual Conference Series, 1994, ACM, SIGGRAPH, </booktitle> <pages> pp. 395-408. </pages>
Reference-contexts: Typically, authors have focused on behavioral models for a specific kind of creature in a given environment, and implemented a limited set of behaviors. There are examples of locomotion [2, 5, 7, 14, 16] , flocking [1 8], grasping <ref> [9] </ref>, and lifting [2]. Tu and Terzopoulos's Fish [20] represent one of the most impressive examples of this approach. Advances in behavioral animation are critically important to the development of creatures for use in interactive virtual environments.
Reference: 10. <author> Latombe, J. C., </author> <title> Robot Motion Planning . 1991, </title> <publisher> Kluwer Aca demic Publishers, </publisher> <address> Boston. </address>
Reference-contexts: While it is important to support all three types of sensing, we have found synthetic vision to be particularly useful for low-level navigation and obstacle avoidance. Several researchers, including Renault [17], Reynolds [18], and Latombe <ref> [10] </ref> have suggested similar approaches. 5.1 Synthetic Vision For Navigation Horswill [8] points out that while vision in general is a very hard problem, there are many tasks for which it is possible to use what he calls lightweight vision.
Reference: 11. <author> Ludlow, A., </author> <title> The Evolution and Simulation of a Decision Maker, in Analysis of Motivational Processes, </title> <editor> F.T.&.T. Halliday, Editor. </editor> <booktitle> 1980, </booktitle> <publisher> Academic Press, London. </publisher>
Reference: 12. <author> Maes, P., </author> <title> Situated Agents Can Have Goals . Journal for Robotics and Autonomous Systems 6(1&2), </title> <booktitle> 1990, </booktitle> <pages> pp. 49-70. </pages>
Reference-contexts: Tu and Terzopoulos's Fish [20] represent one of the most impressive examples of this approach. Advances in behavioral animation are critically important to the development of creatures for use in interactive virtual environments. Research in autonomous robots <ref> [4, 8, 12] </ref> supports the need to couple real-time action with dynamic and unpredictable environments. Their insights only serve to strengthen the argument for autonomous animated creatures. Pure autonomy, perhaps, should not be the ultimate goal. Imagine making an interactive virtual Lassie experience for children. <p> Indeed, it is this ability to perform competently in the absence of external control which makes high level motivational or behavioral control possible. Actionselection has been a topic of some interest among Ethol-ogists and Computer Scientists alike, and a number of algorithms have been proposed <ref> [3, 4, 12, 19-22] </ref>. Earlier work [3], presented a computational model of actionselection which draws heavily on ideas from Ethology. The algorithm presented below is derived from this work but incorporates a number of important new features. The interested reader may consult [3] for the ethological justification for the algorithm.
Reference: 13. <author> Maes, P., T. Darrell, and B. Blumberg. </author> <title> The ALIVE System: Full-body Interaction with Autonomous Agents. </title> <booktitle> in Proceedings of Computer Animation95 Conference , Switzerland, </booktitle> <address> April 1995, </address> <publisher> IEEE Press, </publisher> <pages> pp. 11-18. </pages>
Reference-contexts: This coupled with extensive parameterization reduces the need to create new subclasses. Lastly, an embedded Tcl i nterpreter is provided for interactive runtime control. We have developed several creatures using this tool kit. For the Alive project <ref> [ 13] </ref>, we have developed Silas T. Dog an autonomous animated dog which interacts with a user in a 3D virtual world in a believable manner.
Reference: 14. <author> McKenna, </author> <title> M ichael a nd D avid Z eltzer. </title> <booktitle> Dynamic Simulation of Autonomous Legged Locomotion . Proceedings of SIGGRAPH 90 (Dallas, </booktitle> <address> TX, </address> <month> August 6-10, </month> <year> 1990). </year> <note> In Computer Graphics 24, 4 (August 1990), pp.29-38. </note>
Reference-contexts: Such creatures are, in effect, autonomous agents with their own perceptional, behavioral, and motor systems. Typically, authors have focused on behavioral models for a specific kind of creature in a given environment, and implemented a limited set of behaviors. There are examples of locomotion <ref> [2, 5, 7, 14, 16] </ref> , flocking [1 8], grasping [9], and lifting [2]. Tu and Terzopoulos's Fish [20] represent one of the most impressive examples of this approach. Advances in behavioral animation are critically important to the development of creatures for use in interactive virtual environments.
Reference: 15. <author> Minsky, M., </author> <booktitle> The Society of Mind . 1988, </booktitle> <publisher> Simon & Schuster , New York. </publisher>
Reference-contexts: The major components of an individual Behavior are shown in Figure 6. This model of a distributed collection of goal-directed entities is consistent with ethological models as well as recent theories of the mind <ref> [15] </ref>. Behaviors compete for control of the creature on the basis of a value which is recalculated on every update cycle for each Behavior. The value of a Behavior may be high because the Behavior satisfies an important need of the creature (e.g. its Internal Variables have a high value). <p> Thus, a stopNearAndDo Behavior can be implemented without reference to the kind of object it is stopping near. Pronomes may be shared among behaviors, thus allowing the construction of a generic find and do hierarchy. While the motivation for Pronomes comes from theories of mind <ref> [15] </ref> as opposed to ethology, they make sense in an ethological context as well. <p> Moreover, once a creature is committed to satisfying a goal, it makes sense for it to continue pursuing that goal unless something significantly more important comes along. We rely on a phenomena known as the avalanche effect <ref> [15] </ref> to both arbitrate among Behaviors in a Behavior Group and to provide the right amount of persistence. This is done via mutual inhibition. Specifically, a given Behavior A will inhibit a Behavior B by a gain I AB times Behavior As value.
Reference: 16. <author> Raibert, M arc H. and J essica K . Hodgins. </author> <title> Animation of Dynamic Legged Locomotion. </title> <booktitle> Proceedings of SIGGRAPH 91 (Las Vegas, </booktitle> <address> NV, July 28-August 2, </address> <year> 1991). </year> <booktitle> In Computer Graphics 25, </booktitle> <month> 4 (July </month> <year> 1991), </year> <month> 349358 </month>
Reference-contexts: Such creatures are, in effect, autonomous agents with their own perceptional, behavioral, and motor systems. Typically, authors have focused on behavioral models for a specific kind of creature in a given environment, and implemented a limited set of behaviors. There are examples of locomotion <ref> [2, 5, 7, 14, 16] </ref> , flocking [1 8], grasping [9], and lifting [2]. Tu and Terzopoulos's Fish [20] represent one of the most impressive examples of this approach. Advances in behavioral animation are critically important to the development of creatures for use in interactive virtual environments.

Reference: 18. <author> Reynolds, C raig W. </author> <title> Flocks, Herds, and Schools: A Distrib--uted Behavioral Model. </title> <booktitle> Proceedings of SIGGRAPH 87 (Anaheim, </booktitle> <address> CA, </address> <month> July 27-31, </month> <year> 1987). </year> <note> In Computer Graphics 21, 4 (July 19987), 25-34. </note>
Reference-contexts: While it is important to support all three types of sensing, we have found synthetic vision to be particularly useful for low-level navigation and obstacle avoidance. Several researchers, including Renault [17], Reynolds <ref> [18] </ref>, and Latombe [10] have suggested similar approaches. 5.1 Synthetic Vision For Navigation Horswill [8] points out that while vision in general is a very hard problem, there are many tasks for which it is possible to use what he calls lightweight vision.
Reference: 19. <author> Tinbergen, N., </author> <title> The Study of Instinct . 1950, </title> <publisher> Clarendon press, Oxford. </publisher>
Reference: 20. <author> Tu, </author> <title> X iaoyuan a nd D emetri T erzopoulos. Artificial F ishes: Physics, Locomotion, Perception, </title> <booktitle> Behavior . Proceedings of SIG-GRAPH 94 (Orlando, </booktitle> <address> FL, </address> <month> July 24-29, </month> <year> 1994). </year> <booktitle> In Computer Graphics Proceedings, Annual Conference Series, 1994, ACM, SIGGRAPH, </booktitle> <pages> pp. 43-50. </pages>
Reference-contexts: Typically, authors have focused on behavioral models for a specific kind of creature in a given environment, and implemented a limited set of behaviors. There are examples of locomotion [2, 5, 7, 14, 16] , flocking [1 8], grasping [9], and lifting [2]. Tu and Terzopoulos's Fish <ref> [20] </ref> represent one of the most impressive examples of this approach. Advances in behavioral animation are critically important to the development of creatures for use in interactive virtual environments. Research in autonomous robots [4, 8, 12] supports the need to couple real-time action with dynamic and unpredictable environments.
Reference: 21. <author> Tyrrell, T. </author> <title> The Use of Hierarchies for Action Selection, </title> <booktitle> in Second International Conference on the Simulation of Adaptive Behavior. 1993. </booktitle> <publisher> MIT Press, pp.138-147. </publisher>
Reference: 22. <author> Wilhelms J., R. Skinner. </author> <title> A 'Notion' for Interactive Behavioral Animation Control. </title> <journal> IEEE Computer Graphics and Applications 10(3) May 1990, </journal> <pages> pp. 14-22. </pages>
Reference: 23. <author> David Zeltzer, </author> <title> Task Level Graphical Simulation: Abstraction, Representation and Control , in Making Them Move . Ed. </title> <editor> Badler, N., Barsky, B., and Zeltzer D. </editor> <booktitle> 1991, </booktitle> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference-contexts: These levels are detailed in Figure 1. By providing the ability to direct the creature at multiple levels the animator or developer can choose the appropriate level of control for a given situation. Both Badler and Zeltzer have proposed similar decomposition of control <ref> [2, 23] </ref>. 2.2 A General Behavior Model We propose a distributed behavioral model, inspired by work in Ethology and autonomous robot research, for perception and actionselection in autonomous animated creatures but which also supports external control.
References-found: 22

