URL: ftp://ftp.wins.uva.nl/pub/computer-systems/aut-sys/reports/HagKro97c.ps.gz
Refering-URL: http://www.fwi.uva.nl/research/neuro/publications/publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: email: stephanh@wins.uva.nl  
Title: Towards a Reactive Critic  
Author: Stephan ten Hagen and Ben Krose 
Address: Kruislaan 403, 1098 SJ Amsterdam  
Affiliation: Department of Mathematics, Computer Science, Physics and Astronomy University of Amsterdam  
Date: 49-58, 1997  
Note: In: Proc. BENELEARN-97, 7th Belgian-Dutch Conference on Machine Learning, pp  
Abstract: In this paper we propose a reactive critic, that is able to respond to changing situations. We will explain why this is usefull in reinforcement learning, where the critic is used to improve the control strategy. We take a problem for which we can derive the solution analytically. This enables us to investigate the relation between the parameters and the resulting approximations of the critic. We will also demonstrate how the reactive critic reponds to changing situations.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. Cichosz. </author> <title> Truncating temporal differences: On the efficient implementation of TD() for reinforcement learning. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2 </volume> <pages> 287-318, </pages> <year> 1995. </year>
Reference-contexts: It is defined as a discounted sum of future evaluations: U i = j=i where fl 2 <ref> [0; 1] </ref> is the discount factor to make sure that the sum exists. The scalar r i is the immediate evaluation. A reward function determines r i , based on the true state of the environment. <p> The discount factor 2 <ref> [0; 1] </ref> is used to maintain information about previous updates. Convergence of the on-line TD () method can be proven for a lookup table approxima-tor [2]. Also it can be proven for an approximator that is linear in the weights [7]. <p> Also it shows that for high values of n s n p the optimal approaches the value of fl = 0:9. But the optimal never exceeds fl. This corresponds with the use of = 0 fl with 0 2 <ref> [0; 1] </ref> and fl 2 [0; 1] (see i.e. [1]). For n = 40 the optimal is very close to fl for larger values of n s n p . Then the difference fl &lt; 10 4 for n s n p &gt; 36. <p> Also it shows that for high values of n s n p the optimal approaches the value of fl = 0:9. But the optimal never exceeds fl. This corresponds with the use of = 0 fl with 0 2 <ref> [0; 1] </ref> and fl 2 [0; 1] (see i.e. [1]). For n = 40 the optimal is very close to fl for larger values of n s n p . Then the difference fl &lt; 10 4 for n s n p &gt; 36. <p> Also it shows that for high values of n s n p the optimal approaches the value of fl = 0:9. But the optimal never exceeds fl. This corresponds with the use of = 0 fl with 0 2 [0; 1] and fl 2 [0; 1] (see i.e. <ref> [1] </ref>). For n = 40 the optimal is very close to fl for larger values of n s n p . Then the difference fl &lt; 10 4 for n s n p &gt; 36.
Reference: [2] <author> Tommi Jaakkola, Michael I. Jordan, and Satinder P. Singh. </author> <title> On the convergence of stochastic iterative dynamic programming algorithms. </title> <journal> Neural Computation, </journal> <volume> 6 </volume> <pages> 1185-1201, </pages> <year> 1994. </year>
Reference-contexts: Using the utility is beyond the scope of this paper, we will focus on the approximation of the utility. One method to train the critic is the on-line TD () learning method [4]. This method can be proven to be convergent for a lookup table approximator <ref> [2] </ref>, and for an approximator linear in the weights [7]. Both methods require a decreasing learning rate, with as result a static functional mapping from observed states to the approximation of the utility. This has two shortcomings. <p> The discount factor 2 [0; 1] is used to maintain information about previous updates. Convergence of the on-line TD () method can be proven for a lookup table approxima-tor <ref> [2] </ref>. Also it can be proven for an approximator that is linear in the weights [7]. <p> With these estimates, we have an indication of the quality of the approximations for one complete cycle. In [6] a RBF network with n = 20, = 0:7389 and X c = <ref> [2; 2] </ref> was trained with ff = 0:0325 and = 0:8835 for the situation in Figure 2a, where n s n p = 12. Figure 2b shows that after about 20 cycles the estimations are close to the real values of A and B.
Reference: [3] <author> Long-Ji Lin and Tom M. Mitchell. </author> <title> Memory approaches to reinforcement learning in non-markovian domains. </title> <type> Technical Report CMU-CS-92-138, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: The critic can only respond to non-stationarities if the weights are updated on-line. For this (3) can be used, but then with a learning rate that does not decrease as in (4). Keeping a non-zero learning rate will make the critic dynamic. It has been shown <ref> [3] </ref> that a dynamic critic is also a possible solution for Markov property violation. If the Markov property is violated by state transitions depending not only on the present observations, previous observations can be used to give an approximation of utility. <p> Both methods are shown in <ref> [3] </ref>. Note that (5) and (6) express the result after training, when a fixed value for w fl is found. But the training of the reactive critic does not come to an end.
Reference: [4] <author> Richard S. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44, </pages> <year> 1988. </year>
Reference-contexts: Once the critic approximates the utility well, it can be used to improve the interaction or control. Using the utility is beyond the scope of this paper, we will focus on the approximation of the utility. One method to train the critic is the on-line TD () learning method <ref> [4] </ref>. This method can be proven to be convergent for a lookup table approximator [2], and for an approximator linear in the weights [7]. Both methods require a decreasing learning rate, with as result a static functional mapping from observed states to the approximation of the utility. <p> Only the interpretation becomes more difficult. To find the vector w fl , the approximators weights are updated according to the temporal difference learning method, introduced in <ref> [4] </ref>.
Reference: [5] <author> S.H.G ten Hagen and B.J.A. Krose. </author> <title> Generalizing in TD() learning. </title> <booktitle> In Proceedings of the Third Joint Conference of Information Sciences, </booktitle> <volume> volume 2, </volume> <pages> pages 319-322, </pages> <year> 1997. </year>
Reference-contexts: In our approach the parameters of the critic are adjusted with an update rule of which the learning rate does not decrease. ? This research is supported by the Technology Foundation (STW) Still, the update can be guaranteed to be not divergent <ref> [5] </ref>. In section 4 we describe the setup of the function approximator. Section 5 describes the experiments, where we show that the resulting reactive critic can be used for systems with hidden states. The effect of the choice of the learning parameters on the approximation error is investigated. <p> This provides the possibility to cope with changing systems. 4 The Approximator In order to cope with changing systems, we need a model-free, generalizing approximator. Also we need a guarantee, that the update is not divergent. Such guarantee can be given for an approximator linear in the weights <ref> [5] </ref>. Then the approximation is given by ^ U i = f T i w i , where f i = f (x i ) represents the observation as a feature vector. <p> Update (3) can be rewritten to with F i = t i (f T i+1 ) and t i = j=0 In <ref> [5] </ref> it is shown that (8) is not divergent if 0 &lt; ff (f T i+1 )t i &lt; 1 8 i: (9) Not divergent means that if the values of the feature vectors and rewards are bounded, the values of the weights and ^ U also will be bounded.
Reference: [6] <author> Stephan H.G. ten Hagen and Ben J.A. Krose. </author> <title> On-line adaptive critic for changing systems. </title> <note> Available at "http://carol.wins.uva.nl/~stephanh/research/pub/", 1997. </note>
Reference-contexts: We use the observations and approximations of a complete cycle and apply this to (13) to get a least squares estimates ^ A and ^ B of A and B. With these estimates, we have an indication of the quality of the approximations for one complete cycle. In <ref> [6] </ref> a RBF network with n = 20, = 0:7389 and X c = [2; 2] was trained with ff = 0:0325 and = 0:8835 for the situation in Figure 2a, where n s n p = 12.
Reference: [7] <author> J. N. Tsitsiklis and B. Van Roy. </author> <title> An analysis of temporal-difference learning with function approximation. </title> <journal> IEEE Transactions on Automatic Control, </journal> <month> May, </month> <year> 1997. </year>
Reference-contexts: One method to train the critic is the on-line TD () learning method [4]. This method can be proven to be convergent for a lookup table approximator [2], and for an approximator linear in the weights <ref> [7] </ref>. Both methods require a decreasing learning rate, with as result a static functional mapping from observed states to the approximation of the utility. This has two shortcomings. <p> The discount factor 2 [0; 1] is used to maintain information about previous updates. Convergence of the on-line TD () method can be proven for a lookup table approxima-tor [2]. Also it can be proven for an approximator that is linear in the weights <ref> [7] </ref>. Both convergence proofs require a learning rate ff i that decreases in time according to: 1 X ff i = 1 and i i &lt; 1: (4) The first condition is necessary to make sure that enough observations are used to get the correct mapping.
References-found: 7

