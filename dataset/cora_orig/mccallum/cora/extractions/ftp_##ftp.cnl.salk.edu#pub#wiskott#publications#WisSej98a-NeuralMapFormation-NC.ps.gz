URL: ftp://ftp.cnl.salk.edu/pub/wiskott/publications/WisSej98a-NeuralMapFormation-NC.ps.gz
Refering-URL: http://www.cnl.salk.edu/~wiskott/Abstracts/WisSej98a.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Constrained Optimization for Neural Map Formation: A Unifying Framework for Weight Growth and Normalization  
Author: Laurenz Wiskott and Terrence Sejnowski 
Address: San Diego, CA 92186-5800  La Jolla, CA 92093  
Affiliation: 1 Computational Neurobiology Laboratory 2 Howard Hughes Medical Institute The Salk Institute for Biological Studies  Department of Biology University of California, San Diego  
Note: The final version of this article has been published in Neural Computation, 10(3):671-716 (1998) published by The MIT Press.  
Email: E-mail: wiskott@salk.edu  
Phone: 3  
Web: http://www.cnl.salk.edu/CNL  
Abstract: Computational models of neural map formation can be considered on at least three different levels of abstraction: detailed models including neural activity dynamics, weight dynamics that abstract from the neural activity dynamics by an adiabatic approximation, and constrained optimization from which equations governing weight dynamics can be derived. Constrained optimization uses an objective function, from which a weight growth rule can be derived as a gradient flow, and some constraints, from which normalization rules are derived. In this paper we present an example of how an optimization problem can be derived from detailed non-linear neural dynamics. A systematic investigation reveals how different weight dynamics introduced previously can be derived from two types of objective function terms and two types of constraints. This includes dynamic link matching as a special case of neural map formation. We focus in particular on the role of coordinate transformations to derive different weight dynamics from the same optimization problem. Several examples illustrate how the constrained optimization framework can help in understanding, generating, and comparing different models of neural map formation. The techniques used in this analysis may also be useful in investigating other types of neural dynamics.
Abstract-found: 1
Intro-found: 1
Reference: <author> Amari, S. </author> <year> (1977). </year> <title> Dynamics of pattern formation in lateral-inhibition type neural fields. </title> <journal> Biol. Cybern., </journal> <volume> 27 </volume> <pages> 77-87. </pages>
Reference: <author> Amari, S. </author> <year> (1980). </year> <title> Topographic organization of nerve fields. </title> <journal> Bulletin of Mathematical Biology, </journal> <volume> 42 </volume> <pages> 339-364. </pages>
Reference-contexts: The original equations are listed as well as the classification in terms of growth rules and normalization rules listed in Table 1. Detailed comments for these models and the model in <ref> (Amari, 1980) </ref> follow below. The latter is not listed in Table 2 because it cannot be interpreted within our constrained optimization framework. <p> Similar models were proposed earlier in (Swindale, 1980), though not derived from a receptive-field model, and more recently in (Tanaka, 1991). These approaches and their relationships to our constrained optimization framework need to be investigated more systematically. A neural map formation model of <ref> (Amari, 1980) </ref> could not be formulated within the constrained optimization framework presented here (cf. Sec. 6.2).
Reference: <author> Bauer, H.-U., Brockmann, D., and Geisel, T. </author> <year> (1997). </year> <title> Analysis of ocular dominance pattern formation in a high-dimensional self-organizing-map model. Network: </title> <booktitle> Computation in Neural Systems, </booktitle> <volume> 8(1) </volume> <pages> 17-33. </pages>
Reference-contexts: The second type of learning rule is given by w t (t + 1) = w t (t) + *B tt 0 (B 0 w t (t)) : (53) as used, for example, in <ref> (Bauer et al., 1997) </ref>. For this learning rule the weights and the input stimuli are assumed to be sum normalized, i.e. P P B 0 = 1.
Reference: <author> Behrmann, K. </author> <year> (1993). </year> <institution> Leistungsuntersuchungen des "Dynamischen Link-Matchings" und Vergleich mit dem Kohonen-Algorithmus. </institution> <type> Diploma thesis, internal report IR-INI 93-05, </type> <institution> Institut fur Neuroinformatik, </institution> <address> Ruhr-Universitat Bochum, D-44780 Bochum. </address>
Reference-contexts: Even though the high- and the low-dimensional learning rules are equivalent for a given pair of blobs, the overall behavior of the models is not. This is because the positioning of the output blobs is different in the two models <ref> (Behrmann, 1993) </ref>. It is clear that many different high-dimensional weight configurations having different output blob positioning can lead to the same low-dimensional weight configuration.
Reference: <author> Bell, A. J. and Sejnowski, T. J. </author> <year> (1995). </year> <title> An information-maximization approach to blind separation and blind deconvolution. </title> <journal> Neural Computation, </journal> <volume> 7 </volume> <pages> 1129-1159. </pages>
Reference-contexts: It would be interesting as a next step to consider third-order terms in the objective function and the conditions under which they can be derived from detailed neural dynamics. There may also be an interesting relationship to recent advances in algorithms for independent component analysis <ref> (Bell & Sejnowski, 1995) </ref>, which can be derived from a maximum entropy method and is dominated by higher-order correlations.
Reference: <author> Bienenstock, E. and von der Malsburg, C. </author> <year> (1987). </year> <title> A neural network for invariant pattern recognition. </title> <journal> Europhysics Letters, </journal> <volume> 4(1) </volume> <pages> 121-126. </pages>
Reference: <author> Dirac, P. A. M. </author> <year> (1996). </year> <title> General Theory of Relativity. Princeton landmarks in physics. </title> <publisher> Princeton University Press, </publisher> <address> 41 William Street, Princeton, NJ 08540. </address>
Reference-contexts: This effect of coordinate transformations is known from the general theory of relativity and tensor analysis <ref> (e.g. Dirac, 1996) </ref>. The gradient of a potential (or objective function) is a covariant vector, which adds the factor dw i =dv i through the transformation from W to V .
Reference: <author> Ermentrout, G. B. and Cowan, J. D. </author> <year> (1979). </year> <title> A mathematical theory of visual hallucination patterns. </title> <journal> Biological Cybernetics, </journal> <volume> 34(3) </volume> <pages> 137-150. </pages>
Reference: <author> Erwin, E., Obermayer, K., and Schulten, K. </author> <year> (1995). </year> <title> Models of orientation and ocular dominance columns in the visual cortex: A critical comparison. </title> <journal> Neural Computation, </journal> <volume> 7 </volume> <pages> 425-468. </pages>
Reference: <author> Ginzburg, I. and Sompolinsky, H. </author> <year> (1994). </year> <title> Theory of correlations in stochastic neural networks. </title> <journal> Physical Review E, </journal> <volume> 50(4) </volume> <pages> 3171-3191. </pages>
Reference: <author> Goodhill, G. J. </author> <year> (1993). </year> <title> Topography and ocular dominance: A model exploring positive correlations. </title> <journal> Biol. Cybern., </journal> <volume> 69 </volume> <pages> 109-118. </pages>
Reference: <author> Goodhill, G. J., Finch, S., and Sejnowski, T. J. </author> <year> (1996). </year> <title> Optimizing cortical mappings. </title> <editor> In Touret-zky, D., Mozer, M., and Hasselmo, M., editors, </editor> <booktitle> Proc. Advances in Neural Information Processing Systems, </booktitle> <volume> volume 8, </volume> <pages> pages 330-336, </pages> <address> Cambridge, MA. </address> <publisher> MIT Press. </publisher>
Reference: <author> H aussler, A. F. and von der Malsburg, C. </author> <year> (1983). </year> <title> Development of retinotopic projections | An analytical treatment. </title> <journal> J. Theor. Neurobiol., </journal> <volume> 2 </volume> <pages> 47-73. </pages>
Reference-contexts: A suitable objective function is H (w) = 2 ij (cf. Eq. 15), since it yields _w i = @H (w)=@w i . A dynamics that cannot be generated by an objective function directly is _w i = w i j as used in <ref> (H aussler & von der Malsburg, 1983) </ref>, since for i 6= j we obtain @ _w i =@w j = w i D ij 6= w j D ji = @ _w j =@w i , and _w i is not curl-free. <p> However, difficulties arise when interfering constraints are combined, i.e. different constraints that affect the same weights. This type of formulation is required for certain types of analyses <ref> (e.g. H aussler & von der Malsburg, 1983) </ref>. <p> Otherwise the system may have a tendency to collapse on the input layer (see 6.3), a tendency it does not have in the original formulation as a dynamical system. Only few systems contain the linear term L, which can be used for dynamic link matching. In <ref> (H aussler & von der Malsburg, 1983) </ref> the linear term was introduced for analytical convenience and does not differentiate between different links.
Reference: <author> Horst, R., Pardalos, P. M., and Thoai, N. V. </author> <year> (1995). </year> <title> Introduction to Global Optimization, volume 3 of Nonconvex Optimization and Its Applications. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Dordrecht, The Netherlands. </address> <note> 29 Kohonen, </note> <author> T. </author> <year> (1982). </year> <title> Self-organized formation of topologically correct feature maps. </title> <journal> Biol. Cybern., 43:59--69. </journal>
Reference-contexts: These problems are known to be NP-complete. However, there is a large literature on algorithms that efficiently solve special cases or that find good approximate solutions in polynomial time <ref> (e.g. Horst et al., 1995) </ref>. Many related objective functions are only defined for maps for which each input neuron terminates on exactly one output neuron with weight 1, which makes the index t = t () a function of index .
Reference: <author> Kohonen, T. </author> <year> (1990). </year> <title> The self-organizing map. </title> <journal> Proc. of the IEEE, </journal> <volume> 78(9) </volume> <pages> 1464-1480. </pages>
Reference-contexts: Notice that the low-dimensional learning rule is even formally equivalent to the high-dimensional one and that it is the rule commonly used in low-dimensional models <ref> (Kohonen, 1990) </ref>. Even though the high- and the low-dimensional learning rules are equivalent for a given pair of blobs, the overall behavior of the models is not. This is because the positioning of the output blobs is different in the two models (Behrmann, 1993).
Reference: <author> Konen, W., Maurer, T., and von der Malsburg, C. </author> <year> (1994). </year> <title> A fast dynamic link matching algorithm for invariant pattern recognition. </title> <booktitle> Neural Networks, </booktitle> 7(6/7):1019-1030. 
Reference: <author> Konen, W. and von der Malsburg, C. </author> <year> (1993). </year> <title> Learning to generalize from single examples in the dynamic link architecture. </title> <journal> Neural Computation, </journal> <volume> 5(5) </volume> <pages> 719-735. </pages>
Reference-contexts: Coordinate transformations can be used to generate different models that are equivalent in terms of their constrained optimization problem. Consider the system in <ref> (Konen & von der Malsburg, 1993) </ref>.
Reference: <author> Lades, M., Vorbr uggen, J. C., Buhmann, J., Lange, J., von der Malsburg, C., W urtz, R. P., and Konen, W. </author> <year> (1993). </year> <title> Distortion invariant object recognition in the dynamic link architecture. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 42(3) </volume> <pages> 300-311. </pages>
Reference-contexts: The first term now directly favors links with high similarity values. This may be advantageous because it allows better control over the influence of the topography vs. the feature similarity term. Furthermore, this objective function is more closely related to the similarity function of elastic graph matching in <ref> (Lades et al., 1993) </ref>, which has been developed as an algorithmic abstraction of dynamic link matching (see Sec. 6.7). 6.6 Soft vs. Hard Competitive Normalization Miller & MacKay (1994) have analyzed the role of normalization rules for neural map formation. <p> Thus, the objective function Q is the typical term for topographic maps in other contexts as well. Elastic graph matching is an algorithmic counterpart to dynamic link matching and has been used for applications such as object and face recognition <ref> (Lades et al., 1993) </ref>.
Reference: <author> Linsker, R. </author> <year> (1986). </year> <title> From basic network principles to neural architecture: Emergence of orientation columns. </title> <journal> Ntl. Acad. Sci. USA, </journal> <volume> 83 </volume> <pages> 8779-8783. </pages>
Reference-contexts: The difference for the latter would be an additional constant, which can always be compensated for in the growth rule. The correlation model in <ref> (Linsker, 1986) </ref> differs from the linear one introduced here in two respects. The input (Eq. 20) has an additional constant term and correlations are defined by subtracting positive constants from the activities. However, it can be shown that correlations in the model in (Linsker, 1986) are a linear combination of a <p> The correlation model in <ref> (Linsker, 1986) </ref> differs from the linear one introduced here in two respects. The input (Eq. 20) has an additional constant term and correlations are defined by subtracting positive constants from the activities. However, it can be shown that correlations in the model in (Linsker, 1986) are a linear combination of a constant and the terms of Equations (22, 23). 7 4 Objective Functions In general, there is no systematic way of finding an objective function for a particular dynamical system, but it is possible to determine whether there exists an objective function. <p> As von der Malsburg (1995) has pointed out, this is appropriate only for a subset of phenomena of neural map formation, such as retinotopy and ocular dominance. Although orientation tuning can arise by spontaneous symmetry breaking <ref> (e.g. Linsker, 1986) </ref>, a full understanding of the self-organization of 25 orientation selectivity and other phenomena may require taking higher-order correlations into account.
Reference: <author> MacKay, D. J. C. and Miller, K. D. </author> <year> (1990). </year> <title> Analysis of Linsker's simulations of hebbian rules. </title> <journal> Neural Computation, </journal> <volume> 2 </volume> <pages> 173-187. </pages>
Reference-contexts: An important tool for both methods is the objective function (or energy function) from which the dynamics can be generated as a gradient flow. The objective value (or energy) can be used to estimate which weight configurations would be more likely to arise from the dynamics <ref> (e.g. MacKay & Miller, 1990) </ref>. In computer simulations the objective function is maximized (or the energy function is minimized) numerically in order to find stable solutions of the dynamics (e.g. <p> The role of parameters and effective lateral connectivities might be investigated analytically for a variety of models by means of objective functions, similar to the approach sketched in Section 6.3 or the one taken in <ref> (MacKay & Miller, 1990) </ref>. We have considered here only three levels of abstraction: detailed neural dynamics, abstract weight dynamics, and constrained optimization. There exist even higher levels of abstraction and the relationship between our constrained optimization framework and these more abstract models should be explored.
Reference: <author> Meister, M., Wong, R. O. L., Baylor, D. A., and Shatz, C. J. </author> <year> (1991). </year> <title> Synchronous bursts of action potentials in ganglion cells of the developing mammalian retina. </title> <journal> Science, </journal> <volume> 252 </volume> <pages> 939-943. </pages>
Reference-contexts: A solution to this problem might be found by examining propagating activity patterns in the input as well as the output layer, such as traveling waves (Triesch, 1995) or running blobs (Wiskott & von der Malsburg, 1996). Waves and blobs of activity have been observed in the developing retina <ref> (Meister et al., 1991) </ref>. If the waves or blobs have the same intrinsic velocity in the two layers, they would tend to generate metric maps, regardless of the scaling factor induced by the normalization rules.
Reference: <author> Miller, K. D. </author> <year> (1990). </year> <title> Derivation of linear Hebbian equations from nonlinear Hebbian model of synaptic plasticity. </title> <journal> Neural Computation, </journal> <volume> 2 </volume> <pages> 321-333. </pages>
Reference-contexts: An important tool for both methods is the objective function (or energy function) from which the dynamics can be generated as a gradient flow. The objective value (or energy) can be used to estimate which weight configurations would be more likely to arise from the dynamics <ref> (e.g. MacKay & Miller, 1990) </ref>. In computer simulations the objective function is maximized (or the energy function is minimized) numerically in order to find stable solutions of the dynamics (e.g. <p> The role of parameters and effective lateral connectivities might be investigated analytically for a variety of models by means of objective functions, similar to the approach sketched in Section 6.3 or the one taken in <ref> (MacKay & Miller, 1990) </ref>. We have considered here only three levels of abstraction: detailed neural dynamics, abstract weight dynamics, and constrained optimization. There exist even higher levels of abstraction and the relationship between our constrained optimization framework and these more abstract models should be explored.
Reference: <author> Miller, K. D., Keller, J. B., and Stryker, M. P. </author> <year> (1989). </year> <title> Ocular dominance column development: Analysis and simulation. </title> <journal> Science, </journal> <volume> 245 </volume> <pages> 605-245. </pages>
Reference-contexts: A more systematic treatment of the normalization rules could replace this inconsistent rule (cf. Sec. 5.2.1). Another inconsistency is that weights that reach their upper or lower limit become frozen, i.e. fixed at the limit value. With some exception this seems to have little effect on the resulting maps <ref> (Miller et al., 1989, Note 23.) </ref>. Thus this model has only two minor inconsistencies, which could be modified to make the system consistent. <p> Appendix A). As the model in <ref> (Miller et al., 1989) </ref> this model uses an inconsistent normalization rule as a backup and it freezes weights that reach their upper or lower limit. In addition it uses an inconsistent normalization rule for the input neurons.
Reference: <author> Miller, K. D. and MacKay, D. J. C. </author> <year> (1994). </year> <title> The role of constraints in Hebbian learning. </title> <journal> Neural Computation, </journal> <volume> 6 </volume> <pages> 100-126. </pages>
Reference: <author> Nowlan, S. J. </author> <year> (1990). </year> <title> Maximum likelihood competitive learning. </title> <editor> In Touretzky, D. S., editor, </editor> <booktitle> Proc. Advances in Neural Information Processing Systems, </booktitle> <volume> volume 2, </volume> <pages> pages 574-582, </pages> <address> San Mateo, CA 94403. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference: <author> Obermayer, K., Ritter, H., and Schulten, K. </author> <year> (1990). </year> <title> Large-scale simulations of self-organizing neural networks on parallel computers: Application to biological modelling. </title> <journal> Parallel Computing, </journal> <volume> 14 </volume> <pages> 381-404. </pages>
Reference-contexts: Although it is plausible that such a probabilistic blob location could be approximated by noise in the output layer, it is difficult to develop a concrete model. For a similar but more algorithmic activity model <ref> (Obermayer et al., 1990) </ref> an exact noise model for the probabilistic blob location can be formulated (see Appendix A). <p> One learning rule for the high-dimensional SOM-algorithm is given by ~w t (t) = w t (t 1) + *B tt 0 B 0 (51) ~w t (t) 0 ~w 2 ; (52) as used, for example, in <ref> (Obermayer et al., 1990) </ref>. B tt 0 denotes the neighborhood function (commonly indicated by h) and B 0 denotes the stimulus pattern (sometimes indicated by x) with index 0 . B 0 does not need to have a blob shape, so that 0 may be an arbitrary index.
Reference: <author> Ritter, H., Martinetz, T., and Schulten, K. </author> <year> (1991). </year> <note> Neuronale Netze. Addison-Wesley, second edition. </note>
Reference: <author> Sejnowski, T. J. </author> <year> (1976). </year> <title> On the stochastic dynamics of neuronal interaction. </title> <journal> Biol. Cybern., </journal> <volume> 22 </volume> <pages> 203-211. </pages>
Reference: <author> Sejnowski, T. J. </author> <year> (1977). </year> <title> Storing covariance with nonlinearly interacting neurons. </title> <journal> J. Math. Biology, </journal> <volume> 4 </volume> <pages> 303-321. </pages>
Reference-contexts: = t 0 0 X A ij w j : (22) Assuming a linear correlation function (ha ; ff (a 0 + a 00 )i = ffha ; a 0 i + ffha ; a 00 i with a real constant ff) such as the average product or the covariance <ref> (Sejnowski, 1977) </ref>, the correlation between input and output neurons is ha t ; a i = t 0 0 X D ij w j ; (23) Note that i = f; t g, j = f 0 ; t 0 g, A ij = A ji = D tt 0 A
Reference: <author> Swindale, N. V. </author> <year> (1980). </year> <title> A model for the formation of ocular domance stripes. </title> <journal> Proc. R. Soc. Lond. B, </journal> <volume> 208 </volume> <pages> 243-264. </pages>
Reference-contexts: He derived an energy function to evaluate how the different orientations would be arranged in the output layer due to lateral interactions. The only variables of this energy function were the orientations of the receptive fields, an abstraction from the connectivity. Similar models were proposed earlier in <ref> (Swindale, 1980) </ref>, though not derived from a receptive-field model, and more recently in (Tanaka, 1991). These approaches and their relationships to our constrained optimization framework need to be investigated more systematically.
Reference: <author> Swindale, N. V. </author> <year> (1996). </year> <title> The development of topography in the visual cortex: A review of models. Network: </title> <journal> Comput. in Neural Syst., </journal> <volume> 7(2) </volume> <pages> 161-247. </pages>
Reference: <author> Tanaka, S. </author> <year> (1990). </year> <title> Theory of self-organization of cortical maps: Mathematical framework. </title> <booktitle> Neural Networks, </booktitle> <volume> 3 </volume> <pages> 625-640. </pages>
Reference: <author> Tanaka, S. </author> <year> (1991). </year> <title> Theory of ocular dominance column formation. </title> <journal> Biol. Cybern., </journal> <volume> 64 </volume> <pages> 263-272. </pages>
Reference-contexts: The only variables of this energy function were the orientations of the receptive fields, an abstraction from the connectivity. Similar models were proposed earlier in (Swindale, 1980), though not derived from a receptive-field model, and more recently in <ref> (Tanaka, 1991) </ref>. These approaches and their relationships to our constrained optimization framework need to be investigated more systematically. A neural map formation model of (Amari, 1980) could not be formulated within the constrained optimization framework presented here (cf. Sec. 6.2).

Reference: <author> Whitelaw, D. J. and Cowan, J. D. </author> <year> (1981). </year> <title> Specificity and plasticity of retinotectal connections: A computational model. </title> <journal> J. Neuroscience, </journal> <volume> 1(12) </volume> <pages> 1369-1387. </pages>
Reference: <author> Willshaw, D. J. and von der Malsburg, C. </author> <year> (1976). </year> <title> How patterned neural connections can be set up by self-organization. </title> <booktitle> Proc. R. </booktitle> <publisher> Soc. </publisher> <address> London, B194:431-445. </address>
Reference: <author> Wiskott, L. and von der Malsburg, C. </author> <year> (1996). </year> <title> Face recognition by dynamic link matching. </title> <editor> In Sirosh, J., Miikkulainen, R., and Choe, Y., editors, </editor> <title> Lateral Interactions in the Cortex: Structure and Function, </title> <booktitle> chapter 11. The UTCS Neural Networks Research Group, </booktitle> <address> Austin, TX, </address> <note> http://www.cs.utexas.edu/users/nn/web-pubs/htmlbook96/. Electronic book, ISBN 0-9647060-0-8. 31 </note>
Reference-contexts: This then is the coordinate system in which the model can be most conveniently analyzed. Not all non-orthogonal normalization rules can be transformed into orthogonal ones. In <ref> (Wiskott & von der Malsburg, 1996) </ref>, for example, a normalization rule is used that affects a group of weights if single weights grow beyond their limits. Since the constraint surface depends only on one weight, only that weight can be effected by an orthogonal normalization rule. <p> A solution to this problem might be found by examining propagating activity patterns in the input as well as the output layer, such as traveling waves (Triesch, 1995) or running blobs <ref> (Wiskott & von der Malsburg, 1996) </ref>. Waves and blobs of activity have been observed in the developing retina (Meister et al., 1991).
References-found: 36

