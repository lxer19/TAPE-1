URL: http://iacoma.cs.uiuc.edu/iacoma-papers/comprehen.ps
Refering-URL: http://iacoma.cs.uiuc.edu/papers.html
Root-URL: http://www.cs.uiuc.edu
Email: Email: [xia,torrella]@csrd.uiuc.edu  
Phone: Phone: 217-244-4148. Fax: 217-244-1351  
Title: Comprehensive Hardware and Software Support for Operating Systems to Exploit MP Memory Hierarchies  
Author: Chun Xia and Josep Torrellas 
Address: IL 61801  
Affiliation: Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign,  
Abstract: High-performance multiprocessor workstations are becoming increasingly popular. Since many of the workloads running on these machines are operating-system intensive, we are interested in what sort of support for the operating system should the memory hierarchy of these machines provide. This paper addresses this question. This paper shows that the largest performance losses for the operating system in a sophisticated 3-level cache hierarchy are due to off-chip cache misses, write buffer overflow in block operations, and instruction conflicts in the primary cache. To overcome these problems, we evaluate a comprehensive set of hardware and software supports. These supports, selected from recent papers, are code layout optimization, guarded sequential instruction prefetching, instruction stream buffers, support for block operations, support for coherence activity, and software data prefetching. We evaluate these supports under a uniform environment. We show that they target the problems indicated above, have a largely complementary impact and, when combined, speed up the operating system by an average of 32%. Finally, a cost-performance comparison of these schemes shows that the most cost-effective ones are block operation support and code layout optimization, while the least cost-effective one is software data prefetching.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal, J. Hennessy, and M. Horowitz. </author> <title> Cache Performance of Operating System and Multiprogramming Workloads. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(4) </volume> <pages> 393-431, </pages> <month> November </month> <year> 1988. </year>
Reference-contexts: A large group of researchers have examined the cache performance of the operating system without focusing much on proposing optimizations <ref> [1, 2, 5, 6, 9, 10, 11, 12] </ref>. There is some work specifically focused on optimizing the performance of the operating system [13, 15, 16]. However, it examines part of the problem only, for example, instruction accesses only or prefetching only.
Reference: [2] <author> T. Anderson, H. Levy, B. Bershad, and E. Lazowska. </author> <title> The Interaction of Architecture and Operating System Design. </title> <booktitle> In Proceedings of the 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 108-120, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: A large group of researchers have examined the cache performance of the operating system without focusing much on proposing optimizations <ref> [1, 2, 5, 6, 9, 10, 11, 12] </ref>. There is some work specifically focused on optimizing the performance of the operating system [13, 15, 16]. However, it examines part of the problem only, for example, instruction accesses only or prefetching only.
Reference: [3] <author> J. Archibald and J. L. Baer. </author> <title> Cache Coherence Protocols: Evaluation Using a Multiprocessor Simulation Model. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4(4) </volume> <pages> 273-298, </pages> <month> November </month> <year> 1986. </year>
Reference-contexts: This set of variables includes barriers, active locks, and some variables that sometimes exhibit a producer-consumer behavior. All these variables together use 12 about 380 bytes and are placed in a single page. For this page only, we use the Firefly <ref> [3] </ref> update protocol. Note that this optimization can be supported with off-the-shelf processors. For example, the MIPS R4000 processor supports update/invalidate protocol selection for each individual page. The selection is done with a bit in each TLB entry.
Reference: [4] <author> M. Berry et al. </author> <title> The Perfect Club Benchmarks: Effective Performance Evaluation of Supercomputers. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 3(3) </volume> <pages> 5-40, </pages> <month> Fall </month> <year> 1989. </year>
Reference-contexts: Each day-long session corresponds to about 15 seconds of real time. The loads are as follows: TRFD 4 is a mix of 4 runs of a hand-parallelized version of the TRFD Perfect Club code <ref> [4] </ref>. Each program runs with 4 processes. The code is composed of matrix multiplies and data interchanges. It is highly parallel yet synchronization intensive. The most important operating system activities present are page fault handling, process scheduling, cross-processor interrupts, processor synchronization, and other multiprocessor management functions.
Reference: [5] <author> J. Chapin, S. A. Herrod, M. Rosenblum, and A. Gupta. </author> <title> Memory System Performance of UNIX on CC-NUMA Multiprocessors. </title> <booktitle> In ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 1-13, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: A large group of researchers have examined the cache performance of the operating system without focusing much on proposing optimizations <ref> [1, 2, 5, 6, 9, 10, 11, 12] </ref>. There is some work specifically focused on optimizing the performance of the operating system [13, 15, 16]. However, it examines part of the problem only, for example, instruction accesses only or prefetching only.
Reference: [6] <author> J. B. Chen and B. N. Bershad. </author> <title> The Impact of Operating System Structure on Memory System Performance. </title> <booktitle> In Proceedings of the 14th ACM Symposium on Operating System Principles, </booktitle> <pages> pages 120-133, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: A large group of researchers have examined the cache performance of the operating system without focusing much on proposing optimizations <ref> [1, 2, 5, 6, 9, 10, 11, 12] </ref>. There is some work specifically focused on optimizing the performance of the operating system [13, 15, 16]. However, it examines part of the problem only, for example, instruction accesses only or prefetching only.
Reference: [7] <author> W. W. Hwu and P. P. Chang. </author> <title> Achieving High Instruction Cache Performance with an Optimizing Compiler. </title> <booktitle> In Proceedings of the 16th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 242-251, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: The combined effect of all the optimizations proposed is unknown, especially for very advanced multiprocessor memory hierarchies. Finally, there is a lot of other work proposing optimizations 1 to enhance the cache performance of applications <ref> [7, 8, 14] </ref>, but it is not clear how to apply the results to the operating system. The goal of this paper is to perform a cost-effectiveness evaluation of a comprehensive set of hardware and software supports for the operating system to exploit the memory hierarchy of these machines.
Reference: [8] <author> N. Jouppi. </author> <title> Improving Direct-Mapped Cache Performance by the Addition of a Small Fully-Associative Cache and Prefetch Buffers. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 364-373, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: The combined effect of all the optimizations proposed is unknown, especially for very advanced multiprocessor memory hierarchies. Finally, there is a lot of other work proposing optimizations 1 to enhance the cache performance of applications <ref> [7, 8, 14] </ref>, but it is not clear how to apply the results to the operating system. The goal of this paper is to perform a cost-effectiveness evaluation of a comprehensive set of hardware and software supports for the operating system to exploit the memory hierarchy of these machines. <p> Indeed, S3.I+SM.I=10.4%. The misses that cause this stall often occur in contiguous addresses and tend to be clustered. This is especially true for cold misses. This observation suggests using instruction stream buffers <ref> [8] </ref> between secondary caches and the memory bus to complement the guarded sequential prefetcher [16]. In our experiments, we use two off-chip 8-entry direct-access stream buffers per processor.
Reference: [9] <author> A. Maynard, C. Donnelly, and B. Olszewski. </author> <title> Contrasting Characteristics and Cache Performance of Technical and Multi-User Commercial Workloads. </title> <booktitle> In Proceedings of the 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 145-156, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: A large group of researchers have examined the cache performance of the operating system without focusing much on proposing optimizations <ref> [1, 2, 5, 6, 9, 10, 11, 12] </ref>. There is some work specifically focused on optimizing the performance of the operating system [13, 15, 16]. However, it examines part of the problem only, for example, instruction accesses only or prefetching only.
Reference: [10] <author> J. Ousterhout. </author> <booktitle> Why Aren't Operating Systems Getting Faster as Fast as Hardware? In Proceedings Summer 1990 USENIX Conference, </booktitle> <pages> pages 247-256, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: A large group of researchers have examined the cache performance of the operating system without focusing much on proposing optimizations <ref> [1, 2, 5, 6, 9, 10, 11, 12] </ref>. There is some work specifically focused on optimizing the performance of the operating system [13, 15, 16]. However, it examines part of the problem only, for example, instruction accesses only or prefetching only.
Reference: [11] <author> M. Rosenblum, E. Bugnion, S. A. Herrod, E. Witchel, and A. Gupta. </author> <title> The Impact of Architectural Trends on Operating System Performance. </title> <booktitle> In Proceedings of the 15th ACM Symposium on Operating System Principles, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: A large group of researchers have examined the cache performance of the operating system without focusing much on proposing optimizations <ref> [1, 2, 5, 6, 9, 10, 11, 12] </ref>. There is some work specifically focused on optimizing the performance of the operating system [13, 15, 16]. However, it examines part of the problem only, for example, instruction accesses only or prefetching only.
Reference: [12] <author> J. Torrellas, A. Gupta, and J. Hennessy. </author> <title> Characterizing the Caching and Synchronization Performance of a Multiprocessor Operating System. </title> <booktitle> In Proceedings of the 5th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 162-174, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: A large group of researchers have examined the cache performance of the operating system without focusing much on proposing optimizations <ref> [1, 2, 5, 6, 9, 10, 11, 12] </ref>. There is some work specifically focused on optimizing the performance of the operating system [13, 15, 16]. However, it examines part of the problem only, for example, instruction accesses only or prefetching only.
Reference: [13] <author> J. Torrellas, C. Xia, and R. Daigle. </author> <title> Optimizing Instruction Cache Performance for Operating System Intensive Workloads. </title> <booktitle> In Proceedings of the 1st International Symposium on High-Performance Computer Architecture, </booktitle> <pages> pages 360-369, </pages> <month> January </month> <year> 1995. </year> <month> 17 </month>
Reference-contexts: A large group of researchers have examined the cache performance of the operating system without focusing much on proposing optimizations [1, 2, 5, 6, 9, 10, 11, 12]. There is some work specifically focused on optimizing the performance of the operating system <ref> [13, 15, 16] </ref>. However, it examines part of the problem only, for example, instruction accesses only or prefetching only. The combined effect of all the optimizations proposed is unknown, especially for very advanced multiprocessor memory hierarchies. <p> In the following, we discuss the instruction and data access patterns observed. A more detailed discussion can be found in two previous papers <ref> [13, 15] </ref>. 3.2.1 Instruction Access Patterns It is well-known that the instruction access patterns in the operating system are different from those in typical technical applications. In the former, tight loops account for a relatively small fraction of the execution time. <p> We select a set of supports that target the problems identified. These supports are code layout optimization, guarded sequential instruction prefetching, instruction stream buffers, support for block operations, support for coherence activity, and software data prefetching. We have proposed these supports in three previous papers <ref> [13, 15, 16] </ref>. In this section, we evaluate and compare them under a uniform environment, combining them in a single system. <p> In the next section, we examine tradeoffs among them. 4.1 Code Layout Optimization by the Compiler The first support that we consider is optimizing the layout of the code in memory to expose spatial and temporal locality <ref> [13] </ref>. We do not attempt to expose loop locality because it is too hard to exploit effectively. The main goal of this optimization, which we call Lopt, is to reduce the stall due to conflicts in the primary instruction cache.
Reference: [14] <author> R. Uhlig, D. Nagle, T. Mudge, S. Sechrest, and J. Emer. </author> <title> Instruction Fetching: Coping with Code Bloat. </title> <booktitle> In Proceedings of the 22th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 345-356, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: The combined effect of all the optimizations proposed is unknown, especially for very advanced multiprocessor memory hierarchies. Finally, there is a lot of other work proposing optimizations 1 to enhance the cache performance of applications <ref> [7, 8, 14] </ref>, but it is not clear how to apply the results to the operating system. The goal of this paper is to perform a cost-effectiveness evaluation of a comprehensive set of hardware and software supports for the operating system to exploit the memory hierarchy of these machines.
Reference: [15] <author> C. Xia and J. Torrellas. </author> <title> Improving the Data Cache Performance of Multiprocesor Operating Systems. </title> <booktitle> In Proceedings of the 2nd International Symposium on High-Performance Computer Architecture, </booktitle> <pages> pages 85-94, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: A large group of researchers have examined the cache performance of the operating system without focusing much on proposing optimizations [1, 2, 5, 6, 9, 10, 11, 12]. There is some work specifically focused on optimizing the performance of the operating system <ref> [13, 15, 16] </ref>. However, it examines part of the problem only, for example, instruction accesses only or prefetching only. The combined effect of all the optimizations proposed is unknown, especially for very advanced multiprocessor memory hierarchies. <p> In the following, we discuss the instruction and data access patterns observed. A more detailed discussion can be found in two previous papers <ref> [13, 15] </ref>. 3.2.1 Instruction Access Patterns It is well-known that the instruction access patterns in the operating system are different from those in typical technical applications. In the former, tight loops account for a relatively small fraction of the execution time. <p> We select a set of supports that target the problems identified. These supports are code layout optimization, guarded sequential instruction prefetching, instruction stream buffers, support for block operations, support for coherence activity, and software data prefetching. We have proposed these supports in three previous papers <ref> [13, 15, 16] </ref>. In this section, we evaluate and compare them under a uniform environment, combining them in a single system. <p> Since, according to Section 3.2.2, block operations account for about 40% of operating system misses in the primary data cache, we address this problem first. To optimize block operations, we use an off-chip block transfer engine <ref> [15] </ref>. This engine is a smart cache controller that performs block operations in a memory-to-memory DMA-like fashion while holding the bus for the duration of the block transfer. In the meantime, the originator processor is stalled. Caches are bypassed to avoid cache pollution. <p> This scheme is especially attractive because it does not require modifying the processor chip. Furthermore, it has been shown to be better than simply using software prefetching of the source blocks or cache bypassing of the block data <ref> [15] </ref>. We call the scheme Blk. As Table 2 shows, Blk tries to eliminate all types of stall times. Indeed, block operations can create any type of read miss in any of the three caches, including cold, conflict and coherence misses. <p> Indeed, they miss in the three levels of caches and are, therefore, expensive. To remove most of these misses, we use a hybrid cache coherence protocol: an update-based one for a small set of heavily-shared variables, and an invalidate-based for the rest <ref> [15] </ref>. This set of variables includes barriers, active locks, and some variables that sometimes exhibit a producer-consumer behavior. All these variables together use 12 about 380 bytes and are placed in a single page. For this page only, we use the Firefly [3] update protocol. <p> Other optimizations included are the privatization of variables that can be privatized, the relocation of variables that exhibit false sharing into different cache lines, and the relocation of variables that are accessed in sequence into the same cache line. All these optimizations require software-only support <ref> [15] </ref>. We call the combined scheme Cohr. As Table 2 shows, Cohr tries to reduce stall time due to on-chip and off-chip coherence misses. The actual impact of the scheme on the number of misses is shown in Figure 1. <p> Intuitively, we could use prefetching to hide these misses. However, given that the operating system generates irregular reference patterns, hardware prefetching is hard to apply. Instead, hand-inserted software prefetching seems to be the only alternative <ref> [15] </ref>. To determine where to insert data prefetches, we measure the number of data misses suffered by each basic block of the operating system code. For the few basic blocks with the highest number of misses, we determine the source code statements that cause the misses.
Reference: [16] <author> C. Xia and J. Torrellas. </author> <title> Instruction Prefetching of Systems Codes With Layout Optimized for Reduced Cache Misses. </title> <booktitle> In Proceedings of the 23nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 271-282, </pages> <month> May </month> <year> 1996. </year> <month> 18 </month>
Reference-contexts: A large group of researchers have examined the cache performance of the operating system without focusing much on proposing optimizations [1, 2, 5, 6, 9, 10, 11, 12]. There is some work specifically focused on optimizing the performance of the operating system <ref> [13, 15, 16] </ref>. However, it examines part of the problem only, for example, instruction accesses only or prefetching only. The combined effect of all the optimizations proposed is unknown, especially for very advanced multiprocessor memory hierarchies. <p> We select a set of supports that target the problems identified. These supports are code layout optimization, guarded sequential instruction prefetching, instruction stream buffers, support for block operations, support for coherence activity, and software data prefetching. We have proposed these supports in three previous papers <ref> [13, 15, 16] </ref>. In this section, we evaluate and compare them under a uniform environment, combining them in a single system. <p> 5.9 3.9 3.5 3.7 3.8 Wr buff 10.4 10.3 10.5 10.6 0.4 1.1 1.1 Blk DMA 0.0 0.0 0.0 0.0 14.4 14.4 14.4 Busy 37.0 37.0 37.0 37.0 33.2 33.2 33.2 OS Time 100.0 93.0 87.8 84.2 74.9 69.6 68.2 10 4.2.1 Guarded Sequential Instruction Prefetching Guarded sequential instruction prefetching <ref> [16] </ref> is an on-chip hardware prefetching scheme designed to be combined with the code layout optimization described above. In the previous section, we saw that the code is organized in instruction sequences and then laid out in memory. <p> Indeed, S3.I+SM.I=10.4%. The misses that cause this stall often occur in contiguous addresses and tend to be clustered. This is especially true for cold misses. This observation suggests using instruction stream buffers [8] between secondary caches and the memory bus to complement the guarded sequential prefetcher <ref> [16] </ref>. In our experiments, we use two off-chip 8-entry direct-access stream buffers per processor. The buffers are allowed to look up the tertiary cache, thus avoiding unnecessary bus accesses when the line is in the tertiary cache.
References-found: 16

