URL: http://www.cs.cornell.edu/home/kleinber/vldb98.ps
Refering-URL: http://www.cs.cornell.edu/home/kleinber/kleinber.html
Root-URL: 
Email: dag@cs.berkeley.edu  kleinber@cs.cornell.edu  pragh@almaden.ibm.com  
Title: Clustering Categorical Data: An Approach Based on Dynamical Systems  
Author: David Gibson Jon Kleinberg Prabhakar Raghavan 
Address: Berkeley, CA 94720 USA  Ithaca, NY 14853 USA  San Jose, CA 95120 USA  
Affiliation: Dept. of Computer Science UC Berkeley  Dept. of Computer Science Cornell University  Almaden Research Center IBM  
Abstract: We describe a novel approach for clustering collections of sets, and its application to the analysis and mining of categorical data. By "categorical data," we mean tables with fields that cannot be naturally ordered by a metric | e.g., the names of producers of automobiles, or the names of products offered by a manufacturer. Our approach is based on an iterative method for assigning and propagating weights on the categorical values in a table; this facilitates a type of similarity measure arising from the co-occurrence of values in the dataset. Our techniques can be studied analytically in terms of certain types of non-linear dynamical systems. We discuss experiments on a variety of tables of synthetic and real data; we find that our iterative methods converge quickly to prominently correlated values of various categorical fields.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Agrawal, H. Mannila, R. Srikant, H. Toivonen, and A. I. Verkamo. </author> <title> Fast Discovery of Association Rules. </title> <editor> In U. M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthu 19 rusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, chapter 12, </booktitle> <pages> pages 307-328. </pages> <publisher> AAAI/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: If we are not trying to "focus" the weight in a particular 7 portion of the set of tuples, we can choose the uniform initialization (all weights set to 1, then normalized) or the random initialization (each weight set to an independently chosen random value in <ref> [0; 1] </ref>, then normalized). For combining operators which are sensitive to the choice of initial configuration | the product rule is a basic example | one can focus the weight on a particular portion of the set of tuples by initializing a configuration over a particular node.
Reference: [2] <author> R. Agrawal, T. Imielinski, A. Swami. </author> <title> Mining association rules between sets of items in large databases. </title> <booktitle> Proc. SIGMOD, </booktitle> <year> 1993. </year>
Reference: [3] <author> A. </author> <title> Agresti. Categorical Data Analysis. </title> <publisher> Wiley-Interscience, </publisher> <year> 1990. </year>
Reference: [4] <author> N. Alon, </author> <title> "Eigenvalues and expanders," </title> <journal> Combinatorica, </journal> <year> 1986. </year>
Reference: [5] <author> C. Berge, </author> <title> Hypergraphs, </title> <publisher> North-Holland, </publisher> <year> 1973. </year>
Reference: [6] <author> A. Blum, J. Spencer, </author> <title> "Coloring random and semi-random k-colorable graphs." </title> <editor> J. </editor> <booktitle> Algorithms, </booktitle> <address> 19(1995). </address>
Reference-contexts: This is a technique used in the study of computationally difficult problems. For instance, in combinatorial optimization, the work of Boppana [7] analyzes graph bisection heuristics in quasi-random graphs in which a "small" bisection is hidden; Blum and Spencer <ref> [6] </ref> adopt the same approach to the analysis of graph-coloring algorithms. The advantage of such quasi-random data is that we may control in a precise manner the various parameters associated with the dynamical systems we use, studying in the process their efficacy at discovering the appropriate planted structure.
Reference: [7] <author> R. Boppana, </author> <title> "Eigenvalues and graph bisection: An average-case analysis," </title> <booktitle> Proc. IEEE Symp. on Foundations of Computer Science, </booktitle> <year> 1987. </year>
Reference-contexts: This is a technique used in the study of computationally difficult problems. For instance, in combinatorial optimization, the work of Boppana <ref> [7] </ref> analyzes graph bisection heuristics in quasi-random graphs in which a "small" bisection is hidden; Blum and Spencer [6] adopt the same approach to the analysis of graph-coloring algorithms.
Reference: [8] <author> S. Brin, R. Motwani, J.D. Ullman, S. Tsur. </author> <title> "Dynamic itemset counting and implication rules for market basket data". </title> <booktitle> Proc. ACM SIGMOD, </booktitle> <year> 1997. </year>
Reference: [9] <author> S. Brin, L. </author> <title> Page. "Anatomy of a Large-Scale Hypertextual Web Search Engine," </title> <booktitle> Proc. 7th International World Wide Web Conference, </booktitle> <year> 1998. </year>
Reference: [10] <author> E. Charniak, </author> <title> Statistical Language Learning, </title> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference: [11] <author> T. Chiueh, </author> <title> "Content-based image indexing," </title> <booktitle> Proc. VLDB Conference, </booktitle> <year> 1994. </year>
Reference: [12] <author> F.R.K. Chung, </author> <title> Spectral Graph Theory, </title> <publisher> AMS Press, </publisher> <year> 1997. </year>
Reference: [13] <author> G. Das, H. Mannila, P. Ronkainen, </author> <title> "Simliarity of attributes by external probes," </title> <type> manuscript, </type> <year> 1997. </year>
Reference: [14] <author> S. Deerwester, S. T. Dumais, T.K. Landauer, G.W. Furnas, and R.A. Harshman. </author> <title> Indexing by latent semantic analysis. </title> <journal> Journal of the Society for Information Science, </journal> <volume> 41(6), </volume> <pages> 391-407, </pages> <year> 1990. </year>
Reference: [15] <author> R.L. Devaney, </author> <title> An Introduction to Chaotic Dynamical Systems, </title> <publisher> Benjamin Cummings, </publisher> <year> 1989. </year>
Reference-contexts: That is, it is a point which remains the same under the (repeated) application of f . Fixed points are one of the central objects of study in dynamical systems, and by iterating f one often reaches a fixed point. (See <ref> [15] </ref> for a comprehensive introduction to the mathematical study of dynamical systems.) At the end of this section, we indicate the connections between such systems and spectral graph theory, discussed above as a powerful method for attacking 5 Attribute Tuple a b c 1. A W 1 3.
Reference: [16] <author> W.E. Donath, A.J. Hoffman, </author> <title> "Lower bounds for the partitioning of graphs", </title> <journal> IBM Journal of Research and Development, </journal> <volume> 17(1973). </volume> <pages> 420-425. </pages>
Reference: [17] <author> R.O. Duda and P.E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1973. </year>
Reference: [18] <author> C. Faloutsos. </author> <title> Searching Multimedia Databases by Content. </title> <publisher> Kluwer Academic, </publisher> <year> 1996. </year>
Reference: [19] <author> M. Fiedler, </author> <title> "Algebraic connectivity of graphs," Czech. </title> <journal> Math. Journal 23(1973), </journal> <pages> pp. 298-305. </pages>
Reference: [20] <author> M. Flickner, H. Sawhney, W. Niblack, J. Ashley, Q. Huang, B. Dom, M. Gorkani, J. Hafner, D. Lee, D. Petkovic, D. Steele and P. Yanker. </author> <title> Query by image and video content: The QBIC system. </title> <journal> IEEE Computer, </journal> <volume> 28, </volume> <pages> 23-32, </pages> <year> 1995. </year>
Reference: [21] <author> M. Garey and D. Johnson. </author> <title> Computers and Intractability: A Guide to the Theory of NP-Completeness. W.H. </title> <publisher> Freeman, </publisher> <year> 1979. </year>
Reference: [22] <author> G. Golub, C.F. Van Loan, </author> <title> Matrix Computations, </title> <publisher> Johns Hopkins University Press, </publisher> <year> 1989. </year>
Reference-contexts: Update the set of vectors fw h1i ; : : : ; w hmi g so that it is orthonormal. Assuming that a standard method is used to keep the set fw hii g orthornormal (see e.g. the Gram-Schmidt procedure in <ref> [22] </ref>), the configuration w h1i iterates to a basin as before; we refer to this as the "principal basin." The updating of the other configurations is interleaved with orthonormalization steps, and we refer to these as "non-principal basins." In the experiments that follow, we will see that these non-principal basins provide
Reference: [23] <author> Eui-Hong Han, George Karypis, Vipin Kumar, Bamshad Mobasher, </author> <title> "Clustering Based On Association Rule Hypergraphs," </title> <booktitle> Workshop on Research Issues on Data Mining and Knowledge Discovery, </booktitle> <year> 1997. </year>
Reference: [24] <author> T. Hastie, W. Stuetzle, </author> <title> "Principal curves," </title> <journal> J. Am. Stat. Assoc. </journal> <volume> 84(1989), </volume> <pages> pp. 502-516. </pages>
Reference-contexts: In a related vein, the methodology of principal curves allows for a non-linear approach to dimension-reduction, which can sometimes achieve significant gains over linear methods such as principal component analysis <ref> [24, 30] </ref>. This method thus implicitly requires an embedding in a given space in which to perform the dimension-reduction; once this dimension-reduction is performed, one can then approach the low-dimensional clustering problem on the data in a number of standard ways.
Reference: [25] <author> Zhexue Huang, </author> <title> "A Fast Clustering Algorithm to Cluster Very Large Categorical Data Sets in Data Mining." </title> <booktitle> Workshop on Research Issues on Data Mining and Knowledge Discovery, </booktitle> <year> 1997. </year>
Reference: [26] <author> M. Jerrum, A. Sinclair, </author> <title> "The Markov chain Monte Carlo method: An approach to approximate counting and integration," in Approximation Algorithms for NP-hard Problems, </title> <editor> D.S.Hochbaum ed., </editor> <publisher> PWS Publishing, </publisher> <year> 1996 </year>
Reference: [27] <author> I.T. Jolliffe. </author> <title> Principal Component Analysis. </title> <publisher> Springer-Verlag, </publisher> <year> 1986. </year>
Reference: [28] <author> J. Kleinberg, </author> <title> "Authoritative sources in a hyperlinked environment," </title> <booktitle> Proc. ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <year> 1998. </year> <note> Also available as IBM Research Report RJ 10076(91892) May 1997. </note>
Reference: [29] <author> T. Kohonen, </author> <title> "Self-organized formation of topologically correct feature maps," </title> <journal> Bio. Cy-bern., </journal> <volume> 43(1982), </volume> <pages> pp. 59-69. </pages>
Reference-contexts: In many of these neural network settings, the emphasis is quite different from ours | for example, one is concerned with the approximation of binary decision rules, 8 or with the approximation of connection strengths among nodes on a discrete lattice (e.g. the work on Kohonen maps <ref> [29] </ref>). In our work, on the other hand, the analogues of "connections" | the co-occurrences among tuples in a database | typically do not have any geometric structure that can be exploited, and our approach is to treat the connections as fixed while individual item weights are updated.
Reference: [30] <author> M. Kramer, </author> <title> "Nonlinear principal component analysis using autoassociative neural networks," </title> <journal> AIChE Journal 37(1991), </journal> <pages> pp. 233-243. </pages>
Reference-contexts: In a related vein, the methodology of principal curves allows for a non-linear approach to dimension-reduction, which can sometimes achieve significant gains over linear methods such as principal component analysis <ref> [24, 30] </ref>. This method thus implicitly requires an embedding in a given space in which to perform the dimension-reduction; once this dimension-reduction is performed, one can then approach the low-dimensional clustering problem on the data in a number of standard ways.
Reference: [31] <author> H. Mannila, H. Toivonen, A.I. Verkamo, </author> <title> "Discovering frequent episodes in sequences," </title> <booktitle> Proc. KDD Conf., </booktitle> <year> 1995. </year>
Reference-contexts: For data exhibiting a strong local cause-effect structure, we hope to see such structure emerge from the co-occurrences among events close in time. This framework bears some similarity to a method of Mannila, Toivonen, and Verkamo <ref> [31] </ref> for defining frequent "episodes" in sequential data.
Reference: [32] <author> H. Ritter, T. Martinetz, </author> <title> Neural Computation and Self-Organizing Maps, </title> <publisher> Addison-Wesley, </publisher> <year> 1992. </year>
Reference-contexts: Treating the nodes with positive numbers as one "cluster" in G, and the nodes with negative numbers as another, typically partitions the underlying graph into two relatively dense pieces, with few edges in between. The iteration of systems of non-linear equations arises also in connection with neural networks <ref> [32, 33] </ref>. In many of these neural network settings, the emphasis is quite different from ours | for example, one is concerned with the approximation of binary decision rules, 8 or with the approximation of connection strengths among nodes on a discrete lattice (e.g. the work on Kohonen maps [29]).
Reference: [33] <editor> D.E. Rumelhart, J.L. McClelland, eds., </editor> <booktitle> Parallel and Distributed Processing: Explorations in the Microstructure of Cognition, </booktitle> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: Treating the nodes with positive numbers as one "cluster" in G, and the nodes with negative numbers as another, typically partitions the underlying graph into two relatively dense pieces, with few edges in between. The iteration of systems of non-linear equations arises also in connection with neural networks <ref> [32, 33] </ref>. In many of these neural network settings, the emphasis is quite different from ours | for example, one is concerned with the approximation of binary decision rules, 8 or with the approximation of connection strengths among nodes on a discrete lattice (e.g. the work on Kohonen maps [29]).
Reference: [34] <author> J. Seiferas, </author> <title> A large bibliography on theory/foundations of computer science, </title> <note> at http://liinwww.ira.uka.de/bibliography/Theory/Seiferas/. </note>
Reference-contexts: Basic data sets. First, we consider two data sets that illustrate co-occurrence patterns. (1) Bibliographic data. We took publicly-accessible bibliographic databases of 7000 papers from database research [38], and of 30,000 papers written on theoretical computer science and related fields <ref> [34] </ref>, and constructed two data sets each with four columns as follows. For each paper, we recorded the name of the first author, the name of the second author, the conference or journal of publication, and the year of publication.
Reference: [35] <author> M. Shub, </author> <type> personal communication. 21 </type>
Reference: [36] <author> D. Spielman, S. Teng, </author> <title> "Spectral partitioning works: Planar graphs and finite-element meshes," </title> <booktitle> Proc. IEEE Symp. on Foundations of Computer Science, </booktitle> <year> 1996. </year>
Reference: [37] <author> H. Toivonen. </author> <title> Sampling large databases for finding association rules. </title> <booktitle> Proc. VLDB Conference, </booktitle> <year> 1996. </year>
Reference: [38] <author> G. Wiederhold, </author> <title> Bibliography on database systems, </title> <note> at http://liinwww.ira.uka.de/bibliography/Database/Wiederhold/. </note>
Reference-contexts: Basic data sets. First, we consider two data sets that illustrate co-occurrence patterns. (1) Bibliographic data. We took publicly-accessible bibliographic databases of 7000 papers from database research <ref> [38] </ref>, and of 30,000 papers written on theoretical computer science and related fields [34], and constructed two data sets each with four columns as follows.
Reference: [39] <author> T. Zhang, R. Ramakrishnan, M. Livny. </author> <title> Birch: An efficient data clustering method for very large databases. </title> <booktitle> Proc. ACM SIGMOD, </booktitle> <year> 1996. </year> <month> 22 </month>
References-found: 39

