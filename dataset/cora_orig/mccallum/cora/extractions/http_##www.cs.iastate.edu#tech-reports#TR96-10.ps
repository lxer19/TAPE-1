URL: http://www.cs.iastate.edu/tech-reports/TR96-10.ps
Refering-URL: http://www.cs.iastate.edu/tech-reports/catalog.html
Root-URL: http://www.cs.iastate.edu
Email: lutz@cs.iastate.edu  davids@math.holycross.edu  
Title: Feasible Reductions to Kolmogorov-Loveland Stochastic Sequences 1  
Author: Jack H. Lutz David L. Schweizer 
Note: Loveland.  
Address: Ames, IA 50011 U.S.A.  Worcester, MA 01610 U.S.A.  
Affiliation: Department of Computer Science Iowa State University  Department of Mathematics College of the Holy Cross  
Abstract:  
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. H. Bennett. </author> <title> Logical depth and physical complexity. </title> <editor> In R. Herken, editor, </editor> <booktitle> The Universal Turing Machine: A Half-Century Survey, </booktitle> <pages> pages 227-257. </pages> <publisher> Oxford University Press, </publisher> <year> 1988. </year> <month> 11 </month>
Reference-contexts: It follows immediately from the main theorem that there are sequences S that are Kolmogorov-Loveland stochastic, but also strongly deep in the sense of Bennett <ref> [1] </ref>. Such sequences S are computationally "very far from random" [1, 12]. The main theorem also implies that the class RAND of all random oracles cannot be replaced by the class KL-STOCH of all Kolmogorov-Loveland stochastic oracles in some known characterizations of complexity classes. <p> It follows immediately from the main theorem that there are sequences S that are Kolmogorov-Loveland stochastic, but also strongly deep in the sense of Bennett [1]. Such sequences S are computationally "very far from random" <ref> [1, 12] </ref>. The main theorem also implies that the class RAND of all random oracles cannot be replaced by the class KL-STOCH of all Kolmogorov-Loveland stochastic oracles in some known characterizations of complexity classes. <p> As just one example, a "folklore" result states that P (RAND) " REC = BPP, that is, that a recursive language is P T -reducible to some random language if and only if it is probabilistically decidable with bounded error in polynomial time <ref> [1, 3] </ref>. In contrast, the main theorem immediately implies that P (KL-STOCH) contains every language. <p> convergent if there is a total recursive function (modulus of convergence) m : N ! N such that, for all r 2 N; P 1 A bias sequence is a sequence ~ fi = (fi 0 ; fi 1 ; : : :) of real numbers (biases) fi i 2 <ref> [0; 1] </ref>: A bias sequence ~ fi determines the coin-toss probability measure ~ fi on Cantor space, which corresponds to a random experiment in which a language A 2 C is chosen probabilistically as follows. <p> The [selection rule] is an algorithm that says which card must be turned next and whether it must be turned only for information or [is to be] selected into the subsequence. We also make use of the following large deviation result. Lemma 2.2 (Chernoff bound [11]). Let p 2 <ref> [0; 1] </ref>; let X 1 ; : : :; X n be independent 0/1-valued random variables such that each P [X i = 1] = p; and let S = X 1 + + X n : Then: 6 1. <p> Construction 3.2. We use the functions l; q; r : N ! N; where l (n) = js n j = blog (n + 1)c; q (n) = 12 l (n) + 3 ; r (n) = m=0 7 We also use the function * : N ! <ref> [0; 1] </ref> where *(n) = l (n) + 3 For A 2 C and i 2 N; define the bias fi A i 2 [0; 1] by i = 2 1 + 3*(n)[[s n 2 A]] ; where r (n) i &lt; r (n) + q (n); and let ~ fi <p> (n + 1)c; q (n) = 12 l (n) + 3 ; r (n) = m=0 7 We also use the function * : N ! <ref> [0; 1] </ref> where *(n) = l (n) + 3 For A 2 C and i 2 N; define the bias fi A i 2 [0; 1] by i = 2 1 + 3*(n)[[s n 2 A]] ; where r (n) i &lt; r (n) + q (n); and let ~ fi A = (fi A 1 ; : : :): (Note that the bit [[s n 2 A]] has been encoded into each of q <p> A (rec): For convenience, we write ~ fi = ~ fi A ; n = A n ; and F = F A : For each n 2 N; define the event E n = B 2 C fi o and the function d n : f0; 1g fl ! <ref> [0; 1] </ref>; d n (w) = P (E n j C w ); where the conditional probability refers (as do all subsequent probabilities in this proof) to the coin-toss probability measure ~ fi : It is routine to check that each d n is a ~ fi-martingale, and that the function <p> a strong, quantitative separation of RAND from KL-STOCH, since Juedes, Lathrop, and Lutz [12] have shown that only a meager (that is, negligibly small in the sense of Baire category) set of sequences have the property of being reducible to some random sequence in some computable running time. 10 Bennett <ref> [1] </ref> has introduced the notion of computational depth, which measures the "value" of information in terms of the amount of "computational work" that has been "added to its organization". <p> In the case of infinite binary sequences, Bennett has defined both strong depth and weak depth, and shown that, in a technical, computational sense, strongly deep sequences are "very far from random". (See <ref> [1, 18, 12] </ref> for definitions and discussion of computational depth.) The proof of Shen 0 [27] exhibits a sequence that is Kolmgorov-Loveland stochastic and not random. That sequence, however, is random relative to a computable probability measure and so is not even weakly deep in the sense of Bennett. <p> Nevertheless, we now note that a Kolmogorov-Loveland stochastic sequence may be strongly deep. Corollary 3.6. There exist Kolmogorov-Loveland stochastic sequences that are strongly deep. Proof. Let K be the diagonal halting problem. By Theorem 3.5, let S be a Kolmogorov-Loveland stochastic sequence such that K P tt S: Bennett <ref> [1] </ref> has shown that K is strongly deep and, by his deterministic slow growth law, that strongly deep sequences are only tt -reducible to sequences that are themsleves strongly deep. (Proofs of these results also appear in [12].) Hence, S is strongly deep. 2 Acknowledgment The second author gratefully acknowledges the
Reference: [2] <author> R. V. </author> <title> Book. On languages reducible to algorithmically random lan-guages. </title> <journal> SIAM Journal on Computing, </journal> <volume> 23 </volume> <pages> 1275-1282, </pages> <year> 1994. </year>
Reference-contexts: In contrast, the main theorem immediately implies that P (KL-STOCH) contains every language. See <ref> [3, 22, 2, 4] </ref> for other known characterizations using random oracles that, by the main theorem, cannot be extended to Kolmogorov-Loveland stochastic oracles. 2 Notation and Preliminaries We write f0; 1g fl for the set of all (finite, binary) strings, and we write jxj for the length of a string x.
Reference: [3] <author> R. V. Book, J. H. Lutz, and K. W. Wagner. </author> <title> An observation on probability versus randomness with applications to complexity classes. </title> <journal> Mathematical Systems Theory, </journal> <volume> 27 </volume> <pages> 201-209, </pages> <year> 1994. </year>
Reference-contexts: As just one example, a "folklore" result states that P (RAND) " REC = BPP, that is, that a recursive language is P T -reducible to some random language if and only if it is probabilistically decidable with bounded error in polynomial time <ref> [1, 3] </ref>. In contrast, the main theorem immediately implies that P (KL-STOCH) contains every language. <p> In contrast, the main theorem immediately implies that P (KL-STOCH) contains every language. See <ref> [3, 22, 2, 4] </ref> for other known characterizations using random oracles that, by the main theorem, cannot be extended to Kolmogorov-Loveland stochastic oracles. 2 Notation and Preliminaries We write f0; 1g fl for the set of all (finite, binary) strings, and we write jxj for the length of a string x.
Reference: [4] <author> R. V. Book, H. Vollmer, and K. W. Wagner. </author> <title> On type-2 probabilistic quantifiers. </title> <booktitle> In Proceedings of the 23 rd International Colloquium on Automata, Languages, and Programming, </booktitle> <pages> pages 369-380. </pages> <publisher> Springer-Verlag, </publisher> <year> 1996. </year>
Reference-contexts: In contrast, the main theorem immediately implies that P (KL-STOCH) contains every language. See <ref> [3, 22, 2, 4] </ref> for other known characterizations using random oracles that, by the main theorem, cannot be extended to Kolmogorov-Loveland stochastic oracles. 2 Notation and Preliminaries We write f0; 1g fl for the set of all (finite, binary) strings, and we write jxj for the length of a string x.
Reference: [5] <author> J. M. Breutzmann and J. H. Lutz. </author> <title> Equivalence of measures of complexity classes. </title> <note> Submitted. </note>
Reference-contexts: Given a bias sequence ~ fi; a ~ fi-martingale is a function d : f0; 1g fl ! [0; 1) such that for all w 2 f0; 1g fl ; (The reader is referred to <ref> [30, 5] </ref> for discussion of this definition and its motivation.) The success set of a ~ fi-martingale d is S 1 [d] = A 2 C fi o 4 The unitary success set of a ~ fi-martingale d is S 1 [d] = d (w)1 A sequence R 2 C is
Reference: [6] <author> G. J. Chaitin. </author> <title> A theory of program size formally identical to information theory. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 22 </volume> <pages> 329-340, </pages> <year> 1975. </year>
Reference-contexts: 1 Introduction In the mid-1960's, Martin-Lof [23] used the general theory of algorithms to formulate the first successful definition of the randomness of individual binary sequences. Subsequent definitions, using a variety of conceptual approaches, were introduced by Levin [17], Schnorr [24, 25], Chaitin <ref> [6, 7, 8] </ref>, Solovay [28], and Shen 0 [26]. Each of these definitions was shown to be equivalent to Martin-Lof's, in the sense that a binary sequence R is algorithmically random according to the given definition if and only if R is algorithmically random according to Martin-Lof's definition.
Reference: [7] <author> G. J. Chaitin. </author> <title> Algorithmic Information Theory. </title> <publisher> Cambridge University Press, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: 1 Introduction In the mid-1960's, Martin-Lof [23] used the general theory of algorithms to formulate the first successful definition of the randomness of individual binary sequences. Subsequent definitions, using a variety of conceptual approaches, were introduced by Levin [17], Schnorr [24, 25], Chaitin <ref> [6, 7, 8] </ref>, Solovay [28], and Shen 0 [26]. Each of these definitions was shown to be equivalent to Martin-Lof's, in the sense that a binary sequence R is algorithmically random according to the given definition if and only if R is algorithmically random according to Martin-Lof's definition.
Reference: [8] <author> G. J. Chaitin. </author> <title> Incompleteness theorems for random reals. </title> <booktitle> Advances in Applied Mathematics, </booktitle> <volume> 8 </volume> <pages> 119-146, </pages> <year> 1987. </year>
Reference-contexts: 1 Introduction In the mid-1960's, Martin-Lof [23] used the general theory of algorithms to formulate the first successful definition of the randomness of individual binary sequences. Subsequent definitions, using a variety of conceptual approaches, were introduced by Levin [17], Schnorr [24, 25], Chaitin <ref> [6, 7, 8] </ref>, Solovay [28], and Shen 0 [26]. Each of these definitions was shown to be equivalent to Martin-Lof's, in the sense that a binary sequence R is algorithmically random according to the given definition if and only if R is algorithmically random according to Martin-Lof's definition.
Reference: [9] <author> A. Church. </author> <title> On the concept of a random sequence. </title> <journal> Bulletin of the American Mathematical Society, </journal> <volume> 46 </volume> <pages> 130-135, </pages> <year> 1940. </year>
Reference-contexts: These algorithmic rules (which are described in section 2) are more general than earlier selection rules proposed by von Mises [31], Wald [32], and Church <ref> [9] </ref> in two respects.
Reference: [10] <author> P. Gacs. </author> <title> Every sequence is reducible to a random one. </title> <journal> Information and Control, </journal> <volume> 70 </volume> <pages> 186-192, </pages> <year> 1986. </year>
Reference-contexts: This note refines the method of Shen 0 [27] in order to establish a stronger, more quantitative separation between randomness and Komogorov-Loveland stochasticity. Kucera [16] and Gacs <ref> [10] </ref> have proven that for every sequence A there is a random sequence R such that A is Turing reducible to R: However, it is well known that this does not hold for truth-table reducibility (Turing reducibility with computable running time).
Reference: [11] <author> T. Hagerup and C. Rub. </author> <title> A guided tour of Chernoff bounds. </title> <journal> Information Processing Letters, </journal> <volume> 33 </volume> <pages> 305-308, </pages> <year> 1990. </year>
Reference-contexts: The [selection rule] is an algorithm that says which card must be turned next and whether it must be turned only for information or [is to be] selected into the subsequence. We also make use of the following large deviation result. Lemma 2.2 (Chernoff bound <ref> [11] </ref>). Let p 2 [0; 1]; let X 1 ; : : :; X n be independent 0/1-valued random variables such that each P [X i = 1] = p; and let S = X 1 + + X n : Then: 6 1.
Reference: [12] <author> D. W. Juedes, J. I. Lathrop, and J. H. Lutz. </author> <title> Computational depth and reducibility. </title> <journal> Theoretical Computer Science, </journal> <volume> 132 </volume> <pages> 37-70, </pages> <year> 1994. </year>
Reference-contexts: In fact, Juedes, Lathrop, and Lutz <ref> [12] </ref> have noted that, in the sense of Baire category, almost every se quence A has the property that A is not reducible to any random sequence in any computable running time. <p> It follows immediately from the main theorem that there are sequences S that are Kolmogorov-Loveland stochastic, but also strongly deep in the sense of Bennett [1]. Such sequences S are computationally "very far from random" <ref> [1, 12] </ref>. The main theorem also implies that the class RAND of all random oracles cannot be replaced by the class KL-STOCH of all Kolmogorov-Loveland stochastic oracles in some known characterizations of complexity classes. <p> 3.4 tells us that jA 4 F A (S)j &lt; 1; whence A P tt F A (S): It follows by Observation 3.3 that A P tt S: 2 As noted in the introduction, Theorem 3.5 exhibits a strong, quantitative separation of RAND from KL-STOCH, since Juedes, Lathrop, and Lutz <ref> [12] </ref> have shown that only a meager (that is, negligibly small in the sense of Baire category) set of sequences have the property of being reducible to some random sequence in some computable running time. 10 Bennett [1] has introduced the notion of computational depth, which measures the "value" of information <p> In the case of infinite binary sequences, Bennett has defined both strong depth and weak depth, and shown that, in a technical, computational sense, strongly deep sequences are "very far from random". (See <ref> [1, 18, 12] </ref> for definitions and discussion of computational depth.) The proof of Shen 0 [27] exhibits a sequence that is Kolmgorov-Loveland stochastic and not random. That sequence, however, is random relative to a computable probability measure and so is not even weakly deep in the sense of Bennett. <p> S be a Kolmogorov-Loveland stochastic sequence such that K P tt S: Bennett [1] has shown that K is strongly deep and, by his deterministic slow growth law, that strongly deep sequences are only tt -reducible to sequences that are themsleves strongly deep. (Proofs of these results also appear in <ref> [12] </ref>.) Hence, S is strongly deep. 2 Acknowledgment The second author gratefully acknowledges the hospitality of Dan Ashlock and the Iowa State University Department of Mathematics, where he was a visitor when this research was conducted.
Reference: [13] <author> A. N. </author> <title> Kolmogorov. On tables of random numbers. </title> <journal> Sankhya, Series A, </journal> <volume> 25 </volume> <pages> 369-376, </pages> <year> 1963. </year>
Reference-contexts: A of a random sequence R is chosen according to an "admissible selection rule", then the limiting frequency of 1's in the subsequence A is exactly 1 2 : The broadest class of admissible selection rules that has been studied in this context is the class of Kolmogorov-Loveland selection rules <ref> [13, 14, 19, 20] </ref>. These algorithmic rules (which are described in section 2) are more general than earlier selection rules proposed by von Mises [31], Wald [32], and Church [9] in two respects. <p> Fix a prefix w v R such that d n 0 (w) 1: Then we have 2 c d n 0 (w) 2 c &gt; d (w) 2 c n=m (2c) so n 0 &lt; m (2c): Thus J is finite. 2 The notion of Kolmogorov-Loveland stochasticity was defined in <ref> [13, 14, 19, 20] </ref>; detailed discussions may be found in [29, 15, 18].
Reference: [14] <author> A. N. </author> <title> Kolmogorov. Combinatorial foundations of information theory and calculus of probabilities. </title> <journal> Russian Mathematical Surveys, </journal> <volume> 38 </volume> <pages> 29-40, </pages> <year> 1983. </year>
Reference-contexts: A of a random sequence R is chosen according to an "admissible selection rule", then the limiting frequency of 1's in the subsequence A is exactly 1 2 : The broadest class of admissible selection rules that has been studied in this context is the class of Kolmogorov-Loveland selection rules <ref> [13, 14, 19, 20] </ref>. These algorithmic rules (which are described in section 2) are more general than earlier selection rules proposed by von Mises [31], Wald [32], and Church [9] in two respects. <p> Fix a prefix w v R such that d n 0 (w) 1: Then we have 2 c d n 0 (w) 2 c &gt; d (w) 2 c n=m (2c) so n 0 &lt; m (2c): Thus J is finite. 2 The notion of Kolmogorov-Loveland stochasticity was defined in <ref> [13, 14, 19, 20] </ref>; detailed discussions may be found in [29, 15, 18].
Reference: [15] <author> A. N. Kolmogorov and V. A. Uspenskii. </author> <title> Algorithms and randomness. </title> <journal> translated in Theory of Probability and its Applications, </journal> <volume> 32 </volume> <pages> 389-412, </pages> <year> 1987. </year>
Reference-contexts: bits of R that is chosen according to a Kolmogorov-Loveland selection rule, the limiting frequency of 1's in A is 1 2 : In the late 1980's, Shen 0 [27] proved that the converse does not hold, thereby solving a problem that had been open for some twenty years. (See <ref> [15, 29, 18] </ref> for more detailed histories of this problem and the role of stochasticity in the foundations of probability theory.) Thus, the random sequences form a proper subset of the set of all Kolmogorov-Loveland stochastic sequences. <p> 0 (w) 1: Then we have 2 c d n 0 (w) 2 c &gt; d (w) 2 c n=m (2c) so n 0 &lt; m (2c): Thus J is finite. 2 The notion of Kolmogorov-Loveland stochasticity was defined in [13, 14, 19, 20]; detailed discussions may be found in <ref> [29, 15, 18] </ref>.
Reference: [16] <author> A. Kucera. </author> <title> Measure, 0 1 -classes, and complete extensions of PA. </title> <booktitle> In Recursion Theory Week, volume 1141 of Lecture Notes in Mathematics, </booktitle> <pages> pages 245-259. </pages> <publisher> Springer-Verlag, </publisher> <year> 1985. </year>
Reference-contexts: This note refines the method of Shen 0 [27] in order to establish a stronger, more quantitative separation between randomness and Komogorov-Loveland stochasticity. Kucera <ref> [16] </ref> and Gacs [10] have proven that for every sequence A there is a random sequence R such that A is Turing reducible to R: However, it is well known that this does not hold for truth-table reducibility (Turing reducibility with computable running time).
Reference: [17] <author> L. A. Levin. </author> <title> On the notion of a random sequence. </title> <journal> Soviet Mathematics Doklady, </journal> <volume> 14 </volume> <pages> 1413-1416, </pages> <year> 1973. </year>
Reference-contexts: 1 Introduction In the mid-1960's, Martin-Lof [23] used the general theory of algorithms to formulate the first successful definition of the randomness of individual binary sequences. Subsequent definitions, using a variety of conceptual approaches, were introduced by Levin <ref> [17] </ref>, Schnorr [24, 25], Chaitin [6, 7, 8], Solovay [28], and Shen 0 [26].
Reference: [18] <author> M. Li and P. M. B. Vitanyi. </author> <title> An Introduction to Kolmogorov Complexity and its Applications. </title> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: bits of R that is chosen according to a Kolmogorov-Loveland selection rule, the limiting frequency of 1's in A is 1 2 : In the late 1980's, Shen 0 [27] proved that the converse does not hold, thereby solving a problem that had been open for some twenty years. (See <ref> [15, 29, 18] </ref> for more detailed histories of this problem and the role of stochasticity in the foundations of probability theory.) Thus, the random sequences form a proper subset of the set of all Kolmogorov-Loveland stochastic sequences. <p> 0 (w) 1: Then we have 2 c d n 0 (w) 2 c &gt; d (w) 2 c n=m (2c) so n 0 &lt; m (2c): Thus J is finite. 2 The notion of Kolmogorov-Loveland stochasticity was defined in [13, 14, 19, 20]; detailed discussions may be found in <ref> [29, 15, 18] </ref>. <p> In the case of infinite binary sequences, Bennett has defined both strong depth and weak depth, and shown that, in a technical, computational sense, strongly deep sequences are "very far from random". (See <ref> [1, 18, 12] </ref> for definitions and discussion of computational depth.) The proof of Shen 0 [27] exhibits a sequence that is Kolmgorov-Loveland stochastic and not random. That sequence, however, is random relative to a computable probability measure and so is not even weakly deep in the sense of Bennett.
Reference: [19] <author> D. W. Loveland. </author> <title> The Kleene hierarchy classification of recursively random sequences. </title> <journal> Transactions of the American Mathematical Society, </journal> <volume> 125 </volume> <pages> 497-510, </pages> <year> 1966. </year>
Reference-contexts: A of a random sequence R is chosen according to an "admissible selection rule", then the limiting frequency of 1's in the subsequence A is exactly 1 2 : The broadest class of admissible selection rules that has been studied in this context is the class of Kolmogorov-Loveland selection rules <ref> [13, 14, 19, 20] </ref>. These algorithmic rules (which are described in section 2) are more general than earlier selection rules proposed by von Mises [31], Wald [32], and Church [9] in two respects. <p> Fix a prefix w v R such that d n 0 (w) 1: Then we have 2 c d n 0 (w) 2 c &gt; d (w) 2 c n=m (2c) so n 0 &lt; m (2c): Thus J is finite. 2 The notion of Kolmogorov-Loveland stochasticity was defined in <ref> [13, 14, 19, 20] </ref>; detailed discussions may be found in [29, 15, 18].
Reference: [20] <author> D. W. Loveland. </author> <title> A new interpretation of von Mises' concept of a random sequence. </title> <journal> Zeitschrift fur Mathematische Logik und Grundlagen der Mathematik, </journal> <volume> 12 </volume> <pages> 279-294, </pages> <year> 1966. </year>
Reference-contexts: A of a random sequence R is chosen according to an "admissible selection rule", then the limiting frequency of 1's in the subsequence A is exactly 1 2 : The broadest class of admissible selection rules that has been studied in this context is the class of Kolmogorov-Loveland selection rules <ref> [13, 14, 19, 20] </ref>. These algorithmic rules (which are described in section 2) are more general than earlier selection rules proposed by von Mises [31], Wald [32], and Church [9] in two respects. <p> Fix a prefix w v R such that d n 0 (w) 1: Then we have 2 c d n 0 (w) 2 c &gt; d (w) 2 c n=m (2c) so n 0 &lt; m (2c): Thus J is finite. 2 The notion of Kolmogorov-Loveland stochasticity was defined in <ref> [13, 14, 19, 20] </ref>; detailed discussions may be found in [29, 15, 18].
Reference: [21] <author> J. H. Lutz. </author> <title> Almost everywhere high nonuniform complexity. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 44 </volume> <pages> 220-258, </pages> <year> 1992. </year>
Reference-contexts: ~ firandom relative to A; and the class RAND A ~ fi (rec); consisting of all sequences that are rec-~ firandom relative to A: The following property of rec-~ fi-random sequences (relative to an oracle A) is an easy extension of a special case of the resource-bounded Borel-Cantelli lemma of <ref> [21] </ref>. Lemma 2.1.
Reference: [22] <author> J. H. Lutz. </author> <title> On independent random oracles. </title> <journal> Theoretical Computer Science, </journal> <volume> 92 </volume> <pages> 301-307, </pages> <year> 1992. </year>
Reference-contexts: In contrast, the main theorem immediately implies that P (KL-STOCH) contains every language. See <ref> [3, 22, 2, 4] </ref> for other known characterizations using random oracles that, by the main theorem, cannot be extended to Kolmogorov-Loveland stochastic oracles. 2 Notation and Preliminaries We write f0; 1g fl for the set of all (finite, binary) strings, and we write jxj for the length of a string x.
Reference: [23] <author> P. Martin-Lof. </author> <title> On the definition of random sequences. </title> <journal> Information and Control, </journal> <volume> 9 </volume> <pages> 602-619, </pages> <year> 1966. </year>
Reference-contexts: 1 Introduction In the mid-1960's, Martin-Lof <ref> [23] </ref> used the general theory of algorithms to formulate the first successful definition of the randomness of individual binary sequences. Subsequent definitions, using a variety of conceptual approaches, were introduced by Levin [17], Schnorr [24, 25], Chaitin [6, 7, 8], Solovay [28], and Shen 0 [26].
Reference: [24] <author> C. P. Schnorr. </author> <title> A unified approach to the definition of random sequences. </title> <journal> Mathematical Systems Theory, </journal> <volume> 5 </volume> <pages> 246-258, </pages> <year> 1971. </year>
Reference-contexts: 1 Introduction In the mid-1960's, Martin-Lof [23] used the general theory of algorithms to formulate the first successful definition of the randomness of individual binary sequences. Subsequent definitions, using a variety of conceptual approaches, were introduced by Levin [17], Schnorr <ref> [24, 25] </ref>, Chaitin [6, 7, 8], Solovay [28], and Shen 0 [26]. <p> In the special case where ~ fi = 1 2 ; 1 ~ fi is the uniform probability measure on C: As noted in the introduction, there are several equivalent definitions of algorithmic randomness. The definition in terms of martingales, introduced by Schnorr <ref> [24] </ref>, is most convenient for our purposes here.
Reference: [25] <author> C. P. Schnorr. </author> <title> Process complexity and effective random tests. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 7 </volume> <pages> 376-388, </pages> <year> 1973. </year>
Reference-contexts: 1 Introduction In the mid-1960's, Martin-Lof [23] used the general theory of algorithms to formulate the first successful definition of the randomness of individual binary sequences. Subsequent definitions, using a variety of conceptual approaches, were introduced by Levin [17], Schnorr <ref> [24, 25] </ref>, Chaitin [6, 7, 8], Solovay [28], and Shen 0 [26].
Reference: [26] <author> A. Kh. </author> <title> Shen 0 . The frequency approach to the definition of a random sequence. </title> <journal> Semiotika i Informatika, </journal> <volume> 18 </volume> <pages> 14-42, </pages> <year> 1982. </year> <note> (In Russian.). </note>
Reference-contexts: Subsequent definitions, using a variety of conceptual approaches, were introduced by Levin [17], Schnorr [24, 25], Chaitin [6, 7, 8], Solovay [28], and Shen 0 <ref> [26] </ref>. Each of these definitions was shown to be equivalent to Martin-Lof's, in the sense that a binary sequence R is algorithmically random according to the given definition if and only if R is algorithmically random according to Martin-Lof's definition.
Reference: [27] <author> A. Kh. </author> <title> Shen 0 . On relations between different algorithmic definitions of randomness. </title> <journal> Soviet Mathematics Doklady, </journal> <volume> 38 </volume> <pages> 316-319, </pages> <year> 1989. </year>
Reference-contexts: This means that, for every sequence A of distinct bits of R that is chosen according to a Kolmogorov-Loveland selection rule, the limiting frequency of 1's in A is 1 2 : In the late 1980's, Shen 0 <ref> [27] </ref> proved that the converse does not hold, thereby solving a problem that had been open for some twenty years. (See [15, 29, 18] for more detailed histories of this problem and the role of stochasticity in the foundations of probability theory.) Thus, the random sequences form a proper subset of <p> This note refines the method of Shen 0 <ref> [27] </ref> in order to establish a stronger, more quantitative separation between randomness and Komogorov-Loveland stochasticity. <p> The proof of this result uses a relativization of a method of van Lambalgen [30] and Shen 0 <ref> [27] </ref>, together with a simple encoding of the sequence A into a "nearly uniform" probability measure on the set of all sequences. It follows immediately from the main theorem that there are sequences S that are Kolmogorov-Loveland stochastic, but also strongly deep in the sense of Bennett [1]. <p> The standard intuition is described elegantly in <ref> [27] </ref>: Let us imagine that the members of a sequence are written on cards which lie on an (infinitely long) table (we do not see what is on a card unless we turn it). <p> Our proof makes essential use of the following lemma, which is a straightforward relativization of Lemma 2 of <ref> [27] </ref>. Lemma 3.1 (Shen 0 [27]). <p> Our proof makes essential use of the following lemma, which is a straightforward relativization of Lemma 2 of <ref> [27] </ref>. Lemma 3.1 (Shen 0 [27]). <p> In the case of infinite binary sequences, Bennett has defined both strong depth and weak depth, and shown that, in a technical, computational sense, strongly deep sequences are "very far from random". (See [1, 18, 12] for definitions and discussion of computational depth.) The proof of Shen 0 <ref> [27] </ref> exhibits a sequence that is Kolmgorov-Loveland stochastic and not random. That sequence, however, is random relative to a computable probability measure and so is not even weakly deep in the sense of Bennett. Nevertheless, we now note that a Kolmogorov-Loveland stochastic sequence may be strongly deep. Corollary 3.6.
Reference: [28] <author> R. M. Solovay, </author> <year> 1975. </year> <note> reported in [8]. 13 </note>
Reference-contexts: 1 Introduction In the mid-1960's, Martin-Lof [23] used the general theory of algorithms to formulate the first successful definition of the randomness of individual binary sequences. Subsequent definitions, using a variety of conceptual approaches, were introduced by Levin [17], Schnorr [24, 25], Chaitin [6, 7, 8], Solovay <ref> [28] </ref>, and Shen 0 [26]. Each of these definitions was shown to be equivalent to Martin-Lof's, in the sense that a binary sequence R is algorithmically random according to the given definition if and only if R is algorithmically random according to Martin-Lof's definition.
Reference: [29] <author> V. A. Uspenskii, A. L. Semenov, and A. Kh. </author> <title> Shen 0 . Can an individual sequence of zeros and ones be random? Russian Mathematical Surveys, </title> <booktitle> 45 </booktitle> <pages> 121-189, </pages> <year> 1990. </year>
Reference-contexts: bits of R that is chosen according to a Kolmogorov-Loveland selection rule, the limiting frequency of 1's in A is 1 2 : In the late 1980's, Shen 0 [27] proved that the converse does not hold, thereby solving a problem that had been open for some twenty years. (See <ref> [15, 29, 18] </ref> for more detailed histories of this problem and the role of stochasticity in the foundations of probability theory.) Thus, the random sequences form a proper subset of the set of all Kolmogorov-Loveland stochastic sequences. <p> 0 (w) 1: Then we have 2 c d n 0 (w) 2 c &gt; d (w) 2 c n=m (2c) so n 0 &lt; m (2c): Thus J is finite. 2 The notion of Kolmogorov-Loveland stochasticity was defined in [13, 14, 19, 20]; detailed discussions may be found in <ref> [29, 15, 18] </ref>.
Reference: [30] <author> M. van Lambalgen. </author> <title> Random Sequences. </title> <type> PhD thesis, </type> <institution> Department of Mathematics, University of Amsterdam, </institution> <year> 1987. </year>
Reference-contexts: The proof of this result uses a relativization of a method of van Lambalgen <ref> [30] </ref> and Shen 0 [27], together with a simple encoding of the sequence A into a "nearly uniform" probability measure on the set of all sequences. <p> Given a bias sequence ~ fi; a ~ fi-martingale is a function d : f0; 1g fl ! [0; 1) such that for all w 2 f0; 1g fl ; (The reader is referred to <ref> [30, 5] </ref> for discussion of this definition and its motivation.) The success set of a ~ fi-martingale d is S 1 [d] = A 2 C fi o 4 The unitary success set of a ~ fi-martingale d is S 1 [d] = d (w)1 A sequence R 2 C is
Reference: [31] <author> R. </author> <title> von Mises. </title> <journal> Grundlagen der Wahrscheinlichkeitsrechnung. Mathema-tische Zeitschrift, </journal> <volume> 5 </volume> <pages> 52-99, </pages> <year> 1919. </year>
Reference-contexts: These algorithmic rules (which are described in section 2) are more general than earlier selection rules proposed by von Mises <ref> [31] </ref>, Wald [32], and Church [9] in two respects.
Reference: [32] <author> A. Wald. </author> <title> Die Widerspruchsfreiheit des Kollectivbegriffs in der Wahrscheinlichkeitsrechnung. </title> <journal> Ergebnisse eines Mathematichen Kollo-quiums, </journal> <volume> 8 </volume> <pages> 38-72, </pages> <year> 1938. </year> <month> 14 </month>
Reference-contexts: These algorithmic rules (which are described in section 2) are more general than earlier selection rules proposed by von Mises [31], Wald <ref> [32] </ref>, and Church [9] in two respects.
References-found: 32

