URL: http://www.cs.colostate.edu/~ftppub/TechReports/1997/tr97-106.ps.Z
Refering-URL: http://www.cs.colostate.edu/~ftppub/
Root-URL: 
Email: stevensm@cs.colostate.edu  ross@cs.colostate.edu  goss@hpl.hp.com  
Phone: Phone: (970) 491-5792 Fax: (970) 491-2466  
Title: Visualizing Multisensor Model-Based Object Recognition  
Author: Mark R. Stevens J. Ross Beveridge Michael E. Goss 
Note: This work has been submitted for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible This work was sponsored by the Defense Advanced Research Projects Agency (DARPA) Image Understanding Program under grants DAAH04-93-G-422 and DAAH04-95-1-0447, monitored by the U. S. Army Research Office, and the National Science Foundation under grants CDA-9422007 and IRI-9503366  
Web: WWW: http://www.cs.colostate.edu  
Address: Fort Collins, CO 80523-1873  
Date: March 28, 1997  
Affiliation: Computer Science  Colorado State University  Colorado State University  Hewlett-Packard Laboratories  Computer Science Department Colorado State University  
Pubnum: Technical Report  Technical Report CS-97-106  
Abstract-found: 0
Intro-found: 1
Reference: [ 1 ] <author> Alan Watt and Mark Watt. </author> <title> Advanced Animation and Rendering Techniques, chapter Mapping Techniques: Texture and Environment Mapping. </title> <publisher> ACM Press: Addison-Wesley, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: The imagery is mapped onto visible faces using what are essentially standard texture mapping techniques <ref> [ 1 ] </ref> . For each face, three vertices are selected to define a face specific 2-D coordinate reference frame which is used to map between image and face reference frames. <p> Since the correspondence is not exact, the nearest pixel is selected based upon the geometric mapping discussed in the previous section. For the FLIR, it is also helpful to define normalized FLIR F [0;255] compressed to the ranges [0; 255] (as well as F [0;1] in the range <ref> [0; 1] </ref>). The two combinations used by ModelView and shown in Figure 6 (see color plates).
Reference: [ 2 ] <author> Anthony N. A. Schwickerath and J. Ross Beveridge. </author> <title> Coregistration of Range and Optical Images Using Coplanarity and Orientation Constraints. </title> <booktitle> In 1996 Conference on Computer Vision and Patter Recognition, </booktitle> <pages> pages 899 - 906, </pages> <address> San Francisco, CA, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: However, another very important feature of ModelView is the ability to to visualize changes in pixel-to-pixel alignment between sensors. Sensor-to-sensor visualization is important for two rea sons. First, one product of recognition <ref> [ 2; 21 ] </ref> is an adjustment to the sensor-to-sensor transformations and it helpful to see this adjust-ment.
Reference: [ 3 ] <author> Nicholas Ayache and Bernard Faverjon. </author> <title> Efficient registration of stereo images by matching graph descriptions of edge segments. </title> <journal> International Journal of Computer Vision, </journal> <volume> 1(2), </volume> <year> 1987. </year>
Reference-contexts: ModelView system also serves as a prototype for how raw data from an autonomous vehicle might be fused and displayed to a remote operator. Many of the visualization concepts illustrated in ModelView are applicable to a wide variety of application domains. Numerous sensor modalities, including stereo <ref> [ 3; 6; 26 ] </ref> , Scanning and Scannerless LADARs, and IFSAR [ 40 ] produce range imagery. Whenever range data is inte 1 http://www.cs.colostate.edu/~vision grated with stored 3-D object and terrain models, 3-D visual feedback is important. <p> If the two images are analyzed as simple input signals, a network can be used to find a correlation between the two signals. In addition to neural networks, wavelets [ 17 ] , edges extracted from the imagery <ref> [ 3 ] </ref> , fuzzy logic [ 30 ] , and image contours [ 18 ] have been used to determine the pixel-to-pixel correlation mapping between various types of imagery. 4.2 Sensor and Object Visualization FLIR and color images can be effectively viewed using conventional image display techniques.
Reference: [ 4 ] <author> Mark Bellrichard. </author> <title> Alliant techsystems LADAR field calibration. </title> <type> Personal Correspondence, </type> <month> November </month> <year> 1993. </year>
Reference-contexts: Finally, the range sensor parameters were recovered from calibrated imagery. The maximum range measured by the LADAR is 1074 feet, and hence multiplying a raw pixel value by the ratio 1074=4095 yields a range measurement in feet. The standard deviation of the range measurement is approximately 1 foot <ref> [ 4 ] </ref> .
Reference: [ 5 ] <author> J. Ross Beveridge, Durga P. Panda, and Theodore Yachik. </author> <title> November 1993 Fort Carson RSTA Data Collection Final Report. </title> <type> Technical Report CSS-94-118, </type> <institution> Col-orado State University, </institution> <address> Fort Collins, CO, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: While 3-D visualization might be helpful for working with CAD models even when range data is not available, the ability to visualize 3-D relationships in the presence of such data is paramount. Our efforts began in 1993 when we collected roughly 400 color, IR and range images <ref> [ 5 ] </ref> , available through our web site 1 . It was immediately apparent that viewing these images with separate, unrelated 2-D displays was woefully inadequate. A means was needed to visually fuse the imagery while simultaneously bringing to life the 3-D character of the range data.
Reference: [ 6 ] <author> R.C. Bolles, H.H. Baker, and M.J. Hannah. </author> <title> The jisct stereo evaluation. </title> <journal> IUW, </journal> <volume> 93 </volume> <pages> 263-274, </pages> <year> 1995. </year>
Reference-contexts: ModelView system also serves as a prototype for how raw data from an autonomous vehicle might be fused and displayed to a remote operator. Many of the visualization concepts illustrated in ModelView are applicable to a wide variety of application domains. Numerous sensor modalities, including stereo <ref> [ 3; 6; 26 ] </ref> , Scanning and Scannerless LADARs, and IFSAR [ 40 ] produce range imagery. Whenever range data is inte 1 http://www.cs.colostate.edu/~vision grated with stored 3-D object and terrain models, 3-D visual feedback is important.
Reference: [ 7 ] <author> R. O. Eason and R. C. Gonzalez. </author> <title> Least-Squares Fusion of Multisensor Data. </title> <editor> In Mongi A. Abidi and Rafael C. Gonza-lez, editors, </editor> <booktitle> Data Fusion in Robotics and Machine Intelligence, chapter 9. </booktitle> <publisher> Academic Press, </publisher> <year> 1992. </year>
Reference-contexts: Using multiple sensors instead of a single sensor complicates the alignment task. There are good examples of successful mixed-modality fusion <ref> [ 32; 7; 13 ] </ref> , but this research area is still young. Aggarwal notes, and we agree, that to properly perform mixed-modality sensor fusion, coordinate transformations between images need to be adaptively determined [ 19 ] .
Reference: [ 8 ] <author> J. D. Foley and A. Van Dam. </author> <title> Funda--mentals of Interactive Computer Graphics. </title> <booktitle> The Systems Programming Series. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1982. </year>
Reference-contexts: Next, a standard active edge list polygon fill algorithm <ref> [ 8 ] </ref> is used to enumerate the pixels falling within the face polygon.
Reference: [ 9 ] <author> Aaron D. Fuegi. </author> <title> A tool for visualization of multi-sensor data for automatic target recognition. </title> <type> Master's thesis, </type> <institution> Colorado State University, Fort Collins, Colorado, </institution> <month> September </month> <year> 1994. </year>
Reference-contexts: Consequently, the texture mapping accurately reflects the true underlying 3-D geometry of the scene. RangeView also did not utilize sensor geometry when determining the correspondence between range and optical imagery. Instead, separate affine transformations were used to warp the optical pixels to the range data coordinates <ref> [ 9 ] </ref> . The 2-D affine mappings were determined by solving for the affine parameters which minimize the squared Euclidean distance between hand selected control points. The true mapping between sensors is often not affine and this affine mapping was only a coarse approximation of the true mapping.
Reference: [ 10 ] <author> M. E. Goss, J. R. Beveridge, M. Stevens, and A. Fuegi. </author> <title> Three-dimensional visualization environment for multisensor data analysis, interpretation, and model-based object recognition. </title> <booktitle> In IS&T/SPIE Symposium on Electronic Imaging: Science & Technology, </booktitle> <pages> pages 283 - 291, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: It was immediately apparent that viewing these images with separate, unrelated 2-D displays was woefully inadequate. A means was needed to visually fuse the imagery while simultaneously bringing to life the 3-D character of the range data. This need led us to build our first visualization tool: RangeView <ref> [ 11; 10 ] </ref> . RangeView had limited capacity for embedding and manipulating 3-D CAD object models in the range sensor's native 3-D coordinate system. However, the choice of this coordinate system as the master reference frame limited RangeView, and subsequently a newer more flexible system was built. <p> The RangeView system <ref> [ 11; 10 ] </ref> contained several of the sensor-to-sensor relationships discussed in Section 6. The system was centered around the LADAR coordinate system, and allowed a user to interactively view the output of the color and FLIR images textured onto the range data.
Reference: [ 11 ] <author> Michael E. Goss, J. Ross Beveridge, Mark Stevens, and Aaron Fuegi. </author> <title> Visualization and Verification of Automatic Target Recognition Results Using Combined Range and Optical Imagery. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 491 - 494, </pages> <address> Los Altos, CA, </address> <month> November </month> <year> 1994. </year> <title> ARPA, </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: It was immediately apparent that viewing these images with separate, unrelated 2-D displays was woefully inadequate. A means was needed to visually fuse the imagery while simultaneously bringing to life the 3-D character of the range data. This need led us to build our first visualization tool: RangeView <ref> [ 11; 10 ] </ref> . RangeView had limited capacity for embedding and manipulating 3-D CAD object models in the range sensor's native 3-D coordinate system. However, the choice of this coordinate system as the master reference frame limited RangeView, and subsequently a newer more flexible system was built. <p> The RangeView system <ref> [ 11; 10 ] </ref> contained several of the sensor-to-sensor relationships discussed in Section 6. The system was centered around the LADAR coordinate system, and allowed a user to interactively view the output of the color and FLIR images textured onto the range data.
Reference: [ 12 ] <author> W. E. L. </author> <title> Grimson. The Combinatorics of Object Recognition in Cluttered Environments Using Constrained Search. </title> <journal> Artificial Intelligence, </journal> <volume> 44(1):121 - 165, </volume> <month> July </month> <year> 1990. </year>
Reference-contexts: From these simpler mod els, features to be used in the matching process can be predicted. Visible feature prediction is essential due to the combinatorially explosive nature of model based recognition <ref> [ 12 ] </ref> . The naive approach to model matching would be to determine a match error between all possible combinations of model and data features, and then choose the smallest error set as the optimal object match.
Reference: [ 13 ] <author> Alexander Akerman III, Ronald Patton, Walter H. Delashmit, and Robert Hum-mel. </author> <title> Multisensor fusion using FLIR and LADAR identification. </title> <type> Technical Report NRC-TR-94-052, </type> <institution> Nichols Research Corporation, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: Using multiple sensors instead of a single sensor complicates the alignment task. There are good examples of successful mixed-modality fusion <ref> [ 32; 7; 13 ] </ref> , but this research area is still young. Aggarwal notes, and we agree, that to properly perform mixed-modality sensor fusion, coordinate transformations between images need to be adaptively determined [ 19 ] .
Reference: [ 14 ] <author> J.R. Beveridge, M.R. Stevens, Zhongfei Zhang and M.E. Goss. </author> <title> Approximate Image Mappings Between Nearly Boresight Aligned Optical and Range Sensors. </title> <type> Technical Report CS-96-112, </type> <institution> Computer Science, Colorado State University, </institution> <address> Fort Collins, CO, </address> <month> April </month> <year> 1996. </year>
Reference-contexts: The standard deviation of the range measurement is approximately 1 foot [ 4 ] . Additional details on how the sensors were calibrated, the relationships between parameters, and the justification for the use of the perspective projection mapping for the range sensor can be found in <ref> [ 14 ] </ref> . 3.3 CAD Object Models Highly detailed models of the vehicles in our Fort Carson dataset exist in the model format known as BRL/CAD [ 36 ] . <p> The 2-D affine mappings were determined by solving for the affine parameters which minimize the squared Euclidean distance between hand selected control points. The true mapping between sensors is often not affine and this affine mapping was only a coarse approximation of the true mapping. In previous work <ref> [ 14 ] </ref> we explore in considerable detail the conditions under which the mapping is affine and to what extent our mappings are valid. RangeView was further limited by the lack of model-to-sensor relationships. <p> For the LADAR, which has a lower pixel resolution and smaller field of view, the comparable pixel deviation is about 1 pixel. A much more detailed development of near bore-sight aligned sensors and the use of image plane translation to approximate pan and tilt may be found in <ref> [ 14 ] </ref> . 6 Visualizing the Model-to-Sensor Relationships ModelView may be used to visualize different 3-D relationships between an object model and multisensor data. ModelView visualizes the effects rather than the relationships themselves because the system never actually presents icons of the sensors or their relative 3-D scene position.
Reference: [ 15 ] <author> J. J. Koenderink and A. J. van Doorn. </author> <title> The Internal Representation of Shape with Respect to Vision. </title> <journal> Biological Cybernetics, </journal> <volume> 32 </volume> <pages> 211-216, </pages> <year> 1979. </year>
Reference-contexts: Common approaches to model feature generation have centered around an off-line model analysis in which visible features are determined for all viewpoints [ 27 ] . The results are then grouped into regions of constant topology [ 16 ] and stored in an aspect graph representation <ref> [ 15 ] </ref> . The aspect graph is used at run-time to obtain the list of visible features for a given pose [ 31 ] .
Reference: [ 16 ] <author> Matthew R. Korn and Charles R. Dyer. </author> <title> 3D Multiview Object Representations for Model-Based Object Recognition. </title> <journal> Pattern Recognition, </journal> <volume> 20(1) </volume> <pages> 91-103, </pages> <year> 1987. </year>
Reference-contexts: Common approaches to model feature generation have centered around an off-line model analysis in which visible features are determined for all viewpoints [ 27 ] . The results are then grouped into regions of constant topology <ref> [ 16 ] </ref> and stored in an aspect graph representation [ 15 ] . The aspect graph is used at run-time to obtain the list of visible features for a given pose [ 31 ] .
Reference: [ 17 ] <author> H. Li, B.S. Manjunath, and S.K. Mitra. </author> <title> A contour-based approach to multisensor image registration. </title> <journal> Ieee transactions on image processing, </journal> <volume> 4(3):320, </volume> <month> march </month> <year> 1995. </year>
Reference-contexts: Others have used neural networks to learn a mapping which relates two images [ 37; 38 ] . If the two images are analyzed as simple input signals, a network can be used to find a correlation between the two signals. In addition to neural networks, wavelets <ref> [ 17 ] </ref> , edges extracted from the imagery [ 3 ] , fuzzy logic [ 30 ] , and image contours [ 18 ] have been used to determine the pixel-to-pixel correlation mapping between various types of imagery. 4.2 Sensor and Object Visualization FLIR and color images can be effectively
Reference: [ 18 ] <author> H. Li, B.S. Manjunath, and S.K. Mitra. </author> <title> Multisensor image fusion using the wavelet transform. Graphical models and image processing, </title> <address> 57(3):235, </address> <month> may </month> <year> 1995. </year>
Reference-contexts: In addition to neural networks, wavelets [ 17 ] , edges extracted from the imagery [ 3 ] , fuzzy logic [ 30 ] , and image contours <ref> [ 18 ] </ref> have been used to determine the pixel-to-pixel correlation mapping between various types of imagery. 4.2 Sensor and Object Visualization FLIR and color images can be effectively viewed using conventional image display techniques.
Reference: [ 19 ] <author> M. J. Magee, B. A. Boyter, C. H. Chien, and J. K. Aggarwal. </author> <title> Experiments in Intensity Guided Range Sensing Recognition of Three-Dimensional Objects. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 7(6):629 - 637, </volume> <month> November </month> <year> 1985. </year>
Reference-contexts: There are good examples of successful mixed-modality fusion [ 32; 7; 13 ] , but this research area is still young. Aggarwal notes, and we agree, that to properly perform mixed-modality sensor fusion, coordinate transformations between images need to be adaptively determined <ref> [ 19 ] </ref> . Our recent work [ 22 ] emphasizes global alignment as a basis for optimal matching to multisensor data using local search.
Reference: [ 20 ] <author> Martti Mantyla. </author> <title> An Introduction to Solid Modeling. </title> <publisher> Computer Science Press, </publisher> <year> 1990. </year>
Reference-contexts: This network is con-structed such that if any transformation is altered, any others effected are updated appropriately. For instance if the transformation M W;C is modified, then the M W;F and M W;L are adjusted so the constraints between reference frames are not violated. Within the boundary representation (BREP) <ref> [ 20 ] </ref> used for the CAD object models are structures used to encode the object in each of the major coordinate reference frames.
Reference: [ 21 ] <author> Mark R. Stevens and J. Ross Beveridge. </author> <title> Interleaving 3D Model Feature Prediction and Matching to Support Multi-Sensor Object Recognition. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 699-706, </pages> <address> Los Altos, CA, </address> <month> February </month> <year> 1996. </year> <title> ARPA, </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: However, another very important feature of ModelView is the ability to to visualize changes in pixel-to-pixel alignment between sensors. Sensor-to-sensor visualization is important for two rea sons. First, one product of recognition <ref> [ 2; 21 ] </ref> is an adjustment to the sensor-to-sensor transformations and it helpful to see this adjust-ment.
Reference: [ 22 ] <author> Mark R. Stevens and J. Ross Beveridge. </author> <title> Precise Matching of 3-D Target Models to Multisensor Data. </title> <journal> IEEE Transactions on Image Processing, </journal> <volume> 6(1) </volume> <pages> 126-142, </pages> <month> January </month> <year> 1997. </year>
Reference-contexts: There are good examples of successful mixed-modality fusion [ 32; 7; 13 ] , but this research area is still young. Aggarwal notes, and we agree, that to properly perform mixed-modality sensor fusion, coordinate transformations between images need to be adaptively determined [ 19 ] . Our recent work <ref> [ 22 ] </ref> emphasizes global alignment as a basis for optimal matching to multisensor data using local search.
Reference: [ 23 ] <author> Robin R. Murphy. </author> <title> Robust sensor fusion for teleoperations. </title> <booktitle> In IEEE International Conference on Robotics and Automation, </booktitle> <year> 1993. </year>
Reference-contexts: The discussion is followed by our previous work on the RangeView system. 4.1 Sensor Fusion Sensor fusion is the process of finding commonalities in heterogeneous sensors information. The goal is to provide a semi-autonomous agent with an internal representation of the surrounding world <ref> [ 23 ] </ref> . While the goals are very different from those of visualization, they need to solve many of the same problems associated with combining information from different sensor modalities. Tong has used LADAR masks to segment FLIR imagery, resulting in simple sensor fusion [ 34; 35 ] .
Reference: [ 24 ] <author> Michael John Muuss. </author> <title> Towards real-time ray-tracing of combinatorial solid geometric models. </title> <editor> In Keith Applin, editor, </editor> <booktitle> Symposium of BRL/CAD '95. </booktitle> <institution> Army Research Laboratories, </institution> <month> June </month> <year> 1995. </year> <note> http://ftp.arl.mil:80/ mike/. </note>
Reference-contexts: Model-based visualization tools generally allow wire-frame representations to be interactively manipulated until the desired point of view is obtained. Ray tracing can then be used to render highly detailed images of this scene. In addition, terrain models and sensor imagery can be used to generate highly realistic scenes <ref> [ 24 ] </ref> . 4.3 Model-Based Recognition A long tradition of work on object recognition has emphasized finding matches between object and image features for which there is a single globally consistent alignment. Using multiple sensors instead of a single sensor complicates the alignment task.
Reference: [ 25 ] <author> R. Nevatia, K. Price, and G. Medioni. </author> <title> USC image understanding research: 1993-1994. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 69-80. ARPA, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Work has also been done in the display of range images as 3-D surfaces with illumination and shading, but this requires the prior extraction of surfaces from the raw range data <ref> [ 25; 29 ] </ref> . Previous systems [ 28; 39 ] which locate a 3-D target relative to a LADAR image typically render an image of the model in the image plane of the sensor.
Reference: [ 26 ] <author> M. Okutomi and T. Kanade. </author> <title> A multiple-baseline stereo. </title> <journal> PAMI, </journal> <volume> 15 </volume> <pages> 353-363, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: ModelView system also serves as a prototype for how raw data from an autonomous vehicle might be fused and displayed to a remote operator. Many of the visualization concepts illustrated in ModelView are applicable to a wide variety of application domains. Numerous sensor modalities, including stereo <ref> [ 3; 6; 26 ] </ref> , Scanning and Scannerless LADARs, and IFSAR [ 40 ] produce range imagery. Whenever range data is inte 1 http://www.cs.colostate.edu/~vision grated with stored 3-D object and terrain models, 3-D visual feedback is important.
Reference: [ 27 ] <author> Harry Platinga and Charles Dyer. </author> <title> Visibility, Occlusion, and the Aspect Graph. </title> <type> Technical Report 736, </type> <institution> University of Wis-consin - Madison, </institution> <month> December </month> <year> 1987. </year>
Reference-contexts: Being able to pick a small set of model features for match ing, let us say 5, can greatly simplify the the correspondence space (2 50 ). Common approaches to model feature generation have centered around an off-line model analysis in which visible features are determined for all viewpoints <ref> [ 27 ] </ref> . The results are then grouped into regions of constant topology [ 16 ] and stored in an aspect graph representation [ 15 ] . The aspect graph is used at run-time to obtain the list of visible features for a given pose [ 31 ] .
Reference: [ 28 ] <author> Steven K. Rodgers, Carl W. Tong, Matthew Kabrisky, and James P. Mills. </author> <title> Multisensor fusion of ladar and passive in-fared imagery for target segmentation. </title> <journal> Optical Engineering, </journal> <volume> 28(8) </volume> <pages> 881-886, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: Work has also been done in the display of range images as 3-D surfaces with illumination and shading, but this requires the prior extraction of surfaces from the raw range data [ 25; 29 ] . Previous systems <ref> [ 28; 39 ] </ref> which locate a 3-D target relative to a LADAR image typically render an image of the model in the image plane of the sensor. This 2-D image does not allow a complete understanding of how well the model has been located.
Reference: [ 29 ] <author> Hillel Rom and Gerard Medioni. </author> <title> Part decomposition and description of 3D shapes. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 1505-1522. ARPA, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Work has also been done in the display of range images as 3-D surfaces with illumination and shading, but this requires the prior extraction of surfaces from the raw range data <ref> [ 25; 29 ] </ref> . Previous systems [ 28; 39 ] which locate a 3-D target relative to a LADAR image typically render an image of the model in the image plane of the sensor.
Reference: [ 30 ] <author> F. Russon and G. Ramponi. </author> <title> Fuzzy methods for multisensor data fusion. </title> <journal> Ieee transactions on instrumentation and measurement, </journal> <volume> 43(2):288, </volume> <month> april </month> <year> 1994. </year>
Reference-contexts: If the two images are analyzed as simple input signals, a network can be used to find a correlation between the two signals. In addition to neural networks, wavelets [ 17 ] , edges extracted from the imagery [ 3 ] , fuzzy logic <ref> [ 30 ] </ref> , and image contours [ 18 ] have been used to determine the pixel-to-pixel correlation mapping between various types of imagery. 4.2 Sensor and Object Visualization FLIR and color images can be effectively viewed using conventional image display techniques.
Reference: [ 31 ] <author> W. Brent Seales and Charles R. Dyer. </author> <title> Modeling the Rim Appearance. </title> <booktitle> In Proceedings of the 3rd International Conference on Computer Vision, </booktitle> <pages> pages 698-701, </pages> <year> 1992. </year>
Reference-contexts: The results are then grouped into regions of constant topology [ 16 ] and stored in an aspect graph representation [ 15 ] . The aspect graph is used at run-time to obtain the list of visible features for a given pose <ref> [ 31 ] </ref> . For these systems, visualization is mainly centered around showing which par ticular model features are chosen, rather than relating them to any particular sensor image. Our current feature prediction algorithms use graphics hardware to achieve real-time generation of relevant model features [ 33 ] .
Reference: [ 32 ] <author> A. Stentz and Y. </author> <title> Goto. </title> <booktitle> The CMU Navigational Architecture. In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 440-446, </pages> <address> Los Angeles, CA, </address> <month> February </month> <year> 1987. </year> <title> ARPA, </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Using multiple sensors instead of a single sensor complicates the alignment task. There are good examples of successful mixed-modality fusion <ref> [ 32; 7; 13 ] </ref> , but this research area is still young. Aggarwal notes, and we agree, that to properly perform mixed-modality sensor fusion, coordinate transformations between images need to be adaptively determined [ 19 ] .
Reference: [ 33 ] <author> Mark R. Stevens, J. Ross Beveridge, and Michael E. Goss. </author> <title> Reduction of BRL/CAD Models and Their Use in Automatic Target Recognition Algorithms. </title> <booktitle> In Proceedings: BRL-CAD Symposium. </booktitle> <institution> Army Research Labs, </institution> <month> June </month> <year> 1995. </year>
Reference-contexts: Algorithms to reduce the model complexity to a level more closely related to the sensor granularity have already been developed <ref> [ 33 ] </ref> . From these simpler mod els, features to be used in the matching process can be predicted. Visible feature prediction is essential due to the combinatorially explosive nature of model based recognition [ 12 ] . <p> For these systems, visualization is mainly centered around showing which par ticular model features are chosen, rather than relating them to any particular sensor image. Our current feature prediction algorithms use graphics hardware to achieve real-time generation of relevant model features <ref> [ 33 ] </ref> . The feature prediction is fully integrated into the Mod-elView system and is central to the visualization paradigm. When visible model features are determined, they are rendered into each sensor image.
Reference: [ 34 ] <author> C.W. Tong. </author> <title> Target segmentation and image enhancement through multisensor data fusion. </title> <type> Master's thesis, </type> <institution> Air Force Institute of Technology, </institution> <year> 1986. </year>
Reference-contexts: While the goals are very different from those of visualization, they need to solve many of the same problems associated with combining information from different sensor modalities. Tong has used LADAR masks to segment FLIR imagery, resulting in simple sensor fusion <ref> [ 34; 35 ] </ref> . Unfortunately, the visualizations are rendered only as single 2-D images. Others have used neural networks to learn a mapping which relates two images [ 37; 38 ] .
Reference: [ 35 ] <author> C.W. Tong, S.K. Rodgers, J.P. Mills, </author> <title> and M.K. Kabrinsky. Multisensor data fusion of laser radar and forward looking infared for target segmentation and enhancement. </title> <editor> In R.G. Buser and F.B. Warren, editors, </editor> <title> Infared Sensors and Sensor Fusion. </title> <booktitle> SPIE, </booktitle> <year> 1987. </year>
Reference-contexts: While the goals are very different from those of visualization, they need to solve many of the same problems associated with combining information from different sensor modalities. Tong has used LADAR masks to segment FLIR imagery, resulting in simple sensor fusion <ref> [ 34; 35 ] </ref> . Unfortunately, the visualizations are rendered only as single 2-D images. Others have used neural networks to learn a mapping which relates two images [ 37; 38 ] .
Reference: [ 36 ] <author> U. S. </author> <note> Army Ballistic Research Laboratory. BRL-CAD User's Manual, release 4.0 edition, </note> <month> December </month> <year> 1991. </year>
Reference-contexts: the relationships between parameters, and the justification for the use of the perspective projection mapping for the range sensor can be found in [ 14 ] . 3.3 CAD Object Models Highly detailed models of the vehicles in our Fort Carson dataset exist in the model format known as BRL/CAD <ref> [ 36 ] </ref> . Algorithms to reduce the model complexity to a level more closely related to the sensor granularity have already been developed [ 33 ] . From these simpler mod els, features to be used in the matching process can be predicted.
Reference: [ 37 ] <author> J. W. M. van Dam, B. J. A. Kruse, and F. C. A. Groen. </author> <title> Artificial Neural Networks, chapter Optimizing local Hebbian learning: </title> <booktitle> use the ffi-rule, </booktitle> <pages> pages 631-634. </pages> <publisher> Springer Verlag, </publisher> <year> 1994. </year>
Reference-contexts: Tong has used LADAR masks to segment FLIR imagery, resulting in simple sensor fusion [ 34; 35 ] . Unfortunately, the visualizations are rendered only as single 2-D images. Others have used neural networks to learn a mapping which relates two images <ref> [ 37; 38 ] </ref> . If the two images are analyzed as simple input signals, a network can be used to find a correlation between the two signals.
Reference: [ 38 ] <author> J. W. M. van Dam, B. J. A. Kruse, and F. C. A. Groen. </author> <title> Transforming the ego-centered internal representation of an autonomous robot with the cascaded neural network. </title> <journal> IEEE, </journal> <pages> pages 667-674, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Tong has used LADAR masks to segment FLIR imagery, resulting in simple sensor fusion [ 34; 35 ] . Unfortunately, the visualizations are rendered only as single 2-D images. Others have used neural networks to learn a mapping which relates two images <ref> [ 37; 38 ] </ref> . If the two images are analyzed as simple input signals, a network can be used to find a correlation between the two signals.
Reference: [ 39 ] <author> Jacques G. Verly, Dan E. Dudgeon, and Richard T. Lacoss. </author> <title> Model-Based Automatic Target Recognition System for the UGV/RSTA LADAR. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 559-583. ARPA, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: More interesting is the case of imaging range sensor data which are conventionally displayed as either 2-D grey scale images, or as a 2-D overhead "scatter plots" <ref> [ 39 ] </ref> . For a grey scale representation, the grey level of each pixel corresponds to the distance of the sample from the sensor. <p> Work has also been done in the display of range images as 3-D surfaces with illumination and shading, but this requires the prior extraction of surfaces from the raw range data [ 25; 29 ] . Previous systems <ref> [ 28; 39 ] </ref> which locate a 3-D target relative to a LADAR image typically render an image of the model in the image plane of the sensor. This 2-D image does not allow a complete understanding of how well the model has been located.

References-found: 39

