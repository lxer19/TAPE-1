URL: ftp://www.cs.rutgers.edu/pub/technical-reports/dcs-tr-334.ps.Z
Refering-URL: http://www.cs.rutgers.edu/pub/technical-reports/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: On the Complexity of Real and Integer Semidefinite Programming  
Author: by Lorant Porkolab 
Degree: Dissertation Director: Professor Leonid  
Affiliation: Khachiyan  
Note: ABSTRACT OF THE DISSERTATION  
Abstract: We consider the general feasibility problem for semidefinite programming: Determine whether a given system of linear inequalities has a solution in the cone of symmetric positive semidefinite matrices. We give upper bounds on the size of real feasible solutions and obtain a strongly polynomial-time algorithm for testing the feasibility of semidefinite programs in fixed dimension whose required number of arithmetic operations grows linearly in the number of constraints. We also consider semidefinite systems in integral matrices and extend Lenstra's theorem on the polynomial-time solvability of linear integer programming in fixed dimension to integer semidefinite programming. In fact, we address the more general problem of computing an integral point in an arbitrary convex semi-algebraic set, and show that in fixed dimension this problem can be solved in polynomial time. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> F. Alizadeh. </author> <title> Interior point methods in semidefinite programming with applications to combinatorial optimization. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 5 </volume> <pages> 13-51, </pages> <year> 1995. </year>
Reference-contexts: It is well known that approximately solving semidefinite programs with explicitly given bounds on the size of an optimal solution can be accomplished in polynomial time by interior-point methods <ref> [1] </ref>, [26]. However, the complexity of the general SDP problem, and in particular the complexity of testing the feasibility of a given semidefinite program, remains an open fundamental problem of mathematical programming. <p> Then (2:6) becomes a consequence of (2:5). 2.2.3 Decision Methods for the First-order Theory of the Reals A formula F (y) in the first-order theory of the reals is an expression of the form (Q 1 x <ref> [1] </ref> 2 IR n 1 ) : : : (Q ! x [!] 2 IR n ! ) P (y; x [1] ; : : :; x [!] ); (F ) where: * y = (y 1 ; : : : ; y k ) 2 IR k is the vector <p> Decision Methods for the First-order Theory of the Reals A formula F (y) in the first-order theory of the reals is an expression of the form (Q 1 x <ref> [1] </ref> 2 IR n 1 ) : : : (Q ! x [!] 2 IR n ! ) P (y; x [1] ; : : :; x [!] ); (F ) where: * y = (y 1 ; : : : ; y k ) 2 IR k is the vector of free variables; * each Q i ; i = 1; : : :; !; is one of the quantifiers 9 <p> ); (F ) where: * y = (y 1 ; : : : ; y k ) 2 IR k is the vector of free variables; * each Q i ; i = 1; : : :; !; is one of the quantifiers 9 or 8; * P (y; x <ref> [1] </ref> ; : : : ; x [!] ) is a Boolean function of m polynomial predicates of the form g i (y; x [1] ; : : :; x [!] ) 4 i 0; i = 1; : : : ; m; in which 4 i 2 f&lt;; =g, and <p> free variables; * each Q i ; i = 1; : : :; !; is one of the quantifiers 9 or 8; * P (y; x <ref> [1] </ref> ; : : : ; x [!] ) is a Boolean function of m polynomial predicates of the form g i (y; x [1] ; : : :; x [!] ) 4 i 0; i = 1; : : : ; m; in which 4 i 2 f&lt;; =g, and the g i 's are polynomials of degree at most d 2 with integer coefficients of binary size at most l. 9 We call <p> O (mn 4 ) + n O (minfm;n 2 g) operations with ln O (minfm;n 2 g) -bit numbers. 23 Chapter 3 Computing Integral Points in Convex Semi-algebraic Sets 3.1 Results Recall that a first-order formula F (y) over the reals is an expression of the form (Q 1 x <ref> [1] </ref> 2 IR n 1 ) : : : (Q ! x [!] 2 IR n ! ) P (y; x [1] ; : : :; x [!] ); (F ) where: * y = (y 1 ; : : : ; y k ) 2 IR k is the vector <p> 3 Computing Integral Points in Convex Semi-algebraic Sets 3.1 Results Recall that a first-order formula F (y) over the reals is an expression of the form (Q 1 x <ref> [1] </ref> 2 IR n 1 ) : : : (Q ! x [!] 2 IR n ! ) P (y; x [1] ; : : :; x [!] ); (F ) where: * y = (y 1 ; : : : ; y k ) 2 IR k is the vector of free variables; * each Q i ; i = 1; : : :; !; is one of the quantifiers 9 <p> ); (F ) where: * y = (y 1 ; : : : ; y k ) 2 IR k is the vector of free variables; * each Q i ; i = 1; : : :; !; is one of the quantifiers 9 or 8; * P (y; x <ref> [1] </ref> ; : : : ; x [!] ) is a Boolean function of m atomic predicates g i (y; x [1] ; : : : ; x [!] ) 4 i 0; i = 1; : : :; m; in which 4 i 2 f&lt;; =g, and the g i <p> the vector of free variables; * each Q i ; i = 1; : : :; !; is one of the quantifiers 9 or 8; * P (y; x <ref> [1] </ref> ; : : : ; x [!] ) is a Boolean function of m atomic predicates g i (y; x [1] ; : : : ; x [!] ) 4 i 0; i = 1; : : :; m; in which 4 i 2 f&lt;; =g, and the g i 's are polynomials of degree at most d 2 with integer coefficients of binary size at most l. <p> (y 1 ; y 2 ) j 1 y 1 a; 1 y 2 b; y 1 y 2 b g is at least as hard as factoring (because N (a; b) N (a; b + 1) + a = the number of integer divisors of b in the interval <ref> [1; a] </ref>). 25 This chapter is organized as follows. The next section reviews Kronecker's theorem on simultaneous Diophantine approximations. <p> i 2 IR kp satisfy the assumption of Corollary 3.2.3: f u = (u 1 ; : : : ; u k ) T 2 ZZ kp j fi 1 u = : : : = fi s u = 0g = f0g: (3.21) Consider the partition y = (y <ref> [1] </ref> ; y [2] ), where y [1] = (y 1 ; : : : ; y p ) and y [2] = (y p+1 ; : : : ; y k ). Let : and let Y [1] be the solution set of [1] (y [1] ). Since Y [1] <p> of Corollary 3.2.3: f u = (u 1 ; : : : ; u k ) T 2 ZZ kp j fi 1 u = : : : = fi s u = 0g = f0g: (3.21) Consider the partition y = (y <ref> [1] </ref> ; y [2] ), where y [1] = (y 1 ; : : : ; y p ) and y [2] = (y p+1 ; : : : ; y k ). Let : and let Y [1] be the solution set of [1] (y [1] ). Since Y [1] is a projection of Y , the <p> u = 0g = f0g: (3.21) Consider the partition y = (y <ref> [1] </ref> ; y [2] ), where y [1] = (y 1 ; : : : ; y p ) and y [2] = (y p+1 ; : : : ; y k ). Let : and let Y [1] be the solution set of [1] (y [1] ). Since Y [1] is a projection of Y , the set Y [1] IR p is convex and full-dimensional. Lemma 3.3.3 A point y [1] 2 ZZ p " int Y [1] if and only if there is a point y <p> Consider the partition y = (y <ref> [1] </ref> ; y [2] ), where y [1] = (y 1 ; : : : ; y p ) and y [2] = (y p+1 ; : : : ; y k ). Let : and let Y [1] be the solution set of [1] (y [1] ). Since Y [1] is a projection of Y , the set Y [1] IR p is convex and full-dimensional. Lemma 3.3.3 A point y [1] 2 ZZ p " int Y [1] if and only if there is a point y [2] 2 ZZ kp such that <p> partition y = (y <ref> [1] </ref> ; y [2] ), where y [1] = (y 1 ; : : : ; y p ) and y [2] = (y p+1 ; : : : ; y k ). Let : and let Y [1] be the solution set of [1] (y [1] ). Since Y [1] is a projection of Y , the set Y [1] IR p is convex and full-dimensional. Lemma 3.3.3 A point y [1] 2 ZZ p " int Y [1] if and only if there is a point y [2] 2 ZZ kp such that (y [1] <p> <ref> [1] </ref> ; y [2] ), where y [1] = (y 1 ; : : : ; y p ) and y [2] = (y p+1 ; : : : ; y k ). Let : and let Y [1] be the solution set of [1] (y [1] ). Since Y [1] is a projection of Y , the set Y [1] IR p is convex and full-dimensional. Lemma 3.3.3 A point y [1] 2 ZZ p " int Y [1] if and only if there is a point y [2] 2 ZZ kp such that (y [1] ; y [2] ) <p> Let : and let Y <ref> [1] </ref> be the solution set of [1] (y [1] ). Since Y [1] is a projection of Y , the set Y [1] IR p is convex and full-dimensional. Lemma 3.3.3 A point y [1] 2 ZZ p " int Y [1] if and only if there is a point y [2] 2 ZZ kp such that (y [1] ; y [2] ) 2 ZZ k " int Y . <p> Let : and let Y <ref> [1] </ref> be the solution set of [1] (y [1] ). Since Y [1] is a projection of Y , the set Y [1] IR p is convex and full-dimensional. Lemma 3.3.3 A point y [1] 2 ZZ p " int Y [1] if and only if there is a point y [2] 2 ZZ kp such that (y [1] ; y [2] ) 2 ZZ k " int Y . Proof of Lemma 3.3.3. The fact that (y [1] ; y [2] ) 2 ZZ <p> Let : and let Y <ref> [1] </ref> be the solution set of [1] (y [1] ). Since Y [1] is a projection of Y , the set Y [1] IR p is convex and full-dimensional. Lemma 3.3.3 A point y [1] 2 ZZ p " int Y [1] if and only if there is a point y [2] 2 ZZ kp such that (y [1] ; y [2] ) 2 ZZ k " int Y . Proof of Lemma 3.3.3. The fact that (y [1] ; y [2] ) 2 ZZ k "int Y implies y [1] 2 <p> <ref> [1] </ref> ). Since Y [1] is a projection of Y , the set Y [1] IR p is convex and full-dimensional. Lemma 3.3.3 A point y [1] 2 ZZ p " int Y [1] if and only if there is a point y [2] 2 ZZ kp such that (y [1] ; y [2] ) 2 ZZ k " int Y . Proof of Lemma 3.3.3. The fact that (y [1] ; y [2] ) 2 ZZ k "int Y implies y [1] 2 ZZ p "int Y [1] follows directly from the definition of Y [1] . <p> Lemma 3.3.3 A point y <ref> [1] </ref> 2 ZZ p " int Y [1] if and only if there is a point y [2] 2 ZZ kp such that (y [1] ; y [2] ) 2 ZZ k " int Y . Proof of Lemma 3.3.3. The fact that (y [1] ; y [2] ) 2 ZZ k "int Y implies y [1] 2 ZZ p "int Y [1] follows directly from the definition of Y [1] . Suppose that y [1] 2 ZZ p " int Y [1] . Since y [1] 33 is an interior point of Y [1] <p> Y <ref> [1] </ref> if and only if there is a point y [2] 2 ZZ kp such that (y [1] ; y [2] ) 2 ZZ k " int Y . Proof of Lemma 3.3.3. The fact that (y [1] ; y [2] ) 2 ZZ k "int Y implies y [1] 2 ZZ p "int Y [1] follows directly from the definition of Y [1] . Suppose that y [1] 2 ZZ p " int Y [1] . Since y [1] 33 is an interior point of Y [1] , and Y [1] is a projection of the convex full-dimensional set <p> there is a point y [2] 2 ZZ kp such that (y <ref> [1] </ref> ; y [2] ) 2 ZZ k " int Y . Proof of Lemma 3.3.3. The fact that (y [1] ; y [2] ) 2 ZZ k "int Y implies y [1] 2 ZZ p "int Y [1] follows directly from the definition of Y [1] . Suppose that y [1] 2 ZZ p " int Y [1] . Since y [1] 33 is an interior point of Y [1] , and Y [1] is a projection of the convex full-dimensional set Y , there exists a real <p> kp such that (y <ref> [1] </ref> ; y [2] ) 2 ZZ k " int Y . Proof of Lemma 3.3.3. The fact that (y [1] ; y [2] ) 2 ZZ k "int Y implies y [1] 2 ZZ p "int Y [1] follows directly from the definition of Y [1] . Suppose that y [1] 2 ZZ p " int Y [1] . Since y [1] 33 is an interior point of Y [1] , and Y [1] is a projection of the convex full-dimensional set Y , there exists a real vector ~ 2 IR kp such that (y <p> Proof of Lemma 3.3.3. The fact that (y <ref> [1] </ref> ; y [2] ) 2 ZZ k "int Y implies y [1] 2 ZZ p "int Y [1] follows directly from the definition of Y [1] . Suppose that y [1] 2 ZZ p " int Y [1] . Since y [1] 33 is an interior point of Y [1] , and Y [1] is a projection of the convex full-dimensional set Y , there exists a real vector ~ 2 IR kp such that (y [1] ; ~) 2 int <p> Proof of Lemma 3.3.3. The fact that (y <ref> [1] </ref> ; y [2] ) 2 ZZ k "int Y implies y [1] 2 ZZ p "int Y [1] follows directly from the definition of Y [1] . Suppose that y [1] 2 ZZ p " int Y [1] . Since y [1] 33 is an interior point of Y [1] , and Y [1] is a projection of the convex full-dimensional set Y , there exists a real vector ~ 2 IR kp such that (y [1] ; ~) 2 int Y . <p> The fact that (y <ref> [1] </ref> ; y [2] ) 2 ZZ k "int Y implies y [1] 2 ZZ p "int Y [1] follows directly from the definition of Y [1] . Suppose that y [1] 2 ZZ p " int Y [1] . Since y [1] 33 is an interior point of Y [1] , and Y [1] is a projection of the convex full-dimensional set Y , there exists a real vector ~ 2 IR kp such that (y [1] ; ~) 2 int Y . <p> <ref> [1] </ref> ; y [2] ) 2 ZZ k "int Y implies y [1] 2 ZZ p "int Y [1] follows directly from the definition of Y [1] . Suppose that y [1] 2 ZZ p " int Y [1] . Since y [1] 33 is an interior point of Y [1] , and Y [1] is a projection of the convex full-dimensional set Y , there exists a real vector ~ 2 IR kp such that (y [1] ; ~) 2 int Y . Hence there is a positive * such that the open box B = f (y [1] ; <p> ) 2 ZZ k "int Y implies y <ref> [1] </ref> 2 ZZ p "int Y [1] follows directly from the definition of Y [1] . Suppose that y [1] 2 ZZ p " int Y [1] . Since y [1] 33 is an interior point of Y [1] , and Y [1] is a projection of the convex full-dimensional set Y , there exists a real vector ~ 2 IR kp such that (y [1] ; ~) 2 int Y . Hence there is a positive * such that the open box B = f (y [1] ; y [2] ) : <p> Suppose that y <ref> [1] </ref> 2 ZZ p " int Y [1] . Since y [1] 33 is an interior point of Y [1] , and Y [1] is a projection of the convex full-dimensional set Y , there exists a real vector ~ 2 IR kp such that (y [1] ; ~) 2 int Y . Hence there is a positive * such that the open box B = f (y [1] ; y [2] ) : j y [1] y [1] j &lt; *; j y [2] ~ j &lt; * g belongs to Y . <p> Y <ref> [1] </ref> , and Y [1] is a projection of the convex full-dimensional set Y , there exists a real vector ~ 2 IR kp such that (y [1] ; ~) 2 int Y . Hence there is a positive * such that the open box B = f (y [1] ; y [2] ) : j y [1] y [1] j &lt; *; j y [2] ~ j &lt; * g belongs to Y . <p> projection of the convex full-dimensional set Y , there exists a real vector ~ 2 IR kp such that (y <ref> [1] </ref> ; ~) 2 int Y . Hence there is a positive * such that the open box B = f (y [1] ; y [2] ) : j y [1] y [1] j &lt; *; j y [2] ~ j &lt; * g belongs to Y . <p> the convex full-dimensional set Y , there exists a real vector ~ 2 IR kp such that (y <ref> [1] </ref> ; ~) 2 int Y . Hence there is a positive * such that the open box B = f (y [1] ; y [2] ) : j y [1] y [1] j &lt; *; j y [2] ~ j &lt; * g belongs to Y . <p> This implies that <ref> [1] </ref> (y [1] ) has an interior integral solution y [1] whose binary length can be bounded by applying the induction hypothesis (3.6) in p dimensions: log jy [1] j = ld cp 3 (n+k)+O (sk (n+log s)) ; where the multiplicative constant hidden in the term O (sk (n + <p> This implies that <ref> [1] </ref> (y [1] ) has an interior integral solution y [1] whose binary length can be bounded by applying the induction hypothesis (3.6) in p dimensions: log jy [1] j = ld cp 3 (n+k)+O (sk (n+log s)) ; where the multiplicative constant hidden in the term O (sk (n + log s)) <p> This implies that <ref> [1] </ref> (y [1] ) has an interior integral solution y [1] whose binary length can be bounded by applying the induction hypothesis (3.6) in p dimensions: log jy [1] j = ld cp 3 (n+k)+O (sk (n+log s)) ; where the multiplicative constant hidden in the term O (sk (n + log s)) does not depend on c. Substitute y [1] <p> This implies that <ref> [1] </ref> (y [1] ) has an interior integral solution y [1] whose binary length can be bounded by applying the induction hypothesis (3.6) in p dimensions: log jy [1] j = ld cp 3 (n+k)+O (sk (n+log s)) ; where the multiplicative constant hidden in the term O (sk (n + log s)) does not depend on c. Substitute y [1] into (y) and consider the resulting formula [2] (y [2] ) = (y [1] ; y [2] ): <p> <ref> [1] </ref> whose binary length can be bounded by applying the induction hypothesis (3.6) in p dimensions: log jy [1] j = ld cp 3 (n+k)+O (sk (n+log s)) ; where the multiplicative constant hidden in the term O (sk (n + log s)) does not depend on c. Substitute y [1] into (y) and consider the resulting formula [2] (y [2] ) = (y [1] ; y [2] ): The solution set Y [2] IR kp of (y [2] ) is the intersection of Y with the subspace fy 2 IR k j y [1] = y [1] g. <p> p dimensions: log jy <ref> [1] </ref> j = ld cp 3 (n+k)+O (sk (n+log s)) ; where the multiplicative constant hidden in the term O (sk (n + log s)) does not depend on c. Substitute y [1] into (y) and consider the resulting formula [2] (y [2] ) = (y [1] ; y [2] ): The solution set Y [2] IR kp of (y [2] ) is the intersection of Y with the subspace fy 2 IR k j y [1] = y [1] g. Since y [1] 2 int Y [1] , it follows that Y [2] is convex and <p> Substitute y <ref> [1] </ref> into (y) and consider the resulting formula [2] (y [2] ) = (y [1] ; y [2] ): The solution set Y [2] IR kp of (y [2] ) is the intersection of Y with the subspace fy 2 IR k j y [1] = y [1] g. Since y [1] 2 int Y [1] , it follows that Y [2] is convex and full-dimensional. By Lemma 3.3.3, ZZ kp " int Y [2] 6= ;. <p> Substitute y <ref> [1] </ref> into (y) and consider the resulting formula [2] (y [2] ) = (y [1] ; y [2] ): The solution set Y [2] IR kp of (y [2] ) is the intersection of Y with the subspace fy 2 IR k j y [1] = y [1] g. Since y [1] 2 int Y [1] , it follows that Y [2] is convex and full-dimensional. By Lemma 3.3.3, ZZ kp " int Y [2] 6= ;. <p> into (y) and consider the resulting formula [2] (y [2] ) = (y <ref> [1] </ref> ; y [2] ): The solution set Y [2] IR kp of (y [2] ) is the intersection of Y with the subspace fy 2 IR k j y [1] = y [1] g. Since y [1] 2 int Y [1] , it follows that Y [2] is convex and full-dimensional. By Lemma 3.3.3, ZZ kp " int Y [2] 6= ;. <p> the resulting formula [2] (y [2] ) = (y <ref> [1] </ref> ; y [2] ): The solution set Y [2] IR kp of (y [2] ) is the intersection of Y with the subspace fy 2 IR k j y [1] = y [1] g. Since y [1] 2 int Y [1] , it follows that Y [2] is convex and full-dimensional. By Lemma 3.3.3, ZZ kp " int Y [2] 6= ;. Hence we can use the induction hypothesis (3.6) in k p dimensions to bound the bitlength of an interior integral solution y [2] of [2] (y [2] ). <p> By Lemma 3.3.3, ZZ kp " int Y [2] 6= ;. Hence we can use the induction hypothesis (3.6) in k p dimensions to bound the bitlength of an interior integral solution y [2] of [2] (y [2] ). This yields the following bound: log j (y <ref> [1] </ref> ; y [2] )j = ld cp 3 (n+k)+c (kp) 3 (n+kp)+O (sk (n+log s)) ; where, as before, the constant in the term O (sk (n + log s)) does not depend on c. (Note that this bound remains true after the transformation (3.17).) It is easy to see <p> as before, the constant in the term O (sk (n + log s)) does not depend on c. (Note that this bound remains true after the transformation (3.17).) It is easy to see that the inclusions y [i] 2 int Y [i] ; i = 1; 2; guarantee that (y <ref> [1] </ref> ; y [2] ) 2 int Y: To obtain the required bound (3.6) in k dimensions it remains to show that if k 2, then cp 3 (n + k) + c (k p) 3 (n + k p) + sk (n + log s) ck 3 (n + k) <p> By Lemma 3.3.3, ZZ p " int Y <ref> [1] </ref> = ;. Inductively applying part (ii) of the theorem to [1] (y [1] ) we conclude that Y [1] f y [1] 2 IR p j b 1 y [1] a [1] b 2 g, where a [1] 2 ZZ p n f0g, and log maxfja [1] j; jb 1 <p> By Lemma 3.3.3, ZZ p " int Y <ref> [1] </ref> = ;. Inductively applying part (ii) of the theorem to [1] (y [1] ) we conclude that Y [1] f y [1] 2 IR p j b 1 y [1] a [1] b 2 g, where a [1] 2 ZZ p n f0g, and log maxfja [1] j; jb 1 j; jb 2 jg = ld cp 2 (n+k)+O (sk (n+log <p> By Lemma 3.3.3, ZZ p " int Y <ref> [1] </ref> = ;. Inductively applying part (ii) of the theorem to [1] (y [1] ) we conclude that Y [1] f y [1] 2 IR p j b 1 y [1] a [1] b 2 g, where a [1] 2 ZZ p n f0g, and log maxfja [1] j; jb 1 j; jb 2 jg = ld cp 2 (n+k)+O (sk (n+log s)) . <p> By Lemma 3.3.3, ZZ p " int Y <ref> [1] </ref> = ;. Inductively applying part (ii) of the theorem to [1] (y [1] ) we conclude that Y [1] f y [1] 2 IR p j b 1 y [1] a [1] b 2 g, where a [1] 2 ZZ p n f0g, and log maxfja [1] j; jb 1 j; jb 2 jg = ld cp 2 (n+k)+O (sk (n+log s)) . <p> By Lemma 3.3.3, ZZ p " int Y <ref> [1] </ref> = ;. Inductively applying part (ii) of the theorem to [1] (y [1] ) we conclude that Y [1] f y [1] 2 IR p j b 1 y [1] a [1] b 2 g, where a [1] 2 ZZ p n f0g, and log maxfja [1] j; jb 1 j; jb 2 jg = ld cp 2 (n+k)+O (sk (n+log s)) . <p> By Lemma 3.3.3, ZZ p " int Y <ref> [1] </ref> = ;. Inductively applying part (ii) of the theorem to [1] (y [1] ) we conclude that Y [1] f y [1] 2 IR p j b 1 y [1] a [1] b 2 g, where a [1] 2 ZZ p n f0g, and log maxfja [1] j; jb 1 j; jb 2 jg = ld cp 2 (n+k)+O (sk (n+log s)) . Hence we obtain (3.7) with a = U 1 a [1] ! By (3.18), log maxfjaj; jb <p> By Lemma 3.3.3, ZZ p " int Y <ref> [1] </ref> = ;. Inductively applying part (ii) of the theorem to [1] (y [1] ) we conclude that Y [1] f y [1] 2 IR p j b 1 y [1] a [1] b 2 g, where a [1] 2 ZZ p n f0g, and log maxfja [1] j; jb 1 j; jb 2 jg = ld cp 2 (n+k)+O (sk (n+log s)) . Hence we obtain (3.7) with a = U 1 a [1] ! By (3.18), log maxfjaj; jb 1 j; <p> By Lemma 3.3.3, ZZ p " int Y <ref> [1] </ref> = ;. Inductively applying part (ii) of the theorem to [1] (y [1] ) we conclude that Y [1] f y [1] 2 IR p j b 1 y [1] a [1] b 2 g, where a [1] 2 ZZ p n f0g, and log maxfja [1] j; jb 1 j; jb 2 jg = ld cp 2 (n+k)+O (sk (n+log s)) . Hence we obtain (3.7) with a = U 1 a [1] ! By (3.18), log maxfjaj; jb 1 j; jb 2 jg = ld cp <p> " int Y <ref> [1] </ref> = ;. Inductively applying part (ii) of the theorem to [1] (y [1] ) we conclude that Y [1] f y [1] 2 IR p j b 1 y [1] a [1] b 2 g, where a [1] 2 ZZ p n f0g, and log maxfja [1] j; jb 1 j; jb 2 jg = ld cp 2 (n+k)+O (sk (n+log s)) . Hence we obtain (3.7) with a = U 1 a [1] ! By (3.18), log maxfjaj; jb 1 j; jb 2 jg = ld cp 2 (n+k)+O (sk (n+log s)) : Scaling the constant <p> IR p j b 1 y <ref> [1] </ref> a [1] b 2 g, where a [1] 2 ZZ p n f0g, and log maxfja [1] j; jb 1 j; jb 2 jg = ld cp 2 (n+k)+O (sk (n+log s)) . Hence we obtain (3.7) with a = U 1 a [1] ! By (3.18), log maxfjaj; jb 1 j; jb 2 jg = ld cp 2 (n+k)+O (sk (n+log s)) : Scaling the constant in the term O (sk (n + log s)) to 1, letting c = 1, and taking into account the inequality s k p; we can bound
Reference: [2] <author> L. Babai. </author> <title> On Lovasz' lattice reduction and the nearest lattice point problem. </title> <journal> Combinatorica, </journal> <volume> 6 </volume> <pages> 1-13, </pages> <year> 1986. </year>
Reference-contexts: kp satisfy the assumption of Corollary 3.2.3: f u = (u 1 ; : : : ; u k ) T 2 ZZ kp j fi 1 u = : : : = fi s u = 0g = f0g: (3.21) Consider the partition y = (y [1] ; y <ref> [2] </ref> ), where y [1] = (y 1 ; : : : ; y p ) and y [2] = (y p+1 ; : : : ; y k ). Let : and let Y [1] be the solution set of [1] (y [1] ). <p> k ) T 2 ZZ kp j fi 1 u = : : : = fi s u = 0g = f0g: (3.21) Consider the partition y = (y [1] ; y <ref> [2] </ref> ), where y [1] = (y 1 ; : : : ; y p ) and y [2] = (y p+1 ; : : : ; y k ). Let : and let Y [1] be the solution set of [1] (y [1] ). Since Y [1] is a projection of Y , the set Y [1] IR p is convex and full-dimensional. <p> Since Y [1] is a projection of Y , the set Y [1] IR p is convex and full-dimensional. Lemma 3.3.3 A point y [1] 2 ZZ p " int Y [1] if and only if there is a point y <ref> [2] </ref> 2 ZZ kp such that (y [1] ; y [2] ) 2 ZZ k " int Y . Proof of Lemma 3.3.3. The fact that (y [1] ; y [2] ) 2 ZZ k "int Y implies y [1] 2 ZZ p "int Y [1] follows directly from the definition <p> Lemma 3.3.3 A point y [1] 2 ZZ p " int Y [1] if and only if there is a point y <ref> [2] </ref> 2 ZZ kp such that (y [1] ; y [2] ) 2 ZZ k " int Y . Proof of Lemma 3.3.3. The fact that (y [1] ; y [2] ) 2 ZZ k "int Y implies y [1] 2 ZZ p "int Y [1] follows directly from the definition of Y [1] . <p> A point y [1] 2 ZZ p " int Y [1] if and only if there is a point y <ref> [2] </ref> 2 ZZ kp such that (y [1] ; y [2] ) 2 ZZ k " int Y . Proof of Lemma 3.3.3. The fact that (y [1] ; y [2] ) 2 ZZ k "int Y implies y [1] 2 ZZ p "int Y [1] follows directly from the definition of Y [1] . Suppose that y [1] 2 ZZ p " int Y [1] . <p> Hence there is a positive * such that the open box B = f (y [1] ; y <ref> [2] </ref> ) : j y [1] y [1] j &lt; *; j y [2] ~ j &lt; * g belongs to Y . <p> Hence there is a positive * such that the open box B = f (y [1] ; y <ref> [2] </ref> ) : j y [1] y [1] j &lt; *; j y [2] ~ j &lt; * g belongs to Y . <p> Substitute y [1] into (y) and consider the resulting formula <ref> [2] </ref> (y [2] ) = (y [1] ; y [2] ): The solution set Y [2] IR kp of (y [2] ) is the intersection of Y with the subspace fy 2 IR k j y [1] = y [1] g. <p> Substitute y [1] into (y) and consider the resulting formula <ref> [2] </ref> (y [2] ) = (y [1] ; y [2] ): The solution set Y [2] IR kp of (y [2] ) is the intersection of Y with the subspace fy 2 IR k j y [1] = y [1] g. <p> Substitute y [1] into (y) and consider the resulting formula <ref> [2] </ref> (y [2] ) = (y [1] ; y [2] ): The solution set Y [2] IR kp of (y [2] ) is the intersection of Y with the subspace fy 2 IR k j y [1] = y [1] g. Since y [1] 2 int Y [1] , it follows that Y [2] is convex and full-dimensional. <p> Substitute y [1] into (y) and consider the resulting formula <ref> [2] </ref> (y [2] ) = (y [1] ; y [2] ): The solution set Y [2] IR kp of (y [2] ) is the intersection of Y with the subspace fy 2 IR k j y [1] = y [1] g. Since y [1] 2 int Y [1] , it follows that Y [2] is convex and full-dimensional. <p> Substitute y [1] into (y) and consider the resulting formula <ref> [2] </ref> (y [2] ) = (y [1] ; y [2] ): The solution set Y [2] IR kp of (y [2] ) is the intersection of Y with the subspace fy 2 IR k j y [1] = y [1] g. Since y [1] 2 int Y [1] , it follows that Y [2] is convex and full-dimensional. By Lemma 3.3.3, ZZ kp " int Y [2] 6= ;. <p> ) = (y [1] ; y <ref> [2] </ref> ): The solution set Y [2] IR kp of (y [2] ) is the intersection of Y with the subspace fy 2 IR k j y [1] = y [1] g. Since y [1] 2 int Y [1] , it follows that Y [2] is convex and full-dimensional. By Lemma 3.3.3, ZZ kp " int Y [2] 6= ;. Hence we can use the induction hypothesis (3.6) in k p dimensions to bound the bitlength of an interior integral solution y [2] of [2] (y [2] ). <p> IR kp of (y <ref> [2] </ref> ) is the intersection of Y with the subspace fy 2 IR k j y [1] = y [1] g. Since y [1] 2 int Y [1] , it follows that Y [2] is convex and full-dimensional. By Lemma 3.3.3, ZZ kp " int Y [2] 6= ;. Hence we can use the induction hypothesis (3.6) in k p dimensions to bound the bitlength of an interior integral solution y [2] of [2] (y [2] ). This yields the following bound: log j (y [1] ; y [2] )j = ld cp 3 (n+k)+c (kp) 3 <p> Since y [1] 2 int Y [1] , it follows that Y <ref> [2] </ref> is convex and full-dimensional. By Lemma 3.3.3, ZZ kp " int Y [2] 6= ;. Hence we can use the induction hypothesis (3.6) in k p dimensions to bound the bitlength of an interior integral solution y [2] of [2] (y [2] ). This yields the following bound: log j (y [1] ; y [2] )j = ld cp 3 (n+k)+c (kp) 3 (n+kp)+O (sk (n+log s)) ; where, as before, the constant in the term O (sk (n + log s)) does not depend on c. (Note <p> [1] 2 int Y [1] , it follows that Y <ref> [2] </ref> is convex and full-dimensional. By Lemma 3.3.3, ZZ kp " int Y [2] 6= ;. Hence we can use the induction hypothesis (3.6) in k p dimensions to bound the bitlength of an interior integral solution y [2] of [2] (y [2] ). This yields the following bound: log j (y [1] ; y [2] )j = ld cp 3 (n+k)+c (kp) 3 (n+kp)+O (sk (n+log s)) ; where, as before, the constant in the term O (sk (n + log s)) does not depend on c. (Note that this <p> int Y [1] , it follows that Y <ref> [2] </ref> is convex and full-dimensional. By Lemma 3.3.3, ZZ kp " int Y [2] 6= ;. Hence we can use the induction hypothesis (3.6) in k p dimensions to bound the bitlength of an interior integral solution y [2] of [2] (y [2] ). This yields the following bound: log j (y [1] ; y [2] )j = ld cp 3 (n+k)+c (kp) 3 (n+kp)+O (sk (n+log s)) ; where, as before, the constant in the term O (sk (n + log s)) does not depend on c. (Note that this bound remains <p> By Lemma 3.3.3, ZZ kp " int Y <ref> [2] </ref> 6= ;. Hence we can use the induction hypothesis (3.6) in k p dimensions to bound the bitlength of an interior integral solution y [2] of [2] (y [2] ). This yields the following bound: log j (y [1] ; y [2] )j = ld cp 3 (n+k)+c (kp) 3 (n+kp)+O (sk (n+log s)) ; where, as before, the constant in the term O (sk (n + log s)) does not depend on c. (Note that this bound remains true after the transformation (3.17).) It is easy to see that the inclusions <p> constant in the term O (sk (n + log s)) does not depend on c. (Note that this bound remains true after the transformation (3.17).) It is easy to see that the inclusions y [i] 2 int Y [i] ; i = 1; 2; guarantee that (y [1] ; y <ref> [2] </ref> ) 2 int Y: To obtain the required bound (3.6) in k dimensions it remains to show that if k 2, then cp 3 (n + k) + c (k p) 3 (n + k p) + sk (n + log s) ck 3 (n + k) (3.22) 34 for <p> rational vector a 2 IR k and an interval [b; c] of length 2 O (k) such that Y " ZZ k Y b [ Y b+1 [ : : : [ Y c ; where Y i = Y " fy 2 IR k j ya = ig (see <ref> [2] </ref>; and also [23], [17], [32]). By Lemma 3.4.1, for K = QF (m; d; l) this reduces problem P k to 2 O (k) similarly structured (k 1)- dimensional problems each of which replaces the input set Y by the intersection of Y with a rational hyperplane.
Reference: [3] <author> B. Bank, T. Krick, R Mandel, and P. Solerno. </author> <title> A geometrical bound for integer programming with polynomial constraints. </title> <editor> In L. Budach, editor, </editor> <booktitle> Fundamentals of Computation Theory, Lecture Notes in Computer Science, </booktitle> <volume> volume 529, </volume> <pages> pages 121-125. </pages> <publisher> Springer, </publisher> <address> Berlin, </address> <year> 1991. </year>
Reference-contexts: In addition to linear integer programming, this readily implies the polynomial-time solvability of systems of convex and quasi-convex polynomial inequalities with any fixed number of integer variables ([20], <ref> [3] </ref>). It should be mentioned, however, that the above complexity result is more robust it only uses the convexity of the solution set and does not require that each algebraic constraint be quasi-convex.
Reference: [4] <author> A.I. Barvinok. </author> <title> A polynomial time algorithm for counting integral points in polyhedra when the dimension is fixed. </title> <journal> Mathematics of Operations Research, </journal> <volume> 19 </volume> <pages> 769-779, </pages> <year> 1994. </year>
Reference-contexts: The above result also holds for systems of strict and/or nonstrict linear inequalities in positive definite and/or semidefinite matrices with integer and/or real variables, i.e., for mixed semidefinite programming. Note that Barvinok <ref> [4] </ref> gave a polynomial-time algorithm for counting integral points in a polytope of fixed dimension.
Reference: [5] <author> S. Basu, R. Pollack, and M.-R. Roy. </author> <title> On the combinatorial and algebraic complexity of quantifier elimination. </title> <journal> Journal of ACM, </journal> <volume> 43 </volume> <pages> 1002-1045, </pages> <year> 1996. </year>
Reference-contexts: Then we use the above bound and the decision methods for the first-order theory of the reals due to Renegar [29] and Basu, Pollack and Roy <ref> [5] </ref>, along with Chazelle and Matousek's [9] derandomized variant of Clarkson's algorithm [10] to prove the following result: In fixed dimension, the feasibility of a given semidefinite program can be tested in a number of arithmetic operations which grows linearly in the number of constraints and does not depend on the <p> By applying a quantitative version of Kronecker's 3 theorem on simultaneous Diophantine approximation, and some recent bounds <ref> [5] </ref> on the combinatorial and algebraic complexity of quantifier elimination methods for the first order theory of the reals, we first obtain an upper bound on the minimum binary size of an integral point contained in a given convex semi-algebraic set. <p> The best currently known bounds on the degrees and binary length of the polynomials h ij (y) are due to Basu, Pollack, and Roy <ref> [5] </ref>. Proposition 2.2.3 (cf. Theorem 1 of [5]) Each formula (F) can be transformed into an equivalent quantifier-free formula (QF) such that I m (k+1) ! i=1 (n i +1) d (k+1) ! J i m ! i=1 O (n i ) , deg h ij (y) d ! i=1 O <p> The best currently known bounds on the degrees and binary length of the polynomials h ij (y) are due to Basu, Pollack, and Roy <ref> [5] </ref>. Proposition 2.2.3 (cf. Theorem 1 of [5]) Each formula (F) can be transformed into an equivalent quantifier-free formula (QF) such that I m (k+1) ! i=1 (n i +1) d (k+1) ! J i m ! i=1 O (n i ) , deg h ij (y) d ! i=1 O (n i ) . <p> The following proposition is implicit in <ref> [5] </ref> (see Section 3.1.3). 10 Proposition 2.2.4 Let Y be the solution set of a system of J polynomial equations and inequalities ^ J j=1 (h j (y) 4 j 0); where h j (y) 2 ZZ [y 1 ; : : : ; y k ] are polynomials of degree <p> set, then it has a solution y 2 IR k such that kyk 2 R, where log R = ld O (k) ! i=1 O (n i ) : 2.2.5 Inscribing a Box into a Full-dimensional Semi-algebraic Set Proposition 2.2.8 below is a restatement of Theorems 5 and 6 of <ref> [5] </ref>.
Reference: [6] <author> S. Boyd, L. El Ghaoui, E. Feron, and V. Balakrishnan. </author> <title> Linear Matrix Inequalities in System and Control Theory. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1994. </year>
Reference-contexts: Many convex optimization problems, e.g., linear and convex quadratically constrained quadratic programs, maximum eigenvalue and matrix norm minimization, and also the computation of extremal ellipsoids for polyhedral sets, can be cast as SDP [26]. Applications of semidefinite programming include system and control theory <ref> [6] </ref>, statistics [12], [13], [33], [35], and combinatorial optimization [16], [14], [19], [15]. It is well known that approximately solving semidefinite programs with explicitly given bounds on the size of an optimal solution can be accomplished in polynomial time by interior-point methods [1], [26].
Reference: [7] <author> W.S. Brown and J.F. Traub. </author> <title> On Euclid's algorithm and the theory of subresul-tants. </title> <journal> Journal of ACM, </journal> <volume> 18 </volume> <pages> 505-514, </pages> <year> 1971. </year>
Reference-contexts: Since the polynomial Q 1 (t) mod g (t) can be computed in polynomial time, and the binary length of its rational coefficients can be bounded via sub-resultants by O (deg (gQ) log (jgjjQj deg (gQ))) bits (see e.g. [11], <ref> [7] </ref>), Propositions 2.2.3 and 2.2.4 read ily imply the following result.
Reference: [8] <author> J.W.S. Cassels. </author> <title> An Introduction to Diophantine Approximation. </title> <publisher> University Press, </publisher> <address> Cambridge, </address> <year> 1957. </year>
Reference-contexts: Cassels <ref> [8] </ref>.) The fact that (i) implies (ii) is trivial. Proposition 3.2.1 below can be regarded as a quantitative version of the reverse implication. Proposition 3.2.1 ([8], Chapter V, Theorem XVII, Part B) Let ff 2 IR k be a given vector, and let X and * be given positive numbers.
Reference: [9] <author> B. Chazelle and J. Matousek. </author> <title> On linear-time deterministic algorithms for optimization problems in fixed dimension. </title> <journal> Journal of Algorithms, </journal> <volume> 21(3) </volume> <pages> 579-597, </pages> <year> 1996. </year>
Reference-contexts: Then we use the above bound and the decision methods for the first-order theory of the reals due to Renegar [29] and Basu, Pollack and Roy [5], along with Chazelle and Matousek's <ref> [9] </ref> derandomized variant of Clarkson's algorithm [10] to prove the following result: In fixed dimension, the feasibility of a given semidefinite program can be tested in a number of arithmetic operations which grows linearly in the number of constraints and does not depend on the binary size of the input program. <p> But the expected number of violation tests is bounded by mpoly (n). Hence we conclude that for all n and m, testing the feasibility of (2:1) requires expected mn O (minfm;n 2 g) operations over ln O (minfm;n 2 g) -bit numbers. Chazelle and Matousek <ref> [9] </ref> derandomized Clarkson's algorithm for a wide subclass of LP-type problems, which includes linear programming and the problem of computing the minimum volume circumscribed ellipsoid for a given m-point set in IR n . <p> The derandom-ization in <ref> [9] </ref> is based on an additional assumption which we state here in the following 19 stronger form: for any subset I M , one can compute in O (jIj) ~ D operations a set system R I which includes all of the sets I 0 I such that I 0 = <p> It is easy to see that these steps can be carried out in O (jIj) operations, where d = n (n+1)=2+1. Hence, the additional assumption is satisfied with some integer ~ D = O (n 2 ). Let D = maxfD; ~ Dg. The derandomized algorithm of <ref> [9] </ref> computes an optimal basis of (2:16) by performing mD O (D) operations and mpoly (D) + D O (D) violation tests with subsets I of size at most D.
Reference: [10] <author> K.L. Clarkson. </author> <title> Las Vegas algorithms for linear and integer programming when the dimension is small. </title> <journal> Journal of ACM, </journal> <volume> 42 </volume> <pages> 488-499, </pages> <year> 1995. </year>
Reference-contexts: Then we use the above bound and the decision methods for the first-order theory of the reals due to Renegar [29] and Basu, Pollack and Roy [5], along with Chazelle and Matousek's [9] derandomized variant of Clarkson's algorithm <ref> [10] </ref> to prove the following result: In fixed dimension, the feasibility of a given semidefinite program can be tested in a number of arithmetic operations which grows linearly in the number of constraints and does not depend on the binary size of the input program. <p> If m is bounded by a polynomial in n, the theorem follows from Lemma 2.5.1. We next show that for large m, determining 17 the feasibility of (2:1) via Clarkson's algorithm <ref> [10] </ref> requires an expected mn O (minfm;n 2 g) operations over ln O (minfm;n 2 g) -bit numbers. <p> Clarkson's algorithm <ref> [10] </ref> finds an optimal basis by performing expected N = O (Dm+D 3 p m log m log m) mpoly (n) violation tests. <p> ^ S I (X; ) ^ (kXk 2 2 ) ] g =) (A j X &gt; b j + ) g; where S I (X; ) denotes the quantifier free formula f i2I 2 R 2 ) g: 1 In fact, by using the arguments of Section 4 in <ref> [10] </ref>, it can be verified [21] that the above bounds on the number of violation tests and the size of sample sets are valid for computing an optimal basis for any mapping V : 2 M ! 2 M that satisfies the following two conditions: (i) V (I) M n I
Reference: [11] <author> G.E. Collins. </author> <title> Polynomial reminder sequences and determinants. </title> <journal> American Mathematical Monthly, </journal> <volume> 73 </volume> <pages> 708-712, </pages> <year> 1966. </year>
Reference-contexts: Since the polynomial Q 1 (t) mod g (t) can be computed in polynomial time, and the binary length of its rational coefficients can be bounded via sub-resultants by O (deg (gQ) log (jgjjQj deg (gQ))) bits (see e.g. <ref> [11] </ref>, [7]), Propositions 2.2.3 and 2.2.4 read ily imply the following result.
Reference: [12] <author> R. Fletcher. </author> <title> A nonlinear programming problem in statistics (educational testing). </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 2 </volume> <pages> 257-267, </pages> <year> 1981. </year>
Reference-contexts: Many convex optimization problems, e.g., linear and convex quadratically constrained quadratic programs, maximum eigenvalue and matrix norm minimization, and also the computation of extremal ellipsoids for polyhedral sets, can be cast as SDP [26]. Applications of semidefinite programming include system and control theory [6], statistics <ref> [12] </ref>, [13], [33], [35], and combinatorial optimization [16], [14], [19], [15]. It is well known that approximately solving semidefinite programs with explicitly given bounds on the size of an optimal solution can be accomplished in polynomial time by interior-point methods [1], [26].
Reference: [13] <author> R. Fletcher. </author> <title> Semidefinite matrix constraints in optimization. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 23 </volume> <pages> 493-513, </pages> <year> 1985. </year>
Reference-contexts: Many convex optimization problems, e.g., linear and convex quadratically constrained quadratic programs, maximum eigenvalue and matrix norm minimization, and also the computation of extremal ellipsoids for polyhedral sets, can be cast as SDP [26]. Applications of semidefinite programming include system and control theory [6], statistics [12], <ref> [13] </ref>, [33], [35], and combinatorial optimization [16], [14], [19], [15]. It is well known that approximately solving semidefinite programs with explicitly given bounds on the size of an optimal solution can be accomplished in polynomial time by interior-point methods [1], [26].
Reference: [14] <author> A. Frieze and M. Jerrum. </author> <title> Improved approximation algorithms for MAX k-CUT and MAX BISECTION. </title> <journal> Algorithmica, </journal> <volume> 18(1) </volume> <pages> 67-81, </pages> <year> 1997. </year> <month> 41 </month>
Reference-contexts: Applications of semidefinite programming include system and control theory [6], statistics [12], [13], [33], [35], and combinatorial optimization [16], <ref> [14] </ref>, [19], [15]. It is well known that approximately solving semidefinite programs with explicitly given bounds on the size of an optimal solution can be accomplished in polynomial time by interior-point methods [1], [26].
Reference: [15] <author> M.X. Goemans. </author> <title> Semidefinite programming in combinatorial optimization. </title> <type> Preprint, </type> <month> March </month> <year> 1997. </year>
Reference-contexts: Applications of semidefinite programming include system and control theory [6], statistics [12], [13], [33], [35], and combinatorial optimization [16], [14], [19], <ref> [15] </ref>. It is well known that approximately solving semidefinite programs with explicitly given bounds on the size of an optimal solution can be accomplished in polynomial time by interior-point methods [1], [26].
Reference: [16] <author> M.X. Goemans and D.P. Williamson. </author> <title> Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming. </title> <journal> Journal of ACM, </journal> <volume> 42 </volume> <pages> 1115-1145, </pages> <year> 1995. </year>
Reference-contexts: Applications of semidefinite programming include system and control theory [6], statistics [12], [13], [33], [35], and combinatorial optimization <ref> [16] </ref>, [14], [19], [15]. It is well known that approximately solving semidefinite programs with explicitly given bounds on the size of an optimal solution can be accomplished in polynomial time by interior-point methods [1], [26].
Reference: [17] <author> M. Grotschel, L. Lovasz, and A. Schrijver. </author> <title> Geometric Algorithms and Combinatorial Optimization. </title> <publisher> Springer, </publisher> <address> Berlin, </address> <year> 1988. </year>
Reference-contexts: This convex programming problem can be solved in O (n 4 log (2 l nR=*)) iterations of the ellipsoid method (see, e.g., <ref> [17] </ref>), where each iteration requires O (n 2 (m+ n)) arithmetic operations over log (2 l nR=*)-bit numbers. Hence, we obtain an upper bound of lmn O (minfm;n 2 g) operations with ln O (minfm;n 2 g) -bit numbers for testing the feasibility of (2:1). <p> R is an upper bound on the Euclidean norm of an exact solution (see Renegar [30], Theorem 1.2), the lemma follows. 2 Remark 3.4.2 In fact, any set Y 2 QF (m; d; l) can be (k+1)-rounded in l O (1) (md) O (k) time by the shallow-cut ellipsoid method <ref> [17] </ref>, [32]. Let K be a class of bounded convex full-dimensional sets in IR k . Consider the integer programming problem: P k : Given a set Y 2 K, determine whether Y " ZZ k 6= ;, and if so, find an integral point y 2 Y . <p> IR k and an interval [b; c] of length 2 O (k) such that Y " ZZ k Y b [ Y b+1 [ : : : [ Y c ; where Y i = Y " fy 2 IR k j ya = ig (see [2]; and also [23], <ref> [17] </ref>, [32]). By Lemma 3.4.1, for K = QF (m; d; l) this reduces problem P k to 2 O (k) similarly structured (k 1)- dimensional problems each of which replaces the input set Y by the intersection of Y with a rational hyperplane.
Reference: [18] <author> F. John. </author> <title> Extremum problems with inequalities as subsidiary conditions. In Studies and Essays, </title> <note> presented to R. Courant on his 60th Birthday January 8th, </note> <year> 1948, </year> <pages> pages 187-204. </pages> <publisher> Wiley Interscience, </publisher> <address> New York, </address> <year> 1948. </year>
Reference-contexts: In particular, for fixed k such a transformation can be found in time polynomial in l, m, and d. Proof of Lemma 3.4.1. It is well known that any bounded convex full-dimensional set in IR k can be k-rounded <ref> [18] </ref>. Suppose that Y is defined by a quantifier-free formula P (y).
Reference: [19] <author> D. Karger, R. Motwani, and M. Sudan. </author> <title> Approximate graph coloring by semidef-inite programming. </title> <booktitle> In Proceedings of the 35th Symposium on the Foundation of Computer Science, </booktitle> <pages> pages 2-13, </pages> <year> 1994. </year>
Reference-contexts: Applications of semidefinite programming include system and control theory [6], statistics [12], [13], [33], [35], and combinatorial optimization [16], [14], <ref> [19] </ref>, [15]. It is well known that approximately solving semidefinite programs with explicitly given bounds on the size of an optimal solution can be accomplished in polynomial time by interior-point methods [1], [26].
Reference: [20] <author> L.G. Khachiyan. </author> <title> Convexity and complexity in polynomial programming. </title> <booktitle> In Proceedings of the International Congress of Mathematicians, </booktitle> <pages> pages 1569-1577, </pages> <address> Warszawa, </address> <month> August 16-24 </month> <year> 1983. </year>
Reference: [21] <author> L.G. </author> <title> Khachiyan. </title> <booktitle> Lecture notes. </booktitle> <institution> Department of Computer Science, Rutgers University, </institution> <year> 1992. </year>
Reference-contexts: ^ (kXk 2 2 ) ] g =) (A j X &gt; b j + ) g; where S I (X; ) denotes the quantifier free formula f i2I 2 R 2 ) g: 1 In fact, by using the arguments of Section 4 in [10], it can be verified <ref> [21] </ref> that the above bounds on the number of violation tests and the size of sample sets are valid for computing an optimal basis for any mapping V : 2 M ! 2 M that satisfies the following two conditions: (i) V (I) M n I and (ii) V (I [
Reference: [22] <author> A.K. Lenstra, H.W. Lenstra Jr., and L. Lovasz. </author> <title> Factoring polynomials with rational coefficients. </title> <journal> Mathematische Annalen, </journal> <volume> 261 </volume> <pages> 515-534, </pages> <year> 1982. </year>
Reference-contexts: On the other hand, since G (t) can be factored in polynomial time (Lenstra, Lenstra, Lovasz <ref> [22] </ref>), and the sign of any of its factors at can also be determined in polynomial time, the minimal polynomial g (t) 2 ZZ [t] for can be computed in time polynomial in deg (G) and log jGj.
Reference: [23] <author> H.W. Lenstra Jr. </author> <title> Integer programming with a fixed number of variables. </title> <journal> Mathematics of Operations Research, </journal> <volume> 8 </volume> <pages> 538-548, </pages> <year> 1983. </year>
Reference-contexts: Next we show that this bound implies the following generalization of the celebrated result of Lenstra <ref> [23] </ref> on the polynomial-time solvability of linear integer programming in fixed dimension: For each fixed n, there exists a polynomial-time algorithm that, given a convex semi-algebraic set defined by a first-order formula with n free and quantified vari ables, checks whether the input set contains an integral point, and if so, <p> For any fixed number k + ! i=1 n i of free and quantified variables, the algorithm runs in (lmd) O (1) time and requires (md) O (1) evaluations of the Boolean function P (). Theorem 3.1.2 is a generalization of the well-known theorem of Lenstra <ref> [23] </ref> on the polynomial-time solvability of linear integer programming in fixed dimension. This theorem also implies the polynomial-time solvability of systems of convex and quasi-convex polynomial inequalities with a fixed number of integer variables [20],[3]. <p> 2 IR k and an interval [b; c] of length 2 O (k) such that Y " ZZ k Y b [ Y b+1 [ : : : [ Y c ; where Y i = Y " fy 2 IR k j ya = ig (see [2]; and also <ref> [23] </ref>, [17], [32]). By Lemma 3.4.1, for K = QF (m; d; l) this reduces problem P k to 2 O (k) similarly structured (k 1)- dimensional problems each of which replaces the input set Y by the intersection of Y with a rational hyperplane.
Reference: [24] <author> N. Megiddo. </author> <title> Linear programming in linear time when the dimension is fixed. </title> <journal> Journal of ACM, </journal> <volume> 31 </volume> <pages> 114-127, </pages> <year> 1984. </year>
Reference-contexts: A classical complexity result in linear programming is the linear-time solvability of linear programs in fixed dimension (Megiddo <ref> [24] </ref>). The first part of the dissertation presents an extension of this result to semidefinite programming for the bit model of computation. <p> Therefore, in the bit model of computation, Theorem 2.1.2 can be regarded as an extension of Megiddo's result <ref> [24] </ref> on the linear time solvability of linear programs in fixed dimension. Note also that in the bit model of computation, each arithmetic operation with ln O (minfm;n 2 g) -bit numbers can be replaced by n O (minfm;n 2 g) operations with l-bit numbers.
Reference: [25] <author> M. Mignotte. </author> <title> Some useful bounds. </title> <editor> In B. Buchberger, G.E. Collins, and R. Loos, editors, </editor> <booktitle> Computer Algebra, Symbolic and Algebraic Computation, </booktitle> <pages> pages 259-263. </pages> <publisher> Springer, Wien, </publisher> <year> 1982. </year>
Reference-contexts: Thus, if ~ is a real number, then k~k = minf j~ xj : x = 0; 1; 2; : : :g is the distance from ~ to the nearest in teger. 2.2.2 Some Useful Inequalities and Identities We shall need the following well-known inequalities. Proposition 2.2.1 (see e.g. <ref> [25] </ref>) Let P (x) = a 0 x p + a 1 x p1 + : : : + a p1 x + a p and Q (x) = b 0 x q +b 1 x q1 +: : :+a q1 x+a q be univariate polynomials with integer coefficients such that
Reference: [26] <author> Y. Nesterov and A. </author> <title> Nemirovski. Interior Point Polynomial Methods for Convex Programming: Theory and Applications. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1994. </year>
Reference-contexts: Many convex optimization problems, e.g., linear and convex quadratically constrained quadratic programs, maximum eigenvalue and matrix norm minimization, and also the computation of extremal ellipsoids for polyhedral sets, can be cast as SDP <ref> [26] </ref>. Applications of semidefinite programming include system and control theory [6], statistics [12], [13], [33], [35], and combinatorial optimization [16], [14], [19], [15]. <p> It is well known that approximately solving semidefinite programs with explicitly given bounds on the size of an optimal solution can be accomplished in polynomial time by interior-point methods [1], <ref> [26] </ref>. However, the complexity of the general SDP problem, and in particular the complexity of testing the feasibility of a given semidefinite program, remains an open fundamental problem of mathematical programming.
Reference: [27] <author> L. Porkolab and L. Khachiyan. </author> <title> On the complexity of semidefinite programming. </title> <journal> Journal of Global Optimization, </journal> <volume> 10(4) </volume> <pages> 351-365, </pages> <year> 1997. </year>
Reference-contexts: Let l denote the maximum binary length of the input coefficients. The proofs of the following results (Theorems 2.6.1, 2.6.3 and 2.6.5) are similar to those presented above, and they can be found in <ref> [27] </ref>.
Reference: [28] <author> M.V. Ramana. </author> <title> An exact duality theory for semidefinite programming and its complexity implications. </title> <journal> Mathematical Programming, </journal> <volume> 77(2) </volume> <pages> 129-162, </pages> <year> 1997. </year>
Reference-contexts: In fact, it is not even known whether for the standard bit model of computation, the problem of testing the feasibility of a given semidefinite program belongs to the complexity class NP. (For the real number model of computation this problem is known to be in NP " coNP <ref> [28] </ref>, but the question of polynomial-time solvability remains open.) Since the complexity status of the general feasibility problem seems to be a very difficult question, it is natural to ask what other known complexity results for linear programming can be extended to semidefinite programming.
Reference: [29] <author> J. Renegar. </author> <title> On the computational complexity and geometry of the first order theory of the reals. part i: Introduction; preliminaries; the geometry of semi-algebraic sets; the decision problem for the existential theory of the reals. </title> <journal> Journal of Symbolic Computation, </journal> <volume> 13 </volume> <pages> 255-299, </pages> <year> 1992. </year>
Reference-contexts: Then we use the above bound and the decision methods for the first-order theory of the reals due to Renegar <ref> [29] </ref> and Basu, Pollack and Roy [5], along with Chazelle and Matousek's [9] derandomized variant of Clarkson's algorithm [10] to prove the following result: In fixed dimension, the feasibility of a given semidefinite program can be tested in a number of arithmetic operations which grows linearly in the number of constraints <p> Formulae without free variables are called sentences. We say that y 2 IR k is a solution of (F) if the sentence obtained by substituting y into (F) is true. The following complexity result, due to Renegar <ref> [29] </ref>, deals with the decision problem for the first-order theory of the reals: Determine whether a sentence (F) is true or false. Proposition 2.2.2 (cf. Theorem 1.1 of [29]) There is an algorithm for the decision problem for the first-order theory of the reals that requires (md) ! i=1 O (n <p> The following complexity result, due to Renegar <ref> [29] </ref>, deals with the decision problem for the first-order theory of the reals: Determine whether a sentence (F) is true or false. Proposition 2.2.2 (cf. Theorem 1.1 of [29]) There is an algorithm for the decision problem for the first-order theory of the reals that requires (md) ! i=1 O (n i ) arithmetic operations with l (md) ! i=1 O (n i ) -bit numbers and (md) ! i=1 O (n i ) evaluations of the Boolean function
Reference: [30] <author> J. Renegar. </author> <title> On the computational complexity of approximating solutions for real algebraic formulae. </title> <journal> SIAM Journal on Computing, </journal> <volume> 21 </volume> <pages> 1008-1025, </pages> <year> 1992. </year> <month> 42 </month>
Reference-contexts: ) log log (3+R=*) arithmetic operations with l (md) O (k) i O (n i ) -bit numbers and (md) O (k) i O (n i ) evaluations of the Boolean function P (), where R is an upper bound on the Euclidean norm of an exact solution (see Renegar <ref> [30] </ref>, Theorem 1.2). Therefore we obtain the following bound: Corollary 2.5.4 Under the assumption of Theorem 2.5.3, fl can be approximated to an accuracy of " &gt; 0 in n O (minfm;n 2 g) [log l + log log (3 + 1=")] arithmetic operations. <p> Since an *-approximate solution for an arbitrary formula (F) can be computed in l O (1) (md) O (k) i O (n i ) log log (3 + R=*)-time, where R is an upper bound on the Euclidean norm of an exact solution (see Renegar <ref> [30] </ref>, Theorem 1.2), the lemma follows. 2 Remark 3.4.2 In fact, any set Y 2 QF (m; d; l) can be (k+1)-rounded in l O (1) (md) O (k) time by the shallow-cut ellipsoid method [17], [32].
Reference: [31] <author> T.R. Rockafellar. </author> <title> Convex Analysis. </title> <publisher> Princeton University Press, </publisher> <address> NJ, </address> <year> 1970. </year>
Reference-contexts: : ; m; i=1 y i = 1g; fi (R) = min X2 R maxfA 1 X b 1 ; : : :; A m X b m g: (Recall that C is the cone of symmetric positive semidefinite matrices of order n.) From von Neumann's saddlepoint theorem (see, e.g., <ref> [31] </ref>) and (2:5), it follows that for any R 0, P m = max y2 m min X2 R f ( i=1 y i A i ) X i=1 y i b i g P m P m Consider the formula (R) = 8y 2 m f R n ( i=1 <p> By Helly's theorem (see, e.g., <ref> [31] </ref>) there exists a system of at most 1 + n (n + 1)=2 sets from 1 ; H 1 ; : : :; H m whose intersection is still empty.
Reference: [32] <author> A. Schrijver. </author> <title> Theory of Linear and Integer Programming. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1986. </year>
Reference-contexts: Moreover, since the binary length of U can be bounded by O (k log (kjM j)) bits (see e.g. <ref> [32] </ref>, Ch. 5), from ([25]) it follows that we may assume without loss of generality that log jU j = ld O (sk (n+log s)) : (3.18) Consequently, 0 (y 0 ) has bitlength ld O (sk (n+log s)) : For simplicity of notation, we shall assume henceforth that M = <p> ya = b has the form y = t + y 0 T , where y 0 runs over ZZ k1 and T and t are integral (k 1) fi k matrix and k-vector such that log maxfjT j; jtjg = ld O (k 3 ) : (3.27) (See e.g. <ref> [32] </ref>, Ch. 5.) Substituting t + y 0 T for y into the original formula P (y), we obtain a new quantifier-free formula formula P 0 (y 0 ) = P (t + T y 0 ) whose set of solutions is still convex. <p> is an upper bound on the Euclidean norm of an exact solution (see Renegar [30], Theorem 1.2), the lemma follows. 2 Remark 3.4.2 In fact, any set Y 2 QF (m; d; l) can be (k+1)-rounded in l O (1) (md) O (k) time by the shallow-cut ellipsoid method [17], <ref> [32] </ref>. Let K be a class of bounded convex full-dimensional sets in IR k . Consider the integer programming problem: P k : Given a set Y 2 K, determine whether Y " ZZ k 6= ;, and if so, find an integral point y 2 Y . <p> k and an interval [b; c] of length 2 O (k) such that Y " ZZ k Y b [ Y b+1 [ : : : [ Y c ; where Y i = Y " fy 2 IR k j ya = ig (see [2]; and also [23], [17], <ref> [32] </ref>). By Lemma 3.4.1, for K = QF (m; d; l) this reduces problem P k to 2 O (k) similarly structured (k 1)- dimensional problems each of which replaces the input set Y by the intersection of Y with a rational hyperplane.
Reference: [33] <author> A. Shapiro. </author> <title> Weighted minimum trace factor analysis. </title> <journal> Psychometrika, </journal> <volume> 47 </volume> <pages> 243-264, </pages> <year> 1982. </year>
Reference-contexts: Many convex optimization problems, e.g., linear and convex quadratically constrained quadratic programs, maximum eigenvalue and matrix norm minimization, and also the computation of extremal ellipsoids for polyhedral sets, can be cast as SDP [26]. Applications of semidefinite programming include system and control theory [6], statistics [12], [13], <ref> [33] </ref>, [35], and combinatorial optimization [16], [14], [19], [15]. It is well known that approximately solving semidefinite programs with explicitly given bounds on the size of an optimal solution can be accomplished in polynomial time by interior-point methods [1], [26].
Reference: [34] <author> A. Tarski. </author> <title> A Decision method for Elementary Algebra and Geometry. </title> <institution> University of California Press, </institution> <year> 1951. </year>
Reference-contexts: the reals that requires (md) ! i=1 O (n i ) arithmetic operations with l (md) ! i=1 O (n i ) -bit numbers and (md) ! i=1 O (n i ) evaluations of the Boolean function P (). 2.2.4 Computing Algebraic Solutions for First-Order Formulae It is well known <ref> [34] </ref> that, over the reals, any first-order formula (F) is equivalent to a quantifier-free formula _ I j=1 (h ij (y) 4 ij 0); (QF ) where h ij (y) 2 ZZ [y 1 ; : : : ; y k ] are polynomials with integer coefficients and 4 ij 2
Reference: [35] <author> G.A. Watson. </author> <title> Algorithms for minimum trace factor analysis. </title> <journal> SIAM Journal on Matrix Analysis and Applications, </journal> <volume> 13 </volume> <pages> 1039-1053, </pages> <year> 1992. </year> <month> 43 </month>
Reference-contexts: Many convex optimization problems, e.g., linear and convex quadratically constrained quadratic programs, maximum eigenvalue and matrix norm minimization, and also the computation of extremal ellipsoids for polyhedral sets, can be cast as SDP [26]. Applications of semidefinite programming include system and control theory [6], statistics [12], [13], [33], <ref> [35] </ref>, and combinatorial optimization [16], [14], [19], [15]. It is well known that approximately solving semidefinite programs with explicitly given bounds on the size of an optimal solution can be accomplished in polynomial time by interior-point methods [1], [26].
References-found: 35

