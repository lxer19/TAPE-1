URL: http://www.cs.colorado.edu/~suvas/papers/HICSS96.ps
Refering-URL: http://www.cs.colorado.edu/~suvas/Papers.html
Root-URL: http://www.cs.colorado.edu
Email: (Email:fclive,grunwald,suvasg@cs.colorado.edu)  
Phone: 430,  
Title: Application of an Object-Oriented Parallel Run-Time System to a Grand Challenge 3d Multi-Grid code  
Author: Clive Baillie Dirk Grunwald Suvas Vajracharya 
Date: May 30, 1995  
Address: Campus Box  Boulder, CO 80309-0430  
Affiliation: Department of Computer Science,  University of Colorado,  
Abstract: We have taken a Grand Challenge 3d Multi-Grid code, initially developed on the Cray C-90 and subsequently parallelized for MPPs, and implemented it using the DUDE object-oriented, run-time system which combines both task and data parallelism. The Grand Challenge 3d Multi-Grid code, QGMG (Quasi-Geostrophic Multi-Grid), solves the quasi-geostrophic equations which describe the nonlinear dynamics of rotating, stably stratified fluids; this is the relevant regime for most planetary-scale motions in the Earth's atmosphere and ocean [17]. This code was originally developed on the Cray C-90 and runs at 6 Gflops on all 16 processors. Over the last year the code was implemented it in a portable parallel way on most of todays MPPs (Massively Parallel Processors) [5]. This was done via domain decomposition and message passing using PVM and MPI. Currently, on 256 processors of the Cray T3D the code runs at almost 4 Gflops and on 256 processors of the IBM SP2 at 5 Gflops. We have taken this parallel code and implemented it using the DUDE object-oriented, run-time system [9]. The QGMG solver is a challenging application for two reasons. First, as in all multigrid solvers, the most straight forward implementation requires that most of the processors idle at barrier synchronizations. Secondly, the QGMG solver is an example of an application that requires both task and data parallelism. In this application, two multigrids (task parallelism) must be solved and each multigrid solver contains data-parallelism. To address the first challenge, DUDE loosens the requirement that all processes must wait at barriers. In fact we have been able to eliminate barriers in critical loops. Much like in dataflow computations, processes may go on to the next operations when the data dependencies are satisfied. This is done by providing a dependence vector to the run-time system. To address the second problem, we have built a run-time system that provides integrated task parallelism and data parallelism. This combined approach is achieved by using object parallelism. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. Agerwala and Arvind. </author> <title> Data flow systems. </title> <journal> IEEE Computer, </journal> <volume> 15(2) </volume> <pages> 10-13, </pages> <month> Feb </month> <year> 1982. </year>
Reference-contexts: This distributes synchronization overhead and provides a very flexible scheduling construct. We call our runtime system the Definition-Use Description Environment, or DUDE, and it is currently implemented as a layer on top of the existing AWESIME threads library [9]. Normally, dataflow execution models have been associated with dataflow processors <ref> [1, 13, 13] </ref>, but the macro-dataflow model has been implemented in software as well [2, 15]. Often, as in the case of MENTAT, an entire language is designed around the macro-dataflow approach.
Reference: [2] <author> Robert Babb. </author> <title> Parallel processing with large-grain data flow techniques. </title> <journal> IEEE Computer, </journal> <volume> ??(??):55-61, </volume> <month> July </month> <year> 1984. </year>
Reference-contexts: Normally, dataflow execution models have been associated with dataflow processors [1, 13, 13], but the macro-dataflow model has been implemented in software as well <ref> [2, 15] </ref>. Often, as in the case of MENTAT, an entire language is designed around the macro-dataflow approach. By comparison, we simply use the macro dataflow notions to provide a description of the dependence relations in a program.
Reference: [3] <author> Vasanth Balasundaram. </author> <title> A mechanism for keeping useful internal information in parallel programming tools: The data access descriptor. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 9 </volume> <pages> 151-170, </pages> <year> 1990. </year>
Reference-contexts: First, fixed size chunks simplify maintaining the dependence information, and make that process more efficient. Allowing variable size chunks imply a less efficient data descriptor that take a range of values instead of indices. We initially implemented such a structure, similar to the Data Access Descriptor <ref> [3] </ref>, but found it was too slow in practice. Secondly, and more importantly, fixed size chunks allows the scheduler to establish an affinity between a chunk and the processor improving data locality. Each chunk has a preferred processor when it is rescheduled on the next iteration of the loop.
Reference: [4] <author> A. Brandt. </author> <title> Eliptic problem solvers. </title> <publisher> Academic Press, </publisher> <address> New York, NY, </address> <year> 1981. </year>
Reference-contexts: We employ grid-coarsening only in the horizontal directions, using line Gauss-Seidel relaxation in the vertical. The Multigrid technique has received much attention due to it's importance and the challenge of parallelizing the algorithm MG <ref> [4] </ref> [16] [6] [8]. Multigrid algorithms are used to accelerate the convergence of relaxation methods like Gauss-Seidel for the numerical solution of partial differential equations. They achieve this by using a hierarchy of coarser grids with larger spacings to provide corrections to the approximate solution obtained on the finest grid.
Reference: [5] <author> Jeffrey B. Weiss Clive F. Baillie, James C. McWilliams and Irad Yavneh. </author> <title> Parallel superconvergent multigrid. </title> <note> In submitted to Supercomputing '95., </note> <year> 1995. </year>
Reference: [6] <author> D. Gannon and J. van Rosendale. . J. </author> <booktitle> Parallel Distributed Computing, </booktitle> <volume> 3 </volume> <pages> 106-135, </pages> <year> 1986. </year>
Reference-contexts: We employ grid-coarsening only in the horizontal directions, using line Gauss-Seidel relaxation in the vertical. The Multigrid technique has received much attention due to it's importance and the challenge of parallelizing the algorithm MG [4] [16] <ref> [6] </ref> [8]. Multigrid algorithms are used to accelerate the convergence of relaxation methods like Gauss-Seidel for the numerical solution of partial differential equations. They achieve this by using a hierarchy of coarser grids with larger spacings to provide corrections to the approximate solution obtained on the finest grid.
Reference: [7] <author> Derek L. Eager and John Zahorjan. Chores: </author> <title> Enhanced run-time support fo shared memory parallel computing. </title> <journal> ACM. Trans on Computer Systems, </journal> <volume> 11(1) </volume> <pages> 1-32, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: The QGMG program must solve two multigrid problems to advance a single time-step. A traditional runtime system, or even advanced systems such as the Chores model <ref> [7] </ref> must sequentially schedule the computation in each doall or loop nesting. By allowing all operations to be evaluated in parallel, we increase the scheduling opportunities, allowing the runtime system to select a better schedule.
Reference: [8] <author> P. Frederickson and O. McBryan. </author> <title> Parallel superconvergent multigrid. </title> <booktitle> In Proceedings of the Third Copper Muntain Conference on Multigird Methods. </booktitle> <publisher> Marcel Dekker, </publisher> <year> 1989. </year>
Reference-contexts: We employ grid-coarsening only in the horizontal directions, using line Gauss-Seidel relaxation in the vertical. The Multigrid technique has received much attention due to it's importance and the challenge of parallelizing the algorithm MG [4] [16] [6] <ref> [8] </ref>. Multigrid algorithms are used to accelerate the convergence of relaxation methods like Gauss-Seidel for the numerical solution of partial differential equations. They achieve this by using a hierarchy of coarser grids with larger spacings to provide corrections to the approximate solution obtained on the finest grid.
Reference: [9] <author> Dirk Grunwald. </author> <title> A users guide to awesime: An object oriented parallel programming and simulation system. </title> <type> Technical Report CU-CS-552-91, </type> <institution> University of Colorado, Boulder, </institution> <year> 1991. </year>
Reference-contexts: This distributes synchronization overhead and provides a very flexible scheduling construct. We call our runtime system the Definition-Use Description Environment, or DUDE, and it is currently implemented as a layer on top of the existing AWESIME threads library <ref> [9] </ref>. Normally, dataflow execution models have been associated with dataflow processors [1, 13, 13], but the macro-dataflow model has been implemented in software as well [2, 15]. Often, as in the case of MENTAT, an entire language is designed around the macro-dataflow approach. <p> In Red-Black SOR for example, a point in the matrix is only dependent on its neighboring points, not the entire matrix. Figure 3 shows the possible flow of execution in many applications. 4 The Design of DUDE The DUDE runtime system is based on AWESIME <ref> [9] </ref> (A Widely Extensible Simulation Environment), an existing object-oriented runtime system for shared-address parallel architectures. The AWESIME library currently runs on workstations using Digital Alpha AXP, SPARC, Intel `x86', MIPS R3000 and Motorola 68K processors, as well as the KSR-1 massively parallel processor.
Reference: [10] <author> Dirk Grunwald and Suvas Vajracharya. </author> <title> Efficient barriers for distributed shared memory computers. </title> <booktitle> In Intl. Parallel Processing Symposium. IEEE, IEEE Computer Society, </booktitle> <month> April </month> <year> 1994. </year> <note> (to appear). </note>
Reference-contexts: The most common work-sharing mechanism uses a separate scheduler for each CpuMux, and CpuMux's steal from each other if they are idle. As another example of dynamic dispatch, users can select a barrier algorithm that is most appropriate to the architecture <ref> [10] </ref> or problem. The DUDE runtime system uses the abstraction and inheritance constructs of C++ to keep the scheduling policy, the underlying hardware, the type of objects being scheduling, the type of synchronization and other aspects of the system mutually orthogonal.
Reference: [11] <author> P. Keleher, S. Dwarkadas, A. Cox, and W. Zwaenepoel. Treadmarks: </author> <title> Distributed shared memory on standard workstations and operating systems. </title> <booktitle> In Winter Usenix Conference, </booktitle> <year> 1994. </year>
Reference-contexts: Our runtime system is designed for shared memory computers that may be connected using a message passing interface. We assume the shared memory computers have a pronounced memory 2 hierarchy; examples of such architectures are the KSR-1 [12] and distributed shared memory systems <ref> [11] </ref>. Compilers must target a specific machine model supported by the runtime system, and we feel the art of designing a runtime system is to provide an interface with the most generality that can be implemented efficiently across a number of systems.
Reference: [12] <institution> Kendall Square Research, </institution> <address> Boston, MA. </address> <booktitle> The KSR-1 System Architecture Manual, </booktitle> <month> April </month> <year> 1992. </year>
Reference-contexts: Our runtime system is designed for shared memory computers that may be connected using a message passing interface. We assume the shared memory computers have a pronounced memory 2 hierarchy; examples of such architectures are the KSR-1 <ref> [12] </ref> and distributed shared memory systems [11]. Compilers must target a specific machine model supported by the runtime system, and we feel the art of designing a runtime system is to provide an interface with the most generality that can be implemented efficiently across a number of systems.
Reference: [13] <author> Gregory M. Papadopoulos. </author> <title> Implementation of a General-Purpose Dataflow Multiprocessor. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, 02141, </address> <year> 1991. </year> <type> (1988 MIT Ph.D. Thesis, </type> <note> also published as MIT LCS TR 432). 14 </note>
Reference-contexts: This distributes synchronization overhead and provides a very flexible scheduling construct. We call our runtime system the Definition-Use Description Environment, or DUDE, and it is currently implemented as a layer on top of the existing AWESIME threads library [9]. Normally, dataflow execution models have been associated with dataflow processors <ref> [1, 13, 13] </ref>, but the macro-dataflow model has been implemented in software as well [2, 15]. Often, as in the case of MENTAT, an entire language is designed around the macro-dataflow approach.
Reference: [14] <author> G. Pfister and V. Norton. </author> <title> Hot spot contention and combining in multistage interconnection networks. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-34(10):943-948, </volume> <month> October </month> <year> 1985. </year>
Reference-contexts: Note that the dependence constraints also distribute the synchronization that occurs in the program. In distributed shared memory computers, such as the KSR-1, synchronization among a large number of processors causes particular cache lines to become hot-spots <ref> [14] </ref>.
Reference: [15] <author> Shankar Ramaswamy and Prithviraj Banerjee. </author> <title> Processor allocation and scheduling of macro dataflow graphs on distributed memory multicomputers by the paradigm compiler. </title> <booktitle> In Proceedings of the 1993 International Conference on Parallel Processing, volume II-Software, pages II-134-II-138. </booktitle> <publisher> CRC Press, </publisher> <month> August </month> <year> 1993. </year>
Reference-contexts: Normally, dataflow execution models have been associated with dataflow processors [1, 13, 13], but the macro-dataflow model has been implemented in software as well <ref> [2, 15] </ref>. Often, as in the case of MENTAT, an entire language is designed around the macro-dataflow approach. By comparison, we simply use the macro dataflow notions to provide a description of the dependence relations in a program.
Reference: [16] <author> S.N. Gupta, M. Zubair and C.E. </author> <title> Grosch. </title> . <journal> J. of Scientific Commput., </journal> <volume> 7 </volume> <pages> 263-279, </pages> <year> 1992. </year>
Reference-contexts: We employ grid-coarsening only in the horizontal directions, using line Gauss-Seidel relaxation in the vertical. The Multigrid technique has received much attention due to it's importance and the challenge of parallelizing the algorithm MG [4] <ref> [16] </ref> [6] [8]. Multigrid algorithms are used to accelerate the convergence of relaxation methods like Gauss-Seidel for the numerical solution of partial differential equations. They achieve this by using a hierarchy of coarser grids with larger spacings to provide corrections to the approximate solution obtained on the finest grid.
Reference: [17] <author> Irad Yavneh and James C. McWilliams. </author> <title> Multigrid solution of stably stratified flows: the quasi-geostrophic equations. </title> <note> In submitted to J. Sci. Comp., 1995. 15 </note>
References-found: 17

