URL: ftp://ftp.cs.unc.edu/pub/users/jeffay/papers/ACM-MM-94-VIDEO.ps.Z
Refering-URL: http://www.cs.unc.edu/Research/multimedia.html
Root-URL: http://www.cs.unc.edu
Email: -jeffay,stone-@cs.unc.edu  
Title: Figure 1: Demonstration of latency differential between analog (upper left) and digital (lower right) systems.
Author: Kevin Jeffay Donald L. Stone 
Address: Chapel Hill, NC 27599-3175  
Affiliation: University of North Carolina at Chapel Hill Department of Computer Science  
Note: In: Proc. of the Second ACM International Conference on Multimedia, San Francisco, CA, October 1994, ACM Press, pp. 487-488.  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Ferrari, D., </author> <year> 1990. </year> <title> Client Requirements for Real-Time Communication Services , IEEE Communications, </title> <month> (November), </month> <pages> pp. 65-72. </pages>
Reference-contexts: End-to-end latency is one of the most important performance parameters for a videoconferencing system as latency can severely impair and impede interaction between conference participants [2, 8]. At present there is some agreement that an end-to-end latency of no more than 250 ms. is acceptable <ref> [1] </ref>. In the best case, our system is capable of delivering synchronized audio and video streams with an end-to - end latency of approximately 170 ms.
Reference: [2] <author> Isaacs, E., Tang, </author> <title> J.C., What Video Can and Cant Do for Collaboration: A Case Study , Proc. </title> <booktitle> ACM Multimedia 1993, </booktitle> <pages> pp. 199-205. </pages>
Reference-contexts: The first illustrates the latency inherent in our video conferencing system. End-to-end latency is one of the most important performance parameters for a videoconferencing system as latency can severely impair and impede interaction between conference participants <ref> [2, 8] </ref>. At present there is some agreement that an end-to-end latency of no more than 250 ms. is acceptable [1]. In the best case, our system is capable of delivering synchronized audio and video streams with an end-to - end latency of approximately 170 ms.
Reference: [3] <author> Jeffay, K., Stone, D.L., and Smith, </author> <title> F.D., Transport and Display Mechanisms for Multimedia Conferencing Across PacketSwitched Networks, </title> <journal> Computer Networks and ISDN Systems, </journal> <volume> Vol. 26, No. </volume> <month> 10 (July </month> <year> 1994), </year> <pages> pp. 1281-1304. </pages>
Reference-contexts: Our protocol is a best effort protocol that attempts to ameliorate the effect of three basic phenomena: jitter, congestion, and packet loss, to provide low latency, synchronized audio and video communications <ref> [3] </ref>. This goal is realized through four transport and display mechanisms, and a real-time implementation of these mechanisms that integrates operating system services ( e.g., scheduling and resource allocation, and device management) with network communication services ( e.g., transport protocols), and with ap plication code (e.g. , display routines). <p> It takes approximately 170 ms. for a video frame to propagate from the camera to the display <ref> [3] </ref>. The second demonstration illustrates the effect of varying the synchronization between the audio and video streams. As described in [3], a useful technique for ameliorating the effect of network congestion is to purposely play audio and video out of exact synchronization; specifically, to play audio frames ahead (in time) of <p> It takes approximately 170 ms. for a video frame to propagate from the camera to the display <ref> [3] </ref>. The second demonstration illustrates the effect of varying the synchronization between the audio and video streams. As described in [3], a useful technique for ameliorating the effect of network congestion is to purposely play audio and video out of exact synchronization; specifically, to play audio frames ahead (in time) of their corresponding video frames. <p> A quantitative comparison of a similar set of experiments can be found in <ref> [3] </ref>. TECHNICAL DETAILS The workstations used in this videotape are IBM PS/2 (20 Mhz x386 processor) personal computers using IBM/Intel ActionMedia I audio/video adapters. We use an experimental real-time operating system kernel and video conferencing application we have developed. The kernel is described in [4, 6]; the application in [4]. <p> We use an experimental real-time operating system kernel and video conferencing application we have developed. The kernel is described in [4, 6]; the application in [4]. The adaptations used in the protocol for managing media streams are described in <ref> [3] </ref>. A more detailed description and analysis of the delay jitter management scheme used in this work is presented in [5]. The conferencing system generates 60 audio frames and 30 video frames per second. An average video frame is approximately 8000 bytes; an audio frame is approximately 250 bytes.
Reference: [4] <author> Jeffay, K., Stone, D.L., and Smith, </author> <title> F.D., Kernel Support for Live Digital Audio and Video, </title> <journal> Computer Communications, </journal> <volume> Vol. 16, No. </volume> <month> 6 (July </month> <year> 1992), </year> <pages> pp. 388-395. </pages>
Reference-contexts: TECHNICAL DETAILS The workstations used in this videotape are IBM PS/2 (20 Mhz x386 processor) personal computers using IBM/Intel ActionMedia I audio/video adapters. We use an experimental real-time operating system kernel and video conferencing application we have developed. The kernel is described in <ref> [4, 6] </ref>; the application in [4]. The adaptations used in the protocol for managing media streams are described in [3]. A more detailed description and analysis of the delay jitter management scheme used in this work is presented in [5]. <p> TECHNICAL DETAILS The workstations used in this videotape are IBM PS/2 (20 Mhz x386 processor) personal computers using IBM/Intel ActionMedia I audio/video adapters. We use an experimental real-time operating system kernel and video conferencing application we have developed. The kernel is described in [4, 6]; the application in <ref> [4] </ref>. The adaptations used in the protocol for managing media streams are described in [3]. A more detailed description and analysis of the delay jitter management scheme used in this work is presented in [5]. The conferencing system generates 60 audio frames and 30 video frames per second.
Reference: [5] <author> Stone, D.L., Jeffay, K., </author> <title> An Empirical Study of Delay Jitter Management Policies, </title> <journal> ACM Multimedia Systems, </journal> <note> to appear. </note>
Reference-contexts: The kernel is described in [4, 6]; the application in [4]. The adaptations used in the protocol for managing media streams are described in [3]. A more detailed description and analysis of the delay jitter management scheme used in this work is presented in <ref> [5] </ref>. The conferencing system generates 60 audio frames and 30 video frames per second. An average video frame is approximately 8000 bytes; an audio frame is approximately 250 bytes. This yields an aggregate data stream of approximately 2 Mb/s.
Reference: [6] <author> Jeffay, K., Stone, D.L., Poirier, D., YARTOS: </author> <title> Kernel support for efficient, predictable real-time systems, </title> <booktitle> Proc. Joint Eighth IEEE Workshop on Real-Time Operating Systems and Software and IFAC/IFIP Workshop on Real-Time Programming, </booktitle> <address> Atlanta, GA, </address> <booktitle> Real-Time Systems Newsletter, </booktitle> <volume> Vol. 7, No. 4, </volume> <month> Fall </month> <year> 1991, </year> <pages> pp. 8-13. </pages>
Reference-contexts: TECHNICAL DETAILS The workstations used in this videotape are IBM PS/2 (20 Mhz x386 processor) personal computers using IBM/Intel ActionMedia I audio/video adapters. We use an experimental real-time operating system kernel and video conferencing application we have developed. The kernel is described in <ref> [4, 6] </ref>; the application in [4]. The adaptations used in the protocol for managing media streams are described in [3]. A more detailed description and analysis of the delay jitter management scheme used in this work is presented in [5].
Reference: [7] <author> Steinmetz, R., Meyer, T., </author> <year> 1992. </year> <title> Multimedia Synchronization Techniques: Experiences Based on Different System Structures, </title> <booktitle> IEEE Multimedia Workshop, </booktitle> <address> Monterey, CA, </address> <month> April, </month> <year> 1992. </year>
Reference-contexts: Humans are therefore more tolerant of audio behind video. Our system assumes (somewhat arbitrarily although motivated by <ref> [7] </ref>) that users will tolerate a synchronization differential of at least 100 ms. The second demonstrations varies the degree to which audio is played ahead of video while a person is speaking and while a person claps (Figure 2).
Reference: [8] <author> Wolf, C., </author> <title> Video Conferencing: Delay and Transmission Considerations, in Teleconferencing and Electronic Communications: Applications, Technologies, and Human Factors, </title> <editor> L. Parker and C. Olgren (Eds.), </editor> <year> 1982. </year>
Reference-contexts: The first illustrates the latency inherent in our video conferencing system. End-to-end latency is one of the most important performance parameters for a videoconferencing system as latency can severely impair and impede interaction between conference participants <ref> [2, 8] </ref>. At present there is some agreement that an end-to-end latency of no more than 250 ms. is acceptable [1]. In the best case, our system is capable of delivering synchronized audio and video streams with an end-to - end latency of approximately 170 ms.
References-found: 8

