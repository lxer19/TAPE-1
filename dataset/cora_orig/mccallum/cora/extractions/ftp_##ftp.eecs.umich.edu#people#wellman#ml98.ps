URL: ftp://ftp.eecs.umich.edu/people/wellman/ml98.ps
Refering-URL: http://ai.eecs.umich.edu/people/wellman/Publications.html
Root-URL: http://www.cs.umich.edu
Title: Machine Learning,  Conjectural Equilibrium in Multiagent Learning  
Author: MICHAEL P. WELLMAN AND JUNLING HU fwellman, Editor: Michael Huhns and Gerhard Wei 
Keyword: multiagent learning, conjectural equilibrium, market game  
Address: Ann Arbor, MI 48109-2110  
Affiliation: University of Michigan,  
Note: c 1998 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  This article includes some material previously reported at ICMAS-96 [16].  
Pubnum: 33,  
Email: junlingg@umich.edu  
Date: 123 (1998)  Received February 1997; Revised July 1998  
Abstract: Learning in a multiagent environment is complicated by the fact that as other agents learn, the environment effectively changes. Moreover, other agents' actions are often not directly observable, and the actions taken by the learning agent can strongly bias which range of behaviors are encountered. We define the concept of a conjectural equilibrium, where all agents' expectations are realized, and each agent responds optimally to its expectations. We present a generic multiagent exchange situation, in which competitive behavior constitutes a conjectural equilibrium. We then introduce an agent that executes a more sophisticated strategic learning strategy, building a model of the response of other agents. We find that the system reliably converges to a conjectural equilibrium, but that the final result achieved is highly sensitive to initial belief. In essence, the strategic learner's actions tend to fulfill its expectations. Depending on the starting point, the agent may be better or worse off than had it not attempted to learn a model of the other agents at all. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> K. J. Arrow, H. B. Chenery, B. S. Minhas, and R. M. Solow. </author> <title> Capital-labor substitution and economic efficiency. </title> <journal> Review of Economics and Statistics, </journal> <volume> 43:225250, </volume> <year> 1961. </year>
Reference-contexts: Proof: Given the strict concavity of the objective function, and the existence of a solution (Theorem 5), it suffices to show that the constraint set S is convex. Let z 0 ; z 00 2 S, and z = z 0 + (1 )z 00 , where 2 <ref> [0; 1] </ref>. We need to show that z 2 S. <p> Cobb-Douglas utility is a limiting case of the CES form (constant elasticity of substitution), U (x 1 ; : : : ; x m ) = X a j x ! 1 ; with ! 0 <ref> [1] </ref>. CES is commonly used in general equilibrium modeling [32], including some of our prior work. We also performed experiments with CES agents ( = 1 2 , and a j = 1 for all j), with results qualitatively similar to those reported for the logarithmic case. 9.
Reference: 2. <author> Craig Boutilier. </author> <title> Learning conventions in multiagent stochastic domains using likelihood estimates. </title> <booktitle> In Proceedings of the Twelfth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 106114, </pages> <address> Portland, OR, </address> <year> 1996. </year>
Reference-contexts: Most models in the literature assume that agents observe the joint action, as well as the resulting state. Our framework allows unobservable actions, and in the market game studied in depth, agents can reconstruct only an aggregate of other agents' actions. Boutilier <ref> [2] </ref> also considers a model where only outcomes are observable, demonstrating how to adapt some of the methods for the observable-action case to this setting. Interestingly, he finds that in some circumstances, uncertainty about other agents' actions actually speeds up the convergence to equilibrium for simple coordination games.
Reference: 3. <author> Kevin Boyle. </author> <title> Starting point bias in contingent valuation bidding games. Land Economics, </title> <address> 61:188194, </address> <year> 1985. </year>
Reference-contexts: Researchers typically explore repeated games (especially coordination games), and have tended to find some sort of convergence to coordinated, equilibrium, or near-equilibrium behavior [6, 12, 31]. Economists studying bidding games <ref> [3, 27] </ref> have noticed that biased starting bid prices strongly influence final bids. More generally, researchers have observed that the results of learning or evolution in games are often path-dependent [41], with selection among multiple equilibria varying according to initial or transient conditions.
Reference: 4. <author> Adam Brandenburger. </author> <title> Knowledge and equilibrium in games. </title> <journal> Journal of Economic Perspectives, </journal> <volume> 6(4):83 101, </volume> <year> 1992. </year>
Reference-contexts: Conjectural Equilibrium In game-theoretic analysis, conclusions about equilibria reached are based on assumptions about what knowledge the agents have. For example, choice of iterated undominated strategies follows from common knowledge of rationality and the game setup <ref> [4] </ref>. In the standard game-theoretic model of complete information [10, 11], the joint payoff matrix is known to every agent. Uncertainty can be accommodated in the game-theoretic concept of incomplete information, where agents have probabilities over the payoffs of other agents.
Reference: 5. <author> John Q. Cheng and Michael P. Wellman. </author> <title> The WALRAS algorithm: A convergent distributed implementation of general equilibrium outcomes. </title> <booktitle> Computational Economics, </booktitle> <address> 12(1):124, </address> <year> 1998. </year>
Reference-contexts: If the aggregate demand obeys gross substitutability (an increase in the price of one good raises demand for others, which hence serve as substitutes), then this method is guaranteed to converge to a competitive equilibrium (under the conditions under which it is guaranteed to exist) [24]. The walras algorithm <ref> [5] </ref> is a variant of tatonnement. In walras, agent i submits to the auction for good j at time t its solution to (2), expressed as a function of P j , assuming that the prices of goods other than j take their expected values. <p> Given the bidding behavior described, with expectations formed as by the simple competitive agent, the walras algorithm is guaranteed to converge to competitive equilibrium, under the standard conditions <ref> [5] </ref>. Such an equilibrium also represents a conjectural equilibrium, according to the definition above. Thus, the simple competitive learning regime is convergent, with respect to both the tatonnement and walras price adjustment protocols. 8 M. P. WELLMAN AND J. HU 4.
Reference: 6. <author> Caroline Claus and Craig Boutilier. </author> <title> The dynamics of reinforcement learning in cooperative multiagent systems. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <address> Madison, WI, </address> <year> 1998. </year>
Reference-contexts: The basic idea of reinforcement learning is to revise beliefs and policies based on the success or failure of observed performance [19]. The complication in a multiagent environment is that the rewards to alternative policies may change as other agents' beliefs evolve simultaneously <ref> [6, 25] </ref>. 2. Conjectural Equilibrium In game-theoretic analysis, conclusions about equilibria reached are based on assumptions about what knowledge the agents have. For example, choice of iterated undominated strategies follows from common knowledge of rationality and the game setup [4]. <p> Numerous studies have investigated the behavior of simple learning policies such as Bayesian update or fictitious play, or selection schemes inspired by evolutionary models. Researchers typically explore repeated games (especially coordination games), and have tended to find some sort of convergence to coordinated, equilibrium, or near-equilibrium behavior <ref> [6, 12, 31] </ref>. Economists studying bidding games [3, 27] have noticed that biased starting bid prices strongly influence final bids. More generally, researchers have observed that the results of learning or evolution in games are often path-dependent [41], with selection among multiple equilibria varying according to initial or transient conditions.
Reference: 7. <author> Dave Cliff. </author> <title> Evolving parameter sets for adaptive trading agents in continuous double-auction markets. </title> <booktitle> In Agents-98 Workshop on Artificial Societies and Computational Markets, </booktitle> <pages> pages 3847, </pages> <address> Minneapolis, MN, </address> <month> May </month> <year> 1998. </year>
Reference-contexts: Interestingly, he finds that in some circumstances, uncertainty about other agents' actions actually speeds up the convergence to equilibrium for simple coordination games. The last five years has seen some study of learning methods for agents participating in simple exchange markets. (Cliff's recent contribution <ref> [7] </ref> includes a substantial bibliography.) Some of this work directly compares the effectiveness of learning strategic policies with competitive strategies. Vidal and Durfee examine a particular model of agents exchanging information goods [37], and find that whether strategic learning is beneficial (or how much) is highly context-dependent.
Reference: 8. <author> Jerzy Filar and Koos Vrieze. </author> <title> Competitive Markov Decision Processes. </title> <publisher> Springer-Verlag, </publisher> <year> 1997. </year>
Reference-contexts: A more sophisticated version of this model would have agents form probabilistic conjectures about the effects of actions, and act to maximize expected utility. 4. Investigations of multiagent learning within the Markov game framework brings state dynamics to the fore <ref> [8, 17, 21] </ref>. 5. The relationship between basic computational resources and results of computation can be modeled explicitly by extending the exchange economy to include production. See our prior work for detailed examples of general-equilibrium models of computational problems [23, 39, 40]. 6.
Reference: 9. <author> Drew Fudenberg and David K. Levine. Self-confirming equilibrium. Econometrica, 61:523545, </author> <year> 1993. </year>
Reference-contexts: Nash equilibria are trivially conjectural equilibria where the conjectures are consistent with the equilibrium play of other agents. As we see below, competitive, or Walrasian, equilibria are also conjectural equilibria. The concept of self-confirming equilibrium <ref> [9] </ref> is another relaxation of Nash equilibrium which applies to a situation where no agent ever observes actions of other agents contradicting its beliefs. Conjectures are on the play of other agents, and must be correct for all reachable information sets. This is stronger than conjectural equilibrium in two respects.
Reference: 10. <author> Drew Fudenberg and Jean Tirole. </author> <title> Game Theory. </title> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: Conjectural Equilibrium In game-theoretic analysis, conclusions about equilibria reached are based on assumptions about what knowledge the agents have. For example, choice of iterated undominated strategies follows from common knowledge of rationality and the game setup [4]. In the standard game-theoretic model of complete information <ref> [10, 11] </ref>, the joint payoff matrix is known to every agent. Uncertainty can be accommodated in the game-theoretic concept of incomplete information, where agents have probabilities over the payoffs of other agents. A learning model is an account of how agents form such beliefs.
Reference: 11. <author> Robert Gibbons. </author> <title> Game Theory for Applied Economists. </title> <publisher> Princeton University Press, </publisher> <year> 1992. </year>
Reference-contexts: Conjectural Equilibrium In game-theoretic analysis, conclusions about equilibria reached are based on assumptions about what knowledge the agents have. For example, choice of iterated undominated strategies follows from common knowledge of rationality and the game setup [4]. In the standard game-theoretic model of complete information <ref> [10, 11] </ref>, the joint payoff matrix is known to every agent. Uncertainty can be accommodated in the game-theoretic concept of incomplete information, where agents have probabilities over the payoffs of other agents. A learning model is an account of how agents form such beliefs.
Reference: 12. <author> Itzhak Gilboa and Matsui Akihiko. </author> <title> Social stability and equilibrium. </title> <journal> Econometrica, </journal> <volume> 59:859867, </volume> <year> 1991. </year>
Reference-contexts: Numerous studies have investigated the behavior of simple learning policies such as Bayesian update or fictitious play, or selection schemes inspired by evolutionary models. Researchers typically explore repeated games (especially coordination games), and have tended to find some sort of convergence to coordinated, equilibrium, or near-equilibrium behavior <ref> [6, 12, 31] </ref>. Economists studying bidding games [3, 27] have noticed that biased starting bid prices strongly influence final bids. More generally, researchers have observed that the results of learning or evolution in games are often path-dependent [41], with selection among multiple equilibria varying according to initial or transient conditions.
Reference: 13. <editor> John J. Grefenstette et al., editors. </editor> <booktitle> AAAI Spring Symposium on Adaptation, Coevolution, and Learning in Multiagent Systems. </booktitle> <publisher> AAAI Press, </publisher> <year> 1996. </year>
Reference-contexts: 1. Introduction Machine learning researchers have recently begun to investigate the special issues that multiagent environments present to the learning task. Contributions in this journal issue, along with recent workshops on the topic <ref> [13, 29, 30] </ref>, have helped to frame research problems for the field. Multiagent environments are distinguished in particular by the fact that as the agents learn, they change their behavior, thus effectively changing the environment for all of the other agents.
Reference: 14. <author> Frank H. Hahn. </author> <title> Exercises in conjectural equilibrium analysis. </title> <journal> Scandinavian Journal of Economics, </journal> <volume> 79:210 226, </volume> <year> 1977. </year>
Reference-contexts: Second, it takes individual actions of other agents as observable, whereas in our framework the agents observe only resulting state. The basic concept of conjectural equilibrium was first introduced by Hahn, in the context of a market model <ref> [14] </ref>. Though we also focus on market interactions, our central definition applies the concept to the more general case. Hahn also included a specific model for conjecture formation in the equilibrium concept, whereas we relegate this process to the learning regime of participating agents. CONJECTURAL EQUILIBRIUM 5 3. <p> P. WELLMAN AND J. HU 6.1. Market Conjectural Equilibrium Our experimental analysis considered agents whose conjectures were either constant (competitive) or linear (strategic) functions of their actions. Using Hahn's notion of a conjecture function <ref> [14] </ref>, we provide some more general notation for characterizing the form of an agent's conjectures. Definition 3.
Reference: 15. <author> Reiner Horst, Panos Pardalos, and Nguyen Thoai. </author> <title> Introduction to Global Optimization. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1995. </year>
Reference-contexts: Then there exists a solution to the strategic agent's optimization problem (7): max U (z + e) s.t. (ff + fiz) z 0: (A.1) Proof: To establish the existence of an optimum, we apply Weierstrass's Maximum Theorem <ref> [15] </ref>: if S is a nonempty compact set in &lt; m , and f (x) is a continuous function on S, then f (x) has at least one global optimum point in S.
Reference: 16. <author> Junling Hu and Michael P. Wellman. </author> <title> Self-fulfilling bias in multiagent learning. </title> <booktitle> In Second International Conference on Multiagent Systems, </booktitle> <pages> pages 118125, </pages> <address> Kyoto, Japan, </address> <year> 1996. </year>
Reference: 17. <author> Junling Hu and Michael P. Wellman. </author> <title> Multiagent reinforcement learning: Theoretical framework and an algorithm. </title> <booktitle> In Fifteenth International Conference on Machine Learning, </booktitle> <address> Madison, WI, </address> <year> 1998. </year>
Reference-contexts: A more sophisticated version of this model would have agents form probabilistic conjectures about the effects of actions, and act to maximize expected utility. 4. Investigations of multiagent learning within the Markov game framework brings state dynamics to the fore <ref> [8, 17, 21] </ref>. 5. The relationship between basic computational resources and results of computation can be modeled explicitly by extending the exchange economy to include production. See our prior work for detailed examples of general-equilibrium models of computational problems [23, 39, 40]. 6.
Reference: 18. <author> Junling Hu and Michael P. Wellman. </author> <title> Online learning about other agents in a dynamic multiagent system. </title> <booktitle> In Second International Conference on Autonomous Agents, </booktitle> <pages> pages 239246, </pages> <address> Minneapolis, </address> <year> 1998. </year>
Reference-contexts: Vidal and Durfee examine a particular model of agents exchanging information goods [37], and find that whether strategic learning is beneficial (or how much) is highly context-dependent. We provide further data distinguishing the cases in our recent experiments within a dynamic trading model <ref> [18] </ref>. Finally, Sandholm and Ygge [28] investigate a general-equilibrium scenario very similar to ours. Like us, they find that strategic behavior can be counterproductive when agents have incorrect models. <p> The question of which hypothesis space to adopt for multiagent learning problems is an interesting current research issue. Our investigations to date suggest that the appropriate form of target model can be highly problem specific, depending on observations available, and relative sophistication of other agents <ref> [18] </ref>. We formulate our conjectural equilibrium concept in 0-level terms, to which higher levels can be reduced. 3. A more sophisticated version of this model would have agents form probabilistic conjectures about the effects of actions, and act to maximize expected utility. 4.
Reference: 19. <author> Leslie Pack Kaelbling, Michael L. Littman, and Andrew W. Moore. </author> <title> Reinforcement learning: A survey. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4:237285, </volume> <year> 1996. </year>
Reference-contexts: Much early work on multiagent learning has investigated some form of reinforcement learning (e.g., [35, 38]). The basic idea of reinforcement learning is to revise beliefs and policies based on the success or failure of observed performance <ref> [19] </ref>. The complication in a multiagent environment is that the rewards to alternative policies may change as other agents' beliefs evolve simultaneously [6, 25]. 2. Conjectural Equilibrium In game-theoretic analysis, conclusions about equilibria reached are based on assumptions about what knowledge the agents have.
Reference: 20. <author> J. O. Kephart, T. Hogg, and B. A. Huberman. </author> <title> Dynamics of computational ecosystems. Physical Review A, </title> <address> 40:404421, </address> <year> 1989. </year>
Reference-contexts: For example, the strategic agent's demand could exceed total endowments. For our example case of uniformly weighted logarithmic (Cobb-Douglas) utility, any demand exceeding (m 1)=m times the total endowment of the competitive agents for any good is infeasible. 14. Kephart et al. <ref> [20] </ref> describe another setting where sophisticated agents that try to anticipate the actions of others often make results worse for themselves. In this model, the sophisticated agents' downfall is their failure to account properly for simultaneous adaptation by the other agents.
Reference: 21. <author> Michael L. Littman. </author> <title> Markov games as a framework for multi-agent reinforcement learning. </title> <booktitle> In Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 157163, </pages> <year> 1994. </year>
Reference-contexts: A more sophisticated version of this model would have agents form probabilistic conjectures about the effects of actions, and act to maximize expected utility. 4. Investigations of multiagent learning within the Markov game framework brings state dynamics to the fore <ref> [8, 17, 21] </ref>. 5. The relationship between basic computational resources and results of computation can be modeled explicitly by extending the exchange economy to include production. See our prior work for detailed examples of general-equilibrium models of computational problems [23, 39, 40]. 6.
Reference: 22. <author> Paul Milgrom and John Roberts. </author> <title> Adaptive and sophisticated learning in normal form games. Games and Economic Behavior, </title> <address> 3:82100, </address> <year> 1991. </year> <note> CONJECTURAL EQUILIBRIUM 23 </note>
Reference-contexts: An agent behaving within an equilibrium is often explained in terms of the agent's beliefs about the types or policies of other agents. How agents reach such beliefs through repeated interactions is what game theorists mean by learning <ref> [22] </ref>, and that is the sense of the term we adopt as well.
Reference: 23. <author> Tracy Mullen and Michael P. Wellman. </author> <title> A simple computational market for network information services. </title> <booktitle> In First International Conference on Multiagent Systems, </booktitle> <pages> pages 283289, </pages> <address> San Francisco, CA, </address> <year> 1995. </year>
Reference-contexts: The relationship between basic computational resources and results of computation can be modeled explicitly by extending the exchange economy to include production. See our prior work for detailed examples of general-equilibrium models of computational problems <ref> [23, 39, 40] </ref>. 6. It is possible to express somewhat more general sufficient conditions in terms of underlying preference orders, but the direct utility conditions are adequate for our purposes. 7. In the standard model, no exchanges are executed until the system reaches equilibrium.
Reference: 24. <author> Takashi Negishi. </author> <title> The stability of a competitive economy: A survey article. </title> <journal> Econometrica, </journal> <volume> 30:635669, </volume> <year> 1962. </year>
Reference-contexts: If the aggregate demand obeys gross substitutability (an increase in the price of one good raises demand for others, which hence serve as substitutes), then this method is guaranteed to converge to a competitive equilibrium (under the conditions under which it is guaranteed to exist) <ref> [24] </ref>. The walras algorithm [5] is a variant of tatonnement. In walras, agent i submits to the auction for good j at time t its solution to (2), expressed as a function of P j , assuming that the prices of goods other than j take their expected values.
Reference: 25. <author> Norihiko Ono and Kenji Fukumoto. </author> <title> Multi-agent reinforcement learning: A modular approach. </title> <booktitle> In Second International Conference on Multiagent Systems, </booktitle> <pages> pages 252258, </pages> <address> Kyoto, Japan, </address> <year> 1996. </year>
Reference-contexts: The basic idea of reinforcement learning is to revise beliefs and policies based on the success or failure of observed performance [19]. The complication in a multiagent environment is that the rewards to alternative policies may change as other agents' beliefs evolve simultaneously <ref> [6, 25] </ref>. 2. Conjectural Equilibrium In game-theoretic analysis, conclusions about equilibria reached are based on assumptions about what knowledge the agents have. For example, choice of iterated undominated strategies follows from common knowledge of rationality and the game setup [4].
Reference: 26. <author> Stuart Russell and Peter Norvig. </author> <title> Artificial Intelligence: A Modern Approach. </title> <publisher> Prentice Hall, </publisher> <year> 1995. </year>
Reference-contexts: Here bias is defined as in the standard machine learning literaturethe preference for one hypothesis over another, beyond mere consistency with the examples <ref> [26] </ref>. In reinforcement learning, the initial hypothesis is a source of bias, as is the hypothesis space (in multiagent environments, expressible models of the other agents).
Reference: 27. <author> Karl Samples. </author> <title> A note on the existence of starting point bias in iterative bidding games. </title> <journal> Western Journal of Agricultural Economics, </journal> <volume> 10:3240, </volume> <year> 1985. </year>
Reference-contexts: Researchers typically explore repeated games (especially coordination games), and have tended to find some sort of convergence to coordinated, equilibrium, or near-equilibrium behavior [6, 12, 31]. Economists studying bidding games <ref> [3, 27] </ref> have noticed that biased starting bid prices strongly influence final bids. More generally, researchers have observed that the results of learning or evolution in games are often path-dependent [41], with selection among multiple equilibria varying according to initial or transient conditions.
Reference: 28. <author> Tuomas Sandholm and Fredrik Ygge. </author> <title> On the gains and losses of speculation in equilibrium markets. </title> <booktitle> In Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 632638, </pages> <address> Nagoya, Japan, </address> <year> 1997. </year>
Reference-contexts: Vidal and Durfee examine a particular model of agents exchanging information goods [37], and find that whether strategic learning is beneficial (or how much) is highly context-dependent. We provide further data distinguishing the cases in our recent experiments within a dynamic trading model [18]. Finally, Sandholm and Ygge <ref> [28] </ref> investigate a general-equilibrium scenario very similar to ours. Like us, they find that strategic behavior can be counterproductive when agents have incorrect models.
Reference: 29. <author> Sandip Sen. </author> <booktitle> IJCAI-95 Workshop on Adaptation and Learning in Multiagent Systems. AI Magazine, </booktitle> <address> 17(1):8789, </address> <year> 1996. </year>
Reference-contexts: 1. Introduction Machine learning researchers have recently begun to investigate the special issues that multiagent environments present to the learning task. Contributions in this journal issue, along with recent workshops on the topic <ref> [13, 29, 30] </ref>, have helped to frame research problems for the field. Multiagent environments are distinguished in particular by the fact that as the agents learn, they change their behavior, thus effectively changing the environment for all of the other agents.
Reference: 30. <editor> Sandip Sen, editor. </editor> <booktitle> AAAI-97 Workshop on Multiagent Learning. </booktitle> <publisher> AAAI Press, </publisher> <year> 1997. </year>
Reference-contexts: 1. Introduction Machine learning researchers have recently begun to investigate the special issues that multiagent environments present to the learning task. Contributions in this journal issue, along with recent workshops on the topic <ref> [13, 29, 30] </ref>, have helped to frame research problems for the field. Multiagent environments are distinguished in particular by the fact that as the agents learn, they change their behavior, thus effectively changing the environment for all of the other agents.
Reference: 31. <author> Yoav Shoham and Moshe Tennenholtz. </author> <title> On the emergence of social conventions: Modeling, analysis, and simulations. </title> <journal> Artificial Intelligence, </journal> <volume> 94:139166, </volume> <year> 1997. </year>
Reference-contexts: Numerous studies have investigated the behavior of simple learning policies such as Bayesian update or fictitious play, or selection schemes inspired by evolutionary models. Researchers typically explore repeated games (especially coordination games), and have tended to find some sort of convergence to coordinated, equilibrium, or near-equilibrium behavior <ref> [6, 12, 31] </ref>. Economists studying bidding games [3, 27] have noticed that biased starting bid prices strongly influence final bids. More generally, researchers have observed that the results of learning or evolution in games are often path-dependent [41], with selection among multiple equilibria varying according to initial or transient conditions.
Reference: 32. <author> John B. Shoven and John Whalley. </author> <title> Applying General Equilibrium. </title> <publisher> Cambridge University Press, </publisher> <year> 1992. </year>
Reference-contexts: Cobb-Douglas utility is a limiting case of the CES form (constant elasticity of substitution), U (x 1 ; : : : ; x m ) = X a j x ! 1 ; with ! 0 [1]. CES is commonly used in general equilibrium modeling <ref> [32] </ref>, including some of our prior work. We also performed experiments with CES agents ( = 1 2 , and a j = 1 for all j), with results qualitatively similar to those reported for the logarithmic case. 9.
Reference: 33. <author> Brandeis Spivak. </author> <title> Calculus on Manifolds. </title> <address> Benjamin/Cummings, </address> <year> 1965. </year>
Reference-contexts: The conditions also ensure the existence of a competitive equilibrium P fl , and therefore there is a point (P fl ; (P fl ; 0)) such that F (P fl ; (P fl ; 0)) = 0. Then by the Implicit Function Theorem <ref> [33] </ref>, there exists an open set P containing P fl and an open set B containing (P fl ; 0) such that for each P 2 P, there is a unique g (P ) 2 B such that F (P; g (P )) = 0.
Reference: 34. <author> A. Takayama. </author> <title> Mathematical Economics. </title> <publisher> Cambridge University Press, </publisher> <year> 1985. </year>
Reference-contexts: The excess demand set for consumer i is Z i = fz i 2 &lt; m j e i + z i 2 X i g. A basic result of general equilibrium theory <ref> [34] </ref> states that if the utility function of every agent is quasiconcave and twice differentiable, then E has a unique competitive equilibrium. 6 Observe that any competitive equilibrium can be viewed as a conjectural equilibrium, for an appropriate interpretation of conjectures. <p> It is possible to express somewhat more general sufficient conditions in terms of underlying preference orders, but the direct utility conditions are adequate for our purposes. 7. In the standard model, no exchanges are executed until the system reaches equilibrium. In so-called non-tatonnement processes <ref> [34] </ref>, agents can trade at any time, and so the endowment e is also a function of time. In either formulation, we still assume that agents are myopic, optimizing only with respect to the current time period. 8.
Reference: 35. <author> Ming Tan. </author> <title> Multi-agent reinforcement learning: Independent vs. </title> <booktitle> cooperative agents. In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <address> Amherst, MA, June 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The combination of a limited modeling language (in our experiments, linear demand functions) with an arbitrarily assigned initial hypothesis strongly influences the equilibrium state reached by the multiagent system. Much early work on multiagent learning has investigated some form of reinforcement learning (e.g., <ref> [35, 38] </ref>). The basic idea of reinforcement learning is to revise beliefs and policies based on the success or failure of observed performance [19]. The complication in a multiagent environment is that the rewards to alternative policies may change as other agents' beliefs evolve simultaneously [6, 25]. 2.
Reference: 36. <author> Jos e M. Vidal and Edmund H. Durfee. </author> <title> Agents learning about agents: A framework and analysis. </title> <note> In Sen [30]. </note>
Reference-contexts: We take no position, except to argue that any study that purports to characterize a learning process must clearly define this line, as does the framework proposed here. 2. Elsewhere, following Vidal and Durfee <ref> [36, 37] </ref>, we have distinguished between 0-level learning agents, which form models of the effects of their own actions, and 1-level learning agents, which form models of other agents (as 0-level agents). Recursive application defines higher levels.
Reference: 37. <author> Jos e M. Vidal and Edmund H. Durfee. </author> <title> Learning nested agent models in an information economy. </title> <journal> Journal of Experimental and Theoretical Artificial Intelligence, </journal> <volume> 10(3):291308, </volume> <year> 1998. </year>
Reference-contexts: Vidal and Durfee examine a particular model of agents exchanging information goods <ref> [37] </ref>, and find that whether strategic learning is beneficial (or how much) is highly context-dependent. We provide further data distinguishing the cases in our recent experiments within a dynamic trading model [18]. Finally, Sandholm and Ygge [28] investigate a general-equilibrium scenario very similar to ours. <p> We take no position, except to argue that any study that purports to characterize a learning process must clearly define this line, as does the framework proposed here. 2. Elsewhere, following Vidal and Durfee <ref> [36, 37] </ref>, we have distinguished between 0-level learning agents, which form models of the effects of their own actions, and 1-level learning agents, which form models of other agents (as 0-level agents). Recursive application defines higher levels.
Reference: 38. <author> Gerhard Wei. </author> <title> Learning to coordinate actions in multi-agent systems. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 311316, </pages> <year> 1993. </year>
Reference-contexts: The combination of a limited modeling language (in our experiments, linear demand functions) with an arbitrarily assigned initial hypothesis strongly influences the equilibrium state reached by the multiagent system. Much early work on multiagent learning has investigated some form of reinforcement learning (e.g., <ref> [35, 38] </ref>). The basic idea of reinforcement learning is to revise beliefs and policies based on the success or failure of observed performance [19]. The complication in a multiagent environment is that the rewards to alternative policies may change as other agents' beliefs evolve simultaneously [6, 25]. 2.
Reference: 39. <author> Michael P. Wellman. </author> <title> A market-oriented programming environment and its application to distributed multicommodity flow problems. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 1:123, </volume> <year> 1993. </year>
Reference-contexts: The market context is generic enough to capture a wide range of interesting multiagent systems, yet affords analytically simple characterizations of conjectures and dynamics. Our model is based on the framework of general equilibrium theory from economics, and our implementation uses the walras market-oriented programming system <ref> [39] </ref>, which is also based on general equilibrium theory. 3.1. General Equilibrium Model Definition 2. <p> The relationship between basic computational resources and results of computation can be modeled explicitly by extending the exchange economy to include production. See our prior work for detailed examples of general-equilibrium models of computational problems <ref> [23, 39, 40] </ref>. 6. It is possible to express somewhat more general sufficient conditions in terms of underlying preference orders, but the direct utility conditions are adequate for our purposes. 7. In the standard model, no exchanges are executed until the system reaches equilibrium.
Reference: 40. <author> Michael P. Wellman. </author> <title> A computational market model for distributed configuration design. </title> <booktitle> AI EDAM, </booktitle> <address> 9:125133, </address> <year> 1995. </year>
Reference-contexts: The relationship between basic computational resources and results of computation can be modeled explicitly by extending the exchange economy to include production. See our prior work for detailed examples of general-equilibrium models of computational problems <ref> [23, 39, 40] </ref>. 6. It is possible to express somewhat more general sufficient conditions in terms of underlying preference orders, but the direct utility conditions are adequate for our purposes. 7. In the standard model, no exchanges are executed until the system reaches equilibrium.
Reference: 41. <author> H. Peyton Young. </author> <title> The economics of convention. </title> <journal> Journal of Economic Perspectives, </journal> <volume> 10(2):105122, </volume> <year> 1996. </year>
Reference-contexts: Economists studying bidding games [3, 27] have noticed that biased starting bid prices strongly influence final bids. More generally, researchers have observed that the results of learning or evolution in games are often path-dependent <ref> [41] </ref>, with selection among multiple equilibria varying according to initial or transient conditions. Most models in the literature assume that agents observe the joint action, as well as the resulting state.
References-found: 41

