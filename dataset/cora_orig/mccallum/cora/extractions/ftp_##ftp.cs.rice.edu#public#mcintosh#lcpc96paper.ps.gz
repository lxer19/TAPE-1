URL: ftp://ftp.cs.rice.edu/public/mcintosh/lcpc96paper.ps.gz
Refering-URL: http://www.cs.rice.edu:80/~mcintosh/research.html
Root-URL: 
Title: Cross-loop Reuse Analysis and its Application to Cache Optimizations  
Author: Keith Cooper, Ken Kennedy, and Nathaniel McIntosh 
Address: Houston, Texas USA  
Affiliation: Department of Computer Science Rice University  
Abstract: In this paper we describe the design of a data-flow framework for detecting cross-loop reuse. Cross-loop reuse takes place when a set of data items or cache lines is accessed in a given loop nest and then accessed again within some subsequent portion of the program, usually another outer loop nest. In contrast to intra-loop reuse, which occurs during the execution of a single loop nest, cross-loop reuse is hard to analyze using traditional dependence-based techniques. The framework we have constructed is based on a combination of array section analysis (to capture array access patterns at a high level) and data-flow analysis (to deal with intra-procedural control flow). The framework is designed to account for cache size when gathering reuse information, and when used in an interprocedural setting, the framework also provides a mechanism for summarizing the effects of procedure calls. Cross-loop reuse information can be used to drive a number of transformations that enhance locality and improve cache utilization, including loop fusion and loop reversal. Although these transformations are not new, their impact on cache behavior has not always been possible to predict, making them difficult to apply. As part of this paper we report the results of a comprehensive experimental study in which we apply our techniques to a set of ten programs from the SPEC95 floating point benchmark suite. We were able to obtain modest performance gains overall for several of the programs, based mostly on improvements in cache utilization.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> B. Appelbe and B. Lakshmanan. </author> <title> Program transformations for locality using affinity regions. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: In both cases, the speedup is due to improved cache behavior; both programs show significant reductions in both L1 and L2 misses overall. 5 Related work A number of researchers have developed compiler techniques useful for improving cache behavior <ref> [1, 6, 8, 17, 20] </ref>. Almost all of these techniques apply to individual loop nests, however, and are not designed to detect or exploit cross-loop reuse. Two exceptions are loop fusion and affinity regions. Kennedy and McKinley have proposed using loop fusion to improve locality and cache behavior [16]. <p> Compile-time identification of affinity regions was proposed by Appelbe et al <ref> [1] </ref>. fusion reversal Program loops fused candidates reversed candidates applu 168 2 4 20 111 apsi 298 1 2 5 150 fpppp 39 0 0 0 6 hydro2d 163 5 14 20 136 mgrid 57 0 1 7 36 su2cor 118 0 0 5 47 swim 24 1 4 1 22
Reference: 2. <author> V. Balasundaram. </author> <title> Interactive Parallelization of Numerical Scientific Programs. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: to develop a framework for detecting reuse without considering cache size constraints, and then factor in the cache size if necessary (see section 2.8). 2.2 Array sections Array section analysis is a technique for summarizing the region (s) within an array that are accessed during some portion of the program <ref> [2, 5, 7, 15] </ref>. These summary representations provide a compact way of capturing the array access patterns, making them attractive for applications in which large portions of the program need to be considered. Our particular implementation represents array accesses using Data Access Descriptors [3], or "DAD"s.
Reference: 3. <author> V. Balasundaram. </author> <title> A mechanism for keeping useful internal information in parallel programming tools: The data access descriptor. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 9(2) </volume> <pages> 154-170, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: These summary representations provide a compact way of capturing the array access patterns, making them attractive for applications in which large portions of the program need to be considered. Our particular implementation represents array accesses using Data Access Descriptors <ref> [3] </ref>, or "DAD"s. The region within an array accessed by a given subscripted reference depends on the context that surrounds the reference. For example, consider the reference a (i,j) in Figure 1. <p> This optimization relies on the traversal order component of the DAD representation, which captures the direction and stride of the access in each array dimension (see <ref> [3] </ref> for the details).
Reference: 4. <author> U. Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, MA, </address> <year> 1988. </year>
Reference-contexts: Accurate information on loop-level reuse is a critical component for these methods; if the compiler can't detect the reuse, then there is no way for it to determine how or when to apply transformations. Loop-level reuse analysis is most often based on dependence analysis <ref> [4, 21] </ref>. Dependence analysis can provide very detailed information about the memory access patterns within a loop, but applying it to larger regions within a procedure is difficult, especially if the region in question contains control flow or procedure calls.
Reference: 5. <author> M. Burke and R. Cytron. </author> <title> Interprocedural dependence analysis and parallelization. </title> <booktitle> In Proceedings of the SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <address> Palo Alto, CA, </address> <month> June </month> <year> 1986. </year>
Reference-contexts: to develop a framework for detecting reuse without considering cache size constraints, and then factor in the cache size if necessary (see section 2.8). 2.2 Array sections Array section analysis is a technique for summarizing the region (s) within an array that are accessed during some portion of the program <ref> [2, 5, 7, 15] </ref>. These summary representations provide a compact way of capturing the array access patterns, making them attractive for applications in which large portions of the program need to be considered. Our particular implementation represents array accesses using Data Access Descriptors [3], or "DAD"s.
Reference: 6. <author> D. Callahan, S. Carr, and K. Kennedy. </author> <title> Improving register allocation for sub-scripted variables. </title> <booktitle> In Proceedings of the SIGPLAN '90 Conference on Programming Language Design and Implementation, </booktitle> <address> White Plains, NY, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: In both cases, the speedup is due to improved cache behavior; both programs show significant reductions in both L1 and L2 misses overall. 5 Related work A number of researchers have developed compiler techniques useful for improving cache behavior <ref> [1, 6, 8, 17, 20] </ref>. Almost all of these techniques apply to individual loop nests, however, and are not designed to detect or exploit cross-loop reuse. Two exceptions are loop fusion and affinity regions. Kennedy and McKinley have proposed using loop fusion to improve locality and cache behavior [16].
Reference: 7. <author> D. Callahan, J. Cocke, and K. Kennedy. </author> <title> Analysis of interprocedural side effects in a parallel programming environment. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5(5) </volume> <pages> 517-550, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: to develop a framework for detecting reuse without considering cache size constraints, and then factor in the cache size if necessary (see section 2.8). 2.2 Array sections Array section analysis is a technique for summarizing the region (s) within an array that are accessed during some portion of the program <ref> [2, 5, 7, 15] </ref>. These summary representations provide a compact way of capturing the array access patterns, making them attractive for applications in which large portions of the program need to be considered. Our particular implementation represents array accesses using Data Access Descriptors [3], or "DAD"s.
Reference: 8. <author> S. Carr, K. S. M c Kinley, and C.-W. Tseng. </author> <title> Compiler optimizations for improving data locality. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VI), </booktitle> <address> San Jose, CA, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: Compiler researchers have also been attacking this problem; they have devel-oped a number of optimizations that seek to enhance cache utilization, including loop interchange, loop tiling, and unroll-and-jam <ref> [8, 20] </ref> 1 . For most of these techniques, the compiler first analyzes a loop nest to find out what sort of reuse it contains, and then applies transformations to expose or improve the reuse in some way. <p> Our approach is to estimate the volume of the DAD for the reference, using a simple technique similar to the RefCost algorithm developed by Carr, McKinley, and Tseng <ref> [8] </ref>. Cache organization: Our framework is not equipped to predict cache conflicts due to limited associativity; we instead conservatively assume that cache conflicts will reduce the amount of reuse that takes place by a fixed factor. <p> In both cases, the speedup is due to improved cache behavior; both programs show significant reductions in both L1 and L2 misses overall. 5 Related work A number of researchers have developed compiler techniques useful for improving cache behavior <ref> [1, 6, 8, 17, 20] </ref>. Almost all of these techniques apply to individual loop nests, however, and are not designed to detect or exploit cross-loop reuse. Two exceptions are loop fusion and affinity regions. Kennedy and McKinley have proposed using loop fusion to improve locality and cache behavior [16]. <p> Two exceptions are loop fusion and affinity regions. Kennedy and McKinley have proposed using loop fusion to improve locality and cache behavior [16]. In a subsequent study, McKinley, Tseng, and Carr included loop fusion in their repertoire of transformations for an experimental study on compiler cache optimizations <ref> [8] </ref>. This study used dependence analysis to test for the profitability of loop fusion; loop reversal was not used as a locality-enhancing transformation.
Reference: 9. <author> R. Cmelik and D. Keppel. Shade: </author> <title> A fast instruction-set simulator for execution profiling. </title> <type> Technical Report SMLI 93-12; UWCSE 93-06-06, </type> <institution> Sun Microsystems Laboratories, Inc. and University of Washington, </institution> <year> 1993. </year>
Reference-contexts: In order to concentrate our results primarily on cache effects, we limited loop fusion to the outermost loop in each pair of adjacent loop nests. 4.2 Simulator Our cache simulator is based on the SPARC Performance Analysis Toolkit; it is layered on top of the tool shade <ref> [9] </ref>. Shade provides an extensible mechanism for writing execution-driven simulators; it operates by interpreting a SPARC executable and passing a trace of the instructions to a user-written trace analyzer. In our case, the trace analyzer counts instructions and simulates a particular cache configuration.
Reference: 10. <author> J. Ferrante, V. Sarkar, and W. Thrash. </author> <title> On estimating and enhancing cache effectiveness. </title> <editor> In U. Banerjee, D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, Fourth International Workshop, </booktitle> <address> Santa Clara, CA, </address> <month> August </month> <year> 1991. </year> <note> Springer-Verlag. </note>
Reference-contexts: In order for this scheme to work, we need to have a mechanism for determining the number of cache lines accessed by an array reference within a loop; this is in fact a research problem all by itself <ref> [10] </ref>. Our approach is to estimate the volume of the DAD for the reference, using a simple technique similar to the RefCost algorithm developed by Carr, McKinley, and Tseng [8].
Reference: 11. <author> T. Gross and P. Steenkiste. </author> <title> Structured dataflow analysis for arrays and its use in an optimizing compiler. </title> <journal> Software|Practice and Experience, </journal> <volume> 20(2) </volume> <pages> 133-155, </pages> <month> Febru-ary </month> <year> 1990. </year>
Reference-contexts: Percent change between original and transformed (with profile) Our data-flow framework resembles that of Gross and Steenkiste <ref> [11] </ref>. How--ever their framework is geared towards finding parallelism as opposed to detecting useful reuse for cache optimizations.
Reference: 12. <author> M. Gupta, E. Schonberg, and H. Srinivasan. </author> <title> A unified data-flow framework for optimizing communication. </title> <booktitle> In Proceedings of the Seventh Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Ithaca, NY, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: How--ever their framework is geared towards finding parallelism as opposed to detecting useful reuse for cache optimizations. Our techniques are also similar to those developed by Gupta, Schonberg, and Srinivasan for optimizing communication placement for programs running on distributed-memory multiprocessors <ref> [12] </ref>. 6 Conclusions In this paper we have presented a framework for predicting cross-loop reuse. The framework combines two existing tools: array section analysis and data-flow analysis. By using array sections, we can exploit the characteristics of the program's array access patterns without resorting to potentially costly procedure-wide dependence analysis.
Reference: 13. <author> R. v. Hanxleden. </author> <title> Compiler Support for Machine-Independent Parallelization of Irregular Problems. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> De-cember </month> <year> 1994. </year>
Reference-contexts: This sometimes requires the insertion of synthetic nodes and edges; see <ref> [13] </ref> for details. In addition, we define two types of partial orderings on N , as follows: Forward/Backward: Given a Flow/Jump edge (m; n), a Forward order visits m before n, whereas a Backward order visits m after n.
Reference: 14. <author> R. v. Hanxleden and K. Kennedy. </author> <title> Give-N-Take | A balanced code placement framework. </title> <booktitle> In Proceedings of the SIGPLAN '94 Conference on Programming Language Design and Implementation, </booktitle> <address> Orlando, FL, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: at levels M-1 and below are viewed as invariants, and B) the induction variables at levels M and above are allowed to vary. 2.3 Control flow representation Rather than using a standard control flow graph (CFG), this framework uses an interval-flow graph, developed by Reinhard von Hanxleden and Ken Kennedy <ref> [14] </ref>. This allows us to take the loop structure of the program into account explicitly. The interval-flow graph (IFG) can be constructed by starting with a normal CFG and then partitioning the nodes and edges in the graph into categories based on Tarjan interval analysis [18] 2 .
Reference: 15. <author> P. Havlak and K. Kennedy. </author> <title> An implementation of interprocedural bounded regular section analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 350-360, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: to develop a framework for detecting reuse without considering cache size constraints, and then factor in the cache size if necessary (see section 2.8). 2.2 Array sections Array section analysis is a technique for summarizing the region (s) within an array that are accessed during some portion of the program <ref> [2, 5, 7, 15] </ref>. These summary representations provide a compact way of capturing the array access patterns, making them attractive for applications in which large portions of the program need to be considered. Our particular implementation represents array accesses using Data Access Descriptors [3], or "DAD"s.
Reference: 16. <author> K. Kennedy and K. S. M c Kinley. </author> <title> Maximizing loop parallelism and improving data locality via loop fusion and distribution. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: Selecting the optimal set of transformations is a very difficult problem; optimizing temporal locality using loop fusion alone is NP-hard <ref> [16] </ref>. As a result, the compiler must resort to heuristics to choose the set of transformations to apply. 4 Experimental results In this section, we report the results from an experimental study, in which we apply our techniques to ten programs from the SPEC95 floating point benchmark suite [19]. <p> Almost all of these techniques apply to individual loop nests, however, and are not designed to detect or exploit cross-loop reuse. Two exceptions are loop fusion and affinity regions. Kennedy and McKinley have proposed using loop fusion to improve locality and cache behavior <ref> [16] </ref>. In a subsequent study, McKinley, Tseng, and Carr included loop fusion in their repertoire of transformations for an experimental study on compiler cache optimizations [8]. This study used dependence analysis to test for the profitability of loop fusion; loop reversal was not used as a locality-enhancing transformation.
Reference: 17. <author> W. Li and K. Pingali. </author> <title> Access normalization: Loop restructuring for NUMA compilers. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-V), </booktitle> <address> Boston, MA, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: In both cases, the speedup is due to improved cache behavior; both programs show significant reductions in both L1 and L2 misses overall. 5 Related work A number of researchers have developed compiler techniques useful for improving cache behavior <ref> [1, 6, 8, 17, 20] </ref>. Almost all of these techniques apply to individual loop nests, however, and are not designed to detect or exploit cross-loop reuse. Two exceptions are loop fusion and affinity regions. Kennedy and McKinley have proposed using loop fusion to improve locality and cache behavior [16].
Reference: 18. <author> R. E. Tarjan. </author> <title> Testing flow graph reducibility. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 9 </volume> <pages> 355-365, </pages> <year> 1974. </year>
Reference-contexts: This allows us to take the loop structure of the program into account explicitly. The interval-flow graph (IFG) can be constructed by starting with a normal CFG and then partitioning the nodes and edges in the graph into categories based on Tarjan interval analysis <ref> [18] </ref> 2 . A Tarjan interval T (h) is a set of CFG nodes that corresponds to a loop within the program, where h is a unique header node (with h =2 T (h)). Intuitively, T (h) together with h form a strongly connected region within the CFG.
Reference: 19. <author> J. Uniejewski. </author> <title> SPEC Benchmark Suite: Designed for today's advanced systems. </title> <journal> SPEC Newsletter Volume 1, </journal> <note> Issue 1, SPEC, Fall 1989. </note>
Reference-contexts: As a result, the compiler must resort to heuristics to choose the set of transformations to apply. 4 Experimental results In this section, we report the results from an experimental study, in which we apply our techniques to ten programs from the SPEC95 floating point benchmark suite <ref> [19] </ref>. Our experimental infrastructure consists of a Fortran transformation engine, including the cross-loop reuse analysis framework, and an execution-driven simulator for gathering instruction counts and cache statistics. 4.1 Compiler The phases in our compiler are shown in Figure 6. <p> The "training" input files were used for these runs, in order to yield more reasonable simulation times <ref> [19] </ref>.
Reference: 20. <author> M. E. Wolf and M. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <address> Toronto, Canada, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Compiler researchers have also been attacking this problem; they have devel-oped a number of optimizations that seek to enhance cache utilization, including loop interchange, loop tiling, and unroll-and-jam <ref> [8, 20] </ref> 1 . For most of these techniques, the compiler first analyzes a loop nest to find out what sort of reuse it contains, and then applies transformations to expose or improve the reuse in some way. <p> In both cases, the speedup is due to improved cache behavior; both programs show significant reductions in both L1 and L2 misses overall. 5 Related work A number of researchers have developed compiler techniques useful for improving cache behavior <ref> [1, 6, 8, 17, 20] </ref>. Almost all of these techniques apply to individual loop nests, however, and are not designed to detect or exploit cross-loop reuse. Two exceptions are loop fusion and affinity regions. Kennedy and McKinley have proposed using loop fusion to improve locality and cache behavior [16].
Reference: 21. <author> M. J. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year> <title> This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: Accurate information on loop-level reuse is a critical component for these methods; if the compiler can't detect the reuse, then there is no way for it to determine how or when to apply transformations. Loop-level reuse analysis is most often based on dependence analysis <ref> [4, 21] </ref>. Dependence analysis can provide very detailed information about the memory access patterns within a loop, but applying it to larger regions within a procedure is difficult, especially if the region in question contains control flow or procedure calls. <p> As a result, dependence analysis is not well suited to detecting cross-loop reuse. Transformations also exist that exploit cross-loop reuse, primarily loop fusion and loop reversal <ref> [21] </ref>. Because of the shortcomings of existing reuse analysis methods, however, there has been no effective way until now to detect situations where these transformations can be profitably applied. In this paper we describe a compiler framework for detecting useful cross-loop reuse. <p> the immediately preceding loop nest, then loop fusion will be profitable (the degree of profitability will be dependent on the volume of the intersection). 3 Once it is established that the transformation is profitable, then the compiler can apply the more costly dependence-based techniques to determine whether fusion is safe <ref> [21] </ref>. Loop reversal: A weaker but slightly more widely applicable technique is loop reversal. This optimization can provide benefits only in proportion to the size of the cache, thus it works best for very large (presumably secondary or tertiary) caches.
References-found: 21

