URL: http://http.icsi.berkeley.edu/ftp/global/pub/ai/picls/stolcke-nips5.ps.Z
Refering-URL: http://http.icsi.berkeley.edu/ftp/global/pub/ai/picls/
Root-URL: http://http.icsi.berkeley.edu
Email: stolcke@icsi.berkeley.edu  om@icsi.berkeley.edu  
Title: Hidden Markov Model Induction by Bayesian Model Merging  
Author: C. L. Giles, S. J. Hanson, J. D. Cowan, Andreas Stolcke Stephen Omohundro 
Address: Berkeley, CA 94720  1947 Center Street, Suite 600 Berkeley, CA 94704  
Affiliation: Computer Science Division University of California  International Computer Science Institute  
Date: 1993  
Note: To appear in:  eds., Advances in Neural Information Processing Systems 5, San Mateo, CA, Morgan Kaufman,  
Abstract: This paper describes a technique for learning both the number of states and the topology of Hidden Markov Models from examples. The induction process starts with the most specific model consistent with the training data and generalizes by successively merging states. Both the choice of states to merge and the stopping criterion are guided by the Bayesian posterior probability. We compare our algorithm with the Baum-Welch method of estimating fixed-size models, and find that it can induce minimal HMMs from data in cases where fixed estimation does not converge or requires redundant parameters to converge. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Angluin, D. & Smith, C. H. </author> <year> (1983), </year> <title> `Inductive inference: Theory and methods', </title> <journal> ACM Computing Surveys 15(3), </journal> <pages> 237-269. </pages>
Reference-contexts: For Baum-Welch, both best and worst performance over several initial conditions is listed. to automata learning as well <ref> (Angluin & Smith, 1983) </ref>. Tomita (1982) is an example of finite-state model space search guided by a (non-probabilistic) goodness measure. Horning (1969) describes a Bayesian grammar induction procedure that searches the model space exhaustively for the MAP model.
Reference: <author> Baldi, P., Chauvin, Y., Hunkapiller, T. & McClure, M. A. </author> <year> (1993), </year> <title> `Hidden Markov Models in molecular biology: New algorithms and applications', this volume. </title>
Reference: <author> Baum, L. E., Petrie, T., Soules, G. & Weiss, N. </author> <year> (1970), </year> <title> `A maximization technique occuring in the statistical analysis of probabilistic functions in Markov chains', </title> <journal> The Annals of Mathematical Statistics 41(1), </journal> <pages> 164-171. </pages>
Reference-contexts: The standard approach is to find a maximum likelihood (ML) or maximum a posteriori probability (MAP) estimate of the HMM parameters. The Baum-Welch algorithm uses dynamic programming to approximate these estimates <ref> (Baum, Petrie, Soules & Weiss, 1970) </ref>. A more general problem is to additionally find the best HMM topology. This includes both the number of states and the connectivity (the non-zero transitions and emissions).
Reference: <author> Berger, J. O. </author> <year> (1985), </year> <title> Statistical Decision Theory and Bayesian Analysis, </title> <publisher> Springer Verlag, </publisher> <address> New York. </address>
Reference-contexts: Since both the transition and the emission probabilities are given by multinomial distributions it is natural to use a Dirichlet conjugate prior in this case <ref> (Berger, 1985) </ref>. The effect of this prior is equivalent to having a number of `virtual' samples for each of the possible transitions and emissions which are added to the actual samples when it comes to estimating the most likely parameter settings.
Reference: <author> Buntine, W. </author> <year> (1992), </year> <title> Learning classification trees, </title> <editor> in D. J. Hand, ed., </editor> <booktitle> `Artificial Intelligence Frontiers in Statistics: AI and Statistics III', </booktitle> <publisher> Chapman & Hall. </publisher>
Reference: <author> Haussler, D., Krogh, A., Mian, I. S. & Sjolander, K. </author> <year> (1992), </year> <title> Protein modeling using hidden Markov models: Analysis of globins, </title> <type> Technical Report UCSC-CRL-92-23, </type> <institution> Computer and Information Sciences, University of California, Santa Cruz, Ca. </institution> <note> Revised Sept. </note> <year> 1992. </year>
Reference: <author> Hopcroft, J. E. & Ullman, J. D. </author> <year> (1979), </year> <title> Introduction to Automata Theory, Languages, and Computation, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass. </address>
Reference-contexts: Also, increasing the sample size helped it converge to the target model. 5 RELATED WORK Our approach is related to several other approaches in the literature. The concept of state merging is implicit in the notion of state equivalence classes, which is fundamental to much of automata theory <ref> (Hopcroft & Ullman, 1979) </ref> and has been applied 1 The number of `virtual' samples per transition/emission was held constant at 0.1 throughout. (a) Method Sample Entropy Cross-entropy Language n Merging 8 m.p. 2.295 2.188 - .020 ac * a bc * b 6 Merging 20 random 2.087 2.158 - .033 ac
Reference: <author> Horning, J. J. </author> <year> (1969), </year> <title> A study of grammatical inference, </title> <type> Technical Report CS 139, </type> <institution> Computer Science Department, Stanford University, Stanford, </institution> <address> Ca. </address>
Reference: <author> Lari, K. & Young, S. J. </author> <year> (1990), </year> <title> `The estimation of stochastic context-free grammars using the Inside-Outside algorithm', </title> <booktitle> Computer Speech and Language 4, </booktitle> <pages> 35-56. </pages>
Reference-contexts: Another direction involves an extension of the model space to stochastic context-free grammars, for which a standard estimation method analogous to Baum-Welch exists <ref> (Lari & Young, 1990) </ref>. The notions of sample incorporation and merging carry over to this domain (with merging now involving the non-terminals of the CFG), but need to be complemented with a mechanism that adds new non-terminals to create hierarchical structure (which we call chunking).
Reference: <author> MacKay, D. J. C. </author> <year> (1992), </year> <title> `Bayesian interpolation', </title> <booktitle> Neural Computation 4, </booktitle> <pages> 415-447. </pages>
Reference-contexts: As we merge states, the available evidence gets shared by fewer states, thus allowing the remaining states to produce a better fit to the data. This phenomenon is similar, but not identical, to the Bayesian `Occam factors' that prefer models with fewer parameter <ref> (MacKay, 1992) </ref>.
Reference: <author> Omohundro, S. M. </author> <year> (1992), </year> <title> Best-first model merging for dynamic learning and recognition, </title> <type> Technical Report TR-92-004, </type> <institution> International Computer Science Institute, Berkeley, </institution> <address> Ca. </address>
Reference: <author> Porat, S. & Feldman, J. A. </author> <year> (1991), </year> <title> `Learning automata from ordered examples', </title> <booktitle> Machine Learning 7, </booktitle> <pages> 109-138. </pages>
Reference: <author> Rabiner, L. R. & Juang, B. H. </author> <year> (1986), </year> <title> `An introduction to Hidden Markov Models', </title> <journal> IEEE ASSP Magazine 3(1), </journal> <pages> 4-16. </pages>
Reference-contexts: HMMs can be viewed as a stochastic generalization of finite-state automata, where both the transitions between states and the generation of output symbols are governed by probability distributions. HMMs have been important in speech recognition <ref> (Rabiner & Juang, 1986) </ref>, cryptography, and more recently in other areas such as protein classification and alignment (Haussler, Krogh, Mian & Sj olander, 1992; Baldi, Chauvin, Hunkapiller & McClure, 1993).
Reference: <author> Rissanen, J. </author> <year> (1983), </year> <title> `A universal prior for integers and estimation by minimum description length', </title> <journal> The Annals of Statistics 11(2), </journal> <pages> 416-431. </pages>
Reference: <author> Stolcke, A. & Omohundro, S. </author> <year> (1993), </year> <title> Best-first model merging for Hidden Markov Model induction, </title> <type> Technical Report TR-93-003, </type> <institution> International Computer Science Institute, Berkeley, </institution> <address> Ca. </address>
Reference: <author> Tomita, M. </author> <year> (1982), </year> <title> Dynamic construction of finite automata from examples using hill-climbing, </title> <booktitle> in `Proceedings of the 4th Annual Conference of the Cognitive Science Society', </booktitle> <address> Ann Arbor, </address> <publisher> Mich., </publisher> <pages> pp. 105-108. </pages>
Reference: <author> Wallace, C. S. & Freeman, P. R. </author> <year> (1987), </year> <title> `Estimation and inference by compact coding', </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> Series B 49(3), </volume> <pages> 240-265. </pages>
References-found: 17

