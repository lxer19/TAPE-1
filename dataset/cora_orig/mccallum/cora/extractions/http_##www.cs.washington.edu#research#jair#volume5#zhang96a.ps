URL: http://www.cs.washington.edu/research/jair/volume5/zhang96a.ps
Refering-URL: http://www.cs.washington.edu/research/jair/abstracts/zhang96a.html
Root-URL: 
Email: LZHANG@CS.UST.HK  POOLE@CS.UBC.CA  
Title: Exploiting Causal Independence in Bayesian Network Inference  
Author: Nevin Lianwen Zhang David Poole 
Address: 2366 Main Mall, Vancouver, B.C., Canada V6T 1Z4  
Affiliation: Department of Computer Science, University of Science Technology, Hong Kong  Department of Computer Science, University of British Columbia,  
Note: Journal of Artificial Intelligence Research 5 (1996) 301-328 Submitted 4/96; published 12/96  
Abstract: A new method is proposed for exploiting causal independencies in exact Bayesian network inference. A Bayesian network can be viewed as representing a factorization of a joint probability into the multiplication of a set of conditional probabilities. We present a notion of causal independence that enables one to further factorize the conditional probabilities into a combination of even smaller factors and consequently obtain a finer-grain factorization of the joint probability. The new formulation of causal independence lets us specify the conditional probability of a variable given its parents in terms of an associative and commutative operator, such as or, sum or max, on the contribution of each parent. We start with a simple algorithm VE for Bayesian network inference that, given evidence and a query variable, uses the factorization to find the posterior distribution of the query. We show how this algorithm can be extended to exploit causal independence. Empirical studies, based on the CPCS networks for medical diagnosis, show that this method is more efficient than previous methods and allows for inference in larger networks than previous algorithms.
Abstract-found: 1
Intro-found: 1
Reference: <author> Arnborg, S., Corneil, D. G., & Proskurowski, A. </author> <year> (1987). </year> <title> Complexity of finding embedding in a k-tree. </title> <journal> SIAM J. Alg. Disc. Meth., </journal> <volume> 8(2), </volume> <pages> 277-284. </pages>
Reference-contexts: An optimal elimination ordering is one that results in the least complexity. The problem of finding an optimal elimination ordering is NP-complete <ref> (Arnborg et al., 1987) </ref>. Commonly used heuristics include minimum deficiency search (Bertele & Brioschi, 1972) and maximum cardinality search (Tarjan & Yannakakis, 1984). Kjrulff (1990) has empirically shown that minimum deficiency search is the best existing heuristic.
Reference: <author> Baker, M., & Boult, T. </author> <year> (1990). </year> <title> Pruning Bayesian networks for efficient computation. </title> <booktitle> In Proc. Sixth Conf. on Uncertainty in Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 257-264 Cambridge, Mass. </address>
Reference: <author> Bertele, U., & Brioschi, F. </author> <year> (1972). </year> <title> Nonserial dynamic programming, </title> <booktitle> Vol. 91 of Mathematics in Science and Engineering. </booktitle> <publisher> Academic Press. </publisher>
Reference-contexts: An optimal elimination ordering is one that results in the least complexity. The problem of finding an optimal elimination ordering is NP-complete (Arnborg et al., 1987). Commonly used heuristics include minimum deficiency search <ref> (Bertele & Brioschi, 1972) </ref> and maximum cardinality search (Tarjan & Yannakakis, 1984). Kjrulff (1990) has empirically shown that minimum deficiency search is the best existing heuristic.
Reference: <author> Boutilier, C., Friedman, N., Goldszmidt, M., & Koller, D. </author> <year> (1996). </year> <title> Context-specific independence in Bayesian networks. </title> <editor> In E. Horvitz and F. Jensen (Ed.), </editor> <booktitle> Proc. Twelthth Conf. on Uncertainty in Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 115-123 Portland, Oregon. </address>
Reference: <author> Cooper, G. F. </author> <year> (1990). </year> <title> The computational complexity of probabilistic inference using Bayesian belief networks. </title> <booktitle> Artificial Intelligence, </booktitle> <pages> 42(2-3), 393-405. </pages>
Reference-contexts: This is why inference in a BN can be tractable in many cases, even if the general problem is NP-hard <ref> (Cooper, 1990) </ref>. 3. The Variable Elimination Algorithm Based on the discussions of the previous section, we present a simple algorithm for computing P (XjY =Y 0 ).
Reference: <author> Dagum, P., & Galper, A. </author> <year> (1993). </year> <title> Additive belief-network models. </title> <editor> In D. Heckerman and A. Mamdani (Ed.), </editor> <booktitle> Proc. Ninth Conf. on Uncertainty in Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 91-98 Washington D.C. </address> <month> D'Ambrosio </month> <year> (1995). </year> <title> Local expression languages for probabilistic dependence. </title> <journal> International Journal of Approximate Reasoning, </journal> <volume> 13(1), </volume> <pages> 61-81. </pages>
Reference-contexts: The definition of causal independence given here is slightly different than that given by Hecker-man and Breese (1994) and Srinivas (1993). However, it still covers common causal independence models such as noisy OR-gates (Good, 1961; Pearl, 1988), noisy MAX-gates (Dez, 1993), noisy AND-gates, and noisy adders <ref> (Dagum & Galper, 1993) </ref> as special cases. One can see this in the following examples. Example 1 (Lottery) Buying lotteries affects your wealth. The amounts of money you spend on buying different kinds of lotteries affect your wealth independently.
Reference: <author> D'Ambrosio, B. </author> <year> (1994). </year> <title> Symbolic probabilistic inference in large BN2O networks. </title> <editor> In R. Lopez de Mantaras and D. Poole (Ed.), </editor> <booktitle> Proc. Tenth Conf. on Uncertainty in Artificial Intelligence, </booktitle> <pages> pp. </pages> <month> 128-135 Seattle. </month> <title> 326 EXPLOITING CAUSAL INDEPENDENCE IN BAYESIAN NETWORK INFERENCE Dechter, </title> <editor> R. </editor> <year> (1996). </year> <title> Bucket elimination: A unifying framework for probabilistic inference. </title> <note> In E. </note>
Reference: <editor> Horvits and F. Jensen (Ed.), </editor> <booktitle> Proc. Twelthth Conf. on Uncertainty in Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 211-219 Portland, Oregon. </address>
Reference: <author> Dez, F. J. </author> <year> (1993). </year> <title> Parameter adjustment in bayes networks. the generalized noisy or-gate. </title> <address> In D. </address>
Reference-contexts: We call fl the base combination operator of e. The definition of causal independence given here is slightly different than that given by Hecker-man and Breese (1994) and Srinivas (1993). However, it still covers common causal independence models such as noisy OR-gates (Good, 1961; Pearl, 1988), noisy MAX-gates <ref> (Dez, 1993) </ref>, noisy AND-gates, and noisy adders (Dagum & Galper, 1993) as special cases. One can see this in the following examples. Example 1 (Lottery) Buying lotteries affects your wealth. The amounts of money you spend on buying different kinds of lotteries affect your wealth independently.
Reference: <editor> Heckerman and A. Mamdani (Ed.), </editor> <booktitle> Proc. Ninth Conf. on Uncertainty in Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 99-105 Washington D.C. </address>
Reference: <author> Duda, R. O., Hart, P. E., & Nilsson, N. J. </author> <year> (1976). </year> <title> Subjective Bayesian methods for rule-based inference systems. </title> <booktitle> In Proc. AFIPS Nat. Comp. Conf., </booktitle> <pages> pp. 1075-1082. </pages>
Reference: <author> Geiger, D., & Heckerman, D. </author> <year> (1996). </year> <title> Knowledge representation and inference in similarity networks and Bayesian multinets. </title> <journal> Artificial Intelligence, </journal> <volume> 82, </volume> <pages> 45-74. </pages>
Reference-contexts: One such case is where some dependencies depend on particular values of other variables; such dependencies can be stated as rules (Poole, 1993), trees (Boutilier c fl1996 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved. ZHANG & POOLE et al., 1996) or as multinets <ref> (Geiger & Heckerman, 1996) </ref>. Another is where the the function can be described using a binary operator that can be applied to values from each of the parent variables. It is the latter, known as `causal independencies', that we seek to exploit in this paper.
Reference: <author> Geiger, D., Verma, T., & Pearl, J. </author> <year> (1990). </year> <title> d-separation: from theorems to algorithms. </title> <editor> In M. Henrion et. al. (Ed.), </editor> <booktitle> Uncertainty in Artificial Intelligence 5, </booktitle> <pages> pp. 139-148. </pages> <publisher> North Holland, </publisher> <address> New York. </address>
Reference: <author> Good, I. </author> <year> (1961). </year> <title> A causal calculus (i). </title> <journal> British Journal of Philosophy of Science, </journal> <volume> 11, </volume> <pages> 305-318. </pages>
Reference-contexts: It is the latter, known as `causal independencies', that we seek to exploit in this paper. Causal independence refers to the situation where multiple causes contribute independently to a common effect. A well-known example is the noisy OR-gate model <ref> (Good, 1961) </ref>. Knowledge engineers have been using specific causal independence models in simplifying knowledge acquisition (Henrion, 1987; Olesen et al., 1989; Olesen & Andreassen, 1993). Heckerman (1993) was the first to formalize the general concept of causal independence. The formalization was later refined by Heckerman and Breese (1994).
Reference: <author> Heckerman, D. </author> <year> (1993). </year> <title> Causal independence for knowledge acquisition and inference. </title> <booktitle> In Proc. of the Ninth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pp. 122-127. </pages>
Reference: <author> Heckerman, D., & Breese, J. </author> <year> (1994). </year> <title> A new look at causal independence. </title> <booktitle> In Proc. of the Tenth Conference on Uncertainty in Artificial Ingelligence, </booktitle> <pages> pp. 286-292. </pages>
Reference: <author> Henrion, M. </author> <year> (1987). </year> <title> Some practical issues in constructing belief networks. </title> <editor> In L. Kanal and T. Levitt and J. Lemmer (Ed.), </editor> <booktitle> Uncertainty in Artificial Intelligence, </booktitle> <pages> pp. 161-174. </pages> <publisher> North-Holland. </publisher>
Reference-contexts: This is called the exception independence assumption by Pearl (1988). 2. This is called the accountability assumption by Pearl (1988). The assumption can always be satisfied by introducing a node that represent all other causes <ref> (Henrion, 1987) </ref>. 308 EXPLOITING CAUSAL INDEPENDENCE IN BAYESIAN NETWORK INFERENCE So, the fractions of time a faculty member spends in the three areas are causally independent w.r.t. the contract renewal result.
Reference: <author> Howard, R. A., & Matheson, J. E. </author> <year> (1981). </year> <title> Influence diagrams. </title> <editor> In Howard, R. A., & Matheson, J. (Eds.), </editor> <booktitle> The Principles and Applications of Decision Analysis, </booktitle> <pages> pp. 720-762. </pages> <institution> Strategic Decisions Group, </institution> <address> CA. </address>
Reference: <author> Jensen, F. V., Lauritzen, S. L., & Olesen, K. G. </author> <year> (1990). </year> <title> Bayesian updating in causal probabilistic networks by local computations. </title> <journal> Computational Statistics Quaterly, </journal> <volume> 4, </volume> <pages> 269-282. </pages>
Reference: <author> Kim, J., & Pearl, J. </author> <year> (1983). </year> <title> A computational model for causal and diagnostic reasoning in inference engines. </title> <booktitle> In Proc. of the Eighth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 190-193 Karlsruhe, Germany. </address>
Reference: <author> Kjrulff, U. </author> <year> (1990). </year> <title> Triangulation of graphs algorithms giving small total state space. </title> <type> Tech. rep. </type> <institution> R 90-09, Department of Mathematics and Computer Science, Strandvejen, DK 9000 Aalborg, Denmark. </institution>
Reference: <author> Lauritzen, S. L., Dawid, A. P., Larsen, B. N., & Leimer, H. G. </author> <year> (1990). </year> <title> Independence properties of directed markov fields. </title> <journal> Networks, </journal> <volume> 20, </volume> <pages> 491-506. </pages>
Reference: <author> Lauritzen, S. L., & Spiegelhalter, D. J. </author> <year> (1988). </year> <title> Local computations with probabilities on graphical structures and their application to expert systems. </title> <journal> Journal of the Royal Statistical Society, Series B, </journal> <volume> 50(2), </volume> <pages> 157-224. </pages> <note> 327 ZHANG & POOLE Li, </note> <author> Z., & D'Ambrosio, B. </author> <year> (1994). </year> <title> Efficient inference in Bayes networks as a combinatorial optimization problem. </title> <journal> International Journal of Approximate Reasoning, </journal> <volume> 11(1), </volume> <pages> 55-81. </pages>
Reference: <author> Olesen, K. G., & Andreassen, S. </author> <year> (1993). </year> <title> Specification of models in large expert systems based on causal probabilistic networks. </title> <journal> Artificial Intelligence in Medicine, </journal> <volume> 5, </volume> <pages> 269-281. </pages>
Reference: <author> Olesen, K. G., Kjrulff, U., Jensen, F., Falck, B., Andreassen, S., & Andersen, S. K. </author> <year> (1989). </year> <title> A munin network for the median nerve a case study on loops. </title> <journal> Applied Artificial Intelligence, </journal> <volume> 3, </volume> <pages> 384-403. </pages>
Reference-contexts: Let fl be the base combination operator of e, let ~ i denote the contribution of c i to e, and let f i (e; c i ) be the contributing factor of c i to e. The parent-divorcing method <ref> (Olesen et al., 1989) </ref> transforms the BN into the one in Figure 3 (3). After the transformation, all variables are regular and the new variables e 1 and e 2 have the same possible values as e.
Reference: <author> Parker, R., & Miller, R. </author> <year> (1987). </year> <title> Using causal knowledge to creat simulated patient cases: the CPSC project as an extension of Internist-1. </title> <booktitle> In Proc. 11th Symp. Comp. Appl. in Medical Care, </booktitle> <pages> pp. </pages> <address> 473-480 Los Alamitos, CA. </address> <publisher> IEEE Comp Soc Press. </publisher>
Reference: <author> Pearl, J. </author> <year> (1988). </year> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: Proof: 3 The definition of causal independence entails the independence assertions I (~ 1 ; fc 2 ; : : : ; c m gjc 1 ) and I (~ 1 ; ~ 2 jc 1 ): By the axiom of weak union <ref> (Pearl, 1988, p. 84) </ref>, we have I (~ 1 ; ~ 2 jfc 1 ; : : : ; c m g). Thus all of the ~ i mutually independent given fc 1 ; : : : ; c m g.
Reference: <author> Poole, D. </author> <year> (1993). </year> <title> Probabilistic Horn abduction and Bayesian networks. </title> <journal> Artificial Intelligence, </journal> <volume> 64(1), </volume> <pages> 81-129. </pages>
Reference-contexts: Often, however, there is much structure in these probability functions that can be exploited for knowledge acquisition and inference. One such case is where some dependencies depend on particular values of other variables; such dependencies can be stated as rules <ref> (Poole, 1993) </ref>, trees (Boutilier c fl1996 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved. ZHANG & POOLE et al., 1996) or as multinets (Geiger & Heckerman, 1996).
Reference: <author> Pradhan, M., Provan, G., Middleton, B., & Henrion, M. </author> <year> (1994). </year> <title> Knowledge engineering for large belief networks. </title> <editor> In R. Lopez de Mantaras and D. Poole (Ed.), </editor> <booktitle> Proc. Tenth Conf. on Uncertainty in Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 484-490 Seattle. </address>
Reference: <author> Shachter, R. D., D'Ambrosio, B. D., & Del Favero, B. D. </author> <year> (1990). </year> <title> Symbolic probabilistic inference in belief networks. </title> <booktitle> In Proc. 8th National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 126-131 Boston. </address> <publisher> MIT Press. </publisher>
Reference: <author> Shafer, G., & Shenoy, P. </author> <year> (1990). </year> <title> Probability propagation. </title> <journal> Annals of Mathematics and Artificial Intelligence, </journal> <volume> 2, </volume> <pages> 327-352. </pages>
Reference: <author> Shortliffe, E., & Buchanan, G. B. </author> <year> (1975). </year> <title> A model of inexact reasoning in medicine. </title> <journal> Math. Biosci., </journal> <volume> 23, </volume> <pages> 351-379. </pages>
Reference: <author> Srinivas, S. </author> <year> (1993). </year> <title> A generalization of noisy-or model. </title> <booktitle> In Proc. of the Ninth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pp. 208-215. </pages>
Reference: <author> Tarjan, R. E., & Yannakakis, M. </author> <year> (1984). </year> <title> Simple linear time algorithm to test chordality of graphs, test acyclicity of hypergraphs, and selectively reduce acyclic hypergraphs. </title> <journal> SIAM J. Comput., </journal> <volume> 13, </volume> <pages> 566-579. </pages>
Reference-contexts: An optimal elimination ordering is one that results in the least complexity. The problem of finding an optimal elimination ordering is NP-complete (Arnborg et al., 1987). Commonly used heuristics include minimum deficiency search (Bertele & Brioschi, 1972) and maximum cardinality search <ref> (Tarjan & Yannakakis, 1984) </ref>. Kjrulff (1990) has empirically shown that minimum deficiency search is the best existing heuristic.
Reference: <author> Zhang, N. L., & Poole, D. </author> <year> (1994). </year> <title> A simple approach to Bayesian network computations. </title> <booktitle> In Proc. of the Tenth Canadian Conference on Artificial Intelligence, </booktitle> <pages> pp. 171-178. 328 </pages>
References-found: 35

