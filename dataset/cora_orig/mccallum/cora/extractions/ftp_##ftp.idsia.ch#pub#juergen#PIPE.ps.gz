URL: ftp://ftp.idsia.ch/pub/juergen/PIPE.ps.gz
Refering-URL: http://www.ics.uci.edu/~mlearn/MLlist/v10/1.html
Root-URL: 
Title: Probabilistic Incremental Program Evolution  
Author: Rafa l Sa lustowicz and Jurgen Schmidhuber 
Keyword: Probabilistic Incremental Program Evolution, probabilistic programming languages, stochastic program search, Population-Based Incremental Learning, Genetic Programming, partially observable environments.  
Note: Evolutionary Computation  
Address: Corso Elvezia 36, 6900 Lugano, Switzerland  
Affiliation: IDSIA,  
Email: e-mail: frafal, juergeng@idsia.ch  
Phone: tel.: +41-91-9919838 fax: +41-91-9919839  
Date: 5(2):123-141, 1997.  
Abstract: Probabilistic Incremental Program Evolution (PIPE) is a novel technique for automatic program synthesis. We combine probability vector coding of program instructions, Population-Based Incremental Learning, and tree-coded programs like those used in some variants of Genetic Programming (GP). PIPE iteratively generates successive populations of functional programs according to an adaptive probability distribution over all possible programs. Each iteration it uses the best program to refine the distribution. Thus, it stochastically generates better and better programs. Since distribution refinements depend only on the best program of the current population, PIPE can evaluate program populations efficiently when the goal is to discover a program with minimal runtime. We compare PIPE to GP on a function regression problem and the 6-bit parity problem. We also use PIPE to solve tasks in partially observable mazes, where the best programs have minimal runtime.
Abstract-found: 1
Intro-found: 1
Reference: <author> Baluja, S. </author> <year> (1996). </year> <title> An empirical comparison of seven iterative and evolutionary function optimization heuristics. </title> <type> Technical Report CMU-CS-95-193, </type> <institution> Carnegie Mellon University. </institution>
Reference-contexts: This procedure is repeated until all probabilities are either 1.0 or 0.0. Thus, PBIL does not store domain knowledge in a population, but in a probability distribution. PBIL was shown to outperform GA on several problems <ref> (Baluja, 1996) </ref>. One reason for PBIL's success may be that PBIL explicitly captures first-order dependencies between individual solution parameters and solution quality in a probability distribution (Baluja & Davies, 1997).
Reference: <author> Baluja, S. and Caruana, R. </author> <year> (1995). </year> <title> Removing the genetics from the standard genetic algorithm. </title> <editor> In Prieditis, A. and Russell, S., editors, </editor> <booktitle> Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <pages> pages 38-46. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Francisco, CA. </address>
Reference-contexts: Thus, both can be applied to the same problems. GP, however, stores domain knowledge in program populations, whereas PIPE captures this knowledge in a probability distribution. GP relies on crossover to generate better and better programs, whereas PIPE uses a learning method similar to Population-Based Incremental Learning <ref> (Baluja & Caruana, 1995) </ref>. Population-Based Incremental Learning (PBIL). PBIL generates a population of fixed-length bitstrings (solution candidates for a given task) according to a vector of probabilities (initially 0.5). The probabilities are then adjusted to increase (decrease) the probability of the current population's best (worst) individual.
Reference: <author> Baluja, S. and Davies, S. </author> <year> (1997). </year> <title> Using optimal dependency-trees for combinatorial optimization: Learning the structure of the search space. </title> <type> Technical Report CMU-CS-97-107, </type> <institution> Carnegie Mellon University. </institution>
Reference-contexts: PBIL was shown to outperform GA on several problems (Baluja, 1996). One reason for PBIL's success may be that PBIL explicitly captures first-order dependencies between individual solution parameters and solution quality in a probability distribution <ref> (Baluja & Davies, 1997) </ref>. GAs, on the other hand, maintain a population and rely on crossover to sensibly combine parameters that are collectively responsible for favorable evaluations. Since the choice of crossover points is random, however, crossover often is not beneficial (De Bonet, Isbell, & Viola, 1997).
Reference: <author> Bertsekas, D. P. </author> <year> (1996). </year> <title> Neuro-Dynamic Programming. </title> <publisher> Athena Scientific, </publisher> <address> Belmont, MA. </address>
Reference: <author> Chrisman, L. </author> <year> (1992). </year> <title> Reinforcement learning with perceptual aliasing: The perceptual distinctions approach. </title> <booktitle> In Proceedings of the Tenth International Conference on Artificial Intelligence, </booktitle> <pages> pages 183-188. </pages> <publisher> AAAI Press, </publisher> <address> San Jose, California. </address>
Reference: <author> Cliff, D. and Ross, S. </author> <year> (1994). </year> <title> Adding temporary memory to ZCS. </title> <booktitle> Adaptive Behavior, </booktitle> <volume> 3 </volume> <pages> 101-150. </pages>
Reference: <author> De Bonet, J. S., Isbell, Jr., C. L., and Viola, P. </author> <year> (1997). </year> <title> Mimic: Finding optima by estimating probability densities. </title> <editor> In Jordan, M., Mozer, M., and Perrone, M., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 9, </volume> <pages> pages 424-430. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address> <note> 15 Dickmanns, </note> <author> D., Schmidhuber, J., and Winklhofer, A. </author> <year> (1987). </year> <title> Der genetische Algorithmus: Eine Implementierung in Prolog. </title> <institution> Fortgeschrittenenpraktikum, Institut fur Informatik, Lehrstuhl Prof. Radig, Technische Universitat Munchen. </institution>
Reference-contexts: GAs, on the other hand, maintain a population and rely on crossover to sensibly combine parameters that are collectively responsible for favorable evaluations. Since the choice of crossover points is random, however, crossover often is not beneficial <ref> (De Bonet, Isbell, & Viola, 1997) </ref>. It may act disruptively and tear apart previously discovered, useful parameter groups. One motivation of our paper is that potential advantages of PBIL over GA suggest potential advantages of PIPE over GP. PIPE follows PBIL's update algorithm but uses a different representation.
Reference: <author> Jaakkola, T., Singh, S. P., and Jordan, M. I. </author> <year> (1995). </year> <title> Reinforcement learning algorithm for partially observable Markov decision problems. </title> <editor> In Tesauro, G., Touretzky, D. S., and Leen, T. K., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <pages> pages 345-352. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: The program uses memory cells to distinguish between ambiguous observations at steps 1-7 and 22-28 in an almost deterministic manner. In steps 12, 15, 18, and 21, however, the agent's policy involves a high degree of stochas-ticity <ref> (compare Jaakkola, Singh, & Jordan, 1995) </ref>. Since successive populations generally contain multiple copies of such a program, the shortest path will be discovered within a reasonable time. It can then be stored separately.
Reference: <author> Kaelbling, L., Littman, M., and Cassandra, A. </author> <year> (1995). </year> <title> Planning and acting in partially observable stochastic domains. </title> <type> (unpublished technical report). </type> <institution> Brown University, Providence RI. </institution>
Reference: <author> Koza, J. R. </author> <year> (1992). </year> <title> Genetic Programming On the Programming of Computers by Means of Natural Selection. </title> <publisher> MIT Press. </publisher>
Reference-contexts: Programs with high quality have higher probability of being selected than others. The procedure is repeated for a fixed number of generations or until a satisfactory solution has been found. Koza's GP variant <ref> (Koza, 1992) </ref> encodes programs in parse trees. So does PIPE. Thus, both can be applied to the same problems. GP, however, stores domain knowledge in program populations, whereas PIPE captures this knowledge in a probability distribution. <p> Generic Random Constants. A generic random constant (GRC) (compare also "ephemeral random constant" <ref> (Koza, 1992) </ref>) is a zero argument function (a terminal). When accessed during program creation, it is either instantiated to a random value from a predefined, problem-dependent set of constants or a value previously stored in the PPT (see below). GRCs are useful for solving certain function regression problems. Program Representation. <p> Good parameters for GP are population size = 2000, crossover rate = 0.9, maximal tree depth = 20, initial depth = 2-6 with "half-and-half population initialization" and "over-selection" <ref> (see Koza, 1992) </ref>. Results. Twenty-one independent test runs were conducted for each algorithm. The program with best generalization performance GEN (Prog) = 1.18 was found by PIPE after 99,390 program evaluations. <p> For GP: population size = 2000, crossover rate = 0.9, maximal tree depth = 10, initial depth = 2-6 with "half-and-half population initialization" and "over-selection" <ref> (see Koza, 1992) </ref>. The best GP parameters we found are similar to those used in the function approximation experiment (we tried many combinations). <p> We are currently comparing PIPE and TD-Q learning with linear neural networks on large POE tasks and multiagent learning (Sa lustowicz, Wiering, & Schmidhuber, 1997). There are also many yet untried PIPE variants. For instance, we may apply PIPE to programs with automatically defined functions (ADFs) <ref> (Koza, 1992) </ref> or to programs with even more general jump instructions (Dickmanns et al., 1987). Instead of coding programs by parse trees we may also use grids or directed acyclic graphs (DAGs).
Reference: <author> Levin, L. A. </author> <year> (1973). </year> <title> Universal sequential search problems. </title> <journal> Problems of Information Transmission, </journal> <volume> 9(3) </volume> <pages> 265-266. </pages>
Reference-contexts: The distributions are modified either by a fixed learning algorithm such as Adaptive Levin Search (ALS) (Wiering & 1 Schmidhuber, 1996b; Schmidhuber et al., 1997b) or by an evolving, self-modifying, probabilistic learning algorithm (SMPLA) embedded within the distributions themselves (Schmidhuber et al., 1997a). ALS extends Levin Search (LS) <ref> (Levin, 1973, 1984) </ref>, a theoretically optimal algorithm for nonincremental search in program space, to the incremental case. SMPLAs go one step further they try to improve even the way they learn. <p> There may be ways of extracting additional information that is implicit in the population, particularly in cases where parallel evaluation does not save time. (4) Unlike nonincremental Levin search (LS) <ref> (Levin, 1973, 1984) </ref>, PIPE does not have an optimal way of allocating computation time to programs that do not halt or whose runtimes are unknown. (5) As with with most other comparable algorithms, several control parameters need to be set heuristically. Ongoing and Future Work.
Reference: <author> Levin, L. A. </author> <year> (1984). </year> <title> Randomness conservation inequalities: Information and independence in mathematical theories. </title> <journal> Information and Control, </journal> <volume> 61 </volume> <pages> 15-37. </pages>
Reference: <author> Li, M. and Vitanyi, P. M. B. </author> <year> (1993). </year> <title> An Introduction to Kolmogorov Complexity and its Applications. </title> <publisher> Springer-Verlag New York. </publisher>
Reference: <author> Lin, L. J. </author> <year> (1993). </year> <title> Reinforcement Learning for Robots Using Neural Networks. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, Pittsburgh, PA. </institution> <note> Technical Report CMU-CS-93-103. </note>
Reference: <author> Littman, M. </author> <year> (1994). </year> <title> Memoryless policies: Theoretical limitations and practical results. </title> <editor> In Cliff, D., Husbands, P., Meyer, J. A., and Wilson, S. W., editors, </editor> <booktitle> Proceedings of the International Conference on Simulation of Adaptive Behavior:From Animals to Animats 3, </booktitle> <pages> pages 297-305. </pages> <publisher> MIT Press/Bradford Books. </publisher>
Reference: <author> McCallum, R. A. </author> <year> (1996). </year> <title> Learning to use selective attention and short-term memory in sequential tasks. </title> <editor> In Maes, P., Mataric, M., Meyer, J.-A., Pollack, J., and Wilson, S. W., editors, </editor> <booktitle> From Animals to Animats 4: Proceedings of the Fourth International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pages 315-324. </pages> <publisher> MIT Press, Bradford Books. </publisher>
Reference: <author> O'Reilly, U.-M. </author> <year> (1995). </year> <title> An Analysis of Genetic Programming. </title> <type> PhD thesis, </type> <institution> Carleton University, </institution> <address> Ottawa, Ontario. </address>
Reference: <author> Ring, M. B. </author> <year> (1995). </year> <title> Continual Learning in Reinforcement Environments. </title> <editor> R. </editor> <publisher> Oldenbourg Verlag, Munchen, Wien. </publisher>
Reference: <author> Sa lustowicz, R. P., Wiering, M. A., and Schmidhuber, J. </author> <year> (1997). </year> <title> On learning soccer strategies. </title> <booktitle> In Proceedings of the 7th International Conference on Artificial Neural Networks (ICANN'97), Lecture Notes in Computer Science, </booktitle> <pages> pages 769-774. </pages> <publisher> Springer-Verlag Berlin Heidelberg. </publisher>
Reference-contexts: Ongoing and Future Work. More experiments with varying instruction sets are needed to better analyze PIPE's adaptation dynamics. We are currently comparing PIPE and TD-Q learning with linear neural networks on large POE tasks and multiagent learning <ref> (Sa lustowicz, Wiering, & Schmidhuber, 1997) </ref>. There are also many yet untried PIPE variants. For instance, we may apply PIPE to programs with automatically defined functions (ADFs) (Koza, 1992) or to programs with even more general jump instructions (Dickmanns et al., 1987).
Reference: <author> Schmidhuber, J. </author> <year> (1991). </year> <title> Reinforcement learning in Markovian and non-Markovian environments. </title> <editor> In Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 500-506. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Schmidhuber, J. </author> <year> (1994). </year> <title> On learning how to learn learning strategies. </title> <type> Technical Report FKI-198-94, </type> <institution> Fakultat fur Informatik, Technische Universitat Munchen. </institution>
Reference: <author> Schmidhuber, J. </author> <year> (1997). </year> <title> Discovering neural nets with low Kolmogorov complexity and high generalization capability. </title> <booktitle> Neural Networks, </booktitle> <volume> 10(5) </volume> <pages> 857-873. </pages>
Reference-contexts: Ongoing and Future Work. More experiments with varying instruction sets are needed to better analyze PIPE's adaptation dynamics. We are currently comparing PIPE and TD-Q learning with linear neural networks on large POE tasks and multiagent learning <ref> (Sa lustowicz, Wiering, & Schmidhuber, 1997) </ref>. There are also many yet untried PIPE variants. For instance, we may apply PIPE to programs with automatically defined functions (ADFs) (Koza, 1992) or to programs with even more general jump instructions (Dickmanns et al., 1987).
Reference: <author> Schmidhuber, J., Zhao, J., and Schraudolph, N. </author> <year> (1997a). </year> <title> Reinforcement learning with self-modifying policies. </title> <editor> In Thrun, S. and Pratt, L., editors, </editor> <title> Learning to learn. </title> <type> Kluwer. </type> <note> in press. 16 Schmidhuber, </note> <author> J., Zhao, J., and Wiering, M. </author> <year> (1997b). </year> <title> Shifting inductive bias with success-story algorithm, adaptive Levin search, and incremental self-improvement. </title> <journal> Machine Learning, </journal> <volume> 28 </volume> <pages> 105-130. </pages>
Reference-contexts: The distributions are modified either by a fixed learning algorithm such as Adaptive Levin Search (ALS) (Wiering & 1 Schmidhuber, 1996b; Schmidhuber et al., 1997b) or by an evolving, self-modifying, probabilistic learning algorithm (SMPLA) embedded within the distributions themselves <ref> (Schmidhuber et al., 1997a) </ref>. ALS extends Levin Search (LS) (Levin, 1973, 1984), a theoretically optimal algorithm for nonincremental search in program space, to the incremental case. SMPLAs go one step further they try to improve even the way they learn.
Reference: <author> Solomonoff, R. </author> <year> (1986). </year> <title> An application of algorithmic probability to problems in artificial intelligence. </title> <editor> In Kanal, L. N. and Lemmer, J. F., editors, </editor> <booktitle> Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 473-491. </pages> <publisher> Elsevier Science Publishers. </publisher>
Reference: <author> Teller, A. </author> <year> (1994). </year> <title> The evolution of mental models. </title> <editor> In Kinnear, Jr., K. E., editor, </editor> <booktitle> Advances in Genetic Programming, </booktitle> <pages> pages 199-219. </pages> <publisher> MIT Press. </publisher>
Reference-contexts: Their policy space consists of complete algorithms allowing for temporary memory, and they search policy space directly. Members of this class are Levin Search (Levin, 1973, 1984; Solomonoff, 1986; Li & Vitanyi, 1993; Schmidhuber, 1997), Adaptive Levin Search (ALS) (Wiering & Schmidhuber, 1996b), GP with memory cells <ref> (e.g., Teller, 1994) </ref>, and PIPE with memory cells (see below). All those approaches generate and evaluate solution candidates in an off-line fashion. ALS, GP, and PIPE also update the generator on the basis of the evaluation results. <p> At any given time, MC i denotes the current real-valued contents of the i-th memory cell, i 2 f0::n MC 1g. All MC i are initialized with 0. MCs are accessed by write and read functions that are added to F <ref> (see also Teller, 1994) </ref>. Function set M (arg 1 ; arg 2 ) sets MC (jround (arg 1 )j mod n MC ) := arg 2 and returns arg 2 . Function get M (arg 1 ) returns MC (jround (arg 1 )j mod n MC ) . Experimental Setup.
Reference: <author> Watkins, C. </author> <year> (1989). </year> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> King's College, </institution> <address> Cambridge. </address>
Reference: <author> Whitehead, S. and Ballard, D. H. </author> <year> (1990). </year> <title> Active perception and reinforcement learning. </title> <journal> Neural Computation, </journal> <volume> 2(4) </volume> <pages> 409-419. </pages>
Reference: <author> Wiering, M. A. and Schmidhuber, J. </author> <year> (1996a). </year> <title> HQ-Learning: Discovering Markovian subgoals for non-Markovian reinforcement learning. </title> <type> Technical Report IDSIA-96-96, </type> <institution> IDSIA. </institution>
Reference: <author> Wiering, M. A. and Schmidhuber, J. </author> <year> (1996b). </year> <title> Solving POMDPs with Levin search and EIRA. </title> <editor> In Saitta, L., editor, </editor> <booktitle> Machine Learning: Proceedings of the Thirteenth International Conference, </booktitle> <pages> pages 534-542. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Francisco, CA. </address>
Reference-contexts: Methods from class II do not require EFs. Their policy space consists of complete algorithms allowing for temporary memory, and they search policy space directly. Members of this class are Levin Search (Levin, 1973, 1984; Solomonoff, 1986; Li & Vitanyi, 1993; Schmidhuber, 1997), Adaptive Levin Search (ALS) <ref> (Wiering & Schmidhuber, 1996b) </ref>, GP with memory cells (e.g., Teller, 1994), and PIPE with memory cells (see below). All those approaches generate and evaluate solution candidates in an off-line fashion. ALS, GP, and PIPE also update the generator on the basis of the evaluation results.
Reference: <author> Zhao, J. and Schmidhuber, J. </author> <year> (1996). </year> <title> Incremental self-improvement for life-time multi-agent reinforcement learning. </title> <editor> In Maes, P., Mataric, M., Meyer, J.-A., Pollack, J., and Wilson, S. W., editors, </editor> <booktitle> From Animals to Animats 4: Proceedings of the Fourth International Conference on Simulation of Adaptive Behavior, </booktitle> <address> Cambridge, MA, </address> <pages> pages 516-525. </pages> <publisher> MIT Press, Bradford Books. </publisher> <pages> 17 </pages>
References-found: 30

