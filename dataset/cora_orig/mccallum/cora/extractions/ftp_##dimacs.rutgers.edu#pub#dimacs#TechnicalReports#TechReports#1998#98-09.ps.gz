URL: ftp://dimacs.rutgers.edu/pub/dimacs/TechnicalReports/TechReports/1998/98-09.ps.gz
Refering-URL: http://dimacs.rutgers.edu/TechnicalReports/1998.html
Root-URL: http://www.cs.rutgers.edu
Email: E-mail: fekin,hammer,kogang@rutcor.rutgers.edu  
Author: Peter L. Hammer a Alexander Kogan b;a 
Keyword: Convexity and Logical Analysis of Data  
Address: P.O. Box 5062, New Brunswick, NJ 08903-5062  Ave., Newark, NJ 07102  
Affiliation: a RUTCOR, Rutgers University,  b Accounting and Information Systems, Faculty of Management, Rutgers University, 180 University  
Note: by Oya Ekin a  DIMACS is a partnership of Rutgers University, Princeton University, AT&T Labs-Research, Bell Labs and Bellcore. DIMACS is an NSF Science and Technology Center, funded under contract STC-91-19999; and also receives support from the New Jersey Commission on Science and Technology.  
Abstract: DIMACS Technical Report 98-09 January 1998 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Anthony and N. Biggs. </author> <title> Computational Learning Theory, </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1992. </year>
Reference-contexts: terms string term string mode pointer to terms next list of terms List1,List2 terms headlist1,current,convhull List1 = [T 1 ; T 2 ; : : : ; T m ] for i = 1 to m List1 [i].mode=oldterm List2 = [] Algorithm: begin fmaing while (List1 &lt;&gt; []) do headlist1=List1 <ref> [1] </ref> current=List1 [2] while (current &lt;&gt; ;) do if d (headlist1.term,current.term) k let convhull=[headlist1.term,current.term] delete headlist1 and current from List1 push convhull to List1 List1 [1].mode=newterm headlist1=List1 [1] current=List1 [2] else current=current.next if headlist1.mode=newterm current=List2 [1] while (current &lt;&gt; ;) do if d (headlist1.term,current.term) k let convhull=[headlist1,current] delete headlist1 from List1 <p> T m ] for i = 1 to m List1 [i].mode=oldterm List2 = [] Algorithm: begin fmaing while (List1 &lt;&gt; []) do headlist1=List1 <ref> [1] </ref> current=List1 [2] while (current &lt;&gt; ;) do if d (headlist1.term,current.term) k let convhull=[headlist1.term,current.term] delete headlist1 and current from List1 push convhull to List1 List1 [1].mode=newterm headlist1=List1 [1] current=List1 [2] else current=current.next if headlist1.mode=newterm current=List2 [1] while (current &lt;&gt; ;) do if d (headlist1.term,current.term) k let convhull=[headlist1,current] delete headlist1 from List1 delete current from List2 push convhull to List1 List1 [1].mode=newterm headlist1=List1 [1] current=List2 [1] else current=current.next delete headlist1 from List1 push headlist1 to List2 end fmaing - <p> m List1 [i].mode=oldterm List2 = [] Algorithm: begin fmaing while (List1 &lt;&gt; []) do headlist1=List1 <ref> [1] </ref> current=List1 [2] while (current &lt;&gt; ;) do if d (headlist1.term,current.term) k let convhull=[headlist1.term,current.term] delete headlist1 and current from List1 push convhull to List1 List1 [1].mode=newterm headlist1=List1 [1] current=List1 [2] else current=current.next if headlist1.mode=newterm current=List2 [1] while (current &lt;&gt; ;) do if d (headlist1.term,current.term) k let convhull=[headlist1,current] delete headlist1 from List1 delete current from List2 push convhull to List1 List1 [1].mode=newterm headlist1=List1 [1] current=List2 [1] else current=current.next delete headlist1 from List1 push headlist1 to List2 end fmaing - 12 - Proposition 3.10 Let a Boolean function <p> let convhull=[headlist1.term,current.term] delete headlist1 and current from List1 push convhull to List1 List1 <ref> [1] </ref>.mode=newterm headlist1=List1 [1] current=List1 [2] else current=current.next if headlist1.mode=newterm current=List2 [1] while (current &lt;&gt; ;) do if d (headlist1.term,current.term) k let convhull=[headlist1,current] delete headlist1 from List1 delete current from List2 push convhull to List1 List1 [1].mode=newterm headlist1=List1 [1] current=List2 [1] else current=current.next delete headlist1 from List1 push headlist1 to List2 end fmaing - 12 - Proposition 3.10 Let a Boolean function f be represented by a DNF . <p> delete headlist1 and current from List1 push convhull to List1 List1 <ref> [1] </ref>.mode=newterm headlist1=List1 [1] current=List1 [2] else current=current.next if headlist1.mode=newterm current=List2 [1] while (current &lt;&gt; ;) do if d (headlist1.term,current.term) k let convhull=[headlist1,current] delete headlist1 from List1 delete current from List2 push convhull to List1 List1 [1].mode=newterm headlist1=List1 [1] current=List2 [1] else current=current.next delete headlist1 from List1 push headlist1 to List2 end fmaing - 12 - Proposition 3.10 Let a Boolean function f be represented by a DNF . Then the k-convex hull [] k is the (unique) irredundant prime DNF of the k-convex envelope [f ] k . <p> , return b else if b a = 1 return a k = b a+b apply k-convexification method () if [] k does not cover any point from F , search (k; b) else search (a; k 1) end fsearchg We shall show that each P i for i 2 <ref> [1; : : : ; s] </ref> is majorized by a Q j for some j 2 [1; : : : ; t]. Assume to the contrary that none of the Q j 's is a majorant of P 1 . <p> k-convexification method () if [] k does not cover any point from F , search (k; b) else search (a; k 1) end fsearchg We shall show that each P i for i 2 [1; : : : ; s] is majorized by a Q j for some j 2 <ref> [1; : : : ; t] </ref>. Assume to the contrary that none of the Q j 's is a majorant of P 1 . <p> We can show in a similar way that every term Q j for j 2 <ref> [1; : : : ; t] </ref> is a majorant of a P k for some k 2 [1; : : : ; s]. Assume to the contrary that Q 1 is not a majorant of any of the P i 's. <p> We can show in a similar way that every term Q j for j 2 [1; : : : ; t] is a majorant of a P k for some k 2 <ref> [1; : : : ; s] </ref>. Assume to the contrary that Q 1 is not a majorant of any of the P i 's. Since f u is a theory, Q 1 must cover a true point from T , say x. <p> Additionally, using Proposition 2.5 we can output all such unclassified points in time polynomial in their total number. 6 Probabilistic Properties In order to analyze the predictive performance of our algorithms on random data, we shall follow the probably approximately correct (PAC) model of computational learning theory (see e.g. <ref> [5, 1, 14] </ref>), which assumes that data points are generated randomly according to a fixed unknown probability distribution on B n , and that they are classified by some unknown Boolean function f belonging to a class C (n) of Boolean functions. <p> The results obtained above allow us to characterized the PAC-learnability of the classes of k-convex functions using upper and lower bounds on the sample complexity of PAC-learning algorithms. The following result was shown in [10] (see also <ref> [1, 14] </ref>). Proposition 6.9 Any PAC-learning algorithm for a class C (n) may need to draw a random sample of at least ( d (C (n)) * ) points to satisfy the PAC-learning condition (3). The following result is well known in computational learning theory (see e.g. [5, 1, 14]). <p> Proposition 6.9 Any PAC-learning algorithm for a class C (n) may need to draw a random sample of at least ( d (C (n)) * ) points to satisfy the PAC-learning condition (3). The following result is well known in computational learning theory (see e.g. <ref> [5, 1, 14] </ref>). Proposition 6.10 Any Boolean function g 2 C (n) which correctly classifies a random sam ple of ( 1 jC (n)j ffi ) points satisfies the PAC-learning condition (3). Theorem 6.11 1. If k n 2 (n), then the class F (n; k) is not PAC-learnable. 2.
Reference: [2] <author> M.O. Ball and G.L. Nemhauser. </author> <title> Matroids and a reliability analysis problem, </title> <journal> Mathematics of Operations Research, </journal> <volume> 4 (1979), </volume> <pages> 132-143. - 25 </pages> - 
Reference-contexts: It is also known that there exist Boolean functions that have DNF representations of linear length (in the number of variables), but all of their orthogonal DNFs have exponential length (see <ref> [2] </ref>). Two DNFs are said to be orthogonal if each term of one is orthogonal to all the terms of the other. In the following we use the notation = O ( ) to denote that there exists a constant C such that C . <p> term string mode pointer to terms next list of terms List1,List2 terms headlist1,current,convhull List1 = [T 1 ; T 2 ; : : : ; T m ] for i = 1 to m List1 [i].mode=oldterm List2 = [] Algorithm: begin fmaing while (List1 &lt;&gt; []) do headlist1=List1 [1] current=List1 <ref> [2] </ref> while (current &lt;&gt; ;) do if d (headlist1.term,current.term) k let convhull=[headlist1.term,current.term] delete headlist1 and current from List1 push convhull to List1 List1 [1].mode=newterm headlist1=List1 [1] current=List1 [2] else current=current.next if headlist1.mode=newterm current=List2 [1] while (current &lt;&gt; ;) do if d (headlist1.term,current.term) k let convhull=[headlist1,current] delete headlist1 from List1 delete current <p> ] for i = 1 to m List1 [i].mode=oldterm List2 = [] Algorithm: begin fmaing while (List1 &lt;&gt; []) do headlist1=List1 [1] current=List1 <ref> [2] </ref> while (current &lt;&gt; ;) do if d (headlist1.term,current.term) k let convhull=[headlist1.term,current.term] delete headlist1 and current from List1 push convhull to List1 List1 [1].mode=newterm headlist1=List1 [1] current=List1 [2] else current=current.next if headlist1.mode=newterm current=List2 [1] while (current &lt;&gt; ;) do if d (headlist1.term,current.term) k let convhull=[headlist1,current] delete headlist1 from List1 delete current from List2 push convhull to List1 List1 [1].mode=newterm headlist1=List1 [1] current=List2 [1] else current=current.next delete headlist1 from List1 push headlist1 to List2 end fmaing - 12 - <p> Proof. The proof follows from the algorithm shown in Figure 5. We simply do a binary search for k on <ref> [2; : : : ; n] </ref>, verifying each time whether the pdBf has a k-convex extention by calling the algorithm described in the previous proof. <p> Since f u is a theory, Q 1 must cover a true point from T , say x. Let P k be the term covering this true point. By the argument above, P k Q r for some r 2 <ref> [2; : : : ; s] </ref>.
Reference: [3] <author> A. Beimel, F. Bergadano, N.H. Bshouty, E. Kushilevitz, and S. Varricchio. </author> <title> On the applications of multiplicity automata in learning. </title> <booktitle> In Proceedings of the 37th Annual IEEE Symposium on Foundations of Computer Science (FOCS'1996), </booktitle> <pages> 349-358. </pages>
Reference-contexts: Orthogonal DNFs play an important role in many areas, including operations research (see [13]), reliability theory (see [8, 16]), and computational learning theory (see <ref> [3] </ref>). This paper is devoted to the study of data sets which admit k-convex extensions. We provide polynomial algorithms for finding a k-convex extension of a given data set, if any, and for finding the maximum k for which a k-convex extension exists.
Reference: [4] <author> A. Blake. </author> <title> Canonical expressions in Boolean algebra, </title> <type> Ph.D. Thesis, </type> <institution> University of Chicago, </institution> <month> August </month> <year> 1937. </year>
Reference-contexts: It is easy to notice that all the DNFs produced at every step of the consensus method represent the same function as the original DNF. The following result plays a central role in the theory and applications of Boolean functions <ref> [4, 15] </ref>: Proposition 2.1 (Blake, Quine) The consensus method applied to an arbitrary DNF of a Boolean function f results in the DNF which is the disjunction of all the prime implicants of f .
Reference: [5] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M.K. Warmuth. </author> <title> Learnability and the Vapnik-Chervonenkis Dimension, </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 36 (4) (1989), </volume> <pages> 929-965. </pages>
Reference-contexts: Additionally, using Proposition 2.5 we can output all such unclassified points in time polynomial in their total number. 6 Probabilistic Properties In order to analyze the predictive performance of our algorithms on random data, we shall follow the probably approximately correct (PAC) model of computational learning theory (see e.g. <ref> [5, 1, 14] </ref>), which assumes that data points are generated randomly according to a fixed unknown probability distribution on B n , and that they are classified by some unknown Boolean function f belonging to a class C (n) of Boolean functions. <p> Proposition 6.9 Any PAC-learning algorithm for a class C (n) may need to draw a random sample of at least ( d (C (n)) * ) points to satisfy the PAC-learning condition (3). The following result is well known in computational learning theory (see e.g. <ref> [5, 1, 14] </ref>). Proposition 6.10 Any Boolean function g 2 C (n) which correctly classifies a random sam ple of ( 1 jC (n)j ffi ) points satisfies the PAC-learning condition (3). Theorem 6.11 1. If k n 2 (n), then the class F (n; k) is not PAC-learnable. 2.
Reference: [6] <author> E. Boros, P.L. Hammer, T. Ibaraki, A. Kogan, E. Mayoraz, and I. Muchnik. </author> <title> An Implementation of Logical Analysis of Data, </title> <type> RUTCOR Research Report 22 - 96, </type> <institution> Rutgers University, </institution> <address> New Brunswick, NJ, </address> <year> 1996. </year>
Reference-contexts: The basic concepts of LAD are introduced in [9], and an implementation of LAD is described in <ref> [6] </ref>. A typical data set will usually have exponentially many extensions. In the absence of any additional information about the properties of the data set, the choice of an extension would be totally arbitrary, and therefore would risk to omit the most significant features of the data set.
Reference: [7] <author> H. Chernoff. </author> <title> A measure of asymptotic efficiency for tests of a hypothesis based on the sum of observations, </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 23 (1952), </volume> <pages> 493-509. </pages>
Reference-contexts: Corollary 6.4 If k n 2 (n), then 1. d (F (n; k)) 2 (n) ; . Proof. Using Chernoff's bound <ref> [7] </ref> X 2 m n ! n for 0 m 2 if we let k = n 2 m + 1, then d (F (n; k)) P k i 2 n n 2m 2 2 Lemma 6.5 The number of functions in the class F (n; k) is bounded in the
Reference: [8] <author> C.J. Colbourn. </author> <title> The Combinatorics of Network Reliability, </title> <publisher> Oxford University Press, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: It turns out that k-convex functions (k 2) can be characterized by the property that their prime implicants are pairwise strongly orthogonal, i.e. they "conflict" in at least k + 1 literals. Orthogonal DNFs play an important role in many areas, including operations research (see [13]), reliability theory (see <ref> [8, 16] </ref>), and computational learning theory (see [3]). This paper is devoted to the study of data sets which admit k-convex extensions. We provide polynomial algorithms for finding a k-convex extension of a given data set, if any, and for finding the maximum k for which a k-convex extension exists.
Reference: [9] <author> Y. Crama, P.L. Hammer, and T. Ibaraki. </author> <title> Cause-effect relationships and partially defined Boolean functions, </title> <journal> Annals of Operations Research, </journal> <volume> 16 (1988), </volume> <pages> 299-326. </pages>
Reference-contexts: The basic concepts of LAD are introduced in <ref> [9] </ref>, and an implementation of LAD is described in [6]. A typical data set will usually have exponentially many extensions.
Reference: [10] <author> A. Ehrenfeucht, D. Haussler, M. Kearns, and L. Valiant. </author> <title> A general lower bound on the number of examples needed for learning, </title> <journal> Information and Computation, </journal> <volume> 82 (3) (1989), </volume> <pages> 247-261. </pages>
Reference-contexts: The results obtained above allow us to characterized the PAC-learnability of the classes of k-convex functions using upper and lower bounds on the sample complexity of PAC-learning algorithms. The following result was shown in <ref> [10] </ref> (see also [1, 14]). Proposition 6.9 Any PAC-learning algorithm for a class C (n) may need to draw a random sample of at least ( d (C (n)) * ) points to satisfy the PAC-learning condition (3).
Reference: [11] <author> O. Ekin, P.L. Hammer, and A. Kogan. </author> <title> On Connected Boolean Functions, </title> <type> Rutcor Research Report RRR 33 - 96, </type> <institution> Rutgers University, </institution> <address> New Brunswick, NJ, </address> <year> 1996. </year>
Reference-contexts: Convex Boolean functions were introduced and studied in <ref> [11] </ref>. In this paper, we shall extend this concept to the case of pdBfs. For the presentation that follows, we shall need several definitions. <p> The extremely powerful requirement of convexity puts a severe limitation on the number of functions with this property. In order to provide more flexibility, we introduced in <ref> [11] </ref> the following relaxation of the definition. Definition 3.2 For any integer k 2 f2; : : : ; ng, a Boolean function f is called k-convex if and only if any pair of true points at distance at most k is convexly connected. <p> Example 3.3 The function x 1 x 2 x 3 _ x 1 x 2 x 3 is 2-convex. The following results are obtained in <ref> [11] </ref> and presented here for the sake of completeness. Proposition 3.4 For any k 2, a Boolean function f is k-convex if and only if any two prime implicants of f conflict in at least k + 1 literals. <p> Note that this algorithm stops when any two terms conflict in at least k + 1 literals. The pseudo-code given in Figure 3 provides a more formal representation of the k convexification method. It has been shown in <ref> [11] </ref> that the k-convex hull of a DNF represents the k-convex envelope of the function represented by the given DNF. Let [] k denote the k-convex hull of . <p> In many situations negative points may also possess the same convexity property, i.e. every pair of false points at distance at most k 0 is convexly connected. It was shown in <ref> [11] </ref> that there are only n + 2 distinct fully defined Boolean functions of n variables with the property that both the set of true points and the set of false points are - 18 - 2-convex.
Reference: [12] <author> F. Harary, </author> <title> Graph Theory, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1969. </year>
Reference-contexts: This graph is bipartite: the terms containing x i belong to the first part, the terms containing x i belong to the second part, and the terms not involving x i can be considered as belonging to either part. It follows from Turan's theorem (see e.g. <ref> [12] </ref>) that the number of edges in G i is limited by s 2 4 .
Reference: [13] <author> P.L. Hammer and S. Rudeanu. </author> <title> Boolean Methods in Operations Research, </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, Heidelberg, New York, </address> <year> 1968. </year>
Reference-contexts: It turns out that k-convex functions (k 2) can be characterized by the property that their prime implicants are pairwise strongly orthogonal, i.e. they "conflict" in at least k + 1 literals. Orthogonal DNFs play an important role in many areas, including operations research (see <ref> [13] </ref>), reliability theory (see [8, 16]), and computational learning theory (see [3]). This paper is devoted to the study of data sets which admit k-convex extensions.
Reference: [14] <author> M.J. Kearns and U.V. Vazirani. </author> <title> An Introduction to Computational Learning Theory, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: Additionally, using Proposition 2.5 we can output all such unclassified points in time polynomial in their total number. 6 Probabilistic Properties In order to analyze the predictive performance of our algorithms on random data, we shall follow the probably approximately correct (PAC) model of computational learning theory (see e.g. <ref> [5, 1, 14] </ref>), which assumes that data points are generated randomly according to a fixed unknown probability distribution on B n , and that they are classified by some unknown Boolean function f belonging to a class C (n) of Boolean functions. <p> The results obtained above allow us to characterized the PAC-learnability of the classes of k-convex functions using upper and lower bounds on the sample complexity of PAC-learning algorithms. The following result was shown in [10] (see also <ref> [1, 14] </ref>). Proposition 6.9 Any PAC-learning algorithm for a class C (n) may need to draw a random sample of at least ( d (C (n)) * ) points to satisfy the PAC-learning condition (3). The following result is well known in computational learning theory (see e.g. [5, 1, 14]). <p> Proposition 6.9 Any PAC-learning algorithm for a class C (n) may need to draw a random sample of at least ( d (C (n)) * ) points to satisfy the PAC-learning condition (3). The following result is well known in computational learning theory (see e.g. <ref> [5, 1, 14] </ref>). Proposition 6.10 Any Boolean function g 2 C (n) which correctly classifies a random sam ple of ( 1 jC (n)j ffi ) points satisfies the PAC-learning condition (3). Theorem 6.11 1. If k n 2 (n), then the class F (n; k) is not PAC-learnable. 2.
Reference: [15] <author> W. Quine. </author> <title> A way to simplify truth functions, </title> <journal> American Mathematical Monthly, </journal> <volume> 62 (1955), </volume> <pages> 627-631. </pages>
Reference-contexts: It is easy to notice that all the DNFs produced at every step of the consensus method represent the same function as the original DNF. The following result plays a central role in the theory and applications of Boolean functions <ref> [4, 15] </ref>: Proposition 2.1 (Blake, Quine) The consensus method applied to an arbitrary DNF of a Boolean function f results in the DNF which is the disjunction of all the prime implicants of f .
Reference: [16] <author> K.G. Ramamurthy. </author> <title> Coherent Structures and Simple Games, </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Dordrecht, Boston, </address> <year> 1990. </year>
Reference-contexts: It turns out that k-convex functions (k 2) can be characterized by the property that their prime implicants are pairwise strongly orthogonal, i.e. they "conflict" in at least k + 1 literals. Orthogonal DNFs play an important role in many areas, including operations research (see [13]), reliability theory (see <ref> [8, 16] </ref>), and computational learning theory (see [3]). This paper is devoted to the study of data sets which admit k-convex extensions. We provide polynomial algorithms for finding a k-convex extension of a given data set, if any, and for finding the maximum k for which a k-convex extension exists.
References-found: 16

