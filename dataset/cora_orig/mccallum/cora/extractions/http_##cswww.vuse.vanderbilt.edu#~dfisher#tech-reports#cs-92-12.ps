URL: http://cswww.vuse.vanderbilt.edu/~dfisher/tech-reports/cs-92-12.ps
Refering-URL: http://cswww.vuse.vanderbilt.edu/~dfisher/tech-reports/tr-92-12.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Pessimistic and Optimistic Induction  
Author: Doug Fisher 
Keyword: induction, experimental evaluation, tests of significance, pruning.  
Address: Box 1679, Station B  Nashville, TN 37235  
Affiliation: Department of Computer Science  Vanderbilt University  
Pubnum: Technical Report CS-92-12  
Email: dfisher@vuse.vanderbilt.edu  
Phone: 615-343-4111  
Web: http://www.vuse.vanderbilt.edu/~dfisher/dfisher.html  
Abstract: Learning methods vary in the optimism or pessimism with which they regard the informativeness of learned knowledge. Pessimism is implicit in hypothesis testing, where we wish to draw cautious conclusions from experimental evidence. However, this paper demonstrates that optimism in the utility of derived rules may be the preferred bias for learning systems themselves. We examine the continuum between naive pessimism and naive optimism in the context of a decision tree learner that prunes rules based on stringent (i.e., pessimistic) or weak (i.e., optimistic) tests of their significance. Our experimental results indicate that in most cases optimism is preferred, but particularly in cases of sparse training data and high noise. This work generalizes earlier findings by Fisher and Schlimmer (1988) and Schaffer (1992), and we discuss its relevance to unsupervised learning, small disjuncts, and other issues. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. W., Kibler, D., & Albert, M. K. </author> <year> (1991). </year> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> 37-66. </pages>
Reference-contexts: Even in cases of uniclass leaves where possible information gain is 0:0, further specialization would occur that discriminated individual objects. More generally, approaches that search in a specific-to-general manner, particularly instance-based ap 18 proaches <ref> (Aha, Kibler, & Albert, 1991) </ref>, have an implicit optimistic processing bias - spe-cific instances are exploited until they are deemed unhelpful.
Reference: <author> Clark, P., & Niblett, T. </author> <year> (1989). </year> <title> The Cn2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 261-284. </pages>
Reference: <author> Fisher, D. </author> <year> (1989). </year> <title> Noise-tolerant conceptual clustering. </title> <booktitle> Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 825-830). </pages> <address> Detroit, MI: </address> <publisher> Morgan Kaufmann. 19 Fisher, </publisher> <editor> D., & Schlimmer, J. </editor> <year> (1988). </year> <title> Concept simplification and prediction accuracy. </title> <booktitle> Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <pages> (pp. 22-28). </pages> <address> Ann Arbor, MI: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Cn2 uses prospective pruning based on chi-square with high confidence requirements, while Greedy3 and Grove rely on retrospective pruning using resampling. 15 5.4 Unsupervised Learning Optimism and pessimism have also been explored in the context of unsupervised systems, notably Cobweb <ref> (Fisher, 1989) </ref>, where the task is to make predictions along all attributes that might be unknown in test objects. Fisher experimented with chi-squared pruning at different confidence levels in a manner similar to the experiments reported here.
Reference: <author> Holte, R. C., Acker, L. E., & Porter, B. W. </author> <year> (1989). </year> <title> Concept learning and the accuracy of small disjuncts. </title> <booktitle> Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 813-818). </pages> <address> Detroit, MI: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Lopez de Mantaras, R. </author> <year> (1991). </year> <title> A distance-based attribute selection measure for decision tree induction. </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> 81-92. </pages>
Reference: <author> Michalski, R. </author> <year> (1987). </year> <title> How to learn imprecise concepts. </title> <booktitle> Proceedings of the Fourth International Workshop on Machine Learning, </booktitle> <pages> (pp. 50-58). </pages> <address> Irvine, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Mingers, J. </author> <year> (1989). </year> <title> An empirical comparison of pruning methods for inductive learning. </title> <journal> Machine Learning, </journal> <volume> 4, </volume> <pages> 227-243. </pages>
Reference-contexts: The general trends are the same, but appears slightly less pronounced, due to the implicit procedural optimism of the retrospective approach. 5.2 Other Pruning Strategies There are many alternatives to chi-square pruning <ref> (Mingers, 1989) </ref>. Resampling is sometimes used in conjunction with retrospective pruning.
Reference: <author> Pagallo, G., & Haussler, D. </author> <year> (1990). </year> <title> Boolean feature discovery in empirical learning. </title> <journal> Machine Learning, </journal> <volume> 5, </volume> <pages> 71-100. </pages>
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 81-106. </pages>
Reference-contexts: The particular system that we use selects divisive attributes using Lopez de Mantaras's (1991) normalization of information gain <ref> (Quinlan, 1986) </ref>. This normalization guards against information gain's bias to select attributes with more values. <p> The target categories correspond to the political parties, Republican or Democrat. Our TDIDT implementation handles missing data in a test item by categorizing it down and combining evidence from multiple paths at a node whose divisive attribute value is not observed in the item <ref> (Quinlan, 1986) </ref>. The soybean-l domain includes 307 instances categorized with respect to 19 target categories and described along 35 attributes. Each category is a soybean plant disease. The 4 soybean-s domain is a subset of soybean-l, and includes only four target categories and 47 instances. <p> The data includes continuous attributes. Our TDIDT implementation evaluates continuous attributes by finding the the best binary split of a continuous attribute's values <ref> (Quinlan, 1986) </ref>. Finally, sick-s is a subset of sick-l with all sick-euthyroid instances and an equal number of randomly selected normal instances. Thus, 586 instances are evenly divided between classes.
Reference: <author> Quinlan, J. R. </author> <year> (1987). </year> <title> Simplifying decision trees. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 27, </volume> <pages> 221-234. </pages>
Reference-contexts: Intuitively, a standard strategy of pruning at internal nodes that yield greater accuracy makes sense, since we have tentative positive evidence that such nodes will perform similarly over other samples randomly drawn from the environment. This simple strategy, known as reduced error pruning <ref> (Quinlan, 1987) </ref>, falls in the middle between naive pessimism and naive optimism. 4 Prospective and retrospective pruning have been termed pre-pruning and post-pruning, respectively. 14 Other pruning methods have been motivated, in part by a desire to remove the re-quirement of a second test set in resampling.
Reference: <author> Quinlan, J. R. </author> <year> (1991). </year> <title> Improved estimates for the accuracy of small disjuncts. </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> 93-99. </pages>
Reference: <author> Schaffer, C. </author> <year> (1992). </year> <title> Sparse data and the effect of overfitting avoidance in decision tree induction. </title> <booktitle> Proceedings of the Tenth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 147-152). </pages> <address> San Jose, CA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Schlimmer, J., & Fisher, D. </author> <year> (1986). </year> <title> A case study of incremental concept induction. </title> <booktitle> Proceedings of the Fifth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 496-501). </pages> <address> Philadelphia, PA: </address> <publisher> Morgan Kaufmann. </publisher> <pages> 20 </pages>
References-found: 13

