URL: ftp://wol.ra.phy.cam.ac.uk/pub/www/mackay/mncEL.ps.gz
Refering-URL: http://131.111.48.24/mackay/README.html
Root-URL: 
Email: mackay@mrao.cam.ac.uk  radford@stat.toronto.edu  
Title: Near Shannon Limit Performance of Low Density Parity Check Codes  
Author: David J.C. MacKay Radford M. Neal 
Keyword: Indexing terms: Error-correction codes, probabilistic decoding.  
Note: July 12, 1996|To be published in Electronics Letters  
Address: Cambridge, CB3 0HE, United Kindom.  M5S 1A4, Canada.  
Affiliation: Cavendish Laboratory,  Depts. of Statistics and Computer Science, Univ. of Toronto,  
Abstract: We report the empirical performance of Gallager's low density parity check codes on Gaussian channels. We show that performance substantially better than that of standard convolutional and concatenated codes can be achieved; indeed the performance is almost as close to the Shannon limit as that of Turbo codes. A linear code may be described in terms of a generator matrix G or in terms of a parity check matrix H, which satisfies Hx = 0 for all codewords x. In 1962, Gallager reported work on binary codes defined in terms of low density parity check matrices (abbreviated `GL codes') [5, 6]. The matrix H was defined in a non-systematic form; each column of H had a small weight (e.g., 3) and the weight per row was also uniform; the matrix H was constructed at random subject to these constraints. Gallager proved distance properties of these codes and described a probability-based decoding algorithm with promising empirical performance. However it appears that GL codes have been generally forgotten, the assumption perhaps being that concatenated codes [4] were superior for practical purposes (R.G. Gallager, personal communication). During our work on MN codes [8] we realised that it is possible to create `good' codes from very sparse random matrices, and to decode them (even beyond their minimum distance) using approximate probabilistic algorithms. We eventually reinvented Gallager's decoding algorithm and GL codes. In this paper we report the empirical performance of these codes on Gaussian channels. We have proved theoretical properties of GL codes (essentially, that the channel coding theorem holds for them) elsewhere [9]. GL codes can also be defined over GF (q). We are currently implementing this generalization. We created sparse random parity check matrices in the following ways. Construction 1A. An M by N matrix (M rows, N columns) is created at random with weight per column t (e.g., t = 3), and weight per row as uniform as possible, and overlap between any two columns no greater than 1. (The weight of a column is the number of non-zero elements; the overlap between two columns is their inner product.) 
Abstract-found: 1
Intro-found: 0
Reference: [1] <author> G. Battail. </author> <title> We can think of good codes, and even decode them. </title> <editor> In P. Camion, P. Charpin, and S. Harari, editors, </editor> <booktitle> Eurocode '92. </booktitle> <address> Udine, Italy, </address> <month> 26-30 October, </month> <booktitle> number 339 in CISM Courses and Lectures, </booktitle> <pages> pages 353-368. </pages> <publisher> Springer, Wien, </publisher> <year> 1993. </year>
Reference: [2] <author> C. Berrou, A. Glavieux, and P. Thitimajshima. </author> <title> Near Shannon limit error-correcting coding and decoding: </title> <booktitle> Turbo-codes. In Proc. 1993 IEEE International Conference on Communications, </booktitle> <address> Geneva, Switzerland, </address> <pages> pages 1064-1070, </pages> <year> 1993. </year>
Reference: [3] <author> D. Divsilar and F. Pollara. </author> <title> On the design of turbo codes. </title> <type> Technical Report TDA 42-123, </type> <institution> Jet Propulsion Laboratory, Pasadena, </institution> <month> November </month> <year> 1995. </year>
Reference: [4] <author> G. D. Forney. </author> <title> Concatenated codes. </title> <type> Technical Report 37, </type> <institution> MIT, </institution> <year> 1966. </year>
Reference: [5] <author> R. G. Gallager. </author> <title> Low density parity check codes. </title> <journal> IRE Trans. Info. Theory, </journal> <volume> IT-8:21-28, </volume> <month> Jan </month> <year> 1962. </year>
Reference: [6] <author> R. G. Gallager. </author> <title> Low Density Parity Check Codes. Number 21 in Research monograph series. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1963. </year>
Reference: [7] <author> S. W. Golomb, R. E. Peile, and R. A. Scholtz. </author> <title> Basic Concepts in Information Theory and Coding: The Adventures of Secret Agent 00111. </title> <publisher> Plenum Press, </publisher> <address> New York, </address> <year> 1994. </year>
Reference: [8] <author> D. J. C. MacKay and R. M. Neal. </author> <title> Good codes based on very sparse matrices. In Colin Boyd, editor, Cryptography and Coding. </title> <booktitle> 5th IMA Conference, number 1025 in Lecture Notes in Computer Science, </booktitle> <pages> pages 100-111. </pages> <publisher> Springer, </publisher> <address> Berlin, </address> <year> 1995. </year>
Reference: [9] <author> D. J. C. MacKay and R. M. Neal. </author> <title> Good error correcting codes based on very sparse matrices. </title> <note> Available from http://wol.ra.phy.cam.ac.uk/, 1996. </note>
Reference: [10] <author> J. Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Mor gan Kaufmann, </publisher> <address> San Mateo, </address> <year> 1988. </year> <note> Version 2.0 4 </note>
References-found: 10

