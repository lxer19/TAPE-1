URL: http://www.cs.cmu.edu/afs/cs/user/chal/www/colt92.ps.gz
Refering-URL: http://www.cs.cmu.edu/~chal/Pubs/pub.html
Root-URL: 
Email: avrim@theory.cs.cmu.edu  chal@cs.cmu.edu  
Title: Learning Switching Concepts  
Author: Avrim Blum Prasad Chalasani 
Note: I presented this at COLT'92  
Address: Pittsburgh, PA 15213  Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  School of Computer Science Carnegie Mellon University  
Abstract: We consider learning in situations where the function used to classify examples may switch back and forth between a small number of different concepts during the course of learning. We examine several models for such situations: oblivious models in which switches are made independent of the selection of examples, and more adversarial models in which a single adversary controls both the concept switches and example selection. We show relationships between the more benign models and the p-concepts of Kearns and Schapire, and present polynomial-time algorithms for learning switches between two k-DNF formulas. For the most adversarial model, we present a model of success patterned after the popular competitive analysis used in studying on-line algorithms. We describe a randomized query algorithm for such adversarial switches between two monotone disjunctions that is "1-competitive" in that the total number of mistakes plus queries is with high probability bounded by the number of switches plus some fixed polynomial in n (the number of variables). We also use notions described here to provide sufficient conditions under which learning a p-concept class "with a decision rule" implies being able to learn the class "with a model of probability." 
Abstract-found: 1
Intro-found: 1
Reference: [AL88] <author> Dana Angluin and Philip Laird. </author> <title> Learning from noisy examples. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 343-370, </pages> <year> 1988. </year>
Reference-contexts: One of our motivations for studying mixtures is that we use algorithms for learning in this model in a critical way in our algorithms for the switching concept models. The mixture model is similar to the Angluin and Laird <ref> [AL88] </ref> noise model, except that here the "noise" is systematic in that it is consistent with some other con 1 All the results given for learning disjunctions without queries hold for k-CNF and k-DNF formulas as well, by standard transformations. cept in the class. <p> This is similar to Angluin and Laird's noise model <ref> [AL88] </ref> in which there is a single target concept, but each example has fixed probability of being classified by its complement. It is also a special case of Sloan's malicious misclassification (MMC) model [Slo88] in which with probability -, an adversary may decide the example's classification. <p> Angluin and Laird <ref> [AL88] </ref> show how one can use such an oracle to learn a class C in their random misclassification model with polynomial sample size. Sloan [Slo88] extends their result to the malicious misclassification model.
Reference: [ALRS92] <author> S. Ar, R.J. Lipton, R. Rubinfeld, and M. Su-dan. </author> <title> Reconstructing algebraic functions from mixed data. </title> <booktitle> In Proc. 33rd Annual Symp. on Foundations of Computer Science (FOCS), </booktitle> <pages> pages 503-512, </pages> <year> 1992. </year>
Reference-contexts: Our main goal is to produce polynomial-time algorithms for simple classes, though we will also discuss somewhat the use of a minimum disagreements oracle. Experimental work on learning interleaved functions has been done by E. Levin [Lev91]. Ar et al. <ref> [ALRS92] </ref> have examined a similar problem of identifying a set of polynomials over a finite field where each point is assigned a value by one of the polynomials. We consider two main models for how concept switches are made. The more benign one is the oblivious adversary model.
Reference: [BBK + 90] <author> Shai Ben-David, Allan Borodin, Richard M. Karp, Gabor Tardos, and Avi Wigderson. </author> <title> On the power of randomization in online algorithms. </title> <booktitle> In STOC90, </booktitle> <pages> pages 379-386, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Our style of analysis of this model is motivated by "competitiveness" models of on-line algorithms (e.g. [BLS87], [MMS88], <ref> [BBK + 90] </ref>). We imagine that the adversary pays a cost of 1 for each switch, and present a "1-competitive" randomized algorithm that uses membership queries for the class of monotone disjunctions. <p> Our analysis is patterned after the popular "competitive analysis" model of online algorithms (e.g. [BLS87], [MMS88], <ref> [BBK + 90] </ref>). We will charge the adversary a cost of 1 for each switch, and we will charge the algorithm for the total number of mistakes plus queries made. Note that if there are two concepts, an algorithm that knows both exactly need only make 1 mistake per switch.
Reference: [BLS87] <author> A. Borodin, N. Linial, and M. Saks. </author> <title> An optimal online algorithm for metrical task systems. </title> <booktitle> In ACM STOC, </booktitle> <year> 1987. </year>
Reference-contexts: Our style of analysis of this model is motivated by "competitiveness" models of on-line algorithms (e.g. <ref> [BLS87] </ref>, [MMS88], [BBK + 90]). We imagine that the adversary pays a cost of 1 for each switch, and present a "1-competitive" randomized algorithm that uses membership queries for the class of monotone disjunctions. <p> Our analysis is patterned after the popular "competitive analysis" model of online algorithms (e.g. <ref> [BLS87] </ref>, [MMS88], [BBK + 90]). We will charge the adversary a cost of 1 for each switch, and we will charge the algorithm for the total number of mistakes plus queries made.
Reference: [HL91] <author> D. Helmbold and P. </author> <title> Long. Tracking drifting concepts using random examples. </title> <booktitle> In Proceedings of the 4th Annual Conference on Computational Learning Theory, </booktitle> <pages> pages 267-280, </pages> <year> 1991. </year>
Reference-contexts: Instead of having a fixed concept, a better description of the situation may be that there are two concepts c 1 and c 2 , and classification occasionally switches between one and the other. This type of situation is similar to that considered by Helmbold and Long <ref> [HL91] </ref> in which a concept may drift over time, but there are several important differences between our focus and theirs.
Reference: [KL88] <author> Michael Kearns and Ming Li. </author> <title> Learning in the presence of malicious errors. </title> <booktitle> In Proceedings of the Twentieth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 267-280, </pages> <address> Chicago, Illinois, </address> <month> May </month> <year> 1988. </year>
Reference-contexts: As noted earlier, without queries this problem is at least as hard as the problem of learning disjunctions with worst-case false-negative errors, for which some evidence of intrinsic difficulty has been given by Kearns and Li <ref> [KL88] </ref>. Our analysis is patterned after the popular "competitive analysis" model of online algorithms (e.g. [BLS87], [MMS88], [BBK + 90]). We will charge the adversary a cost of 1 for each switch, and we will charge the algorithm for the total number of mistakes plus queries made.
Reference: [KS90] <author> Michael J. Kearns and Robert E. Schapire. </author> <title> Efficient distribution-free learning of probabilistic concepts. </title> <booktitle> In Proceedings of the Thirty-First Annual Symposium on Founda--tions of Computer Science, </booktitle> <pages> pages 382-391. </pages> <publisher> IEEE, </publisher> <year> 1990. </year>
Reference-contexts: This situation might occur if there is a relevant variable that is hidden from the learner, and its value is independent of the visible variables and biased in one direction (see <ref> [KS90] </ref>, [KSS]). We call such a situation a mixture of two concepts, and our goal is to approximate one or both of them. <p> It could also be thought of as a weaker version of Sloan's "malicious misclassification" model [Slo88] and actually is a special case of Kearns and Schapire's p-concepts <ref> [KS90] </ref>, though the notions of success are somewhat different. <p> For a disjunction c, define R (c) to be the set of all variables disjoined in c. So, R (c) is the set of relevant variables. A p-concept c (defined by Kearns and Schapire <ref> [KS90] </ref>) is a function over X with values in [0; 1], with the value of c (x) interpreted as a probability. An example x classified by c is given label 1 with probability c (x) and 0 with probability 1 c (x). <p> Under this filtered distribution, the Angluin-Laird style analysis can be applied, and the concepts learned can then be OR'ed with h 12 . In fact, a generalization of this procedure is used by Kearns and Schapire <ref> [KS90] </ref> to learn the class of "p-decision lists with decreasing probabilities," and it turns out we can directly use their theorems to get the results we need. 4.1 MIXTURES AS PROBABILISTIC DECISION LISTS We show here that mixtures of disjunctions can be written as a special kind of probabilistic concept called <p> we can directly use their theorems to get the results we need. 4.1 MIXTURES AS PROBABILISTIC DECISION LISTS We show here that mixtures of disjunctions can be written as a special kind of probabilistic concept called a "probabilistic decision list (p-DL) with decreasing probabilities," introduced by Kearns and Schapire in <ref> [KS90] </ref>. p-DL's are probabilistic analogs of standard decision lists [Riv87]. A 1-p-DL c is given by a list (x i 1 ; r 1 ); :::; (x i n ; r n ) where the x i j are distinct variables, and each r i 2 [0; 1]. <p> Kearns and Schapire <ref> [KS90] </ref> present an algorithm (which we call Learn-p-DL) that learns a p-DL that is an *-good model of probability of a target p-DL with decreasing probabilities. We show below how we can use their algorithm to solve our problem of learning monotone disjunctions from a mixture. <p> (:h 1 ^ h 0 ) is an *-approximation to c 2 under the original distribution D. 7 WHEN IS LEARNING A MODEL OF PROBABILITY AS EASY AS LEARNING A DECISION RULE? In addition to the notion of learning with a model of probability (see Section 2), Kearns and Schapire <ref> [KS90] </ref> define a weaker notion of learning a p-concept class with a decision rule. A "decision rule" is a standard f0; 1g-valued concept. Let R D (c; h) be the probability that decision rule h misclassifies an example x chosen from D and labeled according to c. <p> If C is (polynomially) learnable with a decision rule, then C is (polynomially) learnable with a model of probability. The condition of being closed under mixture with fT; F g is satisfied by many natural p-concept classes such as p-DL's (with decreasing probabilities) and the "nondecreasing functions" mentioned in <ref> [KS90] </ref>. Before prov ing Theorem 10, let us first state a simple lemma.
Reference: [KSS] <author> M. J. Kearns, R. E. Schapire, and L. M. Sel-lie. </author> <title> Toward efficient agnostic learning. </title> <booktitle> In Proc. 5th Annual Workshop on Computational Learning Theory. </booktitle>
Reference-contexts: This situation might occur if there is a relevant variable that is hidden from the learner, and its value is independent of the visible variables and biased in one direction (see [KS90], <ref> [KSS] </ref>). We call such a situation a mixture of two concepts, and our goal is to approximate one or both of them.
Reference: [Lev91] <author> E. Levin. </author> <title> Modeling time variant systems using hidden control neural architecture. </title> <booktitle> In Proc. Neural Inf. and Proc. Systems 3 (NIPS), </booktitle> <year> 1991. </year>
Reference-contexts: Our main goal is to produce polynomial-time algorithms for simple classes, though we will also discuss somewhat the use of a minimum disagreements oracle. Experimental work on learning interleaved functions has been done by E. Levin <ref> [Lev91] </ref>. Ar et al. [ALRS92] have examined a similar problem of identifying a set of polynomials over a finite field where each point is assigned a value by one of the polynomials. We consider two main models for how concept switches are made.
Reference: [MMS88] <author> Mark S. Manasse, Lyle A. McGeoch, and Daniel D. Sleator. </author> <title> Competitive algorithms for on-line problems. </title> <booktitle> In Proceedings of the 20th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 322-333, </pages> <month> May </month> <year> 1988. </year> <note> Server results covered in paper in Journal of Algorithms. </note>
Reference-contexts: Our style of analysis of this model is motivated by "competitiveness" models of on-line algorithms (e.g. [BLS87], <ref> [MMS88] </ref>, [BBK + 90]). We imagine that the adversary pays a cost of 1 for each switch, and present a "1-competitive" randomized algorithm that uses membership queries for the class of monotone disjunctions. <p> Our analysis is patterned after the popular "competitive analysis" model of online algorithms (e.g. [BLS87], <ref> [MMS88] </ref>, [BBK + 90]). We will charge the adversary a cost of 1 for each switch, and we will charge the algorithm for the total number of mistakes plus queries made.
Reference: [Riv87] <author> Ronald L. Rivest. </author> <title> Learning decision lists. </title> <journal> Machine Learning, </journal> <volume> 2(3) </volume> <pages> 229-246, </pages> <year> 1987. </year>
Reference-contexts: results we need. 4.1 MIXTURES AS PROBABILISTIC DECISION LISTS We show here that mixtures of disjunctions can be written as a special kind of probabilistic concept called a "probabilistic decision list (p-DL) with decreasing probabilities," introduced by Kearns and Schapire in [KS90]. p-DL's are probabilistic analogs of standard decision lists <ref> [Riv87] </ref>. A 1-p-DL c is given by a list (x i 1 ; r 1 ); :::; (x i n ; r n ) where the x i j are distinct variables, and each r i 2 [0; 1].
Reference: [Slo88] <author> Robert H. Sloan. </author> <title> Types of noise in data for concept learning. </title> <editor> In David Haussler and Leonard Pitt, editors, </editor> <booktitle> First Workshop on Computational Learning Theory, </booktitle> <pages> pages 91-96. </pages> <publisher> Morgan Kaufmann, </publisher> <month> August </month> <year> 1988. </year>
Reference-contexts: It could also be thought of as a weaker version of Sloan's "malicious misclassification" model <ref> [Slo88] </ref> and actually is a special case of Kearns and Schapire's p-concepts [KS90], though the notions of success are somewhat different. <p> This is similar to Angluin and Laird's noise model [AL88] in which there is a single target concept, but each example has fixed probability of being classified by its complement. It is also a special case of Sloan's malicious misclassification (MMC) model <ref> [Slo88] </ref> in which with probability -, an adversary may decide the example's classification. <p> So, if we first create a 2-DNF h 12 of all pairs x i x j such that no example setting both to 1 has appeared negative, then filtering D through h 12 (x) = 0 3 In Sloan <ref> [Slo88] </ref> it is claimed that Angluin and Laird's algorithm can be applied directly to the MMC model. <p> Angluin and Laird [AL88] show how one can use such an oracle to learn a class C in their random misclassification model with polynomial sample size. Sloan <ref> [Slo88] </ref> extends their result to the malicious misclassification model. Thus, Sloan's result immediately implies that given M D (C) we can learn the "majority concept" in any MIX (c 1 ; c 2 ; -) for c 1 ; c 2 2 C and - &lt; 1=2.

References-found: 12

