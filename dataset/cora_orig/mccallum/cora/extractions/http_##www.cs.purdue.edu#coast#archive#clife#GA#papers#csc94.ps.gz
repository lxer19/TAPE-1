URL: http://www.cs.purdue.edu/coast/archive/clife/GA/papers/csc94.ps.gz
Refering-URL: http://www.cs.purdue.edu/coast/archive/clife/GA/papers/
Root-URL: http://www.cs.purdue.edu
Email: khuri@sjsumcs.sjsu.edu  fbaeck,jokeg@ls11.informatik.uni-dortmund.de  
Title: An Evolutionary Approach to Combinatorial Optimization Problems  
Author: Sami Khuri Thomas Back and Jorg Heitkotter 
Date: March 8-10, 1994.  
Note: Copyright c 1993 ACM Press. All rights reserved. To appear in the proceedings of CSC'94 Phoenix Arizona,  
Address: One Washington Square San Jose, CA 95192-0103, U.S.A.  D-44221 Dortmund, Germany  
Affiliation: Department of Mathematics Computer Science San Jose State University  Department of Computer Science University of Dortmund Systems Analysis Research Group, LSXI  
Abstract: The paper reports on the application of genetic algorithms, probabilistic search algorithms based on the model of organic evolution, to NP-complete combinatorial optimization problems. In particular, the subset sum, maximum cut, and minimum tardy task problems are considered. Except for the fitness function, no problem-specific changes of the genetic algorithm are required in order to achieve results of high quality even for the problem instances of size 100 used in the paper. For constrained problems, such as the subset sum and the minimum tardy task, the constraints are taken into account by incorporating a graded penalty term into the fitness function. Even for large instances of these highly multimodal optimization problems, an iterated application of the genetic algorithm is observed to find the global optimum within a number of runs. As the genetic algorithm samples only a tiny fraction of the search space, these results are quite encouraging. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. H. Ackley. </author> <title> A Connectionist Machine for Genetic Hillclimb-ing. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1987. </year>
Reference-contexts: While the typical problem size for the first instance is about twenty, the second problem instance comprises of populations with strings of length about one hundred. In the absence of test problems of significantly large sizes, we proceed by a well known technique (see e.g. <ref> [1] </ref>), and introduce scalable test problems. These can be scaled up to any desired large size, and more importantly, the optimal solution can be computed. Thus, our experimental results can be compared to the optimum solution. <p> The random dense graph cut20-0.9 is generated with a probability of 0:9 of placing an edge between arbitrary pairs of vertices. Both graphs are generated with random weights uniformly chosen in the range <ref> [0; 1] </ref>. The experimental results for these graphs are presented at the end of this section.
Reference: [2] <author> Th. Back. GENEsYs 1.0. </author> <title> Software distribution and installation notes, Systems Analysis Research Group, </title> <type> LSXI, </type> <institution> Department of Computer Science, University of Dort--mund, Germany, </institution> <month> July </month> <year> 1992. </year> <note> (Available via anonymous ftp to lumpi.informatik.uni-dortmund.de as file GENEsYs-1.0.tar.Z in /pub/GA/src). </note>
Reference-contexts: Except for the mutation rate, all parameters are identical to those suggested in the widely used GENESIS software package by Grefenstette (see [7] respectively [4], pp. 374-377). A generalized and more flexible software package called GENEsYs <ref> [2] </ref> is used to perform the experi ments in Section 3. 3 Application Problems We choose three combinatorial optimization problems from the literature of NP-complete problems. We believe that they represent a broad spectrum of the challenging intractable problems. The three problems are ordered according to their degree of difficulty.
Reference: [3] <author> Th. </author> <title> Back. The interaction of mutation rate, selection, and self-adaptation within a genetic algorithm. </title> <booktitle> In Manner and Manderick [12], </booktitle> <pages> pages 85-94. </pages>
Reference-contexts: Mutation is a probabilistic operator, and the probability of a single bit mutation p m is usually very small (e.g., p m 0:001 [10]). Recent theoretical work on the mutation rate setting gives strong evidence for an appropriate choice of p m = 1=n on many problems <ref> [3, 14] </ref>, and this rule is adopted in what follows for parameterization purposes of genetic algorithms. The purpose of recombination (crossover) consists in the combination of useful string segments from different individuals to form new, hopefully better performing offspring.
Reference: [4] <editor> L. Davis, editor. </editor> <booktitle> Handbook of Genetic Algorithms. </booktitle> <publisher> Van Nos-trand Reinhold, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: Except for the mutation rate, all parameters are identical to those suggested in the widely used GENESIS software package by Grefenstette (see [7] respectively <ref> [4] </ref>, pp. 374-377). A generalized and more flexible software package called GENEsYs [2] is used to perform the experi ments in Section 3. 3 Application Problems We choose three combinatorial optimization problems from the literature of NP-complete problems.
Reference: [5] <author> D. E. Goldberg. </author> <title> Genetic algorithms in search, optimization and machine learning. </title> <publisher> Addison Wesley, </publisher> <address> Reading, MA, </address> <year> 1989. </year>
Reference-contexts: The paper concludes by summarizing our experiences gained from these experiments. 2 Genetic Algorithms Genetic Algorithms, developed mainly by Holland in the sixties, are direct random search algorithms based on the model of biological evolution (see e.g. <ref> [5, 9] </ref>). Consequently, the field of Evolutionary Computation, of which genetic algorithms is part, has borrowed much of its terminology from biology. <p> If the problem under consideration is a maximization one, or if the fitness function can take negative values, then F (~x i ) is taken to be a linear function of f (~x i ). This technique known as linear dynamic scaling is commonly used in genetic algorithms (see <ref> [5] </ref> (pp. 123-124) or [6]). Whenever no different parameter setting is stated explicitly, all experiments reported here are performed with a standard genetic algorithm parameter setting: Population size = 50, one-point crossover, crossover rate p c = 0:6, mutation rate p m = 1=n, proportional selection with linear dynamic scaling.
Reference: [6] <author> J. J. Grefenstette. </author> <title> Optimization of control parameters for genetic algorithms. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> SMC-16(1):122-128, </volume> <year> 1986. </year>
Reference-contexts: Usually, the initial population is randomly initialized and the evolution process is stopped after a predefined number of iterations. This informal description leads to the rough outline of a genetic algorithm given below (see also <ref> [6, 18] </ref> for more detailed explanations): Algorithm GA is t := 0; initialize P (t); evaluate P (t); while not terminate P (t) do t := t + 1; P (t) := select P (t 1); recombine P (t); mutate P (t); evaluate P (t); od The procedure evaluate in this <p> This technique known as linear dynamic scaling is commonly used in genetic algorithms (see [5] (pp. 123-124) or <ref> [6] </ref>). Whenever no different parameter setting is stated explicitly, all experiments reported here are performed with a standard genetic algorithm parameter setting: Population size = 50, one-point crossover, crossover rate p c = 0:6, mutation rate p m = 1=n, proportional selection with linear dynamic scaling.
Reference: [7] <author> J. J. Grefenstette. </author> <title> A User's Guide to GENESIS. </title> <booktitle> Navy Center for Applied Research in Artificial Intelligence, </booktitle> <address> Washington, D. C., </address> <year> 1987. </year>
Reference-contexts: Except for the mutation rate, all parameters are identical to those suggested in the widely used GENESIS software package by Grefenstette (see <ref> [7] </ref> respectively [4], pp. 374-377). A generalized and more flexible software package called GENEsYs [2] is used to perform the experi ments in Section 3. 3 Application Problems We choose three combinatorial optimization problems from the literature of NP-complete problems.
Reference: [8] <author> F. Hoffmeister. </author> <title> Scalable parallelism by evolutionary algorithms. </title> <editor> In M. Grauer and D. B. Pressmar, editors, </editor> <booktitle> Parallel Computing and Mathematical Optimization, volume 367 of Lecture Notes in Economics and Mathematical Systems, </booktitle> <pages> pages 177-198. </pages> <publisher> Springer, </publisher> <address> Berlin, </address> <year> 1991. </year>
Reference-contexts: The latter is especially true for fine-grained parallel implementations of evolutionary algorithms <ref> [8] </ref>. In this paper we apply a representative of evolutionary algorithms, the genetic algorithm, to instances of different NP-complete combinatorial optimization problems. Due to their representation scheme for search points, genetic algorithms are the most promising and easily applicable representatives of evolutionary algorithms for the problems discussed here.
Reference: [9] <author> J. H. Holland. </author> <title> Adaptation in natural and artificial systems. </title> <publisher> The University of Michigan Press, </publisher> <address> Ann Arbor, MI, </address> <year> 1975. </year>
Reference-contexts: The paper concludes by summarizing our experiences gained from these experiments. 2 Genetic Algorithms Genetic Algorithms, developed mainly by Holland in the sixties, are direct random search algorithms based on the model of biological evolution (see e.g. <ref> [5, 9] </ref>). Consequently, the field of Evolutionary Computation, of which genetic algorithms is part, has borrowed much of its terminology from biology.
Reference: [10] <author> K. A. De Jong. </author> <title> An analysis of the behaviour of a class of genetic adaptive systems. </title> <type> PhD thesis, </type> <institution> University of Michigan, </institution> <year> 1975. </year> <note> Diss. Abstr. Int. 36(10), 5140B, University Microfilms No. 76-9381. </note>
Reference-contexts: The mutation operator works on bit strings by occasionally inverting single bits of individuals. Mutation is a probabilistic operator, and the probability of a single bit mutation p m is usually very small (e.g., p m 0:001 <ref> [10] </ref>). Recent theoretical work on the mutation rate setting gives strong evidence for an appropriate choice of p m = 1=n on many problems [3, 14], and this rule is adopted in what follows for parameterization purposes of genetic algorithms. <p> The purpose of recombination (crossover) consists in the combination of useful string segments from different individuals to form new, hopefully better performing offspring. A parameter p c (the crossover rate) of the algorithm indicates the probability per individual to undergo recombination; a typical choice is p c = 0:6 <ref> [10] </ref> or larger. <p> However, it can be naturally extended to a generalized m-point crossover by sampling m breakpoints and alternately exchanging each second of the resulting segments <ref> [10] </ref>. In general, a two-point crossover seems to be a good choice, but the number of crossover points may also be driven to extreme by using uniform crossover.
Reference: [11] <author> R. M. Karp. </author> <title> Reducibility among combinatorial problems. </title> <editor> In R. E. Miller and J. W. Thatcher, editors, </editor> <booktitle> Complexity of Computer Computation, </booktitle> <pages> pages 85-103. </pages> <publisher> Plenum, </publisher> <address> New York, </address> <year> 1972. </year>
Reference-contexts: We are interested in finding a subset S of W such that the sum of the elements in S are closest to, without exceeding, C. The subset sum problem is NP-complete. The partition problem can be polynomially transformed into it <ref> [11] </ref>. We note that S can be represented by a vector ~x = (x 1 ; x 2 ; : : : ; x n ) where x i 2 f0; 1g. <p> This problem is NP-complete. The satisfiability problem can be polynomially transformed into it <ref> [11] </ref>. The following is a formal definition of the maximum cut problem in which we make use of Stinson's terminology for combinatorial optimization problems [19]. Problem instance: A weighted graph G = (V; E). <p> In other words, the objective function is defined to be: W = i2S w i . Whether the minimum tardy task problem or its maximization version is considered, the problem remains NP-complete. The partitioning problem can be polynomially transformed into it <ref> [11] </ref>. Checking a subset S of T for feasibility is not as time consuming as one might think at first glance. To see whether the k tasks in S can be feasibly scheduled, one does not have to check all possible k! different orderings.
Reference: [12] <editor> R. Manner and B. Manderick, editors. </editor> <title> Parallel Problem Solving from Nature 2. </title> <publisher> Elsevier, </publisher> <address> Amsterdam, </address> <year> 1992. </year>
Reference: [13] <author> S. Martello and P. Toth. </author> <title> Knapsack Problems: Algorithms and Computer Implementations. </title> <publisher> John Wiley & Sons, </publisher> <address> Chichester, West Sussex, England, </address> <year> 1990. </year>
Reference-contexts: The next problem we study, sus1000-2 of dimension n = 1000, admits no solution of value C. It is constructed according to the method described in <ref> [13] </ref> (p. 128). More precisely, the weights are all even numbers, w i 2 f2; 4; : : :; 1000g and C is chosen to be odd, C = 10 3 n=4 + 1. Our results are summarized in Table 1.
Reference: [14] <author> H. Muhlenbein. </author> <title> How genetic algorithms really work: I. mutation and hillclimbing. </title> <booktitle> In Manner and Manderick [12], </booktitle> <pages> pages 15-25. </pages>
Reference-contexts: Mutation is a probabilistic operator, and the probability of a single bit mutation p m is usually very small (e.g., p m 0:001 [10]). Recent theoretical work on the mutation rate setting gives strong evidence for an appropriate choice of p m = 1=n on many problems <ref> [3, 14] </ref>, and this rule is adopted in what follows for parameterization purposes of genetic algorithms. The purpose of recombination (crossover) consists in the combination of useful string segments from different individuals to form new, hopefully better performing offspring.
Reference: [15] <author> J. T. Richardson, M. R. Palmer, G. Liepins, and M. Hilliard. </author> <title> Some guidelines for genetic algorithms with penalty functions. </title> <booktitle> In Schaffer [16], </booktitle> <pages> pages 191-197. </pages>
Reference-contexts: Two infeasible strings are not treated equally. The penalty is a function of the distance from feasibility. It has been argued that such penalty functions generally outperform other modes of penalties <ref> [15] </ref>. * The best infeasible string can never be better than even the weakest feasible string. Thus, our fitness function will always have an offset term to ensure the strict order between feasible and infeasible strings.
Reference: [16] <editor> J. D. Schaffer, editor. </editor> <booktitle> Proceedings of the 3rd International Conference on Genetic Algorithms. </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1989. </year>
Reference: [17] <author> A. E. Smith and D. M. Tate. </author> <title> Genetic optimization using a penalty function. </title> <editor> In S. Forrest, editor, </editor> <booktitle> Proceedings of the 5th International Conference on Genetic Algorithms, </booktitle> <pages> pages 499-505. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: Finally, our experiments tend to confirm that the nature of the exact penalty function is not of paramount importance. Extending the search to the exploration of infeasible regions is of greater importance. This result agrees with the recent findings of Smith, et al. <ref> [17] </ref>. 3.1 The Subset Sum Problem In the subset sum problem we are given a set W of n integers and a large integer C. We are interested in finding a subset S of W such that the sum of the elements in S are closest to, without exceeding, C.
Reference: [18] <author> W. M. Spears, K. A. De Jong, Th. Back, D. B. Fogel, and H. de Garis. </author> <title> An overview of evolutionary computation. </title> <editor> In P. B. Brazdil, editor, </editor> <booktitle> Machine Learning: ECML-93, volume 667 of Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 442-459. </pages> <publisher> Springer, </publisher> <address> Berlin, </address> <year> 1993. </year>
Reference-contexts: Evolutionary algorithms such as genetic algorithms, evolutionary programming, and evolution strategies (to mention the most widely known representatives see <ref> [18] </ref> for more detailed information on their similarities and differences) are well suitable for scaling up the problem size and the number of search points maintained by the algorithms. The latter is especially true for fine-grained parallel implementations of evolutionary algorithms [8]. <p> Usually, the initial population is randomly initialized and the evolution process is stopped after a predefined number of iterations. This informal description leads to the rough outline of a genetic algorithm given below (see also <ref> [6, 18] </ref> for more detailed explanations): Algorithm GA is t := 0; initialize P (t); evaluate P (t); while not terminate P (t) do t := t + 1; P (t) := select P (t 1); recombine P (t); mutate P (t); evaluate P (t); od The procedure evaluate in this
Reference: [19] <author> D. R. Stinson. </author> <title> An Introduction to the Design and Analysis of Algorithms. </title> <institution> The Charles Babbage Research Center, Winnipeg, Manitoba, </institution> <address> Canada, 2nd edition, </address> <year> 1987. </year>
Reference-contexts: The following is a formal definition of the subset sum problem in which we make use of Stinson's terminology for combinatorial optimization problems <ref> [19] </ref>. Problem instance: A set W = fw 1 ; w 2 ; : : : ; w n g where the w i 's are positive integers, and a large positive integer C. <p> This problem is NP-complete. The satisfiability problem can be polynomially transformed into it [11]. The following is a formal definition of the maximum cut problem in which we make use of Stinson's terminology for combinatorial optimization problems <ref> [19] </ref>. Problem instance: A weighted graph G = (V; E). V = f1; : : : ; ng is the set of vertices and E the set of edges. w ij represents the weight of edge hi; ji, i.e., the weight of the edge between vertices i and j. <p> The following is a formal definition of the minimum tardy task problem <ref> [19] </ref>. <p> It can be shown that S is feasible if and only if the tasks in S can be scheduled in increasing order by deadline without violating any deadline <ref> [19] </ref>.
Reference: [20] <author> G. Syswerda. </author> <title> Uniform crossover in genetic algorithms. </title> <booktitle> In Schaffer [16], </booktitle> <pages> pages 2-9. </pages>
Reference-contexts: In general, a two-point crossover seems to be a good choice, but the number of crossover points may also be driven to extreme by using uniform crossover. This operator decides for each bit of the parent individuals randomly whether the bit is to be exchanged or not <ref> [20] </ref>, therefore causing a strong mixing effect which is sometimes helpful to overcome local optima. The selection operator directs the search process by preferring better individuals to be copied into the next generation, where they are subject to recombination and mutation.
References-found: 20

