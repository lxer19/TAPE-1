URL: http://www.cs.berkeley.edu/~shawnc/281/finalprj/report.ps
Refering-URL: http://www.cs.berkeley.edu/~shawnc/281/finalprj/
Root-URL: http://www.cs.berkeley.edu
Email: &lt;shawnc@cs.berkeley.edu&gt;  
Title: Learning Predictive Evaluation Function for the EM Algorithm  
Author: Prof. Stuart Russell Shawn Chang 
Date: 22 May 1998  
Note: CS281 Final Project,  
Abstract: This paper shows by simple example that the solution space of the popular EM algorithm may contain many local optima of different qualities. Based on the previous works in local search algorithms, an algorithm is developed to learn a predictive evaluation function of state features, which not only provides a measure of goodness of a state, but also gives hints on how promising the state is if used as a starting state for a new EM run. Preliminary experiments show that the algorithm enhances the EM performance in some problem domains.
Abstract-found: 1
Intro-found: 1
Reference: [Bishop 96] <author> Bishop, C. M. </author> <title> Neural Networks for Pattern Recognition. </title> <publisher> Oxford University Press, Oxford, </publisher> <address> England, </address> <year> 1996. </year>
Reference-contexts: To demonstrate that the EM can benefit from the learning of a predictive evaluation function, this project focuses on applying the EM algorithm in learning a mixture of Gaussian distributions. The Gaussian distribution has many nice properties <ref> [Bishop 96] </ref>. In particular, the EM formulation of the mixture of Gaus-sians provides closed form update formula for the state variables, the means, covariance matrices, and the prior probabilities. The [Bishop 96] gives update formulas for a special case of multi-dimensional Gaussians, where each has a covariance matrix which is some <p> The Gaussian distribution has many nice properties <ref> [Bishop 96] </ref>. In particular, the EM formulation of the mixture of Gaus-sians provides closed form update formula for the state variables, the means, covariance matrices, and the prior probabilities. The [Bishop 96] gives update formulas for a special case of multi-dimensional Gaussians, where each has a covariance matrix which is some scalar multiple of the identity matrix.
Reference: [Boese et al. 94] <author> Boese, K. D.; Kahng, A. B.; and Muddu, S. </author> <year> 1994. </year> <title> A new adaptive multi-start technique for combinatorial global optimizations. </title> <journal> Operations Research Letters 9 16 </journal> <pages> 101-113. </pages>
Reference-contexts: Finding a global maxima in a complex state space has been known to be difficult, especially in cases where large number of sub-optimal local maxima exist. Previous works <ref> [Boese et al. 94] </ref> have showed that in some combinatorial global optimization domains, there exist some apparent global geometric structures in the optimization cost surfaces.
Reference: [Boyan 98] <author> Boyan, J. A. </author> <title> "Learning Evaluation Functions for Global Optimizations." </title> <type> Ph.D. Thesis (draft), CMU, </type> <month> May </month> <year> 1998. </year>
Reference: [Boyan and Moore 98] <author> Boyan, J. A. and A. W. Moore. </author> <title> "Learning Evaluation Functions for Global Optimization and Boolean Satisfiability." </title> <booktitle> Fifteenth National Conference on Artificial Intelligence (AAAI), 1998 (to appear). </booktitle>
Reference-contexts: A mixture of two Gaussian distributions (right) was used to model this data set, and learned with the EM. good solutions are located near other good solutions in some predictable way. In recent works <ref> [Boyan and Moore 98] </ref>[Boyan 98], Boyan and Moore developed a strategy for learning to predict good starting states for local search algorithms in global optimization problems.
Reference: [Dempster et al. 77] <author> Dempster, A.P.; Laird, N. M.; and Rubin, D. B. </author> <year> 1977. </year> <title> "Maximum Likelihood from Incomplete Data via the EM Algorithm." </title> <journal> Journal of the Royal Statistical Society. Series B, </journal> <volume> 39(1), </volume> <pages> 1-38. </pages>
Reference-contexts: The EM iterations correspond directly to the trajectories produced by a local search algorithm moving around in the neighborhood structure of a global optimization problem. <ref> [Dempster et al. 77] </ref> shows that convergence of the EM is linear with the rate of convergence proportional to max , where max is the maximal fraction of missing information.
Reference: [McLachlan and Krishnan 97] <author> McLachlan, G. J. and T. Krishnan. </author> <title> The EM Algorithm and Extensions. </title> <publisher> John Wiley & Sons, Inc. </publisher> <address> New York, NY, </address> <year> 1997. </year> <month> 10 </month>
Reference-contexts: The EM associates a given incomplete-data problem with a simpler complete-data problem, and iteratively finds the maximum likelihood estimates of the data. In a typical situation, the EM converges <ref> [McLachlan and Krishnan 97] </ref> monotonically to a fixed point in the state space, usually a local maximum. Like the state space of many local search algorithms in global optimization problems, the state space for the EM in maximum likelihood estimation problems can be potentially very complex. <p> It would be of both theoretical and practical interests to adapt the STAGE-EM to other domains where the EM algorithm succeeds, and to the various extensions <ref> [McLachlan and Krishnan 97] </ref> to the basic EM, such as the Expected-Conditional Maximization, Penalized EM, and Stochastic EM.
References-found: 6

