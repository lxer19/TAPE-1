URL: http://ftp.cs.yale.edu/pub/hager/ieeecsmag.ps.gz
Refering-URL: http://ftp.cs.yale.edu/pub/hager/
Root-URL: http://www.cs.yale.edu
Title: Robot Hand-Eye Coordination Based on Stereo Vision  
Author: Gregory D. Hager, Wen-Chung Chang and A. S. Morse 
Date: 30-39, 1995  
Note: IEEE Control Systems Magazine, 15(1), pp.  
Abstract: This article describes the theory and implementation of a system that positions a robot manipulator using visual information from two cameras. The system simultaneously tracks the robot end-effector and visual features used to define goal positions. An error signal based on the visual distance between the end-effector and the target is defined and a control law that moves the robot to drive this error to zero is derived. The control law has been integrated into a system that performs tracking and stereo control on a single processor with no special purpose hardware at real-time rates. Experiments with the system have shown that the controller is so robust to calibration error that the cameras can be moved several centimeters and rotated several degrees while the system is running with no adverse effects. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. Allen, B. Yoshimi, and A. Timcenko. </author> <title> Hand-eye coordination for robotic tracking and grasping. </title> <editor> In K. Hashimoto, editor, </editor> <booktitle> Visual Servoing, </booktitle> <pages> pages 33-70. </pages> <publisher> World Scientific, </publisher> <year> 1994. </year> <month> 14 </month>
Reference-contexts: In the recent literature, Zheng et al: [29] describe a visual servoing system using window-based feature tracking on a monocular image stream. Allen et al: <ref> [1] </ref> describe a stereo motion based system for grasping a toy train using two stationary cameras. Rizzi and Koditschek [24] describe a stereo vision-based juggling system, and Anderson [2] describes a stereo vision-based ping-pong player.
Reference: [2] <author> R. L. Anderson. </author> <title> Dynamic sensing in a ping-pong playing robot. </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> 5(6) </volume> <pages> 723-739, </pages> <year> 1989. </year>
Reference-contexts: Allen et al: [1] describe a stereo motion based system for grasping a toy train using two stationary cameras. Rizzi and Koditschek [24] describe a stereo vision-based juggling system, and Anderson <ref> [2] </ref> describes a stereo vision-based ping-pong player. None of these systems directly observe the robot end-effector; they only observe and compute a goal position or goal configuration of the system. Moving to the computed position is open-loop with respect to the vision system, and therefore relies on the hand-eye calibration.
Reference: [3] <author> P. I. Corke. </author> <title> Visual control of robot manipulators|a review. </title> <editor> In K. Hashimoto, editor, </editor> <booktitle> Visual Servoing, </booktitle> <pages> pages 1-32. </pages> <publisher> World Scientific, </publisher> <year> 1994. </year>
Reference-contexts: Related work Visual servoing has been an active area of research over the last 30 years with the result that a large variety of experimental systems have been built (see <ref> [3] </ref> for an extensive review and [7, 13] for a recent collection of articles). In the recent literature, Zheng et al: [29] describe a visual servoing system using window-based feature tracking on a monocular image stream.
Reference: [4] <author> O. Faugeras. </author> <title> What can be seen in three dimensions with an uncalibration stereo rig? In Proc. </title> <booktitle> the First European Conference on Computer Vision, </booktitle> <pages> pages 563-578. </pages> <publisher> Springer Verlag, </publisher> <year> 1993. </year>
Reference-contexts: If successful, this should make it simple to use mobile or reconfigurable camera systems. We are continuing to develop other interesting and useful visual control modes and trajectory generation schemes made possible by proper exploitation of image invariants <ref> [4] </ref> and changing the "focus of attention" of the tracking system. Some early results in this area can be found in [8, 9]. We are also investigating methods for automatically initializing the tracking system using prior knowledge of the scene [10].
Reference: [5] <author> J. Feddema, C. Lee, and O. Mitchell. </author> <title> Weighted selection of image features for resolved rate visual feedback control. </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> 7(1) </volume> <pages> 31-47, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: More recently, other authors have seized on the idea of visual feedback. Many authors address the problem of servoing a camera to maintain a constant position relative to a known object with a single camera <ref> [5, 12, 16, 23, 28, 29] </ref>. Wijesoma et al: [27] describe a monocular hand-eye system for planar positioning using image feedback. Other authors [21, 22] describe monocular visual servoing systems that use some type of adaptivity to compensate for unknown system parameters or to refine system calibration.
Reference: [6] <author> G. Franklin, J. Powell, and A. Emami-Naeini. </author> <title> Feedback Control of Dynamic Systems. </title> <publisher> Addison-Wesley, </publisher> <address> 2nd edition, </address> <year> 1991. </year>
Reference-contexts: ~z 0 D 2 ~y 0 1 ((rc 1 )~y 1 ) 1 2 D 2 ~z 0 D 2 ~y 0 2 ((rc 2 )~y 2 ) 2 7 7 7 7 5 Assuming J (r) 0 J (r) is nonsingular, we can use a Position-Integral (PI) control law <ref> [6] </ref> to compute u as g (r)(k 1 e + k 2 z ); (6) where k 1 and k 2 are positive gain constants. <p> This system has characteristic polynomial x 2 + x + tk = 0 <ref> [6] </ref>.
Reference: [7] <author> G. Hager and S. Hutchinson, </author> <title> editors. </title> <booktitle> Proc. the Workshop on Visual Servoing. </booktitle> <month> May </month> <year> 1994. </year>
Reference-contexts: Related work Visual servoing has been an active area of research over the last 30 years with the result that a large variety of experimental systems have been built (see [3] for an extensive review and <ref> [7, 13] </ref> for a recent collection of articles). In the recent literature, Zheng et al: [29] describe a visual servoing system using window-based feature tracking on a monocular image stream. Allen et al: [1] describe a stereo motion based system for grasping a toy train using two stationary cameras.
Reference: [8] <author> G. D. Hager. </author> <title> Real-time feature tracking and projective invariance as a basis for hand-eye coordination. </title> <booktitle> In Proc. IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 533-539. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1994. </year>
Reference-contexts: as well as ratios of distances, hence using the notation defined above, controlling the system to the setpoint fl () places it at a point approximately kp 1 p 2 k units from p 1 on the line in space containing p 1 and p 2 : A companion paper <ref> [8] </ref> provides more detail on ways for performing relative positioning using perspective images. <p> We are continuing to develop other interesting and useful visual control modes and trajectory generation schemes made possible by proper exploitation of image invariants [4] and changing the "focus of attention" of the tracking system. Some early results in this area can be found in <ref> [8, 9] </ref>. We are also investigating methods for automatically initializing the tracking system using prior knowledge of the scene [10].
Reference: [9] <author> G. D. Hager. </author> <title> Six DOF visual control of relative position. </title> <institution> DCS RR-1038, Yale University, </institution> <address> New Haven, CT, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: We are continuing to develop other interesting and useful visual control modes and trajectory generation schemes made possible by proper exploitation of image invariants [4] and changing the "focus of attention" of the tracking system. Some early results in this area can be found in <ref> [8, 9] </ref>. We are also investigating methods for automatically initializing the tracking system using prior knowledge of the scene [10].
Reference: [10] <author> G. D. Hager, G. Grunwald, and G. Hirzinger. </author> <title> Feature-based visual servoing and its application to telerobotics. </title> <booktitle> In Proc. IEEE International Conference on Intelligent Robots and Systems, </booktitle> <pages> pages 164-171. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1994. </year>
Reference-contexts: Some early results in this area can be found in [8, 9]. We are also investigating methods for automatically initializing the tracking system using prior knowledge of the scene <ref> [10] </ref>.
Reference: [11] <author> G. D. Hager, S. Puri, and K. Toyama. </author> <title> A framework for real-time vision-based tracking using off-the-shelf hardware. </title> <institution> DCS RR-988, Yale University, </institution> <address> New Haven, CT, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: System calibration was performed by tracking the robot through a series of known motions and performing optimization to determine the system calibration parameters relative to robot base coordinates [17]. A custom tracking system written in C++ provides visual input for the controller. The system, more fully described in <ref> [11] </ref>, provides fast edge detection on a memory-mapped framebuffer. In addition, it supports simultaneous tracking of multiple edge segments, and can also enforce constraints among segments. The experiments described here are based on tracking corners formed by the intersection of two line segments.
Reference: [12] <author> K. Hashimoto. </author> <title> LQ optimal and nonlinear approaches to visual servoing. </title> <editor> In K. Hashimoto, editor, </editor> <booktitle> Visual Servoing, </booktitle> <pages> pages 165-198. </pages> <publisher> World Scientific, </publisher> <year> 1994. </year>
Reference-contexts: More recently, other authors have seized on the idea of visual feedback. Many authors address the problem of servoing a camera to maintain a constant position relative to a known object with a single camera <ref> [5, 12, 16, 23, 28, 29] </ref>. Wijesoma et al: [27] describe a monocular hand-eye system for planar positioning using image feedback. Other authors [21, 22] describe monocular visual servoing systems that use some type of adaptivity to compensate for unknown system parameters or to refine system calibration.
Reference: [13] <author> K. Hashimoto, </author> <title> editor. Visual Servoing. </title> <publisher> World Scientific, </publisher> <year> 1994. </year>
Reference-contexts: Related work Visual servoing has been an active area of research over the last 30 years with the result that a large variety of experimental systems have been built (see [3] for an extensive review and <ref> [7, 13] </ref> for a recent collection of articles). In the recent literature, Zheng et al: [29] describe a visual servoing system using window-based feature tracking on a monocular image stream. Allen et al: [1] describe a stereo motion based system for grasping a toy train using two stationary cameras.
Reference: [14] <author> N. Hollinghurst and R. Cipolla. </author> <title> Uncalibrated stereo hand eye coordination. </title> <type> Technical Report TR-126, </type> <institution> Cambridge University, Dept. of Engineering, </institution> <month> September </month> <year> 1993. </year>
Reference-contexts: Hence, any application of their system will depend on knowing an exact hand-eye calibration. A stereo-based visual servoing system using free-standing cameras is described in a recent paper by Hollinghurst and Cipolla <ref> [14] </ref>. Their approach uses an affine approximation to the inverse perspective transformation to compute the approximate Cartesian positions of both the end-effector and the goal position. They then compute control signals based on the difference between these positions.
Reference: [15] <author> B. K. Horn. </author> <title> Robot Vision. </title> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: Most stereo camera systems have this geometry. It is possible to define a function which, given the camera calibration parameters, computes three independent values from a stereo projection without making any assumptions about the geometry of the system <ref> [15] </ref>. However, the cost of this transformation is comparable to that of computing position-based control. System Stability In order for vision-based control to be stable, the Jacobian of the projective map, (5), must be of rank 3.
Reference: [16] <author> S. Hutchinson. </author> <title> Exploiting visual constraints in robot motion planning. </title> <booktitle> In Proc. IEEE International Conference on Robotics and Automation, </booktitle> <pages> pages 1722-1727. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1991. </year>
Reference-contexts: More recently, other authors have seized on the idea of visual feedback. Many authors address the problem of servoing a camera to maintain a constant position relative to a known object with a single camera <ref> [5, 12, 16, 23, 28, 29] </ref>. Wijesoma et al: [27] describe a monocular hand-eye system for planar positioning using image feedback. Other authors [21, 22] describe monocular visual servoing systems that use some type of adaptivity to compensate for unknown system parameters or to refine system calibration.
Reference: [17] <author> C. Lu, E. J. Mjolsness, and G. D. Hager. </author> <title> Online computation of exterior orientation with application to hand-eye calibration. </title> <institution> DCS RR-1046, Yale University, </institution> <address> New Haven, CT, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: System calibration was performed by tracking the robot through a series of known motions and performing optimization to determine the system calibration parameters relative to robot base coordinates <ref> [17] </ref>. A custom tracking system written in C++ provides visual input for the controller. The system, more fully described in [11], provides fast edge detection on a memory-mapped framebuffer. In addition, it supports simultaneous tracking of multiple edge segments, and can also enforce constraints among segments. <p> As noted in the analysis section above, the system is most sensitive to the relative orientations of the two cameras. In particular, inward rotations of the physical cameras should be most likely to destabilize the system. To test this empirically, we performed a system calibration as described in <ref> [17] </ref>. We then rotated the cameras approximately 10 degrees inward. The effects are quite marked. As shown in Figure 7, a gain of 1.0 led to large oscillations. However, reducing the gain to 0:5 again stabilized the system. <p> This positioning accuracy could easily be improved by changing the camera configuration to a wider baseline, or by improving the image-processing to be more accurate. The methods are also extremely robust. A moderately accurate camera calibration can be performed quickly using robot motions to supply input <ref> [17] </ref>. In most cases, this calibration would be sufficient to provide adequate performance for positioning and short straight-line motions provided the cameras are not moved substantially during operation. We have constructed an adaptive supervisory controller for a planar version of the servoing problem.
Reference: [18] <author> N. Maru, H. Kase, A. Nishikawa, and F. Miyazaki. </author> <title> Manipulator control by visual servoing with the stereo vision. </title> <booktitle> In Proc. IEEE International Conference on Intelligent Robots and Systems, </booktitle> <pages> pages 1866-1870. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1993. </year>
Reference-contexts: Wijesoma et al: [27] describe a monocular hand-eye system for planar positioning using image feedback. Other authors [21, 22] describe monocular visual servoing systems that use some type of adaptivity to compensate for unknown system parameters or to refine system calibration. Maru et al: <ref> [18] </ref> describe algorithms for performing stereo vision-based positioning. <p> It is also possible to include rotations of the robot in the formulation of stereo projection. When the point r is not at the axis of rotation, this provides additional degrees of freedom of motion. The interested reader may wish to consult <ref> [18] </ref> for the formulation of such a system for stereo cameras mounted on a robot arm. Estimation Thus far, it has been assumed that the value of r; the robot reference position, is known. This information is not directly accessible and must be estimated.
Reference: [19] <author> L. Matthies and S. Shafer. </author> <title> Error modeling in stereo navigation. </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> RA-3(3):239-248, </volume> <month> June </month> <year> 1987. </year>
Reference-contexts: The complete procedure is then reversed, and the entire cycle executed repeatedly. During this experiment, we can alter the position and orientation of the cameras and observe the effects. Positioning Accuracy It is possible to predict the ultimate accuracy of positioning based on the error of edge detection <ref> [19] </ref>. One camera pixel has a width of approximately :013mm. Assuming a maximal error of one pixel and the physical configuration described above, the expected vertical and horizontal positioning accuracy is 0:9mm, and the expected accuracy in depth is approximately 2:75mm.
Reference: [20] <author> J. Mundy and A. Zisserman. </author> <title> Geometric Invariance in Computer Vision. </title> <publisher> MIT Press, </publisher> <year> 1992. </year> <month> 15 </month>
Reference-contexts: This is because perspective projection does not preserve distances in images, or even simple ratios of distances. However, for small motions, perspective projection can be approximated well by an affine projection <ref> [20] </ref>.
Reference: [21] <author> B. Nelson and P. K. Khosla. </author> <title> Increasing the tracking region of an eye-in-hand system by singularity and joint limit avoidance. </title> <booktitle> In Proc. IEEE International Conference on Robotics and Automation, </booktitle> <pages> pages 418-423. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1993. </year>
Reference-contexts: Many authors address the problem of servoing a camera to maintain a constant position relative to a known object with a single camera [5, 12, 16, 23, 28, 29]. Wijesoma et al: [27] describe a monocular hand-eye system for planar positioning using image feedback. Other authors <ref> [21, 22] </ref> describe monocular visual servoing systems that use some type of adaptivity to compensate for unknown system parameters or to refine system calibration. Maru et al: [18] describe algorithms for performing stereo vision-based positioning.
Reference: [22] <author> N. Papanikolopoulos, P. Khosla, and T. Kanade. </author> <title> Vision and control techniques for robotic visual tracking. </title> <booktitle> In Proc. IEEE International Conference on Robotics and Automation, </booktitle> <pages> pages 857-864. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1991. </year>
Reference-contexts: Many authors address the problem of servoing a camera to maintain a constant position relative to a known object with a single camera [5, 12, 16, 23, 28, 29]. Wijesoma et al: [27] describe a monocular hand-eye system for planar positioning using image feedback. Other authors <ref> [21, 22] </ref> describe monocular visual servoing systems that use some type of adaptivity to compensate for unknown system parameters or to refine system calibration. Maru et al: [18] describe algorithms for performing stereo vision-based positioning.
Reference: [23] <author> P. Rives, F. Chaumette, and B. Espiau. </author> <title> Positioning of a robot with respect to an object, tracking it and estimating its velocity by visual servoing. </title> <booktitle> In Proc. IEEE International Conference on Robotics and Automation, </booktitle> <pages> pages 2248-2254. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1991. </year>
Reference-contexts: More recently, other authors have seized on the idea of visual feedback. Many authors address the problem of servoing a camera to maintain a constant position relative to a known object with a single camera <ref> [5, 12, 16, 23, 28, 29] </ref>. Wijesoma et al: [27] describe a monocular hand-eye system for planar positioning using image feedback. Other authors [21, 22] describe monocular visual servoing systems that use some type of adaptivity to compensate for unknown system parameters or to refine system calibration.
Reference: [24] <author> A. Rizzi and D. E. Koditschek. </author> <title> Further progress in robot juggling: The spatial two-juggle. </title> <booktitle> In Proc. IEEE International Conference on Robotics and Automation, </booktitle> <pages> pages 919-924. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1993. </year>
Reference-contexts: In the recent literature, Zheng et al: [29] describe a visual servoing system using window-based feature tracking on a monocular image stream. Allen et al: [1] describe a stereo motion based system for grasping a toy train using two stationary cameras. Rizzi and Koditschek <ref> [24] </ref> describe a stereo vision-based juggling system, and Anderson [2] describes a stereo vision-based ping-pong player. None of these systems directly observe the robot end-effector; they only observe and compute a goal position or goal configuration of the system.
Reference: [25] <author> R. Tsai. </author> <title> A versatile camera calibration technique for high accuracy 3D machine vision metrology using off-the-shelf TV cameras and lenses. </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> RA-3:323-344, </volume> <month> Aug. </month> <year> 1987. </year>
Reference-contexts: In practice, we have found that stability is itself insensitive to errors in camera calibration. The camera intrinsic parameters, including focal length, image scale, and image center, can all be determined o*ine to high accuracy and are fixed for the life of the system <ref> [25] </ref>. Hence, they do not introduce any substantial calibration error into the system.
Reference: [26] <author> L. Weiss, A. Sanderson, and C. Neuman. </author> <title> Dynamic sensor-based control of robots with visual feedback. </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> RA-3(5):404-417, </volume> <month> Oct. </month> <year> 1987. </year>
Reference-contexts: None of these systems directly observe the robot end-effector; they only observe and compute a goal position or goal configuration of the system. Moving to the computed position is open-loop with respect to the vision system, and therefore relies on the hand-eye calibration. Weiss et al: <ref> [26] </ref> proposes a system utilizing visual feedback for controlling a manipulator and simultaneously using adaptive control methods to perform online calibration. More recently, other authors have seized on the idea of visual feedback. <p> There are two generally accepted choices for : The first is to define () = (A 0 ()A ()) 1 A 0 ()b (); (18) where A and b are given by (14) and (15), respectively. This approach leads to what is commonly referred to as "position-based" control <ref> [26] </ref>. As the name suggests, the error term is now computed in robot "position" space. The major advantage of this approach is that, if the system calibration is correct, setpoints correspond to positions in the global coordinate system. Furthermore, point-to-point trajectories will be straight lines in Cartesian space.
Reference: [27] <author> S. Wijesoma, D. Wolfe, and R. Richards. </author> <title> Eye-to-hand coordination for vision-guided robot control applications. </title> <journal> International Journal of Robot Research, </journal> <volume> 12(1) </volume> <pages> 65-78, </pages> <year> 1993. </year>
Reference-contexts: More recently, other authors have seized on the idea of visual feedback. Many authors address the problem of servoing a camera to maintain a constant position relative to a known object with a single camera [5, 12, 16, 23, 28, 29]. Wijesoma et al: <ref> [27] </ref> describe a monocular hand-eye system for planar positioning using image feedback. Other authors [21, 22] describe monocular visual servoing systems that use some type of adaptivity to compensate for unknown system parameters or to refine system calibration. Maru et al: [18] describe algorithms for performing stereo vision-based positioning.
Reference: [28] <author> W. Wilson. </author> <title> Visual servo control of robots using Kalman filter estimates of robot pose relative to work-pieces. </title> <editor> In K. Hashimoto, editor, </editor> <booktitle> Visual Servoing, </booktitle> <pages> pages 71-104. </pages> <publisher> World Scientific, </publisher> <year> 1994. </year>
Reference-contexts: More recently, other authors have seized on the idea of visual feedback. Many authors address the problem of servoing a camera to maintain a constant position relative to a known object with a single camera <ref> [5, 12, 16, 23, 28, 29] </ref>. Wijesoma et al: [27] describe a monocular hand-eye system for planar positioning using image feedback. Other authors [21, 22] describe monocular visual servoing systems that use some type of adaptivity to compensate for unknown system parameters or to refine system calibration.
Reference: [29] <author> J. Y. Zheng, Q. Chen, and S. Tsuji. </author> <title> Active camera guided manipulation. </title> <booktitle> In Proc. the IEEE International Conference on Robotics and Automation, </booktitle> <pages> pages 632-638. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1991. </year> <month> 16 </month>
Reference-contexts: In the recent literature, Zheng et al: <ref> [29] </ref> describe a visual servoing system using window-based feature tracking on a monocular image stream. Allen et al: [1] describe a stereo motion based system for grasping a toy train using two stationary cameras. <p> More recently, other authors have seized on the idea of visual feedback. Many authors address the problem of servoing a camera to maintain a constant position relative to a known object with a single camera <ref> [5, 12, 16, 23, 28, 29] </ref>. Wijesoma et al: [27] describe a monocular hand-eye system for planar positioning using image feedback. Other authors [21, 22] describe monocular visual servoing systems that use some type of adaptivity to compensate for unknown system parameters or to refine system calibration.
References-found: 29

