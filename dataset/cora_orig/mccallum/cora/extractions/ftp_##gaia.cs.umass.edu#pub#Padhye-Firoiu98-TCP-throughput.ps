URL: ftp://gaia.cs.umass.edu/pub/Padhye-Firoiu98-TCP-throughput.ps
Refering-URL: http://www.cs.umass.edu/~vfiroiu/
Root-URL: 
Email: kuroseg@cs.umass.edu  
Title: Modeling TCP Throughput: A Simple Model and its Empirical Validation  
Author: Jitendra Padhye Victor Firoiu Don Towsley Jim Kurose fjitu, vfiroiu, towsley, 
Address: Amherst, MA 01003 USA  
Affiliation: Department of Computer Science University of Massachusetts  
Note: In ACM SIGCOMM'98  
Abstract: In this paper we develop a simple analytic characterization of the steady state throughput, as a function of loss rate and round trip time for a bulk transfer TCP flow, i.e., a flow with an unlimited amount of data to send. Unlike the models in [6, 7, 10], our model captures not only the behavior of TCP's fast retransmit mechanism (which is also considered in [6, 7, 10]) but also the effect of TCP's timeout mechanism on throughput. Our measurements suggest that this latter behavior is important from a modeling perspective, as almost all of our TCP traces contained more timeout events than fast retransmit events. Our measurements demonstrate that our model is able to more accurately predict TCP throughput and is accurate over a wider range of loss rates. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Bolot and A. Vega-Garcia. </author> <title> Control mechanisms for packet audio in the Internet. </title> <booktitle> In Proceedings IEEE Infocom96, </booktitle> <year> 1996. </year>
Reference-contexts: We denote the former event as a "TD" (triple-duplicate) loss indication, and the latter as a "TO" loss indication. We assume that a packet is lost in a round independently of any packets lost in other rounds, a modeling assumption justified to some extent by past studies <ref> [1] </ref> that have shown that periodic UDP packets that are separated by as little as 40 msec tend to get lost only in singleton bursts. <p> This is justified by the fact that packets in different rounds are separated by one RTT or more, and thus they are likely to encounter buffer states that are independent of each other. This is also confirmed by findings in <ref> [1] </ref>. Another assumption we made, that is also implicit in [6, 7, 10], is that the round trip time is independent of the window size. We have measured the coefficient of correlation between the duration of round samples and the number of packets in transit during each sample.
Reference: [2] <author> K. Fall and S. Floyd. </author> <title> Simulation-based comparisons of Tahoe, Reno, and SACK TCP. </title> <journal> Computer Communication Review, </journal> <volume> 26(3), </volume> <month> July </month> <year> 1996. </year>
Reference-contexts: Note that we have also assumed here that the time needed to send all the packets in a window is smaller than the round trip time; this behavior can be seen in observations reported in <ref> [2, 12] </ref>. At the beginning of the next round, a group of W 0 new packets will be sent, where W 0 is the new size of the congestion control window. Let b be the number of packets that are acknowledged by a received ACK. <p> This bursty loss behavior, which has been shown to arise from the drop-tail queuing discipline (adopted in many Internet routers), is discussed in <ref> [2, 3] </ref>. We discuss it further in Section 4. <p> As packets in a round are sent back-to-back, if a packet arrives at a full buffer, it is likely that the same happens with the rest of the packets in the round. Packet loss correlation at drop-tail routers was also pointed out in <ref> [2, 3] </ref>. In addition, we assume that losses in one round are independent of losses in other rounds.
Reference: [3] <author> S. Floyd and V. Jacobson. </author> <title> Random Early Detection gateways for congestion avoidance. </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> 1(4), </volume> <month> August </month> <year> 1997. </year>
Reference-contexts: This bursty loss behavior, which has been shown to arise from the drop-tail queuing discipline (adopted in many Internet routers), is discussed in <ref> [2, 3] </ref>. We discuss it further in Section 4. <p> As packets in a round are sent back-to-back, if a packet arrives at a full buffer, it is likely that the same happens with the rest of the packets in the round. Packet loss correlation at drop-tail routers was also pointed out in <ref> [2, 3] </ref>. In addition, we assume that losses in one round are independent of losses in other rounds.
Reference: [4] <author> V. Jacobson. </author> <title> Modified TCP congestion avoidance algorithm. </title> <note> Note sent to end2end-interest mailing list, </note> <year> 1990. </year>
Reference-contexts: Our model is based on the Reno flavor of TCP, as it is by far the most popular implementation in the Internet today [13, 12]. We assume that the reader is familiar with TCP Reno congestion control (see for example <ref> [4, 17, 16] </ref>) and we adopt most of our terminology from [4, 17, 16]. Our model focuses on TCP's congestion avoidance mechanism, where TCP's congestion control window size, W; is increased by 1=W each time an ACK is received. <p> We assume that the reader is familiar with TCP Reno congestion control (see for example <ref> [4, 17, 16] </ref>) and we adopt most of our terminology from [4, 17, 16]. Our model focuses on TCP's congestion avoidance mechanism, where TCP's congestion control window size, W; is increased by 1=W each time an ACK is received. <p> The results are for a 1 hour connection divided into 100 second intervals. We have also assumed that all of our senders implement TCP-Reno as described in <ref> [4, 17, 16] </ref>. In [13, 12], it is observed that the implementation of the protocol stack in each operating system is slightly different. <p> For example, we have observed that the Linux exponential backoff does not exactly follow the algorithm described in <ref> [4, 17, 16] </ref>. Our observations also seem to indicate that in the Irix implementation, the exponential backoff is limited to 2 5 , instead of 2 6 . We are also aware of the observation made in [13] that the SunOS TCP implementation is derived from Tahoe and not Reno.
Reference: [5] <author> P. Karn and C. Partridge. </author> <title> Improving Round-Trip time estimates in reliable transport protocols. </title> <journal> Computer Communication Review, </journal> <volume> 17(5), </volume> <month> August </month> <year> 1987. </year>
Reference-contexts: The last two columns report the average round trip time, and average duration of a "single" timeout T 0 . These values have been averaged over the entire trace. When calculating round trip time values, we follow Karn's algorithm <ref> [5] </ref>, in an attempt to minimize the impact of timeouts and retransmissions on the RTT estimates. Table 3 reports summary results from additional 13 data sets. In these cases, each data set represents 100 serially-initiated TCP connections between a given sender-receiver pair.
Reference: [6] <author> J. Mahdavi and S. Floyd. </author> <note> TCP-friendly unicast rate-based flow control. Note sent to end2end-interest mailing list, </note> <month> Jan </month> <year> 1997. </year>
Reference-contexts: Traditionally, simulation and implementation/measurement have been the tools of choice for examining the performance of various aspects of TCP. Recently, however, several efforts have been directed at analytically characterizing the throughput of TCP's congestion control mechanism, as a function of packet loss and round trip delay <ref> [6, 10, 7] </ref>. One reason for this recent interest is that a simple quantitative characterization of TCP throughput under given operating conditions offers the possibility of defining a "fair share" or "TCP-friendly" [6] throughput for a non-TCP flow that interacts with a TCP connection. <p> One reason for this recent interest is that a simple quantitative characterization of TCP throughput under given operating conditions offers the possibility of defining a "fair share" or "TCP-friendly" <ref> [6] </ref> throughput for a non-TCP flow that interacts with a TCP connection. <p> Unlike the recent work of <ref> [6, 7, 10] </ref>, our model captures not only the behavior of TCP's fast retransmit mechanism (which is also considered in [6, 7, 10]) but also the effect of TCP's timeout mechanism on throughput. <p> Unlike the recent work of <ref> [6, 7, 10] </ref>, our model captures not only the behavior of TCP's fast retransmit mechanism (which is also considered in [6, 7, 10]) but also the effect of TCP's timeout mechanism on throughput. The measurements we present in Section 3 indicate that this latter behavior is important from a modeling perspective, as we observe more timeout events than fast retransmit events in almost all of our TCP traces. <p> In this model, the duration of a round is equal to the round trip time and is assumed to be independent of the window size, an assumption also adopted (either implicitly or explicitly) in <ref> [6, 7, 10] </ref>. Note that we have also assumed here that the time needed to send all the packets in a window is smaller than the round trip time; this behavior can be seen in observations reported in [2, 12]. <p> p + E [W ] (18) 1p 3b + 8 (1p) 3b RT T 2+b q 3p + ( 2+b (19) Which can be expressed as: B (p) = RT T 3 + o (1= p) (20) Thus, for small values of p, (20) reduces to the throughput formula in <ref> [6] </ref> for b = 1. <p> The line labeled "TD Only" (stands for Triple-Duplicate acks Only) plots the predictions made by the model described in [7], which is essentially the same model as described in <ref> [6] </ref>, while accounting for delayed acks. The line labeled "Proposed (Full)" represents the model described by Equation (31). It has been pointed out in [6] that the "TD Only" model may not be accurate when the frequency of loss indications is higher than 5%. <p> "TD Only" (stands for Triple-Duplicate acks Only) plots the predictions made by the model described in [7], which is essentially the same model as described in <ref> [6] </ref>, while accounting for delayed acks. The line labeled "Proposed (Full)" represents the model described by Equation (31). It has been pointed out in [6] that the "TD Only" model may not be accurate when the frequency of loss indications is higher than 5%. <p> We have also assumed that the time spent in slow start is negligible compared to the length of our traces. Both these assumptions have also been made in <ref> [6, 7, 10] </ref>. We have assumed that packet losses within a round are correlated. Justification for this assumption comes from the fact that the vast majority of the routers in Internet today use the drop-tail policy for packet discard. <p> This is also confirmed by findings in [1]. Another assumption we made, that is also implicit in <ref> [6, 7, 10] </ref>, is that the round trip time is independent of the window size. We have measured the coefficient of correlation between the duration of round samples and the number of packets in transit during each sample. <p> We speculate that this is a combined effect of a slow link and a buffer devoted exclusively to this connection (probably at the ISP, just before the modem). As a result, our model, as well as the models described in <ref> [6, 10, 7] </ref> fail to match the observed data in the case of a receiver at the end of a modem. In Figure 21, we plot results from one such experiment. <p> We have compared our model with the behavior of several real-world TCP connections. We observed that most of these connections suffered from a significant number of timeouts. We found that our model provides a very good match to the observed behavior in most cases, while models proposed in <ref> [6, 7, 10] </ref> significantly overestimate throughput. Thus, we conclude that timeouts have a significant impact on the performance of the TCP protocol, and that our model is able to account for this impact.
Reference: [7] <author> M. Mathis, J. Semske, J. Mahdavi, and T. Ott. </author> <title> The macroscopic behavior of the TCP congestion avoidance algorithm. </title> <journal> Computer Communication Review, </journal> <volume> 27(3), </volume> <month> July </month> <year> 1997. </year>
Reference-contexts: Traditionally, simulation and implementation/measurement have been the tools of choice for examining the performance of various aspects of TCP. Recently, however, several efforts have been directed at analytically characterizing the throughput of TCP's congestion control mechanism, as a function of packet loss and round trip delay <ref> [6, 10, 7] </ref>. One reason for this recent interest is that a simple quantitative characterization of TCP throughput under given operating conditions offers the possibility of defining a "fair share" or "TCP-friendly" [6] throughput for a non-TCP flow that interacts with a TCP connection. <p> Unlike the recent work of <ref> [6, 7, 10] </ref>, our model captures not only the behavior of TCP's fast retransmit mechanism (which is also considered in [6, 7, 10]) but also the effect of TCP's timeout mechanism on throughput. <p> Unlike the recent work of <ref> [6, 7, 10] </ref>, our model captures not only the behavior of TCP's fast retransmit mechanism (which is also considered in [6, 7, 10]) but also the effect of TCP's timeout mechanism on throughput. The measurements we present in Section 3 indicate that this latter behavior is important from a modeling perspective, as we observe more timeout events than fast retransmit events in almost all of our TCP traces. <p> Another important difference between ours and previous work is the ability of our model to accurately predict throughput over a significantly wider range of loss rates than before; measurements presented in <ref> [7] </ref> as well the measurements presented in this paper, indicate that this too is important. We also explicitly model the effects of small receiver-side windows. <p> In this model, the duration of a round is equal to the round trip time and is assumed to be independent of the window size, an assumption also adopted (either implicitly or explicitly) in <ref> [6, 7, 10] </ref>. Note that we have also assumed here that the time needed to send all the packets in a window is smaller than the round trip time; this behavior can be seen in observations reported in [2, 12]. <p> Dividing the total number of loss indications by the total number of packets sent, yields an approximate value of p. This approximation is similar to the one used in <ref> [7] </ref>. The next two columns show a breakdown of the loss indications by type: the number of TD events, and the number of timeouts. Note that p depends only on the total number of loss indications, and not on their type. <p> 2471 0.116 1.879 manic modi4 282547 6072 3976 2096 0.174 2.26 manic pong 358535 4239 2328 1911 0.176 2.137 manic spiff 298465 2035 159 1876 0.253 2.454 manic sutton 348926 6024 3694 2330 0.168 2.185 manic tove 262365 2603 6 2597 0.115 1.955 Table 3: Summary data from 100s traces <ref> [7] </ref>, as well as the predicted throughput from our proposed model given in (31) as described below. <p> The line labeled "TD Only" (stands for Triple-Duplicate acks Only) plots the predictions made by the model described in <ref> [7] </ref>, which is essentially the same model as described in [6], while accounting for delayed acks. The line labeled "Proposed (Full)" represents the model described by Equation (31). <p> We have also assumed that the time spent in slow start is negligible compared to the length of our traces. Both these assumptions have also been made in <ref> [6, 7, 10] </ref>. We have assumed that packet losses within a round are correlated. Justification for this assumption comes from the fact that the vast majority of the routers in Internet today use the drop-tail policy for packet discard. <p> This is also confirmed by findings in [1]. Another assumption we made, that is also implicit in <ref> [6, 7, 10] </ref>, is that the round trip time is independent of the window size. We have measured the coefficient of correlation between the duration of round samples and the number of packets in transit during each sample. <p> We speculate that this is a combined effect of a slow link and a buffer devoted exclusively to this connection (probably at the ISP, just before the modem). As a result, our model, as well as the models described in <ref> [6, 10, 7] </ref> fail to match the observed data in the case of a receiver at the end of a modem. In Figure 21, we plot results from one such experiment. <p> We have compared our model with the behavior of several real-world TCP connections. We observed that most of these connections suffered from a significant number of timeouts. We found that our model provides a very good match to the observed behavior in most cases, while models proposed in <ref> [6, 7, 10] </ref> significantly overestimate throughput. Thus, we conclude that timeouts have a significant impact on the performance of the TCP protocol, and that our model is able to account for this impact.
Reference: [8] <author> S. MCanne and S. Flyod. </author> <title> ns-LBL Network Simulator, </title> <note> 1997. Obtain via http://www-nrg.ee.lbnl.gov/ns/. </note>
Reference-contexts: For example, when we analyze traces from a Linux sender, we account for the fact that TD events occur after getting only two duplicate acks instead of three. Our trace analysis programs were further verified by checking them against tcptrace [9] and ns <ref> [8] </ref>.
Reference: [9] <author> S. Ostermann. tcptrace: </author> <title> TCP dump file analysis tool, </title> <note> 1996. http://jarok.cs.ohiou.edu/software/tcptrace/. </note>
Reference-contexts: For example, when we analyze traces from a Linux sender, we account for the fact that TD events occur after getting only two duplicate acks instead of three. Our trace analysis programs were further verified by checking them against tcptrace <ref> [9] </ref> and ns [8].
Reference: [10] <author> T. Ott, J. Kemperman, and M. Mathis. </author> <title> The stationary behavior of ideal TCP congestion avoidance. </title> <note> in preprint. </note>
Reference-contexts: Traditionally, simulation and implementation/measurement have been the tools of choice for examining the performance of various aspects of TCP. Recently, however, several efforts have been directed at analytically characterizing the throughput of TCP's congestion control mechanism, as a function of packet loss and round trip delay <ref> [6, 10, 7] </ref>. One reason for this recent interest is that a simple quantitative characterization of TCP throughput under given operating conditions offers the possibility of defining a "fair share" or "TCP-friendly" [6] throughput for a non-TCP flow that interacts with a TCP connection. <p> Unlike the recent work of <ref> [6, 7, 10] </ref>, our model captures not only the behavior of TCP's fast retransmit mechanism (which is also considered in [6, 7, 10]) but also the effect of TCP's timeout mechanism on throughput. <p> Unlike the recent work of <ref> [6, 7, 10] </ref>, our model captures not only the behavior of TCP's fast retransmit mechanism (which is also considered in [6, 7, 10]) but also the effect of TCP's timeout mechanism on throughput. The measurements we present in Section 3 indicate that this latter behavior is important from a modeling perspective, as we observe more timeout events than fast retransmit events in almost all of our TCP traces. <p> In this model, the duration of a round is equal to the round trip time and is assumed to be independent of the window size, an assumption also adopted (either implicitly or explicitly) in <ref> [6, 7, 10] </ref>. Note that we have also assumed here that the time needed to send all the packets in a window is smaller than the round trip time; this behavior can be seen in observations reported in [2, 12]. <p> We have also assumed that the time spent in slow start is negligible compared to the length of our traces. Both these assumptions have also been made in <ref> [6, 7, 10] </ref>. We have assumed that packet losses within a round are correlated. Justification for this assumption comes from the fact that the vast majority of the routers in Internet today use the drop-tail policy for packet discard. <p> This is also confirmed by findings in [1]. Another assumption we made, that is also implicit in <ref> [6, 7, 10] </ref>, is that the round trip time is independent of the window size. We have measured the coefficient of correlation between the duration of round samples and the number of packets in transit during each sample. <p> We speculate that this is a combined effect of a slow link and a buffer devoted exclusively to this connection (probably at the ISP, just before the modem). As a result, our model, as well as the models described in <ref> [6, 10, 7] </ref> fail to match the observed data in the case of a receiver at the end of a modem. In Figure 21, we plot results from one such experiment. <p> We have compared our model with the behavior of several real-world TCP connections. We observed that most of these connections suffered from a significant number of timeouts. We found that our model provides a very good match to the observed behavior in most cases, while models proposed in <ref> [6, 7, 10] </ref> significantly overestimate throughput. Thus, we conclude that timeouts have a significant impact on the performance of the TCP protocol, and that our model is able to account for this impact.
Reference: [11] <author> J. Padhye, V. Firoiu, D. Towsley, and J. Kurose. </author> <title> Modeling TCP throughput: A simple model and its empirical validation. </title> <type> Technical report UMASS-CS-TR-1998-08. </type>
Reference-contexts: This underscores the importance of including the effects of timeouts in the model of TCP congestion control. We have also noticed that exponential backoff due to multiple timeouts occurs with significant frequency. More details are provided in <ref> [11] </ref>. Next, we use the measurement data described above to validate our model proposed in Section 2. Figures 7-12 plot the measured throughput in our trace data, the model of Sender Receiver Packets Loss TD TO RTT Time Sent Indic.
Reference: [12] <author> V. Paxson. </author> <title> Automated packet trace analysis of TCP implementations. </title> <booktitle> In Proceedings of SIGCOMM 97, </booktitle> <year> 1997. </year>
Reference-contexts: Our model is based on the Reno flavor of TCP, as it is by far the most popular implementation in the Internet today <ref> [13, 12] </ref>. We assume that the reader is familiar with TCP Reno congestion control (see for example [4, 17, 16]) and we adopt most of our terminology from [4, 17, 16]. <p> Note that we have also assumed here that the time needed to send all the packets in a window is smaller than the round trip time; this behavior can be seen in observations reported in <ref> [2, 12] </ref>. At the beginning of the next round, a group of W 0 new packets will be sent, where W 0 is the new size of the congestion control window. Let b be the number of packets that are acknowledged by a received ACK. <p> All data sets are for unidirectional bulk data transfers. We gathered the measurement data by running tcpdump at the sender, and analyzing its output with a set of analysis programs developed by us. These programs account for various measurement and implementation related problems discussed in <ref> [13, 12] </ref>. For example, when we analyze traces from a Linux sender, we account for the fact that TD events occur after getting only two duplicate acks instead of three. Our trace analysis programs were further verified by checking them against tcptrace [9] and ns [8]. <p> The results are for a 1 hour connection divided into 100 second intervals. We have also assumed that all of our senders implement TCP-Reno as described in [4, 17, 16]. In <ref> [13, 12] </ref>, it is observed that the implementation of the protocol stack in each operating system is slightly different.
Reference: [13] <author> V. Paxson. </author> <title> End-to-End Internet packet dynamics. </title> <booktitle> In Proceedings of SIGCOMM 97, </booktitle> <year> 1997. </year>
Reference-contexts: Our model is based on the Reno flavor of TCP, as it is by far the most popular implementation in the Internet today <ref> [13, 12] </ref>. We assume that the reader is familiar with TCP Reno congestion control (see for example [4, 17, 16]) and we adopt most of our terminology from [4, 17, 16]. <p> All data sets are for unidirectional bulk data transfers. We gathered the measurement data by running tcpdump at the sender, and analyzing its output with a set of analysis programs developed by us. These programs account for various measurement and implementation related problems discussed in <ref> [13, 12] </ref>. For example, when we analyze traces from a Linux sender, we account for the fact that TD events occur after getting only two duplicate acks instead of three. Our trace analysis programs were further verified by checking them against tcptrace [9] and ns [8]. <p> The results are for a 1 hour connection divided into 100 second intervals. We have also assumed that all of our senders implement TCP-Reno as described in [4, 17, 16]. In <ref> [13, 12] </ref>, it is observed that the implementation of the protocol stack in each operating system is slightly different. <p> Our observations also seem to indicate that in the Irix implementation, the exponential backoff is limited to 2 5 , instead of 2 6 . We are also aware of the observation made in <ref> [13] </ref> that the SunOS TCP implementation is derived from Tahoe and not Reno. We have not customized our model for these cases. 5 Conclusion In this paper we have presented a simple model of the TCP-Reno protocol.
Reference: [14] <author> V. Paxson and S. Floyd. </author> <title> Why we don't know how to simulate the Internet. </title> <booktitle> In Proccedings of the 1997 Winter Simulation Conference, </booktitle> <year> 1997. </year>
Reference-contexts: TCP is a protocol that can exhibit complex behavior, especially when considered in the context of the current Internet, where the traffic conditions themselves can be quite complicated and subtle <ref> [14] </ref>.
Reference: [15] <author> S. Ross. </author> <title> Applied Probability Models with Optimization Applications. </title> <publisher> Dover, </publisher> <year> 1970. </year>
Reference-contexts: Considering fW i g i to be a Markov regenerative process with rewards fY i g i (see for example <ref> [15] </ref>), it can be shown that B = E [A] In order to derive an expression for B, the long-term steady-state TCP throughput, we must next derive expressions for the mean of Y and A. Consider a TD period as in Figure 2.
Reference: [16] <author> W. Stevens. </author> <title> TCP/IP Illustrated, Vol.1 The Protocols. </title> <publisher> Addison-Wesley, </publisher> <year> 1994. </year>
Reference-contexts: Our model is based on the Reno flavor of TCP, as it is by far the most popular implementation in the Internet today [13, 12]. We assume that the reader is familiar with TCP Reno congestion control (see for example <ref> [4, 17, 16] </ref>) and we adopt most of our terminology from [4, 17, 16]. Our model focuses on TCP's congestion avoidance mechanism, where TCP's congestion control window size, W; is increased by 1=W each time an ACK is received. <p> We assume that the reader is familiar with TCP Reno congestion control (see for example <ref> [4, 17, 16] </ref>) and we adopt most of our terminology from [4, 17, 16]. Our model focuses on TCP's congestion avoidance mechanism, where TCP's congestion control window size, W; is increased by 1=W each time an ACK is received. <p> Let b be the number of packets that are acknowledged by a received ACK. Many TCP receiver implementations send one cumulative ACK for two consecutive packets received (i.e., delayed ACK, <ref> [16] </ref>), so b is typically 2. If W packets are sent in the first round and are all received and acknowledged correctly, then W=b acknowledgments will be received. <p> The results are for a 1 hour connection divided into 100 second intervals. We have also assumed that all of our senders implement TCP-Reno as described in <ref> [4, 17, 16] </ref>. In [13, 12], it is observed that the implementation of the protocol stack in each operating system is slightly different. <p> For example, we have observed that the Linux exponential backoff does not exactly follow the algorithm described in <ref> [4, 17, 16] </ref>. Our observations also seem to indicate that in the Irix implementation, the exponential backoff is limited to 2 5 , instead of 2 6 . We are also aware of the observation made in [13] that the SunOS TCP implementation is derived from Tahoe and not Reno.
Reference: [17] <author> W. Stevens. </author> <title> TCP Slow Start, Congestion Avoidance, Fast Retransmit, and Fast Recovery Algorithms. </title> <address> RFC2001, </address> <month> Jan </month> <year> 1997. </year>
Reference-contexts: Our model is based on the Reno flavor of TCP, as it is by far the most popular implementation in the Internet today [13, 12]. We assume that the reader is familiar with TCP Reno congestion control (see for example <ref> [4, 17, 16] </ref>) and we adopt most of our terminology from [4, 17, 16]. Our model focuses on TCP's congestion avoidance mechanism, where TCP's congestion control window size, W; is increased by 1=W each time an ACK is received. <p> We assume that the reader is familiar with TCP Reno congestion control (see for example <ref> [4, 17, 16] </ref>) and we adopt most of our terminology from [4, 17, 16]. Our model focuses on TCP's congestion avoidance mechanism, where TCP's congestion control window size, W; is increased by 1=W each time an ACK is received. <p> The results are for a 1 hour connection divided into 100 second intervals. We have also assumed that all of our senders implement TCP-Reno as described in <ref> [4, 17, 16] </ref>. In [13, 12], it is observed that the implementation of the protocol stack in each operating system is slightly different. <p> For example, we have observed that the Linux exponential backoff does not exactly follow the algorithm described in <ref> [4, 17, 16] </ref>. Our observations also seem to indicate that in the Irix implementation, the exponential backoff is limited to 2 5 , instead of 2 6 . We are also aware of the observation made in [13] that the SunOS TCP implementation is derived from Tahoe and not Reno.
Reference: [18] <author> K. Thompson, G. Miller, and M. Wilder. </author> <title> Wide-area internet traffic patterns and charateristics. </title> <journal> IEEE Network, </journal> <volume> 11(6), </volume> <month> November-December </month> <year> 1997. </year>
Reference-contexts: 1 Introduction A significant amount of today's Internet traffic, including WWW (HTTP), file transfer (FTP), email (SMTP), and remote access (Telnet) traffic, is carried by the TCP transport protocol <ref> [18] </ref>. TCP together with UDP form the very core of today's Internet transport layer. Traditionally, simulation and implementation/measurement have been the tools of choice for examining the performance of various aspects of TCP.
Reference: [19] <author> T. Turletti, S. Parisis, and J. Bolot. </author> <title> Experiments with a layered transmission scheme over the Internet. </title> <type> Technical report RR-3296, </type> <institution> INRIA, France. </institution> <note> Obtain via http://www.inria.fr/RRRT/RR-3296.html. </note>
Reference-contexts: Indeed, this notion has already been adopted in the design and development of several multicast congestion control protocols <ref> [19, 20] </ref>. fl This material is based upon work supported by the National Science Foundation under grants NCR-95-08274, NCR-95-23807 and CDA-95-02639. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation. <p> We have also presented a simplified expression for TCP bandwidth in Equation (32), which is a good approximation for the proposed model in most cases. This simple approximation can be used in protocols such as those described in <ref> [19, 20] </ref> to ensure "TCP-friendliness'. A number of avenues for future work remain. First, our model can be enhanced to account for the effects of fast recovery and fast retransmit. Second, a more precise throughput calculation can be obtained if the congestion window size is modeled as a Markov chain.
Reference: [20] <author> L. Vicisano, L. Rizzo, and J. Crowcroft. </author> <title> TCP-like congestion control for layered multicast data transfer. </title> <booktitle> In Proceedings of INFOCOMM'98, </booktitle> <year> 1998. </year>
Reference-contexts: Indeed, this notion has already been adopted in the design and development of several multicast congestion control protocols <ref> [19, 20] </ref>. fl This material is based upon work supported by the National Science Foundation under grants NCR-95-08274, NCR-95-23807 and CDA-95-02639. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation. <p> We have also presented a simplified expression for TCP bandwidth in Equation (32), which is a good approximation for the proposed model in most cases. This simple approximation can be used in protocols such as those described in <ref> [19, 20] </ref> to ensure "TCP-friendliness'. A number of avenues for future work remain. First, our model can be enhanced to account for the effects of fast recovery and fast retransmit. Second, a more precise throughput calculation can be obtained if the congestion window size is modeled as a Markov chain.
References-found: 20

