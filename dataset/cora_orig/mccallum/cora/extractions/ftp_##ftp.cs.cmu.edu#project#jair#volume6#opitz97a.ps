URL: ftp://ftp.cs.cmu.edu/project/jair/volume6/opitz97a.ps
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00298.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: opitz@cs.umt.edu  shavlik@cs.wisc.edu  
Title: Connectionist Theory Refinement: Genetically Searching the Space of Network Topologies  
Author: David W. Opitz Jude W. Shavlik 
Address: Missoula, MT 59812 USA  1210 W. Dayton St. Madison, WI 53706 USA  
Affiliation: Department of Computer Science University of Montana  Computer Sciences Department University of Wisconsin  
Note: Journal of Artificial Intelligence Research 6 (1997) 177-209 Submitted 10/96; published 5/97  
Abstract: An algorithm that learns from a set of examples should ideally be able to exploit the available resources of (a) abundant computing power and (b) domain-specific knowledge to improve its ability to generalize. Connectionist theory-refinement systems, which use background knowledge to select a neural network's topology and initial weights, have proven to be effective at exploiting domain-specific knowledge; however, most do not exploit available computing power. This weakness occurs because they lack the ability to refine the topology of the neural networks they produce, thereby limiting generalization, especially when given impoverished domain theories. We present the Regent algorithm which uses (a) domain-specific knowledge to help create an initial population of knowledge-based neural networks and (b) genetic operators of crossover and mutation (specifically designed for knowledge-based networks) to continually search for better network topologies. Experiments on three real-world domains indicate that our new algorithm is able to significantly increase generalization compared to a standard connectionist theory-refinement system, as well as our previous algorithm for growing knowledge-based networks. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Ackley, D. </author> <year> (1987). </year> <title> A Connectionist Machine for Genetic Hillclimbing. </title> <publisher> Kluwer, Norwell, </publisher> <address> MA. </address>
Reference-contexts: As a future research topic, we plan to investigate incorporating diversity-promoting techniques once we are able to consider tens of thousands of networks. Regent can be considered a Lamarckian 3 , genetic-hillclimbing algorithm <ref> (Ackley, 1987) </ref>, since it performs local optimizations on individuals, then passes the successful optimizations on to offspring. The ability of individuals to learn can smooth the fitness landscape and facilitate subsequent learning.
Reference: <author> Ackley, D., & Littman, M. </author> <year> (1992). </year> <title> Interactions between learning and evolution. </title> <editor> In Langton, C., Taylor, C., Farmer, C., & Rasmussen, S. (Eds.), </editor> <booktitle> Artificial Life II, </booktitle> <pages> pp. 487-509, </pages> <address> Redwood City, CA. </address> <publisher> Addison-Wesley. </publisher>
Reference: <author> Ackley, D., & Littman, M. </author> <year> (1994). </year> <title> A case for Lamarckian evolution. </title> <editor> In Langton, C. (Ed.), </editor> <booktitle> Artificial Life III, </booktitle> <pages> pp. 3-10, </pages> <address> Redwood City, CA. </address> <publisher> Addison-Wesley. </publisher>
Reference: <author> Baffes, P., & Mooney, R. </author> <year> (1993). </year> <title> Symbolic revision of theories with M-of-N rules. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 1135-1140, </pages> <address> Chambery, France. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Bala, J., Huang, J., Vafaie, H., DeJong, K., & Wechsler, H. </author> <year> (1995). </year> <title> Hybrid learning using genetic algorithms and decision trees for pattern classification. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 719-724, </pages> <address> Montreal, Canada. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Baldwin, J. </author> <title> (1896). </title> <journal> Physical and social heredity. American Naturalist, </journal> <volume> 30, </volume> <pages> 441-451. </pages>
Reference: <author> Baum, E., & Haussler, D. </author> <year> (1989). </year> <title> What size net gives valid generalization? Neural Computation, </title> <booktitle> 1, </booktitle> <pages> 151-160. </pages>
Reference: <author> Baum, E., & Lang, K. </author> <year> (1991). </year> <title> Constructing hidden units using examples and queries. </title> <editor> In Lippmann, R., Moody, J., & Touretzky, D. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> Vol. 3, </volume> <pages> pp. 904-910, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher> <editor> 202 Connectionist Theory Refinement Belew, R. </editor> <year> (1990). </year> <title> Evolution, learning and culture: Computational metaphors for adaptive search. </title> <journal> Complex Systems, </journal> <volume> 4, </volume> <pages> 11-49. </pages>
Reference: <author> Belew, R., McInerney, J., & Schraudolph, N. </author> <year> (1992). </year> <title> Evolving networks: Using the genetic algorithm with connectionist learning. </title> <editor> In Langton, C., Taylor, C., Farmer, C., & Rasmussen, S. (Eds.), </editor> <booktitle> Artificial Life II, </booktitle> <pages> pp. 511-547, </pages> <address> Redwood City, CA. </address> <publisher> Addison-Wesley. </publisher>
Reference: <author> Belew, R., & Mitchell, M. </author> <year> (1996). </year> <title> Adaptive Individuals in Evolving Populations: Models and Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Massachusetts. </address>
Reference: <author> Berenji, H. </author> <year> (1991). </year> <title> Refinement of approximate reasoning-based controllers by reinforcement learning. </title> <booktitle> In Proceedings of the Eighth International Machine Learning Workshop, </booktitle> <pages> pp. 475-479, </pages> <address> Evanston, IL. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Bergadano, F., Giordana, A., & Ponsero, S. </author> <year> (1989). </year> <title> Deduction in top-down inductive learning. </title> <booktitle> In Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <pages> pp. 23-25, </pages> <address> Ithaca, NY. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: There have been several genetic-based, first-order logic, multimodal concept learners (Greene & Smith, 1993; Janikow, 1993). Giordana and Saitta (1993) showed how to integrate one of these system, Regal (Giordana, Saitta, & Zini, 1994; Neri & Saitta, 1996), with the deductive engine of ML-SMART <ref> (Bergadano, Giordana, & Ponsero, 1989) </ref> to help refine an incomplete or inconsistent domain theory. This version works by first using an automated theorem prover to recognize unresolved literals in a proof, then uses the GA-based Regal to induce corrections to these literals.
Reference: <author> Blanziere, E., & Katenkamp, P. </author> <year> (1996). </year> <title> Learning radial basis function networks on-line. </title> <booktitle> In Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <pages> pp. 37-45, </pages> <address> Bari, Italy. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Clocksin, W., & Mellish, C. </author> <year> (1987). </year> <title> Programming in Prolog. </title> <publisher> Springer-Verlag, </publisher> <address> New York. </address>
Reference-contexts: Refer to Towell (1991) or Towell and Shavlik (1994) for more details. Kbann has been successfully applied to several real-world problems, such as the control of a chemical plant (Scott, Shavlik, & Ray, 1992), protein folding (Maclin & Shavlik, 1993), a sample propositional rule set in Prolog <ref> (Clocksin & Mellish, 1987) </ref> notation, panel (b) illustrates this rule set's corresponding and/or dependency tree, and panel (c) shows the resulting network created by Kbann's translation. 181 Opitz & Shavlik finding genes in a sequence of DNA (Opitz & Shavlik, 1995; Towell & Shavlik, 1994), and ECG patient monitoring (Watrous, Towell,
Reference: <author> Cohen, W. </author> <year> (1992). </year> <title> Compiling prior knowledge into an explicit bias. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> pp. 102-110, </pages> <address> Aberdeen, Scotland. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Systems such as Focl (Pazzani & Kibler, 1992) and Forte (Richards & Mooney, 1995) are first-order, theory-refinement systems that revise predicate-logic theories. One drawback to these systems is that they currently do not generalize as well as connectionist approaches on many real-world problems, such as the DNA promoter task <ref> (Cohen, 1992) </ref>. There have been several genetic-based, first-order logic, multimodal concept learners (Greene & Smith, 1993; Janikow, 1993).
Reference: <author> Danyluk, A. </author> <year> (1989). </year> <title> Finding new rules for incomplete theories: Explicit biases for induction with contextual information. </title> <booktitle> In Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <pages> pp. 34-36, </pages> <address> Ithaca, NY. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: They term their approach as being more theory-suggested than theory-guided (Hekanaho, 1996). Several systems, including ours, have been proposed for refining propositional rule bases. Early such approaches could only handle improvements to overly specific theories <ref> (Danyluk, 1989) </ref> or specializations to overly general theories (Flann & Dietterich, 1989). Later systems such as Rtls (Ginsberg, 1990), Either (Ourston & Mooney, 1994), Ptr (Koppel, Feldman, & Segre, 1994), and Tgci (Donoho & Rendell, 1995) were later able to handle both types of refinements.
Reference: <author> Das, A., Giles, C., & Sun, G. </author> <year> (1992). </year> <title> Using prior knowledge in an NNPDA to learn context-free languages. </title> <editor> In Hanson, S., Cowan, J., & Giles, C. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> Vol. 5, </volume> <pages> pp. 65-72, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: These systems have been developed to refine many types of rule bases. For instance, a number of systems have been proposed for revising certainty-factor rule bases (Fu, 1989; Lacher et al., 1992; Mahoney & Mooney, 1993), finite-state automata (Maclin & Shavlik, 1993; Omlin & Giles, 1992), push-down automata <ref> (Das, Giles, & Sun, 1992) </ref>, fuzzy-logic rules (Berenji, 1991; Masuoka, Watanabe, Kawamura, Owada, & Asakawa, 1990), and mathematical equations (Roscheisen, Hofmann, & Tresp, 1991; Scott et al., 1992).
Reference: <author> Dean, T., & Boddy, M. </author> <year> (1988). </year> <title> An analysis of time-dependent planning. </title> <booktitle> In Proceedings of the Seventh National Conference on Artificial Intelligence, </booktitle> <pages> pp. 49-54, </pages> <address> St. Paul, MN. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Thus, given the rapid growth in computing power, we believe it is important for learning techniques to be able to trade off the expense of large numbers of computing cycles for gains in predictive accuracy. Analogous to anytime planning techniques <ref> (Dean & Boddy, 1988) </ref>, we believe machine learning researchers should create better anytime learning algorithms. Such learning algorithms should produce a good concept quickly, then continue to search concept space, reporting the new "best" concept whenever one is found.
Reference: <author> DeJong, K. </author> <year> (1975). </year> <title> An Analysis of the Behavior of a class of Genetic Adaptive Systems. </title> <type> Ph.D. thesis, </type> <institution> University of Michigan, </institution> <address> Ann Arbor, MI. </address>
Reference-contexts: We choose GAs for two reasons. First, GAs have been shown to be effective optimization techniques because of their efficient use of global information (Goldberg, 1989; Holland, 1975; Mitchell, 1996). Second, GAs have an inherent quality which makes them suitable for anytime learning. In "off-line" application mode <ref> (DeJong, 1975) </ref>, GAs simulate many alternatives and output the best alternative seen so far. Our new algorithm, Regent, proceeds by first trying to generate, from the domain theory, a diversified initial population.
Reference: <author> Dodd, N. </author> <year> (1990). </year> <title> Optimization of network structure using genetic techniques. </title> <booktitle> In Proceedings of the IEEE International Joint Conference on Neural Networks, </booktitle> <volume> Vol. III, </volume> <pages> pp. 965-970, </pages> <note> Paris. IEEE Press. 203 Opitz & Shavlik Donoho, </note> <author> S., & Rendell, L. </author> <year> (1995). </year> <title> Rerepresenting and restructuring domain theories: A constructive induction approach. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2, </volume> <pages> 411-446. </pages>
Reference: <author> Drucker, H., Cortes, C., Jackel, L., LeCun, Y., & Vapnik, V. </author> <year> (1994). </year> <title> Boosting and other machine learning algorithms. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pp. 53-61, </pages> <address> New Brunswick, NJ. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Fahlman, S., & Lebiere, C. </author> <year> (1989). </year> <title> The cascade-correlation learning architecture. </title> <editor> In Touret-zky, D. (Ed.), </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> Vol. 2, </volume> <pages> pp. 524-532, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Farmer, J., & Belin, A. </author> <year> (1992). </year> <title> Artificial life: The coming evolution. </title> <editor> In Langton, C., Taylor, C., Farmer, J. D., & Rasmussen, S. (Eds.), </editor> <booktitle> Artificial Life II, </booktitle> <pages> pp. 815-840, </pages> <address> Redwood City, CA. </address> <publisher> Addison-Wesley. </publisher>
Reference: <author> Flann, N., & Dietterich, T. </author> <year> (1989). </year> <title> A study of explanation-based methods for inductive learning. </title> <journal> Machine Learning, </journal> <volume> 4, </volume> <pages> 187-226. </pages>
Reference-contexts: They term their approach as being more theory-suggested than theory-guided (Hekanaho, 1996). Several systems, including ours, have been proposed for refining propositional rule bases. Early such approaches could only handle improvements to overly specific theories (Danyluk, 1989) or specializations to overly general theories <ref> (Flann & Dietterich, 1989) </ref>. Later systems such as Rtls (Ginsberg, 1990), Either (Ourston & Mooney, 1994), Ptr (Koppel, Feldman, & Segre, 1994), and Tgci (Donoho & Rendell, 1995) were later able to handle both types of refinements.
Reference: <author> Fletcher, J., & Obradovic, Z. </author> <year> (1993). </year> <title> Combining prior symbolic knowledge and constructive neural network learning. </title> <journal> Connection Science, </journal> <volume> 5, </volume> <pages> 365-375. </pages>
Reference: <author> Forrest, S., & Mitchell, M. </author> <year> (1993). </year> <title> What makes a problem hard for a genetic algorithm? Some anomalous results and their explanation. </title> <journal> Machine Learning, </journal> <volume> 13, </volume> <pages> 285-319. </pages>
Reference: <author> Frean, M. </author> <year> (1990). </year> <title> The upstart algorithm: A method for constructing and training feedfor-ward neural networks. </title> <journal> Neural Computation, </journal> <volume> 2, </volume> <pages> 198-209. </pages>
Reference-contexts: Like most connectionist theory-refinement systems, Rapture first translates the domain theory into a neural network, then refines the weights of the network with a modified backpropagation algorithm. Like Regent, Rapture is then able to dynamically refine the topology of its network. It does this by using the Upstart algorithm <ref> (Frean, 1990) </ref> to add new nodes to the network. Aside from being designed for probabilistic rules, Rapture differs from Regent in that it adds nodes with the intention of completely learning the training set, not generalizing well.
Reference: <author> Fu, L. </author> <year> (1989). </year> <title> Integration of neural heuristics into knowledge-based inference. </title> <journal> Connection Science, </journal> <volume> 1, </volume> <pages> 325-340. </pages>
Reference: <author> Ginsberg, A. </author> <year> (1990). </year> <title> Theory reduction, theory revision, </title> <booktitle> and retranslation. In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pp. 777-782, </pages> <address> Boston, MA. </address> <publisher> AAAI/MIT Press. </publisher>
Reference-contexts: Several systems, including ours, have been proposed for refining propositional rule bases. Early such approaches could only handle improvements to overly specific theories (Danyluk, 1989) or specializations to overly general theories (Flann & Dietterich, 1989). Later systems such as Rtls <ref> (Ginsberg, 1990) </ref>, Either (Ourston & Mooney, 1994), Ptr (Koppel, Feldman, & Segre, 1994), and Tgci (Donoho & Rendell, 1995) were later able to handle both types of refinements.
Reference: <author> Giordana, A., & Saitta, L. </author> <year> (1993). </year> <title> REGAL: An integrated system for relations using genetic algorithms. </title> <booktitle> In Proceedings of the Second International Workshop on Multistrategy Learning, </booktitle> <pages> pp. 234-249, </pages> <address> Harpers Ferry, WV. </address>
Reference: <author> Giordana, A., Saitta, L., & Zini, F. </author> <year> (1994). </year> <title> Learning disjunctive concepts by means of genetic algorithms. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pp. 96-104, </pages> <address> New Brunswick, NJ. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Goldberg, D. </author> <year> (1989). </year> <title> Genetic Algorithms in Search, Optimization, and Machine Learning. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA. </address>
Reference-contexts: When Regent replaces a member, it replaces the member having the lowest correctness (ties are broken by choosing the oldest member). Other techniques <ref> (Goldberg, 1989) </ref>, such as replacing the member nearest the new candidate network, can promote diverse populations; however, we do not want to promote diversity at the expense of decreased generalization.
Reference: <author> Greene, D., & Smith, S. </author> <year> (1993). </year> <title> Competition-based induction of decision models from examples. </title> <journal> Machine Learning, </journal> <volume> 13, </volume> <pages> 229-258. </pages> <note> 204 Connectionist Theory Refinement Grefenstette, </note> <author> J., & Ramsey, C. </author> <year> (1992). </year> <title> An approach to anytime learning. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> pp. 189-195, </pages> <address> Aberdeen, Scotland. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Hansen, L., & Salamon, P. </author> <year> (1990). </year> <title> Neural network ensembles. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 12, </volume> <pages> 993-1001. </pages>
Reference: <author> Harp, S., Samad, T., & Guha, A. </author> <year> (1989). </year> <title> Designing application-specific neural networks using the genetic algorithm. </title> <editor> In Touretzky, D. (Ed.), </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> Vol. 2, </volume> <pages> pp. 447-454, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Hart, W. </author> <year> (1994). </year> <title> Adaptive Global Optimization with Local Search. </title> <type> Ph.D. thesis, </type> <institution> University of California, </institution> <address> San Diego. </address>
Reference-contexts: other hand, perform a more sophisticated search across multiple local minima and are good at finding regions of the search space where near-optimal solutions can be found; however, they are usually not as good at refining a solution (once it is close to a near-optimal solution) as local optimization strategies <ref> (Hart, 1994) </ref>. Recent research has shown that it is desirable to emply both a global and local search strategy (Hart, 1994). Hybrid GAs (such as Regent) combine local search with a more traditional GA. <p> search space where near-optimal solutions can be found; however, they are usually not as good at refining a solution (once it is close to a near-optimal solution) as local optimization strategies <ref> (Hart, 1994) </ref>. Recent research has shown that it is desirable to emply both a global and local search strategy (Hart, 1994). Hybrid GAs (such as Regent) combine local search with a more traditional GA. While we focus on hybrid-GA algorithms in this section, this two-tiered search strategy has been employed by other researchers as well (Kohavi & John, 1997; Provost & Buchanan, 1995; Schaffer, 1993).
Reference: <author> Hassibi, B., & Stork, D. </author> <year> (1992). </year> <title> Second order derivatives for network pruning: Optimal brain surgeon. </title> <editor> In Hanson, S., Cowan, J., & Giles, C. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> Vol. 5, </volume> <pages> pp. 164-171, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Hekanaho, J. </author> <year> (1996). </year> <title> Background knowledge in GA-based concept learning. </title> <booktitle> In Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <pages> pp. 234-242, </pages> <address> Bari, Italy. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Regent, on the other hand, use genetic algorithms (along with neural learning) to refine the whole domain theory at the same time. Dogma <ref> (Hekanaho, 1996) </ref> is a recently proposed GA-based learner that can use background knowledge to learn the same description language as Regal. Current restrictions, however, force the representation language of the domain theory to be propositional rules. <p> Dogma does not focus on theory refinement, rather it builds a completely new theory using substructures from the background knowledge. They term their approach as being more theory-suggested than theory-guided <ref> (Hekanaho, 1996) </ref>. Several systems, including ours, have been proposed for refining propositional rule bases. Early such approaches could only handle improvements to overly specific theories (Danyluk, 1989) or specializations to overly general theories (Flann & Dietterich, 1989).
Reference: <author> Hinton, G., & Nowlan, S. </author> <year> (1987). </year> <title> How learning can guide evolution. </title> <journal> Complex Systems, </journal> <volume> 1, </volume> <pages> 495-502. </pages>
Reference: <author> Holder, L. </author> <year> (1991). </year> <title> Maintaining the Utility of Learned Knowledge Using Model-Based Control. </title> <type> Ph.D. thesis, </type> <institution> Computer Science Department, University of Illinois at Urbana-Champaign. </institution>
Reference-contexts: In fact, if run too long, these algorithms tend to "overfit" the training set <ref> (Holder, 1991) </ref>. Overfitting occurs when the learning algorithm produces a concept that captures too much information about the training examples, and not enough about the general characteristics of the domain as a whole.
Reference: <author> Holland, J. </author> <year> (1975). </year> <title> Adaptation in Natural and Artificial Systems. </title> <publisher> University of Michigan Press, </publisher> <address> Ann Arbor, MI. </address>
Reference: <author> Janikow, C. </author> <year> (1993). </year> <title> A knowledge intensive GA for supervised learning. </title> <journal> Machine Learning, </journal> <volume> 13, </volume> <pages> 198-228. </pages>
Reference: <author> Judson, R., Colvin, M., Meza, J., Huffa, A., & Gutierrez, D. </author> <year> (1992). </year> <title> Do intelligent configuration search techniques outperform random search for large molecules? International Journal of Quantum Chemistry, </title> <type> 277-290. </type>
Reference: <author> Kibler, D., & Langley, P. </author> <year> (1988). </year> <title> Machine learning as an experimental science. </title> <booktitle> In Proceedings of the Third European Working Session on Learning, </booktitle> <pages> pp. 1-12, </pages> <address> Edinburgh, UK. </address>
Reference-contexts: Our results 4. A lesion study is one where components of an algorithm are individually disabled to ascertain their contribution to the full algorithm's performance <ref> (Kibler & Langley, 1988) </ref>. 188 Connectionist Theory Refinement 189 Opitz & Shavlik show Kbann generalizes much better than the best of these standard networks, thus further confirming Kbann's effectiveness in generating good network topologies.
Reference: <author> Kitano, H. </author> <year> (1990a). </year> <title> Designing neural networks using genetic algorithms with graph generation system. </title> <journal> Complex Systems, </journal> <volume> 4, </volume> <pages> 461-476. </pages>
Reference: <author> Kitano, H. </author> <year> (1990b). </year> <title> Empirical studies on the speed of convergence of neural network training using genetic algorithms. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pp. 789-795, </pages> <address> Boston, MA. </address> <publisher> AAAI/MIT Press. 205 Opitz & Shavlik Kohavi, </publisher> <editor> R., & John, G. </editor> <year> (1997). </year> <title> Wrappers for feature subset selection. </title> <journal> Artificial Intelligence. </journal>
Reference: <author> Koppel, M., Feldman, R., & Segre, A. </author> <year> (1994). </year> <title> Bias-driven revision of logical domain theories. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 1, </volume> <pages> 159-208. </pages>
Reference-contexts: Several systems, including ours, have been proposed for refining propositional rule bases. Early such approaches could only handle improvements to overly specific theories (Danyluk, 1989) or specializations to overly general theories (Flann & Dietterich, 1989). Later systems such as Rtls (Ginsberg, 1990), Either (Ourston & Mooney, 1994), Ptr <ref> (Koppel, Feldman, & Segre, 1994) </ref>, and Tgci (Donoho & Rendell, 1995) were later able to handle both types of refinements.
Reference: <author> Koza, J. </author> <year> (1992). </year> <title> Genetic Programming. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: One could prune weights and nodes during Regent's search; however, such pruning can prematurely reduce the variety of structures available for recombination during crossover <ref> (Koza, 1992) </ref>. Real-life organisms, for instance, have superfluous DNA that are believed to enhance the rate of evolution (Watson, Hopkins, Roberts, Argetsinger-Steitz, & Weiner, 1987).
Reference: <author> Koza, J., & Rice, J. </author> <year> (1991). </year> <title> Genetic generation of both the weights and architectures for a neural network. </title> <booktitle> In International Joint Conference on Neural Networks, </booktitle> <volume> Vol. 2, </volume> <pages> pp. 397-404, </pages> <address> Seattle, WA. </address> <publisher> IEEE Press. </publisher>
Reference: <author> Krogh, A., & Vedelsby, J. </author> <year> (1995). </year> <title> Neural network ensembles, cross validation, and active learning. </title> <editor> In Tesauro, G., Touretzky, D., & Leen, T. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> Vol. 7, </volume> <pages> pp. 231-238, </pages> <address> Cambridge, MA. </address> <publisher> MIT Press. </publisher>
Reference: <author> Lacher, R., Hruska, S., & Kuncicky, D. </author> <year> (1992). </year> <title> Back-propagation learning in expert networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 3, </volume> <pages> 62-72. </pages>
Reference: <author> Le Cun, Y., Denker, J., & Solla, S. </author> <year> (1989). </year> <title> Optimal brain damage. </title> <editor> In Touretzky, D. (Ed.), </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> Vol. 2, </volume> <pages> pp. 598-605, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Of these methods, many directly encode each link in the network (Miller, Todd, & Hegde, 1989; Oliker, Furst, & Maimon, 1992; Schiffmann, Joost, & Werner, 1992). These methods are relatively straightforward to implement, and are good at fine tuning small networks <ref> (Miller et al., 1989) </ref>; however, they do not scale well since they require very large matrices to represent all the links in large networks (Yao, 1993).
Reference: <author> Litzkow, M., Livny, M., & Mutka, M. </author> <year> (1988). </year> <title> Condor | a hunter of idle workstations. </title> <booktitle> In Proceedings of the Eighth International Conference on Distributed Computing Systems, </booktitle> <pages> pp. 104-111, </pages> <address> San Jose, CA. </address> <publisher> Computer Society Press. </publisher>
Reference-contexts: A parallel version of Regent trains many candidate networks at the same time using the Condor system <ref> (Litzkow, Livny, & Mutka, 1988) </ref>, which runs jobs on idle workstations.
Reference: <author> MacKay, D. </author> <year> (1992). </year> <title> A practical Bayesian framework for backpropagation networks. </title> <journal> Neural Computation, </journal> <volume> 4, </volume> <pages> 448-472. </pages>
Reference-contexts: Two techniques that try to take into account the effective size of the network are Generalized Prediction Error (Moody, 1991) and Bayesian methods <ref> (MacKay, 1992) </ref>. Quinlan and Cameron-Jones (1995) propose adding an additional term to the accuracy and smoothness term that takes into account length of time spent searching. They coin the term "oversearching" to describe the phenomenon where more extensive searching causes lower predictive accuracy.
Reference: <author> Maclin, R., & Shavlik, J. </author> <year> (1993). </year> <title> Using knowledge-based neural networks to improve algorithms: Refining the Chou-Fasman algorithm for protein folding. </title> <journal> Machine Learning, </journal> <volume> 11, </volume> <pages> 195-215. </pages>
Reference-contexts: Refer to Towell (1991) or Towell and Shavlik (1994) for more details. Kbann has been successfully applied to several real-world problems, such as the control of a chemical plant (Scott, Shavlik, & Ray, 1992), protein folding <ref> (Maclin & Shavlik, 1993) </ref>, a sample propositional rule set in Prolog (Clocksin & Mellish, 1987) notation, panel (b) illustrates this rule set's corresponding and/or dependency tree, and panel (c) shows the resulting network created by Kbann's translation. 181 Opitz & Shavlik finding genes in a sequence of DNA (Opitz & Shavlik,
Reference: <author> Mahoney, J. </author> <year> (1996). </year> <title> Combining Symbolic and Connectionist Learning Methods to Refine Certainty-Factor Rule-Bases. </title> <type> Ph.D. thesis, </type> <institution> University of Texas, Austin, TX. </institution>
Reference: <author> Mahoney, J., & Mooney, R. </author> <year> (1993). </year> <title> Combining connectionist and symbolic learning to refine certainty-factor rule-bases. </title> <journal> Connection Science, </journal> <volume> 5, </volume> <pages> 339-364. </pages>
Reference: <author> Mahoney, J., & Mooney, R. </author> <year> (1994). </year> <title> Comparing methods for refining certainty-factor rule-bases. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pp. 173-180, </pages> <address> New Brunswick, NJ. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In fact, TopGen compared favorably to a similar technique that also added nodes off to the side of Kbann (Opitz & Shavlik, 1993) and Regent outperformed TopGen in this article's experiments. Rapture <ref> (Mahoney & Mooney, 1994) </ref> is designed for domain theories containing probabilistic rules. Like most connectionist theory-refinement systems, Rapture first translates the domain theory into a neural network, then refines the weights of the network with a modified backpropagation algorithm.
Reference: <author> Masuoka, R., Watanabe, N., Kawamura, A., Owada, Y., & Asakawa, K. </author> <year> (1990). </year> <title> Neurofuzzy system | fuzzy inference using a structured neural network. </title> <booktitle> In Proceedings of the International Conference on Fuzzy Logic & Neural Networks, </booktitle> <pages> pp. 173-177, </pages> <address> Iizuka, Japan. </address>
Reference: <author> Michalski, R. </author> <year> (1983). </year> <title> A theory and methodology of inductive learning. </title> <journal> Artificial Intelligence, </journal> <volume> 20, </volume> <pages> 111-161. </pages> <note> Connectionist Theory Refinement Miller, </note> <author> G., Todd, P., & Hegde, S. </author> <year> (1989). </year> <title> Designing neural networks using genetic algorithms. </title> <booktitle> In Proceedings of the Third International Conference on Genetic Algorithms, </booktitle> <pages> pp. 379-384, </pages> <address> Arlington, VA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Mitchell, M. </author> <year> (1996). </year> <title> An Introduction to Genetic Algorithms. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Mitchell, T. </author> <year> (1982). </year> <title> Generalization as search. </title> <journal> Artificial Intelligence, </journal> <volume> 18, </volume> <pages> 203-226. </pages>
Reference: <author> Montana, D., & Davis, L. </author> <year> (1989). </year> <title> Training feedforward networks using genetic algorithms. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 762-767, </pages> <address> Detroit, MI. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Moody, J. </author> <year> (1991). </year> <title> The effective number of parameters: An analysis of generalization and regularization in nonlinear learning systems. </title> <editor> In Moody, J., Hanson, S., & Lippmann, R. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> Vol. 4, </volume> <pages> pp. 847-854, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Two techniques that try to take into account the effective size of the network are Generalized Prediction Error <ref> (Moody, 1991) </ref> and Bayesian methods (MacKay, 1992). Quinlan and Cameron-Jones (1995) propose adding an additional term to the accuracy and smoothness term that takes into account length of time spent searching. They coin the term "oversearching" to describe the phenomenon where more extensive searching causes lower predictive accuracy.
Reference: <author> Mozer, M. C., & Smolensky, P. </author> <year> (1989). </year> <title> Using relevance to reduce network size automatically. </title> <journal> Connection Science, </journal> <volume> 1, </volume> <pages> 3-16. </pages>
Reference: <author> Neri, F., & Saitta, L. </author> <year> (1996). </year> <title> Exploring the power of genetic search in learning symbolic classifiers. </title> <journal> In IEEE Transactions on Pattern Analisys and Machine Intelligence. </journal>
Reference: <author> Oliker, S., Furst, M., & Maimon, O. </author> <year> (1992). </year> <title> A distributed genetic algorithm for neural network design and training. </title> <journal> Complex Systems, </journal> <volume> 6, </volume> <pages> 459-477. </pages>
Reference: <author> Omlin, C., & Giles, C. </author> <year> (1992). </year> <title> Training second-order recurrent neural networks using hints. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> pp. 361-366, </pages> <address> Aberdeen, Scotland. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Opitz, D., & Shavlik, J. </author> <year> (1993). </year> <title> Heuristically expanding knowledge-based neural networks. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 1360-1365, </pages> <address> Chambery, France. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Thus, their system does not take advantage of Kbann's strength of removing unwanted antecedents and rules from the original rule base. In fact, TopGen compared favorably to a similar technique that also added nodes off to the side of Kbann <ref> (Opitz & Shavlik, 1993) </ref> and Regent outperformed TopGen in this article's experiments. Rapture (Mahoney & Mooney, 1994) is designed for domain theories containing probabilistic rules.
Reference: <author> Opitz, D., & Shavlik, J. </author> <year> (1995). </year> <title> Dynamically adding symbolically meaningful nodes to knowledge-based neural networks. </title> <journal> Knowledge-Based Systems, </journal> <volume> 8, </volume> <pages> 301-311. </pages>
Reference-contexts: However, Kbann, and other connectionist theory-refinement systems that do not alter their network topologies, suffer when given impoverished domain theories ones that are missing rules needed to adequately learn the true concept (Opitz & Shavlik, 1995; Towell & Shavlik, 1994). TopGen <ref> (Opitz & Shavlik, 1995) </ref> is an improvement over these systems; it heuristically searches through the space of possible network topologies by adding hidden nodes to the neural representation of the domain theory. TopGen showed statistically significant improvements over Kbann in several real-world domains (Opitz & Shavlik, 1995); however, in this paper <p> TopGen <ref> (Opitz & Shavlik, 1995) </ref> is an improvement over these systems; it heuristically searches through the space of possible network topologies by adding hidden nodes to the neural representation of the domain theory. TopGen showed statistically significant improvements over Kbann in several real-world domains (Opitz & Shavlik, 1995); however, in this paper we empirically show that TopGen nevertheless suffers because it only considers simple expansions of the Kbann network. To address this limitation, we broaden the types of topologies that TopGen considers by using genetic algorithms (GAs). We choose GAs for two reasons. <p> An ideal connectionist theory-refinement algorithm, therefore, should be able to dynamically expand the topology of its network during training. 3.2 The TopGen Algorithm TopGen <ref> (Opitz & Shavlik, 1995) </ref> addresses Kbann's limitation by heuristically searching through the space of possible expansions to a knowledge-based neural network a network whose topology is determined by the direct mapping of the dependencies of a domain theory (e.g., a Kbann network). <p> Refer to Opitz and Shavlik (1993; 1995) for more details. TopGen showed statistically significant improvements over Kbann in several real-world domains, and comparative experiments with a simpler approach to adding nodes verified that new nodes must be added in an intelligent manner <ref> (Opitz & Shavlik, 1995) </ref>. In this article, however, we increase the number of networks TopGen considers during its search and show that the increase in generalization is primarily limited to the first few networks considered. <p> Kbann (Towell & Shavlik, 1994) has been shown to be effective at translating a domain theory into a neural network; however, Kbann suffers in that it does not alter its topology. TopGen <ref> (Opitz & Shavlik, 1995) </ref> improved the Kbann algorithm by using available computer power to search for effective places to add nodes to the Kbann network; however, we show empirically that TopGen suffers from restricting its search to expansions of the Kbann network, and is unable to improve its performance after searching
Reference: <author> Opitz, D., & Shavlik, J. </author> <year> (1996). </year> <title> Actively searching for an effective neural-network ensemble. </title> <journal> Connection Science, </journal> <volume> 8, </volume> <pages> 337-353. </pages>
Reference: <author> Ourston, D., & Mooney, R. </author> <year> (1994). </year> <title> Theory refinement combining analytical and empirical methods. </title> <journal> Artificial Intelligence, </journal> <volume> 66, </volume> <pages> 273-309. </pages>
Reference-contexts: Several systems, including ours, have been proposed for refining propositional rule bases. Early such approaches could only handle improvements to overly specific theories (Danyluk, 1989) or specializations to overly general theories (Flann & Dietterich, 1989). Later systems such as Rtls (Ginsberg, 1990), Either <ref> (Ourston & Mooney, 1994) </ref>, Ptr (Koppel, Feldman, & Segre, 1994), and Tgci (Donoho & Rendell, 1995) were later able to handle both types of refinements.
Reference: <author> Pazzani, M., & Kibler, D. </author> <year> (1992). </year> <title> The utility of knowledge in inductive learning. </title> <journal> Machine Learning, </journal> <volume> 9, </volume> <pages> 57-94. </pages>
Reference-contexts: Systems such as Focl <ref> (Pazzani & Kibler, 1992) </ref> and Forte (Richards & Mooney, 1995) are first-order, theory-refinement systems that revise predicate-logic theories. One drawback to these systems is that they currently do not generalize as well as connectionist approaches on many real-world problems, such as the DNA promoter task (Cohen, 1992).
Reference: <author> Perrone, M. </author> <year> (1993). </year> <title> Improving Regression Estimation: Averaging Methods for Variance Reduction with Extension to General Convex Measure Optimization. </title> <type> Ph.D. thesis, </type> <institution> Brown University, Providence, RI. </institution> <note> 207 Opitz & Shavlik Provost, </note> <author> F., & Buchanan, B. </author> <year> (1995). </year> <title> Inductive policy: The pragmatics of bias selection. </title> <journal> Machine Learning, </journal> <volume> 20, </volume> <pages> 35-61. </pages>
Reference: <author> Quinlan, J. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 81-106. </pages>
Reference-contexts: While these criteria were created for planning and scheduling algorithms, they can apply to inductive learning algorithms as well. 1 Most standard inductive learners, such as backpropagation (Rumelhart et al., 1986) and ID3 <ref> (Quinlan, 1986) </ref>, are unable to continually improve their answers (at least until they receive additional training examples). In fact, if run too long, these algorithms tend to "overfit" the training set (Holder, 1991). <p> Thus, while Rapture hillclimbs until the training set is learned, Regent continually searches topology space looking for a network that minimizes the scoring function's error. Also, Rapture initially only creates links that are specified in the domain theory, and only explicitly adds links through ID3's <ref> (Quinlan, 1986) </ref> information-gain metric. Regent, on the other hand, fully connect consecutive layers in their networks, allowing each rule the possibility of adding antecedents during training. The Daid algorithm (Towell & Shavlik, 1992) is an extension to Kbann that uses the domain theory to help train the Kbann network. <p> Either uses these operators to make revisions to the domain theory that correctly classify some of the previously misclassified training examples without undefining any of the correctly classified examples. Either uses inductive learning algorithms to invent new rules; it currently uses ID3 <ref> (Quinlan, 1986) </ref> as its induction component. Even though Regent's mutation operator add nodes in a manner analogous to how a symbolic system adds antecedents and rules, its underlying learning algorithm is "connectionist." Towell (1991) showed that Kbann outperformed Either on the promoter task, and Regent outperformed Kbann in this article.
Reference: <author> Quinlan, J., & Cameron-Jones, R. </author> <year> (1995). </year> <title> Lookahead and pathology in decision tree induction. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 1019-1024, </pages> <address> Montreal, Canada. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Richards, B., & Mooney, R. </author> <year> (1995). </year> <title> Automated refinement of first-order Horn-clause domain theories. </title> <journal> Machine Learning, </journal> <volume> 19, </volume> <pages> 95-131. </pages>
Reference-contexts: Systems such as Focl (Pazzani & Kibler, 1992) and Forte <ref> (Richards & Mooney, 1995) </ref> are first-order, theory-refinement systems that revise predicate-logic theories. One drawback to these systems is that they currently do not generalize as well as connectionist approaches on many real-world problems, such as the DNA promoter task (Cohen, 1992).
Reference: <author> Roscheisen, M., Hofmann, R., & Tresp, V. </author> <year> (1991). </year> <title> Neural control for rolling mills: Incorporating domain theories to overcome data deficiency. </title> <editor> In Moody, J., Hanson, S., & Lippmann, R. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> Vol. 4, </volume> <pages> pp. 659-666, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Rumelhart, D., Hinton, G., & Williams, R. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart, D., & McClelland, J. (Eds.), </editor> <booktitle> Parallel Distributed Processing: Explorations in the microstructure of cognition. Volume 1: Foundations, </booktitle> <pages> pp. 318-363. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Kbann (Towell & Shavlik, 1994) is an example of such a connectionist system; it translates the provided domain theory into a neural network, thereby determining the network's topology, and then refines the reformulated rules using backpropagation <ref> (Rumelhart, Hinton, & Williams, 1986) </ref>. However, Kbann, and other connectionist theory-refinement systems that do not alter their network topologies, suffer when given impoverished domain theories ones that are missing rules needed to adequately learn the true concept (Opitz & Shavlik, 1995; Towell & Shavlik, 1994). <p> While these criteria were created for planning and scheduling algorithms, they can apply to inductive learning algorithms as well. 1 Most standard inductive learners, such as backpropagation <ref> (Rumelhart et al., 1986) </ref> and ID3 (Quinlan, 1986), are unable to continually improve their answers (at least until they receive additional training examples). In fact, if run too long, these algorithms tend to "overfit" the training set (Holder, 1991).
Reference: <author> Schaffer, C. </author> <year> (1993). </year> <title> Selecting a classification method by cross-validation. </title> <journal> Machine Learning, </journal> <volume> 13, </volume> <pages> 135-143. </pages>
Reference: <author> Schiffmann, W., Joost, M., & Werner, R. </author> <year> (1992). </year> <title> Synthesis and performance analysis of multilayer neural network architectures. </title> <type> Tech. rep. 16, </type> <institution> University of Koblenz, Institute for Physics. </institution>
Reference: <author> Scott, G., Shavlik, J., & Ray, W. </author> <year> (1992). </year> <title> Refining PID controllers using neural networks. </title> <journal> Neural Computation, </journal> <volume> 5, </volume> <pages> 746-757. </pages>
Reference-contexts: Following network initialization, Kbann uses the available training instances to refine the network links. Refer to Towell (1991) or Towell and Shavlik (1994) for more details. Kbann has been successfully applied to several real-world problems, such as the control of a chemical plant <ref> (Scott, Shavlik, & Ray, 1992) </ref>, protein folding (Maclin & Shavlik, 1993), a sample propositional rule set in Prolog (Clocksin & Mellish, 1987) notation, panel (b) illustrates this rule set's corresponding and/or dependency tree, and panel (c) shows the resulting network created by Kbann's translation. 181 Opitz & Shavlik finding genes in
Reference: <author> Tishby, N., Levin, E., & Solla, S. </author> <year> (1989). </year> <title> Consistent inference on probabilities in layered networks, predictions and generalization. </title> <booktitle> In International Joint Conference on Neural Networks, </booktitle> <pages> pp. 403-410, </pages> <address> Washington, D.C. </address> <publisher> IEEE Press. </publisher>
Reference: <author> Towell, G. </author> <year> (1991). </year> <title> Symbolic Knowledge and Neural Networks: Insertion, Refinement, and Extraction. </title> <type> Ph.D. thesis, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, WI. </institution>
Reference-contexts: Kbann's power on this domain is largely attributed to its ability to make "fine-grain" refinements to the domain theory <ref> (Towell, 1991) </ref>. Because of Either's difficulty on this domain, Baffes and Mooney (1993) presented an extension to it called Neither-MofN that is able to learn M -of-N rules - rules that are true if M of the N antecedents are true.
Reference: <author> Towell, G., & Shavlik, J. </author> <year> (1992). </year> <title> Using symbolic learning to improve knowledge-based neural networks. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pp. 177-182, </pages> <address> San Jose, CA. </address> <publisher> AAAI/MIT Press. </publisher>
Reference-contexts: Also, Rapture initially only creates links that are specified in the domain theory, and only explicitly adds links through ID3's (Quinlan, 1986) information-gain metric. Regent, on the other hand, fully connect consecutive layers in their networks, allowing each rule the possibility of adding antecedents during training. The Daid algorithm <ref> (Towell & Shavlik, 1992) </ref> is an extension to Kbann that uses the domain theory to help train the Kbann network. Since Kbann is more effective at dropping antecedents than adding them, Daid tries to find potentially useful inputs features not mentioned in the domain theory.
Reference: <author> Towell, G., & Shavlik, J. </author> <year> (1994). </year> <title> Knowledge-based artificial neural networks. </title> <journal> Artificial Intelligence, </journal> <volume> 70, </volume> <pages> 119-165. </pages>
Reference-contexts: We concentrate on connectionist theory-refinement systems, since they have been shown to frequently generalize better than many other inductive-learning and theory-refinement systems (Fu, 1989; Lacher, Hruska, & Kuncicky, 1992; Towell, 1991). Kbann <ref> (Towell & Shavlik, 1994) </ref> is an example of such a connectionist system; it translates the provided domain theory into a neural network, thereby determining the network's topology, and then refines the reformulated rules using backpropagation (Rumelhart, Hinton, & Williams, 1986). <p> We then continually refine this topology to find the best network for our concept. Before presenting our new algorithm (Regent), we give an overview of the Kbann algorithm as well as our initial approach of refining a Kbann-created network's topology (TopGen). 3.1 The KBANN Algorithm Kbann <ref> (Towell & Shavlik, 1994) </ref> works by translating a domain theory consisting of a set of propositional rules directly into a neural network (see Figure 2). Figure 2a shows a Prolog-like rule set that defines membership in category a. <p> We do this because the bias for an and node needs to be slightly less than the sum of the positive weights on the incoming links <ref> (see Towell and Shavlik, 1994 for more details) </ref>. <p> Conclusion An ideal inductive-learning algorithm should be able to exploit the available resources of extensive computing power and domain-specific knowledge to improve its ability to generalize. Kbann <ref> (Towell & Shavlik, 1994) </ref> has been shown to be effective at translating a domain theory into a neural network; however, Kbann suffers in that it does not alter its topology.
Reference: <author> Turney, P. </author> <year> (1995). </year> <title> Cost-sensitive classification: Empirical evaluation of a hybrid genetic decision tree induction algorithm. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2, </volume> <pages> 369-409. </pages> <booktitle> Connectionist Theory Refinement Waterman, </booktitle> <address> D. </address> <year> (1986). </year> <title> A Guide to Expert Systems. </title> <publisher> Addison Wesley, </publisher> <address> Reading, MA. </address>
Reference: <author> Watrous, R., Towell, G., & Glassman, M. </author> <year> (1993). </year> <title> Synthesize, optimize, analyze, repeat (SOAR): Application of neural network tools to ECG patient monitoring. </title> <booktitle> In Proceedings of the Symposium on Nonlinear Theory and its Applications, </booktitle> <pages> pp. 565-570, </pages> <address> Honolulu, Hawaii. </address>
Reference-contexts: Prolog (Clocksin & Mellish, 1987) notation, panel (b) illustrates this rule set's corresponding and/or dependency tree, and panel (c) shows the resulting network created by Kbann's translation. 181 Opitz & Shavlik finding genes in a sequence of DNA (Opitz & Shavlik, 1995; Towell & Shavlik, 1994), and ECG patient monitoring <ref> (Watrous, Towell, & Glassman, 1993) </ref>. In each case, Kbann was shown to produce improvements in generalization over standard neural networks for small numbers of training examples.
Reference: <author> Watson, J. D., Hopkins, N. H., Roberts, J. W., Argetsinger-Steitz, J., & Weiner, A. M. </author> <year> (1987). </year> <title> Molecular Biology of the Gene (Fourth edition). </title> <address> Benjamin/Cummings, Menlo Park, CA. </address>
Reference-contexts: One could prune weights and nodes during Regent's search; however, such pruning can prematurely reduce the variety of structures available for recombination during crossover (Koza, 1992). Real-life organisms, for instance, have superfluous DNA that are believed to enhance the rate of evolution <ref> (Watson, Hopkins, Roberts, Argetsinger-Steitz, & Weiner, 1987) </ref>. However, while pruning network size during genetic search may be unwise, one could prune Regent's final network using, say, Hassibi and Stork's (1992) Optimal Brain Surgeon algorithm.
Reference: <author> Weigend, A. </author> <year> (1993). </year> <title> On overfitting and the effective number of hidden units. </title> <booktitle> In Proceedings of the 1993 Connectionist Models Summer School, </booktitle> <pages> pp. 335-342, </pages> <address> Boulder, </address> <publisher> CO. Lawrence Erlbaum Associates. </publisher>
Reference-contexts: The complexity of the network cannot simply be estimated by counting the number of possible parameters, since there tends to be significant duplication in the function of each weight in a network, especially early in the training process <ref> (Weigend, 1993) </ref>. Two techniques that try to take into account the effective size of the network are Generalized Prediction Error (Moody, 1991) and Bayesian methods (MacKay, 1992).
Reference: <author> Weigend, A., Huberman, B., & Rumelhart, D. </author> <year> (1990). </year> <title> Predicting the future: A connectionist approach. </title> <journal> International Journal of Neural Systems, </journal> <volume> I, </volume> <pages> 193-209. </pages>
Reference: <author> Whitley, D., Gordon, S., & Mathias, K. </author> <year> (1994). </year> <title> Lamarckian evolution, the Baldwin effect and function optimization. </title> <editor> In Davidor, Y., Schwefel, H., & Manner, R. (Eds.), </editor> <booktitle> Parallel Problem Solving from Nature - PPSN III, </booktitle> <pages> pp. 6-15. </pages> <publisher> Springer-Verlag. </publisher>
Reference-contexts: In fact this form of evolution can sometimes outperform forms of Lamarckian evolution that employ the same local search strategy <ref> (Whitley, Gordon, & Mathias, 1994) </ref>. Future work is to investigate the utility of the Baldwin effect on Regent. In this case we would not cross over the trained networks, but instead cross over the initial weight settings before backpropagation learning took place.
Reference: <author> Whitley, D., & Hanson, T. </author> <year> (1989). </year> <title> Optimizing neural networks using faster, more accurate genetic search. </title> <booktitle> In Proceedings of the Third International Conference on Genetic Algorithms, </booktitle> <pages> pp. 391-396, </pages> <address> Arlington, VA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Yao, X. </author> <year> (1993). </year> <title> Evolutionary artificial neural networks. </title> <journal> International Journal of Neural Systems, </journal> <volume> 4, </volume> <pages> 203-221. 209 </pages>
Reference-contexts: Techniques that solely use genetic algorithms to optimize weights (Montana & Davis, 1989; Whitley & Hanson, 1989) have performed competitively with gradient-based training algorithms; however, one problem with genetic algorithms is their inefficiency in fine-tuned local search, thus the scalability of these methods are in question <ref> (Yao, 1993) </ref>. Kitano (1990b) presents a method that combines genetic algorithms with backpropagation. He does this by using the genetic algorithm to determine the starting weights for a network, which are then refined by backpropagation. <p> These methods are relatively straightforward to implement, and are good at fine tuning small networks (Miller et al., 1989); however, they do not scale well since they require very large matrices to represent all the links in large networks <ref> (Yao, 1993) </ref>. Other techniques (Dodd, 1990; Harp, Samad, & Guha, 1989; Kitano, 1990a) only encode the most important features of the network, such as the number of hidden layers, the number of hidden nodes at each layer, etc. <p> These indirect encoding schemes can evolve different sets of parameters along with the network's topology and have been shown to have good scalability <ref> (Yao, 1993) </ref>. Some techniques (Koza & Rice, 1991; Oliker et al., 1992) evolve both the architecture and connection weights at the same time; however, the combination of the two levels of evolution greatly increases the search space.
References-found: 94

