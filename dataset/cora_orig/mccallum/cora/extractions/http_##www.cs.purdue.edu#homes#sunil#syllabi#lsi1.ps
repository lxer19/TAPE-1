URL: http://www.cs.purdue.edu/homes/sunil/syllabi/lsi1.ps
Refering-URL: http://www.cs.purdue.edu/homes/sunil/syllabi/CS690D_Fall98.html
Root-URL: http://www.cs.purdue.edu
Title: Indexing by Latent Semantic Analysis  
Author: Scott Deerwester Susan T. Dumais George W. Furnas Thomas K. Landauer Richard Harshman 
Address: Chicago, IL 60637  435 South St. Morristown, NJ 07960  Ontario London, Ontario Canada  
Affiliation: Graduate Library School University of Chicago  Bell Communications Research  University of Western  
Abstract: A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents ("semantic structure") in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. Initial tests find this completely automatic method for retrieval to be promising. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Furnas, G.W., Landauer, T.K., Gomez, L.M., and Dumais, S.T. </author> <title> Statistical semantics: Analysis of the potential performance of key-word information systems. </title> <journal> Bell System Technical Journal, 1983, </journal> <volume> 62(6), </volume> <pages> 1753-1806. </pages>
Reference-contexts: Indeed, we have found that the degree of variability in descriptive term usage is much greater than is commonly suspected. For example, two people choose the same main key word for a single well-known object less than 20% of the time <ref> [1] </ref> . Comparably poor agreement has been reported in studies of inter-indexer consistency [2] and in the generation of search terms by either expert intermediaries [3] or less experienced searchers [4] [5] . The prevalence of synonyms tends to decrease the "recall" performance of retrieval systems.
Reference: 2. <author> Tarr, D. and Borko, H. </author> <title> Factors influencing inter-indexer consistency. </title> <booktitle> In Proceedings of the ASIS 37th Annual Meeting, </booktitle> <volume> Vol. 11, </volume> <year> 1974, </year> <pages> 50-55. </pages>
Reference-contexts: For example, two people choose the same main key word for a single well-known object less than 20% of the time [1] . Comparably poor agreement has been reported in studies of inter-indexer consistency <ref> [2] </ref> and in the generation of search terms by either expert intermediaries [3] or less experienced searchers [4] [5] . The prevalence of synonyms tends to decrease the "recall" performance of retrieval systems.
Reference: 3. <editor> Fidel, R. </editor> <title> Individual variability in online searching behavior. </title> <booktitle> In C.A. Parkhurst (Ed.). ASIS'85: Proceedings of the ASIS 48th Annual Meeting, </booktitle> <volume> Vol. 22, </volume> <month> October 20-24, </month> <year> 1985, </year> <pages> 69-72. </pages>
Reference-contexts: For example, two people choose the same main key word for a single well-known object less than 20% of the time [1] . Comparably poor agreement has been reported in studies of inter-indexer consistency [2] and in the generation of search terms by either expert intermediaries <ref> [3] </ref> or less experienced searchers [4] [5] . The prevalence of synonyms tends to decrease the "recall" performance of retrieval systems. By polysemy we refer to the general fact that most words have more than one distinct meaning hhhhhhhhhhhhhhh 1.
Reference: 4. <author> Liley, O. </author> <title> Evaluation of the subject catalog. </title> <journal> American Documentation, 1954, </journal> <volume> 5(2), </volume> <pages> 41-60. </pages>
Reference-contexts: Comparably poor agreement has been reported in studies of inter-indexer consistency [2] and in the generation of search terms by either expert intermediaries [3] or less experienced searchers <ref> [4] </ref> [5] . The prevalence of synonyms tends to decrease the "recall" performance of retrieval systems. By polysemy we refer to the general fact that most words have more than one distinct meaning hhhhhhhhhhhhhhh 1.
Reference: 5. <author> Bates, M.J. </author> <title> Subject access in online catalogs: A design model. </title> <journal> JASIS, 1986, </journal> <volume> 37 (6), </volume> <pages> 357-376. </pages>
Reference-contexts: Comparably poor agreement has been reported in studies of inter-indexer consistency [2] and in the generation of search terms by either expert intermediaries [3] or less experienced searchers [4] <ref> [5] </ref> . The prevalence of synonyms tends to decrease the "recall" performance of retrieval systems. By polysemy we refer to the general fact that most words have more than one distinct meaning hhhhhhhhhhhhhhh 1.
Reference: 6. <author> Sparck Jones, K. </author> <title> A statistical interpretation of term specificity and its applications in retrieval. </title> <journal> Journal of Documentation, 1972, </journal> <volume> 28(1), </volume> <pages> 11-21. </pages>
Reference-contexts: These are presumably advantageous for conscientious and knowledgeable searchers who can use such tools to suggest additional search terms. The drawback for fully automatic methods is that some added terms may have different meaning from that intended (the polysemy effect) leading to rapid degradation of precision <ref> [6] </ref> . It is worth noting in passing that experiments with small interactive data bases have shown monotonic improvements in recall rate without overall loss of precision as more indexing terms, either taken from the documents or from large samples of actual users' words are added [7] [8] .
Reference: 7. <author> Gomez, L. M. and Lochbaum, C. C. </author> <title> People can retrieve more objects with enriched keyword vocabularies. But is there a human performance cost? In Proceedings of Interact 84, </title> <address> London, England, </address> <month> September </month> <year> 1984. </year>
Reference-contexts: It is worth noting in passing that experiments with small interactive data bases have shown monotonic improvements in recall rate without overall loss of precision as more indexing terms, either taken from the documents or from large samples of actual users' words are added <ref> [7] </ref> [8] . Whether this "unlimited aliasing" method, which we have described elsewhere, will be effective in very large data bases remains to be determined.
Reference: 8. <author> Furnas, G.W. </author> <title> Experience with an adaptive indexing scheme. </title> <booktitle> In Human Factors in Computer Systems, CHI'85 Proceedings. </booktitle> <address> San Francisco, Ca., </address> <month> April 15-18, </month> <year> 1985. </year> <note> 9. </note> <editor> van Rijsbergen, C.J. </editor> <title> A theoretical basis for the use of co-occurrence data in information retrieval. </title> <journal> Journal of Documentation, 1977, </journal> <volume> 33(2), </volume> <pages> 106-119. </pages>
Reference-contexts: It is worth noting in passing that experiments with small interactive data bases have shown monotonic improvements in recall rate without overall loss of precision as more indexing terms, either taken from the documents or from large samples of actual users' words are added [7] <ref> [8] </ref> . Whether this "unlimited aliasing" method, which we have described elsewhere, will be effective in very large data bases remains to be determined.
Reference: 10. <author> Carroll, J.D. and Arabie, P. </author> <title> Multidimensional scaling. In M.R. </title> <editor> Rosenzweig and L.W. Porter (Eds.). </editor> <booktitle> Annual Review of Psychology, 1980, </booktitle> <volume> 31, </volume> <pages> 607-649. </pages>
Reference-contexts: This led us to restrict consideration to proximity models, i.e., models that try to put similar items near each other in some space or structure. Such models include: hierarchical, partition and overlapping clusterings; ultrametric and additive trees; and factor-analytic and multidimensional distance models (see Carroll & Arabie <ref> [10] </ref> for a survey). Aiding information retrieval by discovering latent proximity structure has at least two lines of precedence in the literature. Hierarchical classification analyses are frequently used for term and document clustering [11] [12] [13] . <p> Then a query can be placed at the centroid of its term points. Thus for both elegance and retrieval mechanisms, we needed what are called two-mode proximity methods (Carroll and Arabie <ref> [10] </ref> ), that start with a rectangular matrix and construct explicit representations of both row and column objects. One such method is multidimensional unfolding [22] [23] [24] , in which both terms and documents would appear as points in a single space with similarity related monotonically to Euclidean distance.
Reference: 11. <author> Sparck Jones, K. </author> <title> Automatic Keyword Classification for Information Retrieval, </title> <address> Buttersworth, London, </address> <year> 1971. </year>
Reference-contexts: Aiding information retrieval by discovering latent proximity structure has at least two lines of precedence in the literature. Hierarchical classification analyses are frequently used for term and document clustering <ref> [11] </ref> [12] [13] . Latent class analysis [14] and factor analysis [15] [16] [17] have also been explored before for automatic document indexing and retrieval.
Reference: 12. <author> Salton, G. </author> <title> Automatic Information Organization and Retrieval. </title> <publisher> McGraw Hill, </publisher> <year> 1968. </year>
Reference-contexts: Aiding information retrieval by discovering latent proximity structure has at least two lines of precedence in the literature. Hierarchical classification analyses are frequently used for term and document clustering [11] <ref> [12] </ref> [13] . Latent class analysis [14] and factor analysis [15] [16] [17] have also been explored before for automatic document indexing and retrieval.
Reference: 13. <author> Jardin, N. and van Rijsbergen, C.J. </author> <title> The use of hierarchic clustering in information retrieval. Information Storage and Retrieval, </title> <booktitle> 1971, </booktitle> <volume> 7, </volume> <pages> 217-240. </pages>
Reference-contexts: Aiding information retrieval by discovering latent proximity structure has at least two lines of precedence in the literature. Hierarchical classification analyses are frequently used for term and document clustering [11] [12] <ref> [13] </ref> . Latent class analysis [14] and factor analysis [15] [16] [17] have also been explored before for automatic document indexing and retrieval. In document clustering, for example, a notion of distance is defined such that two documents are considered close to the extent that they contain the same terms. <p> Hierarchical clusterings permit no cross classifications, for example, and in general have very few free parameters (essentially only n parameters for n objects). Empirically, clustering improves the computational efficiency of search; whether or not it improves retrieval success is unclear <ref> [13] </ref> [18] [19] .
Reference: 14. <author> Baker, </author> <title> F.B. Information retrieval based on latent class analysis. </title> <journal> Journal of the ACM, 1962, </journal> <volume> 9, </volume> <pages> 512-521. </pages>
Reference-contexts: Aiding information retrieval by discovering latent proximity structure has at least two lines of precedence in the literature. Hierarchical classification analyses are frequently used for term and document clustering [11] [12] [13] . Latent class analysis <ref> [14] </ref> and factor analysis [15] [16] [17] have also been explored before for automatic document indexing and retrieval. In document clustering, for example, a notion of distance is defined such that two documents are considered close to the extent that they contain the same terms.
Reference: 15. <author> Atherton, P. and Borko, H. </author> <title> A test of factor-analytically derived automated classification methods. </title> <address> AIP rept AIP-DRP 65-1, </address> <month> Jan. </month> <year> 1965. </year>
Reference-contexts: Aiding information retrieval by discovering latent proximity structure has at least two lines of precedence in the literature. Hierarchical classification analyses are frequently used for term and document clustering [11] [12] [13] . Latent class analysis [14] and factor analysis <ref> [15] </ref> [16] [17] have also been explored before for automatic document indexing and retrieval. In document clustering, for example, a notion of distance is defined such that two documents are considered close to the extent that they contain the same terms. <p> The explicit representation of both terms and documents in the same space makes retrieving documents relevant to user queries a straightforward matter. Previous work by Borko and his colleagues <ref> [15] </ref> [16] is similar in name to our approach, but used the factor space only for document clustering, not document retrieval, and computational simplifications reduced its representational Deerwester - 21 - power.
Reference: 16. <author> Borko, H and Bernick, </author> <title> M.D. Automatic document classification. </title> <journal> Journal of the ACM, </journal> <month> April </month> <year> 1963, </year> <pages> 10(3), 151-162. </pages>
Reference-contexts: Aiding information retrieval by discovering latent proximity structure has at least two lines of precedence in the literature. Hierarchical classification analyses are frequently used for term and document clustering [11] [12] [13] . Latent class analysis [14] and factor analysis [15] <ref> [16] </ref> [17] have also been explored before for automatic document indexing and retrieval. In document clustering, for example, a notion of distance is defined such that two documents are considered close to the extent that they contain the same terms. <p> However previous attempts along these lines, too, had shortcomings. First, factor analysis is computationally expensive, and since most previous attempts were made 15-20 years ago, they were limited by processing constraints <ref> [16] </ref> . Second, most past attempts considered restricted versions of the factor analytic model, either by using very low dimensionality, or by converting the factor analysis results to a simple binary clustering [16] . <p> computationally expensive, and since most previous attempts were made 15-20 years ago, they were limited by processing constraints <ref> [16] </ref> . Second, most past attempts considered restricted versions of the factor analytic model, either by using very low dimensionality, or by converting the factor analysis results to a simple binary clustering [16] . Third, some attempts have relied on excessively tedious data gathering techniques, requiring the collection of thousands of similarity judgments from humans [17] . Previously reported clustering and factor analytic approaches have also struggled with a certain representational awkwardness. <p> We will use the terminology of factor analysis, since that approach has some precedence in the information retrieval literature. The traditional, one-mode factor analysis begins with a matrix of associations between all pairs of one type of object, e.g., documents <ref> [16] </ref> . This might be a matrix of human judgments of document to document similarity, or a measure of term overlap computed for each pair of documents from an original term by document matrix. <p> rather than important underlying relationships in the pattern of term usage over documents.) It is also important to note that previous attempts to use factor analytic techniques for information retrieval have used small numbers of factors (Koll [20] , 7 dimensions; and Ossario [17] , 13 dimensions; Borko & Bernick <ref> [16] </ref> , 21 dimensions). We show a more than 50% improvement in performance beyond this range, and therefore suspect that some of the limited utility of previous factor analytic approaches may be the result of an impoverished representation. <p> The explicit representation of both terms and documents in the same space makes retrieving documents relevant to user queries a straightforward matter. Previous work by Borko and his colleagues [15] <ref> [16] </ref> is similar in name to our approach, but used the factor space only for document clustering, not document retrieval, and computational simplifications reduced its representational Deerwester - 21 - power. In Borko and Bernick [16] , for example, factor analysis was performed on a term-term correlation matrix (calculated from word <p> Previous work by Borko and his colleagues [15] <ref> [16] </ref> is similar in name to our approach, but used the factor space only for document clustering, not document retrieval, and computational simplifications reduced its representational Deerwester - 21 - power. In Borko and Bernick [16] , for example, factor analysis was performed on a term-term correlation matrix (calculated from word usage over 260 abstracts), and 21 orthogonal factors were selected on the basis of their interpretability.
Reference: 17. <author> Ossorio, P.G. </author> <title> Classification space: A multivariate procedure for automatic document indexing and retrieval. </title> <booktitle> Multivariate Behavioral Research, </booktitle> <month> October </month> <year> 1966, </year> <pages> 479-524. </pages> <address> Deerwester - 29 </address> - 
Reference-contexts: Aiding information retrieval by discovering latent proximity structure has at least two lines of precedence in the literature. Hierarchical classification analyses are frequently used for term and document clustering [11] [12] [13] . Latent class analysis [14] and factor analysis [15] [16] <ref> [17] </ref> have also been explored before for automatic document indexing and retrieval. In document clustering, for example, a notion of distance is defined such that two documents are considered close to the extent that they contain the same terms. <p> Third, some attempts have relied on excessively tedious data gathering techniques, requiring the collection of thousands of similarity judgments from humans <ref> [17] </ref> . Previously reported clustering and factor analytic approaches have also struggled with a certain representational awkwardness. <p> sampling noise or peculiarities of the sample rather than important underlying relationships in the pattern of term usage over documents.) It is also important to note that previous attempts to use factor analytic techniques for information retrieval have used small numbers of factors (Koll [20] , 7 dimensions; and Ossario <ref> [17] </ref> , 13 dimensions; Borko & Bernick [16] , 21 dimensions). We show a more than 50% improvement in performance beyond this range, and therefore suspect that some of the limited utility of previous factor analytic approaches may be the result of an impoverished representation.
Reference: 18. <author> Salton, G. and McGill, M.J. </author> <title> Introduction to Modern Information Retrieval. </title> <publisher> McGraw-Hill, </publisher> <year> 1983. </year>
Reference-contexts: Hierarchical clusterings permit no cross classifications, for example, and in general have very few free parameters (essentially only n parameters for n objects). Empirically, clustering improves the computational efficiency of search; whether or not it improves retrieval success is unclear [13] <ref> [18] </ref> [19] .
Reference: 19. <author> Voorhees, E. </author> <title> The cluster hypothesis revisited. </title> <booktitle> SIGIR, </booktitle> <year> 1985, </year> <pages> 188-196. </pages>
Reference-contexts: Hierarchical clusterings permit no cross classifications, for example, and in general have very few free parameters (essentially only n parameters for n objects). Empirically, clustering improves the computational efficiency of search; whether or not it improves retrieval success is unclear [13] [18] <ref> [19] </ref> . Previously tried factor analytic approaches have taken a square symmetric matrix of similarities between pairs of documents (based on statistical term overlap or human judgments), and used linear algebra to construct a low dimensional spatial model wherein similar documents are placed near one Deerwester - 5 - another. <p> We compare the results of our latent structure indexing (LSI) method against a straightforward term matching method, a version of SMART, and against data reported by Voorhees <ref> [19] </ref> for the same standard datasets. The term overlap comparisons provide a baseline against which to assess the benefits of indexing by means of latent semantic structure rather than raw term matching. <p> This particular invocation of SMART is the same as our term matching method except for the initial choice of index terms. The Voorhees data were obtained directly from her paper in which she used a vector retrieval system with extended Boolean queries (see Voorhees <ref> [19] </ref> for details). Her documents were indexed by removing words on a stop list, mapping word variants into the same term, and weighting terms. Weighted extended Boolean queries were used for retrieval. Performance is evaluated by measuring precision at several different levels of recall.
Reference: 20. <author> Koll, M. </author> <title> An approach to concept-based information retrieval. </title> <booktitle> ACM SIGIR Forum, </booktitle> <address> XIII32-50, </address> <year> 1979. </year>
Reference-contexts: However, representations chosen so far handle only one at a time (e.g., either term clustering or document clustering). Any attempts to put the ignored entity back in the representation have been arbitrary and after the fact. An exception to this is a proposal by Koll <ref> [20] </ref> in which both terms and documents are represented in the same space of concepts (see also Raghavan & Wong [21] ). <p> the extra parameters are modeling the sampling noise or peculiarities of the sample rather than important underlying relationships in the pattern of term usage over documents.) It is also important to note that previous attempts to use factor analytic techniques for information retrieval have used small numbers of factors (Koll <ref> [20] </ref> , 7 dimensions; and Ossario [17] , 13 dimensions; Borko & Bernick [16] , 21 dimensions). <p> In addition, Borko's work addressed the problem of document classification, and not document retrieval. There is, for example, no discussion of how one might use the full factor space (and not just the document clusters derived from it) for document retrieval. Koll's <ref> [20] </ref> work on concept-based information retrieval is very similar in spirit to our latent semantic indexing. Both terms and documents are represented in a single concept space on the basis of statistical term co-occurrences.
Reference: 21. <author> Raghavan, V. and Wong, S. </author> <title> A critical analysis of vector space model for information retrieval. </title> <journal> JASIS, 1986, </journal> <volume> 37(5), </volume> <pages> 279-288. </pages>
Reference-contexts: Any attempts to put the ignored entity back in the representation have been arbitrary and after the fact. An exception to this is a proposal by Koll [20] in which both terms and documents are represented in the same space of concepts (see also Raghavan & Wong <ref> [21] </ref> ).
Reference: 22. <author> Coombs, C.H. </author> <title> A Theory of Data. </title> <address> New York: </address> <publisher> Wiley, </publisher> <year> 1964. </year>
Reference-contexts: Thus for both elegance and retrieval mechanisms, we needed what are called two-mode proximity methods (Carroll and Arabie [10] ), that start with a rectangular matrix and construct explicit representations of both row and column objects. One such method is multidimensional unfolding <ref> [22] </ref> [23] [24] , in which both terms and documents would appear as points in a single space with similarity related monotonically to Euclidean distance.
Reference: 23. <author> Heiser, W.J. </author> <title> Unfolding Analysis of Proximity Data. </title> <address> Leiden, The Netherlands: Reprodienst Psychologie RUL, </address> <year> 1981. </year>
Reference-contexts: Thus for both elegance and retrieval mechanisms, we needed what are called two-mode proximity methods (Carroll and Arabie [10] ), that start with a rectangular matrix and construct explicit representations of both row and column objects. One such method is multidimensional unfolding [22] <ref> [23] </ref> [24] , in which both terms and documents would appear as points in a single space with similarity related monotonically to Euclidean distance.
Reference: 24. <author> Desarbo, W.S., and Carroll, J.D. </author> <title> Three-way metric unfolding via alternating weighted least squares. </title> <journal> Psychometrika, 1985, </journal> <volume> 50(3), </volume> <pages> 275-300. </pages>
Reference-contexts: Thus for both elegance and retrieval mechanisms, we needed what are called two-mode proximity methods (Carroll and Arabie [10] ), that start with a rectangular matrix and construct explicit representations of both row and column objects. One such method is multidimensional unfolding [22] [23] <ref> [24] </ref> , in which both terms and documents would appear as points in a single space with similarity related monotonically to Euclidean distance.
Reference: 25. <author> Harshman, R.A. </author> <title> Foundations of the PARAFAC procedure: Models and conditions for an "explanatory" multi-modal factor analysis. UCLA Work Papers Phonetics, </title> <booktitle> 1970, </booktitle> <volume> 16, </volume> <month> 86pp. </month>
Reference-contexts: One such method is multidimensional unfolding [22] [23] [24] , in which both terms and documents would appear as points in a single space with similarity related monotonically to Euclidean distance. Another is two-mode factor analysis <ref> [25] </ref> [26] [27] [28] , in which terms and documents would again be represented as points in a space, but similarity is given by the inner product between points.
Reference: 26. <author> Harshman, R.A. and Lundy, </author> <title> M.E. The PARAFAC model for three-way factor analysis and multi-dimensional scaling. </title> <editor> In H.G. Law, C.W. Snyder, Jr., J.A. Hattie, and R.P. McDonald (Eds.). </editor> <title> Research Methods for Multimode Data Analysis, </title> <type> Praeger, </type> <year> 1984a. </year>
Reference-contexts: One such method is multidimensional unfolding [22] [23] [24] , in which both terms and documents would appear as points in a single space with similarity related monotonically to Euclidean distance. Another is two-mode factor analysis [25] <ref> [26] </ref> [27] [28] , in which terms and documents would again be represented as points in a space, but similarity is given by the inner product between points.
Reference: 27. <author> Carroll, J.D. and Chang, J.J. </author> <title> Analysis of individual differences in multidimensional scaling via an N-way generalization of "Eckart-Young" decomposition. </title> <journal> Psychometrika, 1970, </journal> <volume> 35, </volume> <pages> 283-319. </pages>
Reference-contexts: One such method is multidimensional unfolding [22] [23] [24] , in which both terms and documents would appear as points in a single space with similarity related monotonically to Euclidean distance. Another is two-mode factor analysis [25] [26] <ref> [27] </ref> [28] , in which terms and documents would again be represented as points in a space, but similarity is given by the inner product between points.
Reference: 28. <author> Kruskal, J.B. </author> <title> Factor analysis and principal components: Bilinear methods. </title> <editor> In H. Kruskal and J.M. Tanur (Eds.). </editor> <booktitle> International Encyclopedia of Statistics, </booktitle> <address> New York: </address> <publisher> Free Press, </publisher> <year> 1978. </year>
Reference-contexts: One such method is multidimensional unfolding [22] [23] [24] , in which both terms and documents would appear as points in a single space with similarity related monotonically to Euclidean distance. Another is two-mode factor analysis [25] [26] [27] <ref> [28] </ref> , in which terms and documents would again be represented as points in a space, but similarity is given by the inner product between points.
Reference: 29. <author> Furnas, G.W. </author> <title> Objects and their features: The metric representation of two-class data. </title> <type> Ph.D. Dissertation. </type> <institution> Stanford University, </institution> <year> 1980. </year>
Reference-contexts: Another is two-mode factor analysis [25] [26] [27] [28] , in which terms and documents would again be represented as points in a space, but similarity is given by the inner product between points. A final candidate is unfolding in trees <ref> [29] </ref> , in which both terms and documents would appear as leaves on a tree, and path length distance through the tree would give the similarity (one version of this is equivalent to simultaneous hierarchical clustering of both terms and objects).
Reference: 30. <author> Forsythe, G.E., Malcolm, M.A., and Moler, C.B. </author> <title> Computer Methods for Mathematical Computations (Chapter 9: Least squares and the singular value decomposition). </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice Hall, </publisher> <year> 1977. </year>
Reference-contexts: The tree unfolding model was considered too representationally restrictive, and along with non-metric multidimensional unfolding, too computationally expensive. Two-mode factor analysis is a generalization of the familiar factor analytic model based on singular value decomposition (SVD). (See Forsythe, Malcolm & Moler <ref> [30] </ref> , Chapter 9, for an introduction to SVD and its applications.) SVD represents both terms and documents as vectors in a space of choosable dimensionality, and the dot product or cosine between points in the space gives their similarity.
Reference: 31. <author> Harshman, R.A. and Lundy, </author> <title> M.E. Data preprocessing and the extended PARAFAC model. </title> <editor> In H.G. Law, C.W. Snyder, Jr., J.A. Hattie, and R.P. McDonald (Eds.). </editor> <title> Research Methods for Multimode Data Analysis, </title> <type> Praeger, </type> <year> 1984b. </year>
Reference-contexts: In addition, a program was available <ref> [31] </ref> that fit the model in time of order N 2 x k 3 . hhhhhhhhhhhhhhh 3. There are several important and interesting issues raised by considering the addition of new terms and documents into the space. First, the addition of new objects introduces some temporal dependencies in the representation. <p> Tests of the SVD Latent Semantic Indexing (LSI) method We have so far tried the LSI method on two standard document collections where queries and relevance judgments were available (MED and CISI). PARAFAC (Harshman & Lundy <ref> [31] </ref> ), a program for the iterative numerical solution of multi-mode factor-analysis problems, was used for the studies reported below. (Other programs for more standard SVD are also available - e.g., [33] [34] .) "Documents" consist of the full text of the title and abstract.
Reference: 32. <author> Jones, W.P. and Furnas, G.W. </author> <title> Pictures of relevance. </title> <journal> JASIS, 1987, </journal> <volume> 38(6), </volume> <pages> 420-442. </pages>
Reference-contexts: Jones & Furnas <ref> [32] </ref> .) A concrete example may make the procedure and its putative advantages clearer. Table 2 gives a sample dataset. In this case, the document set consisted of the titles of 9 Bellcore technical memoranda. Words occurring in more than one title were selected for indexing; they are italicized.
Reference: 33. <author> Golub, G.H., Luk, F.T., and Overton, </author> <title> M.L. A block Lanczos method for computing the singular values and corresponding singular vectors of a matrix. </title> <journal> ACM Transactions on Mathematical Software, 1981, </journal> <volume> 7(2), </volume> <pages> 149-169. </pages>
Reference-contexts: PARAFAC (Harshman & Lundy [31] ), a program for the iterative numerical solution of multi-mode factor-analysis problems, was used for the studies reported below. (Other programs for more standard SVD are also available - e.g., <ref> [33] </ref> [34] .) "Documents" consist of the full text of the title and abstract.
Reference: 34. <editor> Cullum, J., Willoughby, R.A., and Lake, M. </editor> <title> A Lanczos algorithm for computing singular values and vectors of large matrices. </title> <journal> SIAM J. Sci. Stat. Comput., 1983, </journal> <volume> 4(2), </volume> <pages> 197-215. </pages> <address> Deerwester - 30 </address> - 
Reference-contexts: PARAFAC (Harshman & Lundy [31] ), a program for the iterative numerical solution of multi-mode factor-analysis problems, was used for the studies reported below. (Other programs for more standard SVD are also available - e.g., [33] <ref> [34] </ref> .) "Documents" consist of the full text of the title and abstract.
Reference: 35. <author> Lesk, M.E. and Salton, G. </author> <title> Relevance assessments and retrieval system evaluation. Information Storage and Retrieval, </title> <booktitle> 1969, </booktitle> <volume> 4(4), </volume> <pages> 343-359. </pages>
Reference: 36. <author> Amsler, R. </author> <title> Machine-readable dictionaries. </title> <booktitle> In Annual Review of Information Science and Technology (ARIST), </booktitle> <volume> Vol. 19, </volume> <year> 1984, </year> <pages> 161-209. </pages>
Reference-contexts: We have not yet found a satisfactory way to do that (but see Amsler <ref> [36] </ref> ; Choueka & Lusignan [37] ; Lesk [38] ).
Reference: 37. <author> Choueka, Y. and Lusignan, S. </author> <title> Disambiguation by short contexts. Computers and the Humanities, </title> <booktitle> 1985, </booktitle> <volume> 19, </volume> <pages> 147-157. </pages>
Reference-contexts: We have not yet found a satisfactory way to do that (but see Amsler [36] ; Choueka & Lusignan <ref> [37] </ref> ; Lesk [38] ). The latent semantic indexing methods that we have discussed, and in particular the singular-value decomposition technique that we have tested, are capable of improving the way in which we deal with the problem of multiple terms referring to the same object.
Reference: 38. <author> Lesk, </author> <title> M.E. How to tell a pine cone from an ice cream cone. </title> <booktitle> In Proceedings of ACM SIGDOC Conference, </booktitle> <address> Toronto, Ont., </address> <month> June, </month> <year> 1986. </year> <pages> CONTENTS </pages>
Reference-contexts: We have not yet found a satisfactory way to do that (but see Amsler [36] ; Choueka & Lusignan [37] ; Lesk <ref> [38] </ref> ). The latent semantic indexing methods that we have discussed, and in particular the singular-value decomposition technique that we have tested, are capable of improving the way in which we deal with the problem of multiple terms referring to the same object.
Reference: 1. <institution> Introduction . . . . . . . . . . . . . . . . . . . . . . . 1 </institution>
Reference-contexts: Indeed, we have found that the degree of variability in descriptive term usage is much greater than is commonly suspected. For example, two people choose the same main key word for a single well-known object less than 20% of the time <ref> [1] </ref> . Comparably poor agreement has been reported in studies of inter-indexer consistency [2] and in the generation of search terms by either expert intermediaries [3] or less experienced searchers [4] [5] . The prevalence of synonyms tends to decrease the "recall" performance of retrieval systems.
Reference: 2. <institution> Deficiencies of current automatic indexing and retrieval methods . . . . . . . 1 </institution>
Reference-contexts: For example, two people choose the same main key word for a single well-known object less than 20% of the time [1] . Comparably poor agreement has been reported in studies of inter-indexer consistency <ref> [2] </ref> and in the generation of search terms by either expert intermediaries [3] or less experienced searchers [4] [5] . The prevalence of synonyms tends to decrease the "recall" performance of retrieval systems.
Reference: 3. <institution> Rationale of the Latent Semantic Indexing (LSI) method . . . . . . . . . 2 3.1 Illustration of retrieval problems . . . . . . . . . . . . . . . 2 3.2 The choice of method for uncovering latent semantic structure . . . . . . 4 </institution>
Reference-contexts: For example, two people choose the same main key word for a single well-known object less than 20% of the time [1] . Comparably poor agreement has been reported in studies of inter-indexer consistency [2] and in the generation of search terms by either expert intermediaries <ref> [3] </ref> or less experienced searchers [4] [5] . The prevalence of synonyms tends to decrease the "recall" performance of retrieval systems. By polysemy we refer to the general fact that most words have more than one distinct meaning hhhhhhhhhhhhhhh 1.

Reference: 5. <institution> Tests of the SVD Latent Semantic Indexing (LSI) method . . . . . . . . . 15 5.1 MED . . . . . . . . . . . . . . . . . . . . . . . 16 5.2 CISI . . . . . . . . . . . . . . . . . . . . . . . . 19 5.3 Summary of results from LSI analyses . . . . . . . . . . . . . 20 </institution>
Reference-contexts: Comparably poor agreement has been reported in studies of inter-indexer consistency [2] and in the generation of search terms by either expert intermediaries [3] or less experienced searchers [4] <ref> [5] </ref> . The prevalence of synonyms tends to decrease the "recall" performance of retrieval systems. By polysemy we refer to the general fact that most words have more than one distinct meaning hhhhhhhhhhhhhhh 1.
Reference: 6. <institution> Conclusions and discussion . . . . . . . . . . . . . . . . . . 20 REFERENCES . . . . . . . . . . . . . . . . . . . . . . . 28 i </institution>
Reference-contexts: These are presumably advantageous for conscientious and knowledgeable searchers who can use such tools to suggest additional search terms. The drawback for fully automatic methods is that some added terms may have different meaning from that intended (the polysemy effect) leading to rapid degradation of precision <ref> [6] </ref> . It is worth noting in passing that experiments with small interactive data bases have shown monotonic improvements in recall rate without overall loss of precision as more indexing terms, either taken from the documents or from large samples of actual users' words are added [7] [8] .
References-found: 42

