URL: http://www.cse.psu.edu/~barlow/book.ps
Refering-URL: http://www.cse.psu.edu/~barlow/papers.html
Root-URL: http://www.cse.psu.edu
Title: Numerical Aspects of Solving Linear Least Squares Problems  
Author: Jesse L. Barlow 
Keyword: 4 Rank Deficient Least Squares Problems 39  
Note: Contents  4.1 Statement of the Problem 39 4.2 Maximal Column Pivoting 43  
Date: January 26, 1999  
Address: Park, PA 16802-6103 USA  
Affiliation: Computer Science Department The Pennsylvania State University University  
Abstract: 1 Simple Rounding Error Analysis 2 1.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.2 Floating Point Arithmetic . . . . . . . . . . . . . . . . . . . . . 4 1.3 Computation of Sample Means and Variances . . . . . . . . . . . 6 2 Least Squares Computations 12 2.1 The Gauss-Markov Linear Model . . . . . . . . . . . . . . . . . 12 2.2 Orthogonal Transformation Methods . . . . . . . . . . . . . . . . 15 2.2.1 The Householder-Golub Factorization . . . . . . . . . . . 17 2.2.2 The Givens Orthogonal Factorization . . . . . . . . . . . 23 2.3 The Normal Equations . . . . . . . . . . . . . . . . . . . . . . . . 26 3 The Singular Value Decomposition 28 3.1 Relationship to Regression Problems . . . . . . . . . . . . . . . . 28 3.1.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . 28 3.1.2 Principal Component Analysis and the SVD . . . . . . . . 31 3.1.3 The SVD and Least Squares . . . . . . . . . . . . . . . . 32 3.1.4 Algorithms for Computing the SVD . . . . . . . . . . . . 33 3.1.5 The role of the C-S Decomposition . . . . . . . . . . . . . 34 3.2 The Errors in Variables Problem and Total Least Squares . . . . 36 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. DuCroz, A. Greenbaum, S. Hammarling, A. McKenney, S. Ostrouchov, and D. Sorensen. </author> <title> LAPACK User's Guide. </title> <publisher> SIAM Publications, </publisher> <address> Philadelphia, </address> <year> 1992. </year>
Reference-contexts: However, one cannot expect to obtain good results this way. Practical algorithms are given by Stewart [103], Van Loan [106], and Paige [89]. Modifications of Paige's routine are being considered for LAPACK <ref> [1] </ref>. The direction-of-arrival (DOA) problem is an important estimation problem in signal processing. The two most commonly used algorithms for that problem are the MUSIC algorithm of Schmidt [98] and the ESPRIT algorithm of Roy et 34 al. [97]. <p> If (4.43), (4.44), and (4.45) are satisfied then X has no singular values in the interval 2 [*fl 1 ; *fl 2 ] and ae = N ull * (X). We now describe two algorithms to generate the sequence of vectors w <ref> [1] </ref> ; w [2] ; : : : ; w [ae] . It is assumed that one is able to accurately estimate the condition number of a matrix. <p> Call that matrix ^ R k . Let ^ R k = ^ Q k R k+1 be an orthogonal factorization k k + 1 else done true; S = spanfw <ref> [1] </ref> ; : : : ; w [ae] g endif endwhile The above algorithm takes a matrix whose orthogonal decomposition has already been computed and deletes columns that are linearly dependent to machine precision.
Reference: [2] <author> M. Arioli, I.S. Duff, and P.P.M. de Rijk. </author> <title> On the augmented system approach to sparse linear least-squares problems. </title> <institution> Research Report CSS-223, Harwell Laboratory, Computer Science and Systems Division, </institution> <address> Didcot, England, </address> <month> October </month> <year> 1988. </year>
Reference-contexts: On the other hand, this form has recently become important in the solution of large sparse problems <ref> [2, 21] </ref>. Also, it allows one to take advantage of packages such as Harwell's MA27 [41] for the solution of symmetric indefinite systems. Moreover, equation (2.13) makes an important point about the least squares problem (2.5). It should be considered as an equation in both ~ fi and ~e. <p> If (4.43), (4.44), and (4.45) are satisfied then X has no singular values in the interval 2 [*fl 1 ; *fl 2 ] and ae = N ull * (X). We now describe two algorithms to generate the sequence of vectors w [1] ; w <ref> [2] </ref> ; : : : ; w [ae] . It is assumed that one is able to accurately estimate the condition number of a matrix.
Reference: [3] <author> E.H. Bareiss. </author> <title> Numerical solution of weighted least squares problems by G-transformations. </title> <type> Tech. Rep. </type> <institution> 82-03-NAM-03, Department of Electrical Engineering and Computer Science, Northwestern University, </institution> <address> Evanston, IL, </address> <year> 1982. </year>
Reference-contexts: Also, Givens rotations can be implemented in ways that save about half of the multiplications and thus perform the Q-R factorization in about as many operations as the Householder-Golub method. Such implementations have been proposed by Gentleman [49], Hammarling [62], and Bareiss <ref> [3] </ref>. All of these implementations could be used for sparse matrix computations without great difficulty. All of these algorithms are square root free. However, since a square root requires about as much time as a division, this is not an important issue. Bareiss's [3] method is the most storage efficient. <p> by Gentleman [49], Hammarling [62], and Bareiss <ref> [3] </ref>. All of these implementations could be used for sparse matrix computations without great difficulty. All of these algorithms are square root free. However, since a square root requires about as much time as a division, this is not an important issue. Bareiss's [3] method is the most storage efficient. Gentleman's [49] method shows how to implement some of these "fast Givens" rotations in a stable fashion, while also exhibiting some unstable implementations. Barlow [5] shows that Bareiss's method is stable.
Reference: [4] <author> J.L. Barlow. </author> <title> Probabilistic Error Analysis of Floating Point and CRD Arithmetics. </title> <type> PhD thesis, </type> <institution> Northwestern University, </institution> <address> Evanston, IL, </address> <year> 1981. </year>
Reference-contexts: One iterative is described in section five. We discuss two methods for analyzing rounding errors:(1) forward error analysis and (2) backward error analysis. These are the two types that are most commonly discussed in the literature, although other formalisms have been proposed (cf. <ref> [24, 4, 88, 118, 79, 86] </ref> ). <p> Such a rule is never actually implemented in practice. Instead, some guard digits are accumulated for rounding. Cody [33] showed statistically that two guard digits are enough to make most computations nearly as accurate as an idealized round. See also <ref> [4] </ref>. Another method for obtaining f l (a) from a that is implemented on many machines such as those manufactured by CRAY and IBM is chopping. Again this requires the use of guard digits to be properly implemented.
Reference: [5] <author> J.L. Barlow. </author> <title> Stability analysis of the G-algorithm and a note on its application to sparse least squares problems. </title> <journal> BIT, </journal> <volume> 25 </volume> <pages> 507-520, </pages> <year> 1985. </year>
Reference-contexts: Bareiss's [3] method is the most storage efficient. Gentleman's [49] method shows how to implement some of these "fast Givens" rotations in a stable fashion, while also exhibiting some unstable implementations. Barlow <ref> [5] </ref> shows that Bareiss's method is stable. Both of the stability 25 analyses [5, 49] depend upon the ability to rescale. Lawson et al. [81] show that this rescaling problem reduces the observed speedup over standard Gviens rotations to about 1.4-1.6 for sufficiently large problems. <p> Bareiss's [3] method is the most storage efficient. Gentleman's [49] method shows how to implement some of these "fast Givens" rotations in a stable fashion, while also exhibiting some unstable implementations. Barlow [5] shows that Bareiss's method is stable. Both of the stability 25 analyses <ref> [5, 49] </ref> depend upon the ability to rescale. Lawson et al. [81] show that this rescaling problem reduces the observed speedup over standard Gviens rotations to about 1.4-1.6 for sufficiently large problems.
Reference: [6] <author> J.L. Barlow. </author> <title> Error analysis and implementation aspects of deferred correction for equality constrained least squares problems. </title> <journal> SIAM J. Num. Anal., </journal> <volume> 25 </volume> <pages> 1340-1358, </pages> <year> 1988. </year>
Reference-contexts: If C is rank deficient, we must use the reduction (5.5) first and substitute U for C and d 0 1 for d. The perturbation theory for the LSE problem is quite complicated. We do not discuss it here, but instead refer the reader to <ref> [42, 6, 113] </ref>. Ideally, a method for solving (5.1)-(5.2) should compute the exact solution of a "nearby" version of the problem. <p> We now give an error analysis result from <ref> [10, 6] </ref>. Theorem 5.1 Assume that n 1 p n 1 + n 2 , that C has rank n 1 , that rank X = p and that o 1. <p> Also the WLS problem is sometimes the one whose solution is desired, as in, for example, smoothing splines [112]. 5.4 The Deferred Correction Method The deferred correction procedure due to Van Loan [82] is an attempt to improve the accuracy of the weighting method. Barlow <ref> [6] </ref> presents an error analysis of the procedure. <p> However, in [14], it is shown that for all but ill-conditioned problems, acceptable results can be obtained in two iterations with o = 1 where is the machine unit. From the results in <ref> [6, 82] </ref>, this iteration is not recommended for ill-conditioned problems. Thus two iterations are enough for many problems in which it is reasonable to use Algorithm 4.2. The proof of these results is in [6, 13]. <p> From the results in [6, 82], this iteration is not recommended for ill-conditioned problems. Thus two iterations are enough for many problems in which it is reasonable to use Algorithm 4.2. The proof of these results is in <ref> [6, 13] </ref>. It is pointed out in [82, 10, 13] that one can solve (5.28)-(5.29) using the corrected "semi-normal" equations as described by Bjorck.
Reference: [7] <author> J.L. Barlow. </author> <title> Error analysis of update methods for the symmetric eigenvalue problem. </title> <type> Technical Report CS-91-23, </type> <institution> The Pennsylvania State University, Department of Computer Science, University Park, </institution> <address> PA, </address> <year> 1991. </year> <note> to appear, SIAM J. Matrix Anal. Appl. </note>
Reference-contexts: Some of the implementation issues of this procedure in floating point arithmetic are discussed by Sorensen and Tang [99] and error analysis results for such algorithms are given by Barlow <ref> [7] </ref>. A faster implementation of this procedure was recently given by Gu and Eisenstat [59]. 3.1.5 The role of the C-S Decomposition An important and very useful singular value problem is that of computing the C-S decomposition. It is given as follows.
Reference: [8] <author> J.L. Barlow. </author> <title> Error bounds and condition estimates for the computation of null vectors with applications to Markov chains. </title> <type> Technical Report CS-91-20, </type> <institution> The Pennsylvania State University, Department of Computer Science, University Park, </institution> <address> PA, </address> <year> 1991. </year> <note> to appear, SIAM J. Matrix Anal. Appl. </note>
Reference-contexts: Barlow <ref> [8] </ref> proved the following error analysis result for this algorithm. This resolved a conjecture in [29].
Reference: [9] <author> J.L. Barlow and J.W. Demmel. </author> <title> Computing accurate eigensystems of scaled diagonally dominant matrices. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 27 </volume> <pages> 762-791, </pages> <year> 1990. </year>
Reference-contexts: If F is given in the formulation, as is often the case, then forming W = F F T destroys information in exactly the same manner as forming normal equations does. If W is given, it is more robust to obtain F from the Cholesky factorization of W <ref> [9] </ref>, thus avoiding some numerical problems. For instance, it assures that the computed eigenvalues of W are always non-negative. 31 3.1.3 The SVD and Least Squares The SVD is a specific decomposition of the form given in Theorem 2.3. Again consider the least squares problem (2.5). <p> Veselic' and Demmel [39] proved that the Jacobi algorithm is more accurate. In particular, it is shown to meet the stronger stability criterion described by Barlow and Demmel <ref> [9] </ref>. A survey and analysis of parallel implementations of the Jacobi method are given by Luk and Park [83]. The Jessup-Sorensen divide-and-conquer method is very efficient on vector multiprocessor architectures such as the Alliant FX/8.
Reference: [10] <author> J.L. Barlow and S.L. Handy. </author> <title> The direct solution of weighted and equality constrained least squares problems. </title> <journal> SIAM J. Sci. Stat. Computing, </journal> <volume> 9 </volume> <pages> 704-716, </pages> <year> 1988. </year>
Reference-contexts: The construction of the permutation matrix P T C can be done in a variety of ways as are discussed in Chapter 4. The column pivoting algorithm of Businger and Golub [27] is the most well-known approach. Barlow and Handy <ref> [10] </ref> analyze compromise column pivoting routine showing how the particular choice of column ordering can influence the accuracy of methods for solving (5.3)-(5.4). <p> However, this fill is reasonable for direct methods to solve the LSE problem. 59 5.3 The Method of Weighting The technique of weighting is very closely related to the Bjorck-Golub procedure, both as an algorithm and in its stability properties <ref> [10] </ref>. In particular, consider the orthogonal factorization of A o = o C 0 P T (5.18) where o is some large weight, Q is an n fi n orthogonal matrix, R is a p fi p upper triangular matrix, and P is a permutation matrix. Barlow with Handy [10] and <p> properties <ref> [10] </ref>. In particular, consider the orthogonal factorization of A o = o C 0 P T (5.18) where o is some large weight, Q is an n fi n orthogonal matrix, R is a p fi p upper triangular matrix, and P is a permutation matrix. Barlow with Handy [10] and with Vemulapati [14] show that the best way to perform this factorization is to factor C according to (5.5) and factor A = o U 0 P T where X = XP C , and Q 2 is an n fi n orthogonal matrix, P T A is a <p> We now give an error analysis result from <ref> [10, 6] </ref>. Theorem 5.1 Assume that n 1 p n 1 + n 2 , that C has rank n 1 , that rank X = p and that o 1. <p> The factor ^o is important in the accurate solution of the WLS problem by orthogonal factorization. For maximal column pivoting, from tests by Higham [67], the factor ^o seldom exceeds 10. The same seems to hold for the compromise pivoting strategy by Barlow and Handy <ref> [10] </ref>. For sparse problems, maximal column pivoting tends to require too much time and storage and neither strategy can be done without dynamic storage allocation. In theory, the bound on ^o can be large for either procedure. <p> ffiC ffiX d 0 B d 0 d 0 f 1 1 C ffid ffiy where V = Q 1 LQ 2 , Q 1 and Q 2 are orthogonal, L is unit lower triangular, and the backward errors ffiC, ffiX, ffid, and ffiy satisfy the bounds in Theorem 3.1 <ref> [10] </ref>. As pointed out by Barlow and Handy [10], this method always requires at least as much storage as does the Bjorck-Golub algorithm. However, improvements and update strategies can be more easily applied to this method. <p> 0 f 1 1 C ffid ffiy where V = Q 1 LQ 2 , Q 1 and Q 2 are orthogonal, L is unit lower triangular, and the backward errors ffiC, ffiX, ffid, and ffiy satisfy the bounds in Theorem 3.1 <ref> [10] </ref>. As pointed out by Barlow and Handy [10], this method always requires at least as much storage as does the Bjorck-Golub algorithm. However, improvements and update strategies can be more easily applied to this method. <p> 1; : : : ; until convergence Find fi (k) such that A o fi (k) = z (k) where z (k) = o w 1 + o 1 (k) Update fi (k+1) fi (k) + fi (k) w 1 w 1 Cfi (k) (k+1) endfor It is recommended in <ref> [10] </ref> that the iteration be considered to converge when k w (k) k 2 is small. However, in [14], it is shown that for all but ill-conditioned problems, acceptable results can be obtained in two iterations with o = 1 where is the machine unit. <p> From the results in [6, 82], this iteration is not recommended for ill-conditioned problems. Thus two iterations are enough for many problems in which it is reasonable to use Algorithm 4.2. The proof of these results is in [6, 13]. It is pointed out in <ref> [82, 10, 13] </ref> that one can solve (5.28)-(5.29) using the corrected "semi-normal" equations as described by Bjorck. <p> It is shown in <ref> [10] </ref> that if we choose o = 3 as recommended above, under weaker conditions than those necessary for convergence, this value will be as good as can be expected from orthogonal factorization.
Reference: [11] <author> J.L. Barlow and I.C.F. Ipsen. </author> <title> Scaled Givens rotations for the solution of linear least squares problems on systolic arrays. </title> <journal> SIAM J. Sci. Stat. Computing, </journal> <volume> 8 </volume> <pages> 716-733, </pages> <year> 1987. </year>
Reference-contexts: Givens rotations have been very popular for systolic algorithms (cf. [51, 66]), due to the fact that they can easily be implemented in hardware. To reduce the device area in VLSI implementations of Givens rotations, Barlow and Ipsen <ref> [11] </ref> proposed an implementation that involve no square roots, only one division, and that is automatically scaled.
Reference: [12] <author> J.L. Barlow, N.K. Nichols, and R.J. Plemmons. </author> <title> Iterative methods for equality constrained least squares problems. </title> <journal> SIAM J. Sci. Stat. Computing, </journal> <volume> 9 </volume> <pages> 892-906, </pages> <year> 1988. </year>
Reference-contexts: The direct elimination method of Bjorck and Golub [22]; 2. The method of simply solving (5.7)-(5.8) by orthogonal factorization; 3. A deferred correction procedure due to Van Loan [82] that makes the method (5.7)-(5.8) more accurate; 4. An iterative method due to Barlow, Nichols, and Plemmons <ref> [12] </ref>. These four methods use closely related ideas. Method (2) becomes method (1) as o ! 1. Method (3) is an enhancement of method (2) and can be altered to be an enhancement of method (1). <p> This algorithm implicitly iterates in the null space of C. Moreover, every iterate fi (k) satisfies the constraint (5.2). The following theorem from Barlow, Nichols, and Plemmons <ref> [12] </ref> explicitly states that property. Theorem 5.2 Let N = A 1 ^ A 1 where ^ A 1 is a (p n 1 ) fi (p n 1 ) nonsingular matrix. <p> Moreover, every iterate fi (k) in Algorithm 5.1 satisfies (5.2) and N is a basis for N ull (C). In <ref> [12] </ref>, it is shown that the convergence rate of the iteration is given by k e e (k) k 2 ff p 2k where ff =k Y k 2 =k A 2 A 1 1 J k 2 .
Reference: [13] <author> J.L. Barlow and U.B. Vemulapati. </author> <title> A note on deferred correction for equality constrained least squares problems. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 29 </volume> <pages> 249-256, </pages> <year> 1992. </year>
Reference-contexts: It is not desirable to solve the large augmented system (5.22)-(5.24) at each stage of the iteration. However, we note that solving this system is equivalent to solving a least squares problem of the form (5.19)- (5.21) with a single coefficient matrix A (o ). In <ref> [13] </ref>, the following implementation of Algorithm 5.1 is recommended. <p> From the results in [6, 82], this iteration is not recommended for ill-conditioned problems. Thus two iterations are enough for many problems in which it is reasonable to use Algorithm 4.2. The proof of these results is in <ref> [6, 13] </ref>. It is pointed out in [82, 10, 13] that one can solve (5.28)-(5.29) using the corrected "semi-normal" equations as described by Bjorck. <p> From the results in [6, 82], this iteration is not recommended for ill-conditioned problems. Thus two iterations are enough for many problems in which it is reasonable to use Algorithm 4.2. The proof of these results is in [6, 13]. It is pointed out in <ref> [82, 10, 13] </ref> that one can solve (5.28)-(5.29) using the corrected "semi-normal" equations as described by Bjorck.
Reference: [14] <author> J.L. Barlow and U.B. Vemulapati. </author> <title> Rank detection methods for sparse matrices. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 13 </volume> <pages> 1279-1297, </pages> <year> 1992. </year>
Reference-contexts: Such a situation arises in "white noise" signal identification problems. Moreover, in this situation, column pivoting and condition estimation procedures to detect the rank will tend to find it accurately <ref> [44, 28, 14] </ref>. However, if we have a case where l l+1 : : : l+k that is, if several singular values cluster around , the rank of X is very difficult to determine and, in fact, no value that we assign to it will be meaningful. <p> Chan [28] describes a related approach that enhances the accuracy of column pivoting by estimating the null vectors using inverse iteration on R. The approach given below is that of Foster [44]. The implementation ideas are similar to those discussed by Bischof [15] and Barlow and Vemulapati <ref> [14] </ref>. Chan and Hansen [30] show how this technique can be substituted for the SVD in certain ill-posed problems and still obtain good solutions. The approach is based upon the estimation of the condition of a matrix. <p> That fact will be used in finding the rank. 4.3.2 A Rank Revealing Orthogonal Factorization We begin with some of the analysis due to Foster [44] and then a rank revealing orthogonal factorization similar to that discussed by Barlow and Vemulapati <ref> [14] </ref>. Let S be a subspace of &lt; p . <p> The following algorithm, due to Barlow and Vemulapati <ref> [14] </ref> is a rank detection scheme designed for sparse matrices. Its primary purpose is to avoid the dynamic reallocation of storage that would be required for maximum column pivoting. That issue is explained therein [14]. A different approach to this problem is given by Bischof, Pierce, and Lewis [17, 94]. <p> The following algorithm, due to Barlow and Vemulapati <ref> [14] </ref> is a rank detection scheme designed for sparse matrices. Its primary purpose is to avoid the dynamic reallocation of storage that would be required for maximum column pivoting. That issue is explained therein [14]. A different approach to this problem is given by Bischof, Pierce, and Lewis [17, 94]. It can also be used as a preprocessing routine for a rank revealing orthogonal factorization. <p> That is, fl 1 j are the parameters used in column pivoting. This scheme can be implemented in O (maxfn R ; n log ng) operations on top of that for orthogonal factorization where n R is the number of nonzeroes in R. The implementation in <ref> [14] </ref> actually requires O (n R log n) operations. Tests in [14] showed that the algorithm usually gave a slightly lower value for the rank than did column pivoting. <p> This scheme can be implemented in O (maxfn R ; n log ng) operations on top of that for orthogonal factorization where n R is the number of nonzeroes in R. The implementation in <ref> [14] </ref> actually requires O (n R log n) operations. Tests in [14] showed that the algorithm usually gave a slightly lower value for the rank than did column pivoting. In the statement of the algorithm below N onz (x [l] ) means the current nonzero structure of (x l;l+1 ; x l;l+2 ; : : : ; x lp ). <p> Q to the appropriate rows of X. f irstl l; f irstk k + 1; i 0; i = 1; 2; : : : ; p (implicitly). endif endif k k + 1; done l n or k p endwhile R 55 Additional properties of this algorithm are stated in <ref> [14] </ref>. 5 Equality Constrained Least Squares Problems 5.1 Introduction We consider the solution of the equality constrained least squares problem of finding ^ fi 2 &lt; p such that k y X ^ fi k 2 = min subject to the constraint C ^ fi = d (5.2) where X is <p> The column pivoting algorithm of Businger and Golub [27] is the most well-known approach. Barlow and Handy [10] analyze compromise column pivoting routine showing how the particular choice of column ordering can influence the accuracy of methods for solving (5.3)-(5.4). However, as pointed out by Barlow and Vemulapati <ref> [14, 111] </ref>, for large sparse problems, no known column pivoting routine can avoid dynamic storage allocation during the factorization of C. Barlow and Vemulapati [14, 111] give an algorithm to find the rank of C that allows us to use static storage allocation as is done in packages such as SPARSPAK-B <p> However, as pointed out by Barlow and Vemulapati <ref> [14, 111] </ref>, for large sparse problems, no known column pivoting routine can avoid dynamic storage allocation during the factorization of C. Barlow and Vemulapati [14, 111] give an algorithm to find the rank of C that allows us to use static storage allocation as is done in packages such as SPARSPAK-B [52]. An entirely different approach to this problem is given by Bischof, Lewis, and Pierce [17, 94]. <p> Barlow with Handy [10] and with Vemulapati <ref> [14] </ref> show that the best way to perform this factorization is to factor C according to (5.5) and factor A = o U 0 P T where X = XP C , and Q 2 is an n fi n orthogonal matrix, P T A is a permutation matrix, and n <p> For sparse problems, maximal column pivoting tends to require too much time and storage and neither strategy can be done without dynamic storage allocation. In theory, the bound on ^o can be large for either procedure. The rank detection strategy of Barlow and Vemulapati <ref> [14] </ref> insures a non-singular U 1 and can be done with static storage allocation. However, the factor ^o may not be bounded. Foster [45] shows that the rank detection strategy in SPARSPAK-B [52, 65] can make ^o large in practice. <p> However, in <ref> [14] </ref>, it is shown that for all but ill-conditioned problems, acceptable results can be obtained in two iterations with o = 1 where is the machine unit. From the results in [6, 82], this iteration is not recommended for ill-conditioned problems.
Reference: [15] <author> C.H. Bischof. </author> <title> Incremental condition estimation. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 11 </volume> <pages> 312-322, </pages> <year> 1990. </year>
Reference-contexts: Chan [28] describes a related approach that enhances the accuracy of column pivoting by estimating the null vectors using inverse iteration on R. The approach given below is that of Foster [44]. The implementation ideas are similar to those discussed by Bischof <ref> [15] </ref> and Barlow and Vemulapati [14]. Chan and Hansen [30] show how this technique can be substituted for the SVD in certain ill-posed problems and still obtain good solutions. The approach is based upon the estimation of the condition of a matrix. <p> An entirely different approach to this problem is given by Bischof, Lewis, and Pierce [17, 94]. These approaches are based on ideas due to Foster [44, 45], Chan [28], and Bischof <ref> [15] </ref>. The classical statistical context for the LSE is in constrained linear models, i.e., a data fitting problem where some of the function values must be fit exactly.
Reference: [16] <author> C.H. Bischof. </author> <title> Private communication. </title> <booktitle> SIAM Meeting on Parallel Processing in Scientific Computing, </booktitle> <month> March </month> <year> 1991. </year> <month> 71 </month>
Reference-contexts: Thus it may not be much faster than doing the SVD. However, with some slight changes and using a variant of Algorithm 4.2 we can develop a forward rank revealing orthogonal factorization for sparse matrices that is very effective. Bischof <ref> [16] </ref> noted recently that the development of a completely robust forward rank revealing orthogonal factorization is very difficult, perhaps impossible. However, backward algorithms can be very robust.
Reference: [17] <author> C.H. Bischof, J.G. Lewis, and D.J. Pierce. </author> <title> Incremental condition estima-tion for sparse matrices. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 11 </volume> <pages> 644-662, </pages> <year> 1990. </year>
Reference-contexts: Its primary purpose is to avoid the dynamic reallocation of storage that would be required for maximum column pivoting. That issue is explained therein [14]. A different approach to this problem is given by Bischof, Pierce, and Lewis <ref> [17, 94] </ref>. It can also be used as a preprocessing routine for a rank revealing orthogonal factorization. <p> Barlow and Vemulapati [14, 111] give an algorithm to find the rank of C that allows us to use static storage allocation as is done in packages such as SPARSPAK-B [52]. An entirely different approach to this problem is given by Bischof, Lewis, and Pierce <ref> [17, 94] </ref>. These approaches are based on ideas due to Foster [44, 45], Chan [28], and Bischof [15]. The classical statistical context for the LSE is in constrained linear models, i.e., a data fitting problem where some of the function values must be fit exactly.
Reference: [18] <author> A. Bjorck. </author> <title> Solving linear least squares problems by Gram-Schmidt or-thogonalization. </title> <journal> BIT, </journal> <volume> 7 </volume> <pages> 1-21, </pages> <year> 1967. </year>
Reference-contexts: Equaton (2.13) is often used to un-derstand numerical properties of least squares problems, and in the design of iterative improvement procedures (cf. <ref> [18] </ref>). It has less commonly used to solve the least squares problem (2.5): (1) it enlarges the dimension of the problem; (2) it is a symmetric, indefinite system, thus pivoting [26] will be required in its solution. <p> The matrices Q 1 and R satisfy a uniqueness property given in the following theorem from <ref> [18] </ref>. Theorem 2.4 (Bjorck) Let X be an n fi p matrix with linearly independent columns. Then X can be written uniquely in the form X = Q 1 R where Q 1 has orthonormal columns and R is upper triangular with positive diagonal elements.
Reference: [19] <author> A. Bjorck. </author> <title> A general updating algorithm for constrained linear least squares problems. </title> <journal> SIAM J. Sci. Stat. Computing, </journal> <volume> 5 </volume> <pages> 394-402, </pages> <year> 1984. </year>
Reference-contexts: We note that two well-known least squares packages handle constrained problems: SPARSPAK-B [52] developed by George et al. at Waterloo and Oak Ridge and LSQR developed by Paige and Saunders [90, 91, 92] at Stanford. SPARSPAK-B is a package for direct solution and implements a method by Bjorck <ref> [19] </ref> for (5.1)-(5.2) when both C and X have sparse and dense rows. The three direct procedures described here can easily be implemented using SPARSPAK-B. LSQR is an iterative package that uses the Lanczos algorithm. <p> It is the fundamental step in a complicated strategy by Bjorck <ref> [19] </ref> that includes a method for handling sparse and dense rows of C and X. The Bjorck method [19] is that implemented in SPARSPAK-B. Most of the fill for sparse problems occurs in the formation of ~ X 2 . <p> It is the fundamental step in a complicated strategy by Bjorck <ref> [19] </ref> that includes a method for handling sparse and dense rows of C and X. The Bjorck method [19] is that implemented in SPARSPAK-B. Most of the fill for sparse problems occurs in the formation of ~ X 2 .
Reference: [20] <author> A. Bjorck. </author> <title> Least Squares Methods. </title> <publisher> Elsevier/North Holland, </publisher> <address> New York, </address> <year> 1989. </year>
Reference-contexts: Moreover, equation (2.13) makes an important point about the least squares problem (2.5). It should be considered as an equation in both ~ fi and ~e. For a discussion of (2.13), see [57, Chapter 5] or <ref> [20] </ref>. The two equations (2.11) and (2.12) can be combined into the linear system A ~ fi = f (2.14) where A = X T X ; f = X T y: (2.15) Here A is always positive semi-definite (positive definite if X has full column rank). <p> These operations require much less time and device area than square roots and divisions. Many of the ideas for the development of these two subsections came from an unpublished manuscript by Stewart [100] and the long paper on least squares computations by Bjorck <ref> [20] </ref>. The latter gives much more detail than is given here. 2.3 The Normal Equations The normal equations is the most commonly used method for solving the least squares problem (2.5). These equations, given by (2.14)-(2.15), are symmetric and positive definite if X has full column rank.
Reference: [21] <author> A. Bjorck. </author> <title> Pivoting and stability in the augmented system method. </title> <type> Technical Report Lith-Math-R-1991-30, </type> <institution> Department of Mathematics, Linkoping University, Linkoping, Sweden, </institution> <month> June </month> <year> 1991. </year>
Reference-contexts: On the other hand, this form has recently become important in the solution of large sparse problems <ref> [2, 21] </ref>. Also, it allows one to take advantage of packages such as Harwell's MA27 [41] for the solution of symmetric indefinite systems. Moreover, equation (2.13) makes an important point about the least squares problem (2.5). It should be considered as an equation in both ~ fi and ~e.
Reference: [22] <author> A. Bjorck and G.H. Golub. </author> <title> Iterative refinement of linear least squares solutions by Householder transformations. </title> <journal> BIT, </journal> <volume> 7 </volume> <pages> 322-337, </pages> <year> 1967. </year>
Reference-contexts: All of these methods are amenable for use with standard sparse matrix packages such as SPARSPAK-B [52]. We discuss four methods in this survey paper: 1. The direct elimination method of Bjorck and Golub <ref> [22] </ref>; 2. The method of simply solving (5.7)-(5.8) by orthogonal factorization; 3. A deferred correction procedure due to Van Loan [82] that makes the method (5.7)-(5.8) more accurate; 4. An iterative method due to Barlow, Nichols, and Plemmons [12]. These four methods use closely related ideas.
Reference: [23] <author> J.L. </author> <title> Blue. A portable FORTRAN program to find the Euclidean norm of a vector. </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 4 </volume> <pages> 15-23, </pages> <year> 1978. </year>
Reference-contexts: However, writing a code for the Euclidean norm of a vector that is completely resistant to underflows and overflows is a complicated task and is seldom done (it is about three pages of FORTRAN, see Blue <ref> [23] </ref>). Computations with Householder transformations consist of just an inner product followed by a vector addition.
Reference: [24] <author> C.G. </author> <title> Broyden. Some condition number bounds for the Gaussian elimination process. </title> <journal> J. Inst. Math. Appl., </journal> <volume> 12 </volume> <pages> 273-286, </pages> <year> 1973. </year>
Reference-contexts: One iterative is described in section five. We discuss two methods for analyzing rounding errors:(1) forward error analysis and (2) backward error analysis. These are the two types that are most commonly discussed in the literature, although other formalisms have been proposed (cf. <ref> [24, 4, 88, 118, 79, 86] </ref> ).
Reference: [25] <author> J.R. Bunch, C.P. Nielsen, and D.C. Sorensen. </author> <title> Rank-one modification of the symmetric eigenproblem. </title> <journal> Numerische Mathematik, </journal> <volume> 31 </volume> <pages> 31-48, </pages> <year> 1978. </year>
Reference-contexts: The Jessup-Sorensen divide-and-conquer method is very efficient on vector multiprocessor architectures such as the Alliant FX/8. It is based upon an algorithm due to Cuppen [35] and the eigenvalue update procedure due to Bunch, Nielsen, and Sorensen <ref> [25] </ref>. Some of the implementation issues of this procedure in floating point arithmetic are discussed by Sorensen and Tang [99] and error analysis results for such algorithms are given by Barlow [7].
Reference: [26] <author> J.R. Bunch and B.N. Parlett. </author> <title> Direct methods for solving symmetric indefinite systems of linear equations. </title> <journal> SIAM J. Num. Anal., </journal> <volume> 8 </volume> <pages> 639-655, </pages> <year> 1971. </year>
Reference-contexts: It has less commonly used to solve the least squares problem (2.5): (1) it enlarges the dimension of the problem; (2) it is a symmetric, indefinite system, thus pivoting <ref> [26] </ref> will be required in its solution. On the other hand, this form has recently become important in the solution of large sparse problems [2, 21]. Also, it allows one to take advantage of packages such as Harwell's MA27 [41] for the solution of symmetric indefinite systems.
Reference: [27] <author> P.A. Businger and G.H. Golub. </author> <title> Linear least squares solutions by householder transformations. </title> <journal> Numerische Mathematik, </journal> <volume> 7 </volume> <pages> 269-278, </pages> <year> 1965. </year>
Reference-contexts: The construction of the permutation matrix P T C can be done in a variety of ways as are discussed in Chapter 4. The column pivoting algorithm of Businger and Golub <ref> [27] </ref> is the most well-known approach. Barlow and Handy [10] analyze compromise column pivoting routine showing how the particular choice of column ordering can influence the accuracy of methods for solving (5.3)-(5.4).
Reference: [28] <author> T.F. Chan. </author> <title> Rank revealing QR factorization. </title> <journal> Lin. Alg. Appl., </journal> 88/89:67-82, 1987. 
Reference-contexts: Such a situation arises in "white noise" signal identification problems. Moreover, in this situation, column pivoting and condition estimation procedures to detect the rank will tend to find it accurately <ref> [44, 28, 14] </ref>. However, if we have a case where l l+1 : : : l+k that is, if several singular values cluster around , the rank of X is very difficult to determine and, in fact, no value that we assign to it will be meaningful. <p> However, the computation of the SVD is expensive and often cannot take advantage of special structure such as sparsity. We would like to use orthogonal factorization in some way that "reveals the rank" or to use Chan's <ref> [28] </ref> term, perform a "rank revealing" orthogonal factorization. Consider the orthogonal factorization of an exactly rank deficient matrix X = (X 1 X 2 )P T . <p> The kind of failure in Example 4.1 is rare. Several alternative approaches to this problem have been suggested. Foster [44] shows how condition estimation techniques can be used to find the numerical rank without column interchanges. Chan <ref> [28] </ref> describes a related approach that enhances the accuracy of column pivoting by estimating the null vectors using inverse iteration on R. The approach given below is that of Foster [44]. The implementation ideas are similar to those discussed by Bischof [15] and Barlow and Vemulapati [14]. <p> An entirely different approach to this problem is given by Bischof, Lewis, and Pierce [17, 94]. These approaches are based on ideas due to Foster [44, 45], Chan <ref> [28] </ref>, and Bischof [15]. The classical statistical context for the LSE is in constrained linear models, i.e., a data fitting problem where some of the function values must be fit exactly.
Reference: [29] <author> T.F. Chan, G.H. Golub, and R.J. LeVeque. </author> <title> Algorithms for computing the sample variance: analysis and recommendations. </title> <journal> Am. Stat., </journal> <volume> 7 </volume> <pages> 242-247, </pages> <year> 1983. </year>
Reference-contexts: Chan, Golub, and LeVeque <ref> [29] </ref> call this the standard two-pass algorithm. That is because it requires passing through the data twice: once to compute X and again to compute S 2 . Many modern computers have hierarchical memories, where the fastest memory (called a cache ) is limited in size. <p> It is a simple derivation to show that jS (*) Sj * p + O (* 2 ): (1.34) Hence, in [31], the condition number of the problem of computing S 2 is defined to be k X k 2 n 1S Van Nes pointed out to the authors of <ref> [29] </ref> that the O (* 2 ) term in (1.34) can be eliminated. The bound (1.34) can be achieved by judicious choice of V . Thus a relative error of magnitude u in S 2 is possible from just representation errors on the computer. <p> For that reason, we consider an algorithm to compute S 2 to be stable if it produces a computed ~ S 2 that satisfies j ~ S Sj ug (n) + O (u 2 ) (1.36) where g (n) is a modestly sized function of n. In <ref> [29] </ref>, it was shown that the two-pass algorithm does better than (1.36). <p> It is given by M 1 = X 1 ; T 1 = X 2 M k+1 = M k + X k+1 ; T k+1 = T k + X 2 X = n S 2 = n 1 n X 2 : (1.43) Again following <ref> [29] </ref>, we call this the standard one-pass algorithm. It can be computed using just one pass through the data. Thus it would be faster on large samples that cannot all be placed in fast memory. <p> To avoid this problem, several authors [119, 114, 64] derived stable algorithms for computing the sample variance using only one pass through the data. In <ref> [29] </ref>, it was pointed out that all of these algorithms were effectively equivalent. We now give the version due to West [114]. <p> In many applications it is necessary to combine samples. The following algorithm was proposed by Chan, Golub, and LeVeque <ref> [29] </ref> to resolve these issues. Let T ij = k=i 1 M ij ) 2 1 i j n (1.50) where M ij is as defined in (1.25). <p> Barlow [8] proved the following error analysis result for this algorithm. This resolved a conjecture in <ref> [29] </ref>. Theorem 1.1 Let the sample variance S 2 = 1 n1 T 1;n be computed according to (1.52) with m = d n 2 e and l = b n 2 c or vice versa.
Reference: [30] <author> T.F. Chan and P.C. Hansen. </author> <title> Computing truncated singular value decomposition least squares solution by rank revealing QR-factorizations. </title> <journal> SIAM J. Sci. Stat. Computing, </journal> <volume> 11 </volume> <pages> 519-530, </pages> <year> 1990. </year>
Reference-contexts: The approach given below is that of Foster [44]. The implementation ideas are similar to those discussed by Bischof [15] and Barlow and Vemulapati [14]. Chan and Hansen <ref> [30] </ref> show how this technique can be substituted for the SVD in certain ill-posed problems and still obtain good solutions. The approach is based upon the estimation of the condition of a matrix. <p> We call (5.7)-(5.8) the WLS (Weighted Least Squares) problem. This problem arises on its own in the calculation of smoothing splines [112], in trust region methods for nonlinear least squares [87], in the solution of Fredholm integral equations of the first kind <ref> [43, 30] </ref>, and Ridge regression. If rank (C) = n 1 , we have that lim ^ fi (o ) = ^ fi LSE (5.9) where ^ fi LSE is the solution of (5.1)-(5.2). Thus solving (5.7)-(5.8) is often used as a method of approximately solving the LSE problem.
Reference: [31] <author> T.F. Chan and J.G. Lewis. </author> <title> Computing standard deviations: accuracy. </title> <journal> Comm. Assoc. Comput. Mach., </journal> <volume> 22 </volume> <pages> 526-531, </pages> <year> 1979. </year> <month> 72 </month>
Reference-contexts: Or in some instances, the sample could be stored on a tape, and the two-pass algorithm would require reading the tape twice. We use this example to show how a problem can be conditioned and how this conditioning can influence the development of algorithms. Chan and Lewis <ref> [31] </ref> derived a condition number with respect to a data set fX i g n i=1 for the computation of S 2 . <p> We now show how this "small change" in X will affect S 2 . Define S 2 (*) = n 1 i=1 where X (*) = n i=1 It is a simple derivation to show that jS (*) Sj * p + O (* 2 ): (1.34) Hence, in <ref> [31] </ref>, the condition number of the problem of computing S 2 is defined to be k X k 2 n 1S Van Nes pointed out to the authors of [29] that the O (* 2 ) term in (1.34) can be eliminated. <p> We now give the version due to West [114]. M 1 = X 1 ; T 0 = 0 (1.45) T k+1 = T k + k X = n 1 T n (1.48) Chan and Lewis <ref> [31] </ref> showed that West's algorithm satisfied a bound of the form (1.36) where g (n) = O (n): (1.49) This algorithm satisfies all of our needs except two: (1) it is not amenable to pairwise summation; (2) it allows us to add only one new sample at a time.
Reference: [32] <author> A.K. Cline, C.B. Moler, G.W. Stewart, and J.H. Wilkinson. </author> <title> An estimate for the conditon number of a matrix. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 16 </volume> <pages> 368-375, </pages> <year> 1979. </year>
Reference-contexts: Higham [68] showed how this estimator could be generalized to estimate the condition of a large class of matrix functions. Such an estimator will be used in the new matrix package LAPACK [37]. We now present a simpler estimator that is similar to the one employed in LINPACK <ref> [32, 40] </ref>. For that, let L = R T and let L (k) be the first k fi k principal submatrix of L. Define y (1) = (1): (4.32) Then ^ 1 = l 1 Choose a (1) = ( ^ 1 ).
Reference: [33] <author> W.J. Cody, Jr. </author> <title> Static and dynamic numerical characteristics of floating point arithmetic. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-22:598-601, </volume> <year> 1973. </year>
Reference-contexts: For rounding , a simple computation shows that u = 1 2 fi 1t . Such a rule is never actually implemented in practice. Instead, some guard digits are accumulated for rounding. Cody <ref> [33] </ref> showed statistically that two guard digits are enough to make most computations nearly as accurate as an idealized round. See also [4]. Another method for obtaining f l (a) from a that is implemented on many machines such as those manufactured by CRAY and IBM is chopping.
Reference: [34] <author> P. Concus, G.H. Golub, </author> <title> and D.P. O'Leary. A generalized conjugate gradient procedure for the numerical solution of elliptic partial differential equations. </title> <editor> In J.R. Bunch and D.J. Rose, editors, </editor> <title> Sparse Matrix Computations, </title> <address> New York, 1976. </address> <publisher> Academic Press. </publisher>
Reference-contexts: This is an upper bound based upon the standard bounds from Chebyshev polynomials [57, p.371],[60, p.148]. These bounds are considered quite conservative. In practice, the conjugate gradient method is often much faster than that. Also, from a result due to Concus et al. <ref> [34] </ref>, the iteration must terminate in 1+rank (Y ) iterations. Since rank (Y ) minfpn 1 ; n 1 +n 2 pg by a dimension argument, we have finite termination if there are either very few constraints or very many.
Reference: [35] <author> J. Cuppen. </author> <title> A divide and conquer method for the symmetric tridiagonal eigenproblem. </title> <journal> Numerische Mathematik, </journal> <volume> 36 </volume> <pages> 177-195, </pages> <year> 1981. </year>
Reference-contexts: A survey and analysis of parallel implementations of the Jacobi method are given by Luk and Park [83]. The Jessup-Sorensen divide-and-conquer method is very efficient on vector multiprocessor architectures such as the Alliant FX/8. It is based upon an algorithm due to Cuppen <ref> [35] </ref> and the eigenvalue update procedure due to Bunch, Nielsen, and Sorensen [25]. Some of the implementation issues of this procedure in floating point arithmetic are discussed by Sorensen and Tang [99] and error analysis results for such algorithms are given by Barlow [7].
Reference: [36] <author> J.W. Demmel. </author> <title> Underflow and the reliability of numerical software. </title> <journal> SIAM J. Sci. Stat. Computing, </journal> <volume> 5 </volume> <pages> 887-919, </pages> <year> 1984. </year>
Reference-contexts: The floating point versions of the operations addition, subtraction, multipli cation, and division satisfy f l (a op b) = (a op b)(1 + *) j*j u (1.9) where op 2 f+; ; fl; =g except for overflows and underflows. (Except for machines that do not have guard digits <ref> [36] </ref>). Overflows and underflows are the second limitation of floating point compu tation. The rule (1.9) applies as long as the result c = a op b satisfies dlogjcje 2 [e fl ; e fl ]. <p> An underflow is a result whose magnitude is smaller than the smallest nonzero computer number, denoted !. We now give two common philosophies for setting ! as classified by Demmel <ref> [36] </ref>. One of them is called "store zero" (S.Z.). For S.Z., the smallest computer number is ! SZ = fi 1 fi e fl : S.Z. requires all floating point numbers to be normalized and thus all numbers such that dlog fi jaje &lt; e fl are set to zero. <p> For the other, called gradual underflow (G.U.), the smallest computer num ber is 5 The dramatic difference is that we store unnormalized results with the exponent e fl . An obvious advantage is a slightly expanded number range. Demmel <ref> [36] </ref> points out that there are qualitative advantages to G.U. For instance, f l (xy) = 0 if and only if x = y for G.U., but not for S.Z. On the other hand, S.Z. is easier to explain and easier for the hardware designer. <p> Here ! = ! SZ or ! GU depending upon which underflow method is used. Demmel <ref> [36] </ref> discusses how these two un-derflow methods affect the error analyses of a variety of computations including Gaussian elimination, eigenvalue computations, and numerical quadrature. For underflows, the programmer has choice as to whether they are set to zero or to generate an error flag ( as overflows do).
Reference: [37] <author> J.W. Demmel, J.J. Dongarra, J. DuCroz, A. Greenbaum, S. Hammarling, and D. Sorensen. </author> <title> A prospectus for the development of a linear algebra library for high-performance computers. </title> <type> Tech. Rep. </type> <institution> MCS-TM-97, Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, IL, </institution> <month> September </month> <year> 1987. </year>
Reference-contexts: Higham [68] showed how this estimator could be generalized to estimate the condition of a large class of matrix functions. Such an estimator will be used in the new matrix package LAPACK <ref> [37] </ref>. We now present a simpler estimator that is similar to the one employed in LINPACK [32, 40]. For that, let L = R T and let L (k) be the first k fi k principal submatrix of L.
Reference: [38] <author> J.W. Demmel and W.H. Kahan. </author> <title> Accurate singular values of bidiagonal matrices. </title> <journal> SIAM J. Sci. Stat. Computing, </journal> <volume> 11 </volume> <pages> 873-912, </pages> <year> 1990. </year>
Reference-contexts: This is the algorithm implemented as the LINPACK routing SSVDC. The algorithm is quite complicated and is explained in detail there. Thus we do not give it here. Recently, in the development of the LAPACK project, some enhancements to the Golub-Kahan procedure have been suggested by Demmel and Kahan <ref> [38] </ref>. These enhancements tend to obtain the small singular values more accurately with about the same average speed as the original Golub-Kahan algorithm.
Reference: [39] <author> J.W. Demmel and K. Veselic'. </author> <title> Jacobi's method is more accurate than QR. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 13 </volume> <pages> 1204-1245, </pages> <year> 1992. </year>
Reference-contexts: Although the algorithm tends to be slower on conventional one processor computers, it can be faster on high performance computers with many processors. Veselic' and Demmel <ref> [39] </ref> proved that the Jacobi algorithm is more accurate. In particular, it is shown to meet the stronger stability criterion described by Barlow and Demmel [9]. A survey and analysis of parallel implementations of the Jacobi method are given by Luk and Park [83].
Reference: [40] <author> J.J. Dongarra, J.R. Bunch, C.B. Moler, and G.W. Stewart. </author> <title> LINPACK User's Guide. </title> <publisher> SIAM Publications, </publisher> <address> Philadelphia, </address> <year> 1979. </year>
Reference-contexts: The parameters fl k ; k = 1; 2; : : : ; p can either be stored in another extra vector or recomputed as needed. For instance, LINPACK <ref> [40] </ref> chooses to recompute the fl k . Algorithm 2.1 has excellent numerical properties. <p> The C-S decomposition is also important for block Jacobi algorithms for the parallel computation of eigenvalues and singular values. Excluding numerical difficulties, the C-S decomposition could be computed by a sequence of calls to the LINPACK <ref> [40] </ref> SVD routine. However, one cannot expect to obtain good results this way. Practical algorithms are given by Stewart [103], Van Loan [106], and Paige [89]. Modifications of Paige's routine are being considered for LAPACK [1]. The direction-of-arrival (DOA) problem is an important estimation problem in signal processing. <p> The procedure is implemented in the Lawson-Hanson least squares package as the routine HFTI [80, Chapter 14]. There are several heuristics that produce a "good" column ordering P T . The one used in the Lawson-Hanson least squares package and in LINPACK <ref> [40] </ref> is maximal column pivoting. It is detailed in the next subsection. 4.2 Maximal Column Pivoting The maximal column pivoting procedure goes through the columns of X sequentially and uses the column that is least dependent upon the already factored columns as the next pivot column. <p> Higham [68] showed how this estimator could be generalized to estimate the condition of a large class of matrix functions. Such an estimator will be used in the new matrix package LAPACK [37]. We now present a simpler estimator that is similar to the one employed in LINPACK <ref> [32, 40] </ref>. For that, let L = R T and let L (k) be the first k fi k principal submatrix of L. Define y (1) = (1): (4.32) Then ^ 1 = l 1 Choose a (1) = ( ^ 1 ).
Reference: [41] <author> I.S. Duff, A.M. Erisman, and J.K. Reid. </author> <title> Direct Methods for Sparse Matrices. </title> <publisher> Oxford University Press, </publisher> <address> New York, </address> <year> 1986. </year>
Reference-contexts: On the other hand, this form has recently become important in the solution of large sparse problems [2, 21]. Also, it allows one to take advantage of packages such as Harwell's MA27 <ref> [41] </ref> for the solution of symmetric indefinite systems. Moreover, equation (2.13) makes an important point about the least squares problem (2.5). It should be considered as an equation in both ~ fi and ~e. For a discussion of (2.13), see [57, Chapter 5] or [20].
Reference: [42] <author> L. Elden. </author> <title> Perturbation theory for the least squares problem with equality constraints. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 17 </volume> <pages> 338-350, </pages> <year> 1980. </year>
Reference-contexts: If C is rank deficient, we must use the reduction (5.5) first and substitute U for C and d 0 1 for d. The perturbation theory for the LSE problem is quite complicated. We do not discuss it here, but instead refer the reader to <ref> [42, 6, 113] </ref>. Ideally, a method for solving (5.1)-(5.2) should compute the exact solution of a "nearby" version of the problem.
Reference: [43] <author> L. Elden and R. Schreiber. </author> <title> An application of systolic arrays to discrete ill-posed problems. </title> <journal> SIAM J. Sci. Stat. Computing, </journal> <volume> 7 </volume> <pages> 892-903, </pages> <year> 1986. </year>
Reference-contexts: We call (5.7)-(5.8) the WLS (Weighted Least Squares) problem. This problem arises on its own in the calculation of smoothing splines [112], in trust region methods for nonlinear least squares [87], in the solution of Fredholm integral equations of the first kind <ref> [43, 30] </ref>, and Ridge regression. If rank (C) = n 1 , we have that lim ^ fi (o ) = ^ fi LSE (5.9) where ^ fi LSE is the solution of (5.1)-(5.2). Thus solving (5.7)-(5.8) is often used as a method of approximately solving the LSE problem.
Reference: [44] <author> L.V. Foster. </author> <title> Rank and null space calculations using matrix decompositions without column pivoting. </title> <journal> Lin. Alg. Appl., </journal> <volume> 74 </volume> <pages> 47-72, </pages> <year> 1986. </year>
Reference-contexts: Such a situation arises in "white noise" signal identification problems. Moreover, in this situation, column pivoting and condition estimation procedures to detect the rank will tend to find it accurately <ref> [44, 28, 14] </ref>. However, if we have a case where l l+1 : : : l+k that is, if several singular values cluster around , the rank of X is very difficult to determine and, in fact, no value that we assign to it will be meaningful. <p> In finite precision arithmetic, finding P T poses a difficult problem. Rank and linear dependence are seldom exact relations. We begin the following definition of rank and nullity given by Foster <ref> [44] </ref>. <p> Example 4.1 seems to be pathological. Column pivoting is a reliable method for determining the rank of a matrix in practice. The kind of failure in Example 4.1 is rare. Several alternative approaches to this problem have been suggested. Foster <ref> [44] </ref> shows how condition estimation techniques can be used to find the numerical rank without column interchanges. Chan [28] describes a related approach that enhances the accuracy of column pivoting by estimating the null vectors using inverse iteration on R. The approach given below is that of Foster [44]. <p> Foster <ref> [44] </ref> shows how condition estimation techniques can be used to find the numerical rank without column interchanges. Chan [28] describes a related approach that enhances the accuracy of column pivoting by estimating the null vectors using inverse iteration on R. The approach given below is that of Foster [44]. The implementation ideas are similar to those discussed by Bischof [15] and Barlow and Vemulapati [14]. Chan and Hansen [30] show how this technique can be substituted for the SVD in certain ill-posed problems and still obtain good solutions. <p> That fact will be used in finding the rank. 4.3.2 A Rank Revealing Orthogonal Factorization We begin with some of the analysis due to Foster <ref> [44] </ref> and then a rank revealing orthogonal factorization similar to that discussed by Barlow and Vemulapati [14]. Let S be a subspace of &lt; p . <p> An entirely different approach to this problem is given by Bischof, Lewis, and Pierce [17, 94]. These approaches are based on ideas due to Foster <ref> [44, 45] </ref>, Chan [28], and Bischof [15]. The classical statistical context for the LSE is in constrained linear models, i.e., a data fitting problem where some of the function values must be fit exactly.
Reference: [45] <author> L.V. Foster. </author> <title> The probability of large diagonal elements in the QR factorization. </title> <journal> SIAM J. Sci. Stat. Computing, </journal> <volume> 11 </volume> <pages> 531-544, </pages> <year> 1990. </year> <month> 73 </month>
Reference-contexts: An entirely different approach to this problem is given by Bischof, Lewis, and Pierce [17, 94]. These approaches are based on ideas due to Foster <ref> [44, 45] </ref>, Chan [28], and Bischof [15]. The classical statistical context for the LSE is in constrained linear models, i.e., a data fitting problem where some of the function values must be fit exactly. <p> In theory, the bound on ^o can be large for either procedure. The rank detection strategy of Barlow and Vemulapati [14] insures a non-singular U 1 and can be done with static storage allocation. However, the factor ^o may not be bounded. Foster <ref> [45] </ref> shows that the rank detection strategy in SPARSPAK-B [52, 65] can make ^o large in practice. The deferred correction strategy in the next section allows us to be less concerned about the value of ^o as long as the rank is detected accurately.
Reference: [46] <author> J.G.F. Francis. </author> <title> The QR transformation: A unitary analogue to the LR transformation, Parts I and II. </title> <journal> Comp. J., </journal> <volume> 4 </volume> <pages> 265-272 , 332-45, </pages> <year> 1961. </year>
Reference-contexts: The procedure is given in detail by Golub and Van Loan [57, Chapter 8]. It is based upon the QR algorithm of Francis <ref> [46] </ref> (for the eigenvalues of a general matrix) with Wilkinson's [117] enhancements for symmetric matrices. This is the algorithm implemented as the LINPACK routing SSVDC. The algorithm is quite complicated and is explained in detail there. Thus we do not give it here.
Reference: [47] <author> O.L. Frost. </author> <title> An algorithm for linear constrained adaptive array processing. </title> <booktitle> In Proc. IEEE, </booktitle> <volume> volume 60, </volume> <pages> pages 926-935, </pages> <year> 1972. </year>
Reference: [48] <editor> C.F. Gauss. Theoria Motus Corporum Coelestium in Sectionibus Conicus Solem Ambientium. F. Perthes and I.H. Besser, </editor> <booktitle> Hamburg, 1809. (in Latin). </booktitle>
Reference-contexts: The model (2.3)-(2.4) is called the Gauss-Markov linear model <ref> [48] </ref>, [84] . <p> The second important property arises out of the Gauss-Markov theorem <ref> [48, 84] </ref>. Theorem 2.2 A solution ~ fi of the least squares problem (2.5) is the best linear unbiased estimate (BLUE) of fi.
Reference: [49] <author> W.M. Gentleman. </author> <title> Least squares computations by Givens rotations without square roots. </title> <journal> Lin. Alg. Appl., </journal> <volume> 10 </volume> <pages> 329-336, </pages> <year> 1973. </year>
Reference-contexts: Also, Givens rotations can be implemented in ways that save about half of the multiplications and thus perform the Q-R factorization in about as many operations as the Householder-Golub method. Such implementations have been proposed by Gentleman <ref> [49] </ref>, Hammarling [62], and Bareiss [3]. All of these implementations could be used for sparse matrix computations without great difficulty. All of these algorithms are square root free. However, since a square root requires about as much time as a division, this is not an important issue. <p> All of these algorithms are square root free. However, since a square root requires about as much time as a division, this is not an important issue. Bareiss's [3] method is the most storage efficient. Gentleman's <ref> [49] </ref> method shows how to implement some of these "fast Givens" rotations in a stable fashion, while also exhibiting some unstable implementations. Barlow [5] shows that Bareiss's method is stable. Both of the stability 25 analyses [5, 49] depend upon the ability to rescale. <p> Bareiss's [3] method is the most storage efficient. Gentleman's [49] method shows how to implement some of these "fast Givens" rotations in a stable fashion, while also exhibiting some unstable implementations. Barlow [5] shows that Bareiss's method is stable. Both of the stability 25 analyses <ref> [5, 49] </ref> depend upon the ability to rescale. Lawson et al. [81] show that this rescaling problem reduces the observed speedup over standard Gviens rotations to about 1.4-1.6 for sufficiently large problems.
Reference: [50] <author> W.M. Gentleman. </author> <title> Error analysis of QR decompositions by Givens rotations. </title> <journal> Lin. Alg. Appl., </journal> <volume> 12 </volume> <pages> 189-197, </pages> <year> 1975. </year>
Reference-contexts: Indeed, if we let o ! 1, and use Householder transformations to construct Q 2 , it can be shown that the weighting procedure becomes the Bjorck-Golub algorithm as o ! 1. Moreover, their error analyses are related. In particular, using standard results for orthogonal transformations <ref> [116, 50] </ref>, and Gaussian elimination [115, 61 96], one can show that the Bjorck-Golub algorithm produces a computed fac- torization that satisfies B = C 0 P T + ffiC ffiX d 0 B d 0 d 0 f 1 1 C ffid ffiy where V = Q 1 LQ 2
Reference: [51] <author> W.M. Gentleman and H.T. Kung. </author> <title> Matrix triangularization by systolic arrays. </title> <booktitle> In Proc. SPIE Real Time Signal Processing IV, </booktitle> <pages> pages 19-26, </pages> <address> Belling-ham, WA, </address> <year> 1981. </year> <pages> SPIE. </pages>
Reference-contexts: Both of the stability 25 analyses [5, 49] depend upon the ability to rescale. Lawson et al. [81] show that this rescaling problem reduces the observed speedup over standard Gviens rotations to about 1.4-1.6 for sufficiently large problems. Givens rotations have been very popular for systolic algorithms (cf. <ref> [51, 66] </ref>), due to the fact that they can easily be implemented in hardware. To reduce the device area in VLSI implementations of Givens rotations, Barlow and Ipsen [11] proposed an implementation that involve no square roots, only one division, and that is automatically scaled.
Reference: [52] <author> J.A. George and E. Ng. SPARSPAK: </author> <title> Waterloo sparse matrix package user's guide for SPARSPAK-B. </title> <institution> Research Report CS-84-37, Department of Computer Science, University of Waterloo, Waterloo, </institution> <address> Ontario, Canada, </address> <month> November </month> <year> 1984. </year>
Reference-contexts: Barlow and Vemulapati [14, 111] give an algorithm to find the rank of C that allows us to use static storage allocation as is done in packages such as SPARSPAK-B <ref> [52] </ref>. An entirely different approach to this problem is given by Bischof, Lewis, and Pierce [17, 94]. These approaches are based on ideas due to Foster [44, 45], Chan [28], and Bischof [15]. <p> The methods discussed here do not quite achieve this ideal, but are almost always close enough to it to obtain "reasonable" answers to (5.1)-(5.2). All of these methods are amenable for use with standard sparse matrix packages such as SPARSPAK-B <ref> [52] </ref>. We discuss four methods in this survey paper: 1. The direct elimination method of Bjorck and Golub [22]; 2. The method of simply solving (5.7)-(5.8) by orthogonal factorization; 3. A deferred correction procedure due to Van Loan [82] that makes the method (5.7)-(5.8) more accurate; 4. <p> Method (2) is for the WLS problem. Methods (3) and (4) have obvious analogous versions for the WLS problem. We note that two well-known least squares packages handle constrained problems: SPARSPAK-B <ref> [52] </ref> developed by George et al. at Waterloo and Oak Ridge and LSQR developed by Paige and Saunders [90, 91, 92] at Stanford. SPARSPAK-B is a package for direct solution and implements a method by Bjorck [19] for (5.1)-(5.2) when both C and X have sparse and dense rows. <p> The rank detection strategy of Barlow and Vemulapati [14] insures a non-singular U 1 and can be done with static storage allocation. However, the factor ^o may not be bounded. Foster [45] shows that the rank detection strategy in SPARSPAK-B <ref> [52, 65] </ref> can make ^o large in practice. The deferred correction strategy in the next section allows us to be less concerned about the value of ^o as long as the rank is detected accurately. The Bjorck-Golub algorithm is closely related to the weighting procedure.
Reference: [53] <author> W. </author> <title> Givens. Computation of plane unitary rotations transforming a general matrix to triangular form. </title> <journal> SIAM J. Appl. Math., </journal> <volume> 6 </volume> <pages> 26-50, </pages> <year> 1958. </year>
Reference-contexts: In the next subsection, we give another popular approach. 2.2.2 The Givens Orthogonal Factorization We now consider another algorithm for computing the orthogonal factorization of X. It is based upon plane rotations. They are called Givens rotations after W. Givens <ref> [53] </ref> who first used them in the solution of linear equations. They are originally due to Jacobi [74].
Reference: [54] <author> L.J. Gleser. </author> <title> Estimation in a multivariate "errors in variables" regression model: Large sample results. </title> <journal> Ann. Statist., </journal> <volume> 9 </volume> <pages> 24-44, </pages> <year> 1981. </year>
Reference-contexts: algebra terms, this can be written as the problem min y+e2Range (X+F ) k (F e) k F E 2 &lt; nfip e 2 &lt; p (3.6) From the statistical theory of the problem (3.5) k k F could be replaced by k k 2 or any orthogonally invariant norm <ref> [54] </ref>. The problem 3.6 is called the total least squares (TLS) problem. Unfortunately, the existence theory of the TLS problem is more complicated than that for the LS problem. <p> In that case, we choose a "minimal norm" solution. Choose B so that k B k 2 22 k 2 1 2 2 is a minimum. It is possible that V 22 is singular, but that is an unlikely event as shown in the following theorem by Gleser <ref> [54] </ref>. Theorem 3.7 Let V be as defined in Theorem 3.3. Let the columns of (E F ) be independent and identically distribued with mean 0 and covariance matrix = 2 I p+k .
Reference: [55] <author> G.H. Golub. </author> <title> Numerical methods for solving linear least squares problems. </title> <journal> Numerische Mathematik, </journal> <volume> 7 </volume> <pages> 206-216, </pages> <year> 1965. </year>
Reference-contexts: Equation (2.11) is equivalent to the statement that X T ~e = 0: (2.12) If we combine (2.11) and (2.12), we obtain the linear system of equations X T 0 ~e 0 : (2.13) This formulation is due to Golub <ref> [55] </ref>. Equaton (2.13) is often used to un-derstand numerical properties of least squares problems, and in the design of iterative improvement procedures (cf. [18]). <p> Thus the value of ^ fi can be obtained by back substitution. The residual vector ~e can be recovered from ~e = Q 0 g 2 : (2.40) These steps are summarized in the following computational algorithm for solving (2.5). The algorithm is due to Golub <ref> [55] </ref>. Algorithm 2.1 (Householder-Golub Orthogonal Factorization) The following algorithm computes the factorization of an nfip matrix X into an n fi n orthogonal matrix Q and a p fi p upper triangular matrix R. The matrix X is assumed to have full column rank.
Reference: [56] <author> G.H. Golub and W.M. Kahan. </author> <title> Calculating the singular values and pseu-doinverse of a matrix. </title> <journal> SIAM J. Num. Anal. Ser. B, </journal> <volume> 2 </volume> <pages> 205-224, </pages> <year> 1965. </year>
Reference-contexts: Then k ~ fi (k) ^ fi (k) k 2 1 1 k e k 2 : Thus, another advantage is that judicious truncations of the SVD produces a well-conditioned problem. 3.1.4 Algorithms for Computing the SVD The standard algorithm for computing the singular value decomposition is the Golub-Kahan <ref> [56] </ref> procedure. The procedure is given in detail by Golub and Van Loan [57, Chapter 8]. It is based upon the QR algorithm of Francis [46] (for the eigenvalues of a general matrix) with Wilkinson's [117] enhancements for symmetric matrices. This is the algorithm implemented as the LINPACK routing SSVDC.
Reference: [57] <author> G.H. Golub and C.F. Van Loan. </author> <title> Matrix Computations, Second Edition. </title> <publisher> The Johns Hopkins Press, </publisher> <address> Baltimore, </address> <year> 1989. </year>
Reference-contexts: The operations addition, subtraction, multiplication, division, and square root are not exact. Their results are rounded to machine representable numbers. Thus even ~g, the approximate function will be approximated by a machine version g. 2 Input or representation errors can be resolved by the theory of conditioning <ref> [57, 101, 104] </ref>. However, all of the problems considered in this article are linear, thus approximation of general non-linear functions g is not important except for iterative methods. One iterative is described in section five. <p> Moreover, equation (2.13) makes an important point about the least squares problem (2.5). It should be considered as an equation in both ~ fi and ~e. For a discussion of (2.13), see <ref> [57, Chapter 5] </ref> or [20]. <p> The normal equations method satisfies no such bound. Algorithm 2.1 requires p 2 (n p=3) + O (p 2 + np) flops where a flop is defined as a multiplication and an addition. From <ref> [57, p.228] </ref>, the computed solution fi and e = (y + ffiy) (X + ffiX) fi satisfy k fi ^ fi k 2 * 2 + tan 2 + O (* 2 ) k y k 2 *f1 + 2g minf1; n pg + O (* 2 ) where =k X <p> These equations, given by (2.14)-(2.15), are symmetric and positive definite if X has full column rank. We solve (2.14)-(2.15) using the Cholesky factorization of A given by A = X T X = R T R (2.54) for the upper triangular factor R. In <ref> [57, p.141-149] </ref> (or [80, pp.122-129] or [101, pp.139-148] ) an algorithm is given to compute this factorization. We then solve R T R ^ fi = X T y (2.55) using forward and back substitution. <p> v n ) 2 &lt; nfin such that U T XV = diag ( 1 ; 2 ; : : : ; m ) = where 1 2 : : : m 0 m = minfn; pg: 28 The proof of this theorem is given in Golub and Van Loan <ref> [57, pp.16-17] </ref>.For our purposes, it will be useful to write the SVD of X in the form X = U V T or alternatively as X = i=1 i m = minfn; pg: The SVD is intimately connected with the detection of rank. <p> The procedure is given in detail by Golub and Van Loan <ref> [57, Chapter 8] </ref>. It is based upon the QR algorithm of Francis [46] (for the eigenvalues of a general matrix) with Wilkinson's [117] enhancements for symmetric matrices. This is the algorithm implemented as the LINPACK routing SSVDC. The algorithm is quite complicated and is explained in detail there. <p> If a minimizing ( ~ F ~e) can be found then any ~ fi satisfying (X + ~ F ) ~ fi = y + ~e is a TLS solution. However, the following example from <ref> [57, Chapter 12] </ref> has no TLS solution. Example 3.1 X = @ 0 0 1 0 1 1 A ; F * = @ 0 * 1 then for all * &gt; 0, b 2 Range (X + F * ). <p> This is an upper bound based upon the standard bounds from Chebyshev polynomials <ref> [57, p.371] </ref>,[60, p.148]. These bounds are considered quite conservative. In practice, the conjugate gradient method is often much faster than that. Also, from a result due to Concus et al. [34], the iteration must terminate in 1+rank (Y ) iterations.
Reference: [58] <author> J. Gotze and U. Schweiglelshohn. </author> <title> A square root and division free Givens rotation for solving least squares problems on systolic arrays. </title> <journal> SIAM J. Sci. Stat. Computing, </journal> <volume> 12 </volume> <pages> 800-807, </pages> <year> 1991. </year>
Reference-contexts: To reduce the device area in VLSI implementations of Givens rotations, Barlow and Ipsen [11] proposed an implementation that involve no square roots, only one division, and that is automatically scaled. Gotze and Schweigelshohn <ref> [58] </ref> recently showed how to to defer the division until the very end of the factorization (and even then they are only needed to compute the residual), thus creating a Q-R factorization that can be implemented in hardware using only additions and multiplications.
Reference: [59] <author> M. Gu and S.C. Eisenstat. </author> <title> A stable and efficient algorithm for the rank-one modification of the symmetric eigenproblem. </title> <type> Technical Report YALEU/DCS/RR-916, </type> <institution> Department of Computer Science, Yale University, </institution> <address> New Haven, CT, </address> <month> September </month> <year> 1992. </year> <month> 74 </month>
Reference-contexts: Some of the implementation issues of this procedure in floating point arithmetic are discussed by Sorensen and Tang [99] and error analysis results for such algorithms are given by Barlow [7]. A faster implementation of this procedure was recently given by Gu and Eisenstat <ref> [59] </ref>. 3.1.5 The role of the C-S Decomposition An important and very useful singular value problem is that of computing the C-S decomposition. It is given as follows. Suppose that Q = Q 11 Q 12 is unitary and thus satisfies Q H Q = I.
Reference: [60] <author> L.A. Hageman and D.M. Young. </author> <title> Applied Iterative Methods. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1981. </year>
Reference: [61] <author> W.W. Hager. </author> <title> Condition estimates. </title> <journal> SIAM J. Sci. Stat. Computing, </journal> <volume> 5 </volume> <pages> 311-316, </pages> <year> 1984. </year>
Reference-contexts: y k 1 (4.28) where y = (y 1 ; : : : ; y p ) T y i = sign (r 1 ij ) i = 1; 2; : : : ; p: (4.29) Here j is the same index as that in (4.26) as discussed by Hager <ref> [61] </ref>. Note that k y k 1 = 1. <p> There are 2 p1 possibilities. Hager <ref> [61] </ref> discusses the above problem as a discrete optimization problem and uses it to construct a very robust condition estimation. Higham [68] showed how this estimator could be generalized to estimate the condition of a large class of matrix functions.
Reference: [62] <author> S. Hammarling. </author> <title> A note on modifications to the Givens plane rotation. </title> <journal> J. Inst. Math. Appl., </journal> <volume> 13 </volume> <pages> 215-218, </pages> <year> 1974. </year>
Reference-contexts: Also, Givens rotations can be implemented in ways that save about half of the multiplications and thus perform the Q-R factorization in about as many operations as the Householder-Golub method. Such implementations have been proposed by Gentleman [49], Hammarling <ref> [62] </ref>, and Bareiss [3]. All of these implementations could be used for sparse matrix computations without great difficulty. All of these algorithms are square root free. However, since a square root requires about as much time as a division, this is not an important issue.
Reference: [63] <author> P.C. Hansen. </author> <title> Truncated singular value decomposition solutions to discrete ill-posed problems with ill-determined numerical rank. </title> <journal> SIAM J. Sci. Stat. Computing, </journal> <volume> 11 </volume> <pages> 503-518, </pages> <year> 1990. </year>
Reference-contexts: How- ever, in such problems an accurate ^ fi is usually too much to ask for. Using ideas from <ref> [63] </ref> we show that if we assume that ju T i ff &gt; 1; i = k + 1; : : : ; m (3.2) then the approximation ^ fi (k) to ^ fi obtained by the truncated SVD. Hansen calls this the discrete Picard condition. <p> Also, the truncated SVD solution will be better conditioned than the solution of (2.5). The following result from Hansen <ref> [63] </ref> gives a perturbation bound. Theorem 3.5 Let ^ fi (k) denote the solution (3.1) and let ~ fi (k) denote the solution with y perturbed by e.
Reference: [64] <author> R.J. Hanson. </author> <title> Stably updating mean and standard deviation of data. </title> <journal> Comm. Assoc. Comput. Mach., </journal> <volume> 18 </volume> <pages> 57-58, </pages> <year> 1975. </year>
Reference-contexts: The best bound that can be obtained for the relative error in the computed value ~ S 2 is j ~ S Sj ug (n) 2 + O (u 2 ) (1.44) where g (n) satisfies (1.38). To avoid this problem, several authors <ref> [119, 114, 64] </ref> derived stable algorithms for computing the sample variance using only one pass through the data. In [29], it was pointed out that all of these algorithms were effectively equivalent. We now give the version due to West [114].
Reference: [65] <author> M.T. Heath. </author> <title> Some extensions of an algorithm for sparse linear least squares problems. </title> <journal> SIAM J. Sci. Stat. Computing, </journal> <volume> 3 </volume> <pages> 223-237, </pages> <year> 1982. </year>
Reference-contexts: All solutions of (4.1) are of the form ~ fi = ~ fi B + z (4.13) where ~ fi is defined in (2.3)-(2.4) and z satisfies Xz = 0 (4.14) and is thus a vector from the null space of X. From <ref> [65] </ref>, a basis matrix N for the null space of X is given by N = P R 1 I pl : (4.15) Thus the minimum length solution ^ fi is given by ^ fi = ~ fi B + N ^w (4.16) where ^w is (p l)-vector satisfying k ~ <p> The rank detection strategy of Barlow and Vemulapati [14] insures a non-singular U 1 and can be done with static storage allocation. However, the factor ^o may not be bounded. Foster [45] shows that the rank detection strategy in SPARSPAK-B <ref> [52, 65] </ref> can make ^o large in practice. The deferred correction strategy in the next section allows us to be less concerned about the value of ^o as long as the rank is detected accurately. The Bjorck-Golub algorithm is closely related to the weighting procedure.
Reference: [66] <author> D.E. Heller and I.C.F. Ipsen. </author> <title> Systolic networks for orthogonal decomposition. </title> <journal> SIAM J. Sci. Stat. Computing, </journal> <volume> 4 </volume> <pages> 261-269, </pages> <year> 1983. </year>
Reference-contexts: Both of the stability 25 analyses [5, 49] depend upon the ability to rescale. Lawson et al. [81] show that this rescaling problem reduces the observed speedup over standard Gviens rotations to about 1.4-1.6 for sufficiently large problems. Givens rotations have been very popular for systolic algorithms (cf. <ref> [51, 66] </ref>), due to the fact that they can easily be implemented in hardware. To reduce the device area in VLSI implementations of Givens rotations, Barlow and Ipsen [11] proposed an implementation that involve no square roots, only one division, and that is automatically scaled.
Reference: [67] <author> N.J. Higham. </author> <title> Analysis of the choleski decomposition of a semi-definite matrix. Numerical Analysis Report 128, </title> <institution> Department of Mathematics, University of Manchester, </institution> <address> Manchester, England, </address> <month> January </month> <year> 1987. </year>
Reference-contexts: Otherwise ^o = o . Here OE C and OE X are modestly sized functions in n = n 1 + n 2 and p. The factor ^o is important in the accurate solution of the WLS problem by orthogonal factorization. For maximal column pivoting, from tests by Higham <ref> [67] </ref>, the factor ^o seldom exceeds 10. The same seems to hold for the compromise pivoting strategy by Barlow and Handy [10]. For sparse problems, maximal column pivoting tends to require too much time and storage and neither strategy can be done without dynamic storage allocation.
Reference: [68] <author> N.J. Higham. </author> <title> A survey of condition number estimation for triangular matrices. </title> <journal> SIAM Review, </journal> <volume> 29 </volume> <pages> 575-598, </pages> <year> 1987. </year>
Reference-contexts: Thus we try to find a vector v such that k R 1 v k s k R 1 k s k v k s = 1: Higham <ref> [68] </ref> gives an excellent survey of various methods to solve this problem. Below we give a simple method that is easy to update if we add columns to X. Consider the estimation of the 1-norm of R 1 . <p> There are 2 p1 possibilities. Hager [61] discusses the above problem as a discrete optimization problem and uses it to construct a very robust condition estimation. Higham <ref> [68] </ref> showed how this estimator could be generalized to estimate the condition of a large class of matrix functions. Such an estimator will be used in the new matrix package LAPACK [37]. We now present a simpler estimator that is similar to the one employed in LINPACK [32, 40].
Reference: [69] <author> Y.-P. Hong and C-T. Pan. </author> <title> Rank-revealing qr factorizations and the svd. </title> <type> Technical report, </type> <institution> Department of Mathematical Sciences, Northern Illinois University, DeKalb, IL, </institution> <year> 1990. </year>
Reference-contexts: The above problem is equivalent to that of choosing a maximal independent set of variables in regression. First, we show that such an ordering is always possible. This theorem, due to Hong and Pan <ref> [69] </ref>, actually constructs the ordering, but the result is not practical since the algorithm for constructing P T is combinatorical. Still, the result is quite important, since it assures us that the goal (4.7)-(4.9) can be achieved.
Reference: [70] <author> A.S. </author> <title> Householder. Unitary triangularization of a nonsymmetric matrix. </title> <journal> J. Assoc. Comput. Mach., </journal> <volume> 5 </volume> <pages> 339-42, </pages> <year> 1958. </year>
Reference-contexts: Our methods involve constructing an orthogonal factorization for X where the orthogonal matrices are the product of orthogonal matrices of simple form. 2.2.1 The Householder-Golub Factorization The first method that we discuss for constructing orthogonal matrix are Householder transformations <ref> [70] </ref>. An n fi n Householder transformation can be represented in the form H = I n + fl 1 uu T (2.25) where u is an n-vector such that fl = k u k 2 2 =2.
Reference: [71] <author> S. Van Huffel. </author> <title> Analysis of the total least squares problem and its use in parameter estimation. </title> <type> PhD thesis, </type> <institution> Katholieke Universiteit Leuven, Department Elektrotechniek, Leuven, Belgium, </institution> <month> June </month> <year> 1987. </year>
Reference-contexts: Practical TLS problems often come close to those having nonunique solutions, creating difficulties in obtaining a meaningful solution. We give some of the results from the thesis of Van Huffel <ref> [71, 72] </ref> that compare the total least squares problem to the solution of (2.5). Since TLS requires the use of the SVD and takes little advantage of any special structure in the problem, it is computationally much less efficient than solving (2.5). <p> In many cases, the TLS solution and the LS solution will be very close together; in other cases, they will be far apart. The following further results from Van Huffel <ref> [71, 72] </ref> relate the TLS and LS solutions. First, we note that if X has full column rank then B LS = (X T X) 1 X T Y: Corollary 3.1 Assume the hypothesis of Theorem 3.8. <p> As p+1 ! 0, ~ X approaches X and thus the two solutions approach each other. We give two additional corollaries from Van Huffel <ref> [71, 72] </ref> relating the resid uals: R T LS = Y XB T LS 38 Theorem 3.9 Assume the hypothesis of Theorem 3.8. <p> Of course, the LS solution does not require the expense of computing the singular value decomposition. However, in some cases the TLS fit is much better. Examples which demonstrate that are given by Van Huffel <ref> [71, 72] </ref>. It is also appropriate to mention the restricted total least squares (RTLS) problem which is discussed in a recent paper by Van Huffel and Zha [73].
Reference: [72] <author> S. Van Huffel and J. Vandewalle. </author> <title> The Total Least Squares Problem: Computational Aspects and Analysis. </title> <publisher> SIAM Publications, </publisher> <address> Philadelphia, </address> <year> 1991. </year>
Reference-contexts: Practical TLS problems often come close to those having nonunique solutions, creating difficulties in obtaining a meaningful solution. We give some of the results from the thesis of Van Huffel <ref> [71, 72] </ref> that compare the total least squares problem to the solution of (2.5). Since TLS requires the use of the SVD and takes little advantage of any special structure in the problem, it is computationally much less efficient than solving (2.5). <p> In many cases, the TLS solution and the LS solution will be very close together; in other cases, they will be far apart. The following further results from Van Huffel <ref> [71, 72] </ref> relate the TLS and LS solutions. First, we note that if X has full column rank then B LS = (X T X) 1 X T Y: Corollary 3.1 Assume the hypothesis of Theorem 3.8. <p> As p+1 ! 0, ~ X approaches X and thus the two solutions approach each other. We give two additional corollaries from Van Huffel <ref> [71, 72] </ref> relating the resid uals: R T LS = Y XB T LS 38 Theorem 3.9 Assume the hypothesis of Theorem 3.8. <p> Of course, the LS solution does not require the expense of computing the singular value decomposition. However, in some cases the TLS fit is much better. Examples which demonstrate that are given by Van Huffel <ref> [71, 72] </ref>. It is also appropriate to mention the restricted total least squares (RTLS) problem which is discussed in a recent paper by Van Huffel and Zha [73].
Reference: [73] <author> S. Van Huffel and H. Zha. </author> <title> An efficient total least squares algorithms based on a rank-revealing two-sided orthogonal decomposition. </title> <note> Research Report ESAT-SISTA 1991-08, </note> <institution> Department of Electrical Engineering, Katholieke Universiteit Leuven, </institution> <month> May </month> <year> 1991. </year> <month> 75 </month>
Reference-contexts: However, in some cases the TLS fit is much better. Examples which demonstrate that are given by Van Huffel [71, 72]. It is also appropriate to mention the restricted total least squares (RTLS) problem which is discussed in a recent paper by Van Huffel and Zha <ref> [73] </ref>. That problem has the form min Range (Y +Y )Range (X+X) k DEC k F where D 2 &lt; nfiq 1 ,C 2 q 2 fi (p + k) are known matrices while E 2 &lt; q 1 fiq 2 is unknown. <p> The formulation of the RSVD is due to Zha [120] and its relation to the RTLS problem is detailed by Van Huffel and Zha <ref> [73] </ref>.
Reference: [74] <editor> C.G.J. Jacobi. Uber ein Leichtes Verfahren Die in der Theorie der Sacularstorungen Vorkommendern Gleichungen Numerisch Aufzulosen. Crelle's J., </editor> <volume> 30 </volume> <pages> 51-94, 1848. </pages>
Reference-contexts: It is based upon plane rotations. They are called Givens rotations after W. Givens [53] who first used them in the solution of linear equations. They are originally due to Jacobi <ref> [74] </ref>.
Reference: [75] <author> D. James. </author> <title> Conjugate Gradient Methods for Constrained Least Squares Problems. </title> <type> PhD thesis, </type> <institution> North Carolina State University, Department of Mathematics, </institution> <address> Raleigh, NC, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: One can also pose (5.45) as the least squares problem I e 1 = A 2 A 1 0 : (5.46) Using this formulation, the least squares package LSQR could be directly applied to obtain our iteration. An efficient implementation of this iteration was developed by James <ref> [76, 75] </ref>. The actual form of the algorithm is given below. The explicit formulation of (5.45) or (5.46) is not necessary. Algorithm 5.3 (Iterative LSE Procedure) Phase One. Initialization Steps. Factor A 1 and choose fi (0) so that A 1 fi (0) = b 1 . <p> The matrices J and J actually have the form J = ^ P 0 0 I pn 1 : These two matrices are just for bookkeeping. The subtle implementation problems for this method are discussed by James <ref> [76, 75] </ref>. He tests the algorithm on structural engineering and Stokes flow problems which can be formulated as LSE problems [105]. <p> Finite termination occurs at 9,800 iterations for this problem. The reductions of C and X described above required only one to two percent of the processing time necessary for all of the James test problems. Details of these problems and results are given by James <ref> [76, 75] </ref>. These results indicate that Algorithm 5.1 can be very reasonable for large sparse constrained least squares problems. It avoids most of the storage problems of the direct approaches, although as is always true with iterative methods, one has to settle for a less accurate solution.
Reference: [76] <author> D. James. </author> <title> Implicit nullspace iterative methods for constrained least squares problems. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 13 </volume> <pages> 962-978, </pages> <year> 1992. </year>
Reference-contexts: The three direct procedures described here can easily be implemented using SPARSPAK-B. LSQR is an iterative package that uses the Lanczos algorithm. The iterative scheme in this article is easy to implement using LSQR, although the best implementation of it to date is due to James <ref> [76] </ref>. 58 5.2 Direct Elimination of the Constraint Matrix This method is very closely related to Gaussian elimination for square matrices. We consider a factorization of the constraint matrix C given by (5.5). <p> One can also pose (5.45) as the least squares problem I e 1 = A 2 A 1 0 : (5.46) Using this formulation, the least squares package LSQR could be directly applied to obtain our iteration. An efficient implementation of this iteration was developed by James <ref> [76, 75] </ref>. The actual form of the algorithm is given below. The explicit formulation of (5.45) or (5.46) is not necessary. Algorithm 5.3 (Iterative LSE Procedure) Phase One. Initialization Steps. Factor A 1 and choose fi (0) so that A 1 fi (0) = b 1 . <p> The matrices J and J actually have the form J = ^ P 0 0 I pn 1 : These two matrices are just for bookkeeping. The subtle implementation problems for this method are discussed by James <ref> [76, 75] </ref>. He tests the algorithm on structural engineering and Stokes flow problems which can be formulated as LSE problems [105]. <p> Finite termination occurs at 9,800 iterations for this problem. The reductions of C and X described above required only one to two percent of the processing time necessary for all of the James test problems. Details of these problems and results are given by James <ref> [76, 75] </ref>. These results indicate that Algorithm 5.1 can be very reasonable for large sparse constrained least squares problems. It avoids most of the storage problems of the direct approaches, although as is always true with iterative methods, one has to settle for a less accurate solution.
Reference: [77] <author> E. Jessup and D.C. Sorensen. </author> <title> A parallel algorithm for computing the singular value decomposition of a matrix. </title> <type> Tech. Rep. </type> <institution> TM-102, Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, IL, </institution> <year> 1987. </year>
Reference-contexts: There are two alternatives to the Golub-Kahan SVD algorithm that should be mentioned:(1) the Jacobi SVD algorithm and (2) the divide-and-conquer 33 SVD algorithm of Jessup and Sorensen <ref> [77] </ref> The Jacobi algorithm has become important with the advent of parallel processing. Although the algorithm tends to be slower on conventional one processor computers, it can be faster on high performance computers with many processors. Veselic' and Demmel [39] proved that the Jacobi algorithm is more accurate.
Reference: [78] <author> W. Kahan. </author> <title> Numerical linear algebra. </title> <journal> Canad. Math. Bull., </journal> <volume> 9 </volume> <pages> 757-801, </pages> <year> 1966. </year>
Reference-contexts: The converse of that statement is false. That is, if X satisfies (4.7) for some ~ X such that rank ( ~ X) = l, then Algorithm 4.1 will not necessarily conclude that X has rank l. It could overestimate the rank. The following example due to Kahan <ref> [78] </ref> demonstrates the potential weakness of Algorithm 4.1. Example 4.1 X = diag (1; s; : : : ; s p1 ) 0 B B B 1 c c 1 c 1 C C C where s 2 + c 2 = 1. Algorithm 4.1 leaves the matrix undisturbed.
Reference: [79] <author> U.W. Kulisch and W.L. Miranker. </author> <title> Computer Arithmetic in Theory and Practice. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1981. </year>
Reference-contexts: One iterative is described in section five. We discuss two methods for analyzing rounding errors:(1) forward error analysis and (2) backward error analysis. These are the two types that are most commonly discussed in the literature, although other formalisms have been proposed (cf. <ref> [24, 4, 88, 118, 79, 86] </ref> ).
Reference: [80] <editor> C.L. Lawson and R.J. Hanson. </editor> <title> Solving Least Squares Problems. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliff, NJ, </address> <year> 1974. </year>
Reference-contexts: ~e satisfying ~e = y X ~ fi = H 0 where k ~e k 2 =k y X ~ fi k 2 =k g 2 k 2 . * The unique solution of minimum two-norm is ^ fi = K z 1 16 The proof of this theorem in <ref> [80] </ref>. Note that the residual ~e is unique even though ~ fi is not. The Moore-Penrose pseudoinverse of X can be readily obtained from this decomposition. <p> (y + ffiy) k 2 (2.41) where the perturbation can be bounded in norm by k ffiX k F c 1 k X k F ; k ffiy k 2 c 1 k y k 2 ; c 1 = (6n 3p + 41)p: (2.42) These bounds are discussed in <ref> [80, p.30] </ref>. The normal equations method satisfies no such bound. Algorithm 2.1 requires p 2 (n p=3) + O (p 2 + np) flops where a flop is defined as a multiplication and an addition. <p> These equations, given by (2.14)-(2.15), are symmetric and positive definite if X has full column rank. We solve (2.14)-(2.15) using the Cholesky factorization of A given by A = X T X = R T R (2.54) for the upper triangular factor R. In [57, p.141-149] (or <ref> [80, pp.122-129] </ref> or [101, pp.139-148] ) an algorithm is given to compute this factorization. We then solve R T R ^ fi = X T y (2.55) using forward and back substitution. <p> As stated in the previous section, the upper triangular factor R is the same as that for the orthogonal factorization of X except for the signs of the rows. Lawson and Hanson <ref> [80] </ref> point out that if we extend the Cholesky factorization to the matrix ~ A = A f then ~ A = ~ R T ~ R (2.57) where ~ R is given by ~ R = R ~ f 26 Here the vector ~ f and the scalar ae satisfy <p> The other approach is to compute the orthogonal factorization R T = ~ K ~ R T where ~ K is a p fi p orthogonal matrix and ~ R T is lower triangular. Lawson and Hanson <ref> [80] </ref> describe a procedure for computing ~ K as a product of Householder transformations. The information for constructing X, ~ R, and K can all be included in the storage for X plus two extra storage vectors. <p> We can then use Theorem 2.3 to obtain the unique minimum length solution of (2.5). The procedure is implemented in the Lawson-Hanson least squares package as the routine HFTI <ref> [80, Chapter 14] </ref>. There are several heuristics that produce a "good" column ordering P T . The one used in the Lawson-Hanson least squares package and in LINPACK [40] is maximal column pivoting. <p> Usually, the column norms are recomputed under such circumstances. One heuristic ( similar to that in <ref> [80, Chapter 14] </ref>) is to recompute i (k) if k+1jp (k) 1 kjp (k1) Then i (k) j is recomputed from the entries of X (k) . This heuristic will not prevent all loss of precision in calculation of the column norms, but it will prevent much of it. <p> Other applications include the optimal design of structure [95], signal processing (i.e., signal estimation )[47], and as a step in solving linear inequality constrained least squares problems <ref> [80, Chapter 24] </ref>. We consider also the related least squares problem of finding the p-vector ^ fi (o ) such that A o ^ fi (o ) = b o (5.7) where A o = o C y : (5.8) Here o is some very large value.
Reference: [81] <author> C.L. Lawson, R.J. Hanson, D.R. Kincaid, and F.T. Krogh. </author> <title> Basic linear algebra subprograms for FORTRAN usage. </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 5 </volume> <pages> 308-325, </pages> <year> 1979. </year>
Reference-contexts: Gentleman's [49] method shows how to implement some of these "fast Givens" rotations in a stable fashion, while also exhibiting some unstable implementations. Barlow [5] shows that Bareiss's method is stable. Both of the stability 25 analyses [5, 49] depend upon the ability to rescale. Lawson et al. <ref> [81] </ref> show that this rescaling problem reduces the observed speedup over standard Gviens rotations to about 1.4-1.6 for sufficiently large problems. Givens rotations have been very popular for systolic algorithms (cf. [51, 66]), due to the fact that they can easily be implemented in hardware.
Reference: [82] <author> C.F. Van Loan. </author> <title> On the method of weighting for equality-constrained least-squares problems. </title> <journal> SIAM J. Num. Anal., </journal> <volume> 22 </volume> <pages> 851-864, </pages> <year> 1985. </year>
Reference-contexts: We discuss four methods in this survey paper: 1. The direct elimination method of Bjorck and Golub [22]; 2. The method of simply solving (5.7)-(5.8) by orthogonal factorization; 3. A deferred correction procedure due to Van Loan <ref> [82] </ref> that makes the method (5.7)-(5.8) more accurate; 4. An iterative method due to Barlow, Nichols, and Plemmons [12]. These four methods use closely related ideas. Method (2) becomes method (1) as o ! 1. <p> However, improvements and update strategies can be more easily applied to this method. Also the WLS problem is sometimes the one whose solution is desired, as in, for example, smoothing splines [112]. 5.4 The Deferred Correction Method The deferred correction procedure due to Van Loan <ref> [82] </ref> is an attempt to improve the accuracy of the weighting method. Barlow [6] presents an error analysis of the procedure. <p> However, in [14], it is shown that for all but ill-conditioned problems, acceptable results can be obtained in two iterations with o = 1 where is the machine unit. From the results in <ref> [6, 82] </ref>, this iteration is not recommended for ill-conditioned problems. Thus two iterations are enough for many problems in which it is reasonable to use Algorithm 4.2. The proof of these results is in [6, 13]. <p> From the results in [6, 82], this iteration is not recommended for ill-conditioned problems. Thus two iterations are enough for many problems in which it is reasonable to use Algorithm 4.2. The proof of these results is in [6, 13]. It is pointed out in <ref> [82, 10, 13] </ref> that one can solve (5.28)-(5.29) using the corrected "semi-normal" equations as described by Bjorck.
Reference: [83] <author> F.T. Luk and H. Park. </author> <title> On parallel Jacobi orderings. </title> <journal> SIAM J. Sci. Stat. Comp., </journal> <volume> 10 </volume> <pages> 18-26, </pages> <year> 1989. </year>
Reference-contexts: Veselic' and Demmel [39] proved that the Jacobi algorithm is more accurate. In particular, it is shown to meet the stronger stability criterion described by Barlow and Demmel [9]. A survey and analysis of parallel implementations of the Jacobi method are given by Luk and Park <ref> [83] </ref>. The Jessup-Sorensen divide-and-conquer method is very efficient on vector multiprocessor architectures such as the Alliant FX/8. It is based upon an algorithm due to Cuppen [35] and the eigenvalue update procedure due to Bunch, Nielsen, and Sorensen [25].
Reference: [84] <author> A.A. </author> <title> Markov. </title> <publisher> Wahrscheinlinkeitsrechnung. B.G. Teubner, </publisher> <address> Leipzig, </address> <year> 1912. </year> <note> (German translation of second Russian edition). </note>
Reference-contexts: The model (2.3)-(2.4) is called the Gauss-Markov linear model [48], <ref> [84] </ref> . <p> The second important property arises out of the Gauss-Markov theorem <ref> [48, 84] </ref>. Theorem 2.2 A solution ~ fi of the least squares problem (2.5) is the best linear unbiased estimate (BLUE) of fi.
Reference: [85] <author> W. Miller and C. Wrathall. </author> <title> Software for Roundoff Analysis of Matrix Algorithms. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1980. </year>
Reference-contexts: This is about as good as can be expected from a floating point algorithm to compute (1.13). The actual bounds (1.14)-(1.15) and (1.27) are not usually important in and of themselves. As pointed out by Miller and Wrathall <ref> [85] </ref>, "the particular bound verified and the particular nature of the bound . . . are usually less important than the guarantee that some bound exists." Thus the important conclusion is that both summing algorithms are backward stable, i.e., that a reasonable backward error bound exists.
Reference: [86] <author> R.E. Moore. </author> <title> Methods and Applications of Interval Analysis. </title> <publisher> SIAM Publications, </publisher> <address> Philadelphia, </address> <year> 1979. </year>
Reference-contexts: One iterative is described in section five. We discuss two methods for analyzing rounding errors:(1) forward error analysis and (2) backward error analysis. These are the two types that are most commonly discussed in the literature, although other formalisms have been proposed (cf. <ref> [24, 4, 88, 118, 79, 86] </ref> ).
Reference: [87] <author> J.J. </author> <title> More. The Levenberg-Marquardt algorithm: implementation and theory. In G.A. </title> <editor> Watson, editor, </editor> <booktitle> Proc. Dundee Conference on Numerical Analysis, </booktitle> <address> Berlin, 1978. </address> <publisher> Springer-Verlag. </publisher> <pages> 76 </pages>
Reference-contexts: We call (5.7)-(5.8) the WLS (Weighted Least Squares) problem. This problem arises on its own in the calculation of smoothing splines [112], in trust region methods for nonlinear least squares <ref> [87] </ref>, in the solution of Fredholm integral equations of the first kind [43, 30], and Ridge regression. If rank (C) = n 1 , we have that lim ^ fi (o ) = ^ fi LSE (5.9) where ^ fi LSE is the solution of (5.1)-(5.2).
Reference: [88] <author> F.W.J. Olver. </author> <title> A new approach to error arithmetic. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 15 </volume> <pages> 368-393, </pages> <year> 1978. </year>
Reference-contexts: One iterative is described in section five. We discuss two methods for analyzing rounding errors:(1) forward error analysis and (2) backward error analysis. These are the two types that are most commonly discussed in the literature, although other formalisms have been proposed (cf. <ref> [24, 4, 88, 118, 79, 86] </ref> ).
Reference: [89] <author> C.C. Paige. </author> <title> Computing the generalized singular value decomposition. </title> <journal> SIAM J. Sci. Stat. Computing, </journal> <volume> 10 </volume> <pages> 1126-1146, </pages> <year> 1986. </year>
Reference-contexts: Excluding numerical difficulties, the C-S decomposition could be computed by a sequence of calls to the LINPACK [40] SVD routine. However, one cannot expect to obtain good results this way. Practical algorithms are given by Stewart [103], Van Loan [106], and Paige <ref> [89] </ref>. Modifications of Paige's routine are being considered for LAPACK [1]. The direction-of-arrival (DOA) problem is an important estimation problem in signal processing. The two most commonly used algorithms for that problem are the MUSIC algorithm of Schmidt [98] and the ESPRIT algorithm of Roy et 34 al. [97].
Reference: [90] <author> C.C. Paige and M.A. Saunders. </author> <title> A bidiagonalization algorithm for sparse linear equations and sparse least squares. </title> <type> Report SOL 78-19, </type> <institution> Department of Operations Research, Stanford University, Stanford, </institution> <address> CA, </address> <year> 1975. </year>
Reference-contexts: Method (2) is for the WLS problem. Methods (3) and (4) have obvious analogous versions for the WLS problem. We note that two well-known least squares packages handle constrained problems: SPARSPAK-B [52] developed by George et al. at Waterloo and Oak Ridge and LSQR developed by Paige and Saunders <ref> [90, 91, 92] </ref> at Stanford. SPARSPAK-B is a package for direct solution and implements a method by Bjorck [19] for (5.1)-(5.2) when both C and X have sparse and dense rows. The three direct procedures described here can easily be implemented using SPARSPAK-B.
Reference: [91] <author> C.C. Paige and M.A. Saunders. </author> <title> Algorithm 583 LSQR:Sparse linear equations and least squares problems. </title> <journal> ACM Trans. on Math. Software, </journal> <volume> 8 </volume> <pages> 195-209, </pages> <year> 1982. </year>
Reference-contexts: Method (2) is for the WLS problem. Methods (3) and (4) have obvious analogous versions for the WLS problem. We note that two well-known least squares packages handle constrained problems: SPARSPAK-B [52] developed by George et al. at Waterloo and Oak Ridge and LSQR developed by Paige and Saunders <ref> [90, 91, 92] </ref> at Stanford. SPARSPAK-B is a package for direct solution and implements a method by Bjorck [19] for (5.1)-(5.2) when both C and X have sparse and dense rows. The three direct procedures described here can easily be implemented using SPARSPAK-B.
Reference: [92] <author> C.C. Paige and M.A. Saunders. </author> <title> LSQR:An algorithm for sparse linear equations and least squares problems. </title> <journal> ACM Trans. on Math. Software, </journal> <volume> 8 </volume> <pages> 43-71, </pages> <year> 1982. </year>
Reference-contexts: Method (2) is for the WLS problem. Methods (3) and (4) have obvious analogous versions for the WLS problem. We note that two well-known least squares packages handle constrained problems: SPARSPAK-B [52] developed by George et al. at Waterloo and Oak Ridge and LSQR developed by Paige and Saunders <ref> [90, 91, 92] </ref> at Stanford. SPARSPAK-B is a package for direct solution and implements a method by Bjorck [19] for (5.1)-(5.2) when both C and X have sparse and dense rows. The three direct procedures described here can easily be implemented using SPARSPAK-B.
Reference: [93] <author> B.N. Parlett. </author> <title> Analysis of algorithms for reflectors in bisectors. </title> <journal> SIAM Review, </journal> <volume> 13 </volume> <pages> 197-208, </pages> <year> 1971. </year>
Reference-contexts: Thus we always choose s to have the opposite sign of v m . Parlett <ref> [93] </ref> shows how we can choose s so that sign (s) = sign (v m ), and still have a stable procedure.
Reference: [94] <author> D.J. Pierce and J.G. Lewis. </author> <title> Sparse rank revealing QR factorization. </title> <type> Technical Report MEA-TR-193, </type> <institution> Mathematics and Engineering Analysis, Boe-ing Computing Services, </institution> <address> Seattle, WA, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: Its primary purpose is to avoid the dynamic reallocation of storage that would be required for maximum column pivoting. That issue is explained therein [14]. A different approach to this problem is given by Bischof, Pierce, and Lewis <ref> [17, 94] </ref>. It can also be used as a preprocessing routine for a rank revealing orthogonal factorization. <p> Barlow and Vemulapati [14, 111] give an algorithm to find the rank of C that allows us to use static storage allocation as is done in packages such as SPARSPAK-B [52]. An entirely different approach to this problem is given by Bischof, Lewis, and Pierce <ref> [17, 94] </ref>. These approaches are based on ideas due to Foster [44, 45], Chan [28], and Bischof [15]. The classical statistical context for the LSE is in constrained linear models, i.e., a data fitting problem where some of the function values must be fit exactly.
Reference: [95] <author> J.S. Przemienicki. </author> <title> Theory of Matrix Structural Analysis. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1968. </year>
Reference-contexts: The classical statistical context for the LSE is in constrained linear models, i.e., a data fitting problem where some of the function values must be fit exactly. Other applications include the optimal design of structure <ref> [95] </ref>, signal processing (i.e., signal estimation )[47], and as a step in solving linear inequality constrained least squares problems [80, Chapter 24].
Reference: [96] <author> J.K. Reid. </author> <title> A note on the stability of Gaussian elimination. </title> <journal> J. Inst. Math. Appl., </journal> <volume> 8 </volume> <pages> 374-375, </pages> <year> 1971. </year>
Reference: [97] <author> R. Roy, A. Paulraj, and T. Kailath. </author> <title> Estimation of signal parameters via rotational invariance techniques. </title> <editor> In J.M. Speiser, editor, </editor> <booktitle> Advance Algorithms and Architectures for Signal Processing, </booktitle> <pages> pages 94-102, </pages> <address> San Diego, 1986. </address> <publisher> SPIE Publications. </publisher>
Reference-contexts: Modifications of Paige's routine are being considered for LAPACK [1]. The direction-of-arrival (DOA) problem is an important estimation problem in signal processing. The two most commonly used algorithms for that problem are the MUSIC algorithm of Schmidt [98] and the ESPRIT algorithm of Roy et 34 al. <ref> [97] </ref>. Both have parameter estimation problems that can be solved by the C-S decomposition. In the MUSIC algorithm, it is necessary to compute an orthonormal basis associated with minimum generalized singular value of the pair (A; B). <p> The C-S decomposition in its complete form is needed in the more sophisticated ESPRIT algorithm of Roy et al. <ref> [97] </ref>. The problem that it solves is formulated as follows.
Reference: [98] <author> R.O. Schmidt. </author> <title> A signal subspace approach to multiple emitter location and spectral estimation. </title> <type> PhD thesis, </type> <institution> Stanford University, Department of Electrical Engineering, Stanford, </institution> <address> CA, </address> <year> 1981. </year>
Reference-contexts: Modifications of Paige's routine are being considered for LAPACK [1]. The direction-of-arrival (DOA) problem is an important estimation problem in signal processing. The two most commonly used algorithms for that problem are the MUSIC algorithm of Schmidt <ref> [98] </ref> and the ESPRIT algorithm of Roy et 34 al. [97]. Both have parameter estimation problems that can be solved by the C-S decomposition. In the MUSIC algorithm, it is necessary to compute an orthonormal basis associated with minimum generalized singular value of the pair (A; B).
Reference: [99] <author> D.C. Sorensen and P.T. Tang. </author> <title> On the orthogonality of eigenvectors computed by the divide-and-conquer techniques. </title> <journal> SIAM J. Num. Anal., </journal> <volume> 28 </volume> <pages> 1752-1775, </pages> <year> 1991. </year>
Reference-contexts: It is based upon an algorithm due to Cuppen [35] and the eigenvalue update procedure due to Bunch, Nielsen, and Sorensen [25]. Some of the implementation issues of this procedure in floating point arithmetic are discussed by Sorensen and Tang <ref> [99] </ref> and error analysis results for such algorithms are given by Barlow [7]. A faster implementation of this procedure was recently given by Gu and Eisenstat [59]. 3.1.5 The role of the C-S Decomposition An important and very useful singular value problem is that of computing the C-S decomposition.
Reference: [100] <author> G.W. Stewart. </author> <title> Unpublished manuscript on statistics and matrix computations. Untitled, </title> <note> made available to author in 1989. </note>
Reference-contexts: These operations require much less time and device area than square roots and divisions. Many of the ideas for the development of these two subsections came from an unpublished manuscript by Stewart <ref> [100] </ref> and the long paper on least squares computations by Bjorck [20]. The latter gives much more detail than is given here. 2.3 The Normal Equations The normal equations is the most commonly used method for solving the least squares problem (2.5).
Reference: [101] <author> G.W. Stewart. </author> <title> Introduction to Matrix Computations. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1973. </year> <month> 77 </month>
Reference-contexts: The operations addition, subtraction, multiplication, division, and square root are not exact. Their results are rounded to machine representable numbers. Thus even ~g, the approximate function will be approximated by a machine version g. 2 Input or representation errors can be resolved by the theory of conditioning <ref> [57, 101, 104] </ref>. However, all of the problems considered in this article are linear, thus approximation of general non-linear functions g is not important except for iterative methods. One iterative is described in section five. <p> We solve (2.14)-(2.15) using the Cholesky factorization of A given by A = X T X = R T R (2.54) for the upper triangular factor R. In [57, p.141-149] (or [80, pp.122-129] or <ref> [101, pp.139-148] </ref> ) an algorithm is given to compute this factorization. We then solve R T R ^ fi = X T y (2.55) using forward and back substitution.
Reference: [102] <author> G.W. Stewart. </author> <title> The economical storage of plane rotations. </title> <journal> Numerische Mathematik, </journal> <volume> 25 </volume> <pages> 137-138, </pages> <year> 1976. </year>
Reference-contexts: The matrix J (i; k; ) can be represented by the two numbers c and s. In fact, it is possible to store just one number using a scheme described by Stewart <ref> [102] </ref>. One can then use Givens rotations to solve the least squares problem (2.5) by the algorithm given below. 24 Algorithm 2.3 (Givens Orthogonal Factorization) The following algorithm solves the least squares problem (2.5) for a n fi p matrix X of rank p and an n-vector y.
Reference: [103] <author> G.W. Stewart. </author> <title> An algorithm for computing the CS decomposition of a partitioned orthonormal matrix. </title> <journal> Numerische Mathematik, </journal> <volume> 40 </volume> <pages> 297-306, </pages> <year> 1983. </year>
Reference-contexts: Excluding numerical difficulties, the C-S decomposition could be computed by a sequence of calls to the LINPACK [40] SVD routine. However, one cannot expect to obtain good results this way. Practical algorithms are given by Stewart <ref> [103] </ref>, Van Loan [106], and Paige [89]. Modifications of Paige's routine are being considered for LAPACK [1]. The direction-of-arrival (DOA) problem is an important estimation problem in signal processing.
Reference: [104] <author> G.W. Stewart and J.-G. Sun. </author> <title> Matrix Perturbation Theory. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: The operations addition, subtraction, multiplication, division, and square root are not exact. Their results are rounded to machine representable numbers. Thus even ~g, the approximate function will be approximated by a machine version g. 2 Input or representation errors can be resolved by the theory of conditioning <ref> [57, 101, 104] </ref>. However, all of the problems considered in this article are linear, thus approximation of general non-linear functions g is not important except for iterative methods. One iterative is described in section five.
Reference: [105] <author> G. Strang. </author> <title> A framework for equilibrium problems. </title> <journal> SIAM Review, </journal> <volume> 30 </volume> <pages> 283-297, </pages> <year> 1988. </year>
Reference-contexts: The subtle implementation problems for this method are discussed by James [76, 75]. He tests the algorithm on structural engineering and Stokes flow problems which can be formulated as LSE problems <ref> [105] </ref>. For example, for one Stokes flow problem which had 19,800 unknowns and 9,999 constraints, Algorithm 5.1 required 1132 iterations to obtain a residual k v (k) k 2 in Algorithm 5.1 that was less than 310 5 . Running time was 463 seconds on the Alliant FX/40.
Reference: [106] <author> C.F. Van Loan. </author> <title> Computing the CS and generalized singular value decompositions. </title> <journal> Numerische Mathematik, </journal> <volume> 46 </volume> <pages> 479-491, </pages> <year> 1985. </year>
Reference-contexts: Excluding numerical difficulties, the C-S decomposition could be computed by a sequence of calls to the LINPACK [40] SVD routine. However, one cannot expect to obtain good results this way. Practical algorithms are given by Stewart [103], Van Loan <ref> [106] </ref>, and Paige [89]. Modifications of Paige's routine are being considered for LAPACK [1]. The direction-of-arrival (DOA) problem is an important estimation problem in signal processing.
Reference: [107] <author> C.F. Van Loan. </author> <title> Unitary method for the ESPRIT direction-of-arrival estimation algorithm. </title> <editor> In F.T. Luk, editor, </editor> <booktitle> Advanced Algorithms and Architectures for Signal Processing II, </booktitle> <pages> pages 170-176, </pages> <address> Bellingham, WA, </address> <year> 1987. </year> <pages> SPIE. </pages>
Reference-contexts: The problem is to estimate from which the DOAs can be recovered. The parameters in are the solution of a nonsymmetric generalized eigen-problem. Van Loan <ref> [107] </ref> describes an algorithm to compute that uses the C-S decomposition to set up the problem. 35 3.2 The Errors in Variables Problem and Total Least Squares The total least squares problem is a generalization of the problem (2.5) that allows for errors in the matrix of predictors x as well
Reference: [108] <author> C.F. Van Loan and J.M. Speiser. </author> <title> Computation of the C-S decomposition with application to signal processing. </title> <editor> In J.M. Speiser, editor, </editor> <booktitle> Advanced Algorithms and Architectures for Signal Processing, </booktitle> <pages> pages 71-78, </pages> <address> Belling-ham, WA, </address> <year> 1986. </year> <pages> SPIE. </pages>
Reference-contexts: In the MUSIC algorithm, it is necessary to compute an orthonormal basis associated with minimum generalized singular value of the pair (A; B). This basis is used as a "noise subspace" which is subsequently used to identify DOAs. Speiser and Van Loan <ref> [108] </ref> give the following algorithm. Algorithm 3.1 (Algorithm SVL) 1. Compute the orthogonal factorization B = Q 1 Here R is presumed to be nonsingular. 2.
Reference: [109] <author> J. Vandewalle and B. de Moor. </author> <title> On the use of the singular value decom-positon in identification and signal processing. </title> <booktitle> In Proc. of the workshop of the NATO Advanced Study Institute on Numerical Linear Algebra, Digital Signal Processing and Parallel Algorithms, </booktitle> <address> Leuven, Belgium, </address> <month> August </month> <year> 1988. </year>
Reference-contexts: Vandewalle and de Moor <ref> [109, 110] </ref> discuss the importance of the C-S decomposition in computing canonical correlations for signal processing applications. The C-S decomposition is also important for block Jacobi algorithms for the parallel computation of eigenvalues and singular values.
Reference: [110] <author> J. Vandewalle and B. de Moor. </author> <title> A variety of applications of singular value decomposition in identification and signal processing. </title> <editor> In E.F. Depret-tere, editor, </editor> <booktitle> SVD and Signal Processing: Algorithms, Applications, and Architectures, </booktitle> <pages> pages 49-91. </pages> <publisher> North Holland, </publisher> <year> 1988. </year>
Reference-contexts: Vandewalle and de Moor <ref> [109, 110] </ref> discuss the importance of the C-S decomposition in computing canonical correlations for signal processing applications. The C-S decomposition is also important for block Jacobi algorithms for the parallel computation of eigenvalues and singular values.
Reference: [111] <author> U.B. Vemulapati. </author> <title> Solving Least Squares Problems on Distributed Memory Machines. </title> <type> PhD thesis, </type> <institution> The Pennsylvania State University, Department of Computer Science, University Park, </institution> <address> PA, </address> <month> December </month> <year> 1990. </year>
Reference-contexts: The column pivoting algorithm of Businger and Golub [27] is the most well-known approach. Barlow and Handy [10] analyze compromise column pivoting routine showing how the particular choice of column ordering can influence the accuracy of methods for solving (5.3)-(5.4). However, as pointed out by Barlow and Vemulapati <ref> [14, 111] </ref>, for large sparse problems, no known column pivoting routine can avoid dynamic storage allocation during the factorization of C. Barlow and Vemulapati [14, 111] give an algorithm to find the rank of C that allows us to use static storage allocation as is done in packages such as SPARSPAK-B <p> However, as pointed out by Barlow and Vemulapati <ref> [14, 111] </ref>, for large sparse problems, no known column pivoting routine can avoid dynamic storage allocation during the factorization of C. Barlow and Vemulapati [14, 111] give an algorithm to find the rank of C that allows us to use static storage allocation as is done in packages such as SPARSPAK-B [52]. An entirely different approach to this problem is given by Bischof, Lewis, and Pierce [17, 94]. <p> It is shown in [10] that if we choose o = 3 as recommended above, under weaker conditions than those necessary for convergence, this value will be as good as can be expected from orthogonal factorization. Vemulapati <ref> [111] </ref> gives tests with this algorithm on random sparse matrices with 1000 unknowns and about 40,000 nonzeroes in the upper triangular factor R in (5.18). One hundred test matrices were generated and the algorithm was run on an iPSC/2 at Oak Ridge National Laboratory in double precision.
Reference: [112] <author> G. Wahba. </author> <title> Spline Models for Observational Data. </title> <publisher> SIAM Publications, </publisher> <address> Philadelphia, </address> <year> 1989. </year>
Reference-contexts: We call (5.7)-(5.8) the WLS (Weighted Least Squares) problem. This problem arises on its own in the calculation of smoothing splines <ref> [112] </ref>, in trust region methods for nonlinear least squares [87], in the solution of Fredholm integral equations of the first kind [43, 30], and Ridge regression. <p> However, improvements and update strategies can be more easily applied to this method. Also the WLS problem is sometimes the one whose solution is desired, as in, for example, smoothing splines <ref> [112] </ref>. 5.4 The Deferred Correction Method The deferred correction procedure due to Van Loan [82] is an attempt to improve the accuracy of the weighting method. Barlow [6] presents an error analysis of the procedure.
Reference: [113] <author> M. Wei. </author> <title> Algebraic properties of the rank-deficient equality constrained and weighted least squares problems. </title> <type> Unpublished report, </type> <institution> East China Normal University, Shanhai, China, </institution> <year> 1990. </year> <note> to appear, Lin. Alg. Appl. 78 </note>
Reference-contexts: If C is rank deficient, we must use the reduction (5.5) first and substitute U for C and d 0 1 for d. The perturbation theory for the LSE problem is quite complicated. We do not discuss it here, but instead refer the reader to <ref> [42, 6, 113] </ref>. Ideally, a method for solving (5.1)-(5.2) should compute the exact solution of a "nearby" version of the problem.
Reference: [114] <author> D.H.D. West. </author> <title> Updating mean and variance estimates: an improved method. </title> <journal> Comm. Assoc. Comput. Mach., </journal> <volume> 22 </volume> <pages> 532-535, </pages> <year> 1979. </year>
Reference-contexts: The best bound that can be obtained for the relative error in the computed value ~ S 2 is j ~ S Sj ug (n) 2 + O (u 2 ) (1.44) where g (n) satisfies (1.38). To avoid this problem, several authors <ref> [119, 114, 64] </ref> derived stable algorithms for computing the sample variance using only one pass through the data. In [29], it was pointed out that all of these algorithms were effectively equivalent. We now give the version due to West [114]. <p> To avoid this problem, several authors [119, 114, 64] derived stable algorithms for computing the sample variance using only one pass through the data. In [29], it was pointed out that all of these algorithms were effectively equivalent. We now give the version due to West <ref> [114] </ref>.
Reference: [115] <author> J.H. Wilkinson. </author> <title> Error analysis of direct methods of matrix inversion. </title> <journal> J. Assoc. Comput. Mach., </journal> <volume> 8 </volume> <pages> 281-330, </pages> <year> 1961. </year>
Reference-contexts: Moreover, their error analyses are related. In particular, using standard results for orthogonal transformations [116, 50], and Gaussian elimination <ref> [115, 61 96] </ref>, one can show that the Bjorck-Golub algorithm produces a computed fac- torization that satisfies B = C 0 P T + ffiC ffiX d 0 B d 0 d 0 f 1 1 C ffid ffiy where V = Q 1 LQ 2 , Q 1 and Q
Reference: [116] <author> J.H. Wilkinson. </author> <title> The Algebraic Eigenvalue Problem. </title> <publisher> Oxford University Press, </publisher> <address> London, </address> <year> 1965. </year>
Reference-contexts: Indeed, if we let o ! 1, and use Householder transformations to construct Q 2 , it can be shown that the weighting procedure becomes the Bjorck-Golub algorithm as o ! 1. Moreover, their error analyses are related. In particular, using standard results for orthogonal transformations <ref> [116, 50] </ref>, and Gaussian elimination [115, 61 96], one can show that the Bjorck-Golub algorithm produces a computed fac- torization that satisfies B = C 0 P T + ffiC ffiX d 0 B d 0 d 0 f 1 1 C ffid ffiy where V = Q 1 LQ 2
Reference: [117] <author> J.H. Wilkinson. </author> <title> Global convergence of tridiagonal QR algorithm with origin shifts. </title> <journal> Linear Algebra and Its Applications, </journal> <volume> 1 </volume> <pages> 409-420, </pages> <year> 1968. </year>
Reference-contexts: The procedure is given in detail by Golub and Van Loan [57, Chapter 8]. It is based upon the QR algorithm of Francis [46] (for the eigenvalues of a general matrix) with Wilkinson's <ref> [117] </ref> enhancements for symmetric matrices. This is the algorithm implemented as the LINPACK routing SSVDC. The algorithm is quite complicated and is explained in detail there. Thus we do not give it here.
Reference: [118] <author> J.H. Wilkinson. </author> <title> Modern error analysis. </title> <journal> SIAM Review, </journal> <volume> 14 </volume> <pages> 548-568, </pages> <year> 1971. </year>
Reference-contexts: One iterative is described in section five. We discuss two methods for analyzing rounding errors:(1) forward error analysis and (2) backward error analysis. These are the two types that are most commonly discussed in the literature, although other formalisms have been proposed (cf. <ref> [24, 4, 88, 118, 79, 86] </ref> ).
Reference: [119] <author> E.A. Youngs and E.M. Cramer. </author> <title> Some results relevant to choice of sum and sum-of-product algorithms. </title> <journal> Technometrics, </journal> <volume> 13 </volume> <pages> 657-665, </pages> <year> 1971. </year>
Reference-contexts: The best bound that can be obtained for the relative error in the computed value ~ S 2 is j ~ S Sj ug (n) 2 + O (u 2 ) (1.44) where g (n) satisfies (1.38). To avoid this problem, several authors <ref> [119, 114, 64] </ref> derived stable algorithms for computing the sample variance using only one pass through the data. In [29], it was pointed out that all of these algorithms were effectively equivalent. We now give the version due to West [114].
Reference: [120] <author> H. Zha. </author> <title> The restricted singular value decomposition of matrix triplets. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 12 </volume> <pages> 172-194, </pages> <year> 1991. </year> <month> 79 </month>
Reference-contexts: The solution of this problem is closely related to the restricted singular value decomposition (RSVD) of the matrix triple (T; D; C) where T = (X Y ). The formulation of the RSVD is due to Zha <ref> [120] </ref> and its relation to the RTLS problem is detailed by Van Huffel and Zha [73].
References-found: 120

