URL: http://www.cs.utexas.edu/users/less/compilers/eggers-pldi95.ps
Refering-URL: http://www.cs.utexas.edu/users/less/compiler.html
Root-URL: 
Email: -jlo,eggers-@cs.washington.edu  
Title: Improving Balanced Scheduling with Compiler Optimizations that Increase Instruction-Level Parallelism  
Author: Jack L. Lo and Susan J. Eggers 
Address: Washington  
Affiliation: Department of Computer Science and Engineering University of  
Abstract: Traditional list schedulers order instructions based on an optimistic estimate of the load latency imposed by the hardware and therefore cannot respond to variations in memory latency caused by cache hits and misses on non-blocking architectures. In contrast, balanced scheduling schedules instructions based on an estimate of the amount of instruction-level parallelism in the program. By scheduling independent instructions behind loads based on what the program can provide, rather than what the implementation stipulates in the best case (i.e., a cache hit), balanced scheduling can hide variations in memory latencies more effectively. Since its success depends on the amount of instruction-level parallelism in the code, balanced scheduling should perform even better when more parallelism is available. In this study, we combine balanced scheduling with three compiler optimizations that increase instruction-level parallelism: loop unrolling, trace scheduling and cache locality analysis. Using code generated for the DEC Alpha by the Multiow compiler, we simulated a non-blocking processor architecture that closely models the Alpha 21164. Our results show that balanced scheduling benefits from all three optimizations, producing average speedups that range from 1.15 to 1.40, across the optimizations. More importantly, because of its ability to tolerate variations in load interlocks, it improves its advantage over traditional scheduling. Without the optimizations, balanced scheduled code is, on average, 1.05 times faster than that generated by a traditional scheduler; with them, its lead increases to 1.18. 
Abstract-found: 1
Intro-found: 1
Reference: [Asp93] <author> T. Asprey. </author> <title> Performance features of the PA7100 microprocessor. </title> <journal> IEEE Micro, </journal> <volume> 13(3):2235, </volume> <month> June </month> <year> 1993. </year> <note> [BCK + 89] M. </note> <author> Berry, D. Chen, D. Kuck, S. Lo, Y. Pang, L. Pointer, R. Roloff, A. Sameh, E. Clementi, S. Chin, D. Schneider, G. Fox, P. Messina, D. Walker, C. Hsiung, J. Schwarzmeier, K. Lue, S. Orszag, O. Johnson F. Seidl and, R. Goodrum, and J. Martin. </author> <title> The Perfect Club: Effective performance evaluation of supercomputers. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 3(3):540, </volume> <month> December </month> <year> 1989. </year>
Reference-contexts: Blocking processors simplify the design of the code scheduler, by enabling all load instructions to be handled identically, whether they hit or miss in the cache. The traditional blocking processor model has recently been challenged by processors that do not block on loads [ER94] [McL93] <ref> [Asp93] </ref> [DA92] [Gwe94a] [Gwe94b] [Mip94] [CS95]. Rather than stalling until a cache miss is satisfied, they use lockup-free caches [Kro81] [FJ94] to continue executing instructions to hide the latency of outstanding memory requests.
Reference: [CS95] <author> R.P. Colwell and R.L. Steck. </author> <title> A 0.6um BiCMOS Processor with Dynamic Execution. </title> <booktitle> In IEEE International Solid-State Circuits Conference 95, </booktitle> <pages> page 176-177, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: The traditional blocking processor model has recently been challenged by processors that do not block on loads [ER94] [McL93] [Asp93] [DA92] [Gwe94a] [Gwe94b] [Mip94] <ref> [CS95] </ref>. Rather than stalling until a cache miss is satisfied, they use lockup-free caches [Kro81] [FJ94] to continue executing instructions to hide the latency of outstanding memory requests.
Reference: [DA92] <author> K. Diefendorf and M. Allen. </author> <title> Organization of the Mo-torola 88110 superscalar RISC microprocessor. </title> <journal> IEEE Micro, </journal> <volume> 12(2):5161, </volume> <month> April </month> <year> 1992. </year>
Reference-contexts: Blocking processors simplify the design of the code scheduler, by enabling all load instructions to be handled identically, whether they hit or miss in the cache. The traditional blocking processor model has recently been challenged by processors that do not block on loads [ER94] [McL93] [Asp93] <ref> [DA92] </ref> [Gwe94a] [Gwe94b] [Mip94] [CS95]. Rather than stalling until a cache miss is satisfied, they use lockup-free caches [Kro81] [FJ94] to continue executing instructions to hide the latency of outstanding memory requests.
Reference: [Dix92] <author> K.M. Dixit. </author> <title> New CPU benchmark suites from SPEC. </title> <booktitle> In COMPCON, </booktitle> <pages> pages 305310, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: Using code generated for the DEC Alpha and a simulator that models the AXP 21164, we simulated the effects of all three optimizations on both balanced and traditional scheduling, using Perfect Club [BCK+89] and SPEC92 <ref> [Dix92] </ref> benchmarks. Our results confirm the hypothesis. Balanced scheduling interacts well with all three optimizations, producing average speedups that range from 1.15 to 1.40. More importantly, its performance advantage relative to traditional scheduling increases when combined with the optimizations. <p> simulation package su2cor Fortran Computes masses of elementary particles in the framework of the Quark-Gluon theory swm256 Fortran Solves shallow water equations using finite difference equations tomcatv Fortran Vectorized mesh generation program 4.1 Workload Our experiments use benchmarks (Table 1) taken from the Perfect Club [BCK + 89] and SPEC92 <ref> [Dix92] </ref> suites (the Multiow compiler has both Fortran and C front ends). We chose numeric programs because their high percentage of loops make them appropriate testbeds for optimizations targeted for loops.
Reference: [ER94] <author> J. Edmondson and P. Rubinfeld. </author> <title> An overview of the 21164 Alpha AXP microprocessor. </title> <booktitle> In Hot Chips VI, </booktitle> <pages> pages 18, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: Blocking processors simplify the design of the code scheduler, by enabling all load instructions to be handled identically, whether they hit or miss in the cache. The traditional blocking processor model has recently been challenged by processors that do not block on loads <ref> [ER94] </ref> [McL93] [Asp93] [DA92] [Gwe94a] [Gwe94b] [Mip94] [CS95]. Rather than stalling until a cache miss is satisfied, they use lockup-free caches [Kro81] [FJ94] to continue executing instructions to hide the latency of outstanding memory requests.
Reference: [FERN84] <author> J.A. Fisher, J.R. Ellis, J. Ruttenberg, and A. Nicolau. </author> <title> Parallel processing: A smart compiler and a dumb machine. </title> <booktitle> In ACM SIGPLAN 84 Symposium on Compiler Construction, </booktitle> <pages> pages 3646, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: The technique increases basic block size by eliminating branch overhead instructions for all iterations but the last. Consequently, more instructions are available to hide load latencies and more exibility is offered to the scheduler. Trace scheduling increases the amount of instruction-level parallelism by permitting instruction scheduling beyond basic blocks <ref> [FERN84] </ref> [LFK + 93]. It does this by assigning expected execution frequencies to edges of the control ow graph, and optimizing the schedules along the execution paths with the highest frequencies. <p> Second, the consequent increase in the size of the basic block can expose more instruction-level parallelism, thereby providing more opportunities for code scheduling. 3.2 Trace Scheduling Trace scheduling <ref> [FERN84] </ref> [LFK+93] enables more aggressive scheduling by permitting code motion across basic block boundaries. Guided by estimated or profiled execution frequencies for each basic block, it creates traces of paths through each procedure.
Reference: [FGL93] <author> S.M. Freudenberger, T.R. Gross, and P.F. Lowney. </author> <title> Avoidance and suppression of compensation code in a trace scheduling compiler. </title> <type> Technical Report HPL-93-35, </type> <institution> Hewlett Packard, </institution> <year> 1993. </year>
Reference-contexts: We disabled loop unrolling when the unrolled block reached 64 instructions (4) or 128 (8), and did not unroll loops with more than one internal conditional branch. 2 Using the methodology established in other trace scheduling performance studies <ref> [FGL93] </ref>, we first profiled the programs to determine basic block execution frequencies. This information guided the Multiow compiler in picking traces. Since traces do not cross the back edges of loops, we unrolled them, using the same unrolling factor as in the loop unrolling studies.
Reference: [FJ94] <author> K.I. </author> <title> Farkas and N.P. Jouppi. Complexity/performance tradeoffs with non-blocking loads. </title> <booktitle> In 21th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 211222, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: The traditional blocking processor model has recently been challenged by processors that do not block on loads [ER94] [McL93] [Asp93] [DA92] [Gwe94a] [Gwe94b] [Mip94] [CS95]. Rather than stalling until a cache miss is satisfied, they use lockup-free caches [Kro81] <ref> [FJ94] </ref> to continue executing instructions to hide the latency of outstanding memory requests.
Reference: [GM86] <author> P.B. Gibbons and S.S. Muchnick. </author> <title> Efficient instruction scheduling for a pipelined architecture. </title> <booktitle> In ACM SIG-PLAN 86 Symposium on Compiler Construction, </booktitle> <pages> pages 1116, </pages> <month> July </month> <year> 1986. </year>
Reference-contexts: Instruction scheduling seeks to minimize pipeline stalls due to data and control hazards. Given the data and control dependences present in the instruction stream and the estimates for instruction latencies, the scheduler statically determines an instruction order that avoids as many interlocks as possible. The list scheduling approach [HG83] <ref> [GM86] </ref> [Sit78] to this minimization problem relies on a code DAG, instruction latencies, and heuristics for instruction selection. The code DAG represents the instructions to be scheduled and the data dependences between them.
Reference: [Gwe94a] <author> L. Gwennap. </author> <title> UltraSparc Unleashes SPARC Performance. </title> <type> Microprocessor Report, </type> <pages> pages 18, </pages> <month> October 3, </month> <year> 1994. </year>
Reference-contexts: Blocking processors simplify the design of the code scheduler, by enabling all load instructions to be handled identically, whether they hit or miss in the cache. The traditional blocking processor model has recently been challenged by processors that do not block on loads [ER94] [McL93] [Asp93] [DA92] <ref> [Gwe94a] </ref> [Gwe94b] [Mip94] [CS95]. Rather than stalling until a cache miss is satisfied, they use lockup-free caches [Kro81] [FJ94] to continue executing instructions to hide the latency of outstanding memory requests.
Reference: [Gwe94b] <author> L. Gwennap. </author> <title> 620 Fills Out PowerPC Product Line. </title> <type> Microprocessor Report, </type> <pages> pages 1216, </pages> <month> October 24, </month> <year> 1994. </year>
Reference-contexts: Blocking processors simplify the design of the code scheduler, by enabling all load instructions to be handled identically, whether they hit or miss in the cache. The traditional blocking processor model has recently been challenged by processors that do not block on loads [ER94] [McL93] [Asp93] [DA92] [Gwe94a] <ref> [Gwe94b] </ref> [Mip94] [CS95]. Rather than stalling until a cache miss is satisfied, they use lockup-free caches [Kro81] [FJ94] to continue executing instructions to hide the latency of outstanding memory requests.
Reference: [HG83] <author> J.L. Hennessy and T.R. Gross. </author> <title> Postpass code optimization of pipeline constraints. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 5(3) </volume> <pages> 422-448, </pages> <month> July </month> <year> 1983. </year>
Reference-contexts: Instruction scheduling seeks to minimize pipeline stalls due to data and control hazards. Given the data and control dependences present in the instruction stream and the estimates for instruction latencies, the scheduler statically determines an instruction order that avoids as many interlocks as possible. The list scheduling approach <ref> [HG83] </ref> [GM86] [Sit78] to this minimization problem relies on a code DAG, instruction latencies, and heuristics for instruction selection. The code DAG represents the instructions to be scheduled and the data dependences between them.
Reference: [KE93] <author> D.R. Kerns and S.J. Eggers. </author> <title> Balanced scheduling: Instruction scheduling when memory latency is uncertain. </title> <booktitle> In ACM SIGPLAN 93 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 278-289, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Unfortunately, a traditional instruction scheduler fails to exploit this opportunity because of its optimistic and architecture-based estimates; its resulting schedules may be therefore intolerant of variations in load latency. Balanced scheduling <ref> [KE93] </ref> is an algorithm that can generate schedules that adapt more readily to the uncertainties in memory latency. Rather than being determined by a predefined, architecturally-based value, load latency estimates are based on the number of independent instructions that are available to hide the latency of a particular load. <p> Using these estimates as load weights, the balanced scheduler then schedules instructions normally. Previous work has demonstrated that balanced schedules show speedups averaging 8% for several Perfect Club benchmarks for two different cache hit/miss ratios, assuming a workstation-like memory model in which cache misses are normally distributed <ref> [KE93] </ref>. Since its success depends on the amount of instruction-level parallelism in a program, balanced scheduling should perform better when more parallelism is available. In this study, we combine balanced scheduling with three compiler optimizations that increase instruction-level parallelism. <p> This frees more instructions to hide the latencies of other loads (com-piler-determined cache misses, as well as loads with no reuse information), which are balanced scheduled. The original work on balanced scheduling <ref> [KE93] </ref>, which compared it to the traditional approach and without ILP-enhancing optimizations, relied on a stochastic model to simulate cache behavior and network congestion. <p> Second, it distributes load-level parallelism across all the loads in order to be more tolerant of memory latency variability. Third, since it bases its load weight estimates on the code rather than the architecture, it produces schedules that are independent of the memory system implementation. Initial results <ref> [KE93] </ref> indicated that balanced scheduling produced speedups averaging 8% for selected programs in the Perfect Club suite, when simulated on a stochastic model of a microprocessor with 1990 processor and memory latencies, a bus interconnect and cache hit rates of 80% and 95%. <p> When trace scheduling was also applied, these speedups reached 1.29 and 1.40. 5.5 Simulating Real Architectures Examining results from the original comparison of balanced and traditional scheduling (without optimizations) <ref> [KE93] </ref> illustrates both the limitations of simple architecture models and the benefits of a very optimizing compiler for execution cycle analysis. There were several methodological differences between the two studies. First, the original work relied on a stochastic model, rather than execution-driven simulation, to simulate cache behavior and network congestion.
Reference: [Kro81] <author> D. Kroft. </author> <title> Lockup-free instruction fetch/prefetch cache organization. </title> <booktitle> In 8th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 8187, </pages> <month> May </month> <year> 1981. </year> <note> [LFK + 93] P.F. Lowney, S.M. Freudenberger, R.J. </note> <author> Karzes, W.D. Lichtenstein, R.P. Nix, J.S. ODonnell, and J.C. Rut-tenberg. </author> <title> The Multiflow trace scheduling compiler. </title> <journal> The Journal of Supercomputing, </journal> <volume> 7:51143, </volume> <year> 1993. </year>
Reference-contexts: The traditional blocking processor model has recently been challenged by processors that do not block on loads [ER94] [McL93] [Asp93] [DA92] [Gwe94a] [Gwe94b] [Mip94] [CS95]. Rather than stalling until a cache miss is satisfied, they use lockup-free caches <ref> [Kro81] </ref> [FJ94] to continue executing instructions to hide the latency of outstanding memory requests.
Reference: [McL93] <author> E. McLellan. </author> <title> The Alpha AXP architecture and 21064 processor. </title> <journal> IEEE Micro, </journal> <volume> 13(3):3647, </volume> <month> June </month> <year> 1993. </year>
Reference-contexts: Blocking processors simplify the design of the code scheduler, by enabling all load instructions to be handled identically, whether they hit or miss in the cache. The traditional blocking processor model has recently been challenged by processors that do not block on loads [ER94] <ref> [McL93] </ref> [Asp93] [DA92] [Gwe94a] [Gwe94b] [Mip94] [CS95]. Rather than stalling until a cache miss is satisfied, they use lockup-free caches [Kro81] [FJ94] to continue executing instructions to hide the latency of outstanding memory requests.
Reference: [Mip94] <institution> MIPS Technologies, Inc. Mips Open RISC Technology R10000 Microprocessor Technical Brief. </institution> <month> October </month> <year> 1994. </year>
Reference-contexts: Blocking processors simplify the design of the code scheduler, by enabling all load instructions to be handled identically, whether they hit or miss in the cache. The traditional blocking processor model has recently been challenged by processors that do not block on loads [ER94] [McL93] [Asp93] [DA92] [Gwe94a] [Gwe94b] <ref> [Mip94] </ref> [CS95]. Rather than stalling until a cache miss is satisfied, they use lockup-free caches [Kro81] [FJ94] to continue executing instructions to hide the latency of outstanding memory requests.
Reference: [MLG92] <author> T.C. Mowry, M.S. Lam and A. Gupta. </author> <title> Design and evaluation of a compiler algorithm for prefetching. </title> <booktitle> In Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 6273, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: In order to preserve data dependencies and correctness, some code motion is restricted, or may only occur if additional compensation code is added. The third optimization, locality analysis, identifies potential temporal and spatial reuse in array accesses within a loop and transforms the code to exploit it <ref> [MLG92] </ref>. When used in conjunction with balanced scheduling, it enables load instructions to be selectively balanced scheduled. If the compiler can determine that a load instruction is a hit, then the optimistic latency estimate is correct, and should be used. <p> We identify spatial and temporal locality by using the locality analysis algorithm that Mowry, Lam and Gupta developed for selective cache prefetching <ref> [MLG92] </ref>. We apply their algorithm to determine cache hit and miss behavior for load instructions in inner loops. <p> Hence, there is no need to hide more than 50 cycles for any load instruction. We added a locality analysis module to Multiows Phase 2. As in Mowry, Lam and Gupta <ref> [MLG92] </ref>, a predicate 3 is created for each array reference that exhibits reuse and is associated with the refer-encess load instruction. Predicates are used to determine both the unrolling factor (which depends on the cache block and array element sizes) 4 and whether loads should be hits or misses.
Reference: [Sit78] <author> R.L. </author> <title> Sites. Instruction Ordering for the Cray-1 Computer. </title> <type> Technical Report 78-CS-023, </type> <institution> Univ. of Califor-nia, </institution> <address> San Diego, </address> <month> July </month> <year> 1978. </year>
Reference-contexts: Given the data and control dependences present in the instruction stream and the estimates for instruction latencies, the scheduler statically determines an instruction order that avoids as many interlocks as possible. The list scheduling approach [HG83] [GM86] <ref> [Sit78] </ref> to this minimization problem relies on a code DAG, instruction latencies, and heuristics for instruction selection. The code DAG represents the instructions to be scheduled and the data dependences between them.
Reference: [Sta] <author> R. Stallman. </author> <title> The GNU Project Optimizing C Compiler. Free Software Foundation. </title>
Reference-contexts: Code for these studies was generated using gcc <ref> [Sta] </ref>. 3 The Compiler Optimizations Balanced scheduling utilizes load-level parallelism to hide the longer load latencies exposed by non-blocking processors. By applying techniques to increase load-level parallelism, the balanced scheduler should be able to generate schedules that are even more tolerant of uncertain memory latency.
References-found: 19

