URL: ftp://ftp-pubs.lcs.mit.edu/pub/lcs-pubs/tr.outbox/MIT-LCS-TR-674.ps.gz
Refering-URL: ftp://ftp-pubs.lcs.mit.edu/pub/lcs-pubs/listings/tr600.html
Root-URL: 
Title: High-Performance All-Software Distributed Shared Memory  
Author: Kirk L. Johnson 
Address: 545 Technology Square Cambridge, MA 02139, U.S.A.  
Affiliation: Laboratory for Computer Science Massachusetts Institute of Technology  
Abstract: MIT Laboratory for Computer Science Technical Report MIT/LCS/TR-674 18 December 1995 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anant Agarwal, Ricardo Bianchini, David Chaiken, Kirk L. Johnson, David Kranz, John Kubiatowicz, Beng-Hong Lim, Ken Mackenzie, and Donald Yeung. </author> <title> The MIT Alewife Machine: Architecture and Performance. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-13, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Because of these features, CRL scores well in the portability department; porting the original (Thinking Machines' CM-5 [48]) CRL implementation to two other platforms (the MIT Alewife machine <ref> [1] </ref> and a network of Sun workstations communicating with one another using TCP) proved to be straightforward. In addition, by eliminating the need for special-purpose hardware to implement DSM functionality, the implementation effort required to build a system like CRL (or other software-based systems) is greatly reduced. <p> A detailed description of the coherence protocol can be found in Appendix A. The prototype CRL implementation employs a fixed-home, directory-based invalidate protocol similar to that used in many hardware (e.g., Alewife <ref> [1] </ref>, DASH [49]) and software (e.g., Ivy [52], Munin [11]) DSM systems. In this protocol, coherence actions for a each region are coordinated by a particular home node (in the current CRL implementation, the home node is always the node where the region was created). <p> This organization is motivated by studies indicating that small-scale sharing of data is the common case [12, 61, 83]; data shared more widely is relatively uncommon. In general, Alewife's shared memory system performs quite well, enabling speedups comparable to or better than other scalable hardware-based DSM systems <ref> [1, 49] </ref>. In addition to providing support for coherent shared memory, Alewife provides the processor with direct access to the interconnection network for sending and receiving messages [45]. Efficient mechanisms are provided for sending and receiving both short (register-to-register) messages and long (memory-to-memory, bulk data transfer) messages.
Reference: [2] <author> Anant Agarwal, John Kubiatowicz, David Kranz, Beng-Hong Lim, Donald Yeung, Godfrey D'Souza, and Mike Parkin. Sparcle: </author> <title> An Evolutionary Processor Design for Multiprocessors. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 48-61, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: The basic Alewife architecture consists of processor/memory nodes communicating over a packet-switched interconnection network organized as a two-dimensional mesh (see Figure 5-5). Each processor/memory node consists of a Sparcle processor <ref> [2] </ref>, an off-the-shelf floating-point unit (FPU), a 64-kilobyte unified instruction/data cache (direct mapped, 16-byte lines), eight megabytes of DRAM, the local portion of the interconnection network (a Caltech Elko series Mesh Routing Chip [71]), and a Communications and Memory Management Unit (CMMU).
Reference: [3] <author> H.E. Bal, M.F. Kaashoek, </author> <title> and A.S. Tanenbaum. Orca: A language for Parallel Programming of Distributed Systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <pages> pages 190-205, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: By manipulating the error correcting code bits associated with every memory block, Blizzard can control access on a cache-line by cache-line basis. All-Software In an all-software DSM system, all three of the mechanisms identified above are implemented entirely in software (e.g., Orca <ref> [3] </ref>). <p> CRL runs on two large-scale platforms and has been shown to deliver performance competitive with hardware DSM systems. Several all-software DSM systems that employ an object-based approach have been developed (e.g., Amber [17], Concert [35], Orca <ref> [3] </ref>). Like these systems, CRL effects coherence at the level of application-defined regions of memory (objects). Any necessary synchronization, data replication, or thread migration functionality is provided automatically at the entry and exit of methods on shared objects. <p> Any necessary synchronization, data replication, or thread migration functionality is provided automatically at the entry and exit of methods on shared objects. Existing systems of this type either require the use of an entirely new object-oriented language <ref> [3, 35] </ref> or only allow the use of a subset of an existing one [17]. In contrast, CRL is not language specific; the basic CRL interface could easily be provided in any imperative programming language. Scales and Lam [69] have described SAM, a shared object system for distributed memory machines.
Reference: [4] <author> Henri E. Bal and M. Frans Kaashoek. </author> <title> Object Distribution in Orca using Compile-Time and Run-Time Techniques. </title> <booktitle> In Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications (OOPSLA'93), </booktitle> <pages> pages 162-177, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: Dynamic Approaches Dynamic software DSM systems typically support a more general programming model than their static counterparts, typically allowing multiple independent threads of control to operate within the shared address space <ref> [4, 5, 11, 21, 37, 52, 75] </ref>. Given mechanisms for inter-thread synchronization (e.g., semaphores, barriers), a programmer is able to express essentially any form of parallelism. <p> An alternate approach involves moving computation to the data it references. Systems organized along these lines avoid the overhead of frequent remote communication by migrating computation to the node upon which frequently referenced data resides [10, 65]. Implementations utilizing both computation- and data-migration techniques are also possible <ref> [4, 11, 32] </ref>. As with static software DSMs, the high fixed overheads of message-based communication in many current generation systems drive dynamic software DSM implementors toward optimizations that reduce the total number of message sends and receives.
Reference: [5] <author> Brian N. Bershad, Matthew J. Zekauskas, and Wayne A. Sawdon. </author> <title> The Midway Distributed Shared Memory System. </title> <booktitle> In Proceedings of the 38th IEEE Computer Society International Conference (COMPCON'93), </booktitle> <pages> pages 528-537, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: CRL requires such annotations whether reading or writing to shared data, similar to entry consistency <ref> [5] </ref>. Experience with the applications described in this thesis indicates that the additional programming overhead of providing these annotations is quite modest. <p> Dynamic Approaches Dynamic software DSM systems typically support a more general programming model than their static counterparts, typically allowing multiple independent threads of control to operate within the shared address space <ref> [4, 5, 11, 21, 37, 52, 75] </ref>. Given mechanisms for inter-thread synchronization (e.g., semaphores, barriers), a programmer is able to express essentially any form of parallelism. <p> From this perspective, CRL provides sequential consistency for read and write operations in the same sense that a sequentially consistent hardware-based DSM does for individual loads and stores. In terms of individual loads and stores, CRL provides a memory/coherence model similar to entry <ref> [5] </ref> or release consistency [25]. Loads and stores to global data are allowed only within properly synchronized sections (operations), and modifications to a region are only made visible to other processors after the appropriate release operation (a call to rgn_end_write). <p> Annotations of the second sort (delimiting accesses to shared data) are similar to those necessary in aggressive hardware and software DSM implementations (e.g., those providing release consistency [25]) when writing to shared data. CRL requires such annotations whether reading or writing to shared data, similar to entry consistency <ref> [5] </ref>. As discussed in Section 6.2, experience with the applications described in this thesis indicates that the additional programming overhead of providing these annotations is quite modest. <p> In addition, the software overhead of systems like this (e.g., from manipulating virtual memory mechanisms and computing diffs) can be large enough to significantly impact delivered application performance [21]. Midway is a software DSM system based on entry consistency <ref> [5] </ref>. As discussed in Section 3.4, CRL's programming model is similar to that provided by Midway. An important difference, however, is that Midway requires a compiler that can cull user-provided annotations that relate data and synchronization objects from the source code and provide these to the Midway run-time system. <p> Furthermore, annotations similar to those required for CRL operations are necessary in some aggressive hardware DSM implementations when writing to shared data (e.g., those providing release consistency). CRL requires such 91 annotations whether reading or writing shared data, similar to the entry consistency model used in Midway <ref> [5] </ref>.
Reference: [6] <author> G. E. Blelloch. </author> <title> Scans as primitive parallel operations. </title> <journal> IEEE Transactions on Computers, </journal> <pages> pages 1526-1538, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: Broadcasts are implemented using a binary broadcast tree rooted at each node. Barriers and reductions are implemented in a scan <ref> [6] </ref> style: For an n processor system, messages are sent between nodes according to a butterfly network pattern requiring log 2 n stages of n messages each.
Reference: [7] <author> Nanette J. Boden, Danny Cohen, Robert E. Felderman, Alan E. Kulawik, Charles L. Seitz, Jakov N. Seizovic, and Wen-King Su. Myrinet: </author> <title> A Gigabit-per-Second Local Area Network. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 29-36, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: While this approach cuts the effective transfer bandwidth roughly in half, it provides significantly reduced latencies for small transfers by avoiding the need for prenegotiation with the receiving node. Networks of workstations with interprocessor communication performance rivaling that of the CM-5 are rapidly becoming reality <ref> [7, 56, 77, 80] </ref>. For example, Thekkath et al. [78] describe the implementation of a specialized data-transfer mechanism implemented on a pair of 25 MHz DECstations connected with a first-generation FORE ATM network.
Reference: [8] <author> Per Brinch Hansen. </author> <title> Concurrent Programming Concepts. </title> <journal> ACM Computing Surveys, </journal> <volume> 5(4) </volume> <pages> 223-245, </pages> <year> 1973. </year>
Reference-contexts: Finally, it is worth noting that CRL's integration of data access and synchronization into a single mechanism is not unlike that provided by monitors, a linguistic mechanism suggested by Hoare [28] and Brinch Hansen <ref> [8] </ref>, or other linguistic mechanisms that integrate synchronization and data access (e.g., mutexes in Argus [53], mutex operations in COOL [15], etc.). 34 Chapter 4 CRL Internals This chapter describes the general structure of the prototype CRL implementation used in this thesis.
Reference: [9] <author> David Callahan and Ken Kennedy. </author> <title> Compiling Programs for Distributed-Memory Multiprocessors. </title> <journal> Journal of Supercomputing, </journal> <pages> pages 151-169, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: Accordingly, this proposal uses the terms static software DSM and dynamic software DSM to refer to members of the first and second class, respectively. Static Approaches Static software DSM systems are typified by compilers for FORTRAN-style scientific codes targeting message-passing multicomputers <ref> [9, 41, 51, 64, 79, 87] </ref>.
Reference: [10] <author> Martin C. Carlisle, Anne Rogers, John H. Reppy, and Laurie J. Hendren. </author> <title> Early Experiences with Olden. </title> <booktitle> In Conference Record of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1993. </year> <month> 157 </month>
Reference-contexts: An alternate approach involves moving computation to the data it references. Systems organized along these lines avoid the overhead of frequent remote communication by migrating computation to the node upon which frequently referenced data resides <ref> [10, 65] </ref>. Implementations utilizing both computation- and data-migration techniques are also possible [4, 11, 32]. As with static software DSMs, the high fixed overheads of message-based communication in many current generation systems drive dynamic software DSM implementors toward optimizations that reduce the total number of message sends and receives.
Reference: [11] <author> John B. Carter. </author> <title> Efficient Distributed Shared Memory Based On Multi-Protocol Release Consistency. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: For applications in which the complexity of using a message-passing programming model remains manageable, one can often realize better performance using message-passing instead of shared-memory. However, these gains can be relatively modest <ref> [11, 16] </ref> and frequently come at the cost of greatly increased programmer effort. In spite of this fact, message passing environments such as PVM (Parallel Virtual Machine) [24] and MPI (Message Passing Interface) [55] are often the de facto standards for programming multicomputers and networks of workstations. <p> Dynamic Approaches Dynamic software DSM systems typically support a more general programming model than their static counterparts, typically allowing multiple independent threads of control to operate within the shared address space <ref> [4, 5, 11, 21, 37, 52, 75] </ref>. Given mechanisms for inter-thread synchronization (e.g., semaphores, barriers), a programmer is able to express essentially any form of parallelism. <p> An alternate approach involves moving computation to the data it references. Systems organized along these lines avoid the overhead of frequent remote communication by migrating computation to the node upon which frequently referenced data resides [10, 65]. Implementations utilizing both computation- and data-migration techniques are also possible <ref> [4, 11, 32] </ref>. As with static software DSMs, the high fixed overheads of message-based communication in many current generation systems drive dynamic software DSM implementors toward optimizations that reduce the total number of message sends and receives. <p> Mostly Software Many software DSM systems are actually mostly software systems in which the hit/miss check functionality is implemented in hardware (e.g., by leveraging off of virtual memory protection mechanisms to provide access control). Typical examples of mostly software systems include Ivy [52], Munin <ref> [11] </ref>, and TreadMarks [38]; coherence units in these systems are the size of virtual memory pages. Blizzard [70] implements a similar scheme on the CM-5 at the granularity of individual cache lines. <p> A detailed description of the coherence protocol can be found in Appendix A. The prototype CRL implementation employs a fixed-home, directory-based invalidate protocol similar to that used in many hardware (e.g., Alewife [1], DASH [49]) and software (e.g., Ivy [52], Munin <ref> [11] </ref>) DSM systems. In this protocol, coherence actions for a each region are coordinated by a particular home node (in the current CRL implementation, the home node is always the node where the region was created). <p> Kubiatowicz, personal communication, August 1995. 84 and, if so, how frequently can be replaced with carefully selected, efficient message-based primitives. The extent to which any migratory data problem can be addressed using other techniques such as function shipping <ref> [11] </ref> and computation migration [31, 32] probably also warrants further investigation. 85 Chapter 7 Related Work CRL builds on a large body of research into the construction of distributed shared memory systems. <p> In terms of comparing message passing and shared memory, most other previous work has either compared the performance of applications written and tuned specifically for each programming model <ref> [11, 16] </ref> or looked at the performance gains made possible by augmenting a hardware DSM system with message passing primitives [43].
Reference: [12] <author> David Chaiken, Craig Fields, Kiyoshi Kurihara, and Anant Agarwal. </author> <title> Directory-Based Cache-Coherence in Large-Scale Multiprocessors. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 41-58, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: This organization is motivated by studies indicating that small-scale sharing of data is the common case <ref> [12, 61, 83] </ref>; data shared more widely is relatively uncommon. In general, Alewife's shared memory system performs quite well, enabling speedups comparable to or better than other scalable hardware-based DSM systems [1, 49].
Reference: [13] <author> David L. Chaiken and Anant Agarwal. </author> <title> Software-Extended Coherent Shared Memory: Performance and Cost. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 314-324, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Mostly Hardware As discussed in Section 5.2, the MIT Alewife machine implements a mostly hardware DSM systemprocessor-side mechanisms are always implemented in hardware, but memory-side support is handled in software when widespread sharing is detected <ref> [13] </ref>. Dir 1 SW and its variations [27, 85] are also mostly hardware schemes. The Stanford FLASH multiprocessor [46] and Wisconsin Typhoon architecture [63] represent a different kind of mostly hardware DSM system. <p> Shared memory support is provided through an implementation of the LimitLESS cache coherence scheme <ref> [13] </ref>: limited sharing of memory blocks (up to five remote readers) is supported in hardware; higher-degree sharing is handled by trapping 54 the processor on the home memory node and extending the small hardware directory in software. <p> vs. message passing) and the use of simulation to obtain controlled comparisons with cache-coherent hardware DSM (when such a comparison is provided). 7.3 Comparative Studies Several researchers have reported results comparing the performance of systems at adjacent levels of the classification presented in Section 2.2 (e.g., all-hardware vs. mostly hardware <ref> [13, 27, 85] </ref>, mostly software vs. all-software [70, 86]), but to our knowledge, only Cox et al. [18] have published results from a relatively controlled comparison of hardware and software DSM systems.
Reference: [14] <author> Rohit Chandra, Kourosh Gharachorloo, Vijayaraghavan Soundararajan, and Anoop Gupta. </author> <title> Performance Evaluation of Hybrid Hardware and Software Distributed Shared Memory Protocols. </title> <booktitle> In Proceedings of the Eighth International Conference on Supercomputing, </booktitle> <pages> pages 274-288, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: Finally, since Concord performance results are only presented for a relatively outdated platform (a 32-processor Intel iPSC/2), it is unclear how these results compare to similar results measured on other hardware- and software-based DSM systems. Chandra et al. <ref> [14] </ref> propose a hybrid DSM protocol in which region-like annotations are used to demark access to regions of shared data. Coherence for regions thus annotated is provided using software DSM techniques analogous to those used by CRL; hardware DSM mechanisms are used for coherence on all other memory references.
Reference: [15] <author> Rohit Chandra, Anoop Gupta, and John L. Hennessy. </author> <title> Data Locality and Load Balancing in COOL. </title> <booktitle> In Proceedings of the Fourth Symposium on Principles and Practices of Parallel Programming, </booktitle> <pages> pages 249-259, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: noting that CRL's integration of data access and synchronization into a single mechanism is not unlike that provided by monitors, a linguistic mechanism suggested by Hoare [28] and Brinch Hansen [8], or other linguistic mechanisms that integrate synchronization and data access (e.g., mutexes in Argus [53], mutex operations in COOL <ref> [15] </ref>, etc.). 34 Chapter 4 CRL Internals This chapter describes the general structure of the prototype CRL implementation used in this thesis. <p> Except for the notion of mapping and unmapping regions, the programming interface CRL presents to the end user is similar to that provided by Shared Regions [68]; the same basic notion of synchronized access (operations) to regions (objects) also exists in other programming systems for hardware-based DSM systems (e.g., COOL <ref> [15] </ref>). The Shared Regions work arrived at this interface from a different set of constraints, however: their goal was to provide software coherence mechanisms on machines that support non-cache-coherent shared memory in hardware.
Reference: [16] <author> Satish Chandra, James R. Larus, and Anne Rogers. </author> <booktitle> Where is Time Spent in Message-Passing and Shared-Memory Programs? In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 61-73, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: For applications in which the complexity of using a message-passing programming model remains manageable, one can often realize better performance using message-passing instead of shared-memory. However, these gains can be relatively modest <ref> [11, 16] </ref> and frequently come at the cost of greatly increased programmer effort. In spite of this fact, message passing environments such as PVM (Parallel Virtual Machine) [24] and MPI (Message Passing Interface) [55] are often the de facto standards for programming multicomputers and networks of workstations. <p> In terms of comparing message passing and shared memory, most other previous work has either compared the performance of applications written and tuned specifically for each programming model <ref> [11, 16] </ref> or looked at the performance gains made possible by augmenting a hardware DSM system with message passing primitives [43].
Reference: [17] <author> Jeffrey S. Chase, Franz G. Amador, Edward D. Lazowska, Henry M. Levy, and Richard J. Littlefield. </author> <title> The Amber System: Parallel Programming on a Network of Multiprocessors. </title> <booktitle> In Proceedings of the Twelfth Symposium on Operating Systems Principles, </booktitle> <pages> pages 147-158, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: CRL runs on two large-scale platforms and has been shown to deliver performance competitive with hardware DSM systems. Several all-software DSM systems that employ an object-based approach have been developed (e.g., Amber <ref> [17] </ref>, Concert [35], Orca [3]). Like these systems, CRL effects coherence at the level of application-defined regions of memory (objects). Any necessary synchronization, data replication, or thread migration functionality is provided automatically at the entry and exit of methods on shared objects. <p> Existing systems of this type either require the use of an entirely new object-oriented language [3, 35] or only allow the use of a subset of an existing one <ref> [17] </ref>. In contrast, CRL is not language specific; the basic CRL interface could easily be provided in any imperative programming language. Scales and Lam [69] have described SAM, a shared object system for distributed memory machines.
Reference: [18] <author> Alan L. Cox, Sandhya Dwarkadas, Pete Keleher, Honghui Lu, Ramakrishnan Ra-jamony, and Willy Zwaenepoel. </author> <title> Software Versus Hardware Shared-Memory Implementation: A Case Study. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 106-117, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Using the modified Alewife CRL implementation, application 1 The SPLASH-2 version of Water used in this thesis incorporates the M-Water modifications suggested by Cox et al. <ref> [18] </ref>. sizes (128-node CM-5). performance can be measured in various reduced-communication-performance scenarios and compared to that obtained with the baseline Alewife CRL implementation. <p> such a comparison is provided). 7.3 Comparative Studies Several researchers have reported results comparing the performance of systems at adjacent levels of the classification presented in Section 2.2 (e.g., all-hardware vs. mostly hardware [13, 27, 85], mostly software vs. all-software [70, 86]), but to our knowledge, only Cox et al. <ref> [18] </ref> have published results from a relatively controlled comparison of hardware and software DSM systems. <p> It is important to note, however, that the three applications described in Section 6.2 were not hand-picked to show a system like CRL in a favorable light: Water was the most challenging application used in a previous study involving a sophisticated page-based mostly software DSM (TreadMarks) <ref> [18] </ref>; Barnes-Hut and related hierarchical n-body methods have been advanced as sufficiently important and challenging to serve as a cornerstone of an argument in favor of aggressive hardware-based DSM systems [72, 73].
Reference: [19] <author> Alan L. Cox and Robert J. Fowler. </author> <title> The Implementation of a Coherent Memory Abstraction on a NUMA Multiprocessor: Experiences with PLATINUM. </title> <booktitle> In Proceedings of the Twelfth Symposium on Operating Systems Principles, </booktitle> <pages> pages 32-44, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: To the best of our knowledge, Midway has only been implemented on a small cluster of workstations connected with an ATM network. A number of other approaches to providing coherence in software on top of non-cache-coherent shared-memory hardware have also been explored <ref> [19, 42] </ref>.
Reference: [20] <author> Alan L. Cox and Robert J. Fowler. </author> <title> Adaptive Cache Coherency for Detecting Migratory Shared Data. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 98-108, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Although there has been some work related to optimizing coherence protocols to improve performance on migratory data in the context of hardware-based DSM systems <ref> [20, 76] </ref>, it remains unclear whether migratory data sharing will dominate for applications other than those studied in this thesis, 2 John D. Kubiatowicz, personal communication, August 1995. 84 and, if so, how frequently can be replaced with carefully selected, efficient message-based primitives.
Reference: [21] <author> Sandhya Dwarkadas, Pete Keleher, Alan L. Cox, and Willy Zwaenepoel. </author> <title> Evaluation of Release Consistent Software Distributed Shared Memory on Emerging Network Technology. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 144-155, </pages> <month> May </month> <year> 1993. </year> <month> 158 </month>
Reference-contexts: Dynamic Approaches Dynamic software DSM systems typically support a more general programming model than their static counterparts, typically allowing multiple independent threads of control to operate within the shared address space <ref> [4, 5, 11, 21, 37, 52, 75] </ref>. Given mechanisms for inter-thread synchronization (e.g., semaphores, barriers), a programmer is able to express essentially any form of parallelism. <p> As was the case for dynamic software DSM systems, hardware DSMs have proven to be capable of executing a wide range of application classes efficiently. Compared to dynamic software DSM systems, their primary advantage appears to by the ability to support extremely frequent, fine-grained interprocessor communication and synchronization <ref> [21] </ref>. 2.2 Mechanisms for DSM This section presents a framework for classifying and comparing dynamic software and hardware DSM systems. The framework identifies three basic mechanisms required to implement dynamic DSM functionality; systems are classified according to whether those mechanisms are implemented in hardware or software. <p> In addition, the software overhead of systems like this (e.g., from manipulating virtual memory mechanisms and computing diffs) can be large enough to significantly impact delivered application performance <ref> [21] </ref>. Midway is a software DSM system based on entry consistency [5]. As discussed in Section 3.4, CRL's programming model is similar to that provided by Midway.
Reference: [22] <author> S. J. Eggers and R. H. Katz. </author> <title> Evaluating the Performance of Four Snooping Cache Coherency Protocols. </title> <booktitle> In Proceedings of the 16th Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1989. </year>
Reference-contexts: By snooping all bus transactions and modifying cache line states appropriately, the caches in each processing node can kept coherent <ref> [22] </ref>. While such systems are well-understood and relatively simple to build, they are not scalable beyond a modest number of processors.
Reference: [23] <author> Michael Jay Franklin. </author> <title> Caching and Memory Management in Client-Server Database Systems. </title> <type> PhD thesis, </type> <institution> University of Wisconsin - Madison, </institution> <month> July </month> <year> 1993. </year>
Reference-contexts: Interestingly, the implementation techniques used in CRL (in particular, the coherence protocol) is quite similar to callback locking <ref> [23] </ref>, an algorithm for maintaining the coherence of cached data in distributed database systems.
Reference: [24] <author> A. Geist, A. Beguelin, J. J. Dongarra, W. Jiang, R. Manchek, and V. S. Sunderam. </author> <title> PVM 3 User's Guide and Reference Manual. </title> <type> Technical Report ORNL/TM-12187, </type> <institution> Oak Ridge National Laboratory, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: However, these gains can be relatively modest [11, 16] and frequently come at the cost of greatly increased programmer effort. In spite of this fact, message passing environments such as PVM (Parallel Virtual Machine) <ref> [24] </ref> and MPI (Message Passing Interface) [55] are often the de facto standards for programming multicomputers and networks of workstations. This is primarily due to the fact that these systems are portable. <p> A CRL 1.0 distribution containing user documentation, the current CRL implementation, and CRL versions of several applications are available on the World Wide Web [33]. 44 In addition to the platforms employed in this thesis (CM-5 and Alewife), the CRL 1.0 distribution can be compiled for use with PVM <ref> [24] </ref> on a network of Sun workstations communicating with one another using TCP. 4.9 Summary The preceding sections described the general structure of a prototype CRL implementation that provides the features and programming model described in Chapter 3.
Reference: [25] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Annotations of the second sort (delimiting the start and end of accesses to shared data) are similar to those necessary in aggressive hardware and software DSM implementations (e.g., those providing release consistency <ref> [25] </ref>) when writing to shared data. CRL requires such annotations whether reading or writing to shared data, similar to entry consistency [5]. Experience with the applications described in this thesis indicates that the additional programming overhead of providing these annotations is quite modest. <p> From this perspective, CRL provides sequential consistency for read and write operations in the same sense that a sequentially consistent hardware-based DSM does for individual loads and stores. In terms of individual loads and stores, CRL provides a memory/coherence model similar to entry [5] or release consistency <ref> [25] </ref>. Loads and stores to global data are allowed only within properly synchronized sections (operations), and modifications to a region are only made visible to other processors after the appropriate release operation (a call to rgn_end_write). <p> Annotations of the second sort (delimiting accesses to shared data) are similar to those necessary in aggressive hardware and software DSM implementations (e.g., those providing release consistency <ref> [25] </ref>) when writing to shared data. CRL requires such annotations whether reading or writing to shared data, similar to entry consistency [5]. As discussed in Section 6.2, experience with the applications described in this thesis indicates that the additional programming overhead of providing these annotations is quite modest.
Reference: [26] <author> A. Gottlieb, R. Grishman, C. P. Kruskal, K. P. McAuliffe, L. Rudolph, and M. Snir. </author> <title> The NYU Ultracomputer Designing a MIMD Shared-Memory Parallel Machine. </title> <journal> IEEE Transactions on Computers, </journal> <pages> pages 175-189, </pages> <month> February </month> <year> 1983. </year>
Reference-contexts: Examples include the NYU Ultracomputer <ref> [26] </ref>, IBM RP3 [62], Stanford DASH [49], and KSR-1 [39]. Other communication mechanisms (e.g., message passing) are synthesized in software using the shared-memory interface. Like dynamic software DSMs, hardware DSM systems support a very general programming model.
Reference: [27] <author> Mark D. Hill, James R. Larus, Steven K. Reinhardt, and David A. Wood. </author> <title> Cooperative Shared Memory: Software and Hardware for Scalable Multiprocessors. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 262-273, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Mostly Hardware As discussed in Section 5.2, the MIT Alewife machine implements a mostly hardware DSM systemprocessor-side mechanisms are always implemented in hardware, but memory-side support is handled in software when widespread sharing is detected [13]. Dir 1 SW and its variations <ref> [27, 85] </ref> are also mostly hardware schemes. The Stanford FLASH multiprocessor [46] and Wisconsin Typhoon architecture [63] represent a different kind of mostly hardware DSM system. <p> vs. message passing) and the use of simulation to obtain controlled comparisons with cache-coherent hardware DSM (when such a comparison is provided). 7.3 Comparative Studies Several researchers have reported results comparing the performance of systems at adjacent levels of the classification presented in Section 2.2 (e.g., all-hardware vs. mostly hardware <ref> [13, 27, 85] </ref>, mostly software vs. all-software [70, 86]), but to our knowledge, only Cox et al. [18] have published results from a relatively controlled comparison of hardware and software DSM systems.
Reference: [28] <author> C. A. R. Hoare. </author> <title> Monitors: An Operating System Structuring Concept. </title> <journal> Communications of the ACM, </journal> <pages> pages 549-557, </pages> <month> October </month> <year> 1974. </year>
Reference-contexts: Finally, it is worth noting that CRL's integration of data access and synchronization into a single mechanism is not unlike that provided by monitors, a linguistic mechanism suggested by Hoare <ref> [28] </ref> and Brinch Hansen [8], or other linguistic mechanisms that integrate synchronization and data access (e.g., mutexes in Argus [53], mutex operations in COOL [15], etc.). 34 Chapter 4 CRL Internals This chapter describes the general structure of the prototype CRL implementation used in this thesis.
Reference: [29] <author> Waldemar Horwat. </author> <title> Concurrent Smalltalk on the Message-Driven Processor. </title> <type> Technical Report 1321, </type> <institution> Massachusetts Institute of Technology, Artificial Intelligence Laboratory, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: is shared by all threads; communication between threads is effected by reading and writing locations in that address space. 19 20 For the purposes of this thesis, programming systems that implement a shared-address space by directly mapping operations on the shared-address space into message-passing constructs (e.g., Split-C [81], Concurrent Smalltalk <ref> [29] </ref>) are considered to be message-passing programming systems, not DSM systems. 2.1 Implementation Techniques This section provides a brief overview of traditional DSM implementation schemes (see multicomputers or networks of workstations.
Reference: [30] <author> John H. Howard, Michael L. Kazar, Sherri G. Menees, David A. Nichols, M. Satya-narayanan, Robert N. Sidebotham, and Michael J. West. </author> <title> Scale and Performance in a Distributed File System. </title> <journal> ACM Transactions on Computer Systems, </journal> <pages> pages 48-61, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: Furthermore, similar schemes have also been used to maintain cache consistency in distributed file systems such as Andrew <ref> [30] </ref> and Sprite [59]. 7.2 Other Software DSM Systems TreadMarks [38] is a second-generation page-based (mostly software) DSM system that implements a release consistent memory model.
Reference: [31] <author> Wilson C. Hsieh. </author> <title> Dynamic Computation Migration in Distributed Shared Memory Systems. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science, </institution> <year> 1995. </year>
Reference-contexts: Interprocessor synchronization can be effected through region operations, barriers, broadcasts, and reductions. Many shared memory applications (e.g., the SPLASH application suites [74, 84]) are written in this style. Although an experimental version of CRL that supports multiple user threads per processor and migration of threads between processors is operational <ref> [31] </ref>, all results reported in this thesis were obtained using the single-threaded version. CRL is implemented as a library against which user programs are linked; it is written entirely in C. <p> Kubiatowicz, personal communication, August 1995. 84 and, if so, how frequently can be replaced with carefully selected, efficient message-based primitives. The extent to which any migratory data problem can be addressed using other techniques such as function shipping [11] and computation migration <ref> [31, 32] </ref> probably also warrants further investigation. 85 Chapter 7 Related Work CRL builds on a large body of research into the construction of distributed shared memory systems. However, as discussed in Section 1.1, four key properties distinguish CRL from other DSM systems: simplicity, portability, efficiency, and scalability.
Reference: [32] <author> Wilson C. Hsieh, Paul Wang, and William E. Weihl. </author> <title> Computation Migration: Enhancing Locality for Distributed-Memory Parallel Systems. </title> <booktitle> In Proceedings of the Fourth Symposium on Principles and Practice of Parallel Programming (PPoPP), </booktitle> <pages> pages 239-248, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: An alternate approach involves moving computation to the data it references. Systems organized along these lines avoid the overhead of frequent remote communication by migrating computation to the node upon which frequently referenced data resides [10, 65]. Implementations utilizing both computation- and data-migration techniques are also possible <ref> [4, 11, 32] </ref>. As with static software DSMs, the high fixed overheads of message-based communication in many current generation systems drive dynamic software DSM implementors toward optimizations that reduce the total number of message sends and receives. <p> Kubiatowicz, personal communication, August 1995. 84 and, if so, how frequently can be replaced with carefully selected, efficient message-based primitives. The extent to which any migratory data problem can be addressed using other techniques such as function shipping [11] and computation migration <ref> [31, 32] </ref> probably also warrants further investigation. 85 Chapter 7 Related Work CRL builds on a large body of research into the construction of distributed shared memory systems. However, as discussed in Section 1.1, four key properties distinguish CRL from other DSM systems: simplicity, portability, efficiency, and scalability.
Reference: [33] <author> Kirk L. Johnson, Joseph Adler, and Sandeep K. Gupta. </author> <title> CRL 1.0 Software Distribution, </title> <month> August </month> <year> 1995. </year> <note> Available on the World Wide Web at URL http://www.pdos.lcs.mit.edu/crl/. 159 </note>
Reference-contexts: A CRL 1.0 distribution containing user documentation, the current CRL implementation, and CRL versions of several applications are available on the World Wide Web <ref> [33] </ref>. 44 In addition to the platforms employed in this thesis (CM-5 and Alewife), the CRL 1.0 distribution can be compiled for use with PVM [24] on a network of Sun workstations communicating with one another using TCP. 4.9 Summary The preceding sections described the general structure of a prototype CRL <p> The techniques used to address out-of-order delivery are discussed in Section A.11; those interested in further detail are referred to the CRL source code <ref> [33] </ref>. A.1 Protocol States and Events The CRL coherence protocol uses eight states apiece in the home-side and remote-state state machines. These states are described in Tables A-1 and A-2, respectively. Transitions between protocol states are caused by events.
Reference: [34] <author> Kirk L. Johnson, M. Frans Kaashoek, and Deborah A. Wallach. </author> <title> CRL: </title> <booktitle> High--Performance All-Software Distributed Sharded Memory. In Proceedings of the Fifteenth Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1995. </year>
Reference: [35] <author> Vijay Karamcheti and Andrew Chien. </author> <title> Concert Efficient Runtime Support for Concurrent Object-Oriented Programming Languages on Stock Hardware. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 598-607, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: CRL runs on two large-scale platforms and has been shown to deliver performance competitive with hardware DSM systems. Several all-software DSM systems that employ an object-based approach have been developed (e.g., Amber [17], Concert <ref> [35] </ref>, Orca [3]). Like these systems, CRL effects coherence at the level of application-defined regions of memory (objects). Any necessary synchronization, data replication, or thread migration functionality is provided automatically at the entry and exit of methods on shared objects. <p> Any necessary synchronization, data replication, or thread migration functionality is provided automatically at the entry and exit of methods on shared objects. Existing systems of this type either require the use of an entirely new object-oriented language <ref> [3, 35] </ref> or only allow the use of a subset of an existing one [17]. In contrast, CRL is not language specific; the basic CRL interface could easily be provided in any imperative programming language. Scales and Lam [69] have described SAM, a shared object system for distributed memory machines.
Reference: [36] <author> Anna Karlin, Kai Li, Mark Manasse, and Susan Owicki. </author> <title> Empirical Studies of Competitive Spinning for A Shared-Memory Multiprocessor. </title> <booktitle> In Proceedings of the Thirteenth Symposium on Operating Systems Principles, </booktitle> <pages> pages 41-55, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: In an implementation that supports multiple user threads per processor in the system, more sophisticated techniques that involve descheduling the requesting thread until the reply has been received may be necessary. Competitive schemes <ref> [36, 54] </ref> in which requesting threads poll for some period of time in hope of receiving a quick response before being descheduled may also be useful in multithread implementations.
Reference: [37] <author> Pete Keleher, Sandhya Dwarkadas, Alan Cox, and Willy Zwaenepoel. </author> <title> Memo: Distributed Shared Memory on Standard Workstations and Operating Systems. </title> <type> Technical Report TR93-206, </type> <institution> Department of Computer Science, Rice University, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: Dynamic Approaches Dynamic software DSM systems typically support a more general programming model than their static counterparts, typically allowing multiple independent threads of control to operate within the shared address space <ref> [4, 5, 11, 21, 37, 52, 75] </ref>. Given mechanisms for inter-thread synchronization (e.g., semaphores, barriers), a programmer is able to express essentially any form of parallelism.
Reference: [38] <author> Pete Keleher, Sandhya Dwarkadas, Alan Cox, and Willy Zwaenepoel. TreadMarks: </author> <title> Distributed Shared Memory on Standard Workstations and Operating Systems. </title> <booktitle> In Proceedings of the 1994 Winter Usenix Conference, </booktitle> <pages> pages 115-131, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: Mostly Software Many software DSM systems are actually mostly software systems in which the hit/miss check functionality is implemented in hardware (e.g., by leveraging off of virtual memory protection mechanisms to provide access control). Typical examples of mostly software systems include Ivy [52], Munin [11], and TreadMarks <ref> [38] </ref>; coherence units in these systems are the size of virtual memory pages. Blizzard [70] implements a similar scheme on the CM-5 at the granularity of individual cache lines. <p> In spite of this performance gap, CM-5 CRL performs comparably with existing mostly software DSM systems. The CM-5 CRL speedup (5.3 on eight processors) for Water (without reductions) is slightly better than that reported for TreadMarks <ref> [38] </ref>, a second-generation page-based mostly software DSM system (a speedup of 4.0 on an ATM network of DECstation 5000/240 workstations, the largest configuration that results have been reported for) 1 . <p> Furthermore, similar schemes have also been used to maintain cache consistency in distributed file systems such as Andrew [30] and Sprite [59]. 7.2 Other Software DSM Systems TreadMarks <ref> [38] </ref> is a second-generation page-based (mostly software) DSM system that implements a release consistent memory model. Unlike many page-based systems, Tread-Marks is implemented entirely in user space; virtual memory protection mechanisms are manipulated through library wrappers around system calls into the kernel.
Reference: [39] <institution> Kendall Square Research. </institution> <type> KSR-1 Technical Summary, </type> <year> 1992. </year>
Reference-contexts: Examples include the NYU Ultracomputer [26], IBM RP3 [62], Stanford DASH [49], and KSR-1 <ref> [39] </ref>. Other communication mechanisms (e.g., message passing) are synthesized in software using the shared-memory interface. Like dynamic software DSMs, hardware DSM systems support a very general programming model. <p> implemented in hardware or software yields the following breakdown of the spectrum of dynamic DSM systems and implementation techniques that have been discussed in the literature. 24 All-Hardware In all-hardware DSM systems, all three of these mechanisms are imple-mented in specialized hardware; the Stanford DASH multiprocessor [49] and KSR 1 <ref> [39] </ref> are typical all-hardware systems. Mostly Hardware As discussed in Section 5.2, the MIT Alewife machine implements a mostly hardware DSM systemprocessor-side mechanisms are always implemented in hardware, but memory-side support is handled in software when widespread sharing is detected [13].
Reference: [40] <author> Alexander C. Klaiber and Henry M. Levy. </author> <title> A Comparison of Message Passing and Shared Memory Architectures for Data Parallel Programs. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 94-105, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Experimental results comparing hardware and software DSM performance are shown for up to 32 processors (Alewife); software DSM results are shown for up to 128 processors (CM-5). Klaiber and Levy <ref> [40] </ref> describe a set of experiments in which data-parallel (C*) applications are compiled such that all interprocessor communication is provided through a very simple library interface.
Reference: [41] <author> K. Knobe, J. Lukas, and G. Steele Jr. </author> <title> Data Optimization: Allocation of Arrays to Reduce Communication on SIMD Machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <pages> pages 102-118, </pages> <year> 1990. </year>
Reference-contexts: Accordingly, this proposal uses the terms static software DSM and dynamic software DSM to refer to members of the first and second class, respectively. Static Approaches Static software DSM systems are typified by compilers for FORTRAN-style scientific codes targeting message-passing multicomputers <ref> [9, 41, 51, 64, 79, 87] </ref>.
Reference: [42] <author> Leonidas I. Kontothanassis and Michael L. Scott. </author> <title> Software Cache Coherence for Large Scale Multiprocessors. </title> <booktitle> In Proceedings of the First Symposium on High-Performance Computer Architecture, </booktitle> <pages> pages 286-295, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: To the best of our knowledge, Midway has only been implemented on a small cluster of workstations connected with an ATM network. A number of other approaches to providing coherence in software on top of non-cache-coherent shared-memory hardware have also been explored <ref> [19, 42] </ref>.
Reference: [43] <author> David Kranz, Kirk Johnson, Anant Agarwal, John Kubiatowicz, and Beng-Hong Lim. </author> <title> Integrating Message-Passing and Shared-Memory: Early Experience. </title> <booktitle> In Proceedings of the Fourth Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 54-63, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: In terms of comparing message passing and shared memory, most other previous work has either compared the performance of applications written and tuned specifically for each programming model [11, 16] or looked at the performance gains made possible by augmenting a hardware DSM system with message passing primitives <ref> [43] </ref>. Such research addresses a different set of issues than those discussed in this thesis, which takes a distributed shared memory programming model as a given and provides a controlled comparison of hardware and software implementations.
Reference: [44] <author> David A. Kranz. </author> <title> ORBIT: An Optimizing Compiler for Scheme. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <month> February </month> <year> 1988. </year> <note> Also available as Technical Report YALEU/DCS/RR-632. </note>
Reference-contexts: The experiments described in this thesis were run on a 32-node Alewife machine running a locally-developed minimal, single-user operating system. All application and library source code was compiled using the Alewife C compiler, which uses a modified version of ORBIT <ref> [44] </ref> for code generation. The Alewife C compiler delivers approximately 90 percent of the performance of gcc -O2 for integer code.
Reference: [45] <author> John Kubiatowicz and Anant Agarwal. </author> <title> Anatomy of a Message in the Alewife Multiprocessor. </title> <booktitle> In Proceedings of the International Conference on Supercomputing, </booktitle> <pages> pages 195-206, </pages> <month> July </month> <year> 1993. </year> <month> 160 </month>
Reference-contexts: In general, Alewife's shared memory system performs quite well, enabling speedups comparable to or better than other scalable hardware-based DSM systems [1, 49]. In addition to providing support for coherent shared memory, Alewife provides the processor with direct access to the interconnection network for sending and receiving messages <ref> [45] </ref>. Efficient mechanisms are provided for sending and receiving both short (register-to-register) messages and long (memory-to-memory, bulk data transfer) messages.
Reference: [46] <author> Jeffrey Kuskin, David Ofelt, Mark Heinrich, John Heinlein, Richard Simoni, Kourosh Gharachorloo, John Chapin, David Nakahira, Joel Baxter, Mark Horowitz, Anoop Gupta, Mendel Rosenblum, and John Hennessy. </author> <title> The Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 302-313, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Dir 1 SW and its variations [27, 85] are also mostly hardware schemes. The Stanford FLASH multiprocessor <ref> [46] </ref> and Wisconsin Typhoon architecture [63] represent a different kind of mostly hardware DSM system.
Reference: [47] <author> J. William Lee. </author> <title> Concord: Re-Thinking the Division of Labor in a Distributed Shared Memory System. </title> <type> Technical Report UW-CSE-93-12-05, </type> <institution> University of Washington, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: CRL could be provided on such systems using similar implementation techniques and defining rgn_map and rgn_unmap to be null macros. In <ref> [47] </ref>, Lee describes Concord, a software DSM system that employs a region-like approach similar to that used in Shared Regions and CRL.
Reference: [48] <author> Charles E. Leiserson, Zahi S. Abuhamdeh, David C. Douglas, Carl R. Feynman, Mahesh N. Ganmukhi, Jeffrey V. Hill, W. Daniel Hillis, Bradley C. Kuszmaul, Margaret A. St. Pierre, David S. Wells, Monica C. Wong, Shaw-Wen Yang, and Robert Zak. </author> <title> The Network Architecture of the Connection Machine CM-5. </title> <booktitle> In Proceedings of the Fourth Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 272-285, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: Furthermore, CRL is implemented entirely as a library against which application programs are linked; CRL requires no special compilers, binary rewriting packages, program execution environments, or other potentially complex, non-portable software tools. Because of these features, CRL scores well in the portability department; porting the original (Thinking Machines' CM-5 <ref> [48] </ref>) CRL implementation to two other platforms (the MIT Alewife machine [1] and a network of Sun workstations communicating with one another using TCP) proved to be straightforward. <p> cache data structures, which together provide a means of resolving region references (i.e., mapping) and caching shared (region) data. 45 Chapter 5 Experimental Platforms This chapter describes the experimental platforms used in the thesis research: Thinking Machines' CM-5 family of multiprocessors and the MIT Alewife machine. 5.1 CM-5 The CM-5 <ref> [48] </ref> is a commercially-available message-passing multicomputer with relatively efficient support for low-overhead, fine-grained message passing. Each CM-5 node contains a SPARC v7 processor (running at 32 MHz) and 32 Mbytes of physical memory.
Reference: [49] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. Lam. </author> <title> The Stanford Dash Multiprocessor. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Examples include the NYU Ultracomputer [26], IBM RP3 [62], Stanford DASH <ref> [49] </ref>, and KSR-1 [39]. Other communication mechanisms (e.g., message passing) are synthesized in software using the shared-memory interface. Like dynamic software DSMs, hardware DSM systems support a very general programming model. <p> whether these mechanisms are implemented in hardware or software yields the following breakdown of the spectrum of dynamic DSM systems and implementation techniques that have been discussed in the literature. 24 All-Hardware In all-hardware DSM systems, all three of these mechanisms are imple-mented in specialized hardware; the Stanford DASH multiprocessor <ref> [49] </ref> and KSR 1 [39] are typical all-hardware systems. Mostly Hardware As discussed in Section 5.2, the MIT Alewife machine implements a mostly hardware DSM systemprocessor-side mechanisms are always implemented in hardware, but memory-side support is handled in software when widespread sharing is detected [13]. <p> A detailed description of the coherence protocol can be found in Appendix A. The prototype CRL implementation employs a fixed-home, directory-based invalidate protocol similar to that used in many hardware (e.g., Alewife [1], DASH <ref> [49] </ref>) and software (e.g., Ivy [52], Munin [11]) DSM systems. In this protocol, coherence actions for a each region are coordinated by a particular home node (in the current CRL implementation, the home node is always the node where the region was created). <p> This organization is motivated by studies indicating that small-scale sharing of data is the common case [12, 61, 83]; data shared more widely is relatively uncommon. In general, Alewife's shared memory system performs quite well, enabling speedups comparable to or better than other scalable hardware-based DSM systems <ref> [1, 49] </ref>. In addition to providing support for coherent shared memory, Alewife provides the processor with direct access to the interconnection network for sending and receiving messages [45]. Efficient mechanisms are provided for sending and receiving both short (register-to-register) messages and long (memory-to-memory, bulk data transfer) messages.
Reference: [50] <author> Shun-tak Leung and John Zahorjan. </author> <title> Improving the Performance of Runtime Paral-lelization. </title> <booktitle> In Proceedings of the Fourth Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 83-91, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Although recent research has yielded some progress on reducing message-based communication overhead [81] and supporting efficient execution of certain kinds of data-dependent communication patterns <ref> [50, 67] </ref>, the applicability of the static software DSM approach appears to remain fairly limited.
Reference: [51] <author> J. Li and M. Chen. </author> <title> Compiling communication-efficient programs for massively parallel machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <pages> pages 361-376, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Accordingly, this proposal uses the terms static software DSM and dynamic software DSM to refer to members of the first and second class, respectively. Static Approaches Static software DSM systems are typified by compilers for FORTRAN-style scientific codes targeting message-passing multicomputers <ref> [9, 41, 51, 64, 79, 87] </ref>.
Reference: [52] <author> Kai Li. IVY: </author> <title> A Shared Virtual Memory System for Parallel Computing. </title> <booktitle> In Proceedings of the International Conference on Parallel Computing, </booktitle> <pages> pages 94-101, </pages> <year> 1988. </year>
Reference-contexts: Dynamic Approaches Dynamic software DSM systems typically support a more general programming model than their static counterparts, typically allowing multiple independent threads of control to operate within the shared address space <ref> [4, 5, 11, 21, 37, 52, 75] </ref>. Given mechanisms for inter-thread synchronization (e.g., semaphores, barriers), a programmer is able to express essentially any form of parallelism. <p> Mostly Software Many software DSM systems are actually mostly software systems in which the hit/miss check functionality is implemented in hardware (e.g., by leveraging off of virtual memory protection mechanisms to provide access control). Typical examples of mostly software systems include Ivy <ref> [52] </ref>, Munin [11], and TreadMarks [38]; coherence units in these systems are the size of virtual memory pages. Blizzard [70] implements a similar scheme on the CM-5 at the granularity of individual cache lines. <p> A detailed description of the coherence protocol can be found in Appendix A. The prototype CRL implementation employs a fixed-home, directory-based invalidate protocol similar to that used in many hardware (e.g., Alewife [1], DASH [49]) and software (e.g., Ivy <ref> [52] </ref>, Munin [11]) DSM systems. In this protocol, coherence actions for a each region are coordinated by a particular home node (in the current CRL implementation, the home node is always the node where the region was created).
Reference: [53] <author> Barbara H. Liskov. </author> <title> Distributed Programming in Argus. </title> <journal> Communications of the ACM, </journal> <pages> pages 300-313, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: Finally, it is worth noting that CRL's integration of data access and synchronization into a single mechanism is not unlike that provided by monitors, a linguistic mechanism suggested by Hoare [28] and Brinch Hansen [8], or other linguistic mechanisms that integrate synchronization and data access (e.g., mutexes in Argus <ref> [53] </ref>, mutex operations in COOL [15], etc.). 34 Chapter 4 CRL Internals This chapter describes the general structure of the prototype CRL implementation used in this thesis.
Reference: [54] <author> Mark S. Manasse, Lyle A. McGeoch, and Daniel D. Sleator. </author> <title> Competitive Algorithms for On-line Problems. </title> <booktitle> In Proceedings of the 20th Annual Symposium on Theory of Computing, </booktitle> <pages> pages 322-333, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: In an implementation that supports multiple user threads per processor in the system, more sophisticated techniques that involve descheduling the requesting thread until the reply has been received may be necessary. Competitive schemes <ref> [36, 54] </ref> in which requesting threads poll for some period of time in hope of receiving a quick response before being descheduled may also be useful in multithread implementations.
Reference: [55] <author> Message Passing Interface Forum. </author> <title> MPI: A Message-Passing Interface Standard, </title> <month> May </month> <year> 1994. </year>
Reference-contexts: However, these gains can be relatively modest [11, 16] and frequently come at the cost of greatly increased programmer effort. In spite of this fact, message passing environments such as PVM (Parallel Virtual Machine) [24] and MPI (Message Passing Interface) <ref> [55] </ref> are often the de facto standards for programming multicomputers and networks of workstations. This is primarily due to the fact that these systems are portable. They require no special hardware, compiler, or operating system support, thus enabling them to run entirely at user level on unmodified, stock systems.
Reference: [56] <author> Ron Minnich, Dan Burns, and Frank Hady. </author> <title> The Memory-Integrated Network Interface. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 11-20, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: While this approach cuts the effective transfer bandwidth roughly in half, it provides significantly reduced latencies for small transfers by avoiding the need for prenegotiation with the receiving node. Networks of workstations with interprocessor communication performance rivaling that of the CM-5 are rapidly becoming reality <ref> [7, 56, 77, 80] </ref>. For example, Thekkath et al. [78] describe the implementation of a specialized data-transfer mechanism implemented on a pair of 25 MHz DECstations connected with a first-generation FORE ATM network.
Reference: [57] <author> David Mosberger. </author> <title> Memory Consistency Models. </title> <booktitle> Operating Systems Review, </booktitle> <pages> pages 18-26, </pages> <month> January </month> <year> 1993. </year> <month> 161 </month>
Reference-contexts: They differ primarily in the sizes of data units used (e.g., cache lines, virtual memory pages), the mechanisms used to implement data replication and migration, and the memory/coherence model they provide to the programmer <ref> [57] </ref> (and thus the details of the protocol used to implement coherence). Systems utilizing a data-shipping paradigm must address the cache coherence problem.
Reference: [58] <author> Todd C. Mowry, Monica S. Lam, and Anoop Gupta. </author> <title> Design and Evaluation of a Compiler Algorithm for Prefetching. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS V), </booktitle> <pages> pages 62-73, </pages> <month> Octbober </month> <year> 1992. </year>
Reference-contexts: While the data migration in such systems is inherently dynamic, for applications with completely static communication patterns, sophisticated compilers can apply prefetching techniques to approximate the behavior of a static DSM system <ref> [58] </ref>. 23 If only a modest number of processors are to be supported (perhaps up to a few dozen), the complexity of a hardware DSM implementation can be reduced substantially through the use of a bus-based organization in which all processor-memory communication traverses a bus shared by all processing nodes (see
Reference: [59] <author> Michael N. Nelson, Brent B. Welch, and John K. Ousterhout. </author> <title> Caching in the Sprite Network File System. </title> <journal> ACM Transactions on Computer Systems, </journal> <pages> pages 134-154, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: Furthermore, similar schemes have also been used to maintain cache consistency in distributed file systems such as Andrew [30] and Sprite <ref> [59] </ref>. 7.2 Other Software DSM Systems TreadMarks [38] is a second-generation page-based (mostly software) DSM system that implements a release consistent memory model. Unlike many page-based systems, Tread-Marks is implemented entirely in user space; virtual memory protection mechanisms are manipulated through library wrappers around system calls into the kernel.
Reference: [60] <author> Rishiyur S. Nikhil. Cid: </author> <title> A Parallel, Shared-memory C for Distributed-Memory Machines. </title> <booktitle> In Proceedings of the Seventh Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1994. </year>
Reference-contexts: All synchronization must be effected through hardware DSM mechanisms. In contrast, CRL is an all-software DSM system in which all communication and synchronization is implemented using software DSM techniques. Of all other software DSM systems, Cid <ref> [60] </ref> is perhaps closest in spirit to CRL. Like CRL, Cid is an all-software DSM system in which coherence is effected on regions (global objects) according to source code annotations provided by the programmer.
Reference: [61] <author> Brian W. O'Krafka and A. Richard Newton. </author> <title> An Empirical Evaluation of Two Memory-Efficient Directory Methods. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 138-147, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: This organization is motivated by studies indicating that small-scale sharing of data is the common case <ref> [12, 61, 83] </ref>; data shared more widely is relatively uncommon. In general, Alewife's shared memory system performs quite well, enabling speedups comparable to or better than other scalable hardware-based DSM systems [1, 49].
Reference: [62] <author> G. F. Pfister, W. C. Brantley, D. A. George, S. L. Harvey, W. J. Kleinfelder, K. P. McAuliffe, E. A. Melton, A. Norton, and J. Weiss. </author> <title> The IBM Research Parallel Processor Prototype (RP3): Introduction and Architecture. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages 764-771, </pages> <month> August </month> <year> 1985. </year>
Reference-contexts: Examples include the NYU Ultracomputer [26], IBM RP3 <ref> [62] </ref>, Stanford DASH [49], and KSR-1 [39]. Other communication mechanisms (e.g., message passing) are synthesized in software using the shared-memory interface. Like dynamic software DSMs, hardware DSM systems support a very general programming model.
Reference: [63] <author> Steve K. Reinhardt, James R. Larus, and David A. Wood. Tempest and Typhoon: </author> <title> User-Level Shared Memory. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 325-336, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Dir 1 SW and its variations [27, 85] are also mostly hardware schemes. The Stanford FLASH multiprocessor [46] and Wisconsin Typhoon architecture <ref> [63] </ref> represent a different kind of mostly hardware DSM system.
Reference: [64] <author> Anne Rogers and Keshav Pingali. </author> <title> Compiling for Distributed Memory Architectures. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <pages> pages 281-298, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Accordingly, this proposal uses the terms static software DSM and dynamic software DSM to refer to members of the first and second class, respectively. Static Approaches Static software DSM systems are typified by compilers for FORTRAN-style scientific codes targeting message-passing multicomputers <ref> [9, 41, 51, 64, 79, 87] </ref>. <p> For applications that do not meet these requirements, the extra cost of resorting to run-time resolution to determine message endpoints <ref> [64] </ref> and increased communication overhead can easily overwhelm any potential benefits due to the exploitation of parallelism.
Reference: [65] <author> Anne Rogers, John H. Reppy, and Laurie J. Hendren. </author> <title> Supporting SPMD Execution for Dynamic Data Structures. </title> <booktitle> In Conference Record of the Fifth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1992. </year> <note> Also appears in Springer Verlag LNCS 757 (pp. 192-207). </note>
Reference-contexts: An alternate approach involves moving computation to the data it references. Systems organized along these lines avoid the overhead of frequent remote communication by migrating computation to the node upon which frequently referenced data resides <ref> [10, 65] </ref>. Implementations utilizing both computation- and data-migration techniques are also possible [4, 11, 32]. As with static software DSMs, the high fixed overheads of message-based communication in many current generation systems drive dynamic software DSM implementors toward optimizations that reduce the total number of message sends and receives.
Reference: [66] <author> Edward Rothberg, Jaswinder Pal Singh, and Anoop Gupta. </author> <title> Working Sets, Cache Sizes, and Node Granularity Issues for Large-Scale Multiprocessors. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 14-25, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: None of the applications employ any prefetching. 60 6.2.1 Blocked LU Blocked LU implements LU factorization of a dense matrix; the version used in this study is based on one described by Rothberg et al. <ref> [66] </ref>. Unless stated otherwise, the results for Blocked LU presented in this thesis were obtained with a 500x500 matrix using 10x10 blocks.
Reference: [67] <author> J. Saltz, R. Mirchandaney, and K. Crowley. </author> <title> Runtime Parallelization and Scheduling of Loops. </title> <journal> IEEE Transactions on Computers, </journal> <pages> pages 603-612, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Although recent research has yielded some progress on reducing message-based communication overhead [81] and supporting efficient execution of certain kinds of data-dependent communication patterns <ref> [50, 67] </ref>, the applicability of the static software DSM approach appears to remain fairly limited.
Reference: [68] <author> Harjinder S. Sandhu, Benjamin Gamsa, and Songnian Zhou. </author> <title> The Shared Regions Approach to Software Cache Coherence on Multiprocessors. </title> <booktitle> In Proceedings of the Fourth Symposium on Principles and Practices of Parallel Programming, </booktitle> <pages> pages 229-238, </pages> <month> May </month> <year> 1993. </year> <month> 162 </month>
Reference-contexts: third section discusses other research that provides comparative results (e.g., all-software vs. mostly software, message passing vs. shared memory). 7.1 Region- and Object-Based Systems Except for the notion of mapping and unmapping regions, the programming interface CRL presents to the end user is similar to that provided by Shared Regions <ref> [68] </ref>; the same basic notion of synchronized access (operations) to regions (objects) also exists in other programming systems for hardware-based DSM systems (e.g., COOL [15]).
Reference: [69] <author> Daniel J. Scales and Monica S. Lam. </author> <title> The Design and Evaluation of a Shared Object System for Distributed Memory Machines. </title> <booktitle> In Proceedings of the First USENIX Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 101-114, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: In contrast, CRL is not language specific; the basic CRL interface could easily be provided in any imperative programming language. Scales and Lam <ref> [69] </ref> have described SAM, a shared object system for distributed memory machines. SAM is based on a new set of primitives that are motivated by optimizations commonly used on distributed memory machines. Like CRL, SAM is 1 Rishiyur S.
Reference: [70] <author> Ioannis Schoinas, Babak Falsafi, Alvin R. Lebeck, Steven K. Reinhardt, James R. Larus, and David A. Wood. </author> <title> Fine-grain Access Control for Distributed Shared Memory. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 297-306, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Typical examples of mostly software systems include Ivy [52], Munin [11], and TreadMarks [38]; coherence units in these systems are the size of virtual memory pages. Blizzard <ref> [70] </ref> implements a similar scheme on the CM-5 at the granularity of individual cache lines. By manipulating the error correcting code bits associated with every memory block, Blizzard can control access on a cache-line by cache-line basis. <p> All-Software In an all-software DSM system, all three of the mechanisms identified above are implemented entirely in software (e.g., Orca [3]). Several researchers have recently reported on experiences with all-software DSM systems obtained by modifying mostly software DSM systems such that the hit/miss check functionality is provided in software <ref> [70, 86] </ref>. 2.3 Discussion Generally speaking, for applications where static software techniques cannot be effectively employed, increased use of software to provide shared-memory functionality tends to decrease application performance because processor cycles spent implementing memory system functionality might otherwise have been spent in application code. <p> simulation to obtain controlled comparisons with cache-coherent hardware DSM (when such a comparison is provided). 7.3 Comparative Studies Several researchers have reported results comparing the performance of systems at adjacent levels of the classification presented in Section 2.2 (e.g., all-hardware vs. mostly hardware [13, 27, 85], mostly software vs. all-software <ref> [70, 86] </ref>), but to our knowledge, only Cox et al. [18] have published results from a relatively controlled comparison of hardware and software DSM systems. <p> Such research addresses a different set of issues than those discussed in this thesis, which takes a distributed shared memory programming model as a given and provides a controlled comparison of hardware and software implementations. Finally, Schoinas et al. <ref> [70] </ref> describe a taxonomy of shared-memory systems that is similar in spirit to that provided in Section 2.2.
Reference: [71] <author> Charles L. Seitz and Wen-King Su. </author> <title> A Family of Routing and Communication Chips Based on the Mosaic. </title> <booktitle> In Proceedings of the 1993 Symposium on Reseach on Integrated Systems, </booktitle> <pages> pages 320-337, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: Each processor/memory node consists of a Sparcle processor [2], an off-the-shelf floating-point unit (FPU), a 64-kilobyte unified instruction/data cache (direct mapped, 16-byte lines), eight megabytes of DRAM, the local portion of the interconnection network (a Caltech Elko series Mesh Routing Chip <ref> [71] </ref>), and a Communications and Memory Management Unit (CMMU). Because Sparcle was derived from a SPARC v7 processor not unlike that used in the CM-5 nodes, basic processor issues (instruction set, timings, etc.) are quite similar on the two machines. <p> by senders and receivers effectively simulates systems in which network interfaces are less and less closely coupled with the processor core (e.g., on the L2 cache bus, memory bus, or an I/O bus) but the network fabric proper retains the relatively favorable latency and bandwidth characteristics of Alewife's EMRC-based network <ref> [71] </ref>.
Reference: [72] <author> Jaswinder Pal Singh, Anoop Gupta, and John L. Hennessy. </author> <title> Implications of Hierarchical N-Body Techniques for Multiprocessor Architecture. </title> <journal> ACM Transactions on Computer Systems, </journal> <pages> pages 141-202, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: managed without adversely affecting performance for relatively simple applications (e.g., those that communicate infrequently or have sufficiently static communication patterns), doing so can be far more difficult for large, complex applications (e.g., those in which data is shared at a very fine grain or according to irregular, dynamic communication patterns) <ref> [72, 73] </ref>. For applications in which the complexity of using a message-passing programming model remains manageable, one can often realize better performance using message-passing instead of shared-memory. However, these gains can be relatively modest [11, 16] and frequently come at the cost of greatly increased programmer effort. <p> In fact, Barnes-Hut and related hierarchical n-body methods present a challenging enough communication workload that they have been used by some authors as the basis of an argument in favor of aggressive hardware support for cache-coherent shared memory <ref> [72, 73] </ref>. 6.2.4 Performance Table 6-5 summarizes the running times for the sequential, CRL, and shared memory (SM) versions of the three applications. <p> light: Water was the most challenging application used in a previous study involving a sophisticated page-based mostly software DSM (TreadMarks) [18]; Barnes-Hut and related hierarchical n-body methods have been advanced as sufficiently important and challenging to serve as a cornerstone of an argument in favor of aggressive hardware-based DSM systems <ref> [72, 73] </ref>.
Reference: [73] <author> Jaswinder Pal Singh, Anoop Gupta, and Marc Levoy. </author> <title> Parallel Visualization Algorithms: Performance and Architectural Implications. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 45-55, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: managed without adversely affecting performance for relatively simple applications (e.g., those that communicate infrequently or have sufficiently static communication patterns), doing so can be far more difficult for large, complex applications (e.g., those in which data is shared at a very fine grain or according to irregular, dynamic communication patterns) <ref> [72, 73] </ref>. For applications in which the complexity of using a message-passing programming model remains manageable, one can often realize better performance using message-passing instead of shared-memory. However, these gains can be relatively modest [11, 16] and frequently come at the cost of greatly increased programmer effort. <p> In fact, Barnes-Hut and related hierarchical n-body methods present a challenging enough communication workload that they have been used by some authors as the basis of an argument in favor of aggressive hardware support for cache-coherent shared memory <ref> [72, 73] </ref>. 6.2.4 Performance Table 6-5 summarizes the running times for the sequential, CRL, and shared memory (SM) versions of the three applications. <p> light: Water was the most challenging application used in a previous study involving a sophisticated page-based mostly software DSM (TreadMarks) [18]; Barnes-Hut and related hierarchical n-body methods have been advanced as sufficiently important and challenging to serve as a cornerstone of an argument in favor of aggressive hardware-based DSM systems <ref> [72, 73] </ref>.
Reference: [74] <author> Jaswinder Pal Singh, Wolf-Dietrich Weber, and Anoop Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title> <booktitle> Computer Architecture News, </booktitle> <pages> pages 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Interprocessor synchronization can be effected through region operations, barriers, broadcasts, and reductions. Many shared memory applications (e.g., the SPLASH application suites <ref> [74, 84] </ref>) are written in this style. Although an experimental version of CRL that supports multiple user threads per processor and migration of threads between processors is operational [31], all results reported in this thesis were obtained using the single-threaded version. <p> All values are averages computed over three consecutive runs. the results for Barnes-Hut presented in this thesis are for a problem size of 4,096 bodies (one-quarter of the suggested base problem size). Other application parameters (Dt and ) are scaled appropriately for the smaller problem size <ref> [74] </ref>. In the CRL version of the code, a region is created for each of the octree data structure elements in the original code: bodies (108 bytes), tree cells (88 bytes), and tree leaves (100 bytes).
Reference: [75] <author> Alfred Z. Spector. </author> <title> Performing Remote Operations Efficiently on a Local Computer Network. </title> <journal> Communications of the ACM, </journal> <pages> pages 246-260, </pages> <month> April </month> <year> 1982. </year>
Reference-contexts: Dynamic Approaches Dynamic software DSM systems typically support a more general programming model than their static counterparts, typically allowing multiple independent threads of control to operate within the shared address space <ref> [4, 5, 11, 21, 37, 52, 75] </ref>. Given mechanisms for inter-thread synchronization (e.g., semaphores, barriers), a programmer is able to express essentially any form of parallelism.
Reference: [76] <author> Per Stenstrom, Mats Brorsson, and Lars Sandberg. </author> <title> An Adaptive Cache Coherence Protocol Optimized for Migratory Sharing. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 109-118, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Although there has been some work related to optimizing coherence protocols to improve performance on migratory data in the context of hardware-based DSM systems <ref> [20, 76] </ref>, it remains unclear whether migratory data sharing will dominate for applications other than those studied in this thesis, 2 John D. Kubiatowicz, personal communication, August 1995. 84 and, if so, how frequently can be replaced with carefully selected, efficient message-based primitives.
Reference: [77] <author> Chandramohan A. Thekkath and Henry M. Levy. </author> <title> Limits to Low-Latency Communication on High-Speed Networks. </title> <journal> ACM Transactions on Computer Systems, </journal> <pages> pages 179-203, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: While this approach cuts the effective transfer bandwidth roughly in half, it provides significantly reduced latencies for small transfers by avoiding the need for prenegotiation with the receiving node. Networks of workstations with interprocessor communication performance rivaling that of the CM-5 are rapidly becoming reality <ref> [7, 56, 77, 80] </ref>. For example, Thekkath et al. [78] describe the implementation of a specialized data-transfer mechanism implemented on a pair of 25 MHz DECstations connected with a first-generation FORE ATM network.
Reference: [78] <author> Chandramohan A. Thekkath, Henry M. Levy, and Edward D. Lazowska. </author> <title> Separating Data and Control Transfer in Distributed Operating Systems. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 2-11, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Networks of workstations with interprocessor communication performance rivaling that of the CM-5 are rapidly becoming reality [7, 56, 77, 80]. For example, Thekkath et al. <ref> [78] </ref> describe the implementation of a specialized data-transfer mechanism implemented on a pair of 25 MHz DECstations connected with a first-generation FORE ATM network. <p> To gauge the sensitivity of CRL's performance to increased communication costs, we compare the behavior of applications running under Alewife CRL and CM-5 CRL. Although the CM-5 is a tightly-coupled multiprocessor, current-generation network-of-workstations technology is capable of providing similar communication performance <ref> [78] </ref>, so the results for CM-5 CRL are indicative of what should be possible for implementations targeting networks of workstations using current- or next-generation technology.
Reference: [79] <author> Chau-Wen Tseng. </author> <title> An Optimizing Fortran D Compiler for MIMD Distributed-Memory Machines. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: Accordingly, this proposal uses the terms static software DSM and dynamic software DSM to refer to members of the first and second class, respectively. Static Approaches Static software DSM systems are typified by compilers for FORTRAN-style scientific codes targeting message-passing multicomputers <ref> [9, 41, 51, 64, 79, 87] </ref>.
Reference: [80] <author> Thorsten von Eicken, Anindya Basu, </author> <title> and Vineet Buch. Low-Latency Communication Over ATM Networks Using Active Messages. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 46-53, </pages> <month> February </month> <year> 1995. </year> <month> 163 </month>
Reference-contexts: While this approach cuts the effective transfer bandwidth roughly in half, it provides significantly reduced latencies for small transfers by avoiding the need for prenegotiation with the receiving node. Networks of workstations with interprocessor communication performance rivaling that of the CM-5 are rapidly becoming reality <ref> [7, 56, 77, 80] </ref>. For example, Thekkath et al. [78] describe the implementation of a specialized data-transfer mechanism implemented on a pair of 25 MHz DECstations connected with a first-generation FORE ATM network.
Reference: [81] <author> Thorsten von Eicken, David E. Culler, Seth Copen Goldstein, and Klaus Erik Schauser. </author> <title> Active Messages: A Mechanism for Integrated Communication and Computation. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 256-266, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: address space that is shared by all threads; communication between threads is effected by reading and writing locations in that address space. 19 20 For the purposes of this thesis, programming systems that implement a shared-address space by directly mapping operations on the shared-address space into message-passing constructs (e.g., Split-C <ref> [81] </ref>, Concurrent Smalltalk [29]) are considered to be message-passing programming systems, not DSM systems. 2.1 Implementation Techniques This section provides a brief overview of traditional DSM implementation schemes (see multicomputers or networks of workstations. <p> For applications that do not meet these requirements, the extra cost of resorting to run-time resolution to determine message endpoints [64] and increased communication overhead can easily overwhelm any potential benefits due to the exploitation of parallelism. Although recent research has yielded some progress on reducing message-based communication overhead <ref> [81] </ref> and supporting efficient execution of certain kinds of data-dependent communication patterns [50, 67], the applicability of the static software DSM approach appears to remain fairly limited. <p> Both CM-5 and Alewife versions can be compiled from a single set of sources with conditionally compiled sections to handle machine-specific details (e.g., different message-passing interfaces). In both the CM-5 and Alewife versions, all communication is effected using active messages <ref> [81] </ref>. Message delivery is assumed to be reliable but in-order delivery is not required. 4.2 Region Identifiers The prototype CRL implementation represents region identifiers using 32-bit unsigned integers. Each region identifier encodes a 24-bit sequence number and an eight-bit home node number using a simple, fixed encoding scheme.
Reference: [82] <author> Deborah A. Wallach, Wilson C. Hsieh, Kirk L. Johnson, M. Frans Kaashoek, and William E. Weihl. </author> <title> Optimistic Active Messages: A Mechanism for Scheduling Communication with Computation. </title> <booktitle> In Proceedings of the Fifth Symposium on Principles and Practices of Parallel Programming, </booktitle> <pages> pages 217-226, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: cost of disabling and reenabling message delivery can be prohibitively high (e.g., 10 microseconds). 126 To address this issue, the prototype CRL implementation employs a special software--based incoming message queue and an associated enabled/disabled flag on each node; these are used to implement an application-specific form of optimistic active messages <ref> [82] </ref>. When the incoming message queue is enabled, the control portions of incoming protocol messages (those elements shown in Figure A-17) are placed on this queue for later processing instead of being acted upon immediately.
Reference: [83] <author> Wolf-Dietrich Weber and Anoop Gupta. </author> <title> Analysis of Cache Invalidation Patterns in Multiprocessors. </title> <booktitle> In Proceedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 243-256, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: This organization is motivated by studies indicating that small-scale sharing of data is the common case <ref> [12, 61, 83] </ref>; data shared more widely is relatively uncommon. In general, Alewife's shared memory system performs quite well, enabling speedups comparable to or better than other scalable hardware-based DSM systems [1, 49].
Reference: [84] <author> Steven Cameron Woo, Moriyoshi Ohara, Evan Torrie, Jaswinder Pal Singh, and Anoop Gupta. </author> <title> The SPLASH-2 Programs: Characterization and Methodological Considerations. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 24-36, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Interprocessor synchronization can be effected through region operations, barriers, broadcasts, and reductions. Many shared memory applications (e.g., the SPLASH application suites <ref> [74, 84] </ref>) are written in this style. Although an experimental version of CRL that supports multiple user threads per processor and migration of threads between processors is operational [31], all results reported in this thesis were obtained using the single-threaded version. <p> It has been used to run a handful of shared-memory-style applications, including two from the SPLASH-2 suite <ref> [84] </ref>, on a 32-node Alewife system and CM-5 systems with up to 128 processors.
Reference: [85] <author> David A. Wood, Satish Chandra, Babak Falsafi, Mark D. Hill, James R. Larus, Alvin R. Lebeck, James C. Lewis, Shubhendu S. Mukherjee, Subbarao Palacharla, and Steven K. Reinhardt. </author> <title> Mechanisms for Cooperative Shared Memory. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 156-167, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Mostly Hardware As discussed in Section 5.2, the MIT Alewife machine implements a mostly hardware DSM systemprocessor-side mechanisms are always implemented in hardware, but memory-side support is handled in software when widespread sharing is detected [13]. Dir 1 SW and its variations <ref> [27, 85] </ref> are also mostly hardware schemes. The Stanford FLASH multiprocessor [46] and Wisconsin Typhoon architecture [63] represent a different kind of mostly hardware DSM system. <p> vs. message passing) and the use of simulation to obtain controlled comparisons with cache-coherent hardware DSM (when such a comparison is provided). 7.3 Comparative Studies Several researchers have reported results comparing the performance of systems at adjacent levels of the classification presented in Section 2.2 (e.g., all-hardware vs. mostly hardware <ref> [13, 27, 85] </ref>, mostly software vs. all-software [70, 86]), but to our knowledge, only Cox et al. [18] have published results from a relatively controlled comparison of hardware and software DSM systems.
Reference: [86] <author> Matthew J. Zekauskas, Wayne A. Sawdon, and Brian N. Bershad. </author> <title> Software Write Detection for a Distributed Shared Memory. </title> <booktitle> In Proceedings of the First USENIX Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 87-100, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: All-Software In an all-software DSM system, all three of the mechanisms identified above are implemented entirely in software (e.g., Orca [3]). Several researchers have recently reported on experiences with all-software DSM systems obtained by modifying mostly software DSM systems such that the hit/miss check functionality is provided in software <ref> [70, 86] </ref>. 2.3 Discussion Generally speaking, for applications where static software techniques cannot be effectively employed, increased use of software to provide shared-memory functionality tends to decrease application performance because processor cycles spent implementing memory system functionality might otherwise have been spent in application code. <p> Systems that take advantage of more complex hardware or operating system functionality (e.g., page-based mostly software DSM systems) can suffer a performance penalty because of inefficient interfaces for accessing such features <ref> [86] </ref>. 26 Function Effect Argument rgn_create Create a new region Size of region to create rgn_delete Delete an existing region Region identifier rgn_map Map a region into the local address space Region identifier rgn_unmap Unmap a mapped region Pointer returned by rgn_map rgn_rid Returns region identifier of a region Pointer returned <p> By bundling an implicit synchronization object with every region, CRL obviates the need for special compiler support of this sort. Both mostly software and all-software versions of Midway have been implemented <ref> [86] </ref>. To the best of our knowledge, Midway has only been implemented on a small cluster of workstations connected with an ATM network. A number of other approaches to providing coherence in software on top of non-cache-coherent shared-memory hardware have also been explored [19, 42]. <p> simulation to obtain controlled comparisons with cache-coherent hardware DSM (when such a comparison is provided). 7.3 Comparative Studies Several researchers have reported results comparing the performance of systems at adjacent levels of the classification presented in Section 2.2 (e.g., all-hardware vs. mostly hardware [13, 27, 85], mostly software vs. all-software <ref> [70, 86] </ref>), but to our knowledge, only Cox et al. [18] have published results from a relatively controlled comparison of hardware and software DSM systems.
Reference: [87] <author> H. Zima, H.-J. Bast, and M. Gerndt. </author> <title> SUPERB: A tool for semi-automatic MIMD/SIMD parallelization. </title> <booktitle> Parallel Computing, </booktitle> <year> 1988. </year> <month> 164 </month>
Reference-contexts: Accordingly, this proposal uses the terms static software DSM and dynamic software DSM to refer to members of the first and second class, respectively. Static Approaches Static software DSM systems are typified by compilers for FORTRAN-style scientific codes targeting message-passing multicomputers <ref> [9, 41, 51, 64, 79, 87] </ref>.
References-found: 87

