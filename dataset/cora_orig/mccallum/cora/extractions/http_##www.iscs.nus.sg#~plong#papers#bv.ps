URL: http://www.iscs.nus.sg/~plong/papers/bv.ps
Refering-URL: 
Root-URL: 
Email: plong@iscs.nus.edu.sg  
Title: On the sample complexity of learning functions with bounded variation matches a known lower bound
Author: Philip M. Long 
Note: this also  
Address: Singapore 119260, Republic of Singapore  
Affiliation: ISCS Department National University of Singapore  
Abstract: We show that the class F BV of [0; 1]-valued functions with total variation at most 1 can be agnos-tically learned with respect to the absolute loss in polynomial time from O 1 ffi examples, matching a known lower bound to within a constant factor. We establish a bound of O 1 the expected error of a polynomial-time algorithm for learning F BV in the prediction model, also matching a known lower bound to within a constant factor. Applying a known algorithm transformation to our prediction algorithm, we obtain a polynomial-time PAC learning algorithm for F BV with a sample complexity bound of O 1 ffi 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> N. Alon, S. Ben-David, N. Cesa-Bianchi, and D. Haus-sler. </author> <title> Scale-sensitive dimensions, uniform convergence, and learnability. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 44(4):616631, </volume> <year> 1997. </year>
Reference-contexts: 1 Introduction The class F BV of <ref> [0; 1] </ref>-valued functions with total variation at most 1 captures the informal notion of similar inputs tending to yield similar outputs. In this paper, we present upper bounds on the sample complexity of learning F BV according to three models. <p> Please refer to that paper for a high level description of Chaining. A packing number for a class of functions measures the number of significantly different behaviors that functions in the class can have on a certain number of domain elements. While packing bounds for F BV were known <ref> [1, 3, 2] </ref>, we needed new bounds for our application (the difference is de scribed immediately after the proof of Lemma 3). <p> One can apply a bound implicit in this analysis (in terms of packing numbers for F ) together with known packing bounds <ref> [1, 3, 2] </ref> to get bounds on the sample complexity of agnostically learning F BV with respect to the quadratic loss similar to the bounds we present in this paper for the absolute loss. 1 However, the bounds of [14] for learning convex classes with respect to the quadratic loss do <p> This improves on the best previously known bound of O 1 * + log 1 matches a known lower bound [22] to within a constant factor. 2 Preliminaries Denote the reals by R, the rationals by Q and the positive integers by N. Let Y = Q " <ref> [0; 1] </ref> Define an example to be an element of Q fi Y , and a sample to be a finite sequence of examples. A learning algorithm takes a sample as input, and outputs a hypothesis, which is a function from Q to Y . <p> As usual [5, 7], our analysis of this algorithm will proceed by showing that uniformly good estimates of the errors of the hypotheses in F BV can be obtained. Choose a countable set Z. Lemma 1 (see [17]) Choose a set G of functions from Z to <ref> [0; 1] </ref>, * &gt; 0, m 2 N for which m 3=* 2 , and a probability distribution D over Z. <p> Then Pr fi fi i=1 ! i=1 ! fi fi fi ! P m The following lemma, which is proved using a chaining argument (see [18] and [15] for descriptions of Chaining), is the main part of our analysis. Lemma 3 Choose m 2 N and G <ref> [1; 1] </ref> m . <p> Packing bounds for F BV are known <ref> [1, 3, 2] </ref>, but to apply Lemma 3 we need bounds for ` 2 that are independent of m, and we are not aware of previously known bounds of this type. <p> Packing bounds for F BV are known [1, 3, 2], but to apply Lemma 3 we need bounds for ` 2 that are independent of m, and we are not aware of previously known bounds of this type. For each m 2 N, define A m = f~a 2 <ref> [0; 1] </ref> m : a 1 ::: a m g C m = f~c 2 [0; 1] m : c 1 = ::: = c m g: For each x 1 &lt; ::: &lt; x m , each ~ f 2 f (f (x 1 ); :::; f (x m )) <p> For each m 2 N, define A m = f~a 2 <ref> [0; 1] </ref> m : a 1 ::: a m g C m = f~c 2 [0; 1] m : c 1 = ::: = c m g: For each x 1 &lt; ::: &lt; x m , each ~ f 2 f (f (x 1 ); :::; f (x m )) : f 2 F BV g has ~a 1 ; ~a 2 2 A m <p> Lemma 8 ([16]) For any (x 1 ; y 1 ); :::; (x m ; y m ) 2 R fi <ref> [0; 1] </ref>, for any set F of functions from R to [0; 1], N (` 2 ; ff; f (jf (x 1 ) y 1 j; :::; jf (x m ) y m j) : f 2 F g) Lemma 9 For any G [0; 1] 2m , N (` 2 <p> Lemma 8 ([16]) For any (x 1 ; y 1 ); :::; (x m ; y m ) 2 R fi <ref> [0; 1] </ref>, for any set F of functions from R to [0; 1], N (` 2 ; ff; f (jf (x 1 ) y 1 j; :::; jf (x m ) y m j) : f 2 F g) Lemma 9 For any G [0; 1] 2m , N (` 2 ; ff; f (g 1 g m+1 ; :::; g m <p> y m ) 2 R fi <ref> [0; 1] </ref>, for any set F of functions from R to [0; 1], N (` 2 ; ff; f (jf (x 1 ) y 1 j; :::; jf (x m ) y m j) : f 2 F g) Lemma 9 For any G [0; 1] 2m , N (` 2 ; ff; f (g 1 g m+1 ; :::; g m g 2m ) : g 2 Gg) Proof: Choose g; h 2 [0; 1] 2m . ` 2 ((g 1 g m+1 ; :::; g m g 2m ); = u t 1 <p> 1 j; :::; jf (x m ) y m j) : f 2 F g) Lemma 9 For any G <ref> [0; 1] </ref> 2m , N (` 2 ; ff; f (g 1 g m+1 ; :::; g m g 2m ) : g 2 Gg) Proof: Choose g; h 2 [0; 1] 2m . ` 2 ((g 1 g m+1 ; :::; g m g 2m ); = u t 1 m X ((g i g m+1 ) (h i h m+i )) 2 v u m i=1 u t 1 m X 2 ((g i h i ) 2 + <p> ) f (u i )j i 1 1 m1 X jf (u i ) f (u i+1 )j which is at most 1 + 1 m1 since f 2 F BV . 5 The PAC model For some countable set X, and some class F of functions from X to <ref> [0; 1] </ref>, following [24], we say that a learning algorithm A (*; ffi)-PAC learns F from m examples for all probability distributions D on X and all f 2 F , if A is given (x 1 ; f (x 1 )); :::; (x m ; f (x m )) for <p> Since we don't know how to use a subset of Simon's proof to establish the result, we've included a proof here. It implies an * 2 log 1 lower bound on the sample complexity of ag-nostically learning the set of all constant functions on <ref> [0; 1] </ref>, and therefore for F BV . Proposition 17 Choose 0 &lt; * 1=20, 0 &lt; ffi 1=125. <p> If F is (*; ffi)-agnostically learnable from m examples, then m 1 ffi . Proof: Choose an odd m. Let P 0 and P 1 be the distributions over X fi <ref> [0; 1] </ref> such that P 0 (f (x; 0)g) = 1=2 + 2* P 1 (f (x; 0)g) = 1=2 2* Suppose b is chosen uniformly at random from f0; 1g, then m examples are generated according to P b and passed to an algorithm, which outputs a hypothesis h.
Reference: [2] <author> P. L. Bartlett, S. R. Kulkarni, and S. E. Posner. </author> <title> Covering numbers for real-valued function classes. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 43(5):1721 1724, </volume> <year> 1997. </year>
Reference-contexts: Please refer to that paper for a high level description of Chaining. A packing number for a class of functions measures the number of significantly different behaviors that functions in the class can have on a certain number of domain elements. While packing bounds for F BV were known <ref> [1, 3, 2] </ref>, we needed new bounds for our application (the difference is de scribed immediately after the proof of Lemma 3). <p> Our agnostic learning bound improves on the bound of O * 2 * + log 1 that is obtained by combining packing bounds from <ref> [2] </ref> with classical uniform convergence bounds (see [17]). Straightforward application of Simon's [21] techniques yields a lower bound that matches our upper bound to within a constant factor (see Proposition 17). <p> One can apply a bound implicit in this analysis (in terms of packing numbers for F ) together with known packing bounds <ref> [1, 3, 2] </ref> to get bounds on the sample complexity of agnostically learning F BV with respect to the quadratic loss similar to the bounds we present in this paper for the absolute loss. 1 However, the bounds of [14] for learning convex classes with respect to the quadratic loss do <p> Packing bounds for F BV are known <ref> [1, 3, 2] </ref>, but to apply Lemma 3 we need bounds for ` 2 that are independent of m, and we are not aware of previously known bounds of this type. <p> The bin boundaries are shown using dotted lines. As in <ref> [2] </ref>, we will make use of an approximatation to A m by a class of piecewise constant functions. 2 For &gt; 0, construct A ;m by dividing the indices f1; :::; mg into bins, putting roughly the first m indices into the first bin, the next m indices into the second
Reference: [3] <author> P. L. Bartlett and P. M. </author> <title> Long. Prediction, learning, uniform convergence, and scale-sensitive dimensions. </title> <journal> Journal of Computer and System Sciences, </journal> <note> to appear. </note>
Reference-contexts: Please refer to that paper for a high level description of Chaining. A packing number for a class of functions measures the number of significantly different behaviors that functions in the class can have on a certain number of domain elements. While packing bounds for F BV were known <ref> [1, 3, 2] </ref>, we needed new bounds for our application (the difference is de scribed immediately after the proof of Lemma 3). <p> One can apply a bound implicit in this analysis (in terms of packing numbers for F ) together with known packing bounds <ref> [1, 3, 2] </ref> to get bounds on the sample complexity of agnostically learning F BV with respect to the quadratic loss similar to the bounds we present in this paper for the absolute loss. 1 However, the bounds of [14] for learning convex classes with respect to the quadratic loss do <p> Packing bounds for F BV are known <ref> [1, 3, 2] </ref>, but to apply Lemma 3 we need bounds for ` 2 that are independent of m, and we are not aware of previously known bounds of this type.
Reference: [4] <author> R. D. Barve and P. M. </author> <title> Long. On the complexity of learning from drifting distributions. Information and Computation, </title> <address> 138(2):101123, </address> <year> 1997. </year>
Reference: [5] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> JACM, </journal> <volume> 36(4):929965, </volume> <year> 1989. </year>
Reference-contexts: As usual <ref> [5, 7] </ref>, our analysis of this algorithm will proceed by showing that uniformly good estimates of the errors of the hypotheses in F BV can be obtained. Choose a countable set Z.
Reference: [6] <author> R. O. Duda and P. E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <year> 1973. </year>
Reference-contexts: The overall prob ability that er P b (h) inf f2F er P b (f ) &gt; * is known to be minimized by any algorithm that, for each input, minimizes the a posteriori probability that this happens given the exam ples <ref> [6] </ref>.
Reference: [7] <author> D. Haussler. </author> <title> Decision theoretic generalizations of the PAC model for neural net and other learning applications. Information and Computation, </title> <address> 100(1):78150, </address> <year> 1992. </year>
Reference-contexts: Each bound is within a constant factor of the best possible. Throughout, we will measure the error of a prediction ^y of a real-valued quantity y with j^y yj. In the agnostic learning model <ref> [7, 12] </ref>, random examples (x 1 ; y 1 ); :::; (x m ; y m ) are drawn from an arbitrary joint distribution P , and the goal of the learning algorithm is to output a function h such that the expected value of jh (x)yj for another pair (x; <p> Let F BV be the set of all functions f from Q to Y for which for all x 1 &lt; ::: &lt; x n , i=1 jf (x i ) f (x i+1 )j 1: 3 Agnostic learning We begin by studying F BV in the agnostic learning model <ref> [7] </ref>. <p> As usual <ref> [5, 7] </ref>, our analysis of this algorithm will proceed by showing that uniformly good estimates of the errors of the hypotheses in F BV can be obtained. Choose a countable set Z.
Reference: [8] <author> D. Haussler, M. Kearns, N. Littlestone, and M. K. War-muth. </author> <title> Equivalence of models for polynomial learnabil-ity. Information and Computation, </title> <address> 95:129161, </address> <year> 1991. </year>
Reference-contexts: Our algorithm is new, but one can modify our proof to establish an upper bound of 2 m for the nearest-neighbor algorithm. Applying a known algorithm transformation <ref> [8] </ref> to our prediction algorithm, one gets a bound of O ( 1 * log 1 sample complexity of learning F BV in the PAC model; i.e., given O ( 1 * log 1 ffi ) independent examples of the behavior of any f 2 F BV , the resulting algorithm,
Reference: [9] <author> D. Haussler, N. Littlestone, and M. K. Warmuth. </author> <title> Predicting f0; 1g-functions on randomly drawn points. Information and Computation, </title> <address> 115(2):129161, </address> <year> 1994. </year>
Reference-contexts: Our sample complexity bound holds for any algorithm that outputs a hypothesis that minimizes the error on the exam ples. We show how to achieve this in polynomial time using linear programming. 1 We thank Peter Bartlett for pointing this out. In the prediction model <ref> [9] </ref>, an algorithm is given exam-ples (x 1 ; f (x 1 )); :::; (x m1 ; f (x m1 )) of the behavior of an unknown function f chosen from a known class F , and outputs a hypothesis h. <p> The idea of analyzing a prediction algorithm by averaging over permutations of the domain elements is from <ref> [9] </ref>. Theorem 13 M (A fl ; F BV ; m) 1 m + 1 Proof: Fix an arbitrary f 2 F BV and a distribution D over Q.
Reference: [10] <author> D. Haussler and P. M. </author> <title> Long. A generalization of Sauer's Lemma. </title> <journal> Journal of Combinatorial Theory, Series A, </journal> <volume> 71(2):219240, </volume> <year> 1995. </year>
Reference: [11] <author> M. J. Kearns and R. E. Schapire. </author> <title> Efficient distribution-free learning of probabilistic concepts. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 48(3):464497, </volume> <year> 1994. </year>
Reference-contexts: i = 1 P ` 2 (~a; ~a 0 ) = u t m j i2B j i ) 2 v u 1 X X ((max a i ) (min a i )) 2 s m j i2B j i2B j j i2B j i2B j 2 Kearns and Schapire <ref> [11] </ref> described an algorithm for learning monotone p-concepts using piecewise constant hypotheses. = j i2B j i2B j j i2B j i2B j ; since ~a 2 A m , completing the proof. Choose m 2 N.
Reference: [12] <author> M. J. Kearns, R. E. Schapire, and L. M. Sellie. </author> <title> Toward efficient agnostic learning. </title> <journal> Machine Learning, </journal> <volume> 17:115 141, </volume> <year> 1994. </year>
Reference-contexts: Each bound is within a constant factor of the best possible. Throughout, we will measure the error of a prediction ^y of a real-valued quantity y with j^y yj. In the agnostic learning model <ref> [7, 12] </ref>, random examples (x 1 ; y 1 ); :::; (x m ; y m ) are drawn from an arbitrary joint distribution P , and the goal of the learning algorithm is to output a function h such that the expected value of jh (x)yj for another pair (x;
Reference: [13] <author> A. N. Kolmogorov and V. M. Tihomirov. </author> <title> *-entropy and *-capacity of sets in functional spaces. </title> <journal> American Mathematical Society Translations (Ser. </journal> <volume> 2), 17:277 364, </volume> <year> 1961. </year>
Reference-contexts: Define N (; *; S) to be the size of the smallest set T X such that each element of S is within distance * (as measured by ) of some element of T . We will use the following general inequalities <ref> [13] </ref>: M (; 2*; S) N (; *; S) M (; *; S): (1) For d; p 2 N, ~v; ~w 2 R d , define ` p (~v; ~w) = 1 d X jv i w i j p : If P is a probability distribution, denote by P m
Reference: [14] <author> W. S. Lee, P. L. Bartlett, and R. C. Williamson. </author> <title> The importance of convexity in learning with squared loss. </title> <booktitle> Proceedings of the 1996 Conference on Computational Learning Theory, </booktitle> <pages> pages 140146, </pages> <year> 1996. </year>
Reference-contexts: Straightforward application of Simon's [21] techniques yields a lower bound that matches our upper bound to within a constant factor (see Proposition 17). Lee, Bartlett and Williamson <ref> [14] </ref> proved a bound of ~ O (d=*) on the sample complexity of agnostically learning any convex class F of functions with respect to the quadratic loss, where d is the pseudo-dimension [17] of F . <p> of packing numbers for F ) together with known packing bounds [1, 3, 2] to get bounds on the sample complexity of agnostically learning F BV with respect to the quadratic loss similar to the bounds we present in this paper for the absolute loss. 1 However, the bounds of <ref> [14] </ref> for learning convex classes with respect to the quadratic loss do not appear to have a counterpart when the absolute loss is used.
Reference: [15] <author> P. M. </author> <title> Long. The complexity of learning according to two models of a drifting environment. </title> <booktitle> Proceedings of the 1998 Conference on Computational Learning Theory, </booktitle> <year> 1998. </year>
Reference-contexts: We show that an algorithm, given O 1 ffi examples, outputs a hypothesis with error at most * worst than the best in F BV with probability at least 1 ffi. This analysis uses a technique called Chaining (see [18]) from Empirical Process Theory. In a companion paper <ref> [15] </ref>, we apply this technique to obtain improved bounds for agnostic concept learning in a drifting environment. Please refer to that paper for a high level description of Chaining. <p> Then Pr fi fi i=1 ! i=1 ! fi fi fi ! P m The following lemma, which is proved using a chaining argument (see [18] and <ref> [15] </ref> for descriptions of Chaining), is the main part of our analysis. Lemma 3 Choose m 2 N and G [1; 1] m .
Reference: [16] <author> B. K. Natarajan. </author> <title> Occam's razor for functions. </title> <booktitle> In Proceedings of the 1993 ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 370376. </pages> <publisher> ACM Press, </publisher> <year> 1993. </year>
Reference: [17] <author> D. Pollard. </author> <title> Convergence of Stochastic Processes. </title> <publisher> Springer Verlag, </publisher> <year> 1984. </year>
Reference-contexts: Our agnostic learning bound improves on the bound of O * 2 * + log 1 that is obtained by combining packing bounds from [2] with classical uniform convergence bounds (see <ref> [17] </ref>). Straightforward application of Simon's [21] techniques yields a lower bound that matches our upper bound to within a constant factor (see Proposition 17). <p> Lee, Bartlett and Williamson [14] proved a bound of ~ O (d=*) on the sample complexity of agnostically learning any convex class F of functions with respect to the quadratic loss, where d is the pseudo-dimension <ref> [17] </ref> of F . <p> As usual [5, 7], our analysis of this algorithm will proceed by showing that uniformly good estimates of the errors of the hypotheses in F BV can be obtained. Choose a countable set Z. Lemma 1 (see <ref> [17] </ref>) Choose a set G of functions from Z to [0; 1], * &gt; 0, m 2 N for which m 3=* 2 , and a probability distribution D over Z. <p> 9g 2 G; fi fi m i=1 ! Z g (z) dD (z) fi fi &gt; * = 2 sup U &lt; ~ : 9g 2 G; fi fi 1 m X i (g (z i ) g (z m+i )) fi fi &gt; *=2 = : Lemma 2 (see <ref> [17] </ref>) Let Y 1 ; :::; Y m be independent random variables taking values in [a 1 ; b 1 ]; :::; [a m ; b m ] respectively. <p> The pseudo-dimension <ref> [17] </ref> of G is the length of the longest sequence shattered by G. Lemma 5 ([10]) If V R is finite and G V m has pseudo-dimension d, then jGj (emjV j=d) d .
Reference: [18] <author> D. Pollard. </author> <title> Empirical Processes : Theory and Applications, </title> <booktitle> volume 2 of NSF-CBMS Regional Conference Series in Probability and Statistics. </booktitle> <institution> Institute of Math. Stat. and Am. Stat. Assoc., </institution> <year> 1990. </year>
Reference-contexts: We show that an algorithm, given O 1 ffi examples, outputs a hypothesis with error at most * worst than the best in F BV with probability at least 1 ffi. This analysis uses a technique called Chaining (see <ref> [18] </ref>) from Empirical Process Theory. In a companion paper [15], we apply this technique to obtain improved bounds for agnostic concept learning in a drifting environment. Please refer to that paper for a high level description of Chaining. <p> Then Pr fi fi i=1 ! i=1 ! fi fi fi ! P m The following lemma, which is proved using a chaining argument (see <ref> [18] </ref> and [15] for descriptions of Chaining), is the main part of our analysis. Lemma 3 Choose m 2 N and G [1; 1] m .
Reference: [19] <author> S. E. Posner and S. R. Kulkarni. </author> <title> On-line learning of functions of bounded variation under various sampling schemes. </title> <booktitle> In Proceedings of the 1993 Conference on Computational Learning Theory, </booktitle> <pages> pages 439445, </pages> <year> 1993. </year>
Reference-contexts: We prove a 1 m + 1 m (m1) upper bound on the expected error of a polynomial-time algorithm for learn ing F BV in this model, improving on the best previously known bound of O m <ref> [19] </ref>, and matching a known lower bound [19] of 1 2m to within a constant factor. Our algorithm is new, but one can modify our proof to establish an upper bound of 2 m for the nearest-neighbor algorithm. <p> We prove a 1 m + 1 m (m1) upper bound on the expected error of a polynomial-time algorithm for learn ing F BV in this model, improving on the best previously known bound of O m <ref> [19] </ref>, and matching a known lower bound [19] of 1 2m to within a constant factor. Our algorithm is new, but one can modify our proof to establish an upper bound of 2 m for the nearest-neighbor algorithm.
Reference: [20] <author> H. L. Royden. </author> <title> Real Analysis. </title> <publisher> Macmillan, </publisher> <year> 1963. </year>
Reference-contexts: &lt; x m , each ~ f 2 f (f (x 1 ); :::; f (x m )) : f 2 F BV g has ~a 1 ; ~a 2 2 A m and ~c 2 C m such that ~ f = ~c + ~a 1 ~a 2 (see <ref> [20] </ref>), so we will work on A m (C m is easy). b b b ? i and the corresponding ~a 0 (pictured using squares) from the proof of Lemma 4. The bin boundaries are shown using dotted lines. <p> be put in 1-1 correspondence with elements of A 1=d1=e;fi;d1=e , and therefore jA ;fi;m j (ed1=e) b1=fic : Since each ~ f 2 F has ~c 2 C m ; ~a 1 ; ~a 2 2 A m such that ~ f = ~c + ~a 1 ~a 2 <ref> [20] </ref>, By Lemma 4, N (` 2 ; + fi; A m ) jA ;fi;m j: Substituting the definitions of fi and , we get N (` 2 ; ff=4; A m ) jA ;fi;m j; and plugging into (2), we get N (` 2 ; ff; F ) N (`
Reference: [21] <author> H. U. Simon. </author> <title> General lower bounds on the number of examples needed for learning probabilistic concepts. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 52(2):239 254, </volume> <year> 1996. </year>
Reference-contexts: Our agnostic learning bound improves on the bound of O * 2 * + log 1 that is obtained by combining packing bounds from [2] with classical uniform convergence bounds (see [17]). Straightforward application of Simon's <ref> [21] </ref> techniques yields a lower bound that matches our upper bound to within a constant factor (see Proposition 17). <p> The class of all constant functions has pseudo-dimension 1 and is convex, but, again, straightforward application of Simon's <ref> [21] </ref> techniques yields a lower bound of ( 1 * 2 ) on the sample complexity of agnostically learning this class with respect to the absolute loss (see Proposition 17). <p> If a coin with probability 1=2 + fi of coming up heads is flipped an odd number m independent times, the probability that it comes up heads fewer than m=2 times is at least e 5fi 2 m4fi m =11. The following proof makes heavy use of Simon's <ref> [21] </ref> ideas, and the result can easily be generalized in many ways. Since we don't know how to use a subset of Simon's proof to establish the result, we've included a proof here.
Reference: [22] <author> H. U. Simon. </author> <title> Bounds on the number of examples needed for learning functions. </title> <journal> SIAM Journal on Computing, </journal> <volume> 26(3):751763, </volume> <year> 1997. </year>
Reference-contexts: This improves on the best previously known bound of O 1 * + log 1 matches a known lower bound <ref> [22] </ref> to within a constant factor. 2 Preliminaries Denote the reals by R, the rationals by Q and the positive integers by N.
Reference: [23] <author> P. Vaidya. </author> <title> A new algorithm for minimizing convex functions over convex sets. </title> <booktitle> Proceedings of the 30th Annual Symposium on the Foundations of Computer Science, </booktitle> <pages> pages 338343, </pages> <year> 1989. </year>
Reference-contexts: Theorem 10 provides a sample complexity bound for any algorithm that outputs a hypothesis in F BV minimizing the error on the sample. Here, using standard techniques, we describe such an algorithm that uses linear programming. Applying efficient linear programming algorithms (e.g. <ref> [23] </ref>), this algorithm takes time polynomial in the size of its input, where rationals are represented by writing their numerators and denominators in binary.
Reference: [24] <author> L. G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11):11341142, </volume> <year> 1984. </year>
Reference-contexts: i )j i 1 1 m1 X jf (u i ) f (u i+1 )j which is at most 1 + 1 m1 since f 2 F BV . 5 The PAC model For some countable set X, and some class F of functions from X to [0; 1], following <ref> [24] </ref>, we say that a learning algorithm A (*; ffi)-PAC learns F from m examples for all probability distributions D on X and all f 2 F , if A is given (x 1 ; f (x 1 )); :::; (x m ; f (x m )) for x 1 ;
References-found: 24

