URL: http://www.cs.umd.edu/~tseng/papers/ipps97.ps
Refering-URL: http://www.cs.umd.edu/projects/cosmic/papers.html
Root-URL: 
Email: keleher@cs.umd.edu tseng@cs.umd.edu  
Title: Enhancing Software DSM for Compiler-Parallelized Applications  
Author: Pete Keleher Chau-Wen Tseng 
Address: College Park, MD 20742  
Affiliation: Dept. of Computer Science University of Maryland  
Abstract: Current parallelizing compilers for message-passing machines only support a limited class of data-parallel applications. One method for eliminating this restriction is to combine powerful shared-memory parallelizing compilers with software distributed-shared-memory (DSM) systems. We demonstrate such a system by combining the SUIF parallelizing compiler and the CVM software DSM. Innovations of the system include compiler-directed techniques that: 1) combine synchronization and parallelism information communication on parallel task invocation, 2) employ customized routines for evaluating reduction operations, and 3) select a hybrid update protocol that pre-sends data by flushing updates at barriers. For applications with sufficient granularity of parallelism, these optimizations yield very good eight processor speedups on an IBM SP-2 and DEC Alpha cluster, usually matching or exceeding the speedup of equivalent HPF and message-passing versions of each program. Flushing updates, in particular, eliminates almost all nonlocal memory misses and improves performance by 13% on average. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B. Bershad, M. Zekauskas, and W. Sawdon. </author> <title> The Midway distributed shared memory system. </title> <booktitle> In Proceedings of the '93 CompCon Conference, </booktitle> <pages> pages 528-537, </pages> <month> Feb. </month> <year> 1993. </year>
Reference-contexts: Related Work While there has been a large amount of research on software DSMs [2, 7, 22], we are aware of only a few projects combining compilers and software DSMs. Bershad et al. maintain coherence by using a compiler to update a software dirty bit on shared-memory accesses <ref> [1] </ref>. Scales et al. designed Shasta, a software-only approach that supports fine-grain coherence through binary rewriting [24]. Using a number of optimizations, Shasta limits software overhead to within 5-35% for the Splash benchmarks on a DEC Alpha cluster.
Reference: [2] <author> J. Carter, J. Bennett, and W. Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: False Sharing False sharing occurs when two or more processors access different variables within a page, with at least one of the accesses being a write. False sharing is problematic for software DSMs because of the large page-size coherence units. Multiple-writer coherence protocols <ref> [2] </ref> such as that implemented by CVM avoid false sharing by allowing two or more processors to simultaneously modify local copies of the same shared page. These concurrent modifications are merged using diffs to summarize the updates. <p> Run-time improvements include customizing the message library, retargeting CVM to support compiler-parallelized applications, and improved reduction support. These suggestions are described in greater detail in an earlier SUIF/CVM study [17]. 6. Related Work While there has been a large amount of research on software DSMs <ref> [2, 7, 22] </ref>, we are aware of only a few projects combining compilers and software DSMs. Bershad et al. maintain coherence by using a compiler to update a software dirty bit on shared-memory accesses [1].
Reference: [3] <author> S. Chandra and J. Larus. </author> <title> HPF on fine-grain distributed shared memory: Early experience. </title> <booktitle> In Proceedings of the Ninth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> San Jose, CA, </address> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: The flush protocol in CVM also relies on repetitive communication patterns to improve performance, but handles it naturally as an extension of its multi-writer protocol. Chandra and Larus evaluated combining the PGI HPF compiler and the Tempest software DSM system <ref> [3] </ref>. The PGI HPF compiler can generate either message-passing code or shared-memory code relying on Tempest. Preliminary results on a network of workstations connected by Myrinet indicates shared-memory versions of dense matrix programs achieve performance close to the message-passing codes generated. <p> However, since CVM supports multiple writers, the main performance advantage is in avoiding page faults traps for shared data. Large units of coherence can exploit spatial locality, so the PGI compiler can actually improve performance by using larger coherence units <ref> [3] </ref>. In comparison to PGI/Tempest, we implement and evaluate enhancements to the software DSM to improve performance. We are also able to demonstrate good performance on architectures with much longer communication latencies than the Myrinet interconnect, a more difficult task.
Reference: [4] <author> A. Cox, S. Dwarkadas, H. Lu, and W. Zwaenepoel. </author> <title> Evaluating the performance of software distributed shared memory as a target for parallelizing compilers. </title> <booktitle> In Proceedings of the 11th International Parallel Processing Symposium, </booktitle> <address> Geneva, Switzerland, </address> <month> Apr. </month> <year> 1997. </year>
Reference-contexts: Concurrent with our work, Cox et al. conducted an experimental study to evaluate the performance of TreadMarks as a target for the Forge SPF shared-memory compiler from APR <ref> [4] </ref>. They compared its performance against the message-passing code generated by the Forge xHPF compiler, as well as hand-coded shared-memory and message-passing versions of the program. Results show that SPF/TreadMarks is slightly less efficient for dense-matrix programs, but outperforms compiler-generated message-passing code for irregular programs.
Reference: [5] <author> R. Das, M. Uysal, J. Saltz, and Y.-S. Hwang. </author> <title> Communication optimizations for irregular scientific computations on distributed memory architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 22(3) </volume> <pages> 462-479, </pages> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: Results show that with suitable extensions to the coherence protocol, the shared-memory program was able to match the performance of the optimized message-passing program utilizing Chaos <ref> [5] </ref>. The authors point out that a compiler like SUIF can take advantage of the extensible coherence protocol to improve performance. Compared with their approach, we use a single general coherence protocol in the CVM for all applications, exploiting compile-time analysis to provide hints to the software DSM.
Reference: [6] <author> S. Dwarkadas, A. Cox, and W. Zwaenepoel. </author> <title> An integrated compile-time/run-time software distributed shared memory system. </title> <booktitle> In Proceedings of the Eighth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VIII), </booktitle> <address> Boston, MA, </address> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: Their proposal is similar to portions of our SUIF/CVM interface; we differ in requiring less analysis and by providing a more fine-grained API to control the behavior of individual pages. Dwarkadas et al. applied compiler analysis to explicitly parallel programs to improve their performance using a software DSM <ref> [6] </ref>. By combining analysis in the ParaScope programming environment with TreadMarks, they were able to compute data access patterns at compile time and use it to help the runtime system aggregate communication and synchronization. Results for five programs were within 9% of equivalent HPF programs on the IBM SP-2.
Reference: [7] <author> S. Dwarkadas, P. Keleher, A. Cox, and W. Zwaenepoel. </author> <title> Evaluation of release consistent software distributed shared memory on emerging network technology. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 244-255, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Processor q then piggybacks on the release-acquire message to p write notices for all intervals named in q's current vector timestamp but not in the vector timestamp it received from p. Experiments show alternative implementations of release consistency generally cause more communication than LRC <ref> [7] </ref>. False Sharing False sharing occurs when two or more processors access different variables within a page, with at least one of the accesses being a write. False sharing is problematic for software DSMs because of the large page-size coherence units. <p> Run-time improvements include customizing the message library, retargeting CVM to support compiler-parallelized applications, and improved reduction support. These suggestions are described in greater detail in an earlier SUIF/CVM study [17]. 6. Related Work While there has been a large amount of research on software DSMs <ref> [2, 7, 22] </ref>, we are aware of only a few projects combining compilers and software DSMs. Bershad et al. maintain coherence by using a compiler to update a software dirty bit on shared-memory accesses [1].
Reference: [8] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: In order to simplify the comparison process, however, we do not use either of these techniques in this study. Memory Consistency - CVM's primary protocol implements a multiple-writer version of lazy release consistency [15], which is a derivation of release consistency (RC) <ref> [8] </ref>. Release consistency a processor to delay making modifications to shared data visible to other processors until special acquire or release synchronization accesses occur. The propagation of modifications can thus be postponed until the next synchronization operation takes effect. <p> Programs produce the same results for the two memory models provided that (i) all synchronization operations use system-supplied primitives, and (ii) there is a release-acquire pair between conflicting ordinary accesses to the same memory location on different processors <ref> [8] </ref>. In practice, most shared-memory programs require little or no modifications to meet these requirements. Lazy release consistency (LRC) allows the propagation of modifications to be further postponed until the time of the next subsequent acquire of a released synchronization variable.
Reference: [9] <author> E. Granston and H. Wishoff. </author> <title> Managing pages in shared virtual memory systems: Getting the compiler into the game. </title> <booktitle> In Proceedings of the 1993 ACM International Conference on Supercomputing, </booktitle> <address> Tokyo, Japan, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: Previous researchers have examined shared-memory compilation issues such as improving locality [19] and reducing false sharing [25], but their techniques were mostly needed for single-writer hardware coherence protocols. Granston and Wishoff suggest a number of compiler optimizations for software DSMs <ref> [9] </ref>. These include tiling loop iterations so computation is on partitioned matching page boundaries, aligning arrays to pages, and inserting hints to use weak coherence. No implementation or experiments are provided.
Reference: [10] <author> M. Gupta, S. Midkiff, E. Schonberg, V. Seshadri, D. Shields, K.-Y. Wang, W.-M. Ching, and T. Ngo. </author> <title> An HPF compiler for the IBM SP2. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <address> San Diego, CA, </address> <month> Nov. </month> <year> 1995. </year>
Reference-contexts: High Performance Fortran (HPF) applications were created by manually translating from Fortran to Fortran 90, with HPF data decompositions added for each array. On the IBM SP-2 we used the IBM HPF compiler <ref> [10] </ref> with the -O2 flag. On the DEC cluster we used the DEC HPF compiler f90 version 2.0-1 with the -O2 -wsf -fast flags. Message-passing versions of each program were created using calls to communication routines specified under Message Passing Interface (MPI).
Reference: [11] <author> M. Hall, S. Amarasinghe, B. Murphy, S. Liao, and M. Lam. </author> <title> Detecting coarse-grain parallelism using an interprocedural parallelizing compiler. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <address> San Diego, CA, </address> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: We believe that the combination of shared-memory parallelizing compilers and sophisticated runtime systems presents one of the most promising approaches towards addressing this key problem. This paper presents our experience using the CVM [14] software distributed-shared-memory (DSM) system as a compilation target for the SUIF <ref> [11] </ref> shared-memory compiler. SUIF automatically parallelizes sequential applications and allows users to benefit from sophisticated program analysis. The use of CVM as a compilation target hides the details of the underlying message-passing architecture and allows the compiler-generated code to assume shared memory semantics. <p> When all necessary diffs have been received, they are applied to the page in increasing timestamp order. 3. Compiler/Software DSM Interface Our system consists of the Stanford SUIF parallelizing compiler <ref> [11] </ref> and the CVM software DSM system [14]. A simple interface was produced by porting the SUIF run-time system to the CVM API. <p> The large number of customized coherence protocols they used for each application does not appear to be necessary for compiler-parallelized applications. The SUIF compiler draws on a large body of work on techniques for identifying parallelism <ref> [11] </ref>. Previous researchers have examined shared-memory compilation issues such as improving locality [19] and reducing false sharing [25], but their techniques were mostly needed for single-writer hardware coherence protocols. Granston and Wishoff suggest a number of compiler optimizations for software DSMs [9].
Reference: [12] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> Aug. </month> <year> 1992. </year>
Reference-contexts: On the IBM SP-2 we used the MPL version 2 implementation of MPI; on the DEC Alpha cluster we used the MPICH version 1.0.12 implementation of MPI. MPI versions of adi, dot, expl, and jacobi were generated using the Fortran D compiler <ref> [12] </ref>. Previous experiments show the resulting programs achieve performance close to optimized hand-written message-passing programs [13]. MPI versions of mult and rb were created by hand. These programs represent message-passing programs written with a reasonable amount of effort, not programs highly customized for performance.
Reference: [13] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Preliminary experiences with the Fortran D compiler. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <address> Portland, OR, </address> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: MPI versions of adi, dot, expl, and jacobi were generated using the Fortran D compiler [12]. Previous experiments show the resulting programs achieve performance close to optimized hand-written message-passing programs <ref> [13] </ref>. MPI versions of mult and rb were created by hand. These programs represent message-passing programs written with a reasonable amount of effort, not programs highly customized for performance.
Reference: [14] <author> P. Keleher. </author> <title> The relative importance of concurrent writers and weak consistency models. </title> <booktitle> In 16th International Conference on Distributed Computing Systems, </booktitle> <address> Hong Kong, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: We believe that the combination of shared-memory parallelizing compilers and sophisticated runtime systems presents one of the most promising approaches towards addressing this key problem. This paper presents our experience using the CVM <ref> [14] </ref> software distributed-shared-memory (DSM) system as a compilation target for the SUIF [11] shared-memory compiler. SUIF automatically parallelizes sequential applications and allows users to benefit from sophisticated program analysis. <p> When all necessary diffs have been received, they are applied to the page in increasing timestamp order. 3. Compiler/Software DSM Interface Our system consists of the Stanford SUIF parallelizing compiler [11] and the CVM software DSM system <ref> [14] </ref>. A simple interface was produced by porting the SUIF run-time system to the CVM API.
Reference: [15] <author> P. Keleher, A. L. Cox, and W. Zwaenepoel. </author> <title> Lazy release consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: We therefore expect the performance of the fully functional system to improve over the existing base. In order to simplify the comparison process, however, we do not use either of these techniques in this study. Memory Consistency - CVM's primary protocol implements a multiple-writer version of lazy release consistency <ref> [15] </ref>, which is a derivation of release consistency (RC) [8]. Release consistency a processor to delay making modifications to shared data visible to other processors until special acquire or release synchronization accesses occur. The propagation of modifications can thus be postponed until the next synchronization operation takes effect.
Reference: [16] <author> P. Keleher, S. Dwarkadas, A. Cox, and W. Zwaenepoel. Treadmarks: </author> <title> Distributed shared memory on standard workstations and operating systems. </title> <booktitle> In Proceedings of the 1994 Winter Usenix Conference, </booktitle> <pages> pages 115-131, </pages> <month> Jan. </month> <year> 1994. </year>
Reference-contexts: CVM The DSM target used in this work is CVM, a software DSM that supports multiple protocols and consistency models. Like commercially available systems such as TreadMarks <ref> [16] </ref>, CVM is written entirely as a user-level library and runs on most UNIX-like systems. Unlike TreadMarks, CVM was created specifically as a platform for protocol experimentation. The system is written in C++, and opaque interfaces are strictly enforced between different functional units of the system whenever possible.
Reference: [17] <author> P. Keleher and C.-W. Tseng. </author> <title> Improving the compiler/software DSM interface: Preliminary experiences. </title> <booktitle> In Proceedings of the First SUIF Compiler Workshop, </booktitle> <address> Stanford, CA, </address> <month> Jan. </month> <year> 1996. </year>
Reference-contexts: Because shared data in CVM must be global, we also implemented passes in the compiler to promote all shared local variables to globals, and to pack all shared global variables into a single contiguous global structure <ref> [17] </ref>. 3.1. Optimizations The simple interface presented for SUIF and CVM produces a working system, but contains many inefficiencies, some of which may be eliminated with enhancements to the software DSM that rely on lightweight compiler analysis. <p> Run-time improvements include customizing the message library, retargeting CVM to support compiler-parallelized applications, and improved reduction support. These suggestions are described in greater detail in an earlier SUIF/CVM study <ref> [17] </ref>. 6. Related Work While there has been a large amount of research on software DSMs [2, 7, 22], we are aware of only a few projects combining compilers and software DSMs.
Reference: [18] <author> C. Koelbel, D. Loveman, R. Schreiber, G. Steele, Jr., and M. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> The MIT Press, </publisher> <address> Cam-bridge, MA, </address> <year> 1994. </year>
Reference-contexts: 1. Introduction Increasingly powerful processor and network architectures make so-called meta-computers (loosely-coupled computers communicating via messages) a tempting platform on which to run large parallel and distributed applications. Unfortunately, writing efficient message-passing programs is difficult, error-prone, and tedious, and data-parallel languages such as High Performance Fortran (HPF) <ref> [18] </ref> may prove overly restrictive. We believe that the combination of shared-memory parallelizing compilers and sophisticated runtime systems presents one of the most promising approaches towards addressing this key problem.
Reference: [19] <author> E. Markatos and T. LeBlanc. </author> <title> Using processor affinity in loop scheduling on shared-memory multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 5(4) </volume> <pages> 379-400, </pages> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: The large number of customized coherence protocols they used for each application does not appear to be necessary for compiler-parallelized applications. The SUIF compiler draws on a large body of work on techniques for identifying parallelism [11]. Previous researchers have examined shared-memory compilation issues such as improving locality <ref> [19] </ref> and reducing false sharing [25], but their techniques were mostly needed for single-writer hardware coherence protocols. Granston and Wishoff suggest a number of compiler optimizations for software DSMs [9].
Reference: [20] <author> R. Mirchandaney, S. Hiranandani, and A. Sethi. </author> <title> Improving the performance of DSM systems via compiler involvement. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <address> Washington, DC, </address> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: No implementation or experiments are provided. CVM uses a multi-writer release consistency protocol, so these optimizations are not as vital as for a sequentially-consistent single-writer protocol. Mirchandaney et al. described the design of a compiler for TreadMarks, a software DSM <ref> [20] </ref>. They propose section locks and broadcast barriers to guide eager updates of data, integrating send, recv and broadcast operations with the software DSM, and reductions based on multiple-writer protocols.
Reference: [21] <author> S. Mukherjee, S. Sharma, M. Hill, J. Larus, A. Rogers, and J. Saltz. </author> <title> Efficient support for irregular applications on distributed-memory machines. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Results, however, show that this is not a problem since the software communication overhead usually dominates the memory management overhead. Mukherjee et al. compared the performance of explicit message-passing programs with shared-memory programs <ref> [21] </ref> on Typhoon, a Flexible-Shared-Memory machine implemented on top of a CM-5 [23]. Results show that with suitable extensions to the coherence protocol, the shared-memory program was able to match the performance of the optimized message-passing program utilizing Chaos [5].
Reference: [22] <author> B. Nitzberg and V. Lo. </author> <title> Distributed shared memory: A survey of issues and algorithms. </title> <journal> IEEE Computer, </journal> <volume> 24(8) </volume> <pages> 52-60, </pages> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: Run-time improvements include customizing the message library, retargeting CVM to support compiler-parallelized applications, and improved reduction support. These suggestions are described in greater detail in an earlier SUIF/CVM study [17]. 6. Related Work While there has been a large amount of research on software DSMs <ref> [2, 7, 22] </ref>, we are aware of only a few projects combining compilers and software DSMs. Bershad et al. maintain coherence by using a compiler to update a software dirty bit on shared-memory accesses [1].
Reference: [23] <author> S. K. Reinhardt, J. R. Larus, and D. A. Wood. Tempest and Typhoon: </author> <title> User-level shared memory. </title> <booktitle> In Proceedings of the 21th International Symposium on Computer Architecture, </booktitle> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: Results, however, show that this is not a problem since the software communication overhead usually dominates the memory management overhead. Mukherjee et al. compared the performance of explicit message-passing programs with shared-memory programs [21] on Typhoon, a Flexible-Shared-Memory machine implemented on top of a CM-5 <ref> [23] </ref>. Results show that with suitable extensions to the coherence protocol, the shared-memory program was able to match the performance of the optimized message-passing program utilizing Chaos [5]. The authors point out that a compiler like SUIF can take advantage of the extensible coherence protocol to improve performance. <p> Tempest is significantly more 9 efficient than message-passing for programs with irregular access patterns not analyzed at compile time. Unlike CVM, Tempest provides fine-grain access control for units smaller than a page <ref> [23] </ref>. However, since CVM supports multiple writers, the main performance advantage is in avoiding page faults traps for shared data. Large units of coherence can exploit spatial locality, so the PGI compiler can actually improve performance by using larger coherence units [3].
Reference: [24] <author> D. Scales, K. Gharachorloo, and C. Thekkath. </author> <title> Shasta: A low overhead, software-only approach for supporting fine-grained shared memory. </title> <booktitle> In Proceedings of the Eighth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VIII), </booktitle> <address> Boston, MA, </address> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: Bershad et al. maintain coherence by using a compiler to update a software dirty bit on shared-memory accesses [1]. Scales et al. designed Shasta, a software-only approach that supports fine-grain coherence through binary rewriting <ref> [24] </ref>. Using a number of optimizations, Shasta limits software overhead to within 5-35% for the Splash benchmarks on a DEC Alpha cluster. In comparison, CVM, like most software DSMs, relies on the virtual memory system to detect shared memory updates.
Reference: [25] <author> J. Torrellas, M. Lam, and J. Hennessy. </author> <title> False sharing and spatial locality in multiprocessor caches. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 43(6) </volume> <pages> 651-663, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: The SUIF compiler draws on a large body of work on techniques for identifying parallelism [11]. Previous researchers have examined shared-memory compilation issues such as improving locality [19] and reducing false sharing <ref> [25] </ref>, but their techniques were mostly needed for single-writer hardware coherence protocols. Granston and Wishoff suggest a number of compiler optimizations for software DSMs [9]. These include tiling loop iterations so computation is on partitioned matching page boundaries, aligning arrays to pages, and inserting hints to use weak coherence.
Reference: [26] <author> G. Viswanathan and J. Larus. </author> <title> Compiler-directed shared-memory communication for iterative parallel computations. </title> <booktitle> In Proceedings of Supercomputing '96, </booktitle> <address> Pittsburgh, PA, </address> <month> Nov. </month> <year> 1996. </year> <month> 10 </month>
Reference-contexts: Compared to their system, we target compiler-parallelized programs which are less tuned for software DSMs, and require much less precise compile-time communication analysis. Viswanathan and Larus developed a two-part predictive protocol for iterative computations for use in the data-parallel language C** <ref> [26] </ref>. Experiments on a 32 processor CM-5 show 50% improvement for an adaptive grid code and little impact for Water and Barnes. The flush protocol in CVM also relies on repetitive communication patterns to improve performance, but handles it naturally as an extension of its multi-writer protocol.
References-found: 26

