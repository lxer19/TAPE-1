URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3526/3526.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Title: A SOURCE-LEVEL TRANSFORMATION FRAMEWORK FOR RPC-BASED DISTRIBUTED PROGRAMS  
Author: Tae-Hyung Kim James M. Purtilo 
Note: This research has been supported by Office of Naval Research and a grant from the Siemens Corpo ration.  
Address: College Park, MD 20742  
Affiliation: Institute for Advanced Computer Studies and Computer Science Department University of Maryland  
Abstract: The remote procedure call (RPC) paradigm has been a favorite of programmers who write distributed programs because RPC uses a familiar procedure call abstraction as the sole mechanism of operation. The abstraction helps to simplify programming tasks, but this does not mean that the resulting program's RPC-based flow of control will be anything close to ideal for high performance. The purpose of our research is to provide a source-level transformation framework as an alternative way to implement an RPC-based distributed program, so that the code can be optimized through program analysis techniques. This paper describes the transformation tools we have constructed towards this end. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. V. Aho, R. Sethi, and J. D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison Wesley, </publisher> <year> 1986. </year>
Reference-contexts: The more the gap is attained, the more statements can be executed during executing an RPC. Node v dominates node w, denoted by vw, if v appears on every path from Entry to w <ref> [1] </ref>. Node v immediately dominates node w iff vw and there is no node x such that vx and xw. In a dominator tree (DT ) of a CF G, the children of a node v are all immediately dominated by v. <p> As the properties describe, the algorithm to find such R snd and R recv is straightforward; i.e. compute the proper U DC and DU C sets <ref> [1] </ref>, and find the least common ancestors for those elements of the sets, in the P DT and DT , respectively, and repeat for the next RPC.
Reference: [2] <author> S. P. Amarasinghe and M. S. Lam. </author> <title> Communication optimization and code generation for distributed memory machines. </title> <booktitle> In Proceedings of the ACM SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 126-138, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Communication optimization has been an important issue to compiling SPMD (Single Program Multiple Data) programs where communication is necessary to access non-local data <ref> [2, 25] </ref>. Our work is focused on optimization of general RPC-based distributed programs which is not of SPMD form. 2 MOTIVATION This section illustrates situations to motivate source-level transformation of RPC-based distributed programs for higher performance.
Reference: [3] <author> Gregory R. Andrews. </author> <title> Paradigms for process interaction in distributed programs. </title> <journal> ACM Computing Surveys, </journal> <volume> Vol. 23(1), </volume> <month> March </month> <year> 1991. </year>
Reference-contexts: But many practical applications can be naturally expressed in a modular way using procedure call abstraction <ref> [3] </ref>. The previous unavailability of proper optimization methods discouraged programmers from using the otherwise simpler paradigm for high performance programming, in spite of its convenience.
Reference: [4] <author> J. M. Barth. </author> <title> A practical interprocedural data flow analysis algorithm. </title> <journal> Communication of the ACM, </journal> <volume> Vol. 21(9) </volume> <pages> 724-736, </pages> <month> September </month> <year> 1978. </year>
Reference-contexts: An edge without having a control predicate means control independence (always true). The tree forms a calling hierarchy in a program; the parent node is a caller (client), and the children nodes are its callees (servers). An edge represents a positionally different RPC, which conveys flow sensitive information <ref> [4, 7, 15] </ref>. <p> From the viewpoint of each procedure, the interprocedural data flow equations to this end can be expressed as following recursive forms where the Called (P ) is the set of remote procedures called directly from P <ref> [4] </ref>: U se (P ) = LocalU se (P ) [ Q2Called (P ) U se (Q) (5) Def (P ) = LocalDef (P ) [ Q2Called (P ) Def (Q) (6) Since there are no global or reference variables, we can rewrite those equations as following concrete forms: U se
Reference: [5] <author> B. N. Berstad, T. E.Anderson, E. D. Lazowska, and H. M. Levy. </author> <title> Lightweight remote procedure call. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol. 8(8) </volume> <pages> 37-55, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: The RPC paradigm adopts the model of client-server computing; caller and callee correspond to client and server, respectively. Traditional researches 3 (a) Parallelism in depth (b) Parallelism in breadth on improving RPC program performance have focused on reducing latency and transmission time within this pairwise form <ref> [16, 5, 14] </ref>. When this simple topology extends to a network of client-server model computing, more advanced optimization other than just efficient pairwise hooking between client and server is called for. These two topologies are basic units where we can account for our source-level transformation techniques for higher performance.
Reference: [6] <author> A. D. Birrell and B. J. Nelson. </author> <title> Implementing remote procedure calls. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol. 2(1) </volume> <pages> 39-59, </pages> <month> February </month> <year> 1984. </year> <month> 24 </month>
Reference-contexts: For example, the communication between far distant layers might require a series of communications between a series of adjacent layers. The main problem of traditional stub generation based methods <ref> [6, 13] </ref> for implementing RPC paradigm is that it just adopts the natural flow of modularization as its actual flow of the program. <p> In the following section, we present a source transformation framework under constraints (1)-(4) for optimizing RPC-based distributed programs, with preserving the control dependences as well. 4 TRANSFORMATION FRAMEWORK When an RPC is implemented through traditional stub generation based methods <ref> [6, 8, 13] </ref>, a stub performs following three functions. 1. Communication: RPC arguments are transmitted to the "remote" callee via communi cation network, and the result is back to the caller. 2. Synchronization: The caller is suspended until the result is ready to receive. 3.
Reference: [7] <author> D. Callahan. </author> <title> The program summary graph and flow-sensitive interprocedural data flow analysis. </title> <booktitle> In Proceedings of the ACM SIGPLAN '88 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 47-56, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: An edge without having a control predicate means control independence (always true). The tree forms a calling hierarchy in a program; the parent node is a caller (client), and the children nodes are its callees (servers). An edge represents a positionally different RPC, which conveys flow sensitive information <ref> [4, 7, 15] </ref>.
Reference: [8] <author> J. Callahan and J. M. Purtilo. </author> <title> A packaging system for heterogeneous execution environments. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Vol. 17(6) </volume> <pages> 626-635, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: In the following section, we present a source transformation framework under constraints (1)-(4) for optimizing RPC-based distributed programs, with preserving the control dependences as well. 4 TRANSFORMATION FRAMEWORK When an RPC is implemented through traditional stub generation based methods <ref> [6, 8, 13] </ref>, a stub performs following three functions. 1. Communication: RPC arguments are transmitted to the "remote" callee via communi cation network, and the result is back to the caller. 2. Synchronization: The caller is suspended until the result is ready to receive. 3.
Reference: [9] <author> J. B. Carter. </author> <title> Efficient Distributed Shared Memory Based On Multi-Protocol Release Consistency. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> sep </month> <year> 1993. </year>
Reference-contexts: Message passing primitives [12, 22, 24] are expressive enough to program for efficiency; however, they are too low-level to write large distributed programs. Programmers are fully responsible for matching send/receive pairs, allocating buffers, and marshaling/unmarshaling data correctly. Programming under DSM systems <ref> [9] </ref> eases such difficulties, but the resulting programs suffer efficiency due to false sharing and coherence maintaining overhead. (Some researchers claimed that DSM would be more efficient in some particular applications that have irregular communication patterns [23].) Our work presented in this paper is an effort to strike a compromise between
Reference: [10] <author> J. R. Corbin. </author> <title> SUN RPC: The art of distributed applications: programming techniques for remote procedure calls. </title> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference-contexts: Data conversion: A data type in a caller machine needs not to be identical to the "remote" machine. For flexibility as well as convenience 2 , a data type is converted into a standardized type like XDT <ref> [10] </ref> (encoding) before converted into a specific type (decoding).
Reference: [11] <author> J. Ferrante, K. J. Ottenstein, and J. D. Warren. </author> <title> The program dependence graph and its use in optimization. </title> <journal> ACM Transactions on Programming Languages and systems, </journal> <volume> 9(3) </volume> <pages> 319-349, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: An edge represents a positionally different RPC, which conveys flow sensitive information [4, 7, 15]. We also define "v + ) w" to mean that node v can reach node w via one or more control dependence edges. 2 A control flow graph <ref> [11] </ref> is a directed graph CF G = (CV; CE) with unique node Entry; Exit 2 CV such that there exists a path from Entry to every node in CV and a path from every node to Exit; Entry has no incoming edges, and Exit has no outgoing edges. <p> Control dependence <ref> [11] </ref> captures the essential control flow relationships in a program. Informally, for node v and w in CF G, w is control dependent on v if v can directly affect whether w is executed or not. <p> When v is a closer descendent to x than y in the DT , the dominator x is called closer to v than y. Node v post-dominates node w, denoted by v p w, if v appears on every path from w to Exit <ref> [11] </ref>. Node v immediately post-dominates node w iff v p w and there is no node x such that v p x and x p w. In a post-dominator tree (P DT ), the children of a node v are all immediately post-dominated by v.
Reference: [12] <author> The MPI Forum. </author> <title> MPI: A Message Passing Interface. </title> <booktitle> In Proceedings Supercomputing '93, </booktitle> <year> 1985. </year>
Reference-contexts: In either case, the task of creating software for such a system is far more demanding than for a single host. Two representative programming models for distributed memory machines are available for programmers, message passing (MP) and distributed share memory (DSM). Message passing primitives <ref> [12, 22, 24] </ref> are expressive enough to program for efficiency; however, they are too low-level to write large distributed programs. Programmers are fully responsible for matching send/receive pairs, allocating buffers, and marshaling/unmarshaling data correctly. <p> For flexibility as well as convenience 2 , a data type is converted into a standardized type like XDT [10] (encoding) before converted into a specific type (decoding). Since those three functions can be implemented by a combination of message passing primitives that are provided by underlying MP systems <ref> [12, 24] </ref> or (distributed) operating systems, we approach the transformation problem from RPC statements to a series of such low-level primitives 2 Without having external data conversion, if L different languages and M different machines are intermixed in a distributed application, then potentially (L fi M) 2 cases of data conversion <p> Non-blocking receive primitives are commonly supported by other MP systems like PVM [24] and MPI <ref> [12] </ref>. Finally, let's consider what has been improved in Figure 7 (b) from (a). There is no difference regarding the degree of parallelism, that is constrained by inherent data dependences.
Reference: [13] <author> P. B. Gibbons. </author> <title> A stub generator for multilanguage RPC in heterogeneous environments. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Vol. 13(1), </volume> <month> January </month> <year> 1987. </year>
Reference-contexts: For example, the communication between far distant layers might require a series of communications between a series of adjacent layers. The main problem of traditional stub generation based methods <ref> [6, 13] </ref> for implementing RPC paradigm is that it just adopts the natural flow of modularization as its actual flow of the program. <p> In the following section, we present a source transformation framework under constraints (1)-(4) for optimizing RPC-based distributed programs, with preserving the control dependences as well. 4 TRANSFORMATION FRAMEWORK When an RPC is implemented through traditional stub generation based methods <ref> [6, 8, 13] </ref>, a stub performs following three functions. 1. Communication: RPC arguments are transmitted to the "remote" callee via communi cation network, and the result is back to the caller. 2. Synchronization: The caller is suspended until the result is ready to receive. 3. <p> operating systems, we approach the transformation problem from RPC statements to a series of such low-level primitives 2 Without having external data conversion, if L different languages and M different machines are intermixed in a distributed application, then potentially (L fi M) 2 cases of data conversion must be used <ref> [13] </ref>. 8 in a static manner. The transformation based RPC implementation approach opens an opportu-nity to apply various static analysis techniques for optimizing RPC-based distributed programs, which have been shown useful for automatic parallelization.
Reference: [14] <author> D. K. Gifford and N. Glasser. </author> <title> Remote pipes and procedures for efficient distributed communication. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol. 6(3) </volume> <pages> 258-283, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: The RPC paradigm adopts the model of client-server computing; caller and callee correspond to client and server, respectively. Traditional researches 3 (a) Parallelism in depth (b) Parallelism in breadth on improving RPC program performance have focused on reducing latency and transmission time within this pairwise form <ref> [16, 5, 14] </ref>. When this simple topology extends to a network of client-server model computing, more advanced optimization other than just efficient pairwise hooking between client and server is called for. These two topologies are basic units where we can account for our source-level transformation techniques for higher performance.
Reference: [15] <author> Mary W. Hall. </author> <title> Managing Interprocedural Optimization. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> October </month> <year> 1990. </year>
Reference-contexts: An edge without having a control predicate means control independence (always true). The tree forms a calling hierarchy in a program; the parent node is a caller (client), and the children nodes are its callees (servers). An edge represents a positionally different RPC, which conveys flow sensitive information <ref> [4, 7, 15] </ref>.
Reference: [16] <author> D. B. Johnson and W. Zwaenepoel. </author> <title> The Peregrine high-performance RPC system. </title> <journal> Journal of Software Practice and Experience, </journal> <volume> Vol. 23(2) </volume> <pages> 201-221, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: The RPC paradigm adopts the model of client-server computing; caller and callee correspond to client and server, respectively. Traditional researches 3 (a) Parallelism in depth (b) Parallelism in breadth on improving RPC program performance have focused on reducing latency and transmission time within this pairwise form <ref> [16, 5, 14] </ref>. When this simple topology extends to a network of client-server model computing, more advanced optimization other than just efficient pairwise hooking between client and server is called for. These two topologies are basic units where we can account for our source-level transformation techniques for higher performance.
Reference: [17] <author> T.-H. Kim and J. M. Purtilo. </author> <title> Configuration-level optimization of RPC-based distributed programs. </title> <booktitle> In Proceedings of the 15th International Conference On Distributed Computing Systems, </booktitle> <month> May </month> <year> 1995. </year>
Reference-contexts: Our technique transforms an RPC statement into a set of low-level message passing primitives to enable an RPC-based distributed program to run in an optimized way. Though not discussed in this paper, an experimental environment implementing these techniques has been successfully implemented in an environment called CORD <ref> [17] </ref>. 3 CONSTRAINTS ON SOURCE TRANSFORMATION FOR RPC Exploitation of parallelism is limited by data and control dependences in the program and resource constraints of the executing environment. Dependence constraints are directly related to the semantics of a program. <p> Such an issue has been discussed from the perspective of configuration level optimization in a separate paper <ref> [17] </ref>. In the following section, we present a source transformation framework under constraints (1)-(4) for optimizing RPC-based distributed programs, with preserving the control dependences as well. 4 TRANSFORMATION FRAMEWORK When an RPC is implemented through traditional stub generation based methods [6, 8, 13], a stub performs following three functions. 1.
Reference: [18] <author> L. Lamport. </author> <title> Time, clocks, and the ordering of events in a distributed system. </title> <journal> Communication of the ACM, </journal> <volume> Vol. 21(7) </volume> <pages> 558-565, </pages> <month> July </month> <year> 1978. </year> <month> 25 </month>
Reference-contexts: This behavioral difference in accessing variables between S snd and S recv helps to widen the gap by placing S snd 1 It resembles the happened-before relation on a set of distributed events <ref> [18] </ref>. While the associated events in that relation are distributed, the relation P is an ordering between statements in a single program.
Reference: [19] <author> B. Liskov and L. Shrira. </author> <title> Promises: Linguistic support for efficient asynchronous proce-dure calls in distributed systems. </title> <booktitle> In Proceedings of the ACM SIGPLAN '88 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 260-267, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Executing an RPC is involved in rather longer delay. Aggregating remote messages can drastically reduce the inter-networking overhead by sharing the overhead by multiple messages. If a loop contains RPCs, the chance to reduce the overhead through aggregation is higher <ref> [19] </ref>, thus careful loop transformation provides good opportunity for aggregation. Loop distribution (also called loop splitting or loop fission) breaks a single loop into multiple loops with the same iteration space but each enclosing a subset of the statements in the original loop [20]. <p> It is called call streaming, which was proposed to effectively support asynchronous calls with an aid of a special data type called "promises" <ref> [19] </ref>. Our method presents a static solution for call streaming without employing special programming language constructs. <p> Moreover, connecting the output of one remote procedure to the input of another is also automatically achieved as presented in the previous section while results must be returned to the original caller before sending them on the next stream in <ref> [19] </ref>. Data aggregation to amortize kernel overhead and the transmission delays over several calls can be achieved transparently by an aid of underlying MP systems or statically by an aid of compiler that properly generates finer grain MP primitives.
Reference: [20] <author> D. A. Padua and M. Wolfe. </author> <title> Advanced compiler optimizations for supercomputers. </title> <journal> Communication of the ACM, </journal> <volume> Vol. 29(12) </volume> <pages> 1184-1201, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: Loop distribution (also called loop splitting or loop fission) breaks a single loop into multiple loops with the same iteration space but each enclosing a subset of the statements in the original loop <ref> [20] </ref>. It is used to improve instruction and data locality by shortening loop bodies and to allow parallelism that is hindered by loop-carried dependences in the original loop. The latter effect is important in applying the technique to a loop that contains RPC statements.
Reference: [21] <author> C. D. Polychronopoulos. </author> <title> Parallel Programming and Compilers. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1988. </year>
Reference-contexts: (i = 0; i &lt; N; i++) for (i = 0; i &lt; N; i++) send req (callrpc (), a [i], x [i]); for (i = 0; i &lt; N; i++) x [i+1] = recv res (callrpc ()); (d) After loop distribution for loops are hot spots in the program <ref> [21] </ref>. We are interested in transforming a loop as well, especially when RPC statements are surrounded by a loop construct. Executing an RPC is involved in rather longer delay. Aggregating remote messages can drastically reduce the inter-networking overhead by sharing the overhead by multiple messages.
Reference: [22] <author> J. M. Purtilo. </author> <title> The polylith software bus. </title> <journal> ACM Transactions on Programming Languages and systems, </journal> <volume> Vol. 16(1) </volume> <pages> 151-174, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: In either case, the task of creating software for such a system is far more demanding than for a single host. Two representative programming models for distributed memory machines are available for programmers, message passing (MP) and distributed share memory (DSM). Message passing primitives <ref> [12, 22, 24] </ref> are expressive enough to program for efficiency; however, they are too low-level to write large distributed programs. Programmers are fully responsible for matching send/receive pairs, allocating buffers, and marshaling/unmarshaling data correctly. <p> This kind of construct can be implemented by special message passing primitives that allows a non-blocking receive operation | for example, in Polylith system <ref> [22] </ref>, mh readselect () allows us to read the next message to arrive on any interface (it will be blocked if no message arrives), then mh readback () completes the receipt. Non-blocking receive primitives are commonly supported by other MP systems like PVM [24] and MPI [12].
Reference: [23] <author> J. P. Singh, A. Gupta, and M. Levoy. </author> <title> Parallel Visualization Algorithms: Performance and architectural implications. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 45-55, </pages> <month> jul </month> <year> 1994. </year>
Reference-contexts: Programming under DSM systems [9] eases such difficulties, but the resulting programs suffer efficiency due to false sharing and coherence maintaining overhead. (Some researchers claimed that DSM would be more efficient in some particular applications that have irregular communication patterns <ref> [23] </ref>.) Our work presented in this paper is an effort to strike a compromise between these two models, using the RPC paradigm for writing distributed programs plus a source transformation framework for improving performance.
Reference: [24] <author> V. S. Sunderam. </author> <title> PVM: A framework for parallel distributed computing. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 2(4) </volume> <pages> 315-339, </pages> <month> dec </month> <year> 1990. </year>
Reference-contexts: In either case, the task of creating software for such a system is far more demanding than for a single host. Two representative programming models for distributed memory machines are available for programmers, message passing (MP) and distributed share memory (DSM). Message passing primitives <ref> [12, 22, 24] </ref> are expressive enough to program for efficiency; however, they are too low-level to write large distributed programs. Programmers are fully responsible for matching send/receive pairs, allocating buffers, and marshaling/unmarshaling data correctly. <p> For flexibility as well as convenience 2 , a data type is converted into a standardized type like XDT [10] (encoding) before converted into a specific type (decoding). Since those three functions can be implemented by a combination of message passing primitives that are provided by underlying MP systems <ref> [12, 24] </ref> or (distributed) operating systems, we approach the transformation problem from RPC statements to a series of such low-level primitives 2 Without having external data conversion, if L different languages and M different machines are intermixed in a distributed application, then potentially (L fi M) 2 cases of data conversion <p> Non-blocking receive primitives are commonly supported by other MP systems like PVM <ref> [24] </ref> and MPI [12]. Finally, let's consider what has been improved in Figure 7 (b) from (a). There is no difference regarding the degree of parallelism, that is constrained by inherent data dependences.
Reference: [25] <author> R. von Hanxleden and K. Kennedy. </author> <title> Give-N-Take | A balanced code placement framework. </title> <booktitle> In Proceedings of the ACM SIGPLAN '94 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 107-120, </pages> <month> June </month> <year> 1994. </year> <month> 26 </month>
Reference-contexts: Communication optimization has been an important issue to compiling SPMD (Single Program Multiple Data) programs where communication is necessary to access non-local data <ref> [2, 25] </ref>. Our work is focused on optimization of general RPC-based distributed programs which is not of SPMD form. 2 MOTIVATION This section illustrates situations to motivate source-level transformation of RPC-based distributed programs for higher performance.
References-found: 25

