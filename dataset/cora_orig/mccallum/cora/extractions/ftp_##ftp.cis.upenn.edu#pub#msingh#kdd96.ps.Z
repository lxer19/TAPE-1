URL: ftp://ftp.cis.upenn.edu/pub/msingh/kdd96.ps.Z
Refering-URL: http://www.cis.upenn.edu/~msingh/frames/papers_list.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: provan@risc.rockwell.com  msingh@gradient.cis.upenn.edu  
Title: Mining and Model Simplicity: A Case Study in Diagnosis  
Author: Gregory M. Provan Moninder Singh 
Address: 1049 Camino dos Rios Thousand Oaks, CA 91360.  Philadelphia, PA 19104-6389  
Affiliation: Rockwell Science Center  Dept. of Computer and Information Science University of Pennsylvania  
Note: Data  
Abstract: Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining (KDD), 1996. The official version of this paper has been published by the American Association for Artificial Intelligence (http://www.aaai.org) c fl 1996, American Association for Artificial Intelligence. All rights reserved. Abstract We describe the results of performing data mining on a challenging medical diagnosis domain, acute abdominal pain. This domain is well known to be difficult, yielding little more than 60% predictive accuracy for most human and machine diagnosticians. Moreover, many researchers argue that one of the simplest approaches, the naive Bayesian classifier, is optimal. By comparing the performance of the naive Bayesian classifier to its more general cousin, the Bayesian network classifier, and to selective Bayesian classifiers with just 10% of the total attributes, we show that the simplest models perform at least as well as the more complex models. We argue that simple models like the selective naive Bayesian classifier will perform as well as more complicated models for similarly complex domains with relatively small data sets, thereby calling into question the extra expense necessary to induce more complex models. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Anderson, S.; Olesen, K.; Jensen, F.; and Jensen, F. </author> <year> 1989. </year> <title> HUGIN A Shell for building Bayesian Belief Universes for Expert Systems. </title> <booktitle> In Proceedings IJCAI, </booktitle> <pages> 1080-1085. </pages>
Reference-contexts: The performance measure we used was the classification accuracy of a model on the test data, where the classification accuracy is the percentage of test cases that were diagnosed correctly. Inference on the Bayesian networks was carried out using the the HUGIN <ref> (Anderson et al. 1989) </ref> system. Results Our first set of experiments compared the performance of the four different approaches listed above.
Reference: <author> Cooper, G. </author> <year> 1990. </year> <title> The computational complexity of probabilistic inference using Belief networks. </title> <booktitle> Artificial Intelligence 42 </booktitle> <pages> 393-405. </pages> <editor> de Dombal, F.; Leaper, D.; Staniland, J.; McCann, A.; and Horrocks, J. </editor> <year> 1972. </year> <title> Computer-aided diagnosis of acute abdominal pain. </title> <journal> British Medical Journal </journal> 2:9-13. de Dombal, F. 1991. The diagnosis of acute abdominal pain with computer assistance. Annals Chir. 45:273-277. 
Reference-contexts: Inference in naive Bayesian networks is linear in the number of attributes, and is done using Bayes' rule and the assumption of feature independence within each class. Inference is more complicated for Bayesian networks: it is NP-hard <ref> (Cooper 1990) </ref>. Pearl (1988), among others, reviews Bayesian network inference; details are beyond the scope of this paper. Application Domain This section discusses the domain of acute abdominal pain, focusing on the models used for the diagnosis.
Reference: <author> Domingos, P., and Pazzani, M. </author> <year> 1996. </year> <title> Beyond independence: Conditions for the optimality of the simple Bayesian classifier. </title> <booktitle> In Proc. Machine Learning Conference. </booktitle>
Reference-contexts: Based on this matrix (which we do not reproduce due to its size), and on computing a pairwise correlation measure described in <ref> (Domingos & Pazzani 1996) </ref>, we observe that many variables are relatively highly correlated, yet the naive classifier performed better than any other approach. A second observation is that the variables with the highest variance are always included in the selective networks.
Reference: <author> Edwards, F., and Davies, R. </author> <year> 1984. </year> <title> Use of a Bayesian algorithm in the computer-assisted diagnosis of appendicitis. </title> <journal> Surg. Gynecol. Obstet. </journal> <volume> 158 </volume> <pages> 219-222. </pages>
Reference: <author> Fryback, D. G. </author> <year> 1978. </year> <title> Bayes' theorem and conditional nonindependence of data in medical diagnosis. </title> <booktitle> Computers and Biomedical Research 11 </booktitle> <pages> 429-435. </pages>
Reference: <author> Hilden, J. </author> <year> 1984. </year> <title> Statistical diagnosis based on conditional independence does not require it. </title> <journal> Comput. Biol. Med. </journal> <volume> 14 </volume> <pages> 429-435. </pages>
Reference: <author> Langley, P.; Iba, W.; and Thompson, K. </author> <year> 1992. </year> <title> An analysis of Bayesian classifiers. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> 223-228. </pages> <publisher> AAAI Press. </publisher>
Reference: <author> Lauritzen, S.L., and Spiegelhalter, D.J. </author> <year> 1988. </year> <title> Local computations with probabilities on graphical structures and their application to expert systems. </title> <journal> Journal of the Royal Statistical Society (Series B), </journal> <volume> 50 </volume> <pages> 157-224. </pages>
Reference: <author> Lopez de Mantaras, R. </author> <year> 1991. </year> <title> A distance-based attribute selection measure for decision tree induction. </title> <booktitle> Machine Learning 6 </booktitle> <pages> 81-92. </pages>
Reference: <author> Norusis, M., and Jacquez, J. </author> <year> 1975. </year> <title> Diagnosis I: Symptom nonindependence in mathematical models for diagnosis. </title> <journal> Comput. Biomed. Res. </journal> <volume> 8 </volume> <pages> 156-172. </pages>
Reference: <author> Pearl, J. </author> <year> 1988. </year> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufman. </publisher>
Reference: <author> Provan, G. M., and Singh, M. </author> <year> 1996. </year> <title> Learning Bayesian networks using feature selection. </title> <editor> In Fisher, D., and Lenz, H., eds., </editor> <booktitle> Learning from Data: AI and Statistics V, Lecture Notes in Statistics, </booktitle> <volume> 112. </volume> <publisher> Springer Verlag. </publisher> <pages> 291-300. </pages>
Reference-contexts: We ran a set of preliminary experiments to determine the selection algorithm that produced final networks with the highest predictive accuracy. In comparing three information-based algorithms (as described in <ref> (Singh & Provan 1996) </ref>) and a belief-network wrapper approach (as described in (Singh & Provan 1995; Provan & Singh 1996)), we decided on using an information-based approach called CDC (Singh & Provan 1996) to select attributes. To define the CDC metric, we need to introduce some notation. <p> In comparing three information-based algorithms (as described in <ref> (Singh & Provan 1996) </ref>) and a belief-network wrapper approach (as described in (Singh & Provan 1995; Provan & Singh 1996)), we decided on using an information-based approach called CDC (Singh & Provan 1996) to select attributes. To define the CDC metric, we need to introduce some notation. Let C be the class variable and A represent the attribute under consideration. Let be the subset of attributes already selected. <p> It then adds incrementally (to ) that at-tribute A (from the available attributes) that maximizes CDC (A; ). The algorithm stops adding attributes when there is no single attribute whose addition results in a positive value of the information metric. 2 Complete details are given in <ref> (Singh & Provan 1996) </ref>. To maintain consistency with Todd and Stamper's careful comparison (Todd & Stamper 1994) of several induction approaches, we adopted their experimental method of using a cross-validation strategy to evaluate the different methods.
Reference: <author> Provan, G. </author> <year> 1994. </year> <title> Tradeoffs in knowledge-based construction of probabilistic models. </title> <journal> IEEE Trans. on SMC 11: </journal> <pages> 287-294. </pages>
Reference: <author> Seroussi, B. </author> <year> 1986. </year> <title> Computer-aided diagnosis of acute abdominal pain when taking into account interactions. Method. </title> <journal> Inform. Med. </journal> <volume> 25 </volume> <pages> 194-198. </pages>
Reference: <author> Singh, M., and Provan, G. M. </author> <year> 1995. </year> <title> A comparison of induction algorithms for selective and non-selective Bayesian classifiers. </title> <booktitle> In Proc. 12th Intl. Conference on Machine Learning, </booktitle> <pages> 497-505. </pages>
Reference-contexts: The Bayesian network classifier has been shown to outperform the naive Bayesian classifier on several UC-Irvine domains <ref> (Singh & Provan 1995) </ref>, so it may prove better than the naive Bayesian classifier on this domain. <p> A second key finding is that feature selection further increases model simplicity with no performance penalty. In addition, the more complex the model, the better feature selection increases performance <ref> (Singh & Provan 1995) </ref>. We have shown how only 10% of the attributes provide accuracy comparable to using all the attributes. This can lead to significant savings in data collection and inference. Our experiments with synthetic networks have shown that attribute dependencies given the class variable do affect classifier performance.
Reference: <author> Singh, M., and Provan, G. M. </author> <year> 1996. </year> <title> Efficient learning of selective Bayesian network classifiers. </title> <booktitle> In Proc. 13th Intl. Conference on Machine Learning. To appear. </booktitle>
Reference-contexts: We ran a set of preliminary experiments to determine the selection algorithm that produced final networks with the highest predictive accuracy. In comparing three information-based algorithms (as described in <ref> (Singh & Provan 1996) </ref>) and a belief-network wrapper approach (as described in (Singh & Provan 1995; Provan & Singh 1996)), we decided on using an information-based approach called CDC (Singh & Provan 1996) to select attributes. To define the CDC metric, we need to introduce some notation. <p> In comparing three information-based algorithms (as described in <ref> (Singh & Provan 1996) </ref>) and a belief-network wrapper approach (as described in (Singh & Provan 1995; Provan & Singh 1996)), we decided on using an information-based approach called CDC (Singh & Provan 1996) to select attributes. To define the CDC metric, we need to introduce some notation. Let C be the class variable and A represent the attribute under consideration. Let be the subset of attributes already selected. <p> It then adds incrementally (to ) that at-tribute A (from the available attributes) that maximizes CDC (A; ). The algorithm stops adding attributes when there is no single attribute whose addition results in a positive value of the information metric. 2 Complete details are given in <ref> (Singh & Provan 1996) </ref>. To maintain consistency with Todd and Stamper's careful comparison (Todd & Stamper 1994) of several induction approaches, we adopted their experimental method of using a cross-validation strategy to evaluate the different methods.
Reference: <author> Todd, B. S., and Stamper, R. </author> <year> 1993. </year> <title> The formal design and evaluation of a variety of medical diagnostic programs. </title> <type> Technical Monograph PRG-109, </type> <institution> Oxford University Computing Laboratory. </institution>
Reference-contexts: In contrast, other studies have shown no statistically significant difference between the two approaches <ref> (Todd & Stamper 1993) </ref>, and some have 1 We ignore the cost of data collection in model construction. even found independence Bayesian classifiers to be op-timal (Todd & Stamper 1994; Edwards & Davies 1984; de Dombal 1991). <p> The class variable, final diagnosis, has 19 possible values, and the variables have a number of values ranging from 2 to 32 values. This data was collected and pre-screened by Todd and Stamper, as described in <ref> (Todd & Stamper 1993) </ref>. The resulting database addresses acute abdominal pain of gynaecological origin, based on case-notes for patients of reproductive age admitted to hospital, with no recent history of abdominal or back pain.
Reference: <author> Todd, B. S., and Stamper, R. </author> <year> 1994. </year> <title> The relative accuracy of a variety of medical diagnostic programs. </title> <journal> Methods Inform. Med. </journal> <volume> 33 </volume> <pages> 402-416. </pages>
Reference-contexts: The diagnosis of acute abdominal pain is well known to be a difficult task both for physician and machine. Depending on the assumptions used in reporting statistics the accuracy rates vary, but most machine diagnostic systems achieve accuracy of little more than 60% <ref> (Todd & Stamper 1994) </ref>. Moreover, there has been great debate in the literature about whether the naive Bayesian classifier is optimal for this domain (Todd & Stamper 1994). <p> Depending on the assumptions used in reporting statistics the accuracy rates vary, but most machine diagnostic systems achieve accuracy of little more than 60% <ref> (Todd & Stamper 1994) </ref>. Moreover, there has been great debate in the literature about whether the naive Bayesian classifier is optimal for this domain (Todd & Stamper 1994). The naive Bayesian model makes the strong assumption that the attributes are conditionally independent given the class variable, yet has been shown to perform remarkably well in this domain, and possibly better than any other approach (Todd & Stamper 1994). <p> whether the naive Bayesian classifier is optimal for this domain <ref> (Todd & Stamper 1994) </ref>. The naive Bayesian model makes the strong assumption that the attributes are conditionally independent given the class variable, yet has been shown to perform remarkably well in this domain, and possibly better than any other approach (Todd & Stamper 1994). One paradoxical question we attempt to address is that fact that no approach outperformed the naive Bayesian classifier on this domain. <p> Fryback suggests that model size should be increased incrementally in cases where conditional independence assumptions are not all known, rather than starting from a large model. The most detailed comparison of several different approaches <ref> (Todd & Stamper 1994) </ref> has shown the naive classifier to outperform classifiers based on a neural network, decision tree, Bayesian network (hand-crafted network structure with induced parameters), and nearest neighbor model, among others. <p> The algorithm stops adding attributes when there is no single attribute whose addition results in a positive value of the information metric. 2 Complete details are given in (Singh & Provan 1996). To maintain consistency with Todd and Stamper's careful comparison <ref> (Todd & Stamper 1994) </ref> of several induction approaches, we adopted their experimental method of using a cross-validation strategy to evaluate the different methods. Since the first 202 cases had been used during the construction of the database itself, they were not used for testing purposes.
References-found: 18

