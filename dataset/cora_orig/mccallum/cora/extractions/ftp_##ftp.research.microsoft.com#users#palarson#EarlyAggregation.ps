URL: ftp://ftp.research.microsoft.com/users/palarson/EarlyAggregation.ps
Refering-URL: http://www.research.microsoft.com/db/
Root-URL: http://www.research.microsoft.com
Title: Grouping and Duplicate Elimination: Benefits of Early Aggregation  
Author: Per -Ake Larson 
Date: January 7, 1997  
Affiliation: Microsoft Corporation  
Abstract: Early aggregation is a technique for speeding up the processing of GROUP BY queries by reducing the amount of intermediate data transferred between main memory and disk. It can also be applied to duplicate elimination because duplicate elimination is equivalent to grouping with no aggregation functions. This paper describes six different algorithms for grouping and aggregation, shows how to incorporate early aggregation in each of them, and analyzes the resulting reduction in intermediate data. In addition to the grouping algorithm used, the reduction depends on several factors: the number of groups, the skew in group size distribution, the input size, and the amount of main memory available. All six algorithms considered benefit from early aggregation with grouping by hash partitioning producing the least amount of intermediate data. If the group size distribution is skewed, the overall reduction can be very significant, even with a modest amount of additional main memory. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Abdelguerfi and A. K. Sood. </author> <title> Computational complexity of sorting and joining relations with duplicates. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 3(4) </volume> <pages> 497-503, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: Munro and Spira [5] gave a computational bound for the number of comparisons required to sort a multiset with early duplicate removal. Several algorithms, based on various sorting algorithms, e.g., quick sort, hash sort and merge sort, have been proposed for duplicate elimination. Abdelguerfi and Sood <ref> [1] </ref> gave the computational complexity of the merge sort method based on the number of three-way comparisons. Teuhola and Wegner [7] gave a duplicate elimination algorithm based on hashing with early duplicate removal, which requires linear time on the average and O (1) extra space.
Reference: [2] <author> D. Bitton and D. J. Dewitt. </author> <title> Duplicate record elimination in large data files. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 8(2) </volume> <pages> 255-265, </pages> <month> June </month> <year> 1983. </year>
Reference-contexts: However, we are mainly interested in large-scale grouping and aggregation requiring external storage. The processing cost is then dominated by the cost of I/O, and the CPU time can be largely ignored. D. Bitton and D.J. Dewitt <ref> [2] </ref> analyzed the benefits of early duplicate elimination during run merging in external merge sort.
Reference: [3] <author> G. Graefe. </author> <title> Query evaluation techniques for large databases. </title> <journal> ACM Computing Surveys, </journal> <volume> 25(2) </volume> <pages> 73-170, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Their analysis is based on several simplifying assumptions: all groups are assumed to be of the same size, the only merge pattern considered is balanced two-way merge, and duplicate elimination during run formation is not considered. This analysis is also summarized in <ref> [3] </ref>. Parallel database systems running on shared-nothing systems normally perform aggregation in two steps: each node first performs grouping and aggregation on its local data and then ships the result to one or more nodes where the partial results are integrated.
Reference: [4] <author> D. E. Knuth. </author> <booktitle> The art of computer programming, </booktitle> <volume> volume 3. </volume> <publisher> Addison Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1973. </year> <month> 24 </month>
Reference-contexts: The actual group labels do not matter so we assume that the labels are assigned so that p 1 p 2 p D . For the numerical results reported in this paper, we model the group size distribution with a generalized Zipf distribution <ref> [4] </ref>. The distribution function is defined by: p i = c where ff is a positive constant and c = P D i=1 (1=i) ff . Setting ff = 1 gives the traditional Zipf distribution, and ff = 0 gives a uniform distribution. <p> New records with a key greater than or equal to the key of the last record output, are added to the current run. Those whose key is too low become part of the next run. The standard algorithm is replacement selection, see reference <ref> [4] </ref> for details of the algorithm. E. F. Moore showed that, for randomly ordered input, the expected length of each run is twice the available memory size [4]. When the input exhibits some degree of pre-sortedness, runs are likely to be even longer. <p> Those whose key is too low become part of the next run. The standard algorithm is replacement selection, see reference <ref> [4] </ref> for details of the algorithm. E. F. Moore showed that, for randomly ordered input, the expected length of each run is twice the available memory size [4]. When the input exhibits some degree of pre-sortedness, runs are likely to be even longer. Replacement selection produces runs that are, on average, twice as large as memory 8 used during run formation. Even so, load-and-sort algorithms are often used in practice. <p> So given S initial runs, possibly of variable length, and a maximum merge fan-in of K, which merge pattern results in the minimum data transmission? The surprisingly simple solution can be found in reference <ref> [4] </ref>, pp 365-366: first add enough dummy runs of length zero to make the number of runs divisible by K 1 and then repeatedly merge together the K shortest existing runs until only one run remains. 10 Analysis We first analyze the output from one merge step and then show how
Reference: [5] <author> I. Munro and P.M. Spira. </author> <title> Sorting and searching in multisets. </title> <journal> SIAM Journal of Comput--ing, </journal> <volume> 5(1), </volume> <month> March </month> <year> 1976. </year>
Reference-contexts: The analysis shows that the reduction can be very significant, even when using a modest amount of main memory. 2 Previous Work Early aggregation is not a new idea. Most published work deals with duplicate elimination, typically in main memory. Munro and Spira <ref> [5] </ref> gave a computational bound for the number of comparisons required to sort a multiset with early duplicate removal. Several algorithms, based on various sorting algorithms, e.g., quick sort, hash sort and merge sort, have been proposed for duplicate elimination.
Reference: [6] <author> A. Shatdal and J. F. Naughton. </author> <title> Adaptive parallel aggregation algorithms. </title> <booktitle> In Proceedings of ACM SIGMOD 1995 International Conference on Management of Data, </booktitle> <address> San Jose, CA, </address> <pages> pages 104-114, </pages> <address> San Jose, CA, </address> <month> May </month> <year> 1995. </year> <note> ACM. </note>
Reference-contexts: Parallel database systems running on shared-nothing systems normally perform aggregation in two steps: each node first performs grouping and aggregation on its local data and then ships the result to one or more nodes where the partial results are integrated. Shatdal and Naughton <ref> [6] </ref> pointed out that if the input is large and the duplication factor is low (few records per group), then the first step may do a lot of work for a relatively small reduction 2 in output size.
Reference: [7] <author> J. Teuhola and L. Wegner. </author> <title> Minimal space, average linear time duplicate deletion. </title> <journal> Communications of ACM, </journal> <volume> 34(3) </volume> <pages> 62-73, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: Several algorithms, based on various sorting algorithms, e.g., quick sort, hash sort and merge sort, have been proposed for duplicate elimination. Abdelguerfi and Sood [1] gave the computational complexity of the merge sort method based on the number of three-way comparisons. Teuhola and Wegner <ref> [7] </ref> gave a duplicate elimination algorithm based on hashing with early duplicate removal, which requires linear time on the average and O (1) extra space. Wegner [8] gave a quick sort algorithm for the run formation phase and analyzed its computational complexity.
Reference: [8] <author> L. Wegner. </author> <title> Quicksort for equal keys. </title> <journal> IEEE Computer, </journal> <volume> C-34(4):362-367, </volume> <month> April </month> <year> 1985. </year>
Reference-contexts: Abdelguerfi and Sood [1] gave the computational complexity of the merge sort method based on the number of three-way comparisons. Teuhola and Wegner [7] gave a duplicate elimination algorithm based on hashing with early duplicate removal, which requires linear time on the average and O (1) extra space. Wegner <ref> [8] </ref> gave a quick sort algorithm for the run formation phase and analyzed its computational complexity. However, we are mainly interested in large-scale grouping and aggregation requiring external storage. The processing cost is then dominated by the cost of I/O, and the CPU time can be largely ignored. D.
Reference: [9] <author> Weipeng P. Yan and Per -Ake Larson. </author> <title> Data reduction through early grouping. </title> <booktitle> In Proceedings of the 1994 IBM CAS Conference, </booktitle> <address> Toronto, Ontario, </address> <month> November </month> <year> 1994. </year> <month> 25 </month>
Reference-contexts: If so, it is better to skip local aggregation and simply send the input tuples directly to the nodes performing the final aggregation. A paper by Yan and Larson <ref> [9] </ref> contains some early results (based on a simulation study) of the benefits of applying early aggregation to grouping by sorting. 3 Preliminaries In this section we derive three functions that will be needed when analyzing the various GROUP BY algorithms.
References-found: 9

