URL: http://arch.cs.ucdavis.edu/~chong/250C/scheduling/hive-asplos94.ps
Refering-URL: http://arch.cs.ucdavis.edu/~chong/250C/scheduling/
Root-URL: http://www.cs.ucdavis.edu
Title: Scheduling and Page Migration for Multiprocessor Compute Servers  
Author: Rohit Chandra, Scott Devine, Ben Verghese, Anoop Gupta, and Mendel Rosenblum 
Address: Stanford CA 94305  
Affiliation: Computer Systems Laboratory Stanford University,  
Abstract: Several cache-coherent shared-memory multiprocessors have been developed that are scalable and offer a very tight coupling between the processing resources. They are therefore quite attractive for use as compute servers for multiprogramming and parallel application workloads. Process scheduling and memory management, however, remain challenging due to the distributed main memory found on such machines. This paper examines the effects of OS scheduling and page migration policies on the performance of such compute servers. Our experiments are done on the Stan-ford DASH, a distributed-memory cache-coherent multiprocessor. We show that for our multiprogramming workloads consisting of sequential jobs, the traditional Unix scheduling policy does very poorly. In contrast, a policy incorporating cluster and cache affinity along with a simple page-migration algorithm offers up to twofold performance improvement. For our workloads consisting of multiple parallel applications, we compare space-sharing policies that divide the processors among the applications to time-slicing policies such as standard Unix or gang scheduling. We show that space-sharing policies can achieve better processor utilization due to the operating point effect, but time-slicing policies benefit strongly from user-level data distribution. Our initial experience with automatic page migration suggests that policies based only on TLB miss information can be quite effective, and useful for addressing the data distribution problems of space-sharing sched-ulers. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. E. Anderson, B. N. Bershad, E. D. Lazowska, and H. M. Levy. </author> <title> Scheduler activations: Effective kernel support for the user-level management of parallelism. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating System Principles, </booktitle> <pages> pages 95-109, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Several scheduling and page migration policies for multiprocessor systems have been explored in the literature. Common scheduling approaches include cache affinity [12, 22], gang scheduling [6, 9, 10, 19], processor sets [3], and process control [26] or scheduler activations <ref> [1] </ref>. These policies have so far been evaluated only in the context of small bus-based multiprocessors with uniform memory access times [1, 24, 25]. <p> Common scheduling approaches include cache affinity [12, 22], gang scheduling [6, 9, 10, 19], processor sets [3], and process control [26] or scheduler activations [1]. These policies have so far been evaluated only in the context of small bus-based multiprocessors with uniform memory access times <ref> [1, 24, 25] </ref>. Substantial work on page migration policies has also been done [4, 14], but the target of this work has been NUMA machines that are not cache coherent, like the BBN Butterfly. <p> A parallel application therefore executes more efficiently (i.e., achieves better processor utilization) with fewer processors. This is labeled the operating point effect. The process control/scheduler activations approaches <ref> [26, 1] </ref> are an extension of processor sets in which a parallel application dynamically adjusts its number of active processes to match the number of physical processors assigned to its processor set. As a result the application executes at a more efficient operating point along its speedup curve. <p> As a result the application executes at a more efficient operating point along its speedup curve. Process control is most easily exploited by parallel applications written using the task-queue model of parallelism <ref> [1, 5] </ref>, in which user-level tasks are scheduled onto a number of kernel processes. Adjustments to the number of active processes can therefore be embedded within the runtime system, becoming transparent to the application programmer. <p> Adjustments to the number of active processes can therefore be embedded within the runtime system, becoming transparent to the application programmer. Evaluations of these policies <ref> [11, 1] </ref> on bus-based machines such as SGI workstations and the DEC Firefly report significant performance improvements that range from 8-22%.
Reference: [2] <author> D. Black, A. Gupta, and W.-D. Weber. </author> <title> Competitive management of distributed shared memory. </title> <booktitle> In Proceedings of COMPCON, </booktitle> <pages> pages 184-190, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: We evaluated the following policies: (a) a base with no page migration, (b) static post-facto distribution of pages based on cache misses (therefore perfect static placement), (c) competitive page migration based on cache misses <ref> [2] </ref> with a miss threshold of 1000 misses, (d) single page migrate upon the first cache miss, (e) single migrate upon the first TLB miss, (f) the policy that we actually tried on DASH (described earlier in this section): migrate after 4 consecutive remote misses and freeze a page for one
Reference: [3] <author> D. L. Black. </author> <title> Scheduling support for concurrency and parallelism in the Mach operating system. </title> <journal> IEEE Computer, </journal> <volume> 23(5) </volume> <pages> 35-43, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Several scheduling and page migration policies for multiprocessor systems have been explored in the literature. Common scheduling approaches include cache affinity [12, 22], gang scheduling [6, 9, 10, 19], processor sets <ref> [3] </ref>, and process control [26] or scheduler activations [1]. These policies have so far been evaluated only in the context of small bus-based multiprocessors with uniform memory access times [1, 24, 25]. <p> However, these studies have been performed on bus-based machines; we explore the usefulness of the gang-scheduling approach on CC-NUMA machines. Processor Sets: In contrast to the above time-multiplexing approaches, the processor sets technique <ref> [3] </ref> space partitions the machine. The machine is partitioned into sets of processors, each of which executes a single parallel application. This reduces the cache interference between multiple applications running on the same processor.
Reference: [4] <author> W. J. Bolosky, M. L. Scott, R. P. Fitzgerald, R. J. Fowler, and A. L. Cox. </author> <title> Numa policies and their relation to memory architecture. </title> <booktitle> In 12 Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 212-221, </pages> <address> Santa Clara CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: These policies have so far been evaluated only in the context of small bus-based multiprocessors with uniform memory access times [1, 24, 25]. Substantial work on page migration policies has also been done <ref> [4, 14] </ref>, but the target of this work has been NUMA machines that are not cache coherent, like the BBN Butterfly. These earlier results from small-scale bus-based machines and non-cache-coherent NUMA machines, unfortunately, are difficult to extrapolate to CC-NUMA multiprocessors. <p> To avoid this ping-ponging effect, pages are usually frozen in memory after a certain number of migrations, and defrosted after a timeout period so that they are again eligible for migration. Several variations of this basic strategy have been studied <ref> [4, 14] </ref>, and have reported significant gains. In contrast, we focus on the usefulness of automatic page migration on cache-coherent NUMA machines. Furthermore, while page migration is most beneficial for pages with high cache miss counts, this information is not available to the operating system on current machines. <p> However, after every reallocation of processors in the space-partitioning schemes, the operating system could potentially move each application's data to memory that is local to it. Several page migration schemes of varying sophistication have been proposed in the literature <ref> [4, 14] </ref>. We performed some initial experiments with a simple extension of our page migration strategy presented earlier for uniprocessor applications (we have not yet attempted page replication in our experiments).
Reference: [5] <author> R. Chandra, A. Gupta, and J. L. Hennessy. </author> <title> Integrating concurrency and data abstraction in the COOL parallel programming language. </title> <journal> IEEE Computer, </journal> <volume> 27(8), </volume> <month> August </month> <year> 1994. </year>
Reference-contexts: As a result the application executes at a more efficient operating point along its speedup curve. Process control is most easily exploited by parallel applications written using the task-queue model of parallelism <ref> [1, 5] </ref>, in which user-level tasks are scheduled onto a number of kernel processes. Adjustments to the number of active processes can therefore be embedded within the runtime system, becoming transparent to the application programmer. <p> potential to exploit data locality, efficiency due to the operating point effect, and on the synchronization behavior of parallel applications. 5.1.1 Data Locality The primary mechanism to improve data locality in a parallel application is to distribute tasks and data so that tasks execute close to the data they reference <ref> [5] </ref>. However, it is common to assume exclusive use of the machine in performing these optimizations, an assumption that is no longer valid in a multiprogrammed environment. Because of its coscheduling property, gang scheduling provides the illusion of an exclusive machine, with each application process assigned to a particular processor. <p> All four applications are written in the Cool <ref> [5] </ref> parallel programming language, an extension of C++ that supports dynamic task-level parallelism. (The applications are originally from the SPLASH [20] suite.) The use of task-level parallelism is an important prerequisite for benefiting from process control scheduling. The Ocean program models eddy currents in an ocean basin.
Reference: [6] <author> M. Crovella. </author> <title> The costs and benefits of coscheduling. </title> <type> Technical report, </type> <institution> University of Rochester Computer Science Department, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: Several scheduling and page migration policies for multiprocessor systems have been explored in the literature. Common scheduling approaches include cache affinity [12, 22], gang scheduling <ref> [6, 9, 10, 19] </ref>, processor sets [3], and process control [26] or scheduler activations [1]. These policies have so far been evaluated only in the context of small bus-based multiprocessors with uniform memory access times [1, 24, 25]. <p> However, this is largely a non-issue for applications using two-phase synchronization (spin for a while and then block); all our applications use two-phase locks. 5.2 Implementation of Scheduling Policies We implement gang scheduling using the matrix method <ref> [6, 7, 25] </ref>, in which rows represent time slices and columns represent processors. When a parallel application starts up, its processes are placed within a single row.
Reference: [7] <author> M. Crovella, P. Das, C. Dubnicki, T. LeBlanc, and E. Markatos. </author> <title> Multiprogramming on multiprocessors. </title> <booktitle> In Proceedings of the Third IEEE Symposium on Parallel and Distributed Computing, </booktitle> <pages> pages 590-597, </pages> <month> Dec. </month> <year> 1991. </year>
Reference-contexts: However, this is largely a non-issue for applications using two-phase synchronization (spin for a while and then block); all our applications use two-phase locks. 5.2 Implementation of Scheduling Policies We implement gang scheduling using the matrix method <ref> [6, 7, 25] </ref>, in which rows represent time slices and columns represent processors. When a parallel application starts up, its processes are placed within a single row.
Reference: [8] <author> M. Devarakonda and A. Mukherjee. </author> <title> Issues in implementation of cache-affinity scheduling. </title> <booktitle> In Proceedings Winter 1992 USENIX Conference, </booktitle> <pages> pages 345-357, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: Cache affinity scheduling [22, 12, 24] attempts to reschedule a process on the processor it last used, thereby reducing process migration and cache interference. The effectiveness of cache affinity has been evaluated through analytical studies [22, 21], simulation [12], and actual implementations <ref> [24, 8, 27] </ref>. Experiments on bus-based multiprocessors [24], however, show that realistic applications achieve only moderate gains (less than 10%) from affinity scheduling. We will show that the gains can be much larger on CC-NUMA machines.
Reference: [9] <author> J. Edler, J. Lipkis, and E. Schonberg. </author> <title> Process management for highly parallel UNIX systems. </title> <booktitle> In Proceedings of the USENIX Workshop on UNIX and Supercomputers, </booktitle> <pages> pages 1-17, </pages> <year> 1988. </year>
Reference-contexts: Several scheduling and page migration policies for multiprocessor systems have been explored in the literature. Common scheduling approaches include cache affinity [12, 22], gang scheduling <ref> [6, 9, 10, 19] </ref>, processor sets [3], and process control [26] or scheduler activations [1]. These policies have so far been evaluated only in the context of small bus-based multiprocessors with uniform memory access times [1, 24, 25].
Reference: [10] <author> D. G. Feitelson and L. Rudolph. </author> <title> Distributed hierarchical control for parallel processing. </title> <journal> IEEE Computer, </journal> <volume> 23(5) </volume> <pages> 65-77, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Several scheduling and page migration policies for multiprocessor systems have been explored in the literature. Common scheduling approaches include cache affinity [12, 22], gang scheduling <ref> [6, 9, 10, 19] </ref>, processor sets [3], and process control [26] or scheduler activations [1]. These policies have so far been evaluated only in the context of small bus-based multiprocessors with uniform memory access times [1, 24, 25].
Reference: [11] <author> A. Gupta, A. Tucker, and L. Stevens. </author> <title> Making effective use of shared-memory multiprocessors: the process control approach. </title> <type> Technical Report CSL-TR-91-475A, </type> <institution> Computer Systems Lab, Stanford University, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: Adjustments to the number of active processes can therefore be embedded within the runtime system, becoming transparent to the application programmer. Evaluations of these policies <ref> [11, 1] </ref> on bus-based machines such as SGI workstations and the DEC Firefly report significant performance improvements that range from 8-22%.
Reference: [12] <author> A. Gupta, A. Tucker, and S. Urushibara. </author> <title> The impact of operating system scheduling policies and synchronization methods on the performance of parallel applications. </title> <booktitle> In Proceedings of SIGMETRICS '91, </booktitle> <pages> pages 120-132, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Several scheduling and page migration policies for multiprocessor systems have been explored in the literature. Common scheduling approaches include cache affinity <ref> [12, 22] </ref>, gang scheduling [6, 9, 10, 19], processor sets [3], and process control [26] or scheduler activations [1]. These policies have so far been evaluated only in the context of small bus-based multiprocessors with uniform memory access times [1, 24, 25]. <p> Furthermore, multiple processes can be time-shared on the same processor, resulting in cache interference. Cache affinity scheduling <ref> [22, 12, 24] </ref> attempts to reschedule a process on the processor it last used, thereby reducing process migration and cache interference. The effectiveness of cache affinity has been evaluated through analytical studies [22, 21], simulation [12], and actual implementations [24, 8, 27]. <p> Cache affinity scheduling [22, 12, 24] attempts to reschedule a process on the processor it last used, thereby reducing process migration and cache interference. The effectiveness of cache affinity has been evaluated through analytical studies [22, 21], simulation <ref> [12] </ref>, and actual implementations [24, 8, 27]. Experiments on bus-based multiprocessors [24], however, show that realistic applications achieve only moderate gains (less than 10%) from affinity scheduling. We will show that the gains can be much larger on CC-NUMA machines. <p> Using processor sets can also ensure that different applications get an equal portion of the machine. (In contrast, in a gang scheduled machine, an application that has more processes gets a larger fraction of the machine.) Such equi-partitioning of resources has been shown to reduce average response time <ref> [16, 12] </ref>. In contrast to previous simulation-based studies, we evaluate the performance of processor sets and its interaction with the NUMA memory hierarchy on a real machine.
Reference: [13] <institution> Kendall Square Research. </institution> <type> KSR1 Technical Summary. </type> <address> Waltham, MA, </address> <year> 1992. </year>
Reference-contexts: 1 Introduction While small-scale shared-memory multiprocessors have been available for a long time [17, 18], large-scale shared-memory machines have only recently become available <ref> [15, 13] </ref>. To achieve scalability these machines have distributed main memory and use a scalable interconnect and directory techniques to provide low-overhead cache-coherent access to shared data.
Reference: [14] <author> R. P. Larowe and C. S. Ellis. </author> <title> Experimental comparison of memory management policies for numa multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(4) </volume> <pages> 319-363, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: These policies have so far been evaluated only in the context of small bus-based multiprocessors with uniform memory access times [1, 24, 25]. Substantial work on page migration policies has also been done <ref> [4, 14] </ref>, but the target of this work has been NUMA machines that are not cache coherent, like the BBN Butterfly. These earlier results from small-scale bus-based machines and non-cache-coherent NUMA machines, unfortunately, are difficult to extrapolate to CC-NUMA multiprocessors. <p> To avoid this ping-ponging effect, pages are usually frozen in memory after a certain number of migrations, and defrosted after a timeout period so that they are again eligible for migration. Several variations of this basic strategy have been studied <ref> [4, 14] </ref>, and have reported significant gains. In contrast, we focus on the usefulness of automatic page migration on cache-coherent NUMA machines. Furthermore, while page migration is most beneficial for pages with high cache miss counts, this information is not available to the operating system on current machines. <p> However, after every reallocation of processors in the space-partitioning schemes, the operating system could potentially move each application's data to memory that is local to it. Several page migration schemes of varying sophistication have been proposed in the literature <ref> [4, 14] </ref>. We performed some initial experiments with a simple extension of our page migration strategy presented earlier for uniprocessor applications (we have not yet attempted page replication in our experiments).
Reference: [15] <author> D. Lenoski, J. Laudon, T. Joe, D. Nakahira, L. Stevens, A. Gupta, and J. L. Hennessy. </author> <title> The DASH prototype: Implementation and performance. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 92-103, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: 1 Introduction While small-scale shared-memory multiprocessors have been available for a long time [17, 18], large-scale shared-memory machines have only recently become available <ref> [15, 13] </ref>. To achieve scalability these machines have distributed main memory and use a scalable interconnect and directory techniques to provide low-overhead cache-coherent access to shared data. <p> These earlier results from small-scale bus-based machines and non-cache-coherent NUMA machines, unfortunately, are difficult to extrapolate to CC-NUMA multiprocessors. In this paper we evaluate scheduling and page migration policies using a recent CC-NUMA multiprocessor, the Stan-ford DASH <ref> [15] </ref>. We target two different multiprogrammed environments|the first consisting primarily of sequential applications, and the second consisting of parallel applications. We have modified the kernel to implement several scheduling policies and a simple page migration policy, and we perform a detailed evaluation of the effectiveness of each. <p> We therefore also explore the effectiveness of using TLB miss counts as an approximation to cache miss counts for making page migration decisions. 3 Experimental Environment Our experiments are performed on a directory-based CC-NUMA multiprocessor, the Stanford DASH <ref> [15] </ref>. We use a machine with sixteen 33MHz MIPS R3000 processors organized into four clusters, with each cluster containing four processors and some physical memory (56 MB each). Each processor has a 64 KB first level cache and a 256 KB second-level cache. <p> Since both context switch and page allocation are sufficiently expensive operations, maintaining these counters incurs negligible overhead. We ran each experiment three times, and present results from the median run. Finally, we used the hardware performance monitor on DASH <ref> [15] </ref> to monitor the bus and network activity in a nonintrusive manner. For example, we tracked the number of cache misses to local and remote memories for each of the processors.
Reference: [16] <author> S. T. Leutenegger and M. K. Vernon. </author> <title> The performance of multipro-grammed multiprocessor scheduling policies. </title> <booktitle> In Proceedings of the 1990 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 226-236, </pages> <address> Boulder CO, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: Using processor sets can also ensure that different applications get an equal portion of the machine. (In contrast, in a gang scheduled machine, an application that has more processes gets a larger fraction of the machine.) Such equi-partitioning of resources has been shown to reduce average response time <ref> [16, 12] </ref>. In contrast to previous simulation-based studies, we evaluate the performance of processor sets and its interaction with the NUMA memory hierarchy on a real machine.
Reference: [17] <author> T. Lovett and S. Thakkar. </author> <title> The Symmetry Multiprocessor System. </title> <booktitle> In Proceedings of the 1988 International Conference on Parallel Processing, </booktitle> <pages> pages I:303-310, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: 1 Introduction While small-scale shared-memory multiprocessors have been available for a long time <ref> [17, 18] </ref>, large-scale shared-memory machines have only recently become available [15, 13]. To achieve scalability these machines have distributed main memory and use a scalable interconnect and directory techniques to provide low-overhead cache-coherent access to shared data.
Reference: [18] <institution> Multimax technical summary. </institution> <type> Technical report, </type> <institution> Encore Computer Corporation, </institution> <year> 1986. </year>
Reference-contexts: 1 Introduction While small-scale shared-memory multiprocessors have been available for a long time <ref> [17, 18] </ref>, large-scale shared-memory machines have only recently become available [15, 13]. To achieve scalability these machines have distributed main memory and use a scalable interconnect and directory techniques to provide low-overhead cache-coherent access to shared data.
Reference: [19] <author> J. K. Ousterhout. </author> <title> Scheduling techniques for concurrent systems. </title> <booktitle> In 3rd International Conference on Distributed Computing Systems, </booktitle> <pages> pages 22-30, </pages> <year> 1982. </year>
Reference-contexts: Several scheduling and page migration policies for multiprocessor systems have been explored in the literature. Common scheduling approaches include cache affinity [12, 22], gang scheduling <ref> [6, 9, 10, 19] </ref>, processor sets [3], and process control [26] or scheduler activations [1]. These policies have so far been evaluated only in the context of small bus-based multiprocessors with uniform memory access times [1, 24, 25]. <p> Experiments on bus-based multiprocessors [24], however, show that realistic applications achieve only moderate gains (less than 10%) from affinity scheduling. We will show that the gains can be much larger on CC-NUMA machines. Gang Scheduling: For parallel applications, Ousterhout <ref> [19] </ref> proposed the gang scheduling or coscheduling approach in which all the processes of a parallel application are scheduled to run at the same time. This improves the synchronization and communication behavior of applications, particularly those using busy-wait synchronization.
Reference: [20] <author> J. P. Singh, W.-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford parallel applications for shared memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <year> 1992. </year>
Reference-contexts: All four applications are written in the Cool [5] parallel programming language, an extension of C++ that supports dynamic task-level parallelism. (The applications are originally from the SPLASH <ref> [20] </ref> suite.) The use of task-level parallelism is an important prerequisite for benefiting from process control scheduling. The Ocean program models eddy currents in an ocean basin. The main data structures in Ocean are several matrices, and the basic operations are very regular, such as adding two matrices.
Reference: [21] <author> M. Squillante and R. Nelson. </author> <title> Analysis of task migration in shared-memory multiprocessor scheduling. </title> <booktitle> In Proceedings of SIGMETRICS '91, </booktitle> <pages> pages 143-155, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Furthermore, multiple processes can be time-shared on the same processor, resulting in cache interference. Cache affinity scheduling [22, 12, 24] attempts to reschedule a process on the processor it last used, thereby reducing process migration and cache interference. The effectiveness of cache affinity has been evaluated through analytical studies <ref> [22, 21] </ref>, simulation [12], and actual implementations [24, 8, 27]. Experiments on bus-based multiprocessors [24], however, show that realistic applications achieve only moderate gains (less than 10%) from affinity scheduling. We will show that the gains can be much larger on CC-NUMA machines.
Reference: [22] <author> M. S. Squillante and E. D. Lazowska. </author> <title> Using processor-cache affinity in shared-memory multiprocessor scheduling. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(2) </volume> <pages> 131-143, </pages> <month> Feb. </month> <year> 1993. </year>
Reference-contexts: Several scheduling and page migration policies for multiprocessor systems have been explored in the literature. Common scheduling approaches include cache affinity <ref> [12, 22] </ref>, gang scheduling [6, 9, 10, 19], processor sets [3], and process control [26] or scheduler activations [1]. These policies have so far been evaluated only in the context of small bus-based multiprocessors with uniform memory access times [1, 24, 25]. <p> Furthermore, multiple processes can be time-shared on the same processor, resulting in cache interference. Cache affinity scheduling <ref> [22, 12, 24] </ref> attempts to reschedule a process on the processor it last used, thereby reducing process migration and cache interference. The effectiveness of cache affinity has been evaluated through analytical studies [22, 21], simulation [12], and actual implementations [24, 8, 27]. <p> Furthermore, multiple processes can be time-shared on the same processor, resulting in cache interference. Cache affinity scheduling [22, 12, 24] attempts to reschedule a process on the processor it last used, thereby reducing process migration and cache interference. The effectiveness of cache affinity has been evaluated through analytical studies <ref> [22, 21] </ref>, simulation [12], and actual implementations [24, 8, 27]. Experiments on bus-based multiprocessors [24], however, show that realistic applications achieve only moderate gains (less than 10%) from affinity scheduling. We will show that the gains can be much larger on CC-NUMA machines.
Reference: [23] <author> P. Stenstrom, T. Joe, and A. Gupta. </author> <title> Comparative performance evaluation of cache-coherent NUMA and COMA architectures. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 80-91, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Affinity scheduling helps improve cache reuse, and can be successfully combined with OS page migration strategies to bring the remote pages into local memory (providing COMA <ref> [23] </ref> style benefits).
Reference: [24] <author> J. Torrellas, A. Tucker, and A. Gupta. </author> <title> Evaluating the benefits of cache-affinity scheduling in shared-memory multiprocessors. </title> <type> Technical Report CSL-TR-92-536, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <month> Aug. </month> <year> 1992. </year> <booktitle> Published in short form in the Proceedings of SIGMETRICS '93, </booktitle> <pages> pages 272-274, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Common scheduling approaches include cache affinity [12, 22], gang scheduling [6, 9, 10, 19], processor sets [3], and process control [26] or scheduler activations [1]. These policies have so far been evaluated only in the context of small bus-based multiprocessors with uniform memory access times <ref> [1, 24, 25] </ref>. Substantial work on page migration policies has also been done [4, 14], but the target of this work has been NUMA machines that are not cache coherent, like the BBN Butterfly. <p> Furthermore, multiple processes can be time-shared on the same processor, resulting in cache interference. Cache affinity scheduling <ref> [22, 12, 24] </ref> attempts to reschedule a process on the processor it last used, thereby reducing process migration and cache interference. The effectiveness of cache affinity has been evaluated through analytical studies [22, 21], simulation [12], and actual implementations [24, 8, 27]. <p> Cache affinity scheduling [22, 12, 24] attempts to reschedule a process on the processor it last used, thereby reducing process migration and cache interference. The effectiveness of cache affinity has been evaluated through analytical studies [22, 21], simulation [12], and actual implementations <ref> [24, 8, 27] </ref>. Experiments on bus-based multiprocessors [24], however, show that realistic applications achieve only moderate gains (less than 10%) from affinity scheduling. We will show that the gains can be much larger on CC-NUMA machines. <p> Cache affinity scheduling [22, 12, 24] attempts to reschedule a process on the processor it last used, thereby reducing process migration and cache interference. The effectiveness of cache affinity has been evaluated through analytical studies [22, 21], simulation [12], and actual implementations [24, 8, 27]. Experiments on bus-based multiprocessors <ref> [24] </ref>, however, show that realistic applications achieve only moderate gains (less than 10%) from affinity scheduling. We will show that the gains can be much larger on CC-NUMA machines. <p> These affinity policies work together to improve both cache and memory locality in a CC-NUMA machine like DASH. We base our implementation of affinity scheduling on the traditional priority mechanism in Unix <ref> [24] </ref>, in which the priority of a process is decreased as it accumulates CPU time (one point for every 20ms of execution time). We implemented affinity scheduling through temporary boosts in the priority of desirable processes.
Reference: [25] <author> A. Tucker. </author> <title> Efficient Scheduling on Multiprogrammed Shared-Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Stanford University, </institution> <month> November </month> <year> 1993. </year> <note> Technical Report CSL-TR-94-601. </note>
Reference-contexts: The scheduler faces complex choices in deciding whether to migrate processes or data in order to improve locality. Given these characteristics, it is hardly surprising that the scheduling policies of systems with uniform memory access times perform poorly in scalable multiprocessor environments <ref> [25] </ref>, and that new scheduling and page migration policies are needed. Several scheduling and page migration policies for multiprocessor systems have been explored in the literature. <p> Common scheduling approaches include cache affinity [12, 22], gang scheduling [6, 9, 10, 19], processor sets [3], and process control [26] or scheduler activations [1]. These policies have so far been evaluated only in the context of small bus-based multiprocessors with uniform memory access times <ref> [1, 24, 25] </ref>. Substantial work on page migration policies has also been done [4, 14], but the target of this work has been NUMA machines that are not cache coherent, like the BBN Butterfly. <p> Experimental evaluations of gang scheduling have found only limited gains (ranging from -5 to 15% <ref> [25] </ref>) for multiprogrammed work-loads. However, these studies have been performed on bus-based machines; we explore the usefulness of the gang-scheduling approach on CC-NUMA machines. Processor Sets: In contrast to the above time-multiplexing approaches, the processor sets technique [3] space partitions the machine. <p> However, this is largely a non-issue for applications using two-phase synchronization (spin for a while and then block); all our applications use two-phase locks. 5.2 Implementation of Scheduling Policies We implement gang scheduling using the matrix method <ref> [6, 7, 25] </ref>, in which rows represent time slices and columns represent processors. When a parallel application starts up, its processes are placed within a single row. <p> Details of the implementation correspond closely to those in Tucker's thesis <ref> [25] </ref>. Table 4: The parallel applications used in the controlled experiments and their standalone running times. Appl. Description Time (16 procs) Ocean Eddy and boundary currents in an ocean basin. 40.9s Input: 192x192 grid. Water N-body molecular dynamics application. 29.4s Input: 512 molecules.
Reference: [26] <author> A. Tucker and A. Gupta. </author> <title> Process control and scheduling issues for multiprogrammed shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 159-166, </pages> <year> 1989. </year>
Reference-contexts: Several scheduling and page migration policies for multiprocessor systems have been explored in the literature. Common scheduling approaches include cache affinity [12, 22], gang scheduling [6, 9, 10, 19], processor sets [3], and process control <ref> [26] </ref> or scheduler activations [1]. These policies have so far been evaluated only in the context of small bus-based multiprocessors with uniform memory access times [1, 24, 25]. <p> A parallel application therefore executes more efficiently (i.e., achieves better processor utilization) with fewer processors. This is labeled the operating point effect. The process control/scheduler activations approaches <ref> [26, 1] </ref> are an extension of processor sets in which a parallel application dynamically adjusts its number of active processes to match the number of physical processors assigned to its processor set. As a result the application executes at a more efficient operating point along its speedup curve.
Reference: [27] <author> R. Vaswani and J. Zahorjan. </author> <title> The implications of cache affinity on processor scheduling for multiprogrammed, shared memory multiprocessors. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 26-40, </pages> <address> Pacific Grove CA, </address> <month> Oc-tober </month> <year> 1991. </year> <month> 13 </month>
Reference-contexts: Cache affinity scheduling [22, 12, 24] attempts to reschedule a process on the processor it last used, thereby reducing process migration and cache interference. The effectiveness of cache affinity has been evaluated through analytical studies [22, 21], simulation [12], and actual implementations <ref> [24, 8, 27] </ref>. Experiments on bus-based multiprocessors [24], however, show that realistic applications achieve only moderate gains (less than 10%) from affinity scheduling. We will show that the gains can be much larger on CC-NUMA machines.
References-found: 27

