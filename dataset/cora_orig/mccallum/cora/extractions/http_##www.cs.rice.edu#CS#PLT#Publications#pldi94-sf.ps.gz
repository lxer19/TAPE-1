URL: http://www.cs.rice.edu/CS/PLT/Publications/pldi94-sf.ps.gz
Refering-URL: http://www.cs.rice.edu/CS/PLT/Publications/
Root-URL: 
Title: Is Continuation-Passing Useful for Data Flow Analysis?  
Author: Amr Sabry Matthias Felleisen 
Address: Houston, TX 77251-1892  
Affiliation: Department of Computer Science Rice University  
Abstract: The widespread use of the continuation-passing style (CPS) transformation in compilers, optimizers, abstract interpreters, and partial evaluators reflects a common belief that the transformation has a positive effect on the analysis of programs. Investigations by Nielson [13] and Burn/Filho [5, 6] support, to some degree, this belief with theoretical results. However, they do not pinpoint the source of increased abstract information and do not explain the observation of many people that continuation-passing confuses some conventional data flow analyses. To study the impact of the CPS transformation on program analysis, we derive three canonical data flow analyzers for the core of an applicative higher-order programming language. The first analyzer is based on a direct semantics of the language, the second on a continuation-semantics of the language, and the last on the direct semantics of CPS terms. All analyzers compute the control flow graph of the source program and hence our results apply to a large class of data flow analyses. A comparison of the information gathered by our analyzers establishes the following points: 1. The results of a direct analysis of a source program 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Appel, A. </author> <title> Compiling with Continuations. </title> <publisher> Cam-bridge University Press (1992). </publisher>
Reference-contexts: these results, we argue that, in practice, a direct data flow analysis that relies on some amount of duplication would be as satisfactory as a CPS analysis. 1 Compiling with CPS Many compilers for higher-order applicative languages (Scheme, ML, Common Lisp) map source programs to programs in continuation-passing style (CPS) <ref> [1, 10, 11, 17] </ref>. Compiler writers believe that the intermediate representation based on CPS eases the production of code and facilitates optimizations.
Reference: [2] <author> Bondorf, A. </author> <title> Improving binding times without explicit CPS-conversion. </title> <booktitle> In Proceedings of the ACM Conference on Lisp and Functional Programming (1992) 1-10. </booktitle>
Reference-contexts: Compiler writers believe that the intermediate representation based on CPS eases the production of code and facilitates optimizations. Numerous people also argue that the CPS transformation increases the precision of the data flow analysis that is necessary for advanced optimizations <ref> [2, 3, 5, 6, 16] </ref>. 1 Even though CPS programs are widely accepted as an advantageous intermediate representation, few compiler writers can pinpoint the advantages of the CPS representation over other intermediate representations. 1 Researchers often express this view in informal discussions as opposed to formal papers.
Reference: [3] <author> Consel, C. and Danvy, O. </author> <title> For a better support of static data flow. </title> <booktitle> In Proceedings of the Conference on Functional Programming Languages and Computer Architecture (1991) 496-519. </booktitle>
Reference-contexts: Compiler writers believe that the intermediate representation based on CPS eases the production of code and facilitates optimizations. Numerous people also argue that the CPS transformation increases the precision of the data flow analysis that is necessary for advanced optimizations <ref> [2, 3, 5, 6, 16] </ref>. 1 Even though CPS programs are widely accepted as an advantageous intermediate representation, few compiler writers can pinpoint the advantages of the CPS representation over other intermediate representations. 1 Researchers often express this view in informal discussions as opposed to formal papers.
Reference: [4] <author> Cousot, P. and Cousot, R. </author> <title> Abstract interpretation: A unified lattice model for static analysis of programs by construction of approximation of fixpoints. </title> <booktitle> In Conference Record of the 4th ACM Symposium on Principles of Programming Languages (1977) 238-252. </booktitle>
Reference-contexts: c : (Val fi Ans) ! Ans hP; [x := new (x)]; s [new (x) := u]i M c A hhco x; P; i; hu; sii appr c A hstop; Ai appr c A 4 Constant Propagation by Ab stract Interpretation Using well-known ideas from the area of abstract interpretation <ref> [4, 8, 13, 16] </ref>, we now derive a data flow analyzer from each of the three interpreters.
Reference: [5] <author> Filho, J. Muylaert. </author> <title> Improving abstract interpretations with CPS-translation. (1993). </title> <type> Unpublished Manuscript. </type>
Reference-contexts: Compiler writers believe that the intermediate representation based on CPS eases the production of code and facilitates optimizations. Numerous people also argue that the CPS transformation increases the precision of the data flow analysis that is necessary for advanced optimizations <ref> [2, 3, 5, 6, 16] </ref>. 1 Even though CPS programs are widely accepted as an advantageous intermediate representation, few compiler writers can pinpoint the advantages of the CPS representation over other intermediate representations. 1 Researchers often express this view in informal discussions as opposed to formal papers.
Reference: [6] <author> Filho, J. Muylaert and Burn, G. </author> <title> Continuation passing transformation and abstract interpretation. </title> <editor> In Burn, G., Gay, S., and Ryan, M., editors, </editor> <booktitle> Proceedings of the First Imperial College, Department of Computing, Workshop on Theory and Formal Methods (1993). </booktitle>
Reference-contexts: Compiler writers believe that the intermediate representation based on CPS eases the production of code and facilitates optimizations. Numerous people also argue that the CPS transformation increases the precision of the data flow analysis that is necessary for advanced optimizations <ref> [2, 3, 5, 6, 16] </ref>. 1 Even though CPS programs are widely accepted as an advantageous intermediate representation, few compiler writers can pinpoint the advantages of the CPS representation over other intermediate representations. 1 Researchers often express this view in informal discussions as opposed to formal papers. <p> Nielson [13] proved that, for a small imperative language, the semantic-CPS analysis computes the MOP (meet over all paths) solution and the direct analysis computes the less precise MFP (maximum fixed point) solution; Filho and Burn <ref> [6] </ref> improved the abstract interpretations of typed call-by-name languages using the CPS transformation. Our result shows that the gain in all cases is entirely due to the duplication of the analysis of the continuation along different execution paths.
Reference: [7] <author> Flanagan, C., Sabry, A., Duba, B. F., and Felleisen, M. </author> <title> The essence of compiling with continuations. </title> <booktitle> In Proceedings of the ACM Sigplan Conference on Programming Language Design and Implementation (1993) 237-247. </booktitle>
Reference-contexts: The code generation phase of a non-optimizing CPS compiler only requires that the intermediate representation be normalized according to simple transformations. It is unnecessary to perform a full CPS translation <ref> [7] </ref>. Prompted by conversations with G. Burn [July 92] and with H. Boehm [August 92], and by the observation that the CPS transformation obscures some obvious properties of programs, we started to investigate the exact effect of the CPS program representation on the data flow analysis of programs. <p> For example, the code fragment (f (let (x 1) (g x))) becomes: (let (x 1 1) (let (x 2 (g x 1 )) (let (x 3 (f x 2 )) x 3 ))): In general, the normalization process uses the reductions that we identified in previous work as the A-reductions <ref> [7, 14] </ref>. 2 The semantics of the restricted subset of fl is specified by the two predicates M and app defined in Figure 1. It is straightforward to show that M is a partial function from terms, environments, and stores to answers. <p> The evaluation of the body of the procedure N 1 may itself push frames on the control stack. Thus the continuation can in general be represented as a list of frames where each frame consists of an eval uation context and an environment <ref> [7] </ref>: = hE 1 ; 1 i :: :: nil where E i = (let (x i [ ]) M i ) 3.2 Semantic-CPS Transformation The CPS interpreter (see Figure 2) maps expressions, environments, continuations, and stores to answers. <p> k [[(let (x (if0 V 0 M 1 M 2 )) M )]] = V : fl (V ) ! cps (fl)(W ) V [[x]] = x V [[sub1]] = sub1k The evaluation for CPS programs is defined by M c , a specialized version of the direct interpreter M <ref> [7] </ref>. It handles procedures of two arguments and manipulates a larger set of run-time values than the direct interpreter that includes continuations of the form hco x; P; i (see Figure 3).
Reference: [8] <author> Hudak, P. and Young, J. </author> <title> Collecting interpretations of expressions. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13, </volume> <month> 2 (April </month> <year> 1991) </year> <month> 269-290. </month>
Reference-contexts: Intuitively, the first phase of A-normalization gives every subexpression a name to which the data flow analyzer can associate information about the expression. Without A-normalization, the analyzer would typically associate a "label" with every expression and attach the information about each expression at the corresponding label <ref> [8, 13, 16] </ref>. The two treatments are identical but the replacement of labels by variables simplifies the analyzers. The second phase of A-normalization re-orders the expressions to reflect the order in which the interpreters will traverse them. <p> c : (Val fi Ans) ! Ans hP; [x := new (x)]; s [new (x) := u]i M c A hhco x; P; i; hu; sii appr c A hstop; Ai appr c A 4 Constant Propagation by Ab stract Interpretation Using well-known ideas from the area of abstract interpretation <ref> [4, 8, 13, 16] </ref>, we now derive a data flow analyzer from each of the three interpreters.
Reference: [9] <author> Kam, J.B. and Ullman, J. D. </author> <title> Monotone data flow analysis frameworks. </title> <journal> Acta Informatica, </journal> <month> 7 </month> <year> (1977) </year> <month> 305-317. </month>
Reference-contexts: Since these infinite chains may cause the analysis to diverge, we approximate sets of numbers to abstract numbers <ref> [9] </ref>: ; = ?; fng = n; and fn 1 ; n 2 ; . . .g = &gt;: At this point, the universe of abstract values consists of abstract numbers and abstract closures. <p> values similar to the order on collected values that coincides with the relation "is more precise than." For the direct and semantic-CPS interpreters, we organize the abstract values in a lattice that is the product of two lattices: the first is the traditional lattice N &gt; ? for constant propagation <ref> [9] </ref>, and the second is the power set of abstract closures (ordered by the subset relation) for control flow analysis [16]. The ordering relation v and the least upper bound operation t are defined component-wise. <p> all , A i , and n: h; i=1;n A i i appr A f iff h; A 1 i appr B 1 . . . h; A n i appr B n and A f = i=1;n B n : When Distributivity does not hold, e.g., for constant propagation <ref> [9] </ref>, the semantic-CPS data flow analyzer may gain information by duplicating the continuation along every execution path as in the right hand side of the condition. Otherwise, the results of the analyses are identical. <p> In the semantic-CPS case, the computation of F i=0;1 B i is undecidable. The proof is an adaptation of Kam and Ullman's proof <ref> [9] </ref> that it is undecidable to compute the MOP solution for a general program in an arbitrary monotone framework. 6.3 Conclusion In conclusion, a practical analysis based on the CPS transformation should not perform any duplication when the analysis is distributive since the duplication would not yield more precise answers.
Reference: [10] <author> Kelsey, R. and Hudak, P. </author> <title> Realistic compilation by program transformation. </title> <booktitle> In Conference Record of the 16th ACM Symposium on Principles of Programming Languages (1989) 281-292. </booktitle>
Reference-contexts: these results, we argue that, in practice, a direct data flow analysis that relies on some amount of duplication would be as satisfactory as a CPS analysis. 1 Compiling with CPS Many compilers for higher-order applicative languages (Scheme, ML, Common Lisp) map source programs to programs in continuation-passing style (CPS) <ref> [1, 10, 11, 17] </ref>. Compiler writers believe that the intermediate representation based on CPS eases the production of code and facilitates optimizations.
Reference: [11] <author> Kranz, D., Kelsey, R., Rees, J., Hudak, P., Philbin, J., and Adams, N. </author> <title> Orbit: An optimizing compiler for Scheme. </title> <booktitle> In Proceedings of the ACM Sigplan Symposium on Compiler Construction, Sigplan Notices, </booktitle> <volume> 21, </volume> <month> 7 </month> <year> (1986) </year> <month> 219-233. </month>
Reference-contexts: these results, we argue that, in practice, a direct data flow analysis that relies on some amount of duplication would be as satisfactory as a CPS analysis. 1 Compiling with CPS Many compilers for higher-order applicative languages (Scheme, ML, Common Lisp) map source programs to programs in continuation-passing style (CPS) <ref> [1, 10, 11, 17] </ref>. Compiler writers believe that the intermediate representation based on CPS eases the production of code and facilitates optimizations.
Reference: [12] <author> Marlowe, T.J. and Ryder, B.G. </author> <title> Properties of data flow frameworks: A unified model. </title> <note> Acta Informat-ica (1990). </note>
Reference: [13] <author> Nielson, F. </author> <title> A denotational framework for data flow analysis. </title> <journal> Acta Informatica, </journal> <month> 18 </month> <year> (1982) </year> <month> 265-287. </month>
Reference-contexts: Intuitively, the first phase of A-normalization gives every subexpression a name to which the data flow analyzer can associate information about the expression. Without A-normalization, the analyzer would typically associate a "label" with every expression and attach the information about each expression at the corresponding label <ref> [8, 13, 16] </ref>. The two treatments are identical but the replacement of labels by variables simplifies the analyzers. The second phase of A-normalization re-orders the expressions to reflect the order in which the interpreters will traverse them. <p> c : (Val fi Ans) ! Ans hP; [x := new (x)]; s [new (x) := u]i M c A hhco x; P; i; hu; sii appr c A hstop; Ai appr c A 4 Constant Propagation by Ab stract Interpretation Using well-known ideas from the area of abstract interpretation <ref> [4, 8, 13, 16] </ref>, we now derive a data flow analyzer from each of the three interpreters. <p> In contrast, the analysis of the source program and the semantic-CPS analysis do not collect continuations, but only consider the current continuation at any program point. 6.2 Duplication The gain of information in semantic-CPS analyzers is folklore knowledge. Nielson <ref> [13] </ref> proved that, for a small imperative language, the semantic-CPS analysis computes the MOP (meet over all paths) solution and the direct analysis computes the less precise MFP (maximum fixed point) solution; Filho and Burn [6] improved the abstract interpretations of typed call-by-name languages using the CPS transformation.
Reference: [14] <author> Sabry, A. and Felleisen, M. </author> <title> Reasoning about programs in continuation-passing style. </title> <booktitle> Lisp and Symbolic Computation, 6, 3/4 (1993) 289-360. Also in Proceedings of the ACM Conference on Lisp and Functional Programming, </booktitle> <year> 1992, </year> <type> and Technical Report 92-180, </type> <institution> Rice University. </institution>
Reference-contexts: We discussed and re-confirmed this idea with, among others, Charles Consel, and Olivier Danvy at LFP '92 [June 92], following the presentation of our paper on equational reasoning about programs in CPS <ref> [14] </ref>; in an email exchange with Geoffrey Burn and Juarez Filho [July 92]; in further discussions at POPL '93 with Daniel Weise; in email discussions with Kelsey [July 93] and Shivers [May 93]; and in discussions with Burn at FPCA '93 [June 93]. <p> The theory of equational manipulations of programs in CPS based on the fi rule of the lambda calculus has a simple counterpart for source programs; in other words, whatever optimizations are expressible via fi-steps on CPS programs, can equally well be formulated for source pro grams <ref> [14] </ref>. 2. The code generation phase of a non-optimizing CPS compiler only requires that the intermediate representation be normalized according to simple transformations. It is unnecessary to perform a full CPS translation [7]. Prompted by conversations with G. Burn [July 92] and with H. <p> For example, the code fragment (f (let (x 1) (g x))) becomes: (let (x 1 1) (let (x 2 (g x 1 )) (let (x 3 (f x 2 )) x 3 ))): In general, the normalization process uses the reductions that we identified in previous work as the A-reductions <ref> [7, 14] </ref>. 2 The semantics of the restricted subset of fl is specified by the two predicates M and app defined in Figure 1. It is straightforward to show that M is a partial function from terms, environments, and stores to answers.
Reference: [15] <author> Sabry, A. and Felleisen, M. </author> <title> Is Continuation-Passing Useful for Data Flow Analysis? Technical Report TR94-223, </title> <institution> Rice University (1994). </institution>
Reference-contexts: Section 5 presents all the formal theorems, which are discussed from a practical perspective in Section 6. For full proofs, we refer the reader to the technical report version of our paper <ref> [15] </ref>. 2 fl: Syntax and Semantics Our source language is a simple extension of the language fl of the -calculus. It corresponds to the core of typical higher-order languages like Scheme, Lisp, and ML.
Reference: [16] <author> Shivers, O. </author> <title> Control-Flow Analysis of Higher-Order Languages or Taming Lambda. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University (1991). </institution>
Reference-contexts: Compiler writers believe that the intermediate representation based on CPS eases the production of code and facilitates optimizations. Numerous people also argue that the CPS transformation increases the precision of the data flow analysis that is necessary for advanced optimizations <ref> [2, 3, 5, 6, 16] </ref>. 1 Even though CPS programs are widely accepted as an advantageous intermediate representation, few compiler writers can pinpoint the advantages of the CPS representation over other intermediate representations. 1 Researchers often express this view in informal discussions as opposed to formal papers. <p> Intuitively, the first phase of A-normalization gives every subexpression a name to which the data flow analyzer can associate information about the expression. Without A-normalization, the analyzer would typically associate a "label" with every expression and attach the information about each expression at the corresponding label <ref> [8, 13, 16] </ref>. The two treatments are identical but the replacement of labels by variables simplifies the analyzers. The second phase of A-normalization re-orders the expressions to reflect the order in which the interpreters will traverse them. <p> c : (Val fi Ans) ! Ans hP; [x := new (x)]; s [new (x) := u]i M c A hhco x; P; i; hu; sii appr c A hstop; Ai appr c A 4 Constant Propagation by Ab stract Interpretation Using well-known ideas from the area of abstract interpretation <ref> [4, 8, 13, 16] </ref>, we now derive a data flow analyzer from each of the three interpreters. <p> One of the simple approximations, known as 0CFA analy sis <ref> [16] </ref>, is to associate one location for each variable and to collect all the values to which the variable is bound at that location. <p> and semantic-CPS interpreters, we organize the abstract values in a lattice that is the product of two lattices: the first is the traditional lattice N &gt; ? for constant propagation [9], and the second is the power set of abstract closures (ordered by the subset relation) for control flow analysis <ref> [16] </ref>. The ordering relation v and the least upper bound operation t are defined component-wise. It is easy to check that, if S 1 and S 2 are sets of collected values, then S 1 S 2 implies S 1 v S 2 . <p> In the remainder of this section, we discuss each of the properties of the CPS analyzers in detail. 6.1 False Returns In practice, many analyses do indeed confuse continuations when applied to CPS programs. For example, Shivers's 0CFA analysis of CPS programs <ref> [16] </ref> merges distinct control paths unnecessarily. Shivers did not relate the problem to CPS but his example [16:p.33] is essentially the example for Theorem 5.1. Given our result, we can explain how the CPS transformation confuses some data flow analyzers that associate (approximate) information with program points.
Reference: [17] <author> Steele, G. L. Rabbit: </author> <title> A Compiler for Scheme. MIT AI Memo 474, </title> <institution> Massachusetts Institute of Technology (1978). </institution> <note> Page 12 </note>
Reference-contexts: these results, we argue that, in practice, a direct data flow analysis that relies on some amount of duplication would be as satisfactory as a CPS analysis. 1 Compiling with CPS Many compilers for higher-order applicative languages (Scheme, ML, Common Lisp) map source programs to programs in continuation-passing style (CPS) <ref> [1, 10, 11, 17] </ref>. Compiler writers believe that the intermediate representation based on CPS eases the production of code and facilitates optimizations.
References-found: 17

